---
ver: rpa2
title: Noise Balance and Stationary Distribution of Stochastic Gradient Descent
arxiv_id: '2308.06671'
source_url: https://arxiv.org/abs/2308.06671
tags:
- distribution
- learning
- stationary
- when
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the stationary distribution of stochastic
  gradient descent (SGD) for deep linear networks. The authors prove a "law of balance"
  showing that SGD regularizes solutions towards a balanced state when the loss function
  has a rescaling symmetry.
---

# Noise Balance and Stationary Distribution of Stochastic Gradient Descent

## Quick Facts
- arXiv ID: 2308.06671
- Source URL: https://arxiv.org/abs/2308.06671
- Reference count: 40
- Primary result: Derives exact stationary distribution of SGD for deep diagonal linear networks showing power-law tails that thin with depth and exhibiting fluctuation inversion

## Executive Summary
This paper provides the first analytical expression for the stationary distribution of stochastic gradient descent in a highly nonlinear, nonconvex setting without approximations. The authors prove a "law of balance" showing that SGD regularizes solutions toward a balanced state when the loss function has rescaling symmetry. For deep diagonal linear networks, they derive exact stationary distributions that exhibit complex phenomena including phase transitions, broken ergodicity, and fluctuation inversion, revealing fundamental differences between SGD and gradient flow.

## Method Summary
The paper uses a continuous-time approximation of SGD, converting discrete updates to stochastic differential equations. The authors apply Ito calculus to derive the Fokker-Planck equation describing the evolution of the probability distribution. For diagonal linear networks, they solve this equation analytically to obtain exact stationary distributions. The analysis focuses on the effects of minibatch noise and rescaling symmetry in the loss function, deriving conditions under which SGD exhibits balance and computing how depth affects the stationary distribution's properties.

## Key Results
- SGD exhibits a "law of balance" that drives parameters toward balanced norms when loss functions have rescaling symmetry
- The stationary distribution has power-law tails v^(-5+3/(D+1)) that become thinner as network depth D increases
- SGD shows fluctuation inversion where increased noise can decrease parameter variance in certain regimes
- The stationary distribution exhibits phase transitions and broken ergodicity in deep networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SGD regularizes solutions towards a balanced state when the loss function contains rescaling symmetry
- Mechanism: The noise covariance in SGD creates an effective force that drives the parameter norms toward balance, breaking the conservation law that exists in gradient descent
- Core assumption: The loss function exhibits rescaling symmetry (ℓ(u, w, x) = ℓ(λu, w/λ, x))
- Evidence anchors:
  - [abstract] "SGD regularizes the solution towards a balanced solution whenever the loss function contains a rescaling symmetry"
  - [section 2] "d/dt(‖u‖²-‖w‖²) = -T(uᵀC₁u-wᵀC₂w)" showing the decay of the imbalance term
  - [corpus] Weak evidence - neighbor papers discuss noise equilibrium but not specifically rescaling symmetry
- Break condition: If the loss function does not contain rescaling symmetry, the balance law does not apply

### Mechanism 2
- Claim: The stationary distribution of SGD exhibits power-law tails that become thinner with increasing depth
- Mechanism: The depth-dependent noise structure in deep networks creates a bias toward balanced solutions, affecting the tail behavior of the parameter distribution
- Core assumption: The network architecture has diagonal structure with depth D
- Evidence anchors:
  - [abstract] "power-law tails in the parameter distribution that become thinner with increasing depth"
  - [section 3.3] "p(v) ∝ 1/v³(¹⁻¹/(D+1))(α₁v²-2α₂v+α₃)" showing depth-dependent tail exponent
  - [corpus] Weak evidence - neighbor papers discuss weight fluctuations but not depth-dependent tail behavior
- Break condition: If the network is not diagonal or has depth D=1, the tail behavior changes

### Mechanism 3
- Claim: SGD exhibits fluctuation inversion where increasing noise level can decrease parameter variance
- Mechanism: The complex interaction between noise covariance and parameter dynamics creates regimes where noise injection stabilizes the system rather than destabilizing it
- Core assumption: The noise covariance structure has specific parameter dependence
- Evidence anchors:
  - [abstract] "fluctuation inversion" mentioned as one of the phenomena exhibited by the stationary distribution
  - [section 3.2.1] "the variance of v and u both decrease to zero as we increase T" describing the fluctuation inversion effect
  - [corpus] No direct evidence in neighbor papers
- Break condition: If the noise covariance structure is simplified to be parameter-independent, the fluctuation inversion effect disappears

## Foundational Learning

- Concept: Rescaling symmetry in loss functions
  - Why needed here: This symmetry is the key condition under which the balance law applies, allowing SGD to regularize toward balanced solutions
  - Quick check question: For a two-layer network with loss ℓ(u,w,x) = (uw*x - y)², does the loss satisfy ℓ(u,w,x) = ℓ(λu, w/λ, x)?

- Concept: Stochastic differential equations and Ito calculus
  - Why needed here: The continuous-time approximation of SGD requires understanding how noise affects parameter dynamics through Ito's lemma
  - Quick check question: Given dX = μdt + σdW(t), what is the SDE for Y = X² using Ito's lemma?

- Concept: Stationary distributions of stochastic processes
  - Why needed here: Understanding the stationary distribution is crucial for characterizing SGD's long-term behavior and its differences from gradient flow
  - Quick check question: For the Fokker-Planck equation ∂P/∂t = -∂J/∂x, what condition must J satisfy for P to be a stationary distribution?

## Architecture Onboarding

- Component map:
  - Loss function analysis: Identify rescaling symmetries and compute gradient noise covariance
  - Continuous-time approximation: Convert discrete SGD updates to SDE form
  - Stationary distribution solver: Apply Fokker-Planck equation to find analytical solutions
  - Phase transition detector: Identify critical points where qualitative behavior changes

- Critical path:
  1. Analyze loss function for symmetries
  2. Compute gradient noise covariance matrix
  3. Derive continuous-time SDE
  4. Solve stationary distribution using Fokker-Planck equation
  5. Characterize phase transitions and critical points

- Design tradeoffs:
  - Analytical vs numerical solutions: The paper provides analytical solutions but only for specific architectures
  - Depth vs width effects: Deeper networks show different behaviors than shallow ones, requiring different analysis approaches
  - Noise level vs learning rate: The ratio T = η/S is crucial, creating a tradeoff between exploration and convergence

- Failure signatures:
  - If no rescaling symmetry exists, the balance law won't apply
  - If the noise covariance is oversimplified as constant, key phenomena like fluctuation inversion will be missed
  - If the continuous-time approximation breaks down, the SDE analysis becomes invalid

- First 3 experiments:
  1. Implement the two-layer linear network and verify the balance law numerically by tracking ‖u‖²-‖w‖² over training
  2. Test the fluctuation inversion by training networks with different noise levels and measuring parameter variance
  3. Explore phase transitions by varying the learning rate/batch size ratio and observing changes in stationary distribution shape

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the stationary distribution of SGD change for loss functions with different types of symmetries beyond the rescaling symmetry studied in this work?
- Basis in paper: [explicit] The paper concludes by suggesting that studying SGD's behavior on landscapes with different types of symmetries is a promising direction, as the law of balance was derived specifically for the rescaling symmetry.
- Why unresolved: This work only proves the law of balance for the rescaling symmetry case. Other symmetries could lead to different balance laws or fundamentally different stationary distributions.
- What evidence would resolve it: Analytical or empirical characterization of SGD's stationary distribution for loss functions with other common symmetries (permutation, translation, etc.) would directly address this question.

### Open Question 2
- Question: Can the theoretical predictions about the power-law tails of deeper models (v^(-5+3/(D+1))) be verified experimentally for realistic neural networks?
- Basis in paper: [explicit] The paper derives that the tail of the parameter distribution scales as v^(-5+3/(D+1)) for deep linear networks, becoming thinner with increasing depth, and suggests this explains the edge-of-stability phenomenon.
- Why unresolved: While the theory is derived for deep linear networks, the paper only provides limited experimental verification on simple models like MNIST with fully connected tanh networks.
- What evidence would resolve it: Systematic experiments measuring the tail exponents of parameter distributions in trained deep neural networks across different architectures and depths would test this prediction.

### Open Question 3
- Question: How does the width-to-depth ratio scaling law (d/D ∝ 1/T) derived for deep networks generalize to networks with non-diagonal architectures or nonlinear activations?
- Basis in paper: [explicit] The paper derives that for diagonal deep networks as depth D approaches infinity, the ratio d/D appears simultaneously with 1/T in the stationary distribution, suggesting a scaling law linking learning rate, batch size, width, and depth.
- Why unresolved: This result is proven for the highly simplified case of diagonal linear networks. Real neural networks have different architectures and activations that could modify this relationship.
- What evidence would resolve it: Empirical studies measuring the optimal width-to-depth ratios across different network architectures and training configurations would test whether this scaling law holds more generally.

## Limitations
- The analytical framework relies heavily on the continuous-time approximation of SGD, which may break down for very small batch sizes or when noise-induced effects become dominant
- The assumption of diagonal network architecture, while enabling exact solutions, limits generalizability to practical deep networks with non-diagonal weight structures
- The phase transition analysis depends on specific parameter regimes that may be difficult to achieve in practice

## Confidence

- Stationary distribution derivation: High
- Balance law: High
- Depth-dependent effects: Medium
- Fluctuation inversion: Medium

## Next Checks

1. Numerically verify the balance law by tracking parameter norm differences during SGD training across different batch sizes and learning rates
2. Test the fluctuation inversion effect by systematically varying noise levels in trained networks and measuring parameter variance changes
3. Validate the phase transition predictions by training networks at critical parameter values and observing qualitative changes in the stationary distribution