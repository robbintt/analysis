---
ver: rpa2
title: Masked and Swapped Sequence Modeling for Next Novel Basket Recommendation in
  Grocery Shopping
arxiv_id: '2308.01308'
source_url: https://arxiv.org/abs/2308.01308
tags:
- items
- basket
- masking
- task
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the task of next novel basket recommendation
  (NNBR) in grocery shopping, which focuses on recommending a set of novel items that
  meet a user's preferences in the next basket. The authors propose a bi-directional
  transformer basket recommendation model (BTBR) that learns item-to-item correlations
  across baskets.
---

# Masked and Swapped Sequence Modeling for Next Novel Basket Recommendation in Grocery Shopping

## Quick Facts
- arXiv ID: 2308.01308
- Source URL: https://arxiv.org/abs/2308.01308
- Reference count: 40
- Key outcome: BTBR with select masking and item-swapping achieves significant improvements over existing methods for next novel basket recommendation in grocery shopping

## Executive Summary
This paper addresses next novel basket recommendation (NNBR) in grocery shopping, where the goal is to recommend novel items in a user's next basket. The authors propose BTBR, a bidirectional transformer model that learns item-to-item correlations across baskets, and several masking strategies including item-select masking and basket-level masking. They also introduce an item-basket swapping strategy to enrich item interactions within baskets. Experiments on three grocery datasets demonstrate that BTBR with appropriate masking and swapping strategies significantly outperforms existing methods for NNBR.

## Method Summary
The method uses a bidirectional transformer (BTBR) that processes flattened basket sequences with shared basket position embeddings. Training employs multiple masking strategies: item-level random/select masking (where select masking removes all occurrences of chosen items), basket-level all/explore masking, and joint masking (pretrain-then-finetune). An item-basket swapping strategy randomly moves items between baskets to create augmented training sequences. The model predicts the next basket by computing dot products between output representations and item embeddings.

## Key Results
- BTBR with item-select masking achieves the best performance for NNBR, outperforming baselines by 5-9% on Dunnhumby and Instacart datasets
- Joint masking strategy shows consistent robustness across all three datasets (TaFeng, Dunnhumby, Instacart)
- Item-basket swapping improves performance when used with appropriate swap ratios (0.1-0.2) and hop distances (1-2)
- Proper selection of masking strategy (item-select) and swapping ratio is critical for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BTBR with item-level select masking improves novel item prediction by forcing the model to infer masked items from cross-basket item-to-item correlations rather than relying on item self-repetition.
- Mechanism: Select masking removes all occurrences of randomly chosen items from the flattened sequence, ensuring the masked item is not present elsewhere in the input. The bidirectional transformer must therefore rely on interactions with other items across baskets to predict the masked novel item.
- Core assumption: Item-to-item correlations contain sufficient signal to infer novel items even when no repetition of the target item exists in the sequence.
- Evidence anchors:
  - [abstract] "item-level select masking: a cloze-task loss designed for exploration, in which we first select the items we need to mask and then mask all the occurrences of the selected item"
  - [section] "the proposed item-select masking strategy will remove all occurrences of the same item, which can ensure that the masked items are novel items w.r.t. the remaining masked sequence, and the model has to infer the masked novel item via learning the masked item's relation with other items"
- Break condition: If item-to-item correlations are weak or irrelevant for the novel item prediction task, the model will fail to predict masked items accurately, leading to poor performance.

### Mechanism 2
- Claim: The item swapping strategy enriches item interactions within baskets by allowing items to be moved between baskets, thereby increasing the diversity of co-occurrence patterns the model can learn from.
- Mechanism: Random items are selected and moved to other baskets according to a swap ratio and hop distance. This creates augmented training sequences where items that would not normally appear together in the same basket now do, allowing the transformer to learn richer item-item relationships.
- Core assumption: Item interactions within baskets (rather than strict sequential order) are informative for predicting novel items in the next basket.
- Evidence anchors:
  - [abstract] "an item-basket swapping strategy is proposed to enrich the item interactions within the same baskets"
  - [section] "we propose the item swapping strategy to create augmentations for the BTBR. Specifically, as illustrated in Figure 3, we randomly select items according to a swap ratio and then move them to another basket to enrich the items' interactions within the same basket"
- Break condition: If the original sequential order is highly informative and the swap introduces too much noise, performance will degrade, especially on datasets with strict temporal dependencies like Tafeng.

### Mechanism 3
- Claim: Joint masking (pretrain-then-finetune) provides robustness by first learning general item-to-item correlations through self-supervised item-level masking, then fine-tuning for the supervised basket-level prediction task.
- Mechanism: The model is first trained with item-select masking to capture broad item-item correlations without temporal constraints, then fine-tuned with basket-all masking to respect the sequential structure and predict the complete next basket.
- Core assumption: General item-item correlations learned in the pretraining phase transfer well to the specific next basket prediction task during fine-tuning.
- Evidence anchors:
  - [abstract] "joint masking: a loss that follows the pre-train-then-fine-tune paradigm, in which we first adopt item-level masking for the cloze task, then fine-tune the model using basket-level masking"
  - [section] "The joint masking strategy under the pretrain-then-finetune paradigm is still valuable due to its robustness w.r.t.NNBR task (i.e., it consistently achieves the best performance) on various datasets with different characteristics"
- Break condition: If the pretraining phase learns patterns that are too general or irrelevant to the fine-tuning task, the benefits of joint training may not materialize.

## Foundational Learning

- Concept: Bidirectional transformer architecture with shared basket position embeddings
  - Why needed here: Allows modeling of item-to-item correlations across baskets while maintaining temporal information through basket-level position embeddings rather than item-level order.
  - Quick check question: How does sharing position embeddings across items in the same basket help capture both within-basket and across-basket relationships?

- Concept: Masking strategies for sequential recommendation (random, select, all, explore)
  - Why needed here: Different masking strategies balance between respecting sequential order and learning item correlations; select masking is crucial for the NNBR task to avoid reliance on item repetition.
  - Quick check question: Why does item-select masking outperform item-random masking for novel item prediction?

- Concept: Swapping augmentation for set-based interactions
  - Why needed here: Addresses the uncertainty about whether strict sequential order exists in grocery shopping baskets by enriching within-basket item interactions through controlled perturbations.
  - Quick check question: How does the swap hop parameter control the trade-off between enriching interactions and preserving temporal information?

## Architecture Onboarding

- Component map: Embedding layer → bidirectional transformer layers → prediction layer. Embedding layer uses shared basket position embeddings. Transformer layers learn item-to-item correlations. Prediction layer uses item embeddings as keys for scoring.
- Critical path: Flatten basket sequence → shared position embedding → transformer self-attention → output representations → dot product with item embeddings → softmax prediction
- Design tradeoffs: Item-level masking vs basket-level masking (flexibility vs temporal order); swapping (interaction richness vs noise); shared vs unique position embeddings (efficiency vs granularity)
- Failure signatures: Poor performance on datasets with strict sequential order (overuse of swapping); overfitting on datasets with long sequences (basket-level masking); failure to predict novel items (inadequate select masking)
- First 3 experiments:
  1. Train BTBR with item-random masking on Tafeng dataset to observe overfitting behavior
  2. Train BTBR with item-select masking with varying mask ratios to find optimal ratio
  3. Apply swapping strategy with different swap hops on Dunnhumby dataset to evaluate interaction enrichment vs temporal preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the item-select masking strategy improve performance in other sequential recommendation tasks beyond grocery shopping basket recommendation?
- Basis in paper: [explicit] The paper demonstrates that item-select masking outperforms item-random masking for the NNBR task, and suggests this could be extended to other sequential recommendation scenarios.
- Why unresolved: The paper only evaluates this strategy on grocery shopping datasets. It's unclear if the benefits would generalize to other sequential recommendation domains with different characteristics.
- What evidence would resolve it: Experiments applying item-select masking to diverse sequential recommendation tasks (e.g., music, movies, e-commerce) and comparing its performance against item-random masking.

### Open Question 2
- Question: How do the proposed masking strategies and item-swapping strategy affect the model's ability to recommend novel items over longer time horizons?
- Basis in paper: [inferred] The paper focuses on next-basket recommendation and evaluates performance on short-term predictions. The long-term effects of the masking and swapping strategies on novelty recommendation are not explored.
- Why unresolved: The experiments are limited to predicting the immediate next basket. The paper doesn't investigate how these strategies influence the model's ability to recommend novel items over multiple time steps.
- What evidence would resolve it: Experiments evaluating the proposed strategies on next-basket recommendation tasks with varying time horizons (e.g., 2, 5, 10 steps ahead) and comparing their performance in terms of novelty.

### Open Question 3
- Question: What is the impact of the mask ratio on the model's ability to balance repetition and exploration in recommendations?
- Basis in paper: [explicit] The paper explores the effect of mask ratio on model performance but doesn't specifically analyze its impact on the balance between recommending repeat and novel items.
- Why unresolved: The paper focuses on overall performance metrics (Recall@10, NDCG@10) but doesn't investigate how different mask ratios affect the model's tendency to recommend repeat vs. novel items.
- What evidence would resolve it: Experiments analyzing the distribution of repeat and novel items in the model's recommendations under different mask ratios and identifying the optimal mask ratio for balancing repetition and exploration.

## Limitations
- The effectiveness of joint masking lacks ablation studies comparing its pretraining phase against random initialization
- Item-basket swapping's impact on datasets with strong sequential dependencies (like TaFeng) is not thoroughly examined
- The paper doesn't explore whether the pretraining benefits persist when using larger models or different pretraining objectives

## Confidence
- **High Confidence**: The overall superiority of BTBR over baseline methods for NNBR task (established through consistent improvements across three datasets and four metrics)
- **Medium Confidence**: The effectiveness of item-select masking for novel item prediction (supported by mechanism but lacks direct comparison with alternative masking approaches)
- **Medium Confidence**: The contribution of item-basket swapping to performance gains (mechanism explained but interaction with sequential dependencies needs more exploration)

## Next Checks
1. Conduct ablation studies comparing joint masking pretraining against random initialization with equivalent training time to isolate pretraining benefits
2. Test BTBR with progressive swap hop distances on TaFeng to quantify the trade-off between interaction enrichment and temporal preservation
3. Evaluate whether item-select masking performance holds when novel items appear frequently in historical sequences (beyond the grocery domain)