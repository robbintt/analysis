---
ver: rpa2
title: Neurosymbolic Grounding for Compositional World Models
arxiv_id: '2310.12690'
source_url: https://arxiv.org/abs/2310.12690
tags:
- object
- e-03
- world
- learning
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Cosmos, a neurosymbolic framework for object-centric
  world modeling that achieves compositional generalization (CG) on unseen input scenes.
  The key innovation is using neurosymbolic scene encodings, where each entity is
  represented by a real vector and composable symbols for attributes.
---

# Neurosymbolic Grounding for Compositional World Models

## Quick Facts
- **arXiv ID:** 2310.12690
- **Source URL:** https://arxiv.org/abs/2310.12690
- **Authors:** Not specified in provided content
- **Reference count:** 40
- **Key outcome:** Neurosymbolic framework (Cosmos) achieves compositional generalization on unseen input scenes by automatically extracting symbolic attributes from images using CLIP, outperforming baselines in next-state prediction and downstream planning.

## Executive Summary
This paper introduces Cosmos, a neurosymbolic framework for object-centric world modeling that achieves compositional generalization (CG) on unseen input scenes. The key innovation is using neurosymbolic scene encodings, where each entity is represented by a real vector and composable symbols for attributes. Cosmos employs a neurosymbolic attention mechanism that binds entities to learned interaction rules using symbolic keys and neural queries. Unlike traditional neurosymbolic methods, Cosmos automatically computes symbolic attributes using vision-language foundation models (CLIP). The framework is end-to-end differentiable and demonstrates state-of-the-art performance on two forms of CG in a blocks-pushing domain.

## Method Summary
Cosmos combines slot-based autoencoders with neurosymbolic attention to learn object-centric world models that generalize compositionally. The method extracts entity features using Segment Anything and ResNet-18, then automatically assigns symbolic attributes via CLIP-based attention. Action conditioning and rule selection are performed through permutation-equivariant attention mechanisms. A modular transition network with learned rule encodings updates entity representations based on selected interactions. The entire system is trained end-to-end except for frozen symbolic modules, enabling joint optimization while maintaining compositional generalization properties.

## Key Results
- Cosmos achieves lower next-state prediction error (MSE) compared to AlignedNPS and GNN baselines on scene and object composition tasks
- The framework demonstrates better downstream planning performance using greedy planner with Hungarian distance matching
- Autoencoder reconstruction quality (AE-MSE) and equivariant mean reciprocal rank (Eq.MRR) show superior latent state separation compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Neurosymbolic attention improves compositional generalization by using symbolic keys to select interaction rules that generalize to novel compositions.
- **Mechanism:** The model represents each entity with a neural vector plus symbolic attributes (e.g., color, shape). Rules are encoded as neural modules with symbolic keys. During attention, symbolic keys are matched to entity attributes, allowing the system to select appropriate rules for unseen object compositions while maintaining equivariance to slot permutations.
- **Core assumption:** Symbolic attributes can be automatically extracted from images using vision-language models (CLIP) without manual annotation, and these symbols capture the relevant attributes for dynamics generalization.
- **Evidence anchors:**
  - [abstract] "the framework introduces two new tools: (i) neurosymbolic scene encodings, which represent each entity in a scene using a real vector computed using a neural encoder, as well as a vector of composable symbols describing attributes of the entity, and (ii) a neurosymbolic attention mechanism that binds these entities to learned rules of interaction."
  - [section 3.2] "the symbolic labelling module (SLM) processes an image and a predefined list of attributes. Assuming this list is comprehensive (though not exhaustive), the module employs a pretrained CLIP model to compute attention scores between the image features and each entity encoding."

### Mechanism 2
- **Claim:** Automatic symbol extraction from vision-language models removes the need for manual symbolic labeling while maintaining neurosymbolic benefits.
- **Mechanism:** Instead of manually mapping perceptual inputs to symbols, the model uses CLIP to compute attention scores between image features and attribute descriptors. A Gumbel-softmax operation selects the most aligned attribute, producing a learned latent encoding for each symbol value rather than a discrete one-hot vector.
- **Core assumption:** Vision-language models like CLIP have sufficient semantic understanding to extract meaningful symbolic attributes from object images without additional training.
- **Evidence anchors:**
  - [abstract] "unlike traditional neurosymbolic methods that require representations to be manually mapped to symbols, it computes an entity's symbolic attributes using vision-language foundation models."
  - [section 3.2] "To achieve robust CG in our setting, our representation must be resilient to both object and attribute compositions... we propose describing each entity with a composition of symbol vectors."

### Mechanism 3
- **Claim:** Modular transition models with symbolic attention reduce spurious correlations compared to monolithic GNNs.
- **Mechanism:** Instead of modeling all pairwise interactions simultaneously, the model selects a primary slot, contextual slot, and interaction rule using symbolic attention. This localizes updates to relevant entity pairs, avoiding the accumulation of spurious correlations in sparse interaction domains.
- **Core assumption:** In the target environments, object interactions are sparse enough that modular selection is more effective than modeling all pairwise interactions.
- **Evidence anchors:**
  - [section 3.3] "Monolithic transition models like GNNs and MLPs model every pairwise object interaction, which, in domains with sparse object interactions, leads to accumulating spurious correlations. Our modular transition model addresses this problem by selecting a relevant pairwise interaction and updating the encodings for those objects locally."
  - [section 4] "Results are presented in Table 2. First, we find that COSMOS achieves the best next state prediction performance (MSE) on all benchmarks."

## Foundational Learning

- **Concept:** Slot-based autoencoders for object-centric representation learning
  - Why needed here: The method requires decomposing scenes into individual objects with separate representations to apply different dynamics rules to different object types
  - Quick check question: What is the key assumption that allows slot-based autoencoders to learn object-centric representations, and how does this relate to compositional generalization?

- **Concept:** Attention mechanisms for dynamic routing
  - Why needed here: Attention is used both for action conditioning (aligning actions to slots) and rule selection (matching symbolic attributes to interaction rules), enabling the model to dynamically select appropriate behavior for novel compositions
  - Quick check question: How does the permutation equivariance of attention help COSMOS avoid requiring a canonical ordering of slots?

- **Concept:** Vision-language foundation models for perception-to-symbol grounding
  - Why needed here: CLIP provides the semantic understanding necessary to automatically extract symbolic attributes from raw images, enabling the neurosymbolic approach without manual annotation
  - Quick check question: What is the key difference between using CLIP for symbol extraction versus using it for direct image classification in this context?

## Architecture Onboarding

- **Component map:** Image → Entity Extractor → SLM → Action Attention → Module Selector → MLP Bank → Spatial Decoder → Next Image Prediction
- **Critical path:** Image → Entity Extractor → SLM → Action Attention → Module Selector → MLP Bank → Spatial Decoder → Next Image Prediction
- **Design tradeoffs:**
  - Using Segment Anything provides perfect segmentation but requires an additional foundation model
  - Symbolic attributes enable compositionality but depend on CLIP's semantic understanding
  - Modular transition avoids spurious correlations but adds computational overhead for rule selection
  - End-to-end differentiability enables joint optimization but may propagate errors through the pipeline

- **Failure signatures:**
  - Poor next-state prediction but good autoencoder reconstruction → Issues in the transition model or symbolic grounding
  - Good next-state prediction but poor autoencoder reconstruction → Representation collapse in slot encodings
  - Inconsistent performance across seeds → Sensitivity to initialization or hyperparameter choices
  - Good performance on scene composition but poor on object composition → Insufficient symbolic attribute coverage for shared dynamics

- **First 3 experiments:**
  1. Train with symbolic attributes disabled (pure neural version) to establish baseline performance
  2. Train with fixed symbolic attributes (manually assigned) to test if automatic extraction is necessary
  3. Test with different CLIP models or prompt engineering to evaluate sensitivity to vision-language model choice

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions in the provided content. The limitations section addresses potential concerns about the method's scalability and applicability to more complex domains.

## Limitations

- Automatic symbol extraction through CLIP assumes sufficient semantic understanding of domain attributes, which may fail for specialized domains or novel object types not well-represented in CLIP's training data
- Experimental validation is limited to a single synthetic domain (blocks-pushing with controlled attribute variations), leaving questions about performance in more complex environments
- Downstream planning evaluation uses a relatively simple greedy planner, which may not fully stress-test the quality of the learned world model

## Confidence

**High Confidence**: The architectural design of Cosmos (slot-based autoencoders + neurosymbolic attention + modular transitions) is internally consistent and the proposed mechanisms for compositional generalization are theoretically sound. The empirical results showing improved MSE and planning performance over baselines are robust across multiple seeds.

**Medium Confidence**: The specific implementation details of the symbolic labeling module and their impact on performance. While the paper describes the high-level approach (CLIP-based attention with Gumbel-softmax), the exact implementation choices (prompt engineering, temperature settings, attribute vocabulary) could significantly affect results and are not fully specified.

**Low Confidence**: Generalization claims beyond the evaluated domain. The paper demonstrates compositional generalization within a controlled blocks-pushing environment but provides limited evidence for how the approach would scale to more complex visual scenes or domains with different types of compositional structures.

## Next Checks

1. **Cross-Domain Robustness Test**: Evaluate Cosmos on a completely different compositional generalization benchmark (e.g., CLEVR-CoGenT or a multi-object physics environment) to assess whether CLIP-based symbolic extraction remains effective when object attributes and dynamics differ substantially from the blocks-pushing domain.

2. **Ablation of Vision-Language Model**: Replace CLIP with a weaker/frozen vision model or manual symbolic labeling to quantify the actual contribution of the automatic symbol extraction mechanism versus the neurosymbolic architecture itself. This would help isolate whether the gains come from the symbol extraction or the modular transition approach.

3. **Dense Interaction Scenario**: Test Cosmos in an environment where most objects interact with most other objects (opposite of the sparse interaction assumption). This would reveal whether the modular transition overhead becomes a liability and whether the symbolic attention mechanism can still effectively select relevant interactions in dense regimes.