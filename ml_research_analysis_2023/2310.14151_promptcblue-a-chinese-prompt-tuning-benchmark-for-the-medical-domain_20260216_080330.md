---
ver: rpa2
title: 'PromptCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain'
arxiv_id: '2310.14151'
source_url: https://arxiv.org/abs/2310.14151
tags:
- medical
- llms
- task
- prompt
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents PromptCBLUE, a Chinese biomedical language
  understanding benchmark that addresses the need for a comprehensive multi-task evaluation
  platform for Chinese medical LLMs. The authors re-build the existing CBLUE benchmark
  into a prompt-tuning format, creating a large-scale dataset covering medical entity
  recognition, text classification, natural language inference, dialogue understanding,
  and content generation tasks.
---

# PromptCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain

## Quick Facts
- **arXiv ID**: 2310.14151
- **Source URL**: https://arxiv.org/abs/2310.14151
- **Reference count**: 40
- **Key outcome**: A comprehensive Chinese medical prompt-tuning benchmark that transforms CBLUE tasks into instruction-following format, demonstrating that fine-tuned models significantly outperform commercial APIs on medical language understanding tasks.

## Executive Summary
This paper introduces PromptCBLUE, a large-scale Chinese biomedical language understanding benchmark designed for prompt-tuning evaluation of medical LLMs. The benchmark converts the existing CBLUE tasks into instruction-following generation format, covering entity recognition, text classification, natural language inference, dialogue understanding, and content generation. The evaluation shows that fine-tuned open-source models like Baichuan-13B significantly outperform commercial APIs like GPT-4 on medical tasks, with LoRA emerging as the most effective parameter-efficient fine-tuning method. The benchmark keeps test ground truths private to prevent data leakage during pre-training, ensuring valid evaluation of model generalization.

## Method Summary
The authors transformed the Chinese Biomedical Language Understanding Evaluation (CBLUE) benchmark into a prompt-tuning format called PromptCBLUE. They created instruction-following prompt-response pairs covering five medical task types, limited training samples to 3000-5000 per task, and kept test labels private. The evaluation used 9 Chinese LLMs fine-tuned with various parameter-efficient methods including LoRA, P-tuning, adapters, and AdaLoRA. Performance was measured using task-specific metrics including micro-F1 for information extraction, macro-F1 for classification, and ROUGE-L for generation tasks.

## Key Results
- Fine-tuned open-source models (Baichuan-13B) achieved 71.0% overall score vs. 51.8% for commercial APIs like GPT-4
- LoRA consistently outperformed other PEFT methods across most tasks
- Different PEFT methods should be selected based on training settings (LoRA > P-tuning-v2 for large-scale, P-tuning-v2 > LoRA for few-shot)
- Current open-source models still struggle with long medical text understanding tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark provides comprehensive multi-task evaluation by transforming CBLUE tasks into instruction-following generation format.
- Mechanism: By reformulating traditional NLP tasks into unified prompt-response pairs, the benchmark creates a standardized evaluation framework that tests both task understanding and generation capabilities.
- Core assumption: LLMs can effectively handle diverse biomedical tasks when presented in instruction-following format with clear output specifications.
- Evidence anchors: [abstract] "re-build the Chinese Biomedical Language Understanding Evaluation (CBLUE) benchmark into a large scale prompt-tuning benchmark, PromptCBLUE"; [section] "we created an extensive multi-task test suite in the medical domain for the LLMs that supports Chinese"; [corpus] "Average neighbor FMR=0.482"

### Mechanism 2
- Claim: Private test ground truths prevent data leakage during pre-training, ensuring valid evaluation of generalization.
- Mechanism: By keeping test labels private and not releasing them publicly, the benchmark prevents models from being exposed to test data during pre-training, maintaining evaluation integrity.
- Core assumption: Models evaluated on PromptCBLUE haven't been pre-trained on the test data, making performance measurements reflect true generalization.
- Evidence anchors: [abstract] "PromptCBLUE's test ground truths are kept from the public corpus, which can effectively evaluate LLMs' generalization and instruction-following capabilities"; [section] "we limit the training samples of each task to 3000 to 5000, and the dev/test set to 600-800"; [corpus] "Found 25 related papers"

### Mechanism 3
- Claim: Parameter-efficient fine-tuning methods can achieve competitive performance with significantly fewer trainable parameters.
- Mechanism: By using PEFT techniques like LoRA, P-tuning, and adapters that modify less than 1% of LLM parameters, the benchmark demonstrates that efficient adaptation is possible for biomedical tasks.
- Core assumption: Biomedical domain adaptation can be achieved through parameter-efficient methods without full fine-tuning.
- Evidence anchors: [abstract] "we have experimented and report the results with the current 9 Chinese LLMs fine-tuned with different fine-tuning techniques"; [section] "For parameter efficient fine-tuning, we adopted the following methods: (a) P-tuning Liu et al. (2021). (b) P-tuning v2 Liu et al. (2022). (c) Parallel adapter by He et al. (2021). (d) LoRA Hu et al. (2021). (e) AdaLoRA by Zhang et al. (2023)"; [corpus] "average citations=0.0"

## Foundational Learning

- Concept: Instruction tuning and prompt engineering
  - Why needed here: The entire benchmark relies on converting traditional tasks into instruction-following format, requiring understanding of how to craft effective prompts
  - Quick check question: What are the key components of an effective instruction prompt for biomedical tasks (task instruction, label explanations, output format specifications, demonstrations)?

- Concept: Parameter-efficient fine-tuning techniques
  - Why needed here: The benchmark evaluates multiple PEFT methods, requiring understanding of how they differ in approach and when to apply each
  - Quick check question: How do LoRA, P-tuning, and adapter methods differ in their approach to parameter efficiency, and what are their respective strengths?

- Concept: Multi-task learning evaluation metrics
  - Why needed here: Different task types require different evaluation metrics (F1 for extraction, ROUGE for generation), requiring understanding of appropriate metric selection
  - Quick check question: Why is instance-level strict micro-F1 used for medical information extraction tasks instead of token-level metrics?

## Architecture Onboarding

- Component map: Prompt template creation → Task reformulation → Model fine-tuning/running → Output processing → Metric calculation → Result analysis
- Critical path: Prompt template creation → Task reformulation → Model fine-tuning/running → Output processing → Metric calculation → Result analysis
- Design tradeoffs: Balancing prompt diversity vs. consistency, comprehensive task coverage vs. manageable evaluation time, private test data vs. community accessibility.
- Failure signatures: Poor model performance due to inadequate prompt instructions, data leakage invalidating results, PEFT methods not converging, evaluation metrics not capturing true performance.
- First 3 experiments:
  1. Test single prompt template on one task type to validate instruction clarity
  2. Run small-scale fine-tuning with one PEFT method on a subset of tasks
  3. Evaluate commercial LLM APIs on few-shot setting to establish baseline performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of fine-tuned Chinese LLMs compare to English LLMs on medical tasks when using similar prompt-tuning approaches?
- Basis in paper: [explicit] The paper demonstrates that fine-tuned Chinese LLMs outperform commercial APIs like GPT-4 on Chinese medical tasks, but does not compare performance to English LLMs under similar conditions.
- Why unresolved: The study focuses exclusively on Chinese medical language understanding and does not provide cross-lingual comparisons with English models.
- What evidence would resolve it: Direct comparison experiments using the same prompt-tuning methodology on both Chinese and English medical benchmark datasets.

### Open Question 2
- Question: Does further pre-training in the medical domain significantly improve LLM performance on medical text understanding tasks, or are general language understanding capabilities sufficient?
- Basis in paper: [explicit] The paper observes that ChatMed (medical pre-trained) does not perform significantly better than Chinese LLaMA-2-7b, and Ziya-medical-13B does not outperform Baichuan 13B, suggesting general capabilities may be sufficient.
- Why unresolved: The study only tests a limited set of medical pre-trained models and cannot draw definitive conclusions about the necessity of medical domain pre-training.
- What evidence would resolve it: Systematic experiments comparing models with varying degrees of medical domain pre-training against general LLMs on comprehensive medical benchmarks.

### Open Question 3
- Question: Which parameter-efficient fine-tuning methods should be selected based on training settings (large-scale vs few-shot) for optimal performance on medical tasks?
- Basis in paper: [explicit] The paper shows different rankings of PEFT methods under large-scale fine-tuning (LoRA > AdaLoRA > Adapter > P-tuning-v2 > P-tuning) versus few-shot settings (P-tuning-v2 > LoRA > Adapter).
- Why unresolved: The study only tests a limited set of PEFT methods and training scenarios, leaving questions about optimal method selection for other settings.
- What evidence would resolve it: Comprehensive evaluation of a wider range of PEFT methods across multiple training regimes and medical task types.

## Limitations

- Data privacy assumptions: The effectiveness of keeping test ground truths private depends on unverified assumptions about pre-training corpora of evaluated models.
- Task format generalization: Converting diverse biomedical tasks to instruction format may not preserve original task characteristics and difficulty levels.
- PEFT method analysis: The recommendation for selecting PEFT methods based on training settings lacks systematic ablation studies and comprehensive hyperparameter analysis.

## Confidence

- Benchmark Construction and Task Coverage: High confidence - The transformation methodology and multi-task framework are well-documented and reproducible.
- Performance Comparisons Between Models: Medium confidence - Relative performance differences are reported, but absolute numbers may be influenced by prompt quality and evaluation conditions.
- PEFT Method Effectiveness: Medium confidence - LoRA's superiority is supported, but method selection guidance requires additional validation across more diverse tasks.

## Next Checks

1. **Prompt Template Ablation Study**: Systematically test the impact of different prompt template variations (instruction phrasing, demonstration inclusion, output format specifications) on the same model and task to quantify how much performance variance stems from prompt engineering versus model capability.

2. **Pre-training Data Verification**: Conduct a thorough search of the actual pre-training corpora used by evaluated commercial APIs and open-source models to verify that CBLUE test data was not included, or alternatively, design experiments that test model performance on known pre-training data versus truly unseen data.

3. **Cross-task PEFT Hyperparameter Analysis**: Perform a comprehensive grid search of hyperparameters (learning rate, rank, batch size) for each PEFT method across all task types to establish more robust guidelines for method selection beyond the current observation-based recommendations.