---
ver: rpa2
title: 'Measurement in the Age of LLMs: An Application to Ideological Scaling'
arxiv_id: '2312.09203'
source_url: https://arxiv.org/abs/2312.09203
tags:
- ideological
- tweets
- which
- ideal
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using large language models (LLMs) to measure
  political ideology in text and for legislators. The core method involves directly
  eliciting ideological scores from LLMs via natural language prompts, without quantifying
  data or fitting statistical models.
---

# Measurement in the Age of LLMs: An Application to Ideological Scaling

## Quick Facts
- arXiv ID: 2312.09203
- Source URL: https://arxiv.org/abs/2312.09203
- Reference count: 40
- Primary result: GPT-3.5-turbo's ideal point scores for U.S. Senators highly correlate with established scaling methods

## Executive Summary
This paper explores using large language models to measure political ideology in text and for legislators. The core method involves directly eliciting ideological scores from LLMs via natural language prompts, without quantifying data or fitting statistical models. Key findings include high correlation between LLM-elicted scores and established methods like DW-NOMINATE, successful detection of subtle ideological signals, and consistent ideology scores for tweets matching their authors' positions.

## Method Summary
The approach uses direct elicitation of ideological scores from LLMs through natural language prompts. The method relies on LLMs' linguistic fluency to generate scales for legislators and text, using dynamic self-anchoring through autoregressive generation. Zero-shot chain-of-thought prompting is employed to improve reliability. The technique is applied to U.S. Senate data and Twitter content, with validation against established scaling methods.

## Key Results
- GPT-3.5-turbo's ideal point scores for U.S. Senators highly correlate with DW-NOMINATE, campaign finance scores, and text-based ideal point models
- LLMs successfully detect subtle ideological signals in text, including neo-Nazi dog whistles and nuanced dinner table conversations
- Ideology scores assigned to tweets closely match ideal points elicited for their authors

## Why This Works (Mechanism)

### Mechanism 1
LLMs can directly elicit ideological scales from natural language prompts without prior data quantification. The autoregressive nature allows generation of contextually appropriate numeric scores based on prompt framing, leveraging training corpus knowledge. Core assumption: training corpus contains sufficient political discourse examples. Evidence: correlation results and prompt engineering examples. Break condition: insufficient political discourse in training data.

### Mechanism 2
Dynamic self-anchoring through autoregressive generation ensures score comparability. By scoring multiple entities in a single response, each score becomes anchored relative to previously generated scores. Core assumption: LLMs maintain internal consistency when generating related scores. Evidence: described approach and validation against established methods. Break condition: failure of autoregressive generation to maintain consistency.

### Mechanism 3
Zero-shot chain-of-thought prompting improves detection of subtle ideological signals. Encouraging the LLM to explain reasoning before scoring leads to more accurate detection. Core assumption: reasoning process reveals hidden patterns. Evidence: dog whistle detection experiments and visualization results. Break condition: superficial reasoning or noise introduction.

## Foundational Learning

- Concept: Autoregressive language models
  - Why needed here: Understanding sequence generation is crucial for grasping the self-anchoring mechanism
  - Quick check question: What happens to the context window as an autoregressive model generates more tokens?

- Concept: Measurement validity
  - Why needed here: Understanding validity types (face, criterion, construct) is essential for evaluating LLM-based measurements
  - Quick check question: What's the difference between face validity and criterion validity in measurement theory?

- Concept: Political ideology scaling
  - Why needed here: The application domain requires familiarity with established methods like DW-NOMINATE and ideal point estimation
  - Quick check question: How do traditional ideal point models like DW-NOMINATE differ from text-based approaches?

## Architecture Onboarding

- Component map: LLM prompt engineering -> score elicitation -> normalization -> validation against reference methods
- Critical path: Prompt design -> score generation -> quality control -> comparison with established metrics
- Design tradeoffs: Flexibility vs. reliability (direct elicitation allows varied measurements but may be less consistent than pairwise comparison methods)
- Failure signatures: Inconsistent scaling across prompt orders, lack of correlation with established methods, inability to detect subtle ideological signals
- First 3 experiments:
  1. Test self-anchoring by eliciting scores for same entities with different orderings
  2. Validate face validity by checking if LLM scores align with known political positions
  3. Test chain-of-thought prompting by comparing ideological signal detection with and without CoT

## Open Questions the Paper Calls Out

### Open Question 1
How robust are LLM-based ideological scaling methods to adversarial prompt engineering designed to manipulate resulting scores? The paper notes LLMs can be "distracted by spurious details" and are "not robust to minor prompt changes," but doesn't test deliberate adversarial manipulation. Experiments systematically testing various adversarial prompt modifications would resolve this.

### Open Question 2
To what extent do LLM-based methods capture stable ideological positions versus transient reactions to current events? While the paper shows temporal changes in ideological content, it doesn't distinguish between stable positions and event reactions. Analysis of ideological content during stable vs. high-stakes periods would provide evidence.

### Open Question 3
How well do LLM-based methods generalize across different political contexts and cultures? The paper only tests U.S. context and notes terms have different meanings in different contexts. Testing on political texts from different countries and cultures, comparing results to local "ground truth" measures, would reveal systematic differences.

## Limitations
- Reliance on LLM training corpus containing sufficient political discourse examples
- Lack of comprehensive empirical validation for self-anchoring mechanism across different ordering scenarios
- Limited testing of chain-of-thought prompting on diverse examples
- Incomplete specification of hand-crafted examples and exact tweet sampling procedures

## Confidence
- **High**: Correlation between LLM-elicted scores and established methods (DW-NOMINATE, CFscores)
- **Medium**: Self-anchoring mechanism and chain-of-thought prompting effectiveness
- **Low**: Generalizability of the approach to different political contexts and languages

## Next Checks
1. Test score consistency across multiple prompt orderings for the same set of entities to validate the self-anchoring mechanism
2. Evaluate the LLM's performance on a larger, diverse set of hand-crafted examples with known ideological content
3. Conduct a corpus analysis to verify the presence of sufficient political discourse examples in the LLM's training data