---
ver: rpa2
title: 'CovarNav: Machine Unlearning via Model Inversion and Covariance Navigation'
arxiv_id: '2311.12999'
source_url: https://arxiv.org/abs/2311.12999
tags:
- unlearning
- data
- learning
- machine
- forgetting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of selective data removal (machine
  unlearning) from trained deep neural networks. Given a trained model and a subset
  of data to be forgotten, the authors propose a three-step framework called CovarNav.
---

# CovarNav: Machine Unlearning via Model Inversion and Covariance Navigation

## Quick Facts
- arXiv ID: 2311.12999
- Source URL: https://arxiv.org/abs/2311.12999
- Reference count: 40
- Key outcome: Three-step framework achieves superior machine unlearning performance on CIFAR-10 and VGGFace2

## Executive Summary
This paper addresses the challenge of selective data removal from trained deep neural networks through a novel framework called CovarNav. The method enables forgetting specific classes from a trained model without requiring access to the original training data of retained classes. By combining model inversion, strategic mislabeling, and covariance navigation through gradient projection, CovarNav achieves state-of-the-art performance in terms of both accuracy preservation on retained data and effective forgetting of target classes, as measured by the Anamnesis Index.

## Method Summary
CovarNav is a three-step framework for machine unlearning that operates on a trained model and a forget set without requiring the original training data. First, model inversion generates proxy samples for retained classes by optimizing inputs to match feature statistics and produce realistic samples. Second, the forget set is mislabeled using the second-largest logit class to create an effective forgetting objective. Third, gradient updates are projected into the null space of the feature covariance matrix of retained data, ensuring updates don't affect performance on those samples while still allowing unlearning of the forget set. The method uses ResNet-18 architecture and is evaluated on CIFAR-10 and VGGFace2 datasets.

## Key Results
- Achieves higher accuracy on retained data while effectively forgetting target classes compared to state-of-the-art unlearning approaches
- Competitive Anamnesis Index (AIN) scores approaching 1, indicating successful unlearning
- Outperforms existing methods in both CIFAR-10 and VGGFace2 benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Covariance Navigation projects gradient updates into the null space of the feature covariance matrix to preserve performance on retained data.
- Mechanism: By ensuring gradient updates are orthogonal to the activations of retained data, the method maintains the original model's behavior on those samples while still allowing unlearning of the forget set.
- Core assumption: The null space of the uncentered feature covariance matrix of retained data approximates the space of gradient updates that won't affect performance on that data.
- Evidence anchors:
  - [section] "Eq 6 implies that if the gradient updates at each layer are orthogonal to the activations of all the data in Dr, i.e., X l r = {xl r,i}Nr i=1, then the network is guaranteed to satisfy f(xr; θ) = f(xr; θ∗)."
  - [section] "Eq 6 implies that if the gradient updates at each layer are orthogonal to the activations of all the data in Dr, i.e., X l r = {xl r,i}Nr i=1, then the network is guaranteed to satisfy f(xr; θ) = f(xr; θ∗)."
  - [corpus] No direct evidence in corpus neighbors; method appears novel.
- Break condition: If the feature covariance matrix has full rank (small null space), the method cannot effectively navigate gradients without affecting retained data performance.

### Mechanism 2
- Claim: Model inversion generates a proxy dataset for retained data that closely matches the original training distribution.
- Mechanism: The model inversion attack optimizes inputs to maximize output activation for retained classes while incorporating regularization terms that enforce natural image statistics and feature statistics matching.
- Core assumption: The trained model retains sufficient information about retained classes to reconstruct representative samples through optimization.
- Evidence anchors:
  - [section] "We propose utilizing model inversion attacks, which exploit this retained information, to derive pseudo samples for Dr and construct a proxy set ˆDr, representing the remaining data."
  - [section] "Let cf denote the class id we intend to forget and define the set of remaining labels as Yr = Y\{cf }. In line with the work of Yin et al. [25], we formulate the model inversion attack for a batch of target labels {yj ∈ Y r}B j=1 as:"
  - [corpus] No direct evidence in corpus neighbors; method appears novel.
- Break condition: If the model has poor generalization or the retained data distribution is too complex, the inversion may produce samples that poorly represent the original data.

### Mechanism 3
- Claim: Mislabeling the forget set with the largest incorrect logit effectively creates a forgetting objective while maintaining computational efficiency.
- Mechanism: Instead of random labeling or complex adversarial attacks, selecting the second-largest logit as the wrong label provides a strong yet computationally simple way to define incorrect targets.
- Core assumption: The model's second-largest logit for forget set samples reliably represents a class that is perceptually similar enough to create meaningful confusion.
- Evidence anchors:
  - [section] "Through ablation studies, we show that this strategy is at least as effective as the one used in [15]. We denote this mislabeled forget set as ˆDf = {(xf,j , ˆyf,j)}Nf j=1 where ˆyj,f correspond to the wrong class with the largest logit."
  - [section] "Following the prior work in the literature, we replace the labels of Df with those of the closest neighboring class (specifically, the second largest logit) to produce ˆDf ."
  - [corpus] No direct evidence in corpus neighbors; method appears novel.
- Break condition: If the forget set samples are already highly uncertain or near decision boundaries, the second-largest logit may not provide meaningful incorrect labels.

## Foundational Learning

- Concept: Gradient projection in null spaces
  - Why needed here: To ensure unlearning operations don't affect retained data performance while still allowing modification for forget set
  - Quick check question: How do you compute the null space of a feature covariance matrix and project gradients onto it?

- Concept: Model inversion attacks
  - Why needed here: To create a proxy dataset for retained data when the original training data is unavailable
  - Quick check question: What regularization terms are typically used in model inversion to ensure realistic sample generation?

- Concept: Cross-entropy loss manipulation
  - Why needed here: To define the forgetting objective by optimizing for incorrect labels on forget set
  - Quick check question: How does minimizing cross-entropy for wrong labels differ from maximizing it for correct labels in terms of optimization dynamics?

## Architecture Onboarding

- Component map: Model inversion module → Forget set mislabeling → Covariance Navigation gradient projection → Parameter update loop
- Critical path: Model inversion (Step 1) → Mislabeling (Step 2) → Covariance Navigation (Step 3) → Parameter update
- Design tradeoffs: Model inversion quality vs. computational cost; null space approximation vs. retained data preservation
- Failure signatures: Low Anamnesis Index (AIN) values indicate successful unlearning; significant accuracy drops on retained data indicate over-aggressive unlearning
- First 3 experiments:
  1. Run model inversion on a simple dataset (like MNIST) to verify proxy data quality
  2. Test mislabeling strategy with largest wrong logit vs. random labels on small network
  3. Implement basic covariance navigation without null space approximation to understand gradient projection behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CovarNav scale when dealing with unlearning multiple classes simultaneously versus sequentially?
- Basis in paper: [inferred] The paper focuses on unlearning a single class, but the scalability to multiple classes is an open question.
- Why unresolved: The paper does not explore scenarios where multiple classes need to be forgotten, either simultaneously or sequentially, which is a more complex and realistic scenario.
- What evidence would resolve it: Experiments showing the performance of CovarNav when unlearning multiple classes at once or in a sequential manner, compared to its performance on single-class unlearning.

### Open Question 2
- Question: How does the quality of the inverted dataset (ˆDr) affect the unlearning performance, and can this quality be further improved?
- Basis in paper: [explicit] The paper discusses the use of model inversion to approximate the retained dataset but does not explore the impact of the quality of this approximation on unlearning performance.
- Why unresolved: While the paper mentions the use of model inversion and its effectiveness, it does not investigate how variations in the quality of the inverted dataset might influence the overall unlearning process.
- What evidence would resolve it: Comparative studies showing how different quality levels of the inverted dataset affect the unlearning performance, possibly through varying the model inversion parameters or techniques.

### Open Question 3
- Question: What are the long-term effects of using CovarNav on model robustness and generalization to unseen data?
- Basis in paper: [inferred] The paper evaluates the immediate effects of unlearning but does not discuss the long-term impact on model robustness or generalization.
- Why unresolved: The focus is on immediate unlearning performance, with no exploration of how the unlearning process might affect the model's ability to handle new, unseen data over time.
- What evidence would resolve it: Longitudinal studies assessing the model's performance on new datasets after unlearning, comparing models that have undergone CovarNav with those that haven't.

## Limitations
- The method's effectiveness depends heavily on the quality of model inversion, which may fail for complex data distributions or poorly generalized models
- The null space approximation introduces a hyperparameter (p) that requires careful tuning without theoretical guidance for optimal selection
- Performance on imbalanced datasets and multiple-class unlearning scenarios remains unexplored

## Confidence

**High confidence**: The mathematical framework for covariance navigation and gradient projection is sound and well-derived from established linear algebra principles.

**Medium confidence**: The effectiveness of the mislabeled forget set strategy, while shown to work empirically, lacks deeper theoretical justification for why second-largest logits provide optimal forgetting.

**Low confidence**: The model inversion component's robustness across different model architectures and data distributions remains unproven, as the method relies heavily on regularization hyperparameters not fully specified in the paper.

## Next Checks

1. **Null Space Sensitivity Analysis**: Systematically vary the p parameter (fraction of eigenvectors retained) across multiple datasets to establish guidelines for hyperparameter selection and identify failure thresholds.

2. **Cross-Architecture Generalization**: Test CovarNav on architectures beyond ResNet-18 (e.g., EfficientNet, Vision Transformers) to verify the framework's applicability to different network designs and feature hierarchies.

3. **Long-Tail Distribution Robustness**: Evaluate performance on datasets with imbalanced class distributions to assess whether model inversion can effectively capture minority class distributions when generating proxy data.