---
ver: rpa2
title: Learning to Rank for Active Learning via Multi-Task Bilevel Optimization
arxiv_id: '2310.17044'
source_url: https://arxiv.org/abs/2310.17044
tags:
- utility
- learning
- acquisition
- active
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel active learning approach called RAMBO
  (Ranking-based Active learning via Multitask Bilevel Optimization) to address the
  limitations of existing methods that rely on expensive acquisition functions or
  overly generic heuristics. RAMBO aims to select batches of unlabeled instances through
  a learned surrogate model for data acquisition, using a bilevel multi-task optimization
  framework to predict the relative utility of different training sets.
---

# Learning to Rank for Active Learning via Multi-Task Bilevel Optimization

## Quick Facts
- arXiv ID: 2310.17044
- Source URL: https://arxiv.org/abs/2310.17044
- Reference count: 32
- Key outcome: RAMBO achieves higher validation accuracy compared to state-of-the-art baselines like Margin Sampling, BADGE, CoreSet, and GLISTER across various datasets and labeling budgets.

## Executive Summary
This paper introduces RAMBO (Ranking-based Active learning via Multitask Bilevel Optimization), a novel active learning approach that learns a surrogate model to predict the relative utility of different training sets. The method addresses limitations of existing approaches that rely on expensive acquisition functions or generic heuristics by using a bilevel multi-task optimization framework. RAMBO employs interpolation-based surrogate models to estimate utility functions when validation accuracy is expensive to evaluate, reducing evaluation costs while maintaining performance.

## Method Summary
RAMBO is a single-round active learning method that selects a batch of B unlabeled instances in one shot. The approach pretrains a RankNet-based utility model via bilevel multi-task optimization using utility samples from an initial labeled set S0. The utility model predicts the relative utility of different training subsets using a set-based neural network architecture. During acquisition, the method applies Greedy-Margin selection using the pretrained utility model to select the most informative batch from the unlabeled pool U0. The bilevel framework enables the model to generalize to longer histories of labeled data, while interpolation-based augmentation reduces the need for expensive ground truth utility evaluations.

## Key Results
- RAMBO significantly improves validation accuracy over traditional techniques like Margin Sampling, BADGE, CoreSet, and GLISTER
- The approach demonstrates strong performance across standard active classification benchmarks including MNIST, FashionMNIST, CIFAR10, and SVHN
- RAMBO reduces evaluation costs through efficient interpolation-based surrogate models while maintaining or improving accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The bilevel optimization framework enables the utility model to generalize to longer history of labeled data by learning regularization parameters that adapt to growing training sets.
- Mechanism: The outer optimization finds optimal hyperparameters that minimize validation loss on longer utility samples, while the inner optimization finds model parameters that minimize training loss on shorter utility samples. This creates a meta-learning effect where the model learns to adapt to increasing input sizes.
- Core assumption: The validation loss computed on longer utility samples serves as a good proxy for generalization ability to unseen longer sequences.
- Break condition: If the validation loss computed on longer utility samples becomes a poor proxy for generalization, or if the bilevel optimization fails to converge due to computational constraints.

### Mechanism 2
- Claim: The ranking-based approach using RankNet mitigates the stochasticity in validation accuracy evaluation by focusing on relative utility comparisons rather than absolute values.
- Mechanism: Instead of predicting exact validation accuracy values, the model learns to rank pairs of training subsets by their utility. This reduces sensitivity to noise in individual accuracy measurements and focuses on the relative ordering which is more stable.
- Core assumption: The relative ordering of utility between pairs of training subsets is more stable than absolute utility values across different model initializations.
- Break condition: If the relative ordering between pairs becomes unstable or if the ranking model fails to capture meaningful utility differences.

### Mechanism 3
- Claim: The interpolation-based utility sample augmentation reduces the need for ground truth utility samples by leveraging latent space consistency in the feature extractor.
- Mechanism: By computing distances in the feature space between current and previous labeled pools, the method generates pseudo utility values for augmented samples through linear interpolation, reducing the number of expensive ground truth evaluations needed.
- Core assumption: The latent space of the classifier's feature extractor contains meaningful representations that can be interpolated to generate valid utility samples.
- Break condition: If the interpolation assumptions break down (non-linear relationships in latent space) or if the generated samples don't reflect true utility.

## Foundational Learning

- Concept: Set-based neural networks
  - Why needed here: The utility function must map subsets of data to utility values, requiring architectures that can handle variable-sized inputs and capture interactions between elements.
  - Quick check question: Can a standard feed-forward network handle inputs of varying sizes without modification?

- Concept: Bilevel optimization
  - Why needed here: The growing nature of labeled data requires a meta-learning approach where the model learns to adapt to increasing input sizes over time.
  - Quick check question: What is the difference between bilevel optimization and standard supervised learning in terms of objectives?

- Concept: Optimal transport distance
  - Why needed here: Serves as a regularization signal to align the distribution of selected samples with the validation set distribution, improving generalization.
  - Quick check question: How does optimal transport distance differ from KL divergence in measuring distribution similarity?

## Architecture Onboarding

- Component map: Feature extractor -> RankNet head -> OT distance prediction head -> Interpolation module
- Critical path: Pretraining -> Feature extraction -> Ranking prediction -> Batch selection
- Design tradeoffs:
  - Model complexity vs. pretraining data requirements
  - Number of augmented samples vs. computational cost
  - RankNet pairwise comparisons vs. regression approach
  - OT distance regularization strength vs. ranking performance
- Failure signatures:
  - Validation accuracy plateaus or degrades over rounds
  - Model fails to generalize to larger utility sample sizes
  - Batch selection becomes unstable or random
  - Pretraining convergence issues
- First 3 experiments:
  1. Test RankNet ranking performance on synthetic utility data with known ground truth
  2. Evaluate interpolation quality by comparing generated vs. real utility samples
  3. Benchmark bilevel vs. standard training on a small dataset with varying training set sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal budget allocation between the pretraining and acquisition stages for maximizing validation accuracy?
- Basis in paper: [inferred] The authors mention that "one potential direction for future work could be to determine an optimal budget allocation for both the pretraining and acquisition stages."
- Why unresolved: The paper does not explore different budget allocations between pretraining and acquisition stages, only fixing them to specific values.
- What evidence would resolve it: Experiments comparing validation accuracy for different budget allocations between pretraining and acquisition stages.

### Open Question 2
- Question: How does RAMBO perform in few-rounds active learning settings, where multiple rounds of interaction with annotators are allowed?
- Basis in paper: [explicit] The authors state "the extension of RAMBO to the few-rounds setting" as a potential direction for future work.
- Why unresolved: The paper only evaluates RAMBO in a single-round active learning setting.
- What evidence would resolve it: Experiments evaluating RAMBO's performance in few-rounds active learning settings.

### Open Question 3
- Question: How does the performance of RAMBO scale with increasing dataset sizes and complexity?
- Basis in paper: [inferred] The experiments are conducted on relatively small image datasets (MNIST, FashionMNIST, CIFAR10, SVHN). It is unclear how RAMBO would perform on larger, more complex datasets.
- Why unresolved: The paper does not evaluate RAMBO on larger or more complex datasets.
- What evidence would resolve it: Experiments evaluating RAMBO's performance on larger, more complex datasets.

## Limitations

- The method's complexity introduces potential implementation challenges, particularly around bilevel optimization and RankNet architecture details
- Interpolation-based augmentation relies on assumptions about latent space consistency that may not hold across all datasets or feature extractors
- Exact implementation details for set-based neural networks and bilevel optimization hyperparameters are not fully specified

## Confidence

- High confidence: The core ranking-based active learning framework and bilevel optimization approach are well-established concepts
- Medium confidence: The interpolation-based utility estimation and multi-task learning components show promise but require careful tuning
- Low confidence: The exact implementation details for set-based neural networks and bilevel optimization hyperparameters are not fully specified

## Next Checks

1. Conduct ablation studies to isolate the impact of the bilevel optimization framework versus standard supervised learning
2. Test the interpolation-based utility estimation on datasets with known non-linear relationships in latent space
3. Evaluate the sensitivity of RAMBO's performance to different feature extractors and ranking architectures