---
ver: rpa2
title: Comparing Generative Chatbots Based on Process Requirements
arxiv_id: '2312.03741'
source_url: https://arxiv.org/abs/2312.03741
tags:
- process
- task
- book
- chatbot
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the performance of generative chatbots GPT-3.5
  and PaLM 2 in supporting business process execution based on BPMN constructs. Using
  a trip planning use case, both models were assessed across six evaluation categories
  including start/end events, forward flow, gateway handling, and unintended paths.
---

# Comparing Generative Chatbots Based on Process Requirements

## Quick Facts
- arXiv ID: 2312.03741
- Source URL: https://arxiv.org/abs/2312.03741
- Reference count: 13
- GPT-3.5 met 92.31% of evaluation criteria, while PaLM 2 met 69.23% for BPMN process execution support

## Executive Summary
This study evaluates how well generative chatbots can support business process execution based on BPMN specifications. The researchers tested GPT-3.5 and PaLM 2 using a trip planning use case, providing them with BPMN XML files to interpret and execute. The chatbots were assessed across six evaluation categories including start/end events, forward flow, gateway handling, and task dependencies. Results show GPT-3.5 performed significantly better than PaLM 2, meeting 92.31% versus 69.23% of evaluation criteria, though both struggled with exclusive gateway evaluation and preventing out-of-order task execution.

## Method Summary
The researchers created a BPMN process model for trip planning using Camunda Modeler and exported it as XML. They then provided this XML to GPT-3.5 and PaLM 2 chatbots with instructions to act as process execution agents. The chatbots were evaluated against 13 specific questions across six categories: Start Event, Forward Flow, Previous History, End Event, Process Variables, and Unintended paths. The evaluation measured whether chatbots correctly interpreted BPMN constructs, maintained process state, and enforced task sequencing throughout the conversation.

## Key Results
- GPT-3.5 met 92.31% of evaluation criteria while PaLM 2 met 69.23%
- Both models successfully handled basic forward flow and end event scenarios
- Exclusive gateway evaluation and task dependency enforcement were major challenges for both models
- Neither chatbot fully prevented users from executing tasks out of sequence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative chatbots can parse BPMN XML and map it to executable conversational flows
- Mechanism: By receiving the raw XML, the model uses its language understanding to interpret XML tags as BPMN elements (tasks, gateways, events) and dynamically construct a process-aware dialogue state machine
- Core assumption: The model's pretraining includes sufficient exposure to structured markup and process modeling concepts
- Evidence anchors: [abstract] "provide the chatbot with the entire XML file related to a BPMN model, and let it interpret it on its own" [section] "allow it to autonomously parse and comprehend all the elements within the BPMN model" [corpus] Weak - no direct evidence in neighbor papers that models are pretrained on XML/BPMN
- Break condition: XML structure deviates from expected BPMN schema or contains nested subprocesses beyond model's parsing ability

### Mechanism 2
- Claim: Exclusive gateways can be evaluated by asking users for preference and branching accordingly
- Mechanism: The chatbot identifies exclusive gateway tags, prompts user for decision criteria, and routes to the appropriate sequence flow based on response
- Core assumption: The model can correctly map user input to the conditional logic expressed in the gateway's outgoing flows
- Evidence anchors: [section] "After completing a task and encountering an exclusive gateway, does the chatbot correctly evaluate the gateway based on the conditions of its outgoing sequence flows" [section] "Since you indicated that you wanted to book a transfer, the process follows the path that leads to the 'Book transfer' task" [corpus] Weak - no evidence in neighbors that models understand BPMN-specific conditional evaluation
- Break condition: Gateway conditions are complex or user response is ambiguous, leading to incorrect flow selection

### Mechanism 3
- Claim: Process state tracking can be maintained through conversational history to enforce task sequencing
- Mechanism: The model tracks completed tasks and available next steps by maintaining a mental model of the process instance, preventing out-of-order execution
- Core assumption: The model's context window is sufficient to track all relevant state changes across the conversation
- Evidence anchors: [section] "Does the chatbot allow the user to perform a task that is not yet available in the process?" [section] "After booking the flight and hotel, the next step is to book the hotel, and the decision to book a transfer or not will be determined after booking the hotel" [corpus] Weak - no evidence in neighbors that models maintain state across extended conversations
- Break condition: Conversation exceeds context window or state becomes too complex to track accurately

## Foundational Learning

- Concept: Business Process Model and Notation (BPMN)
  - Why needed here: Understanding BPMN constructs is essential for evaluating whether chatbots correctly interpret and execute process models
  - Quick check question: What BPMN element represents a decision point where the flow diverges based on conditions?

- Concept: Large Language Model (LLM) context and limitations
  - Why needed here: Knowing LLM capabilities and constraints helps explain why certain BPMN features (like complex gateways) are challenging
  - Quick check question: What happens when an LLM's context window is exceeded during a multi-turn process execution conversation?

- Concept: XML parsing and structure
  - Why needed here: The evaluation relies on chatbots correctly interpreting BPMN XML files
  - Quick check question: What XML tag would you expect to find for a BPMN user task element?

## Architecture Onboarding

- Component map: BPMN XML -> LLM parsing -> Process state tracking -> Conversational responses -> BPMN compliance evaluation
- Critical path: 1. Load BPMN XML into chatbot 2. Initialize process instance 3. Present available tasks based on current state 4. Receive user action and update state 5. Evaluate gateway conditions if encountered 6. Continue until end event reached
- Design tradeoffs: Flexibility vs. strict BPMN compliance: Generative models allow natural conversation but may deviate from process rules; Context window vs. complex process tracking: Larger processes may exceed LLM's ability to maintain state; Prompt engineering vs. model capability: More explicit instructions may improve compliance but reduce natural interaction
- Failure signatures: Tasks presented out of sequence; Gateways not properly evaluated; Process instance ended prematurely; User able to execute unavailable tasks
- First 3 experiments: 1. Simple linear process with 3 sequential tasks, no gateways 2. Process with one exclusive gateway requiring binary choice 3. Process with task dependencies where certain tasks unlock others

## Open Questions the Paper Calls Out

- Question: How do generative chatbots perform with other BPMN constructs beyond events, tasks, and exclusive gateways?
  - Basis in paper: [inferred] The paper mentions limitations in evaluating only a subset of BPMN constructs and suggests future work should include other BPMN features
  - Why unresolved: The current study only tested a limited subset of BPMN elements (events, tasks, and exclusive gateways)
  - What evidence would resolve it: Testing generative chatbots with a comprehensive range of BPMN constructs including inclusive gateways, event-based gateways, message flows, timers, and subprocesses

- Question: Which generative chatbot models perform best for BPMN process execution support?
  - Basis in paper: [explicit] The paper explicitly states that only GPT and PaLM were tested and suggests future work should investigate other models like Meta's OPT and NVIDIA's Megatron-Turing
  - Why unresolved: The evaluation was limited to only two specific generative models (GPT-3.5 and PaLM 2)
  - What evidence would resolve it: Systematic comparison of multiple generative models across the same BPMN evaluation criteria to identify performance differences

- Question: Can generative chatbots be effectively used for BPMN modeling rather than just execution?
  - Basis in paper: [explicit] The paper suggests future work could explore using chatbots to assist in the initial modeling of BPMN processes from natural language descriptions
  - Why unresolved: The current research only evaluated chatbot performance for process execution, not process modeling
  - What evidence would resolve it: Testing whether generative chatbots can successfully translate natural language process descriptions into valid BPMN XML files that accurately represent the intended process flow

## Limitations

- The evaluation framework relies heavily on subjective interpretation of chatbot responses against BPMN specifications
- The trip planning use case represents a relatively simple process model that may not capture enterprise BPMN complexity
- Binary scoring (met/not met) may oversimplify complex conversational interactions and LLM capabilities

## Confidence

- High Confidence: Both GPT-3.5 and PaLM 2 can parse basic BPMN XML and support simple process execution flows
- Medium Confidence: Generative chatbots show promise for business process execution but require improvements in handling exclusive gateways and task sequencing
- Low Confidence: Neither model fully prevents out-of-order task execution, but the study doesn't establish whether this represents fundamental LLM limitations or prompt engineering issues

## Next Checks

1. Test chatbot performance on more complex BPMN models with multiple nested subprocesses and complex gateway conditions to validate scalability claims
2. Conduct A/B testing with enhanced prompt engineering strategies to determine if task sequencing failures are model limitations or prompting issues
3. Implement quantitative metrics for evaluating gateway condition evaluation accuracy rather than binary pass/fail scoring