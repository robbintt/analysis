---
ver: rpa2
title: Explaining Black-Box Models through Counterfactuals
arxiv_id: '2308.07198'
source_url: https://arxiv.org/abs/2308.07198
tags:
- counterfactual
- package
- counterfactuals
- explanations
- generators
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CounterfactualExplanations.jl, a Julia package
  for generating counterfactual explanations and algorithmic recourse for black-box
  models. The package provides a simple and intuitive interface to explain various
  predictive models, including those trained in Julia, Python, and R.
---

# Explaining Black-Box Models through Counterfactuals

## Quick Facts
- arXiv ID: 2308.07198
- Source URL: https://arxiv.org/abs/2308.07198
- Reference count: 8
- Key outcome: Introduces CounterfactualExplanations.jl, a Julia package for generating counterfactual explanations and algorithmic recourse for black-box models

## Executive Summary
This paper presents CounterfactualExplanations.jl, a comprehensive Julia package designed to generate counterfactual explanations for black-box machine learning models. The package supports a wide range of models including those built in Flux, MLJ, Python, and R, and implements various counterfactual generators addressing different desiderata such as proximity, actionability, plausibility, and diversity. By providing a simple interface and extensive customization options, the package aims to be a valuable tool for explainable AI and algorithmic recourse in the Julia ecosystem.

## Method Summary
The package implements counterfactual explanations through gradient-based optimization that directly targets black-box model predictions, ensuring full fidelity by construction. It provides native support for Julia models (Flux, MLJ) and extends interoperability to Python and R models through language bridges. The framework supports composable generators using chained macros, allowing users to blend different search objectives and optimization strategies. Key components include model compatibility layers, various counterfactual generators, a data catalogue with benchmark datasets, and visualization tools for interpreting results.

## Key Results
- Provides first comprehensive counterfactual explanation package for Julia ecosystem
- Implements 7 distinct generators addressing different desiderata (proximity, actionability, plausibility, sparsity, robustness, diversity, causality)
- Supports interoperability with Python and R models through PythonCall.jl and RCall.jl
- Includes data catalogue of synthetic and real-world benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual explanations achieve full fidelity by construction because they are searched directly with respect to the black-box classifier rather than a proxy model
- Mechanism: The counterfactual search objective explicitly optimizes for the black-box model's prediction to match the target class, ensuring the explanation perfectly aligns with the model's actual decision boundary
- Core assumption: The black-box model is differentiable or can be queried for predictions in a way that allows optimization
- Break condition: If the black-box model is non-differentiable and gradient-based methods cannot be applied, or if the optimization gets stuck in local minima that don't achieve the target class

### Mechanism 2
- Claim: Composability of generators through user-friendly macros allows blending different desiderata without rewriting search algorithms
- Mechanism: The package uses chained macros (@objective, @search_latent_space, @with_optimiser) to combine different aspects of counterfactual search objectives and optimization strategies in a modular way
- Core assumption: Different search strategies and objectives can be mathematically composed without conflict
- Break condition: If the mathematical objectives conflict or if the optimization becomes ill-posed when combining multiple penalties

### Mechanism 3
- Claim: Native support for interoperability with Python and R models extends the package's reach beyond the Julia ecosystem
- Mechanism: The package leverages PythonCall.jl and RCall.jl to interface with foreign language models, allowing users to explain models trained in other languages
- Core assumption: The foreign language models can be loaded and queried in a way compatible with the counterfactual search framework
- Break condition: If the foreign language interface fails to properly load models or if the model prediction API is incompatible with the required counterfactual search interface

## Foundational Learning

- Concept: Differentiable programming and gradient-based optimization
  - Why needed here: Most counterfactual generators rely on gradient descent to optimize the counterfactual search objective
  - Quick check question: What is the role of the loss function ℓ in Equation 2 and why is it computed with respect to logits rather than predicted probabilities?

- Concept: Variational Autoencoders (VAEs) and latent space representations
  - Why needed here: Some generators search counterfactuals in the latent embedding of generative models to ensure plausibility
  - Quick check question: How does searching in latent space differ from searching in feature space, and what advantage does this provide for counterfactual explanations?

- Concept: Interpretability vs Explainability in machine learning
  - Why needed here: Understanding the distinction helps clarify why counterfactual explanations are needed for black-box models
  - Quick check question: What is the key difference between interpretable AI and explainable AI, and which category do counterfactual explanations fall into?

## Architecture Onboarding

- Component map: Models module -> Generators module -> Data catalogue -> Plotting functionality -> Core function (generate_counterfactual)
- Critical path: Model → Generator selection → Counterfactual search → Evaluation → Visualization
- Design tradeoffs:
  - Flexibility vs simplicity: The package offers extensive customization but this adds complexity
  - Performance vs generality: Supporting many model types may sacrifice optimization efficiency
  - Interpretability vs mathematical rigor: Some generators prioritize human-understandable explanations over optimal mathematical properties
- Failure signatures:
  - No valid counterfactual found: Indicates the optimization couldn't reach the target class within constraints
  - Implausible counterfactuals: Suggests the generator didn't adequately address plausibility desiderata
  - Slow computation: May indicate inefficient model compatibility layer or optimization parameters
- First 3 experiments:
  1. Load a synthetic linearly separable dataset and fit a simple linear classifier, then generate counterfactuals using the generic generator
  2. Try composing two generators (e.g., DiCE + REVISE) on the same dataset to observe the effect of combining desiderata
  3. Apply mutability constraints to a feature and observe how this affects the counterfactual search on a real-world dataset like Give Me Some Credit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CounterfactualExplanations.jl compare to existing Python libraries like CARLA for generating counterfactual explanations?
- Basis in paper: [explicit] The paper mentions CARLA as the only other unifying framework for counterfactual explanations in Python, but does not provide a direct comparison of performance or functionality.
- Why unresolved: The paper does not include benchmark results comparing CounterfactualExplanations.jl to CARLA or other Python libraries.
- What evidence would resolve it: Benchmark results comparing the performance, accuracy, and usability of CounterfactualExplanations.jl against CARLA and other relevant Python libraries.

### Open Question 2
- Question: How effective are the mutability constraints in ensuring that generated counterfactuals are actionable and realistic in real-world scenarios?
- Basis in paper: [explicit] The paper mentions the implementation of mutability constraints but notes that they are not yet implemented for Latent Space search.
- Why unresolved: The paper does not provide empirical evidence or case studies demonstrating the effectiveness of mutability constraints in generating actionable counterfactuals.
- What evidence would resolve it: Case studies or experiments showing how mutability constraints affect the actionability and realism of counterfactuals in various real-world applications.

### Open Question 3
- Question: What are the limitations and potential biases introduced by the generative models used for latent space search in counterfactual explanations?
- Basis in paper: [explicit] The paper discusses the use of generative models like VAEs for latent space search but highlights that the quality of counterfactuals depends on the performance of these models.
- Why unresolved: The paper does not explore the potential biases or limitations of generative models in detail.
- What evidence would resolve it: Studies or analyses examining the biases and limitations of generative models used in counterfactual explanations, including their impact on the quality and fairness of the explanations.

## Limitations
- Interoperability with Python and R models may be brittle due to reliance on external language bridges
- Effectiveness of generator composition depends on mathematical compatibility that may not always hold
- Performance implications of supporting diverse model types through multiple interfaces remain unclear

## Confidence

- High confidence: Basic counterfactual generation for native Julia models (Flux, MLJ) with standard generators
- Medium confidence: Advanced features like generator composition and interoperability with foreign language models
- Medium confidence: Claims about full fidelity by construction, as this depends heavily on optimization success and model compatibility

## Next Checks

1. Test counterfactual generation across all supported model interfaces (Flux, MLJ, Python, R) on the same dataset to verify consistent performance
2. Attempt to compose conflicting generators (e.g., diversity + sparsity objectives) to identify breaking conditions in the composition mechanism
3. Benchmark counterfactual generation speed and quality for black-box models versus white-box models to quantify the "full fidelity" claim empirically