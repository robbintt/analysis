---
ver: rpa2
title: 'SAFE: Machine Unlearning With Shard Graphs'
arxiv_id: '2304.13169'
source_url: https://arxiv.org/abs/2304.13169
tags:
- forgetting
- safe
- shards
- data
- shard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SAFE introduces a shard graph-based approach to selective forgetting
  in machine learning. The core idea is to partition data into shards and train lightweight
  adapters on each, connected in a directed graph where edges represent data sharing
  between shards.
---

# SAFE: Machine Unlearning With Shard Graphs

## Quick Facts
- arXiv ID: 2304.13169
- Source URL: https://arxiv.org/abs/2304.13169
- Authors: 
- Reference count: 40
- Primary result: Shard graph-based selective forgetting approach outperforming uniform sharding methods by up to 14% accuracy for same forgetting cost

## Executive Summary
SAFE introduces a novel approach to selective forgetting in machine learning by partitioning data into shards and training lightweight adapters connected in a directed graph. The shard graph structure enables controlled information sharing between shards while limiting retraining costs when forgetting samples. Using InCA adapters and prototype classifiers, SAFE achieves significantly higher accuracy than uniform sharding methods while reducing forgetting costs by an order of magnitude on fine-grained vision datasets.

## Method Summary
SAFE partitions data into shards and trains lightweight InCA adapters on each shard, connected via a directed shard graph where edges represent data sharing between shards. The adapters are trained on data from connected shards to improve accuracy through synergistic information, while maintaining forgetting guarantees by controlling retraining scope. At inference, predictions from all adapters are ensembled, optionally with prototype classifiers for regularization. The method enables scalable training and inference on hundreds of shards while providing strong unlearning guarantees.

## Key Results
- Outperforms uniform sharding methods (SISA, ProtoSISA) by up to 14% accuracy for same forgetting cost
- Reduces forgetting costs by an order of magnitude compared to state-of-the-art methods
- Handles up to 256 shards while maintaining high accuracy
- Enables extensions like DP-based stochastic forgetting and a-la-carte models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shard graphs enable selective information sharing while preserving forgetting guarantees by controlling retraining scope
- Mechanism: The shard graph structure allows each adapter to train on data from connected shards, improving accuracy via synergistic information. When forgetting a sample, only adapters that depend on the forgotten shard need retraining, limiting the retraining cost while maintaining unlearning guarantees
- Core assumption: Graph connectivity is optimized to maximize synergistic information gain while minimizing expected forgetting cost
- Evidence anchors: Abstract mentions shard graphs allow incorporating limited information from other shards; section defines total data needed to retrain adapters as union of connected shards

### Mechanism 2
- Claim: InCA adapters enable scalable training and inference on hundreds of shards by sharing computation and enabling parallel training
- Mechanism: InCA adapters use frozen backbone with lightweight cross-attention modules trained per shard. Compositionality of cross-attention allows all shard-specific queries to be concatenated and processed in single forward pass, drastically reducing inference cost. Parallel training enabled by independent gradients for each query
- Core assumption: Frozen backbone provides sufficient feature representations for downstream task, and cross-attention adapters can effectively adapt these features for each shard
- Evidence anchors: Abstract mentions SAFE uses lightweight system of adapters that can be trained while reusing most computations; section describes using Open-InCA cross-attention adapters for massive parallel training and inference

### Mechanism 3
- Claim: Prototype classifiers provide regularization that improves accuracy at high sharding levels by mitigating overfitting on small shards
- Mechanism: Prototype classifier computes class centroids in embedding space and uses cosine similarity for classification. Combined with shard-specific classifier using interpolation weight that decreases with shard size, providing stronger regularization when shards are small
- Core assumption: Embedding space from frozen backbone preserves class structure, allowing prototypes to be effective regularizers
- Evidence anchors: Section mentions classifiers trained on individual shards may overfit when shards consist of only a few samples; section shows SAFE combines prototypical classifier with model output using interpolation weight

## Foundational Learning

- Concept: Shard graphs and information compartmentalization
  - Why needed here: Understanding how to partition data into shards and connect them via directed graph is fundamental to SAFE's approach of balancing accuracy and forgetting cost
  - Quick check question: How does shard graph structure control trade-off between accuracy (via synergistic information) and forgetting cost (via retraining scope)?

- Concept: Differential privacy and its application to unlearning
  - Why needed here: SAFE-DP extends SAFE by using DP to bound information leakage when sharing data between shards, enabling trade-off between number of forget requests before full retraining and model accuracy
  - Quick check question: How does privacy budget (α,β) in SAFE-DP determine number of forget requests before full retraining is required?

- Concept: Cross-attention adapters and their compositionality
  - Why needed here: InCA adapters are key architectural component that enables SAFE to scale to hundreds of shards by sharing computation and enabling parallel training
  - Quick check question: How does compositionality of cross-attention allow all shard-specific queries to be processed in single forward pass?

## Architecture Onboarding

- Component map:
  Frozen backbone -> InCA adapters (cross-attention modules with shard-specific queries and heads) -> Shard graph (directed graph defining data sharing between shards) -> Prototype classifier (optional regularization component) -> Ensemble mechanism (combining shard-specific classifiers)

- Critical path:
  1. Partition data into shards based on class distribution and desired connectivity level
  2. Construct shard graph (e.g., disjoint cliques for optimal forgetting cost)
  3. Train InCA adapters in parallel on each shard using data from connected shards
  4. At inference, compute logits from all adapters and ensemble them (optionally with prototype classifier)

- Design tradeoffs:
  - Number of shards vs. accuracy: More shards reduce forgetting cost but also reduce accuracy due to loss of synergistic information
  - Shard graph connectivity vs. forgetting cost: Higher connectivity improves accuracy but increases expected cost of forgetting a sample
  - Use of prototypes vs. accuracy: Prototypes provide regularization at high sharding levels but may slightly reduce accuracy at low sharding levels

- Failure signatures:
  - Low accuracy: Backbone not well-aligned with data distribution, suboptimal shard graph structure, or insufficient synergistic information
  - High forgetting cost: Shard graph connectivity not optimized, leading to excessive retraining scope
  - Slow training/inference: Inefficient implementation of InCA adapters or shard graph processing

- First 3 experiments:
  1. Verify InCA adapters can be trained in parallel on small number of shards and their predictions can be ensembled correctly
  2. Test effect of different shard graph structures (isolated shards, disjoint cliques, fully connected) on accuracy and forgetting cost
  3. Evaluate impact of adding prototype classifier on accuracy at different sharding levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can optimal shard graph structure be automatically determined for very large datasets while preventing information leakage and satisfying user constraints?
- Basis in paper: [explicit] Paper discusses importance of constructing shard graphs to maximize synergistic information while minimizing dataset overlap, but notes that automatically inferring best graph structure on very large datasets while preventing information leakage and satisfying user constraints is interesting and important direction left for future work
- Why unresolved: Automatically determining optimal shard graph structure is complex problem that involves balancing multiple factors including synergistic information, information leakage prevention, and user constraints
- What evidence would resolve it: Method that can automatically determine optimal shard graph structure for very large datasets while preventing information leakage and satisfying constraints would resolve this open question

### Open Question 2
- Question: How does accuracy of SAFE compare to other unlearning methods when applied to non-vision datasets, such as natural language processing or tabular data?
- Basis in paper: [inferred] Paper focuses on evaluating SAFE on vision datasets, but does not explore its performance on non-vision datasets. Would be valuable to understand how SAFE generalizes to other data modalities
- Why unresolved: Paper does not provide any evidence of SAFE's performance on non-vision datasets, making it open question how well it would perform in these domains
- What evidence would resolve it: Empirical results comparing SAFE's accuracy to other unlearning methods on non-vision datasets would resolve this open question

### Open Question 3
- Question: How does choice of backbone architecture affect performance of SAFE, and what are trade-offs between using stronger backbones and lightweight adapters?
- Basis in paper: [explicit] Paper mentions accuracy of SAFE may be limited by use of lightweight adapters rather than fine-tuning full network, but also notes baseline performance of adapters is acceptable and can be further improved by using stronger backbones
- Why unresolved: Paper does not provide detailed analysis of how choice of backbone architecture affects SAFE's performance, making it open question what optimal trade-offs are between using stronger backbones and lightweight adapters
- What evidence would resolve it: Systematic study comparing performance of SAFE using different backbone architectures and lightweight adapters would resolve this open question

## Limitations

- Automatic determination of optimal shard graph structure for very large datasets while preventing information leakage and satisfying user constraints remains an open challenge
- Performance on non-vision datasets (NLP, tabular data) has not been evaluated and may differ from vision results
- The choice of backbone architecture and its impact on SAFE's performance has not been systematically studied

## Confidence

**High confidence**: The core concept of using shard graphs to enable selective information sharing while preserving forgetting guarantees is well-supported by theoretical framework and empirical results

**Medium confidence**: The effectiveness of InCA adapters in enabling scalable training and inference on hundreds of shards is supported by evidence, but detailed ablation study on impact of different backbone architectures or cross-attention designs is lacking

**Medium confidence**: The claim that prototype classifiers provide regularization that improves accuracy at high sharding levels is supported by evidence, but detailed ablation study on impact of prototypes across different datasets and sharding levels is lacking

## Next Checks

1. Ablation study on shard graph structures: Evaluate impact of different structures (disjoint cliques, fully connected, random) on accuracy and forgetting cost across multiple datasets and sharding levels

2. Ablation study on InCA adapter architectures: Evaluate impact of different backbone architectures (ViT, ResNet) and cross-attention designs (number of heads, query dimensions) on effectiveness of InCA adapters

3. Ablation study on prototype classifiers: Evaluate impact of prototype classifiers on accuracy across different datasets, sharding levels, and embedding spaces (frozen backbone vs. fine-tuned backbone)