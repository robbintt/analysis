---
ver: rpa2
title: Zero-Shot Conditioning of Score-Based Diffusion Models by Neuro-Symbolic Constraints
arxiv_id: '2308.16534'
source_url: https://arxiv.org/abs/2308.16534
tags:
- constraint
- score
- samples
- constraints
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a method for sampling from unconditional score-based
  generative models under arbitrary logical constraints, without requiring additional
  training. The core idea is to modify the learned score function by adding the gradient
  of a soft constraint function, allowing sampling from an unnormalized distribution
  that combines the original model and the constraint.
---

# Zero-Shot Conditioning of Score-Based Diffusion Models by Neuro-Symbolic Constraints

## Quick Facts
- arXiv ID: 2308.16534
- Source URL: https://arxiv.org/abs/2308.16534
- Authors: 
- Reference count: 8
- One-line primary result: Zero-shot conditional sampling from score-based models using neuro-symbolic constraints without retraining

## Executive Summary
This work proposes a method for sampling from unconditional score-based generative models under arbitrary logical constraints without requiring additional training. The core idea is to modify the learned score function by adding the gradient of a soft constraint function, allowing sampling from an unnormalized distribution that combines the original model and the constraint. A neuro-symbolic framework is introduced to encode logical constraints as differentiable functions, enabling the application of complex constraints to various data types including tabular data, images, and time series.

## Method Summary
The method modifies the learned score function of an unconditional score-based generative model by adding the gradient of a soft constraint function, weighted by a time-dependent factor g(t). This allows sampling from an unnormalized distribution that combines the original model and the constraint. Logical constraints are encoded using a neuro-symbolic framework that converts them into differentiable functions in log-probability space using operations like log-probabilistic sum and product t-norm. The approach is validated on tabular data, images, and time series, showing good performance in approximating conditional distributions and generating high-quality samples while enforcing complex constraints.

## Key Results
- Successfully enforces logical constraints on tabular data (wine quality) and images (MNIST, CelebA) without retraining
- Achieves symmetry constraints on images with reasonable accuracy and efficiency
- Demonstrates constraint satisfaction on time series data (eSIRS model) while maintaining sample quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The conditional score at time t=0 can be computed exactly by adding the gradient of the constraint to the unconditional score.
- Mechanism: The method leverages the mathematical property that the score of a product of distributions (unconditional model times constraint) equals the sum of the scores of each distribution. At t=0, the corrupted distribution is close enough to the original that this exact relationship holds.
- Core assumption: The diffusion process starts with minimal corruption at t=0, making the corrupted distribution effectively equal to the original distribution.
- Evidence anchors:
  - [section]: "At t = 0 one can easily obtain an estimate of the score by summing the gradient of the constraint to the estimate of the score of the unconstrained distribution."
  - [abstract]: "Firstly, we show how to manipulate the learned score in order to sample from an un-normalized distribution conditional on a user-defined constraint."
- Break condition: If the diffusion process adds significant noise even at t=0, the approximation breaks down and the exact relationship no longer holds.

### Mechanism 2
- Claim: The time-dependent constraint guidance g(t) allows smooth interpolation between exact conditional guidance at t=0 and no guidance at t=0.
- Mechanism: By weighting the constraint gradient contribution by g(t), the method gradually reduces the influence of the constraint as noise increases, allowing the reverse diffusion process to work properly while still guiding samples toward satisfying the constraint.
- Core assumption: The constraint gradient can be safely downweighted without losing its guiding effect, as the noise in early diffusion steps provides enough exploration.
- Evidence anchors:
  - [section]: "A simple way to obtain this is by weighting the contribution of the gradient of the constraint depending on time: ˜sc(x, t) = s(x, t) + g(t)∇xc(x)"
  - [corpus]: Weak evidence - the corpus contains papers on "conditional diffusion models" and "zero-shot conditional samplers" but none specifically address time-dependent constraint weighting.
- Break condition: If g(t) decreases too rapidly, the constraint may not guide samples effectively enough; if it decreases too slowly, it may disrupt the reverse diffusion process.

### Mechanism 3
- Claim: The neuro-symbolic framework provides numerically stable, differentiable constraints that approximate hard logical constraints while maintaining useful properties like consistency with negation and conjunctions.
- Mechanism: By defining constraint functions in log-probability space using operations like the log-probabilistic sum and product t-norm, the framework creates soft constraints that are both differentiable and numerically stable for gradient-based optimization.
- Core assumption: The log-probability space operations preserve the logical semantics of the original constraints while being suitable for gradient computation.
- Evidence anchors:
  - [section]: "We choose to define the inequality a(x) ≥ b(x) as c(x) = − ln(1 + e−k(a(x)−b(x))), introducing an extra parameter k that regulates the 'hardness' of the constraint."
  - [abstract]: "Then, we define a flexible and numerically stable neuro-symbolic framework for encoding soft logical constraints."
- Break condition: If the parameter k is set too high, the constraint function may become numerically unstable; if set too low, the constraint may not effectively guide samples.

## Foundational Learning

- Concept: Score-based generative models and denoising score matching
  - Why needed here: The entire method builds on the foundation of score-based models, using the learned score function as the base for conditional sampling
  - Quick check question: What is the relationship between the score function and the gradient of the log-probability density?

- Concept: Diffusion processes and reverse sampling
  - Why needed here: The method uses a diffusion process to gradually add noise, then reverses it while applying constraints, so understanding this process is crucial
  - Quick check question: Why is it necessary to add noise to the data in score-based generative models?

- Concept: Langevin dynamics and MCMC sampling
  - Why needed here: The method employs Langevin dynamics as a corrector step to improve constraint satisfaction at the final sampling stage
  - Quick check question: What is the role of the noise term in the Langevin dynamics update equation?

## Architecture Onboarding

- Component map: Score model → Constraint encoding → Guided sampling (with time-dependent weighting) → Optional Langevin correction → Output samples

- Critical path: Score model → Constraint encoding → Guided sampling (with time-dependent weighting) → Optional Langevin correction → Output samples

- Design tradeoffs:
  - Hard vs. soft constraints: Hard constraints require careful formulation as soft constraints to be differentiable
  - Constraint intensity vs. sample quality: Stronger constraints may reduce sample quality or realism
  - Approximation scheme choice: Different g(t) functions affect both constraint satisfaction and sampling stability

- Failure signatures:
  - Constraint not satisfied: May indicate insufficient constraint intensity or poor choice of approximation scheme
  - Poor sample quality: May indicate constraint is too strong or g(t) is poorly chosen
  - Numerical instability: May indicate constraint function is poorly scaled or k parameter is too large

- First 3 experiments:
  1. Toy 2D Gaussian mixture with simple constraint (e.g., x > 0) to verify basic functionality
  2. Tabular dataset (e.g., wine quality) with logical constraint combining multiple features
  3. Image dataset (e.g., MNIST) with symmetry constraint to test high-dimensional capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical limits of the proposed neuro-symbolic constraint framework in terms of expressiveness and computational complexity for high-dimensional data?
- Basis in paper: [inferred] The paper mentions that the neuro-symbolic framework is flexible but does not explore its theoretical limits, especially for high-dimensional data like images.
- Why unresolved: The paper does not provide a theoretical analysis of the framework's expressiveness or computational complexity, focusing instead on empirical results.
- What evidence would resolve it: A theoretical analysis of the framework's expressiveness and computational complexity, including proofs or simulations showing its limits for high-dimensional data.

### Open Question 2
- Question: How does the choice of the score approximation scheme (g(t)) affect the quality of conditional sampling in terms of constraint satisfaction and sample realism?
- Basis in paper: [explicit] The paper discusses the sensitivity of results to the choice of g(t) and suggests using SNR as the first choice, but does not provide a comprehensive analysis of its effects.
- Why unresolved: The paper only provides a general suggestion for g(t) without exploring its impact on conditional sampling quality.
- What evidence would resolve it: A detailed study comparing different g(t) functions and their effects on constraint satisfaction and sample realism, including quantitative metrics and visualizations.

### Open Question 3
- Question: Can the proposed method be extended to handle constraints that involve temporal dependencies in time series data more effectively?
- Basis in paper: [inferred] The paper demonstrates the application of the method to time series data but does not explore its potential for handling temporal dependencies.
- Why unresolved: The paper focuses on simple constraints and does not address the challenge of temporal dependencies in time series data.
- What evidence would resolve it: An extension of the method to handle temporal dependencies, including experiments and evaluations on time series datasets with complex temporal patterns.

### Open Question 4
- Question: What are the potential applications of the proposed method in real-world scenarios beyond the ones explored in the paper?
- Basis in paper: [explicit] The paper mentions potential applications in surrogate models and image generation but does not explore them in detail.
- Why unresolved: The paper only provides a limited set of applications and does not explore the full potential of the method in real-world scenarios.
- What evidence would resolve it: A comprehensive study of the method's applications in various real-world scenarios, including case studies and evaluations of its effectiveness.

## Limitations
- Limited empirical validation of approximation quality degradation away from t=0
- Insufficient exploration of complex constraint combinations and their formulation challenges
- No comprehensive computational efficiency analysis compared to alternative approaches

## Confidence
- **High Confidence**: The core mathematical framework connecting score-based models with constraint guidance through gradient addition is sound and well-established in the literature
- **Medium Confidence**: The empirical results demonstrating constraint satisfaction on various datasets are convincing for the specific examples shown, but lack comprehensive statistical analysis
- **Low Confidence**: The generalization capability to arbitrary logical constraints remains largely theoretical without systematic analysis of failure modes

## Next Checks
1. **Constraint Complexity Scaling Test**: Systematically evaluate the method's performance as constraint complexity increases from simple univariate constraints to multivariate logical combinations on synthetic datasets where ground truth conditional distributions are known.

2. **Time-dependent Guidance Function Analysis**: Conduct ablation studies varying g(t) functions (SNR, linear, exponential) and their parameters to quantify the trade-off between constraint satisfaction accuracy and sample quality/fidelity across different noise levels.

3. **Comparison with Alternative Approaches**: Implement and compare against established conditional sampling methods including rejection sampling from unconditional models, classifier-based guidance, and conditional retraining to establish the practical advantages and limitations of the proposed zero-shot approach.