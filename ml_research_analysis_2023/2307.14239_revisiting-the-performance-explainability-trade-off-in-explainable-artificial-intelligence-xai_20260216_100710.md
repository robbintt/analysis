---
ver: rpa2
title: Revisiting the Performance-Explainability Trade-Off in Explainable Artificial
  Intelligence (XAI)
arxiv_id: '2307.14239'
source_url: https://arxiv.org/abs/2307.14239
tags:
- explainability
- explainable
- rudin
- performance
- trade-off
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper critically examines the performance-explainability trade-off
  in AI from the perspective of Requirements Engineering. The authors argue against
  the common view that there is a simple trade-off between model performance and explainability.
---

# Revisiting the Performance-Explainability Trade-Off in Explainable Artificial Intelligence (XAI)

## Quick Facts
- arXiv ID: 2307.14239
- Source URL: https://arxiv.org/abs/2307.14239
- Reference count: 40
- Key outcome: Reframes the performance-explainability trade-off as a multi-objective optimization problem incorporating development resources, domain characteristics, and risk considerations

## Executive Summary
This paper critically examines the widely-cited performance-explainability trade-off (PET) in AI systems from the perspective of Requirements Engineering. The authors argue against the common view that there is a simple trade-off between model performance and explainability. Instead, they propose a more nuanced framework that considers development resources, domain characteristics, and risk considerations as key factors constraining feasible solutions. The paper reframes the PET as a sequential decision problem where researchers minimize regret over time, balancing competing objectives of performance, explainability, and time constraints.

## Method Summary
The paper employs philosophical argumentation and literature analysis to examine claims about the performance-explainability trade-off. The authors critically evaluate Rudin's arguments against post-hoc explainability methods and the Rashomon Set argument supporting simpler, more explainable models. They propose a refined framework treating the PET as a multi-objective optimization problem constrained by development resources, domain feature properties, and risk considerations. The method involves theoretical analysis of existing literature, logical argumentation, and proposal of an alternative conceptual framework for approaching the trade-off.

## Key Results
- The performance-explainability trade-off is better viewed as a multi-objective optimization problem rather than a simple trade-off
- Post-hoc explainability techniques, particularly model-specific approaches, have more potential than commonly assumed
- The Rashomon Set argument used to support simpler models is weaker than presented due to unverified assumptions about Rashomon set composition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The paper reframes the PET as a multi-objective optimization problem incorporating time, performance, and explainability
- Mechanism: By treating the trade-off as a sequential decision problem where researchers minimize regret over time, the framework captures how development resources constrain achievable performance-explainability pairs
- Core assumption: The cost function is inversely proportional to a linear combination of performance and explainability, and researchers can accurately estimate the value of different sequences of actions
- Evidence anchors:
  - [abstract] "We argue that it is best approached in a nuanced way that incorporates resource availability, domain characteristics, and considerations of risk."
  - [section IV.B] "we suggest to cast PET+ as a multi-objective optimization problem as follows."
- Break condition: If the linear cost assumption doesn't hold or if researchers cannot accurately estimate action values, the optimization framework breaks down

### Mechanism 2
- Claim: The paper identifies three key factors constraining the PET+: development resources, domain feature properties, and risk considerations
- Mechanism: These factors form a constraint system where resource limits (time, expertise) interact with domain complexity (feature quantity, complexity) and risk preferences to determine feasible performance-explainability outcomes
- Core assumption: The relationship between these three factors is additive rather than multiplicative, and each factor independently constrains the solution space
- Evidence anchors:
  - [section IV.A] "Development Resources, Features, Risks" and Figure 2 explicitly categorizing these factors
  - [abstract] "incorporates development resources, domain characteristics, and considerations of risk."
- Break condition: If factors interact non-linearly or if one factor dominates the others in unexpected ways, the constraint model fails

### Mechanism 3
- Claim: The paper argues that post-hoc explainability techniques have more potential than Rudin acknowledges, particularly model-specific approaches
- Mechanism: By distinguishing between model-agnostic and model-specific explainability methods, the paper shows that fidelity can be achieved through approaches that leverage model internals, countering Rudin's blanket dismissal of post-hoc methods
- Core assumption: Model-specific explainability techniques can provide fidelitous explanations without requiring ante-hoc design, and these explanations are sufficient for high-stakes contexts
- Evidence anchors:
  - [section III.A] "we think that there are post-hoc approaches that can be usefully employed to explain the output of black-box models even in high-stakes situations."
  - [section III.A.a] Discussion of Vanilla Backpropagation and Grad-CAM passing sanity checks
- Break condition: If model-specific techniques cannot consistently achieve high fidelity across different model architectures or domains, the argument for post-hoc methods weakens

## Foundational Learning

- Concept: Multi-objective optimization
  - Why needed here: The paper reframes the PET+ as a problem of balancing competing objectives (performance, explainability, time) rather than a simple trade-off
  - Quick check question: Can you write the formal optimization problem that minimizes cost as a function of performance and explainability over time?

- Concept: Regret minimization in sequential decision making
  - Why needed here: The framework treats model development as a sequence of actions where researchers aim to minimize the difference between actual and optimal outcomes
  - Quick check question: How does the regret framework differ from a static optimization approach in terms of modeling the development process?

- Concept: Feature engineering vs. learned representations
  - Why needed here: The paper contrasts iterative feature engineering approaches with deep learning's ability to discover hidden patterns in data
  - Quick check question: What are the key differences between handcrafted features and learned representations in terms of their ability to capture domain complexity?

## Architecture Onboarding

- Component map: PET+ problem definition -> Three-factor constraint system (resources, features, risks) -> Regret minimization framework -> Feasible solution space identification

- Critical path:
  1. Characterize domain features (complexity, quantity, representation)
  2. Assess available development resources (time, expertise, tools)
  3. Define risk priorities (performance vs explainability weighting)
  4. Apply optimization framework to identify feasible solutions
  5. Validate through case studies or empirical testing

- Design tradeoffs:
  - Simplicity vs. accuracy in the optimization model
  - Generality vs. specificity in constraint formulation
  - Theoretical elegance vs. practical applicability
  - Focus on high-stakes domains vs. broader applicability

- Failure signatures:
  - Linear cost assumption breaks down
  - Constraint factors interact non-linearly
  - Optimization becomes intractable for realistic problem sizes
  - Real-world validation contradicts theoretical predictions

- First 3 experiments:
  1. Compare optimization framework predictions against case studies of successful explainable AI deployments
  2. Test sensitivity of feasible solution space to variations in each constraint factor
  3. Validate the regret minimization approach using synthetic development scenarios with known optimal solutions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Rashomon Set argument actually hold across different domains and datasets, or is it an artifact of specific conditions?
- Basis in paper: [explicit] The authors discuss the Rashomon Set argument and argue that it is weaker than commonly assumed, noting that its conclusions depend on critical assumptions about the size and composition of Rashomon sets that are not rigorously justified
- Why unresolved: The paper points out that the Rashomon Set argument lacks rigorous mathematical proof and may not apply universally across all domains
- What evidence would resolve it: Empirical studies measuring Rashomon ratios across diverse datasets and domains would help determine the argument's validity and scope

### Open Question 2
- Question: Can post-hoc explainability techniques be developed that are both highly faithful to the original model and complete in capturing all relevant factors?
- Basis in paper: [explicit] The authors discuss the potential of post-hoc explainability approaches, noting that while some techniques like model-specific methods may offer high fidelity, achieving both fidelity and completeness simultaneously remains challenging
- Why unresolved: Current research on post-hoc explainability has not fully addressed the trade-off between fidelity and completeness, and there is limited evidence on whether both can be achieved together
- What evidence would resolve it: Systematic evaluation of post-hoc explainability techniques across multiple models and tasks, measuring both fidelity and completeness, would provide insight into this trade-off

### Open Question 3
- Question: How do different types of development resources (time, expertise, tools) affect the ability to achieve both high performance and explainability in AI models?
- Basis in paper: [inferred] The authors argue that development resources are a crucial factor in the performance-explainability trade-off and suggest that more resources can enable Pareto improvements in both performance and explainability
- Why unresolved: The paper does not provide a quantitative framework for measuring the impact of different resources on model development outcomes
- What evidence would resolve it: Empirical studies tracking resource allocation and model outcomes across multiple projects would help establish the relationship between resources and the ability to achieve both high performance and explainability

## Limitations

- The theoretical framework relies heavily on philosophical argumentation rather than empirical validation
- The three-factor constraint model may oversimplify complex interactions between development resources, domain characteristics, and risk considerations
- Claims about post-hoc explainability technique potential lack comprehensive empirical support across diverse model architectures and domains

## Confidence

- The critique of the Rashomon Set argument: High
- The reframing of PET as multi-objective optimization: Medium
- The assertion that post-hoc methods can be sufficiently fidelitous: Medium
- The three-factor constraint model: Medium

## Next Checks

1. **Empirical validation of the optimization framework**: Test the proposed multi-objective optimization approach against case studies of successful explainable AI deployments, measuring how well predicted feasible solution spaces match actual development outcomes

2. **Constraint factor interaction analysis**: Systematically vary development resources, domain complexity, and risk priorities in controlled experiments to determine whether these factors interact linearly or exhibit non-linear dependencies

3. **Post-hoc method fidelity evaluation**: Conduct comprehensive benchmarking of model-specific post-hoc explainability techniques across multiple model architectures and domains, measuring fidelity scores and comparing against ante-hoc approaches in high-stakes contexts