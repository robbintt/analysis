---
ver: rpa2
title: Chain-of-Thought Reasoning is a Policy Improvement Operator
arxiv_id: '2309.08589'
source_url: https://arxiv.org/abs/2309.08589
tags:
- training
- addition
- arxiv
- reasoning
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores whether large language models can teach themselves
  new skills without human-generated data, focusing on addition as a benchmark task.
  The authors introduce SECToR (Self-Education via Chain-of-Thought Reasoning), which
  uses chain-of-thought reasoning as a policy improvement operator to enable self-learning.
---

# Chain-of-Thought Reasoning is a Policy Improvement Operator

## Quick Facts
- arXiv ID: 2309.08589
- Source URL: https://arxiv.org/abs/2309.08589
- Reference count: 23
- 582M parameter ByT5 model autonomously learns to add numbers with up to 29 digits with 98+% accuracy

## Executive Summary
This paper introduces SECToR (Self-Education via Chain-of-Thought Reasoning), a method enabling language models to teach themselves new skills without human-generated data beyond initial supervised fine-tuning. The key insight is treating chain-of-thought reasoning as a policy improvement operator that allows models to solve problems they cannot directly solve. By generating solutions through reasoning and then fine-tuning on those solutions, models can progressively learn to handle increasingly complex tasks.

The approach demonstrates that language models can achieve significant self-improvement, with a 582M parameter ByT5 model learning to perform 29-digit addition with 98+% accuracy after an initial supervised phase on 6-digit numbers. This represents a substantial extension of model capabilities through self-learning, showing that chain-of-thought reasoning can serve as a bridge between what models can currently do and what they can learn to do.

## Method Summary
SECToR uses a self-training loop where a pre-trained language model first undergoes supervised fine-tuning on simple addition problems (1-6 digits). The model then uses chain-of-thought reasoning to solve harder problems it cannot directly answer, generating correct solutions through step-by-step reasoning. These solutions are filtered through self-consistency checks (simplify-then-guess and commutativity verification) to reduce error accumulation, then used to fine-tune the model to directly produce answers without reasoning. The process repeats with increasingly difficult problems following a curriculum learning schedule, where satisfactory performance on N-digit addition is required before progressing to N+1 digit problems.

## Key Results
- 582M parameter ByT5 model achieves 98+% accuracy on 29-digit addition without chain-of-thought reasoning
- Self-learning approach extends model capabilities beyond initial 6-digit supervised training distribution
- Self-consistency checks effectively mitigate error avalanching during self-training

## Why This Works (Mechanism)

### Mechanism 1: Chain-of-Thought as Policy Improvement Operator
Chain-of-thought reasoning acts as a policy improvement operator, analogous to MCTS in AlphaZero, allowing models to surpass their baseline performance. By prompting models to use chain-of-thought reasoning to solve problems they cannot directly solve, the model generates correct solutions that are then distilled into direct answer generation capability through fine-tuning.

### Mechanism 2: Curriculum Learning for Gradual Complexity
Curriculum learning enables systematic progression through increasingly complex tasks. The model masters simpler addition problems before advancing to harder ones, ensuring stable learning progression and preventing overwhelming the model with complexity it cannot handle.

### Mechanism 3: Self-Consistency for Error Mitigation
Self-consistency checks (simplify-then-guess and commutativity verification) prevent error avalanching by ensuring generated training data is accurate. Multiple independent solution attempts and verification between equivalent problem formulations filter out incorrect examples before they corrupt the training process.

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: Enables breaking down complex problems into solvable steps that models can handle
  - Quick check question: Can the model solve a complex addition problem by decomposing it into simpler steps?

- Concept: Curriculum learning
  - Why needed here: Provides structured progression that prevents overwhelming the model while ensuring mastery at each level
  - Quick check question: Does the model achieve satisfactory performance on simpler tasks before advancing to harder ones?

- Concept: Error avalanching and mitigation
  - Why needed here: Prevents compounding errors during self-training that would otherwise degrade model performance
  - Quick check question: Do self-consistency checks effectively identify and filter out erroneous training examples?

## Architecture Onboarding

- Component map: Pre-trained language model -> Supervised fine-tuning (curriculum) -> Self-training with consistency checks -> Model checkpointing
- Critical path: 1) Pre-trained model 2) Supervised fine-tuning with curriculum 3) Self-training with self-consistency checks 4) Model checkpointing and evaluation
- Design tradeoffs: Larger models may need less supervised fine-tuning but cost more; stricter consistency checks reduce errors but slow training; curriculum ensures stability but may slow progress
- Failure signatures: Model fails to generalize beyond training distribution; error avalanching causes accuracy drops; consistency checks filter too much data
- First 3 experiments: 1) Test direct vs. chain-of-thought performance on addition problems 2) Track curriculum progression effectiveness 3) Compare model performance with and without self-consistency checks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can language models achieve truly open-ended self-learning, autonomously discovering and solving novel problems without human-defined tasks?
- Basis in paper: [explicit] Future direction asking whether SECToR generalizes to complex tasks and whether models can self-discover the learning process itself
- Why unresolved: Current work only demonstrates self-learning for specific, well-defined mathematical tasks with structured curriculum
- What evidence would resolve it: A model that autonomously identifies novel problems, defines solution approaches, and iteratively improves without human intervention

### Open Question 2
- Question: What is the theoretical limit of self-learning in language models? Will larger models eventually self-improve indefinitely or face insurmountable error accumulation?
- Basis in paper: [explicit] Notes that self-learning eventually terminates due to accumulated errors, hypothesizes larger models might forgo supervised training entirely
- Why unresolved: Only tested up to 29-digit addition; unclear if error accumulation is fundamental or surmountable
- What evidence would resolve it: Demonstrating indefinite self-improvement on increasingly complex tasks, or proving theoretical bounds on self-learning limits

### Open Question 3
- Question: How does self-learning impact model safety and alignment with human values?
- Basis in paper: [explicit] Discusses safety concerns about amplifying biases or erroneous information, questions about controlling models with unpredictable capabilities
- Why unresolved: Paper focuses on mathematical tasks where safety concerns are minimal; impact on broader domains is unknown
- What evidence would resolve it: Empirical studies showing self-learning effects on safety-critical tasks, or theoretical frameworks for ensuring alignment during self-learning

## Limitations
- Only validated on addition tasks with ByT5 architecture, limiting generalizability to other operations or non-mathematical reasoning
- Error avalanching remains a fundamental challenge requiring extensive consistency checking, suggesting scalability issues for complex domains
- Curriculum learning relies on achieving specific performance thresholds that may not always be attainable or optimal

## Confidence
**High Confidence:**
- Chain-of-thought reasoning generates correct solutions for problems models cannot directly solve
- Self-consistency checks effectively reduce error propagation during self-training
- Self-training approach extends model capabilities beyond initial training distribution

**Medium Confidence:**
- Chain-of-thought acts as policy improvement operator analogous to MCTS
- Curriculum learning enables systematic progression to complex problems
- Specific thresholds and parameters (75% accuracy, 128-example validation) are optimal

**Low Confidence:**
- Approach will generalize to other mathematical operations or non-mathematical tasks
- Method will scale efficiently to larger models or more complex domains
- Specific implementation details are the only viable configuration

## Next Checks
1. **Cross-task Generalization Test**: Apply SECToR to multiplication and division problems to determine if policy improvement operator effect holds across arithmetic operations and whether error mitigation remains effective.

2. **Model Size Scaling Analysis**: Repeat experiments with smaller (250M parameters) and larger (1B+ parameters) models to assess whether effectiveness scales predictably with model capacity and whether larger models require less supervised fine-tuning.

3. **Curriculum Flexibility Evaluation**: Test alternative threshold values (e.g., 60% vs 75% accuracy) and non-linear progression schedules to quantify sensitivity to design choices and identify optimal configuration ranges.