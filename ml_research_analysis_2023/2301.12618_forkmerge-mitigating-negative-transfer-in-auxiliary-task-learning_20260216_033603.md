---
ver: rpa2
title: 'ForkMerge: Mitigating Negative Transfer in Auxiliary-Task Learning'
arxiv_id: '2301.12618'
source_url: https://arxiv.org/abs/2301.12618
tags:
- task
- tasks
- learning
- forkmerge
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses negative transfer in multi-task learning (MTL),
  where learning multiple tasks simultaneously can degrade performance compared to
  learning tasks individually. The authors analyze negative transfer from optimization,
  generalization, and hypothesis space perspectives, finding that it's related to
  distribution shift and hypothesis distance rather than just gradient conflicts.
---

# ForkMerge: Mitigating Negative Transfer in Auxiliary-Task Learning

## Quick Facts
- arXiv ID: 2301.12618
- Source URL: https://arxiv.org/abs/2301.12618
- Reference count: 40
- This paper proposes ForkMerge, a multi-task learning method that mitigates negative transfer through dynamic task weight search and parameter merging, achieving up to 4.59% performance improvement on NYUv2.

## Executive Summary
ForkMerge addresses negative transfer in multi-task learning by periodically forking the model into multiple branches with different task weight combinations, optimizing each branch independently, and merging them based on validation performance. The method challenges the conventional view that gradient conflicts are the primary cause of negative transfer, instead identifying distribution shift and hypothesis distance as key factors. Through extensive experiments on diverse benchmarks including NYUv2, DomainNet, QM9, and AliExpress, ForkMerge significantly outperforms existing methods while requiring only O(log K) additional computational cost through effective branch pruning.

## Method Summary
ForkMerge operates by periodically forking the current model parameters into K branches, each optimized with different task weight combinations for a fixed number of steps (Δ_t). After optimization, each branch is evaluated on validation data, and the method searches for optimal linear combination weights to merge the branches while maximizing target validation performance. The merged parameters are then synchronized across all branches, and the process repeats. The method cycles through each task as the target with others as auxiliaries, allowing dynamic discovery of beneficial task combinations while avoiding harmful ones. Branch pruning strategies reduce computational complexity from O(K²) to O(log K) while maintaining performance.

## Key Results
- Achieved average performance improvements of up to 4.59% on NYUv2 and 3.00% on DomainNet
- Outperformed existing methods including EW, MGDA, PCGrad, GradNorm, Auto-λ, and PTFT across all tested benchmarks
- Demonstrated effectiveness across diverse tasks: molecule regression, scene understanding, CTR prediction, and image recognition

## Why This Works (Mechanism)

### Mechanism 1
ForkMerge mitigates negative transfer by dynamically adjusting task weights based on validation performance rather than gradient conflicts. The method periodically forks the model into multiple branches with different task weights, optimizes each branch independently for Δ_t steps, then merges branches by linear combination to maximize target validation performance. This works under the assumption that validation performance on the target task reliably indicates beneficial task combinations, even when gradient conflicts exist.

### Mechanism 2
Negative transfer occurs primarily due to distribution shift between multi-task training data and target test data, not gradient conflicts. By extending optimization from one step to Δ_t steps before merging, ForkMerge allows exploration of longer-term generalization effects and reduces noise in task weight estimation. This approach assumes distribution shift is a primary driver of negative transfer and that longer optimization horizons better capture generalization impacts.

### Mechanism 3
Asymmetric task relationships require treating target and auxiliary tasks differently to avoid negative transfer. ForkMerge cycles through each task as target with others as auxiliaries, allowing dynamic discovery of beneficial task combinations while avoiding harmful ones. This assumes task relationships in MTL are asymmetric and dynamic, requiring flexible treatment rather than fixed grouping.

## Foundational Learning

- **Concept**: Multi-task learning optimization dynamics
  - **Why needed here**: Understanding how gradient conflicts, distribution shifts, and hypothesis distances interact is crucial for grasping why ForkMerge's approach differs from traditional methods
  - **Quick check question**: Why might gradient conflicts that slow optimization still lead to better generalization in some cases?

- **Concept**: Linear combination hypothesis spaces
  - **Why needed here**: ForkMerge's merging mechanism relies on the assumption that combining model parameters linearly can approach optimal hypotheses
  - **Quick check question**: Under what conditions would linear parameter combination fail to capture beneficial task interactions?

- **Concept**: Bi-level optimization
  - **Why needed here**: ForkMerge's task weight search involves optimizing inner model parameters while searching for optimal outer task weights
  - **Quick check question**: How does extending optimization from one step to Δ_t steps affect the computational complexity and noise in bi-level optimization?

## Architecture Onboarding

- **Component map**: Model forking module -> Independent optimization module -> Validation performance estimator -> Linear combination searcher -> Parameter merging module -> Branch pruning module

- **Critical path**: 
  1. Fork initial parameters into K branches
  2. Optimize each branch independently for Δ_t steps
  3. Evaluate validation performance of each branch
  4. Search optimal linear combination weights
  5. Merge branches using optimal weights
  6. Synchronize merged parameters across branches
  7. Repeat until convergence

- **Design tradeoffs**:
  - Number of branches (K) vs computation cost: More branches capture more task combinations but increase computation quadratically
  - Merge interval (Δ_t) vs optimization quality: Longer intervals allow better exploration but increase risk of parameter drift
  - Branch pruning vs completeness: Pruning reduces computation but may miss beneficial task combinations
  - Validation-based vs gradient-based weight selection: Validation provides better generalization signals but requires additional computation

- **Failure signatures**:
  - Validation performance plateaus despite continued training
  - Task weight distributions become extremely skewed (all weight on one branch)
  - Gradient norms explode or vanish during branch optimization
  - Distribution shift between validation and test sets increases over time

- **First 3 experiments**:
  1. Run ForkMerge on NYUv2 with varying Δ_t values (1, 10, 30 epochs) to find optimal merge frequency
  2. Test branch pruning effectiveness by comparing pruned vs unpruned versions on DomainNet
  3. Validate asymmetric task relationship handling by comparing ForkMerge with symmetric task weighting baselines

## Open Questions the Paper Calls Out

- **Open Question 1**: How can ForkMerge be effectively combined with task grouping methods to optimize performance with limited model capacity?
  - **Basis in paper**: The paper mentions that ForkMerge is complementary to different multi-task architectures and that combining it with task grouping methods could be a future research direction.
  - **Why unresolved**: While the paper suggests this as a potential direction, it does not provide empirical results or theoretical analysis of such combinations.
  - **What evidence would resolve it**: Experimental results comparing ForkMerge with and without task grouping on datasets with varying model capacities, along with analysis of how task relationships influence the effectiveness of this combination.

- **Open Question 2**: What is the optimal merging interval (ΔT) for ForkMerge across different datasets and task types?
  - **Basis in paper**: The paper shows that the optimal ΔT varies by dataset (10 epochs for NYUv2) and discusses the trade-offs between short-term and long-term parameter updates.
  - **Why unresolved**: The paper only provides specific ΔT values for individual datasets without a general framework for determining optimal intervals.
  - **What evidence would resolve it**: A systematic study varying ΔT across multiple datasets with different characteristics, potentially leading to a principled method for selecting ΔT based on dataset/task properties.

- **Open Question 3**: How does ForkMerge's performance compare to task weighting methods when computational resources are severely constrained?
  - **Basis in paper**: The paper notes that ForkMerge's computational cost is O(log K) with pruning and that it doesn't require more time than most other MTL methods, but doesn't directly compare resource efficiency.
  - **Why unresolved**: The paper focuses on performance improvements rather than direct resource efficiency comparisons under constraints.
  - **What evidence would resolve it**: Controlled experiments comparing ForkMerge to gradient balancing methods under identical computational budgets, measuring both performance and resource utilization.

## Limitations

- The paper lacks formal theoretical analysis of convergence rates or regret bounds for the ForkMerge algorithm
- ForkMerge still requires maintaining multiple model copies and performing additional validation evaluations, potentially limiting scalability to very large models or datasets
- The effectiveness depends on the assumption that distribution shift is the primary driver of negative transfer, which may not hold in all multi-task learning scenarios

## Confidence

- **High confidence**: The empirical results demonstrating performance improvements across diverse benchmarks and the fundamental insight that negative transfer is not solely caused by gradient conflicts
- **Medium confidence**: The claim that validation performance is a reliable signal for beneficial task combinations, given potential distribution shifts between validation and test sets
- **Low confidence**: The theoretical analysis of why extending optimization from one step to Δ_t steps specifically addresses negative transfer

## Next Checks

1. **Robustness to validation set distribution**: Evaluate ForkMerge's performance when validation data distribution differs significantly from test distribution to verify the reliability of validation-based merging

2. **Theoretical convergence analysis**: Develop formal convergence guarantees for the bi-level optimization problem in ForkMerge, particularly analyzing the trade-off between merge interval Δ_t and optimization stability

3. **Comparison with oracle baselines**: Compare ForkMerge against an oracle that has access to optimal task weightings to quantify the suboptimality gap and identify scenarios where ForkMerge may fail