---
ver: rpa2
title: 'PostRainBench: A comprehensive benchmark and a new model for precipitation
  forecasting'
arxiv_id: '2310.02676'
source_url: https://arxiv.org/abs/2310.02676
tags:
- rain
- learning
- precipitation
- forecasting
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PostRainBench, a comprehensive multi-variable
  benchmark for precipitation forecasting and proposes CAMT, a Channel Attention Enhanced
  Multi-task Learning framework. CAMT addresses challenges in NWP post-processing
  including variable selection, class imbalance, and lack of unified benchmarks.
---

# PostRainBench: A comprehensive benchmark and a new model for precipitation forecasting

## Quick Facts
- arXiv ID: 2310.02676
- Source URL: https://arxiv.org/abs/2310.02676
- Reference count: 21
- First deep learning model to outperform NWP in heavy rain CSI (31.8% improvement)

## Executive Summary
This paper introduces PostRainBench, a comprehensive multi-variable benchmark for precipitation forecasting that addresses key challenges in NWP post-processing. The authors propose CAMT (Channel Attention Enhanced Multi-task Learning), which integrates a channel attention module with a Swin-Unet backbone and employs a hybrid weighted loss function. Extensive experiments across three diverse datasets demonstrate that CAMT outperforms state-of-the-art methods by 6.3%, 4.7%, and 26.8% in rain CSI, and notably achieves improvements of 15.6%, 17.4%, and 31.8% over NWP predictions in heavy rain CSI, marking the first time a deep learning model surpasses NWP in extreme precipitation conditions.

## Method Summary
CAMT is a Channel Attention Enhanced Multi-task Learning framework that refines NWP predictions through selective variable weighting and joint classification-regression objectives. The model uses a Swin-Unet backbone with channel attention to process NWP input variables, then applies multi-task heads for both rain level classification and intensity regression. A hybrid weighted loss function balances these competing objectives while addressing class imbalance through dataset-specific weights. The framework is evaluated on three datasets (Korea, Germany, China) with varying variable counts, timesteps, and resolutions, all interpolated to 64×64 grids.

## Key Results
- CAMT outperforms state-of-the-art methods by 6.3%, 4.7%, and 26.8% in rain CSI across three datasets
- First deep learning model to outperform NWP in heavy rain CSI (31.8% improvement)
- Ablation studies show channel attention and weighted loss are critical for performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Channel Attention Module (CAM) selectively weights NWP input variables based on their spatial context, enabling the model to prioritize relevant meteorological features.
- Mechanism: CAM aggregates spatial information using both average-pooling and max-pooling operations, producing a channel attention map that is applied to the input before feeding it into the backbone. This map is generated by a shared multi-layer perceptron (MLP) that processes the pooled descriptors, and a residual connection adds the attention map back to the original input.
- Core assumption: The most informative meteorological variables exhibit distinct spatial patterns that can be captured through pooling operations and shared MLP weights.
- Evidence anchors:
  - [abstract] "integrates a channel attention module with a Swin-Unet backbone"
  - [section 4.1] "The first part is a channel attention module (Woo et al., 2018)"
  - [corpus] Weak evidence; no direct citations about CAM performance in precipitation post-processing.
- Break condition: If meteorological variables do not exhibit clear spatial context patterns, or if the reduction ratio r = 16 is too aggressive, the attention map may fail to distinguish useful variables, leading to degraded performance.

### Mechanism 2
- Claim: The hybrid weighted loss function balances the competing demands of classification and regression tasks while addressing class imbalance in precipitation data.
- Mechanism: The loss combines weighted cross-entropy (Lcls) for classification and mean squared error (Lreg) for regression, scaled by a hyperparameter α. Class weights wc are derived from dataset distributions to mitigate imbalance.
- Core assumption: Precipitation forecasting benefits from joint classification (rain levels) and regression (intensity values), and that weighted loss can correct for skewed distributions without harming majority class performance.
- Evidence anchors:
  - [abstract] "employs a hybrid weighted loss function for multi-task learning"
  - [section 4.2] "we apply class weights wc based on the class distribution of each dataset"
  - [section 5.3.1] Ablation shows performance drops when weighted loss is removed, especially for heavy rain CSI.
- Break condition: If α is set too high, regression may dominate and degrade classification accuracy; if too low, regression may provide insufficient signal for fine-grained intensity prediction.

### Mechanism 3
- Claim: Swin-Unet provides a hierarchical, local-global feature learning structure that is better suited for multi-variable, multi-scale precipitation patterns than traditional CNNs or ViTs alone.
- Mechanism: Swin-Unet uses a U-shaped encoder-decoder with shifted window self-attention blocks, enabling both local and global context modeling at multiple scales. Skip connections preserve spatial resolution.
- Core assumption: Precipitation patterns exhibit both local texture (e.g., storm edges) and global structure (e.g., large-scale weather systems) that require hierarchical feature extraction.
- Evidence anchors:
  - [abstract] "integrates a channel attention module with a Swin-Unet backbone"
  - [section A.2] Describes Swin-Unet's encoder-decoder structure with patch merging/expanding layers.
  - [corpus] No direct performance comparison with other backbones in the paper; ablation in Table 4 shows ViT+CAMT underperforms Swin-Unet+CAMT.
- Break condition: If the spatial resolution or variable interdependencies are not well captured by Swin's fixed window sizes, performance may degrade, especially in heterogeneous terrains.

## Foundational Learning

- Concept: Numerical Weather Prediction (NWP) post-processing
  - Why needed here: This paper focuses on refining NWP model outputs, not raw observations, so understanding the NWP-to-deep-learning pipeline is essential.
  - Quick check question: What distinguishes NWP post-processing from direct precipitation nowcasting?

- Concept: Multi-task learning with hybrid loss
  - Why needed here: The model outputs both classification and regression forecasts; balancing them is key to optimal performance.
  - Quick check question: How does the hyperparameter α influence the trade-off between classification and regression objectives?

- Concept: Channel attention in vision transformers
  - Why needed here: CAM is a core differentiator in this architecture; knowing how it aggregates spatial context is crucial for debugging.
  - Quick check question: What is the role of the reduction ratio r in the channel attention module, and how might changing it affect model capacity?

## Architecture Onboarding

- Component map:
  NWP input → CAM → Swin-Unet encoder → bottleneck → Swin-Unet decoder → classification head + regression head → hybrid loss

- Critical path:
  NWP input → CAM → Swin-Unet encoder → bottleneck → Swin-Unet decoder → classification head + regression head → hybrid loss

- Design tradeoffs:
  - Using Swin-Unet instead of vanilla U-Net or ViT: better local-global feature modeling but higher complexity.
  - Channel attention vs. expert variable selection: more flexible but requires careful tuning of reduction ratio.
  - Multi-task learning: improves overall forecasting but may require careful balancing of α.

- Failure signatures:
  - Overfitting on small datasets: monitor validation loss gap; consider data augmentation or regularization.
  - Heavy rain under-prediction: check class weights and α; inspect attention map sparsity.
  - Low accuracy despite high CSI: indicates imbalanced dataset; consider additional metrics or resampling.

- First 3 experiments:
  1. Run CAMT with Swin-Unet backbone, varying α (e.g., 50, 100, 200) to observe classification vs. regression trade-off.
  2. Replace Swin-Unet with ViT backbone (as in ablation) to test backbone sensitivity.
  3. Disable CAM module to quantify its contribution to variable selection and performance.

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions. However, based on the content, several important questions remain:

- How can the performance gap between NWP and deep learning models for heavy rain prediction be further reduced, particularly in datasets with severe class imbalance?
- What is the optimal number and selection of input variables for NWP post-processing to maximize forecasting accuracy?
- How does the performance of the CAMT model vary with different lead times and spatial resolutions?
- Can the CAMT framework be effectively extended to other meteorological variables beyond precipitation?

## Limitations

- Limited geographic diversity in evaluation datasets (Korea, Germany, China only)
- No ablation studies for optimal CAM reduction ratio selection
- Performance gains over NWP not validated on additional datasets
- Swin-Unet superiority not conclusively established against other modern architectures

## Confidence

- **High confidence**: The hybrid loss function improves multi-task performance (supported by ablation)
- **Medium confidence**: CAMT's overall performance superiority (consistent across datasets but limited geographic diversity)
- **Medium confidence**: Swin-Unet backbone effectiveness (better than ViT in ablation but no comparison to other modern backbones)

## Next Checks

1. Conduct ablation studies varying the CAM reduction ratio r to determine optimal value and robustness.
2. Test CAMT on additional NWP post-processing datasets from different geographic regions to verify generalization.
3. Compare Swin-Unet backbone performance against other modern architectures (e.g., ConvNeXt, EfficientNet) to establish its relative advantage.