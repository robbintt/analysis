---
ver: rpa2
title: Do Androids Know They're Only Dreaming of Electric Sheep?
arxiv_id: '2312.17249'
source_url: https://arxiv.org/abs/2312.17249
tags:
- hallucination
- response
- hallucinations
- probe
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a method for detecting hallucinations in
  grounded generation tasks by training probes on the internal representations of
  transformer language models. The authors create a high-quality dataset of annotated
  hallucinations across three tasks: abstractive summarization, knowledge-grounded
  dialogue generation, and data-to-text.'
---

# Do Androids Know They're Only Dreaming of Electric Sheep?

## Quick Facts
- arXiv ID: 2312.17249
- Source URL: https://arxiv.org/abs/2312.17249
- Reference count: 13
- Key outcome: Probes trained on transformer hidden states can detect hallucinations in grounded generation tasks, with organic hallucination training generalizing better than synthetic

## Executive Summary
This paper introduces a method for detecting hallucinations in grounded generation tasks by training probes on transformer internal representations. The authors create a high-quality dataset of annotated hallucinations across three tasks and train probes of varying complexity on both synthetic and organic hallucinations. They find that probes trained on organic data generalize better and can reliably detect hallucinations at many transformer layers, outperforming several baselines and even surpassing expert human annotators in response-level detection F1.

## Method Summary
The authors train probe classifiers on transformer hidden states to detect hallucinations during grounded generation. They generate organic responses using Llama-2 with controlled sampling, create synthetic hallucinations by editing reference inputs/outputs, and annotate for span-level hallucinations. Single-layer probes (linear and attention-pooling) are trained on hidden states from each transformer sublayer, then combined into ensemble classifiers. The approach is evaluated across three tasks with both organic and synthetic test data, comparing against baseline hallucination detection methods using F1 scores at response and span levels.

## Key Results
- Probes trained on organic hallucinations generalize better than those trained on synthetic hallucinations
- Ensemble probes achieve higher response-level F1 than expert human annotators
- Extrinsic hallucinations are more salient in transformer representations than intrinsic hallucinations
- Probe accuracy varies significantly across transformer layers, with many layers showing reliable detection capability

## Why This Works (Mechanism)

### Mechanism 1
Hallucination signals are encoded in transformer hidden states before the hallucinatory token is generated. The model's internal representations capture the mismatch between grounding knowledge and generation intent early in the decoding process. These signals are detectable by linear probes trained on the hidden states.

### Mechanism 2
Probes trained on organic hallucinations generalize better than those trained on synthetic hallucinations because organic hallucinations reflect the actual distribution of errors made by the model during generation, while synthetic hallucinations are artificially created and may not capture the same internal representations.

### Mechanism 3
Extrinsic hallucinations are more salient in transformer internal representations than intrinsic hallucinations because the model's hidden states more clearly encode the absence of grounding information for extrinsic hallucinations (where information is simply not supported) compared to intrinsic hallucinations (where information directly contradicts).

## Foundational Learning

- **Transformer architecture and hidden states**: Understanding how transformers process information is essential since the approach depends on probing internal representations. Quick check: What are the two main sublayers in each transformer layer and what do they compute?

- **Linear probes and representation analysis**: The method uses linear classifiers trained on hidden states, requiring knowledge of probe methodology. Quick check: How does a linear probe differ from a full fine-tuning approach in terms of what it learns about the model's representations?

- **Grounded generation and hallucination taxonomy**: The work operates in the context of in-context grounded generation where hallucinations are defined relative to provided knowledge sources. Quick check: What's the difference between intrinsic and extrinsic hallucinations in the context of grounded generation?

## Architecture Onboarding

- **Component map**: Prompt (history, knowledge, instruction) → Response model (Llama-2) → Hidden states during generation → Probe classifiers (linear or attention-pooling) → Ensemble classifier → Hallucination predictions

- **Critical path**: 1. Generate responses using Llama-2 with controlled sampling, 2. Extract hidden states during forced decoding of responses, 3. Annotate hallucinations at span level, 4. Train probe classifiers on hidden states, 5. Ensemble single-layer probes, 6. Evaluate on held-out data

- **Design tradeoffs**: Single-layer vs ensemble probes (simplicity vs comprehensive signal), Linear vs attention-pooling (simplicity vs context consideration), Synthetic vs organic training data (ease vs generalization), Token-level vs response-level detection (granularity vs sufficiency)

- **Failure signatures**: Poor performance on organic data when trained on synthetic data, layer-specific performance drops, task-specific failure, model size dependencies

- **First 3 experiments**: 1. Train linear probes on synthetic data and evaluate on organic data to demonstrate ecological invalidity, 2. Compare single-layer probe performance across different transformer layers to identify where signals are strongest, 3. Train ensemble probes on organic data and compare against baseline hallucination detection methods

## Open Questions the Paper Calls Out

### Open Question 1
How do probe architectures perform when trained on a mixture of organic and synthetic hallucinations versus training exclusively on one modality? The paper shows that probes trained on organic data generally outperform those trained on synthetic data, but doesn't explore whether a mixed training approach could bridge the gap between modalities and improve generalization.

### Open Question 2
Can probe architectures be designed to distinguish between task-specific confounds and true hallucination signals to improve cross-task generalization? The paper finds that probes don't generalize well across tasks, suggesting they may be overfitting to task-specific features rather than learning general hallucination patterns.

### Open Question 3
How does the choice of annotation boundary criteria affect probe performance and what would be the optimal annotation strategy for training hallucination detectors? The paper notes that annotators often disagree on the exact boundaries of hallucinated spans, suggesting this could impact probe training and evaluation.

## Limitations
- Ecological invalidity of synthetic hallucination training data leading to poor generalization to organic LLM-generated hallucinations
- Lack of cross-model generalization testing across different transformer architectures
- Assumption that hallucination signals are linearly separable in hidden states may not hold for more complex patterns

## Confidence

- **High Confidence**: Ensemble probes trained on organic hallucinations outperform baselines and expert human annotators at response-level detection
- **Medium Confidence**: Extrinsic hallucinations are more salient in transformer representations than intrinsic hallucinations
- **Low Confidence**: Generalizability of probe performance across different model sizes and hidden state types

## Next Checks

1. **Cross-Architecture Validation**: Test the probe methodology on non-Llama transformer models (e.g., GPT, OPT, or BERT variants) to assess whether hallucination signals are similarly encoded across different transformer architectures.

2. **Synthetic Data Quality Analysis**: Conduct a detailed ablation study comparing different synthetic hallucination generation methods to understand which aspects of synthetic data creation lead to ecological invalidity.

3. **Temporal Stability Assessment**: Evaluate probe performance across multiple generations of the same model (e.g., different Llama versions) and across different training checkpoints to understand how stable the hallucination detection capability is over model evolution.