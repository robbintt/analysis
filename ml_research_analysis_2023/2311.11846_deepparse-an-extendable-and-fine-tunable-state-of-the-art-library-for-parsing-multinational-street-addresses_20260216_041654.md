---
ver: rpa2
title: 'Deepparse : An Extendable, and Fine-Tunable State-Of-The-Art Library for Parsing
  Multinational Street Addresses'
arxiv_id: '2311.11846'
source_url: https://arxiv.org/abs/2311.11846
tags:
- address
- parsing
- addresses
- deepparse
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Deepparse is an open-source Python library for parsing multinational
  street addresses. It uses state-of-the-art deep learning models to achieve high
  parsing accuracy across over 60 countries and multiple languages.
---

# Deepparse : An Extendable, and Fine-Tunable State-Of-The-Art Library for Parsing Multinational Street Addresses

## Quick Facts
- arXiv ID: 2311.11846
- Source URL: https://arxiv.org/abs/2311.11846
- Authors: [List not provided in source]
- Reference count: 4
- Key outcome: Deepparse achieves 99% parsing accuracy on over 60 countries using deep learning with optional fine-tuning.

## Executive Summary
Deepparse is an open-source Python library for parsing multinational street addresses. It uses state-of-the-art deep learning models to achieve high parsing accuracy across over 60 countries and multiple languages. The library offers four pre-trained models based on different embedding techniques (fastText and MultiBPEmb) and tagging architectures (with and without attention mechanisms). Deepparse achieves an average parsing accuracy of 99% on the countries used for training without requiring complex preprocessing or postprocessing. Additionally, the library supports fine-tuning with new data to create custom address parsers tailored to specific needs.

## Method Summary
Deepparse uses a sequence-to-sequence (Seq2Seq) tagging architecture with BiLSTM encoder and optional attention mechanisms. It employs two embedding strategies: fastText (monolingual French embeddings for Latin-script languages) and MultiBPEmb (subword tokenization with BiLSTM for multilingual coverage). The library provides four pre-trained models combining these embeddings with and without attention. No complex preprocessing or postprocessing is required, and the system supports fine-tuning on custom datasets through a retrain() method.

## Key Results
- Achieves 99% parsing accuracy on trained countries without preprocessing/postprocessing
- Supports over 60 countries and multiple languages
- Offers four pre-trained models with different embedding and architecture combinations
- Enables fine-tuning for custom address parsers
- Slightly slower than Libpostal in inference time but more flexible

## Why This Works (Mechanism)

### Mechanism 1
The dual embedding strategy (fastText + MultiBPEmb) enables multilingual parsing without language detection overhead. fastText uses fixed pre-trained monolingual embeddings (French) for languages with shared Latin roots; MultiBPEmb decomposes words into subword units and merges them via BiLSTM to handle unseen languages. Both feed into the same downstream Seq2Seq tagging model.

### Mechanism 2
Attention mechanisms improve generalization on incomplete addresses without sacrificing overall accuracy. Attention layers in the decoder learn soft alignments between encoder hidden states and decoder outputs, allowing the model to focus on relevant parts of the address even if some components are missing.

### Mechanism 3
Fine-tuning with custom data allows adaptation to new address patterns without retraining from scratch. The library exposes a retrain() method that loads pre-trained weights, replaces the tag dictionary, and continues training on user data, leveraging transfer learning to preserve learned embeddings while adapting to new schemas.

## Foundational Learning

- **Subword tokenization (BPE)**: Needed to handle unseen words and country-specific terms without requiring full vocabularies per language. Quick check: What happens if an address contains a rare street suffix not seen during training—will BPE split it into known subwords or treat it as unknown?

- **Sequence-to-sequence (Seq2Seq) modeling**: Needed to map address sequences to labeled components end-to-end without hand-crafted rules. Quick check: How does the decoder know when to stop predicting if it always produces a fixed-length output equal to the input?

- **Transfer learning and fine-tuning**: Needed to adapt pre-trained models to new address formats with minimal data. Quick check: If you fine-tune on a dataset with only 100 examples, what regularization or early stopping strategies would prevent overfitting?

## Architecture Onboarding

- **Component map**: Raw address → preprocessor → embedding → BiLSTM encoder → (attention) → decoder → classifier → tags

- **Critical path**: Raw address → preprocessor → embedding → BiLSTM encoder → (attention) → decoder → classifier → tags

- **Design tradeoffs**: 
  - fastText: Faster inference, good for Latin-script languages, less flexible for unseen scripts
  - MultiBPEmb: Slower but more generalizable, handles unknown words via subword units
  - Attention: Better for incomplete addresses, higher inference cost
  - No pre/post processing: Simpler pipeline but may fail on noisy inputs

- **Failure signatures**: 
  - All tags predicted as EOS → embedding or encoder failure
  - Random or repeated tags → decoder misalignment or softmax collapse
  - Consistently wrong country-specific components → embedding mismatch for that language

- **First 3 experiments**:
  1. Parse a known address from the training set; verify output matches expected tags
  2. Parse an address from a new country (e.g., Japanese) and observe tag accuracy; test both embedding models
  3. Fine-tune on a small custom dataset with new tags; measure performance gain vs. baseline

## Open Questions the Paper Calls Out

### Open Question 1
How does Deepparse's performance compare to Libpostal when parsing addresses from countries not included in the training data? The paper mentions that Deepparse can parse addresses from over 60 countries and achieve 99% accuracy on trained countries, but does not provide specific results for unseen countries.

### Open Question 2
What is the impact of different embedding techniques (fastText and MultiBPEmb) on Deepparse's performance for addresses in various languages? The paper mentions that Deepparse uses two embedding techniques but does not provide a detailed analysis of their impact on different languages.

### Open Question 3
How does Deepparse's fine-tuning capability affect its performance on specific address formats or domains? The paper mentions that Deepparse supports fine-tuning with new data but does not provide specific examples or evaluations of this capability.

## Limitations

- Performance on countries not in training set remains unverified
- No independent validation or public test sets available
- Fine-tuning claims lack empirical validation on small datasets
- Inference speed comparison with Libpostal insufficiently detailed

## Confidence

**High confidence**: Architectural choices are well-established in NLP literature and implementation follows standard practices.

**Medium confidence**: 99% accuracy claim based on internal evaluation; exact dataset composition and hyperparameters unknown.

**Low confidence**: Fine-tuning claims lack empirical support; no ablation studies or concrete examples provided.

## Next Checks

1. **Cross-country generalization test**: Parse addresses from 5 countries not in the training set (e.g., Japan, UAE, Russia, Brazil, South Africa) and measure accuracy drop compared to trained countries. Compare results using both fastText and MultiBPEmb models.

2. **Fine-tuning validation**: Fine-tune a pre-trained model on a small custom dataset (100-500 examples) with new tags or address formats. Measure convergence behavior, overfitting indicators, and accuracy improvement over the base model.

3. **Robustness to noisy inputs**: Test parsing accuracy on addresses with common real-world issues: missing components, abbreviations, typos, and mixed languages. Compare Deepparse performance to Libpostal under identical conditions.