---
ver: rpa2
title: 'What''s meant by explainable model: A Scoping Review'
arxiv_id: '2307.09673'
source_url: https://arxiv.org/abs/2307.09673
tags:
- methods
- explainable
- explanations
- evaluation
- papers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This scoping review investigated the practice of labeling AI models
  as "explainable" simply because post-hoc XAI methods like SHAP or LIME are used,
  without evaluating the explanations produced. The study analyzed 187 application
  papers that claimed to use explainable models and found that 81% did not conduct
  any form of evaluation on the explanations generated by their XAI methods.
---

# What's meant by explainable model: A Scoping Review

## Quick Facts
- arXiv ID: 2307.09673
- Source URL: https://arxiv.org/abs/2307.09673
- Reference count: 16
- 81% of application papers claiming explainable models do not evaluate their XAI explanations

## Executive Summary
This scoping review investigates a critical gap in explainable AI (XAI) research: the widespread practice of labeling models as "explainable" simply by applying post-hoc methods like SHAP or LIME without evaluating the quality of generated explanations. Analyzing 187 application papers, the study finds that 81% do not conduct any evaluation of their XAI methods, with most focusing only on quantitative metrics when evaluation does occur. This reveals a significant disconnect between the assumption that XAI methods inherently provide meaningful explanations and the reality that explanation quality remains largely unverified.

## Method Summary
The study conducted a scoping review using PRISMA extension guidelines, searching the Scopus database for application papers containing keywords related to explainable AI. Papers were screened and manually reviewed to identify those that use popular XAI libraries (SHAP, LIME) and refer to their models as "explainable" or "interpretable." The review then categorized papers based on whether they conducted any form of evaluation (qualitative or quantitative) on the explanations generated by their XAI methods.

## Key Results
- 81% of application papers that refer to their approaches as explainable models do not conduct any form of evaluation on the XAI method used
- Among papers that did evaluate explanations, most focused on quantitative metrics like stability and fidelity
- Only 4.5% of papers incorporated human-centered or qualitative assessments of explanation quality
- 64% of papers used SHAP and LIME feature attribution methods without any evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Researchers often label models as "explainable" simply by applying post-hoc XAI methods without verifying explanation quality.
- Mechanism: Papers adopt popular XAI libraries (SHAP, LIME) and claim explainability without evaluation, assuming feature attribution suffices for explanation.
- Core assumption: Using a post-hoc XAI method automatically makes a model explainable.
- Evidence anchors:
  - [abstract] "81% of the application papers that refer to their approaches as an explainable model do not conduct any form of evaluation on the XAI method they used."
  - [section] "Many of those papers implemented one of the popular XAI libraries without any evaluation."
- Break condition: When researchers explicitly evaluate explanations using qualitative or quantitative methods before labeling models as explainable.

### Mechanism 2
- Claim: The lack of standardized evaluation metrics for XAI methods enables the practice of unverified explainability claims.
- Mechanism: Without accepted evaluation frameworks, researchers default to applying XAI methods without verification, assuming generated attributions are sufficient explanations.
- Core assumption: The absence of standard evaluation metrics means no verification is necessary.
- Evidence anchors:
  - [section] "Despite the importance of an evaluation metric for XAI methods, there is currently no commonly recognized scale that is widely accepted by XAI researchers."
  - [section] "The absence of human-centered and standard evaluation approaches suggest lack of rigour in the field."
- Break condition: When standardized evaluation metrics become widely adopted and required for XAI method claims.

### Mechanism 3
- Claim: The popularity of certain XAI libraries creates a false perception that explainability problems are solved.
- Mechanism: High citation counts and ease of use of SHAP and LIME lead researchers to assume these methods provide reliable explanations without verification.
- Core assumption: Popular libraries with many citations are inherently reliable for explanation.
- Evidence anchors:
  - [abstract] "SHAP [Lundberg and Lee, 2017] and LIME [Ribeiro et al., 2016] have easy-to-use libraries that help them be quite popular."
  - [section] "64% of the papers used SHAP and LIME feature attribution methods without any evaluation."
- Break condition: When researchers critically assess library limitations and require evaluation regardless of popularity.

## Foundational Learning

- Concept: Post-hoc XAI methods
  - Why needed here: Understanding that post-hoc methods generate explanations after model training is essential for grasping why evaluation is critical.
  - Quick check question: What distinguishes post-hoc explanation methods from intrinsic explainable models?

- Concept: Evaluation metrics for XAI
  - Why needed here: Knowledge of existing evaluation approaches (fidelity, stability, human-centered assessments) is crucial for understanding the gap identified in the review.
  - Quick check question: Name three common metrics used to evaluate the quality of XAI explanations.

- Concept: Explainability vs. interpretability
  - Why needed here: Differentiating between generating explanations and ensuring they are meaningful to users is fundamental to understanding the research gap.
  - Quick check question: How does the concept of application-specific explanations challenge the notion that one-size-fits-all XAI methods are sufficient?

## Architecture Onboarding

- Component map:
  - Literature search and screening pipeline
  - Manual review process for inclusion criteria
  - Classification system for evaluated vs. non-evaluated papers
  - Analysis framework for evaluation types and methods

- Critical path:
  1. Initial keyword search in Scopus
  2. Screening for application papers using XAI methods
  3. Manual verification of explainable model claims
  4. Classification based on evaluation presence
  5. Analysis of evaluation approaches in included papers

- Design tradeoffs:
  - Inclusion criteria focused on popular methods (SHAP, LIME) for tractability vs. comprehensive coverage of all XAI methods
  - Manual review for accuracy vs. automated screening for efficiency
  - Focus on application papers vs. including methodological papers

- Failure signatures:
  - High false positive rate in initial screening (papers claiming explainability without actual XAI use)
  - Incomplete coverage of evaluation types (missing qualitative assessments)
  - Over-reliance on quantitative metrics at expense of human-centered evaluation

- First 3 experiments:
  1. Test screening criteria on a small sample to validate inclusion/exclusion accuracy
  2. Pilot manual review process with multiple reviewers to establish inter-rater reliability
  3. Analyze a subset of papers using both citation metrics and content analysis to validate classification system

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What percentage of AI applications in high-stakes domains (healthcare, finance, security) that claim to use explainable models actually evaluate their explanations?
- Basis in paper: [explicit] The paper notes that 81% of application papers do not evaluate XAI methods, and mentions high-stakes applications as particularly concerning
- Why unresolved: The study did not specifically analyze whether papers were in high-stakes domains or track evaluation rates by domain
- What evidence would resolve it: A systematic analysis of XAI applications by domain, tracking evaluation practices in high-stakes versus low-stakes applications

### Open Question 2
- Question: What are the most effective evaluation metrics for post-hoc explanations that balance quantitative rigor with human-centered assessment?
- Basis in paper: [explicit] The paper identifies a lack of qualitative evaluation and standardized metrics, noting that most evaluation focuses on quantitative measures like stability and fidelity
- Why unresolved: Current literature shows diverse evaluation approaches but no consensus on best practices for comprehensive assessment
- What evidence would resolve it: Comparative studies of different evaluation frameworks across multiple applications, measuring their effectiveness in capturing both technical quality and human comprehension

### Open Question 3
- Question: How does the lack of explanation evaluation in XAI applications impact user trust and decision-making outcomes?
- Basis in paper: [inferred] The paper discusses concerns about misleading users when XAI methods are used without evaluation, particularly in safety-critical applications
- Why unresolved: The study focuses on the prevalence of non-evaluation rather than its real-world consequences on users and decision quality
- What evidence would resolve it: Empirical studies measuring user trust and decision outcomes when using evaluated versus non-evaluated explanations in actual applications

## Limitations

- The study focuses only on papers using SHAP and LIME libraries, potentially missing evaluation practices in other XAI methods
- Limited ability to determine why researchers skip evaluation despite awareness of its importance
- Cannot assess the real-world impact of non-evaluation on user trust and decision-making outcomes

## Confidence

- Confidence: Medium for the core finding that 81% of application papers fail to evaluate XAI methods
- Confidence: Low for claims about why this gap exists

## Next Checks

1. Conduct a follow-up systematic review with expanded inclusion criteria to include all major XAI libraries, not just SHAP and LIME, to verify if the 81% non-evaluation rate holds across the broader XAI landscape.

2. Survey researchers who publish XAI application papers to understand their decision-making process regarding evaluation, including perceived barriers, time constraints, and assumptions about evaluation necessity.

3. Analyze whether papers that do evaluate XAI methods show different citation patterns or impact metrics compared to those that don't, which could indicate whether the research community implicitly rewards or penalizes evaluation practices.