---
ver: rpa2
title: Understanding and Mitigating the Label Noise in Pre-training on Downstream
  Tasks
arxiv_id: '2309.17002'
source_url: https://arxiv.org/abs/2309.17002
tags:
- singular
- value
- rank
- index
- noisy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of label noise in pre-training
  data on downstream task performance. Through extensive experiments on ImageNet-1K
  and YFCC15M datasets, the authors demonstrate that slight noise (up to 5-10%) in
  pre-training can benefit in-domain tasks but consistently harms out-of-domain generalization.
---

# Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks

## Quick Facts
- arXiv ID: 2309.17002
- Source URL: https://arxiv.org/abs/2309.17002
- Reference count: 40
- This paper investigates how label noise in pre-training data affects downstream task performance and proposes a regularization method (NMTune) to mitigate negative effects.

## Executive Summary
This paper investigates the impact of label noise in pre-training data on downstream task performance. Through extensive experiments on ImageNet-1K and YFCC15M datasets, the authors demonstrate that slight noise (up to 5-10%) in pre-training can benefit in-domain tasks but consistently harms out-of-domain generalization. Analysis reveals that noise shapes the feature space by reducing dominant singular values and increasing feature dimensionality, which explains the observed performance trends. To mitigate these effects, the authors propose NMTune, a lightweight black-box fine-tuning method that regularizes the feature space to maintain pre-trained knowledge while improving generalization. NMTune achieves superior performance compared to standard linear probing and MLP tuning across various vision and language models pre-trained on noisy data, showing average accuracy improvements of 1-2% on both in-domain and out-of-domain tasks.

## Method Summary
The paper investigates label noise effects in pre-training and proposes NMTune to mitigate them. Pre-training experiments use ImageNet-1K and YFCC15M with synthetic noise at 0%, 5%, 10%, 20%, and 30% ratios. The NMTune method applies consistency, covariance, and dominant singular value regularization to transformed features from a frozen pre-trained model, followed by training a downstream classifier. The approach is evaluated against linear probing and MLP tuning baselines across 14 in-domain and 4 out-of-domain vision tasks using various model architectures.

## Key Results
- Slight noise (5-10%) in pre-training benefits in-domain tasks but consistently harms out-of-domain generalization
- Noise reshapes feature space by reducing dominant singular values and increasing feature dimensionality
- NMTune achieves 1-2% average accuracy improvements over baselines on both in-domain and out-of-domain tasks
- NMTune outperforms standard linear probing and MLP tuning across vision and language models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Noise in pre-training data reshapes the feature space, reducing dominant singular values and increasing feature dimensionality.
- Mechanism: Pre-training on noisy labels causes the learned feature extractor to distribute capacity across more dimensions to fit noise structure, flattening the singular value spectrum.
- Core assumption: The feature space can be characterized via singular value decomposition, and its shape affects downstream generalization.
- Evidence anchors:
  - [abstract] "noise in pre-training shapes the feature space differently... reducing dominant singular values and increasing feature dimensionality"
  - [section] "Noise in pre-training results in the decreasing largest singular value and flatter singular value distribution with a higher dimension span in the feature space"
  - [corpus] No direct support; corpus focuses on noisy supervision but not on singular value spectrum effects.
- Break condition: If noise structure is too complex or correlated, the model may overfit and lose generalization capability.

### Mechanism 2
- Claim: Slight noise (up to 5-10%) in pre-training can benefit in-domain tasks by increasing feature space dimensionality.
- Mechanism: A slight increase in feature space spanning dimension improves discriminability for in-domain tasks by providing more diverse feature representations.
- Core assumption: In-domain tasks share the same data distribution as pre-training, so the expanded feature space aligns well with downstream needs.
- Evidence anchors:
  - [abstract] "slight noise in pre-training can benefit in-domain transfer performance"
  - [section] "An initial increase in the spanning dimension of the feature space is beneficial to the discriminability on ID tasks"
  - [corpus] No direct support; corpus does not discuss specific noise thresholds for ID performance.
- Break condition: If noise ratio exceeds 10%, the feature space may memorize noise rather than learn useful structure, harming performance.

### Mechanism 3
- Claim: NMTune improves generalization by regularizing the feature space to maintain pre-trained knowledge and improve singular value properties.
- Mechanism: NMTune uses consistency, covariance, and dominant singular value regularization to reshape the transformed feature space, reducing noise effects and improving transferability.
- Core assumption: The regularization terms can effectively guide the feature space toward desirable properties without losing pre-trained knowledge.
- Evidence anchors:
  - [abstract] "NMTune achieves superior performance compared to standard linear probing and MLP tuning"
  - [section] "We propose three regularization objectives on the singular value spectrum that help affine the feature space"
  - [corpus] No direct support; corpus does not discuss specific regularization techniques for feature space reshaping.
- Break condition: If regularization weights are improperly tuned, the method may fail to balance noise mitigation and knowledge preservation.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD)
  - Why needed here: SVD is used to analyze and characterize the feature space learned by pre-trained models, revealing how noise affects its structure.
  - Quick check question: Can you explain how SVD decomposes a matrix and what the singular values represent in terms of data structure?

- Concept: Linear Probing
  - Why needed here: Linear probing is the evaluation protocol used to assess the quality of pre-trained features without fine-tuning the entire model.
  - Quick check question: Why is linear probing considered a black-box tuning method, and what does it reveal about feature quality?

- Concept: Covariance Regularization
  - Why needed here: Covariance regularization encourages the transformed feature space to have zero off-diagonal elements in its covariance matrix, improving feature independence and discriminability.
  - Quick check question: How does covariance regularization relate to decorrelating features, and why might this be beneficial for downstream tasks?

## Architecture Onboarding

- Component map:
  Pre-trained feature extractor (frozen) -> MLP transformation layer (trainable) -> Regularization terms -> Downstream classifier

- Critical path:
  1. Extract features from frozen pre-trained model
  2. Transform features using MLP
  3. Apply regularization to transformed features
  4. Train downstream classifier on regularized features

- Design tradeoffs:
  - Adding MLP increases parameter count but allows feature space reshaping
  - Regularization terms improve generalization but add computational overhead
  - Freezing pre-trained model limits adaptation but ensures knowledge preservation

- Failure signatures:
  - Over-regularization: Performance drops on both ID and OOD tasks
  - Under-regularization: No improvement over baseline methods
  - Improper MLP architecture: Feature transformation fails to improve generalization

- First 3 experiments:
  1. Verify that noise in pre-training affects singular value spectrum as predicted
  2. Test NMTune with different regularization weight combinations on a single ID task
  3. Compare NMTune against MLP tuning and linear probing on multiple OOD tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal noise ratio for pre-training that maximizes in-domain generalization while minimizing out-of-domain degradation?
- Basis in paper: Explicit - The paper shows that slight noise (up to 5-10%) benefits in-domain tasks but harms out-of-domain performance.
- Why unresolved: The study only tests discrete noise ratios (0%, 5%, 10%, 20%, 30%). There may be a non-linear relationship between noise ratio and performance that requires finer-grained analysis.
- What evidence would resolve it: A comprehensive grid search over noise ratios with smaller increments (e.g., 1%, 2%, 3%, etc.) to identify the exact point where in-domain benefits peak and out-of-domain degradation begins.

### Open Question 2
- Question: How does NMTune perform on datasets with instance-dependent or asymmetric label noise compared to synthetic symmetric noise?
- Basis in paper: Inferred - The paper mentions that future exploration should investigate asymmetric and instance-dependent noise, suggesting current results may not generalize to these more realistic scenarios.
- Why unresolved: The study only evaluates NMTune on synthetic symmetric noise. Real-world datasets often contain more complex noise patterns.
- What evidence would resolve it: Experiments applying NMTune to datasets known to have asymmetric or instance-dependent label noise, such as real-world web-scraped datasets or corrupted benchmark datasets.

### Open Question 3
- Question: Can NMTune be extended to self-supervised pre-training methods, such as contrastive learning, to mitigate the effects of noise in pre-training data?
- Basis in paper: Inferred - The paper focuses on supervised pre-training and does not explore self-supervised methods, which are also widely used in practice.
- Why unresolved: The analysis and regularization techniques are tailored to supervised learning, and their effectiveness on self-supervised methods is unknown.
- What evidence would resolve it: Applying NMTune to models pre-trained with self-supervised methods (e.g., CLIP, SimCLR) and evaluating their performance on downstream tasks to determine if the regularization techniques generalize.

### Open Question 4
- Question: What is the computational trade-off between NMTune and full fine-tuning for very large models, considering both performance and resource constraints?
- Basis in paper: Explicit - The paper acknowledges that full fine-tuning is often impractical due to resource constraints and compares NMTune's runtime to MLP tuning and linear probing.
- Why unresolved: While NMTune is shown to be computationally efficient, the paper does not provide a direct comparison to full fine-tuning for very large models.
- What evidence would resolve it: Benchmarking NMTune against full fine-tuning on large models (e.g., GPT-3, CLIP) to quantify the performance difference and resource savings, considering factors like GPU hours and memory usage.

## Limitations

- The proposed mechanisms for how noise affects feature space geometry rely on empirical observations rather than theoretical guarantees.
- The analysis focuses primarily on synthetic label noise in classification tasks, with limited validation on real-world noisy datasets or non-classification problems.
- The specific regularization terms proposed in NMTune are presented as effective solutions without comprehensive ablation studies examining alternative formulations or hyperparameter sensitivity.

## Confidence

**High Confidence**: The empirical findings that slight noise (5-10%) can improve in-domain performance while harming out-of-domain generalization are well-supported by extensive experiments across multiple vision and language models. The NMTune method's superior performance compared to baselines on both ID and OOD tasks is consistently demonstrated.

**Medium Confidence**: The characterization of noise effects through singular value decomposition provides a plausible explanation for observed performance trends, but the theoretical connection between singular value properties and generalization remains incompletely established. The optimal noise thresholds (5-10% beneficial, >10% harmful) are empirically observed but may vary with model architecture and dataset characteristics.

**Low Confidence**: The specific regularization terms proposed in NMTune are presented as effective solutions without comprehensive ablation studies examining alternative formulations or hyperparameter sensitivity. The method's performance on non-classification tasks and real-world noisy data remains underexplored.

## Next Checks

1. Conduct ablation studies on NMTune's regularization terms to determine which components contribute most to performance improvements and test sensitivity to hyperparameter choices.

2. Evaluate the proposed framework on real-world noisy datasets (rather than synthetic noise) to assess practical applicability and identify potential domain-specific considerations.

3. Test the method's effectiveness on non-classification tasks (e.g., object detection, semantic segmentation) to establish broader applicability beyond the current experimental scope.