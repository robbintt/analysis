---
ver: rpa2
title: 'Beyond Deep Ensembles: A Large-Scale Evaluation of Bayesian Deep Learning
  under Distribution Shift'
arxiv_id: '2306.12306'
source_url: https://arxiv.org/abs/2306.12306
tags:
- deep
- accuracy
- learning
- posterior
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive evaluation of modern Bayesian
  deep learning (BDL) algorithms under realistic distribution shifts using the WILDS
  benchmark suite. The study compares multiple BDL methods including Bayes By Backprop,
  Monte Carlo Dropout, SWAG, and Rank-1 VI on diverse tasks including regression,
  image classification, and text classification using pre-trained transformers.
---

# Beyond Deep Ensembles: A Large-Scale Evaluation of Bayesian Deep Learning under Distribution Shift

## Quick Facts
- arXiv ID: 2306.12306
- Source URL: https://arxiv.org/abs/2306.12306
- Reference count: 40
- Primary result: Ensembling single-mode posterior approximations consistently improves accuracy and calibration across most tasks, but fails when fine-tuning large pre-trained transformers

## Executive Summary
This paper provides a comprehensive evaluation of modern Bayesian deep learning (BDL) algorithms under realistic distribution shifts using the WILDS benchmark suite. The study compares multiple BDL methods including Bayes By Backprop, Monte Carlo Dropout, SWAG, and Rank-1 VI on diverse tasks including regression, image classification, and text classification using pre-trained transformers. A key contribution is the introduction of signed expected calibration error (sECE), which distinguishes between over- and under-confidence in model predictions. The results show that ensembling single-mode posterior approximations consistently improves both accuracy and calibration across most tasks, with multi-mode methods like SWAG ensembles and MCD ensembles performing particularly well. However, an interesting failure mode was identified: when fine-tuning large pre-trained transformers, ensembles did not provide benefits, and last-layer variational inference methods like BBB achieved superior accuracy while modern methods like SWAG maintained better calibration.

## Method Summary
The study evaluates BDL algorithms on WILDS datasets using ResNet and DenseNet architectures for vision tasks, and DistilBERT for text classification. Multiple BDL methods are implemented including Bayes By Backprop (BBB), Monte Carlo Dropout (MCD), SWAG, and Rank-1 VI. The authors introduce signed expected calibration error (sECE) to distinguish over- and under-confidence. Ensembling is performed using MultiX, combining multiple posterior approximations. Experiments cover CIFAR-10-C for robustness testing, UCI regression datasets, and WILDS tasks including POVERTY MAP, IWILD CAM, FM OW, RXRX1, CIVIL COMMENTS, and AMAZON. The study systematically compares single-mode approximations against their ensemble counterparts and evaluates calibration, accuracy, and posterior approximation quality.

## Key Results
- Ensembling single-mode approximations (BBB, SWAG, MCD) significantly improves both accuracy and calibration across most tasks
- Multi-mode methods like SWAG ensembles and MCD ensembles perform particularly well for calibration
- Last-layer variational inference (BBB) outperforms full-model approaches when fine-tuning large pre-trained transformers
- Signed calibration metrics reveal systematic over-confidence in most models
- Ensemble benefits disappear when fine-tuning large transformers, where last-layer VI methods excel

## Why This Works (Mechanism)

### Mechanism 1
Ensembling improves calibration and accuracy by capturing multiple posterior modes. Individual posterior approximations (e.g., BBB, SWAG) capture shape of single mode, but miss diversity across modes. Ensembling combines multiple modes into mixture distribution, improving posterior coverage and uncertainty estimates. Core assumption: Different training runs converge to different posterior modes, and these modes capture meaningful uncertainty structure. Evidence anchors: [abstract] "While we find that ensembling single-mode approximations generally improves the generalization capability and calibration of the models by a significant margin"; [section 5.4] "MultiX is almost always less overconfident than single-mode approximations"; [corpus] Weak - corpus neighbors don't directly address ensembling mechanics. Break condition: If training runs converge to same mode (e.g., fine-tuning large transformers from same checkpoint), ensembles won't capture diversity.

### Mechanism 2
Last-layer variational inference scales better to large pretrained transformers than full-model approaches. By keeping earlier layers fixed and only making last layer variational, reduces parameter count dramatically while still capturing task-specific uncertainty. Core assumption: Task-specific uncertainty is primarily captured in final layers, earlier layers capture general features. Evidence anchors: [abstract] "we also identify a failure mode of ensembles when finetuning large transformer-based language models. In this setting, variational inference based approaches such as last-layer Bayes By Backprop outperform other methods"; [section 5.3] "BBB and Rank-1 VI are the most accurate models on both tasks, with no benefit from the multiple components of Rank-1 VI"; [corpus] Weak - corpus neighbors don't address last-layer approaches specifically. Break condition: If task requires significant modification of earlier layers, last-layer approaches will underfit.

### Mechanism 3
Signed calibration metrics reveal whether models are systematically over/under-confident. Standard ECE averages absolute errors, masking whether models are consistently one way or the other. Signed version preserves sign, showing systematic bias. Core assumption: Models tend to be consistently over or under confident rather than random in their calibration errors. Evidence anchors: [abstract] "In particular, we investigate a signed version of the expected calibration error that reveals whether the methods are over- or under-confident"; [section 4.2] "A positive signed calibration error indicates that a model makes predominantly underconfident predictions and a negative signed calibration error indicates predominantly overconfident predictions"; [section C] "we show that for most models nearly all predictions are overconfident or nearly all predictions are underconfident". Break condition: If model makes random calibration errors (some over, some under), signed metric may average to zero despite poor calibration.

## Foundational Learning

- Concept: Bayesian model averaging
  - Why needed here: All BDL methods aim to approximate Bayesian model averaging to capture uncertainty
  - Quick check question: What is the difference between MAP estimation and Bayesian model averaging?

- Concept: Variational inference optimization
  - Why needed here: Most BDL methods use VI to approximate posterior, understanding ELBO optimization is crucial
  - Quick check question: How does the reparameterization trick enable gradient-based optimization in VI?

- Concept: Ensemble diversity
  - Why needed here: Understanding why ensembles work requires grasping how different modes capture different explanations
  - Quick check question: What conditions cause ensemble members to converge to different posterior modes?

## Architecture Onboarding

- Component map: Dataset → Model (ResNet/DenseNet/Transformer) → BDL Algorithm (MAP, BBB, SWAG, etc.) → Evaluation (accuracy, ECE, sECE)
- Critical path: Data loading → Model instantiation → BDL algorithm application → Evaluation metrics computation
- Design tradeoffs: Accuracy vs. calibration vs. computational cost; full-model vs. last-layer approaches; ensemble size vs. runtime
- Failure signatures: Poor calibration (high ECE/sECE), divergence in training, memory issues with large ensembles
- First 3 experiments:
  1. Run MAP baseline on CIFAR-10 to establish baseline accuracy
  2. Compare BBB vs. SWAG on same dataset to see single-mode differences
  3. Add ensemble to best single-mode method to test MultiX benefits

## Open Questions the Paper Calls Out

### Open Question 1
Why do ensembles fail to provide benefits when fine-tuning large pre-trained transformers like DistilBERT, while they work well on CNN-based architectures and regression tasks? Basis in paper: [explicit] The paper explicitly states this as a key finding: "we also identify a failure mode of ensembles when finetuning large transformer-based language models. In this setting, variational inference based approaches such as last-layer Bayes By Backprop outperform other methods in terms of accuracy by a large margin". Why unresolved: The authors only hypothesize that this might be due to "the finetuning nature of the tasks, where all ensemble members start close to each other in parameter space and therefore converge to the same posterior mode," but this is speculative and doesn't explain why this specifically affects transformers but not CNNs. What evidence would resolve it: Systematic experiments varying initialization strategies, training schedules, and architecture-specific hyperparameters (dropout rates, layer freezing, etc.) to determine what architectural or training characteristics cause this failure mode.

### Open Question 2
How do the different Bayesian deep learning methods scale with model size and dataset complexity beyond what was tested? Basis in paper: [inferred] The paper notes they used "large, convolutional and transformer-based neural network architectures" but the scaling behavior is not explicitly characterized, and the authors mention limitations regarding task diversity. Why unresolved: The paper evaluates on specific datasets and architectures but doesn't provide systematic analysis of how computational costs, accuracy, and calibration scale with model size or dataset complexity. What evidence would resolve it: Empirical studies measuring runtime, memory usage, and performance metrics across multiple orders of magnitude in model parameters and dataset sizes for each BDL method.

### Open Question 3
What is the fundamental reason for the discrepancy between the relative performance ordering of single-mode approximations versus their ensemble counterparts across different tasks? Basis in paper: [explicit] The paper states "Overall, the relative ordering of the MultiX models depends on the dataset and in many cases does not correlate with the relative ordering of the corresponding single-mode approximations." Why unresolved: While the authors observe this phenomenon, they don't provide a theoretical explanation for why the ensemble performance doesn't simply inherit the ranking from the single-mode versions. What evidence would resolve it: Analysis of the interaction between ensemble diversity mechanisms and the inherent characteristics of different posterior approximation methods, potentially through visualization of the parameter space trajectories.

### Open Question 4
How do the proposed signed calibration metrics (sECE and sQCE) relate to practical decision-making in safety-critical applications? Basis in paper: [explicit] The authors introduce sECE/sQCE to "differentiate between overconfidence and underconfidence" and note that "overconfidence is worse in practice when applications want to rely on the model's confidence to assess whether they can trust a prediction". Why unresolved: The paper doesn't empirically demonstrate how the distinction between over- and under-confidence translates to actual risk or utility in downstream applications. What evidence would resolve it: Case studies or simulations showing how the sECE/sQCE values correlate with decision quality or risk metrics in specific application domains like medical diagnosis or autonomous driving.

### Open Question 5
Why does last-layer variational inference (like BBB) perform better than full-network approaches when fine-tuning transformers, and can this advantage be extended to other architectures? Basis in paper: [explicit] The paper finds that "last-layer VI scales well to these models and generalizes better than SOTA BDL algorithms such as SWAG and MCD" on transformer-based text classification tasks. Why unresolved: The authors don't explain the mechanism behind this advantage or test whether it generalizes beyond transformers to other architectures. What evidence would resolve it: Ablation studies testing different layer configurations for VI (last 2 layers, last 3 layers, etc.) across multiple architecture types to identify the optimal trade-off between computational efficiency and performance.

## Limitations
- Focus on WILDS benchmark datasets may not capture all real-world scenarios
- Transformer-based experiments limited to two text classification tasks, limiting generalizability
- Computational cost of ensembling multiple posterior approximations may limit practical deployment

## Confidence

- **High Confidence**: The finding that ensembling improves calibration and accuracy across most tasks is well-supported by extensive experiments across multiple datasets and algorithms. The signed calibration error metric (sECE) effectively reveals systematic over/under-confidence patterns.
- **Medium Confidence**: The failure mode of ensembles on fine-tuned transformers is demonstrated on two tasks but requires further validation on additional transformer architectures and domains to establish it as a general phenomenon.
- **Medium Confidence**: The superiority of last-layer variational inference for transformer fine-tuning is shown on specific tasks but may depend on the particular pre-training and fine-tuning protocols used.

## Next Checks

1. Test the ensemble failure mode on additional transformer-based tasks (e.g., vision transformers, BERT-based classification) to determine if this is architecture-specific or task-specific.

2. Evaluate the computational efficiency trade-offs of different BDL methods under realistic deployment constraints, including memory usage and inference latency for large ensembles.

3. Validate the sECE metric on additional datasets and tasks to confirm that it consistently reveals systematic calibration biases rather than random variation.