---
ver: rpa2
title: Finding Stakeholder-Material Information from 10-K Reports using Fine-Tuned
  BERT and LSTM Models
arxiv_id: '2308.07522'
source_url: https://arxiv.org/abs/2308.07522
tags:
- page
- information
- baseline
- environment
- recall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of efficiently extracting stakeholder-material
  information from lengthy 10-K reports using fine-tuned BERT and LSTM models. A baseline
  keyword search approach was outperformed by the fine-tuned models, achieving an
  F1 score of 0.899 and accuracy of 0.904 on test data, compared to 0.749 and 0.781
  for the baseline.
---

# Finding Stakeholder-Material Information from 10-K Reports using Fine-Tuned BERT and LSTM Models

## Quick Facts
- arXiv ID: 2308.07522
- Source URL: https://arxiv.org/abs/2308.07522
- Reference count: 11
- Best model achieved F1 score of 0.899 and accuracy of 0.904 on test data

## Executive Summary
This study addresses the challenge of efficiently extracting stakeholder-material information from lengthy 10-K reports using fine-tuned BERT and LSTM models. A baseline keyword search approach was outperformed by the fine-tuned models, achieving an F1 score of 0.899 and accuracy of 0.904 on test data, compared to 0.749 and 0.781 for the baseline. The same approach was applied to four stakeholder groups (customers, investors, employees, community/environment), with BERT models consistently outperforming both the baseline and LSTM models across all groups. The best results included F1 scores of 0.970 for customer-related content and 0.915 for investor-related content. The study demonstrates that pre-trained language models can significantly improve stakeholder-material information extraction from financial reports compared to traditional keyword search methods.

## Method Summary
The research fine-tuned five BERT models (BERT-en-uncased, ALBERT-en-base, BERT-experts-wiki-books, BERT-talking-heads-base, DistilBERT-en-uncased) and two LSTM models on a dataset of 6,000+ labeled sentences from 62 10-K reports (2022). The models were trained to classify sentences as containing stakeholder-material information or not, with 80% of data used for training and 20% for testing. The fine-tuned models were compared against a keyword search baseline using pre-defined stakeholder-related terms. Performance was evaluated using accuracy, precision, recall, and F1-score, with F1-score being the primary metric due to class imbalance in the dataset.

## Key Results
- Fine-tuned BERT models achieved F1 score of 0.899 and accuracy of 0.904 on test data
- BERT models outperformed keyword search baseline (F1: 0.749, accuracy: 0.781) and LSTM models across all stakeholder groups
- Best individual stakeholder performance: 0.970 F1 for customer-related content and 0.915 F1 for investor-related content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuned BERT models outperform keyword search for stakeholder-material information extraction.
- Mechanism: BERT models learn contextual embeddings from large pre-training corpora, allowing them to understand semantic relationships beyond exact keyword matching.
- Core assumption: The training data adequately represents the variety of stakeholder-related expressions in 10-K reports.
- Evidence anchors:
  - [abstract] "The existing practice uses keyword search to identify such information, which is my baseline model. Using business expert-labeled training data of nearly 6,000 sentences... the best model has achieved an accuracy of 0.904 and an F1 score of 0.899 in test data, significantly above the baseline model's 0.781 and 0.749 respectively."
  - [section] "A comprehensive list of keywords that may indicate stakeholder mentions include... * represents any trailing character(s) in order to take into account any pluralistic or adjective terms. As keywords need to be pre-defined, this approach is rule-based and inflexible."

### Mechanism 2
- Claim: Fine-tuning pre-trained language models on domain-specific data improves classification accuracy for stakeholder material detection.
- Mechanism: The fine-tuning process adapts general language understanding capabilities to the specific task and domain vocabulary of financial reports, creating task-specific decision boundaries.
- Core assumption: The 6,000 labeled sentences provide sufficient coverage of stakeholder-related content variations.
- Evidence anchors:
  - [abstract] "Using business expert-labeled training data of nearly 6,000 sentences from 62 10-K reports published in 2022, the best model has achieved an accuracy of 0.904 and an F1 score of 0.899 in test data"
  - [section] "Specifically, I fine-tune each pre-trained model to do text classification and compare the performance metrics against the baseline model which is based on keyword search."

### Mechanism 3
- Claim: Different BERT variants have varying performance based on their pre-training objectives and architectures.
- Mechanism: Models pre-trained on different corpora or with different architectural modifications (like ALBERT's parameter reduction) show performance differences when fine-tuned for the same task.
- Core assumption: Pre-training data and architectural choices impact downstream task performance.
- Evidence anchors:
  - [section] "BERT is a pre-trained deep learning model that uses transformer architecture to generate contextually rich word embeddings... Specifically, I have experimented with five of the most used BERT models, including BERT-en-uncased, ALBERT-en-base, BERT-experts-wiki-books, BERT-talking-heads-base, and DistilBERT-en-uncased."
  - [section] "Overall, pre-trained BERT models do perform better than the LSTM models, suggesting that our task may have benefited from pre-training from a massive corpus of text data and context awareness."

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: BERT models use transformer architecture to understand context and relationships between words, which is critical for distinguishing stakeholder-material information from non-material content
  - Quick check question: How does the self-attention mechanism in transformers differ from traditional RNN approaches in handling long-range dependencies?

- Concept: Fine-tuning vs. training from scratch
  - Why needed here: The paper leverages pre-trained models and fine-tunes them on specific 10-K report data, which is more efficient than training a model from scratch
  - Quick check question: What are the key differences in computational requirements and performance between fine-tuning a pre-trained model versus training a new model on the same task?

- Concept: Evaluation metrics for classification tasks
  - Why needed here: The paper uses accuracy, precision, recall, and F1-score to evaluate model performance, with F1-score being the primary metric due to class imbalance
  - Quick check question: In a highly imbalanced classification problem, why might F1-score be preferred over accuracy as the primary evaluation metric?

## Architecture Onboarding

- Component map:
  - Data preprocessing pipeline → Model fine-tuning module → Evaluation metrics calculation → Model comparison framework
  - Key components: Text tokenizer, BERT/LSTM model architecture, loss function, optimizer, evaluation metrics

- Critical path:
  1. Load and preprocess 6,000 labeled sentences from 62 10-K reports
  2. Split data into training (80%) and test/validation (20%) sets
  3. Fine-tune multiple BERT and LSTM models on training data
  4. Evaluate models on test data using accuracy, precision, recall, and F1-score
  5. Compare performance against keyword search baseline

- Design tradeoffs:
  - BERT vs. LSTM: BERT provides better contextual understanding but requires more computational resources; LSTMs are lighter but less effective at capturing complex relationships
  - Model selection: Five different BERT variants were tested, each with different pre-training objectives and architectural choices
  - Training epochs: BERT models limited to 5 epochs due to GPU constraints, while LSTM models trained for 10 epochs on CPUs

- Failure signatures:
  - Low F1-score with high accuracy: Model is biased toward majority class (non-material information)
  - Poor performance on specific stakeholder groups: Insufficient training examples for that stakeholder category
  - High variance between training runs: Model overfitting or unstable training process

- First 3 experiments:
  1. Reproduce baseline keyword search results to establish performance floor
  2. Fine-tune a standard BERT model (e.g., BERT-en-uncased) on the full dataset and evaluate
  3. Fine-tune the same BERT model separately for each stakeholder group to identify performance variations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of fine-tuned BERT models compare when applied to 10-K reports from industries outside of financial services?
- Basis in paper: [explicit] The authors note that the current project is built on a very specific industry (financial companies) and suggest that generalization would require collecting more data across other industries.
- Why unresolved: The study only tested models on financial company 10-K reports, limiting generalizability to other sectors.
- What evidence would resolve it: Testing the same fine-tuned models on 10-K reports from non-financial industries and comparing F1 scores and accuracy to the baseline.

### Open Question 2
- Question: Would using human-GPT collaboration to create a larger training dataset significantly improve model performance compared to the current expert-labeled dataset?
- Basis in paper: [explicit] The authors suggest that experimenting with human-GPT collaboration to create a larger training dataset would be beneficial.
- Why unresolved: The study used a relatively small, manually labeled dataset and did not explore automated or semi-automated labeling methods.
- What evidence would resolve it: Training models on a larger dataset created through human-GPT collaboration and measuring improvements in F1 score and accuracy.

### Open Question 3
- Question: How close can model performance get to human readers in identifying stakeholder-material information from 10-K reports?
- Basis in paper: [explicit] The authors conclude that while fine-tuned models significantly outperform keyword searches, further improvement is needed to catch up with human readers.
- Why unresolved: The study did not include a comparison between model performance and human reading accuracy on the same dataset.
- What evidence would resolve it: Conducting a controlled study comparing the best model's performance against human readers on the same set of 10-K reports.

## Limitations
- Small training dataset (6,000 sentences from 62 reports) limits generalizability across industries and time periods
- BERT models trained for only 5 epochs due to GPU constraints, potentially affecting fair comparison with LSTM models trained for 10 epochs
- Focus on four specific stakeholder categories without exploring overlaps or additional stakeholder types

## Confidence
- High Confidence: Overall performance superiority of fine-tuned BERT models over keyword search baseline
- Medium Confidence: Specific performance metrics reported due to small dataset size and single-year scope
- Medium Confidence: Comparative advantage of BERT over LSTM models due to unequal training durations

## Next Checks
1. Dataset Expansion Validation: Replicate the study using a significantly larger and more diverse dataset spanning multiple years and industries to assess whether the reported performance gains hold across different temporal and sectoral contexts.

2. Training Duration Parity Test: Conduct a controlled experiment where both BERT and LSTM models are trained for the same number of epochs under identical computational conditions to determine if the current performance gap persists when training duration is equalized.

3. Stakeholder Category Expansion: Extend the classification framework to include additional stakeholder categories and test for potential overlaps between existing categories to evaluate the model's ability to handle more complex stakeholder relationship scenarios beyond the current four-group structure.