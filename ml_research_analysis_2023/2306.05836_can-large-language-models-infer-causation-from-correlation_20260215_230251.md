---
ver: rpa2
title: Can Large Language Models Infer Causation from Correlation?
arxiv_id: '2306.05836'
source_url: https://arxiv.org/abs/2306.05836
tags:
- causal
- inference
- llms
- variables
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel task and dataset, CORR 2CAUSE, to
  test large language models' (LLMs) pure causal inference skills. The task involves
  inferring causal relationships from correlational statements using insights from
  causal discovery.
---

# Can Large Language Models Infer Causation from Correlation?

## Quick Facts
- arXiv ID: 2306.05836
- Source URL: https://arxiv.org/abs/2306.05836
- Reference count: 15
- Primary result: All evaluated LLMs perform near-random baseline on pure causal inference task

## Executive Summary
This paper introduces CORR 2CAUSE, a novel benchmark dataset designed to test large language models' pure causal inference abilities. Unlike existing causal inference datasets that rely on empirical knowledge, CORR 2CAUSE focuses on formal causal discovery principles to assess whether models can distinguish causation from correlation. The dataset contains over 400K samples generated from causal graphs with 2-6 nodes, presenting models with correlational statements and asking them to infer causal relationships. The primary finding reveals that all 17 evaluated LLMs, including state-of-the-art models, perform close to random baseline, indicating a fundamental limitation in their causal reasoning capabilities.

The study demonstrates that while finetuning on the CORR 2CAUSE dataset improves performance, the gains are superficial - models fail to generalize to out-of-distribution settings created through paraphrasing and variable refactorization. This suggests that LLMs memorize spurious correlations rather than learning robust causal inference rules. The CORR 2CAUSE benchmark provides a valuable tool for evaluating and improving LLMs' pure reasoning skills, highlighting the gap between statistical pattern recognition and formal causal reasoning that future research must address.

## Method Summary
The authors generate the CORR 2CAUSE dataset using formal causal discovery principles, creating over 400K samples from causal graphs with 2-6 nodes. They evaluate 17 existing LLMs (BERT-based, GPT-based, LLaMa-based) on this task using zero-shot classification, then finetune selected models on the training set. To test generalization, they create perturbed test sets through paraphrasing and variable refactorization. The primary metric is F1 score, with precision, recall, and accuracy also reported.

## Key Results
- All 17 evaluated LLMs perform near-random baseline on the CORR 2CAUSE task
- Finetuning improves performance but models fail to generalize to paraphrased and variable-refactored test sets
- Performance drops significantly when variable names are changed, indicating memorization of spurious correlations
- Models struggle with larger causal graphs and more complex causal relationships

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models fail at pure causal inference because they rely on statistical correlations rather than formal causal reasoning rules
- Mechanism: LLMs learn patterns from training data, which often contains statistical correlations, but they do not inherently apply formal causal discovery rules (e.g., d-separation, Markov property) to distinguish correlation from causation
- Core assumption: The training corpus contains abundant correlations but lacks explicit causal inference rules
- Evidence anchors:
  - [abstract]: "existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge... In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models"
  - [section 1]: "the success of LLMs... lies in capturing a vast set of statistical correlations among terms... the crucial yet missing step is how to process such correlations and infer causal relationships"

### Mechanism 2
- Claim: Finetuning improves performance but leads to overfitting, resulting in poor out-of-distribution generalization
- Mechanism: Finetuning on the CORR 2CAUSE dataset allows models to learn task-specific patterns, but they exploit spurious correlations in the training data rather than learning robust causal inference rules
- Core assumption: The models memorize spurious correlations during finetuning instead of learning generalizable causal reasoning
- Evidence anchors:
  - [abstract]: "LLMs can demonstrate better performance after being finetuned on the data, the causal inference skills attained by them are not robust... fail in out-of-distribution settings"
  - [section 4.5]: "All the models drop drastically... when we paraphrase the test set... and they decrease substantially... when we refactor the variable names"

### Mechanism 3
- Claim: The CORR 2CAUSE task formulation using causal discovery principles provides a valid benchmark for testing pure causal inference
- Mechanism: By grounding the dataset in formal causal discovery (e.g., d-separation, Markov equivalence), the task requires models to apply rigorous causal reasoning rather than rely on empirical knowledge or spurious correlations
- Core assumption: The task formulation accurately captures the essence of pure causal inference as defined by causal discovery literature
- Evidence anchors:
  - [abstract]: "we base our dataset design on the formal framework of causal discovery... which provides rules about how to deduce causal relations among variables given their statistical correlation"
  - [section 3.2]: "we generate the ground-truth validity label... by looking up all the causal graphs in the same MEC corresponding to the given set of correlations, and check the necessity of the hypothesized causal relation"

## Foundational Learning

- Concept: Directed Graphical Causal Models (DGCMs)
  - Why needed here: Understanding DGCMs is crucial for grasping how causal relationships are represented and inferred in the CORR 2CAUSE task
  - Quick check question: Can you explain the difference between a confounder and a collider in a DAG?

- Concept: D-separation and Markov Property
  - Why needed here: D-separation is used to determine conditional independence, which is fundamental to inferring causal relationships from correlations in the dataset
  - Quick check question: Given a DAG, can you identify if two variables are d-separated by a third variable?

- Concept: Markov Equivalence of Graphs
  - Why needed here: Understanding Markov equivalence is important because the task involves identifying causal graphs that induce the same joint distribution
  - Quick check question: How can you tell if two DAGs are in the same Markov equivalence class?

## Architecture Onboarding

- Component map: Dataset generation module (causal discovery principles) -> Model evaluation module (LLM testing) -> Finetuning module (training assessment)
- Critical path: Generate dataset → Evaluate baseline LLMs → Finetune models → Test generalization → Analyze results
- Design tradeoffs: The choice between generating a larger dataset (more coverage) versus a smaller, more carefully curated dataset (higher quality) impacts model performance and generalization
- Failure signatures: Poor performance on the CORR 2CAUSE task, especially after finetuning, indicates a lack of robust causal inference skills. Large drops in performance after paraphrasing or variable refactorization suggest overfitting to spurious correlations
- First 3 experiments:
  1. Evaluate a diverse set of LLMs on the CORR 2CAUSE test set to establish baseline performance
  2. Finetune a subset of high-performing LLMs on the CORR 2CAUSE training set and evaluate on the original test set
  3. Test the finetuned models on paraphrased and variable-refactored versions of the test set to assess generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on the CORR 2CAUSE task when given larger causal graphs (more than 6 nodes)?
- Basis in paper: [inferred] The paper focuses on graphs with 2-6 nodes, but mentions that future work can explore larger graphs
- Why unresolved: The current dataset and experiments only cover graphs with up to 6 nodes
- What evidence would resolve it: Creating a larger dataset with graphs of 7 or more nodes and evaluating LLM performance on this extended dataset

### Open Question 2
- Question: Can LLMs learn to handle hidden confounders in causal inference tasks?
- Basis in paper: [explicit] The paper mentions that it doesn't assume hidden confounders and suggests future work to generate a dataset to infer the existence of hidden confounders
- Why unresolved: The current CORR 2CAUSE dataset doesn't include scenarios with hidden confounders
- What evidence would resolve it: Developing a new dataset that includes hidden confounders and testing LLM performance on this dataset

### Open Question 3
- Question: What is the impact of different data augmentation techniques on improving LLM performance on the CORR 2CAUSE task?
- Basis in paper: [inferred] The paper discusses the poor performance of LLMs on the task and suggests that future work should explore more ways to enhance pure causal inference skills
- Why unresolved: The paper only explores finetuning as a method to improve LLM performance and doesn't investigate other data augmentation techniques
- What evidence would resolve it: Experimenting with various data augmentation techniques (e.g., data synthesis, adversarial training) and evaluating their impact on LLM performance in the CORR 2CAUSE task

## Limitations

- The study lacks transparency in finetuning process with unspecified hyperparameters and training duration
- The dataset generation relies heavily on synthetic examples from a relatively small set of seed causal graphs
- Evaluation is limited to English language models with no assessment of cross-lingual generalization

## Confidence

- **High confidence**: The observation that LLMs perform near-random baseline on the CORR 2CAUSE task is well-supported by the evaluation methodology and results
- **Medium confidence**: The claim that finetuning leads to overfitting rather than robust causal inference is supported by the generalization tests, though the underlying mechanism could benefit from more detailed analysis
- **Medium confidence**: The assertion that LLMs fundamentally lack pure causal inference skills based on this task is reasonable but may not generalize to all forms of causal reasoning or real-world applications

## Next Checks

1. **Prompt Engineering Analysis**: Systematically test different prompt templates and formulations across all model types to determine if performance improvements are possible through better task framing rather than model capability changes

2. **Training Data Audit**: Conduct a detailed analysis of the pre-training corpora of the evaluated models to quantify the actual presence (or absence) of explicit causal discovery rules and formal causal inference content

3. **Out-of-Distribution Robustness**: Create additional test sets that vary not just in surface form (paraphrasing, variable refactoring) but in underlying causal structure complexity, including examples with confounding, mediation, and collider bias to better understand the limits of current LLMs' causal reasoning capabilities