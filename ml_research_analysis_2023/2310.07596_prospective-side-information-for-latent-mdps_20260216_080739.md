---
ver: rpa2
title: Prospective Side Information for Latent MDPs
arxiv_id: '2310.07596'
source_url: https://arxiv.org/abs/2310.07596
tags:
- hard
- information
- explore
- shard
- prospective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of learning in Latent Markov Decision
  Processes (LMDPs) with prospective side information. The authors consider a setting
  where, in addition to the standard partially observable setting, the agent receives
  weakly revealing information about the latent context at the beginning of each episode.
---

# Prospective Side Information for Latent MDPs

## Quick Facts
- arXiv ID: 2310.07596
- Source URL: https://arxiv.org/abs/2310.07596
- Authors: 
- Reference count: 40
- Key outcome: Proves K^{2/3} regret lower bound for LMDP-Ψ setting, fundamentally different from standard POMDP √K bounds

## Executive Summary
This paper studies learning in Latent Markov Decision Processes (LMDPs) with prospective side information, where agents receive weakly revealing information about the latent context at the beginning of each episode. The authors show that this setting (LMDP-Ψ) cannot be handled by standard POMDP algorithms due to the non-trivial correlation between observations introduced by fixed side information within episodes. They establish both upper bounds of poly(A, α⁻¹)K²/³ and lower bounds of Ω(A/α²ϵ² K²/³), demonstrating that the K^{2/3} scaling is unavoidable when exploiting side information. The work highlights a key deficiency in standard POMDP modeling assumptions and opens new directions for research in more general POMDP settings.

## Method Summary
The authors build upon a pure exploration scheme for LMDP-Ψ, constructing confidence sets using likelihood ratio concentration bounds and terminating when sufficient exploration bonuses are collected. The algorithm maintains a set of plausible models and selects optimistic policies within this set. For the regret analysis, they use a carefully constructed hard instance that forces algorithms to distinguish between revealing and non-revealing side information patterns, establishing the K^{2/3} lower bound through KL-divergence calculations between trajectory distributions.

## Key Results
- Establishes K^{2/3} regret lower bound for LMDP-Ψ, fundamentally different from standard POMDP √K bounds
- Proves that O(√K) regret is impossible when exploiting prospective side information unless K is exponentially large
- Shows that LMDP-Ψ setting breaks standard POMDP conditional independence assumptions due to fixed side information within episodes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prospective side information weakly reveals the latent context at episode start.
- Mechanism: Side information ι is emitted from MDP m via matrix I, and satisfies TV-distance bound: dTV(P(ι|¯v1), P(ι|¯v2)) ≥ α/2 · ∥¯v1 - ¯v2∥1.
- Core assumption: α > 0 (weak revelation) and I is finite.
- Evidence anchors:
  - [abstract]: "prospective side information, when an agent receives additional, weakly revealing, information on the latent context at the beginning of each episode"
  - [section 2]: Definition of α-revealing via Assumption 2 and condition (1)
- Break condition: α → 0 makes side information uninformative; if I continuous, discretization error breaks guarantees.

### Mechanism 2
- Claim: LMDP-Ψ breaks standard POMDP conditional independence.
- Mechanism: Observations ˜ot = (ot, ι) share fixed ι across episode, so P(˜ot | st, m) ≠ P(˜ot | st, m, ˜ot−1).
- Core assumption: Fixed ι per episode introduces temporal correlation.
- Evidence anchors:
  - [section 3]: "the observation is independent of historical information conditioning on the latent state" contrasted with LMDP-Ψ
  - [section 3]: "there is a non-trivial correlation between observations"
- Break condition: If ι sampled fresh each step, conditional independence restored; if I is identity, revelation too strong.

### Mechanism 3
- Claim: K^{2/3} regret lower bound arises from balancing exploration vs. exploitation under partial observability.
- Mechanism: Hard instance construction forces algorithm to spend episodes distinguishing between "uninformative" ιhard and revealing ι≠ιhard, leading to Ω(A/(α²ϵ²)·K^{2/3}) bound.
- Core assumption: Mixture of MDPs Glearn, Gref, Gobs with asymmetric priors depending on ι.
- Evidence anchors:
  - [section 5.2]: Hard instance construction and Lemma 5.3 on KL-divergence bounds
  - [section 4.3]: Lower bound Corollary 4.3
- Break condition: If α large or ϵ large, lower bound loosens; if number of contexts M small, exponential dependence drops.

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: LMDP-Ψ is a special POMDP subclass; understanding belief states and conditional independence is core.
  - Quick check question: In a standard POMDP, is P(ot | st, m, ot-1) = P(ot | st, m)? Why or why not in LMDP-Ψ?

- Concept: Total Variation Distance (TV) and Hellinger Distance (H)
  - Why needed here: Regret bounds use TV and H bounds between trajectory distributions; concentration lemmas rely on them.
  - Quick check question: Show that dTV(P,Q) ≤ √(2·d²_H(P,Q)) and explain why this helps in Lemma A.2.

- Concept: Confidence Set Construction via Likelihood Ratios
  - Why needed here: Algorithm 1 and 2 maintain sets of plausible models; correctness depends on concentration of log-likelihood ratios.
  - Quick check question: What is the role of β = log(K|Θ|/δ) in the confidence set Ck and why does it scale this way?

## Architecture Onboarding

- Component map: Prospective side info ι → Trajectory (s₁,a₁,o₁,…,s_H,a_H,r_H) → Model class Θ → Policy class Π → Algorithm 1/2 → Estimated model ˆθ

- Critical path:
  1. Episode starts → observe ι
  2. Execute policy π (from Π or Πblind)
  3. Collect trajectory τ
  4. Update confidence set Ck
  5. Select next (θ,π) via optimistic search
  6. Repeat until termination

- Design tradeoffs:
  - Use Πblind vs. Π: Πblind yields √K regret but ignores ι; Π uses ι but suffers K^{2/3} bound
  - Choice of λ₀ in Algorithm 2: Larger λ₀ increases exploration bonus, slows termination
  - Confidence set width β: Larger β more conservative but more computation

- Failure signatures:
  - If α too small → ι provides no signal → algorithm behaves like no-side-info case
  - If I matrix ill-conditioned → left-inverse amplifies noise → concentration fails
  - If model class Θ too large → log|KΘ| term dominates regret

- First 3 experiments:
  1. Test O-MLE on synthetic LMDP with known I, α=1, verify √K regret bound empirically.
  2. Construct LMDP-Ψ with I revealing one-hot, run Algorithm 2, check if K^{2/3} bound holds.
  3. Vary α from 0.1 to 1.0, measure how regret scales, confirm α dependence in bounds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the prospective side information in LMDP-Ψ differ from the rich observation setting in Block MDPs, and what are the implications for learning algorithms?
- Basis in paper: Explicit - The paper discusses the differences between LMDP-Ψ and related settings like Block MDPs, highlighting the non-trivial correlation between observations in LMDP-Ψ.
- Why unresolved: The paper establishes the differences but does not provide a comprehensive comparison of learning algorithms in these settings or a clear guideline for choosing between them.
- What evidence would resolve it: Empirical studies comparing the performance of learning algorithms in LMDP-Ψ and Block MDPs on various tasks, along with theoretical analysis of the trade-offs involved.

### Open Question 2
- Question: Can the regret bounds for LMDP-Ψ be improved by considering different assumptions on the structure of the latent MDPs or the prospective side information?
- Basis in paper: Inferred - The paper presents regret bounds for LMDP-Ψ but does not explore the impact of different structural assumptions on these bounds.
- Why unresolved: The paper focuses on a specific class of LMDP-Ψ and does not investigate the effects of varying assumptions on the regret bounds.
- What evidence would resolve it: Theoretical analysis of regret bounds under different structural assumptions, supported by empirical results demonstrating the performance of algorithms in these settings.

### Open Question 3
- Question: How does the sample complexity of learning in LMDP-Ψ scale with the size of the observation and prospective side information spaces?
- Basis in paper: Inferred - The paper discusses the regret bounds but does not explicitly analyze the sample complexity in terms of the size of the observation and prospective side information spaces.
- Why unresolved: The paper does not provide a detailed analysis of the sample complexity scaling with the problem dimensions.
- What evidence would resolve it: Theoretical analysis of the sample complexity bounds in terms of the size of the observation and prospective side information spaces, along with empirical studies demonstrating the scaling behavior.

### Open Question 4
- Question: Are there practical applications where the LMDP-Ψ setting is more suitable than other partially observable settings, and what are the key factors that make it advantageous?
- Basis in paper: Explicit - The paper mentions applications like dialogue and recommender systems as examples of LMDP-Ψ, but does not provide a comprehensive analysis of the advantages of LMDP-Ψ over other settings.
- Why unresolved: The paper does not provide a detailed comparison of the suitability of LMDP-Ψ for different practical applications.
- What evidence would resolve it: Case studies of practical applications where LMDP-Ψ is more suitable than other partially observable settings, along with theoretical analysis of the key factors that make it advantageous.

## Limitations
- The K^{2/3} regret bound assumes specific structure of prospective side information and may not extend to all POMDP variants
- The confidence set construction relies on concentration bounds that may become loose for high-dimensional observation spaces
- The analysis requires the side information to be fixed within episodes, which is a departure from standard POMDP modeling assumptions

## Confidence
- High confidence: The impossibility of O(√K) regret when exploiting prospective side information (Section 4.3)
- Medium confidence: The K^{2/3} upper bound (Section 5)
- Medium confidence: The lower bound construction (Section 5.2)

## Next Checks
1. Implement a synthetic LMDP-Ψ instance with known ground truth and verify whether Algorithm 2 achieves the predicted K^{2/3} regret scaling across different α values
2. Test whether allowing policies to depend on side information in a limited way (e.g., through a finite history of past ι) can bridge the gap between Πblind and Π
3. Construct a case study where the prospective side information has additional structure (e.g., is itself Markovian) to determine if the K^{2/3} barrier can be broken under relaxed assumptions