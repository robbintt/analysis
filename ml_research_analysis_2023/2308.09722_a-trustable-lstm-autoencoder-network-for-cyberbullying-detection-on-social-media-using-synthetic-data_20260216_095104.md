---
ver: rpa2
title: A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on Social
  Media Using Synthetic Data
arxiv_id: '2308.09722'
source_url: https://arxiv.org/abs/2308.09722
tags:
- data
- lstm
- accuracy
- social
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses cyberbullying detection on social media by
  introducing a trustable LSTM-Autoencoder Network that works on both raw and synthetic
  (machine-translated) data. To overcome the lack of labeled datasets in languages
  like Hindi and Bangla, the authors augment the TRAC-2 dataset with machine-translated
  English data, creating raw, semi-noisy, and fully noisy versions.
---

# A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on Social Media Using Synthetic Data

## Quick Facts
- arXiv ID: 2308.09722
- Source URL: https://arxiv.org/abs/2308.09722
- Reference count: 40
- Primary result: Proposed model achieves 95% accuracy on English, 91% on Bangla, and 91% on Hindi raw datasets, outperforming baselines including BERT and GPT-2

## Executive Summary
This paper addresses cyberbullying detection on social media by introducing a trustable LSTM-Autoencoder Network that works on both raw and synthetic (machine-translated) data. To overcome the lack of labeled datasets in languages like Hindi and Bangla, the authors augment the TRAC-2 dataset with machine-translated English data, creating raw, semi-noisy, and fully noisy versions. They compare the proposed model against LSTM, BiLSTM, Word2vec, BERT, GPT-2, and standard LSTM-Autoencoder on three languages. The proposed model outperforms all baselines, achieving state-of-the-art accuracy of 95% on English, 91% on Bangla, and 91% on Hindi raw datasets. It also maintains high performance on semi-noisy (93%, 92%, 90%) and fully translated (92%) data, demonstrating robustness even with noisy inputs.

## Method Summary
The authors propose an LSTM-Autoencoder architecture with three stacked LSTM encoders operating in parallel, whose outputs are combined by a meta learner (RNN with attention) to produce a single encoded representation. The model is trained to minimize reconstruction loss while simultaneously learning classification. They augment the TRAC-2 dataset with machine-translated English versions of Hindi and Bangla comments to create semi-noisy and fully noisy datasets. The model is compared against LSTM, BiLSTM, Word2vec, BERT, GPT-2, and standard LSTM-Autoencoder using accuracy, F1-score, precision, and recall metrics.

## Key Results
- Proposed model achieves 95% accuracy on English, 91% on Bangla, and 91% on Hindi raw datasets
- Maintains strong performance on semi-noisy data: 93% English, 92% Bangla, 90% Hindi
- Achieves 92% accuracy even on fully machine-translated English data with significant noise
- Outperforms all baseline models including BERT and GPT-2 on TRAC-2 dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LSTM-Autoencoder architecture learns both sequential dependencies and reconstructs clean embeddings even from noisy machine-translated text.
- Mechanism: Stacked LSTM encoders process the input in parallel to generate multiple sequence encodings. These encodings are reshaped via Repeat vectors and decoded to minimize reconstruction loss, forcing the model to filter noise and preserve semantic meaning.
- Core assumption: Noise in machine-translated text is distributed enough that the model can learn to ignore it while retaining class-relevant patterns.
- Evidence anchors:
  - [abstract]: "Our proposed model outperformed all the models on all datasets, achieving the highest accuracy of 95%... It also performed well on semi-noisy datasets... Even on fully translated English data, which contained a lot of noise, our proposed model provided 92% accuracy."
  - [section]: "The modelâ€™s performance is based on its ability to correctly reconstruct the sequence... The LSTM autoencoder takes inputs i1,..., it, encodes them into hidden representations h11, h12..., h1t and h21, h22..., h2t, respectively, and decodes them into outputs o1, o2,..., ot, where oi = ii."

### Mechanism 2
- Claim: Parallel stacked encoders with a meta learner allow the model to aggregate complementary feature views and improve robustness across languages.
- Mechanism: Three stacked LSTM encoders process the input sequence in parallel, each producing a hidden representation. These are concatenated and fed to a meta learner (RNN with attention) that learns a weighted combination based on relevance to the task.
- Core assumption: Different encoder stacks capture distinct aspects of the input (e.g., syntax vs semantics), and the meta learner can learn to prioritize useful ones per class.
- Evidence anchors:
  - [abstract]: "We carried out experimental identification of aggressive comments on Hindi, Bangla, and English datasets using the proposed model... achieving the highest accuracy of 95% on English, 91% on Bangla, and 91% on Hindi raw datasets."
  - [section]: "The outputs from the three stacked LSTM encoders are fed to a meta learner to provide a single encoded output... The meta learner is a simple RNN model that learns quickly due to the weighted inputs."

### Mechanism 3
- Claim: Data augmentation via semi-noisy and fully machine-translated datasets improves model generalization in low-resource settings.
- Mechanism: By adding noise and translating Hindi/Bangla to English, the dataset size increases and model is exposed to varied linguistic forms, forcing it to learn robust, language-agnostic features.
- Core assumption: Translated data, though noisy, preserves enough semantic signal to allow meaningful learning when combined with raw data.
- Evidence anchors:
  - [abstract]: "To overcome the lack of labeled datasets in languages like Hindi and Bangla, the authors augment the TRAC-2 dataset with machine-translated English data... Our model achieves state-of-the-art results among all the previous works on the dataset we used in this paper."
  - [section]: "We utilized the TRAC-2 dataset... Additionally, we created a fully machine-translated English dataset to overcome data accessibility difficulties... The translated data contained noise that might not be suitable for training deep learning models."

## Foundational Learning

- Concept: LSTM cell gates (input, forget, output) and their role in mitigating vanishing gradients.
  - Why needed here: The proposed model relies on LSTM layers to process sequences and maintain long-term dependencies, which is critical when dealing with noisy or lengthy social media comments.
  - Quick check question: What is the function of the forget gate in an LSTM cell?

- Concept: Autoencoder reconstruction loss and its use as a denoising objective.
  - Why needed here: The LSTM-Autoencoder is trained to minimize reconstruction loss, which indirectly encourages it to filter out noise and retain only the most salient features for classification.
  - Quick check question: How does reconstruction loss differ from classification loss in an autoencoder?

- Concept: Cross-lingual embeddings and their limitations when translating low-resource languages.
  - Why needed here: The dataset includes machine-translated text from Bangla and Hindi

## Architecture Onboarding

### Component Map
Input comments -> Preprocessing (stop word removal, tokenization) -> Three stacked LSTM encoders (parallel) -> Concatenation -> Meta learner (RNN with attention) -> Classification output; simultaneously -> LSTM decoder -> Reconstruction output

### Critical Path
Input sequence -> Three parallel stacked LSTM encoders -> Concatenated representation -> Meta learner with attention -> Classification decision

### Design Tradeoffs
- Parallel stacked encoders increase model capacity and potential for capturing diverse features but add computational complexity
- Autoencoder reconstruction objective helps filter noise but may slow convergence compared to direct classification
- Meta learner adds flexibility in combining encoder outputs but introduces additional hyperparameters

### Failure Signatures
- High reconstruction loss with low classification accuracy suggests model is not effectively filtering noise
- Similar performance across languages may indicate meta learner is not learning language-specific weightings
- Performance degradation on translated data indicates noise is overwhelming semantic content

### First 3 Experiments
1. Train model on raw English data only, compare to baseline LSTM
2. Train on raw + semi-noisy data, measure improvement in noise robustness
3. Remove meta learner component, compare performance to full model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed LSTM-Autoencoder model's performance degrade or adapt when applied to datasets in languages with significantly different linguistic structures (e.g., agglutinative vs. analytic languages)?
- Basis in paper: [inferred] The study tested on English, Hindi, and Bangla, but these languages share similar Indo-Aryan characteristics; performance on structurally different languages remains unexplored.
- Why unresolved: The paper does not evaluate the model on languages with fundamentally different syntax or morphology, such as Turkish, Finnish, or Mandarin Chinese.
- What evidence would resolve it: Systematic testing on a diverse set of languages with varied linguistic features, comparing performance metrics to the current study.

### Open Question 2
- Question: What is the impact of varying the degree of noise in synthetic data on the model's ability to generalize to real-world data?
- Basis in paper: [explicit] The paper discusses using semi-noisy and fully noisy datasets created via machine translation, but does not analyze how different noise levels affect generalization.
- Why unresolved: While the study evaluates performance on noisy data, it does not systematically vary noise intensity to measure its effect on model robustness.
- What evidence would resolve it: Experiments with controlled noise injection at multiple levels, tracking accuracy and other metrics to identify optimal noise thresholds.

### Open Question 3
- Question: How does the proposed model compare in terms of computational efficiency and resource usage to existing transformer-based models like BERT or GPT-2 for cyberbullying detection?
- Basis in paper: [inferred] The paper emphasizes accuracy but does not discuss computational cost, training time, or memory usage relative to baselines.
- Why unresolved: No runtime, memory, or energy consumption data are provided, making it unclear if the model is practical for large-scale deployment.
- What evidence would resolve it: Detailed benchmarking of training/inference time, GPU/CPU usage, and model size across all tested architectures.

## Limitations
- Limited ablation studies to quantify the benefit of the complex parallel encoder architecture
- No systematic analysis of how translation noise level affects model performance
- Computational efficiency and resource usage not compared to transformer baselines

## Confidence

**High** for raw dataset performance claims (95% English, 91% Bangla/Hindi)
**Medium** for semi-noisy and fully translated data performance claims
**Medium** for claims about parallel encoder architecture benefits

## Next Checks

1. **Noise sensitivity analysis**: Systematically vary the amount of machine translation noise in the input data and measure performance degradation to validate the claimed robustness.

2. **Architecture ablation study**: Remove the meta learner component and compare performance against the full model to quantify the benefit of the parallel stacked encoder architecture.

3. **Cross-validation on language-specific features**: Perform detailed analysis of feature importance across the three languages to determine if the meta learner is actually learning language-specific weightings or if performance gains are primarily from the autoencoder reconstruction objective.