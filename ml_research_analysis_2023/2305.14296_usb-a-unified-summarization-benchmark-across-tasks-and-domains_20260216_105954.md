---
ver: rpa2
title: 'USB: A Unified Summarization Benchmark Across Tasks and Domains'
arxiv_id: '2305.14296'
source_url: https://arxiv.org/abs/2305.14296
tags:
- summary
- tasks
- summarization
- training
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces USB, a unified benchmark for evaluating summarization
  models across 8 tasks and 6 domains. The benchmark includes tasks such as extractive
  and abstractive summarization, topic-based summarization, and factuality-related
  tasks like evidence extraction and unsupported span prediction.
---

# USB: A Unified Summarization Benchmark Across Tasks and Domains

## Quick Facts
- **arXiv ID**: 2305.14296
- **Source URL**: https://arxiv.org/abs/2305.14296
- **Reference count**: 22
- **Key outcome**: Moderately-sized fine-tuned models outperform larger few-shot prompted language models across 8 summarization tasks and 6 domains.

## Executive Summary
This paper introduces USB, a unified benchmark for evaluating summarization models across 8 tasks and 6 domains using Wikipedia articles. The benchmark includes tasks such as extractive and abstractive summarization, topic-based summarization, and factuality-related tasks like evidence extraction and unsupported span prediction. The authors find that fine-tuned models consistently outperform few-shot prompted LLMs, and human-annotated data is more effective than heuristically generated data even at 20x the scale. The study reveals that training data amount matters more than domain for some tasks, while domain-specific training is more beneficial for others.

## Method Summary
The USB benchmark curates a high-quality human-annotated dataset from Wikipedia articles spanning 6 domains. The evaluation framework includes 8 summarization tasks ranging from extractive and abstractive summarization to factuality-related tasks. Models are evaluated using task-specific metrics including Rouge, F1, AUC, and Exact Match. The study compares fine-tuned T5 and Flan-T5 models against few-shot prompted Llama-13B, Vicuna-13B, and GPT-3.5-turbo. Both human-labeled and heuristically generated training data are evaluated to assess their relative effectiveness.

## Key Results
- Fine-tuned moderately-sized models consistently outperform larger few-shot prompted language models across multiple tasks
- Human-annotated data outperforms heuristically generated data by 20x margin for factuality-related tasks
- Training data amount matters more than domain for some tasks, while domain-specific training is more beneficial for others
- Models trained on small amounts of human-labeled data outperform those trained on 20x more heuristically generated labeled data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Fine-tuned moderately-sized models consistently outperform larger few-shot prompted language models on summarization tasks.
- **Mechanism**: Fine-tuning adapts the model parameters to the specific task and domain, allowing it to learn the nuances and patterns required for effective summarization. Few-shot prompting, while leveraging the broad knowledge of larger models, may not provide sufficient task-specific guidance.
- **Core assumption**: The summarization tasks in the USB benchmark require task-specific knowledge and understanding that can be better captured through fine-tuning than few-shot prompting.
- **Evidence anchors**:
  - [abstract]: "we find that moderately-sized fine-tuned models consistently outperform larger few-shot prompted language models across multiple tasks."
  - [section]: "Compared to factuality-based tasks, ChatGPT does better on tasks that involve generating summaries. Compared to ChatGPT, finetuned models perform better on every task."

### Mechanism 2
- **Claim**: Training on human-annotated data outperforms training on heuristically generated data, even when the latter is 20 times larger.
- **Mechanism**: Human annotations provide high-quality, task-specific labels that capture the nuances and complexities of the summarization tasks. Heuristically generated labels, while easier to obtain in large quantities, may contain noise and inaccuracies that hinder model performance.
- **Core assumption**: The quality of training data labels is more important than the quantity for learning effective summarization models.
- **Evidence anchors**:
  - [abstract]: "For factuality-related tasks, we also evaluate existing heuristics to create training data and find that training on them results in worse performance than training on 20× less human-labeled data."
  - [section]: "Training on human-annotated data performs better than all heuristic-based alternatives across all tasks."

### Mechanism 3
- **Claim**: The amount of training data matters more than the domain for some tasks, while for others, domain-specific training data is more beneficial.
- **Mechanism**: Different summarization tasks have varying degrees of domain dependence. Tasks that require general summarization skills, such as factuality classification, can benefit from larger amounts of out-of-domain data. However, tasks that require domain-specific knowledge, such as abstractive summarization, may perform better with smaller amounts of in-domain data.
- **Evidence anchors**:
  - [abstract]: "The study also reveals that the amount of training data matters more than the domain for some tasks, while for others, domain-specific training data is more beneficial."
  - [section]: "For tasks requiring summary generation, using the large biographies training set often does worse than using the 15× smaller in-domain training set."

## Foundational Learning

- **Concept**: Fine-tuning vs. few-shot prompting
  - **Why needed here**: Understanding the differences between fine-tuning and few-shot prompting is crucial for interpreting the results and drawing conclusions about the effectiveness of different training approaches for summarization models.
  - **Quick check question**: What are the key differences between fine-tuning and few-shot prompting, and how do they impact model performance on specific tasks?

- **Concept**: Human-annotated vs. heuristically generated data
  - **Why needed here**: Recognizing the trade-offs between using human-annotated data and heuristically generated data is essential for evaluating the quality and reliability of the training data used in the USB benchmark.
  - **Quick check question**: What are the advantages and disadvantages of using human-annotated data versus heuristically generated data for training summarization models?

- **Concept**: Domain adaptation and transfer learning
  - **Why needed here**: Understanding the concepts of domain adaptation and transfer learning is important for interpreting the results on out-of-domain performance and the impact of training data domain on model performance.
  - **Quick check question**: How do domain adaptation and transfer learning techniques impact the performance of summarization models when trained on out-of-domain data?

## Architecture Onboarding

- **Component map**: Wikipedia articles (6 domains) -> Human annotations (8 tasks) -> Fine-tuned models vs. few-shot prompted LLMs -> Task-specific evaluation metrics
- **Critical path**: Select tasks → Prepare training/evaluation data → Choose and fine-tune models → Evaluate performance → Analyze results
- **Design tradeoffs**: Balancing number of tasks and domains vs. dataset size, choosing between human-annotated and heuristically generated data, selecting appropriate model architectures and training approaches
- **Failure signatures**: Overfitting to benchmark tasks, not addressing factuality and controllability, failing to generalize to real-world scenarios
- **First 3 experiments**:
  1. Evaluate fine-tuned T5-Large on extractive summarization using USB benchmark
  2. Compare fine-tuned model vs few-shot prompted model on unsupported span prediction
  3. Analyze impact of training data domain on abstractive summarization performance

## Open Questions the Paper Calls Out

The paper identifies several open questions that remain unresolved:

- How do different summarization tasks compare in terms of domain transfer performance, and what factors contribute to these differences?
- What is the optimal balance between human-annotated and heuristically generated training data for different summarization tasks?
- How do different model architectures (encoder-only, encoder-decoder, decoder-only) compare in performance across the USB benchmark tasks?
- What is the relationship between summary abstractiveness and performance on factuality-related tasks?
- How do different types of factual errors (e.g., entity swapping vs. numerical errors) impact the difficulty of factuality correction tasks?

## Limitations

- Benchmark relies heavily on Wikipedia-derived data, limiting generalizability to other domains
- Study focuses primarily on English-language summarization
- Doesn't explore cost-benefit tradeoff between human annotations and heuristic generation methods
- Doesn't investigate semi-supervised approaches that could combine both annotation strategies

## Confidence

- **High Confidence**: Fine-tuned moderately-sized models outperform few-shot prompted LLMs consistently across tasks
- **Medium Confidence**: Human annotations are superior to heuristic labels, though dependent on specific heuristic methods used
- **Medium Confidence**: Domain-specific vs. data-amount findings show patterns but may be influenced by Wikipedia domain selection

## Next Checks

1. Reproduce key results with alternative datasets to assess generalizability beyond Wikipedia
2. Conduct cost-effectiveness analysis comparing human annotations vs heuristic generation methods, including semi-supervised approaches
3. Test out-of-distribution robustness on adversarially generated summaries or unseen domains