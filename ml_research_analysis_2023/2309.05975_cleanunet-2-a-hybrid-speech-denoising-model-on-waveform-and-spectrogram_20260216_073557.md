---
ver: rpa2
title: 'CleanUNet 2: A Hybrid Speech Denoising Model on Waveform and Spectrogram'
arxiv_id: '2309.05975'
source_url: https://arxiv.org/abs/2309.05975
tags:
- speech
- cleanunet
- waveform
- spectrogram
- denoising
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CleanUNet 2 is a hybrid speech denoising model that combines a
  spectrogram-based denoiser (CleanSpecNet) and a waveform-based denoiser (CleanUNet).
  CleanSpecNet predicts a clean spectrogram from noisy input, which is then used to
  condition CleanUNet to generate a clean waveform.
---

# CleanUNet 2: A Hybrid Speech Denoising Model on Waveform and Spectrogram

## Quick Facts
- arXiv ID: 2309.05975
- Source URL: https://arxiv.org/abs/2309.05975
- Authors: 
- Reference count: 0
- Key outcome: CleanUNet 2 achieves state-of-the-art speech denoising performance by combining CleanSpecNet (spectrogram-based) and CleanUNet (waveform-based) in a two-stage framework, improving PESQ scores by over 0.1 compared to previous methods.

## Executive Summary
CleanUNet 2 introduces a hybrid speech denoising approach that combines the complementary strengths of spectrogram and waveform-based methods. The model uses CleanSpecNet to predict clean spectrograms from noisy inputs, which are then used to condition CleanUNet for generating clean waveforms. This two-stage framework achieves superior performance on the DNS dataset, outperforming existing state-of-the-art methods in both objective metrics (PESQ, STOI) and subjective evaluations. The approach demonstrates the effectiveness of leveraging both time-frequency and time-domain representations for speech enhancement.

## Method Summary
CleanUNet 2 employs a two-stage framework where CleanSpecNet first predicts a clean spectrogram from noisy input using convolutional layers and self-attention blocks with ℓ1 + Frobenius loss. The predicted spectrogram is then up-sampled and element-wise added to the noisy waveform features before being fed into CleanUNet, which generates the final clean waveform using a modified U-Net architecture with self-attention. The model is trained on 500 hours of clean-noisy speech pairs with 31 SNR levels from -5 to 25dB using Adam optimizer with linear warmup and cosine annealing. Element-wise addition is used as the conditioning method for its simplicity and minimal model footprint.

## Key Results
- CleanUNet 2 achieves PESQ improvement of over 0.1 compared to previous state-of-the-art methods
- The hybrid model outperforms both CleanSpecNet and CleanUNet individually across all noise conditions
- Statistical tests confirm CleanUNet 2's consistent superiority in subjective MOS evaluations across SIG, BAK, and OVRL dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hybrid two-stage approach leverages complementary strengths of spectrogram and waveform-based methods
- Mechanism: CleanSpecNet predicts a clean spectrogram from noisy input, providing high-quality phase information and preserving speech quality. This predicted spectrogram then conditions CleanUNet to generate the clean waveform, combining accurate spectral features with waveform-based noise removal capabilities
- Core assumption: The predicted spectrogram contains sufficient information to guide CleanUNet in generating high-quality clean waveforms
- Evidence anchors:
  - [abstract]: "CleanUNet 2 uses a two-stage framework inspired by popular speech synthesis methods that consist of a waveform model and a spectrogram model"
  - [section]: "We note that spectrogram and waveform-based models are 'complementary' under high noise levels"
  - [corpus]: Weak - related papers focus on spectrogram enhancement and waveform generation but don't directly discuss this specific two-stage conditioning approach
- Break condition: If the predicted spectrogram quality degrades significantly, CleanUNet loses its conditioning signal and performance drops to that of CleanUNet alone

### Mechanism 2
- Claim: Element-wise addition conditioning provides optimal balance between information preservation and model simplicity
- Mechanism: The up-sampled spectrogram is element-wise added to the noisy waveform features before being fed into CleanUNet, allowing CleanUNet to directly use both sources of information without complex transformation
- Core assumption: Element-wise addition preserves the most important features from both sources while maintaining computational efficiency
- Evidence anchors:
  - [section]: "We use element-wise addition as our main conditioning method. Other methods such as concatenation on channels, or FiLM [36] lead to similar results"
  - [section]: "Since the element-wise addition is the simplest and leads to the smallest model footprint, we use this conditioning method in CleanUNet 2"
  - [corpus]: Weak - related papers discuss different conditioning methods but don't provide comparative analysis for speech denoising
- Break condition: If the conditioning signal becomes too weak relative to the noisy waveform features, CleanUNet may ignore the spectrogram information

### Mechanism 3
- Claim: STFT hyperparameter selection critically affects hybrid model performance
- Mechanism: The STFT parameters (window length, hop size, FFT bins) determine the spectrogram quality, which in turn affects both CleanSpecNet performance and the conditioning signal for CleanUNet
- Core assumption: Optimal STFT parameters for CleanSpecNet differ from those that maximize CleanUNet 2 performance due to the conditioning interaction
- Evidence anchors:
  - [section]: "Results show that our hybrid model can outperform SOTA speech denoisers in both objective and subjective evaluation metrics"
  - [section]: "Interestingly, the best performance of CleanUNet 2 (PESQ: 3.262) is achieved by combining the waveform model with a spectrogram-based model (CleanSpecNet) using typical neural vocoder STFT parameters"
  - [corpus]: Weak - related papers discuss STFT parameter selection but don't analyze their impact on hybrid models
- Break condition: If STFT parameters create artifacts or insufficient frequency resolution, the hybrid approach loses its advantage

## Foundational Learning

- Concept: STFT and spectrogram representation
  - Why needed here: Understanding how audio signals are transformed into spectrograms is crucial for grasping CleanSpecNet's role and the conditioning mechanism
  - Quick check question: What information is lost when converting from waveform to spectrogram, and why does this matter for denoising?

- Concept: WaveNet and U-Net architectures
  - Why needed here: CleanUNet uses a modified U-Net architecture, and understanding its components (encoder, decoder, skip connections) is essential for understanding how the conditioning signal is incorporated
  - Quick check question: How do skip connections in U-Net architecture help preserve fine-grained information during denoising?

- Concept: Self-attention mechanisms in speech processing
  - Why needed here: Both CleanSpecNet and CleanUNet use self-attention blocks, and understanding their role in capturing long-range dependencies is important for model comprehension
  - Quick check question: Why are self-attention mechanisms particularly useful for speech denoising compared to traditional convolutional approaches?

## Architecture Onboarding

- Component map: Noisy waveform → STFT → CleanSpecNet → Predicted spectrogram → Upsample → Add to waveform → CleanUNet → Clean waveform
- Critical path: The predicted spectrogram must be properly aligned and conditioned with the waveform features before CleanUNet processing
- Design tradeoffs:
  - Spectrogram resolution vs. computational cost
  - Conditioning method complexity vs. performance
  - Model size vs. real-time inference requirements
  - Training stability vs. convergence speed
- Failure signatures:
  - High-frequency artifacts suggest STFT parameter issues
  - Residual noise indicates CleanSpecNet underperformance
  - Speech quality degradation suggests conditioning signal problems
  - Phase distortion indicates issues in the spectrogram-to-waveform conversion
- First 3 experiments:
  1. Test CleanSpecNet alone with different STFT parameters to establish baseline performance
  2. Verify CleanUNet performance without conditioning to establish baseline
  3. Test different conditioning methods (addition vs. concatenation vs. FiLM) to confirm element-wise addition is optimal

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of conditioning method (addition, concatenation, FiLM) quantitatively impact the performance of CleanUNet 2 beyond the qualitative observations presented?
- Basis in paper: [explicit] Table 5 shows that different conditioning methods lead to very similar results, with element-wise addition being chosen for simplicity and model footprint.
- Why unresolved: The paper does not provide a detailed quantitative analysis of how each conditioning method affects model performance across different noise levels or types.
- What evidence would resolve it: Comprehensive experiments comparing the three conditioning methods across diverse noise scenarios, SNR levels, and objective/subjective metrics would clarify their relative effectiveness.

### Open Question 2
- Question: Can the CleanUNet 2 architecture be effectively extended to multi-channel speech denoising scenarios, and what modifications would be necessary?
- Basis in paper: [inferred] The paper focuses on single-channel denoising and mentions the model is causal for online streaming, but does not explore multi-channel extensions or spatial audio processing.
- Why unresolved: Multi-channel audio processing involves additional considerations like inter-channel phase relationships and spatial coherence that are not addressed in the single-channel framework.
- What evidence would resolve it: Experiments applying CleanUNet 2 to multi-channel datasets with appropriate architectural modifications (e.g., multi-channel spectrogram processing, spatial conditioning) would demonstrate feasibility and performance gains.

### Open Question 3
- Question: What is the optimal balance between spectrogram and waveform-based components in the hybrid model for different noise characteristics (e.g., stationary vs. non-stationary noise)?
- Basis in paper: [inferred] The paper demonstrates that the hybrid approach outperforms individual components but does not analyze how component weighting should vary with noise type.
- Why unresolved: Different noise characteristics may require different trade-offs between spectrogram-based quality preservation and waveform-based noise suppression that are not explored.
- What evidence would resolve it: Systematic experiments varying the relative contributions of CleanSpecNet and CleanUNet across different noise types and SNR levels would identify optimal hybrid configurations.

## Limitations

- Limited analysis of alternative conditioning methods beyond the three tested, leaving uncertainty about whether element-wise addition is truly optimal
- Performance evaluation focused primarily on the DNS dataset without extensive testing on diverse real-world noise conditions
- Lack of systematic ablation studies on critical design choices like STFT parameters and their interaction with the hybrid architecture

## Confidence

- **High confidence**: The hybrid architecture concept and its basic implementation are sound, with clear objective improvements over baselines
- **Medium confidence**: The element-wise addition conditioning method is optimal, though alternative methods were not exhaustively tested
- **Medium confidence**: The claim of state-of-the-art performance, as subjective evaluations were conducted on a subset of conditions and may not fully represent real-world performance

## Next Checks

1. **Ablation study on STFT parameters**: Systematically vary STFT window length, hop size, and FFT bins to identify optimal configurations for the hybrid model across different noise conditions

2. **Extended conditioning method analysis**: Compare element-wise addition against more sophisticated conditioning approaches (e.g., FiLM, attention-based gating) across multiple datasets to verify the optimality claim

3. **Real-world deployment testing**: Evaluate CleanUNet 2 on out-of-domain datasets with diverse acoustic environments and noise types to assess generalization beyond the DNS benchmark