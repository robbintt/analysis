---
ver: rpa2
title: Cascade Speculative Drafting for Even Faster LLM Inference
arxiv_id: '2312.11462'
source_url: https://arxiv.org/abs/2312.11462
tags:
- speculative
- decoding
- drafting
- draft
- cascade
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the inefficiency of speculative decoding
  in large language model (LLM) inference, which involves slow autoregressive generation
  and equal time allocation for generating tokens of different importance. The authors
  propose Cascade Speculative Drafting (CS Drafting), a novel approach that employs
  two types of cascades: the Vertical Cascade eliminates autoregressive generation
  from neural models, and the Horizontal Cascade optimizes time allocation in drafting
  for improved efficiency.'
---

# Cascade Speculative Drafting for Even Faster LLM Inference

## Quick Facts
- arXiv ID: 2312.11462
- Source URL: https://arxiv.org/abs/2312.11462
- Reference count: 29
- Key result: Achieves up to 72% additional speedup over speculative decoding while preserving target model output distribution

## Executive Summary
This paper addresses inefficiencies in speculative decoding for LLM inference by proposing Cascade Speculative Drafting (CS Drafting), which employs two cascades: the Vertical Cascade eliminates autoregressive generation from neural models by using a non-neural language model (Max-Gram), and the Horizontal Cascade optimizes time allocation by assigning smaller draft models to generate tokens with lower acceptance probability. The method achieves significant speed improvements while maintaining output quality equivalent to the target model.

## Method Summary
CS Drafting combines vertical and horizontal cascades to optimize speculative decoding. The vertical cascade replaces autoregressive generation in neural draft models with a simple non-neural language model (Max-Gram) that greedily finds maximal matches in the input. The horizontal cascade assigns different draft models to generate tokens at different positions based on their acceptance probability, with smaller models handling later tokens that have lower acceptance rates. The method uses multiple draft models of varying sizes, with each model reviewing generations from smaller models and proposing its reviewed content to larger models or the target model.

## Key Results
- Achieves up to 72% additional speedup over speculative decoding baselines
- Preserves the same output distribution as the target model
- Demonstrates effectiveness across GSM8k and MMLU datasets
- Provides theoretical analysis proving optimality of the horizontal cascade

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Vertical Cascade eliminates autoregressive generation from neural models by replacing it with a simple non-neural language model (Max-Gram) for the initial token generation.
- Mechanism: Instead of having the smallest draft model generate tokens autoregressively, the system uses Max-Gram to generate tokens greedily based on maximal matches in the input or existing generation. This eliminates the sequential generation bottleneck while maintaining reasonable output quality.
- Core assumption: The Max-Gram algorithm can generate tokens with sufficient quality to be accepted by the target model, while being significantly faster than autoregressive neural generation.
- Evidence anchors: [abstract]: "The Vertical Cascade eliminates autoregressive generation from neural models", [section 3.3]: "Max-Gram(MaG) algorithm that greedily finds maximal matches in the initial input"

### Mechanism 2
- Claim: The Horizontal Cascade optimizes time allocation by using smaller, faster draft models for generating tokens that are more likely to be rejected.
- Mechanism: The system assigns different draft models to generate tokens at different positions in the draft sequence, based on the probability of acceptance. Tokens later in the sequence (which have lower acceptance probability) are generated by smaller, faster models.
- Core assumption: The acceptance probability decreases exponentially with token position in the draft sequence, making it worthwhile to use faster but less accurate models for later tokens.
- Evidence anchors: [abstract]: "The Horizontal Cascade optimizes time allocation in drafting for improved efficiency", [section 3.1]: "tokens generated later in the sequence by the draft model exhibit a progressively lower probability of being accepted"

### Mechanism 3
- Claim: The combination of vertical and horizontal cascades achieves greater speedup than either approach alone.
- Mechanism: By combining the elimination of autoregressive generation (vertical cascade) with optimized model assignment based on acceptance probability (horizontal cascade), the system achieves multiplicative speed improvements.
- Core assumption: The speed gains from each cascade are independent and additive, allowing their combination to achieve greater total speedup.
- Evidence anchors: [abstract]: "Combining both cascades, CS Drafting achieves greater speedup compared to the baselines", [section 4.1]: Theoretical analysis showing how the combination affects expected walltime improvement

## Foundational Learning

- Concept: Speculative decoding and its limitations
  - Why needed here: Understanding the baseline method and its inefficiencies is crucial for appreciating the improvements offered by CS Drafting
  - Quick check question: What are the two main inefficiencies in speculative decoding that CS Drafting addresses?

- Concept: Expected walltime improvement factor (EWIF)
  - Why needed here: EWIF is the key metric for evaluating the performance of CS Drafting and comparing it to baseline methods
  - Quick check question: How does the formula for EWIF change when multiple draft models are involved, as in CS Drafting?

- Concept: Acceptance rate and its relationship to token position
  - Why needed here: The Horizontal Cascade relies on the observation that acceptance rate decreases with token position
  - Quick check question: According to the paper, how does the probability of acceptance change for the i-th token compared to the first token?

## Architecture Onboarding

- Component map:
  Max-Gram (simple non-neural language model) -> T5-small (neural draft model) -> T5-base (neural draft model) -> Target model (FLAN-T5-xxl)

- Critical path:
  1. Max-Gram generates initial tokens
  2. T5-small or T5-base reviews Max-Gram output and generates next set of tokens
  3. T5-base (if used) reviews T5-small output and generates additional tokens
  4. Target model verifies final draft and generates any remaining tokens

- Design tradeoffs:
  - Speed vs. quality: Using simpler models (Max-Gram) speeds up generation but may reduce quality
  - Model selection: Choosing which models to use for which positions affects both speed and quality
  - Leniency: Allowing some deviation from the target model's output can improve speed but may affect quality

- Failure signatures:
  - Low acceptance rates across all draft models
  - Significant difference in generation quality compared to baseline autoregressive generation
  - Unexpected latency spikes during certain types of input

- First 3 experiments:
  1. Implement Max-Gram as a standalone component and measure its acceptance rate with the target model
  2. Test the Horizontal Cascade by manually assigning different draft models to different token positions
  3. Combine vertical and horizontal cascades and compare performance against baseline speculative decoding on a simple dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strategy for selecting the size and number of draft models in the Vertical Cascade to maximize speed improvements without compromising output quality?
- Basis in paper: [inferred] The paper discusses the use of multiple draft models of varying sizes in the Vertical Cascade to eliminate autoregressive generation and improve efficiency. However, it does not provide a specific strategy for selecting the optimal size and number of draft models.
- Why unresolved: The paper does not explore the impact of different configurations of draft models on the overall performance of the Cascade Speculative Drafting (CS Drafting) algorithm.
- What evidence would resolve it: Experimental results comparing the performance of CS Drafting with different configurations of draft models in terms of speed improvements and output quality.

### Open Question 2
- Question: How does the Lenience parameter affect the balance between speed and output quality in the Cascade Speculative Drafting algorithm?
- Basis in paper: [explicit] The paper mentions the use of lenience in speculative decoding to achieve a tradeoff between quality and speed, but it does not explore its impact on the CS Drafting algorithm.
- Why unresolved: The paper does not provide a detailed analysis of how the lenience parameter affects the performance of the CS Drafting algorithm in terms of speed and output quality.
- What evidence would resolve it: Experimental results showing the performance of CS Drafting with different levels of lenience, comparing speed improvements and output quality.

### Open Question 3
- Question: What are the limitations of the Max-Gram algorithm in terms of generating diverse and contextually relevant tokens compared to neural language models?
- Basis in paper: [explicit] The paper introduces the Max-Gram algorithm as a simple non-neural language model that is effective in aligning with language model output. However, it does not discuss the limitations of the algorithm in terms of generating diverse and contextually relevant tokens.
- Why unresolved: The paper does not provide a comprehensive evaluation of the Max-Gram algorithm's ability to generate diverse and contextually relevant tokens compared to neural language models.
- What evidence would resolve it: Comparative analysis of the Max-Gram algorithm and neural language models in terms of the diversity and contextual relevance of generated tokens.

## Limitations
- Empirical validation is constrained to FLAN-T5 models, raising questions about generalizability to other architectures
- Acceptance rate decay assumptions underlying the horizontal cascade may not hold uniformly across different domains and tasks
- Max-Gram algorithm's effectiveness appears heavily dependent on input characteristics that aren't well-characterized

## Confidence

**High Confidence** - The vertical cascade mechanism that replaces autoregressive generation with Max-Gram is well-supported by both theoretical analysis and experimental results. The elimination of sequential generation bottlenecks is mathematically sound and the empirical speedup measurements are robust.

**Medium Confidence** - The horizontal cascade optimization for time allocation shows promise but relies on assumptions about acceptance rate decay that may not generalize. While the theoretical framework is rigorous, the empirical validation is limited to specific model combinations and datasets.

**Low Confidence** - The combined speedup claims of 72% additional improvement over speculative decoding, while impressive, are based on a limited experimental scope. The interaction effects between vertical and horizontal cascades in diverse real-world scenarios remain underexplored.

## Next Checks

1. **Cross-Architecture Validation**: Test CS Drafting with LLaMA, Mistral, and other popular LLM architectures beyond FLAN-T5 to verify generalizability of the 72% speedup claim. This would involve implementing the cascades with different model families and measuring performance across at least 3-5 diverse architectures.

2. **Domain Robustness Testing**: Evaluate acceptance rate decay patterns and overall performance across 10+ diverse domains (creative writing, code generation, summarization, dialogue, etc.) to identify scenarios where the horizontal cascade assumptions break down. This would help establish the method's practical limitations.

3. **Max-Gram Failure Characterization**: Systematically identify input characteristics and token distributions where Max-Gram performance degrades significantly, then measure the cascading effects on overall CS Drafting performance. This would involve controlled experiments varying input complexity, vocabulary diversity, and structural patterns to map the boundary conditions of the vertical cascade's effectiveness.