---
ver: rpa2
title: Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter
  Sharing
arxiv_id: '2309.15826'
source_url: https://arxiv.org/abs/2309.15826
tags:
- speech
- data
- text
- multi-tasking
- hard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of data scarcity in end-to-end
  speech-to-text translation (ST) by proposing a novel multi-tasking framework with
  hard parameter sharing. The core idea involves converting continuous speech into
  discrete token sequences using k-means clustering on self-supervised learning representations,
  enabling a unified vocabulary for both speech and text inputs.
---

# Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing

## Quick Facts
- arXiv ID: 2309.15826
- Source URL: https://arxiv.org/abs/2309.15826
- Reference count: 0
- Improves various sequence-to-sequence models by +0.5 BLEU on MuST-C without external MT data

## Executive Summary
This paper addresses the data scarcity challenge in end-to-end speech-to-text translation (ST) by proposing a novel multi-tasking framework with hard parameter sharing. The core innovation involves converting continuous speech into discrete token sequences using k-means clustering on self-supervised learning representations, enabling a unified vocabulary for both speech and text inputs. This approach allows the same model parameters to be shared across ST and machine translation (MT) tasks, eliminating the need for separate encoders or cross-modal regularization modules.

The method demonstrates significant improvements across multiple sequence-to-sequence architectures including attentional encoder-decoder, CTC, transducer, and joint CTC/attention models. Experiments on the MuST-C corpus show an average improvement of +0.5 BLEU without requiring external MT data. Further enhancements of +0.8 BLEU are achieved by incorporating external MT data, and transfer learning from pre-trained textual models yields an additional +1.8 BLEU improvement, demonstrating the effectiveness of the discrete tokenization approach in enabling cross-modal parameter sharing.

## Method Summary
The approach converts speech and text inputs into discrete token sequences using k-means clustering on WavLM representations, creating a shared vocabulary that allows hard parameter sharing between ST and MT tasks. Speech is discretized by clustering WavLM SSL representations (21st layer) into 2000 centroids, followed by repetition removal and subword tokenization with SentencePiece (4000 units). Text is similarly tokenized and up-sampled by repeating tokens 4x to match speech sequence lengths. The combined vocabulary (8000 units) is processed by shared encoder-decoder models trained on both ST and MT data. The framework supports multiple architectures including AED, CTC, RNN-T, and CTC/Attn models with hierarchical encoding and shared parameters.

## Key Results
- Improves various sequence-to-sequence models by +0.5 BLEU on MuST-C without external MT data
- Achieves +0.8 BLEU improvement when incorporating external MT data
- Gains +1.8 BLEU from transfer learning with pre-trained textual models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discrete speech tokenization reduces the modality gap between speech and text inputs
- Mechanism: K-means clustering on WavLM representations converts continuous speech into discrete token sequences, creating a shared input vocabulary that allows speech and text to be processed interchangeably by the same model
- Core assumption: The discrete speech tokens preserve sufficient semantic information for translation tasks
- Evidence anchors:
  - [abstract] "Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length"
  - [section] "With discrete speech inputs, the ST task seeks to map a sequence of tokens X DISC into another sequence of tokens Y TGT"
  - [corpus] Weak evidence - no direct corpus citations supporting this specific mechanism
- Break condition: If discrete speech tokens lose critical acoustic information needed for accurate translation

### Mechanism 2
- Claim: Hard parameter sharing with discrete inputs eliminates need for modality-specific modules
- Mechanism: By converting both speech and text to discrete tokens in a joint vocabulary, the same model parameters can process both modalities without separate encoders or cross-modal regularization
- Core assumption: Discrete representations from different modalities can be effectively aligned through shared parameters
- Evidence anchors:
  - [abstract] "we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally"
  - [section] "To incorporate textual inputs into our discrete ST models, we simply extend the input vocabulary V CROSS = V SPE ∪ V SRC"
  - [corpus] Weak evidence - no direct corpus citations supporting this specific mechanism
- Break condition: If the shared parameters cannot effectively handle the different statistical properties of speech and text tokens

### Mechanism 3
- Claim: Text up-sampling balances sequence lengths for effective multi-tasking
- Mechanism: Repeating source text tokens by a factor of 4 brings text sequence lengths closer to discrete speech sequences, enabling better parameter sharing and regularization effects
- Core assumption: The up-sampled text tokens can still be effectively processed by the model despite repetition
- Evidence anchors:
  - [section] "Since the discrete speech sequences X DISC are still longer than their corresponding source text Y SRC , we repeat the source text tokens by a factor of 4"
  - [section] "We found that 4x up-sampling was best" (from ablation study)
  - [corpus] Weak evidence - no direct corpus citations supporting this specific mechanism
- Break condition: If excessive up-sampling introduces too much redundancy or harms translation quality

## Foundational Learning

- Concept: K-means clustering for vector quantization
  - Why needed here: Creates discrete speech tokens from continuous WavLM representations
  - Quick check question: How does the choice of k (number of clusters) affect the granularity of speech representation?

- Concept: Subword tokenization with SentencePiece
  - Why needed here: Further reduces sequence length of discrete speech tokens while preserving semantic information
  - Quick check question: What's the trade-off between vocabulary size and sequence length when applying subword tokenization?

- Concept: Sequence-to-sequence model architectures (AED, CTC, RNN-T, CTC/Attn)
  - Why needed here: Different model types benefit from hard parameter sharing in different ways
  - Quick check question: How do the different loss functions (CE, CTC, RNN-T) contribute to the overall training objective?

## Architecture Onboarding

- Component map: WavLM SSL model (21st layer) → K-means clustering (2000 centroids) → Discrete speech tokens → SentencePiece subword model (4000 units) → Joint vocabulary (8000 units) → Embedding layer → Seq2Seq model → Translation output
- Critical path: Discrete speech/text → Joint embedding → Encoder layers → Decoder layers → Translation output
- Design tradeoffs:
  - K-means vs. other discretization methods (HuBERT, VQ-VAE)
  - Vocabulary size vs. model capacity and training efficiency
  - Up-sampling factor vs. sequence alignment quality
  - Single-task vs. multi-task training dynamics
- Failure signatures:
  - BLEU scores plateau or decrease when adding MT data
  - Training instability with large up-sampling factors
  - Disproportionate improvement on one language pair vs. others
  - Overfitting to specific tokenization patterns
- First 3 experiments:
  1. Baseline single-task model vs. hard multi-task model without external data
  2. Ablation study on up-sampling factors (1x, 2x, 4x, 6x)
  3. Transfer learning experiment with mBART initialization vs. from-scratch training

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important questions arise from the methodology and results presented.

## Limitations
- The quality of discrete speech tokens may not consistently preserve semantic information across all language pairs and domains
- The 4x text up-sampling strategy lacks theoretical justification and may introduce unwanted redundancy in certain contexts
- Hard parameter sharing assumes a single model can effectively handle both speech and text modalities, which may lead to suboptimal performance if modalities have fundamentally different statistical properties

## Confidence

**High Confidence**: The experimental results showing +0.5 BLEU improvement on MuST-C without external MT data are well-supported by the methodology and consistent with the proposed mechanism of reducing modality gaps through discrete tokenization.

**Medium Confidence**: The additional +0.8 BLEU improvement from incorporating external MT data is plausible but depends heavily on the quality and relevance of the external corpus. The transfer learning results showing +1.8 BLEU from mBART initialization are promising but may be sensitive to the specific pre-trained model used.

**Low Confidence**: The claim that this approach generalizes to all sequence-to-sequence architectures (AED, CTC, RNN-T, CTC/Attn) equally well is not fully validated, as the paper doesn't provide detailed per-architecture ablation studies or analyze potential architecture-specific failure modes.

## Next Checks

1. **Cross-language generalization test**: Evaluate the discrete tokenization and hard parameter sharing approach on additional language pairs beyond the three tested (En-De, En-Es, En-Fr) to assess whether the improvements generalize to languages with different phonological and syntactic properties.

2. **Tokenization sensitivity analysis**: Systematically vary the k-means clustering parameters (number of centroids) and subword vocabulary size to identify optimal configurations and understand how tokenization choices affect translation quality across different language pairs.

3. **Modality-specific performance isolation**: Conduct controlled experiments to measure how much of the improvement comes from speech-to-text learning versus text-to-text learning when both modalities are present, by comparing against models trained on only one modality with the same tokenization approach.