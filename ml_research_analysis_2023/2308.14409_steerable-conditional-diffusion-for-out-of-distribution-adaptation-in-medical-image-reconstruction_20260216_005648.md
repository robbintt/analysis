---
ver: rpa2
title: Steerable Conditional Diffusion for Out-of-Distribution Adaptation in Medical
  Image Reconstruction
arxiv_id: '2308.14409'
source_url: https://arxiv.org/abs/2308.14409
tags:
- data
- diffusion
- image
- aapm
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of out-of-distribution (OOD)
  performance in diffusion models for medical image reconstruction. The core method,
  Steerable Conditional Diffusion (SCD), adapts the denoising network during sampling
  based on measured data to enforce data consistency and improve reconstruction accuracy.
---

# Steerable Conditional Diffusion for Out-of-Distribution Adaptation in Medical Image Reconstruction

## Quick Facts
- **arXiv ID**: 2308.14409
- **Source URL**: https://arxiv.org/abs/2308.14409
- **Reference count**: 31
- **Primary result**: SCD achieves up to 4dB higher PSNR than state-of-the-art diffusion models for OOD medical image reconstruction by adapting denoising parameters per-sample.

## Executive Summary
This paper introduces Steerable Conditional Diffusion (SCD), a method for adapting pretrained diffusion models to out-of-distribution (OOD) medical image reconstruction tasks. SCD improves reconstruction accuracy by updating denoising network parameters during sampling based on measured data, using a memory-efficient LoRA injection mechanism. The approach demonstrates significant gains in PSNR and SSIM across CT and MRI datasets while reducing hallucinatory features in reconstructions.

## Method Summary
SCD adapts a pretrained diffusion model during reverse sampling by injecting LoRA residual pathways into the U-Net denoising network. At each timestep, the method estimates the denoised image, projects it toward measured data consistency using Tweedie's formula, then updates the LoRA parameters to minimize data consistency loss. This per-sample adaptation allows the model to handle domain shifts without full fine-tuning. The method supports both preconditioned conjugate gradient and gradient descent for data consistency projection.

## Key Results
- Achieves up to 4dB higher PSNR compared to state-of-the-art diffusion models on OOD datasets
- Demonstrates substantial improvements in SSIM metrics across multiple imaging modalities
- Effectively reduces hallucinatory features in reconstructions from OOD data
- Maintains memory efficiency through LoRA injection (small fraction of full model parameters)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SCD improves OOD reconstruction by adapting the denoising network parameters per-sample using measured data.
- Mechanism: At each reverse sampling step, SCD injects a low-rank residual pathway (LoRA) into the U-Net, then updates the residual parameters by minimizing the data consistency loss between the denoised estimate and the measured data.
- Core assumption: The LoRA residual can effectively capture domain shift without altering the pretrained backbone, and a small number of optimization steps per sample is sufficient to align the prior with the new distribution.
- Evidence anchors:
  - [abstract] "SCD adapts the denoising network during sampling based on measured data to enforce data consistency and improve reconstruction accuracy."
  - [section] "We augment each convolutional layer in ϵ(•; θ∗) with learnable rank decomposition matrices via Low Rank (LoRA) injection... The residual ∆θ is then updated based on ˆx′0 as per line 8 in Algorithm 1."
  - [corpus] No strong evidence in corpus papers that LoRA is used for per-sample OOD adaptation in diffusion models; this appears to be novel to SCD.
- Break condition: If the domain shift is too large, the small residual may not capture necessary changes, or if the measured data is too noisy, the adaptation may overfit.

### Mechanism 2
- Claim: SCD reduces hallucinated features by conditioning the denoising estimate on the measured data at every step.
- Mechanism: SCD replaces the standard Tweedie estimate with a data-consistent version: ˆx′0 = CG(p)(ˆx0), where CG applies conjugate gradient steps toward minimizing the negative log-likelihood of the measured data.
- Core assumption: The posterior mean E[x0|xt, y] can be approximated via Tweedie's formula plus a data-consistency projection, and this projection is sufficiently close to the true posterior mean.
- Evidence anchors:
  - [abstract] "adapts the diffusion model... based solely on the information provided by the available measurement."
  - [section] "To condition the de-noised posterior mean on y, SCD uses the results from (Ravula et al. 2023)... Tweedie estimate conditioned on y is E[x0|xt, y] ≈ ˆx0 − γA⊤(Aˆx0 − y) =: ˆx′0."
  - [corpus] Related papers mention conditional diffusion for MRI, but none explicitly describe per-step Tweedie + data-consistency projection in this way.
- Break condition: If the approximation in eq. (8) is poor (e.g., due to ill-conditioning of A or strong noise), the conditioning may not improve reconstruction and could introduce bias.

### Mechanism 3
- Claim: SCD is memory-efficient because it avoids full fine-tuning of the diffusion model and instead uses LoRA.
- Mechanism: LoRA adds small residual matrices (A,B) to convolutional layers, initialized randomly, so the number of trainable parameters is a small fraction of the full model.
- Core assumption: The rank r of the residual is small enough (e.g., r ≪ min(m,n)) that it captures domain shift without overfitting, yet large enough to improve reconstruction.
- Evidence anchors:
  - [section] "Given a learned matrix W... the LoRA injection re-writes W as ˜W = W + ∆W, with ∆W = AB⊤... r ≪ min(m, n)."
  - [section] "At t=0 we have AB⊤=0... Note that LoRA injection is utilised; however, SCD is also adaptable to incorporate alternative fine-tuning strategies."
  - [corpus] LoRA is mentioned in corpus as a fine-tuning strategy for diffusion models, but not specifically for OOD adaptation in inverse problems.
- Break condition: If r is too small, the adaptation may be insufficient; if r is too large, memory savings are lost and overfitting risk increases.

## Foundational Learning

- **Concept**: Diffusion probabilistic models and reverse sampling
  - Why needed here: SCD builds on pretrained diffusion models; understanding DDIM and score-based sampling is essential to grasp how the model is adapted during generation.
  - Quick check question: What is the difference between ancestral sampling and DDIM in diffusion models, and why is DDIM preferred here?

- **Concept**: Tweedie's formula and conditional posterior estimation
  - Why needed here: SCD uses Tweedie's formula to estimate E[x0|xt] and its conditional variant E[x0|xt,y]; understanding these estimates is key to the data-consistency mechanism.
  - Quick check question: How does Tweedie's formula relate the posterior mean to the denoising score, and why is the conditional version needed for OOD adaptation?

- **Concept**: Low-rank adaptation (LoRA) and residual injection
  - Why needed here: SCD injects LoRA into U-Net layers for per-sample adaptation; understanding how LoRA works is necessary for tuning and debugging.
  - Quick check question: What are the trade-offs in choosing the rank r for LoRA, and how does this affect adaptation capacity and memory usage?

## Architecture Onboarding

- **Component map**: Pretrained U-Net (frozen) -> LoRA residual injection -> Data consistency projection -> Sampling loop
- **Critical path**:
  1. Sample xt from previous step
  2. Compute denoised estimate ˆx0 using current θ*, ∆θ
  3. Apply Γ to enforce data consistency → ˆx′0
  4. Update ∆θ by minimizing ∥Aˆx′0 − y∥²₂
  5. Use adjusted estimate in DDIM update to get xt−1
- **Design tradeoffs**:
  - LoRA rank r vs. adaptation capacity: higher r = more flexibility, more parameters
  - Number of CG steps p vs. data consistency: more steps = better consistency, more compute
  - Learning rate and optimization steps K vs. overfitting: higher rate or more steps = faster adaptation but risk of instability
- **Failure signatures**:
  - If PSNR/SSIM drops on OOD data: adaptation may be insufficient or over-regularized
  - If reconstruction is noisy or unstable: LoRA learning rate may be too high or too few CG steps
  - If hallucinations persist: data consistency projection may be too weak or poorly tuned
- **First 3 experiments**:
  1. Run SCD on a simple OOD CT reconstruction task (e.g., AAPM→Ellipses) and compare PSNR to DDS baseline
  2. Sweep LoRA rank r and optimization steps K to find stable hyperparameters
  3. Test data consistency strength by varying CG steps p and observe impact on hallucination reduction

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the traditional sense, but several areas remain unexplored based on the methodology and results presented.

## Limitations
- Lacks systematic analysis of LoRA rank and optimization hyperparameter sensitivity
- Theoretical guarantees for Tweedie approximation are not rigorously established
- Computational overhead and scalability for large datasets remain unclear

## Confidence
- High confidence in the novelty and general framework of SCD for OOD adaptation
- Medium confidence in the empirical results given the lack of hyperparameter sensitivity analysis
- Low confidence in the theoretical guarantees of the Tweedie approximation and LoRA adaptation capacity for large domain shifts

## Next Checks
1. Conduct a systematic ablation study varying LoRA rank r, optimization steps K, and CG steps p to quantify their impact on reconstruction quality and computational cost
2. Test SCD on a deliberately challenging OOD scenario (e.g., domain shift with vastly different noise statistics or forward operators) to assess the limits of LoRA adaptation capacity
3. Perform error analysis on the Tweedie approximation (equation 8) by comparing against ground-truth conditional posteriors or measuring reconstruction bias in high-noise or ill-conditioned settings