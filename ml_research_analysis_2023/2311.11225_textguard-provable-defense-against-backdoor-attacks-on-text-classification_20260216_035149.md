---
ver: rpa2
title: 'TextGuard: Provable Defense against Backdoor Attacks on Text Classification'
arxiv_id: '2311.11225'
source_url: https://arxiv.org/abs/2311.11225
tags:
- trigger
- text
- backdoor
- training
- textguard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TextGuard, the first provable defense against
  backdoor attacks on text classification. The key idea is to partition words from
  the training inputs into disjoint groups using a hash function, ensuring most groups
  remain clean from backdoor triggers.
---

# TextGuard: Provable Defense against Backdoor Attacks on Text Classification

## Quick Facts
- **arXiv ID:** 2311.11225
- **Source URL:** https://arxiv.org/abs/2311.11225
- **Reference count:** 40
- **Primary result:** First provable defense against backdoor attacks on text classification with certified accuracy guarantees

## Executive Summary
TextGuard introduces a novel provable defense against backdoor attacks on text classification by partitioning words into disjoint groups using a hash function and training base classifiers on sub-training sets. The method guarantees that predictions remain unaffected by backdoor triggers when their length is below a threshold. Evaluations on three benchmark datasets show TextGuard significantly outperforms existing certified defenses while demonstrating superior empirical performance against various backdoor attacks compared to state-of-the-art empirical defenses.

## Method Summary
TextGuard defends against backdoor attacks by partitioning words from training inputs into disjoint groups using a hash function, ensuring most groups remain clean from backdoor triggers. Base classifiers are trained on these sub-training sets, and their ensemble provides the final prediction. The method theoretically proves that when the length of the backdoor trigger falls within a certain threshold, predictions remain unaffected by the presence of triggers in training and testing inputs.

## Key Results
- TextGuard achieves significantly higher certified accuracy compared to existing certified defenses on SST-2, HSOL, and AG's News datasets
- The method demonstrates superior empirical performance against various backdoor attacks compared to state-of-the-art empirical defenses
- TextGuard provides meaningful certification guarantees even under adaptive attacks with trigger sizes up to 3 words

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Partitioning words into disjoint groups using a hash function ensures most groups remain clean from backdoor triggers
- **Mechanism:** The hash function assigns each word a group index. Since the same word always maps to the same group, a trigger word can only affect one group regardless of where it appears in the text. By creating enough groups, the majority will remain uncorrupted when trigger size is bounded.
- **Core assumption:** Hash function distributes words uniformly across groups
- **Evidence anchors:**
  - [abstract]: "The key idea is to partition words from the training inputs into disjoint groups using a hash function, ensuring most groups remain clean from backdoor triggers."
  - [section]: "We use a hash function H (e.g., MD5 [41]) to divide a text x into m groups... This guarantees that the trigger words, no matter where they are in the original input, will always be assigned to the same group."
  - [corpus]: Weak evidence - no direct mentions of hash function distribution properties in related papers
- **Break condition:** If trigger size exceeds ⌊(m-1)/2⌋, then more than half of groups could be corrupted, breaking the majority vote guarantee

### Mechanism 2
- **Claim:** Sorting words within each group by pre-defined word ID makes the defense robust to structure-level attacks
- **Mechanism:** By sorting words in each group according to a pre-defined order (e.g., BERT word IDs), the sequence becomes independent of the original input order. This prevents structure-level attacks from manipulating word order to inject backdoors.
- **Core assumption:** The pre-defined word ID ordering is fixed and known to both defender and attacker
- **Evidence anchors:**
  - [abstract]: "We sort the words within each group according to a pre-defined word ID... This sorting process ensures that the sequence of words within each group becomes independent of their original order in the input, which can be spitefully manipulated by the structure-level attacks to inject backdoors."
  - [section]: "We sort each word in gj(x) based on a pre-defined order, e.g., the word ID used by BERT [13]. As a result, the orders of words in each group are independent of their orders in x."
  - [corpus]: Weak evidence - no direct mentions of using word IDs for structure-level attack defense in related papers
- **Break condition:** If attacker discovers the sorting mechanism and designs triggers that work regardless of word order

### Mechanism 3
- **Claim:** Ensemble voting over base classifiers trained on clean groups provides provable robustness
- **Mechanism:** When most groups are clean, most base classifiers learn correct patterns. The majority vote aggregates these correct predictions, making the final prediction provably unaffected by backdoor triggers when trigger size is bounded.
- **Core assumption:** Base classifiers trained on clean data learn the true decision boundary
- **Evidence anchors:**
  - [abstract]: "We theoretically prove that when the length of the backdoor trigger falls within a certain threshold, TextGuard guarantees that its prediction will remain unaffected by the presence of the triggers in training and testing inputs."
  - [section]: "By ensuring the majority of base models are trained from clean data and are unaffected by the backdoor, we can guarantee the final prediction is provably unaffected by the backdoor."
  - [corpus]: Weak evidence - related work mentions ensemble methods but not with provable guarantees against backdoor attacks
- **Break condition:** If base classifiers are too weak individually, even clean groups may not provide enough signal for correct classification

## Foundational Learning

- **Concept:** Hash functions and their distribution properties
  - **Why needed here:** The uniform distribution of words across groups is critical for the security guarantee
  - **Quick check question:** If you have 1000 words and 10 groups, approximately how many words should each group contain for optimal security?

- **Concept:** Ensemble learning and majority voting
  - **Why needed here:** The final prediction relies on majority voting across base classifiers to achieve robustness
  - **Quick check question:** If 7 base classifiers vote and 4 predict class A while 3 predict class B, what is the final prediction?

- **Concept:** Provable robustness in adversarial machine learning
  - **Why needed here:** The method provides formal guarantees about prediction invariance under bounded trigger sizes
  - **Quick check question:** What mathematical condition must hold for a defense to provide a "certified accuracy" guarantee?

## Architecture Onboarding

- **Component map:** Hash function module -> Group sorting module -> Dataset partitioning module -> Base classifier training module -> Ensemble voting module -> Certification module

- **Critical path:**
  1. Hash words → Assign to groups → Sort within groups
  2. Create sub-datasets → Train base classifiers
  3. For inference: Hash and sort test text → Get predictions from each base classifier → Majority vote
  4. Certification: Compute which trigger sizes maintain correctness guarantee

- **Design tradeoffs:**
  - More groups → Better provable guarantees but weaker individual classifiers
  - Larger group size → Stronger base classifiers but less provable protection
  - Hash function choice → Affects distribution uniformity
  - Word sorting → Adds robustness but loses some semantic context

- **Failure signatures:**
  - If majority of base classifiers are corrupted → Ensemble fails
  - If trigger size exceeds certified bound → Provable guarantee breaks
  - If hash function is biased → Some groups become disproportionately clean/corrupted
  - If base classifiers are too weak → Even clean groups may not learn correctly

- **First 3 experiments:**
  1. Test hash function distribution: Hash 10,000 random words and verify uniform distribution across 10 groups
  2. Verify group independence: Create synthetic texts with known trigger words and confirm they only affect one group
  3. Baseline ensemble test: Train ensemble on clean data and measure performance degradation as more base classifiers are corrupted

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does TextGuard's certified accuracy change under adaptive attacks where the attacker knows the hash function and deliberately assigns trigger words to different groups?
- **Basis in paper:** [explicit] The paper evaluates an adaptive attack where the attacker assigns each trigger word to a unique group instead of the same group.
- **Why unresolved:** The paper only evaluates this for trigger size |e| = 3. The performance degradation and threshold for meaningful certification under larger trigger sizes remains unclear.
- **What evidence would resolve it:** Empirical results showing certified accuracy degradation across various trigger sizes (e.g., |e| = 1 to 10) under the adaptive attack would clarify TextGuard's robustness limits.

### Open Question 2
- **Question:** What is the impact of hash function choice on TextGuard's empirical performance across different datasets and attack types?
- **Basis in paper:** [explicit] The paper compares MD5, SHA1, and SHA256 hash functions in ablation studies and finds minor variations.
- **Why unresolved:** The ablation studies only compare three hash functions on a single dataset (HSOL). Whether certain hash functions perform better on specific datasets or attack types remains unknown.
- **What evidence would resolve it:** Comprehensive experiments comparing TextGuard's performance across multiple hash functions (including cryptographic and non-cryptographic options) on all three datasets against all attack types would reveal optimal hash function choices.

### Open Question 3
- **Question:** How does TextGuard perform when defending against backdoor attacks on tasks beyond text classification, such as question answering or named entity recognition?
- **Basis in paper:** [inferred] The discussion section acknowledges that TextGuard may be less effective in tasks where context between words is essential.
- **Why unresolved:** The paper only evaluates TextGuard on text classification tasks. Its performance on sequence-based tasks requiring inter-word context is unknown.
- **What evidence would resolve it:** Experiments applying TextGuard to sequence-based NLP tasks like question answering or named entity recognition, measuring both certification guarantees and empirical accuracy, would determine its applicability to broader NLP domains.

## Limitations

- The certified accuracy bounds depend heavily on hash function distribution uniformity, which has not been thoroughly validated for text word distribution
- The method may be less effective in tasks where context between words is essential, limiting applicability to broader NLP domains
- The claim of "significantly outperforming" existing defenses requires careful examination of baseline implementations and optimization levels

## Confidence

- **High confidence:** The core mechanism of word partitioning and ensemble voting is well-established and the mathematical framework for provable guarantees is sound
- **Medium confidence:** The empirical evaluation results are promising but depend on specific implementation details of baselines and attack scenarios
- **Medium confidence:** The claim about robustness to structure-level attacks assumes the attacker cannot discover the sorting mechanism, which may be optimistic in practice

## Next Checks

1. **Hash distribution validation:** Implement a statistical test to verify that the MD5 hash function produces uniform word distributions across groups for realistic text corpora, measuring deviation from expected group sizes

2. **Certified accuracy sensitivity:** Systematically vary the number of groups (m) and measure the resulting certified accuracy to understand the tradeoff between provable guarantees and practical performance

3. **Adaptive attack testing:** Design and evaluate an adaptive attacker that attempts to discover the sorting mechanism and construct triggers that work regardless of word order within groups