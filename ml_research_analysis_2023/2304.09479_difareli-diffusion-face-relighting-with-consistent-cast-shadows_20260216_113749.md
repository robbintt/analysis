---
ver: rpa2
title: 'DiFaReli++: Diffusion Face Relighting with Consistent Cast Shadows'
arxiv_id: '2304.09479'
source_url: https://arxiv.org/abs/2304.09479
tags:
- image
- face
- ddim
- input
- lighting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel diffusion-based approach to single-view
  face relighting in the wild, addressing challenges such as global illumination and
  cast shadows. The method bypasses the need for accurate intrinsic estimation and
  can be trained solely on 2D images without any light stage data, relit pairs, multi-view
  images, or lighting ground truth.
---

# DiFaReli++: Diffusion Face Relighting with Consistent Cast Shadows

## Quick Facts
- arXiv ID: 2304.09479
- Source URL: https://arxiv.org/abs/2304.09479
- Authors: 
- Reference count: 40
- Key outcome: Single-view face relighting in the wild using diffusion models, achieving state-of-the-art results on Multi-PIE and highest user study rankings

## Executive Summary
This paper presents a novel diffusion-based approach to single-view face relighting that addresses challenges like global illumination and cast shadows without requiring accurate intrinsic estimation or specialized training data. The method leverages a conditional DDIM to decode a disentangled light encoding along with 3D shape and facial identity features inferred from off-the-shelf estimators. By using rendered shading references and a modulator network for spatial conditioning, the approach achieves superior relighting quality compared to existing methods while requiring only one network pass for relighting.

## Method Summary
The method encodes input images into a feature vector containing spherical harmonic lighting coefficients, 3D shape parameters, camera parameters, face embedding, cast shadow scalar, and background segmentation. A conditional DDIM decoder with modified UNet architecture and a modulator network processes these features to generate relit images. The modulator spatially conditions the DDIM using rendered shading references, while adaptive group normalization handles non-spatial features. The approach enables controllable shadow manipulation through a learned shadow scalar and achieves high-quality relighting without requiring multi-view images, light stage data, or lighting ground truth.

## Key Results
- Achieves state-of-the-art performance on Multi-PIE benchmark across all metrics
- Ranks highest in user studies for relighting quality
- Enables controllable cast shadow manipulation without explicit shadow map estimation
- Outperforms teacher model with single-shot relighting framework

## Why This Works (Mechanism)

### Mechanism 1
The DDIM-based decoder can disentangle lighting from identity and geometry when conditioned on explicit SH lighting and shape parameters. The conditional DDIM maps a noise tensor xT to an image x0, learning to isolate the lighting effect while preserving identity and geometry in the latent encoding through joint training of image encoder and DDIM.

### Mechanism 2
Spatial conditioning via rendered shading reference + modulator network improves relighting quality over global conditioning. The shading reference R, rendered from estimated SH and shape, is concatenated with background and passed through a modulator network to produce per-layer modulation weights that spatially align with the output, enabling the DDIM to condition on local shading cues.

### Mechanism 3
The cast shadow scalar c allows controllable manipulation of shadow strength without explicit shadow map estimation. A learned scalar c indicates the degree of visible cast shadows, and modifying c during decoding enables realistic shadow strengthening or removal as the diffusion model learns to map c to appropriate shadow intensities implicitly.

## Foundational Learning

- Concept: Spherical harmonic lighting representation
  - Why needed here: SH coefficients provide explicit, interpretable encoding of lighting direction and intensity for user manipulation and DDIM conditioning
  - Quick check question: What are the limitations of using 2nd-order SH lighting for representing complex real-world illumination?

- Concept: Diffusion probabilistic models and DDIM inference
  - Why needed here: DDIM's deterministic reverse process allows encoding an image into a noise tensor xT that captures information not represented in conditioning features, enabling high-fidelity reconstruction and manipulation
  - Quick check question: How does DDIM's non-Markovian inference process differ from standard diffusion models, and why is this important for our use case?

- Concept: Adaptive group normalization (AdaGN) for conditional generation
  - Why needed here: AdaGN allows the DDIM to condition on non-spatial features (face embedding, shape, shadow scalar) by modulating feature statistics across spatial dimensions
  - Quick check question: How does AdaGN differ from standard conditional normalization techniques like SPADE or AdaIN in terms of conditioning flexibility?

## Architecture Onboarding

- Component map: Input image → DECA estimator → SH lighting (l), shape (s), camera (cam) → Input image → ArcFace → face embedding (ξ) → Input image → shadow estimator → cast shadow scalar (c) → Input image → face segmentation → background (bg) → l, s, cam, bg → shading reference renderer → shading reference R → R, bg → modulator network → spatial modulation weights → s, cam, ξ, c → AdaGN MLPs → non-spatial modulation parameters → xT + spatial + non-spatial modulation → DDIM decoder → relit image
- Critical path: Input → encoders → feature vector → DDIM reversal (x0 → xT) → feature modification → DDIM decoding (xT → relit output)
- Design tradeoffs: Using SH lighting limits expressiveness but provides user control and explicit conditioning; rendering shading reference requires geometry estimation but enables spatial conditioning; diffusion models are slower than GANs but provide better inversion and control
- Failure signatures: Identity changes (issues with face embedding conditioning or DDIM capacity); shading artifacts (issues with spatial conditioning or geometry estimation); shadow inconsistencies (issues with shadow scalar estimation or shadow modeling)
- First 3 experiments: 1) Test DDIM inversion fidelity: input → xT → x0' comparison (should be near-perfect); 2) Test conditioning effectiveness: modify SH lighting only, verify shading changes without identity/geometry changes; 3) Test spatial vs non-spatial conditioning: compare full pipeline against variants with only spatial or only non-spatial conditioning on Multi-PIE dataset

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the limitations and evaluation scope, several important questions remain unanswered.

## Limitations
- Performance on images with multiple faces or non-frontal face poses remains untested
- Method's behavior under extreme lighting conditions (HDR or very low light) is not evaluated
- Impact of different off-the-shelf estimator choices on final relighting quality is unknown

## Confidence
- High confidence in core diffusion-based relighting mechanism
- Medium confidence in spatial conditioning approach effectiveness
- Medium confidence in scalar-based shadow control capability

## Next Checks
1. **Multi-dataset evaluation**: Test the method on diverse datasets beyond Multi-PIE, including images with complex lighting and shadows from FFHQ or CelebA-HQ, to assess generalization performance
2. **Ablation study on spatial conditioning**: Compare the full pipeline against variants with only spatial or only non-spatial conditioning on a wider range of lighting conditions to quantify the contribution of each conditioning type
3. **User study on identity preservation**: Conduct a comprehensive user study evaluating the method's ability to preserve identity and facial details across different lighting scenarios, especially when modifying shadow strength