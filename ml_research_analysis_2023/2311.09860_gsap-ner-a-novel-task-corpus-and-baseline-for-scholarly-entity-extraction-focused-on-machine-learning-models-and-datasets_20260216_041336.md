---
ver: rpa2
title: 'GSAP-NER: A Novel Task, Corpus, and Baseline for Scholarly Entity Extraction
  Focused on Machine Learning Models and Datasets'
arxiv_id: '2311.09860'
source_url: https://arxiv.org/abs/2311.09860
tags:
- entity
- dataset
- datasets
- entities
- annotation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents GSAP-NER, a novel dataset and baseline model
  for Named Entity Recognition (NER) focused on machine learning models and datasets
  in scientific publications. The dataset contains 100 manually annotated full-text
  publications with over 54,000 entity mentions across 10 entity types, including
  ML models, model architectures, datasets, and related concepts.
---

# GSAP-NER: A Novel Task, Corpus, and Baseline for Scholarly Entity Extraction Focused on Machine Learning Models and Datasets

## Quick Facts
- **arXiv ID:** 2311.09860
- **Source URL:** https://arxiv.org/abs/2311.09860
- **Reference count:** 18
- **Primary result:** GSAP-NER dataset contains 100 manually annotated full-text publications with 54,378 entity mentions across 10 entity types

## Executive Summary
This paper presents GSAP-NER, a novel dataset and baseline model for Named Entity Recognition (NER) focused on machine learning models and datasets in scientific publications. The dataset contains 100 manually annotated full-text publications with over 54,000 entity mentions across 10 entity types, including ML models, model architectures, datasets, and related concepts. The authors fine-tuned three state-of-the-art baseline models (SciBERT, RoBERTa, and SciDeBERTa-CS) on the dataset, achieving the best performance with SciDeBERTa-CS (F1 scores of 0.71 for ML models and 0.81 for datasets). The study also explores the minimum number of annotated publications needed to achieve satisfactory performance, finding that 40 publications provide a good balance. GSAP-NER addresses the lack of fine-grained entity types in existing NER datasets and offers a valuable resource for information extraction and knowledge graph creation in the machine learning domain.

## Method Summary
The authors developed a novel NER dataset focused on machine learning models and datasets by manually annotating 100 full-text computer science publications. The annotation process used INCEpTION tool with three computer science annotators, covering 10 entity types including MLModel, ModelArchitecture, Dataset, and Method. The dataset includes nested entity annotations to capture hierarchical relationships between informal mentions and specific entities. Three state-of-the-art PLMs (SciBERT, RoBERTa, and SciDeBERTa-CS) were fine-tuned using 10-fold cross-validation to establish baseline performance metrics.

## Key Results
- GSAP-NER dataset contains 54,378 entity mentions across 100 full-text publications
- SciDeBERTa-CS achieves best overall performance with F1 scores of 0.71 for ML models and 0.81 for datasets
- 40 annotated publications provide optimal balance between annotation effort and model performance
- Nested entity annotations successfully capture hierarchical relationships in scientific text

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-grained entity differentiation (MLModel vs ModelArchitecture) enables more accurate scientific concept extraction than treating all ML-related terms as generic "Method" entities.
- **Mechanism:** The dataset explicitly separates machine learning models (executable resources) from their underlying architectures (conceptual structures), allowing models to learn distinct representations for each type.
- **Core assumption:** Scientific literature uses ML model and architecture mentions with sufficiently different contexts and syntactic patterns to be learnable by current PLMs.
- **Evidence anchors:**
  - [abstract]: "existing ground truth datasets do not treat fine-grained types like ML model and model architecture as separate entity types"
  - [section 3.1.1]: "MLModel refers to a string span that represents a named entity of a machine learning model... ModelArchitecture refers to a named entity corresponding to the conceptual or structural information of a machine learning model"
  - [corpus]: Weak - the corpus statistics show 5,012 MLModel mentions vs 2,612 ModelArchitecture mentions, suggesting sufficient volume but no explicit contextual evidence provided
- **Break condition:** If scientific writing patterns become too ambiguous between model references and architecture discussions, or if PLMs cannot distinguish contextual cues between these types.

### Mechanism 2
- **Claim:** Nested entity annotations improve model performance by capturing hierarchical relationships between informal mentions and their referenced entities.
- **Mechanism:** By annotating both "ResNets" (informal mention) and the nested "ResNet-50" (specific model), models learn to recognize when generic terms implicitly reference specific entities.
- **Core assumption:** Informal mentions in scientific text consistently contain nested references to named entities that follow predictable patterns.
- **Evidence anchors:**
  - [section 3]: "our dataset also contains annotations for informal mentions like 'our BERT-based model' or 'an image CNN'"
  - [section 3.1]: "nested entity annotations (Finkel and Manning, 2009; Katiyar and Cardie, 2018), enabling the annotation of multiple sub-parts of a text span within a single noun phrase"
  - [corpus]: Moderate - Table 2 shows examples of nested annotations, though overall corpus statistics don't quantify how many mentions are nested
- **Break condition:** If informal mentions become too contextually dependent or if nested patterns become too diverse for PLMs to learn effectively.

### Mechanism 3
- **Claim:** Full-text annotation provides more comprehensive contextual information than abstract-only datasets, leading to better model generalization.
- **Mechanism:** By training on complete publications rather than just abstracts, models learn entity usage patterns across different sections (methods, results, discussions) and can handle varied contexts.
- **Core assumption:** Entity mentions and their contexts vary systematically across different sections of scientific papers in ways that improve model learning.
- **Evidence anchors:**
  - [abstract]: "Our dataset offers two significant advantages... our dataset features nested entity annotations... enabling the annotation of multiple sub-parts of a text span within a single noun phrase"
  - [section 3.3]: "We have three annotators with computer science background to conduct the annotation using INCEpTION" (implying full-text scope)
  - [corpus]: Strong - Table 1 shows GSAP-NER has 100 full-text publications vs 500 abstracts for SciERC, and section 6.2 explores minimum training set size effects
- **Break condition:** If full-text noise (footnotes, figures, tables) overwhelms the signal, or if section-specific patterns are too inconsistent.

## Foundational Learning

- **Concept:** Named Entity Recognition fundamentals (BIO tagging, span identification)
  - Why needed here: The paper uses BIO tag scheme for token labeling and evaluates entity-level F1 scores
  - Quick check question: What does "B-METHOD" indicate in BIO tagging, and how does it differ from "I-METHOD"?

- **Concept:** Transformer-based language models and fine-tuning
  - Why needed here: The baseline models (SciBERT, RoBERTa, and SciDeBERTa-CS) are all transformer-based PLMs fine-tuned for NER
  - Quick check question: How does the masked language modeling objective in pre-training help PLMs learn better representations for downstream NER tasks?

- **Concept:** Inter-rater agreement metrics (F1 score, partial vs exact match)
  - Why needed here: The paper reports inter-annotator agreement using F1 scores and explores both exact-match and partial-match evaluation
  - Quick check question: Why might partial-match F1 scores be higher than exact-match scores for conceptual entity types like "Method"?

## Architecture Onboarding

- **Component map:** PDF-to-text conversion → paragraph extraction → corrupt paragraph filtering → BIO tagging conversion → PLM encoder → token classification layer → entity span reconstruction → entity-level F1 calculation
- **Critical path:** PDF conversion → annotation → model training → evaluation → analysis
- **Design tradeoffs:** Full-text vs abstract-only annotation (comprehensive context vs annotation cost), nested vs flat annotation (rich information vs model complexity), exact vs partial match evaluation (precision vs practical utility)
- **Failure signatures:** Low inter-rater agreement on specific entity types, performance gap between exact and partial match scores, degradation when reducing training set size below 40 publications
- **First 3 experiments:**
  1. Replicate the 10-fold cross-validation with SciBERT to establish baseline performance
  2. Test the effect of corrupt paragraph filtering by training on data with and without filtered paragraphs
  3. Evaluate the impact of nested annotation simplification by comparing model performance on nested vs flattened annotations

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the minimum number of annotated publications needed to achieve satisfactory performance in fine-grained scholarly NER tasks?
- **Basis in paper:** [explicit] The authors explore the minimum number of annotated publications needed to achieve satisfactory performance in their fine-grained scholarly NER task, finding that 40 publications provide a good balance.
- **Why unresolved:** While the paper identifies 40 publications as a good balance, it does not explore whether this number is optimal or if further performance improvements can be achieved with additional annotations.
- **What evidence would resolve it:** Conducting experiments with varying numbers of annotated publications beyond 40 and analyzing the corresponding performance metrics would provide insights into the optimal number of annotations needed for satisfactory performance.

### Open Question 2
- **Question:** How can the performance of the baseline models be improved for entity types with lower F1 scores?
- **Basis in paper:** [inferred] The paper mentions that the performance varies across entity types, with some entity types having lower F1 scores than others. However, it does not provide specific strategies for improving the performance of these entity types.
- **Why unresolved:** The paper does not explore potential methods or techniques to enhance the performance of the baseline models for entity types with lower F1 scores.
- **What evidence would resolve it:** Conducting experiments with different model architectures, hyperparameter tuning, or incorporating additional contextual information for entity types with lower F1 scores would provide insights into improving their performance.

### Open Question 3
- **Question:** How can the GSAP-NER dataset be extended to cover additional domains or languages?
- **Basis in paper:** [inferred] The paper focuses on the development of the GSAP-NER dataset for machine learning models and datasets in scientific publications. However, it does not discuss the potential for extending the dataset to cover other domains or languages.
- **Why unresolved:** The paper does not explore the feasibility or challenges of extending the dataset to include entities from different domains or languages.
- **What evidence would resolve it:** Conducting experiments with annotation guidelines and models for different domains or languages, and evaluating their performance on the extended dataset, would provide insights into the potential for extending the GSAP-NER dataset.

## Limitations
- Entity Type Coverage Gaps: Several important scholarly entities remain unannotated, limiting dataset applicability
- Annotation Ambiguity: Significant challenges with conceptual entity types like Method and ModelArchitecture
- PDF Conversion Artifacts: Text fragmentation from Grobid parsing affects entity boundary detection
- Dataset Size Constraints: 100 publications may be insufficient for capturing full diversity of scientific writing

## Confidence
**High Confidence:**
- Dataset contains 100 manually annotated full-text publications with 54,378 entity mentions
- SciDeBERTa-CS achieves best performance (F1: 0.71 for ML models, 0.81 for datasets)
- 40 publications provide good balance between effort and performance
- Nested entity annotations successfully implemented

**Medium Confidence:**
- Fine-grained entity differentiation provides meaningful improvements
- Full-text annotation offers better contextual information than abstracts
- Informal mention annotations improve recognition of nested references

**Low Confidence:**
- Precise minimum of 40 publications for satisfactory performance
- Proposed entity types will generalize well to non-ML domains
- PDF conversion artifacts have minimal impact on overall performance

## Next Checks
1. **Annotation Quality Validation:** Conduct blind re-annotation study on 10 randomly selected papers to measure inter-rater agreement on most ambiguous entity types
2. **Cross-Domain Generalization:** Test trained models on held-out dataset from different ML subfield to assess performance degradation
3. **PDF Conversion Impact Analysis:** Compare model performance when trained on manually corrected, automatically parsed, and mixed paragraph datasets to quantify conversion artifact effects