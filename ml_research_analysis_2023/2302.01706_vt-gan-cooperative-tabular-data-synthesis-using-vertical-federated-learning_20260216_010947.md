---
ver: rpa2
title: 'VT-GAN: Cooperative Tabular Data Synthesis using Vertical Federated Learning'
arxiv_id: '2302.01706'
source_url: https://arxiv.org/abs/2302.01706
tags:
- data
- clients
- training
- server
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VT-GAN, a vertical federated learning framework
  for training generative adversarial networks (GANs) to synthesize tabular data while
  preserving privacy. The framework addresses challenges in distributed GAN training,
  including capturing column dependencies across clients and incorporating conditional
  vectors without privacy leakage.
---

# VT-GAN: Cooperative Tabular Data Synthesis using Vertical Federated Learning

## Quick Facts
- **arXiv ID:** 2302.01706
- **Source URL:** https://arxiv.org/abs/2302.01706
- **Reference count:** 40
- **Primary result:** VT-GAN generates high-fidelity synthetic tabular data with ML utility differences as low as 2.7% compared to centralized GANs

## Executive Summary
VT-GAN introduces a vertical federated learning framework for training generative adversarial networks to synthesize tabular data while preserving privacy across multiple clients. The framework partitions both the generator and discriminator between server and clients, ensuring that no single party can reconstruct the full model or training data. Through a novel training-with-shuffling mechanism and secure synthetic data publication strategy, VT-GAN prevents membership inference attacks while maintaining high synthetic data quality. Extensive experiments on five datasets demonstrate that VT-GAN achieves comparable performance to centralized GANs with minimal ML utility degradation.

## Method Summary
VT-GAN is a vertical federated learning framework that trains GANs to synthesize tabular data while preserving privacy. The framework partitions the generator and discriminator between server and clients, with the generator split into G_t (server-side) and G_b (client-side), and the discriminator split into D_t (server-side), D_s (server-side), and D_b (client-side). A training-with-shuffling mechanism ensures that the server cannot reconstruct training data using conditional vectors. The framework is evaluated on five tabular datasets with various neural network partition configurations and tested under different data distribution scenarios.

## Key Results
- VT-GAN generates high-fidelity synthetic data with ML utility differences as low as 2.7% compared to centralized GANs
- Performance remains robust under imbalanced data distributions and varying numbers of clients (2-5)
- The framework achieves comparable statistical similarity metrics (JSD, WD, correlation difference) to centralized approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VT-GAN preserves privacy by partitioning both generator and discriminator between server and clients
- Mechanism: The generator is split into G_t (top, server-side) and G_b (bottom, client-side), while the discriminator is split into D_t (top, server-side), D_s (conditional vector filter, server-side), and D_b (bottom, client-side)
- Core assumption: The server is trusted but curious, and clients do not collude with the server
- Evidence anchors: [abstract] "The framework achieves this through a novel training-with-shuffling mechanism and a secure synthetic data publication strategy"
- Break condition: If the server and clients collude, the privacy guarantees of VT-GAN break down

### Mechanism 2
- Claim: VT-GAN prevents membership inference attacks by ensuring the server never has access to both conditional vector and data indices
- Mechanism: VT-GAN implements training-with-shuffling where all clients shuffle local training data using the same random seed at the end of each training round
- Core assumption: The shuffle function and random seed are securely negotiated among all clients before training and isolated from the server
- Evidence anchors: [abstract] "To accommodate conditional vector into training without privacy leakage, GTV designs a mechanism training-with-shuffling"
- Break condition: If the server gains access to the shuffle function or random seed, privacy guarantees break down

### Mechanism 3
- Claim: VT-GAN maintains high synthetic data quality by ensuring the discriminator's top model is sufficiently large
- Mechanism: The discriminator's top model (D_t) is designed to be large enough to handle concatenated intermediate logits from all clients, allowing it to learn column dependencies across distributed data
- Core assumption: The size of D_t is proportional to the number of clients and data complexity
- Evidence anchors: [abstract] "Extensive experiments on five datasets demonstrate that VT-GAN generates high-fidelity synthetic data with comparable quality to centralized GANs"
- Break condition: If the top model of the discriminator is too small relative to data complexity, synthetic data quality degrades significantly

## Foundational Learning

- Concept: Vertical Federated Learning (VFL)
  - Why needed here: VT-GAN is built on VFL principles to enable distributed GAN training without sharing raw data
  - Quick check question: What is the key difference between horizontal and vertical federated learning?

- Concept: Generative Adversarial Networks (GANs)
  - Why needed here: VT-GAN uses GANs as the underlying architecture for tabular data synthesis
  - Quick check question: What are the two main components of a GAN and their roles?

- Concept: Conditional GANs
  - Why needed here: VT-GAN uses conditional vectors to control generation of specific features in synthetic data
  - Quick check question: How does a conditional GAN differ from a standard GAN in terms of input?

## Architecture Onboarding

- Component map:
  Server-side: G_t (generator top), D_t (discriminator top), D_s (conditional vector filter)
  Client-side: G_b (generator bottom), D_b (discriminator bottom), local training data
  Shared: conditional vectors, random seed for shuffling

- Critical path:
  1. Server generates conditional vectors
  2. Clients process conditional vectors through local components
  3. Server combines intermediate results and updates discriminator
  4. Server generates synthetic data and updates generator
  5. Clients shuffle local data using shared random seed

- Design tradeoffs:
  - Privacy vs. performance: Partitioning models increases privacy but may impact training efficiency
  - Server trust: VT-GAN assumes a trusted server but curious clients, which may not hold in all scenarios
  - Data distribution: Imbalanced data distribution across clients can affect synthetic data quality

- Failure signatures:
  - Privacy breach: If server and clients collude, or if shuffle mechanism is compromised
  - Poor synthetic data quality: If discriminator's top model is too small or data distribution is highly imbalanced
  - Training instability: If generator and discriminator are not properly balanced in their respective partitions

- First 3 experiments:
  1. Test basic functionality of VT-GAN with two clients and simple dataset
  2. Evaluate impact of different neural network partitions on synthetic data quality
  3. Test training-with-shuffling mechanism by attempting to reconstruct data indices with and without shuffling

## Open Questions the Paper Calls Out

- Question: How can VT-GAN be extended to handle cases where clients have overlapping features?
  - Basis in paper: [explicit] The paper mentions that in real-world scenarios, clients may possess overlapping features, and assumes that clients resolve any overlap by excluding redundant features
  - Why unresolved: The paper does not explore how to handle overlapping features in VT-GAN, which is a common scenario in practice
  - What evidence would resolve it: Experimental results demonstrating effectiveness of VT-GAN when clients have overlapping features

- Question: What is the optimal size of the neural network in VT-GAN under different dataset and different number of clients?
  - Basis in paper: [inferred] The paper mentions that output dimension of the enlarged setting is determined through trial and error
  - Why unresolved: The paper does not provide clear guideline for choosing optimal size of neural network in VT-GAN
  - What evidence would resolve it: A study exploring relationship between neural network size, dataset characteristics, and number of clients

- Question: How can VT-GAN be made more robust against collusion between the server and clients?
  - Basis in paper: [explicit] The paper mentions that the system becomes vulnerable when multiple parties collude
  - Why unresolved: The paper does not provide specific measures to counteract collusion between server and clients
  - What evidence would resolve it: Experimental results demonstrating effectiveness of countermeasures against collusion attacks

## Limitations
- Lacks detailed implementation specifications for neural network architectures and training-with-shuffling mechanism
- Privacy guarantees rely on assumptions of trusted server and non-colluding clients that may not hold in real-world scenarios
- Limited experimental validation with only up to 5 clients raises questions about scalability

## Confidence

- **High confidence** in basic VT-GAN framework design and ability to generate high-fidelity synthetic data with proper model partitioning
- **Medium confidence** in privacy guarantees, as mechanisms are described but lack detailed implementation specifics
- **Low confidence** in scalability claims due to limited experimental validation

## Next Checks

1. Implement and test the training-with-shuffling mechanism to verify that the server cannot reconstruct training data when using synchronized shuffling across clients
2. Conduct experiments with more than 5 clients to validate the scalability claims of VT-GAN
3. Perform adversarial attacks (membership inference, model inversion) to empirically verify the privacy guarantees of the framework