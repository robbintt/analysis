---
ver: rpa2
title: Bridging RL Theory and Practice with the Effective Horizon
arxiv_id: '2304.09853'
source_url: https://arxiv.org/abs/2304.09853
tags:
- effective
- horizon
- sample
- bounds
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the effective horizon, a new complexity measure
  for Markov decision processes (MDPs) that more accurately predicts the sample complexity
  of deep reinforcement learning (RL) algorithms compared to prior bounds. The authors
  construct a new dataset, BRIDGE, of 155 deterministic MDPs from common deep RL benchmarks
  with tabular representations, enabling exact computation of instance-dependent bounds.
---

# Bridging RL Theory and Practice with the Effective Horizon

## Quick Facts
- **arXiv ID**: 2304.09853
- **Source URL**: https://arxiv.org/abs/2304.09853
- **Reference count**: 40
- **Primary result**: Effective horizon more accurately predicts RL sample complexity than prior bounds, typically differing by about an order of magnitude versus 3-4 orders of magnitude for other bounds.

## Executive Summary
This paper introduces the effective horizon, a new complexity measure for Markov decision processes that more accurately predicts the sample complexity of deep reinforcement learning algorithms compared to prior theoretical bounds. The authors construct the BRIDGE dataset of 155 deterministic MDPs with tabular representations, enabling exact computation of instance-dependent bounds. They prove sample complexity bounds based on the effective horizon that correlate closely with PPO and DQN performance across four metrics, and demonstrate that unlike prior bounds, the effective horizon can predict the effects of reward shaping and pre-trained policies.

## Method Summary
The method involves constructing a dataset of deterministic MDPs with tabular representations from common deep RL benchmarks, then computing theoretical sample complexity bounds based on the effective horizon measure. PPO and DQN are trained on each MDP for 5 million timesteps, with empirical sample complexity measured as the median timesteps to first reach the optimal policy over 5 seeds. The effective horizon is calculated through a binary search procedure that combines planning depth and random rollout precision, and is used to derive sample complexity bounds that are compared against empirical performance using correlation analysis, median ratio calculations, and ROC/AUC metrics.

## Key Results
- Effective horizon-based bounds correlate with PPO/DQN performance (Spearman ~0.5-0.6) across four metrics, significantly better than worst-case bounds (Spearman ~0.2-0.3)
- Effective horizon predictions differ from empirical performance by about an order of magnitude, versus 3-4 orders of magnitude for other bounds
- Effective horizon accurately captures the effects of reward shaping and pre-trained policies, unlike prior bounds
- The surprising property that actions with highest Q-values under random policy also have highest Q-values under optimal policy holds in many environments and correlates with RL success

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The effective horizon predicts RL performance better than prior bounds because it captures how far ahead an algorithm must plan before leaf nodes can be evaluated with random rollouts.
- **Mechanism**: GORP (Greedy Over Random Policy) constructs an optimal policy by exhaustively planning k steps ahead and then evaluating leaf nodes using m random rollouts. The effective horizon H = k + logAm combines both parameters into a single complexity measure. When H ≪ T, sample complexity scales as T²AH instead of TAT, reflecting practical RL efficiency.
- **Core assumption**: The MDP is k-QVI-solvable, meaning policies that act greedily with respect to the k-th Q-value iteration of the random policy are optimal.
- **Evidence anchors**:
  - [abstract]: "The effective horizon combines both k and m into a single measure" and "we prove sample complexity bounds based on the effective horizon that correlate closely with the real performance of PPO"
  - [section 5]: "GORP resembles a Monte Carlo planning algorithm: it mimics exhaustively planning ahead k steps and then perform m random rollouts from each leaf node"
  - [corpus]: Weak evidence - no direct corpus papers on effective horizon
- **Break condition**: If an MDP is not k-QVI-solvable for small k values, the effective horizon-based bounds become vacuous or loose, failing to predict RL performance accurately.

### Mechanism 2
- **Claim**: The effective horizon can predict the effects of reward shaping and pre-trained policies because it depends on both the reward function and the exploration policy.
- **Mechanism**: When reward shaping makes the MDP more "dense" (dense rewards vs sparse rewards), the gap between Q-values decreases, reducing the required precision for random rollouts and thus lowering the effective horizon. Similarly, a pre-trained policy closer to optimal reduces the exploration gap and improves the effective horizon bound.
- **Core assumption**: The effective horizon formula H = k + logAmk depends on the reward function through the gap term ∆k_t(s) and on the exploration policy through the occupancy measure µexpl_t(s,a).
- **Evidence anchors**:
  - [section 6.1]: "the effective horizon does well on both metrics, showing that it can accurately capture the effects of reward shaping on RL performance"
  - [section 6.1]: "the effective horizon is accurate at predicting the change in sample complexity due to using a pre-trained policy"
  - [corpus]: Weak evidence - no direct corpus papers on reward shaping effects on effective horizon
- **Break condition**: If reward shaping changes the optimal policy structure in ways not captured by gap reduction, or if the pre-trained policy's exploration distribution is poorly aligned with the MDP structure, predictions may fail.

### Mechanism 3
- **Claim**: The effective horizon provides tighter bounds than EPW (Effective Planning Window) because it incorporates reward information while EPW ignores rewards beyond the planning window.
- **Mechanism**: EPW-based bounds scale as T²AW and depend only on planning depth W, ignoring reward distribution. The effective horizon bounds scale as T²AH and incorporate both planning depth and the precision needed to distinguish optimal actions based on reward gaps, giving tighter bounds when rewards are informative.
- **Core assumption**: In MDPs with informative reward structure, the gap between Q-values of optimal and suboptimal actions is smaller than the worst-case, allowing for shorter effective horizons than planning windows.
- **Evidence anchors**:
  - [section 5.1]: "the EPW does manages to capture the same intuition as the effective horizon in the MDP in Figure 3b: in this case, W = 1. However, the analysis based on the EPW is unsatisfactory because it entirely ignores rewards beyond the planning window"
  - [section 6]: "The effective horizon-based bounds are nearly as good at predicting the sample complexity of PPO and DQN as one algorithm's sample complexity is at predicting the other's"
  - [corpus]: Weak evidence - no direct corpus papers comparing EPW vs effective horizon with reward incorporation
- **Break condition**: In MDPs where rewards are only available at the final timestep or where reward structure is uniform across action sequences, the effective horizon and EPW may give similar bounds, reducing the advantage of the effective horizon approach.

## Foundational Learning

- **Concept**: Markov Decision Processes (MDPs) and their tabular representation
  - **Why needed here**: The entire analysis depends on computing exact Q-values and state transitions, which requires knowing the full tabular representation of deterministic MDPs. Without this foundation, calculating effective horizon bounds would be impossible.
  - **Quick check question**: Given a deterministic MDP with states S, actions A, and transition function f, how would you compute the Q-value Qπ_t(s,a) for a given policy π?

- **Concept**: Value iteration and Q-value iteration
  - **Why needed here**: The effective horizon is defined in terms of k-QVI-solvability, which requires understanding how Q-value iteration transforms Q-functions. GORP's analysis also depends on estimating k-step Q-values.
  - **Quick check question**: If Q1 = Qπ_expl and Qi+1 = QVI(Qi), what does the Q-value iteration operation QVI(Q) compute for a given Q-function Q?

- **Concept**: Concentration inequalities and statistical estimation
  - **Why needed here**: Bounding the sample complexity of GORP requires showing that empirical estimates of Q-values converge to their true values with sufficient probability. This uses Bernstein's inequality and other concentration bounds.
  - **Quick check question**: Given m i.i.d. samples from a distribution with mean μ and variance σ², what is the probability that the sample mean deviates from μ by more than ε?

## Architecture Onboarding

- **Component map**: MDP enumeration -> Q-value computation -> Effective horizon bounds -> Deep RL training -> Performance comparison -> Theoretical validation
- **Critical path**: MDP enumeration → Q-value computation → Effective horizon bounds → Deep RL training → Performance comparison → Theoretical validation
- **Design tradeoffs**: 
  - Exact vs approximate MDP representations: Exact tabular representations enable precise theoretical analysis but are computationally expensive for large MDPs
  - k vs m in GORP: Larger k captures more complex MDP structure but increases computational cost exponentially
  - Exploration policy choice: Random policy gives theoretical guarantees but may be suboptimal in practice
- **Failure signatures**: 
  - Effective horizon bounds much larger than empirical sample complexity indicates k-QVI-solvability assumption may be too strong
  - Poor correlation between bounds and RL performance suggests MDP properties not captured by current measures
  - Computational intractability in MDP enumeration suggests need for approximation methods
- **First 3 experiments**:
  1. Verify k-QVI-solvability on a simple deterministic MDP with known optimal policy and random policy structure
  2. Compute effective horizon bounds for a goal-based MDP (like Minigrid) and compare to empirical GORP performance
  3. Test reward shaping on a sparse-reward MDP and measure changes in both effective horizon and RL algorithm performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the effective horizon framework be extended to stochastic environments and environments with continuous action spaces?
- **Basis in paper**: [inferred] The paper focuses on deterministic MDPs with discrete action spaces and acknowledges this as a limitation in the discussion section.
- **Why unresolved**: The paper does not provide theoretical extensions or empirical results for stochastic or continuous action space environments.
- **What evidence would resolve it**: Formal theoretical analysis showing how the effective horizon bounds would change in stochastic settings, and empirical results comparing effective horizon-based bounds to performance in stochastic continuous control benchmarks.

### Open Question 2
- **Question**: How can the effective horizon be computed or estimated without requiring full tabular representations of the MDP?
- **Basis in paper**: [explicit] The paper states "The effective horizon is not easy to calculate without access to the full tabular representation of an MDP" and notes this as a limitation.
- **Why unresolved**: The paper only provides methods to calculate the effective horizon when full tabular representations are available, which is often impractical.
- **What evidence would resolve it**: Development of practical approximation methods or proxy metrics that correlate with the effective horizon, validated on benchmark environments.

### Open Question 3
- **Question**: How can variants of the effective horizon be developed to capture the performance effects of generalization in deep RL?
- **Basis in paper**: [explicit] The paper identifies that the effective horizon cannot capture generalization effects, noting that PPO and DQN perform better than predicted in environments like PONG-30 due to their ability to generalize skills across multiple rounds.
- **Why unresolved**: The current effective horizon formulation considers learning separately at every timestep and cannot account for transfer of learned behaviors across similar states or situations.
- **What evidence would resolve it**: Theoretical analysis of how generalization affects sample complexity bounds, and empirical results showing improved prediction of deep RL performance when accounting for generalization.

## Limitations
- Assumes deterministic MDPs with known tabular representations, limiting applicability to real-world stochastic environments
- k-QVI-solvability assumption may be overly restrictive for complex MDP structures
- Computational intractability prevents application to large-scale MDPs with millions of states
- BRIDGE dataset represents curated subset of environments that may not generalize to all RL domains

## Confidence

- **High confidence**: The correlation between effective horizon bounds and empirical PPO/DQN performance (Spearman correlations around 0.5-0.6) is well-supported by experimental results across multiple metrics and algorithm comparisons. The mechanism by which effective horizon captures planning requirements is clearly demonstrated through the GORP algorithm analysis.

- **Medium confidence**: Claims about predicting reward shaping effects and pre-trained policy benefits rely on limited experiments (4-6 environments) and assume the effective horizon formula accurately captures these phenomena across diverse MDP structures. The extension to stochastic environments remains largely theoretical.

- **Low confidence**: The assertion that effective horizon differs from empirical performance by "about an order of magnitude" is based on median ratios that vary significantly across environments (0.6-21.6), suggesting high variability that isn't fully characterized.

## Next Checks

1. Test k-QVI-solvability on a broader class of deterministic MDPs with varying state-action transition structures to determine the robustness of the effective horizon framework.

2. Implement approximate effective horizon computation methods for large MDPs using state abstraction or sampling to assess scalability beyond tabular representations.

3. Extend effective horizon analysis to stochastic MDPs by incorporating uncertainty quantification and comparing predictions against empirical performance in partially observable environments.