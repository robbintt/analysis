---
ver: rpa2
title: 'Paloma: A Benchmark for Evaluating Language Model Fit'
arxiv_id: '2312.10523'
source_url: https://arxiv.org/abs/2312.10523
tags:
- domains
- data
- perplexity
- language
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PALOMA is a benchmark that measures language model fit across 585
  English and code domains, rather than assuming performance on one distribution extrapolates
  to others. It includes new datasets of the top 100 subreddits and programming languages,
  and provides 6 baseline 1B models pretrained on popular corpora with controls for
  fair comparison.
---

# Paloma: A Benchmark for Evaluating Language Model Fit

## Quick Facts
- arXiv ID: 2312.10523
- Source URL: https://arxiv.org/abs/2312.10523
- Reference count: 40
- Primary result: Benchmark measures language model fit across 585 English and code domains, revealing hidden weaknesses when evaluating on diverse distributions

## Executive Summary
PALOMA is a benchmark designed to evaluate language model fit across 585 diverse domains rather than assuming performance on one distribution generalizes to others. It includes new datasets of top 100 subreddits and programming languages, and provides 6 baseline 1B parameter models with controls for fair comparison. The benchmark reveals that models pretrained without data beyond Common Crawl exhibit anomalous gaps in fit to many domains, and that perplexity is dominated by the most frequently occurring strings in the vocabulary.

## Method Summary
PALOMA evaluates language models on 585 domains ranging from news sites to Reddit communities and programming languages. The benchmark provides 6 baseline 1B parameter models trained on popular corpora with contamination controls and fixed data order. Evaluation uses perplexity, bits per byte (BPB), and type-level likelihood metrics, with stratified subsampling to ensure stability across domains. Contamination is removed at the sub-document level using Bloom filters, and inference is standardized for fair comparisons.

## Key Results
- Models pretrained without data beyond Common Crawl exhibit anomalous gaps in fit to many domains
- Perplexity is dominated by the most frequently occurring strings in the vocabulary
- Some types are better predicted by smaller models, showing inverse scaling effects at the type level

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Measuring perplexity on many domains rather than one monolithic corpus exposes model weaknesses that would otherwise be hidden.
- Mechanism: Different textual domains have distinct statistical distributions of language. A model that performs well on one domain may perform poorly on another even if overall perplexity looks good. By evaluating on hundreds of domains, the benchmark reveals these disparities.
- Core assumption: The distributions of language across domains are sufficiently different that performance on one does not generalize to others.
- Evidence anchors: Abstract states "PALOMA measures LM fit to 546 English and code domains, instead of assuming perplexity on one distribution extrapolates to others."

### Mechanism 2
- Claim: Controlling for contamination and data order during pretraining ensures fair comparisons between models.
- Mechanism: If training data contains text from the evaluation set, the model will overestimate its performance. Different random orderings of training data can lead to different results due to recency effects. PALOMA enforces sub-document level decontamination and fixed data order to eliminate these confounders.
- Core assumption: Without these controls, comparisons between models are not scientifically valid.
- Evidence anchors: Abstract mentions "6 baseline 1B LMs carefully controlled to provide fair comparisons" and the paper states a basic tenet that training and test data need to be non-overlapping.

### Mechanism 3
- Claim: Fixing vocabulary and evaluation format enables direct comparison of perplexity across models.
- Mechanism: Perplexity per token is not comparable between models with different vocabularies or tokenizers. PALOMA either enforces a common vocabulary or uses bits per byte (BPB) when vocabularies differ, ensuring that differences in perplexity reflect true model performance rather than tokenization artifacts.
- Core assumption: The tokenizer used for evaluation does not significantly impact the semantic content being evaluated.
- Evidence anchors: The paper states "Perplexity per token is not comparable between models with different vocabularies" and elects to measure abstract cost values for efficiency comparisons.

## Foundational Learning

- Concept: Sub-document level contamination detection
  - Why needed here: Large pretraining corpora often contain evaluation data embedded in unrelated documents. Document-level deduplication is insufficient because contamination can occur within paragraphs.
  - Quick check question: What is the minimum paragraph size that PALOMA considers for contamination checks to avoid coincidental collisions?
  - Answer: 13 unicode segmented tokens, as shorter paragraphs are ignored to prevent false positives.

- Concept: Stratified subsampling for metric stability
  - Why needed here: Uniform subsampling can lead to underrepresentation of rare domains. Stratified sampling ensures each domain is equally represented, improving the stability of perplexity estimates.
  - Quick check question: What is the minimum number of tokens PALOMA aims to sample from each domain?
  - Answer: 100,000 tokens per domain, based on empirical estimates of variance reduction.

- Concept: Bits per byte (BPB) normalization
  - Why needed here: When comparing models with different vocabularies, perplexity per token is not directly comparable. BPB normalizes likelihood by the number of bytes, allowing fair comparison.
  - Quick check question: Why does BPB follow THE PILE's approach rather than using character-level perplexity?
  - Answer: BPB is a compromise that works well when tokenizers are not identical, as it uses a segmentation intrinsic to the text (bytes) rather than characters.

## Architecture Onboarding

- Component map: PALOMA benchmark consists of evaluation data (18 sources, 585 domains) -> baseline models (6 pretrained 1B parameter models) -> metrics (perplexity, BPB, type-level likelihood) -> submission process for fair comparisons. The evaluation data is stratified by domain, and contamination is removed at the sub-document level.
- Critical path: 1) Collect and preprocess evaluation data with domain metadata. 2) Train baseline models with contamination controls and fixed data order. 3) Implement standardized inference code for metric computation. 4) Set up submission guidelines for the community.
- Design tradeoffs: Stratified sampling increases inference cost but improves metric stability. Fixed vocabulary enables direct perplexity comparison but limits research on tokenization. Decontamination at the paragraph level is conservative but may remove useful data.
- Failure signatures: High perplexity variance across subsamples indicates insufficient data per domain. Erratic perplexity curves suggest contamination or data order issues. Inconsistent improvement rates across domains may indicate unequal learning dynamics.
- First 3 experiments:
  1. Run inference on a single domain (e.g., Wikipedia) using the provided code to verify correct setup and understand the output format.
  2. Compare perplexity of two baseline models on a shared domain to confirm that contamination controls are working (should see consistent results).
  3. Vary the evaluation subsample size for a domain and plot perplexity variance to empirically verify the stability threshold.

## Open Questions the Paper Calls Out

- How does language model fit to fine-grained domains differ across languages beyond English and code? The authors explicitly acknowledge this as a limitation, stating "The largest limitation of PALOMA is that we elect to focus just on the language modeling of English and code data."
- What specific features of language in domains do humans find most salient, and how can we develop metrics that better capture these features? The authors note that "we are unable to fully disentangle data and compute efficiency" and suggest that "examining discrepancies in existing metrics over domains will lead to a deeper understanding of language modeling dynamics."
- How do inverse scaling effects manifest at the type level, and what training dynamics cause certain types to be better predicted by smaller models? The authors find that "there are always types that are better predicted by Pythia-1B" and observe "various patterns" in training dynamics, including cases where "Pythia-7B is getting worse over time."

## Limitations
- Benchmark focuses only on English and code data, limiting generalizability to other languages
- Exact training code for baseline models is not publicly released, requiring author assistance for full reproduction
- Some evaluation datasets (THE PILE and ICE) have access restrictions, limiting reproducibility

## Confidence
- PALOMA exposes hidden model weaknesses through multi-domain evaluation: **High**
- Contamination controls ensure fair model comparisons: **Medium**
- BPB normalization enables valid cross-vocabulary comparisons: **High**
- Baseline models trained without CC data show anomalous gaps: **Low** (limited to 1B models, may not generalize to larger scales)
- Perplexity is dominated by frequent strings: **Medium** (empirical finding needs broader validation)

## Next Checks
1. Reproduce contamination detection independently: Implement the described Bloom filter approach and apply it to a subset of training/evaluation data pairs to verify that it correctly identifies sub-document level overlaps without excessive false positives.

2. Test domain distinctiveness empirically: Perform pairwise KL divergence analysis on the 585 domains to quantify their statistical independence and confirm that performance on one domain does not predict performance on others.

3. Validate BPB as perplexity substitute: Train two identical models with different vocabularies on the same data, then compare perplexity and BPB across domains to verify that BPB correlates with perplexity differences and enables valid cross-vocabulary comparisons.