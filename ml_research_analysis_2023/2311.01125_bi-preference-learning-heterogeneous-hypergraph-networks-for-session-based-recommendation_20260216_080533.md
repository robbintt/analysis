---
ver: rpa2
title: Bi-Preference Learning Heterogeneous Hypergraph Networks for Session-based
  Recommendation
arxiv_id: '2311.01125'
source_url: https://arxiv.org/abs/2311.01125
tags:
- price
- user
- preference
- item
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of incorporating price preference
  into session-based recommendation, which is crucial for predicting user purchase
  decisions but often overlooked. The proposed Bi-Preference Learning Heterogeneous
  Hypergraph Networks (BiPNet) method tackles this by constructing a customized heterogeneous
  hypergraph to capture user preferences from various item features (ID, price, category,
  brand) and designing a triple-level convolution to aggregate information at co-occurrence,
  intra-type, and inter-type levels.
---

# Bi-Preference Learning Heterogeneous Hypergraph Networks for Session-based Recommendation

## Quick Facts
- arXiv ID: 2311.01125
- Source URL: https://arxiv.org/abs/2311.01125
- Reference count: 40
- Primary result: Outperforms SOTA baselines on three real-world datasets, achieving up to 5.74% and 2.22% relative improvements in Prec@20 and MRR@20 respectively on Cosmetics

## Executive Summary
This paper addresses the challenge of incorporating price preference into session-based recommendation, which is crucial for predicting user purchase decisions but often overlooked. The proposed Bi-Preference Learning Heterogeneous Hypergraph Networks (BiPNet) method tackles this by constructing a customized heterogeneous hypergraph to capture user preferences from various item features (ID, price, category, brand) and designing a triple-level convolution to aggregate information at co-occurrence, intra-type, and inter-type levels. Additionally, BiPNet employs a bi-preference learning schema under multi-task learning to jointly model price and interest preferences. Experimental results on three real-world datasets demonstrate that BiPNet outperforms state-of-the-art baselines, achieving up to 5.74% and 2.22% relative improvements in Prec@20 and MRR@20 respectively on the Cosmetics dataset. The study highlights the significance of price in session-based recommendation and provides insights into user behavior across different price levels and session lengths.

## Method Summary
BiPNet constructs a heterogeneous hypergraph to model user-item interactions at multiple levels: feature hyperedges connect item ID, price, category, and brand; price hyperedges capture price preferences within sessions; session hyperedges link items in the same session. A triple-level convolution is then applied: co-occurrence convolution aggregates information from all same-type nodes in the session, intra-type convolution uses attention to weight importance within each type, and inter-type convolution fuses these via gating to produce the updated embedding. BiPNet employs a bi-preference learning schema under multi-task learning to jointly model price and interest preferences. Price preference is extracted using attention on price embeddings and position, while interest preference is extracted using attention on ID embeddings and position. The two preferences are then gated together to produce the final prediction.

## Key Results
- Outperforms SOTA baselines on three real-world datasets (Cosmetics, Grocery, Toys)
- Achieves up to 5.74% and 2.22% relative improvements in Prec@20 and MRR@20 respectively on the Cosmetics dataset
- Demonstrates the significance of price in session-based recommendation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The triple-level convolution captures heterogeneous node interactions better than single-type GCNs by explicitly modeling co-occurrence, intra-type, and inter-type relations.
- Mechanism: For each target node, it aggregates co-occurrence information from all same-type nodes in the session, then uses attention to weight importance within each type, and finally fuses these via gating to produce the updated embedding.
- Core assumption: Nodes of the same type have homogeneous semantics, while different types provide complementary information.
- Evidence anchors:
  - [abstract] "triple-level convolution are devised to capture user price and interest preference from heterogeneous features of items"
  - [section] "For a target node in the heterogeneous hypergraph, there are multiple relations we should consider...co-occurrence, intra-type and inter-type relations"
  - [corpus] Weak - no direct corpus match; assumption is based on the model description.
- Break condition: If the hypergraph structure is not sparse enough, the average pooling over co-occurrence nodes may drown out important signals, causing over-smoothing.

### Mechanism 2
- Claim: Multi-task learning with price and interest prediction enables mutual enhancement because the two preferences share underlying latent factors.
- Mechanism: The model predicts both price level and next-item probability using shared embeddings, then gates the two preference vectors together so that price signals inform interest and vice versa.
- Core assumption: User price sensitivity and interest preference are interdependent and jointly determine purchase decisions.
- Evidence anchors:
  - [abstract] "develop a Bi-Preference Learning schema to explore mutual relations between price and interest preference and collectively learn these two preferences under the multi-task learning architecture"
  - [section] "price preference and interest preference are interdependent and collectively determine user choice"
  - [corpus] Weak - corpus lacks explicit evidence; assumption is inferred from the proposed design.
- Break condition: If price and interest preferences are largely independent for a dataset, the gating may introduce noise and degrade performance.

### Mechanism 3
- Claim: Discretizing price into logistic-based levels preserves the true distribution of user sensitivity, improving modeling of price preference.
- Mechanism: Price is mapped to discrete levels by partitioning the logistic CDF so that each level has equal probability mass, matching the empirical price distribution per category.
- Core assumption: Price sensitivity is higher around the mode of the distribution, requiring finer granularity there.
- Evidence anchors:
  - [section] "we divide price into ðœŒ levels (e.g., ðœŒ = 5) by making the probability of every interval equal...price distribution tends to be logistic distribution"
  - [abstract] "price discretization" is used in the experiments
  - [corpus] Weak - no corpus evidence for logistic assumption; inferred from paper methodology.
- Break condition: If real price distributions deviate strongly from logistic, equal-probability binning may allocate levels poorly, harming price modeling.

## Foundational Learning

- Concept: Hypergraph structure and hyperedges
  - Why needed here: Standard graphs only model pairwise item transitions, but item-category-brand-price interactions are higher-order; hyperedges naturally capture these.
  - Quick check question: How does a hyperedge differ from a regular edge in terms of node connectivity?
- Concept: Multi-task learning and loss fusion
  - Why needed here: Jointly predicting price level and next item allows the model to leverage shared embeddings and explore inter-preference relations.
  - Quick check question: Why does simply adding the two task losses (instead of weighting them) work here?
- Concept: Price discretization and logistic CDF
  - Why needed here: Raw price values are category-dependent; discretizing via logistic CDF aligns price levels with user sensitivity distribution.
  - Quick check question: What is the effect of choosing uniform vs logistic-based price binning?

## Architecture Onboarding

- Component map: Input (item ID, price, category, brand embeddings) -> Heterogeneous hypergraph construction -> Triple-level convolution (co-occurrence -> intra-type -> inter-type) -> Price preference branch (price embeddings + position + multi-head self-attention) -> Interest preference branch (ID embeddings + position + attention-weighted sum) -> Bi-preference gating (fusion of both preference vectors) -> Output (joint probability for next item)
- Critical path: Graph construction -> triple-level convolution -> preference mining -> gating -> prediction
- Design tradeoffs:
  - Coarser price discretization -> faster training, less granularity
  - More hyperedge types -> richer semantics, higher complexity
  - Larger â„Ž (attention heads) -> better long-range modeling, more params
- Failure signatures:
  - Over-smoothing in triple-level convolution -> all node embeddings converge
  - Imbalanced price/interest loss -> one task dominates training
  - Hypergraph too sparse -> co-occurrence pooling uninformative
- First 3 experiments:
  1. Remove co-occurrence convolution -> observe drop in precision
  2. Use uniform price levels -> compare against logistic discretization
  3. Train with single-task (only interest) -> measure loss in performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed price discretization method compare to other price discretization techniques in terms of recommendation accuracy and computational efficiency?
- Basis in paper: [explicit] The paper introduces a novel price discretization method based on logistic distribution and compares it to uniform quantization, showing improved performance.
- Why unresolved: While the paper demonstrates the superiority of their method over uniform quantization, it does not compare it to other advanced price discretization techniques used in the field.
- What evidence would resolve it: Conducting experiments comparing the proposed method to other state-of-the-art price discretization techniques, such as clustering-based or quantile-based methods, would provide insights into its relative performance.

### Open Question 2
- Question: How does the performance of the proposed BiPNet model vary across different types of e-commerce domains (e.g., fashion, electronics, books)?
- Basis in paper: [inferred] The paper evaluates the model on three datasets from different e-commerce domains (cosmetics, grocery, toys) but does not explore its performance across a wider range of domains.
- Why unresolved: The current evaluation only covers three domains, which may not be representative of all e-commerce sectors. Different domains might have varying price sensitivities and user behavior patterns that could affect the model's performance.
- What evidence would resolve it: Testing the model on a diverse set of e-commerce datasets spanning various product categories and consumer behaviors would reveal its generalizability across different domains.

### Open Question 3
- Question: What is the impact of incorporating additional item features, such as product ratings, reviews, or images, on the performance of the BiPNet model?
- Basis in paper: [inferred] The paper considers item ID, price, category, and brand as features but does not explore the potential benefits of incorporating other item attributes like ratings or images.
- Why unresolved: While the paper demonstrates the effectiveness of using basic item features, it does not investigate whether adding more complex or high-dimensional features could further enhance the model's performance.
- What evidence would resolve it: Conducting experiments by incorporating additional item features, such as product ratings, user reviews, or image embeddings, and comparing the results to the baseline model would provide insights into the potential benefits of these features.

## Limitations
- The paper's claims rely heavily on the assumption that price and interest preferences are interdependent, which may not hold across all domains.
- The logistic discretization of price is theoretically motivated but lacks empirical validation against alternative binning strategies.
- The hypergraph construction details are sparse, making exact reproduction challenging without additional implementation specifics.

## Confidence
- Triple-level convolution effectiveness: High
- Multi-task learning benefits: Medium
- Logistic price discretization: Low

## Next Checks
1. Test model performance with uniform price binning to validate logistic CDF assumption
2. Perform controlled ablation removing price preference branch to isolate its contribution
3. Analyze attention weight distributions to verify inter-type relation learning actually occurs