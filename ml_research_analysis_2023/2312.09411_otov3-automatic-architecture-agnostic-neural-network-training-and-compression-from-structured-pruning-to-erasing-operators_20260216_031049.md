---
ver: rpa2
title: 'OTOv3: Automatic Architecture-Agnostic Neural Network Training and Compression
  from Structured Pruning to Erasing Operators'
arxiv_id: '2312.09411'
source_url: https://arxiv.org/abs/2312.09411
tags:
- relu
- pruning
- conv
- erasing
- otov3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OTOv3 introduces an automated framework for training and compressing
  deep neural networks (DNNs) via both structured pruning and erasing operators. It
  addresses the challenge of manually crafting search spaces for DNN compression,
  which demands substantial domain knowledge and engineering effort.
---

# OTOv3: Automatic Architecture-Agnostic Neural Network Training and Compression from Structured Pruning to Erasing Operators

## Quick Facts
- arXiv ID: 2312.09411
- Source URL: https://arxiv.org/abs/2312.09411
- Authors: 
- Reference count: 24
- Key outcome: OTOv3 achieves competitive or superior performance compared to state-of-the-art methods across benchmarks, reducing FLOPs and parameters while maintaining high accuracy.

## Executive Summary
OTOv3 introduces an automated framework for training and compressing deep neural networks (DNNs) via both structured pruning and erasing operators. It addresses the challenge of manually crafting search spaces for DNN compression, which demands substantial domain knowledge and engineering effort. OTOv3 automates this process using dependency graph analysis to construct search spaces for pruning and erasing modes. For pruning, it employs a Dual Half-Space Projected Gradient (DHSPG) optimizer to identify redundant structures and train compact models without fine-tuning. For erasing, it introduces a Hierarchical Half-Space Projected Gradient (H2SPG) optimizer to handle the added complexity of hierarchical dependencies while ensuring sub-network validity. OTOv3 achieves competitive or superior performance compared to state-of-the-art methods across benchmarks, reducing FLOPs and parameters while maintaining high accuracy. It is the first framework to unify automated pruning and erasing, significantly simplifying DNN compression.

## Method Summary
OTOv3 automates the training and compression of DNNs through a pipeline that includes dependency graph analysis, search space generation, sparse optimization, and sub-network construction. The framework uses a trace graph to represent the DNN architecture and applies distinct algorithms to construct pruning and erasing dependency graphs. For pruning, DHSPG separates variables into redundant and important groups, updates important groups via standard gradient descent, and projects redundant groups toward zero using half-space projection with saliency-driven identification. For erasing, H2SPG extends this with a hierarchical search phase that ensures graph validity when erasing operators. OTOv3 achieves zero-invariant groups (ZIGs) where setting all parameters to zero produces zero output tensors, enabling sub-network construction without fine-tuning.

## Key Results
- OTOv3 achieves 83.8%-87.7% F1-scores and 74.6%-80.0% exact match rates in identifying redundant structures compared to baselines
- Outperforms state-of-the-art methods in reducing FLOPs and parameters while maintaining accuracy across multiple benchmarks
- First framework to unify automated pruning and erasing operators in a single framework

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Automated search space generation via dependency graph analysis enables OTOv3 to handle arbitrary DNN architectures without manual engineering.
- Mechanism: OTOv3 constructs a trace graph from the DNN, identifies vertex types (stem, joint, accessory, unknown), and groups them based on their dependencies. For pruning, it forms node groups of interdependent vertices; for erasing, it uses breadth-first traversal to identify valid minimally removal structures while ensuring the sub-network remains functional.
- Core assumption: The dependency graph accurately captures all architectural constraints required to maintain sub-network validity after pruning or erasing.
- Evidence anchors:
  - [abstract] "automatic search space construction for general DNNs based on dependency graph analysis"
  - [section] "We propose distinct novel graph algorithms to automatically exploit the architecture and establish pruning and erasing dependency graphs"
  - [corpus] Weak - no corpus papers directly validate the dependency graph automation mechanism
- Break condition: If the trace graph construction fails to capture complex operator dependencies (e.g., custom operators with hidden state), the automated search space may include invalid removal structures, causing sub-network construction to fail.

### Mechanism 2
- Claim: DHSPG and H2SPG sparse optimizers enable one-shot training with controlled sparsity and high performance.
- Mechanism: DHSPG separates variables into redundant and important groups, updates important groups via standard gradient descent, and projects redundant groups toward zero using half-space projection with saliency-driven identification. H2SPG extends this with a hierarchical search phase that ensures graph validity when erasing operators.
- Core assumption: The saliency scoring (gradient magnitude/cosine similarity) reliably identifies truly redundant structures without harming model performance.
- Evidence anchors:
  - [abstract] "Dual Half-Space Projected Gradient (DHSPG) and its enhanced version with hierarchical search (H2SPG) to reliably solve (hierarchical) structured sparsity problems"
  - [section] "DHSPG significantly outperforms HSPG and ProxSSI by achieving 83.8%-87.7% F1-scores and 74.6%-80.0% exact match rates"
  - [corpus] Weak - no direct corpus evidence for half-space projection superiority in DNN pruning
- Break condition: If saliency scores misidentify important structures as redundant (e.g., due to gradient noise), the optimizer may degrade performance below acceptable thresholds.

### Mechanism 3
- Claim: Zero-invariant groups (ZIGs) enable sub-network construction without fine-tuning.
- Mechanism: OTOv3 identifies ZIGs where setting all parameters to zero produces zero output tensors. During training, these groups are projected to zero, and the resulting sub-network naturally returns identical outputs to the full network, eliminating the need for post-compression fine-tuning.
- Core assumption: All redundant structures in the optimal solution correspond to ZIGs, ensuring functional equivalence between full and compressed networks.
- Evidence anchors:
  - [abstract] "automated sub-network construction using solutions from DHSPG/H2SPG and dependency graphs"
  - [section] "Based on the property of PZIGs, M*prune returns the same inference outputs as the full M parameterized as x*DHSPG, thus no further fine-tuning is necessary"
  - [corpus] Weak - no corpus validation of ZIG-based sub-network construction
- Break condition: If some redundant structures are not ZIGs (e.g., structures that contribute to gradient flow but not output), the sub-network will produce different outputs than the full network, requiring fine-tuning.

## Foundational Learning

- Concept: Graph theory and dependency analysis
  - Why needed here: Understanding how to represent DNN architectures as graphs and analyze dependencies between operators is crucial for automated search space generation
  - Quick check question: Can you explain why joint vertices in a DNN require special handling during pruning vs. erasing dependency analysis?

- Concept: Structured sparsity optimization
  - Why needed here: DHSPG and H2SPG solve structured sparsity problems with group sparsity constraints, which is fundamentally different from standard weight pruning
  - Quick check question: What's the difference between unstructured sparsity and the group sparsity enforced by DHSPG/H2SPG?

- Concept: Half-space projection methods
  - Why needed here: These methods are used to project redundant groups toward zero while maintaining descent directions for both the objective and the group magnitude
  - Quick check question: How does half-space projection differ from proximal operators in handling structured sparsity?

## Architecture Onboarding

- Component map: Graph Analysis Module -> Search Space Generator -> Sparse Optimizer (DHSPG for pruning, H2SPG for erasing) -> Sub-network Constructor -> Integration Layer
- Critical path: Graph Analysis → Search Space Generation → Sparse Optimization → Sub-network Construction
- Design tradeoffs: Automated search space generation sacrifices some optimality for generality vs. manual handcrafted search spaces
- Failure signatures: Invalid sub-networks (disconnected graphs), performance degradation beyond acceptable thresholds, failure to converge during optimization
- First 3 experiments:
  1. Test on DemoNet with both pruning and erasing modes to verify basic functionality
  2. Apply to VGG16-BN on CIFAR10 to validate pruning performance against baselines
  3. Use Bert on SQuAD to test transformer compatibility and DHSPG vs. HSPG comparison

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the automated search space generation for erasing operators handle cases where the removal of a single operator causes cascading failures in dependent operators?
- Basis in paper: [inferred] The paper discusses the creation of erasing dependency graphs but does not fully detail how to handle cascading failures when removing operators.
- Why unresolved: The paper introduces the concept of erasing dependency graphs but does not elaborate on the algorithmic steps to prevent or handle cascading failures in the network.
- What evidence would resolve it: A detailed algorithm or set of rules that ensures the validity of the network after removing an operator, including how to handle dependent operators that might be affected.

### Open Question 2
- Question: How does the H2SPG optimizer ensure the validity of the sub-network architecture during the hierarchical structured sparsity problem solving process?
- Basis in paper: [explicit] The paper introduces H2SPG as a solution for hierarchical structured sparsity problems but does not fully explain how it ensures the validity of the sub-network architecture.
- Why unresolved: The paper describes the hierarchical search phase and hybrid training phase of H2SPG but does not detail how it maintains the validity of the network architecture during the optimization process.
- What evidence would resolve it: A step-by-step explanation of how H2SPG maintains network validity during the hierarchical search and optimization phases, including any checks or adjustments made.

### Open Question 3
- Question: What are the specific criteria used to determine the salience scores for variable groups in the erasing mode, and how do these criteria affect the performance of the resulting sub-network?
- Basis in paper: [inferred] The paper mentions the use of salience scores for identifying redundant structures but does not specify the exact criteria or their impact on performance.
- Why unresolved: The paper refers to salience scores but does not provide a detailed explanation of the criteria used to calculate them or how these scores influence the selection of redundant structures.
- What evidence would resolve it: A clear definition of the criteria used to calculate salience scores, along with experimental results showing how different criteria affect the performance and efficiency of the resulting sub-network.

## Limitations
- The paper's automated search space generation relies heavily on the accuracy of dependency graph construction, which is not validated against complex real-world architectures with custom operators or unusual connectivity patterns
- The claim of "zero fine-tuning" depends on identifying valid zero-invariant groups (ZIGs), but there's no empirical validation that all redundant structures in practice are indeed ZIGs
- The half-space projection methods (DHSPG/H2SPG) are presented as superior to existing approaches, but the comparison is limited to specific scenarios without broader ablation studies across different network types and sparsity targets

## Confidence
- High confidence: The overall framework architecture and the concept of automated search space generation are well-defined and theoretically sound
- Medium confidence: The dependency graph analysis approach and its ability to handle arbitrary architectures without manual intervention
- Low confidence: The claim of zero fine-tuning requirements and the superiority of half-space projection methods over all existing approaches

## Next Checks
1. Test OTOv3 on a complex architecture with custom operators (e.g., Vision Transformers or Swin Transformers) to validate the dependency graph construction handles non-standard operator dependencies
2. Perform ablation studies comparing DHSPG/H2SPG with ProxSSI and other structured sparsity methods across multiple sparsity targets and network architectures to verify claimed performance improvements
3. Conduct controlled experiments where fine-tuning is explicitly measured on models where OTOv3 claims zero fine-tuning is required, to empirically validate the ZIG identification mechanism