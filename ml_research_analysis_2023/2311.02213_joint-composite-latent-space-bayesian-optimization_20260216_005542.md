---
ver: rpa2
title: Joint Composite Latent Space Bayesian Optimization
arxiv_id: '2311.02213'
source_url: https://arxiv.org/abs/2311.02213
tags:
- optimization
- joco
- space
- bayesian
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces JoCo, a novel Bayesian optimization framework
  for composite functions with high-dimensional input and intermediate output spaces.
  JoCo jointly trains neural network encoders and probabilistic models to adaptively
  compress these spaces into manageable latent representations, enabling effective
  optimization.
---

# Joint Composite Latent Space Bayesian Optimization

## Quick Facts
- arXiv ID: 2311.02213
- Source URL: https://arxiv.org/abs/2311.02213
- Reference count: 40
- Key outcome: JoCo outperforms state-of-the-art on 9 high-dimensional BO tasks including LLM adversarial prompts and text-to-image generation

## Executive Summary
This work introduces JoCo, a Bayesian optimization framework for composite functions with high-dimensional input and intermediate output spaces. JoCo jointly trains neural network encoders and probabilistic models to compress these spaces into manageable latent representations, enabling effective optimization. The method demonstrates superior performance on nine high-dimensional BO tasks, particularly excelling in optimizing adversarial prompts for large language models and text-to-image generative models where intermediate outputs can reach half a million dimensions.

## Method Summary
JoCo addresses Bayesian optimization of composite functions f(x) = g(h(x)) where both inputs X and intermediate outputs Y are high-dimensional. The framework uses two neural network encoders (EX for X→ˆX and EY for Y→ˆY) to map to latent spaces, along with probabilistic models ˆh(ˆX→P(ˆY)) and ˆg(ˆY→P(f)) trained jointly using a combined marginal log-likelihood loss. Thompson sampling with trust regions guides exploration in the compressed space, with all models updated continuously using recent observations.

## Key Results
- Outperforms existing methods on 9 high-dimensional BO tasks including synthetic functions, environmental modeling, and generative AI applications
- Demonstrates superior performance in optimizing adversarial prompts for large language models and text-to-image generative models
- Shows robustness to choice of data points used for model updating (Nb=20 performs well)

## Why This Works (Mechanism)

### Mechanism 1
JoCo jointly trains encoders and probabilistic models to learn compressed latent representations that preserve only information relevant to the optimization goal. The joint training loss combines negative marginal log-likelihoods of both the encoded intermediate outputs and the final rewards, with the shared embedded intermediate output space tying the two components together. This creates a feedback loop where the encoder learns to discard irrelevant information based on supervision from function values.

### Mechanism 2
Thompson sampling with trust regions enables effective exploration in high-dimensional spaces by focusing search on promising regions. The two-stage sampling procedure first draws latent space samples from the probabilistic model, then maps them back to the original space within a trust region, balancing exploration and exploitation while avoiding the curse of dimensionality.

### Mechanism 3
Joint and continuous updating of models during optimization allows JoCo to adapt to changing optimization landscapes. After each evaluation, JoCo updates all four components using the most recent data points, allowing the latent space to evolve as optimization progresses.

## Foundational Learning

- **Gaussian Process surrogate modeling and kernel selection** - Needed for modeling the mapping from embedded input to embedded output space and from embedded output to rewards. Quick check: What kernel would you choose for a composite function where the intermediate space is high-dimensional but the relevant dimensions are unknown?

- **Variational autoencoders and latent space representation learning** - Needed for the encoder architectures that map high-dimensional inputs and intermediate outputs to lower-dimensional latent spaces. Quick check: How does using function values as supervision differ from standard VAE training objectives?

- **Thompson sampling and trust-region optimization** - Needed for the core exploration strategies used to generate new candidate points in high-dimensional spaces. Quick check: Why might Thompson sampling be preferred over expected improvement in trust-region BO settings?

## Architecture Onboarding

- **Component map**: EX (Input encoder X→ˆX) → ˆh (Intermediate model ˆX→P(ˆY)) → ˆg (Reward model ˆY→P(f)) with EY (Output encoder Y→ˆY) parallel to ˆh, all jointly trained
- **Critical path**: Data collection → Joint training → Thompson sampling within trust region → Evaluate → Update models
- **Design tradeoffs**: Joint training provides better adaptation but increases computational cost; continuous updating improves accuracy but risks overfitting; trust regions improve efficiency but may miss global optima
- **Failure signatures**: Poor performance on synthetic benchmarks suggests encoder architecture issues; failure to improve after initial iterations suggests trust region problems; inconsistent results across runs suggests training instability
- **First 3 experiments**: 1) Implement JoCo on a simple 2D composite function with known structure to verify basic functionality; 2) Compare JoCo with and without joint training on a moderate-dimensional problem to demonstrate the benefit; 3) Test JoCo with different encoder architectures on a high-dimensional synthetic problem to find optimal design

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several unresolved issues emerge from the analysis:

### Open Question 1
How does JoCo's performance scale with increasingly high-dimensional intermediate outputs (e.g., beyond 500,000 dimensions)? The paper demonstrates JoCo's effectiveness on intermediate outputs up to 451,584 dimensions but does not explore higher dimensions.

### Open Question 2
How does the choice of encoder architecture (e.g., convolutional vs. fully connected) impact JoCo's performance on different types of intermediate outputs? While the paper uses different encoder architectures for different tasks, it does not systematically compare the impact of these choices on performance across various task types.

### Open Question 3
What is the impact of the number of data points used for updating the models (Nb) on JoCo's performance, and is there an optimal value for this hyperparameter? While the paper shows that JoCo's performance is robust to the choice of Nb, it does not explore the full range of possible values or identify an optimal setting.

## Limitations
- Computational complexity due to joint training of four models, particularly challenging for high-dimensional problems
- Performance critically depends on quality of encoder architectures and the assumption that function values contain sufficient information to guide compression
- Limited validation on problems where the relationship between intermediate outputs and final rewards is intentionally obfuscated

## Confidence
- Mechanism 1 (joint training with supervision): Medium - Limited empirical evidence specifically validating that function values effectively identify relevant intermediate output dimensions
- Mechanism 2 (Thompson sampling in latent space): Medium - Theoretically justified but lacks direct experimental validation on composite functions specifically
- Mechanism 3 (continuous updating): Medium - Claims supported by synthetic benchmarks but not comprehensively validated across all problem types

## Next Checks
1. Conduct ablation studies systematically removing joint training, continuous updating, and Thompson sampling to quantify the contribution of each component
2. Test JoCo on problems where the relationship between intermediate outputs and final rewards is intentionally obfuscated to evaluate robustness to irrelevant information
3. Compare JoCo's computational efficiency against baseline methods on the same hardware to verify claimed performance advantages account for computational cost