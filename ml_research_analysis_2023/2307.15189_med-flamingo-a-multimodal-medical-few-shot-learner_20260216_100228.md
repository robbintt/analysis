---
ver: rpa2
title: 'Med-Flamingo: a Multimodal Medical Few-shot Learner'
arxiv_id: '2307.15189'
source_url: https://arxiv.org/abs/2307.15189
tags:
- medical
- med-flamingo
- few-shot
- dataset
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce Med-Flamingo, a multimodal few-shot learner
  tailored for the medical domain. Built on the OpenFlamingo-9B architecture, Med-Flamingo
  is pre-trained on large-scale, interleaved medical image-text data from textbooks
  and publications.
---

# Med-Flamingo: a Multimodal Medical Few-shot Learner

## Quick Facts
- arXiv ID: 2307.15189
- Source URL: https://arxiv.org/abs/2307.15189
- Authors: 
- Reference count: 10
- Improves clinical evaluation score by up to 20% over prior models in multimodal medical VQA

## Executive Summary
Med-Flamingo is a multimodal few-shot learner built on OpenFlamingo-9B, pre-trained on large-scale interleaved medical image-text data from textbooks and publications. The model enables few-shot generative visual question answering across medical specialties, evaluated on VQA-RAD, PathVQA, and a newly created Visual USMLE dataset. Clinician-led human evaluation shows Med-Flamingo achieves up to 20% improvement over prior models and the lowest average rank (1.67) in clinical evaluation scores. The work demonstrates proof-of-concept for generalist medical AI with multimodal few-shot capabilities, though limitations include occasional hallucinations and the need for further alignment to human preferences.

## Method Summary
Med-Flamingo continues pre-training from the OpenFlamingo-9B checkpoint on paired (PMC-OA) and interleaved (MTB) medical image-text data using a joint objective. The MTB dataset contains 4,721 medical textbooks converted to HTML with 0.8M images and 584M tokens, while PMC-OA provides 1.3M image-caption pairs. Training uses 8x80GB A100 GPUs with DeepSpeed ZeRO Stage 2, 8-bit AdamW, and memory-efficient attention for 2,700 steps. The model employs gated cross-attention layers to integrate visual and textual information. Evaluation uses few-shot prompting (6 shots for VQA-RAD/PathVQA, 4 shots for Visual USMLE) and human clinical evaluation via a Streamlit app where physicians rate generations on a 0-10 scale.

## Key Results
- Clinician evaluation shows up to 20% improvement over prior models in medical VQA performance
- Achieves lowest average rank (1.67) across datasets in clinical evaluation scores
- Demonstrates ability to generate rationales for VQA answers through few-shot prompting
- First model to enable multimodal medical few-shot adaptations for generative VQA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning from interleaved medical image-text pairs enables Med-Flamingo to synthesize multimodal clinical information without fine-tuning.
- Mechanism: The model leverages gated cross-attention layers to attend across interleaved image and text tokens, integrating visual findings with clinical narratives during inference.
- Core assumption: Interleaved format preserves alignment between images and descriptive context better than paired data alone.
- Evidence anchors:
  - [abstract] "Based on OpenFlamingo-9B, we continue pre-training on paired and interleaved medical image-text data from publications and textbooks."
  - [section] "we propose Med-Flamingo, the first medical foundation model that can perform multimodal in-context learning specialized for the medical domain."
  - [corpus] Weak evidence; related works focus on discriminative or fine-tuned models, not few-shot generative learners.
- Break condition: If interleaving disrupts natural document flow, the model may fail to learn correct multimodal associations.

### Mechanism 2
- Claim: Clinical expert human evaluation provides a more reliable metric than automated similarity scores for generative medical VQA.
- Mechanism: Blinded raters score model generations on 0-10 scale based on clinical usefulness, incorporating nuanced judgment that automated metrics miss.
- Core assumption: Human raters can assess contextual appropriateness and medical accuracy better than token-level similarity metrics.
- Evidence anchors:
  - [abstract] "We conduct the first human evaluation for generative medical VQA where physicians review the problems and blinded generations in an interactive app."
  - [section] "for sake of clinical utility, we employ the following evaluation metrics that directly assess the quality of the generated answer."
  - [corpus] Weak evidence; most cited papers use discriminative metrics, not human evaluation for generative outputs.
- Break condition: If raters are not calibrated or evaluation app introduces bias, scores may not reflect true clinical utility.

### Mechanism 3
- Claim: Few-shot prompting allows Med-Flamingo to generate rationales and customize responses to specific medical tasks.
- Mechanism: By providing example question-answer-rationale triples in the prompt, the model learns to mimic format and produce structured reasoning in output.
- Core assumption: Few-shot examples serve as templates that the model can generalize from to new cases.
- Evidence anchors:
  - [abstract] "Med-Flamingo improves performance in generative medical VQA by up to 20% in clinician's rating and firstly enables multimodal medical few-shot adaptations, such as rationale generation."
  - [section] "we showcase few examples of Med-Flamingo generations in more detail... exemplifying that a medical few-shot learner like Med-Flamingo can be prompted to generate rationale for its VQA answer."
  - [corpus] Weak evidence; related works focus on classification or discriminative outputs, not rationale generation in generative VQA.
- Break condition: If few-shot examples are too few or not representative, the model may hallucinate or produce incomplete rationales.

## Foundational Learning

- Concept: Multimodal pre-training on interleaved image-text data
  - Why needed here: To teach the model how to integrate visual and textual medical information in a unified representation space.
  - Quick check question: Does the model need to learn the correspondence between a chest X-ray and its radiology report text?

- Concept: In-context learning via few-shot prompting
  - Why needed here: To enable the model to adapt to new medical tasks without parameter updates, crucial for rare or evolving clinical scenarios.
  - Quick check question: Can the model generate a differential diagnosis from a few example clinical vignettes and images?

- Concept: Human evaluation for clinical utility
  - Why needed here: To ensure that generated answers are not only textually similar but also clinically meaningful and actionable.
  - Quick check question: Would a board-certified radiologist rate a generated answer as clinically useful based on accuracy and completeness?

## Architecture Onboarding

- Component map:
  Vision encoder (CLIP ViT/L-14) -> Gated cross-attention layers -> Language model backbone (LLaMA-7B) -> Perceiver resampler -> Output

- Critical path:
  1. Preprocess interleaved image-text medical data
  2. Initialize from OpenFlamingo-9B checkpoint
  3. Continue pre-training on MTB and PMC-OA datasets
  4. Evaluate on VQA-RAD, PathVQA, and Visual USMLE
  5. Conduct human evaluation with clinical experts

- Design tradeoffs:
  - Large model size (8.3B params) vs. inference efficiency
  - Interleaved vs. paired data format for multimodal learning
  - Generative vs. discriminative VQA formulation

- Failure signatures:
  - Hallucinations or low-quality generations despite few-shot prompting
  - Poor performance on pathology due to limited pathology-specific training data
  - Data leakage inflating evaluation scores

- First 3 experiments:
  1. Fine-tune OpenFlamingo-9B on a small subset of MTB and evaluate on VQA-RAD to check if interleaving helps.
  2. Compare human evaluation scores vs. BERT similarity on a held-out set of Med-Flamingo generations.
  3. Test few-shot rationale generation by prompting with example QAR triples and measuring clinical expert ratings.

## Open Questions the Paper Calls Out

- How does the performance of Med-Flamingo compare to specialized medical models on tasks outside the scope of its pre-training data (e.g., pathology images)?
- What is the impact of hallucination on the clinical utility of Med-Flamingo's generated answers, and how can this be mitigated?
- How does the clinical evaluation score correlate with other automated evaluation metrics, and what are the limitations of each metric?

## Limitations

- Model occasionally generates hallucinations or low-quality responses despite medical domain pre-training
- Large model size (8.3B parameters) poses deployment challenges in resource-constrained clinical settings
- Performance may be limited by availability and diversity of training data, particularly for pathology tasks

## Confidence

- High confidence: The technical implementation of OpenFlamingo architecture with gated cross-attention and the pre-training methodology on described datasets
- Medium confidence: The comparative performance improvements (up to 20%) over baseline models and clinical evaluation ranking (1.67 average rank)
- Low confidence: The generalizability of few-shot learning capabilities to unseen medical specialties and the relative advantage of interleaved vs paired training data formats

## Next Checks

1. **Inter-rater reliability assessment**: Calculate and report Fleiss' kappa or similar statistics for the human evaluation scores across the 8 physicians to establish consistency and reliability of clinical ratings.

2. **Cross-specialty generalization test**: Evaluate Med-Flamingo on an external, independently curated medical VQA dataset from a specialty not represented in the training data (e.g., dermatology or ophthalmology) to test true few-shot generalization.

3. **Paired vs interleaved ablation study**: Train two versions of the model - one on paired medical image-text data only and another on interleaved data only - then compare their performance on the same evaluation sets to empirically validate the interleaving advantage.