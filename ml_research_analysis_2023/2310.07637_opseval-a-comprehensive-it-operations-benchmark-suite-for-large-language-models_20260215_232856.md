---
ver: rpa2
title: 'OpsEval: A Comprehensive IT Operations Benchmark Suite for Large Language
  Models'
arxiv_id: '2310.07637'
source_url: https://arxiv.org/abs/2310.07637
tags:
- llms
- questions
- chinese
- performance
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces OpsEval, the first comprehensive English-Chinese\
  \ bilingual and task-oriented benchmark for evaluating large language models (LLMs)\
  \ in AIOps. OpsEval includes 7,200 questions covering three key IT operation scenarios\u2014\
  Wired Network Operation, 5G Communication Operation, and Database Operation\u2014\
  categorized into eight tasks and three cognitive ability levels."
---

# OpsEval: A Comprehensive IT Operations Benchmark Suite for Large Language Models

## Quick Facts
- **arXiv ID**: 2310.07637
- **Source URL**: https://arxiv.org/abs/2310.07637
- **Reference count**: 33
- **Primary result**: OpsEval introduces the first bilingual English-Chinese benchmark for evaluating LLMs in AIOps, covering 7,200 questions across 8 tasks and 3 cognitive ability levels.

## Executive Summary
OpsEval is a comprehensive benchmark suite designed to evaluate large language models (LLMs) in AIOps (Artificial Intelligence for IT Operations) tasks. It includes 7,200 questions covering three key IT operation scenarios—Wired Network Operation, 5G Communication Operation, and Database Operation—categorized into eight tasks and three cognitive ability levels. The benchmark evaluates LLMs using both objective multiple-choice and subjective question-answering formats. Experiments with leading LLMs show that GPT-4 consistently outperforms others, with prompt techniques like chain-of-thought and self-consistency significantly boosting performance. The study also finds that GPT-4-based scoring aligns better with expert judgments than traditional metrics like Bleu and Rouge, suggesting its suitability for large-scale qualitative evaluation.

## Method Summary
The OpsEval benchmark consists of 7,200 questions (7,184 multiple-choice and 1,736 question-answering formats) in both English and Chinese, covering three IT operation scenarios across eight tasks and three cognitive ability levels. The evaluation framework tests LLMs using various prompt engineering techniques including zero-shot, few-shot, chain-of-thought, and self-consistency approaches. Performance is measured using accuracy for objective questions and multiple metrics including Rouge, Bleu, GPT4-Score, and Expert-Evaluation for subjective questions. The benchmark is open-sourced with a public leaderboard, though 80% of the question set remains private to maintain integrity.

## Key Results
- GPT-4 consistently outperforms other LLMs across all task types and languages in the OpsEval benchmark
- Chain-of-thought and self-consistency prompting techniques significantly improve LLM performance on complex reasoning tasks
- GPT-4-based scoring shows better alignment with expert judgments than traditional NLP metrics like Bleu and Rouge for subjective question evaluation
- Quantization from 4-bit to 3-bit causes significant performance degradation, while 4-bit quantization maintains near-original performance

## Why This Works (Mechanism)

### Mechanism 1
The benchmark achieves reliable evaluation by combining multiple-choice and subjective question formats to assess both recall and practical reasoning abilities. Multiple-choice questions provide objective scoring metrics for knowledge recall, while subjective questions require LLM-generated answers that can be evaluated using semantic similarity and expert scoring. This dual-format approach tests knowledge recognition through objective questions and applied reasoning through subjective questions without option bias.

### Mechanism 2
Prompt engineering techniques (CoT, SC, few-shot) significantly improve LLM performance on complex reasoning tasks. Chain-of-thought prompts guide LLMs through intermediate reasoning steps, while self-consistency selects the most coherent answer across multiple generations. LLMs can leverage intermediate reasoning steps to better solve complex tasks when explicitly prompted, with few-shot settings showing potential improvements over zero-shot approaches from a developer's perspective.

### Mechanism 3
GPT-4-based scoring aligns better with expert judgments than traditional NLP metrics for subjective question evaluation. GPT-4 scores are generated using detailed prompts that compare LLM outputs against ground truth keypoints and detailed answers. GPT-4 can accurately assess answer quality based on fluency, accuracy, and evidence criteria, with rankings closely resembling those based on Expert-Evaluation, particularly showing promise for large-scale qualitative evaluation.

## Foundational Learning

- **Concept**: Prompt engineering and in-context learning
  - Why needed here: Different prompt techniques (zero-shot, few-shot, CoT, SC) dramatically affect LLM performance on benchmark tasks
  - Quick check question: How does chain-of-thought prompting differ from naive prompting in guiding LLM responses?

- **Concept**: Evaluation metrics for subjective questions
  - Why needed here: Traditional metrics like Bleu and Rouge may not align with human expert judgments for qualitative tasks
  - Quick check question: Why might GPT-4 scoring be more reliable than Bleu/Rouge for evaluating LLM-generated answers?

- **Concept**: Quantization and model efficiency
  - Why needed here: Different quantization levels (4-bit vs 3-bit) significantly impact model performance and inference efficiency
  - Quick check question: What performance tradeoff occurs when reducing quantization from 4-bit to 3-bit?

## Architecture Onboarding

- **Component map**: Data collection (objective/subjective questions) -> Task/ability categorization -> Prompt engineering settings -> Evaluation metrics (accuracy, Rouge, Bleu, GPT4-Score, Expert-Evaluation) -> Performance analysis
- **Critical path**: Data collection → Question categorization → Prompt engineering → Model evaluation → Performance analysis
- **Design tradeoffs**: Private question sets ensure benchmark integrity but limit transparency; multiple evaluation metrics provide comprehensive assessment but increase complexity
- **Failure signatures**: Poor CoT performance indicates reasoning limitations; high Rouge/Bleu but low GPT4-Score suggests keyword-driven responses; significant performance drops in quantized models indicate information loss
- **First 3 experiments**:
  1. Compare zero-shot vs few-shot performance across different prompt settings (Naive, CoT, SC, CoT+SC)
  2. Evaluate GPT4-Score correlation with expert judgments versus traditional metrics
  3. Test quantization impact by comparing 4-bit and 3-bit quantized models on the same benchmark tasks

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of large language models (LLMs) vary across different quantization parameters in AIOps tasks? The paper evaluates LLaMA-2-70B with different quantization parameters (3-bit and 4-bit) and shows that 4-bit quantization has minimal performance degradation compared to the full model, while 3-bit quantization causes significant degradation. While the paper provides initial evidence, it only tests two quantization levels on a single model, and the exact thresholds where quantization begins to significantly impact performance remain unclear.

### Open Question 2
What factors contribute to the language-dependent performance differences observed in LLMs for AIOps tasks? The paper observes that some LLMs perform better on English questions than Chinese ones, even when specifically fine-tuned for Chinese, and explores potential reasons including training data composition and translation quality. The paper identifies several potential factors but does not conclusively determine which factors have the greatest impact or how they interact.

### Open Question 3
How can the reliability of automated evaluation metrics for subjective AIOps questions be improved? The paper finds that GPT-4 scoring correlates better with expert evaluation than traditional metrics like Bleu and Rouge, but notes discrepancies particularly in the "Evidence" criterion. While GPT-4 scoring shows promise, the paper identifies specific areas where it falls short, suggesting room for improvement in automated evaluation.

## Limitations
- Benchmark relies on a private question set (80% of data), limiting full reproducibility and external validation
- GPT-4-based scoring may introduce bias toward GPT-4's own language patterns and reasoning approaches
- Performance differences between quantization levels suggest potential information loss, but underlying causes are not explored
- Evaluation framework focuses primarily on English and Chinese, potentially limiting applicability to other languages

## Confidence
- **High Confidence**: Objective evaluation results showing GPT-4's consistent outperformance across all task types
- **Medium Confidence**: Subjective evaluation results showing GPT-4 scoring alignment with expert judgments
- **Medium Confidence**: Prompt engineering effectiveness claims, particularly for chain-of-thought and self-consistency techniques
- **Low Confidence**: Claims about the benchmark's generalizability to other IT domains beyond the three covered scenarios

## Next Checks
1. **Cross-validation with alternative scoring methods**: Compare GPT-4-based scoring against human expert evaluations on a held-out subset of subjective questions to quantify scoring consistency
2. **Quantization analysis**: Conduct ablation studies on different quantization levels (4-bit, 3-bit, 2-bit) to identify the optimal balance between model size and performance degradation
3. **Domain transfer testing**: Apply OpsEval evaluation framework to a different IT domain (e.g., cloud operations or cybersecurity) to assess benchmark generalizability beyond the current three scenarios