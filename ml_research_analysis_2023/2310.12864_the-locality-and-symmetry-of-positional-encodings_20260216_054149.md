---
ver: rpa2
title: The Locality and Symmetry of Positional Encodings
arxiv_id: '2310.12864'
source_url: https://arxiv.org/abs/2310.12864
tags:
- positional
- uni00000013
- encodings
- locality
- symmetry
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies the properties of positional encodings (PEs)
  in BERT-style bidirectional masked language models. The authors identify two key
  properties: Locality and Symmetry.'
---

# The Locality and Symmetry of Positional Encodings

## Quick Facts
- arXiv ID: 2310.12864
- Source URL: https://arxiv.org/abs/2310.12864
- Reference count: 30
- Primary result: Positional encodings with high locality and symmetry improve sentence representation quality in BERT-style models

## Executive Summary
This paper investigates the properties of positional encodings (PEs) in BERT-style bidirectional masked language models, identifying two key characteristics: Locality and Symmetry. Locality measures the concentration of attention weights on local positions, while Symmetry measures the balance of weights around the current position. The authors find that learned PEs exhibit both properties after pre-training, and that PEs with good locality and symmetry provide better inductive bias for sentence representation, leading to improved performance on downstream tasks. They also design two new word swap probing tasks to reveal a potential flaw of symmetry: insensitivity to global swaps of semantic roles.

## Method Summary
The authors introduce two metrics to measure Locality and Symmetry of positional encodings, then analyze these properties across three pre-trained language models (BERT, XLNet, DeBERTa) and their learned PEs. They design a handcrafted Attenuated Encoding with adjustable Locality and Symmetry parameters to control these properties systematically. The study includes fine-tuning experiments on downstream tasks, dependency relation prediction, and two novel word swap probing tasks to evaluate the impact of different PE properties on model performance.

## Key Results
- Learned positional encodings converge toward high locality and symmetry during pre-training
- PEs with good locality and symmetry provide better inductive bias for sentence representation
- Positional encodings serve syntactic-level processing while contextual encodings handle semantic-level processing
- High symmetry can lead to insensitivity to global swaps of semantic roles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Positional encodings with high locality and symmetry values improve sentence representation quality in BERT-style models.
- Mechanism: Locality concentrates attention weights on adjacent positions, enabling local composition of sub-token/word units. Symmetry balances forward and backward contributions, making local processing robust to minor word swaps.
- Core assumption: Local structure is more important than global word order for sentence representation tasks.
- Evidence anchors:
  - [abstract] "PEs with good locality and symmetry provide better inductive bias for sentence representation, leading to improved performance on downstream tasks."
  - [section 3.3] "We observe that the accuracy constantly increases as the locality of encodings strengthens"
  - [corpus] Weak - related papers focus on positional encoding design but don't directly address locality/symmetry correlation.
- Break condition: Tasks requiring strict global word order (e.g., certain NLI or semantic role labeling tasks) would degrade under high symmetry.

### Mechanism 2
- Claim: Learned positional encodings converge toward high locality and symmetry during pre-training.
- Mechanism: During masked language modeling, the model learns to prioritize local context for predicting masked tokens, naturally inducing locality. Symmetry emerges because forward and backward neighbors are equally informative for local composition.
- Core assumption: Masked language modeling objectives inherently favor local contextual information.
- Evidence anchors:
  - [section 3.2] "The three language models all become much more local and symmetrical after pre-training"
  - [section 3.4] "dependency locality provides a potential explanation for the formal features of natural language word order"
  - [corpus] Assumption: While RoPE and other encodings show locality/symmetry, this doesn't directly prove the learning mechanism during MLM.

### Mechanism 3
- Claim: Positional encodings serve syntactic-level processing while contextual encodings handle semantic-level processing.
- Mechanism: Positional weights capture short-distance dependencies (syntax) while contextual weights handle long-distance semantic relations. This separation allows efficient processing of both levels.
- Core assumption: Syntactic and semantic information processing can be effectively separated in transformer attention.
- Evidence anchors:
  - [section 3.4] "positional encodings play more of a role at the syntactic level while contextual encodings serve more at the semantic level"
  - [section 3.6.2] "positional attentional heads outperform contextual heads on short-distance relations, e.g., auxpass and compound"
  - [corpus] Weak - related papers focus on positional encoding design rather than syntactic vs semantic separation.

## Foundational Learning

- Concept: Self-attention mechanism
  - Why needed here: Understanding how positional encodings modify attention weights is fundamental to grasping their role.
  - Quick check question: How does the attention weight αᵢⱼ change when positional encoding is added in absolute vs relative modes?

- Concept: Masked Language Modeling (MLM)
  - Why needed here: Pre-training method used to learn positional encodings and their properties.
  - Quick check question: Why would MLM objectives naturally lead to high locality in positional encodings?

- Concept: Syntactic vs semantic dependency
  - Why needed here: Explains why positional encodings handle syntax while contextual encodings handle semantics.
  - Quick check question: What is the difference between a syntactic dependency (e.g., compound) and a semantic dependency (e.g., dobj)?

## Architecture Onboarding

- Component map:
  Input embeddings (content + positional) -> Multi-head self-attention (contextual + positional correlations) -> Feed-forward network

- Critical path:
  1. Tokenize input and add positional encodings
  2. Compute contextual correlation γᵢⱼ = (xᵢWQ)(xⱼWK)ᵀ
  3. Compute positional correlation δᵢⱼ
  4. Combine: αᵢⱼ = (γᵢⱼ + δᵢⱼ)/√d
  5. Apply softmax and compute weighted sum

- Design tradeoffs:
  - Absolute vs relative positional encodings
  - Fixed vs learned positional encodings
  - Position-independent vs position-aware attention heads
  - Trade-off between locality strength and global information capture

- Failure signatures:
  - Poor performance on tasks requiring global word order
  - Inability to distinguish swapped semantic roles
  - Over-reliance on local context leading to context collapse

- First 3 experiments:
  1. Implement handcrafted attenuated encoding with adjustable locality/symmetry parameters and measure impact on downstream tasks
  2. Compare learned positional encodings before and after pre-training using locality/symmetry metrics
  3. Design and evaluate the two new word swap probing tasks (Constituency Shuffling and Semantic Role Shuffling)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do positional encodings interact with the self-attention mechanism in decoder-only models like GPT?
- Basis in paper: [inferred] The authors mention that their study focuses on BERT-style models and leave decoder-only models as a future study. They also note that recent research suggests decoder-only transformers without positional encodings can outperform other explicit positional encoding methods.
- Why unresolved: The paper does not investigate decoder-only models, leaving a gap in understanding the role of positional encodings in these architectures.
- What evidence would resolve it: Conducting experiments comparing the performance of decoder-only models with and without positional encodings, and analyzing how positional encodings interact with the self-attention mechanism in these models.

### Open Question 2
- Question: How do positional encodings affect the performance of language models on different types of languages with varying word order properties?
- Basis in paper: [inferred] The authors acknowledge that their analysis is limited to English and that different languages display different word order properties. They also mention that natural language generation tasks are not included in their work.
- Why unresolved: The paper focuses solely on English, and it is unclear how positional encodings would perform in languages with different word order properties or in natural language generation tasks.
- What evidence would resolve it: Extending the analysis to multiple languages with different word order properties and evaluating the performance of language models on natural language generation tasks.

### Open Question 3
- Question: Can the symmetry property of positional encodings be modified to improve the sensitivity of language models to global swaps of semantic roles?
- Basis in paper: [explicit] The authors identify a potential flaw of symmetry: insensitivity to global swaps of semantic roles. They introduce two new word swap probing tasks to reveal this weakness.
- Why unresolved: The authors do not provide a solution to address the flaw of symmetry and improve the sensitivity of language models to global swaps of semantic roles.
- What evidence would resolve it: Developing and evaluating new positional encoding methods that incorporate mechanisms to enhance the sensitivity of language models to global swaps of semantic roles.

## Limitations

- The analysis focuses solely on BERT-style models and does not investigate decoder-only architectures like GPT
- The study is limited to English, leaving questions about how these properties generalize to languages with different word order characteristics
- The exact mechanisms driving the convergence of learned positional encodings toward high locality and symmetry during pre-training remain incompletely characterized

## Confidence

- **High confidence**: The empirical observations about locality and symmetry metrics, and their basic correlation with downstream performance on the tested tasks
- **Medium confidence**: The claims about positional encodings serving syntactic-level processing while contextual encodings handle semantic-level processing, as this requires additional validation across diverse linguistic phenomena
- **Medium confidence**: The mechanistic explanation that MLM objectives naturally induce locality and symmetry, as this is inferred rather than directly demonstrated

## Next Checks

1. **Cross-linguistic validation**: Test whether the locality and symmetry properties hold across typologically diverse languages, particularly those with flexible word order or different dependency structures.

2. **Architectural generalization**: Evaluate whether the identified properties transfer to other transformer architectures beyond BERT, such as GPT-style causal transformers or modern decoder-only models.

3. **Fine-tuning stability analysis**: Investigate how different levels of locality and symmetry affect fine-tuning stability and catastrophic forgetting across multiple downstream tasks.