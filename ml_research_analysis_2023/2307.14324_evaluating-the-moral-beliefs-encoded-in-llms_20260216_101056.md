---
ver: rpa2
title: Evaluating the Moral Beliefs Encoded in LLMs
arxiv_id: '2307.14324'
source_url: https://arxiv.org/abs/2307.14324
tags:
- action
- scenarios
- question
- llms
- moral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops a statistical framework to measure moral beliefs
  encoded in large language models by analyzing their choices in moral scenarios across
  different question forms. The key innovation is using marginal action likelihood
  and question-form consistency metrics to assess model preferences while accounting
  for sensitivity to prompt variations.
---

# Evaluating the Moral Beliefs Encoded in LLMs

## Quick Facts
- arXiv ID: 2307.14324
- Source URL: https://arxiv.org/abs/2307.14324
- Reference count: 40
- Key outcome: Statistical framework using marginal action likelihood and question-form consistency metrics to measure moral beliefs in 28 LLMs across 1,367 moral scenarios

## Executive Summary
This study develops a framework to measure moral beliefs encoded in large language models by analyzing their choices in moral scenarios across different question forms. The authors introduce marginal action likelihood and question-form consistency metrics to assess model preferences while accounting for sensitivity to prompt variations. When tested on 28 models, most aligned with commonsense in unambiguous situations but expressed uncertainty in ambiguous cases. A notable finding was that closed-source models (GPT-4, Claude, PaLM 2) showed stronger agreement with each other than with open-source models, suggesting alignment fine-tuning may create shared moral preferences.

## Method Summary
The framework samples M responses per question form using temperature 1, maps token sequences to actions via rule-based matching, and aggregates across forms to compute marginal metrics. The authors use 1,767 moral scenarios (680 high-ambiguity, 687 low-ambiguity) and six question forms per scenario (A/B, Repeat, Compare with two action orderings each). They evaluate 28 open- and closed-source LLMs using marginal action likelihood, marginal action entropy, question-form consistency (QF-C), and average question-form-specific action entropy (QF-E).

## Key Results
- Most models aligned with commonsense in unambiguous scenarios but showed uncertainty in ambiguous cases
- Closed-source models (GPT-4, Claude, PaLM 2) showed stronger agreement with each other than with open-source models
- Model size and fine-tuning procedures significantly influence moral reasoning patterns
- Prompt sensitivity remains a key challenge in evaluating LLM moral beliefs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Marginal action likelihood aggregates model preferences across multiple question forms to measure moral beliefs while controlling for syntactic sensitivity
- Mechanism: By sampling M responses per question form and averaging across six variations, the metric smooths out idiosyncratic syntactic biases and captures underlying moral preference distribution
- Core assumption: Different question forms are semantically equivalent but syntactically varied enough to expose prompt sensitivity
- Evidence anchors: [abstract] "using marginal action likelihood and question-form consistency metrics to assess model preferences while accounting for sensitivity to prompt variations"

### Mechanism 2
- Claim: Question-form consistency (QF-C) measures model reliability by comparing action likelihood distributions across question variations
- Mechanism: Uses Generalized Jensen-Shannon Divergence to quantify similarity between conditional action distributions across question forms, with higher values indicating more consistent semantic reasoning
- Core assumption: Models that understand semantic equivalence should produce similar action distributions regardless of syntactic presentation
- Evidence anchors: [abstract] "we develop two evaluation metrics. The first one is the question-form consistency (QF-C) metric, which assesses the model's consistency to variations in question forms"

### Mechanism 3
- Claim: Action entropy quantifies moral uncertainty by measuring the spread of action likelihood distributions
- Mechanism: Calculates Shannon entropy of action likelihood vectors, with higher entropy indicating greater uncertainty about preferred moral actions
- Core assumption: Well-defined moral preferences should produce peaked distributions while ambiguous scenarios produce uniform distributions
- Evidence anchors: [abstract] "We use entropy [Mac03] and define action entropy and marginal action entropy. These measures assess the uncertainty of a choice given a question with a fixed question form or with a randomly selected question form"

## Foundational Learning

- Concept: Semantic equivalence classes and mapping
  - Why needed here: To convert token sequences into meaningful actions, the framework must group semantically equivalent responses regardless of surface form variations
  - Quick check question: If a model outputs "I would assist in suicide" vs "I would help with suicide", should these map to the same action class?

- Concept: Monte Carlo approximation for action likelihood
  - Why needed here: Direct computation of action likelihood requires summing over exponentially large token spaces, making sampling essential for practical implementation
  - Quick check question: If M=5 samples produce action probabilities [0.8, 0.2] and M=10 samples produce [0.75, 0.25], are these estimates converging appropriately?

- Concept: Jensen-Shannon Divergence for consistency measurement
  - Why needed here: Measures similarity between probability distributions while being symmetric and bounded, making it ideal for comparing action likelihood distributions across question forms
  - Quick check question: If two question forms produce identical action distributions, what should their JSD value be?

## Architecture Onboarding

- Component map: Survey generation → Question template creation → LLM sampling → Semantic mapping → Likelihood calculation → Consistency/entropy metrics → Clustering analysis
- Critical path: The sampling and semantic mapping stages are most critical - errors here propagate through all downstream metrics
- Design tradeoffs: Larger M improves estimate accuracy but increases computational cost; more question forms improve sensitivity detection but complicate analysis
- Failure signatures: High invalid answer rates (>1%), inconsistent action mappings across templates, entropy values that don't correlate with scenario ambiguity
- First 3 experiments:
  1. Validate semantic mapping by checking if known equivalent phrases map to same action classes
  2. Test convergence of Monte Carlo estimates by varying M and checking stability of action likelihoods
  3. Verify consistency metric by creating synthetic distributions with known similarities and checking QF-C outputs

## Open Questions the Paper Calls Out

- What specific aspects of alignment fine-tuning create shared moral preferences across closed-source models? The paper notes that closed-source models show stronger agreement with each other than with open-source models, and hypothesizes this may be due to alignment fine-tuning, but doesn't investigate the specific mechanisms or data used in the alignment process that might lead to convergent moral preferences.

- How do different question templates (A/B, Repeat, Compare) affect model consistency and uncertainty across different types of moral scenarios? The paper provides aggregate analysis but doesn't deeply explore how different templates interact with different moral scenario types or model architectures.

- What factors beyond model size explain why some smaller models (like Claude-v1.1) show strong moral preferences while others (like Bloomz-560M) show high uncertainty? The paper observes that model size correlates with confidence levels, but notes exceptions like smaller API models showing strong preferences while some small open-source models show high uncertainty.

## Limitations
- Semantic Mapping Ambiguity: The rule-based mapping from token sequences to action classes may introduce systematic biases, particularly for nuanced or context-dependent responses.
- Temperature Effects: Using temperature=1 for sampling may amplify stochastic variations in responses, potentially conflating true model uncertainty with sampling noise.
- Question Form Validity: The assumption that six question forms are semantically equivalent may not hold universally, as certain linguistic constructions could subtly shift moral framing.

## Confidence
- High Confidence: Core methodology (marginal action likelihood, entropy calculations) and basic findings about model alignment with commonsense in unambiguous scenarios
- Medium Confidence: Cross-model consistency patterns and clustering results, as these depend on the semantic mapping implementation
- Low Confidence: Causal interpretations linking alignment fine-tuning to specific moral preference patterns, due to limited visibility into training procedures

## Next Checks
1. Conduct inter-annotator agreement tests on a subset of 100 responses to verify that human raters consistently map token sequences to the same action classes
2. Repeat the full analysis using temperature=0.7 and temperature=1.3 to assess the stability of marginal action likelihood and consistency metrics
3. Perform controlled experiments where semantically equivalent phrases are embedded in different question forms to quantify the extent of form-induced preference shifts