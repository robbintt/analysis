---
ver: rpa2
title: On the Adversarial Robustness of Generative Autoencoders in the Latent Space
arxiv_id: '2307.02202'
source_url: https://arxiv.org/abs/2307.02202
tags:
- latent
- adversarial
- attack
- robustness
- autoencoders
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper empirically investigates the adversarial robustness
  of generative autoencoders in the latent space, an area not previously studied.
  The authors show that VAEs are vulnerable to attacks in the latent space through
  targeted and untargeted attack experiments on MNIST, FashionMNIST, and CelebA datasets.
---

# On the Adversarial Robustness of Generative Autoencoders in the Latent Space

## Quick Facts
- arXiv ID: 2307.02202
- Source URL: https://arxiv.org/abs/2307.02202
- Reference count: 40
- This paper empirically investigates the adversarial robustness of generative autoencoders in the latent space, an area not previously studied.

## Executive Summary
This paper presents the first comprehensive empirical study of adversarial robustness in the latent space of generative autoencoders. The authors systematically evaluate VAEs and deterministic autoencoders under adversarial attacks, revealing that VAEs are vulnerable due to their probabilistic structure while deterministic autoencoders show greater robustness. The study also uncovers a trade-off between disentanglement and robustness, suggesting that more disentangled representations are more susceptible to adversarial perturbations.

## Method Summary
The authors train VAE and deterministic autoencoder models on MNIST, FashionMNIST, and CelebA datasets with 4-layer convolutional architectures. They implement PGD attacks in the latent space with perturbation bounds of [0,1] for MNIST/FashionMNIST and [0,2] for CelebA. The evaluation uses SSIM and LPIPS metrics to measure reconstruction quality degradation, with AADDC scores quantifying overall robustness. The study compares different regularization methods (KL, MMD, SWT) and analyzes the disentanglement-robustness trade-off using β-TCVAE with varying β values.

## Key Results
- VAEs are vulnerable to adversarial attacks in the latent space, showing significant reconstruction quality degradation
- Deterministic autoencoders demonstrate greater latent robustness compared to VAEs
- Increased disentanglement (higher β values) reduces latent robustness, revealing a fundamental trade-off
- Adversarial training can improve the latent robustness of VAEs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VAEs are vulnerable to adversarial attacks in the latent space because their probabilistic structure creates regions in the latent space where reconstruction quality deteriorates sharply.
- Mechanism: The KL divergence regularization in VAEs enforces a smooth prior distribution (typically Gaussian) on the latent space, but this global structure can mismatch the true data manifold, creating "holes" or valleys where small perturbations in latent variables lead to large reconstruction errors.
- Core assumption: The adversarial attack can find directions in the latent space that exploit these mismatched regions to degrade reconstruction quality.
- Evidence anchors:
  - [abstract] "Due to the probabilistic latent structure, variational autoencoders (VAEs) may confront problems such as a mismatch between the posterior distribution of the latent and real data manifold, or discontinuity in the posterior distribution of the latent."
  - [section] "insufficiency of the training data may cause holes or valleys in the latent space [13, 14], as illustrated in Figure 1a, from where sampling a latent may lead to bad or even invalid reconstruction or generation."
  - [corpus] Weak evidence - no direct corpus papers address latent space vulnerability specifically, though related adversarial robustness papers exist.
- Break condition: If the latent space regularization perfectly matches the true data manifold, or if the attack magnitude is constrained below the size of these vulnerable regions.

### Mechanism 2
- Claim: Deterministic autoencoders (DAEs) are more robust to latent space attacks than VAEs because they lack the probabilistic structure that creates vulnerable regions.
- Mechanism: DAEs learn a deterministic mapping from input to latent space and back, avoiding the KL divergence regularization that can create mismatched regions. This makes them less susceptible to small perturbations causing large reconstruction errors.
- Core assumption: The deterministic structure provides more stable reconstruction across the latent space compared to the probabilistic VAE structure.
- Evidence anchors:
  - [section] "we identify a potential trade-off between the adversarial robustness and the degree of the disentanglement of the latent codes. Comparison attack experiments with different β are conducted to further reveal the mystery of whether there are trade-offs among the reconstruction accuracy, disentangling strength, and latent robustness."
  - [section] "experiments are conducted to investigate the difference in adversarial latent robustness between VAEs and DAEs. This involves a key question: whether VAE or DAE is more robust to attacks and potential for safe practical applications."
  - [corpus] Weak evidence - no direct corpus papers address DAE vs VAE robustness comparison in latent space.
- Break condition: If the deterministic mapping itself has regions of instability or if the training data is insufficient to learn a stable mapping.

### Mechanism 3
- Claim: Increased disentanglement in VAEs reduces latent robustness because it creates more sensitive directions in the latent space.
- Mechanism: Disentanglement methods like β-VAE encourage the latent space to separate different factors of variation, but this separation can make certain dimensions more sensitive to perturbations, increasing vulnerability to adversarial attacks.
- Core assumption: The trade-off between disentanglement and robustness exists because disentangled representations create more interpretable but also more attack-sensitive dimensions.
- Evidence anchors:
  - [section] "we identify a potential trade-off between the adversarial robustness and the degree of the disentanglement of the latent codes."
  - [section] "Comparison attack experiments with different β are conducted to further reveal the mystery of whether there are trade-offs among the reconstruction accuracy, disentangling strength, and latent robustness."
  - [corpus] Weak evidence - no direct corpus papers address the trade-off between disentanglement and adversarial robustness.
- Break condition: If the disentanglement is achieved in a way that doesn't create sensitive directions, or if the attack cannot exploit the separated dimensions effectively.

## Foundational Learning

- Concept: KL divergence regularization in VAEs
  - Why needed here: Understanding how the KL divergence term creates the global structure in latent space that can mismatch the true data manifold
  - Quick check question: What happens to the KL divergence term when the encoder posterior q(z|x) perfectly matches the prior p(z)?

- Concept: Adversarial attack optimization (PGD)
  - Why needed here: Understanding how the Projected Gradient Descent method finds adversarial perturbations in the latent space
  - Quick check question: What role does the constraint ε play in the PGD attack formulation?

- Concept: Disentanglement in latent representations
  - Why needed here: Understanding how methods like β-VAE encourage separation of factors of variation and why this might affect robustness
  - Quick check question: How does increasing β in β-VAE affect the trade-off between reconstruction accuracy and disentanglement?

## Architecture Onboarding

- Component map:
  - Encoder network: Maps input to latent space parameters [µ, σ]
  - Sampler: Applies reparameterization trick to sample z from the posterior
  - Decoder network: Maps latent z back to reconstruction
  - Regularization terms: KL divergence, MMD, or SWT for prior fitting
  - Attack module: PGD optimization for generating adversarial latents

- Critical path:
  1. Encode input to obtain latent parameters
  2. Sample latent representation using reparameterization
  3. Decode to reconstruction
  4. Apply adversarial attack in latent space
  5. Measure reconstruction quality degradation

- Design tradeoffs:
  - Deterministic vs probabilistic latent structure: Trade-off between generative capability and robustness
  - Disentanglement strength (β parameter): Trade-off between interpretability and robustness
  - Regularization type (KL, MMD, SWT): Trade-off between flexibility and stability

- Failure signatures:
  - High LPIPS scores with low SSIM scores indicate reconstruction quality degradation
  - T-SNE visualization showing adversarial latents clustering differently from original latents
  - Dimension-wise mean absolute difference showing concentrated changes in specific latent dimensions

- First 3 experiments:
  1. Un-targeted attack on VAE: Apply PGD to generate adversarial latents and measure reconstruction quality degradation
  2. DAE vs VAE robustness comparison: Apply same attack to both and compare LPIPS/SSIM scores
  3. Disentanglement vs robustness trade-off: Vary β parameter in β-VAE and measure both disentanglement metrics and attack robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is there a theoretical trade-off between latent robustness and generation diversity in VAEs?
- Basis in paper: [explicit] The paper states "Latent robust generative autoencoders tend to generate more homogenized samples" and suggests this as an important direction for further research.
- Why unresolved: The paper only hints at this trade-off and does not provide theoretical analysis or empirical evidence to support or refute this claim.
- What evidence would resolve it: Empirical studies comparing generation diversity (e.g., using metrics like LPIPS, IS) across VAEs with varying levels of latent robustness, along with theoretical analysis of the relationship between robustness and diversity.

### Open Question 2
- Question: How does the adversarial robustness of VAEs compare to other generative models like GANs in the latent space?
- Basis in paper: [inferred] The paper focuses on VAEs and deterministic autoencoders but does not compare their latent robustness to other generative models like GANs.
- Why unresolved: The paper's experiments are limited to VAEs and deterministic autoencoders, leaving a gap in understanding how VAEs' latent robustness compares to other generative models.
- What evidence would resolve it: Empirical studies attacking the latent space of GANs and other generative models, comparing their robustness to VAEs using similar metrics and attack methods.

### Open Question 3
- Question: Can the findings on latent robustness be extended to other domains beyond computer vision, such as natural language processing?
- Basis in paper: [explicit] The paper suggests extending the investigation of latent robustness to other research areas like natural language processing.
- Why unresolved: The paper's experiments are limited to computer vision datasets, and the framework and methodology may need adaptation for other domains.
- What evidence would resolve it: Applying the attack methods and robustness evaluation to generative models in other domains (e.g., NLP) and comparing the results to those obtained in the computer vision experiments.

## Limitations
- The analysis focuses on image datasets (MNIST, FashionMNIST, CelebA) and may not generalize to other data modalities
- The adversarial attack evaluation uses relatively simple PGD methods in latent space without exploring more sophisticated attacks
- The disentanglement analysis relies on β-VAE variants and may not capture the full spectrum of disentanglement approaches

## Confidence
- **Confidence: Low** for the claimed trade-off between disentanglement and robustness. While the paper presents experimental evidence, the theoretical mechanism connecting increased disentanglement to reduced robustness remains unclear.
- **Confidence: Medium** for the comparison between VAEs and DAEs. The paper shows DAEs are more robust in experiments, but the difference may be due to factors beyond the probabilistic vs deterministic structure.
- **Confidence: Medium** for the effectiveness of adversarial training. While the paper demonstrates improved robustness with adversarial training, the magnitude of improvement and whether it generalizes beyond the tested datasets is uncertain.

## Next Checks
1. **Theoretical analysis**: Develop a mathematical framework explaining why increased disentanglement leads to reduced robustness, moving beyond empirical correlation to establish causal mechanisms
2. **Cross-domain validation**: Test the robustness claims on non-image datasets (text, audio, or structured data) to assess generalizability
3. **Attack sophistication comparison**: Implement and compare stronger adversarial attacks (e.g., Carlini-Wagner style) to verify that the observed robustness differences hold under more sophisticated threat models