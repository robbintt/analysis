---
ver: rpa2
title: Learning to Rank Context for Named Entity Recognition Using a Synthetic Dataset
arxiv_id: '2310.10118'
source_url: https://arxiv.org/abs/2310.10118
tags:
- context
- retrieval
- sentences
- dataset
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose to generate a synthetic context retrieval dataset using
  an instruction-tuned large language model (LLM) and train a neural context retriever
  on this dataset for the named entity recognition (NER) task. Our method outperforms
  several unsupervised retrieval baselines for NER on an English literary dataset,
  achieving around 1 F1 point gain over a raw NER model.
---

# Learning to Rank Context for Named Entity Recognition Using a Synthetic Dataset

## Quick Facts
- arXiv ID: 2310.10118
- Source URL: https://arxiv.org/abs/2310.10118
- Reference count: 2
- Key outcome: Neural context retriever trained on synthetic data achieves ~1 F1 point gain over raw NER model on literary dataset

## Executive Summary
This paper addresses the challenge of context retrieval for Named Entity Recognition (NER) in long documents where entities may be ambiguous without broader context. The authors propose generating a synthetic context retrieval dataset using an instruction-tuned LLM (Alpaca) to create labeled examples of relevant and irrelevant context sentences. They train a neural BERT-based re-ranker on this synthetic data and use it to improve NER performance by retrieving and augmenting input sentences with relevant context from the full document. The method shows modest but consistent improvements over unsupervised baselines.

## Method Summary
The authors generate synthetic context retrieval training data by prompting an instruction-tuned LLM to create relevant and irrelevant context sentences for given input sentences and entities. They train a BERT-based neural re-ranker on this dataset to score the relevance of candidate contexts. For NER, they first retrieve candidate contexts using simple heuristics (BM25, samenoun, surrounding sentences), then re-rank these candidates using the trained neural retriever, and finally augment the input sentence with the top-k contexts before passing to the NER model.

## Key Results
- Neural context retriever trained on synthetic data achieves ~1 F1 point gain over raw NER model on literary dataset
- Performance is robust to the number of candidate contexts retrieved (4n) and model size (7B vs 13B)
- Context retrieval from full document outperforms chapter-level retrieval for NER disambiguation
- Method outperforms BM25 and samenoun unsupervised baselines for NER context retrieval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using instruction-tuned LLMs to generate synthetic training data for context retrieval enables supervised training despite the absence of real-world labeled datasets.
- Mechanism: The LLM is prompted with input sentences and entity mentions to generate context sentences that describe or relate to the entity, producing labeled (sentence, context, relevance) tuples.
- Core assumption: The LLM can generate high-quality, relevant context sentences that reflect the true usefulness of context for NER.
- Evidence anchors:
  - [abstract] "generate a synthetic context retrieval training dataset using Alpaca, an instruction-tuned large language model (LLM)"
  - [section] "We define a context retrieval dataset as a set of 3-tuples (si, sj, y), where... We design a prompt for each of these types of sentences that, given an input sentence and an entity, can instruct Alpaca to generate a sentence of this type."
  - [corpus] Weak evidence: the corpus lists several related papers on synthetic data for NER, but none directly confirm LLM-generated context quality.

### Mechanism 2
- Claim: A neural re-ranker trained on the synthetic dataset outperforms unsupervised heuristics by learning to score relevance more effectively.
- Mechanism: The re-ranker uses a BERT encoder to compute relevance scores for candidate contexts, then selects the top-k to augment the input sentence before NER prediction.
- Core assumption: The learned relevance scoring generalizes beyond the synthetic data to real documents.
- Evidence anchors:
  - [abstract] "Using this dataset, we train a neural context retriever based on a BERT model that is able to find relevant context for NER. We show that our method outperforms several retrieval baselines for the NER task"
  - [section] "For a given sentence and a candidate context, the retriever outputs the estimated relevance of the candidate context between 0 and 1."
  - [corpus] Weak evidence: no direct corpus support for neural re-ranker superiority in NER context retrieval.

### Mechanism 3
- Claim: Context retrieval from the full document (rather than just the current chapter) improves NER performance because relevant context may appear anywhere in the document.
- Mechanism: The retriever searches the entire novel for sentences related to entities in the input sentence, increasing the chance of finding disambiguating context.
- Core assumption: Longer context windows contain more useful information for entity disambiguation.
- Evidence anchors:
  - [section] "We use our neural context retriever as a re-ranker... we first retrieve some candidate contexts using simple heuristics... then compute their estimated relevance with respect to the input sentence using our neural context retriever"
  - [section] "Figure 7 shows the performance of the bm25, samenoun and neural alpaca-7b configurations depending on the context window."
  - [corpus] Weak evidence: no direct corpus support for full-document retrieval benefit.

## Foundational Learning

- Concept: Instruction-tuned LLMs can generate task-specific synthetic data when prompted appropriately.
  - Why needed here: We lack labeled context retrieval datasets for NER, so synthetic data generation is the only feasible way to create supervision.
  - Quick check question: What types of prompts would you design to generate positive and negative context retrieval examples?

- Concept: BERT-based neural re-rankers can learn to score relevance of context sentences given a target sentence.
  - Why needed here: Simple heuristics like BM25 or noun overlap are insufficient for capturing nuanced relevance for NER disambiguation.
  - Quick check question: How would you structure the input and output for a BERT model to perform context relevance ranking?

- Concept: Context window size impacts retrieval effectiveness and computational cost.
  - Why needed here: Retrieving from too small a window may miss useful context; too large increases cost and noise.
  - Quick check question: What trade-offs arise when expanding the context window from a chapter to the full document?

## Architecture Onboarding

- Component map: Input sentence -> Candidate retrieval (BM25, samenoun, surrounding) -> Neural re-ranker (BERT + regression) -> Top-k contexts -> NER model

- Critical path:
  1. Retrieve candidate contexts using heuristics
  2. Score candidates with neural re-ranker
  3. Select top-k and augment input
  4. Run NER model on augmented input

- Design tradeoffs:
  - Number of candidates (4n) vs. retrieval cost vs. recall
  - Size of LLM for synthetic data (7B vs 13B) vs. dataset quality
  - Context window size vs. computational load vs. relevance

- Failure signatures:
  - Low precision: re-ranker scores irrelevant contexts highly
  - Low recall: too few candidates retrieved or top-k too small
  - Overfitting: performance good on synthetic data but poor on real data

- First 3 experiments:
  1. Vary n (candidates per heuristic) to find sweet spot between recall and cost
  2. Compare re-ranker vs. heuristics alone on a validation set
  3. Test synthetic data quality by training re-ranker on different proportions of synthetic vs. real data (if available)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance gain of the neural context retriever over unsupervised methods vary significantly across different genres of literature?
- Basis in paper: [inferred] The paper tests on a dataset of 40 novels but does not analyze performance by genre or systematically vary the literary content.
- Why unresolved: The study uses a homogeneous dataset of literary novels without genre-specific analysis, leaving open the question of whether the method generalizes across different types of text.
- What evidence would resolve it: Testing the neural context retriever on diverse text genres (e.g., scientific papers, news articles, social media) and comparing performance gains relative to unsupervised methods.

### Open Question 2
- Question: How does the synthetic context retrieval dataset generation process scale when applied to less common entity types or domains with limited pre-training data?
- Basis in paper: [explicit] The paper acknowledges that the Alpaca model used for dataset generation may have biases and limited knowledge of certain entities, particularly in specialized domains.
- Why unresolved: The study focuses on general literary entities (person, location, organization) and does not explore the challenges of generating relevant context for domain-specific or rare entity types.
- What evidence would resolve it: Experiments generating synthetic datasets for specialized domains (e.g., biomedical, legal) and evaluating the quality and diversity of generated contexts.

### Open Question 3
- Question: Would incorporating interactions between candidate contexts during retrieval improve the neural context retriever's performance?
- Basis in paper: [explicit] The paper identifies as a limitation that the current neural retriever estimates relevance separately for each candidate context, potentially leading to redundancy or missed disambiguation opportunities.
- Why unresolved: The proposed method does not implement iterative context selection or consider context-context interactions, leaving the potential benefits of such approaches unexplored.
- What evidence would resolve it: Implementing an iterative retrieval approach where contexts are added sequentially and re-ranking remaining candidates, then comparing performance to the non-iterative method.

## Limitations
- The synthetic context generation quality is uncertain and may not reflect truly useful contexts for NER
- Limited comparison with sophisticated unsupervised or dense retrieval methods
- Claims about model size having little influence are based on limited experimentation
- No validation of generalization to domains beyond literary texts

## Confidence
- High confidence: The basic approach of using a neural re-ranker to improve context retrieval for NER is technically sound and the implementation details (BERT encoder, relevance scoring) are well-specified.
- Medium confidence: The synthetic data generation approach using instruction-tuned LLMs is feasible and can produce usable training data, though quality validation is limited.
- Low confidence: The claim that full-document retrieval significantly improves NER performance over chapter-level retrieval is weakly supported and may be task-specific.
- Medium confidence: The observation that the number of candidates and model size have little influence on final performance, though this requires more extensive validation.

## Next Checks
1. **Synthetic data quality validation**: Manually evaluate a sample of the synthetic context sentences to verify they are truly relevant and useful for NER disambiguation. This would involve human annotators scoring the relevance of generated contexts for actual entity mentions in the literary corpus.

2. **Generalization experiment**: Test the trained neural re-ranker on a held-out set of real documents (not used in synthetic data generation) to measure whether the relevance scoring generalizes beyond the synthetic distribution. This would reveal potential overfitting to the synthetic dataset.

3. **Ablation study on context window size**: Systematically vary the context window from chapter-level to full-document retrieval while keeping all other components constant, measuring both NER performance and computational cost. This would provide stronger evidence for the claimed benefits of full-document retrieval.