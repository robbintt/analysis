---
ver: rpa2
title: Reinforcement Learning Friendly Vision-Language Model for Minecraft
arxiv_id: '2303.10571'
source_url: https://arxiv.org/abs/2303.10571
tags:
- video
- tasks
- agent
- clip4mc
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CLIP4MC, a vision-language model designed to
  serve as an intrinsic reward function for reinforcement learning in Minecraft. The
  method incorporates a motion encoder to capture action-level information from video
  snippets, addressing limitations of existing models that only capture entity-level
  similarity.
---

# Reinforcement Learning Friendly Vision-Language Model for Minecraft

## Quick Facts
- arXiv ID: 2303.10571
- Source URL: https://arxiv.org/abs/2303.10571
- Reference count: 40
- Key outcome: CLIP4MC achieves higher success rates on MineDojo programmatic tasks by incorporating motion encoders for action-level similarity, outperforming baselines including MineCLIP.

## Executive Summary
This paper proposes CLIP4MC, a vision-language model designed specifically to serve as an intrinsic reward function for reinforcement learning in Minecraft. The key innovation is incorporating a motion encoder that captures action-level information from video snippets, addressing limitations of existing models that only capture entity-level similarity. The authors construct a filtered YouTube dataset containing 640K video-text pairs and demonstrate that CLIP4MC achieves higher success rates on MineDojo's programmatic tasks compared to baseline methods.

## Method Summary
CLIP4MC is trained using a contrastive learning framework that aligns video, motion, and text embeddings. The model processes video snippets through both a video encoder (for entity-level features) and a motion encoder (for action-level features extracted from frame differences at multiple temporal intervals). The training objective combines video-to-text, text-to-video, motion-to-text, and text-to-motion contrastive losses. The YouTube dataset is filtered in two rounds to ensure relevance and correlation, first for coverage of essential Minecraft entities and then for high cosine similarity between video and text embeddings using a pre-trained MineCLIP model.

## Key Results
- CLIP4MC achieves higher success rates on MineDojo programmatic tasks compared to MineCLIP and a variant without the motion encoder
- The motion encoder captures action-level similarity between video snippets and language prompts, making the reward signal more instructive for RL agents
- Two rounds of filtering operations guarantee high correlation between video and text pairs while maintaining coverage of essential Minecraft information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The motion encoder captures action-level similarity between video snippets and language prompts, making the reward signal more instructive for RL agents.
- Mechanism: By stacking frame features with different intervals, the model creates action representations that capture motion information at various amplitudes. The temporal transformer then processes these representations to generate motion embeddings that, when combined with video embeddings, provide a more comprehensive similarity measure.
- Core assumption: Actions are implicitly encoded in the differences between frames at various temporal intervals.
- Evidence anchors:
  - [abstract] "To achieve RL-friendliness, we incorporate the task completion degree into the VLM training objective, as this information can assist agents in distinguishing the importance between different states."
  - [section] "Intuitively, the atomic actions are captured in the difference between two adjacent frames. A sequence of such actions corresponds to the behavior described in the language prompt."

### Mechanism 2
- Claim: The filtered YouTube dataset ensures high correlation between video and text pairs, improving the quality of learned embeddings.
- Mechanism: Two rounds of filtering operations are performed - first to ensure coverage of essential Minecraft entities and semantic events, then to select video-text pairs with high cosine similarity in embedding space using a pre-trained MineCLIP model.
- Core assumption: Higher correlation between video and text pairs during training leads to better aligned embeddings and more reliable reward signals.
- Evidence anchors:
  - [abstract] "two rounds of filtering operations guarantee that the dataset covers enough essential information and that the video-text pair is highly correlated."
  - [section] "we utilize the pre-trained MineCLIP model to obtain the embeddings of selected video clips and transcript clips. Then, we calculate the cosine similarity distribution of their embeddings to represent their correlation."

### Mechanism 3
- Claim: The contrastive learning framework with both video-to-text and motion-to-text losses improves the RL-friendliness of the model.
- Mechanism: By training with multiple contrastive losses (video-to-text, text-to-video, motion-to-text, and text-to-motion), the model learns to align both entity-level and action-level information between modalities, creating a more informative reward signal for RL agents.
- Core assumption: Incorporating motion information as a separate modality in contrastive learning improves the model's ability to distinguish between different states and actions.
- Evidence anchors:
  - [section] "we aim to minimize the sum of the multi-modal contrastive losses, including video-to-text, text-to-video, motion-to-text, and text-to-motion."
  - [section] "CLIP4MC achieves the best performance on RL tasks, but CLIP4MC-simple obtained the best R@1 value. Through the model analysis in Section 6.3, we provide one of the possible answers."

## Foundational Learning

- Concept: Contrastive learning for cross-modal embeddings
  - Why needed here: The core approach relies on learning aligned embeddings between video and text modalities to measure similarity as a reward signal.
  - Quick check question: Can you explain how the NCE loss works in the context of video-text retrieval?

- Concept: Reinforcement learning with learned reward functions
  - Why needed here: The paper proposes using the vision-language model as an intrinsic reward function for RL training, eliminating the need for manually designed rewards.
  - Quick check question: How does using a learned reward function differ from using a manually designed reward function in RL?

- Concept: Video encoding and temporal modeling
  - Why needed here: The model needs to extract meaningful features from video snippets, including both spatial (entity) and temporal (action) information.
  - Quick check question: What is the difference between the video encoder and motion encoder in terms of what they capture from the video input?

## Architecture Onboarding

- Component map: Video snippet → Video Encoder → Entity embeddings; Video snippet → Motion Encoder → Motion embeddings; Text prompt → Text Encoder → Text embeddings → Similarity Calculator → Reward signal

- Critical path: Video/Motion/Text encoders → Similarity Calculator → Reward generation for RL agent

- Design tradeoffs:
  - Using pre-trained CLIP weights vs. training from scratch: Pre-trained weights provide better initialization but may not be optimal for Minecraft-specific tasks
  - Single interval vs. multi-interval motion encoding: Multi-interval captures more comprehensive motion information but increases computational cost
  - Correlation filtering vs. dataset size: Higher correlation improves quality but reduces dataset size

- Failure signatures:
  - Low RL performance despite high video-text retrieval performance: Indicates the model is good at retrieval but not at providing RL-friendly rewards
  - Unstable learning curves in RL tasks: Suggests the reward signal is not sufficiently informative or consistent
  - Poor performance on tasks with specific entities not well-represented in training data: Indicates dataset coverage issues

- First 3 experiments:
  1. Train CLIP4MC on the filtered YouTube dataset and evaluate video-text retrieval performance (R@1, R@5, R@10 metrics)
  2. Compare RL performance of CLIP4MC vs. MineCLIP(pre-trained) on Harvest tasks using the same RL training setup
  3. Ablation study: Train CLIP4MC-single (single interval motion encoder) and compare RL performance to full CLIP4MC model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CLIP4MC compare to other state-of-the-art video-language models, such as CLIP4CLIP and HiT, on RL tasks?
- Basis in paper: [inferred] The paper mentions that CLIP4MC outperforms MineCLIP(pre-trained) and MineCLIP(scratch) on RL tasks, but does not compare it to other video-language models.
- Why unresolved: The paper does not provide a direct comparison between CLIP4MC and other video-language models on RL tasks.
- What evidence would resolve it: Experimental results showing the performance of CLIP4MC and other video-language models on the same RL tasks.

### Open Question 2
- Question: How does the performance of CLIP4MC vary with different values of the hyperparameters, such as the number of intervals in the motion encoder and the weight coefficient λ in the contrastive loss?
- Basis in paper: [inferred] The paper mentions that the motion encoder processes motion representations with different intervals and that λ is a weight coefficient in the contrastive loss, but does not provide a detailed analysis of how these hyperparameters affect the performance.
- Why unresolved: The paper does not provide a sensitivity analysis of the hyperparameters.
- What evidence would resolve it: Experimental results showing the performance of CLIP4MC with different values of the hyperparameters.

### Open Question 3
- Question: How does the performance of CLIP4MC vary with different sizes of the training dataset?
- Basis in paper: [inferred] The paper mentions that the proposed method is trained on a YouTube dataset with 640K video-text pairs, but does not provide a detailed analysis of how the size of the training dataset affects the performance.
- Why unresolved: The paper does not provide a study on the impact of the size of the training dataset on the performance.
- What evidence would resolve it: Experimental results showing the performance of CLIP4MC with different sizes of the training dataset.

## Limitations

- The filtering methodology relies on pre-trained MineCLIP, which may introduce bias and limit the diversity of the training dataset
- The RL performance improvements are demonstrated only on specific MineDojo tasks, with limited evidence of generalization to other environments
- The paper does not provide detailed hyperparameter sensitivity analysis or ablation studies for the motion encoder architecture

## Confidence

- **High Confidence**: The basic architecture of combining video, motion, and text encoders with contrastive learning is technically sound and well-established in the literature
- **Medium Confidence**: The claim that incorporating motion information improves RL performance is supported by experimental results, but the effect size and generality across different task types remain uncertain
- **Low Confidence**: The assertion that the model provides more "instructive" reward signals is somewhat qualitative and would benefit from more rigorous analysis of the reward signal quality

## Next Checks

1. **Ablation study with different motion encoder configurations**: Test whether the multi-interval approach is necessary by comparing against single-interval motion encoding and frame-difference baselines to isolate the specific contribution of temporal multi-resolution processing.

2. **Cross-dataset generalization test**: Evaluate CLIP4MC on MineDojo tasks using video-text pairs from a different source (not filtered by MineCLIP) to assess whether the model's RL-friendliness is dataset-dependent or represents a genuine architectural improvement.

3. **Reward signal analysis**: Conduct a detailed analysis of the learned reward signals by visualizing similarity distributions for successful vs. unsuccessful trajectories, and measure whether the motion encoder produces more discriminative signals for RL-relevant state distinctions.