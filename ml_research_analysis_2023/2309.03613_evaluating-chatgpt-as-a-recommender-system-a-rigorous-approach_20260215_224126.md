---
ver: rpa2
title: 'Evaluating ChatGPT as a Recommender System: A Rigorous Approach'
arxiv_id: '2309.03613'
source_url: https://arxiv.org/abs/2309.03613
tags:
- chatgpt
- cuto
- items
- recommendation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study presents the first comprehensive evaluation of ChatGPT''s
  ability to act as a recommender system using a rigorous pipeline. The authors compare
  ChatGPT-3.5 and ChatGPT-4 against state-of-the-art baselines and other large language
  models on three datasets across three domains: movies, music, and books.'
---

# Evaluating ChatGPT as a Recommender System: A Rigorous Approach

## Quick Facts
- arXiv ID: 2309.03613
- Source URL: https://arxiv.org/abs/2309.03613
- Reference count: 40
- Key outcome: First comprehensive evaluation of ChatGPT as a recommender system across three domains, showing domain-specific performance and hybrid recommendation behavior

## Executive Summary
This study presents the first comprehensive evaluation of ChatGPT's ability to act as a recommender system using a rigorous pipeline. The authors compare ChatGPT-3.5 and ChatGPT-4 against state-of-the-art baselines and other large language models on three datasets across three domains: movies, music, and books. They evaluate accuracy, diversity, novelty, and bias metrics. Key findings include ChatGPT's superior accuracy on books, strong performance in re-ranking and cold-start scenarios, and ability to maintain reasonable beyond-accuracy metrics. ChatGPT exhibits hybrid recommender behavior, leveraging both collaborative and content-based information. The evaluation pipeline is publicly released for future research.

## Method Summary
The study uses a zero-shot prompting approach with a "persona pattern" to guide ChatGPT to act as a recommender system, avoiding complex prompt engineering to focus on its inherent capabilities. The evaluation pipeline processes user profiles containing liked items, generates recommendations via ChatGPT API calls, matches generated items to dataset items using Damerau-Levenshtein distance, and computes metrics using the Elliot framework. The study compares ChatGPT-3.5 and ChatGPT-4 against state-of-the-art baselines (EASE R, RP 3β, UserKNN, ItemKNN) and other LLMs (GPT-3.5, PaLM-2) across three datasets: MovieLens (movies), Last.FM (music), and Facebook Books (books).

## Key Results
- ChatGPT exhibits higher accuracy than baselines on books domain, outperforming other models for cut-off values of 10, 20, and 50-item recommendations
- ChatGPT excels in re-ranking and cold-start scenarios, showing strong performance when provided with a fixed list of popular items or minimal user interaction history
- ChatGPT demonstrates hybrid recommender behavior, with recommendations more similar to collaborative-based models than purely content-based approaches

## Why This Works (Mechanism)

### Mechanism 1
ChatGPT can function as a recommender system without fine-tuning by leveraging its pre-trained knowledge and a simple prompt-based approach. The study uses a "persona pattern" prompt to guide ChatGPT to act as a recommender system, avoiding complex prompt engineering and instead focusing on its inherent capabilities. Core assumption: ChatGPT's pre-training on vast amounts of text data includes sufficient information about items and user preferences to generate meaningful recommendations. Evidence anchors: [abstract] states: "we propose a robust evaluation pipeline to assess ChatGPT's ability as an RS and post-process ChatGPT recommendations to account for these aspects." [section 3] explains: "ChatGPT is explicitly designed to excel in user-oriented tasks... its vanilla Large Language Model (LLM) is naturally more inclined to address user-related assignments, such as item recommendation."

### Mechanism 2
ChatGPT's performance varies significantly across different recommendation domains (movies, music, books). The study evaluates ChatGPT across three distinct datasets, revealing that its accuracy and effectiveness depend on the domain's characteristics and the availability of relevant training data. Core assumption: Different domains have varying amounts of information in ChatGPT's training corpus, affecting its ability to generate relevant recommendations. Evidence anchors: [abstract] mentions: "ChatGPT exhibits higher accuracy than the baselines on books domain. It also excels in re-ranking and cold-start scenarios." [section 5] shows: "ChatGPT consistently outperforms other models for cut-off values of 10, 20, and in most cases, 50-item recommendations" specifically for books, while showing mixed results for movies and music.

### Mechanism 3
ChatGPT demonstrates hybrid recommender behavior, leveraging both collaborative and content-based information. Through similarity analysis using Jaccard and Kendall metrics, the study finds ChatGPT's recommendations align more closely with hybrid and collaborative models than purely content-based ones. Core assumption: ChatGPT can implicitly combine different recommendation strategies based on the input context and its pre-trained knowledge. Evidence anchors: [abstract] states: "Furthermore, we measure the similarity between the ChatGPT recommendations and the other recommenders, providing insights about how ChatGPT could be categorized in the realm of recommender systems." [section 5.4] shows: "ChatGPT's behavior aligns more closely with collaborative-based models like EASE R, RP 3β, AttributeUserKNN (a hybrid model), and UserKNN."

## Foundational Learning

- Concept: Prompt Engineering and Prompt Learning
  - Why needed here: The study explicitly chooses to avoid prompt engineering to evaluate ChatGPT's vanilla capabilities, making it essential to understand what prompt engineering involves and why it was deliberately excluded.
  - Quick check question: What is the difference between prompt learning and traditional fine-tuning in the context of large language models?

- Concept: Recommendation System Evaluation Metrics
  - Why needed here: The study uses a comprehensive set of metrics including accuracy, diversity, novelty, and bias measures, requiring understanding of what each metric captures and how they complement each other.
  - Quick check question: How do ItemCoverage and Gini index differ in measuring recommendation diversity?

- Concept: Cold Start Problem in Recommender Systems
  - Why needed here: The study specifically evaluates ChatGPT's performance in cold-start scenarios, making it crucial to understand what constitutes a cold-start user and why this is challenging for traditional recommenders.
  - Quick check question: What distinguishes a cold-start user from a regular user in recommendation system evaluation?

## Architecture Onboarding

- Component map:
  User profile → Prompt generation → ChatGPT API call → Recommendation list → Item matching → Metric computation → Analysis

- Critical path:
  User profile → Prompt generation → ChatGPT API call → Recommendation list → Item matching → Metric computation → Analysis

- Design tradeoffs:
  - Zero-shot vs. fine-tuning: Zero-shot preserves ChatGPT's general capabilities but may limit domain-specific performance
  - Simple prompt vs. engineered prompts: Simple prompts test inherent capabilities but may underutilize ChatGPT's potential
  - API usage vs. local deployment: API usage enables testing but introduces token limits and reproducibility challenges

- Failure signatures:
  - Token limit violations (ChatGPT API restricts to 4096 tokens)
  - Hallucination (generating items not in the dataset)
  - Domain mismatch (poor performance in domains with limited training data)
  - Bias amplification (recommending popular items disproportionately)

- First 3 experiments:
  1. Top-N recommendation evaluation on MovieLens dataset with cut-off values 10, 20, 50
  2. Re-ranking experiment using a fixed list of most popular items
  3. Cold-start evaluation using users with minimal interaction history

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does ChatGPT's performance advantage on books come from specialized book knowledge in its training data, or from better utilization of content features compared to other LLMs?
- Basis in paper: [explicit] "it is worth mentioning that incorporating content from curated sources such as Linked Data enhances the performance of content-based models significantly" and "ChatGPT may still rely on collaborative data, although some content information is utilized in performing recommendations"
- Why unresolved: The paper notes ChatGPT's superior book performance but doesn't definitively establish whether this stems from training data content or superior feature utilization.
- What evidence would resolve it: Comparative experiments with LLMs fine-tuned on book-specific data, or ablation studies removing different types of content features.

### Open Question 2
- Question: What is the impact of different prompt engineering techniques on ChatGPT's recommendation performance compared to its vanilla implementation?
- Basis in paper: [explicit] "We deliberately avoid applying the Prompt Engineering approach" and "Creating an optimized prompt to maximize ChatGPT's performance in the recommendation task falls outside the scope of this study"
- Why unresolved: The study deliberately used a simple prompt to establish baseline performance, leaving the impact of more sophisticated prompt engineering unexplored.
- What evidence would resolve it: Systematic comparison of different prompt engineering techniques (Chain-of-Thought, Tree-of-Thought, persona-based) against the baseline performance.

### Open Question 3
- Question: How does ChatGPT's knowledge-based approach compare to traditional collaborative filtering methods in terms of scalability and handling data sparsity in cold-start scenarios?
- Basis in paper: [explicit] "Does the substantial amount of knowledge utilized to train ChatGPT compensate for the absence of a complete user history in a cold-start scenario?" and results showing ChatGPT outperforms baselines in cold-start
- Why unresolved: While ChatGPT shows promise in cold-start scenarios, the study doesn't explore how its knowledge-based approach scales with larger datasets or handles varying degrees of data sparsity.
- What evidence would resolve it: Large-scale experiments with datasets of varying sparsity levels and comparison of computational resources required versus traditional methods.

## Limitations
- Zero-shot performance evaluation without prompt engineering may underestimate ChatGPT's true potential as a recommender system
- Token limit constraint (4096 tokens) necessitated preprocessing choices that could affect recommendation quality
- Reliance on similarity matching through Damerau-Levenshtein distance introduces potential errors in item identification

## Confidence

**High confidence**: ChatGPT's superior accuracy on books domain compared to baselines, as this finding is consistent across multiple metrics and cut-off values.

**Medium confidence**: ChatGPT's hybrid recommender behavior, as the similarity metrics provide suggestive but not definitive evidence of underlying recommendation strategies.

**Medium confidence**: Performance in cold-start scenarios, though this requires validation across more diverse user profiles and interaction patterns.

## Next Checks
1. Systematically test different prompt formulations and in-context examples to establish upper bounds on ChatGPT's performance as a recommender
2. Apply the same evaluation pipeline to additional recommendation domains to verify whether the books domain superiority generalizes or is dataset-specific
3. Conduct deeper analysis of the ACLT and ARP metrics to understand which user/item attributes ChatGPT's recommendations are most sensitive to, and whether this introduces systematic fairness concerns