---
ver: rpa2
title: 'RA-DIT: Retrieval-Augmented Dual Instruction Tuning'
arxiv_id: '2310.01352'
source_url: https://arxiv.org/abs/2310.01352
tags:
- language
- fine-tuning
- tasks
- https
- retriever
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces RA-DIT, a lightweight fine-tuning methodology
  that retrofits any pre-trained LLM with retrieval capabilities through dual instruction
  tuning. The approach consists of two stages: (1) updating the LLM to better utilize
  retrieved information, and (2) updating the retriever to return more relevant results
  as preferred by the LLM.'
---

# RA-DIT: Retrieval-Augmented Dual Instruction Tuning

## Quick Facts
- **arXiv ID**: 2310.01352
- **Source URL**: https://arxiv.org/abs/2310.01352
- **Reference count**: 40
- **Primary result**: RA-DIT achieves state-of-the-art performance on knowledge-intensive tasks, outperforming existing RALM approaches by up to +8.9% in zero-shot and +1.4% in few-shot settings

## Executive Summary
This paper introduces RA-DIT, a lightweight fine-tuning methodology that retrofits any pre-trained LLM with retrieval capabilities through dual instruction tuning. The approach consists of two stages: (1) updating the LLM to better utilize retrieved information, and (2) updating the retriever to return more relevant results as preferred by the LLM. The method achieves state-of-the-art performance on knowledge-intensive tasks, outperforming existing RALM approaches by up to +8.9% in zero-shot and +1.4% in few-shot settings. The RA-DIT 65B model demonstrates significant improvements over base LLaMA and other RALM baselines across multiple benchmarks, while preserving the LLM's parametric knowledge and reasoning capabilities.

## Method Summary
RA-DIT operates in two distinct fine-tuning steps: language model fine-tuning (LM-ft) and retriever fine-tuning (R-ft). For LM-ft, the approach uses supervised fine-tuning with retrieval-augmented prompts, training the LLM to better utilize retrieved information. The R-ft stage employs Language Model-Supervised Retrieval (LSR) training with KL-divergence minimization to update the retriever to return results preferred by the LLM. The framework uses a dual-encoder based retriever architecture and can be applied to LLMs of varying sizes, with smaller models showing even bigger improvements from retrieval augmentation.

## Key Results
- RA-DIT 65B outperforms ATLAS 11B on 8 knowledge-intensive tasks (+7.2% on average in 64-shot setting)
- Achieves up to +8.9% improvement in zero-shot and +1.4% in few-shot settings over existing RALM approaches
- Demonstrates significant improvements over base LLaMA and other RALM baselines across multiple benchmarks
- Shows effectiveness across LLMs of varying sizes, with smaller models witnessing even bigger improvements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Dual instruction tuning improves both the LLM's ability to utilize retrieved knowledge and the retriever's ability to return relevant results, with each stage yielding significant performance improvements.
- **Mechanism**: The approach operates in two distinct fine-tuning steps: (1) updating a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM.
- **Core assumption**: Language models can learn to better utilize retrieved information when explicitly trained with retrieval-augmented prompts, and retrievers can be optimized to return results preferred by the LLM through supervised fine-tuning.
- **Evidence anchors**:
  - [abstract]: "Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM."
  - [section]: "We perform instruction-tuning in two separate steps. For language model fine-tuning (LM-ft), we adopt the supervised fine-tuning objective while augmenting each fine-tuning prompt with a retrieved 'background' field prepended to the instructions."
  - [corpus]: Weak - The corpus does not provide specific experimental evidence for this mechanism, but mentions that "each fine-tuning step offers significant performance gains."
- **Break condition**: If the LLM cannot learn to distinguish between relevant and irrelevant retrieved information, or if the retriever cannot learn to return results preferred by the LLM.

### Mechanism 2
- **Claim**: The fine-tuning approach preserves the LLM's parametric knowledge and reasoning capabilities while improving its ability to utilize external knowledge.
- **Mechanism**: By training the LLM to make correct predictions when a wrong retrieved chunk is given, the model learns to ignore misleading retrieval content and rely on its parametric knowledge in such cases.
- **Core assumption**: The LLM's parametric knowledge can serve as a fallback when the retriever makes mistakes, and the model can be trained to effectively balance between retrieved information and its own knowledge.
- **Evidence anchors**:
  - [abstract]: "RA-DIT 65B also substantially outperforms ATLAS 11B on 8 knowledge-intensive tasks (+7.2% on average in the 64-shot fine-tuning setting)."
  - [section]: "By training the LLM to make correct predictions when a wrong retrieved chunk is given, we enable the LLM to ignore misleading retrieval content and lean into its parametric knowledge in such cases."
  - [corpus]: Weak - The corpus does not provide specific experimental evidence for this mechanism, but mentions that "the parametric knowledge and reasoning capabilities of the LLM component are in general preserved."
- **Break condition**: If the LLM becomes overly reliant on retrieved information and loses its ability to reason based on its own knowledge.

### Mechanism 3
- **Claim**: Retrieval-augmented instruction tuning is effective across LLMs of varying sizes, with smaller models witnessing even bigger improvements.
- **Mechanism**: The approach provides significant performance gains across different model sizes, with smaller models showing larger relative improvements compared to larger models.
- **Core assumption**: Smaller language models can benefit more from retrieval augmentation as they have less inherent knowledge to rely on.
- **Evidence anchors**:
  - [abstract]: "We further conduct a comprehensive model analysis, showing the effectiveness of our approach across LLMs of varying sizes."
  - [section]: "We investigate the impact of the base language model size when retrieval-augmented instruction tuning is applied, and summarize the results in Figure 2. Overall, all models substantially benefit from retrieval augmentation, with smaller models witnessing even bigger improvements."
  - [corpus]: Weak - The corpus does not provide specific experimental evidence for this mechanism, but mentions that "retrieval augmentation can be an effective strategy for enhancing the performance of smaller models."
- **Break condition**: If the performance gains for smaller models do not outweigh the additional complexity introduced by retrieval augmentation.

## Foundational Learning

- **Concept**: Retrieval-Augmented Language Models (RALMs)
  - Why needed here: Understanding RALMs is crucial as the paper builds upon this concept to introduce RA-DIT, a lightweight fine-tuning methodology for retrofitting LLMs with retrieval capabilities.
  - Quick check question: What are the two main high-level challenges that existing RALM architectures focus on, as mentioned in the paper?

- **Concept**: Instruction Tuning
  - Why needed here: Instruction tuning is a key component of RA-DIT, as it is used to improve the LLM's ability to utilize retrieved information and the retriever's ability to return relevant results.
  - Quick check question: How does retrieval-augmented instruction tuning (RA-IT) differ from conventional instruction tuning, according to the paper?

- **Concept**: Dense Retrievers
  - Why needed here: Dense retrievers are used in the RA-DIT framework to retrieve relevant text chunks based on the language model prompt.
  - Quick check question: What is the role of the dual-encoder based retriever architecture in the RA-DIT framework, and how is it fine-tuned?

## Architecture Onboarding

- **Component map**: Pre-trained LLM (e.g., LLaMA) -> Dense retriever (e.g., DRAGON+) -> Retrieval corpus (e.g., Wikipedia + CommonCrawl) -> Fine-tuning datasets (e.g., MMLU, Natural Questions, TriviaQA) -> Evaluation datasets (e.g., KILT benchmark tasks)

- **Critical path**: The critical path involves initializing the framework using a pre-trained LLM and a dense retriever, performing dual instruction tuning to update the LLM and retriever, and evaluating the performance on knowledge-intensive tasks.

- **Design tradeoffs**:
  - Using a lightweight fine-tuning approach vs. extensive pre-training
  - Balancing between the LLM's parametric knowledge and retrieved information
  - Choosing the appropriate size of the retrieval corpus and the number of retrieved chunks

- **Failure signatures**:
  - Poor performance on knowledge-intensive tasks
  - Over-reliance on retrieved information or inability to utilize it effectively
  - Inability to ignore irrelevant or distracting retrieved information

- **First 3 experiments**:
  1. Compare the performance of RA-DIT with base LLaMA and R-EPLUG models on knowledge-intensive tasks in zero-shot and few-shot settings.
  2. Evaluate the impact of language model fine-tuning strategies on the performance of RA-DIT.
  3. Assess the influence of different retriever configurations (e.g., corpus size, number of retrieved chunks) on the overall performance of RA-DIT.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RA-DIT scale when using more than 10 retrieved chunks?
- Basis in paper: [explicit] The paper experiments with 1, 3, and 10 retrieved chunks but doesn't explore beyond 10.
- Why unresolved: The authors mention diminishing returns and inference cost as reasons for stopping at 10, but don't provide data on whether even more chunks could help.
- What evidence would resolve it: Performance comparison of RA-DIT using 10, 20, 50, and 100 retrieved chunks on benchmark tasks.

### Open Question 2
- Question: How does RA-DIT perform when retrieving from up-to-date knowledge sources like post-2023 Wikipedia?
- Basis in paper: [inferred] The paper notes that RA-DIT is faithful to its retrieval corpus and shows Wiki 2018 vs Wiki 2021 comparisons, suggesting recency matters.
- Why unresolved: All experiments use pre-2023 knowledge sources, leaving open whether RA-DIT can effectively incorporate truly current information.
- What evidence would resolve it: Performance comparison of RA-DIT using pre-2023 vs post-2023 Wikipedia snapshots on time-sensitive tasks.

### Open Question 3
- Question: What is the impact of iterative dual instruction tuning (multiple RA-DIT cycles) on performance?
- Basis in paper: [explicit] The authors mention attempting iterative dual instruction tuning but didn't observe further gains, and leave exploration to future work.
- Why unresolved: Only one iteration of RA-DIT was tested; the authors don't report results for 2+ iterations or explain why they stopped.
- What evidence would resolve it: Performance comparison of RA-DIT after 1, 2, and 3 iterations of dual instruction tuning.

## Limitations

- Experimental validation relies heavily on English-language benchmarks with no cross-lingual evaluation
- Method's performance on domain-specific tasks outside general knowledge domains remains unexplored
- Computational overhead of two-stage fine-tuning process not thoroughly analyzed for practical deployment

## Confidence

**High Confidence**: The core methodology of dual instruction tuning is well-defined and the overall performance improvements over baseline models are clearly demonstrated across multiple benchmarks. The experimental setup and evaluation metrics are standard and reproducible.

**Medium Confidence**: The mechanism claims about how the LLM learns to utilize retrieved information and ignore irrelevant content are plausible but lack direct experimental validation. The assertion that smaller models benefit more from retrieval augmentation is supported by results but needs further investigation.

**Low Confidence**: The claim about preserving parametric knowledge during fine-tuning is mentioned but not directly measured or validated. The generalizability of the approach to non-English languages and specialized domains remains untested.

## Next Checks

1. **Ablation Study Validation**: Conduct controlled experiments isolating the contribution of each fine-tuning stage (LM-ft and R-ft) by testing models after only one stage of fine-tuning to quantify individual performance impacts.

2. **Parametric Knowledge Preservation Test**: Design experiments that specifically measure the retention of pre-trained knowledge by comparing performance on tasks that rely solely on parametric knowledge versus retrieval-augmented knowledge before and after fine-tuning.

3. **Cross-Lingual and Domain Generalization**: Evaluate RA-DIT performance on non-English benchmark datasets and specialized domain tasks (e.g., biomedical, legal) to assess the approach's generalization beyond general knowledge domains.