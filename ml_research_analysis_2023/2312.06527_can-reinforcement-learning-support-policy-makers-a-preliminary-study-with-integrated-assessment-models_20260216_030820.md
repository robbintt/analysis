---
ver: rpa2
title: Can Reinforcement Learning support policy makers? A preliminary study with
  Integrated Assessment Models
arxiv_id: '2312.06527'
source_url: https://arxiv.org/abs/2312.06527
tags:
- reward
- environment
- agents
- policy
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether Reinforcement Learning can effectively
  probe Integrated Assessment Models (IAMs) to explore climate policy solutions. The
  authors employ the AYS IAM, a simplified climate-economic model, as an RL environment.
---

# Can Reinforcement Learning support policy makers? A preliminary study with Integrated Assessment Models

## Quick Facts
- arXiv ID: 2312.06527
- Source URL: https://arxiv.org/abs/2312.06527
- Reference count: 28
- Primary result: RL algorithms can successfully learn effective climate policies in simplified IAM environments, with DQN variants showing the most consistent performance.

## Executive Summary
This study investigates whether Reinforcement Learning can effectively probe Integrated Assessment Models (IAMs) to explore climate policy solutions. The authors employ the AYS IAM, a simplified climate-economic model, as an RL environment and train multiple RL algorithms (DQN, D3QN, A2C, PPO) using different reward functions. The results demonstrate that modern RL algorithms can successfully learn effective policies, with different reward functions producing qualitatively diverse policy trajectories. The study identifies a "natural bottleneck" where early aggressive climate action is necessary for success, aligning with real-world climate science.

## Method Summary
The study employs the AYS IAM as a simplified environment with three state variables (excess atmospheric carbon, economic output, renewable knowledge stock) and four discrete policy actions. Four RL algorithms (DQN, D3QN, A2C, PPO) are trained using three different reward functions: planetary boundaries (PB), policy cost (PC), and sparse rewards. Each algorithm is trained for 500k steps with Bayesian-optimized hyperparameters across three random seeds. The experiments evaluate success rates, action distributions, and robustness to initialization states to compare algorithm performance and policy outcomes.

## Key Results
- Modern RL algorithms can successfully learn effective climate policies in the AYS environment, with DQN variants showing the most consistent performance.
- Different reward functions and RL algorithms produce qualitatively diverse policy trajectories, highlighting the importance of reward design.
- The environment exhibits a "natural bottleneck" where early aggressive climate action is necessary for success.
- Model dynamics naturally lead to early action being optimal, aligning with real-world climate science.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modern RL algorithms can successfully learn effective climate policies in simplified IAM environments.
- Mechanism: RL agents interact with the AYS IAM environment over many episodes, learning to map observed states to actions that maximize cumulative reward through trial and error.
- Core assumption: The environment is Markovian and fully observable.
- Evidence anchors: Modern RL algorithms produce well-behaved policies in the AYS environment under different reward functions.

### Mechanism 2
- Claim: Different reward functions lead to qualitatively different policy trajectories in the AYS environment.
- Mechanism: The reward function shapes the agent's objective function, leading it to learn different optimal policies based on whether it's incentivized to stay away from boundaries, minimize costs, or reach goals directly.
- Core assumption: The reward function is well-designed and aligned with desired policy outcomes.
- Evidence anchors: Different agents and reward functions generate a significantly diverse set of solutions.

### Mechanism 3
- Claim: The AYS environment has a "natural bottleneck" where early aggressive climate action is necessary for success.
- Mechanism: The environment's dynamics create exponential economic growth that requires exponentially more energy, forcing early aggressive action to build renewable knowledge stock before economic output grows too large.
- Core assumption: The model's dynamics accurately represent real-world climate-economic interactions.
- Evidence anchors: The model's dynamics naturally lead to early action being optimal, aligning with real-world climate science.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The AYS environment is formulated as an MDP, where the agent observes states, takes actions, and receives rewards over time.
  - Quick check question: What are the four key components of an MDP?

- Concept: Reinforcement Learning algorithms (DQN, D3QN, A2C, PPO)
  - Why needed here: The paper employs these four RL algorithms to learn policies in the AYS environment.
  - Quick check question: What is the key difference between value-based and policy-based RL algorithms?

- Concept: Reward function design
  - Why needed here: The paper uses three different reward functions to shape the agent's learning objective.
  - Quick check question: How does adding action-dependent costs to a reward function affect the learned policy?

## Architecture Onboarding

- Component map: AYS IAM environment -> RL agents (DQN, D3QN, A2C, PPO) -> Reward functions (PB, PC, Sparse) -> Training loop

- Critical path:
  1. Initialize environment and agent
  2. Agent observes state
  3. Agent selects action
  4. Environment transitions to next state and provides reward
  5. Agent stores experience
  6. Agent updates policy based on experiences
  7. Repeat until training is complete

- Design tradeoffs:
  - Model complexity vs. tractability: The AYS model is simplified to be computationally tractable, but this may limit its real-world applicability.
  - Reward function design: Different reward functions lead to different policies, but designing effective reward functions is challenging and requires domain expertise.
  - Algorithm choice: Different RL algorithms have different strengths and weaknesses, and the choice of algorithm can affect the learned policies.

- Failure signatures:
  - Agent fails to learn: This could be due to poor hyperparameter choices, insufficient exploration, or a poorly designed reward function.
  - Agent learns suboptimal policies: This could be due to local optima, reward function misalignment, or model inaccuracies.
  - Agent is sensitive to initial conditions: This could be due to the environment's dynamics or the agent's exploration strategy.

- First 3 experiments:
  1. Train a DQN agent with the PB reward function and evaluate its performance.
  2. Train a PPO agent with the PC reward function and compare its policy to the DQN agent.
  3. Introduce noise to the environment parameters and evaluate the agents' robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different RL algorithms compare in their ability to explore and solve more complex IAMs beyond the AYS model?
- Basis in paper: The paper shows that DQN-based agents perform better than on-policy methods in the simple AYS environment, particularly under sparse rewards and noisy conditions.
- Why unresolved: The study only tests RL algorithms on a single, simplified IAM. Scaling to more complex IAMs remains untested.
- What evidence would resolve it: Comparative experiments applying the same RL algorithms to multiple IAMs of varying complexity.

### Open Question 2
- Question: What is the optimal reward function design for guiding RL agents toward effective climate policies in IAMs?
- Basis in paper: The authors demonstrate that different reward functions produce qualitatively different policy trajectories and success rates.
- Why unresolved: While the paper shows reward functions impact policy quality, it doesn't establish systematic principles for reward design.
- What evidence would resolve it: Systematic comparison of diverse reward function designs across multiple IAMs.

### Open Question 3
- Question: Can RL agents discover novel, counterintuitive climate policies that human experts might overlook?
- Basis in paper: The authors highlight RL's potential to explore policy spaces without requiring deep model-specific knowledge.
- Why unresolved: The study only examines whether RL can learn known effective policies rather than discovering truly novel approaches.
- What evidence would resolve it: Experiments where RL agents discover and successfully implement policies that contradict conventional wisdom.

## Limitations
- The AYS IAM environment is simplified and may not capture the full complexity of real-world climate-economic systems.
- The study only tests RL algorithms on a single IAM, limiting generalizability to other models.
- The transferability of findings to more sophisticated IAMs with higher-dimensional state spaces remains untested.

## Confidence

- **High confidence**: RL algorithms can successfully learn policies in simplified IAM environments
- **Medium confidence**: Different reward functions produce meaningfully different policy trajectories
- **Medium confidence**: Early aggressive action is necessary due to model dynamics

## Next Checks
1. Test algorithm performance on a more complex IAM with additional state variables and non-linear dynamics to assess scalability.
2. Conduct sensitivity analysis on model parameters to determine which dynamics drive the early action requirement.
3. Compare learned policies against established climate economic models (e.g., DICE, FUND) to evaluate policy consistency.