---
ver: rpa2
title: Self-Augmentation Improves Zero-Shot Cross-Lingual Transfer
arxiv_id: '2309.10891'
source_url: https://arxiv.org/abs/2309.10891
tags:
- cross-lingual
- language
- data
- languages
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes SALT, a self-augmentation method for improving
  zero-shot cross-lingual transfer of multilingual pretrained language models. SALT
  distills cross-lingual knowledge from PLMs through code-switching and embedding
  mixup, without requiring external alignment data.
---

# Self-Augmentation Improves Zero-Shot Cross-Lingual Transfer

## Quick Facts
- arXiv ID: 2309.10891
- Source URL: https://arxiv.org/abs/2309.10891
- Reference count: 14
- The paper proposes SALT, a self-augmentation method for improving zero-shot cross-lingual transfer of multilingual pretrained language models. SALT distills cross-lingual knowledge from PLMs through code-switching and embedding mixup, without requiring external alignment data. On XNLI and PAWS-X benchmarks, SALT achieves significant improvements over strong baselines, with 1.4% and 4.1% gains in average accuracy respectively.

## Executive Summary
This paper introduces SALT, a self-augmentation method that improves zero-shot cross-lingual transfer for multilingual pretrained language models without requiring external alignment data. The method combines code-switching with embedding mixup to distill cross-lingual knowledge from PLMs during fine-tuning. SALT demonstrates significant performance gains on two benchmark datasets (XNLI and PAWS-X) compared to strong baselines, showing particular effectiveness for languages with high vocabulary overlap with the PLM.

## Method Summary
SALT implements a two-stage self-augmentation approach: offline code-switching followed by online embedding mixup. The method generates code-switched training examples by using the PLM's masked language modeling head to predict high-probability target language tokens for each source token, then applies random embedding interpolation between source and target forms during training. This process creates synthetic multilingual examples from monolingual source data while smoothing the cross-lingual representation space, effectively distilling task-relevant cross-lingual knowledge from the PLM without external alignment data.

## Key Results
- SALT achieves 1.4% average accuracy improvement on XNLI across target languages compared to strong baselines
- SALT shows 4.1% average accuracy improvement on PAWS-X benchmark
- The method is particularly effective for languages with high vocabulary overlap with the PLM (French, German, Spanish)
- SALT outperforms existing methods that require external parallel corpora or word alignment data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Code-switching with self-predicted cross-lingual tokens distills task-relevant cross-lingual knowledge from the PLM without external data.
- Mechanism: The PLM's MLM head is used to predict high-probability target language tokens for each source token, creating synthetic code-switched training examples that preserve the original label but expose the model to cross-lingual context during fine-tuning.
- Core assumption: The PLM has sufficient cross-lingual lexical alignment to provide meaningful token-level translations for the target languages.
- Evidence anchors:
  - [abstract] "By incorporating code-switching and embedding mixup with self-augmentation, SALT effectively distills cross-lingual knowledge from the multilingual PLM"
  - [section] "SALT makes two modifications to the original MLM learning objective... First, we let the model predict tokens only in a specific target language"
  - [corpus] Weak evidence; no corpus neighbor directly addresses this specific mechanism of using MLM for code-switching distillation.
- Break condition: If the PLM's cross-lingual alignment is poor for the target languages, predicted tokens will be noisy and hurt transfer.

### Mechanism 2
- Claim: Embedding mixup between source and target language representations smooths the embedding space and improves generalization by exposing the model to interpolated cross-lingual feature distributions.
- Mechanism: During training, each token's embedding is interpolated between its original source form and its code-switched target form using a random coefficient, creating a continuum of cross-lingual representations.
- Core assumption: The interpolated embeddings preserve semantic information while encouraging the model to learn more robust, language-agnostic features.
- Evidence anchors:
  - [abstract] "incorporating code-switching and embedding mixup with self-augmentation"
  - [section] "the interpolation coefficient of each embedding dimension j is independently generated from a uniform distribution to allow for more diverse combinations of embeddings in two languages"
  - [corpus] No direct corpus evidence; mixup is mentioned in related work but not specifically for cross-lingual transfer.
- Break condition: If mixup coefficients are too extreme, semantic meaning may be lost; if too conservative, insufficient smoothing occurs.

### Mechanism 3
- Claim: Offline code-switching followed by online mixup creates a two-stage augmentation that first aligns tokens cross-lingually, then smooths the representation space, leading to more robust cross-lingual transfer.
- Mechanism: Offline generation creates diverse code-switched examples; online mixup adds stochastic perturbations during training, combining the benefits of data augmentation and feature smoothing.
- Core assumption: The two-stage approach is more effective than either technique alone because it addresses both data-level and representation-level cross-lingual alignment.
- Evidence anchors:
  - [abstract] "SALT introduces two self-augmentation techniques on monolingual task-specific training data"
  - [section] "Before training the task model... we generate an augmented sample in each target language offline with code-switching, then during training, we further apply embedding mixup"
  - [corpus] Weak evidence; no corpus neighbor explicitly discusses this two-stage augmentation strategy.
- Break condition: If either stage introduces too much noise, the combined effect may degrade rather than improve performance.

## Foundational Learning

- Concept: Masked Language Modeling (MLM) pretraining
  - Why needed here: SALT relies on the PLM's MLM capability to predict cross-lingual token substitutes during code-switching
  - Quick check question: How does MLM pretraining enable the model to predict plausible translations during code-switching?

- Concept: Embedding interpolation and mixup regularization
  - Why needed here: Embedding mixup is the core mechanism for smoothing cross-lingual representations during training
  - Quick check question: What is the effect of interpolating embeddings between source and target languages on the model's decision boundary?

- Concept: Code-switching as data augmentation
  - Why needed here: Code-switching creates synthetic multilingual examples from monolingual data, expanding the effective training distribution
  - Quick check question: Why might code-switching with self-predicted tokens be preferable to using external parallel corpora for zero-shot transfer?

## Architecture Onboarding

- Component map:
  PLM (mBERT base) → MLM head for code-switching prediction → Code-switched data generator → Mixup layer (during training) → Task head (XNLI/PAWS-X classifier)
  Training pipeline: Monolingual source data → Offline code-switching augmentation → Online mixup + task training

- Critical path:
  1. Load mBERT and monolingual source training data
  2. For each source example, generate code-switched versions for target languages using MLM predictions
  3. During training, apply random embedding mixup between source and target forms
  4. Train task head with original labels on augmented examples

- Design tradeoffs:
  - Fixed vs. adaptive token substitution thresholds (chosen fixed for simplicity)
  - Number of target languages for code-switching (3 chosen based on vocabulary overlap)
  - Mixup coefficient distribution (uniform for diversity vs. fixed ratio for stability)

- Failure signatures:
  - Performance degradation when target languages have low vocabulary overlap with PLM
  - Code-switched examples with semantically inconsistent token substitutions
  - Over-smoothing from aggressive mixup causing loss of task-relevant features

- First 3 experiments:
  1. Baseline: Train task head directly on monolingual source data without any augmentation
  2. Code-switching only: Apply offline code-switching but no mixup during training
  3. Mixup only: Skip code-switching, apply only embedding mixup during training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the optimal probability thresholds for code-switching vary across different languages and instances?
- Basis in paper: [inferred] The paper mentions using fixed thresholds for token substitution but notes that optimal thresholds may vary across languages and instances.
- Why unresolved: The paper uses a fixed threshold for all target languages without exploring language-specific or instance-specific optimization.
- What evidence would resolve it: Experimental results comparing performance using different thresholds for each language and instance, or a method for automatically determining optimal thresholds.

### Open Question 2
- How does SALT perform on generative multilingual tasks using models like mT5 or mBART?
- Basis in paper: [inferred] The paper only evaluates SALT on discriminative natural language understanding tasks and mentions potential extension to generative models.
- Why unresolved: The paper does not conduct experiments on generative tasks or with generative models.
- What evidence would resolve it: Experimental results showing SALT's performance on text generation tasks using mT5, mBART, or similar models.

### Open Question 3
- What is the impact of using different source languages with SALT?
- Basis in paper: [explicit] The paper states they chose English as the source language but acknowledges extending to other source languages could enhance comprehensiveness.
- Why unresolved: The paper only evaluates SALT with English as the source language.
- What evidence would resolve it: Experimental results comparing SALT's performance when using different source languages.

### Open Question 4
- How does SALT affect the catastrophic forgetting of cross-lingual knowledge during fine-tuning?
- Basis in paper: [inferred] The paper mentions that multilingual PLMs may catastrophically forget cross-lingual knowledge during fine-tuning, which SALT aims to address.
- Why unresolved: The paper does not provide direct evidence or analysis of how SALT affects catastrophic forgetting.
- What evidence would resolve it: Experimental results showing the retention of cross-lingual knowledge during fine-tuning with and without SALT, or analysis of the model's cross-lingual capabilities before and after fine-tuning.

## Limitations
- The method relies heavily on the PLM's internal cross-lingual alignment quality, limiting effectiveness for language pairs with minimal lexical similarity
- Optimal hyperparameter settings (token substitution thresholds, mixup coefficients) are not systematically explored, raising questions about robustness
- The approach has only been validated on European languages with high vocabulary overlap, limiting generalizability to distant language families

## Confidence
- **High Confidence**: The core architectural design combining code-switching with embedding mixup is well-specified and technically sound
- **Medium Confidence**: The reported performance improvements appear significant but lack detailed hyperparameter information and sensitivity analysis
- **Low Confidence**: Generalizability to language pairs beyond selected European languages and effectiveness on other task types remain unproven

## Next Checks
1. **Ablation study rigor**: Conduct systematic ablation experiments comparing SALT against code-switching-only and mixup-only variants across multiple datasets to quantify the marginal contribution of each component.

2. **Cross-lingual alignment quality**: Measure the semantic consistency of automatically generated code-switched examples by human evaluation or automated semantic similarity metrics to verify that token substitutions preserve meaning.

3. **Hyperparameter sensitivity analysis**: Perform comprehensive grid search or Bayesian optimization over token substitution thresholds and mixup coefficient distributions to identify whether the reported performance represents a robust optimum or a local maximum dependent on specific hyperparameter choices.