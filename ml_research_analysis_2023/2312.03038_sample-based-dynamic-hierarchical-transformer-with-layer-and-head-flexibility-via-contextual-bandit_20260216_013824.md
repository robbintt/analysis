---
ver: rpa2
title: Sample-based Dynamic Hierarchical Transformer with Layer and Head Flexibility
  via Contextual Bandit
arxiv_id: '2312.03038'
source_url: https://arxiv.org/abs/2312.03038
tags:
- heads
- training
- layers
- layer
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a dynamic hierarchical transformer (DHT) model
  that can automatically adjust the number of layers and heads based on individual
  samples during training. The key idea is to formulate the layer and head selection
  as contextual bandit problems, using UCB for numerical decisions (layer count, head
  count) and Thompson Sampling for combinatorial decisions (specific head combinations).
---

# Sample-based Dynamic Hierarchical Transformer with Layer and Head Flexibility via Contextual Bandit

## Quick Facts
- **arXiv ID**: 2312.03038
- **Source URL**: https://arxiv.org/abs/2312.03038
- **Reference count**: 0
- **Key outcome**: DHT achieves up to 74% computational savings while maintaining competitive accuracy compared to baseline models like BERT, DistilBERT, and FastBERT.

## Executive Summary
This paper proposes a dynamic hierarchical transformer (DHT) model that automatically adjusts the number of layers and heads based on individual samples during training. The key innovation is formulating layer and head selection as contextual bandit problems, using UCB for numerical decisions (layer count, head count) and Thompson Sampling for combinatorial decisions (specific head combinations). The model employs batch-level rewards that balance prediction loss and model complexity to update the bandit algorithms. Experiments on text classification tasks demonstrate significant computational savings while maintaining accuracy.

## Method Summary
The DHT model implements a dynamic transformer architecture where each sample's complexity is estimated through contextual bandit algorithms. The model uses UCB for selecting numerical parameters (number of layers and heads per layer) and Thompson Sampling for combinatorial decisions (which specific heads to activate). A queuing strategy categorizes samples based on their optimal layer count, enabling parallel processing. The bandit algorithms are updated using batch-level rewards that consider both prediction loss and computational complexity, mitigating performance reductions that can occur with step-level updates.

## Key Results
- Achieves up to 74% computational savings for both training and inference
- Maintains competitive accuracy compared to static models like BERT and DistilBERT
- Successfully balances model complexity and performance through batch-level reward formulation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model reduces computational cost by dynamically pruning layers and heads based on sample complexity.
- Mechanism: Uses contextual bandits (UCB for numerical decisions like layer count and head count, Thompson Sampling for combinatorial decisions like specific head combinations) to select optimal architecture configurations per sample.
- Core assumption: Sample complexity can be accurately inferred from early layers and used to guide architecture selection.
- Evidence anchors:
  - [abstract] "DHT is not only advantageous for adaptively optimizing the underlying network architecture during training but also has a flexible network for efficient inference"
  - [section] "We use the Uniform Confidence Bound while we deploy combinatorial Thompson Sampling in order to select specific head combinations given their number"
  - [corpus] Weak - related papers focus on different optimization strategies (layer optimization, window attention, etc.) rather than sample-based dynamic selection
- Break condition: If early layer representations do not correlate well with final task performance, the bandit decisions will be suboptimal.

### Mechanism 2
- Claim: The model maintains accuracy while reducing complexity by formulating rewards that balance loss and model size.
- Mechanism: Batch-level rewards consider both prediction loss and computational complexity (number of layers and heads) to guide the contextual bandit updates.
- Core assumption: A reward function that penalizes both high loss and high model complexity will find an optimal balance.
- Evidence anchors:
  - [abstract] "achieve up to 74% computational savings for both training and inference with a minimal loss of accuracy"
  - [section] "rewards of batch level rather than onestep gains, which successfully mitigates model performance reductions"
  - [corpus] Weak - related papers don't discuss reward formulation for dynamic architecture selection
- Break condition: If the reward function doesn't properly balance accuracy and efficiency, the model may sacrifice too much accuracy or not achieve sufficient efficiency gains.

### Mechanism 3
- Claim: The queuing strategy enables parallel execution of contextual bandits across different sample complexities.
- Mechanism: Samples are categorized into queues based on their optimal layer count, allowing batches to be formed for parallel processing.
- Core assumption: Samples with similar optimal layer counts can be processed together without significantly impacting convergence.
- Evidence anchors:
  - [section] "our model implement a queuing strategy that categorizes each sample into a particular queue based on the optimal L = Lâˆ—, determined for it enabling the parallel execution of UCBs and TSB"
  - [corpus] Weak - related papers don't discuss parallel execution strategies for dynamic architectures
- Break condition: If samples in the same queue have significantly different complexities, the batch processing may be inefficient or cause convergence issues.

## Foundational Learning

- Concept: Multi-head attention mechanism
  - Why needed here: Understanding how heads contribute to representation learning is crucial for the head pruning strategy
  - Quick check question: What is the role of multiple attention heads in transformer models and how do they differ from single-head attention?

- Concept: Contextual bandit algorithms
  - Why needed here: The entire dynamic architecture selection relies on contextual bandits to make decisions
  - Quick check question: How do UCB and Thompson Sampling differ in their approach to exploration-exploitation trade-offs?

- Concept: Knowledge distillation
  - Why needed here: The paper mentions DistilBERT as a baseline, and understanding distillation helps contextualize the efficiency gains
  - Quick check question: What is the fundamental difference between knowledge distillation and the sample-based dynamic approach proposed in this paper?

## Architecture Onboarding

- Component map: Input embedding layer -> Dynamic hierarchical transformer (UCB layer count -> UCB head count -> Thompson Sampling head combinations) -> Output layer -> Queuing system

- Critical path:
  1. Sample passes through initial fixed layers
  2. UCB selects optimal layer count
  3. UCB selects optimal head count per layer
  4. Thompson Sampling selects specific head combinations
  5. Sample processes through selected layers/heads
  6. Batch-level reward calculated
  7. Bandit algorithms updated

- Design tradeoffs:
  - More queues (smaller queue size) = better parallelism but potentially less efficient batching
  - More complex reward function = potentially better optimization but harder to tune
  - Earlier layer-based decisions = faster but potentially less accurate complexity estimation

- Failure signatures:
  - Accuracy degradation despite complexity reduction
  - Very long training times due to inefficient batching
  - Convergence issues due to inconsistent reward signals

- First 3 experiments:
  1. Implement the UCB-based layer selection only (no head pruning) and verify it reduces FLOPs while maintaining accuracy
  2. Add Thompson Sampling for head combination selection and measure impact on accuracy and efficiency
  3. Implement the queuing strategy and measure parallel execution efficiency compared to sequential processing

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several emerge from the methodology and results presented.

## Limitations
- Limited evaluation to text classification tasks, raising questions about generalizability to other NLP tasks
- Lack of ablation studies to isolate contributions of individual components (UCB, Thompson Sampling, queuing)
- No discussion of computational overhead introduced by running contextual bandits during training

## Confidence

- **High Confidence**: The fundamental approach of using contextual bandits for dynamic architecture selection is sound and well-grounded in existing bandit literature. The reward formulation that balances loss and complexity is a reasonable approach.
- **Medium Confidence**: The specific implementation details and hyperparameter choices (b=3, c=3 for queue thresholds) appear reasonable but may require tuning for different datasets or tasks.
- **Low Confidence**: The claim of maintaining accuracy while achieving 74% computational savings relies heavily on the specific reward function formulation, which is not extensively validated across different scenarios.

## Next Checks
1. **Ablation Study**: Implement and test each component (UCB layer selection, Thompson Sampling head pruning, queuing strategy) in isolation to quantify their individual contributions to accuracy and efficiency.
2. **Overhead Analysis**: Measure the computational overhead introduced by the contextual bandit algorithms themselves and calculate the net efficiency gain after accounting for this overhead.
3. **Generalization Test**: Apply the DHT framework to a different NLP task (e.g., named entity recognition or machine translation) to evaluate its effectiveness beyond text classification.