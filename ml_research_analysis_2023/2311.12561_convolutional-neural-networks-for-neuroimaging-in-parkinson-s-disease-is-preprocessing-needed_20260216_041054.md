---
ver: rpa2
title: 'Convolutional Neural Networks for Neuroimaging in Parkinson''s Disease: Is
  Preprocessing Needed?'
arxiv_id: '2311.12561'
source_url: https://arxiv.org/abs/2311.12561
tags:
- normalization
- intensity
- neural
- spatial
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Neuroimaging analysis typically requires spatial and intensity
  normalization, but these preprocessing steps can introduce artifacts and are computationally
  expensive. We evaluate whether Convolutional Neural Networks can bypass these steps
  when analyzing nuclear brain imaging for Parkinson's Disease diagnosis.
---

# Convolutional Neural Networks for Neuroimaging in Parkinson's Disease: Is Preprocessing Needed?

## Quick Facts
- arXiv ID: 2311.12561
- Source URL: https://arxiv.org/abs/2311.12561
- Reference count: 18
- Key outcome: ALEXNET3D CNN achieves 94.1% accuracy without spatial normalization in Parkinson's Disease diagnosis from DaTSCAN SPECT images

## Executive Summary
This study evaluates whether Convolutional Neural Networks can bypass traditional preprocessing steps (spatial and intensity normalization) when analyzing nuclear brain imaging for Parkinson's Disease diagnosis. The researchers trained four CNN models on DaTSCAN SPECT images with different preprocessing combinations. Results demonstrate that sufficiently complex CNN architectures like ALEXNET3D can effectively account for spatial differences without explicit spatial normalization, achieving 94.1% accuracy and 0.984 AUC. However, intensity normalization proved crucial for performance, with integral normalization being the most effective approach. The findings suggest that deep CNNs can potentially replace spatial normalization in neuroimaging analysis, simplifying the pipeline while maintaining high accuracy.

## Method Summary
The study utilized 642 DaTSCAN SPECT images (448 PD patients, 194 controls) from the PPMI database, each with 57x69x57 resolution. Two 3D CNN architectures were implemented: LENET53D and ALEXNET3D, both with ReLU and SELU activation variants. The models were trained using cross-validation with different normalization combinations: no normalization, spatial only, intensity only, and both normalizations. Integral and max intensity normalization methods were compared. The networks were trained for 60 epochs with batch size 64, using cross-entropy loss, and evaluated using accuracy, sensitivity, specificity, F1-score, balanced accuracy, and AUC metrics.

## Key Results
- ALEXNET3D achieved 94.1% accuracy and 0.984 AUC without spatial normalization
- Intensity normalization was crucial for performance, with integral normalization outperforming max intensity normalization
- Saliency maps confirmed the model learned patterns matching literature findings for Parkinson's Disease
- Spatial normalization was not required for high performance when using sufficiently complex CNN architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Convolutional Neural Networks can replace spatial normalization in neuroimaging analysis for Parkinson's Disease.
- Mechanism: CNNs introduce positional invariance through parameter sharing and local connectivity, allowing them to recognize patterns regardless of spatial orientation or size differences in brain images.
- Core assumption: The complexity of the CNN model is sufficient to account for spatial variations without explicit normalization.
- Evidence anchors:
  - [abstract] "The results show that a sufficiently complex model such as our three-dimensional version of the ALEXNET can effectively account for spatial differences, achieving a diagnosis accuracy of 94.1% with an area under the ROC curve of 0.984."
  - [section] "Deep learning, and in particular Convolutional Neural Networks (CNN) have shown great ability in classifying objects into images regardless of their orientation, size, angle, etc."
- Break condition: If the CNN model lacks sufficient complexity or depth to capture the spatial variations in the data, or if the data contains large-scale artifacts that the CNN cannot handle.

### Mechanism 2
- Claim: Intensity normalization is crucial for CNN performance in neuroimaging analysis.
- Mechanism: Intensity normalization ensures that similar intensity levels correspond to similar physical measures across subjects, which is essential for comparing brain activity or function between individuals.
- Core assumption: The intensity normalization strategy used effectively standardizes the intensity values across the dataset.
- Evidence anchors:
  - [abstract] "The intensity normalization -- and its type -- is revealed as very influential in the results and accuracy of the trained model, and therefore must be well accounted."
  - [section] "In nuclear imaging, the use of intensity normalization is key in order to compare brain activity or function between subjects."
- Break condition: If the intensity normalization method fails to properly standardize the intensity values, leading to inconsistent or misleading intensity patterns across subjects.

### Mechanism 3
- Claim: Deeper CNN models can effectively learn spatial invariance without explicit normalization.
- Mechanism: Deeper models with more layers can learn higher-level abstractions and complex patterns, reducing the need for preprocessing steps like spatial normalization.
- Core assumption: The depth and complexity of the CNN are sufficient to capture the necessary spatial variations.
- Evidence anchors:
  - [abstract] "The results show that a sufficiently complex model such as our three-dimensional version of the ALEXNET can effectively account for spatial differences."
  - [section] "The fact that a CNN model can learn neuroimaging patterns without the need for nonlinear registration or complex diffeomorphisms completely changes the paradigm."
- Break condition: If the model is not deep or complex enough to capture the spatial variations, or if the data contains artifacts that the CNN cannot handle.

## Foundational Learning

- Concept: Spatial Normalization
  - Why needed here: Spatial normalization aligns brain images to a common space, eliminating differences in shape and size, which is crucial for voxel-wise comparisons.
  - Quick check question: What is the primary purpose of spatial normalization in neuroimaging analysis?

- Concept: Intensity Normalization
  - Why needed here: Intensity normalization standardizes the intensity values across subjects, ensuring that similar intensity levels correspond to similar physical measures, which is essential for comparing brain activity or function between individuals.
  - Quick check question: How does intensity normalization affect the comparability of brain images across different subjects?

- Concept: Convolutional Neural Networks (CNNs)
  - Why needed here: CNNs can learn spatial invariance through parameter sharing and local connectivity, allowing them to recognize patterns regardless of spatial orientation or size differences in brain images.
  - Quick check question: What key feature of CNNs allows them to classify objects regardless of their orientation, size, or angle?

## Architecture Onboarding

- Component map:
  - Input layer: 3D brain images (57x69x57)
  - Convolutional layers: Multiple layers with varying filter sizes (3x3x3, 5x5x5) and numbers of filters
  - Pooling layers: Max pooling to introduce positional invariance
  - Dense layers: Fully connected layers for high-level reasoning
  - Output layer: Softmax activation for binary classification (PD vs. control)

- Critical path:
  - Input -> Convolutional layers -> Pooling layers -> Dense layers -> Output

- Design tradeoffs:
  - Model complexity vs. computational cost: Deeper models may require more computational resources but can learn more complex patterns.
  - Spatial normalization vs. CNN depth: More complex CNNs may reduce the need for spatial normalization but increase computational cost.

- Failure signatures:
  - Poor performance on spatially variant data: Indicates insufficient model complexity to account for spatial variations.
  - Overfitting on training data: Indicates the model is too complex or the data is insufficient.

- First 3 experiments:
  1. Train a shallow CNN (e.g., LENET53D) with and without spatial normalization to compare performance.
  2. Train a deeper CNN (e.g., ALEXNET3D) with different intensity normalization strategies to evaluate their impact.
  3. Visualize saliency maps for both shallow and deep models to understand which regions are most influential in the classification.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum depth required for a CNN to effectively learn spatial patterns in neuroimaging without requiring spatial normalization?
- Basis in paper: [explicit] The paper shows that the ALEXNET3D model (deeper architecture) performs well without spatial normalization, while the shallower LENET53D model shows more dependence on spatial normalization
- Why unresolved: The paper only compares two specific architectures and doesn't systematically test how depth affects the need for spatial normalization across a range of depths
- What evidence would resolve it: A systematic study varying CNN depth (e.g., 3, 5, 7, 10 layers) while keeping other parameters constant and measuring performance with and without spatial normalization

### Open Question 2
- Question: Which intensity normalization strategy is optimal for different types of neuroimaging modalities when using deep learning?
- Basis in paper: [explicit] The paper demonstrates that integral normalization performs best for FP-CIT SPECT in Parkinson's Disease diagnosis, outperforming normalization to maximum
- Why unresolved: The study only examines one neuroimaging modality (FP-CIT SPECT) and doesn't test whether these findings generalize to other modalities like MRI, PET-FDG, or other SPECT tracers
- What evidence would resolve it: Comparative studies applying multiple normalization strategies across different neuroimaging modalities using the same CNN architecture and dataset

### Open Question 3
- Question: How do capsule networks compare to traditional CNNs in eliminating the need for spatial normalization in neuroimaging analysis?
- Basis in paper: [inferred] The paper discusses how CNNs provide positional invariance and mentions that capsule networks preserve relative positions, suggesting they might perform differently regarding spatial normalization needs
- Why unresolved: The paper uses traditional CNN architectures but only mentions capsule networks as a future possibility without testing them
- What evidence would resolve it: Direct comparison of capsule networks versus CNNs on the same neuroimaging dataset, testing performance with and without spatial normalization across different architectures

## Limitations

- The study focused on a single neuroimaging modality (DaTSCAN SPECT) and disease (Parkinson's Disease), limiting generalizability to other imaging types or neurological conditions
- The 3D CNN models were trained and tested on data from the PPMI database, which may not represent broader clinical populations
- The spatial normalization method used a custom DaTSCAN template that is not publicly available, making exact replication challenging
- The study did not investigate the impact of different CNN architectures beyond the four tested models

## Confidence

- **High confidence**: CNNs can effectively learn spatial invariance without explicit normalization (proven by ALEXNET3D achieving 94.1% accuracy without spatial normalization)
- **High confidence**: Intensity normalization is crucial for CNN performance in neuroimaging (shown by significant performance differences across normalization strategies)
- **Medium confidence**: The specific normalization methods and model architectures are optimal for this task (based on limited architectural comparisons)

## Next Checks

1. Test the same CNN models on other neuroimaging modalities (e.g., MRI, PET) to assess generalizability of the findings
2. Compare performance against state-of-the-art spatial normalization methods using quantitative metrics like Dice coefficient and deformation field magnitude
3. Investigate the impact of different CNN architectures (e.g., ResNet, DenseNet) and attention mechanisms on the need for preprocessing