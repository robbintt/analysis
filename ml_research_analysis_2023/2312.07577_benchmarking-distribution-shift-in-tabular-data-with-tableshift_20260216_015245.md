---
ver: rpa2
title: Benchmarking Distribution Shift in Tabular Data with TableShift
arxiv_id: '2312.07577'
source_url: https://arxiv.org/abs/2312.07577
tags:
- data
- shift
- domain
- label
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TableShift, a benchmark for evaluating distribution
  shift in tabular machine learning. It addresses the gap in understanding how tabular
  models perform under real-world data shifts.
---

# Benchmarking Distribution Shift in Tabular Data with TableShift

## Quick Facts
- arXiv ID: 2312.07577
- Source URL: https://arxiv.org/abs/2312.07577
- Reference count: 40
- Primary result: Strong correlation between in-distribution and out-of-distribution accuracy in tabular models; no model consistently outperforms baseline methods

## Executive Summary
This paper introduces TableShift, a benchmark for evaluating distribution shift in tabular machine learning. It addresses the gap in understanding how tabular models perform under real-world data shifts. The benchmark includes 15 diverse binary classification tasks across domains like finance, healthcare, and public policy, with a Python API for easy access. A large-scale study compares 19 models, including baselines, neural networks, and robustness methods. Key findings: in-distribution and out-of-distribution accuracy are strongly correlated; no model consistently outperforms baselines; and shift gaps correlate with changes in label distribution, which robustness methods fail to mitigate. The results suggest improving in-distribution accuracy is key to better out-of-distribution performance. TableShift provides a rigorous, accessible platform for future research on tabular data robustness.

## Method Summary
TableShift is a benchmark for evaluating distribution shift in tabular machine learning that includes 15 curated binary classification datasets across diverse domains. The benchmark provides a Python API with standardized preprocessing pipelines and implements 19 models including baseline tree-based methods (XGBoost, LightGBM, CatBoost), neural networks, and robustness techniques. A large-scale study evaluates these models using fixed train/test splits with hyperparameter tuning to compare in-distribution and out-of-distribution performance. The benchmark focuses on three types of distribution shifts: covariate shift, concept shift, and label shift.

## Key Results
- Strong linear correlation (ρ = 0.81) between in-distribution and out-of-distribution accuracy across all models and tasks
- No single model consistently outperforms baseline methods (XGBoost, LightGBM, CatBoost) in both ID and OOD accuracy
- Shift gap strongly correlates with changes in label distribution (ρ = 0.71), and robustness methods fail to mitigate this effect

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Improving in-distribution (ID) accuracy leads to improved out-of-distribution (OOD) accuracy in tabular data models.
- Mechanism: The study demonstrates a strong linear correlation (ρ = 0.81) between ID and OOD accuracy across 15 benchmark tasks and 19 model types. This suggests that gains in ID performance translate directly to OOD performance gains.
- Core assumption: The relationship between ID and OOD accuracy is consistent across different tabular data tasks and model architectures.
- Evidence anchors:
  - [abstract]: "Our study demonstrates (1) a linear trend between in-distribution (ID) and out-of-distribution (OOD) accuracy; (2) no model consistently outperforms baseline methods; and (3) a correlation between the shift gap and the shift in label distribution."
  - [section]: "Our results show that, across all models and tasks, in-distribution (ID) and out-of-distribution (OOD) accuracy are correlated: as ID performance improves, OOD performance also tends to improve (see Figure 1; ρ = 0.81)."
- Break condition: The correlation breaks down if the distribution shift is extreme or if the ID and OOD datasets have fundamentally different characteristics that are not captured by the linear relationship.

### Mechanism 2
- Claim: No single model consistently outperforms baseline methods (XGBoost, LightGBM, CatBoost) in both ID and OOD accuracy for tabular data.
- Mechanism: The benchmark study compares state-of-the-art tabular data models, including neural networks and robustness methods, against established baseline models. The results show that no model achieves consistently better performance across all tasks.
- Core assumption: The baseline models (XGBoost, LightGBM, CatBoost) are sufficiently robust and effective for tabular data tasks under distribution shift.
- Evidence anchors:
  - [abstract]: "Our study demonstrates (1) a linear trend between in-distribution (ID) and out-of-distribution (OOD) accuracy; (2) no model consistently outperforms baseline methods; and (3) a correlation between the shift gap and the shift in label distribution."
  - [section]: "Our results show that, across all models and tasks, in-distribution (ID) and out-of-distribution (OOD) accuracy are correlated: as ID performance improves, OOD performance also tends to improve (see Figure 1; ρ = 0.81)."
- Break condition: The conclusion changes if new models are developed that significantly outperform the baselines or if the benchmark tasks are not representative of real-world tabular data scenarios.

### Mechanism 3
- Claim: Changes in the label distribution are strongly correlated with the shift gap (difference between ID and OOD performance).
- Mechanism: The study finds a strong relationship between the shift in label distribution (∆y) and the shift gap (∆Acc). This suggests that changes in the proportion of positive labels between ID and OOD data contribute significantly to performance drops.
- Core assumption: The correlation between label distribution shift and shift gap is causal or at least a strong indicator of the impact of label shift on model performance.
- Evidence anchors:
  - [abstract]: "Our study demonstrates (1) a linear trend between in-distribution (ID) and out-of-distribution (OOD) accuracy; (2) no model consistently outperforms baseline methods; and (3) a correlation between the shift gap and the shift in label distribution."
  - [section]: "Our results, in Figures 5 and 8, show that change in the label distribution ∆y is correlated with shift gap (∆Acc) (Pearson correlation ρ = 0.71). This persists even after accounting for ID accuracy: a simple linear regression of OOD accuracy on [ID accuracy, ∆y] obtains R2 = 0.996."
- Break condition: The correlation weakens or disappears if other factors, such as covariate shift or concept shift, become dominant drivers of the shift gap.

## Foundational Learning

- Concept: Distribution shift in machine learning
  - Why needed here: Understanding distribution shift is crucial for interpreting the results of the TableShift benchmark and the implications for tabular data models.
  - Quick check question: What are the three main types of distribution shift mentioned in the paper, and how do they differ?

- Concept: Domain generalization and robustness methods
  - Why needed here: The paper evaluates various domain generalization and robustness methods to assess their effectiveness in mitigating distribution shift in tabular data. Understanding these methods is essential for interpreting the experimental results.
  - Quick check question: What is the difference between domain generalization and domain robustness methods, and how do they attempt to improve model performance under distribution shift?

- Concept: Tabular data preprocessing and feature engineering
  - Why needed here: Tabular data requires specific preprocessing and feature engineering techniques that differ from other data modalities. Understanding these techniques is important for effectively using the TableShift benchmark and interpreting the results.
  - Quick check question: What are some common preprocessing techniques for tabular data, and how do they differ from preprocessing techniques for image or text data?

## Architecture Onboarding

- Component map: TableShift API -> Dataset loading -> Preprocessing pipeline -> Model training -> ID/OOD evaluation
- Critical path: Load dataset via API → Apply standardized preprocessing → Train model with hyperparameter tuning → Evaluate on both ID and OOD test sets → Compare performance across models
- Design tradeoffs: Focuses on binary classification only, which limits applicability to regression tasks; datasets are curated for sufficient size and dimensionality, potentially excluding smaller but relevant datasets
- Failure signatures: Overfitting to training data, poor generalization to OOD data, sensitivity to hyperparameter choices
- First 3 experiments:
  1. Load a dataset from TableShift using the API and explore its features and distribution shift
  2. Train a baseline model (e.g., XGBoost) on the training data and evaluate its performance on both in-distribution and out-of-distribution test sets
  3. Train a domain generalization method (e.g., Group DRO) on the training data and compare its performance to the baseline model on both test sets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the correlation between in-distribution (ID) and out-of-distribution (OOD) accuracy observed in TableShift generalize to other tabular datasets with different distribution shifts?
- Basis in paper: [explicit] The paper states: "Our results show that, across all models and tasks, in-distribution (ID) and out-of-distribution (O OOD) accuracy are correlated: as ID performance improves, OOD performance also tends to improve (see Figure 1; ρ = 0.81). This linear trend holds across datasets and model classes."
- Why unresolved: The paper only tests this correlation on the 15 specific tasks in TableShift. It is unclear if this correlation holds for other tabular datasets with different types of distribution shifts not present in TableShift.
- What evidence would resolve it: Evaluating the correlation between ID and OOD accuracy on a diverse set of additional tabular datasets with varying distribution shifts.

### Open Question 2
- Question: What are the underlying mechanisms that cause the correlation between label shift and shift gap in TableShift?
- Basis in paper: [explicit] The paper states: "Our results, in Figures 5 and 8, show that change in the label distribution ∆y is correlated with shift gap (Pearson correlation ρ = 0.70). This persists even after accounting for ID accuracy: a simple linear regression of OOD accuracy on [ID accuracy, ∆y] obtains R2 = 0.996."
- Why unresolved: The paper does not provide a theoretical explanation for why changes in the label distribution are related to shift gaps. It only shows an empirical correlation.
- What evidence would resolve it: Theoretical analysis or empirical studies investigating the relationship between label shift and shift gaps in tabular data, potentially exploring the impact of different types of distribution shifts on this relationship.

### Open Question 3
- Question: How do hybrid methods that combine robustness-enhancing losses (e.g., Group DRO) with improved neural network architectures (e.g., FT-Transformer) perform on TableShift compared to the individual components?
- Basis in paper: [inferred] The paper mentions: "Our initial exploratory evaluation of hybrid methods (see Section E.5), however, does not suggest that hybrid methods led to qualitative changes in our results, but these methods warrant a more extensive evaluation."
- Why unresolved: The paper only conducts a preliminary investigation of hybrid methods and does not provide conclusive results. It is unclear if combining robustness-enhancing losses with improved neural network architectures can lead to better OOD performance.
- What evidence would resolve it: Conducting a comprehensive evaluation of hybrid methods on TableShift, comparing their performance to the individual components and other state-of-the-art methods.

## Limitations
- Benchmark focuses exclusively on binary classification tasks, limiting generalizability to regression or multi-class scenarios
- While 15 tasks are included, they may not fully capture the diversity of real-world tabular data distribution shifts
- The correlation between ID and OOD accuracy may not hold for extreme distribution shifts or fundamentally different dataset characteristics

## Confidence
- **High confidence**: The linear correlation between ID and OOD accuracy (ρ = 0.81) and the strong relationship between label distribution shift and performance degradation are well-supported by experimental results
- **Medium confidence**: The conclusion that no model consistently outperforms baselines may change as new methods are developed or if benchmark tasks are not fully representative
- **Medium confidence**: The assertion that improving ID accuracy is key to better OOD performance assumes the correlation holds across all types of distribution shifts

## Next Checks
1. Test the ID-OOD correlation on datasets with extreme distribution shifts to verify the relationship breaks down as predicted
2. Evaluate additional model architectures beyond those included in the benchmark to confirm the conclusion about baseline methods
3. Analyze the impact of different types of distribution shifts (covariate, concept, label) on the correlation between shift gap and label distribution changes