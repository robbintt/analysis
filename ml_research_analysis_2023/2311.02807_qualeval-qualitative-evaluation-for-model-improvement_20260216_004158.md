---
ver: rpa2
title: 'QualEval: Qualitative Evaluation for Model Improvement'
arxiv_id: '2311.02807'
source_url: https://arxiv.org/abs/2311.02807
tags:
- domains
- evaluation
- qualeval
- insights
- sub-tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QUALEVAL addresses the limitations of scalar evaluation metrics
  in LLMs by introducing a qualitative evaluation framework that provides actionable
  insights for model improvement. It automatically discovers relevant domains and
  sub-tasks in a dataset, assigns attributes to instances using a novel flexible linear
  programming solver, and generates human-readable insights backed by comprehensive
  dashboards.
---

# QualEval: Qualitative Evaluation for Model Improvement

## Quick Facts
- **arXiv ID**: 2311.02807
- **Source URL**: https://arxiv.org/abs/2311.02807
- **Reference count**: 15
- **Primary result**: Up to 15% relative improvement in ROUGE-L score on DialogSum through targeted data augmentation guided by qualitative insights

## Executive Summary
QUALEVAL introduces a qualitative evaluation framework that automatically discovers relevant domains and sub-tasks in a dataset, assigns attributes to instances using a novel flexible linear programming solver, and generates human-readable insights backed by comprehensive dashboards. This approach addresses the limitations of scalar evaluation metrics by providing actionable insights that enable targeted model improvements. The method is task-agnostic and has demonstrated significant performance gains across diverse tasks including code generation, dialogue summarization, and clinical knowledge assessment.

## Method Summary
QUALEVAL works by first discovering relevant domains and sub-tasks in a dataset using an LLM evaluator, then assigning these attributes to instances through a flexible linear programming solver that respects prior probabilities while maximizing affinity scores. The system generates a dashboard visualizing proficiency breakdowns and skill alignment, along with natural language insights that guide data augmentation. When applied to the Llama 2 model, QUALEVAL achieved up to 15% relative improvement in ROUGE-L score on DialogSum by selectively augmenting training data with instances from under-performing domains.

## Key Results
- Up to 15% relative improvement in ROUGE-L score on DialogSum dataset
- 0.7% absolute accuracy gain on MBPP code generation task
- 0.5% absolute gain on MMLU clinical knowledge task
- Successfully demonstrated across three diverse tasks: code generation, dialogue summarization, and clinical knowledge assessment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: QUALEVAL improves model performance by providing fine-grained domain and sub-task insights that enable targeted data augmentation.
- Mechanism: The system automatically discovers relevant domains and sub-tasks, assigns instances to these attributes using a flexible linear programming solver, and generates actionable insights through a comprehensive dashboard. These insights allow practitioners to selectively augment training data with instances from under-performing domains, leading to improved model performance on those specific areas.
- Core assumption: The discovered domains and sub-tasks are faithful representations of the dataset's structure, and the model's performance varies significantly across these attributes.
- Evidence anchors:
  - [abstract]: "QUALEVAL uses a powerful LLM reasoner and our novel flexible linear programming solver to generate human-readable insights that when applied, accelerate model improvement."
  - [section 4.4]: "Across different sets of domains (rows), QUALEVAL consistently and significantly increases the proficiency of the selected domains and the overall performance."
  - [corpus]: Weak - no direct citations found for the specific domain/sub-task discovery and improvement mechanism.
- Break condition: If the discovered domains/sub-tasks do not align with the actual structure of the dataset, or if the model's performance is relatively uniform across all attributes.

### Mechanism 2
- Claim: The flexible linear programming solver ensures that attribute assignments respect the prior probabilities of domains/sub-tasks while maximizing affinity.
- Mechanism: The solver assigns each instance to exactly 2 domains and 2 sub-tasks, with the number of assignments to each attribute proportional to its prior probability. It also maximizes the total affinity between instances and their assigned attributes.
- Core assumption: The affinity scores between instances and attributes can be reliably computed using the LLM evaluator, and the prior probability constraint is necessary for meaningful analysis.
- Evidence anchors:
  - [section 2.2]: "We use a novel flexible linear programming solver to perform the attribute assignment by ensuring the following properties: (1) An instance is assigned 2 domains and 2 sub-tasks each so that we can give concrete insights. (2) The number of assignments to an attribute is proportional to the prior probability of the attribute."
  - [corpus]: Weak - no direct citations found for the specific LP formulation used in QUALEVAL.
- Break condition: If the affinity scores are unreliable or if the prior probability constraint is too restrictive or too lenient for the dataset.

### Mechanism 3
- Claim: The skill usage calibration metric reveals whether the model is leveraging the expected sub-tasks when generating responses.
- Mechanism: The system computes the correlation between the sub-task affinity scores of the ground truth and the model's generated answer. A smaller distance indicates better alignment between the model's reasoning and the intended sub-tasks.
- Core assumption: The sub-task affinity scores can be meaningfully computed for both ground truth and model-generated responses, and the correlation between these scores is a valid measure of skill usage calibration.
- Evidence anchors:
  - [section 5.1]: "We quantify the calibration by first identifying the affinity of the ground truth and model generated answer to different sub-tasks discovered. We then measure the distance between the affinity scores."
  - [section 5.2]: "Interestingly, the generated output in the final example (right) is a more robust solution than the ground truth. The ground truth solution assumes that the input is a list of booleans, while the model generation can accept any list with any data type."
  - [corpus]: Weak - no direct citations found for the specific skill usage calibration metric used in QUALEVAL.
- Break condition: If the sub-task affinity scores are not reliable or if the correlation between ground truth and model scores does not accurately reflect skill usage.

## Foundational Learning

- **Concept: Linear Programming**
  - Why needed here: The flexible LP solver is used to assign instances to domains and sub-tasks while respecting prior probabilities and maximizing affinity.
  - Quick check question: Can you formulate the attribute assignment problem as a linear program with the given constraints?

- **Concept: Natural Language Processing**
  - Why needed here: The LLM evaluator is used to discover domains and sub-tasks, compute affinity scores, and generate insights from the dashboard.
  - Quick check question: How would you prompt an LLM to discover relevant domains and sub-tasks from a dataset?

- **Concept: Data Augmentation**
  - Why needed here: The insights from QUALEVAL are used to selectively augment the training data with instances from under-performing domains, leading to improved model performance.
  - Quick check question: How would you design an experiment to evaluate the effectiveness of data augmentation based on QUALEVAL's insights?

## Architecture Onboarding

- **Component map**: LLM Evaluator -> Flexible Linear Programming Solver -> Dashboard -> Data Augmentation Pipeline
- **Critical path**: 
  1. Discover domains and sub-tasks using the LLM evaluator
  2. Compute affinity scores for each instance-attribute pair
  3. Solve the linear program to assign attributes to instances
  4. Evaluate model proficiency on each attribute
  5. Generate insights and visualizations in the dashboard
  6. Use insights to augment training data and improve model
- **Design tradeoffs**:
  - Context window limitations of LLMs vs. scalability of attribute discovery
  - Flexibility of prior probability constraint vs. adherence to true attribute distribution
  - Granularity of domains/sub-tasks vs. interpretability of insights
- **Failure signatures**:
  - Poor alignment between discovered attributes and actual dataset structure
  - Inconsistent or unreliable affinity scores from the LLM evaluator
  - Ineffective data augmentation leading to minimal or no performance improvement
- **First 3 experiments**:
  1. Evaluate QUALEVAL's domain/sub-task discovery on a dataset with known ground truth attributes
  2. Test the impact of different prior probability constraints on attribute assignment quality
  3. Measure the correlation between QUALEVAL's insights and actual model performance improvements after data augmentation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of attribute discovery in QUALEVAL scale with the size of the dataset?
- Basis in paper: [explicit] The paper mentions that QUALEVAL iteratively samples k instances from the dataset and repeats the prompting process |D|/k times to generate a large list of attributes, but does not provide empirical evidence on how this scaling affects attribute quality.
- Why unresolved: The paper does not present experiments or analysis demonstrating the relationship between dataset size and the quality of discovered attributes.
- What evidence would resolve it: Experiments showing attribute discovery performance on datasets of varying sizes, along with quantitative measures of attribute quality (e.g., precision, recall) across different dataset sizes.

### Open Question 2
- Question: What is the impact of the flexible linear programming solver's slack parameter (ϵ) on the quality of attribute assignments?
- Basis in paper: [explicit] The paper mentions that the prior probability constraint is made flexible by adding slack ϵ × pj × |D| × 2 to accommodate for noisiness, but does not explore how different values of ϵ affect the assignment quality.
- Why unresolved: The paper does not provide experiments or analysis on how varying the slack parameter influences the faithfulness or accuracy of the attribute assignments.
- What evidence would resolve it: Experiments varying the slack parameter ϵ and measuring the resulting attribute assignment quality, along with analysis of the trade-off between assignment flexibility and assignment accuracy.

### Open Question 3
- Question: How does QUALEVAL's performance compare to human-annotated attribute assignments?
- Basis in paper: [inferred] The paper mentions that an expert verification of attribute assignments found them to be correct on average 84% and 90% of the time, suggesting that human-annotated assignments could serve as a gold standard for comparison.
- Why unresolved: The paper does not present a direct comparison between QUALEVAL's attribute assignments and human-annotated assignments, which would provide a more concrete measure of QUALEVAL's accuracy.
- What evidence would resolve it: A comparison of QUALEVAL's attribute assignments to a set of human-annotated assignments, with quantitative measures of agreement (e.g., F1 score) and qualitative analysis of differences.

## Limitations

- The additive value compared to existing scalar evaluation methods is not quantified, making it unclear whether QUALEVAL provides incremental benefits.
- The computational cost of LLM-based reasoning and LP solving at scale is not measured, which is critical for practical deployment.
- The choice of exactly 2 domains and 2 sub-tasks per instance appears arbitrary without ablation studies showing the impact of different values.

## Confidence

- **High confidence**: The core mechanism of using LLM reasoning for qualitative analysis and LP-based attribute assignment is well-specified and mathematically sound.
- **Medium confidence**: The reported performance improvements (15% relative on DialogSum, 0.7% absolute on MBPP, 0.5% on MMLU) are likely valid for the tested scenarios, though the additive value compared to existing methods remains unclear.
- **Low confidence**: The scalability of QUALEVAL for larger datasets or more complex attribute structures is not demonstrated.

## Next Checks

1. **Baseline comparison validation**: Replicate the DialogSum experiments using established scalar evaluation methods (e.g., perplexity-based analysis, traditional error analysis) to quantify the incremental benefit of QUALEVAL's qualitative insights.

2. **Scalability testing**: Apply QUALEVAL to a larger dataset (e.g., 10K+ instances) and measure the computational overhead of the LLM evaluator and LP solver, including token costs and runtime scaling.

3. **Parameter sensitivity analysis**: Conduct ablation studies varying the number of assigned domains/sub-tasks (1, 2, 3, 4) and different prior probability constraint formulations to identify optimal configurations for different task types.