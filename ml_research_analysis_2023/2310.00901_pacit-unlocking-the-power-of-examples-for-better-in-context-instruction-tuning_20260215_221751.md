---
ver: rpa2
title: 'PACIT: Unlocking the Power of Examples for Better In-Context Instruction Tuning'
arxiv_id: '2310.00901'
source_url: https://arxiv.org/abs/2310.00901
tags:
- examples
- tadis
- thinking
- training
- example
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TADIS, a novel in-context instruction tuning
  method that encourages models to actively learn distinctions between positive and
  negative examples rather than merely reading them. The method prompts the model
  to first verify the correctness of provided examples according to the task description,
  then uses this verification as a condition to generate better responses.
---

# PACIT: Unlocking the Power of Examples for Better In-Context Instruction Tuning

## Quick Facts
- arXiv ID: 2310.00901
- Source URL: https://arxiv.org/abs/2310.00901
- Reference count: 10
- Primary result: TADIS achieves up to 9.16 and 3.14 average ROUGE-L score improvements over ICIT baselines on in-domain and out-domain tasks respectively

## Executive Summary
This paper proposes TADIS (Task-Aware Dynamic In-Context Tuning), a novel method for in-context instruction tuning that addresses the limitation of models passively reading examples. Instead of merely presenting positive and negative examples, TADIS prompts the model to actively verify the correctness of each example according to the task definition before generating responses. The method consists of two sequential stages: thinking (example verification) and answering (response generation conditioned on verification results). Experiments demonstrate that TADIS outperforms the standard ICIT baseline across both in-domain and out-domain tasks, with particular effectiveness when using self-generated examples.

## Method Summary
TADIS is a two-stage in-context instruction tuning method that modifies how models process examples during training. The method first requires the model to verify the correctness of each provided example (positive or negative) based on the task definition, then uses these verification results as conditions for generating the final answer. During training, the model is fine-tuned on the SuperNI-V2 dataset using the Adam optimizer with a learning rate of 2 × 10^-4 for five epochs. The overall training loss combines both the thinking (verification) and answering (generation) losses. The method is evaluated using ROUGE-L metrics in both zero-shot and few-shot settings, comparing performance against standard ICIT baselines on held-in and held-out datasets.

## Key Results
- TADIS achieves up to 9.16 average ROUGE-L improvement over ICIT on in-domain tasks
- TADIS achieves up to 3.14 average ROUGE-L improvement over ICIT on out-domain tasks
- The method shows enhanced performance when using self-generated examples, particularly effective in low-resource settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TADIS steers models to actively verify the correctness of examples rather than passively reading them.
- Mechanism: By requiring the model to classify examples as positive or negative based on the task definition before generating an answer, TADIS forces deeper semantic processing of examples.
- Core assumption: Models exhibit "illusion of competence" and learn surface formats instead of underlying mappings when examples are explicitly labeled as correct/incorrect.
- Evidence anchors:
  - [abstract] "we propose PACIT, a simple and effective in-context instruction tuning method, inspired by the pedagogical concept of desirable difficulty. The PACIT method unlocks the power of examples by encouraging the model to actively learn to grasp the distinctions between the positive and negative examples instead of merely reading."
  - [section] "We suppose that is because the model tends to merely see instead of thinking when the correctness of examples is explicitly presented."
  - [corpus] Weak - no direct corpus evidence on illusion of competence in models.
- Break condition: If models already naturally learn input-output mappings without explicit prompting, or if the verification step adds too much cognitive load.

### Mechanism 2
- Claim: TADIS improves performance by combining thinking and answering in a sequential manner.
- Mechanism: The model first predicts the correctness of examples (thinking stage), then uses this prediction as context to generate the final answer (answering stage), creating a coherent chain of reasoning.
- Core assumption: Sequential processing of verification followed by generation is more effective than parallel or separate processing.
- Evidence anchors:
  - [abstract] "The model is expected to first verify the correctness of the provided example according to the task definition, which is then set as the condition for generating a better response to the task instance."
  - [section] "The overall training loss is the sum of these two losses L = Lthink + Lanswer."
  - [corpus] Weak - no direct corpus evidence on sequential vs parallel processing benefits.
- Break condition: If the sequential approach introduces latency that outweighs performance gains, or if the model cannot maintain context between stages.

### Mechanism 3
- Claim: TADIS effectiveness scales with model size, with larger models benefiting more from the thinking stage.
- Mechanism: Larger models have greater capacity to engage in complex reasoning, so the additional cognitive load of verifying examples is more effectively leveraged.
- Core assumption: Model size correlates with reasoning capacity, and the thinking stage provides a form of "warm-up" that primes larger models for better generation.
- Evidence anchors:
  - [abstract] "Moreover, we construct three types of thinking labels with different model sizes (770M and 3B). The results indicate that small models learn from the format of provided examples, while large models can be steered for careful thinking."
  - [section] "The smaller model( T5-Large-LM-Adapt) only demonstrated a marginal increase of 0.94 and 0.72 average ROUGEL-L improvement. We suppose that is because larger models have stronger learning capabilities and can benefit more from TADIS methods."
  - [corpus] Weak - no direct corpus evidence on scaling laws for verification-based reasoning.
- Break condition: If the benefit plateaus or reverses for very large models, or if the thinking stage becomes redundant for models with strong zero-shot reasoning.

## Foundational Learning

- Concept: Desirable difficulty
  - Why needed here: TADIS is inspired by the pedagogical concept that making learning slightly more challenging (by requiring verification) improves retention and understanding.
  - Quick check question: What is the difference between desirable difficulty and simple task complexity?

- Concept: In-context learning (ICL)
  - Why needed here: TADIS operates within the ICL paradigm, modifying how examples are presented and processed during instruction tuning.
  - Quick check question: How does ICL differ from traditional fine-tuning in terms of data presentation?

- Concept: Zero-shot and few-shot learning
  - Why needed here: TADIS is evaluated in both zero-shot and few-shot settings, so understanding these paradigms is critical for interpreting results.
  - Quick check question: What distinguishes zero-shot from few-shot learning in the context of instruction tuning?

## Architecture Onboarding

- Component map: Task definition → Example verification (thinking) → Answer generation (answering)
- Critical path: Task definition → Example verification (thinking) → Answer generation (answering). The thinking stage must complete before the answering stage can begin.
- Design tradeoffs: TADIS increases prompt length and computational cost due to the additional verification step, but this is offset by performance gains. The sequential nature may limit parallelization.
- Failure signatures: Performance degradation if the model cannot accurately verify examples, if the thinking stage introduces noise, or if the sequential processing disrupts the flow of generation.
- First 3 experiments:
  1. Train TADIS on a small subset of tasks with ground-truth verification labels to validate the basic mechanism.
  2. Compare TADIS with and without the thinking stage on a held-out set to measure the impact of verification.
  3. Test TADIS with generated examples (using Self-instruct) to assess scalability in low-resource settings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the TADIS method's effectiveness scale with model size beyond the tested 3B parameters?
- Basis in paper: [explicit] The paper tested TADIS on T5-Large (770M) and T5-XL (3B) models, observing that larger models benefit more, but did not test models larger than 3B.
- Why unresolved: The paper mentions computational resource limitations prevented testing larger models like LLaMA, leaving open the question of whether TADIS would provide similar or greater benefits for even larger models.
- What evidence would resolve it: Experiments applying TADIS to models with 10B+ parameters, comparing performance improvements against baselines across various tasks and settings.

### Open Question 2
- Question: How does TADIS perform when the ratio of positive to negative examples is highly imbalanced?
- Basis in paper: [inferred] The paper tested TADIS with various example combinations but did not specifically analyze performance under highly imbalanced positive/negative ratios, which is common in real-world scenarios.
- Why unresolved: The paper focused on balanced or near-balanced example ratios, leaving uncertainty about TADIS's robustness when one type of example vastly outnumbers the other.
- What evidence would resolve it: Systematic experiments varying the positive-to-negative example ratio from 10:1 to 1:10, measuring TADIS performance against baselines across multiple tasks.

### Open Question 3
- Question: What is the relationship between the quality of self-generated examples and TADIS performance?
- Basis in paper: [explicit] The paper found TADIS works with self-generated examples but noted only 54% were valid, suggesting quality impacts performance.
- Why unresolved: The paper didn't analyze how varying the validity rate of generated examples affects TADIS effectiveness, nor did it explore methods to improve example quality.
- What evidence would resolve it: Experiments systematically varying the percentage of valid generated examples (e.g., 30%, 54%, 70%, 90%) and measuring TADIS performance, plus analysis of which example characteristics most impact effectiveness.

## Limitations
- Evaluation relies solely on ROUGE-L metric, potentially missing task-specific performance nuances
- SuperNI-V2 dataset is limited to English tasks, raising questions about cross-lingual applicability
- Computational overhead of sequential verification-generation pipeline is not quantified

## Confidence
- **High Confidence (4/5)**: Core empirical finding that TADIS improves ROUGE-L scores over ICIT baselines
- **Medium Confidence (3/5)**: Claim that TADIS scales with model size is supported but mechanism remains hypothesized
- **Low Confidence (2/5)**: Pedagogical inspiration from "desirable difficulty" lacks direct empirical validation

## Next Checks
1. **Ablation Study on Sequential Processing**: Compare TADIS with a parallel processing variant where thinking and answering stages operate independently, to quantify the contribution of sequential reasoning versus simple addition of verification tasks.

2. **Cross-Architectural Validation**: Implement TADIS on at least two additional model families (e.g., GPT-style and LLaMA-style architectures) across a broader range of parameter sizes (1B, 7B, 13B) to test whether the scaling benefits hold beyond T5-LM-Adapt.

3. **Metric Diversity Assessment**: Evaluate TADIS using task-specific metrics (e.g., accuracy for classification, BLEU for translation) alongside ROUGE-L across all task types to determine whether the method's benefits extend beyond text generation quality to actual task performance.