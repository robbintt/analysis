---
ver: rpa2
title: An Incremental Update Framework for Online Recommenders with Data-Driven Prior
arxiv_id: '2312.15903'
source_url: https://arxiv.org/abs/2312.15903
tags:
- data
- feature
- prior
- incremental
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the overfitting problem in online recommender
  systems during incremental updates, where models trained on newly arrived data ignore
  long-term user interests and long-tail items. The authors propose a model-agnostic
  framework called DDP (Data-Driven Prior), which consists of Feature Prior (FP) and
  Model Prior (MP).
---

# An Incremental Update Framework for Online Recommenders with Data-Driven Prior

## Quick Facts
- arXiv ID: 2312.15903
- Source URL: https://arxiv.org/abs/2312.15903
- Reference count: 35
- Primary result: DDP framework achieves 1.99% CTR gain and 2.97% eCPM gain in online A/B testing

## Executive Summary
This paper addresses the overfitting problem in online recommender systems during incremental updates, where models trained on newly arrived data ignore long-term user interests and long-tail items. The authors propose a model-agnostic framework called DDP (Data-Driven Prior), which consists of Feature Prior (FP) and Model Prior (MP). FP estimates average CTR values for specific feature values to provide stable learning, while MP incorporates previous model outputs into current updates using Bayes rules. The framework is validated on two public datasets and an industrial dataset, showing consistent improvements in AUC and LogLoss metrics compared to state-of-the-art methods.

## Method Summary
The DDP framework combines Feature Prior (FP) and Model Prior (MP) components to address overfitting during incremental updates. FP estimates CTR values at the feature level using sigmoid-activated embeddings, providing stability through aggregated data. MP regularizes updates by minimizing distance between current and previous model outputs following Bayes rules. The framework is end-to-end trainable with different optimizers for different components - SGD for FP and Adam for the main model. It integrates seamlessly with various interaction modules without modifying their architectures.

## Key Results
- Consistent improvements in AUC and LogLoss metrics across two public datasets and industrial dataset
- 1.99% CTR gain and 2.97% eCPM gain demonstrated in online A/B testing on advertising platform
- Framework is model-agnostic and compatible with six different interaction modules (DNN, DCN, W&D, DeepFM, FwFM, AutoInt)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature Prior (FP) provides more stable learning by estimating CTR at feature-level rather than instance-level.
- Mechanism: FP estimates average CTR for each feature value using a sigmoid activation over a learned embedding vector. This exploits the more aggregated nature of feature-level data, reducing instability from sparse instance-level clicks.
- Core assumption: CTR distributions at the feature granularity are more stable than at the instance level, especially for long-tail items.
- Evidence anchors: [abstract] "FP performs the click estimation for each specific value to enhance the stability of the training process." [section] "The distribution of CTR value at the feature granularity presents more stable than the distribution at the instance level, since the data are more aggregated for the features."

### Mechanism 2
- Claim: Model Prior (MP) regularizes incremental updates by minimizing distance between current and previous model outputs.
- Mechanism: MP approximates posterior on complete data by optimizing likelihood on incremental data while constraining output distance to previous model, following Bayes rules.
- Core assumption: Output-level regularization is more stable than parameter-level regularization for continual learning.
- Evidence anchors: [abstract] "MP incorporates previous model output into the current update while strictly following the Bayes rules." [section] "MRNFS [1] further simplifies the parameter space into the output of the models, with the aim of more stable learning..."

### Mechanism 3
- Claim: End-to-end integration of FP and MP creates a model-agnostic framework compatible with various interaction modules.
- Mechanism: FP output is discretized and concatenated with original embeddings before interaction module; MP loss is added to training objective without modifying interaction architecture.
- Core assumption: Adding feature prior information as auxiliary features and output regularization doesn't interfere with interaction module's ability to capture complex feature interactions.
- Evidence anchors: [abstract] "Both the FP and MP are well integrated into the unified framework, which is model-agnostic and can accommodate various advanced interaction models." [section] "Thus, the objective function follows: L (x, s, ùë¶, ùúÉùë° ‚àí1; Œò) = Lùëô + ùúÜ 2 Lùëù"

## Foundational Learning

- Concept: Incremental update framework
  - Why needed here: Industrial scenarios require quick response with streaming data, making full retraining impractical
  - Quick check question: What's the main challenge when using only newly arrived data for model updates?

- Concept: Feature-level vs instance-level CTR estimation
  - Why needed here: Feature-level CTR is more stable due to aggregated data, helping with long-tail item prediction
  - Quick check question: Why does the paper claim feature-level CTR estimation is more stable than instance-level?

- Concept: Bayes rules for continual learning
  - Why needed here: Provides theoretical foundation for incorporating previous model knowledge into current updates
  - Quick check question: How does the paper use Bayes rules to justify their Model Prior approach?

## Architecture Onboarding

- Component map: Feature ‚Üí Original Embedding ‚Üí FP Estimation ‚Üí Discretization ‚Üí Concatenation ‚Üí Interaction ‚Üí Output ‚Üí Model Prior Loss ‚Üí Backpropagation

- Critical path: Feature values are converted to embeddings, FP estimates CTR for each value, discretization creates bins, concatenated with original embeddings, fed to interaction module, output compared to previous model for regularization

- Design tradeoffs:
  - SGD vs Adam for FP training: SGD chosen to prevent gradient vanishing and maintain long-term knowledge
  - Number of discretization bins: Tradeoff between granularity and overfitting
  - MP weight Œª: Controls balance between fitting new data and maintaining old knowledge

- Failure signatures:
  - Performance degrades on long-tail items: May indicate FP discretization losing too much information
  - Model becomes unstable during incremental updates: May indicate MP weight too high or learning rate issues
  - Training becomes very slow: May indicate too many discretization bins or overly complex interaction module

- First 3 experiments:
  1. Implement DDP with simple DNN interaction module on Criteo dataset, compare AUC with base DNN
  2. Test different discretization bin counts (10, 50, 100) and measure impact on long-tail vs short-head items
  3. Vary MP weight Œª (0.1, 1, 10) and observe stability vs adaptability tradeoff during consecutive incremental updates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DDP compare when applied to different types of deep CTR models beyond the six mentioned in the paper?
- Basis in paper: [explicit] The paper states that DDP is a model-agnostic framework and has been tested with six different interaction modules (DNN, DCN, W&D, DeepFM, FwFM, and AutoInt). However, it does not provide a comprehensive comparison with all possible interaction modules.
- Why unresolved: The paper only tests a limited set of interaction modules, and there could be other models that might show different performance when combined with DDP.
- What evidence would resolve it: Testing DDP with a broader range of interaction modules and comparing their performance would provide a clearer understanding of the framework's compatibility and effectiveness across different model types.

### Open Question 2
- Question: What is the impact of varying the number of bins (B) in the discretization strategy of the Feature Prior layer on the performance of DDP?
- Basis in paper: [explicit] The paper mentions the use of a discretization strategy in the Feature Prior layer but does not explore how different numbers of bins might affect the model's performance.
- Why unresolved: The choice of the number of bins could significantly impact the granularity of the feature prior information and, consequently, the model's ability to capture long-tail characteristics.
- What evidence would resolve it: Conducting experiments with varying numbers of bins and analyzing their impact on AUC, LogLoss, and long-tail performance would help determine the optimal discretization strategy.

### Open Question 3
- Question: How does DDP perform in scenarios with extremely high data drift or when the distribution of online data changes drastically?
- Basis in paper: [explicit] The paper discusses the framework's ability to handle data drift and maintain performance in scenarios with significant changes in data distribution. However, it does not provide specific results for extreme cases of data drift.
- Why unresolved: Understanding the framework's robustness in extreme data drift scenarios is crucial for its practical application in highly dynamic environments.
- What evidence would resolve it: Conducting experiments with datasets that simulate extreme data drift conditions and evaluating DDP's performance would provide insights into its effectiveness in such scenarios.

## Limitations

- Theoretical justifications for design choices (FP stability, MP effectiveness) lack strong empirical validation within the paper
- Ablation studies are limited, only testing MP weight at 1.0 without exploring the full tradeoff space
- No direct comparison between output-level vs parameter-level regularization to validate the stability claims

## Confidence

- High confidence: The overall framework architecture and integration approach
- Medium confidence: The effectiveness of individual components (FP and MP) in isolation
- Low confidence: The theoretical justifications for why specific design choices work

## Next Checks

1. Implement ablation study comparing SGD vs Adam for Feature Prior training to validate the gradient vanishing claim
2. Test different discretization bin counts (10, 50, 100) to quantify the stability vs information loss tradeoff in FP
3. Compare output-level vs parameter-level regularization in Model Prior to directly test the stability claims