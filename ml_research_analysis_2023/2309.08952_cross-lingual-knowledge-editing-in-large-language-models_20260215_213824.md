---
ver: rpa2
title: Cross-Lingual Knowledge Editing in Large Language Models
arxiv_id: '2309.08952'
source_url: https://arxiv.org/abs/2309.08952
tags:
- knowledge
- editing
- language
- edited
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the cross-lingual effect of knowledge editing
  in large language models (LLMs), a topic that has been largely overlooked in previous
  studies. The authors create a large-scale cross-lingual dataset, Bi-ZsRE, by translating
  ZsRE from English to Chinese.
---

# Cross-Lingual Knowledge Editing in Large Language Models

## Quick Facts
- **arXiv ID:** 2309.08952
- **Source URL:** https://arxiv.org/abs/2309.08952
- **Authors:** Not specified
- **Reference count:** 6
- **Key outcome:** Cross-lingual knowledge editing faces challenges in transferring edited knowledge across languages, with performance gaps influenced by language modeling disparities in multilingual LLMs.

## Executive Summary
This paper investigates the cross-lingual effects of knowledge editing in large language models, a topic largely overlooked in previous research. The authors create Bi-ZsRE, a large-scale cross-lingual dataset by translating ZsRE from English to Chinese, and conduct experiments on various knowledge editing methods across different LLMs. The study reveals that language modeling gaps across languages significantly influence knowledge editing efficiency, and existing methods struggle to transfer edited knowledge between languages. The findings highlight the challenges of maintaining consistent model behaviors across languages when editing multilingual LLMs.

## Method Summary
The authors create the Bi-ZsRE dataset by translating the ZsRE dataset from English to Chinese using advanced LLMs, followed by human verification. They then apply seven knowledge editing methods (covering memory-based, meta-learning, and locate-then-edit paradigms) to multilingual LLMs including Chinese-LLaMA-Plus-7B, Chinese-LLaMA-2-7B, and Baichuan-7B. The edited models are evaluated on four metrics: reliability (accuracy on edited knowledge), generality (accuracy on rephrased questions), locality (accuracy on irrelevant samples), and portability (ability to reason based on edited knowledge). Performance is measured using exact match (EM) and F1 scores across both source and target languages.

## Key Results
- Language modeling gaps across languages significantly impact the efficiency of knowledge editing in multilingual LLMs
- Edited knowledge in one language does not easily transfer to other languages, resulting in inconsistent model behaviors
- Editing knowledge in one language can influence locality preservation in other languages

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Language modeling gaps across different languages influence the efficiency of knowledge editing in multilingual LLMs.
- **Mechanism:** Multilingual LLMs exhibit differing language modeling abilities due to imbalanced pre-training data distribution, with stronger performance typically in English. This gap impacts the effectiveness of knowledge editing across languages.
- **Core assumption:** The editing process is more efficient in languages where the LLM has higher language modeling ability.
- **Evidence anchors:**
  - [abstract] "the language modeling gaps across different languages influence the efficiency of knowledge editing"
  - [section] "This is because the language modeling ability of different languages might be different in a single integrated multi-lingual LLM. Many LLMs show their strong English ability perhaps due to the high-quality English data dominating the pre-training corpora"
  - [corpus] Weak evidence - no direct corpus citations for this mechanism
- **Break condition:** If the LLM achieves balanced language modeling performance across languages through balanced pre-training data or specialized fine-tuning, this mechanism would break down.

### Mechanism 2
- **Claim:** Edited knowledge in one language does not easily transfer to other languages in multilingual LLMs.
- **Mechanism:** Multilingual LLMs do not inherently share learned knowledge across languages. Knowledge edited in one language remains largely confined to that language, resulting in inconsistent model behaviors when queried in different languages.
- **Core assumption:** Multilingual LLMs treat different languages as separate knowledge domains rather than sharing cross-lingual representations.
- **Evidence anchors:**
  - [abstract] "it is still hard for existing knowledge editing methods to transfer the edited knowledge from one language to another in a multi-lingual LLM"
  - [section] "This finding also indicates that the cross-lingual performance of knowledge editing is still limited. It is hard for existing knowledge editing methods to transfer the edited knowledge from one language to another in multi-lingual LLMs, and reflect consistent behaviors when querying with different languages."
  - [corpus] Weak evidence - no direct corpus citations for this mechanism
- **Break condition:** If multilingual LLMs develop strong cross-lingual knowledge sharing mechanisms through architecture design or training methodology, this mechanism would break down.

### Mechanism 3
- **Claim:** Editing knowledge in one language can influence locality in other languages.
- **Mechanism:** Knowledge editing in a source language can inadvertently affect the model's performance on unrelated samples in target languages, demonstrating cross-lingual interference in locality preservation.
- **Core assumption:** The editing process in one language has unintended side effects on the model's behavior in other languages.
- **Evidence anchors:**
  - [abstract] "when editing LLMs in a language, the locality in the other languages could also be influenced"
  - [section] "When editing LLMs in a source language, the locality in other languages could also be influenced. The degree of influence appears to be similar in different languages."
  - [corpus] Weak evidence - no direct corpus citations for this mechanism
- **Break condition:** If knowledge editing methods can be designed to perfectly isolate edits to the source language without affecting other languages, this mechanism would break down.

## Foundational Learning

- **Concept:** Knowledge editing techniques
  - **Why needed here:** Understanding the different paradigms of knowledge editing (memory-based, meta-learning, locate-then-edit) is crucial for analyzing their cross-lingual performance.
  - **Quick check question:** What are the three main paradigms of knowledge editing mentioned in the paper?

- **Concept:** Multilingual language models
  - **Why needed here:** Understanding how multilingual LLMs are trained and how they handle different languages is essential for interpreting cross-lingual knowledge editing results.
  - **Quick check question:** Why do many multilingual LLMs show stronger performance in English compared to other languages?

- **Concept:** Cross-lingual transfer
  - **Why needed here:** Understanding the concept of cross-lingual transfer and its challenges is crucial for interpreting the difficulty of transferring edited knowledge across languages.
  - **Quick check question:** What is the main challenge in achieving cross-lingual transfer of edited knowledge in multilingual LLMs?

## Architecture Onboarding

- **Component map:** Multilingual LLMs (Chinese-LLaMA-Plus-7B, Chinese-LLaMA-2-7B, Baichuan-7B) -> Knowledge editing methods (SERAC, IKE, MEND, KN, ROME, MEMIT) -> Bi-ZsRE dataset for evaluation

- **Critical path:** 1) Translate ZsRE dataset to create Bi-ZsRE, 2) Apply knowledge editing methods in source language, 3) Evaluate edited models in both source and target languages across four metrics

- **Design tradeoffs:** The main tradeoff is between the cost of creating high-quality cross-lingual datasets (using advanced LLMs for translation and human verification) and the benefits of studying cross-lingual knowledge editing effects. Another tradeoff is between the generality of edited knowledge across languages and the specificity of edits to the source language.

- **Failure signatures:** 1) Significant performance gaps in reliability when editing in different languages, 2) Poor cross-lingual generality (edited knowledge not transferring across languages), 3) Reduced locality in target languages after source language editing, 4) Limited portability in both languages despite successful editing

- **First 3 experiments:**
  1. Replicate the reliability analysis by applying SERAC in English and Chinese on Chinese-LLaMA-Plus-7B and comparing the F1 scores.
  2. Test cross-lingual generality by editing with IKE in English and evaluating on Chinese generality questions.
  3. Investigate locality influence by applying MEND in English and measuring locality performance in both English and Chinese.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do different pre-training data distributions across languages affect the cross-lingual knowledge editing performance in multilingual LLMs?
- **Basis in paper:** [inferred] The paper discusses how "language modeling gaps across different languages influence the efficiency of knowledge editing" and notes that "Many LLMs show their strong English ability perhaps due to the high-quality English data dominating the pre-training corpora."
- **Why unresolved:** While the paper identifies this as a factor, it does not quantify the relationship between pre-training data distribution and cross-lingual editing performance.
- **What evidence would resolve it:** Experiments varying the proportion of language-specific data in pre-training while keeping model architecture constant, then measuring cross-lingual editing performance.

### Open Question 2
- **Question:** What architectural modifications to multilingual LLMs could improve their ability to transfer edited knowledge across languages?
- **Basis in paper:** [explicit] "it is still hard for existing knowledge editing methods to transfer the edited knowledge from one language to another in a multi-lingual LLM" and the paper calls this a significant challenge.
- **Why unresolved:** The paper identifies the problem but does not propose or test specific architectural solutions.
- **What evidence would resolve it:** Testing modified architectures (e.g., language-specific adapters, cross-lingual parameter sharing schemes) and measuring their cross-lingual editing performance compared to baseline models.

### Open Question 3
- **Question:** How does the complexity and type of knowledge (factual vs. procedural, simple vs. complex reasoning) affect cross-lingual knowledge editing success?
- **Basis in paper:** [inferred] The paper mentions evaluating "portability" which involves reasoning based on edited facts, and notes that "portability questions may involve more intricate reasoning which should be carefully considered during editing knowledge."
- **Why unresolved:** While the paper distinguishes between different types of questions, it doesn't systematically vary knowledge complexity to measure its impact on cross-lingual editing.
- **What evidence would resolve it:** Experiments with knowledge editing datasets systematically varying complexity and type of knowledge, measuring cross-lingual transfer success rates across these dimensions.

### Open Question 4
- **Question:** Can the locality preservation observed when editing in one language (affecting locality in other languages) be mitigated through specific training techniques or architectural constraints?
- **Basis in paper:** [explicit] "when editing LLMs in a language, the locality in the other languages could also be influenced" and this "presents a significant challenge for multi-lingual LLMs in maintaining consistent behaviors across different languages."
- **Why unresolved:** The paper identifies this cross-lingual locality interference but does not explore methods to prevent it.
- **What evidence would resolve it:** Testing techniques like language-specific parameter isolation, regularization methods, or training procedures designed to maintain language-specific locality, then measuring locality preservation across languages after editing.

## Limitations
- Lack of detailed implementation specifications for knowledge editing methods makes exact reproduction challenging
- Translation process for creating Bi-ZsRE lacks transparency in prompt engineering and quality control procedures
- Study focuses on Chinese as target language, limiting generalizability to other language pairs

## Confidence
- **High confidence:** The observed performance gaps across languages in multilingual LLMs (Mechanism 1)
- **Medium confidence:** The difficulty of cross-lingual knowledge transfer (Mechanism 2)
- **Medium confidence:** The cross-lingual influence on locality (Mechanism 3)

## Next Checks
1. Conduct ablation studies to isolate the effect of pre-training data quality from other factors influencing cross-lingual knowledge editing performance
2. Extend experiments to additional language pairs (e.g., English-Spanish, English-German) to test generalizability beyond Chinese
3. Implement controlled experiments to measure the impact of translation quality on cross-lingual knowledge editing effectiveness by comparing results using different translation approaches