---
ver: rpa2
title: An In-Context Learning Agent for Formal Theorem-Proving
arxiv_id: '2310.04353'
source_url: https://arxiv.org/abs/2310.04353
tags:
- proof
- copra
- state
- opra
- proofs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a language-agent approach to formal theorem-proving
  using a high-capacity large language model (GPT-4) within a stateful backtracking
  search. The method, called COPRA, interacts with proof environments like Coq or
  Lean, executing tactics and using feedback to iteratively refine prompts.
---

# An In-Context Learning Agent for Formal Theorem-Proving
## Quick Facts
- arXiv ID: 2310.04353
- Source URL: https://arxiv.org/abs/2310.04353
- Reference count: 30
- Key outcome: COPRA achieves state-of-the-art performance on miniF2F (23.36% pass@1) and CompCert (64.41% success rate) using GPT-4 with 46x fewer inferences than ReProver

## Executive Summary
This paper introduces COPRA, a language-agent approach to formal theorem-proving that uses GPT-4 within a stateful backtracking search framework. The agent interacts with proof environments like Coq or Lean, executing tactics and using feedback to iteratively refine prompts. COPRA can retrieve relevant lemmas from external databases and leverages domain-specific information from search history to reduce hallucinations and unnecessary LLM queries. The method significantly outperforms few-shot GPT-4 invocations and finetuned approaches on benchmarks like miniF2F and CompCert.

## Method Summary
COPRA employs GPT-4 as a policy within a Markov Decision Process framework for theorem proving. The agent maintains a stack of proof states and performs depth-first search with backtracking. When tactics fail, the policy records this information in a Bad(O) dictionary to avoid future failures. The agent uses a partial order ⊑ over states to ensure tactics simplify proof obligations and prevent cycles. Prompts are serialized using a context-free grammar that incorporates the current proof state, history, and failure dictionary. No training or fine-tuning is performed - the approach relies entirely on in-context learning with GPT-4.

## Key Results
- COPRA achieves 23.36% pass@1 on miniF2F, outperforming state-of-the-art ReProver (22.13%) while using 46x fewer inferences
- On CompCert, COPRA proves 64.41% of theorems with an average of 12.9 inferences per proof
- COPRA significantly outperforms baseline Proverbot 9001 (83.05% success rate, 184.7 inferences on average) in inference efficiency
- GPT-4 is essential for success - GPT-3.5 performs significantly worse in the same framework

## Why This Works (Mechanism)
### Mechanism 1: Failure Tracking Dictionary
The COPRA agent maintains a Bad(O) dictionary mapping states to failed tactics, preventing repetition of unsuccessful attempts. When a tactic fails, it's added to this dictionary and excluded from future prompts for that state. Core assumption: failure persistence at given states. Break condition: state changes that could make previously failed tactics successful.

### Mechanism 2: Partial Order State Simplification
Before applying tactics, the agent checks if the resulting state is simpler than the current one using a symbolic procedure verifying O1 ⊑ O2. Tactics that don't simplify the state are rejected. Core assumption: efficient symbolic procedure exists for determining state complexity. Break condition: errors in symbolic procedure could reject useful tactics or accept unproductive ones.

### Mechanism 3: Stateful Backtracking Search
The agent maintains a stack of MDP states and performs depth-first search with backtracking. When reaching dead ends, it pops the stack and tries alternative tactics from previous states. Core assumption: problem space allows efficient exploration of alternative paths. Break condition: search space too large or backtracking inefficient.

## Foundational Learning
- **Concept**: Markov Decision Process formulation of theorem proving
  - Why needed here: Provides formal framework for modeling agent-environment interaction as states, actions, and transitions
  - Quick check question: What are the three components of an MDP state in this context, and how does the partial order ⊑ relate to state transitions?

- **Concept**: In-context learning with large language models
  - Why needed here: Enables tactic generation without fine-tuning using prompts with proof history and environment feedback
  - Quick check question: How does the prompt serialization protocol incorporate both current proof state and history of failed actions?

- **Concept**: Formal proof environments (Lean and Coq)
  - Why needed here: Understanding tactics, goals, and hypotheses syntax is essential for interpreting agent actions
  - Quick check question: What is the difference between a goal and a hypothesis in the proof state, and how do tactics transform these elements?

## Architecture Onboarding
- **Component map**: LLM (GPT-4) wrapper -> Proof environment (Lean/Coq) -> State stack -> Failure dictionary -> Lemma retrieval -> Prompt serialization
- **Critical path**: State → Prompt generation → LLM inference → Action parsing → Environment execution → Feedback incorporation → State update (with possible backtracking)
- **Design tradeoffs**: GPT-4 provides strong reasoning but is expensive; 60-inference cap balances cost vs thoroughness; partial order check prevents cycles but may reject valid tactics
- **Failure signatures**: Agent gets stuck in loops (repeated states), fails to make progress (no simplifying tactics found), or exceeds inference budget without proof
- **First 3 experiments**:
  1. Run agent on simple theorem with known solution to verify basic functionality and prompt parsing
  2. Test failure tracking by providing failing tactic and confirming exclusion from future prompts
  3. Evaluate partial order check by creating tactic cycle and verifying agent detects and avoids it

## Open Questions the Paper Calls Out
1. What is the upper limit of proof complexity COPRA can handle within the 60 inference budget?
   - Basis: Paper caps inferences at 60 due to cost reasons but doesn't explore larger budgets
   - Why unresolved: No experiments with increased inference budgets to find diminishing returns
   - Evidence needed: Performance comparisons with increasing budgets (60, 120, 240)

2. Is GPT-4 truly essential, or could a more affordable foundation model achieve similar results?
   - Basis: GPT-4 outperforms GPT-3.5 significantly, but other models untested
   - Why unresolved: Only compares GPT-4 with GPT-3.5
   - Evidence needed: Benchmarking with various foundation models (GPT-3.5, Claude, LLaMA)

3. How would fine-tuning an LLM policy using reinforcement learning compare to in-context learning?
   - Basis: Paper mentions fine-tuning with RL as possibility but doesn't explore
   - Why unresolved: Focuses on in-context learning without investigating RL fine-tuning
   - Evidence needed: Implementation and comparison of fine-tuned RL policy vs in-context learning

## Limitations
- Relies on GPT-4's capabilities without fine-tuning, making it dependent on base model's reasoning abilities and costly to operate
- Stateful backtracking may struggle with highly complex proofs requiring search depth beyond 60-inference cap
- Symbolic procedure for checking state simplification (⊑) is mentioned but not detailed, raising questions about completeness

## Confidence
- **High Confidence**: COPRA's state-of-the-art performance on miniF2F and CompCert benchmarks with significantly fewer inferences than baselines is well-supported by experimental results
- **Medium Confidence**: Mechanism explanations for failure tracking and partial order checking are logically consistent but lack implementation specifics for full verification
- **Medium Confidence**: GPT-4's essential role is supported by comparative results with GPT-3.5, though additional ablation studies could strengthen this claim

## Next Checks
1. Implement the partial order ⊑ procedure: Create test cases with known complexity relationships and verify the agent correctly identifies and avoids non-progressing tactics
2. Stress-test failure tracking: Run agent on problems requiring multiple backtracking steps and measure efficient avoidance of previously failed tactics without premature elimination
3. Cost-performance tradeoff analysis: Evaluate COPRA's performance with progressively lower inference budgets to quantify relationship between computational cost and proof success rate, particularly for harder problems