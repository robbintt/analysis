---
ver: rpa2
title: Semantic Consistency for Assuring Reliability of Large Language Models
arxiv_id: '2308.09138'
source_url: https://arxiv.org/abs/2308.09138
tags:
- consistency
- semantic
- answer
- question
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for measuring semantic consistency
  in language models and a prompting strategy called Ask-to-Choose (A2C) to improve
  it. The authors argue that existing consistency metrics rely too heavily on lexical
  similarity and do not capture semantic equivalence.
---

# Semantic Consistency for Assuring Reliability of Large Language Models

## Quick Facts
- arXiv ID: 2308.09138
- Source URL: https://arxiv.org/abs/2308.09138
- Authors: 
- Reference count: 9
- Key outcome: This paper introduces a framework for measuring semantic consistency in language models and a prompting strategy called Ask-to-Choose (A2C) to improve it.

## Executive Summary
This paper addresses the challenge of measuring and improving semantic consistency in large language models (LLMs). The authors propose a novel framework that moves beyond traditional lexical consistency metrics to capture true semantic equivalence. They introduce the Ask-to-Choose (A2C) prompting strategy, which leverages the model's reasoning capabilities to select the most consistent answer from multiple candidates. The framework is evaluated on the TruthfulQA benchmark, demonstrating improved correlation with human judgments and significant improvements in both accuracy and consistency metrics.

## Method Summary
The authors develop a semantic consistency framework that generates paraphrases of questions using in-context learning with an auxiliary LLM. They then generate answers from both original and paraphrased questions, calculating semantic consistency using multiple metrics including entropy-based clustering, entailment/contradiction detection, and paraphrase detection. The Ask-to-Choose (A2C) strategy involves presenting multiple answer candidates to the model and asking it to choose the most correct one. The framework is evaluated across multiple LLMs with varying sizes and training approaches, comparing results with and without A2C application.

## Key Results
- Semantic clustering entropy correlates strongly with human judgments (ρ = 0.83)
- A2C improves accuracy metrics for pretrained and finetuned LLMs by up to 47%
- Semantic consistency metrics show significantly higher correlation with human evaluations than traditional lexical metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic consistency metrics capture true meaning equivalence better than lexical metrics
- Mechanism: Replaces exact token matching with semantic similarity functions (entailment, contradiction, paraphrase detection) to evaluate whether generated outputs convey the same meaning despite lexical differences
- Core assumption: Semantic similarity functions accurately reflect human judgments of meaning equivalence
- Evidence anchors:
  - [abstract] "Our proposal demonstrates significantly higher consistency and stronger correlation with human evaluations of output consistency than traditional metrics based on lexical consistency"
  - [section] "Entropy shows strong correlation with human annotations, followed by entailment, PP (ρ = 0.83, 0.73, 0.55, respectively)"
  - [corpus] Weak evidence - related papers focus on structured output reliability but don't directly validate semantic metrics against human judgment
- Break condition: If semantic similarity functions fail to capture nuance in meaning (e.g., sarcasm, context-dependent meaning) or are biased toward certain linguistic patterns

### Mechanism 2
- Claim: Ask-to-Choose (A2C) prompting improves both accuracy and consistency by leveraging model's reasoning capabilities
- Mechanism: Presents multiple answer candidates to model and asks it to choose the most correct one, effectively adding a reasoning layer that filters inconsistent outputs
- Core assumption: Instruction-tuned models can effectively rank answer candidates based on semantic consistency with original question
- Evidence anchors:
  - [abstract] "A2C increases accuracy metrics for pretrained and finetuned LLMs by up to 47%, and semantic consistency metrics for instruction-tuned models by up to 7-fold"
  - [section] "We see the highest improvement in Flan-T5 XL, whose R1-C score increases by more than 7-fold (4 to 32.2)"
  - [corpus] Weak evidence - related papers on consistency scoring don't explore answer selection prompting strategies
- Break condition: If model fails to properly rank candidates (e.g., when candidates are all equally plausible or model lacks instruction-following capability)

### Mechanism 3
- Claim: Larger models show higher consistency but lower accuracy on TruthfulQA, revealing an inverse scaling phenomenon
- Mechanism: As model size increases, parameter space captures more linguistic patterns leading to consistent but potentially less accurate factual responses
- Core assumption: Model size correlates with ability to generate semantically consistent outputs regardless of factual accuracy
- Evidence anchors:
  - [section] "As the size of LLMs increases, their generated answers tend to be less accurate, illustrating an inverse scaling phenomenon"
  - [section] "Models with higher parameter size tend to be more consistent, especially for contextual paraphrasing"
  - [corpus] Moderate evidence - related papers on uncertainty quantification discuss model reliability but not this specific inverse relationship
- Break condition: If model architecture or training objectives fundamentally change the relationship between size and consistency/accuracy

## Foundational Learning

- Concept: Semantic similarity and entailment
  - Why needed here: Core to evaluating whether generated answers convey the same meaning despite lexical differences
  - Quick check question: Can you explain the difference between lexical and semantic similarity in 2-3 sentences?

- Concept: Prompt engineering and in-context learning
  - Why needed here: A2C strategy relies on carefully designed prompts to elicit consistent outputs from models
  - Quick check question: What's the difference between few-shot prompting and chain-of-thought prompting?

- Concept: Information entropy and clustering
  - Why needed here: Semantic clustering entropy provides an implicit measure of consistency across multiple outputs
  - Quick check question: How does semantic entropy relate to consistency in multi-output scenarios?

## Architecture Onboarding

- Component map: TruthfulQA questions -> Paraphrase generator (auxiliary LLM with context templates) -> Answer generator (main LLM with temperature variations) -> Semantic consistency evaluator (entailment, contradiction, paraphrase detection, entropy) -> A2C module (answer selection prompt template) -> Output layer (consistency and accuracy scores)

- Critical path: 1. Generate paraphrases → 2. Generate answers → 3. Calculate pairwise semantic similarity → 4. Compute consistency metrics → 5. Apply A2C (optional) → 6. Re-evaluate metrics

- Design tradeoffs:
  - Semantic vs. lexical metrics: More computationally expensive but better reflects human judgment
  - Temperature variation vs. contextual paraphrasing: Temperature is simpler but may generate less diverse outputs
  - Multiple LLM calls vs. single call: Higher accuracy but increased cost and latency

- Failure signatures:
  - High consistency but low accuracy: Model generates consistent but factually incorrect answers
  - Low correlation with human judgment: Semantic metrics misaligned with human perception of meaning
  - A2C doesn't improve consistency: Model cannot effectively rank answer candidates

- First 3 experiments:
  1. Baseline: Run semantic consistency evaluation on OPT-125M without A2C to verify correlation with human judgment
  2. Temperature sweep: Compare consistency across temperature variations for a single question
  3. A2C validation: Apply A2C to Flan-T5 XL and measure improvement in both accuracy and consistency metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the semantic consistency metric generalize to multimodal generative tasks beyond text?
- Basis in paper: [inferred] The paper mentions that the framework could be used for multimodal generative tasks if semantic equivalence agreement functions are available across domains like text, image, and audio generation.
- Why unresolved: The paper only evaluates the framework on text generation tasks. Extending it to multimodal tasks requires developing domain-specific semantic equivalence functions and evaluating their effectiveness.
- What evidence would resolve it: Developing and testing semantic equivalence functions for multimodal tasks, then evaluating the framework's performance on multimodal benchmarks.

### Open Question 2
- Question: How does the choice of paraphrasing strategy affect semantic consistency across different model architectures and sizes?
- Basis in paper: [explicit] The paper uses two paraphrasing strategies (in-context and cross-temperature) but doesn't deeply analyze how the choice of strategy impacts consistency across different model types.
- Why unresolved: The paper shows that different models exhibit different consistency patterns, but doesn't systematically explore how paraphrasing strategy interacts with model architecture and size.
- What evidence would resolve it: A comprehensive study comparing consistency across multiple paraphrasing strategies, model architectures, and sizes.

### Open Question 3
- Question: What is the optimal trade-off between inference cost and consistency improvement when applying the A2C method?
- Basis in paper: [explicit] The paper notes that A2C requires six LLM calls per question, which may be too costly for some applications, but doesn't explore optimization strategies.
- Why unresolved: While the paper demonstrates A2C's effectiveness, it doesn't investigate ways to reduce the inference cost while maintaining consistency improvements.
- What evidence would resolve it: Experiments comparing different A2C variants with reduced inference steps and their impact on consistency and accuracy.

## Limitations
- The framework relies heavily on auxiliary LLMs for paraphrase generation and similarity detection
- Human evaluation component has limited sample size (60 examples across 4 annotators)
- Findings are based exclusively on the TruthfulQA benchmark
- A2C strategy's effectiveness may be highly dependent on prompt template design

## Confidence
### High Confidence Claims
- Semantic clustering entropy metric correlates well with human judgments (ρ = 0.83)
- Larger models demonstrate higher consistency but lower accuracy on TruthfulQA
- A2C strategy improves both accuracy and consistency for instruction-tuned models

### Medium Confidence Claims
- Semantic consistency metrics outperform lexical metrics in human evaluation alignment
- The inverse scaling phenomenon between model size and accuracy
- Temperature variations and contextual paraphrasing generate meaningfully different outputs

### Low Confidence Claims
- Generalizability of findings to non-TruthfulQA domains
- Robustness of A2C strategy across different prompt engineering approaches
- Scalability of semantic consistency evaluation for production use

## Next Checks
1. Apply the semantic consistency framework to additional benchmark datasets (e.g., MMLU, GSM8K) to assess generalizability beyond TruthfulQA.

2. Conduct a larger-scale human evaluation study (n > 300 examples) with more annotators to validate the statistical significance of the high inter-annotator agreement.

3. Systematically vary the A2C prompt templates and measure sensitivity of results to template design, including testing zero-shot, few-shot, and chain-of-thought variations.