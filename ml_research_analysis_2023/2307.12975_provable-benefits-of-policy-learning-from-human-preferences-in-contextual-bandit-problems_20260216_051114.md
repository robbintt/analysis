---
ver: rpa2
title: Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit
  Problems
arxiv_id: '2307.12975'
source_url: https://arxiv.org/abs/2307.12975
tags:
- human
- learning
- reward
- rating
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the benefits of using human preferences versus
  direct ratings in offline contextual bandit problems. The authors propose a new
  human rating model that accounts for both bias and uncertainty, addressing limitations
  of existing models.
---

# Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems

## Quick Facts
- arXiv ID: 2307.12975
- Source URL: https://arxiv.org/abs/2307.12975
- Authors: 
- Reference count: 40
- One-line primary result: Preference-based methods achieve O(1/√n) suboptimality decay while direct rating methods can have constant suboptimality even under uniform data coverage.

## Executive Summary
This paper provides the first theoretical comparison between policy learning from human preferences versus direct ratings in offline contextual bandits. The authors introduce a new human rating model that accounts for both bias and uncertainty through general monotone transformations. They prove that standard LCB algorithms using human ratings suffer from constant suboptimality under uniform data coverage assumptions, while pessimistic MLE using preferences achieves O(1/√n) convergence. The analysis demonstrates the theoretical advantage of preference-based methods, particularly in mitigating the impact of human bias on learning performance.

## Method Summary
The paper analyzes policy learning from human feedback in offline contextual bandit settings. For rating methods, a general rating model is introduced where ratings follow h(r(s,a), ε) with r(s,a) as true reward and ε as noise, subject to monotonicity and regularity conditions. The LCB algorithm uses pessimistic penalties based on variance estimates. For preference methods, the Bradley-Terry-Luce model is used with pessimistic MLE algorithm incorporating confidence regions based on covariance estimates. Theoretical analysis establishes lower bounds for rating methods and upper bounds for preference methods, comparing their suboptimality rates under different data coverage assumptions.

## Key Results
- Preference-based methods achieve O(1/√n) suboptimality decay while direct rating methods can have constant suboptimality even under strong data coverage assumptions
- Human rating models with complex noise structures can lead to constant suboptimality due to bias amplification through non-additive noise
- Pessimistic MLE using preferences maintains O(1/√n) convergence by avoiding the amplification of noise by human bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preference-based methods achieve lower suboptimality than direct rating methods in offline contextual bandits.
- Mechanism: Preference-based methods avoid the amplification of noise by human bias through pairwise comparison models (e.g., Bradley-Terry-Luce), whereas direct rating methods suffer from additive bias that distorts the true reward signal.
- Core assumption: The monotonicity condition holds for the rating transformation function, ensuring that expected ratings preserve the true reward ordering.
- Evidence anchors:
  - [abstract]: "They prove lower bounds on suboptimality for standard LCB algorithms using human ratings, showing constant suboptimality is unavoidable even under strong data coverage assumptions."
  - [section 3]: "Our rating model is based on a general class of monotone functions that can account for both human bias and uncertainty during the rating process."
  - [corpus]: "Contrastive Preference Learning: Learning from Human Feedback without RL" suggests preference methods avoid some RLHF challenges.
- Break condition: If the monotonicity condition is violated, preference-based methods lose their advantage because the pairwise comparison no longer reliably preserves reward ordering.

### Mechanism 2
- Claim: Pessimistic maximum likelihood estimation (MLE) using human preferences achieves O(1/√n) suboptimality decay.
- Mechanism: The preference model (BTL) allows consistent estimation of the true reward function from pairwise comparisons, and the pessimistic penalty ensures robust performance under data coverage assumptions.
- Core assumption: The sampling distribution covers state-action pairs that the optimal policy can reach (Assumption 3).
- Evidence anchors:
  - [abstract]: "They adapt results for preference-based methods (pessimistic MLE) to show suboptimality decays at rate O(1/√n)."
  - [section 5]: "Zhu et al. (2023) propose pessimistic MLE... and prove the convergence of pessimistic MLE in the linear bandit setting."
  - [corpus]: "Kernelized Offline Contextual Dueling Bandits" suggests kernelized methods can handle offline preference feedback.
- Break condition: If the pairwise concentrability coefficient C† is unbounded, the pessimistic penalty cannot guarantee convergence.

### Mechanism 3
- Claim: Human rating models with complex noise structures can lead to constant suboptimality even under uniform data coverage.
- Mechanism: When the rating transformation function introduces bias that amplifies uncertainty (e.g., through polynomial noise), the LCB algorithm's pessimism penalty based only on variance is insufficient to guarantee convergence.
- Core assumption: The uncertainty noise depends on the true reward through a polynomial transformation (Condition 2).
- Evidence anchors:
  - [section 4.1]: "Using our model, we establish suboptimality lower bounds for using standard LCB algorithms on human rating..."
  - [section 3]: "Unlike existing models that consider only simple additive subgaussian noise, our model accommodates a more general form of noise..."
  - [corpus]: "Neural Dueling Bandits: Preference-Based Optimization with Human Feedback" suggests neural methods may handle complex noise.
- Break condition: If the learner has full knowledge of the rating model and designs the penalty accordingly, the suboptimality bound improves but still depends on the bias function.

## Foundational Learning

- Concept: Contextual bandit problems and offline policy learning
  - Why needed here: The paper's theoretical comparison is set in the contextual bandit framework with offline data, requiring understanding of how policy learning works without online exploration.
  - Quick check question: What is the difference between online and offline contextual bandits, and why does offline learning require different assumptions?

- Concept: Human feedback modeling and bias
  - Why needed here: The paper introduces a new rating model that accounts for both bias and uncertainty, which is central to understanding why preference methods outperform direct ratings.
  - Quick check question: How does the monotonicity condition in the new rating model ensure consistency of policy learning?

- Concept: Pessimistic algorithms and concentrability coefficients
  - Why needed here: The paper uses pessimistic MLE for preference-based methods and analyzes concentrability coefficients for data coverage assumptions.
  - Quick check question: What role does the pairwise concentrability coefficient C† play in the convergence of pessimistic MLE?

## Architecture Onboarding

- Component map:
  - Human rating model (Section 3) -> LCB algorithm (Algorithm 1) -> Suboptimality analysis under Assumptions 1-2
  - Bradley-Terry-Luce model -> Pessimistic MLE (Algorithm 2) -> Suboptimality analysis with pairwise concentrability
  - Lower bounds comparison -> Upper bounds comparison -> Theoretical advantage conclusion

- Critical path:
  1. Define the human rating model with conditions 1-3
  2. Establish lower bounds for LCB under Assumptions 1 and 2
  3. Prove upper bound for LCB with full model knowledge
  4. Compare with pessimistic MLE results from Zhu et al. (2023)
  5. Conclude theoretical advantage of preference-based methods

- Design tradeoffs:
  - Direct ratings provide more information per sample but suffer from bias amplification
  - Preference comparisons are less informative per sample but more robust to bias
  - Pessimistic algorithms trade off exploration for safety in offline settings

- Failure signatures:
  - Constant suboptimality despite increasing sample size (violation of monotonicity)
  - Suboptimality decay slower than O(1/√n) (insufficient pessimism)
  - Algorithm fails to converge even under uniform data coverage (unbounded concentrability)

- First 3 experiments:
  1. Implement the example rating model h(r, ε) = r² + r²ε|ε| and verify monotonicity condition
  2. Simulate LCB algorithm with this rating model under uniform data coverage and measure suboptimality decay
  3. Implement pessimistic MLE with synthetic preference data and compare suboptimality against LCB results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the suboptimality of preference-based methods compare to human rating methods when the human feedback is generated from a diverse set of models rather than a single consistent model?
- Basis in paper: [explicit] The paper states "our results are only focused on the case that all human feedback samples are generated following the same model. It is still open for future work to investigate the case when the human feedback is generated from a diverse set of models and provide a comparison between rating methods and preference-based methods in this setting."
- Why unresolved: The paper only analyzes the case where all human feedback follows the same model, but real-world scenarios may involve feedback from multiple annotators with different biases and uncertainty patterns.
- What evidence would resolve it: Theoretical analysis comparing the suboptimality of preference-based methods versus human rating methods when human feedback is generated from multiple, potentially diverse models. This could involve extending the current analysis to heterogeneous feedback models.

### Open Question 2
- Question: Can the theoretical advantage of preference-based methods over human rating methods be maintained in more complex settings beyond tabular contextual bandits, such as with function approximation or continuous action spaces?
- Basis in paper: [inferred] The paper focuses on tabular contextual bandits and mentions that preference-based methods have been studied in linear and general function approximation settings, but does not provide theoretical comparisons in these more complex settings.
- Why unresolved: The current theoretical results are limited to tabular settings, and extending them to more complex function approximation scenarios would require different analysis techniques.
- What evidence would resolve it: Theoretical results showing the suboptimality bounds for both preference-based methods and human rating methods in settings with function approximation (linear or nonlinear) and/or continuous action spaces, demonstrating whether the theoretical advantage persists.

### Open Question 3
- Question: How does the suboptimality of human rating methods change when the bias function h(r,ϵ) has more complex forms beyond polynomials, such as exponential or logarithmic functions?
- Basis in paper: [explicit] The paper considers a general class of monotone functions h(r,ϵ) for human ratings but focuses on polynomial forms in their theoretical analysis. The model is described as accommodating "a more general form of noise that can depend on the state-action pair."
- Why unresolved: The theoretical analysis is limited to polynomial forms of h(r,ϵ), but the model itself allows for more general bias functions. Understanding the impact of different bias function forms on suboptimality is important for practical applications.
- What evidence would resolve it: Theoretical analysis deriving suboptimality bounds for human rating methods when the bias function h(r,ϵ) takes forms other than polynomials, such as exponential, logarithmic, or other monotonic functions. This would show how the choice of bias function affects the learning performance.

## Limitations
- Theoretical analysis assumes specific monotonicity and regularity conditions that may not hold in real-world human feedback scenarios
- The comparison relies on idealized synthetic scenarios rather than empirical validation with actual human feedback data
- Analysis is limited to tabular contextual bandits, leaving open questions about more complex function approximation settings

## Confidence

**High Confidence**: The theoretical framework for comparing rating vs preference methods is well-established, and the O(1/√n) convergence rate for preference-based methods is consistent with existing literature on pessimistic MLE.

**Medium Confidence**: The lower bounds for direct rating methods under uniform data coverage are mathematically sound, but their practical relevance depends on whether real-world rating noise exhibits the polynomial amplification structure assumed in the model.

**Low Confidence**: The extent to which the monotonicity condition holds for actual human raters, and how violations would affect the theoretical conclusions, remains unclear.

## Next Checks
1. Test the monotonicity condition empirically by collecting human ratings on synthetic bandit problems with known reward structures and checking if the transformation function preserves reward ordering.
2. Implement the example rating model h(r, ε) = r² + r²ε|ε| and verify through simulation that LCB algorithm indeed shows constant suboptimality even under uniform data coverage.
3. Compare the empirical convergence rates of LCB and pessimistic MLE on a common synthetic dataset to validate whether the theoretical O(1/√n) advantage of preference methods manifests in practice.