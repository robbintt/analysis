---
ver: rpa2
title: Large Language Models with Retrieval-Augmented Generation for Zero-Shot Disease
  Phenotyping
arxiv_id: '2312.06457'
source_url: https://arxiv.org/abs/2312.06457
tags:
- clinical
- pulmonary
- prompt
- disease
- phenotype
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a zero-shot LLM-based method for disease
  phenotyping in electronic health records. The method uses retrieval-augmented generation
  and MapReduce to pre-identify disease-related text snippets, which are then used
  as queries for the LLM to establish diagnosis.
---

# Large Language Models with Retrieval-Augmented Generation for Zero-Shot Disease Phenotyping

## Quick Facts
- arXiv ID: 2312.06457
- Source URL: https://arxiv.org/abs/2312.06457
- Authors: 
- Reference count: 40
- Primary result: Zero-shot LLM-based phenotyping achieves F1=0.75 for pulmonary hypertension vs 0.62 for physician logic rules

## Executive Summary
This paper introduces a zero-shot LLM-based method for disease phenotyping in electronic health records, using retrieval-augmented generation and MapReduce to pre-identify disease-related text snippets. The approach is applied to pulmonary hypertension (PH) identification, significantly outperforming traditional physician logic rules with an F1 score of 0.75 compared to 0.62. The method has potential to enhance rare disease cohort identification and expand clinical research capabilities.

## Method Summary
The method uses a retrieval-augmented generation pipeline where regular expressions identify disease-related text snippets from clinical notes, which are then processed in parallel by an LLM using a MapReduce approach. Generated outputs are aggregated to establish final diagnosis. The system employs zero-shot prompting with PaLM-2, using prompt engineering strategies including chain-of-thought reasoning and context steering. The approach deliberately avoids complex retrieval methods to prevent bias from hyperparameter tuning.

## Key Results
- Achieved F1 score of 0.75 for pulmonary hypertension phenotyping
- Outperformed physician-defined structured phenotype rules (F1=0.62)
- Demonstrated effectiveness of zero-shot prompting with retrieval-augmented generation
- Showed importance of aggregation strategy, with max aggregation achieving highest performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The retrieval-augmented generation (RAG) step effectively reduces the context window burden on the LLM by pre-filtering relevant text snippets.
- Mechanism: By using regular expressions to identify disease-related text snippets before LLM processing, the method avoids overwhelming the model with irrelevant clinical documentation, allowing the LLM to focus on reasoning rather than filtering.
- Core assumption: The regular expression patterns capture most, if not all, relevant mentions of the disease across diverse clinical note types.
- Evidence anchors:
  - [abstract] "pre-identifies disease-related text snippets to be used in parallel as queries for the LLM to establish diagnosis"
  - [section] "Regex rules encompassed a broad spectrum of patterns that could potentially be associated with any mention of a specific medical condition, such as PH"
  - [corpus] Weak evidence; the corpus does not explicitly discuss retrieval filtering performance.
- Break condition: If regex patterns miss disease mentions, the downstream LLM diagnosis will be incomplete, leading to false negatives.

### Mechanism 2
- Claim: The MapReduce approach allows parallel processing of many text snippets, improving scalability and reducing latency.
- Mechanism: Each snippet is processed independently by the LLM in parallel, then aggregated using either a max function or a secondary LLM-based aggregation step, enabling the system to handle large volumes of clinical notes.
- Core assumption: Parallel snippet evaluation does not introduce conflicting signals that aggregation cannot resolve.
- Evidence anchors:
  - [abstract] "pre-identifies disease-related text snippets to be used in parallel as queries for the LLM"
  - [section] "we employed a MapReduce approach where each snippet is concurrently presented as context to the LLM... Generated outputs, one per snippet, are then aggregated"
  - [corpus] Weak evidence; no corpus neighbor discusses MapReduce or parallel snippet processing.
- Break condition: If conflicting snippet-level diagnoses are common, the aggregation step may produce incorrect final diagnoses.

### Mechanism 3
- Claim: Excluding echocardiogram (ECHO) and CT reports reduces false positives by filtering out reports that only suggest possible PH without definitive diagnosis.
- Mechanism: The method removes snippets containing specific headers or language patterns typical of ECHO and CT reports, and/or instructs the LLM to ignore such reports.
- Core assumption: ECHO and CT reports without confirmatory testing are unreliable indicators of PH diagnosis.
- Evidence anchors:
  - [abstract] "We did observe a notable drop in F1 scores... which might be attributed to the larger evaluation cohort and potentially to some overfitting on the training set"
  - [section] "we observed that a noteworthy percentage of false positives originated from echocardiogram (ECHO) or computerized tomography (CT) reports noting suspicion of PH without any confirmatory diagnostic testing"
  - [corpus] Weak evidence; corpus does not discuss report exclusion strategies.
- Break condition: If ECHO or CT reports contain definitive PH diagnoses, excluding them would cause false negatives.

## Foundational Learning

- Concept: Regular expressions (regex)
  - Why needed here: Regex is used to identify relevant text snippets from clinical notes before LLM processing.
  - Quick check question: What is the purpose of using regex in the retrieval step?
- Concept: MapReduce paradigm
  - Why needed here: MapReduce enables parallel processing of text snippets to scale the phenotyping approach.
  - Quick check question: How does MapReduce improve the scalability of the phenotyping pipeline?
- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: RAG combines information retrieval with LLM generation to focus the model on relevant text.
  - Quick check question: Why is RAG preferred over providing the entire clinical note to the LLM?

## Architecture Onboarding

- Component map: Clinical notes → Regex retrieval → Snippet filtering → LLM querying (Map phase) → Aggregation (Reduce phase) → Final diagnosis
- Critical path: Regex retrieval → LLM querying → Aggregation
- Design tradeoffs:
  - Using regex vs. more complex retrieval models balances simplicity and potential recall loss
  - Max aggregation vs. LLM-based aggregation trades interpretability and stability for nuanced reasoning
  - Excluding ECHO/CT reports reduces false positives but risks missing confirmed diagnoses
- Failure signatures:
  - Low recall: Regex patterns miss disease mentions
  - Low precision: Aggregation fails to resolve conflicting snippet-level outputs
  - System instability: Performance varies significantly across different prompts or aggregation methods
- First 3 experiments:
  1. Evaluate regex recall by comparing retrieved snippets against manually labeled disease mentions
  2. Compare max aggregation vs. LLM-based aggregation F1 scores on validation set
  3. Test performance impact of including vs. excluding ECHO/CT reports

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal aggregation method for combining LLM-generated diagnoses from multiple text snippets?
- Basis in paper: [explicit] The paper evaluates three aggregation methods (LLM-based with same/different prompts and Max) and notes that Max aggregation achieved the highest F1 score of 0.73, but doesn't explore other potential aggregation methods.
- Why unresolved: The paper only compares three specific aggregation methods and doesn't explore alternative approaches like weighted voting, confidence scoring, or more sophisticated ensemble techniques.
- What evidence would resolve it: Direct comparison of Max aggregation against other aggregation methods (e.g., weighted voting based on snippet confidence scores, ensemble methods combining multiple LLM outputs) on the same test dataset would provide clearer evidence of the optimal approach.

### Open Question 2
- Question: How would more advanced retrieval methods (e.g., Cohere's re-rank, Instructor embeddings) compare to the simple Regex-based approach in terms of both performance and computational efficiency?
- Basis in paper: [explicit] The paper deliberately chose Regex over more complex retrieval methods to avoid bias from hyperparameter tuning, but acknowledges this as a limitation and suggests future work in this direction.
- Why unresolved: The paper only evaluates one simple retrieval method (Regex) and doesn't provide any comparison with more sophisticated approaches that could potentially improve retrieval accuracy and reduce false positives.
- What evidence would resolve it: Head-to-head comparison of the current Regex-based retrieval method against more advanced retrieval techniques (Cohere's re-rank, Instructor embeddings) on the same test dataset, measuring both F1 score and computational cost.

### Open Question 3
- Question: How well does this zero-shot phenotyping approach generalize to other rare diseases beyond pulmonary hypertension?
- Basis in paper: [inferred] The paper focuses exclusively on pulmonary hypertension as a case study, despite mentioning the method's potential for rare disease cohort identification. The authors note that Regex rules require domain knowledge and may not be easily extensible.
- Why unresolved: The paper only demonstrates effectiveness on one specific rare disease (PH) and doesn't provide evidence of how the approach would perform on other rare diseases with different clinical presentations and documentation patterns.
- What evidence would resolve it: Application and evaluation of the same methodology on multiple different rare diseases (e.g., cystic fibrosis, Duchenne muscular dystrophy, rare autoimmune disorders) using the same evaluation framework to assess generalization across disease types.

## Limitations
- The zero-shot nature creates uncertainty about generalizability to other diseases and clinical contexts
- Reliance on regex for retrieval may miss disease mentions, limiting recall
- Exclusion of ECHO and CT reports may cause false negatives if these reports contain definitive diagnoses

## Confidence

**High Confidence**: The core architectural approach (RAG + MapReduce + aggregation) is well-specified and the comparative advantage over physician logic rules is demonstrated with statistical significance.

**Medium Confidence**: The generalizability of the method to other rare diseases and different EHR systems is plausible but not empirically validated in this work.

**Low Confidence**: The optimal prompt engineering strategies and regex patterns are likely to require significant adaptation for different diseases or clinical contexts.

## Next Checks

1. **Cross-disease validation**: Apply the same methodology to at least two other rare diseases with different clinical presentations to evaluate generalizability of both the architectural approach and the prompt engineering patterns.

2. **Retrieval recall analysis**: Manually audit a stratified sample of negative cases to quantify the false negative rate attributable to regex patterns missing disease mentions, and compare against alternative retrieval approaches.

3. **Prompt sensitivity analysis**: Systematically vary prompt templates, few-shot examples, and aggregation strategies across the PH cohort to establish robustness and identify optimal configurations that generalize beyond the single best-performing combination reported.