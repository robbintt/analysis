---
ver: rpa2
title: 'MultiLegalPile: A 689GB Multilingual Legal Corpus'
arxiv_id: '2306.02069'
source_url: https://arxiv.org/abs/2306.02069
tags:
- legal
- language
- corpus
- dataset
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MultiLegalPile is a 689GB multilingual legal corpus in 24 languages
  from 17 jurisdictions. It is created by combining four large subsets: Native Multi
  Legal Pile (112GB), Eurlex Resources (179GB), Legal mC4 (106GB), and Pile of Law
  (292GB).'
---

# MultiLegalPile: A 689GB Multilingual Legal Corpus

## Quick Facts
- arXiv ID: 2306.02069
- Source URL: https://arxiv.org/abs/2306.02069
- Reference count: 40
- Primary result: New SotA on LEXTREME and LexGLUE benchmarks using multilingual legal models

## Executive Summary
MultiLegalPile is a comprehensive 689GB multilingual legal corpus spanning 24 languages from 17 jurisdictions. The corpus is constructed by combining four large-scale legal datasets: Native Multi Legal Pile (112GB), Eurlex Resources (179GB), Legal mC4 (106GB), and Pile of Law (292GB). The authors pretrain RoBERTa and Longformer models both multilingually and monolingually, achieving new state-of-the-art performance on legal benchmarks while releasing all models and code under open licenses.

## Method Summary
The authors pretrain language models by warm-starting from XLM-R checkpoints and continuing pretraining on the MultiLegalPile corpus. They train new tokenizers (128K BPEs for multilingual, 32K for monolingual models) on the corpus, then continue pretraining for 1M steps (base) or 500K steps (large) with standard masked language modeling objectives. The corpus contains diverse legal text types including legislation, case law, and contracts across 24 languages. Models are evaluated on LEXTREME and LexGLUE benchmarks using the evaluation strategies described in the original papers.

## Key Results
- Multilingual models set new SotA on LEXTREME benchmark, outperforming XLM-R by 12.5 macro-F1 points
- Monolingual models achieve huge gains over multilingual base models in some languages, setting language-specific SotA in five languages with up to 11 macro-F1 improvement
- Legal Longformer outperforms Legal RoBERTa on tasks requiring longer context handling (4096 vs 512 tokens)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pretraining on multilingual legal corpora improves downstream performance on multilingual legal benchmarks compared to general-purpose models.
- **Mechanism:** Legal language exhibits domain-specific terminology, syntactic structures, and conceptual relationships that are not well captured by models trained on general web data. By exposing the model to a large volume of legal texts across multiple languages during pretraining, it learns legal domain knowledge and multilingual representations simultaneously.
- **Core assumption:** The legal corpora contain sufficient variety and quality to teach both legal concepts and multilingual representations effectively.
- **Evidence anchors:**
  - [abstract]: "Our multilingual models set a new SotA on LEXTREME and our English models on LexGLUE."
  - [section 4]: Describes pretraining on the MULTILEGAL PILE corpus with diverse legal sources.
  - [corpus]: The corpus contains 689GB across 24 languages from 17 jurisdictions, with varied text types including legislation, caselaw, and contracts. However, the quality and legal specificity of mC4-derived content may vary.
- **Break condition:** If the legal corpora are too noisy, lack sufficient legal specificity, or are imbalanced across languages, the pretraining signal weakens and performance gains diminish.

### Mechanism 2
- **Claim:** Warm-starting from XLM-R checkpoints and continuing pretraining on legal corpora is more effective than training from scratch.
- **Mechanism:** XLM-R provides strong multilingual representations learned from 100 languages. Continuing pretraining on legal corpora allows the model to adapt these representations to legal domain while preserving multilingual capabilities.
- **Core assumption:** The multilingual representations in XLM-R are sufficiently general and transferable to the legal domain.
- **Evidence anchors:**
  - [section 4]: "We warm-start (initialize) our models from the original XLM-R checkpoints (base or large) of Conneau & Lample (2019)."
  - [section 5]: Legal-XLM-R-base model with 184M parameters performs on par with XLM-R large (560M parameters) on LEXTREME.
- **Break condition:** If the legal corpora introduce domain-specific features that conflict with general multilingual patterns, or if the continued pretraining overfits to legal language, the multilingual capabilities may degrade.

### Mechanism 3
- **Claim:** Monolingual legal models outperform multilingual models in specific low-resource languages.
- **Mechanism:** When sufficient in-domain data exists for a specific language, training a monolingual model allows it to focus entirely on that language's legal patterns without being distracted by multilingual objectives.
- **Core assumption:** The language-specific subsets in MULTILEGAL PILE contain enough legal data to train effective monolingual models.
- **Evidence anchors:**
  - [section 5]: "Monolingual models achieve huge gains over their base model XLM-R in some languages and even set language specific SotA in five languages outperforming other models by as much as 11 macro F1."
  - [corpus]: Figure 3 shows varying amounts of data per language, with some languages having substantial corpora (e.g., Dutch with 810M words).
- **Break condition:** If the language-specific subsets are too small or lack legal specificity, monolingual models cannot outperform multilingual ones.

## Foundational Learning

- **Concept: Domain-specific pretraining**
  - Why needed here: General-purpose models lack legal domain knowledge necessary for legal tasks.
  - Quick check question: What types of legal texts are included in the corpus and how do they differ from general web text?

- **Concept: Multilingual representation learning**
  - Why needed here: Legal systems operate in multiple languages, requiring models to understand legal concepts across languages.
  - Quick check question: How does the model handle legal terminology that may not translate directly between languages?

- **Concept: Continued pretraining vs. training from scratch**
  - Why needed here: Leveraging existing multilingual knowledge from XLM-R while adapting to legal domain is more efficient than starting from random initialization.
  - Quick check question: Why initialize from XLM-R rather than training legal models from scratch on the legal corpus?

## Architecture Onboarding

- **Component map:** Raw legal text -> Text cleaning and preprocessing -> Tokenization with legal-specific BPE vocabulary (128K tokens) -> Model initialization from XLM-R checkpoint -> Continued pretraining on MULTILEGAL PILE with masked language modeling objective -> Evaluation on LEXTREME and LexGLUE benchmarks

- **Critical path:** Data preprocessing → Tokenization → Model initialization → Continued pretraining → Evaluation

- **Design tradeoffs:**
  - Monolingual vs. multilingual models: Monolingual models may perform better on specific languages but lose cross-lingual transfer; multilingual models have broader coverage but may underperform on low-resource languages
  - Context length: Standard models handle 512 tokens; Longformer handles 4096 tokens for longer legal documents but with increased computational cost
  - Vocabulary size: 128K for multilingual models balances coverage and efficiency; 32K for monolingual models allows more tokens per language

- **Failure signatures:**
  - Performance degradation on non-legal tasks
  - Overfitting to majority language (English) in multilingual models
  - Poor performance on long documents with standard models
  - Vocabulary coverage issues for rare legal terms

- **First 3 experiments:**
  1. Evaluate the base multilingual model on a subset of LEXTREME tasks to establish baseline performance
  2. Compare multilingual model performance with monolingual models on low-resource languages
  3. Test Longformer model on tasks with longer documents to verify context handling benefits

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important questions emerge from the methodology and results:

- Does deduplication of the MultiLegalPile corpus significantly improve model performance during pretraining?
- How does the performance of Legal-XLM models compare to other domain-specific legal models trained on different legal corpora?
- What is the optimal vocabulary size for multilingual legal language models, and how does it affect performance across different languages?

## Limitations

- The quality and legal specificity of the mC4-derived content (106GB) remains uncertain, as this subset is machine-translated from English Wikipedia and contains variable legal relevance
- The evaluation relies on a relatively small number of benchmark tasks (5 in LEXTREME, 6 in LexGLUE), which may not comprehensively capture all aspects of legal reasoning
- Resource constraints prevented full monolingual model training, with training stopped at 200K steps rather than optimal convergence

## Confidence

**High Confidence:**
- The corpus compilation methodology and resulting dataset characteristics (689GB, 24 languages, 17 jurisdictions) are well-documented and verifiable
- The claim that multilingual legal models outperform general-purpose models (XLM-R) on LEXTREME is strongly supported by reported results showing 12.5 macro-F1 improvement
- The performance gains of monolingual models over multilingual ones in specific languages are clearly demonstrated with concrete F1 score improvements

**Medium Confidence:**
- The mechanism by which continued pretraining from XLM-R checkpoints transfers to legal domain knowledge is plausible but not directly validated through ablation studies comparing with training from scratch
- The claim that Legal Longformer outperforms Legal RoBERTa on longer context tasks is supported by specific task results but not comprehensively validated across all long-document scenarios

**Low Confidence:**
- The assumption that all 24 languages have sufficient legal data quality and quantity for effective pretraining, given the varying sizes shown in Figure 3 and unknown quality of mC4 content
- The assertion that the released models are "the most open possible licenses" without specifying exact license terms for different components of the corpus and models

## Next Checks

1. **Ablation study on pretraining initialization:** Train a model from scratch on MultiLegalPile and compare its performance with the warm-started XLM-R models on LEXTREME tasks to quantify the contribution of continued pretraining versus training from random initialization.

2. **General-domain task evaluation:** Test the legal models on established general-domain benchmarks (GLUE, XTREME) to assess potential performance degradation and determine the specialization-generalization tradeoff.

3. **mC4 content quality analysis:** Sample and manually annotate a subset of documents from the Legal mC4 portion to quantify the proportion of truly legal content versus general text, and correlate this quality metric with downstream task performance.