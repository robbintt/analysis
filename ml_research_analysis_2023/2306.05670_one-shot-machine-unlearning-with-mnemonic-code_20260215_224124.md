---
ver: rpa2
title: One-Shot Machine Unlearning with Mnemonic Code
arxiv_id: '2306.05670'
source_url: https://arxiv.org/abs/2306.05670
tags:
- forgetting
- data
- classes
- mnemonic
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for forgetting some classes from a
  trained deep learning model without additional training. To achieve this, it uses
  Fisher Information Matrix (FIM) to identify the model parameters sensitive to the
  forgetting classes, and adds noise to those parameters.
---

# One-Shot Machine Unlearning with Mnemonic Code

## Quick Facts
- arXiv ID: 2306.05670
- Source URL: https://arxiv.org/abs/2306.05670
- Reference count: 5
- Key outcome: Proposed method achieves effective forgetting of specific classes from trained models without retraining by using Fisher Information Matrix with mnemonic codes, outperforming existing methods in forgetting capability and processing time.

## Executive Summary
This paper introduces a novel approach to machine unlearning (MU) that enables selective forgetting of specific classes from a trained deep learning model without requiring additional training or retaining original training data. The method uses Fisher Information Matrix (FIM) to identify sensitive model parameters for the forgetting classes and adds targeted noise to these parameters. To avoid storing training data for FIM calculation, the authors employ class-specific synthetic signals called mnemonic codes during training, which also improve forgetting performance by reducing parameter sensitivity overlap between classes.

## Method Summary
The proposed method trains models with mnemonic code augmentation, then calculates FIM using these codes instead of original training data. Sensitive parameters are identified by comparing FIM values between forgetting and retaining classes, and noise is added proportionally to the ratio of these sensitivities. This one-shot approach eliminates the need for retraining while achieving effective forgetting, with the mnemonic codes serving dual purposes of reducing storage requirements and improving class separation in parameter sensitivities.

## Key Results
- Outperforms existing MU methods in terms of forgetting capability (FECF and RECF metrics)
- Achieves faster processing time by eliminating retraining requirements
- Mnemonic codes improve forgetting performance by reducing overlap in sensitive model parameters between classes
- Maintains reasonable accuracy on retaining classes while effectively forgetting target classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sensitive parameters for forgetting classes are identified via Fisher Information Matrix diagonal approximation, enabling targeted noise injection without retraining.
- Mechanism: The FIM diagonal elements measure parameter sensitivity to forgetting class data. Parameters with large diagonal values relative to retaining class sensitivity are perturbed with noise proportional to the ratio f_CF,i / f_CR,i, effectively reducing forgetting class accuracy while preserving retaining class performance.
- Core assumption: Laplace approximation and Bayes rule transformations are valid for approximating posterior distributions in high-dimensional deep networks.
- Evidence anchors:
  - [abstract] "We identify the sensitive parameters by calculating the Fisher Information Matrix (FIM)."
  - [section] "Laplace's approximation is applied" and "f_CF and f_CR represent the sensitivity of each model parameter"
  - [corpus] No direct corpus evidence for Laplace approximation validity in deep networks
- Break condition: If parameter sensitivities are not class-specific (high cosine similarity between f0 and fi), noise injection will damage both forgetting and retaining classes.

### Mechanism 2
- Claim: Mnemonic codes enable class-specific synthetic signals that reduce storage requirements and improve forgetting performance by creating class-orthogonal sensitivity patterns.
- Mechanism: Class-specific random patterns (mnemonic codes) are added to training data as augmentation. During forgetting, these codes replace real training data for FIM computation. The codes create distinct sensitivity patterns for each class, reducing overlap between forgetting and retaining class parameters.
- Core assumption: Random patterns can effectively capture class-specific information and create orthogonal sensitivity patterns.
- Evidence anchors:
  - [abstract] "we use class-specific synthetic signals called mnemonic code to reduce the cost of FIM calculation"
  - [section] "Mnemonic codes also help to reduce the calculation steps required to obtain the FIM" and "mnemonic codes improve the MU capability of our method by affecting sensitive model parameters"
  - [corpus] Weak evidence - no corpus papers directly support mnemonic code effectiveness
- Break condition: If mnemonic codes fail to create orthogonal sensitivity patterns (high cosine similarity persists), forgetting performance degrades.

### Mechanism 3
- Claim: One-shot unlearning achieves practical scalability by eliminating retraining and reducing computation to O(#classes) instead of O(training_data_size).
- Mechanism: The method adds noise directly to pre-trained model parameters without additional training. FIM computation uses only mnemonic codes (one per class) instead of entire training datasets, making the process independent of dataset size and enabling fast execution even on large models.
- Core assumption: Direct parameter perturbation can achieve effective forgetting without catastrophic forgetting of retaining classes.
- Evidence anchors:
  - [abstract] "our method is faster and better at forgetting than existing MU methods" and "does not need additional training for forgetting"
  - [section] "Our method uses mnemonic codes and reduces the cost of data storage and the time for the MU process" and "Experimental results demonstrate that our method outperforms existing MU methods in terms of speed"
  - [corpus] Weak evidence - no corpus papers demonstrate one-shot unlearning on large datasets
- Break condition: If parameter perturbation causes unintended catastrophic forgetting of retaining classes or fails to achieve sufficient forgetting accuracy.

## Foundational Learning

- Concept: Fisher Information Matrix as parameter sensitivity metric
  - Why needed here: FIM quantifies how much model parameters affect the likelihood of different classes, enabling identification of which parameters are most important for forgetting specific classes
  - Quick check question: How does the diagonal approximation of FIM simplify computation while potentially losing information about parameter interactions?

- Concept: Laplace approximation and Bayesian parameter estimation
  - Why needed here: The transformation from likelihood difference to FIM-based objective relies on approximating posterior distributions using Laplace's method, which connects parameter sensitivity to the forgetting objective
  - Quick check question: What are the assumptions required for Laplace approximation to be valid in high-dimensional deep networks?

- Concept: Class-orthogonal representation learning
  - Why needed here: Effective forgetting requires that forgetting class parameters can be perturbed without affecting retaining class performance, which depends on the degree of overlap in parameter sensitivities between classes
  - Quick check question: How does the cosine similarity between parameter sensitivity vectors indicate the difficulty of forgetting one class without affecting others?

## Architecture Onboarding

- Component map:
  Training phase → FIM computation phase → Noise injection phase → Evaluation phase

- Critical path: Training → FIM Computation → Noise Injection → Evaluation
  - Each phase must complete successfully for effective forgetting
  - Mnemonic code quality directly impacts FIM computation and final performance

- Design tradeoffs:
  - Mnemonic code intensity vs. baseline model accuracy (higher intensity may degrade initial accuracy)
  - Noise coefficient magnitude (λ1, λ2) vs. forgetting effectiveness vs. retaining class preservation
  - FIM diagonal approximation vs. full FIM computation (accuracy vs. computational cost)

- Failure signatures:
  - High cosine similarity between f0 and fi indicates mnemonic codes are ineffective
  - Large RECF values after additional training indicate information remains in hidden layers
  - Significant ACR degradation indicates noise is damaging retaining class representations

- First 3 experiments:
  1. Train ResNet-18 on CIFAR-10 with mnemonic codes (λ=0.9), verify baseline accuracy degradation
  2. Compute cosine similarity between f0 and other class sensitivities with and without mnemonic codes
  3. Apply unlearning to class 0 with varying (λ1, λ2) hyperparameters and measure ACR, FECF, RECF metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we theoretically prove that mnemonic codes are the optimal choice for achieving effective forgetting in machine unlearning?
- Basis in paper: [inferred] The paper mentions that mnemonic codes improve the forgetting performance but does not provide theoretical support for their optimality.
- Why unresolved: The paper only experimentally demonstrates the effectiveness of mnemonic codes without providing theoretical justification.
- What evidence would resolve it: A theoretical analysis demonstrating why mnemonic codes are the best choice for achieving effective forgetting, possibly by comparing them to other synthetic signals or data augmentation techniques.

### Open Question 2
- Question: Can the proposed method be extended to generative models and language models, which are known to be robust to catastrophic forgetting?
- Basis in paper: [explicit] The paper mentions that studies have shown these large-scale models are robust to catastrophic forgetting and effective MU methods for these models are needed.
- Why unresolved: The paper only evaluates the proposed method on discriminative models (ResNet-18) and does not explore its applicability to generative or language models.
- What evidence would resolve it: Experiments applying the proposed method to generative models (e.g., GANs) and language models (e.g., transformers) and demonstrating its effectiveness in forgetting specific classes or data points.

### Open Question 3
- Question: How do the hyperparameters λ1 and λ2 affect the trade-off between forgetting capability and retention of other classes?
- Basis in paper: [explicit] The paper mentions that λ1 and λ2 are hyperparameters that control the intensity of noise added during the forgetting process but does not provide a detailed analysis of their effects.
- Why unresolved: The paper only mentions the chosen values for λ1 and λ2 but does not explore their impact on the forgetting performance and retention of other classes.
- What evidence would resolve it: A sensitivity analysis of the hyperparameters λ1 and λ2, exploring their effects on forgetting capability, retention of other classes, and overall performance. This could involve varying the values of λ1 and λ2 and observing the resulting changes in the evaluation metrics.

## Limitations
- The effectiveness depends on the assumption that mnemonic codes can create orthogonal sensitivity patterns, which lacks strong theoretical foundation
- Performance on larger, more complex datasets remains uncertain as experiments only use relatively small datasets
- The Laplace approximation assumption for high-dimensional deep networks is not well-validated in the literature

## Confidence
- High confidence: The basic mechanism of using FIM to identify sensitive parameters for noise injection is well-established in the literature and theoretically sound
- Medium confidence: The mnemonic code approach shows empirical promise but lacks strong theoretical foundation and broader validation across diverse datasets
- Low confidence: The one-shot claim for very large models and datasets, and the assertion that this method is universally superior to all existing approaches

## Next Checks
1. **Cosine similarity analysis**: Reproduce the sensitivity vector similarity analysis with and without mnemonic codes on multiple datasets to verify the claimed improvement in class-orthogonality consistently holds across different data distributions and model architectures.

2. **Ablation study on mnemonic code intensity**: Systematically vary the mnemonic code intensity parameter λ and measure the trade-off between baseline accuracy degradation and forgetting performance to determine optimal values and identify potential breaking points.

3. **Hidden layer information analysis**: After applying the forgetting noise, perform additional training and measure RECF values across different layers to verify that the claim about hidden layer information retention holds consistently, and test whether this pattern persists with different forgetting scenarios and model depths.