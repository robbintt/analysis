---
ver: rpa2
title: Zenkai -- Framework For Exploring Beyond Backpropagation
arxiv_id: '2311.09663'
source_url: https://arxiv.org/abs/2311.09663
tags:
- layer
- learning
- zenkai
- self
- update
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Zenkai is an open-source framework that enables researchers to
  explore beyond backpropagation by dividing deep learning machines into layers of
  semi-autonomous learning machines, each with its own target and learning algorithm.
  This allows for the use of non-differentiable layers or learning algorithms beyond
  those based on error backpropagation.
---

# Zenkai -- Framework For Exploring Beyond Backpropagation

## Quick Facts
- arXiv ID: 2311.09663
- Source URL: https://arxiv.org/abs/2311.09663
- Reference count: 40
- Key outcome: Zenkai is an open-source framework enabling researchers to explore beyond backpropagation by dividing deep learning machines into layers of semi-autonomous learning machines, each with its own target and learning algorithm.

## Executive Summary
Zenkai is an open-source framework designed to enable researchers to explore beyond traditional backpropagation in deep learning. It achieves this by dividing deep learning machines into layers of semi-autonomous learning machines, each with its own target and learning algorithm. This architecture allows for the use of non-differentiable layers or learning algorithms beyond those based on error backpropagation, providing flexibility and ease-of-use to stimulate deep learning research.

## Method Summary
Zenkai implements a framework that splits deep learning machines into semi-autonomous components, each with separate methods for parameter updates (`step()`) and target computation (`step_x()`). This design enables flexible implementation of non-backpropagation learning algorithms by decoupling how targets are computed from how parameters are updated. The framework integrates with PyTorch while allowing arbitrary backward pass implementations, supporting biologically plausible algorithms like Feedback Alignment and enabling local error monitoring through Global Error Reduction (GER) and Local Error Reduction (LER) metrics.

## Key Results
- Zenkai successfully implements various non-backpropagation algorithms including decision-linear, target propagation, feedback alignment, and direct feedback alignment
- The framework demonstrates flexibility in handling both differentiable and non-differentiable learning approaches
- MNIST experiments show the framework's ability to support alternative training methodologies beyond standard backpropagation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zenkai enables training of non-differentiable layers by decoupling target computation from parameter updates.
- Mechanism: By splitting each layer into autonomous learning machines with separate `step()` and `step_x()` methods, Zenkai allows researchers to define custom target propagation mechanisms for layers that cannot backpropagate gradients.
- Core assumption: The target for a preceding layer can be computed independently of gradient flow through the current layer.
- Evidence anchors: [abstract] "This is to allow researchers greater exploration such as the use of non-differentiable layers or learning algorithms beyond those based on error backpropagation." [section] "While in some cases, learning algorithms can still can still be found to cast the problem to the backpropagation framework like with straight-through-estimation Hinton [2012], Bengio [2013] or like with differentiable decision trees Silva et al. [2020], limits are still exist on what can be learned and how it learns." [corpus] Weak evidence - no direct mentions of non-differentiable layers in corpus neighbors.

### Mechanism 2
- Claim: Zenkai supports biologically plausible training algorithms like Feedback Alignment by allowing arbitrary backward pass implementations.
- Mechanism: The `step_x()` method can be overridden to use a fixed random matrix instead of the layer's own weights for backward propagation, enabling algorithms like Feedback Alignment and Direct Feedback Alignment.
- Core assumption: Fixed random matrices can effectively replace learned weight matrices in the backward pass for certain architectures.
- Evidence anchors: [section] "Feedback alignment Lillicrap et al. [2016] is a biologically plausible training algorithm for neural networks that makes use of a randomly generated weight matrix used for backpropagation, which stays fixed." [section] "While there are ways to implement this with only PyTorch, it will possibly result in implementing an AutoGrad functor and defining the backward() method to use a random matrix which is passed in." [corpus] Weak evidence - no direct mentions of Feedback Alignment in corpus neighbors.

### Mechanism 3
- Claim: Zenkai enables local error monitoring through Global Error Reduction (GER) and Local Error Reduction (LER) metrics.
- Mechanism: By computing error reduction after each intermediate update, researchers can diagnose training issues like instability or insufficient learning progress in individual layers.
- Core assumption: Error reduction metrics correlate with learning effectiveness and can identify problematic layers.
- Evidence anchors: [section] "Global error reduction (GER) can be defined as the reduction in error of the global cost function after an update" [section] "Local error reduction can be calculated as the reduction in error for a local cost function (i.e. the loss for an intermediate layer)." [section] "This can be helpful to identify the causes of unsuccessful learning." [corpus] Weak evidence - no direct mentions of error reduction metrics in corpus neighbors.

## Foundational Learning

- Concept: Semi-autonomous learning machines with separate parameter and target update methods
  - Why needed here: Enables flexible implementation of non-backpropagation learning algorithms by decoupling how targets are computed from how parameters are updated.
  - Quick check question: What is the difference between `step()` and `step_x()` methods in Zenkai's LearningMachine class?

- Concept: Target propagation and inverse operations
  - Why needed here: Provides alternative to gradient-based learning by propagating targets backward through the network using inverse operations or approximations.
  - Quick check question: How does target propagation differ from standard backpropagation in terms of information flow through the network?

- Concept: Population-based optimization algorithms for parameter updates
  - Why needed here: Enables exploration of evolutionary and swarm-based learning approaches that don't rely on gradient information.
  - Quick check question: What role do the Individual and Population classes play in Zenkai's Tansaku subpackage?

## Architecture Onboarding

- Component map: LearningMachine -> IO -> StepTheta -> StepX -> Assessment -> State -> Individual/Population (for metaheuristics)
- Critical path: 1) Define LearningMachine subclass with forward(), assess_y(), step(), and step_x() methods, 2) Implement target computation logic in step_x(), 3) Connect machines in desired architecture, 4) Implement training loop with forward propagation and parameter updates
- Design tradeoffs: Flexibility vs complexity (more abstract interfaces enable more algorithms but increase learning curve), performance vs research freedom (PyTorch integration provides speed but may limit some unconventional approaches), ease of use vs customization (familiar PyTorch patterns vs novel learning algorithms)
- Failure signatures: Training stalls (check target computation and error reduction metrics), memory issues (verify tensor shapes and device placement), incorrect gradients (ensure proper target propagation logic), poor performance (tune hyperparameters and verify algorithm implementation)
- First 3 experiments:
  1. Implement a simple baseline network using standard gradient descent to verify framework integration.
  2. Create a Feedback Alignment network to test non-standard backward pass implementation.
  3. Build a network with a non-differentiable layer (e.g., decision tree) to test target computation mechanisms.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the upper bounds of performance for non-backpropagation learning algorithms when implemented with Zenkai?
- Basis in paper: [inferred] The paper demonstrates several non-backpropagation algorithms (decision-linear, target propagation, feedback alignment, direct feedback alignment, neural decision) but does not optimize hyperparameters or explore the full potential of these methods.
- Why unresolved: The experiments presented are preliminary and not systematically optimized, making it unclear what the maximum achievable performance is for these algorithms.
- What evidence would resolve it: Systematic hyperparameter optimization and ablation studies for each non-backpropagation algorithm implemented in Zenkai, comparing performance to traditional backpropagation methods.

### Open Question 2
- Question: How does Zenkai's flexibility in layer learning algorithms impact the overall training stability and convergence properties of deep networks?
- Basis in paper: [explicit] The paper mentions that Zenkai allows for non-differentiable layers and learning algorithms beyond backpropagation, but notes that error reductions tending to be negative might indicate instability.
- Why unresolved: While the framework provides flexibility, the paper does not thoroughly investigate the stability and convergence implications of mixing different learning algorithms within a single network.
- What evidence would resolve it: Empirical studies comparing training stability, convergence rates, and final performance across networks using uniform backpropagation versus heterogeneous learning algorithms in Zenkai.

### Open Question 3
- Question: What are the most effective strategies for implementing target propagation in Zenkai to achieve competitive results with backpropagation?
- Basis in paper: [explicit] The paper presents several target propagation experiments but notes that results were not impressive and suggests that more sophisticated approaches might yield better results.
- Why unresolved: The experiments with target propagation did not achieve competitive results, and the paper suggests improvements but does not implement them.
- What evidence would resolve it: Implementation and comparison of various target propagation strategies (e.g., different inverse approximations, layer-wise training approaches) in Zenkai, with systematic evaluation of their performance relative to backpropagation.

### Open Question 4
- Question: How can Zenkai be extended to support more complex metaheuristic optimization algorithms for updating layer parameters?
- Basis in paper: [explicit] The paper mentions the Tansaku subpackage for population-based optimizers but does not extensively demonstrate its capabilities.
- Why unresolved: While the framework provides a foundation for metaheuristic optimization, the paper does not explore its full potential or provide comprehensive examples of complex optimization algorithms.
- What evidence would resolve it: Development and evaluation of advanced metaheuristic optimization algorithms (e.g., evolutionary strategies, swarm intelligence methods) for parameter updates in Zenkai, with comparisons to traditional gradient-based methods.

## Limitations

- The framework's effectiveness for truly non-differentiable operations remains partially unverified, as most demonstrated experiments still operate within differentiable regimes or use approximations like straight-through estimation.
- Biological plausibility claims for algorithms like Feedback Alignment lack empirical validation beyond basic functionality tests.
- The error reduction metrics (GER and LER) are introduced without systematic validation of their diagnostic value across diverse failure modes.

## Confidence

- **High confidence**: The core architectural design of semi-autonomous learning machines with separate `step()` and `step_x()` methods is well-specified and implementable. The framework successfully integrates with PyTorch and provides the promised flexibility for implementing non-standard learning algorithms.
- **Medium confidence**: The Feedback Alignment and Direct Feedback Alignment implementations work as described, but their performance characteristics compared to optimized implementations remain unclear. The MNIST experiment results are plausible but lack detailed hyperparameter specifications.
- **Low confidence**: Claims about the framework's ability to handle truly non-differentiable layers lack empirical support in the presented experiments. The utility of the error reduction metrics for diagnosing training failures needs systematic validation.

## Next Checks

1. **Non-differentiable layer test**: Implement and train a network with an actual non-differentiable layer (such as a hard thresholding operation or discrete decision tree) to verify that Zenkai's target computation mechanism works beyond differentiable approximations.

2. **Error metric validation**: Conduct controlled experiments where specific layers are intentionally misconfigured, then verify whether GER and LER metrics correctly identify the problematic components compared to standard monitoring approaches.

3. **Performance benchmarking**: Compare Zenkai's implementation of Feedback Alignment and Direct Feedback Alignment against optimized reference implementations on standard benchmarks to quantify any performance overhead introduced by the framework abstraction layer.