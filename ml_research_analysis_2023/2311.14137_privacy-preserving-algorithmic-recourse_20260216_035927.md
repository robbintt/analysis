---
ver: rpa2
title: Privacy-Preserving Algorithmic Recourse
arxiv_id: '2311.14137'
source_url: https://arxiv.org/abs/2311.14137
tags:
- recourse
- data
- path
- privacy
- private
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PrivRecourse, a method to generate realistic
  and actionable algorithmic recourse paths under differential privacy (DP) guarantees.
  It addresses the privacy vulnerability of instance-based counterfactual explanations,
  which can expose sensitive data.
---

# Privacy-Preserving Algorithmic Recourse

## Quick Facts
- arXiv ID: 2311.14137
- Source URL: https://arxiv.org/abs/2311.14137
- Reference count: 40
- Primary result: PrivRecourse achieves PDensity of 2.38 vs -0.34 for baselines and PDistanceManifold of 0 vs 0.21, indicating more realistic recourse paths under DP guarantees.

## Executive Summary
This paper addresses the privacy vulnerability of instance-based counterfactual explanations by introducing PrivRecourse, a method to generate realistic and actionable algorithmic recourse paths under differential privacy (DP) guarantees. The approach uses DP clustering to create private cluster centers, forms a graph on these centers, and applies shortest-path search to generate privacy-preserving recourse paths that follow the data manifold. Empirical results on finance datasets show PrivRecourse produces more realistic paths than baselines while maintaining strong DP guarantees and improved runtime efficiency.

## Method Summary
PrivRecourse is a pipeline that generates privacy-preserving recourse paths through three main steps: (1) DP clustering of training data to create cluster centers, (2) graph construction on cluster centers with edges representing feasible transitions under domain constraints, and (3) shortest-path search for counterfactual queries. The method allocates privacy budgets between model training (ε_f) and data publishing (ε_k), using sequential composition to ensure overall DP guarantees. Once the DP graph is published, multiple counterfactual queries can be answered without additional privacy cost due to the post-processing property of DP.

## Key Results
- PrivRecourse achieves PDensity of 2.38 versus -0.34 for baselines, indicating more realistic paths
- PDistanceManifold is 0 for PrivRecourse versus 0.21 for baselines, showing paths closer to data manifold
- Runtime is 31.96s versus 70.98s for Adult dataset, demonstrating improved efficiency
- Maintains (ε_k, δ_k)-DP guarantees throughout, enabling multiple queries with shared privacy budget

## Why This Works (Mechanism)

### Mechanism 1
DP clustering better preserves data manifold structure under noise than perturbing individual samples or generating synthetic data. By partitioning the dataset into non-overlapping subsets via DP clustering, each cluster center represents a dense region of the data. The graph built on these centers follows the manifold more closely than noise-added or synthetic samples that may distort local density.

### Mechanism 2
The post-processing property of DP allows the published graph to be queried infinitely without further privacy loss. Once the DP cluster centers and graph are published, any number of counterfactual queries can be answered using the same graph without additional privacy expenditure, since DP guarantees hold for any post-processing function.

### Mechanism 3
DP clustering + graph-based shortest path search yields recourse paths that are both private and actionable. Clustering ensures that graph nodes are representative of dense data regions; shortest-path search on this graph generates sequential steps along feasible data points toward the favorable outcome, satisfying actionability.

## Foundational Learning

- Concept: Differential Privacy (DP)
  - Why needed here: DP is the core privacy guarantee that allows publishing the clustering results and graph without exposing individual data
  - Quick check question: What is the difference between global and local DP models in terms of who holds the data?

- Concept: Clustering under noise
  - Why needed here: Understanding how DP clustering algorithms converge despite added noise is critical to trusting the manifold representation
  - Quick check question: How does the exponential mechanism help in selecting cluster centers under DP?

- Concept: Graph construction with constraints
  - Why needed here: Ensuring that edges in the recourse graph respect domain-specific constraints is key to producing actionable paths
  - Quick check question: What happens to the graph connectivity if a constraint forbids all transitions in one direction?

## Architecture Onboarding

- Component map: DP Model Trainer → DP Clustering Engine → Graph Builder → Counterfactual Query Handler
- Critical path: 1) Train DP model (ε_f, δ_f) 2) Generate DP clusters (ε_k, δ_k) 3) Build and publish graph 4) Query graph for shortest recourse path
- Design tradeoffs: Higher privacy budget → better cluster fidelity but more privacy cost; More clusters → finer-grained graph but higher runtime and DP cost; Stricter constraints → more realistic paths but possibly disconnected graph
- Failure signatures: No recourse path found → graph disconnected or constraints too tight; Unrealistic paths → clusters too coarse or privacy budget too low; Slow queries → graph too dense or inefficient shortest-path implementation
- First 3 experiments: 1) Run DP clustering with varying ε_k to observe cluster quality vs privacy 2) Compare graph connectivity with/without constraints on a toy dataset 3) Measure recourse path realism (PDensity, PDistanceManifold) across datasets

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of recourse paths change when using different clustering algorithms in PrivRecourse? The paper only evaluates PrivRecourse using a specific privacy-preserving clustering algorithm but does not explore the impact of different clustering algorithms on the quality of recourse paths.

### Open Question 2
Can PrivRecourse be extended to handle continuous and categorical features simultaneously without prior knowledge of feature bounds? The paper assumes domain knowledge for feature preprocessing or employs one-hot encoding for categorical features.

### Open Question 3
How does the choice of distance function impact the feasibility and actionability of recourse paths in PrivRecourse? The paper uses different distance functions to evaluate proximity and sparsity but does not explore how the choice affects feasibility and actionability.

### Open Question 4
How does the privacy budget allocation between model training and data publishing affect the utility and privacy of recourse paths in PrivRecourse? The paper allocates a fixed privacy budget for each component but does not explore the impact of different budget allocations.

## Limitations
- DP clustering may produce poor cluster quality under extreme privacy budgets, leading to unrealistic paths
- Method effectiveness depends on data structure amenable to clustering, with potential graph disconnection under restrictive constraints
- Privacy analysis assumes sequential composition without accounting for correlations between queries

## Confidence

High confidence: DP clustering preserves manifold structure better than baselines (validated by PDensity and PDistanceManifold metrics)
Medium confidence: Scalability claims based on limited dataset size and scope
High confidence: Privacy guarantees via explicit DP mechanisms and post-processing property

## Next Checks

1. Test PrivRecourse on larger, more diverse datasets to validate scalability claims
2. Systematically vary privacy budgets (ε_k) to identify threshold where cluster quality degrades
3. Evaluate recourse path quality under different constraint strictness levels to quantify actionability vs feasibility tradeoff