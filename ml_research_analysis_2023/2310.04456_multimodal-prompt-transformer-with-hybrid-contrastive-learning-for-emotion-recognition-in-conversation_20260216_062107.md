---
ver: rpa2
title: Multimodal Prompt Transformer with Hybrid Contrastive Learning for Emotion
  Recognition in Conversation
arxiv_id: '2310.04456'
source_url: https://arxiv.org/abs/2310.04456
tags:
- information
- multimodal
- emotion
- learning
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes a Multimodal Prompt Transformer with Hybrid
  Contrastive Learning (MPT-HCL) model for Emotion Recognition in Conversation (ERC).
  The model addresses two main problems in multimodal ERC: noise in cross-modal information
  fusion and prediction of emotion labels with few samples.'
---

# Multimodal Prompt Transformer with Hybrid Contrastive Learning for Emotion Recognition in Conversation

## Quick Facts
- arXiv ID: 2310.04456
- Source URL: https://arxiv.org/abs/2310.04456
- Authors: Multiple authors
- Reference count: 40
- Key outcome: MPT-HCL outperforms state-of-the-art models on IEMOCAP and MELD datasets for multimodal emotion recognition

## Executive Summary
This paper introduces MPT-HCL, a novel model for Emotion Recognition in Conversation (ERC) that addresses two key challenges: noise in cross-modal fusion and handling emotion labels with few training samples. The approach combines speaker and context-aware RGCNs for deep emotion cue extraction, modal feature filters to reduce noise in audio/visual features, and a Multimodal Prompt Transformer that incorporates filtered features as prompt information for text. The model is trained using a hybrid contrastive learning strategy that combines unsupervised and supervised contrastive objectives to improve feature representations and handle class imbalance.

## Method Summary
MPT-HCL processes multimodal conversational data through a pipeline that first extracts features using pre-trained models (RoBERTa-Large for text, OpenSmile for audio, DenseNet for visual). These features are then processed by speaker and context-aware RGCNs to capture emotional dependencies, filtered through modal feature filters to reduce noise, and fed into a Multimodal Prompt Transformer that performs cross-modal fusion by embedding the filtered features as prompt information into each attention layer. The model is trained using hybrid contrastive learning that combines unsupervised contrastive learning to improve multimodal fusion representations and supervised contrastive learning to better handle minority emotion classes.

## Key Results
- Achieves state-of-the-art performance on IEMOCAP and MELD benchmark datasets
- Outperforms existing models in weighted F1-score and weighted average accuracy
- Demonstrates effectiveness of hybrid contrastive learning for handling minority emotion classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal prompt information improves cross-modal fusion by embedding filtered modality features into each Transformer attention layer
- Mechanism: The model uses a modal feature filter to extract high-level features from audio and visual modalities, treating these as "prompt information" for text. The Multimodal Prompt Transformer (MPT) then incorporates this prompt information into every attention layer, allowing the text features to be continuously updated with multimodal context throughout the encoding process
- Core assumption: Text modality has stronger representation ability than audio/visual modalities, making it the primary carrier of semantic information
- Evidence anchors:
  - [abstract]: "deep emotion cues extraction was performed on modalities with strong representation ability, and feature filters were designed as multimodal prompt information for modalities with weak representation ability"
  - [section 4.4]: "We consider the audio and visual features obtained by the modal feature filters module as textual prompt information, and update all text states with the prompt information when executing cross-modal attention sequentially"
  - [corpus]: Weak evidence - while the model uses prompt information, there's no direct experimental comparison showing superiority of this approach over alternatives
- Break condition: If audio/visual modalities contain critical emotional information that gets filtered out, or if the filtering introduces significant information loss

### Mechanism 2
- Claim: Hybrid contrastive learning optimizes multimodal fusion features and handles labels with few samples through two complementary objectives
- Mechanism: The model employs both unsupervised contrastive learning (UCL) to maximize mutual information between fused features and individual modalities, and supervised contrastive learning (SCL) to aggregate samples with the same label, giving more weight to minority classes during training
- Core assumption: Contrastive learning can effectively mine relationships between multimodal features and improve minority class representation without explicit oversampling
- Evidence anchors:
  - [abstract]: "hybrid contrastive learning strategy to optimize the model's ability to handle labels with few samples. This strategy uses unsupervised contrastive learning to improve the representation ability of multimodal fusion and supervised contrastive learning to mine the information of labels with few samples"
  - [section 4.5]: Detailed explanation of both UCL and SCL losses with mathematical formulations
  - [corpus]: Moderate evidence - the t-SNE visualization shows improved class separation, but no ablation study quantifies the exact contribution of each contrastive component
- Break condition: If the contrastive objectives conflict with each other or if the temperature coefficient is poorly tuned, leading to collapsed representations

### Mechanism 3
- Claim: Speaker and context-aware RGCNs extract emotional cues at different levels, enhancing text representation for emotion recognition
- Mechanism: The model uses two parallel RGCNs - one capturing speaker dependencies (who said what) and another capturing contextual dependencies (temporal relationships), then combines their outputs to create enhanced text features
- Core assumption: Emotional cues in conversations are encoded both in speaker identity and temporal context, and these two sources provide complementary information
- Evidence anchors:
  - [abstract]: "deep emotion cues extraction was performed on modalities with strong representation ability" with subsequent mention of speaker and context extraction
  - [section 4.3]: Detailed architecture of Sa-RGCN and Ca-RGCN with edge types and aggregation mechanisms
  - [corpus]: Weak evidence - the ablation study shows performance drop when removing RGCNs, but doesn't isolate speaker vs. context contributions
- Break condition: If the conversation has very few speakers or if temporal dependencies are not the primary source of emotional information

## Foundational Learning

- Concept: Multimodal representation learning
  - Why needed here: The model needs to effectively combine text, audio, and visual features for emotion recognition
  - Quick check question: What are the key challenges in aligning features from different modalities with different dimensionalities and semantic spaces?

- Concept: Contrastive learning
  - Why needed here: The hybrid contrastive learning strategy is central to handling class imbalance and optimizing feature representations
  - Quick check question: How does supervised contrastive learning differ from traditional cross-entropy loss in terms of how it treats samples with the same label?

- Concept: Graph neural networks for sequence modeling
  - Why needed here: RGCNs are used to capture speaker and context dependencies in the conversation graph
  - Quick check question: What advantages do graph-based approaches offer over sequential models like RNNs for modeling speaker relationships in conversations?

## Architecture Onboarding

- Component map: Input → Bi-LSTM context capture → Modal Feature Filter → Sa-RGCN + Ca-RGCN → Multimodal Prompt Transformer → Hybrid Contrastive Learning → Output
- Critical path: Text features → Sa-RGCN/Ca-RGCN → MPT with prompt information → Final fusion → Emotion prediction
- Design tradeoffs: The prompt-based approach trades complexity for potentially better fusion quality, while the contrastive learning adds training complexity but addresses class imbalance
- Failure signatures: Poor performance on minority classes suggests contrastive learning isn't working; random-looking class distributions in t-SNE suggest feature collapse; significant performance drop when removing RGCNs suggests they're critical
- First 3 experiments:
  1. Ablation study: Remove MPT and use simple concatenation instead to quantify fusion improvement
  2. Sensitivity analysis: Vary the modal feature filter threshold to find optimal balance between noise reduction and information preservation
  3. Contrastive learning ablation: Remove UCL vs. SCL separately to measure individual contributions to minority class performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the hybrid contrastive learning strategy perform on datasets with different label distributions, particularly those with more balanced or more imbalanced class distributions?
- Basis in paper: [explicit] The paper mentions that the hybrid contrastive learning strategy is effective for handling labels with few samples, particularly on the MELD dataset which has more minority class labels. However, it does not explore the performance on datasets with different label distributions.
- Why unresolved: The paper only tests the model on two benchmark datasets, IEMOCAP and MELD, which may not represent all possible label distributions in real-world scenarios.
- What evidence would resolve it: Testing the model on a wider range of datasets with varying label distributions, including those with more balanced and more imbalanced classes, would provide insights into the generalizability of the hybrid contrastive learning strategy.

### Open Question 2
- Question: What is the impact of using different pre-trained language models (e.g., BERT, RoBERTa, GPT) on the performance of the MPT-HCL model?
- Basis in paper: [inferred] The paper uses RoBERTa-Large for text feature extraction, but it does not explore the impact of using different pre-trained language models on the model's performance.
- Why unresolved: The choice of pre-trained language model can significantly influence the quality of text features and, consequently, the overall performance of the model. However, this aspect is not investigated in the paper.
- What evidence would resolve it: Conducting experiments with different pre-trained language models and comparing their performance on the same tasks would provide insights into the impact of model choice on the MPT-HCL's effectiveness.

### Open Question 3
- Question: How does the MPT-HCL model handle conversations with more than two speakers, especially in terms of speaker-aware and context-aware RGCN modules?
- Basis in paper: [explicit] The paper mentions that the MELD dataset includes conversations with three or more speakers, but it does not provide detailed analysis on how the model performs in such scenarios.
- Why unresolved: The speaker-aware and context-aware RGCN modules are designed to capture dependencies between speakers and context, but their effectiveness in multi-party conversations is not thoroughly explored.
- What evidence would resolve it: Conducting experiments on datasets with a higher number of speakers per conversation and analyzing the performance of the speaker-aware and context-aware RGCN modules in these scenarios would provide insights into the model's scalability and robustness in handling complex conversational structures.

## Limitations

- The modal feature filter mechanism lacks detailed specification, making exact reproduction challenging
- The contribution of individual components to overall performance is not fully isolated through comprehensive ablation studies
- The computational complexity of the Multimodal Prompt Transformer architecture is not discussed

## Confidence

- **High Confidence**: The overall methodology of using speaker and context-aware RGCNs for text feature extraction, and the general framework of multimodal prompt information and hybrid contrastive learning are well-established concepts with reasonable implementation
- **Medium Confidence**: The specific implementation details of the modal feature filter and prompt attention mechanisms, as well as the exact configuration of the Multimodal Prompt Transformer, are described but lack sufficient detail for complete verification
- **Low Confidence**: The relative importance of different components to overall performance cannot be precisely determined due to incomplete ablation studies and sensitivity analyses

## Next Checks

1. **Component Ablation Study**: Perform a systematic ablation study removing MPT, modal feature filters, and each contrastive learning component separately to quantify their individual contributions to performance gains

2. **Hyperparameter Sensitivity Analysis**: Conduct experiments varying the modal feature filter thresholds and contrastive learning temperature coefficients to identify optimal settings and robustness to hyperparameter changes

3. **Computational Complexity Evaluation**: Measure and compare the computational requirements (FLOPs, inference time) of MPT-HCL against baseline models to assess practical deployment implications