---
ver: rpa2
title: Efficient Pre-training for Localized Instruction Generation of Videos
arxiv_id: '2311.15964'
source_url: https://arxiv.org/abs/2311.15964
tags:
- dataset
- videos
- video
- procx
- transcripts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present Sieve-&-Swap, a method to automatically curate a small
  high-quality training dataset for procedural video understanding by filtering irrelevant
  transcripts and replacing them with human-written instructions from a text-only
  recipe dataset. This reduces the domain gap between transcribed speech and written
  instructions.
---

# Efficient Pre-training for Localized Instruction Generation of Videos

## Quick Facts
- arXiv ID: 2311.15964
- Source URL: https://arxiv.org/abs/2311.15964
- Reference count: 40
- Key outcome: ProcX achieves SOTA on YouCook2 and Tasty using 48K videos and 124M params, outperforming Vid2Seq despite 3x less training data

## Executive Summary
This paper introduces Sieve-&-Swap, a technique for curating high-quality training data by filtering irrelevant transcripts and replacing them with human-written instructions from a text-only recipe dataset. This reduces the domain gap between transcribed speech and written instructions. The authors also propose Procedure Transformer (ProcX), a model that jointly localizes steps and generates instructions in procedural videos. ProcX, pre-trained on the curated dataset, achieves state-of-the-art performance on YouCook2 and Tasty while using a fraction of the training data and computational resources compared to prior work.

## Method Summary
The method involves two key components: (1) Sieve-&-Swap, which filters irrelevant transcripts from HowTo100M and replaces them with human-written instructions from RecipeNLG based on title overlap, creating a curated dataset of 48K videos; and (2) ProcX, a transformer-based model with key-aware deformable attention, contrastive loss, and gVFL for confidence prediction. The model is pre-trained on the curated dataset and fine-tuned on downstream datasets YouCook2 and Tasty.

## Key Results
- ProcX achieves 14% improvement on SODA-D and 5.97% on SODA-C compared to Vid2Seq on Tasty dataset
- Outperforms state-of-the-art methods on YouCook2 and Tasty while using 3x less training data
- Demonstrates strong performance with only 124M parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sieve-&-Swap reduces domain gap between transcribed speech and human-written instructions by replacing noisy transcripts with cleaner, domain-matched text.
- Mechanism: Transcripts often contain filler, greetings, and non-instructional content. By retrieving and swapping in recipe steps from a curated text-only recipe dataset, the training data becomes more aligned with the downstream instruction-generation task.
- Core assumption: The semantic overlap between video titles and recipe titles is sufficient to map videos to correct recipes, enabling accurate text replacement.
- Evidence anchors:
  - [abstract]: "Sieve-&-Swap, a technique that fuses two distinct instructional datasets... (i) Sieve filters irrelevant transcripts and (ii) Swap enhances the quality of the text instruction by automatically replacing the transcripts..."
  - [section]: "We develop a novel method to retrieve relevant and high-quality reference descriptions for procedural learning, which involves extracting human-written instructional sentences from RecipeNLG."
- Break condition: If title overlap is insufficient, mapping fails and swapped text may be irrelevant or misaligned temporally.

### Mechanism 2
- Claim: ProcX's key-aware deformable attention improves multi-scale temporal feature utilization compared to vanilla deformable attention.
- Mechanism: Standard deformable attention collapses focus onto single sampled points across scales, losing context. Key-aware deformable attention incorporates keys into the attention computation, spreading attention weights and enabling richer cross-scale feature aggregation.
- Core assumption: Attention collapse in vanilla deformable attention is the primary bottleneck for temporal localization and caption generation.
- Evidence anchors:
  - [section]: "We observe that applying deformable attention directly to multi-scale temporal features is ineffective for attending to cross-scale features... With our modified key-aware deformable attention, the attention weights spread across the video and help to utilize the entire video context..."
- Break condition: If the video features lack meaningful multi-scale structure, key-aware attention offers no advantage.

### Mechanism 3
- Claim: The contrastive transformer module with InfoNCE/MIL-InfoNCE loss improves text generation quality by learning better clip-to-text and text-to-clip representations.
- Mechanism: The contrastive loss aligns video segments with corresponding instructions in embedding space, providing global supervision beyond the token-level captioning loss. This improves generation coherence.
- Core assumption: Text generation quality improves when the model learns global multimodal alignment rather than relying solely on token-level prediction.
- Evidence anchors:
  - [section]: "We use the contrastive objective which can be employed both in supervised settings and in pre-training, unlike the denoising objective... We follow the decoupled architecture from the CoCa model and employ the InfoNCE loss."
- Break condition: If the pre-training dataset is too noisy or lacks proper alignment, contrastive learning may learn spurious correlations.

## Foundational Learning

- Concept: Text-to-text translation vs. video-to-language understanding
  - Why needed here: Prior methods like Vid2Seq rely on transcripts at inference time, reducing the problem to text-to-text translation. Sieve-&-Swap and ProcX focus on visual understanding.
  - Quick check question: Why is it problematic if a model can only work when transcripts are available?

- Concept: Multi-scale temporal feature fusion
  - Why needed here: Videos have hierarchical temporal structure (short actions to long steps). ProcX uses multi-scale features and a fusion module to capture both local and global context.
  - Quick check question: What happens if you only use single-scale features for procedural video understanding?

- Concept: Contrastive learning in multimodal settings
  - Why needed here: Unlike text-only contrastive models, ProcX learns joint video-text representations. This improves generation coherence and alignment.
  - Quick check question: How does InfoNCE loss differ from token-level cross-entropy loss in text generation?

## Architecture Onboarding

- Component map: Visual Encoder -> Localization Decoder -> Text Generation -> Pre-training -> Fine-tuning
- Critical path:
  1. Extract multi-scale visual features
  2. Encode with key-aware deformable attention
  3. Generate N proposal queries
  4. Predict temporal boundaries + confidence scores (with gVFL)
  5. Generate captions using contrastive transformer
  6. Compute set-based matching loss + contrastive loss

- Design tradeoffs:
  - Larger model size (124M params) vs. computational cost
  - Multi-scale features vs. simpler single-scale approach
  - Contrastive loss vs. token-level captioning loss only
  - gVFL vs. standard confidence loss

- Failure signatures:
  - Poor localization: confidence scores not correlated with IoU, attention collapse in encoder
  - Poor captions: contrastive loss not converging, vocabulary mismatch, incorrect segment-text alignment
  - Training instability: large batch size issues, contrastive loss weight too high

- First 3 experiments:
  1. Validate Sieve-&-Swap dataset: compare pre-training with raw transcripts vs. curated dataset on downstream metrics.
  2. Validate key-aware attention: replace with vanilla deformable attention and measure localization drop.
  3. Validate contrastive module: remove contrastive loss and compare caption coherence (SODA-C).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ProcX compare when pre-trained on the SnS-HTC dataset versus the Vid2Seq approach using the same number of videos (48K) from HowTo100M?
- Basis in paper: [explicit] The paper states that ProcX pre-trained on the SnS-HTC dataset achieves a 14% improvement on SODA-D and 5.97% on SODA-C compared to the Vid2Seq method on the Tasty dataset, despite using the same number of videos (48K).
- Why unresolved: The paper does not provide a direct comparison between ProcX and Vid2Seq when both are pre-trained on the same 48K videos from HowTo100M.
- What evidence would resolve it: A comparison of ProcX and Vid2Seq performance metrics (SODA-D, SODA-C, METEOR, CIDER, BLEU-4) on the YouCook2 and Tasty datasets when both models are pre-trained on the same 48K videos from HowTo100M.

### Open Question 2
- Question: What is the impact of the IoU-aware segment confidence loss (gVFL) on the performance of ProcX when pre-trained on the SnS-HTC dataset versus when pre-trained on raw transcripts from HowTo100M?
- Basis in paper: [explicit] The paper introduces the gVFL and shows that it improves the performance of ProcX when pre-trained on the SnS-HTC dataset. However, it does not provide a comparison of the gVFL's impact when ProcX is pre-trained on raw transcripts from HowTo100M.
- Why unresolved: The paper does not provide a direct comparison of ProcX performance with and without gVFL when pre-trained on raw transcripts from HowTo100M.
- What evidence would resolve it: A comparison of ProcX performance metrics (SODA-D, SODA-C, METEOR, CIDER, BLEU-4) on the YouCook2 and Tasty datasets when pre-trained on raw transcripts from HowTo100M, with and without the gVFL.

### Open Question 3
- Question: How does the performance of ProcX vary with different thresholds for filtering irrelevant transcripts during the Sieve step of the Sieve-&-Swap approach?
- Basis in paper: [inferred] The paper mentions that the threshold for filtering irrelevant transcripts (λiou) is set to 0.1 to compromise between dataset size and noise level. However, it does not explore the impact of different threshold values on ProcX performance.
- Why unresolved: The paper does not provide a systematic study of the impact of different λiou threshold values on ProcX performance.
- What evidence would resolve it: A comparison of ProcX performance metrics (SODA-D, SODA-C, METEOR, CIDER, BLEU-4) on the YouCook2 and Tasty datasets when pre-trained on SnS-HTC datasets created with different λiou threshold values.

## Limitations
- Dataset Construction Ambiguity: The Sieve-&-Swap method relies on title overlap for retrieving instructions, but the exact retrieval threshold and quality control measures are not specified.
- Architecture Complexity Trade-offs: The model introduces several architectural modifications, making it difficult to attribute performance gains to specific components.
- Computational Resource Claims: The paper claims "fraction of the training data and computational resources" compared to prior work, but provides limited quantitative comparison.

## Confidence
- High Confidence: The core claim that Sieve-&-Swap creates a more domain-appropriate training dataset is well-supported by the methodology description and the improved downstream performance.
- Medium Confidence: The performance improvements on YouCook2 and Tasty benchmarks are convincing, but the relative contribution of each architectural innovation remains unclear.
- Low Confidence: The efficiency claims lack quantitative backing. Without FLOPs, parameter counts of comparison models, or training time comparisons, the assertion that ProcX uses "a fraction of computational resources" cannot be independently verified.

## Next Checks
- Validation Check 1: Conduct a controlled experiment comparing ProcX trained on three different datasets: (1) raw HowTo100M transcripts, (2) Sieve-&-Swap curated dataset, and (3) RecipeNLG text-only data.
- Validation Check 2: Perform an ablation study where ProcX is modified to use: (a) vanilla deformable attention instead of key-aware, (b) only single-scale features, and (c) token-level captioning loss only (no contrastive loss).
- Validation Check 3: Implement a manual evaluation of the Sieve-&-Swap dataset quality by sampling 100 videos and verifying that the retrieved RecipeNLG instructions accurately describe the video content.