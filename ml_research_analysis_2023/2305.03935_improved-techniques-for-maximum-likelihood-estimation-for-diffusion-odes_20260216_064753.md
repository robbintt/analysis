---
ver: rpa2
title: Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs
arxiv_id: '2305.03935'
source_url: https://arxiv.org/abs/2305.03935
tags:
- diffusion
- likelihood
- odes
- which
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the challenge of improving likelihood estimation
  for diffusion ordinary differential equations (ODEs), which are continuous normalizing
  flows enabling exact inference and likelihood evaluation. The authors propose several
  techniques: (1) A training-free truncated-normal dequantization method tailored
  for diffusion models, bridging the training-evaluation gap in handling discrete
  data.'
---

# Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs

## Quick Facts
- arXiv ID: 2305.03935
- Source URL: https://arxiv.org/abs/2305.03935
- Reference count: 40
- Primary result: Achieves state-of-the-art likelihood of 2.56 bits/dim on CIFAR-10 and 3.43 bits/dim on ImageNet-32 for diffusion ODEs

## Executive Summary
This work addresses key challenges in maximum likelihood estimation for diffusion ordinary differential equations (ODEs), a class of continuous normalizing flows that enable exact inference. The authors propose several techniques to improve both the accuracy and efficiency of diffusion ODE training: a training-free truncated-normal dequantization method that bridges the training-evaluation gap for discrete data, velocity parameterization that reformulates the flow matching objective for faster convergence, an error-bounded high-order flow matching objective for fine-tuning, and variance reduction through importance sampling. These innovations enable state-of-the-art likelihood performance on standard image benchmarks without requiring variational dequantization or data augmentation, while achieving 2-3x faster convergence during pretraining compared to prior work.

## Method Summary
The authors propose four main techniques to improve diffusion ODE training. First, they introduce truncated-normal dequantization, which addresses the training-evaluation gap by ensuring the continuous data distribution during training matches the evaluation distribution. Second, velocity parameterization reformulates the flow matching objective by predicting velocity (drift) rather than noise, leading to more balanced prediction difficulty across time steps. Third, they develop a high-order flow matching objective with error bounds for fine-tuning, which improves ODE likelihood and smooths trajectories. Fourth, they employ importance sampling to reduce variance in the Monte Carlo estimator for the flow matching objective. The training procedure involves first-order flow matching with the proposed importance sampling, followed by fine-tuning with high-order flow matching. The method achieves state-of-the-art likelihood on CIFAR-10 (2.56 bits/dim) and ImageNet-32 (3.43 bits/dim) while being more computationally efficient than previous approaches.

## Key Results
- Achieves 2.56 bits/dim likelihood on CIFAR-10, surpassing previous diffusion ODE results
- Achieves 3.43 bits/dim on ImageNet-32 without variational dequantization or data augmentation
- Demonstrates 2-3x faster convergence during pretraining compared to prior work
- Shows improved NFE efficiency and better sample quality metrics (FID) compared to baselines

## Why This Works (Mechanism)

### Mechanism 1: Truncated-Normal Dequantization
- Claim: Reduces the training-evaluation gap by ensuring training and evaluation data distributions align
- Mechanism: Uniform dequantization causes mismatch because training data (α_ϵ x₀ + σ_ϵ ϵ) is not uniformly distributed while evaluation data (x₀ + u) is uniform. Truncated-normal dequantization chooses u such that α_ϵ x₀ + σ_ϵ ϵ ≈ x₀ + u
- Core assumption: The truncated-normal distribution can approximate the training data distribution well enough to minimize the gap
- Evidence anchors: [abstract] "we propose a training-free truncated-normal dequantization to fill the training-evaluation gap commonly existing in diffusion ODEs"

### Mechanism 2: Velocity Parameterization
- Claim: Improves training efficiency by predicting velocity rather than noise, leading to balanced prediction difficulty
- Mechanism: Traditional noise prediction suffers from imbalance where noise prediction is harder at small t (data-like) and easier at large t (noise-like). Velocity parameterization predicts diffusion path velocity, which is less dependent on input
- Core assumption: Velocity of the diffusion path is a more stable and informative target than noise itself
- Evidence anchors: [abstract] "we propose velocity parameterization and explore variance reduction techniques for faster convergence"

### Mechanism 3: Importance Sampling
- Claim: Reduces variance of Monte Carlo estimator for flow matching objective, leading to faster convergence
- Mechanism: The flow matching objective involves integral over time. Importance sampling estimates this integral with lower variance by drawing samples from a distribution emphasizing high-variance regions
- Core assumption: The importance sampling distribution can effectively reduce variance of the Monte Carlo estimator
- Evidence anchors: [abstract] "explore variance reduction techniques for faster convergence"

## Foundational Learning

- Concept: Continuous Normalizing Flows (CNFs)
  - Why needed here: Diffusion ODEs are a special case of CNFs, and understanding CNFs is crucial for grasping underlying principles
  - Quick check question: What is the main advantage of CNFs over discrete normalizing flows?

- Concept: Score Matching
  - Why needed here: Score matching is a key technique used in training diffusion models
  - Quick check question: How does score matching differ from maximum likelihood estimation?

- Concept: Importance Sampling
  - Why needed here: Used to reduce variance of Monte Carlo estimators in training process
  - Quick check question: What is the primary benefit of using importance sampling in Monte Carlo integration?

## Architecture Onboarding

- Component map: Noise Schedule -> Score Network -> Velocity Predictor -> Truncated-Normal Dequantization -> Importance Sampling
- Critical path:
  1. Define the noise schedule
  2. Train the score network using score matching
  3. Use velocity parameterization to improve training efficiency
  4. Apply truncated-normal dequantization for accurate likelihood evaluation
  5. Implement importance sampling for faster convergence
- Design tradeoffs:
  - Velocity parameterization vs. noise prediction: May improve training efficiency but could be more complex to implement
  - Truncated-normal dequantization vs. uniform dequantization: Reduces training-evaluation gap but may be more computationally intensive
  - Importance sampling vs. uniform sampling: Reduces variance but requires designing appropriate proposal distribution
- Failure signatures:
  - Poor likelihood performance: May indicate issues with dequantization method or score network
  - Slow convergence: May suggest problems with importance sampling strategy or velocity parameterization
  - High variance in training objective: Could indicate issues with importance sampling distribution
- First 3 experiments:
  1. Implement uniform dequantization and evaluate the training-evaluation gap
  2. Implement velocity parameterization and compare training efficiency to noise prediction
  3. Implement importance sampling and measure reduction in variance of Monte Carlo estimator

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical relationship between truncated-normal dequantization and variational dequantization for diffusion ODEs?
- Basis in paper: [explicit] The authors discuss both dequantization methods in Appendix A, noting they have "close relationship both theoretically and empirically" and provide mathematical formulations for both approaches
- Why unresolved: The paper mentions the relationship is close but doesn't provide a formal proof or deeper theoretical analysis of how these two approaches relate, particularly regarding their optimality or convergence properties
- What evidence would resolve it: A formal mathematical proof showing the conditions under which these two approaches converge to the same solution, or experimental results demonstrating their performance equivalence under various conditions

### Open Question 2
- Question: How do different noise schedules beyond VP and SP affect the performance of diffusion ODEs for likelihood estimation?
- Basis in paper: [inferred] The authors use VP and SP schedules but note "Our training and evaluation procedure is feasible for any noise schedule αγ,σγ" without exploring alternatives
- Why unresolved: The paper only tests two specific noise schedules despite acknowledging the method's generality, leaving open questions about whether other schedules might yield better results
- What evidence would resolve it: Comprehensive experiments comparing multiple noise schedules on the same benchmarks, with analysis of how schedule properties affect convergence and likelihood

### Open Question 3
- Question: What is the optimal balance between first-order and second-order flow matching objectives during fine-tuning?
- Basis in paper: [explicit] The authors mention they "simply set the coefficient λ in the mixed loss JFM(θ) + λJFM,tr(θ) as 0.1 without no further tuning" and note that "a large λ will make the training unstable or even degenerate the likelihood performance"
- Why unresolved: The paper uses a fixed λ value without systematic exploration of the hyperparameter space, and the authors acknowledge this could affect stability and performance
- What evidence would resolve it: A systematic hyperparameter sweep of λ values with analysis of the trade-off between likelihood improvement and training stability

### Open Question 4
- Question: How does the proposed method scale to higher resolution datasets and more complex data distributions?
- Basis in paper: [inferred] The experiments are limited to CIFAR-10 and ImageNet-32 (32x32 resolution), with the authors noting "Due to resource limitation, we didn't explore tuning of hyperparameters and network architectures"
- Why unresolved: The paper demonstrates state-of-the-art results on relatively small datasets but doesn't address scalability to larger, more complex datasets
- What evidence would resolve it: Experiments on higher resolution datasets (e.g., ImageNet-128, LSUN) showing whether the same techniques generalize, or theoretical analysis of computational complexity as resolution increases

## Limitations
- Limited evaluation to relatively small image datasets (CIFAR-10, ImageNet-32) without testing scalability to higher resolutions
- Fixed hyperparameter choices (e.g., λ=0.1 for fine-tuning) without systematic exploration of optimal values
- Theoretical relationship between truncated-normal and variational dequantization remains unexplored
- Effectiveness of importance sampling depends on proper design of proposal distribution, which isn't rigorously analyzed

## Confidence

**High confidence**: The mathematical framework for velocity parameterization and flow matching objectives is sound, building on established diffusion model theory

**Medium confidence**: The truncated-normal dequantization method shows promise, but its effectiveness depends on accurate approximation of training data distribution

**Low confidence**: The high-order flow matching fine-tuning procedure's impact on overall model performance needs additional validation

## Next Checks

1. **Cross-dataset validation**: Test the truncated-normal dequantization on non-image datasets (e.g., speech or tabular data) to verify the training-evaluation gap reduction generalizes beyond natural images

2. **Sample quality assessment**: Conduct user studies or FID comparisons between velocity parameterization and standard noise prediction models to validate that faster convergence translates to better sample quality, not just improved likelihood

3. **Robustness analysis**: Perform sensitivity analysis on the importance sampling distribution parameters to establish the method's robustness to hyperparameter choices and validate the variance reduction claims across different model scales