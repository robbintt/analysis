---
ver: rpa2
title: Analyzing the Efficacy of an LLM-Only Approach for Image-based Document Question
  Answering
arxiv_id: '2309.14389'
source_url: https://arxiv.org/abs/2309.14389
tags:
- text
- reading
- order
- question
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the efficacy of using a large language model
  (LLM) alone for document question answering tasks, without relying on a vision encoder.
  The approach involves serializing the textual information in document images using
  reading order and feeding it directly to an instruction-tuned LLM.
---

# Analyzing the Efficacy of an LLM-Only Approach for Image-based Document Question Answering

## Quick Facts
- arXiv ID: 2309.14389
- Source URL: https://arxiv.org/abs/2309.14389
- Reference count: 37
- Primary result: LLM-only approach achieves state-of-the-art or near-SOTA performance on document QA tasks by serializing text via reading order and feeding to instruction-tuned LLMs

## Executive Summary
This paper investigates whether large language models can perform document question answering without vision encoders by using reading order to serialize document text. The authors test this approach across six benchmark datasets and four model scales (250M to 11B parameters), finding that LLM-only methods can achieve competitive results with state-of-the-art models. The study identifies key factors affecting performance, including reading order quality, OCR context length, and the presence of answers in extracted text. The results suggest that strong LLMs can effectively handle document QA tasks, particularly when questions can be answered using text content alone or with the aid of world knowledge.

## Method Summary
The approach serializes document images into text sequences using reading order extraction, then feeds this serialized text directly to instruction-tuned LLMs without any vision encoder. The method processes six benchmark datasets (OCR-VQA, DocVQA, InfoVQA, TextVQA, ChartQA, AI2D) across four FlanT5 model scales (B, L, XL, XXL). The serialized text serves as input to the LLM, which generates answers to questions about the document content. The paper also conducts ablation studies, qualitative analysis, and examines factors like context length and reading order quality that influence performance.

## Key Results
- LLM-only approach achieves results on par with or closely approaching state-of-the-art performance across multiple document QA datasets
- Larger LLMs (FlanT5-XXL) consistently outperform smaller models, suggesting world knowledge compensates for missing visual information
- Reading order quality and OCR context length are critical factors affecting performance, with longer texts and poor reading order leading to more errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reading order serialization allows language models to infer document structure without explicit layout encoders
- Mechanism: By ordering text tokens in the sequence a human would read them, the LLM can reconstruct a coherent representation of the document that approximates spatial relationships
- Core assumption: A sufficiently strong LLM can infer missing layout information from the reading order and context alone
- Evidence anchors: [abstract] "strategies for serializing textual information within document images and feeding it directly to an instruction-tuned LLM, thus bypassing the need for an explicit vision encoder"
- Break condition: When reading order is ambiguous or incorrect, leading to jumbled context that the LLM cannot reconstruct

### Mechanism 2
- Claim: Larger LLMs can handle more complex reasoning by leveraging world knowledge alongside OCR context
- Mechanism: As model size increases, the LLM's capacity to store and retrieve relevant world knowledge improves, enabling it to answer questions that require external knowledge even when visual cues are missing
- Core assumption: World knowledge can compensate for missing visual information in certain question types
- Evidence anchors: [abstract] "the language model's knowledge may be enough to answer a question, even without the image content or layout"
- Break condition: When questions require visual reasoning that cannot be inferred from text and world knowledge alone

### Mechanism 3
- Claim: Zero-shot performance indicates that instruction-tuning enables LLMs to adapt to document QA tasks without domain-specific fine-tuning
- Mechanism: The instruction-tuning process exposes the LLM to diverse tasks, allowing it to generalize to new domains like document understanding using only the reading order text
- Core assumption: Instruction-tuned models have learned generalizable patterns that transfer to document QA
- Evidence anchors: [abstract] "the effectiveness of instruction-tuned LLMs, which exhibit remarkable adaptability to new tasks"
- Break condition: When task requires domain-specific knowledge or reasoning patterns not covered by instruction-tuning

## Foundational Learning

- Concept: Reading order extraction
  - Why needed here: Converts 2D document layout into 1D text sequence that preserves reading flow
  - Quick check question: Can you explain how reading order differs from simple top-to-bottom, left-to-right text extraction?

- Concept: OCR context length management
  - Why needed here: Determines whether the LLM can process the entire document in one pass or needs truncation
  - Quick check question: What happens to performance when context length exceeds the model's maximum input length?

- Concept: Zero-shot vs fine-tuned performance analysis
  - Why needed here: Helps understand how much the LLM relies on instruction-tuning versus domain adaptation
  - Quick check question: How does zero-shot performance vary across datasets with different text densities?

## Architecture Onboarding

- Component map: Input image → OCR engine → Reading order predictor → Text serialization → LLM prompt template → Answer generation
- Critical path: OCR → Reading order → Text serialization → LLM → Answer generation
- Design tradeoffs: Reading order quality vs. computational cost, context length vs. truncation, model size vs. inference speed
- Failure signatures: Poor reading order leads to jumbled context, insufficient context length causes information loss, small models fail on complex reasoning
- First 3 experiments:
  1. Test zero-shot performance across all datasets to establish baseline
  2. Compare different reading order strategies (standard vs. raster scan) on DocVQA
  3. Analyze performance correlation with OCR context length and reading order perplexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLM-only approaches compare to state-of-the-art models that incorporate vision encoders across different document QA datasets?
- Basis in paper: [explicit] The paper explicitly states that the LLM-only approach yields results that are on par with or closely approach state-of-the-art performance across a range of datasets
- Why unresolved: While the paper provides comparative results, it does not fully explore the nuances of performance differences across various datasets and question types
- What evidence would resolve it: Conducting a more detailed analysis of performance variations across datasets and question types, possibly through ablation studies or additional experiments

### Open Question 2
- Question: What are the limitations of using reading order as the sole source of layout information in document QA tasks?
- Basis in paper: [explicit] The paper identifies reading order quality as a key factor influencing the effectiveness of the LLM-only setup
- Why unresolved: The paper does not delve into the specific limitations or challenges of using reading order, especially in complex documents
- What evidence would resolve it: Further experiments that test the model's performance with different reading order strategies, including those designed for complex documents

### Open Question 3
- Question: How does the context length of OCR text affect the performance of LLM-only models in document QA tasks?
- Basis in paper: [explicit] The paper discusses the effect of OCR context length on model performance, noting that longer text lengths in images lead to more errors
- Why unresolved: The paper provides initial observations but does not explore the relationship between context length and performance in depth
- What evidence would resolve it: Conducting experiments that systematically vary context lengths and analyze their impact on performance

### Open Question 4
- Question: Can multi-tasking during training improve the generalization capabilities of LLM-only models for document QA tasks?
- Basis in paper: [explicit] The paper experiments with multi-tasking by training a single model on multiple datasets and observes that multi-tasking can boost performance
- Why unresolved: The paper does not fully explore the optimal strategies for multi-tasking or how different mixing proportions affect performance across various datasets
- What evidence would resolve it: Further experiments that test different multi-tasking strategies, including varying mixing proportions and dataset combinations

### Open Question 5
- Question: What role does zero-shot perplexity play in predicting the performance of LLM-only models on unseen document QA tasks?
- Basis in paper: [explicit] The paper introduces the concept of reading order perplexity and zero-shot perplexity as metrics to evaluate model performance and potential on unseen tasks
- Why unresolved: While the paper suggests a correlation between zero-shot perplexity and performance, it does not fully explore how this metric can be used to predict or guide model development for new tasks
- What evidence would resolve it: Conducting a comprehensive study that correlates zero-shot perplexity with performance across a wide range of tasks and datasets

## Limitations
- Performance heavily depends on reading order quality, which can be error-prone for documents with complex layouts
- Method's effectiveness is constrained by OCR context length, potentially missing critical information for longer documents
- Results may not fully capture the performance of larger frontier models that could potentially handle these tasks with even greater efficacy

## Confidence

- Reading order serialization replacing layout encoders: Medium confidence
- Larger LLMs compensating for missing visual information: Medium confidence
- Zero-shot adaptability of instruction-tuned LLMs: High confidence

## Next Checks

1. **Reading Order Robustness Test**: Evaluate performance across datasets with intentionally corrupted or randomized reading orders to quantify the impact of reading order quality on overall task performance.

2. **Context Length Saturation Analysis**: Systematically vary the OCR context length and measure the point at which performance plateaus or degrades, establishing the minimum viable context for each dataset type.

3. **Visual Reasoning Task Benchmark**: Test the approach on datasets requiring explicit visual reasoning (e.g., spatial relationships, visual patterns) to determine the hard limits of LLM-only approaches versus vision-encoder models.