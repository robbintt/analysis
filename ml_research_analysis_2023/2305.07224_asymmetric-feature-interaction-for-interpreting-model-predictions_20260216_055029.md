---
ver: rpa2
title: Asymmetric feature interaction for interpreting model predictions
arxiv_id: '2305.07224'
source_url: https://arxiv.org/abs/2305.07224
tags:
- interaction
- shapley
- feature
- asymmetric
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an asymmetric feature interaction attribution
  method to explain higher-order feature interactions in deep NLP models. The authors
  extend Shapley value to Asymmetric Shapley Interaction Value (ASIV), which measures
  the asymmetric influence between two subsets conditioned on their presence.
---

# Asymmetric feature interaction for interpreting model predictions

## Quick Facts
- arXiv ID: 2305.07224
- Source URL: https://arxiv.org/abs/2305.07224
- Authors: 
- Reference count: 6
- This paper proposes ASIV, which outperforms five baseline methods in identifying influential features for sentiment classification.

## Executive Summary
This paper introduces Asymmetric Shapley Interaction Value (ASIV) to explain higher-order feature interactions in deep NLP models. ASIV extends Shapley value to measure directional influence between feature subsets, capturing how one feature's presence asymmetrically affects another's contribution to model predictions. The method is evaluated on sentiment classification tasks using SST-2 and Yelp-2 datasets, demonstrating superior performance compared to five baseline attribution methods. Three sampling strategies are explored to approximate marginal contributions, with different strategies showing optimal performance for short versus long text sequences.

## Method Summary
The method extends Shapley value to asymmetric interactions by computing the difference in marginal contributions between two feature subsets conditioned on each other's presence. For feature subsets T1 and T2, ASIV measures how T2's presence changes T1's marginal contribution to the model's prediction. Three sampling strategies approximate the value function: marginal expectation (random sampling), conditional expectation (context-aware sampling), and in-domain expectation (domain-specific sampling). The method builds directed interaction graphs where edge weights represent asymmetric influences, and PageRank algorithm ranks feature importance by analyzing the graph structure.

## Key Results
- ASIV outperforms five baseline methods (HEDGE, SOC, Archipelago, Bivariate Shapley, Shapley interaction index) in identifying influential features
- Random sampling performs best for short text sequences, while conditional and in-domain sampling work better for long sequences
- The directed interaction graph reveals nuanced asymmetric influences, such as "not" modifying "buy" more than vice versa in negative sentiment prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Asymmetric Shapley Interaction Value captures directional feature influence by measuring marginal contribution differences between two subsets conditioned on each other's presence
- Mechanism: ASIV computes the difference between the marginal contribution of subset T1 to coalition S and the marginal contribution of T1 to coalition S without T2, using permutation-based sampling to approximate this value
- Core assumption: The asymmetric influence between features can be meaningfully captured by measuring how the presence of one subset changes the marginal contribution of another subset
- Evidence anchors:
  - [abstract] "ASIV outperforms five baseline methods in identifying influential features, with random sampling performing best on short sequences and conditional/in-domain sampling better for long sequences"
  - [section 3.2] "ASIV is defined as φT2(T1) = C1 Σ(∆T1v(S) - ∆T1v(S\T2))" which directly computes the asymmetric difference
  - [corpus] Weak - no direct citations to asymmetric interaction literature found in corpus
- Break condition: If features are truly independent or if the model doesn't learn asymmetric interactions, the difference measurements become meaningless

### Mechanism 2
- Claim: Different sampling strategies (marginal, conditional, in-domain) provide complementary ways to approximate the value function, with effectiveness depending on sequence length
- Mechanism: Each sampling strategy generates different reference distributions for the masked features, affecting how well the model's extrapolation behavior is captured
- Core assumption: The choice of reference distribution significantly impacts the faithfulness of attribution explanations
- Evidence anchors:
  - [section 3.3] "We investigate three different sampling strategies... marginal expectation, conditional expectation, and in-domain expectation"
  - [section 4.5.1] "random sampling strategy is effective for the short text sequence and for the long text sequence in-domain sampling could produce more smooth and domain-dependent context"
  - [corpus] Weak - no direct citations to sampling strategy comparisons found in corpus
- Break condition: If all sampling strategies produce similar reference distributions or if the model's behavior is uniform across different distributions

### Mechanism 3
- Claim: PageRank algorithm applied to directed interaction graphs effectively ranks feature importance by considering both incoming and outgoing interaction strengths
- Mechanism: Features with strong positive incoming edges and weak outgoing edges are ranked as more important, capturing the asymmetric nature of feature influence
- Core assumption: Feature importance can be inferred from the pattern of directed interactions rather than just individual feature attributions
- Evidence anchors:
  - [section 4.3] "we apply PageRank algorithm to obtain feature importance ranking"
  - [section 4.4] "the important features are expected to have more positive incoming edges and less outgoing edges"
  - [corpus] Weak - no direct citations to PageRank for interaction graphs found in corpus
- Break condition: If the interaction graph structure doesn't reflect true feature importance or if PageRank converges poorly

## Foundational Learning

- Shapley Value and Cooperative Game Theory:
  - Why needed here: ASIV extends Shapley value to asymmetric interactions, so understanding the original Shapley framework is essential
  - Quick check question: How does Shapley value compute a feature's contribution by averaging marginal contributions across all possible coalitions?

- Feature Attribution Methods:
  - Why needed here: The paper compares ASIV against multiple baseline methods, requiring understanding of their differences
  - Quick check question: What distinguishes model-agnostic methods like LIME from interaction-based methods like Shapley interaction index?

- PageRank Algorithm:
  - Why needed here: PageRank is used to derive feature importance rankings from directed interaction graphs
  - Quick check question: How does PageRank handle directed graphs differently from undirected ones in terms of node ranking?

## Architecture Onboarding

- Component map:
  Input preprocessing -> Model layer (BERT/RoBERTa) -> ASIV computation -> Baseline comparison -> Evaluation metrics -> PageRank ranking

- Critical path:
  1. Load dataset and model
  2. For each test instance, compute ASIV using chosen sampling strategy
  3. Build directed interaction graph
  4. Apply PageRank to obtain feature rankings
  5. Calculate AOPC and LOR metrics
  6. Compare against baselines

- Design tradeoffs:
  - Sampling strategy choice: Random sampling is computationally cheaper but may produce incoherent sequences for long texts; conditional and in-domain sampling are more computationally expensive but produce better results for long sequences
  - Approximation quality vs computation: Higher sample sizes improve ASIV accuracy but increase computation time quadratically
  - Graph construction: Building complete pairwise interaction graphs is O(n²) in number of features

- Failure signatures:
  - All features have similar importance rankings across sampling strategies (suggests model doesn't learn asymmetric interactions)
  - AOPC and LOR metrics don't improve with increased sample size (suggests poor sampling strategy choice)
  - PageRank produces unstable rankings across different random seeds (suggests graph structure is noisy)

- First 3 experiments:
  1. Verify ASIV computation on a simple synthetic dataset where ground truth asymmetric interactions are known
  2. Compare sampling strategy performance on short vs long sequences to validate the paper's claims
  3. Test PageRank ranking stability by varying the damping factor and random seed parameters

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The paper lacks direct citations to asymmetric interaction literature, making it difficult to assess novelty against existing work in this specific area
- Implementation details for baseline methods (particularly sampling parameters) are not fully specified
- The effectiveness of different sampling strategies may be highly dataset-dependent, with limited testing across diverse domains
- Computational complexity of O(n²) pairwise interactions may limit scalability to longer sequences or larger feature sets

## Confidence
- **High confidence**: Core mechanism of ASIV as an extension of Shapley value to asymmetric interactions
- **Medium confidence**: Empirical results showing ASIV outperforms baselines, though reproducibility is limited by unspecified implementation details
- **Low confidence**: Claims about PageRank-based feature ranking without sufficient validation of graph structure stability

## Next Checks
1. Implement a synthetic dataset with known asymmetric interactions to verify ASIV computes correct directional influences
2. Test sampling strategy performance across diverse datasets beyond SST-2 and Yelp-2 to assess generalizability
3. Evaluate PageRank ranking stability by varying damping factors and measuring correlation across different random seeds