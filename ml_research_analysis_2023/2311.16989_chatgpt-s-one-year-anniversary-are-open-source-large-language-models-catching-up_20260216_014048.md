---
ver: rpa2
title: 'ChatGPT''s One-year Anniversary: Are Open-Source Large Language Models Catching
  up?'
arxiv_id: '2311.16989'
source_url: https://arxiv.org/abs/2311.16989
tags:
- arxiv
- llms
- preprint
- language
- open-source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of open-source large
  language models (LLMs) that match or surpass ChatGPT's performance across various
  tasks. The authors systematically review open-source LLMs in domains including general
  capabilities, agent capabilities, logical reasoning, long-context modeling, application-specific
  tasks, and trustworthiness.
---

# ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?

## Quick Facts
- arXiv ID: 2311.16989
- Source URL: https://arxiv.org/abs/2311.16989
- Reference count: 25
- Key outcome: Open-source LLMs are rapidly closing the performance gap with closed models like ChatGPT, with several models matching or exceeding GPT-3.5-turbo on specific benchmarks

## Executive Summary
This paper provides a comprehensive survey of open-source large language models (LLMs) that match or surpass ChatGPT's performance across various tasks. The authors systematically review open-source LLMs in domains including general capabilities, agent capabilities, logical reasoning, long-context modeling, application-specific tasks, and trustworthiness. They find that while closed-source LLMs like GPT-4 generally outperform open-source models, the gap is rapidly narrowing. Several open-source models already match or exceed GPT-3.5-turbo on specific benchmarks, demonstrating significant progress in the field.

## Method Summary
The paper reviews existing open-source LLMs and their performance on various benchmarks. It discusses the training regimes (pre-training, fine-tuning, continual pre-training, inference) and best practices for training open-source LLMs. The authors evaluate models across multiple domains using established benchmarks such as MT-Bench, AlpacaEval, GSM8K, HumanEval, and others. The methodology involves systematic literature review and performance comparison across different categories of LLM capabilities.

## Key Results
- Llama-2-chat-70B surpasses GPT-3.5-turbo on AlpacaEval
- WizardLM-70B achieves comparable performance to GPT-3.5-turbo on MT-bench
- WizardCoder outperforms GPT-3.5-turbo on HumanEval with 19.1% absolute improvements
- Lemur-chat-70B surpasses GPT-3.5-turbo on coding tasks without task-specific fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Open-source LLMs can match or surpass GPT-3.5-turbo on specific benchmarks through enhanced instruction tuning and specialized pre-training
- Mechanism: By using task-specific instruction tuning datasets and evolving instruction complexity, models like WizardCoder and WizardMath achieve significant performance gains over GPT-3.5-turbo
- Core assumption: High-quality, diverse instruction tuning data is more effective than simply scaling model size
- Evidence anchors: "Several open-source models already match or exceed GPT-3.5-turbo on specific benchmarks" and "WizardCoder outperforms GPT-3.5-turbo on HumanEval with 19.1% absolute improvements"
- Break condition: If the instruction tuning data quality is poor or not diverse enough, the performance gains will not materialize

### Mechanism 2
- Claim: Pre-training on higher quality data mixtures can induce stronger abilities in open-source LLMs without task-specific fine-tuning
- Mechanism: Models like Lemur and Phi achieve superior performance on logical reasoning and coding tasks by pre-training on a balanced mixture of natural language and code data
- Core assumption: The quality and relevance of pre-training data is more important than the sheer quantity
- Evidence anchors: "Lemur-chat-70B surpasses the performance of GPT-3.5-turbo when exploring the environment or following feedback on coding tasks" and "Lemur-70B achieves significant improvements over GPT-3.5-turbo on both HumanEval and GSM8K without task-specific fine-tuning"
- Break condition: If the pre-training data mixture is not carefully curated, the model may not learn the desired abilities effectively

### Mechanism 3
- Claim: Specialized fine-tuning for specific applications can enable open-source LLMs to outperform GPT-3.5-turbo in domain-specific tasks
- Mechanism: Models like MentalLlama-chat-13B and Radiology-Llama2 achieve superior performance in medical tasks through fine-tuning on domain-specific datasets
- Core assumption: Task-specific fine-tuning on high-quality, relevant data can significantly improve model performance in specialized domains
- Evidence anchors: "MentalLlama-chat-13B model with zero-shot prompting outperforms ChatGPT with few-shot prompting or with zero-shot prompting for 9 out of 10 tasks in IMHI" and "Radiology-Llama-2 model outperforms ChatGPT and GPT-4 by a large margin on both MIMIC-CXR and OpenI datasets"
- Break condition: If the domain-specific datasets are not comprehensive or representative, the model's performance may not generalize well

## Foundational Learning

- Concept: Large Language Models (LLMs)
  - Why needed here: Understanding the basics of LLMs is crucial to grasp the advancements and comparisons discussed in the paper
  - Quick check question: What are the key differences between decoder-only and encoder-decoder architectures in LLMs?

- Concept: Instruction Tuning
  - Why needed here: Instruction tuning is a key technique used to improve the performance of open-source LLMs on various tasks
  - Quick check question: How does instruction tuning differ from traditional fine-tuning, and why is it more effective for LLMs?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF is a critical component in aligning LLMs with human preferences and improving their safety and reliability
  - Quick check question: What are the main steps involved in the RLHF process, and how does it help in reducing harmful outputs from LLMs?

## Architecture Onboarding

- Component map: Pre-training -> Fine-tuning -> RLHF -> Inference
- Critical path: 1) Pre-train the model on a massive corpus of text data, 2) Fine-tune the model using high-quality instruction tuning datasets, 3) Apply RLHF to align the model with human preferences and improve safety, 4) Evaluate the model on various benchmarks and task-specific datasets
- Design tradeoffs: Model size vs. efficiency, pre-training data quality vs. quantity, task-specific vs. general capabilities
- Failure signatures: Poor performance on benchmarks, hallucinations or unsafe outputs, overfitting to specific datasets
- First 3 experiments: 1) Evaluate the model's performance on a diverse set of general benchmarks (e.g., MT-bench, AlpacaEval) to assess its overall capabilities, 2) Test the model's performance on task-specific datasets (e.g., HumanEval for coding, medical datasets) to gauge its specialized abilities, 3) Assess the model's safety and reliability by evaluating its outputs on safety benchmarks and checking for hallucinations or harmful content

## Open Questions the Paper Calls Out
The paper identifies several open questions and areas for future research:
- Improving the efficiency and effectiveness of open-source LLMs
- Enhancing the safety and reliability of open-source LLMs through better alignment techniques
- Developing more robust evaluation methodologies for LLMs
- Addressing data contamination and quality issues in pre-training data

## Limitations
- The comparison between open-source and closed models is challenging due to differences in training data, model architecture, and evaluation methodologies
- Many cited performance improvements are based on benchmark results that may not generalize to real-world applications
- The paper does not fully address the resource requirements and computational costs associated with training and deploying these large models
- The rapid pace of development in this field means some comparisons may become outdated quickly

## Confidence
- High: The paper accurately documents the growing capabilities of open-source LLMs and identifies key trends in the field
- Medium: The claim that several open-source models match or exceed GPT-3.5-turbo on specific benchmarks is supported by the evidence presented, but the generalizability of these results is uncertain
- Low: The assertion that open-source LLMs are "catching up" to GPT-4 in general capabilities is not fully supported by the evidence, as the paper focuses more on specific tasks and benchmarks

## Next Checks
1. Conduct a head-to-head comparison of open-source LLMs and GPT-4 on a diverse set of real-world tasks and applications, rather than relying solely on benchmark results
2. Investigate the resource requirements and computational costs associated with training and deploying the most advanced open-source LLMs, and compare these to the costs of using closed models like GPT-4
3. Perform a longitudinal study tracking the progress of open-source LLMs over time, to better understand the rate of improvement and the factors driving it