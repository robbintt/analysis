---
ver: rpa2
title: Hadamard Domain Training with Integers for Class Incremental Quantized Learning
arxiv_id: '2310.03675'
source_url: https://arxiv.org/abs/2310.03675
tags:
- learning
- quantization
- training
- accuracy
- forgetting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Hadamard Domain Quantized Training (HDQT),
  a technique that leverages inexpensive Hadamard transforms to enable low-precision
  training with only integer matrix multiplications. HDQT is evaluated in the context
  of class-incremental learning (CIL) on CIFAR100 and three human activity recognition
  datasets.
---

# Hadamard Domain Training with Integers for Class Incremental Quantized Learning

## Quick Facts
- arXiv ID: 2310.03675
- Source URL: https://arxiv.org/abs/2310.03675
- Reference count: 40
- Key outcome: Achieves less than 0.5% and 3% accuracy degradation while quantizing all matrix multiplications inputs down to 4-bits with 8-bit accumulators in class-incremental learning.

## Executive Summary
This paper introduces Hadamard Domain Quantized Training (HDQT), a technique that leverages inexpensive Hadamard transforms to enable low-precision training with only integer matrix multiplications. The method is evaluated in the context of class-incremental learning (CIL) on CIFAR100 and three human activity recognition datasets. HDQT reduces initial training accuracy but minimally impacts CIL performance, achieving similar learning performance to floating-point models while enabling potential deployment on energy-efficient platforms.

## Method Summary
HDQT uses 4-bit integer inputs and 8-bit integer accumulators with Hadamard transforms for backward-pass matrix multiplications, stochastic rounding for sensitive tensors (gradients and activations), and tiled matrix multiplication for forward-pass accumulation. The method is integrated with standard CIL algorithms (LwF, iCaRL, BiC) and evaluated on ResNet-32 for CIFAR100 and 3-layer fully connected networks for HAR datasets. Preprocessing includes normalization, random crops, flips, and color jitter for CIFAR100, and feature extraction from sensor data for HAR.

## Key Results
- Less than 0.5% accuracy degradation on CIFAR100 with 4-bit inputs and 8-bit accumulators
- Less than 3% accuracy degradation on human activity recognition datasets
- Initial training accuracy drops but CIL performance remains stable, similar to floating-point models
- Demonstrates potential for learning continually on energy-efficient platforms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hadamard domain operations spread tensor information more evenly across quantization levels, reducing empty bins.
- Mechanism: The Hadamard transform is an orthogonal, energy-preserving linear transform implemented with only additions. When tensors are transformed, their distributions become more uniform, allowing integer quantization to use the full available range without large empty bins.
- Core assumption: Tensor distributions in neural network operations have heavy-tailed or skewed structures that cause inefficient quantization.
- Evidence anchors: [abstract] "We propose a technique that leverages inexpensive Hadamard transforms to enable low-precision training with only integer matrix multiplications." [section 3] "The information-spreading property of the Hadamard transform allows us to make better use of all available quantization levels, capturing information from the tails of the distribution."

### Mechanism 2
- Claim: Stochastic rounding of gradients and activations mitigates quantization bias in sensitive tensors.
- Mechanism: During backward pass, gradients and activations are quantized using stochastic rounding instead of deterministic rounding. This unbias gradient estimation and prevents training divergence caused by systematic quantization errors in these sensitive tensors.
- Core assumption: Quantization bias in gradients and activations significantly impacts learning more than in weights.
- Evidence anchors: [abstract] "We further determine which tensors need stochastic rounding..." [section 3] "To mitigate the impact of bias introduced due to quantization in these sensitive tensors, we use stochastic rounding..."

### Mechanism 3
- Claim: Tiled matrix multiplication enables low-bit accumulators while preventing overflow in forward pass.
- Mechanism: Forward pass matrix multiplications are broken into tiles, with partial sums accumulated in low-bit (8-bit) accumulators. This reduces accumulator bit-growth while maintaining numerical stability. Gradient sparsity in backward pass prevents similar issues there.
- Core assumption: Forward pass requires more careful accumulator management than backward pass due to gradient sparsity.
- Evidence anchors: [abstract] "... propose tiled matrix multiplication to enable low-bit width accumulators." [section 3] "We propose tile-based matrix multiplication for the forward pass to enable low-bit width accumulation."

## Foundational Learning

- Concept: Orthogonal transforms and their properties (energy preservation, symmetry)
  - Why needed here: Hadamard transform's orthogonality allows domain multiplication without transforming back, critical for efficient integer-only computation
  - Quick check question: What property of orthogonal matrices allows HkHk = kI to hold?

- Concept: Quantization noise and bias effects on neural network training
  - Why needed here: Understanding why stochastic rounding is needed and which tensors are sensitive to quantization
  - Quick check question: Why does deterministic rounding introduce bias in gradient estimation?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The paper evaluates Hadamard domain training specifically in class-incremental learning scenarios
  - Quick check question: What is the stability-plasticity dilemma in continual learning?

## Architecture Onboarding

- Component map:
  - Hadamard Transform Unit: Applies block-diagonal Hadamard transform to tensors
  - Stochastic Rounding Unit: Applies stochastic rounding to gradients and activations during backward pass
  - Tiled Matrix Multiply Unit: Performs forward pass matrix multiplications with tiled accumulation
  - Quantizer: Uniform integer quantization with scaling and clipping
  - Calibration Unit: Determines scaling factors from tensor statistics

- Critical path:
  1. Forward pass: Input → Hadamard transform → quantized matrix multiply (tiled) → output
  2. Backward pass: Error → Hadamard transform → quantized matrix multiply → gradients (stochastic rounding) → weight update

- Design tradeoffs:
  - 4-bit inputs vs 8-bit accumulators: Balance between memory efficiency and numerical stability
  - Block size for accumulators: Trade-off between tiling overhead and accumulator bit-growth
  - Stochastic rounding vs deterministic: Unbiased gradients vs increased variance

- Failure signatures:
  - Training divergence: Likely stochastic rounding variance too high or accumulator overflow
  - Random accuracy: Numerical instability in LUQ-style quantization or insufficient accumulator bits
  - Accuracy plateau: Accumulator bits insufficient or transform overhead overwhelming benefits

- First 3 experiments:
  1. Implement Hadamard transform on CIFAR100 with 4-bit inputs and 16-bit accumulators, compare to baseline
  2. Add stochastic rounding to gradients and activations, measure impact on CIFAR100 accuracy
  3. Reduce accumulator bits to 8, evaluate CIFAR100 accuracy degradation and identify failure modes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Hadamard Domain Quantized Training (HDQT) perform on other continual learning scenarios beyond class-incremental learning, such as task-incremental learning or domain-incremental learning?
- Basis in paper: [inferred] The paper focuses on class-incremental learning but mentions that the method is "broadly applicable to different continual learning scenarios."
- Why unresolved: The paper only evaluates HDQT on class-incremental learning and does not explore its performance on other continual learning scenarios.
- What evidence would resolve it: Experiments evaluating HDQT on task-incremental learning and domain-incremental learning scenarios with various datasets and comparing the results to the current state-of-the-art methods.

### Open Question 2
- Question: What is the impact of using different block sizes for the accumulators in HDQT on the final accuracy and energy efficiency?
- Basis in paper: [explicit] The paper mentions using a block size of 32 for accumulators but does not explore the impact of using different block sizes.
- Why unresolved: The paper only uses a fixed block size for accumulators and does not investigate how changing this parameter affects the performance.
- What evidence would resolve it: Experiments varying the block size for accumulators and evaluating the impact on accuracy and energy efficiency for different datasets and continual learning scenarios.

### Open Question 3
- Question: How does the performance of HDQT scale with larger models and more complex datasets, such as ImageNet or larger language models?
- Basis in paper: [inferred] The paper evaluates HDQT on CIFAR100 and smaller human activity recognition datasets, but does not explore its performance on larger models and datasets.
- Why unresolved: The paper only uses relatively small models and datasets, and it is unclear how HDQT would perform on larger-scale problems.
- What evidence would resolve it: Experiments applying HDQT to larger models and more complex datasets, such as ImageNet or language models, and comparing the results to the current state-of-the-art methods.

## Limitations
- Limited comparison with other quantized training approaches in continual learning contexts
- Choice of 4-bit quantization appears arbitrary without ablation studies showing performance across different bit-widths
- Lack of hardware-specific optimizations or actual computational overhead measurements
- Forward pass matrix multiplication implementation lacks detailed justification for the specific configuration chosen

## Confidence

**High confidence**: The core claim that Hadamard domain training with integer operations can achieve near-floating-point accuracy in class-incremental learning tasks.

**Medium confidence**: The mechanism explanation regarding information spreading through Hadamard transforms and the selection of which tensors require stochastic rounding.

**Low confidence**: The assertion that this approach is particularly well-suited for edge devices and resource-constrained systems without actual hardware measurements.

## Next Checks

1. **Ablation study on quantization bit-width**: Systematically evaluate model performance with 2-bit, 4-bit, and 8-bit quantization to identify the optimal precision trade-off.

2. **Comparison with standard quantized training**: Implement a direct comparison between HDQT and conventional quantized training methods in the same CIL framework.

3. **Hardware implementation analysis**: Measure actual memory usage, computational latency, and power consumption on representative edge hardware to validate claimed benefits.