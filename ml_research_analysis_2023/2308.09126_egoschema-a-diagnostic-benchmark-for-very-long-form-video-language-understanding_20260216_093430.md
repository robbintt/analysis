---
ver: rpa2
title: 'EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding'
arxiv_id: '2308.09126'
source_url: https://arxiv.org/abs/2308.09126
tags:
- video
- answer
- question
- data
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "EgoSchema introduces a new diagnostic benchmark for evaluating\
  \ very long-form video-language understanding. It introduces the concept of temporal\
  \ certificate length to measure the intrinsic temporal complexity of video tasks,\
  \ finding EgoSchema to have certificates 5.7\xD7 longer than the next closest dataset."
---

# EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding

## Quick Facts
- arXiv ID: 2308.09126
- Source URL: https://arxiv.org/abs/2308.09126
- Reference count: 40
- Primary result: Introduces a new diagnostic benchmark with temporal certificates 5.7× longer than existing datasets, where state-of-the-art models achieve <33% accuracy while humans reach 76%.

## Executive Summary
EgoSchema introduces a diagnostic benchmark for evaluating very long-form video-language understanding, featuring 5,000+ multiple-choice QA pairs over 250 hours of real video. The benchmark introduces the concept of temporal certificate length to measure the intrinsic temporal complexity of video tasks, finding EgoSchema to have certificates 5.7× longer than the next closest dataset. State-of-the-art models achieve less than 33% accuracy, while humans reach 76%, indicating significant room for improvement in long-term video understanding.

## Method Summary
EgoSchema is generated through a staged pipeline: raw data filtering selects 3-minute clips with at least 30 narrations from the Ego4D dataset, question-answer generation uses large language models (LLMs) like GPT-4, BARD, and Claude with multiple inference call chaining procedures, filtering applies rule-based and LLM-based methods to remove ungrounded or low-quality outputs, and manual curation ensures high quality and long temporal certificate lengths. The dataset consists of multiple-choice QA pairs where each question requires selecting the correct answer from five options based on a three-minute-long video clip.

## Key Results
- State-of-the-art video-language models achieve less than 33% accuracy on EgoSchema
- Human performance reaches 76% accuracy, establishing a strong baseline
- EgoSchema temporal certificates are 5.7× longer than the next closest dataset (ActivityNet-QA)
- Blind filtering (removing questions answerable without video) reduces dataset size by only 5.1%, indicating high visual grounding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal certificates provide a principled measure of intrinsic temporal complexity for video understanding tasks.
- Mechanism: By defining the certificate as the minimal set of video subclips needed to verify an annotation, it decouples temporal difficulty from video clip length and focuses on the essential information needed for understanding.
- Core assumption: Human verifiers can consistently identify minimal necessary subclips for any given annotation.
- Evidence anchors: [abstract] "temporal certificate sets, a general notion for capturing the intrinsic temporal understanding length associated with a broad range of video understanding tasks & datasets"

### Mechanism 2
- Claim: LLM-based pipeline can generate high-quality long-form video question-answer pairs with proper filtering.
- Mechanism: Chain multiple LLM inference calls with different prompts to generate diverse questions and answers, then apply rule-based and LLM-based filtering to remove ungrounded or low-quality outputs before human curation.
- Core assumption: LLMs can generate sufficiently diverse and relevant questions when prompted with appropriate techniques and filtered properly.
- Evidence anchors: [section] "We experiment with several LLM inference call chaining procedures with trade-offs between quality and cost of generation"

### Mechanism 3
- Claim: Two-stage human curation effectively ensures data quality while managing costs.
- Mechanism: First round filters for basic quality (answerable questions, correct answers, long certificates), second round reinforces these standards with higher accuracy.
- Core assumption: Human curators can reliably identify and filter low-quality question-answer pairs when given clear guidelines.
- Evidence anchors: [section] "We find that more than 97% of the questions that pass the first round also pass the second round"

## Foundational Learning

- Concept: Temporal reasoning in video
  - Why needed here: Understanding long-form video requires tracking changes and relationships across extended time periods
  - Quick check question: If a person is seen holding an object at t=10s and the object is on a table at t=120s, what reasoning capability is needed to connect these observations?

- Concept: Multiple choice question design
  - Why needed here: Questions must be challenging yet answerable, with distractors that are plausible but clearly wrong
  - Quick check question: What makes a good distractor answer in a multiple choice question - should it be obviously wrong or subtly incorrect?

- Concept: Video-language model adaptation
  - Why needed here: State-of-the-art models need to be adapted from their original training tasks to handle long-form video QA
  - Quick check question: When adapting a model trained on short videos to long-form QA, what are the two main architectural challenges that must be addressed?

## Architecture Onboarding

- Component map: Data generation pipeline: LLM calls → rule-based filtering → LLM filtering → human curation → Model evaluation with different frame sampling strategies → Certificate estimation with human annotation

- Critical path: 1. Generate questions/answers with LLM 2. Apply filtering rounds to remove low-quality outputs 3. Human curation to ensure quality and certificate length 4. Model evaluation on filtered dataset

- Design tradeoffs: More LLM calls improves quality but increases cost; stricter filtering reduces dataset size but improves quality; longer certificates increase temporal complexity but may reduce data availability

- Failure signatures: High false positive rate in human curation indicates ambiguous questions; models performing well on short clips but poorly on long clips indicates temporal reasoning gap; certificate length distribution skewed toward short durations suggests questions are not truly long-form

- First 3 experiments: 1. Evaluate model performance with 1 frame vs 30 frames to establish temporal reasoning gap 2. Test human performance with and without video to establish importance of visual grounding 3. Compare certificate length distributions across different question types to identify which require longest temporal reasoning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal clip length and narration density for maximizing both the number of viable clips and the quality of generated questions in long-form video datasets?
- Basis in paper: [inferred] The paper discusses choosing 3-minute clips with 10 narrations per minute for EgoSchema, but also explores other combinations in Figure 8.
- Why unresolved: The paper does not provide a systematic analysis of how different clip lengths and narration densities affect the quality and quantity of generated questions.
- What evidence would resolve it: A comprehensive study comparing various clip lengths and narration densities on metrics like question diversity, answer difficulty, and human annotation effort.

### Open Question 2
- Question: How does the performance of long-form video understanding models scale with the temporal certificate length of the dataset?
- Basis in paper: [explicit] The paper introduces the concept of temporal certificate length and finds EgoSchema to have certificates 5.7× longer than the next closest dataset.
- Why unresolved: While the paper demonstrates that current models struggle with long-form video understanding, it does not explore how their performance changes as the temporal certificate length increases.
- What evidence would resolve it: Benchmarking models on datasets with varying temporal certificate lengths and analyzing their accuracy curves.

### Open Question 3
- Question: What are the key differences in human performance when answering questions based solely on video versus video combined with text narration?
- Basis in paper: [explicit] The paper reports that humans achieve 76.2% accuracy when answering questions after watching the video without reading the text, compared to 75.0% when they can use both video and text.
- Why unresolved: While the paper notes a slight improvement in accuracy when using both video and text, it does not investigate the underlying reasons or the potential trade-offs.
- What evidence would resolve it: Conducting user studies with detailed eye-tracking, think-aloud protocols, and post-experiment interviews.

## Limitations
- Temporal Certificate Estimation Uncertainty: The methodology for determining temporal certificates relies heavily on human annotator judgment without detailed operationalization or inter-annotator agreement metrics.
- LLM Generation Pipeline Transparency: Critical implementation details like prompt structures, temperature settings, and filtering thresholds are missing, making reproducibility difficult.
- Dataset Scope Constraints: EgoSchema is derived exclusively from the Ego4D dataset with specific constraints, potentially limiting generalizability to other long-form video domains.

## Confidence
**High Confidence Claims:**
- State-of-the-art models' performance (<33% accuracy) represents a significant challenge for current long-form video understanding systems
- Human performance baseline of 76% accuracy is reliable given the two-stage curation process
- EgoSchema has substantially longer temporal certificates than existing benchmarks (5.7× longer than ActivityNet-QA)

**Medium Confidence Claims:**
- The temporal certificate metric meaningfully captures intrinsic temporal complexity of video understanding tasks
- The LLM-based pipeline produces high-quality, diverse question-answer pairs after filtering
- The two-stage human curation effectively balances quality and cost

**Low Confidence Claims:**
- The specific quantitative differences in temporal certificate lengths between question types
- The exact impact of different LLM generation strategies on final dataset quality
- The generalizability of EgoSchema's findings to non-Ego4D video domains

## Next Checks
1. **Inter-annotator Agreement Study**: Conduct a controlled experiment with multiple annotators independently estimating temporal certificates for the same questions to establish reliability metrics and identify sources of disagreement.

2. **Ablation Analysis of LLM Pipeline**: Systematically vary key LLM generation parameters (temperature, sampling strategy, prompt structure) and measure their impact on question diversity, quality, and temporal certificate length to identify optimal configurations.

3. **Cross-dataset Generalization Test**: Evaluate EgoSchema-trained models on long-form video QA tasks from different domains (e.g., movies, surveillance footage, educational videos) to assess whether the benchmark captures universal long-form understanding capabilities or remains domain-specific.