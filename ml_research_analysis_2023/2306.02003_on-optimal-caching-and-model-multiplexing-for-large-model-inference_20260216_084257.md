---
ver: rpa2
title: On Optimal Caching and Model Multiplexing for Large Model Inference
arxiv_id: '2306.02003'
source_url: https://arxiv.org/abs/2306.02003
tags: []
core_contribution: This paper addresses the problem of efficient inference for large
  language models by combining caching and model multiplexing. The key idea is to
  jointly optimize both approaches to reduce inference costs.
---

# On Optimal Caching and Model Multiplexing for Large Model Inference

## Quick Facts
- arXiv ID: 2306.02003
- Source URL: https://arxiv.org/abs/2306.02003
- Reference count: 40
- Primary result: Up to 50x improvement in inference cost by combining caching and model multiplexing

## Executive Summary
This paper addresses the challenge of efficient inference for large language models by combining caching and model multiplexing. The authors propose a joint optimization framework that caches query responses and uses a model multiplexer to select between small and large models based on estimated costs. The approach achieves optimal rates in both offline and online settings by leveraging Least Expected Cost (LEC) caching and pessimism-based cost estimation. Empirical results show significant improvements over baseline methods, with up to 50x cost reduction in simulations and 4.3x improvement in FLOPs on real datasets.

## Method Summary
The method combines caching and model multiplexing through a joint optimization framework. For caching, the paper uses Least Expected Cost (LEC) or Greedy Dual-Size with Frequency (GDSF) algorithms to select which queries to cache based on their frequency and cost. The model multiplexer selects between small and large models for cache misses, using cost estimators with pessimism to handle long-tailed distributions. The approach works in both offline settings (with known query distributions) and online settings (with sequential learning), providing theoretical regret bounds and practical algorithms for implementation.

## Key Results
- Up to 50x improvement over baseline methods when cost ratios reach 100x
- 4.3x improvement in FLOPs and 1.8x improvement in latency on real datasets
- Theoretical optimal regret bounds achieved for both offline and online settings

## Why This Works (Mechanism)

### Mechanism 1
Combining caching and model multiplexing reduces inference cost by avoiding redundant computation and selecting cheaper models for suitable queries. When a query hits the cache, cost is zero. For cache misses, the model multiplexer selects between small and large models based on estimated cost, with the small model being cheaper but less accurate.

### Mechanism 2
Pessimism in cost estimation for less-frequently-seen queries prevents overfitting and ensures conservative cache replacement. Lower-confidence bounds are used in the cost estimator, subtracting a term proportional to log(N)/frequency, which biases against caching rare queries.

### Mechanism 3
The optimal policy in the population setting caches the L queries with the highest P(q) * min(c_s(q), c_l(q)). The cache selection maximizes the expected savings by prioritizing queries that are both frequent and have a large cost difference between models.

## Foundational Learning

- **Regret analysis in online learning**: Needed to quantify the performance of online caching and multiplexing algorithms against the optimal offline policy. Quick check: What does a regret bound of O(âˆšT) imply about the algorithm's convergence to the optimal policy?

- **Concentration inequalities (Chernoff, Hoeffding)**: Needed to prove high-probability bounds on cost estimation errors and cache selection quality. Quick check: Why is pessimism necessary when the cost estimator has high variance for rare queries?

- **Cache replacement policies (LFU, LRU, GDSF, LEC)**: Needed to compare the proposed LEC-based approach against traditional policies that ignore cost. Quick check: How does LEC differ from LFU when costs vary significantly across queries?

## Architecture Onboarding

- **Component map**: Query -> Semantic search oracle -> Cache storage layer -> Model multiplexer -> Small/Large model -> Cost estimator -> Online learning loop

- **Critical path**: 1) Receive query, 2) Semantic search in cache, 3) If hit, return cached response, 4) If miss, estimate costs with current models, 5) Select model via multiplexer, 6) Process query, observe cost, 7) Update cache and estimators

- **Design tradeoffs**: Cache size vs. freshness (larger cache reduces misses but increases memory and stale data risk), Estimator accuracy vs. computational overhead (more sophisticated estimators improve selection but add latency), Multiplexer complexity vs. adaptability (complex models may generalize better but train slower)

- **Failure signatures**: High regret despite low cache miss rate (multiplexer consistently choosing wrong model), Oscillating cache contents (cost estimator variance too high, causing instability), Cache size grows uncontrollably (eviction policy too conservative or semantic search too broad)

- **First 3 experiments**: 1) Synthetic dataset with known P(q) and c(q): Verify LEC achieves lower regret than LFU, 2) Real dataset with two models of different sizes: Measure FLOPs/latency reduction from combined caching+multiplexing, 3) Ablation study: Remove pessimism from cost estimator and observe cache stability under long-tail distributions

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of LEC caching with model multiplexing scale with the number of models in the ensemble beyond the two-model case studied in this paper? The paper mentions generalization to multiple models but only provides analysis for the two-model case.

### Open Question 2
What is the optimal cache size L that balances the trade-off between storage costs and inference cost reduction in practical LLM deployment scenarios? The paper studies caching with fixed cache size but does not explore how to determine the optimal cache size.

### Open Question 3
How does the performance of LEC caching and model multiplexing degrade when the model multiplexer has limited accuracy, and what are the best strategies for handling such cases? The paper briefly discusses this scenario but does not provide detailed analysis or comprehensive strategies.

## Limitations

- The semantic equivalence oracle assumption for fuzzy query matching is not implemented or evaluated
- Cost estimation framework assumes stationary and independent query costs, which may not hold in practice
- Empirical validation is limited to synthetic simulations and single real-world next-token prediction task

## Confidence

- **High confidence**: Theoretical regret bounds for LEC and GDSF caching algorithms
- **Medium confidence**: Pessimism-based estimator approach and empirical results on limited datasets
- **Low confidence**: Semantic search oracle assumption and claims about long-tail elimination

## Next Checks

1. **Semantic search implementation**: Replace oracle assumption with actual semantic search using vector embeddings and measure how search quality affects caching performance

2. **Cost estimator sensitivity analysis**: Systematically vary pessimism parameters and confidence levels across different query frequency distributions to identify robust settings

3. **Multi-turn conversation evaluation**: Extend experiments beyond next-token prediction to multi-turn dialogue tasks with context windows to test caching effectiveness with growing context