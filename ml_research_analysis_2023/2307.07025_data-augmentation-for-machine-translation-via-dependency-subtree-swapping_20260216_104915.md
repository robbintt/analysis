---
ver: rpa2
title: Data Augmentation for Machine Translation via Dependency Subtree Swapping
arxiv_id: '2307.07025'
source_url: https://arxiv.org/abs/2307.07025
tags:
- augmentation
- data
- translation
- dependency
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method for data augmentation for neural machine
  translation using dependency subtree swapping. The authors propose a framework for
  extracting corresponding subtrees from the dependency parse trees of source and
  target sentences and swapping them to create augmented samples.
---

# Data Augmentation for Machine Translation via Dependency Subtree Swapping

## Quick Facts
- arXiv ID: 2307.07025
- Source URL: https://arxiv.org/abs/2307.07025
- Reference count: 9
- One-line primary result: The proposed dependency subtree swapping method improves BLEU scores for three out of four language pairs (En-De, En-He, En-Hu) while performance degrades for En-Vi due to parser quality issues.

## Executive Summary
This paper introduces a data augmentation method for neural machine translation that leverages dependency subtree swapping across parallel sentences. The approach extracts corresponding subtrees (subjects and objects) from source and target dependency parse trees, filters them using graph-based similarity metrics (GED and EM), and swaps them simultaneously in both sentences to create augmented training samples. The method demonstrates consistent improvements in translation quality for English-German, English-Hebrew, and English-Hungarian language pairs, with gains of 1.0-1.8 BLEU points. The authors also highlight the method's dependency on parser quality, noting that poor Vietnamese parser performance led to subpar results for English-Vietnamese translation.

## Method Summary
The method works by first parsing source and target sentences into dependency trees using tools like Stanza, then identifying eligible subtree pairs based on syntactic roles (NSUBJ, OBJ). Graph Edit Distance (GED) and Edge Mapping (EM) metrics are computed to measure syntactic similarity between subtrees, with only pairs exceeding a 0.4 similarity threshold being selected. The authors simultaneously swap the identified subtrees in both the source and target sentences, preserving parallelism while creating diverse training examples. This augmented data is then mixed with original training data at various ratios (0.5, 2, 3) and used to train Transformer-based NMT models with standard hyperparameters.

## Key Results
- Consistent BLEU improvements of 1.0-1.8 points for English-German, English-Hebrew, and English-Hungarian language pairs
- Subpar performance for English-Vietnamese attributed to low-quality Vietnamese dependency parser
- Best performance achieved with augmentation ratio of 2, where augmented samples equal original training data
- Graph-based filtering effectively removes noisy pairs while maintaining sufficient diversity for training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Swapping dependency subtrees across parallel sentences preserves semantic alignment when syntactic structures are similar.
- Mechanism: The method extracts corresponding subtrees (e.g., subject and object) from the dependency parse trees of source and target sentences, then swaps them across bisentences. This creates augmented training samples while maintaining syntactic and semantic coherence.
- Core assumption: Corresponding subtrees in source and target languages share similar syntactic structures and meanings, making them interchangeable without breaking translation quality.
- Evidence anchors:
  - [abstract]: "We extract corresponding subtrees from the dependency parse trees of the source and target sentences and swap these across bisentences to create augmented samples."
  - [section]: "We hypothesize that if the syntactic composition of the subtrees of the source and target sentences are significantly different, the subtrees likely have a different meaning."
- Break condition: If dependency parsers produce low-quality trees, or if the syntactic structures of source and target sentences differ significantly, swapped subtrees may introduce noise and degrade translation quality.

### Mechanism 2
- Claim: Graph-based filtering (GED and EM) ensures that swapped subtrees correspond to the same meaning, improving augmentation quality.
- Mechanism: GED measures the minimal cost to transform one dependency tree into another, while EM computes edge mappings to find syntactic similarities. Only subtree pairs with high similarity scores are used for augmentation.
- Core assumption: High graph similarity between subtrees indicates semantic correspondence, reducing noise in augmented data.
- Evidence anchors:
  - [abstract]: "We perform thorough filtering based on graph-based similarities of the dependency trees and additional heuristics to ensure that extracted subtrees correspond to the same meaning."
  - [section]: "To study the syntactic similarities of the dependency trees, we explore two graph-based methods: Graph Edit Distance (GED) and Edge Mapping (EM)."
- Break condition: If the similarity thresholds are too low, noisy pairs pass through; if too high, too few pairs are selected, reducing augmentation diversity.

### Mechanism 3
- Claim: Simultaneous alteration of source and target sentences preserves parallelism in augmented samples, improving model training stability.
- Mechanism: By swapping subtrees in both source and target sentences at the same time, the augmented bisentences maintain alignment, unlike methods that alter only one side.
- Core assumption: Parallel alignment is critical for training neural machine translation models; breaking it introduces noise.
- Evidence anchors:
  - [section]: "One clear advantage of this method compared to many other non-model based data augmentation methods for NMT, is that this approach alters the source and target sentences simultaneously, so it has a better chance of maintaining parallelism in the augmented samples."
  - [abstract]: "swap these across bisentences to create augmented samples."
- Break condition: If subtree swapping introduces syntactic mismatches (e.g., mismatched verb tenses or agreement), parallelism may still break down.

## Foundational Learning

- Concept: Dependency parsing and Universal Dependencies (UD) scheme
  - Why needed here: The method relies on extracting subtrees based on UD edge types (NSUBJ, OBJ). Understanding these concepts is essential for implementing the swapping logic.
  - Quick check question: What are the UD labels for subject and object dependencies, and how are they used to identify subtrees?

- Concept: Graph similarity metrics (GED and EM)
  - Why needed here: These metrics are used to filter candidate subtrees. Understanding their computation and interpretation is critical for tuning similarity thresholds.
  - Quick check question: How does GED differ from EM in measuring tree similarity, and what does a similarity score of 0.4 mean in practice?

- Concept: BLEU score and its role in evaluating NMT models
  - Why needed here: The paper measures success by comparing BLEU scores of baseline and augmented models. Knowing how BLEU works helps interpret results.
  - Quick check question: What does a 1.2-point increase in BLEU score indicate about translation quality improvement?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Dependency parsing -> Graph similarity computation -> Subtree extraction and swapping -> NMT training -> BLEU evaluation

- Critical path:
  1. Parse source and target sentences into dependency trees
  2. Compute graph similarities and filter eligible pairs
  3. Extract and swap subtrees
  4. Generate augmented dataset
  5. Train NMT model on mixed original + augmented data
  6. Evaluate BLEU score

- Design tradeoffs:
  - Higher similarity thresholds → cleaner data but fewer samples
  - Lower thresholds → more diversity but risk of noise
  - Swapping only NSUBJ/OBJ → simpler but less diverse augmentation
  - Using more edge types → richer augmentation but harder filtering

- Failure signatures:
  - BLEU scores plateau or drop after augmentation
  - Generated sentences contain ungrammatical or nonsensical phrases
  - Similarity scores cluster near threshold, suggesting poor filtering

- First 3 experiments:
  1. Baseline: Train NMT model on original IWSLT En-De data, measure BLEU.
  2. Simple augmentation: Swap NSUBJ subtrees with GED similarity > 0.4, train and evaluate.
  3. Dual augmentation: Swap both NSUBJ and OBJ subtrees, compare performance with different similarity thresholds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the augmentation method change with different quality levels of dependency parsers across various languages?
- Basis in paper: [explicit] The paper notes that the Vietnamese dependency parser's poor quality led to subpar performance, suggesting that parser quality affects results.
- Why unresolved: The paper does not provide comparative analysis across multiple languages with varying dependency parser qualities.
- What evidence would resolve it: Comparative experiments using multiple languages with known differences in dependency parser quality would show how parser quality impacts augmentation effectiveness.

### Open Question 2
- Question: What is the impact of swapping more complex syntactic substructures beyond subjects and objects on translation quality?
- Basis in paper: [explicit] The paper mentions that applying the method to more complex syntactic substructures is possible but challenging due to translation quality and parser performance dependencies.
- Why unresolved: The paper does not explore or provide results for more complex subtree swaps, leaving the potential benefits and challenges unexplored.
- What evidence would resolve it: Experiments testing the augmentation method with various complex syntactic substructures and measuring the resulting translation quality improvements would provide insights.

### Open Question 3
- Question: How does the choice of graph similarity metric (GED vs. EM) affect the quality and diversity of augmented data?
- Basis in paper: [explicit] The paper uses both GED and EM for filtering but does not compare their effectiveness in detail.
- Why unresolved: While both metrics are applied, the paper does not analyze which metric yields better results in terms of data quality or diversity.
- What evidence would resolve it: A detailed comparison of the outcomes using each metric, focusing on the quality and diversity of the generated augmented data, would clarify their relative effectiveness.

## Limitations
- The method's effectiveness heavily depends on the quality of dependency parsers, particularly for low-resource languages like Vietnamese
- The choice of 0.4 similarity threshold appears somewhat arbitrary without systematic sensitivity analysis
- Results are based on IWSLT datasets and may not generalize to larger-scale or domain-specific translation tasks

## Confidence
**High Confidence Claims**:
- The augmentation method consistently improves BLEU scores for English-German, English-Hebrew, and English-Hungarian language pairs (observed improvements of 1.0-1.8 BLEU points)
- The simultaneous swapping of source and target sentences helps maintain parallelism compared to one-sided augmentation methods
- Graph-based filtering effectively reduces noise in augmented data by removing semantically dissimilar subtree pairs

**Medium Confidence Claims**:
- The specific choice of GED and EM metrics is optimal for measuring subtree similarity across languages
- The 0.4 similarity threshold represents the best tradeoff between augmentation diversity and quality
- The observed improvements are primarily due to semantic preservation rather than increased training data diversity

**Low Confidence Claims**:
- The method will generalize equally well to all language pairs, including distant language pairs not tested
- The improvement patterns observed would hold with different NMT architectures or larger models
- The specific edge types (NSUBJ, OBJ) are the optimal choices for all language pairs

## Next Checks
1. **Parser Robustness Test**: Systematically evaluate how parser quality affects augmentation performance by testing with multiple parsers (e.g., Stanza vs. UDPipe) for each language pair, and quantify the relationship between parser accuracy and BLEU improvements.

2. **Threshold Sensitivity Analysis**: Conduct a comprehensive grid search over similarity thresholds (0.2 to 0.8) for each language pair, measuring both BLEU improvements and augmented sample quality metrics to identify optimal thresholds and their stability across languages.

3. **Cross-Domain Generalization**: Apply the method to a diverse set of translation tasks including news translation, patent translation, and conversational data, measuring whether the improvements observed on IWSLT datasets transfer to these different domains and data scales.