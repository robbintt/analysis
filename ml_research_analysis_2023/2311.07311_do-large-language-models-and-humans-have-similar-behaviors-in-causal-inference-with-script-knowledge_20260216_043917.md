---
ver: rpa2
title: Do large language models and humans have similar behaviors in causal inference
  with script knowledge?
arxiv_id: '2311.07311'
source_url: https://arxiv.org/abs/2311.07311
tags:
- event
- language
- condition
- story
- script
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study investigates how large language models (LLMs) and humans\
  \ process causal relationships in script-based stories. Humans showed significantly\
  \ longer reading times when causal conflicts existed (\xACA \u2192 B) compared to\
  \ logical conditions (A \u2192 B), while reading times remained similar when cause\
  \ A was omitted, indicating humans easily infer event B from script knowledge."
---

# Do large language models and humans have similar behaviors in causal inference with script knowledge?

## Quick Facts
- arXiv ID: 2311.07311
- Source URL: https://arxiv.org/abs/2311.07311
- Authors: 
- Reference count: 27
- Key outcome: Humans show increased reading times for causal conflicts (¬A → B) compared to logical conditions (A → B), while only recent LLMs like GPT-3 and Vicuna correlate with human behavior. All models fail to predict nil → B is less surprising than ¬A → B, indicating difficulties integrating script knowledge.

## Executive Summary
This study investigates how large language models (LLMs) and humans process causal relationships in script-based stories. The research compares human reading times with LLM surprisal values across three conditions: logical (A → B), conflict (¬A → B), and omission (nil → B). Humans showed significantly longer reading times when causal conflicts existed, while reading times remained similar when cause A was omitted, indicating humans easily infer event B from script knowledge. Only recent LLMs like GPT-3 and Vicuna correlated with human behavior in the ¬A → B condition, but all models failed to predict that nil → B is less surprising than ¬A → B, suggesting difficulties in integrating script knowledge for predicting upcoming words.

## Method Summary
The study used two datasets: CSK with 21 short stories about everyday activities (baking, laundry, etc.) containing events A and B with causal dependency, and TRIP with 1472 pairs of stories differing by one causally conflicting sentence. Human participants completed self-paced reading experiments, reading stories chunk-by-chunk while their reading times were recorded. After each story, participants rated certainty (0–7) about events A and B occurring. For LLMs, surprisal was calculated as negative log probability of tokens in segment B across three conditions. Both human reading times and LLM surprisal were analyzed using linear mixed-effects regression models to test for significant differences between conditions.

## Key Results
- Humans showed significantly longer reading times for ¬A → B conditions compared to A → B conditions (p < .05 for all comparisons)
- Reading times remained similar for nil → B conditions, indicating humans infer event B from script knowledge
- Only recent LLMs (GPT-3, Vicuna) correlated with human behavior in detecting causal conflicts
- All models failed to predict nil → B is less surprising than ¬A → B, indicating difficulties integrating script knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human reading times increase when causal conflicts exist (¬A → B) because readers detect the mismatch between prior context and current event.
- Mechanism: Humans activate script knowledge and monitor causal consistency. When B occurs despite ¬A, the activated script predicts A, creating a conflict that slows processing.
- Core assumption: Humans can represent event A or ¬A across intervening sentences and compare it to B.
- Evidence anchors:
  - [abstract] "humans exhibit significantly longer reading times when causal conflicts exist (¬A → B) than under logical conditions (A → B)"
  - [section 3.5] "subjects read chunks with event B significantly more slowly when event A was explicitly negated"
  - [corpus] Weak: corpus shows similar studies but no direct mechanistic evidence
- Break condition: If event A is not represented across intervening sentences, conflict detection fails and reading times don't increase.

### Mechanism 2
- Claim: When cause A is omitted (nil → B), humans infer A from script knowledge and process B without difficulty.
- Mechanism: Script knowledge primes event A automatically when the script is invoked. B is then processed smoothly because A is implicitly present in the activated script.
- Core assumption: Script knowledge is an "indistinguishable part of the memory representation" (Bower et al., 1979).
- Evidence anchors:
  - [abstract] "reading times remain similar when cause A is not explicitly mentioned, indicating that humans can easily infer event B from their script knowledge"
  - [section 3.5] "the absence of event A, which serves as a direct causal link to event B, does not slow event's B processing"
  - [corpus] Moderate: corpus shows script knowledge effects in recall but less direct evidence for online processing
- Break condition: If script knowledge is insufficiently activated or the script is unfamiliar, inference of A fails and nil → B becomes difficult like ¬A → B.

### Mechanism 3
- Claim: Only recent large LLMs (GPT-3, Vicuna) detect causal conflicts because they can represent events across long contexts.
- Mechanism: Large models with sufficient parameters can track event contingencies across intervening sentences, but only those with specific training (like RLHF in GPT-3.5) consistently show conflict detection.
- Core assumption: Model size and training approach affect ability to represent and compare events across distance.
- Evidence anchors:
  - [abstract] "only recent LLMs, like GPT-3 or Vicuna, correlate with human behavior in the ¬A → B condition"
  - [section 4.4] "only some of the largest CLM models showed a reliable difference in surprisal estimates between the coherent and the incoherent (¬A → B) condition"
  - [corpus] Strong: corpus includes multiple model comparisons showing size and training effects
- Break condition: If model lacks capacity to represent events across intervening sentences or hasn't learned causal contingencies, it cannot detect conflicts.

## Foundational Learning

- Concept: Causal inference in discourse comprehension
  - Why needed here: The study tests how both humans and LLMs process causal relationships in script-based stories
  - Quick check question: What distinguishes A → B, ¬A → B, and nil → B conditions in terms of causal structure?

- Concept: Script knowledge as procedural memory
  - Why needed here: Script knowledge allows humans to infer missing events and process script-typical events smoothly
  - Quick check question: How does script knowledge differ from general commonsense knowledge in this study?

- Concept: Surprisal as a measure of processing difficulty
  - Why needed here: The study uses surprisal values from LLMs as an analog to human reading times
  - Quick check question: Why might surprisal from masked LMs be less comparable to human processing than surprisal from causal LMs?

## Architecture Onboarding

- Component map: Human experiment data collection -> LLM surprisal calculation -> Statistical analysis comparison
- Critical path: Generate stories → Run human experiment → Collect reading times → Run LLM predictions → Calculate surprisal → Compare patterns
- Design tradeoffs: Using zero-shot prompting vs. few-shot examples; choosing between CLMs and MLMs for surprisal estimation
- Failure signatures: Models failing to show ¬A → B effect might have context window issues; models showing wrong nil → B effects might over-rely on lexical overlap
- First 3 experiments:
  1. Replicate the human reading time analysis on a subset of stories to validate data collection
  2. Calculate surprisal for one model (e.g., GPT-3) on all conditions to check calculation pipeline
  3. Compare human reading times with model surprisal on a single story to verify expected pattern (A → B < ¬A → B, nil → B ≈ A → B)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanism allows GPT-3.5 to consistently detect causal conflicts while other models fail?
- Basis in paper: [explicit] The paper shows GPT-3.5 (particularly text-davinci-003) performs significantly better than other models in detecting causal conflicts, but doesn't explain why.
- Why unresolved: The paper identifies that GPT-3.5 uses reinforcement learning from human feedback, but doesn't investigate what specific aspect of this training allows it to better handle causal reasoning.
- What evidence would resolve it: Comparative analysis of GPT-3.5's attention patterns, internal representations, or specific training examples that might explain its superior causal reasoning capabilities.

### Open Question 2
- Question: How does script knowledge differ across domains, and would models perform differently on less common scripts?
- Basis in paper: [inferred] The study used everyday scripts (baking, laundry) where participants had high familiarity, but didn't test scripts requiring specialized knowledge.
- Why unresolved: The paper only tested common everyday activities, leaving open whether model performance generalizes to less familiar or more specialized scripts.
- What evidence would resolve it: Experiments using scripts from specialized domains (medical procedures, legal processes) and comparison of model performance across different script familiarity levels.

### Open Question 3
- Question: What specific limitations prevent all models from integrating script knowledge effectively, even when they can detect causal conflicts?
- Basis in paper: [explicit] The paper shows models fail to predict that nil → B is less surprising than ¬A → B, despite being able to detect conflicts.
- Why unresolved: While the paper demonstrates this failure, it doesn't investigate the specific architectural or training limitations that prevent script knowledge integration.
- What evidence would resolve it: Analysis of model internal states when processing nil → B vs. ¬A → B conditions, or ablation studies testing different model components' contributions to script knowledge integration.

### Open Question 4
- Question: Would incorporating explicit script structure or knowledge graphs improve model performance on causal reasoning tasks?
- Basis in paper: [inferred] The paper tests models without additional script knowledge integration, while other studies have shown benefits from incorporating such knowledge.
- Why unresolved: The study focuses on zero-shot capabilities but doesn't explore whether explicit knowledge integration could bridge the gap between model and human performance.
- What evidence would resolve it: Experiments comparing model performance with and without explicit script knowledge integration, or testing models specifically designed to incorporate such knowledge.

### Open Question 5
- Question: How does the length of causal chains affect model performance, and what are the specific limitations?
- Basis in paper: [explicit] The study tested different dependency lengths but found that removing intermediate context didn't improve performance.
- Why unresolved: While the paper shows distance doesn't explain failures, it doesn't investigate what specific aspects of longer causal chains models struggle with.
- What evidence would resolve it: Systematic analysis of model performance across varying chain lengths, identifying specific points where reasoning breaks down.

## Limitations

- Small dataset with only 21 stories limits statistical power and representation of diverse causal structures
- Use of pretrained models without fine-tuning means results reflect general capabilities rather than optimized performance
- Study only examines zero-shot inference, leaving open questions about how fine-tuning or few-shot examples might improve LLM performance

## Confidence

High confidence: The finding that humans show increased reading times for ¬A → B conditions compared to A → B conditions, as this replicates established psycholinguistic effects with appropriate statistical support (p < .05 for all comparisons).

Medium confidence: The claim that only recent LLMs like GPT-3 and Vicuna correlate with human behavior, as this depends on specific model selection and the particular way surprisal is calculated.

Low confidence: The assertion that all models fail to predict nil → B is less surprising than ¬A → B, as this conclusion is based on comparisons across multiple models with varying architectures and training regimes, making it difficult to isolate the specific cause of this failure.

## Next Checks

1. Replicate the human experiment with an expanded dataset (minimum 50 stories) to verify the robustness of the reading time patterns and ensure the original sample size didn't mask subtler effects.

2. Test a subset of models (GPT-3, Vicuna, and one smaller model) with few-shot prompting using explicit causal relationship examples to determine if this improves their ability to detect causal conflicts.

3. Conduct an ablation study removing script-typical content from the nil → B condition to determine whether LLMs are relying on lexical overlap with the script rather than true causal inference.