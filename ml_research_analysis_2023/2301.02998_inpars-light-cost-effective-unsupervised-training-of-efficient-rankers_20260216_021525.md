---
ver: rpa2
title: 'InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers'
arxiv_id: '2301.02998'
source_url: https://arxiv.org/abs/2301.02998
tags:
- u1d44e0
- inpars
- queries
- training
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes InPars-Light, a cost-effective method to improve
  unsupervised training of neural rankers. It modifies InPars to use smaller models
  (30M vs 3B parameters) and a freely available language model (BLOOM).
---

# InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers

## Quick Facts
- arXiv ID: 2301.02998
- Source URL: https://arxiv.org/abs/2301.02998
- Authors: 
- Reference count: 40
- Key outcome: Cost-effective unsupervised training of neural rankers using 30M parameter models that outperform BM25 on five English retrieval collections

## Executive Summary
This paper introduces InPars-Light, a modified version of the InPars method that significantly reduces computational costs while maintaining or improving ranking performance. By using smaller models (30M vs 3B parameters) and open-source language models like BLOOM, InPars-Light achieves 7-30% improvements over BM25 baselines across five English retrieval datasets. The method demonstrates that effective neural rankers can be trained without expensive proprietary models or massive parameter counts, making unsupervised neural ranking truly cost-effective.

## Method Summary
InPars-Light modifies the original InPars approach by using much smaller ranking models (30M MiniLM vs 3B MonoT5) and open-source language models (BLOOM, GPT-J) instead of proprietary ones. The method generates synthetic training queries using few-shot prompting, applies consistency checking to filter spurious queries, and trains rankers through all-domain pretraining followed by in-domain fine-tuning. The approach requires re-ranking only 100 candidate documents instead of 1000, further reducing computational costs while maintaining performance.

## Key Results
- MiniLM-L6-30M consistently outperforms BM25 by 7-30% in nDCG@MRR across five English retrieval collections
- DeBERTA-v3-435M matches the performance of a 7x larger MonoT5 model while using only 30% of the parameters
- Open-source language models (BLOOM, GPT-J) generate queries that are as effective as or better than GPT-3 Curie, at 10x lower cost
- All-domain pretraining improves MiniLM performance but degrades DeBERTA-v3-435M performance in most cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using smaller models (30M vs 3B parameters) with InPars-Light training recipe still achieves comparable or better performance than large models on retrieval tasks.
- Mechanism: The InPars-Light recipe leverages consistency checking and all-domain pretraining to compensate for the smaller model size, allowing smaller models to learn effective ranking functions without requiring massive parameter counts.
- Core assumption: The effectiveness of neural rankers depends more on quality of training data and training procedure than on raw model size, provided the model is sufficiently large to capture the necessary patterns.
- Evidence anchors:
  - [abstract]: "we developed a simple-yet-effective modification of InPars, which we called InPars-light. Unlike InPars, InPars-light uses only a freely available language model BLOOM and 7x-100x smaller ranking models."
  - [section]: "Unlike the original InPars study where a 220M million MonoT5 model fails to outperform BM25 on three out of five datasets, we show that a much smaller MiniLM model with only 30 million parameters [53] can always outperform BM25 by 7-30% in key metrics (nDCG@k or MRR)."
  - [corpus]: Weak evidence - only 25 related papers found, suggesting this specific approach is relatively novel and under-explored in existing literature.

### Mechanism 2
- Claim: Open-source language models (BLOOM, GPT-J) can generate effective synthetic training queries that match or exceed proprietary models (GPT-3 Curie).
- Mechanism: The open-source models, when prompted with few-shot examples, can generate diverse and relevant queries for documents, and these queries when used for training produce strong rankers despite being 10x cheaper to generate.
- Core assumption: Information retrieval capabilities can emerge from large-scale next-token-prediction training on open-source models, not just proprietary ones.
- Evidence anchors:
  - [abstract]: "Not only open-source LLMs BLOOM [44] and GPT-J [50] trained in a fully unsupervised fashion can be prompted to generate effective synthetic queries, but using them leads to consistent improvement compared to GPT-3 Curie model [6]."
  - [section]: "We discover that in a purely unsupervised setting we can replace an impractical three-billion parameter MonoT5-3B model [35] with a 7x smaller bi-directional BERT model while obtaining comparable results."
  - [corpus]: Weak evidence - only 25 related papers found, suggesting limited exploration of open-source model performance in this specific context.

### Mechanism 3
- Claim: Consistency checking significantly improves the quality of synthetic training data by filtering out spurious queries, leading to better model performance.
- Mechanism: By training an initial model on generated queries and then checking if the document that generated a query appears in the top-K results for that query, spurious queries can be identified and removed, resulting in cleaner training data.
- Core assumption: Many synthetic queries generated by LLMs are only tangentially relevant or spurious, and filtering these out through consistency checking leads to more effective training data.
- Evidence anchors:
  - [abstract]: "Dai et al. [9] introduced a different filtering procedure, which was a variant of consistency checking [1]. Dai et al. first trained a retriever model using all the generated queries. Then, for each query they retrieved a set of documents. The query passed the consistency check if the first retrieved document was the document from which the query was generated."
  - [section]: "We can see that for both MiniLM-L6-30M and DeBERTA-v3-435M, fine-tuning on consistency-checked data improves outcomes: For 12 measurements out of 14, these differences are also statistically significant (denoted by super-script label 'b')."
  - [corpus]: Weak evidence - only 25 related papers found, suggesting limited exploration of consistency checking effectiveness in this specific context.

## Foundational Learning

- Concept: Few-shot prompting with large language models
  - Why needed here: This is the core technique used to generate synthetic training queries from documents, which is essential for the unsupervised training approach.
  - Quick check question: How does few-shot prompting differ from zero-shot prompting, and why is it more effective for generating task-specific queries?

- Concept: Cross-encoder vs bi-encoder architectures for ranking
  - Why needed here: Understanding the difference is crucial because the paper uses cross-encoders for re-ranking (which are more accurate but slower) and discusses consistency checking which is naturally applied to cross-encoders.
  - Quick check question: What are the key architectural differences between cross-encoders and bi-encoders, and how do these differences affect their suitability for re-ranking vs retrieval?

- Concept: Knowledge distillation and model compression
  - Why needed here: The paper discusses attempts to distill larger models into smaller ones (MiniLM) and why this failed due to overfitting, which is relevant to understanding model size tradeoffs.
  - Quick check question: Why might knowledge distillation fail when trying to compress a large model trained on synthetic data into a much smaller model?

## Architecture Onboarding

- Component map: Query generation (BLOOM/GPT-J with few-shot prompting) -> Consistency checking (filter spurious queries) -> Training pipeline (all-domain pretraining + in-domain fine-tuning) -> Evaluation module (test on benchmark datasets) -> Inference pipeline (BM25 retrieval + neural re-ranking)

- Critical path: Query generation → Consistency checking → Training → Evaluation
  The most critical sequence is generating high-quality queries, filtering them, and then using them to train effective rankers.

- Design tradeoffs:
  - Model size vs. performance: Smaller models are more efficient but may require better training procedures (consistency checking, all-domain pretraining) to match larger models
  - Query generation cost vs. quality: More expensive generation (GPT-3) vs. cheaper open-source models with potentially lower quality
  - Re-ranking depth: Re-ranking 100 vs 1000 documents affects both performance and efficiency

- Failure signatures:
  - Model fails to outperform BM25: Likely indicates poor quality training data or insufficient model capacity
  - High variance across seeds: May indicate instability in training or sensitivity to initialization
  - Consistency checking removes too many queries: May indicate overly strict filtering or poor initial query quality

- First 3 experiments:
  1. Generate synthetic training queries using BLOOM with few-shot prompting and evaluate query quality manually
  2. Train a small MiniLM model using InPars-generated queries (without consistency checking) and evaluate on a validation set
  3. Apply consistency checking to the generated queries and retrain the model, comparing performance to the previous experiment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does all-domain pre-training affect different model architectures differently?
- Basis in paper: [explicit] The paper found that all-domain pre-training improved MiniLM-L6-30M performance across all datasets, but degraded DeBERTA-v3-435M performance in most cases, with one seed failing to converge properly.
- Why unresolved: The paper only tested two model architectures and one pre-training approach. The reasons for differential impacts are not fully explored.
- What evidence would resolve it: Testing all-domain pre-training across a wider range of model architectures and pre-training approaches, with detailed analysis of why certain models benefit while others degrade.

### Open Question 2
- Question: What is the optimal consistency-checking threshold (k) for different datasets and model sizes?
- Basis in paper: [explicit] The paper tested k=1 and k=3 only on MS MARCO, finding k=3 provided a small boost. No systematic exploration across datasets or model sizes was conducted.
- Why unresolved: The paper used a limited exploration of the consistency-checking parameter space, leaving open the question of optimal settings for different scenarios.
- What evidence would resolve it: Comprehensive grid search of k values (e.g., 1-10) across all datasets and multiple model sizes, with statistical analysis of optimal settings.

### Open Question 3
- Question: Can MiniLM-L6-30M models achieve similar performance to DeBERTA-v3-435M through knowledge distillation instead of all-domain pre-training?
- Basis in paper: [inferred] The paper mentions that distillation was attempted but failed due to overfitting. However, this was not systematically investigated, and the paper opted for all-domain pre-training as a workaround.
- Why unresolved: The paper only attempted one distillation approach and did not explore alternative distillation techniques or hyperparameter settings that might have succeeded.
- What evidence would resolve it: Systematic exploration of distillation techniques (e.g., different teacher-student architectures, distillation objectives, and hyperparameter settings) to determine if MiniLM-L6-30M can match or exceed DeBERTA-v3-435M performance through distillation.

## Limitations
- Results primarily validated on English retrieval collections, with limited testing on non-English or multilingual datasets
- Focuses on re-ranking scenarios rather than end-to-end retrieval performance
- Consistency checking adds computational overhead that may offset some efficiency gains

## Confidence
- High confidence: MiniLM-L6-30M outperforming BM25 on all five datasets (consistent with statistical significance testing)
- Medium confidence: Open-source language models matching GPT-3 Curie performance (limited comparison scope)
- Medium confidence: Consistency checking significantly improving results (demonstrated but optimal parameters not fully explored)

## Next Checks
1. Test InPars-Light performance on non-English retrieval collections to assess generalizability across languages and cultural contexts.
2. Evaluate end-to-end retrieval performance by replacing BM25 with neural retrievers trained using InPars-Light synthetic data, measuring whether the efficiency gains translate to complete retrieval pipelines.
3. Conduct ablation studies on consistency checking parameters, varying the top-K threshold and examining the tradeoff between query quality and computational overhead to identify optimal configurations.