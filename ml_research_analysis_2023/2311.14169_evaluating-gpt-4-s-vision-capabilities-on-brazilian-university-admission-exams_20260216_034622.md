---
ver: rpa2
title: Evaluating GPT-4's Vision Capabilities on Brazilian University Admission Exams
arxiv_id: '2311.14169'
source_url: https://arxiv.org/abs/2311.14169
tags:
- language
- enem
- images
- questions
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work evaluates the capabilities of OpenAI's large language
  models, including GPT-4 with and without vision, on the Brazilian university admission
  exam (ENEM). The study incorporates image understanding to address the diverse challenges
  posed by the exam, which requires deep comprehension across multiple domains and
  the ability to synthesize information from varied sources.
---

# Evaluating GPT-4's Vision Capabilities on Brazilian University Admission Exams

## Quick Facts
- arXiv ID: 2311.14169
- Source URL: https://arxiv.org/abs/2311.14169
- Reference count: 26
- Primary result: GPT-4 achieves 90.50% accuracy on ENEM 2023 when using text captions instead of images

## Executive Summary
This work evaluates OpenAI's large language models, including GPT-4 with and without vision, on the Brazilian university admission exam (ENEM). The study incorporates image understanding to address the diverse challenges posed by the exam, which requires deep comprehension across multiple domains and the ability to synthesize information from varied sources. Results show that GPT-4 performs well in handling complex multidisciplinary questions using step-by-step reasoning when applied in realistic settings with all necessary visual and textual components. However, the models exhibit relatively more difficulty in mathematics compared to other subjects. Notably, text captions transcribing visual content outperform the direct use of images, suggesting that the vision model has room for improvement.

## Method Summary
The study evaluates multiple OpenAI models (GPT-3.5 Turbo, GPT-4, GPT-4 Turbo, and GPT-4 Turbo with Vision) on ENEM 2022 and 2023 datasets using few-shot learning with Chain-of-Thought explanations. The methodology involves extracting text, tables, and images from ENEM PDF files, creating multimodal datasets with human-generated captions, and evaluating model performance across different input modalities (without images, with images, and with captions). The experiments use 3-shot CoT prompting to guide complex reasoning across multidisciplinary questions.

## Key Results
- GPT-4 achieves 90.50% accuracy on ENEM 2023 when using text captions instead of images
- Text captions outperform direct image inputs, with captions achieving 75.00% accuracy in mathematics versus 65.91% with images
- Mathematics remains the most challenging subject for all models, even with multimodal enhancements
- GPT-4 shows consistent performance across subjects when using captions, with 96.59% in languages, 85.00% in mathematics, 88.89% in natural sciences, and 95.45% in human sciences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's superior performance on ENEM exams is primarily driven by its ability to perform step-by-step reasoning when decomposing complex, multidisciplinary questions into smaller parts.
- Mechanism: The model applies Chain-of-Thought (CoT) prompting, which guides it to break down complex reasoning into intermediate steps before arriving at a final answer. This decomposition allows the model to handle the integration of multiple knowledge domains that ENEM questions often require.
- Core assumption: The reasoning process can be effectively guided through explicit intermediate steps that mimic human problem-solving approaches.
- Evidence anchors:
  - [abstract] "Results show that GPT-4 performs well in handling complex multidisciplinary questions using step-by-step reasoning when applied in realistic settings with all necessary visual and textual components."
  - [section] "Our study shows that language models can have impressive performance in solving multidisciplinary questions using step-by-step reasoning when applied in realistic settings with all the necessary visual and textual components."
- Break condition: If the question structure is too complex for the model to effectively decompose into sequential steps, or if the reasoning chain becomes too long and exceeds context limitations.

### Mechanism 2
- Claim: Text captions outperform direct image inputs for visual comprehension tasks in ENEM exams, suggesting current vision models have encoding limitations.
- Mechanism: When captions are provided instead of raw images, the model demonstrates higher accuracy, particularly in the ENEM 2023 exam where captions achieved 75.00% accuracy in mathematics versus 65.91% with images. This suggests the model can better process structured textual descriptions than raw visual data.
- Core assumption: The vision model's image encoding capability is currently less effective than human-generated textual descriptions for conveying necessary visual information.
- Evidence anchors:
  - [abstract] "One of the highlights is that text captions transcribing visual content outperform the direct use of images, suggesting that the vision model has room for improvement."
  - [section] "Significant improvements are noticeable when incorporating either textual or visual representations of images, with the difference nearing 10 points, particularly when utilizing captions."
- Break condition: If caption quality degrades or if the visual information is too complex to be accurately transcribed into text, the advantage of captions over images would diminish.

### Mechanism 3
- Claim: Language models show consistent subject-specific performance patterns, with mathematics remaining more challenging than other domains despite multimodal enhancements.
- Mechanism: The model's performance varies significantly across subjects, with mathematics consistently scoring lower than languages, natural sciences, and human sciences. This pattern persists even with multimodal input, suggesting inherent difficulty in mathematical reasoning rather than just visual comprehension challenges.
- Core assumption: Mathematical reasoning requires different cognitive processes than other domains, and current models have not yet achieved parity in mathematical problem-solving compared to other subjects.
- Evidence anchors:
  - [abstract] "Despite improvements afforded by images or captions, mathematical questions remain a challenge for these state-of-the-art models."
  - [section] "However, the LMs exhibit relatively more difficulty in mathematics compared to other subjects."
- Break condition: If future model iterations develop specialized mathematical reasoning capabilities or if the mathematical questions become less complex, this pattern could change.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) Prompting
  - Why needed here: ENEM questions require complex reasoning across multiple domains, and CoT helps the model decompose these complex problems into manageable steps.
  - Quick check question: What is the primary benefit of using 3-shot CoT prompting for complex multiple-choice questions?

- Concept: Multimodal vs. Text-only Processing
  - Why needed here: The study compares performance with images, captions, and without visual elements to understand the relative effectiveness of different input modalities.
  - Quick check question: Why might human-generated captions outperform direct image inputs for vision models?

- Concept: Subject-specific Performance Analysis
  - Why needed here: Understanding that models perform differently across subjects (mathematics vs. others) helps identify specific areas for improvement and guides future model development.
  - Quick check question: Which subject area showed the most persistent difficulty for models even with multimodal enhancements?

## Architecture Onboarding

- Component map: PDF extraction -> data parsing -> dataset creation -> model prompting (3-shot CoT) -> evaluation -> result analysis
- Critical path: PDF extraction → data parsing → dataset creation → model prompting (3-shot CoT) → evaluation → result analysis
- Design tradeoffs: Using human-generated captions provides better performance but requires manual effort; direct image processing is more automated but less effective currently; excluding questions requiring visual comprehension would simplify evaluation but would not reflect realistic exam conditions
- Failure signatures: Poor performance in mathematics despite multimodal input suggests reasoning limitations rather than just visual comprehension issues; GPT-3.5 Turbo performance decline with captions indicates model-specific limitations; significant performance differences between ENEM 2022 and 2023 suggest potential data contamination or difficulty variation
- First 3 experiments:
  1. Without images: Establish baseline performance by excluding all visual elements from questions
  2. With images: Evaluate multimodal models using direct image inputs to test vision capabilities
  3. With captions: Replace images with human-generated textual descriptions to compare effectiveness against raw images

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does GPT-4 Turbo with Vision perform worse than text-based GPT-4 on the ENEM 2023 exam when using captions?
- Basis in paper: [explicit] The paper states that "even though the GPT-4 Turbo was trained with more recent data (April 2023), it exhibited the smallest discrepancy in accuracy between ENEM 2023 (never seen during training) and ENEM 2022 (potentially seen during training). This observation challenges the contamination hypothesis, given that the model’s performance was inferior to that of GPT-4."
- Why unresolved: The paper does not provide a clear explanation for why the multimodal GPT-4 Turbo with Vision performs worse than the text-based GPT-4 when using captions, despite having more recent training data.
- What evidence would resolve it: Additional experiments comparing the performance of different models on other exams or tasks, or a detailed analysis of the training data and model architecture, could help explain this discrepancy.

### Open Question 2
- Question: How does the performance of GPT-4 on the ENEM 2023 exam compare to the average performance of human students?
- Basis in paper: [inferred] The paper does not provide any data on the performance of human students on the ENEM 2023 exam, but it does state that GPT-4 achieved an accuracy of 90.50% on the exam when equipped with captions.
- Why unresolved: The paper does not provide any data on the performance of human students on the ENEM 2023 exam, so it is impossible to compare the performance of GPT-4 to that of human students.
- What evidence would resolve it: Data on the performance of human students on the ENEM 2023 exam would be needed to compare the performance of GPT-4 to that of human students.

### Open Question 3
- Question: What is the impact of using different types of captions (e.g., human-generated vs. machine-generated) on the performance of language models on the ENEM exam?
- Basis in paper: [explicit] The paper states that "our datasets incorporate table contents in Markdown formatting. This standardized structure is easily understandable by the language models, and because of this, we ignore the reader descriptions related to tables." However, the paper does not explore the impact of using different types of captions on the performance of language models.
- Why unresolved: The paper does not explore the impact of using different types of captions on the performance of language models, so it is unclear whether the use of human-generated captions is optimal.
- What evidence would resolve it: Experiments comparing the performance of language models using different types of captions (e.g., human-generated vs. machine-generated) would be needed to determine the impact of caption type on performance.

## Limitations

- The manual caption generation process introduces potential subjectivity and limits scalability of the approach
- The large performance gap between ENEM 2022 and 2023 (15.18 percentage points) raises concerns about data contamination or difficulty variation between years
- The study doesn't explore automated image description methods or alternative captioning strategies that could provide more scalable solutions

## Confidence

- **High Confidence**: GPT-4's superior performance over GPT-3.5 Turbo and the consistent subject-specific patterns (mathematics being more challenging)
- **Medium Confidence**: The advantage of captions over images, given the manual nature of caption generation and potential for bias
- **Medium Confidence**: The effectiveness of step-by-step reasoning through CoT prompting, as this depends on prompt engineering quality

## Next Checks

1. Replicate the experiment with automated image captioning using multiple models to compare against human-generated captions and assess whether the advantage is due to caption quality or the textual representation format itself.

2. Test model performance on additional ENEM years to determine whether the large performance gap between 2022 and 2023 is consistent or an anomaly, and investigate potential data contamination.

3. Evaluate alternative prompting strategies for mathematical reasoning (such as specialized math-focused prompts or external tools) to determine if the mathematics difficulty is fundamental to the models or can be mitigated through different approaches.