---
ver: rpa2
title: Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution
arxiv_id: '2307.00925'
source_url: https://arxiv.org/abs/2307.00925
tags:
- similarity
- semantic
- ensemble
- ensembles
- measures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an automated method for designing semantic
  similarity ensembles using grammatical evolution (GE). The approach evolves aggregation
  functions that combine multiple similarity measures to maximize correlation with
  human-labeled scores.
---

# Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution

## Quick Facts
- arXiv ID: 2307.00925
- Source URL: https://arxiv.org/abs/2307.00925
- Reference count: 40
- Key outcome: Grammatical evolution evolves aggregation functions that combine multiple similarity measures to maximize correlation with human-labeled scores, showing superior performance over existing ensemble techniques.

## Executive Summary
This paper introduces a method for automatically designing semantic similarity ensembles using grammatical evolution (GE). The approach evolves aggregation functions that combine multiple semantic similarity measures to maximize correlation with human-labeled similarity scores. Experiments on benchmark datasets demonstrate that GE can discover ensemble structures that outperform existing ensemble techniques, while providing flexibility in evolving diverse ensemble structures. The method shows promise for automated semantic similarity measurement system design, though interpretability of evolved solutions can be challenging.

## Method Summary
The method uses grammatical evolution to evolve aggregation functions that combine multiple semantic similarity measures. A Backus-Naur Form (BNF) grammar defines valid ensemble structures, which are generated and evolved through a genotype-to-phenotype mapping. Candidate solutions are evaluated using a fitness function based on correlation (PCC or SRCC) with human-curated ground truth datasets. The process involves crossover, mutation, and selection over generations to evolve optimal ensemble structures. The approach is tested on benchmark datasets (MC30 and GeReSiD50) using BERT-based similarity measures.

## Key Results
- GE-evolved ensembles achieve superior correlation with human judgments compared to linear regression, tree-based GP, linear GP, and Cartesian GP baselines
- The approach successfully combines BERT-based similarity measures (cosine, Manhattan, Euclidean, inner product, angular distances) into effective ensembles
- GE-i variant provides interpretable ensembles with reasonable performance, though slightly worse than fully optimized versions
- Experiments demonstrate the method's effectiveness on both word-level (MC30) and sentence-level (GeReSiD50) semantic similarity tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Grammatical evolution (GE) evolves aggregation functions that combine multiple semantic similarity measures to maximize correlation with human-labeled similarity scores.
- **Mechanism:** GE uses a genotype-to-phenotype mapping defined by a formal grammar (BNF) to generate and evolve programs (ensembles) that aggregate similarity measures. Each candidate solution is evaluated using a fitness function based on correlation (PCC or SRCC) with a human-curated ground truth dataset.
- **Core assumption:** The fitness function based on correlation with human judgment effectively guides the evolution toward high-quality ensembles.
- **Evidence anchors:**
  - [abstract] "The method evolves aggregation functions that maximize correlation with human-labeled similarity scores."
  - [section] "We aim to optimize the correlation between the ensemble results and a human-curated ground truth dataset."
  - [corpus] Weak - related papers focus on ensemble methods but not specifically on GE for semantic similarity.

### Mechanism 2
- **Claim:** Using GE allows for more flexible and efficient evolution of ensemble structures compared to traditional genetic programming approaches.
- **Mechanism:** GE operates on integer strings (genotypes) that are mapped to program structures (phenotypes) via grammar rules, enabling controlled evolution and generation of executable code. This approach is more efficient than tree-based GP because it directly generates code rather than manipulating syntax trees.
- **Core assumption:** The genotype-to-phenotype mapping via grammar rules effectively captures the space of valid ensemble structures and enables efficient search.
- **Evidence anchors:**
  - [section] "GE is a particular form of genetic programming (GP) that uses a formal grammar (FG) to generate computer programs... GE uses an FG definition to describe the language that the model might produce."
  - [section] "This approach is advantageous in domains where the capability of understanding the solution is essential."
  - [corpus] Missing - no direct comparison with GP in the corpus.

### Mechanism 3
- **Claim:** The ensemble approach compensates for the limitations of individual semantic similarity measures by leveraging their diversity.
- **Mechanism:** By combining multiple similarity measures (e.g., different BERT-based variants using cosine, Manhattan, Euclidean, inner product, and angular distances), the ensemble captures different aspects of semantic similarity and mitigates individual measure biases.
- **Core assumption:** The individual similarity measures are sufficiently diverse and complementary that their combination improves overall performance.
- **Evidence anchors:**
  - [section] "The motivation behind this approach comes from the idea that a diversified pool of semantic similarity measures can compensate for the inherent limitations of individual measures."
  - [section] "Through the aggregation of multiple measures, our proposed approach seeks to leverage the diversity of these measures to achieve a higher level of agreement and consistency."
  - [corpus] Weak - related papers mention ensemble methods but don't provide specific evidence for semantic similarity measure diversity.

## Foundational Learning

- **Concept:** Grammatical Evolution and Formal Grammars
  - **Why needed here:** Understanding how GE uses formal grammars (BNF) to define valid program structures and guide the evolution process.
  - **Quick check question:** What is the role of the Backus-Naur Form (BNF) grammar in Grammatical Evolution, and how does it differ from traditional genetic programming?

- **Concept:** Semantic Similarity Measures and Evaluation Metrics
  - **Why needed here:** Familiarity with different semantic similarity measures (e.g., BERT-based variants) and correlation metrics (PCC, SRCC) used to evaluate ensemble performance.
  - **Quick check question:** How do Pearson Correlation Coefficient (PCC) and Spearman Rank Correlation Coefficient (SRCC) differ in evaluating semantic similarity ensemble performance?

- **Concept:** Ensemble Learning and Genetic Algorithms
  - **Why needed here:** Understanding the principles of ensemble learning and how genetic algorithms can be used to evolve ensemble structures.
  - **Quick check question:** What are the key differences between Grammatical Evolution and traditional Genetic Programming, and why might GE be more suitable for evolving semantic similarity ensembles?

## Architecture Onboarding

- **Component map:** BNF Grammar -> Population of candidate ensembles -> Fitness function (correlation metrics) -> Genetic operators (crossover, mutation) -> Benchmark datasets (MC30, GeReSiD50) -> BERT-based similarity measures

- **Critical path:**
  1. Define BNF grammar for valid ensemble structures
  2. Initialize population of candidate ensembles
  3. Evaluate fitness of each ensemble using correlation with ground truth
  4. Apply genetic operators to evolve population
  5. Repeat until termination condition (max generations or satisfactory fitness)
  6. Select best ensemble and evaluate on unseen data

- **Design tradeoffs:**
  - Grammar expressiveness vs. search space complexity
  - Ensemble size (number of measures) vs. computational efficiency
  - Correlation metric choice (PCC vs. SRCC) vs. evaluation criteria
  - Interpretability of evolved ensembles vs. performance optimization

- **Failure signatures:**
  - Poor convergence or stagnation in fitness values
  - Ensembles that overfit to training data but perform poorly on unseen data
  - Generated code that is invalid or non-executable
  - No significant improvement over individual measures or simple averaging

- **First 3 experiments:**
  1. Run GE with default parameters on MC30 dataset using PCC as fitness function, compare results with individual BERT measures and linear regression baseline.
  2. Vary the grammar rules to allow more complex ensemble structures, evaluate impact on performance and interpretability.
  3. Experiment with different correlation metrics (PCC vs. SRCC) as fitness functions, analyze which metric leads to better generalization on unseen data.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the performance of GE-based semantic similarity ensembles compare to transformer-based approaches like BERT on domain-specific datasets?
  - **Basis in paper:** [explicit] The paper focuses on BERT variants as similarity measures but does not directly compare GE ensembles against pure transformer models
  - **Why unresolved:** The experiments only compare GE ensembles against other ensemble methods (LR, TGP, LGP, CGP) using BERT variants as inputs, not against standalone transformer models
  - **What evidence would resolve it:** Direct comparison experiments of GE ensembles versus transformer-only approaches on the same domain-specific datasets

- **Open Question 2:** What is the optimal balance between interpretability and performance in GE-based semantic similarity ensembles for real-world applications?
  - **Basis in paper:** [explicit] The paper mentions interpretability as an advantage of GE and presents GE-i as an interpretable variant, but notes it achieves worse performance
  - **Why unresolved:** The paper doesn't empirically determine where the trade-off point should be for practical applications or investigate hybrid approaches
  - **What evidence would resolve it:** User studies or application-specific performance benchmarks comparing different interpretability-performance trade-offs

- **Open Question 3:** How do different genetic operators (beyond crossover and mutation) affect the quality and diversity of evolved semantic similarity ensembles?
  - **Basis in paper:** [explicit] The paper mentions crossover and mutation as commonly used operators but notes that "the specific crossover mechanism can vary" without exploring alternatives
  - **Why unresolved:** The experiments only use basic genetic operators without investigating how other operators (selection methods, replacement strategies, etc.) impact results
  - **What evidence would resolve it:** Systematic experiments comparing various genetic operator combinations and their effects on ensemble quality metrics

## Limitations

- Small dataset sizes (30 word pairs and 50 sentence pairs) may not generalize to broader applications
- Limited ablation studies on grammar design choices and their impact on performance
- No comparison with alternative ensemble learning methods beyond linear regression
- Potential overfitting concerns given the small training sets and lack of cross-validation

## Confidence

- **High Confidence:** The basic GE framework for evolving ensemble structures is well-established and technically sound.
- **Medium Confidence:** Performance improvements over baselines are demonstrated but limited to two small benchmark datasets (MC30 with 30 pairs, GeReSiD50 with 50 pairs).
- **Low Confidence:** Claims about interpretability benefits and general applicability beyond the tested domains lack sufficient empirical support.

## Next Checks

1. **External validation:** Test the evolved ensembles on additional semantic similarity benchmarks (e.g., WordSim-353, SimLex-999) to assess generalizability
2. **Grammar sensitivity analysis:** Systematically vary BNF grammar rules to quantify their impact on ensemble performance and interpretability
3. **Baseline expansion:** Compare against more sophisticated ensemble methods (e.g., stacking, boosting) to better contextualize the GE approach's advantages