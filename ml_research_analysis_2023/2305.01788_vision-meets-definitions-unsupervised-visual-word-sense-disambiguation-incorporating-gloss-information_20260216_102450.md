---
ver: rpa2
title: 'Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating
  Gloss Information'
arxiv_id: '2305.01788'
source_url: https://arxiv.org/abs/2305.01788
tags:
- sense
- word
- nitions
- nition
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new unsupervised Visual Word Sense Disambiguation
  (VWSD) approach incorporating gloss information from lexical knowledge bases. The
  method uses Bayesian inference to integrate sense definitions with pretrained image-text
  matching models, significantly improving disambiguation accuracy.
---

# Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information

## Quick Facts
- arXiv ID: 2305.01788
- Source URL: https://arxiv.org/abs/2305.01788
- Reference count: 24
- Key outcome: Context-aware definition generation with GPT-3 improves OOD word handling in VWSD by over 8 percentage points

## Executive Summary
This paper introduces a novel unsupervised approach to Visual Word Sense Disambiguation (VWSD) that incorporates gloss information from lexical knowledge bases. The method employs Bayesian inference to combine sense definitions with pretrained image-text matching models, significantly improving disambiguation accuracy. For out-of-dictionary words, the authors propose a context-aware definition generation approach using GPT-3 that conditions definition generation on both the target word and its context. Experiments on the SemEval-2023 dataset demonstrate that this approach increases VWSD performance by over 8 percentage points compared to baseline models, with the context-aware generation showing particular effectiveness for OOD cases.

## Method Summary
The approach uses Bayesian inference to incorporate sense definitions when sense information is not provided. For each candidate image, the model computes P(v|c,t) by marginalizing over sense definitions: P(v|c,t) = Σ P(v|Dt_i,c,t)P(Dt_i|c,t). The first term is computed via image-text matching (using pretrained models like CLIP or FLA V A), while the second is computed via context-definition matching. For out-of-dictionary words, the method generates definitions using GPT-3 with context-aware prompts that include both the target word and its surrounding context. The approach doesn't require additional training - it directly uses pretrained image-text matching models with the Bayesian framework.

## Key Results
- The proposed method increases VWSD performance by over 8 percentage points compared to baseline models
- Context-aware definition generation with GPT-3 outperforms context-agnostic generation for OOD words
- The approach achieves significant improvements on the SemEval-2023 dataset using both CLIP and FLA V A models
- Student's t-test confirms statistical significance of the improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bayesian inference with latent sense definitions improves disambiguation by combining ITM outputs with gloss probabilities
- Mechanism: The model marginalizes over sense definitions using P(v|c,t) = Σ P(v|Dt_i,c,t)P(Dt_i|c,t), where the first term is computed via image-text matching and the second via context-definition matching
- Core assumption: The joint probability of an image given a definition and context is independent of other definitions, and the definitions are conditionally independent given context and target word
- Evidence anchors:
  - [abstract] "we suggest employing Bayesian inference to incorporate the sense definitions when sense information of the answer is not provided"
  - [section 4] "Suppose Dt is a set of definitions... by using the chain rule, the posterior can be divided into two conditional probabilities associated with a latent variable Dt"
  - [corpus] Weak corpus support; no direct neighbor papers explicitly describe this Bayesian marginalization formulation
- Break condition: If definitions are not mutually exclusive or if the conditional independence assumption is violated, the Bayesian combination could introduce noise rather than signal

### Mechanism 2
- Claim: Context-aware definition generation improves OOD word handling by conditioning definition generation on both target word and context
- Mechanism: Instead of generating definitions independently of context (e.g., "Define 'angora'"), the prompt includes context ("Define 'angora' in 'angora city'"), leading to more contextually accurate definitions
- Core assumption: GPT-3 can generate definitions that are sensitive to the intended sense when given context-aware prompts, and these definitions remain useful for downstream VWSD
- Evidence anchors:
  - [abstract] "we propose a context-aware definition generation with GPT-3" and "our context-aware definition generation achieved prominent performance improvement in OOD examples"
  - [section 5] "we suggest generating a definition with the prompt that considers both the context and the target word together"
  - [corpus] Weak; corpus mentions definition generation but does not compare context-aware vs. context-agnostic generation
- Break condition: If GPT-3's generation is dominated by context-independent priors or if the context is ambiguous, the generated definition may not align with the correct sense

### Mechanism 3
- Claim: Using gloss definitions mitigates over-reliance on image features by enriching the text representation with semantic disambiguation
- Mechanism: The text encoder is fed not just the context but also the sense definition, so the matching score P(v|Dt_i,c,t) is computed between images and definition-enriched context rather than raw context alone
- Core assumption: The ITM model's text encoder can effectively combine context and definition into a unified representation that improves matching with images
- Evidence anchors:
  - [abstract] "we suggest employing Bayesian inference to incorporate the sense definitions when sense information of the answer is not provided"
  - [section 4] "we utilize the sense definitions... that have been widely exploited in previous lexical semantic tasks"
  - [corpus] No direct support; corpus neighbors focus on VWSD but not on the specific role of definition-enriched text encoding
- Break condition: If the text encoder cannot properly fuse context and definition, or if definitions are noisy, the benefit could be negated

## Foundational Learning

- Concept: Bayesian inference and marginalization over latent variables
  - Why needed here: The model needs to reason over multiple possible senses without knowing the correct one; Bayesian marginalization provides a principled way to combine evidence across senses
  - Quick check question: If you have two possible senses with probabilities 0.7 and 0.3, and their respective image likelihoods are 0.8 and 0.2, what is the marginal probability of the correct image?

- Concept: Zero-shot image-text matching and ITM models
  - Why needed here: The approach builds on pretrained ITM models (CLIP, FLA V A) without fine-tuning, so understanding how they compute P(v|c) is essential
  - Quick check question: In CLIP, how is the probability P(v|c) typically computed from image and text embeddings?

- Concept: Definition generation and controllable text generation
  - Why needed here: The method relies on GPT-3 to generate sense definitions for OOD words; understanding how prompts influence generation is critical
  - Quick check question: How might the prompt "Define X in Y" differ in output from "Define X" when X is polysemous?

## Architecture Onboarding

- Component map:
  - Input: Context + target word, candidate images, WordNet definitions (or GPT-3 generated)
  - Text encoder: Encodes context + each definition (for C2D) and context + target + definition (for D2I)
  - Image encoder: Encodes each candidate image
  - Bayesian layer: Computes P(Dt_i|c,t) and P(v|Dt_i,c,t), then marginalizes
  - Output: Selected image with highest P(v|c,t)

- Critical path:
  1. Retrieve or generate definitions for target word
  2. Compute C2D probabilities (context to definition)
  3. Compute D2I probabilities (definition+context to image)
  4. Combine via Bayesian marginalization
  5. Select argmax image

- Design tradeoffs:
  - Using external definitions adds dependency on WordNet quality and GPT-3 availability; removing them would revert to baseline ITM
  - Generating definitions on the fly introduces latency and potential noise; caching could help but risks staleness
  - Bayesian marginalization increases computation (O(|Dt|) vs O(1)) but improves accuracy for ambiguous words

- Failure signatures:
  - Overconfident wrong sense selection: C2D probability peaks on wrong definition → cascade error
  - OOD word with poor generation: GPT-3 outputs unrelated definition → downstream matching fails
  - Ambiguous word with near-uniform definitions: Marginalization yields no signal → reverts to ITM baseline

- First 3 experiments:
  1. Compare P(v|c) from baseline CLIP vs. with WordNet definitions on a small ambiguous word set; measure accuracy gain
  2. Test context-aware vs. context-agnostic GPT-3 definition generation on OOD examples; evaluate semantic match to WordNet
  3. Vary the number of definitions (|Dt|) and measure impact on Hits@1; check for diminishing returns

## Open Questions the Paper Calls Out
None explicitly identified in the paper.

## Limitations
- The conditional independence assumptions in the Bayesian formulation may not hold for all word-sense-image relationships, potentially limiting the approach's robustness to highly polysemous words or ambiguous contexts
- The quality of GPT-3 generated definitions for OOD words remains an open question, as the paper doesn't provide systematic evaluation of definition quality independent of downstream task performance
- The approach's scalability to languages other than English is unclear, given its reliance on WordNet and English-language GPT-3

## Confidence
- **High confidence**: The Bayesian inference framework for incorporating sense definitions (Mechanism 1) - well-grounded in established probabilistic modeling and clearly implemented
- **Medium confidence**: The context-aware definition generation approach (Mechanism 2) - theoretically sound but dependent on GPT-3's generation quality which varies
- **Low confidence**: The claim that gloss definitions mitigate over-reliance on image features (Mechanism 3) - while plausible, lacks direct empirical support in the paper

## Next Checks
1. **Independence assumption validation**: Systematically test the conditional independence assumption by measuring correlation between P(v|Dt_i,c,t) and P(v|Dt_j,c,t) for different senses i,j. If correlations are high, the Bayesian formulation may be overconfident

2. **Definition quality ablation**: Evaluate the semantic accuracy of GPT-3 generated definitions independently of VWSD performance by comparing them to WordNet definitions using embedding similarity or human judgment

3. **Context sensitivity test**: Create controlled experiments with ambiguous contexts where the correct sense is clearly determined by context, and measure whether the context-aware generation actually produces definitions aligned with the correct sense more often than context-agnostic generation