---
ver: rpa2
title: Robustness Tests for Automatic Machine Translation Metrics with Adversarial
  Attacks
arxiv_id: '2311.00508'
source_url: https://arxiv.org/abs/2311.00508
tags:
- perturbed
- original
- metrics
- bertscore
- bleurt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the robustness of automatic machine translation
  evaluation metrics under adversarial attacks. The authors apply word- and character-level
  adversarial attacks (CLARE, Faster Genetic Algorithm, Input Reduction, and DeepWordBug)
  to three popular MT metrics: BERTScore, BLEURT, and COMET.'
---

# Robustness Tests for Automatic Machine Translation Metrics with Adversarial Attacks

## Quick Facts
- **arXiv ID:** 2311.00508
- **Source URL:** https://arxiv.org/abs/2311.00508
- **Reference count:** 23
- **Primary result:** Automatic MT metrics (BERTScore, BLEURT, COMET) are vulnerable to adversarial attacks, showing overpenalization and self-inconsistency

## Executive Summary
This paper investigates the robustness of automatic machine translation evaluation metrics under adversarial attacks. The authors apply word- and character-level adversarial attacks (CLARE, Faster Genetic Algorithm, Input Reduction, and DeepWordBug) to three popular MT metrics: BERTScore, BLEURT, and COMET. They find that mask-filling and word substitution attacks are particularly effective at degrading metric performance. The experiments show that BERTScore, BLEURT, and COMET tend to overpenalize adversarially-degraded translations, with BLEURT and COMET being especially vulnerable to perturbations on high-quality translations outside their training data. The authors also identify self-inconsistencies in BERTScore, where it judges the original and perturbed translations as similar but the perturbed translation as notably worse with respect to the reference. These findings motivate the development of more robust MT metrics and suggest using adversarial examples in metric training.

## Method Summary
The authors apply four adversarial attack methods (CLARE, Faster Genetic Algorithm, Input Reduction, and DeepWordBug) to generate perturbed translations of German-to-English system outputs from WMT 12, 17, and 22. They then evaluate these original and perturbed translations using BERTScore, BLEURT, and COMET metrics. Human evaluations are collected for a subset of translations to compare metric degradation with human-perceived degradation. The study analyzes overpenalization (metrics showing larger degradation than humans) and self-inconsistency (BERTScore judging original and perturbed as similar while judging perturbed as worse relative to reference).

## Key Results
- BERTScore, BLEURT, and COMET overpenalize adversarially-degraded translations, with degradation scores significantly larger than human perception
- BLEURT and COMET are especially vulnerable to perturbations on high-quality translations outside their training data distribution
- BERTScore exhibits self-inconsistency by judging original and perturbed translations as similar while judging the perturbed as notably worse relative to the reference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BERTScore, BLEURT, and COMET overpenalize adversarially-degraded translations
- Mechanism: The metrics use semantic similarity embeddings that are sensitive to minor perturbations. Adversarial attacks exploit this by introducing small but semantically disruptive changes that cause large drops in metric scores.
- Core assumption: Small perturbations that preserve fluency but degrade semantic quality are judged by humans as less severe than the metrics indicate.
- Evidence anchors:
  - [abstract] "Our human experiments validate that automatic metrics tend to overpenalize adversarially-degraded translations."
  - [section 3.1] "We observe that, in most cases, the metrics assign higher differences between the original and perturbed translations... the degradation as measured by the metrics is significantly larger than that as measured by humans."

### Mechanism 2
- Claim: BLEURT and COMET are more susceptible to perturbations on high-quality translations outside their training data
- Mechanism: BLEURT and COMET are trained on system outputs from previous years (lower quality). When applied to higher-quality translations from WMT 22, small perturbations cause larger relative drops in scores because the models are not calibrated for this distribution.
- Core assumption: The metrics' learned regression functions are not robust to out-of-distribution inputs that are of higher quality than their training data.
- Evidence anchors:
  - [abstract] "BLEURT and COMET are especially vulnerable to perturbations on high-quality translations outside their training data."
  - [section 3.1] "Both CLARE and the Faster Alzantot Genetic Algorithm lead to degradations of over 0.2 in Pearson correlations for BLEURT and over 0.4 for COMET... BLEURT and COMET are trained on data from previous years and cannot easily generalize to higher-quality translations."

### Mechanism 3
- Claim: BERTScore exhibits self-inconsistency - judging original and perturbed translations as similar while judging the perturbed as worse with respect to the reference
- Mechanism: BERTScore measures semantic similarity between any two sentences. When the perturbed translation is semantically close to the original (high BERTScore(perturbed, original)), but semantically far from the reference (low BERTScore(perturbed, reference)), this creates an inconsistency if the original was a good translation.
- Core assumption: The perturbed translation maintains semantic similarity to the original but loses alignment with the reference, and BERTScore's symmetric nature allows this inconsistency to be detected.
- Evidence anchors:
  - [abstract] "We also identify inconsistencies in BERTScore ratings, where it judges the original sentence and the adversarially-degraded one as similar, while judging the degraded translation as notably worse than the original with respect to the reference translation."
  - [section 2.1] "When the original and perturbed translations are measured as similar, the robust behavior would be for them to have similar ratings with regard to the reference."

## Foundational Learning

- Concept: Adversarial attacks in NLP
  - Why needed here: Understanding how word- and character-level attacks work is essential to grasp why the metrics fail and how to defend against such attacks.
  - Quick check question: What is the difference between white-box and black-box adversarial attacks, and which type is used in this paper?

- Concept: Semantic similarity metrics and their limitations
  - Why needed here: The paper relies on understanding how BERTScore, BLEURT, and COMET measure translation quality and why they might be vulnerable to perturbations.
  - Quick check question: How does BERTScore compute similarity between sentences, and why might this make it vulnerable to adversarial attacks?

- Concept: Machine translation evaluation metrics
  - Why needed here: Understanding the role and limitations of automatic metrics like BLEU, BERTScore, BLEURT, and COMET is crucial for appreciating the paper's findings.
  - Quick check question: What are the key differences between reference-based metrics (like BLEU) and reference-free metrics (like BERTScore)?

## Architecture Onboarding

- Component map: Adversarial attacks (CLARE, Faster Genetic Algorithm, Input Reduction, DeepWordBug) -> Original translations (WMT 12, 17, 22) -> Perturbed translations -> Metric evaluation (BERTScore, BLEURT, COMET) -> Human evaluation -> Analysis

- Critical path:
  1. Generate adversarial perturbations for translations
  2. Apply metrics to original and perturbed translations
  3. Collect human ratings for a subset
  4. Compare metric degradation vs. human-perceived degradation
  5. Identify patterns of overpenalization and self-inconsistency

- Design tradeoffs:
  - Using black-box attacks vs. white-box attacks (simplicity vs. effectiveness)
  - Balancing perturbation severity (detectable by metrics vs. still fluent)
  - Sampling strategy for human evaluation (representativeness vs. cost)

- Failure signatures:
  - Metrics showing large score drops for minor perturbations
  - Human ratings showing less severe degradation than metrics
  - BERTScore showing high similarity between original and perturbed but low similarity to reference

- First 3 experiments:
  1. Reproduce the overpenalization results by applying CLARE and Faster Genetic Algorithm to WMT data and comparing metric scores with human ratings.
  2. Test the self-inconsistency of BERTScore by generating perturbations that maintain semantic similarity to the original but degrade reference alignment.
  3. Investigate the effect of sentence length on overpenalization by stratifying results based on translation length.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can learned metrics like BLEURT and COMET be made more robust to adversarial perturbations through data augmentation?
- Basis in paper: [explicit] The authors suggest augmenting training data with adversarially-generated examples to improve robustness of learned metrics like BLEURT and COMET.
- Why unresolved: The paper identifies the vulnerability of learned metrics to adversarial attacks but does not propose specific data augmentation strategies or evaluate their effectiveness.
- What evidence would resolve it: Experiments comparing the robustness of learned metrics trained with and without adversarially-generated examples against a range of attack methods.

### Open Question 2
- Question: How can self-consistency be incorporated as a regularization term for other embedding-based metrics like XMoverDistance or UScore?
- Basis in paper: [explicit] The authors propose using self-consistency as a regularization term for BERTScore and suggest it could be applied to other embedding-based metrics, but do not provide implementation details or evaluate its effectiveness.
- Why unresolved: The paper introduces the concept of self-consistency for BERTScore but does not explore its application to other metrics or provide a framework for incorporating it as a regularization term.
- What evidence would resolve it: Experiments demonstrating improved robustness of other embedding-based metrics when trained with self-consistency as a regularization term against adversarial attacks.

### Open Question 3
- Question: How does the effectiveness of adversarial attacks vary across different language pairs, translation systems, and domains?
- Basis in paper: [inferred] The paper focuses on German-to-English translation in the news domain, but acknowledges that results may not generalize to other language pairs, systems, or domains.
- Why unresolved: The paper's experiments are limited to a specific language pair, translation system, and domain, and does not explore how the effectiveness of adversarial attacks may differ in other contexts.
- What evidence would resolve it: Experiments evaluating the robustness of automatic metrics to adversarial attacks across a diverse set of language pairs, translation systems, and domains.

## Limitations
- Limited human evaluation sample size (2,373 sentences rated) may not fully capture the range of perturbation effects across all metric types and attack methods
- Study focuses on German-English translation, limiting generalizability to other language pairs
- Specific quality distribution differences between WMT 22 outputs and the training data used for BLEURT and COMET are not quantified

## Confidence
- Mechanism 1 (overpenalization): Medium confidence - well-supported by human validation experiments but limited sample size
- Mechanism 2 (out-of-distribution vulnerability): Medium confidence - plausible based on training data mismatch but not extensively quantified
- Mechanism 3 (BERTScore self-inconsistency): Medium confidence - demonstrated through specific attack success patterns but limited to certain attack types

## Next Checks
1. Scale human evaluation: Increase the number of human-rated sentences by 3-5x across multiple language pairs to better establish the true severity of metric overpenalization and validate the distributional differences between metric training data and test data.

2. Quantify quality distributions: Characterize the quality distribution differences between WMT 22 outputs and the training data used for BLEURT and COMET, using automated quality estimation or additional human evaluation to confirm the out-of-distribution vulnerability claim.

3. Test mitigation strategies: Implement and evaluate potential defenses against adversarial attacks, such as adversarial training or quality-conditioned metric calibration, to determine if the identified vulnerabilities can be effectively addressed.