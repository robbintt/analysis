---
ver: rpa2
title: Conformal Prediction with Large Language Models for Multi-Choice Question Answering
arxiv_id: '2305.18404'
source_url: https://arxiv.org/abs/2305.18404
tags:
- prediction
- conformal
- computer
- college
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies conformal prediction to large language models
  for uncertainty quantification in multiple-choice question-answering. The method
  constructs prediction sets from model softmax outputs to guarantee coverage of the
  true answer with a user-specified probability.
---

# Conformal Prediction with Large Language Models for Multi-Choice Question Answering

## Quick Facts
- arXiv ID: 2305.18404
- Source URL: https://arxiv.org/abs/2305.18404
- Reference count: 40
- Key outcome: Conformal prediction guarantees coverage for LLMs on multiple-choice questions when calibration and test data are exchangeable, with prediction set size correlating with accuracy for selective classification.

## Executive Summary
This paper applies conformal prediction to large language models for uncertainty quantification in multiple-choice question-answering. The method constructs prediction sets from model softmax outputs to guarantee coverage of the true answer with a user-specified probability. Experiments on the MMLU benchmark show that prediction set size correlates strongly with accuracy, enabling selective classification by filtering low-confidence predictions. Coverage guarantees hold when calibration and test data are exchangeable, but degrade when subjects differ, especially across unrelated domains.

## Method Summary
The paper uses conformal prediction to generate uncertainty estimates for LLMs answering multiple-choice questions. LLaMA-13B processes questions using one-shot prompting with GPT-4 generated examples, producing softmax probabilities for each answer choice. Conformal calibration on 50% of the data estimates a threshold to construct prediction sets achieving target coverage. The least ambiguous set classifier (LAC) score function determines which answers to include. Coverage and prediction set sizes are evaluated on the remaining 50% of questions across 16 MMLU subjects.

## Key Results
- Conformal prediction achieves 90% coverage when calibration and test data are from the same subject
- Prediction set size correlates strongly (r = -0.71 to -0.92) with top-1 accuracy across subjects
- Coverage drops to 79% when calibrating on one subject and testing on an unrelated subject
- Average prediction set size is 1.96 for 90% coverage, enabling selective classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conformal prediction can guarantee that prediction sets contain the true answer at a user-specified rate when calibration and test data are exchangeable.
- Mechanism: Conformal prediction estimates a threshold on calibration scores so that prediction sets constructed from model softmax outputs achieve the desired coverage.
- Core assumption: Calibration data must be exchangeable with test data in distribution.
- Evidence anchors:
  - [abstract] "Coverage guarantees hold when calibration and test data are exchangeable"
  - [section] "Conformal methods generate prediction sets that ensure a certain user-specified probability of containing the true label, regardless of the underlying model or distribution"
- Break condition: When calibration and test data are not exchangeable, such as when calibrated on one subject and tested on another unrelated subject.

### Mechanism 2
- Claim: Prediction set size correlates with model accuracy, enabling selective classification.
- Mechanism: Conformal prediction outputs adaptive prediction sets where larger sets indicate lower model confidence, which correlates with lower accuracy.
- Core assumption: Model softmax outputs are well-calibrated enough that uncertainty signals (set sizes) reflect true prediction quality.
- Evidence anchors:
  - [abstract] "prediction set size correlates strongly with accuracy, enabling selective classification"
  - [section] "we find a strong negative correlation between set size and top-1 accuracy for all subjects"
- Break condition: If model calibration is very poor, the correlation between set size and accuracy may weaken.

### Mechanism 3
- Claim: Conformal prediction provides more reliable coverage than naive top-k methods.
- Mechanism: Conformal prediction adapts prediction set sizes per instance to achieve target coverage, while naive top-k uses fixed sizes that don't guarantee coverage.
- Core assumption: Exchangeability holds between calibration and test data.
- Evidence anchors:
  - [section] "We show the coverage when all prediction sets have a fixed set size and find that coverage decreases sharply with size. This is in contrast to prediction sets formed by conformal prediction in Figure 6"
  - [section] "Conformal prediction can be thought of as outputting 'adaptive' prediction sets that try to attain the proper level of coverage"
- Break condition: When exchangeability assumption is violated, even adaptive conformal prediction sets may fail to achieve target coverage.

## Foundational Learning

- Concept: Exchangeability assumption in conformal prediction
  - Why needed here: Conformal prediction guarantees coverage only when calibration and test data are exchangeable
  - Quick check question: What happens to coverage guarantees when we calibrate on computer science questions but test on business ethics questions?

- Concept: Selective classification
  - Why needed here: The paper uses prediction set size to filter low-confidence predictions, improving overall accuracy
  - Quick check question: How does filtering predictions with set size > 3 affect the accuracy distribution across subjects?

- Concept: Conformal prediction vs naive top-k
  - Why needed here: The paper compares adaptive conformal prediction sets to fixed-size naive top-k sets
  - Quick check question: Why does coverage drop sharply for naive top-k sets as k decreases?

## Architecture Onboarding

- Component map: LLM model (LLaMA-13B) → Softmax probabilities → Conformal calibration → Prediction sets → Coverage evaluation
- Critical path: 1. Generate softmax probabilities from LLM, 2. Calibrate threshold using calibration data, 3. Construct prediction sets at inference time, 4. Evaluate coverage and set size statistics
- Design tradeoffs:
  - One-shot prompting vs zero-shot: One-shot provides better accuracy but requires careful prompt engineering
  - Exchangeability assumption vs real-world deployment: Strong assumption that may not hold across different subjects
  - Coverage rate vs set size: Higher coverage requires larger prediction sets
- Failure signatures:
  - Coverage significantly below target: Likely exchangeability violation
  - Set size poorly correlated with accuracy: Possible calibration issues
  - High ECE/MCE: Model softmax probabilities not well-calibrated
- First 3 experiments:
  1. Verify coverage guarantee on same-subject calibration/evaluation split
  2. Test exchangeability violation by calibrating on one subject, evaluating on another
  3. Measure correlation between set size and accuracy for selective classification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of conformal prediction degrade when calibration and test data are drawn from different but related domains (e.g., college and high school versions of the same subject)?
- Basis in paper: [explicit] The paper investigates exchangeability assumptions when calibration and test data come from different subjects, finding that coverage degrades for unrelated domains.
- Why unresolved: The paper only tests exchangeability across entirely different subjects, not variations within the same subject at different educational levels.
- What evidence would resolve it: Experiments comparing coverage when calibrating on college-level questions and testing on high school-level questions (and vice versa) for the same subject.

### Open Question 2
- Question: How sensitive is conformal prediction's coverage guarantee to the choice of score function beyond the least ambiguous classifier (LAC) approach?
- Basis in paper: [explicit] The paper uses LAC as the score function but notes this is "a common choice" without exploring alternatives.
- Why unresolved: Only one score function is evaluated, leaving open questions about whether coverage guarantees hold with different scoring methods.
- What evidence would resolve it: Comparative experiments using alternative score functions (e.g., softmax entropy, margin-based scores) while maintaining coverage guarantees across multiple subjects.

### Open Question 3
- Question: Does the correlation between prediction set size and accuracy persist for zero-shot or few-shot prompting strategies beyond the one-shot approach used in this paper?
- Basis in paper: [explicit] The paper finds strong correlation between set size and accuracy for one-shot prompting but only briefly mentions zero-shot and few-shot prompting.
- Why unresolved: The analysis is limited to one-shot prompting, leaving open whether the uncertainty-accuracy relationship generalizes to other prompting strategies.
- What evidence would resolve it: Experiments measuring the correlation between set size and accuracy for zero-shot and few-shot prompts across the same subjects.

### Open Question 4
- Question: How does the exchangeability assumption violation affect coverage guarantees for subjects with different levels of inherent similarity (e.g., related medical fields vs. unrelated business subjects)?
- Basis in paper: [inferred] The paper groups subjects into broad categories (business, medicine, computer science) and finds better exchangeability within categories, but doesn't systematically measure similarity effects.
- Why unresolved: The paper doesn't quantify the relationship between subject similarity and exchangeability violation effects.
- What evidence would resolve it: Experiments measuring coverage degradation as a function of measurable similarity metrics (e.g., vocabulary overlap, knowledge graph distance) between calibration and test subjects.

## Limitations

- Exchangeability assumption is critical and may fail when calibration and test data come from different subjects or domains
- Coverage guarantees come at the cost of larger prediction sets that include multiple answer choices, reducing practical utility
- One-shot prompting with GPT-4 is required for competitive accuracy, adding complexity and potential reproducibility challenges

## Confidence

**Coverage Guarantee Achievement**: High confidence - consistent results across multiple subjects in Table 1
**Correlation Between Set Size and Accuracy**: High confidence - strong negative correlation coefficients (-0.71 to -0.92) across all subjects
**Cross-Subject Exchangeability Failure**: Medium confidence - coverage degradation observed but specific patterns need further systematic exploration

## Next Checks

1. **Exchangeability Robustness Analysis**: Systematically measure coverage degradation when calibrating on one subject and testing on others, quantifying the relationship between subject similarity and coverage failure.

2. **Prompt Engineering Impact Study**: Conduct ablation studies varying the quality and structure of one-shot prompts to quantify their impact on both accuracy and conformal prediction calibration.

3. **Adaptive Threshold Calibration**: Investigate whether subject-specific calibration or hierarchical calibration approaches can maintain coverage guarantees when exchangeability assumptions are partially violated.