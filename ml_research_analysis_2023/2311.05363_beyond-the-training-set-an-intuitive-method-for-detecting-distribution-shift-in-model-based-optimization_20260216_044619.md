---
ver: rpa2
title: 'Beyond the training set: an intuitive method for detecting distribution shift
  in model-based optimization'
arxiv_id: '2311.05363'
source_url: https://arxiv.org/abs/2311.05363
tags:
- design
- distribution
- training
- shift
- surrogate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Offline model-based optimization is used to design new samples
  that outperform those in a fixed training set, but distribution shifts between training
  and design samples can negatively impact model accuracy and design quality. To address
  this, the authors propose a method that trains a binary classifier to distinguish
  between training data and design samples, using the classifier's logit scores as
  a proxy for distribution shift.
---

# Beyond the training set: an intuitive method for detecting distribution shift in model-based optimization

## Quick Facts
- arXiv ID: 2311.05363
- Source URL: https://arxiv.org/abs/2311.05363
- Reference count: 40
- Key outcome: Offline model-based optimization uses a binary classifier to detect distribution shifts between training and design samples, improving design quality by filtering unreliable predictions

## Executive Summary
This paper addresses the critical challenge of distribution shift in offline model-based optimization (MBO) for designing new samples that outperform those in a fixed training set. The authors propose an intuitive method that trains a binary classifier to distinguish between training data and design samples, using the classifier's logit scores as a proxy for distribution shift intensity. This approach enables users to constrain their search to regions where the model's predictions are reliable, thereby increasing the quality of designs.

The method is particularly valuable in protein design applications where feedback covariate shift can occur as optimization progresses, potentially leading to adversarial examples with poor experimental performance. By providing a continuous measure of distribution shift, the approach allows for nuanced filtering and selection of design candidates, outperforming traditional uncertainty metrics like Deep Ensemble uncertainties in distinguishing between reliable and unreliable predictions.

## Method Summary
The proposed method involves training a binary classifier using knowledge of the unlabeled design distribution to separate the training data from the design data. The classifier's logit scores are then used as a proxy measure of distribution shift, approximating the density ratio between the design and training distributions. This approach can be applied to any MBO problem where a fixed dataset is available for training, and new designs are generated through optimization. The method is particularly effective in detecting and mitigating feedback covariate shift, where the test distribution depends on the training data through the optimization process.

## Key Results
- The OOD classifier successfully detects distribution shifts in protein design tasks, including AAV capsid sequence optimization
- OOD scores provide better discrimination between reliable and unreliable predictions compared to Deep Ensemble uncertainties
- Experimental validation shows the method improves design quality by filtering out adversarial examples with high predicted properties but low reliability
- The approach works across different optimization algorithms and protein design tasks, demonstrating broad applicability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A binary classifier can approximate the density ratio between training and design distributions using the logit scores from a sigmoid output layer.
- Mechanism: The logit scores from a binary classifier trained to distinguish training data from design samples approximate the density ratio s(x) = pde(x)/ptr(x), which serves as a proxy for distribution shift intensity.
- Core assumption: The classifier can effectively separate in-distribution training samples from out-of-distribution design samples when trained with appropriate positive and negative examples.
- Evidence anchors:
  - [abstract] "This method trains a binary classifier using knowledge of the unlabeled design distribution to separate the training data from the design data. The classifier's logit scores are then used as a proxy measure of distribution shift."
  - [section] "It is well known that a binary classifier can be trained such that its output values approximate a density ratio such as that in Equation 1."
- Break condition: If the design distribution overlaps significantly with the training distribution, the classifier cannot distinguish between the two, making the density ratio approximation ineffective.

### Mechanism 2
- Claim: The logit scores increase monotonically with the severity of distribution shift, allowing them to serve as a continuous measure of shift intensity rather than just a binary indicator.
- Mechanism: As optimization progresses and samples move further from the training distribution, the classifier becomes more confident in labeling them as OOD, resulting in higher logit scores that correlate with increasing prediction error.
- Core assumption: The optimization process creates a gradual shift in the design distribution that can be captured by a classifier trained on discrete examples.
- Evidence anchors:
  - [section] "Figure 3a shows that the OOD scores steadily increase over the course of the trajectory, in concert with the increase in surrogate model MSE"
  - [section] "This shows that the OOD scores can be effectively used as a continuous predictor of the intensity of distribution shift at points along a design trajectory"
- Break condition: If the optimization process creates abrupt jumps in distribution shift rather than gradual changes, the continuous scoring may not accurately reflect the severity of shift at intermediate points.

### Mechanism 3
- Claim: Using OOD scores to filter or rank design candidates improves the selection of high-quality sequences by avoiding adversarial examples with unreliable predictions.
- Mechanism: By setting thresholds or using stratified selection based on OOD scores, practitioners can prioritize sequences that are both predicted to have high properties and are likely to be in-distribution, reducing the selection of adversarial examples.
- Core assumption: The OOD scores correlate with both the likelihood of being adversarial and the reliability of surrogate model predictions.
- Evidence anchors:
  - [abstract] "This enables users to constrain their search to regions where the model's predictions are reliable, thereby increasing the quality of designs."
  - [section] "Clearly, the OOD scores are better able to distinguish between these two categories than the Deep Ensemble uncertainties"
- Break condition: If the relationship between OOD scores and prediction reliability breaks down due to changes in the optimization landscape or model architecture, filtering based on OOD scores may exclude good candidates or include poor ones.

## Foundational Learning

- Concept: Density ratio estimation using binary classification
  - Why needed here: The method relies on the density ratio trick to convert a binary classification problem into an estimate of the distribution shift between training and design samples.
  - Quick check question: What is the mathematical relationship between the sigmoid output of a binary classifier and the density ratio between two distributions?

- Concept: Feedback covariate shift vs. standard covariate shift
  - Why needed here: The method specifically addresses feedback covariate shift, where the test distribution depends on the training data through the optimization process, which is different from standard covariate shift.
  - Quick check question: How does feedback covariate shift differ from standard covariate shift in terms of the relationship between training and test distributions?

- Concept: Deep ensemble uncertainty as a baseline
  - Why needed here: The paper compares OOD scores against deep ensemble uncertainties, so understanding how deep ensembles estimate predictive uncertainty is important for evaluating the proposed method.
  - Quick check question: How do deep ensemble uncertainties estimate prediction uncertainty, and why might they fail to capture distribution shift in this context?

## Architecture Onboarding

- Component map: Surrogate model (trained on initial data) -> Optimization algorithm (generates designs) -> OOD classifier (trained on training vs. design samples) -> Design selection (using OOD scores)
- Critical path: Train surrogate model → Run optimization to generate designs → Train OOD classifier on training data vs. designed samples → Use OOD scores to filter/select designs → Validate selected designs experimentally
- Design tradeoffs: Simplicity and flexibility vs. potential loss of information compared to more complex distribution shift detection methods
- Failure signatures: Poor separation between training and design distributions in OOD classifier training data, resulting in scores that don't correlate with distribution shift
- First 3 experiments:
  1. Implement the 2D toy example with the Himmelblau function to verify the OOD classifier can detect regions where surrogate model error is high.
  2. Run the protein structure prediction simulation using ESMFold as ground truth to test the method on a more realistic but still simulated problem.
  3. Apply the method to a simple sequence design problem with a small dataset (e.g., 100-500 samples) to verify it works at scale before attempting the full AAV experiment.

## Open Questions the Paper Calls Out

- Question: How can the proposed OOD classifier be adapted to handle online model-based optimization (MBO) scenarios, where the training set is continuously updated with new data from the design process?
- Question: Can the OOD classifier be extended to detect distribution shifts in other types of data, such as time series or graph-structured data, beyond the sequence-based data explored in the paper?
- Question: How does the performance of the OOD classifier compare to other distribution shift detection methods, such as conformal prediction or domain adaptation techniques, in terms of accuracy and computational efficiency?

## Limitations
- The method requires a sufficient number of design samples to effectively train the OOD classifier
- There is a sequential dependency requiring optimization to run before OOD classifier training
- The approach may struggle with high-dimensional input spaces where training and design distributions become less distinguishable
- The method assumes OOD scores correlate with prediction reliability, which may not hold across all domains

## Confidence
- **High confidence**: The fundamental mechanism of using binary classification for density ratio estimation is well-established in machine learning theory
- **Medium confidence**: The continuous scoring interpretation works well in tested scenarios but may not generalize to all optimization landscapes
- **Medium confidence**: The filtering effectiveness shows promise but needs further validation across diverse problem domains

## Next Checks
1. Test the method on a high-dimensional design problem (e.g., 50+ input dimensions) to assess scalability and verify that OOD scores remain discriminative in higher dimensions.
2. Evaluate performance when the optimization algorithm generates only a small number of samples (e.g., 10-50) to determine the minimum sample requirement for effective OOD classification.
3. Compare OOD scores against ground truth prediction errors in a controlled setting where the true distribution shift can be precisely measured, to validate the correlation assumption.