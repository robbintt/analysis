---
ver: rpa2
title: Blending Reward Functions via Few Expert Demonstrations for Faithful and Accurate
  Knowledge-Grounded Dialogue Generation
arxiv_id: '2311.00953'
source_url: https://arxiv.org/abs/2311.00953
tags:
- knowledge
- learning
- reward
- generation
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating faithful and accurate
  responses in knowledge-grounded dialogue systems, where models often suffer from
  hallucinations and distractions from irrelevant information in the knowledge texts.
  The authors propose a novel reinforcement learning approach that blends two automatic
  metrics - accuracy and faithfulness - to approximate a balanced quality judgment
  of generated responses.
---

# Blending Reward Functions via Few Expert Demonstrations for Faithful and Accurate Knowledge-Grounded Dialogue Generation

## Quick Facts
- arXiv ID: 2311.00953
- Source URL: https://arxiv.org/abs/2311.00953
- Reference count: 22
- Primary result: Novel RL approach blending accuracy and faithfulness metrics, calibrated by expert demonstrations, improves knowledge-grounded dialogue generation quality.

## Executive Summary
This paper addresses the challenge of generating faithful and accurate responses in knowledge-grounded dialogue systems, where models often suffer from hallucinations and distractions from irrelevant information in the knowledge texts. The authors propose a novel reinforcement learning approach that blends two automatic metrics - accuracy and faithfulness - to approximate a balanced quality judgment of generated responses. The blending coefficient is learned from a few expert demonstrations of pair-wise quality judgment on two LLMs' outputs. Empirical experiments on two conversational information-seeking datasets demonstrate that the proposed method can improve the faithfulness and accuracy of generated responses compared to strong supervised learning baselines, with the best overall performance achieved by learning the blending coefficient from expert demonstrations.

## Method Summary
The proposed method uses reinforcement learning to train a dialogue generation model that balances faithfulness to knowledge texts with response accuracy. The key innovation is a reward function that combines accuracy and faithfulness metrics, weighted by a blending coefficient α learned from expert demonstrations. The model is initialized with a T5-base model fine-tuned via supervised learning, then optimized using PPO with the blended reward function. The α value is determined by having experts compare pairs of model outputs, with the optimal coefficient found to be dataset-dependent (0.04 for MultiDoc2Dial, 0.92 for FaithDial).

## Key Results
- The blended reward function outperforms using either accuracy or faithfulness metrics alone
- Learning α from expert demonstrations provides better generalization than grid search on validation data
- The proposed method achieves state-of-the-art performance on MultiDoc2Dial and FaithDial datasets
- Empirical results show improvements in both faithfulness and accuracy metrics compared to supervised learning baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Blending two automatic metrics (accuracy and faithfulness) with a learned coefficient approximates human preference better than using either metric alone
- Mechanism: The α coefficient balances the trade-off between semantic coherence (accuracy) and factual consistency (faithfulness) based on the specific characteristics of the input knowledge text
- Core assumption: The optimal balance between accuracy and faithfulness varies depending on whether the input knowledge text contains redundant/irrelevant information or precise relevant information
- Evidence anchors:
  - [abstract]: "Our reward function combines an accuracy metric and a faithfulness metric to provide a balanced quality judgment of generated responses"
  - [section]: "we find the optimal value of αhuman on MultiDoc2Dial is 0.04 with a Pearson correlation coefficient of 0.2278, and the optimal value of αhuman on FaithDial is 0.92 with a Pearson correlation coefficient of 0.2865"
  - [corpus]: Weak - related papers discuss reward design but don't specifically address metric blending for dialogue generation
- Break condition: If the expert demonstrations are not representative of the actual test distribution, or if the Pearson correlation between expert and automated judgments is very low

### Mechanism 2
- Claim: Reinforcement learning with the blended reward function can learn to generate responses that are both faithful to knowledge and coherent with conversation context
- Mechanism: PPO algorithm updates the policy to maximize the expected sum of blended rewards, where each reward component guides the model toward different aspects of response quality
- Core assumption: The initial policy (T5-SFT) provides a reasonable starting point, and the reward function provides useful gradient signals for improvement
- Evidence anchors:
  - [abstract]: "empirical experiments on two conversational information-seeking datasets demonstrate that our method can compete with other strong supervised learning baselines"
  - [section]: "For all PPO experiments, the policy network and value network share the same base model initialized from T5-SFT but separate the last output layer"
  - [corpus]: Weak - related papers discuss RL for NLP but don't specifically address knowledge-grounded dialogue with blended metrics
- Break condition: If the reward signal is too sparse or noisy, or if the KL penalty prevents meaningful policy updates

### Mechanism 3
- Claim: Learning α from few expert demonstrations provides better generalization than grid search on validation data
- Mechanism: Expert demonstrations calibrate the reward function to align with human judgment, reducing overfitting to validation set characteristics
- Core assumption: Expert pairwise comparisons between LLM outputs provide sufficient signal to learn an effective α value
- Evidence anchors:
  - [abstract]: "Our reward function can be used as a cost-effective approximation to a human preference reward model when only a few preference annotations are available"
  - [section]: "we find αhuman achieves better overall performance in both datasets, which indicates few expert demonstrations can not only help calibrate the values to learn a better policy, but also provide a good generalization ability for the reward function"
  - [corpus]: Weak - related papers discuss few-shot learning but don't specifically address α calibration for metric blending
- Break condition: If experts are not consistent in their pairwise comparisons, or if the demonstration set is too small to capture the diversity of response quality

## Foundational Learning

- Concept: Markov Decision Process formulation of dialogue generation
  - Why needed here: Provides the mathematical framework for applying reinforcement learning to the response generation task
  - Quick check question: What are the state, action, and reward components in this MDP formulation?

- Concept: Proximal Policy Optimization algorithm
  - Why needed here: Enables stable policy updates while preventing large deviations from the pre-trained language model
  - Quick check question: How does the KL penalty in PPO help maintain coherence with the pre-trained model?

- Concept: Automatic evaluation metrics for dialogue quality (BLEU, ROUGE, BERTScore)
  - Why needed here: Provides differentiable reward signals that can guide the RL training process
  - Quick check question: Why might BERTScore be more appropriate than BLEU for measuring faithfulness?

## Architecture Onboarding

- Component map: T5-base → PPO policy/value networks → blended reward function → expert α calibration
- Critical path: Knowledge text + conversation history → policy network → generated response → reward calculation → policy update
- Design tradeoffs: Accuracy vs. faithfulness balance (α), reward metric selection, KL penalty strength
- Failure signatures: Degraded faithfulness with high α, reduced coherence with low α, unstable training with poor reward design
- First 3 experiments:
  1. Train with α = 0.00 (faithfulness only) and α = 1.00 (accuracy only) to verify the trade-off exists
  2. Train with grid search α values to establish baseline performance
  3. Train with αhuman from expert demonstrations to compare generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the limits of using automatic metrics (e.g., SacreBLEU, BERTScore) to approximate human preference for faithfulness and accuracy in knowledge-grounded dialogue generation?
- Basis in paper: [explicit] The paper uses SacreBLEU and BERTScore as automatic metrics to evaluate accuracy and faithfulness, and proposes learning the blending coefficient α from expert demonstrations to better approximate human preference.
- Why unresolved: The paper does not provide a comprehensive analysis of the limitations of using these automatic metrics as proxies for human judgment, especially in cases where the knowledge text contains redundant or irrelevant information.
- What evidence would resolve it: Empirical studies comparing the performance of models optimized using different combinations of automatic metrics against human evaluations on a wide range of knowledge-grounded dialogue datasets.

### Open Question 2
- Question: How does the performance of the proposed method scale with the size of the expert demonstration dataset used to learn the blending coefficient α?
- Basis in paper: [explicit] The paper uses 25 pair-wise comparison demonstrations from an NLP expert to learn the optimal value of α for each dataset.
- Why unresolved: The paper does not investigate the impact of using different numbers of expert demonstrations on the performance of the proposed method or the generalizability of the learned α value.
- What evidence would resolve it: Experiments comparing the performance of the proposed method using varying numbers of expert demonstrations, ranging from a few to hundreds, on multiple knowledge-grounded dialogue datasets.

### Open Question 3
- Question: Can the proposed method be extended to handle knowledge-grounded dialogue generation tasks with more complex knowledge sources, such as multi-modal information or knowledge bases with structured data?
- Basis in paper: [inferred] The paper focuses on knowledge-grounded dialogue generation tasks where the knowledge is provided as a single text passage, but does not explore more complex knowledge sources.
- Why unresolved: The paper does not address the challenges and potential solutions for applying the proposed method to knowledge-grounded dialogue generation tasks with more diverse and complex knowledge sources.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of the proposed method on knowledge-grounded dialogue generation tasks using multi-modal knowledge sources (e.g., text, images, videos) or structured knowledge bases (e.g., databases, knowledge graphs).

## Limitations
- The specific procedure for collecting expert demonstrations and determining pairwise quality judgments is not specified
- The evidence for RL improvements relies primarily on abstract-level statements without specific performance metrics
- The claimed generalization ability of αhuman versus grid search is supported by limited evidence

## Confidence
- **High confidence**: The problem statement and general approach (blending accuracy and faithfulness metrics with RL) are well-founded based on related literature
- **Medium confidence**: The experimental results and specific performance claims, as these rely on detailed implementation choices and dataset characteristics not fully described
- **Low confidence**: The exact methodology for expert demonstration collection and α calibration, as these critical details are missing

## Next Checks
1. Replicate the α calibration process on a small subset of the data to verify that the expert-learned values (0.04 and 0.92) produce consistent results across multiple runs and are not sensitive to the specific expert demonstrations used

2. Conduct an ablation study comparing performance when using only the accuracy metric (α = 1.0), only the faithfulness metric (α = 0.0), and various intermediate values to quantify the benefit of the blended approach over individual metrics

3. Test the generalization of the learned α values by applying them to a held-out test set from each dataset and measuring whether the correlation between the blended reward and human judgments holds, or if the values overfit to the demonstration set