---
ver: rpa2
title: Understanding and Estimating Domain Complexity Across Domains
arxiv_id: '2312.13487'
source_url: https://arxiv.org/abs/2312.13487
tags:
- complexity
- domain
- game
- space
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a domain-independent framework for estimating
  the complexity of environments that AI systems operate in. It distinguishes between
  intrinsic (agent-independent) and extrinsic (agent-dependent) complexity, further
  dividing the intrinsic part into "environment space" and "task solution space".
---

# Understanding and Estimating Domain Complexity Across Domains

## Quick Facts
- arXiv ID: 2312.13487
- Source URL: https://arxiv.org/abs/2312.13487
- Reference count: 6
- Primary result: A domain-independent framework for estimating environment complexity across AI domains

## Executive Summary
This paper introduces a comprehensive framework for estimating the complexity of environments where AI systems operate. The framework distinguishes between intrinsic (agent-independent) and extrinsic (agent-dependent) complexity, further dividing intrinsic complexity into "environment space" and "task solution space." By analyzing dimensionality, sparsity, and diversity within these categories, the framework offers a systematic way to measure and compare domain complexity across diverse AI applications.

The authors demonstrate their framework through multiple case studies including action domains (tic-tac-toe, Monopoly, CartPole), perception domains (MNIST, CIFAR-10), and data science domains (Iris). The results show how these measures can capture the relative complexity of different domains and tasks within them, providing valuable insights for predicting AI system difficulty when transitioning between environments and avoiding bias in novel situations.

## Method Summary
The framework separates complexity into intrinsic (agent-independent) and extrinsic (agent-dependent) components. Intrinsic complexity is further divided into environment space (all possible states) and task solution space (valid solutions to the task). The framework employs three measures: dimensionality (size of problem space), sparsity (information distribution), and diversity (information importance across components). For action domains, state transition graphs are used, while perception and data science domains use feature space representations. The framework calculates metrics like state-space complexity, game-tree complexity, sparsity indices, and normalized Shannon entropy to estimate complexity levels across domains.

## Key Results
- The framework successfully distinguishes between intrinsic and extrinsic complexity components across diverse domains
- Dimensionality, sparsity, and diversity measures provide a comprehensive view of domain complexity
- The framework enables quantitative predictions of AI difficulty during environment transitions
- Case studies demonstrate the framework's applicability across action, perception, and data science domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework's separation of intrinsic and extrinsic complexity enables systematic complexity estimation across domains.
- Mechanism: By distinguishing between agent-independent (intrinsic) and agent-dependent (extrinsic) complexity components, the framework can measure domain complexity without conflating environmental properties with agent capabilities.
- Core assumption: Intrinsic and extrinsic complexity components can be meaningfully separated and measured independently.
- Evidence anchors: [abstract] "This framework distinguishes between intrinsic complexity (inherent to the domain) and extrinsic complexity (dependent on the AI agent)."
- Break condition: If intrinsic and extrinsic components cannot be meaningfully separated, the framework's estimates would become inaccurate.

### Mechanism 2
- Claim: The three-category measure system (dimensionality, sparsity, diversity) provides a comprehensive view of domain complexity.
- Mechanism: These three measures capture different aspects of complexity - dimensionality measures problem space size, sparsity measures information distribution, and diversity measures information importance across components.
- Core assumption: These three categories together capture all meaningful aspects of domain complexity.
- Evidence anchors: [abstract] "By analyzing dimensionality, sparsity, and diversity within these categories, we offer a comprehensive view of domain challenges."
- Break condition: If critical aspects of complexity are not captured by these three measures, the framework would miss important complexity factors.

### Mechanism 3
- Claim: Domain-independent measures enable quantitative predictions of AI difficulty during environment transitions.
- Mechanism: By providing standardized metrics that can be calculated across diverse domains, the framework enables comparison of complexity levels between different environments.
- Core assumption: Complexity measures are comparable across fundamentally different domains (action, perception, data science).
- Evidence anchors: [abstract] "This systematic approach to estimating domain complexity can help predict the difficulty AI systems will face when transitioning between environments"
- Break condition: If the measures are not truly comparable across domains, predictions would be unreliable.

## Foundational Learning

- Concept: State transition graphs vs feature spaces
  - Why needed here: The paper uses different representations for action/planning domains (state transition graphs) versus perception/data science domains (feature spaces).
  - Quick check question: What is the fundamental difference between how a chess game and an image classification task would be represented in this framework?

- Concept: Agent-independent vs agent-dependent complexity
  - Why needed here: The framework's core contribution is separating complexity into these two categories.
  - Quick check question: In a self-driving car scenario, what aspects of complexity would be considered intrinsic versus extrinsic?

- Concept: Combinatorial game theory measures
  - Why needed here: The paper uses state-space complexity and game-tree complexity from combinatorial game theory.
  - Quick check question: How does state-space complexity differ from game-tree complexity in the context of tic-tac-toe?

## Architecture Onboarding

- Component map:
  - Core framework module -> Representation module -> Measure calculation module -> Domain abstraction layer -> Case study module

- Critical path:
  1. Identify domain type (action, perception, data science)
  2. Select appropriate representation (state transition graph or feature space)
  3. Calculate intrinsic complexity measures (environment space and task solution space)
  4. Calculate extrinsic complexity measures (performance, goal, planning, skills spaces)
  5. Combine measures for final complexity estimate

- Design tradeoffs:
  - Generality vs specificity: The framework aims to work across domains but may sacrifice some domain-specific precision
  - Analytical vs empirical measures: Some measures can be calculated analytically while others require empirical estimation
  - Static vs dynamic complexity: The framework focuses on static complexity but may need extensions for dynamic domains

- Failure signatures:
  - Inaccurate comparisons between domains: If measures aren't truly comparable across domains
  - Missing complexity factors: If important complexity aspects aren't captured by the three measures
  - Domain type misclassification: If the framework incorrectly identifies which representation to use

- First 3 experiments:
  1. Calculate complexity measures for tic-tac-toe vs chess to validate the framework captures known complexity differences
  2. Compare MNIST vs CIFAR-10 complexity to test the framework's ability to distinguish perception domain complexity
  3. Apply the framework to a simple action domain (CartPole) and its variants to validate measure consistency across similar domains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we derive strong theoretical connections between domain complexity and difficulty? Will tasks in more complex domains necessarily be more difficult, or just more difficult in expectation?
- Basis in paper: [explicit] Section 8 "Next Steps" mentions this as a key open question for future research.
- Why unresolved: The paper distinguishes between complexity (a description of state space) and difficulty (challenge posed by specific novelty), but doesn't provide a formal theoretical framework linking the two.
- What evidence would resolve it: A formal mathematical framework showing how domain complexity metrics relate to task difficulty, validated through controlled experiments across multiple domains showing consistent patterns.

### Open Question 2
- Question: Can our measures of domain complexity be used to define and quantify complexity in complex systems like networks, dynamical systems, and other nonlinear systems?
- Basis in paper: [explicit] Section 8 asks this as a next step, wondering if one system can be proven more complex than another along specific perspectives/components.
- Why unresolved: The paper's framework is designed for AI domains but hasn't been tested on traditional complex systems.
- What evidence would resolve it: Successful application of the framework to analyze and compare the complexity of well-studied complex systems using the same metrics.

### Open Question 3
- Question: In infinite action and state spaces, what are the appropriate mathematical frameworks for distinguishing between domains of differing complexity?
- Basis in paper: [explicit] Section 8 asks about mathematical frameworks for infinite spaces, drawing an analogy to hierarchies of infinity in real analysis.
- Why unresolved: The paper's measures work well for finite domains but doesn't address how to handle domains with infinite state spaces.
- What evidence would resolve it: Development of a mathematical framework that can meaningfully compare the complexity of infinite domains.

## Limitations

- The framework's effectiveness depends on the validity of separating intrinsic from extrinsic complexity, which may not hold in tightly coupled agent-environment systems
- The three measures (dimensionality, sparsity, diversity) may not capture all relevant aspects of complexity, particularly temporal dynamics and uncertainty
- The framework assumes static complexity measures, which may not apply well to dynamic environments

## Confidence

- **High confidence**: Theoretical framework and ability to separate intrinsic from extrinsic complexity
- **Medium confidence**: Practical applicability across diverse domains and completeness of the three-measure system
- **Low confidence**: Framework's ability to handle dynamic environments and tightly coupled agent-environment systems

## Next Checks

1. Apply the framework to domains where complexity is well-understood (e.g., compare simple vs complex games) to validate measure consistency
2. Test the framework's predictions against actual AI performance when transitioning between domains
3. Extend the framework to dynamic environments and evaluate whether complexity measures remain meaningful over time