---
ver: rpa2
title: 'LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise Comparisons
  using Large Language Models'
arxiv_id: '2307.07889'
source_url: https://arxiv.org/abs/2307.07889
tags:
- comparisons
- evaluation
- comparative
- assessment
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores zero-shot NLG evaluation using large language
  models (LLMs) through pairwise comparisons. The core idea is that LLMs can judge
  which of two generated texts is better for a given attribute, rather than assigning
  absolute scores.
---

# LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise Comparisons using Large Language Models

## Quick Facts
- arXiv ID: 2307.07889
- Source URL: https://arxiv.org/abs/2307.07889
- Reference count: 11
- Core finding: Comparative assessment using FlanT5-3B (3B parameters) outperforms ChatGPT scoring (42.5% vs 43.8% Spearman correlation) despite 175x fewer parameters

## Executive Summary
This paper introduces a novel zero-shot approach to NLG evaluation using pairwise comparisons with large language models. Rather than asking LLMs to assign absolute quality scores to generated texts, the method prompts them to compare two texts and choose which is better for a given attribute. The approach is tested on summarization and dialogue response evaluation tasks using FlanT5 models of varying sizes. The key finding is that smaller open-source LLMs can outperform larger API models when using comparative assessment instead of traditional scoring methods.

## Method Summary
The method involves using FlanT5 models in zero-shot mode with simple prompts to perform pairwise comparisons between generated texts. For each comparison, the LLM is asked to choose which of two texts is better for a specific attribute (e.g., coherence, consistency, relevance). The win ratios from these pairwise comparisons are then aggregated to produce overall rankings, which are evaluated using Spearman correlation against human scores. The paper also identifies and addresses positional bias in LLM comparisons, where models tend to favor the first option presented.

## Key Results
- FlanT5-3B achieves 43.8% average Spearman correlation on Summeval, outperforming ChatGPT scoring (42.5%) despite having 175x fewer parameters
- Comparative assessment consistently outperforms scoring across multiple tasks and model sizes
- Positional bias is detected where LLMs select the first option 60-70% of the time in pairwise comparisons
- Using symmetric comparison subsets improves performance while reducing computational cost

## Why This Works (Mechanism)

### Mechanism 1
Comparative assessment leverages human-like preference judgment where LLMs find it easier to compare two options than assign absolute scores. Instead of rating each text independently, we prompt the LLM to compare two texts and choose the better one, then aggregate pairwise wins to rank all texts. This approach assumes LLMs have emergent ability to make consistent relative judgments even when they struggle with absolute scoring.

### Mechanism 2
Smaller open-source LLMs can achieve performance competitive with large API models when using comparative assessment instead of absolute scoring. By shifting from absolute score prediction to pairwise comparison, FlanT5-3B (3B parameters) outperforms ChatGPT scoring (42.5%) with 43.8% average Spearman correlation. This suggests the relative comparison task is easier for LLMs than absolute scoring, reducing the parameter requirements.

### Mechanism 3
Positional bias in LLM pairwise comparisons can be detected and mitigated through debiasing methods. LLMs exhibit systematic bias toward selecting the first option in comparisons, which can be measured and addressed by using symmetric comparison subsets. This assumes LLMs have quantifiable positional preferences that can be corrected to improve reliability.

## Foundational Learning

- **Spearman correlation**: Measures rank correlation between predicted and human scores, appropriate for ordinal quality assessment. Why needed: Provides the primary metric for evaluating how well LLM judgments align with human quality assessments.
- **Pairwise comparison theory (Thurstone's Law)**: Provides theoretical foundation that humans (and potentially LLMs) can make more reliable relative judgments than absolute ratings. Why needed: Justifies why comparative assessment might work better than scoring for LLM evaluation.
- **Prompt-based classifier probability estimation**: Enables efficient comparison decisions without expensive text generation by using label word probabilities. Why needed: Provides the mathematical framework for converting LLM output probabilities to comparison decisions.

## Architecture Onboarding

- **Component map**: LLM model → Prompt template → Comparison selection strategy → Win ratio aggregation → Spearman correlation evaluation
- **Critical path**: Model inference → Prompt formatting → Comparison decision → Ranking aggregation → Performance measurement
- **Design tradeoffs**: Full comparison matrix (O(N²) comparisons) vs. subset selection (faster but potentially less accurate)
- **Failure signatures**: Poor correlation indicates either model incapability or prompt design issues; systematic bias toward first position indicates positional bias
- **First 3 experiments**:
  1. Implement basic pairwise comparison with FlanT5-220M on Summeval with all pairs, measure correlation
  2. Add positional bias measurement by comparing first vs second position selection rates
  3. Implement symmetric subset selection and compare performance vs full matrix

## Open Questions the Paper Calls Out

1. How does the performance of comparative assessment scale with the number of candidates (N) in the evaluation set? The paper discusses that using a full matrix comparison requires N(N-1) comparisons, which becomes computationally expensive, and explores using subsets of comparisons instead, but doesn't systematically study how performance degrades or improves as N increases.

2. What is the optimal prompt design for comparative assessment across different NLG tasks? The authors note they "consider simple prompts that are basic yet sensible and do not investigate prompt-engineering highly effective prompts," using only 3 simple prompts per task without exploring the impact of prompt complexity, phrasing, or task-specific adaptations.

3. How transferable is comparative assessment performance across different domains or attribute types? The paper evaluates only on summarization and dialogue response generation, using attributes like coherence, consistency, and fluency, but doesn't test cross-domain generalization to other NLG tasks like story generation, code generation, or image captioning.

## Limitations

- The positional bias finding (LLMs favoring the first option in 60-70% of comparisons) suggests systematic preference that could undermine reliability
- The absolute performance gains are modest (43.8% vs 42.5% Spearman correlation), raising questions about practical significance
- Computational efficiency claims are unclear since pairwise comparisons require O(N²) evaluations, though subset methods may compromise accuracy

## Confidence

- **High Confidence**: The observation that LLMs exhibit positional bias in pairwise comparisons is well-supported by empirical evidence showing consistent preference for the first option across multiple datasets and model sizes.
- **Medium Confidence**: The claim that comparative assessment outperforms scoring is supported by the data, but the modest absolute improvements and potential confounding factors reduce certainty about general superiority.
- **Low Confidence**: The assertion that smaller models achieve "175x fewer parameters" while outperforming larger models is technically accurate but potentially misleading, as it compares FlanT5-3B to ChatGPT scoring rather than direct model comparisons.

## Next Checks

1. Test the comparative assessment approach on additional NLG tasks (e.g., machine translation, data-to-text generation) to verify whether performance gains generalize beyond summarization and dialogue response evaluation.

2. Systematically evaluate different debiasing methods (symmetric subsets, position swapping, learned bias correction) to determine which most effectively improves correlation while maintaining computational efficiency.

3. Conduct human evaluations comparing LLM pairwise judgments to human pairwise preferences directly, rather than relying on absolute score correlations, to validate whether the LLM judgments reflect actual human preference patterns.