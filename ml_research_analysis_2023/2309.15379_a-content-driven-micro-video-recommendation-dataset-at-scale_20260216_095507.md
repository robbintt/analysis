---
ver: rpa2
title: A Content-Driven Micro-Video Recommendation Dataset at Scale
arxiv_id: '2309.15379'
source_url: https://arxiv.org/abs/2309.15379
tags:
- video
- recommendation
- features
- reve
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MicroLens, a large-scale micro-video recommendation
  dataset consisting of one billion user-item interactions, 34 million users, and
  one million micro-videos. Unlike existing datasets that only provide item IDs or
  pre-extracted features, MicroLens contains rich raw modalities including titles,
  cover images, audio, and full-length videos.
---

# A Content-Driven Micro-Video Recommendation Dataset at Scale

## Quick Facts
- arXiv ID: 2309.15379
- Source URL: https://arxiv.org/abs/2309.15379
- Authors: 
- Reference count: 40
- Key outcome: Introduces MicroLens dataset with 1B interactions and benchmarks VideoRec approach achieving superior performance over ID-based methods

## Executive Summary
This paper introduces MicroLens, a large-scale micro-video recommendation dataset with one billion user-item interactions, 34 million users, and one million micro-videos. The dataset is unique in containing raw multimodal data including titles, cover images, audio, and full-length videos, unlike existing datasets that only provide item IDs or pre-extracted features. The authors propose VideoRec, an end-to-end approach that learns item representations directly from raw video features, achieving superior performance compared to traditional ID-based methods and pre-extracted feature approaches. Their experiments demonstrate that raw video features are crucial for content-driven video recommendation, though a significant semantic gap remains between video understanding tasks and recommendation systems.

## Method Summary
The paper introduces MicroLens, a large-scale micro-video recommendation dataset with rich raw modalities. The proposed VideoRec approach uses end-to-end training to learn item representations from raw video features through video encoders like SlowFast and VideoMAE. The method processes 5 consecutive frames from the video midsection with a 1-frame interval, encodes them through the video encoder, and fuses the resulting features with user interaction sequences using sequential models like SASRec. The system is trained using in-batch softmax loss and evaluated with leave-one-out strategy, comparing against ID-based methods and pre-extracted feature approaches across Hit Ratio and NDCG metrics.

## Key Results
- VideoRec with end-to-end training achieves approximately 2-fold improvement over frozen feature approaches
- Raw video features provide significant benefits over ID-based methods even in warm item settings
- A significant semantic gap exists between video classification performance and recommendation performance

## Why This Works (Mechanism)

### Mechanism 1
End-to-end training of recommender models with video encoders achieves superior performance compared to using pre-extracted frozen features. E2E training allows the video encoder to learn representations specifically optimized for recommendation tasks by incorporating both raw video features and collaborative signals from user-item interactions. The core assumption is that semantic representations learned from video classification tasks are not universally optimal for recommendation tasks, and retraining on recommendation data is necessary.

### Mechanism 2
Raw video features are more effective than pre-extracted frozen features for content-driven video recommendation. Raw video features contain richer information about the video content that can be better captured through end-to-end learning, while pre-extracted features may lose important details or context. The core assumption is that the video classification task has a significant semantic gap with the recommendation task.

### Mechanism 3
VideoRec can potentially replace ID-based recommender systems by using raw video features instead of item IDs. VideoRec uses raw video features as the primary representation for items, eliminating the need for item IDs and their associated limitations (non-shareability, cold-start issues). The core assumption is that video features can capture the same or better information about items compared to item IDs, including popularity and category information.

## Foundational Learning

- Concept: Understanding of micro-videos and their characteristics
  - Why needed here: The paper focuses on micro-video recommendation, which has unique characteristics compared to traditional video or item recommendation
  - Quick check question: What are the typical length and content characteristics of micro-videos?

- Concept: Knowledge of multimodal data processing
  - Why needed here: The dataset contains multiple modalities (text, images, audio, video) that need to be processed and potentially fused for recommendation
  - Quick check question: What are common techniques for processing and fusing multimodal data?

- Concept: Familiarity with end-to-end learning approaches
  - Why needed here: The proposed VideoRec approach uses end-to-end training of both the recommender model and video encoder
  - Quick check question: What are the advantages and challenges of end-to-end learning compared to using pre-trained models?

## Architecture Onboarding

- Component map: User interaction sequence encoder -> Video encoder -> Fusion layer (optional) -> Prediction layer
- Critical path: Input: User interaction sequence and video content -> Processing: Encode user sequence and video content, fuse if necessary -> Output: Predicted relevance scores for candidate videos
- Design tradeoffs:
  - Raw video features vs. pre-extracted features: Raw features provide more information but are computationally expensive to process
  - End-to-end training vs. fixed video encoder: E2E training can optimize for the recommendation task but requires more computational resources
  - Sequential vs. non-sequential models: Sequential models can capture temporal patterns but may be more complex to train
- Failure signatures:
  - Overfitting to the training data: Check for large gaps between training and validation performance
  - Underutilization of video content: Compare performance with and without video features to ensure they are being effectively used
  - Computational bottlenecks: Monitor training time and GPU memory usage
- First 3 experiments:
  1. Compare VideoRec with a baseline that uses only item IDs to verify the importance of video content
  2. Compare VideoRec with a baseline that uses pre-extracted video features to demonstrate the benefits of end-to-end training
  3. Evaluate VideoRec on a cold-start scenario to assess its performance with new items

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of VideoRec compare to IDRec when incorporating additional side features like popularity and tags on MicroLens-100K? The paper mentions investigating the impact of side features like item popularity level (Pop) and tag categories (Tag) on recommendation performance using MicroLens-100K dataset, but only reports that incorporating these features did not clearly improve the algorithm's performance without providing specific quantitative comparisons between VideoRec and IDRec with side features.

### Open Question 2
What is the optimal way to utilize multimodal features for video recommendation, considering the semantic gap between video understanding tasks and recommendation systems? The paper discusses the semantic gap between video understanding tasks and recommendation systems, and the need for specialized research on video understanding technologies for video recommendation tasks, but does not provide a definitive answer on the optimal approach to utilize multimodal features for video recommendation.

### Open Question 3
How does the performance of VideoRec vary across different cold-start scenarios, and what factors contribute to its success in these scenarios? The paper mentions that VideoRec achieves superior performance compared to traditional ID-based methods and pre-extracted feature approaches, and that it has a natural advantage in transfer learning due to the generality of video or visual features, but does not provide a detailed analysis of VideoRec's performance in different cold-start scenarios or identify the specific factors that contribute to its success.

## Limitations
- Scalability concerns due to computational costs of processing raw video features
- Significant semantic gap between video understanding and recommendation tasks persists
- Dataset focuses on single platform, limiting generalizability to other micro-video platforms

## Confidence

**High Confidence** (Well-supported by evidence):
- End-to-end training of video encoders with recommender models improves performance over frozen features
- Raw video features provide significant benefits compared to ID-based methods
- The MicroLens dataset represents a valuable contribution to the research community

**Medium Confidence** (Partially supported):
- VideoRec can fully replace ID-based systems in warm item settings
- The semantic gap between video understanding and recommendation is significant and persistent

**Low Confidence** (Limited evidence):
- The proposed approach scales effectively to production-level systems
- The dataset captures all relevant aspects of micro-video recommendation challenges

## Next Checks

1. **Ablation Study on Video Encoder Architecture**: Conduct experiments systematically removing different components of the video encoder (e.g., temporal vs. spatial features) to identify which aspects contribute most to recommendation performance.

2. **Cross-Platform Generalization Test**: Evaluate VideoRec on datasets from different micro-video platforms to assess whether the learned representations transfer across different content distributions and user behaviors.

3. **Computational Cost Analysis**: Perform detailed benchmarking of training and inference costs for the end-to-end approach versus pre-extracted features to quantify the practical deployment trade-offs.