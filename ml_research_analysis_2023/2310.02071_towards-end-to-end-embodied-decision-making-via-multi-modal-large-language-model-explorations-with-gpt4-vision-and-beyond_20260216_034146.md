---
ver: rpa2
title: 'Towards End-to-End Embodied Decision Making via Multi-modal Large Language
  Model: Explorations with GPT4-Vision and Beyond'
arxiv_id: '2310.02071'
source_url: https://arxiv.org/abs/2310.02071
tags:
- action
- image
- embodied
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces PCA-EVAL, a new benchmark for evaluating
  embodied decision-making in AI agents. The benchmark assesses agents across three
  dimensions: perception, cognition, and action.'
---

# Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond

## Quick Facts
- arXiv ID: 2310.02071
- Source URL: https://arxiv.org/abs/2310.02071
- Reference count: 40
- Key outcome: GPT-4V outperforms open-source models and HOLMES by 26% on PCA-EVAL benchmark

## Executive Summary
This paper addresses the challenge of embodied decision-making in AI agents by introducing PCA-EVAL, a comprehensive benchmark that evaluates agents across perception, cognition, and action dimensions. The authors propose two approaches: End2End evaluation using multimodal large language models (MLLMs) and HOLMES, a multi-agent cooperation framework that enables LLMs to leverage multimodal models and APIs for informed decision-making. Experiments demonstrate that GPT-4V achieves state-of-the-art performance with significant improvements over open-source alternatives, particularly in action accuracy and cognition scores.

## Method Summary
The authors introduce PCA-EVAL, a benchmark for evaluating embodied decision-making across three dimensions: perception, cognition, and action. They propose two evaluation methods: End2End using VLLMs (InstructBLIP, MMICL, GPT-4V) and HOLMES using multi-agent cooperation with LLMs and visual models/APIs. The benchmark contains 300 multimodal multiple-choice questions across autonomous driving, domestic assistance, and game-playing domains. Performance is measured through P-Score, C-Score, and A-Score, with human evaluation for perception and cognition components.

## Key Results
- GPT-4V outperforms open-source models and HOLMES by 26% in action accuracy
- HOLMES framework achieves competitive performance through multi-turn reasoning and API integration
- Human evaluators show reasonable inter-rater reliability (kappa 0.66-0.83) for perception and cognition scoring

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MLLMs can process multimodal information end-to-end without requiring explicit conversion to text, reducing information loss during decision-making.
- Mechanism: The MLLM directly takes in the visual observation and question, processes them through its multimodal understanding capabilities, and generates the answer triplet <description, reasoning, action> in one step.
- Core assumption: The MLLM's training includes sufficient visual understanding and reasoning capabilities to handle the perception-cognition-action chain without intermediate steps.
- Evidence anchors:
  - [abstract]: "MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities."
  - [section]: "Distinct from the MDP-based evaluation that solely focuses on maximizing cumulative rewards, we divide the sequential decision making process into multiple one-step decision problems"
- Break condition: If the MLLM lacks sufficient visual reasoning capabilities or domain-specific knowledge, it may fail to generate appropriate action decisions.

### Mechanism 2
- Claim: The HOLMES framework enables LLMs to leverage multimodal models and APIs for informed decision-making through a detective-game-like process.
- Mechanism: The LLM initiates a search for clues by invoking various multimodal models or APIs, analyzes the discovered clues, and forms subsequent responses until it has sufficient information to propose the final action.
- Core assumption: The LLM can understand API descriptions and correspondingly call different APIs based on the question context.
- Evidence anchors:
  - [abstract]: "HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making."
  - [section]: "This process involves alternating between invoking models or APIs to gather relevant information and analyzing the discovered clues to facilitate informed decision making."
- Break condition: If the LLM cannot properly understand API descriptions or lacks the capability to analyze the gathered information effectively.

### Mechanism 3
- Claim: PCA-EVAL provides a comprehensive evaluation framework that assesses embodied decision-making across three distinct dimensions: perception, cognition, and action.
- Mechanism: Each instance in the benchmark consists of a 6-element tuple that allows for fine-grained diagnosis of the decision-making process, with separate scores for perception, cognition, and action.
- Core assumption: Human evaluators can consistently assess perception and cognition scores, while action scores can be automatically computed.
- Evidence anchors:
  - [abstract]: "PCA-EV AL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action."
  - [section]: "We compute the average kappa correlation coefficient for these evaluators, resulting in 0.66 for the Perception Score and 0.83 for the Cognition Score."
- Break condition: If human evaluators cannot maintain consistent scoring or if action score computation becomes unreliable.

## Foundational Learning

- Concept: Multimodal understanding in large language models
  - Why needed here: The core of this work relies on MLLMs' ability to process and reason over both visual and textual information simultaneously.
  - Quick check question: How do MLLMs differ from traditional LLMs in their approach to handling multimodal inputs?

- Concept: Embodied decision-making evaluation
  - Why needed here: The PCA-EVAL benchmark requires understanding how to evaluate decision-making processes beyond simple success/failure metrics.
  - Quick check question: What are the advantages of evaluating decision-making through perception-cognition-action decomposition versus traditional MDP-based approaches?

- Concept: Multi-agent cooperation frameworks
  - Why needed here: The HOLMES framework relies on coordinating multiple specialized models/APIs through a central LLM to accomplish complex tasks.
  - Quick check question: How does the detective-game-like process in HOLMES differ from traditional multi-agent systems?

## Architecture Onboarding

- Component map:
  - PCA-EVAL Benchmark: Contains instances with images, questions, action candidates, answers, reasons, and key concepts
  - End2End Evaluation: Direct MLLM evaluation with prompts for <description, reasoning, action>
  - HOLMES Framework: LLM + multimodal models/APIs + environment APIs
  - Evaluation Metrics: P-Score, C-Score, A-Score

- Critical path:
  1. Load PCA-EVAL instance
  2. For End2End: Prompt MLLM with image and question
  3. For HOLMES: LLM iteratively calls APIs based on descriptions
  4. Generate answer triplet <description, reasoning, action>
  5. Score against ground truth

- Design tradeoffs:
  - End2End vs HOLMES: End2End is simpler but may lose information; HOLMES is more complex but can leverage specialized tools
  - Open-source vs closed-source models: Performance vs accessibility
  - Human vs automated evaluation: Accuracy vs scalability

- Failure signatures:
  - Low P-Score: MLLM fails to identify key concepts in images
  - Low C-Score: MLLM struggles with reasoning or lacks domain knowledge
  - Low A-Score: MLLM generates inappropriate actions

- First 3 experiments:
  1. Test baseline MLLM performance on PCA-EVAL with simple perception tasks
  2. Evaluate HOLMES framework with a single API type to verify cooperation mechanism
  3. Compare End2End vs HOLMES on a domain where specialized APIs exist

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using MLLMs on the generalizability of embodied agents across diverse environments?
- Basis in paper: [inferred] The paper suggests that MLLMs like GPT4-Vision have the potential to improve embodied decision-making across various domains, but the current PCA-EVAL benchmark only covers three domains.
- Why unresolved: The paper's benchmark is limited to three domains, which may not fully capture the generalizability of MLLMs in diverse environments.
- What evidence would resolve it: Extending the PCA-EVAL benchmark to include more diverse domains and environments, and evaluating the performance of MLLMs in these new settings.

### Open Question 2
- Question: How can the HOLMES framework be further optimized to improve decision-making accuracy and efficiency?
- Basis in paper: [explicit] The paper introduces HOLMES as a multi-agent cooperation framework but acknowledges that it relies on multi-step reasoning, which can lead to the accumulation of reasoning errors.
- Why unresolved: The current HOLMES framework, while effective, may not be the most efficient or accurate method for decision-making in all scenarios.
- What evidence would resolve it: Developing and testing alternative optimization techniques for HOLMES, such as incorporating real-time feedback mechanisms or using more advanced reasoning algorithms.

### Open Question 3
- Question: How can the alignment between agent decisions and human values be improved in embodied decision-making systems?
- Basis in paper: [explicit] The paper discusses instances where the decisions made by the agent contradict human values, highlighting the need for agents to make decisions that are in harmony with human values.
- Why unresolved: The current systems, while effective in making logical decisions, may not always align with human values, which is crucial for real-world applications.
- What evidence would resolve it: Developing and implementing ethical guidelines and value alignment techniques in embodied decision-making systems, and evaluating their impact on decision-making outcomes.

## Limitations
- Exclusive focus on GPT-4V's superior performance without establishing whether the observed gains stem from fundamental architectural advantages or simply access to larger training data
- Human evaluation component may not scale effectively for larger benchmark deployment
- HOLMES framework's reliance on API descriptions for LLM-based tool selection introduces potential brittleness

## Confidence
- **High Confidence**: The PCA-EVAL benchmark design and its three-dimensional evaluation framework are well-specified and reproducible.
- **Medium Confidence**: The comparative performance results between End2End and HOLMES approaches, though limited by lack of detailed implementation specifications.
- **Low Confidence**: Claims about GPT-4V's fundamental superiority in embodied decision-making without understanding the underlying mechanisms.

## Next Checks
1. **Mechanism Isolation Test**: Conduct ablation studies to determine which components of GPT-4V contribute most to its performance advantage - is it visual reasoning, reasoning depth, or simply better training data coverage?
2. **Open-Source Bridge Test**: Implement a knowledge distillation approach where GPT-4V's successful strategies are encoded into open-source models through fine-tuning, testing whether the performance gap can be narrowed.
3. **Robustness Stress Test**: Evaluate all approaches on adversarially modified PCA-EVAL instances where key concepts are partially obscured or presented in unusual contexts to test generalization beyond the benchmark's current scope.