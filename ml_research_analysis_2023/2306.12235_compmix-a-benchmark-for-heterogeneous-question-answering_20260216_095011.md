---
ver: rpa2
title: 'CompMix: A Benchmark for Heterogeneous Question Answering'
arxiv_id: '2306.12235'
source_url: https://arxiv.org/abs/2306.12235
tags:
- sources
- question
- answer
- text
- answering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CompMix is a new benchmark for heterogeneous question answering
  that requires integration of multiple knowledge sources (KB, text, tables, and infoboxes).
  The benchmark contains 9,410 crowdsourced questions across five domains (books,
  movies, music, TV series, soccer) with answers grounded to Wikidata.
---

# CompMix: A Benchmark for Heterogeneous Question Answering

## Quick Facts
- arXiv ID: 2306.12235
- Source URL: https://arxiv.org/abs/2306.12235
- Reference count: 40
- Primary result: A new benchmark requiring integration of KB, text, tables, and infoboxes, where even GPT-3 achieves only 50% precision@1

## Executive Summary
CompMix is a new benchmark for heterogeneous question answering that requires integrating multiple knowledge sources including knowledge bases, text collections, tables, and infoboxes. The benchmark contains 9,410 crowdsourced questions across five domains (books, movies, music, TV series, soccer) with answers grounded to Wikidata. Analysis shows that using all sources improves answer coverage from 80.7% to 86.5%, and 57.8% of questions have answers in two or more sources. Evaluation of four recent QA models reveals that none exceed 45% precision@1, and even GPT-3 only achieves 50% precision@1, highlighting the benchmark's difficulty.

## Method Summary
CompMix provides a dataset of 9,410 crowdsourced questions with answers grounded to Wikidata and annotations indicating which of four knowledge sources (KB, text, tables, infoboxes) contain the answers. The benchmark evaluates QA systems that must retrieve and integrate evidence from these heterogeneous sources. Four recent QA models were evaluated: UniK-QA, Convinse, Explaignn, and GPT-3.0. The evaluation uses precision@1, MRR, and Hit@5 metrics, comparing generated answers against gold answers using case-insensitive exact match for Wikidata IDs or plaintext labels.

## Key Results
- Answer coverage improves from 80.7% (KB alone) to 86.5% when using all four knowledge sources
- 57.8% of questions have answers in two or more sources, demonstrating answer redundancy
- No evaluated model exceeds 45% precision@1, with GPT-3 achieving only 50% precision@1
- Long-tail entities pose a significant challenge for LLM-based methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Heterogeneous QA requires combining multiple knowledge sources to improve answer coverage
- Mechanism: The benchmark evaluates systems that can retrieve and integrate evidence from KB, text, tables, and infoboxes to answer questions
- Core assumption: Each source has unique information that complements other sources
- Evidence anchors:
  - [abstract] "By jointly considering several sources like a knowledge base (KB), a text collection, and tables from the web, QA systems can enhance their answer coverage and confidence."
  - [section 3.1] "By including all sources, the answer coverage goes up to about 87%."
  - [corpus] Found 25 related papers discussing heterogeneous QA approaches, average neighbor FMR=0.457
- Break condition: If a single source becomes complete enough to cover most questions, the need for heterogeneous integration diminishes

### Mechanism 2
- Claim: Answer redundancy across sources can boost confidence in predictions
- Mechanism: When multiple sources contain the same answer, systems can use this redundancy to validate and strengthen their predictions
- Core assumption: Multiple sources providing the same answer increases reliability
- Evidence anchors:
  - [abstract] "Answer redundancy is also present, with 57.8% of questions having answers in two or more sources."
  - [section 3.2] "For a substantial proportion of questions, the answer is located in two (≃ 17%) or three (≃ 34%), out of four, sources."
  - [corpus] UniHGKR paper discusses unified instruction-aware heterogeneous knowledge retrievers
- Break condition: If sources frequently contradict each other, redundancy may reduce rather than increase confidence

### Mechanism 3
- Claim: Complex questions require multiple reasoning steps across heterogeneous sources
- Mechanism: Questions with joins, temporal conditions, and aggregations need evidence from multiple sources to be answered correctly
- Core assumption: Complex queries cannot be answered by a single source alone
- Evidence anchors:
  - [abstract] "CompMix has a total of 9,410 questions, and features several complex intents like joins and temporal conditions."
  - [section 2.2] "CompMix has a total of 9,410 questions, split into train set (4,966), development set (1,680), and test set (2,764)."
  - [corpus] SPAGHETTI paper discusses open-domain QA from heterogeneous data sources
- Break condition: If questions become simpler or more straightforward, requiring only single-source retrieval

## Foundational Learning

- Concept: Knowledge source complementarity
  - Why needed here: Understanding why different sources (KB, text, tables, infoboxes) are needed and how they complement each other
  - Quick check question: If a question asks for a specific date that's missing from the KB but present in a Wikipedia infobox, which source should the system prioritize?

- Concept: Answer redundancy exploitation
  - Why needed here: Learning how to use multiple matching answers across sources to increase prediction confidence
  - Quick check question: If three sources all contain the same answer to a question, how should a QA system weight this versus a single-source answer?

- Concept: Multi-hop reasoning
  - Why needed here: Understanding how to combine evidence from multiple sources to answer complex questions requiring joins or aggregations
  - Quick check question: How would you answer a question that requires first finding a list of winners from a KB, then checking text sources for goal differences?

## Architecture Onboarding

- Component map: Question -> Entity disambiguation -> Multi-source retrieval -> Evidence fusion -> Answer generation/validation
- Critical path: Question → Entity disambiguation → Multi-source retrieval → Evidence fusion → Answer generation/validation
- Design tradeoffs: Precision vs recall in retrieval, complexity of integration logic vs performance, single vs multi-stage reasoning approaches
- Failure signatures: Low coverage despite multiple sources (retrieval failure), correct answers present but not selected (ranking failure), correct answers across sources not recognized as matching (validation failure)
- First 3 experiments:
  1. Evaluate baseline BM25 retriever on each source individually to establish coverage baselines
  2. Implement simple majority voting across sources for answer selection and measure impact on P@1
  3. Test FiD reader on concatenated evidence from all sources vs evidence from best single source

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the limitations of using GPT-3 for answering heterogeneous questions, and how can these limitations be addressed?
- Basis in paper: explicit
- Why unresolved: The paper mentions that GPT-3 only achieves 50% precision@1 on the CompMix benchmark, indicating that there is still room for improvement in leveraging heterogeneous sources. However, the paper does not provide a detailed analysis of the specific limitations of GPT-3 or potential strategies to address them.
- What evidence would resolve it: A comprehensive study comparing GPT-3 with other QA models on heterogeneous benchmarks, along with an analysis of the types of questions GPT-3 struggles with and potential improvements.

### Open Question 2
- Question: How can heterogeneous QA models be improved to better handle long-tail entities and emerging entities in real-world scenarios?
- Basis in paper: explicit
- Why unresolved: The paper highlights the presence of a significant fraction of questions with long-tail entities in CompMix, which is a major vulnerability of LLM methods. However, it does not provide specific insights into how QA models can be enhanced to handle such entities more effectively.
- What evidence would resolve it: Research demonstrating improved performance of QA models on benchmarks with long-tail entities, along with techniques or architectures specifically designed to handle such cases.

### Open Question 3
- Question: What are the challenges and potential solutions for integrating heterogeneous sources in QA systems, beyond the traditional retriever-reader pipeline approach?
- Basis in paper: explicit
- Why unresolved: The paper evaluates several recent QA models that follow a retriever-reader pipeline, but it does not explore alternative approaches or discuss the challenges of integrating heterogeneous sources beyond this framework.
- What evidence would resolve it: Studies comparing different integration strategies for heterogeneous sources, along with experimental results demonstrating the effectiveness of novel approaches.

### Open Question 4
- Question: How can QA systems be designed to effectively exploit answer redundancy across multiple sources, and what are the benefits and limitations of such approaches?
- Basis in paper: explicit
- Why unresolved: The paper mentions that a substantial proportion of questions in CompMix have answer redundancy across sources, but it does not provide insights into how QA systems can leverage this redundancy to improve performance or discuss the potential drawbacks of such approaches.
- What evidence would resolve it: Research demonstrating improved performance of QA systems by exploiting answer redundancy, along with a thorough analysis of the benefits and limitations of such approaches.

## Limitations
- The exact implementation details of the heterogeneous QA models (UniK-QA, Convinse, Explaignn) are not fully specified, making it difficult to assess whether the performance gap is due to model limitations or implementation differences
- The paper does not provide ablation studies on individual source contributions, making it unclear which sources are most critical for performance improvements
- The dataset construction process may have introduced biases in question complexity or source selection that affect generalizability

## Confidence
**High confidence**: The benchmark's value proposition (addressing heterogeneous QA with multiple knowledge sources) and the general performance trends across models are well-supported by the evidence presented.

**Medium confidence**: The specific performance numbers and comparisons between models are reasonably reliable but may vary with different implementations or evaluation settings.

**Low confidence**: The exact contribution of each knowledge source to answer coverage and the optimal integration strategies remain uncertain without further ablation studies.

## Next Checks
1. **Source ablation study**: Run experiments with each knowledge source individually and in combinations to quantify the marginal contribution of each source type to answer coverage and precision.

2. **Implementation verification**: Compare the reported model performances with baseline implementations using standard configurations to ensure the evaluation methodology is sound.

3. **Cross-dataset validation**: Test the CompMix models on established heterogeneous QA datasets to assess whether the performance patterns generalize beyond the CompMix benchmark.