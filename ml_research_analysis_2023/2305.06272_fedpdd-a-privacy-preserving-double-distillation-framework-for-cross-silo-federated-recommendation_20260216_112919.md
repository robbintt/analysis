---
ver: rpa2
title: 'FedPDD: A Privacy-preserving Double Distillation Framework for Cross-silo
  Federated Recommendation'
arxiv_id: '2305.06272'
source_url: https://arxiv.org/abs/2305.06272
tags:
- local
- knowledge
- data
- federated
- fedpdd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FedPDD, a privacy-preserving double distillation
  framework for cross-silo federated recommendation. The method addresses the challenge
  of limited overlapped users across platforms by enabling local models to learn both
  explicit knowledge from other parties and implicit knowledge from their own past
  predictions through a double distillation strategy.
---

# FedPDD: A Privacy-preserving Double Distillation Framework for Cross-silo Federated Recommendation

## Quick Facts
- arXiv ID: 2305.06272
- Source URL: https://arxiv.org/abs/2305.06272
- Reference count: 38
- Key outcome: FedPDD improves local model performance by up to 3.94% and outperforms state-of-the-art approaches by up to 3.98% on HetRec-MovieLens and Criteo datasets

## Executive Summary
This paper introduces FedPDD, a privacy-preserving double distillation framework for cross-silo federated recommendation. The framework addresses the challenge of limited overlapped users across platforms by enabling local models to learn both explicit knowledge from other parties and implicit knowledge from their own past predictions through a double distillation strategy. The approach employs an offline training scheme and differential privacy to reduce communication costs and protect transmitted information. Experiments show that FedPDD significantly improves local model performance, especially when overlapped data is limited.

## Method Summary
FedPDD is a double distillation framework that enables cross-silo federated recommendation without requiring raw data sharing. The method uses an offline training scheme where local models are pretrained independently, then learn from both explicit ensemble knowledge (from other parties on overlapped data) and implicit self-knowledge (from their own past predictions). Differential privacy is applied to perturbed model outputs to ensure privacy guarantees. The framework is specifically designed for scenarios where overlapped users between parties are limited, a common challenge in real-world cross-silo settings.

## Key Results
- FedPDD improves local model performance by up to 3.94% compared to standalone training
- Outperforms state-of-the-art federated learning approaches by up to 3.98% on HetRec-MovieLens and Criteo datasets
- Maintains superior performance when overlapped data is limited (α=0.01), where other methods show significant degradation
- Provides formal (ε,δ)-differential privacy guarantees with minimal accuracy trade-off (only 1.53% drop for local models)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Double distillation enables local models to learn both explicit knowledge from the other party and implicit knowledge from their own past predictions, improving performance when overlapped data is limited.
- Mechanism: The framework maintains a historical best model for each party. In each round, the current model distills knowledge from this best model (self-distillation) while also learning from the ensemble of both parties' predictions on overlapped data (explicit knowledge transfer). This creates multiple informative sources for training.
- Core assumption: Past model predictions contain valuable information that can be distilled to improve future model performance.
- Evidence anchors:
  - [abstract]: "our double distillation strategy enables local models to learn not only explicit knowledge from the other party but also implicit knowledge from its past predictions"
  - [section III-C]: "We propose a self-distillation strategy to enable the local model to distill knowledge from itself. That is to say, the teacher is the student model itself."
  - [corpus]: Weak evidence - corpus papers focus on general federated learning or privacy-preserving methods but don't specifically address double distillation strategies.
- Break condition: If the self-distillation temperature is too low, the model may overfit to its past predictions. If too high, the regularization effect is lost.

### Mechanism 2
- Claim: Offline training with only model output transmission reduces communication costs and privacy leakage risk compared to online gradient-based methods.
- Mechanism: The framework performs local training offline and only communicates model outputs (predictions) during the federated ensemble stage. This eliminates the need for frequent gradient exchanges and reduces the amount of information exposed to the server.
- Core assumption: Model outputs contain less sensitive information than gradients or raw parameters.
- Evidence anchors:
  - [abstract]: "to ensure privacy and high efficiency, we employ an offline training scheme to reduce communication needs and privacy leakage risk"
  - [section III-F]: "We use an offline training scheme due to efficiency and privacy concerns. Offline training can largely reduce communication needs and therefore limit the exposure of private information to the server."
  - [section III-H]: "The overall communication cost is 2mn|Dc|. Our offline training strategy only has O(1) communication rounds"
- Break condition: If model outputs are too predictable or can be reverse-engineered, they may still leak private information despite being less sensitive than gradients.

### Mechanism 3
- Claim: Differential privacy applied to perturbed ensemble logits provides formal privacy guarantees while maintaining model performance.
- Mechanism: The framework adds Gaussian noise to local model outputs before aggregation. This perturbation ensures that individual contributions cannot be easily distinguished, providing (ε,δ)-differential privacy guarantees.
- Core assumption: Adding calibrated noise to model outputs can protect privacy while maintaining sufficient utility for learning.
- Evidence anchors:
  - [abstract]: "we adopt differential privacy to further protect the transmitted information"
  - [section III-E]: "we perturb the local output logits with a Gaussian noise to ensure a higher privacy guarantee"
  - [section III-I]: Formal privacy analysis using Gaussian mechanism with definitions of (ε,δ)-DP and sensitivity calculations
- Break condition: If privacy budget ε is too small, added noise degrades model performance significantly. If too large, privacy guarantees become insufficient.

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: The framework relies on transferring knowledge between models through soft target distributions rather than raw data. Understanding KL divergence loss and temperature scaling is essential for implementing the distillation components.
  - Quick check question: What is the mathematical difference between hard labels and soft targets in knowledge distillation?

- Concept: Differential Privacy
  - Why needed here: The framework applies differential privacy to protect transmitted model outputs. Understanding sensitivity, privacy budget, and the composition theorem is necessary for implementing the privacy protection mechanism.
  - Quick check question: How does the l2-sensitivity of a function affect the amount of noise needed for differential privacy?

- Concept: Federated Learning Architecture
  - Why needed here: The framework operates in a cross-silo setting with specific communication patterns and privacy constraints. Understanding the difference between cross-device and cross-silo settings, and the implications for model aggregation, is crucial.
  - Quick check question: What are the key differences between cross-device and cross-silo federated learning in terms of participant characteristics and communication patterns?

## Architecture Onboarding

- Component map: Local models (DeepFM) -> Offline training -> Ensemble server (perturbation + aggregation) -> Double distillation loss -> Updated local models

- Critical path:
  1. Pretrain local models on private data
  2. Generate predictions on overlapped data
  3. Perturb predictions with differential privacy
  4. Aggregate perturbed predictions on server
  5. Distribute ensemble knowledge back to parties
  6. Perform local training with double distillation loss
  7. Update historical best models if improved

- Design tradeoffs:
  - Privacy vs. performance: Higher privacy (smaller ε) requires more noise, degrading accuracy
  - Communication vs. efficiency: Offline training reduces communication but may slow convergence
  - Model complexity vs. generalization: Double distillation adds complexity but improves performance with limited overlapped data

- Failure signatures:
  - Performance degradation when privacy budget is too small
  - Slow convergence if communication rounds are insufficient
  - Instability if self-distillation temperature is poorly tuned
  - Privacy leakage if noise variance is inadequate

- First 3 experiments:
  1. Test local model performance with and without self-distillation to isolate its contribution
  2. Vary the privacy budget ε to find the optimal tradeoff between privacy and accuracy
  3. Compare communication efficiency between offline and online training strategies using different numbers of rounds

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the results raise several important unresolved issues regarding performance with extremely limited overlapped data, applicability to different backbone models, and optimal privacy-utility tradeoffs.

## Limitations
- Only evaluated on two datasets (HetRec-MovieLens and Criteo), limiting generalizability
- Privacy analysis assumes honest-but-curious server behavior, not considering malicious attacks
- Requires a small overlapped dataset to function, which may not always be available in practice

## Confidence

- High confidence: The double distillation mechanism's effectiveness with limited overlapped data (supported by ablation studies and comparison with baselines)
- Medium confidence: The privacy guarantees from differential privacy (theoretical analysis provided but practical attacks not evaluated)
- Low confidence: Scalability to more than two parties and real-world deployment scenarios (only evaluated in two-party setting)

## Next Checks

1. Evaluate the framework on additional recommendation datasets with varying degrees of data heterogeneity to test generalizability
2. Conduct robustness analysis against different privacy attack scenarios, including membership inference and model inversion attacks
3. Test the framework's performance and communication efficiency when scaled to three or more parties in the cross-silo setting