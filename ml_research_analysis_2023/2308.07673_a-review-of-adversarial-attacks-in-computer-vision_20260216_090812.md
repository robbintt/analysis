---
ver: rpa2
title: A Review of Adversarial Attacks in Computer Vision
arxiv_id: '2308.07673'
source_url: https://arxiv.org/abs/2308.07673
tags:
- adversarial
- attack
- attacks
- target
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reviews adversarial attacks in computer vision, focusing
  on how deep neural networks can be misled by imperceptible perturbations. It categorizes
  attacks into white-box (full model access) and black-box (limited query/output access)
  settings, and further divides them by purpose (targeted vs.
---

# A Review of Adversarial Attacks in Computer Vision

## Quick Facts
- arXiv ID: 2308.07673
- Source URL: https://arxiv.org/abs/2308.07673
- Reference count: 40
- Primary result: Reviews adversarial attack methods in computer vision, categorizing them by access level and task type, and highlights vulnerabilities in safety-critical applications.

## Executive Summary
This paper provides a comprehensive review of adversarial attacks in computer vision, focusing on how deep neural networks can be deceived by imperceptible perturbations. It categorizes attacks into white-box and black-box settings, further dividing them by purpose and generation method. The review covers key attack methods like FGSM, PGD, C&W, and MI-FGSM, and extends the discussion to object detection and semantic segmentation tasks. The paper emphasizes the transferability of adversarial examples across models and tasks, highlighting the security risks in safety-critical applications like autonomous driving.

## Method Summary
The paper synthesizes existing literature on adversarial attack methodologies, organizing them by attack setting (white-box vs. black-box), purpose (targeted vs. non-targeted), and generation approach (optimization-based, generative, universal perturbations). It describes the mathematical foundations of key attack algorithms and their application to various computer vision tasks. The review does not introduce new attack methods but provides a structured overview of the state-of-the-art in adversarial attacks.

## Key Results
- Adversarial attacks can generate imperceptible perturbations that cause DNN misclassification in safety-critical applications.
- Transferability of adversarial examples across models and tasks poses broad security risks.
- Regression-based tasks like semantic segmentation are more vulnerable to adversarial attacks than classification tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial examples can be generated with imperceptible perturbations that fool DNNs in safety-critical applications like autonomous driving.
- Mechanism: Small input perturbations aligned with the gradient of the loss function can cause misclassification. Methods like FGSM and PGD iteratively adjust inputs to maximize the loss.
- Core assumption: DNNs are locally linear around the input, allowing gradient-based perturbations to effectively cross decision boundaries.
- Evidence anchors:
  - [abstract] "adversarial attacks can be invisible to human eyes, but can lead to DNN misclassification"
  - [section] "In FGSM the amount of change in the adversarial perturbation is aligned with the direction of change in the gradient of the model loss"
  - [corpus] "Deep learning techniques have enabled vast improvements in computer vision technologies. Nevertheless, these models are vulnerable to adversarial patch attacks which catastrophically impair performance."

### Mechanism 2
- Claim: Adversarial attacks exhibit transferability across models and tasks, posing a broad security risk.
- Mechanism: Adversarial examples generated for one model can often fool other models due to shared features and decision boundaries learned from similar data distributions.
- Core assumption: Different models trained on similar datasets learn similar feature representations, allowing adversarial perturbations to generalize.
- Evidence anchors:
  - [abstract] "adversarial attacks often exhibits transferability between deep learning and machine learning models"
  - [section] "Transfer-based attacks often require the use of a white-box surrogate model to create adversarial perturbations"
  - [corpus] "Adversarial examples pose significant challenges to Machine Learning (ML) systems and especially Deep Neural Networks (DNNs)"

### Mechanism 3
- Claim: Regression-based tasks like semantic segmentation are more vulnerable to adversarial attacks than classification tasks.
- Mechanism: In regression tasks, predictions change continuously with input changes, lacking the threshold behavior of classification, making them more sensitive to small perturbations.
- Core assumption: The absence of discrete decision boundaries in regression tasks increases susceptibility to adversarial perturbations.
- Evidence anchors:
  - [abstract] "regression-based tasks like segmentation are particularly vulnerable"
  - [section] "Object detection models tend to be more vulnerable because they need to predict the class and location, which is equivalent to both classification and regression tasks"
  - [corpus] "Adversarial attacks pose significant challenges to Machine Learning (ML) systems and especially Deep Neural Networks (DNNs)"

## Foundational Learning

- Concept: Gradient-based optimization for generating adversarial examples
  - Why needed here: Core to methods like FGSM, PGD, and their variants; understanding how gradients are used to craft perturbations is essential.
  - Quick check question: How does the sign of the gradient influence the direction of the perturbation in FGSM?

- Concept: Transferability of adversarial examples across models
  - Why needed here: Critical for black-box attacks and understanding the broader security implications; affects how attacks are designed and defended against.
  - Quick check question: What factors influence the transferability of adversarial examples between different neural network architectures?

- Concept: Differences between classification and regression tasks in adversarial vulnerability
  - Why needed here: Explains why certain tasks like semantic segmentation are more susceptible to attacks; informs defense strategies.
  - Quick check question: Why might a regression task like object detection be more vulnerable to adversarial attacks than a classification task?

## Architecture Onboarding

- Component map: Input preprocessing -> Model inference -> Loss computation -> Gradient calculation -> Perturbation application -> Output evaluation
- Critical path:
  1. Input image → preprocessing
  2. Forward pass through DNN
  3. Compute loss with respect to target
  4. Backward pass to obtain gradients
  5. Generate perturbation (e.g., FGSM: x' = x + ε * sign(∇x L))
  6. Apply perturbation and clip to valid range
  7. Evaluate attack success
- Design tradeoffs:
  - Perturbation size vs. imperceptibility (L∞ norm constraint)
  - Single-step vs. iterative methods (speed vs. effectiveness)
  - Targeted vs. non-targeted attacks (difficulty vs. impact)
  - White-box vs. black-box access (information vs. practicality)
- Failure signatures:
  - Attack fails due to small perturbation budget
  - Transferability fails when surrogate model is too different
  - Overfitting to surrogate model reduces black-box success
  - Defenses like adversarial training mitigate the attack
- First 3 experiments:
  1. Implement FGSM on a simple CNN on CIFAR-10; measure success rate vs. ε.
  2. Generate adversarial examples on one model, test transferability to another architecture.
  3. Compare attack success on classification vs. segmentation models with same perturbation budget.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are universal adversarial perturbations (UAPs) on semantic segmentation tasks compared to image classification tasks?
- Basis in paper: [explicit] The paper discusses the effectiveness of UAPs on various computer vision tasks, including semantic segmentation, but does not provide a direct comparison of their effectiveness across different tasks.
- Why unresolved: While the paper mentions the existence of UAPs and their potential to deceive multiple models across different tasks, it does not quantify or compare their effectiveness specifically for semantic segmentation versus image classification.
- What evidence would resolve it: Comparative studies measuring the attack success rates of UAPs on semantic segmentation models versus image classification models under similar conditions.

### Open Question 2
- Question: What are the long-term impacts of adversarial training on model robustness and performance in real-world scenarios?
- Basis in paper: [inferred] The paper reviews various adversarial attack methods and mentions the need for robust models, implying the importance of adversarial training, but does not discuss its long-term effects.
- Why unresolved: Adversarial training is a common defense strategy, but its long-term impacts on model robustness and performance in practical applications are not well-documented.
- What evidence would resolve it: Longitudinal studies tracking model performance and robustness over time in real-world deployments, comparing models with and without adversarial training.

### Open Question 3
- Question: How can generative adversarial networks (GANs) be further optimized to create more effective adversarial perturbations?
- Basis in paper: [explicit] The paper discusses the use of GANs for generating adversarial perturbations and mentions their potential for creating diverse and effective attacks.
- Why unresolved: While GANs are highlighted as a promising method for generating adversarial perturbations, the paper does not explore potential optimizations or improvements to enhance their effectiveness.
- What evidence would resolve it: Research demonstrating improved GAN architectures or training techniques that result in more effective and transferable adversarial perturbations.

### Open Question 4
- Question: What are the implications of adversarial attacks on the safety and reliability of autonomous driving systems?
- Basis in paper: [explicit] The paper emphasizes the importance of understanding adversarial attacks in safety-critical scenarios like autonomous driving, but does not delve into specific implications.
- Why unresolved: While the paper acknowledges the relevance of adversarial attacks to autonomous driving, it does not explore the specific safety and reliability implications in detail.
- What evidence would resolve it: Case studies or simulations demonstrating the potential impact of adversarial attacks on autonomous driving systems, including safety risks and mitigation strategies.

## Limitations
- The review relies on existing literature without providing novel experimental validation.
- Claims about transferability across tasks are supported by citations but lack systematic empirical analysis within this paper.
- The paper does not provide quantitative comparisons of attack success rates across different task types under controlled conditions.

## Confidence

- **High Confidence:** The categorization of attack methods (white-box vs. black-box, targeted vs. non-targeted) and the description of core attack algorithms (FGSM, PGD, C&W) are well-established in the literature and accurately represented.
- **Medium Confidence:** Claims about the relative vulnerability of regression-based tasks versus classification tasks are supported by citations but would benefit from direct experimental comparison within the paper.
- **Medium Confidence:** The discussion of transferability across models and tasks is theoretically sound but lacks comprehensive empirical validation specific to this review.

## Next Checks
1. **Quantitative Task Vulnerability Comparison:** Conduct controlled experiments comparing adversarial attack success rates on classification, object detection, and semantic segmentation tasks using identical perturbation budgets and attack methods.

2. **Transferability Analysis:** Systematically evaluate the transferability of adversarial examples across different model architectures (CNN, ResNet, Vision Transformer) and tasks, measuring success rates when attacking surrogate versus target models.

3. **Real-world Attack Simulation:** Implement black-box attacks on deployed vision systems (e.g., public object detection APIs) to validate the practical effectiveness of the reviewed attack methodologies beyond academic datasets.