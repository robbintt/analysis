---
ver: rpa2
title: Building Variable-sized Models via Learngene Pool
arxiv_id: '2312.05743'
source_url: https://arxiv.org/abs/2312.05743
tags:
- learngene
- pool
- auxiliary
- sn-net
- ancestry
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method called Learngene Pool to build
  variable-sized models by distilling a large pre-trained model into smaller models,
  and then using the blocks of the smaller models as learngene instances to construct
  a learngene pool. By stitching the learngene instances in the pool, Learngene Pool
  can generate models of varying sizes to satisfy diverse resource constraints.
---

# Building Variable-sized Models via Learngene Pool

## Quick Facts
- arXiv ID: 2312.05743
- Source URL: https://arxiv.org/abs/2312.05743
- Reference count: 4
- Primary result: Learngene Pool achieves 77.42% accuracy on ImageNet-1K with 65.03M parameters

## Executive Summary
Learngene Pool is a novel method for building variable-sized models by distilling a large pre-trained model into smaller models, then using the blocks of these smaller models as reusable learngene instances. By stitching these instances together, Learngene Pool can generate models of varying sizes to satisfy diverse resource constraints. The method reduces storage overhead compared to SN-Net by using a single distilled pool instead of multiple large anchors, and can construct smaller models by reducing block output dimensions during distillation.

## Method Summary
Learngene Pool distills a large pre-trained model (DeiT-Base) into smaller auxiliary models with fewer blocks and lower output dimensions. The blocks from these auxiliary models become learngene instances in a pool. During model construction, these instances are stitched together with learnable transformation matrices that were pre-trained during distillation, reducing the need for costly least-squares initialization. The method uses prediction-layer-based, block-based, and attention-based distillation losses, and fine-tunes the constructed models on ImageNet-1K for 50 epochs.

## Key Results
- Achieves 77.42% accuracy on ImageNet-1K with 65.03M parameters
- Can construct models smaller than SN-Net's smallest anchor (down to 3.05M parameters)
- Consumes fewer storage resources than SN-Net by using a single distilled pool

## Why This Works (Mechanism)

### Mechanism 1
Learngene Pool reduces storage overhead by distilling one large model into a single pool of small, reusable blocks. Instead of storing multiple pre-trained anchors, it distills the large ancestry model into auxiliary models with fewer blocks and lower output dimensions. Each block becomes a "learngene instance" in the pool. During model construction, these instances are stitched together with learnable transformation matrices that were pre-trained during distillation, reducing the need for costly least-squares initialization.

### Mechanism 2
Learngene Pool can construct models smaller than the smallest SN-Net anchor by reducing the output dimensions of blocks in auxiliary models. By reducing block output dimensions while preserving essential features via distillation, Learngene Pool can create very small instances. Stitching these smaller instances allows building models with far fewer parameters than the smallest available anchor in SN-Net.

### Mechanism 3
Initializing stitch layers with transformation matrices learned during distillation improves performance over unlearned least-squares initialization. During distillation, transformation matrices align feature dimensions between ancestry and auxiliary models. These matrices are reused to initialize stitch layers between learngene instances, providing a better starting point than SN-Net's least-squares method.

## Foundational Learning

- Concept: Knowledge distillation
  - Why needed here: Learngene Pool relies on distilling a large model into smaller auxiliary models to extract essential knowledge into compact learngene instances.
  - Quick check question: What are the three types of distillation losses used in Learngene Pool?

- Concept: Model stitching
  - Why needed here: The method builds variable-sized models by stitching together blocks from the learngene pool, similar to SN-Net but with different initialization.
  - Quick check question: How does Learngene Pool's stitching initialization differ from SN-Net's?

- Concept: Variable model sizing
  - Why needed here: The goal is to efficiently generate models of different sizes to meet diverse resource constraints without training each from scratch.
  - Quick check question: What is the smallest model size Learngene Pool can build compared to SN-Net?

## Architecture Onboarding

- Component map: Ancestry model (DeiT-Base) → Auxiliary models (reduced blocks/dimensions) → Learngene instances (blocks) → Learngene pool (instances + stitch layers) → Descendant models (stitched instances)
- Critical path: Distillation of ancestry to auxiliary models → Extraction of learngene instances → Construction of learngene pool → Stitching instances into descendant models → Fine-tuning
- Design tradeoffs: Fewer instances in pool save storage but may limit model diversity; more instances increase storage but allow finer granularity in model sizing.
- Failure signatures: Poor accuracy if distillation loses too much information; stitching failures if transformation matrices do not align dimensions well; inability to build sufficiently small models if auxiliary models are not small enough.
- First 3 experiments:
  1. Train auxiliary models with 6 blocks and verify distillation loss decreases.
  2. Build a small descendant model (e.g., 3M params) and evaluate accuracy vs SN-Net.
  3. Compare performance of stitch layers initialized with transformation matrices vs least-squares method.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Learngene Pool compare to other model compression techniques like pruning or quantization when applied to resource-constrained devices? The paper focuses on Learngene Pool as an alternative to SN-Net and discusses its advantages in terms of storage resource consumption and ability to build smaller models, but does not directly compare it to other model compression techniques.

### Open Question 2
How does the choice of the ancestry model impact the performance of Learngene Pool? The paper mentions that DeiT-Base is used as the ancestry model in the experiments, but does not explore the impact of using different ancestry models.

### Open Question 3
How does the performance of Learngene Pool scale with the number of learngene instances in the pool? The paper mentions that Learngene Pool can construct models with varying numbers of learngene instances, but does not explore how the performance scales with the number of instances.

## Limitations

- Limited empirical evidence for storage efficiency improvements beyond comparisons with SN-Net
- No ablation studies on the impact of transformation matrix initialization
- Generalizability beyond image classification on ImageNet-1K remains untested

## Confidence

- High Confidence: Learngene Pool achieves higher accuracy than SN-Net on comparable model sizes on ImageNet-1K
- Medium Confidence: Learngene Pool reduces storage overhead by using one distilled pool instead of multiple large anchors
- Medium Confidence: Learngene Pool can build smaller models than SN-Net's smallest anchor
- Low Confidence: Transformation matrix initialization significantly improves stitching performance over least-squares

## Next Checks

1. Measure and compare the actual storage footprint of Learngene Pool against SN-Net to quantify storage efficiency improvements.
2. Train descendant models with stitch layers initialized by least-squares versus transformation matrices to quantify the initialization benefit.
3. Evaluate the accuracy of Learngene Pool's smallest models (e.g., 3M parameters) and compare against training tiny models from scratch or using other compression methods.