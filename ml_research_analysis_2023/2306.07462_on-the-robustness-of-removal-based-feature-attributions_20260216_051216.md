---
ver: rpa2
title: On the Robustness of Removal-Based Feature Attributions
arxiv_id: '2306.07462'
source_url: https://arxiv.org/abs/2306.07462
tags:
- feature
- attributions
- removal
- input
- conditional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive theoretical and empirical analysis
  of the robustness of removal-based feature attribution methods, such as LIME, Shapley
  values, and Banzhaf values, under input and model perturbations. The authors derive
  upper bounds for the difference in attributions under both types of perturbations,
  showing that these bounds depend on the model's Lipschitz regularity and the feature
  removal technique used.
---

# On the Robustness of Removal-Based Feature Attributions

## Quick Facts
- arXiv ID: 2306.07462
- Source URL: https://arxiv.org/abs/2306.07462
- Reference count: 40
- This paper provides a comprehensive theoretical and empirical analysis of the robustness of removal-based feature attribution methods under input and model perturbations.

## Executive Summary
This paper presents a unified theoretical framework for understanding the robustness of removal-based feature attribution methods such as LIME, Shapley values, and Banzhaf values. The authors derive upper bounds for attribution differences under both input and model perturbations, showing these bounds depend on the model's Lipschitz regularity and the specific feature removal technique used. The theoretical results are validated through experiments on synthetic and real-world datasets, demonstrating that weight decay regularization improves attribution robustness and that removal-based methods pass the model randomization sanity check.

## Method Summary
The authors analyze removal-based feature attribution methods by deriving theoretical bounds on attribution differences under input and model perturbations. They examine three feature removal approaches (baseline, marginal, conditional) and how each affects attribution robustness. The theoretical framework uses Lipschitz continuity to bound input perturbation effects and functional distances to bound model perturbation effects. Attribution methods are represented as linear operators on subset predictions, with operator norms determining their robustness properties. The theory is validated through experiments on synthetic Gaussian data, UCI white wine quality dataset, and MNIST images using logistic regression and neural network models with varying weight decay regularization.

## Key Results
- Attribution differences under input perturbations are bounded by the model's Lipschitz constant and feature removal approach
- Attribution differences under model perturbations are proportional to the functional distance between models
- Banzhaf summary technique achieves better robustness than Shapley values due to lower matrix operator norms
- Weight decay regularization improves attribution robustness by enhancing model Lipschitz regularity
- Removal-based attributions pass the model randomization sanity check, unlike gradient-based methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Removal-based attributions are Lipschitz continuous in input space, with Lipschitz constant controlled by model smoothness and feature removal approach.
- **Mechanism**: When removing features, the prediction function f(x_S) inherits Lipschitz continuity from the original model f. For baseline/marginal removal, f(x_S) is L-Lipschitz. For conditional removal, f(x_S) is (L + 2BM)-Lipschitz, where M bounds how quickly the conditional distribution p(x̄_S|x_S) can change.
- **Core assumption**: The model f is globally L-Lipschitz continuous, and for conditional removal, the total variation distance between conditional distributions changes smoothly with observed features (Assumption 3).
- **Evidence anchors**:
  - [abstract]: "The authors derive upper bounds for the difference in attributions under both types of perturbations, showing that these bounds depend on the model's Lipschitz regularity"
  - [section 3.1]: Lemmas 1, 2, 3 provide formal proofs for baseline/marginal and conditional removal cases
  - [corpus]: Weak evidence - corpus papers focus on gradient-based attributions, not removal-based Lipschitz properties
- **Break condition**: If the model is not Lipschitz continuous (e.g., ReLU networks with large weights), or if the conditional distribution changes rapidly (large M), the Lipschitz constant becomes large and attributions lose robustness.

### Mechanism 2
- **Claim**: Attribution robustness to model perturbations depends on functional distance between models and feature removal approach.
- **Mechanism**: The difference in attributions is proportional to the functional distance ∥f - f'∥ between models. For baseline/marginal removal, this is ∥f - f'∥∞. For conditional removal, it's ∥f - f'∥∞,X (distance on data manifold). This captures how attributions remain similar when models are close in function space.
- **Core assumption**: Small changes to model parameters result in small changes to model predictions (Lipschitz continuity in function space).
- **Evidence anchors**:
  - [abstract]: "the bounds show that the difference in attributions is proportional to the functional distance between models for model perturbations"
  - [section 3.2]: Lemmas 4, 5, 6 prove bounds for baseline/marginal and conditional removal cases
  - [corpus]: Weak evidence - corpus papers focus on gradient-based attributions and adversarial attacks, not functional distance bounds for removal-based methods
- **Break condition**: If the model perturbation is large (models differ significantly in predictions), or if the removal approach allows significant off-manifold imputation (QxS(X') → 0), attributions can differ arbitrarily.

### Mechanism 3
- **Claim**: Summary technique choice affects attribution robustness through matrix operator norms.
- **Mechanism**: Attribution scores are linear combinations of predictions for feature subsets, ϕ(f,x) = Av. The difference in attributions is bounded by ∥A∥₂ · ∥v - v'∥₂ or ∥A∥₁,∞ · ∥v - v'∥∞. Methods like Banzhaf achieve better robustness (lower norms) than Shapley or leave-one-out.
- **Core assumption**: Attribution methods can be represented as linear operators on subset predictions, and meaningful attributions satisfy properties like boundedness and symmetry.
- **Evidence anchors**:
  - [abstract]: "the difference in attributions is proportional to the input perturbation strength for input perturbations"
  - [section 3.3]: Lemma 7 and subsequent results derive bounds using matrix norms; Lemma 9 shows Banzhaf has lower spectral norm than Shapley
  - [corpus]: Weak evidence - corpus papers don't discuss linear operator representations of attribution methods or their robustness properties
- **Break condition**: If the summary technique is chosen to maximize rather than minimize operator norms, robustness degrades arbitrarily. Also, if the attribution method doesn't satisfy boundedness/symmetry, the bounds may not apply.

## Foundational Learning

- **Concept: Lipschitz continuity**
  - Why needed here: The main theoretical results rely on the model being Lipschitz continuous to bound how much predictions (and thus attributions) can change under input perturbations
  - Quick check question: If a function f is L-Lipschitz, what bound can you give on |f(x) - f(x')| in terms of ∥x - x'∥?

- **Concept: Total variation distance between distributions**
  - Why needed here: For conditional feature removal, the difference in predictions depends on how quickly the conditional distribution p(x̄_S|x_S) can change as x_S changes
  - Quick check question: If two distributions have total variation distance d, what is the maximum difference between their expectations of a bounded random variable?

- **Concept: Functional norms (L₁, L∞, spectral norms)**
  - Why needed here: The robustness bounds for model perturbations use functional norms to measure distance between models, and the summary technique bounds use matrix operator norms
  - Quick check question: For a linear operator A, what is the relationship between ∥A∥₂, ∥A∥₁,∞ and the entries of A?

## Architecture Onboarding

- **Component map**: Feature removal (baseline/marginal/conditional) → Model prediction with partial features f(x_S) → Summary technique (linear operator A) → Attribution scores ϕ(f,x) = Av

- **Critical path**: For input perturbations: input perturbation → prediction difference for each subset → attribution difference via linear operator. For model perturbations: model change → prediction difference for each subset → attribution difference.

- **Design tradeoffs**: Conditional removal provides better information but worse robustness to input perturbations (larger Lipschitz constant). Banzhaf summary technique provides better robustness than Shapley but may violate efficiency. The choice depends on whether input perturbation robustness or model perturbation robustness is more critical.

- **Failure signatures**: Large attribution differences under small input perturbations suggest either the model isn't Lipschitz continuous or the conditional distribution changes rapidly. Large differences under model perturbations suggest either the models differ significantly or the removal approach allows significant off-manifold imputation.

- **First 3 experiments**:
  1. Verify Lipschitz continuity of your model by checking if ∥f(x) - f(x')∥/∥x - x'∥ is bounded for various inputs
  2. Compare attribution robustness for baseline vs conditional removal on your dataset to quantify the tradeoff
  3. Test attribution stability under weight decay regularization to confirm improved Lipschitz regularity leads to more robust attributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do removal-based feature attributions behave under adversarial attacks designed to manipulate the model's predictions while preserving the input?
- Basis in paper: [explicit] The paper discusses model perturbation robustness, showing that attributions can be sensitive to model changes, and mentions that models can be modified to hide their reliance on sensitive features in LIME and SHAP.
- Why unresolved: The paper focuses on robustness under input and model perturbations but does not explicitly address adversarial attacks on model predictions.
- What evidence would resolve it: Empirical studies comparing the robustness of removal-based attributions to adversarial attacks versus gradient-based methods, potentially using techniques like those in Ghorbani et al. [27] or Dombrowski et al. [20].

### Open Question 2
- Question: Can the theoretical bounds for attribution robustness be tightened for specific classes of models or data distributions?
- Basis in paper: [inferred] The paper acknowledges that its bounds are conservative and based on global properties like the Lipschitz constant, suggesting that localized versions might yield tighter bounds.
- Why unresolved: The paper uses worst-case global bounds, which may be overly conservative for many practical scenarios.
- What evidence would resolve it: Developing localized bounds that depend on specific perturbation strengths or input regions, potentially using techniques from certified robustness literature [35, 36, 48, 54, 59].

### Open Question 3
- Question: How do removal-based feature attributions compare to gradient-based methods in terms of robustness when features are highly correlated or dependent?
- Basis in paper: [explicit] The paper discusses the role of the conditional distribution in feature removal, noting that it can lead to worse robustness to input perturbations but improved robustness to model perturbations, and mentions that correlated features can affect the total variation constant M.
- Why unresolved: The paper provides theoretical analysis and empirical results for synthetic data with varying correlation, but does not directly compare removal-based and gradient-based methods under these conditions.
- What evidence would resolve it: Empirical studies comparing the robustness of removal-based and gradient-based attributions on datasets with high feature correlation or dependence, potentially using techniques like those in Aas et al. [1] or Frye et al. [26].

## Limitations

- The theoretical bounds rely on global Lipschitz continuity assumptions that may be overly conservative for practical models
- Empirical validation is limited to relatively simple models (logistic regression, FCNs, CNNs) and may not generalize to more complex architectures
- The focus on individual feature removal doesn't address group feature importance, which is often more relevant in practice

## Confidence

- Theoretical bounds on input perturbation robustness: High
- Theoretical bounds on model perturbation robustness: Medium
- Empirical validation of attribution stability with weight decay: Medium
- Model randomization sanity check results: High

## Next Checks

1. Test the attribution robustness bounds on transformer-based models to assess generalizability to modern architectures
2. Compare attribution differences under group feature removal vs individual feature removal to quantify the practical relevance of the current framework
3. Measure attribution stability across different random seeds for model training to assess sensitivity to initialization and optimization dynamics