---
ver: rpa2
title: 'RobustMQ: Benchmarking Robustness of Quantized Models'
arxiv_id: '2308.02350'
source_url: https://arxiv.org/abs/2308.02350
tags:
- robustness
- uni00000057
- uni0000004c
- uni00000010
- uni00000045
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RobustMQ, a benchmark designed to comprehensively
  evaluate the robustness of quantized deep neural network models against various
  types of noise, including adversarial attacks, natural corruptions, and systematic
  noises. The study investigates four classical architectures (ResNet18, ResNet50,
  RegNetX600M, MobileNetV2) and three popular quantization methods (DoReFa, PACT,
  LSQ) with four bit-widths (2, 4, 6, 8) on the ImageNet dataset.
---

# RobustMQ: Benchmarking Robustness of Quantized Models

## Quick Facts
- **arXiv ID**: 2308.02350
- **Source URL**: https://arxiv.org/abs/2308.02350
- **Reference count**: 40
- **Primary result**: Quantized models exhibit higher adversarial robustness but are more vulnerable to natural corruptions and systematic noises compared to floating-point models.

## Executive Summary
This paper introduces RobustMQ, a comprehensive benchmark for evaluating the robustness of quantized deep neural network models against various types of noise. The study investigates four classical architectures (ResNet18, ResNet50, RegNetX600M, MobileNetV2) and three popular quantization methods (DoReFa, PACT, LSQ) with four bit-widths (2, 4, 6, 8) on the ImageNet dataset. The key findings reveal that quantized models exhibit higher adversarial robustness but are more vulnerable to natural corruptions and systematic noises compared to floating-point models. Increasing the quantization bit-width generally decreases adversarial robustness while increasing natural and systematic robustness. The study identifies impulse noise and glass blur as the most harmful natural corruptions, and nearest neighbor interpolation as the most impactful systematic noise for quantized models.

## Method Summary
The study applies three quantization methods (DoReFa, PACT, LSQ) to four classical architectures (ResNet18, ResNet50, RegNetX600M, MobileNetV2) with four bit-widths (2, 4, 6, 8) on the ImageNet dataset. Robustness is evaluated using adversarial attacks (FGSM, PGD, AutoAttack), natural corruptions (15 types), and systematic noises (14 types). Three metrics are used: WCAR (adversarial), mNR (natural), and ACCs (systematic). The evaluation covers all combinations of quantization methods, architectures, and bit-widths to provide a comprehensive robustness analysis.

## Key Results
- Quantized models exhibit higher adversarial robustness but are more vulnerable to natural corruptions and systematic noises compared to floating-point models.
- Increasing quantization bit-width generally decreases adversarial robustness while increasing natural and systematic robustness.
- Impulse noise and glass blur are the most harmful natural corruptions for quantized models.
- Nearest neighbor interpolation is the most impactful systematic noise for quantized models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantized models exhibit higher adversarial robustness compared to their floating-point counterparts.
- Mechanism: Lower bit-width quantization reduces the model's capacity to fit complex adversarial perturbations, effectively limiting the adversary's ability to craft successful attacks. The discrete nature of quantized weights and activations constrains the search space for adversarial examples.
- Core assumption: The adversarial attack algorithms are not optimized for quantized models and assume continuous input space.
- Evidence anchors:
  - [abstract] "quantized models exhibit higher adversarial robustness than their floating-point counterparts"
  - [section 4.2] "quantized models exhibit higher worst-case adversarial robustness and are almost better than FP networks"
  - [corpus] Weak evidence - no direct citations found in neighboring papers
- Break condition: If attack algorithms are specifically adapted for quantized models or if the quantization method introduces new vulnerabilities.

### Mechanism 2
- Claim: Increasing quantization bit-width improves natural and systematic robustness but decreases adversarial robustness.
- Mechanism: Higher bit-width quantization provides finer granularity in representing weights and activations, allowing the model to better capture subtle features needed for handling natural corruptions and systematic noises. However, this increased capacity also makes the model more susceptible to adversarial attacks by providing more degrees of freedom.
- Core assumption: Natural corruptions and systematic noises require more precise feature representation than adversarial perturbations.
- Evidence anchors:
  - [abstract] "increasing the quantization bit-width generally leads to a decrease in adversarial robustness, an increase in natural robustness, and an increase in systematic robustness"
  - [section 4.2] "as the quantization bit-width decreases, the adversarial robustness of quantized ResNet18 models increases"
  - [section 4.3] "the natural robustness of quantized ResNet50 models generally increases as the quantization bit-width increases"
  - [section 4.4] "lower-bit models present less robustness (i.e., lower stability) generally"
  - [corpus] Weak evidence - no direct citations found in neighboring papers
- Break condition: If certain corruption types or noise patterns are better handled by lower bit-width models due to their inherent regularization effect.

### Mechanism 3
- Claim: Different quantization methods (DoReFa, PACT, LSQ) exhibit varied robustness performance across architectures.
- Mechanism: Each quantization method has distinct approaches to handling weight and activation quantization, which interact differently with specific network architectures. For example, LSQ's learned step size quantization may provide better robustness for deeper networks, while PACT's parameterized clipping might work better for lightweight architectures.
- Core assumption: The interaction between quantization method and network architecture is non-trivial and depends on factors like network depth, width, and computational blocks.
- Evidence anchors:
  - [section 4.2] "PACT outperforms other quantization methods under the worst-case adversarial robustness" but "quantized models demonstrate varying adversarial robustness against different attack methods"
  - [section 4.3] "quantization methods exhibit varied performance on different architectures" and "the order of impact on model performance is as follows: Noise > Blur > Weather(except Brightness) > Digital"
  - [section 4.4] "LSQ shows dominance in ResNet and RegNetX600M architectures, while PACT performs better in MobileNetV2"
  - [corpus] Weak evidence - no direct citations found in neighboring papers
- Break condition: If the interaction patterns change significantly with different network architectures or if new quantization methods are introduced.

## Foundational Learning

- Concept: Adversarial robustness and attack methods (FGSM, PGD, AutoAttack)
  - Why needed here: Understanding the different attack methods and their impact on quantized models is crucial for interpreting the benchmark results and comparing robustness across quantization methods and bit-widths.
  - Quick check question: What is the main difference between FGSM and PGD attacks in terms of their impact on model robustness evaluation?

- Concept: Natural corruption methods and their categorization (noise, blur, weather, digital)
  - Why needed here: The benchmark evaluates quantized models against 15 different natural corruption methods, and understanding their categorization helps in analyzing which types of corruptions are most harmful to quantized models.
  - Quick check question: Which category of natural corruptions (noise, blur, weather, digital) is generally the most harmful to quantized models according to the benchmark results?

- Concept: Systematic noises and their sources (image decoding, image resize)
  - Why needed here: Systematic noises represent real-world deployment challenges, and understanding their sources and impact on quantized models is essential for deploying robust quantized models in practical applications.
  - Quick check question: What are the two main sources of systematic noises evaluated in the benchmark, and how do they affect model performance?

## Architecture Onboarding

- Component map: Data preparation (ImageNet dataset, corruption methods application) -> Model quantization (DoReFa, PACT, LSQ quantization methods, 2/4/6/8 bit-widths) -> Robustness evaluation (Adversarial attacks, natural corruptions, systematic noises) -> Metrics calculation (WCAR, mNR, ACCs, FP)
- Critical path: Quantize model → Apply perturbations → Evaluate accuracy → Calculate robustness metrics
- Design tradeoffs:
  - Bit-width vs. robustness: Lower bit-width provides better adversarial robustness but worse natural and systematic robustness
  - Quantization method choice: Different methods perform better on different architectures
  - Evaluation comprehensiveness vs. computational cost: More attack methods and corruption types provide better insights but require more resources
- Failure signatures:
  - Models failing to converge (labeled as "NC")
  - Significant accuracy drop under specific perturbations
  - Inconsistent performance across different architectures
- First 3 experiments:
  1. Evaluate ResNet18 with DoReFa quantization at 4-bit width against FGSM-ℓ∞ attack with small budget
  2. Compare natural robustness of ResNet50 models quantized with PACT vs. LSQ methods against impulse noise
  3. Test systematic robustness of MobileNetV2 models against nearest neighbor interpolation using Pillow library

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training modifications could further improve the adversarial robustness of quantized models without significantly impacting their clean accuracy?
- Basis in paper: [explicit] The paper observes that quantized models exhibit higher adversarial robustness than their floating-point counterparts but are more vulnerable to natural corruptions and systematic noises. It also notes that as quantization bit-width increases, adversarial robustness generally decreases.
- Why unresolved: While the paper provides insights into the current state of quantized model robustness, it does not explore potential architectural or training modifications that could further enhance adversarial robustness without compromising clean accuracy.
- What evidence would resolve it: Experimental results comparing the adversarial robustness of quantized models with and without specific architectural or training modifications, while maintaining comparable clean accuracy.

### Open Question 2
- Question: How do different quantization methods (e.g., DoReFa, PACT, LSQ) impact the robustness of quantized models against various types of noise, and what are the underlying reasons for these differences?
- Basis in paper: [explicit] The paper compares the performance of three quantization methods (DoReFa, PACT, LSQ) under different types of noise (adversarial attacks, natural corruptions, and systematic noises). It observes that PACT outperforms other methods in worst-case adversarial robustness, while LSQ performs better against FGSM and PGD attacks.
- Why unresolved: The paper provides empirical results but does not delve into the underlying reasons for the observed differences in robustness across quantization methods. A deeper understanding of the mechanisms behind these differences could lead to more targeted improvements in robustness.
- What evidence would resolve it: A detailed analysis of the quantization process for each method, including how they handle different types of noise and their impact on the model's decision boundaries.

### Open Question 3
- Question: What is the relationship between model size (e.g., FLOPs, parameters) and robustness against different types of noise in quantized models, and how does this relationship vary across different network architectures?
- Basis in paper: [explicit] The paper observes that quantized models with larger network capacity exhibit better adversarial robustness, consistent with findings in floating-point models. However, it also notes that the relationship between model size and robustness may vary across different network architectures.
- Why unresolved: While the paper provides some insights into the relationship between model size and robustness, it does not comprehensively explore how this relationship varies across different network architectures and types of noise. A more thorough investigation could reveal valuable patterns and inform the design of more robust quantized models.
- What evidence would resolve it: A systematic study comparing the robustness of quantized models with varying sizes and architectures against different types of noise, while controlling for other factors such as quantization method and bit-width.

## Limitations
- The ImageNet dataset may not fully represent real-world deployment scenarios.
- The study focuses on vision models while robustness patterns may differ for other domains like NLP.
- The computational cost of evaluating all combinations limited the exploration of additional quantization methods or attack strategies.

## Confidence
- **High confidence**: General trends of quantized model robustness across different noise types and bit-widths, supported by comprehensive experiments across four architectures, three quantization methods, and multiple robustness dimensions.
- **Medium confidence**: Specific quantitative rankings of quantization methods due to limited comparisons with alternative benchmarks and absence of statistical significance testing.
- **Low confidence**: Claims about practical deployment recommendations without validation on actual edge devices or in production environments.

## Next Checks
1. Verify the implementation of quantization methods (DoReFa, PACT, LSQ) matches the paper's specifications by comparing intermediate results with provided tables.
2. Reproduce the adversarial robustness results for ResNet18 with 4-bit DoReFa quantization against FGSM-ℓ∞ attack with small budget.
3. Validate the natural robustness ranking of corruption types by testing quantized ResNet50 models against impulse noise and glass blur.