---
ver: rpa2
title: 'RAFIC: Retrieval-Augmented Few-shot Image Classification'
arxiv_id: '2312.06868'
source_url: https://arxiv.org/abs/2312.06868
tags:
- images
- image
- retrieval
- classification
- meta-learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents RAFIC (Retrieval-Augmented Few-shot Image Classification),
  a method for improving few-shot image classification by retrieving additional relevant
  images from a large-scale dataset. The core idea is to augment the limited set of
  K training examples per class with an additional set of A retrieved images using
  CLIP embeddings, LAION-5B dataset, and faiss for efficient retrieval.
---

# RAFIC: Retrieval-Augmented Few-shot Image Classification

## Quick Facts
- **arXiv ID**: 2312.06868
- **Source URL**: https://arxiv.org/abs/2312.06868
- **Reference count**: 23
- **Key outcome**: RAFIC significantly improves few-shot image classification accuracy by retrieving relevant images from LAION-5B and meta-learning retrieval strategies

## Executive Summary
This paper presents RAFIC (Retrieval-Augmented Few-shot Image Classification), a method that improves few-shot image classification by augmenting limited training examples with additional retrieved images. The approach uses CLIP embeddings to retrieve semantically relevant images from the large-scale LAION-5B dataset, then employs meta-learning strategies to optimally utilize these retrieved images. Experiments on challenging birds and aircraft datasets demonstrate substantial performance improvements over baseline methods, with CLIP embeddings providing the most dramatic gains. The method is particularly effective in cross-task evaluation settings where the test distribution differs from training data.

## Method Summary
RAFIC augments the set of K support images per class with A retrieved images using CLIP embeddings and FAISS for efficient similarity search in the LAION-5B dataset. The retriever finds images most similar to support images using a weighted combination of class name and image embeddings. Retrieved images are concatenated with support images before training classifiers (Logistic Regression, MAML, or ProtoNet). The method also includes meta-learning strategies that append retrieval cosine similarity scores to embeddings and adjust learning rates separately for support and retrieval components, allowing the model to adaptively weight retrieved images by relevance.

## Key Results
- CLIP embeddings vastly outperform raw pixels (accuracy increase from 0.26 to 0.88)
- Zero-shot retrieval using class names is highly effective for finding relevant images
- Adding retrieved images consistently boosts accuracy, especially for the aircraft dataset
- Meta-learning the retrieval strategy provides additional improvements, particularly in cross-dataset evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval augmentation improves few-shot classification by expanding the support set with semantically relevant images
- Mechanism: CLIP embeddings find visually and contextually similar images in LAION-5B that are appended to the support set before training
- Core assumption: Retrieved images are relevant enough to improve model performance
- Evidence anchors: Paper demonstrates augmented support sets improve accuracy; related work exists on retrieval-augmented few-shot methods
- Break condition: If CLIP embeddings fail to capture relevant semantic similarity, retrieved images may be noisy or irrelevant

### Mechanism 2
- Claim: Meta-learning the retrieval strategy allows adaptive weighting of retrieved images
- Mechanism: Fine-grained meta-learning appends retrieval cosine similarity scores to CLIP embeddings; coarse-grained adjusts learning rates separately for support and retrieval components
- Core assumption: The model can learn to distinguish useful from less useful retrieved images
- Evidence anchors: Paper shows meta-learning improves accuracy; demonstrates effectiveness of similarity score weighting
- Break condition: If meta-learning overfits to noise in similarity scores or fails to learn meaningful weighting, performance gains may be lost

### Mechanism 3
- Claim: CLIP embeddings dramatically improve few-shot classification over raw pixels
- Mechanism: CLIP embeddings capture rich semantic information from images and text, providing better feature space than raw pixels
- Core assumption: Semantic embeddings are more informative than raw pixels for few-shot classification
- Evidence anchors: Paper shows accuracy increase from 0.26 to 0.88 using CLIP embeddings; CLIP learns shared embedding space for image-text comparison
- Break condition: If tasks require fine-grained visual details that CLIP embeddings smooth over, performance may degrade

## Foundational Learning

- **Concept: Few-shot learning paradigm**
  - Why needed here: The paper addresses learning from very few examples per class
  - Quick check question: What distinguishes few-shot learning from traditional supervised learning?

- **Concept: Meta-learning vs. standard learning**
  - Why needed here: The paper compares meta-learning approaches (MAML, ProtoNet) against standard logistic regression
  - Quick check question: How does MAML's inner-loop/outer-loop structure enable faster adaptation?

- **Concept: Retrieval augmentation**
  - Why needed here: Core contribution is augmenting support sets with retrieved images to improve classification
  - Quick check question: Why might retrieved images help when we already have support images?

## Architecture Onboarding

- **Component map**: CLIP encoder (image + text) → faiss ANN index → retriever → augmented support set → classifier (MAML/ProtoNet/LR) → query prediction
- **Critical path**: CLIP embedding extraction → retrieval via faiss → concatenation with support images → model training/prediction
- **Design tradeoffs**: Real-time retrieval vs. precomputed index (compact index used for efficiency); fine-grained vs. coarse-grained meta-learning; number of retrieved images vs. computational cost
- **Failure signatures**: Performance plateaus or degrades with more retrieved images; meta-learning fails to improve accuracy; cross-dataset evaluation shows large drops
- **First 3 experiments**:
  1. Compare CLIP embeddings vs. raw pixels on a simple MAML setup
  2. Measure accuracy vs. number of retrieved images (A=0,1,2,5,20,50)
  3. Test meta-learning retrieval strategy (fine + coarse) vs. no meta-learning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RAFIC's performance scale with dataset size beyond LAION-5B?
- Basis in paper: The paper demonstrates effectiveness using LAION-5B but doesn't explore larger datasets
- Why unresolved: Only LAION-5B (5B+ images) was tested without exploring scalability
- What evidence would resolve it: Systematic experiments comparing performance across datasets of increasing size (10B, 100B, 1T images)

### Open Question 2
- Question: What is the impact of different CLIP model sizes on RAFIC's performance?
- Basis in paper: The paper uses ViT-L/14 but doesn't explore other CLIP variants
- Why unresolved: Only one CLIP model size was tested, leaving uncertainty about optimal model selection
- What evidence would resolve it: Controlled experiments comparing RAFIC performance using different CLIP model sizes across multiple datasets

### Open Question 3
- Question: How does RAFIC perform on datasets from domains completely different from birds and aircraft?
- Basis in paper: The paper only tests on birds and aircraft datasets
- Why unresolved: Limited domain diversity in testing datasets
- What evidence would resolve it: Extensive evaluation on diverse datasets (medical imaging, satellite imagery, microscopy, etc.)

### Open Question 4
- Question: What is the optimal balance between support images (K) and retrieved images (A) for different task complexities?
- Basis in paper: The paper uses K=1 and varying A, but doesn't systematically explore different K values
- Why unresolved: Only minimal K values were tested, leaving uncertainty about optimal K-A trade-offs
- What evidence would resolve it: Systematic grid search across different K and A values for tasks of varying complexity

## Limitations
- Exact LAION-5B subset used for retrieval is not specified, affecting reproducibility
- MAML hyperparameters and architecture details are incomplete, potentially impacting replication
- Paper lacks ablation studies on retrieval quality to assess relevance rate of retrieved images

## Confidence
- **High confidence**: Core claim that retrieval augmentation improves few-shot classification, supported by strong empirical results
- **Medium confidence**: Meta-learning strategy effectiveness, as results are positive but ablation studies are limited
- **Low confidence**: Generalizability across all dataset types, as only two fine-grained classification datasets were tested

## Next Checks
1. Perform ablation study on retrieval quality - manually inspect retrieved images for different classes to assess relevance rate
2. Test cross-dataset generalization with additional datasets (e.g., CIFAR-FS, miniImageNet) to verify robustness
3. Implement and compare alternative retrieval strategies (e.g., using class name embeddings vs. support image embeddings) to isolate the source of performance gains