---
ver: rpa2
title: Large-scale Training of Foundation Models for Wearable Biosignals
arxiv_id: '2312.05409'
source_url: https://arxiv.org/abs/2312.05409
tags:
- arxiv
- learning
- embeddings
- pre-training
- participants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first study on pre-training foundation
  models using large-scale photoplethysmography (PPG) and electrocardiogram (ECG)
  data collected from wearable consumer devices, specifically from ~141K participants
  in the Apple Heart and Movement Study over ~3 years. The authors employ self-supervised
  learning with a contrastive loss and momentum training, using participant-level
  positive pairs and a stochastic augmentation module.
---

# Large-scale Training of Foundation Models for Wearable Biosignals

## Quick Facts
- arXiv ID: 2312.05409
- Source URL: https://arxiv.org/abs/2312.05409
- Reference count: 40
- Primary result: First study on pre-training foundation models using large-scale PPG and ECG data from wearable devices

## Executive Summary
This paper presents the first study on pre-training foundation models using large-scale photoplethysmography (PPG) and electrocardiogram (ECG) data collected from wearable consumer devices, specifically from ~141K participants in the Apple Heart and Movement Study over ~3 years. The authors employ self-supervised learning with a contrastive loss and momentum training, using participant-level positive pairs and a stochastic augmentation module. The pre-trained models encode information about participants' demographics and health conditions, with PPG embeddings performing better than ECG in predicting age, biological sex, and BMI.

## Method Summary
The study uses self-supervised contrastive learning with participant-level positive pairs to pre-train foundation models on PPG and ECG data from wearable devices. The framework employs InfoNCE loss with KoLeo regularization, momentum training with exponential moving average, and modality-specific stochastic augmentation. The models are evaluated through linear probing on demographic and health condition prediction tasks, comparing performance against baseline features.

## Key Results
- PPG embeddings outperform ECG embeddings in predicting age, biological sex, and BMI
- Pre-trained models encode information predictive of a broad range of self-reported health conditions and medication categories
- Participant-level positive pair selection significantly improves embedding quality compared to segment-level alternatives
- Ablation studies confirm the importance of the pre-training framework and suggest potential for modality-specific design choices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Participant-level positive pair selection is critical for extracting participant-specific physiological information
- Mechanism: By selecting positive pairs as augmented views of segments from the same participant rather than random segments, the model is forced to learn features that are consistent across different time points for that individual, capturing stable physiological patterns
- Core assumption: Different segments from the same participant contain sufficient within-participant variability while maintaining core physiological characteristics
- Evidence anchors: "To motivate the model to extract information relevant to participant level physiology, we use a participant level positive pair selection strategy" and "We observed that consistently for both PPG and ECG, pre-trained models with participant level positive pairs were significantly more predictive of downstream targets"

### Mechanism 2
- Claim: Contrastive loss with KoLeo regularization enables learning informative representations without supervision
- Mechanism: The InfoNCE loss maximizes mutual information between positive pairs while the KoLeo regularization encourages uniform feature distribution, preventing collapse and promoting rich, discriminative embeddings
- Core assumption: The contrastive objective can effectively push apart representations of different participants while keeping similar participants' representations close
- Evidence anchors: "We use InfoNCE (also known as NT-Xent) to maximize the mutual information between the positive pair representations while allowing for contrast to other positive pairs in each batch" and "Both of the contrastive and regularization losses are calculated after l2-normalization of the embeddings"

### Mechanism 3
- Claim: Stochastic augmentation with modality-specific intensity improves robustness and generalization
- Mechanism: The augmentation module applies random distortions (crop, noise, time warp, magnitude warp, channel swap) with different intensities for PPG vs ECG, making the model invariant to these transformations while preserving essential physiological information
- Core assumption: The chosen augmentation functions preserve the core physiological information while introducing sufficient variability for the model to learn robust features
- Evidence anchors: "Our augmentation module T (·) consists of a stochastic sequence of time-series augmentations, including crop, add Gaussian noise, time warp, magnitude warp, and channel swap" and "Also, to address the fact that ECG has less within-participant variability compared to PPG... we assign higher probability values to our ECG augmentation module to introduce stronger signal distortions"

## Foundational Learning

- Concept: Self-supervised learning and contrastive learning
  - Why needed here: Traditional supervised learning requires large labeled datasets which are expensive to obtain in medical domains; self-supervised learning allows the model to learn useful representations from unlabeled data by creating its own supervisory signal through contrastive learning
  - Quick check question: Why is participant-level positive pair selection more effective than segment-level for this application?

- Concept: Momentum training and exponential moving average
  - Why needed here: Momentum training stabilizes the learning process by maintaining a slowly-updating target network, which provides consistent targets for the online network and prevents representation collapse in contrastive learning
  - Quick check question: How does momentum training differ from regular gradient descent in this context?

- Concept: Differential entropy regularization (KoLeo)
  - Why needed here: The KoLeo regularization encourages the model to spread out its embeddings in feature space, preventing the model from collapsing to a single point and ensuring the learned representations are diverse and informative
  - Quick check question: What would happen to the learned representations if we removed the KoLeo regularization term?

## Architecture Onboarding

- Component map: Input → Augmentation → Encoder → Projection Head → Contrastive Loss → Backpropagation → Momentum Update
- Critical path: Input → Augmentation → Encoder → Projection Head → Contrastive Loss → Backpropagation → Momentum Update
- Design tradeoffs: 1) Encoder size vs. model capacity vs. deployment constraints; 2) Augmentation intensity vs. signal preservation; 3) Batch size vs. negative pair quality vs. computational cost
- Failure signatures: 1) InfoNCE loss plateaus early (possible collapse); 2) Smooth effective rank drops (feature degeneracy); 3) Downstream performance poor despite good pre-training metrics (superficial learning)
- First 3 experiments:
  1. Compare participant-level vs. segment-level positive pair selection on downstream age/sex/BMI prediction
  2. Test different augmentation intensities (PPG vs ECG) and measure effect on InfoNCE loss and downstream performance
  3. Ablate KoLeo regularization to quantify its contribution to representation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do longitudinal changes in PPG and ECG embeddings affect downstream prediction performance, and can accounting for these changes improve predictions?
- Basis in paper: The authors mention that "Future work can also investigate the longitudinal changes in PPG and ECG embeddings, and whether accounting for those can improve downstream predictions" in the discussion section.
- Why unresolved: This is explicitly stated as a future work direction, indicating it has not been investigated yet.
- What evidence would resolve it: Experimental results showing the effect of longitudinal changes on downstream prediction performance, and whether incorporating these changes improves prediction accuracy.

### Open Question 2
- Question: How effective are different positive pair selection strategies on various health-related targets, and what factors should be considered when designing these strategies?
- Basis in paper: The authors state "Last but not least, we observed that the choice of positive pairs significantly affects the quality of the embeddings; future work can investigate the efficacy of different positive pair selection strategies on different health-related targets, by accounting for temporal and other physiological information."
- Why unresolved: This is explicitly mentioned as a future research direction, suggesting it has not been thoroughly explored.
- What evidence would resolve it: Comparative studies of different positive pair selection strategies on various health-related targets, demonstrating their effectiveness and identifying key factors for optimal selection.

### Open Question 3
- Question: How does scaling up model size and using transformer-based models affect downstream performance on PPG and ECG embeddings?
- Basis in paper: The authors note "An interesting future direction is scaling up the model size further, particularly with transformer-based models (e.g., 1D-ViT) given their scalability (Kaplan et al., 2020), and larger datasets to study its effect on downstream performance."
- Why unresolved: This is presented as a future research opportunity, indicating it has not been investigated yet.
- What evidence would resolve it: Experiments comparing the performance of larger models and transformer-based architectures on PPG and ECG embeddings, demonstrating the impact of model size and architecture on downstream tasks.

## Limitations
- The study lacks direct comparisons with alternative positive pair strategies, making it difficult to quantify the precise contribution of participant-level selection
- Evaluation relies entirely on linear probing rather than fine-tuning, which may underestimate the true potential of learned representations
- Model performance on rare health conditions may be limited by class imbalance in the dataset

## Confidence
- **High confidence**: The overall self-supervised learning framework with participant-level positive pairs improves downstream prediction performance compared to baseline features
- **Medium confidence**: The superiority of PPG embeddings over ECG for demographic prediction, though this may be influenced by specific evaluation tasks chosen
- **Medium confidence**: The effectiveness of modality-specific augmentation intensities, as analysis shows improvements but lacks comprehensive ablation studies across all augmentation types

## Next Checks
1. Conduct controlled experiments comparing participant-level positive pairs against instance-level pairs using identical model architectures and training procedures to quantify the specific contribution of participant-level selection
2. Evaluate the pre-trained models using fine-tuning rather than just linear probing on downstream tasks to assess whether representations can be adapted more effectively with task-specific training
3. Perform extensive ablation studies on each augmentation type (crop, noise, time warp, magnitude warp, channel swap) to determine which components are most critical for learning robust representations