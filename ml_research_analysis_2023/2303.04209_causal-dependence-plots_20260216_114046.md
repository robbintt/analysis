---
ver: rpa2
title: Causal Dependence Plots
arxiv_id: '2303.04209'
source_url: https://arxiv.org/abs/2303.04209
tags:
- causal
- dependence
- plots
- learning
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Causal Dependence Plots (CDPs) address the problem of misleading
  interpretations in explainable AI when predictors have causal relationships. The
  core method uses an auxiliary causal model to visualize how one predictor affects
  model output, along with consequent changes in other predictors.
---

# Causal Dependence Plots

## Quick Facts
- arXiv ID: 2303.04209
- Source URL: https://arxiv.org/abs/2303.04209
- Reference count: 8
- Key outcome: CDP method addresses misleading interpretations in explainable AI by using causal models to show how predictors affect model output along with consequent changes in other predictors.

## Executive Summary
Causal Dependence Plots (CDPs) provide a framework for visualizing causal relationships in machine learning models, addressing the problem of misleading interpretations when predictors have causal dependencies. Unlike standard methods that hold other predictors constant or assume independence, CDPs use an auxiliary causal model to determine how other predictors change when one predictor is manipulated. This approach enables visualization of both direct and indirect effects through mediators, revealing causal structures that standard plots like PDPs and ICE cannot capture.

## Method Summary
CDPs work by generating counterfactual datasets using an auxiliary causal model for predictors, then passing these through the black-box prediction model to create visualizations. The method differs from standard approaches by modeling how predictors causally depend on each other rather than treating them as independent. CDPs can reveal indirect effects through mediators and provide causal interpretations that include standard methods as special cases. The framework uses interventions to generate synthetic data that respects causal dependencies, producing plots that show total, direct, and indirect effects.

## Key Results
- CDP visualizations reveal qualitative differences between direct/partial dependence and total dependence
- CDPs can identify causal structures including indirect effects through mediators in both simulated and real breast cancer data
- The method unifies existing interpretation techniques while adding causal meaning through different intervention types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CDPs avoid misleading interpretations by explicitly modeling how predictors causally depend on each other, rather than holding them fixed.
- Mechanism: An auxiliary causal model determines how other predictors change when one predictor is manipulated, producing counterfactual datasets that respect causal dependencies.
- Core assumption: The auxiliary causal model accurately represents the causal relationships among predictors.
- Evidence anchors:
  - [abstract] "CDPs differ from standard methods based on holding other predictors constant or assuming they are independent."
  - [section] "The differences between blue and green curves show there can be qualitative differences between direct (or partial) dependence and total dependence..."
  - [corpus] Weak - corpus papers focus on adversarial attacks and robustness of interpretations, not on causal modeling approaches.
- Break condition: If the auxiliary causal model is misspecified or inaccurate, the CDP visualizations will misrepresent causal effects and potentially be more misleading than standard methods.

### Mechanism 2
- Claim: CDPs can reveal indirect effects through mediators that standard methods miss, enabling richer causal interpretations.
- Mechanism: By modeling the full causal chain, CDPs capture both direct effects of a predictor on the outcome and indirect effects mediated through other variables.
- Core assumption: The causal structure includes relevant mediators that transmit indirect effects.
- Evidence anchors:
  - [abstract] "CDPs can reveal causal structures in models, including indirect effects through mediators."
  - [section] "The second is that the different effects on the outcome Y that we may want to investigate will show up visually across the different plots."
  - [corpus] Weak - corpus papers don't directly address mediation analysis or indirect effects.
- Break condition: If there are no mediating variables in the true causal structure, or if the mediator relationships are incorrectly modeled, the indirect effect visualization will be misleading.

### Mechanism 3
- Claim: CDPs provide a unified framework that includes standard interpretation methods as special cases while extending them with causal interpretations.
- Mechanism: By defining different intervention types (total, direct, indirect effects), CDPs generalize existing methods like PDPs and ICE plots while adding causal meaning.
- Core assumption: The causal interpretation framework is mathematically consistent with existing methods when appropriate interventions are chosen.
- Evidence anchors:
  - [abstract] "Our framework of using causal models to produce explanation plots includes, as special cases, some existing model explanation plots like ICE and PDPs."
  - [section] "From this construction of NDDP, we see by comparing it to Definition 2.2 that it is equivalent to the PDP, confirming what we observed in Figure 1."
  - [corpus] Weak - corpus papers focus on vulnerabilities of existing methods rather than causal unification.
- Break condition: If the mathematical equivalence between CDP variants and standard methods breaks down due to incorrect intervention modeling, the claimed unification fails.

## Foundational Learning

- Concept: Structural Causal Models (SCMs)
  - Why needed here: CDPs require an auxiliary causal model to determine how predictors change when one is manipulated
  - Quick check question: What are the three components of a structural causal model and how do they relate to each other?

- Concept: Interventions vs. Counterfactuals
  - Why needed here: CDPs use both interventions (do-calculus) and counterfactuals to generate the synthetic data for visualization
  - Quick check question: What's the key difference between an intervention and a counterfactual in causal inference?

- Concept: Mediation Analysis
  - Why needed here: The paper uses mediation triangles to decompose total effects into direct and indirect components
  - Quick check question: In a simple mediation model X → M → Y, what are the three types of effects that can be identified?

## Architecture Onboarding

- Component map: Causal Model -> Counterfactual Generator -> Black-box Model -> Visualization Engine
- Critical path: 1) Define the causal model and identify the target predictor, 2) Generate counterfactual datasets using interventions, 3) Pass counterfactual data through the black-box model, 4) Aggregate predictions and plot results. The causal model specification is the most critical step.
- Design tradeoffs: Using a more complex causal model can capture richer dependencies but increases computational cost and model uncertainty. Simpler models are faster but may miss important indirect effects. The choice of intervention type (total, direct, indirect) also affects interpretability.
- Failure signatures: 1) Plots that show counterintuitive relationships may indicate model misspecification, 2) Computational failures when sampling counterfactuals suggest the causal model is too complex, 3) Plots that closely resemble standard PDPs may indicate the causal model is too simple or independence assumptions hold.
- First 3 experiments:
  1. Replicate the simulation example with known causal structure to verify CDP generation works
  2. Apply CDPs to a dataset with clear mediating relationships to demonstrate indirect effect visualization
  3. Compare CDP outputs to standard PDPs on a real dataset to quantify the differences in interpretation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive are CDP interpretations to errors in the causal model structure versus errors in the learned causal relationships among predictors?
- Basis in paper: [explicit] The paper discusses limitations related to mismatch between the explanatory SCM and the true/target DGP, noting that "the causal interpretations displayed in our plots may be invalid if the true DGP differs in important ways from the explanatory causal model."
- Why unresolved: The paper acknowledges this limitation but does not empirically test how different types of errors (structural vs. parameter) affect CDP reliability.
- What evidence would resolve it: Systematic experiments varying causal model structure accuracy and parameter estimation quality, measuring how each affects CDP interpretation validity.

### Open Question 2
- Question: Can CDP methods be effectively extended to non-tabular data types (images, text, graphs) through causal representation learning?
- Basis in paper: [explicit] The conclusion states "Additional future work in this direction could explore other relatively small canonical causal structures...or interface with other kinds of models, for example extending to non-tabular data by applying causal representation learning."
- Why unresolved: The paper only demonstrates CDPs on tabular data and suggests this as future work without providing implementation details or preliminary results.
- What evidence would resolve it: Implementation of CDP methods on image/text/graph datasets with corresponding causal representation learning techniques, showing interpretable results.

### Open Question 3
- Question: How can uncertainty quantification methods be integrated with CDPs to provide confidence intervals for causal effect estimates?
- Basis in paper: [explicit] The applications section mentions "Future work can apply existing methods for uncertainty to show, for example, confidence bands around the plotted dependencies" and cites conformal prediction as a potential approach.
- Why unresolved: The paper only mentions this as a future direction without demonstrating how uncertainty quantification would work with CDPs.
- What evidence would resolve it: Implementation of CDP methods with uncertainty quantification (e.g., conformal prediction, bootstrap methods) showing confidence intervals around causal effect estimates.

## Limitations

- The reliability of CDPs depends critically on the accuracy of the auxiliary causal model, which may be misspecified or incomplete
- Computational complexity increases significantly with more complex causal structures, potentially limiting practical applicability
- The paper lacks systematic evaluation of how causal model errors propagate to final plots and how to validate causal model accuracy

## Confidence

**High Confidence:** The mathematical framework for CDP construction and the relationship between CDP variants and standard methods (PDPs, ICE plots) are well-established and rigorously proven.

**Medium Confidence:** The empirical demonstration on synthetic and real data provides reasonable evidence for the method's utility, but the sample sizes and diversity of test cases are limited. The sensitivity of results to different causal model specifications is not thoroughly explored.

**Low Confidence:** The claims about CDP superiority over existing methods in real-world applications are not fully substantiated. The paper lacks comparison with alternative causal interpretation approaches and does not address how to validate the accuracy of the causal model in practice.

## Next Checks

1. **Robustness to Causal Model Misspecification:** Systematically vary the accuracy of the auxiliary causal model (from perfect to severely misspecified) and measure how CDP outputs change compared to ground truth causal effects.

2. **Computational Scalability Test:** Apply CDPs to increasingly high-dimensional datasets (100+ predictors) to identify practical limits on computational resources and processing time.

3. **Comparison with Alternative Methods:** Compare CDPs against other causal interpretation approaches like path-specific effects or counterfactual explanations on benchmark datasets with known causal structures to quantify relative performance.