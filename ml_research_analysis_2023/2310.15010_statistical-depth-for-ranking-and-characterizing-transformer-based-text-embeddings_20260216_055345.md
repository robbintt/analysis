---
ver: rpa2
title: Statistical Depth for Ranking and Characterizing Transformer-Based Text Embeddings
arxiv_id: '2310.15010'
source_url: https://arxiv.org/abs/2310.15010
tags:
- depth
- text
- corpus
- texts
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces transformer-based text embedding (TTE) depth,
  a statistical tool for measuring the centrality and distribution of transformer-based
  text embeddings. TTE depth assigns a score to each text in a corpus based on how
  representative it is of the corpus as a whole, providing a center-outward ordering.
---

# Statistical Depth for Ranking and Characterizing Transformer-Based Text Embeddings

## Quick Facts
- arXiv ID: 2310.15010
- Source URL: https://arxiv.org/abs/2310.15010
- Reference count: 10
- This paper introduces transformer-based text embedding (TTE) depth, a statistical tool for measuring the centrality and distribution of transformer-based text embeddings.

## Executive Summary
This paper introduces TTE depth, a statistical tool for measuring the centrality and distribution of transformer-based text embeddings. TTE depth assigns a score to each text in a corpus based on how representative it is of the corpus as a whole, providing a center-outward ordering. The authors also introduce a Wilcoxon rank sum test for comparing the distributions of two corpora in embedding space. They demonstrate the usefulness of TTE depth for two tasks: prompt selection for in-context learning and characterizing distributional shifts in synthetic data augmentation. Results show that TTE depth improves performance on prompt selection tasks and can detect significant distributional differences between human-generated and synthetic corpora.

## Method Summary
The method introduces TTE depth, a statistical depth function that ranks k-dimensional objects by measuring centrality with respect to some observed k-dimensional distribution. TTE depth is computed using angular distance (cosine or chord distance) between a text's embedding and the rest of the corpus. The authors also introduce a Wilcoxon rank sum test for determining whether two sets of transformer-based embeddings can reasonably be assumed to come from significantly different distributions. The test uses the order induced by TTE depth to compare the centrality of two corpora.

## Key Results
- TTE depth improves performance on prompt selection tasks for in-context learning, outperforming statistical baseline approaches across six text classification tasks.
- The Wilcoxon rank sum test based on TTE depth can detect significant distributional differences between human-generated and synthetic corpora.
- TTE depth provides a center-outward ordering of texts within a corpus, with the most central text (highest depth score) representing the most semantically central text.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TTE depth leverages angular distance properties of transformer-based embeddings to assign centrality scores that reflect semantic representativeness.
- Mechanism: By measuring cosine distance between a text's embedding and the rest of the corpus, TTE depth quantifies how close a text is to the center of the embedding distribution. The median of the corpus (highest depth score) represents the most semantically central text.
- Core assumption: Transformer embeddings exhibit consistent angular relationships where similar texts have high cosine similarity.
- Evidence anchors:
  - [abstract]: "TTE depth is a function for ranking k-dimensional objects by measuring centrality with respect to some observed k-dimensional distribution"
  - [section 3]: "Angular distance properties of text embeddings, such as high cosine similarity between embeddings of two similar texts, are often sought as a desired property"
  - [corpus]: Weak - no explicit empirical validation that transformer embeddings consistently exhibit the assumed angular properties across diverse domains.
- Break condition: If the embedding model fails to preserve semantic similarity in angular space, TTE depth scores become meaningless.

### Mechanism 2
- Claim: The Wilcoxon rank sum test on TTE depth order provides a statistical framework for detecting distributional shifts between corpora.
- Mechanism: By computing Q(F,G), which measures how "central" corpus G is relative to corpus F, and testing whether Q < 1/2, the method determines if two corpora differ significantly in embedding space.
- Core assumption: The depth-induced ordering is sufficiently sensitive to distributional differences to enable reliable statistical inference.
- Evidence anchors:
  - [abstract]: "We introduce the use of a Wilcoxon rank sum test for determining whether two sets of transformer-based embeddings can reasonably be assumed to come from significantly different distributions"
  - [section 3]: "Since R(y, F ) is the fraction of the F corpus which is 'less central' to F than the value y, Q(F, G) is the average of such fractions over all y's from the G corpus"
  - [corpus]: Moderate - the simulation study in Section 4.1 shows that Q estimates stabilize with n=500 samples, supporting the feasibility of the approach.
- Break condition: If the sample size is too small, Q estimates become unreliable and the test loses statistical power.

### Mechanism 3
- Claim: TTE depth enables effective prompt selection for in-context learning by ranking texts by their representativeness.
- Mechanism: Texts with high TTE depth scores are more representative of the corpus distribution, making them better exemplars for prompting language models to generalize to unseen examples.
- Core assumption: Language models benefit from in-context examples that are central to the training distribution.
- Evidence anchors:
  - [abstract]: "We then use TTE depth for the task of in-context learning prompt selection, showing that this approach reliably improves performance over statistical baseline approaches across six text classification tasks"
  - [section 4.2]: "We hypothesize that texts which better represent the corpus distribution, i.e. texts with higher TTE depth, will improve downstream ICL performance"
  - [corpus]: Strong - Table 1 shows TTE depth-based ranking strategies outperform baselines in 5 out of 6 tasks, providing direct empirical support.
- Break condition: If the ICL task requires diversity rather than representativeness, high-depth prompts may underperform.

## Foundational Learning

- Concept: Transformer-based text embeddings and their angular distance properties
  - Why needed here: TTE depth fundamentally relies on transformer embeddings having meaningful angular relationships where semantic similarity maps to cosine distance.
  - Quick check question: If two texts are semantically similar, what should their cosine similarity be under a good transformer embedding model?

- Concept: Statistical depth functions and their application to multivariate data
  - Why needed here: TTE depth is a specific instance of statistical depth applied to text embeddings, providing a principled way to rank data points by centrality.
  - Quick check question: What is the relationship between statistical depth and notions of median and outlier in multivariate distributions?

- Concept: Wilcoxon rank sum test and its extension to multivariate data via depth ordering
  - Why needed here: The statistical test for detecting distributional shifts between corpora relies on the rank sum test applied to the depth-induced ordering of texts.
  - Quick check question: What is the null hypothesis being tested when using Wilcoxon rank sum on TTE depth rankings?

## Architecture Onboarding

- Component map:
  - Text embedding model (e.g., S-BERT, GenSE+) -> TTE depth calculator (cosine or chord distance) -> Wilcoxon rank sum test implementation -> Prompt selection module (for ICL tasks) -> Distributional analysis module (for corpus comparison)

- Critical path:
  1. Embed texts from corpus using chosen transformer model
  2. Compute TTE depth scores for all texts
  3. For ICL: Select top-N texts by depth as prompts
  4. For distributional analysis: Sample texts from two corpora, compute Q parameter, run Wilcoxon test

- Design tradeoffs:
  - Embedding model choice affects depth quality; S-BERT is general-purpose while GenSE+ may be task-specific
  - Distance function: cosine distance is standard but chord distance may be more sensitive to outliers
  - Sample size: larger samples improve Q estimate stability but increase computational cost

- Failure signatures:
  - Low depth score variance across corpus (suggests embedding model isn't capturing semantic structure)
  - Q estimates near 0.5 (suggests corpora are similar or sample size too small)
  - Wilcoxon test p-values consistently above 0.05 (suggests distributional differences aren't being detected)

- First 3 experiments:
  1. Verify TTE depth ordering on a small corpus with known outliers (e.g., remove one document and confirm it gets low depth)
  2. Test Q estimate stability by varying sample size from 50 to 1000 on two synthetic corpora with known distributional differences
  3. Compare ICL performance using TTE depth prompts vs random prompts on a simple text classification task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of samples to draw from two corpora for accurate TTE depth estimation and Wilcoxon rank sum test results?
- Basis in paper: Explicit - The authors investigate this in Section 4.1 through a simulation study.
- Why unresolved: The authors propose n=500 as a good sample size but acknowledge this may not be optimal for all cases and do not provide a definitive answer.
- What evidence would resolve it: Further empirical studies testing different sample sizes across various corpus sizes and characteristics, along with theoretical analysis of the trade-off between accuracy and computational efficiency.

### Open Question 2
- Question: How does the choice of text embedding model and distance function affect the results of TTE depth analysis?
- Basis in paper: Explicit - The authors briefly compare different embedding models and distance functions in Appendix A.1.
- Why unresolved: The comparison is limited to one corpus pair and does not provide a comprehensive analysis of the impact of different choices on TTE depth results.
- What evidence would resolve it: Systematic evaluation of TTE depth using various embedding models and distance functions across multiple corpus pairs and tasks, along with analysis of the factors influencing the choice of model and distance function.

### Open Question 3
- Question: Can TTE depth be extended to handle more complex corpus structures, such as hierarchical or multi-modal data?
- Basis in paper: Inferred - The authors mention potential future applications but do not address this directly.
- Why unresolved: The current definition of TTE depth assumes a single embedding space for all texts in a corpus, which may not be suitable for more complex structures.
- What evidence would resolve it: Development of extensions to TTE depth that can handle hierarchical or multi-modal data, along with evaluation of these extensions on appropriate tasks and datasets.

## Limitations
- The method assumes transformer embeddings preserve angular relationships where semantic similarity maps to cosine distance. If this assumption fails for a given embedding model or domain, TTE depth becomes unreliable.
- Sample size requirements for the Wilcoxon test are not fully characterized. The simulation study shows n=500 provides stable Q estimates, but optimal sample sizes for different tasks and distributional differences remain unclear.
- The method's sensitivity to embedding model choice is not extensively validated. While S-BERT and GenSE+ are used, performance with other models or domains is unknown.

## Confidence
- High confidence: TTE depth improves prompt selection performance in 5/6 tested tasks
- Medium confidence: Wilcoxon test framework for detecting distributional shifts, limited by simulation scope
- Medium confidence: General mechanism of using statistical depth for text embeddings, pending broader domain validation

## Next Checks
1. Test TTE depth sensitivity by comparing results across different transformer embedding models (e.g., S-BERT, SimCSE, MPNet) on the same corpora
2. Conduct power analysis for the Wilcoxon test by varying known distributional differences between synthetic corpora and measuring detection rates
3. Validate TTE depth ordering on corpora with known semantic structures (e.g., topic clusters, outliers) to confirm it captures intended properties