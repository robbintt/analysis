---
ver: rpa2
title: 'CValues: Measuring the Values of Chinese Large Language Models from Safety
  to Responsibility'
arxiv_id: '2307.09705'
source_url: https://arxiv.org/abs/2307.09705
tags:
- safety
- evaluation
- llms
- prompts
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CValues, the first Chinese human values evaluation
  benchmark for large language models, focusing on both safety and responsibility
  criteria. The benchmark includes 2,100 adversarial safety prompts across 10 scenarios
  and 800 responsibility prompts from 8 domains, collected through expert-in-the-loop
  methods involving both crowdworkers and professional experts.
---

# CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility

## Quick Facts
- arXiv ID: 2307.09705
- Source URL: https://arxiv.org/abs/2307.09705
- Reference count: 10
- Most Chinese LLMs perform well on safety but show considerable room for improvement on responsibility

## Executive Summary
This paper introduces CValues, the first Chinese human values evaluation benchmark for large language models, focusing on both safety and responsibility criteria. The benchmark includes 2,100 adversarial safety prompts across 10 scenarios and 800 responsibility prompts from 8 domains, collected through expert-in-the-loop methods involving both crowdworkers and professional experts. The evaluation employs both human annotation and automatic multi-choice formats. Results show that while Chinese LLMs perform well on safety tasks, they demonstrate significant gaps in responsibility understanding and generation, with domain-specific performance varying considerably. The study provides practical insights for developing more responsible AI systems and releases the benchmark and code for broader research use.

## Method Summary
The CValues benchmark uses two complementary evaluation methods: human evaluation and automatic multi-choice evaluation. Safety prompts are collected through adversarial testing where crowdworkers attempt to trigger safety issues in early model versions, while responsibility prompts are created by professional domain experts across 8 different fields. Human evaluation involves specialized annotators scoring model responses on safety and responsibility criteria, while automatic evaluation uses multiple-choice prompts with safe and unsafe response pairs to test comprehension. The benchmark evaluates Chinese LLMs across 10 safety scenarios and 8 responsibility domains, providing both comprehensive assessment and practical insights for AI development.

## Key Results
- Most Chinese LLMs achieve high scores on safety evaluation but show considerable room for improvement on responsibility tasks
- Automatic evaluation shows high accuracy on safety tasks but much lower accuracy on responsibility tasks, indicating a gap between understanding and generation
- Domain-specific performance varies significantly, with some areas like psychology showing stronger results than others like environment science
- Ziya-LLaMA demonstrates high accuracy on multi-choice prompts but low scores in human evaluation, highlighting the need for dual evaluation methods

## Why This Works (Mechanism)

### Mechanism 1
Expert-in-the-loop collection improves prompt quality for responsibility evaluation. Professional domain experts create induced prompts based on specialized knowledge, targeting specific social value dimensions that crowdworkers cannot reliably generate. Domain experts have better understanding of responsibility-related issues in their fields than general crowdworkers. The study invites professional experts from 8 domains such as environment science, law, and psychology to provide induced questions as responsibility prompts.

### Mechanism 2
Dual evaluation methods (human + automatic) provide comprehensive assessment of different aspects of value alignment. Automatic multi-choice evaluation tests comprehension of safe/responsible behavior, while human evaluation tests actual generation capability and nuanced judgment. Models can distinguish correct answers in multiple choice without being able to generate responsible responses independently. Automatic evaluation tends to test models' comprehension of unsafe or irresponsible behaviors, while human evaluation can measure the actual generation ability.

### Mechanism 3
Adversarial prompt collection reveals model vulnerabilities that benign prompts miss. Crowdworkers actively attempt to "attack" early model versions to generate safety issues, producing more challenging test cases than standard prompt writing. Attack-oriented prompt generation surfaces edge cases and bypasses simple safety filters. The study deploys a chatbot based on the early version of ChatPLUG and asks crowdworkers to try their best to attack the chatbot.

## Foundational Learning

- Concept: Adversarial testing methodology
  - Why needed here: The benchmark needs challenging prompts that reveal model weaknesses, not just benign test cases
  - Quick check question: What's the difference between writing prompts that test safety versus writing prompts that attempt to bypass safety mechanisms?

- Concept: Multi-dimensional value alignment
  - Why needed here: The benchmark distinguishes between safety (no harm) and responsibility (positive guidance), requiring different evaluation approaches
  - Quick check question: Can a model be safe but irresponsible? Provide an example from the paper.

- Concept: Expert-in-the-loop data collection
  - Why needed here: Responsibility prompts require domain-specific knowledge that general crowdworkers lack
  - Quick check question: Why wouldn't crowdworkers be sufficient for collecting responsibility evaluation prompts?

## Architecture Onboarding

- Component map: Data collection -> Prompt construction -> Model evaluation -> Result aggregation -> Benchmark release
- Critical path: Expert prompt collection -> Human evaluation -> Automatic evaluation -> Benchmark publication
- Design tradeoffs: Human evaluation provides reliability but is expensive; automatic evaluation is scalable but may miss nuanced cases
- Failure signatures: High automatic accuracy but low human scores indicates comprehension without generation capability; low scores in both indicates fundamental alignment issues
- First 3 experiments:
  1. Run safety evaluation on a small set of models to verify automatic vs human score correlation
  2. Test automatic evaluation with different prompt difficulties to find comprehension ceiling
  3. Validate expert scoring consistency by having multiple experts score identical responses

## Open Questions the Paper Calls Out

### Open Question 1
What are the key factors that contribute to the difference in performance between automatic and human evaluation of human values alignment in Chinese LLMs? The paper highlights the differences in performance between automatic and human evaluation, but does not provide a comprehensive analysis of the underlying factors that contribute to these differences.

### Open Question 2
How can the responsibility evaluation of Chinese LLMs be improved to better assess their alignment with human values across different domains? The paper identifies the need for improvement in the responsibility performance of Chinese LLMs and suggests that exploring the alignment of values across various domains is worthwhile.

### Open Question 3
What are the potential risks and benefits of using multi-choice prompts for automatic evaluation of human values alignment in Chinese LLMs? The paper introduces the use of multi-choice prompts for automatic evaluation of human values alignment and discusses the advantages and limitations of this approach.

## Limitations

- Expert-in-the-loop methodology lacks detailed documentation on expert selection and validation processes
- Automatic evaluation may be biased due to 71% of safe responses coming from ChatGPT, potentially creating an overly conservative standard
- Cultural specificity of prompts may limit applicability to non-Chinese contexts

## Confidence

- Safety evaluation results: High - Multiple evaluation methods converge on similar findings
- Responsibility evaluation methodology: Medium - Expert involvement is sound but lacks transparency in implementation
- Cross-model comparisons: Medium - Results appear robust but may be influenced by evaluation setup

## Next Checks

1. Conduct inter-rater reliability analysis on expert responsibility evaluations to assess consistency and potential bias
2. Test model performance using prompts from multiple cultural contexts to validate cultural specificity claims
3. Perform ablation study comparing results when using different proportions of safe vs unsafe response pairs in automatic evaluation