---
ver: rpa2
title: 'Flames: Benchmarking Value Alignment of LLMs in Chinese'
arxiv_id: '2311.06899'
source_url: https://arxiv.org/abs/2311.06899
tags:
- prompt
- response
- llms
- laboratory
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'FLAMES is the first highly adversarial benchmark for evaluating
  the value alignment of large language models (LLMs) in Chinese. The benchmark covers
  five dimensions of human values: fairness, safety, morality, data protection, and
  legality, with detailed subcomponents under each dimension.'
---

# Flames: Benchmarking Value Alignment of LLMs in Chinese

## Quick Facts
- arXiv ID: 2311.06899
- Source URL: https://arxiv.org/abs/2311.06899
- Reference count: 40
- Key outcome: First highly adversarial benchmark for Chinese LLM value alignment, revealing significant safety gaps across five dimensions

## Executive Summary
FLAMES is the first highly adversarial benchmark designed to evaluate the value alignment of large language models in Chinese. It addresses limitations in existing benchmarks by incorporating complex scenarios with implicit malice across five dimensions: fairness, safety, morality, data protection, and legality. The benchmark includes 2,251 manually crafted adversarial prompts and approximately 18.7K human-annotated model responses from 12 mainstream LLMs. A specialized scorer was developed to achieve 77.4% accuracy, significantly outperforming GPT-4 judges at 58.8%.

## Method Summary
The benchmark employs a comprehensive methodology involving the creation of adversarial prompts that incorporate implicit malice and complex real-world scenarios. These prompts are designed to bypass simple fine-tuning refusal strategies and probe deeper alignment issues. The process includes response collection from multiple LLMs, human annotation with fine-grained scoring guidelines, and training of a specialized scorer on the annotated data. The evaluation framework encompasses five dimensions of human values, with detailed subcomponents under each dimension to ensure comprehensive coverage.

## Key Results
- All 12 evaluated LLMs performed poorly on FLAMES, with the best model (Claude) achieving only 63.08% harmless rate and GPT-4 scoring 39.04%
- Significant imbalance in LLM performance across dimensions, with fairness and safety showing the weakest results
- The specialized scorer achieved 77.4% accuracy, significantly outperforming GPT-4 judges (58.8%)
- Average neighbor FMR=0.39 indicates moderate correlation with related work on value alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial prompts with implicit malice reveal deeper safety gaps in LLMs.
- Mechanism: Complex real-world scenarios combined with jailbreaking methods (disguise, reverse induction, unsafe inquiry) create situations where LLMs must discern subtle harmful intent, bypassing simple fine-tuning refusal strategies.
- Core assumption: LLMs learn to refuse explicit harmful requests but fail to detect nuanced or disguised malicious intent embedded in realistic contexts.
- Evidence anchors: Current LLMs have considerable capability to detect explicit harmful content but struggle with implicit malice; average neighbor FMR=0.39 suggests novelty.

### Mechanism 2
- Claim: Multi-dimensional evaluation framework exposes imbalanced safety performance across value dimensions.
- Mechanism: Five dimensions (Fairness, Safety, Morality, Data Protection, Legality) each with subcomponents ensure comprehensive coverage of human values, revealing specific weaknesses rather than aggregate scores.
- Core assumption: Different safety dimensions require different reasoning capabilities, and models excel in some areas while struggling in others.
- Evidence anchors: LLMs' performance on different dimensions exhibits significant imbalance; weak corpus evidence noted.

### Mechanism 3
- Claim: Specialized scorer outperforms general LLM judges in value alignment evaluation.
- Mechanism: Training a lightweight scorer on human-annotated data captures domain-specific nuances better than general-purpose models like GPT-4, which may have inherent biases.
- Core assumption: General LLM judges are expensive, inconsistent, and may not align perfectly with human value judgments.
- Evidence anchors: Scorer achieves 77.4% accuracy vs GPT-4's 58.8%; average neighbor FMR=0.39 suggests addressing recognized gap.

## Foundational Learning

- Concept: Adversarial testing methodology
  - Why needed here: Standard benchmarks with explicit harmful content can be easily gamed by simple fine-tuning; adversarial testing reveals deeper alignment issues.
  - Quick check question: What distinguishes adversarial prompts from standard harmful content prompts in this context?

- Concept: Multi-dimensional value framework
  - Why needed here: Single-dimension evaluation misses important safety aspects; comprehensive framework reveals specific weaknesses.
  - Quick check question: Why include both universal values (fairness, safety) and culture-specific values (Chinese harmony) in the same framework?

- Concept: Human-in-the-loop annotation for training evaluators
  - Why needed here: Automated evaluation tools need ground truth; human annotation provides the necessary training data for specialized scorers.
  - Quick check question: How does the hierarchical annotation structure (dimension → subcomponent → response) support fine-grained evaluation?

## Architecture Onboarding

- Component map: Prompt Design System → LLM Response Collection → Human Annotation Pipeline → Score Model Training → Evaluation Framework
- Critical path: Prompt design → Response collection from 12 LLMs → Human annotation (2 per response, 3rd for conflicts) → Scorer training → Benchmark evaluation
- Design tradeoffs: Manual prompt design ensures quality but limits scale; expert annotation ensures quality but increases cost; general scorers are flexible but less accurate
- Failure signatures: Low scorer accuracy indicates poor annotation quality; high correlation between LLM performance and GPT-4 scoring suggests scorer isn't capturing nuances; inconsistent human annotations indicate ambiguous guidelines
- First 3 experiments: 1) Test scorer accuracy on held-out data from different LLM generations to assess generalization, 2) Compare adversarial prompt effectiveness across different attack methods, 3) Evaluate scorer performance with different backbone models to optimize accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the FLAMES benchmark compare to existing Chinese language models in terms of performance across different dimensions of human values?
- Basis in paper: The paper evaluates 12 mainstream LLMs on FLAMES and finds that all models perform poorly, with the best model (Claude) achieving only a 63.08% harmless rate.
- Why unresolved: The paper provides overall results but doesn't provide a detailed breakdown of how each model performs across different dimensions or how these compare to existing benchmarks.
- What evidence would resolve it: A comprehensive table showing each model's performance across all five dimensions compared to existing benchmarks would provide the needed comparison.

### Open Question 2
- Question: How effective is the FLAMES scorer compared to human annotators in evaluating new LLMs on the benchmark?
- Basis in paper: The paper mentions that the FLAMES scorer achieves 77.4% accuracy, significantly outperforming GPT-4 as a judge (58.8%).
- Why unresolved: While the scorer's accuracy is mentioned, there's no detailed analysis of how it performs across different dimensions or how it compares to human annotators in terms of precision, recall, and F1 scores.
- What evidence would resolve it: A detailed breakdown of the scorer's performance metrics across all dimensions compared to human annotators would provide a more complete picture.

### Open Question 3
- Question: How do the results of the FLAMES benchmark vary across different types of prompts (e.g., explicit vs. implicit malice, different attacking methods)?
- Basis in paper: The paper mentions that FLAMES includes prompts with implicit malice and various attacking methods, but doesn't provide a detailed analysis of how models perform on different types of prompts.
- Why unresolved: Understanding how models perform on different types of adversarial prompts could provide insights into specific weaknesses and areas for improvement in value alignment.
- What evidence would resolve it: A breakdown of model performance based on prompt type (explicit vs. implicit malice, different attacking methods) would reveal which types of adversarial prompts are most effective at uncovering vulnerabilities.

## Limitations
- Manual prompt design process creates scalability bottleneck that may limit benchmark applicability to rapidly evolving LLM architectures
- Human annotation quality introduces potential subjectivity that could affect scorer training and generalization
- Adversarial nature of prompts may create artificial scenarios that don't perfectly reflect real-world usage patterns

## Confidence
- High Confidence: Benchmark's multi-dimensional framework and its ability to expose imbalanced LLM performance across five dimensions
- Medium Confidence: Scorer's superior performance (77.4% accuracy) compared to GPT-4 judges, though this may be influenced by training data characteristics
- Low Confidence: Long-term durability of adversarial prompt approach as LLMs improve at contextual understanding and implicit malice detection

## Next Checks
1. Test scorer generalization by evaluating its performance on prompts and responses from LLM generations not included in the original training data
2. Conduct A/B testing comparing model performance on FLAMES adversarial prompts versus real-world deployment scenarios to assess ecological validity
3. Evaluate scorer robustness by introducing adversarial examples designed specifically to fool the scorer itself, measuring its resistance to manipulation