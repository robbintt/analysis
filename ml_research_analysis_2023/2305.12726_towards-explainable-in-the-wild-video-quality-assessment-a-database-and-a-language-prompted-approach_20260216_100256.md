---
ver: rpa2
title: 'Towards Explainable In-the-Wild Video Quality Assessment: A Database and a
  Language-Prompted Approach'
arxiv_id: '2305.12726'
source_url: https://arxiv.org/abs/2305.12726
tags:
- quality
- video
- videos
- factors
- plcc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Maxwell database, a large-scale in-the-wild
  video quality assessment database containing over two million human opinions on
  4,543 videos across 13 quality-related factors and 3 abstract quality ratings. This
  database enables objective VQA methods to provide specific quality evaluations.
---

# Towards Explainable In-the-Wild Video Quality Assessment: A Database and a Language-Prompted Approach

## Quick Facts
- arXiv ID: 2305.12726
- Source URL: https://arxiv.org/abs/2305.12726
- Authors: 
- Reference count: 40
- Key outcome: Introduces Maxwell database with 4,543 videos and 2.54M opinions across 13 quality factors, and proposes MaxVQA language-prompted approach achieving SOTA on all dimensions

## Executive Summary
This paper addresses the challenge of explainable video quality assessment (VQA) by introducing the Maxwell database, a large-scale in-the-wild VQA database containing over two million human opinions on 4,543 videos across 13 specific quality-related factors and 3 abstract quality ratings. Based on this database, the authors develop MaxVQA, a language-prompted VQA approach that jointly predicts specific quality factors and overall quality scores. MaxVQA integrates CLIP with FAST-VQA features and uses learnable contextual prompts to achieve state-of-the-art accuracy on all dimensions of Maxwell and existing VQA datasets, demonstrating excellent generalization ability.

## Method Summary
The Maxwell database was collected through an in-lab study with trained subjects evaluating videos across 13 quality dimensions using ternary choice questions. MaxVQA combines CLIP's semantic understanding with FAST-VQA's detail-aware features, fusing them through an MLP-based module. The approach uses learnable contextual prompts optimized during training, which are inserted before initial dimension-specific prompts. Quality evaluation is performed through vision-language similarity calculation and softmax pooling, with the model fine-tuned on the unified Maxwell database.

## Key Results
- Maxwell database contains 4,543 videos with 2.54 million human opinions across 16 dimensions
- MaxVQA achieves SOTA performance on all dimensions of Maxwell database
- MaxVQA demonstrates excellent generalization ability on existing VQA datasets
- Learnable prompts show significant improvement over fixed prompts in quality prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Maxwell database bridges the gap between abstract quality scores and specific quality factors by collecting large-scale human opinions on 13 distinct quality-related factors.
- Mechanism: By prompting subjects to evaluate each factor with a ternary choice (positive, neutral, negative), the database captures nuanced human perception of quality issues that are otherwise obscured when only overall scores are considered.
- Core assumption: Human perception of video quality is multi-dimensional and affected by multiple factors simultaneously.
- Evidence anchors:
  - [abstract]: "how the abstract quality scores relate with specific factors is still obscure, hindering VQA methods from more concrete quality evaluations"
  - [section 3.1.2]: "we design a ternary choice question for each dimension, including the neutral choice and a pair of antonyms to describe the positive and negative choices"
- Break condition: If the neutral choice leads to ambiguous or inconsistent labeling, reducing the reliability of the collected data.

### Mechanism 2
- Claim: MaxVQA achieves superior performance by integrating CLIP with FAST-VQA features to enhance low-level and temporal distortion detection.
- Mechanism: CLIP provides strong semantic understanding but lacks sensitivity to low-level details and temporal modeling; FAST-VQA compensates by offering detail-aware and temporally-aligned features, which are fused to create a more comprehensive quality representation.
- Core assumption: Low-level textures and temporal variations are critical for accurate VQA, especially for in-the-wild videos.
- Evidence anchors:
  - [abstract]: "MaxVQA integrates CLIP with FAST-VQA features and uses learnable contextual prompts"
  - [section 4.2.2]: "we fuse the CLIP feature with the FAST-VQA features... the state-of-the-art VQA-specific features which proved excellent performance on several VQA datasets"
- Break condition: If the feature fusion module introduces noise or misalignment between CLIP and FAST-VQA feature spaces, degrading performance.

### Mechanism 3
- Claim: Learnable contextual prompts in MaxVQA enable fine-grained quality assessment by optimizing text prompts for each dimension.
- Mechanism: Unlike fixed prompts, learnable prompts are initialized with dimension-specific descriptions and optimized during training, allowing the model to better align textual descriptions with human perception of specific quality factors.
- Core assumption: Standard text prompts (e.g., "good/bad") are insufficient for capturing the nuances of specific quality factors.
- Evidence anchors:
  - [abstract]: "MaxVQA... modifies vision-language foundation model CLIP to better capture important quality issues"
  - [section 4.3]: "we choose the simple and efficient contextual prompt [66] to optimize these initial prompts, by inserting a single context token before the initial prompts"
- Break condition: If the learnable prompts overfit to the training data and fail to generalize to new quality factors or datasets.

## Foundational Learning

- Concept: Vision-language models (e.g., CLIP)
  - Why needed here: CLIP provides a pre-trained foundation for aligning visual and textual features, which is essential for the language-prompted VQA approach.
  - Quick check question: What is the primary advantage of using CLIP in MaxVQA compared to training a VQA model from scratch?

- Concept: Video quality assessment (VQA)
  - Why needed here: Understanding the challenges of VQA, especially for in-the-wild videos, is crucial for appreciating the design choices in the Maxwell database and MaxVQA.
  - Quick check question: How does VQA for in-the-wild videos differ from traditional VQA focused on specific distortions?

- Concept: Feature fusion techniques
  - Why needed here: The fusion of CLIP and FAST-VQA features is a key component of MaxVQA's architecture, requiring knowledge of how to effectively combine features from different modalities.
  - Quick check question: What are the potential challenges of fusing features from CLIP and FAST-VQA, and how might they be addressed?

## Architecture Onboarding

- Component map: Maxwell database -> Feature extraction (CLIP + FAST-VQA) -> Prompt optimization -> Quality prediction
- Critical path: Data collection → Feature extraction (CLIP + FAST-VQA) → Prompt optimization → Quality prediction
- Design tradeoffs:
  - Using CLIP provides strong semantic understanding but requires careful integration with domain-specific features (FAST-VQA) to capture low-level details.
  - Learnable prompts offer flexibility but introduce additional parameters that need to be optimized.
- Failure signatures:
  - Poor performance on specific quality factors: Likely due to inadequate feature fusion or prompt optimization.
  - Overfitting to training data: May occur if learnable prompts are not regularized or if the dataset is too small.
- First 3 experiments:
  1. Evaluate MaxVQA's performance on each quality factor separately to identify weaknesses.
  2. Ablation study: Remove FAST-VQA features to assess their impact on performance.
  3. Cross-dataset evaluation: Test MaxVQA's generalization ability on existing VQA datasets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific quality factors interact with each other in real-world videos, and can we develop a more sophisticated model to capture these interactions?
- Basis in paper: [explicit] The paper mentions that "different quality issues can concurrently exist and affect the quality of an in-the-wild video" and provides examples of factors that tend to happen together (e.g., Sharpness and Noise).
- Why unresolved: While the paper analyzes correlations between factors, it doesn't fully explore the complex interactions and potential causal relationships between them.
- What evidence would resolve it: A comprehensive study analyzing the joint distributions of factor scores and their impact on overall quality, potentially using causal inference techniques.

### Open Question 2
- Question: Can we develop a more efficient and effective way to collect subjective opinions on specific quality factors for large-scale video datasets?
- Basis in paper: [explicit] The paper describes a labor-intensive in-lab study with trained subjects, which may not be scalable for larger datasets.
- Why unresolved: The current approach relies on human subjects, which is time-consuming and may not be practical for very large video collections.
- What evidence would resolve it: Development and validation of a semi-automated or crowdsourced approach that can efficiently collect reliable subjective opinions on specific quality factors.

### Open Question 3
- Question: How do cultural and individual differences influence the perception of specific quality factors in videos?
- Basis in paper: [inferred] The paper mentions that "human visual system is especially sensitive to temporal quality," but doesn't explore potential differences in perception across different demographics or cultures.
- Why unresolved: The study only considers a single group of subjects, and there's no analysis of how different viewers might perceive the same video differently based on their background or preferences.
- What evidence would resolve it: A cross-cultural study comparing subjective opinions on specific quality factors across different demographic groups or regions.

## Limitations
- The ternary choice methodology for collecting human opinions may introduce subjectivity and potential inconsistencies in labeling
- The database's focus on UGC videos may limit generalizability to other video domains
- MaxVQA's reliance on CLIP and FAST-VQA features assumes these models capture the relevant aspects of video quality

## Confidence
- Maxwell database design and collection: High
- MaxVQA's architectural claims: Medium
- Cross-dataset generalization claims: Medium
- Learnable prompts effectiveness: Low

## Next Checks
1. Conduct an inter-rater reliability analysis on the Maxwell database to quantify the consistency of ternary choices across different annotators.
2. Perform an ablation study removing the FAST-VQA features to quantify their contribution to overall performance.
3. Test MaxVQA's performance on out-of-distribution videos (e.g., professionally produced content) to assess real-world generalization.