---
ver: rpa2
title: 'lfads-torch: A modular and extensible implementation of latent factor analysis
  via dynamical systems'
arxiv_id: '2309.01230'
source_url: https://arxiv.org/abs/2309.01230
tags:
- lfads-torch
- neural
- lfads
- data
- implementation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: lfads-torch is a modern PyTorch-based implementation of the LFADS
  neural population model that leverages dynamic computation graphs, modular configuration,
  and simplified large-scale training workflows. Built on PyTorch Lightning and Hydra,
  it provides intuitive interfaces for data augmentation, modular priors and reconstruction
  models, and easy hyperparameter tuning.
---

# lfads-torch: A modular and extensible implementation of latent factor analysis via dynamical systems

## Quick Facts
- arXiv ID: 2309.01230
- Source URL: https://arxiv.org/abs/2309.01230
- Reference count: 19
- lfads-torch achieves near-identical performance to TensorFlow implementation on Neural Latents Benchmark datasets with velocity correlation (R2) and firing rate correlation (fp-bps) metrics matching closely.

## Executive Summary
lfads-torch is a modern PyTorch implementation of the LFADS neural population model that addresses limitations of the original TensorFlow version. Built on PyTorch Lightning and Hydra, it provides a modular, extensible framework for latent factor analysis of neural population activity. The implementation supports flexible data augmentation, modular prior distributions, and multiple observation models, making it applicable to diverse neural recording modalities including spike counts, EMG, and calcium imaging.

## Method Summary
lfads-torch implements a variational sequential autoencoder with RNN dynamics for denoising and inferring latent structure in high-dimensional neural data. The architecture uses PyTorch Lightning for training infrastructure and Hydra for composable configuration management. Key innovations include an AugmentationStack for modular data augmentation, customizable prior distributions through user-implemented modules, and support for multiple reconstruction models (Poisson, Gaussian, Gamma, ZeroInflatedGamma) to handle different neural data modalities.

## Key Results
- Nearly identical performance metrics (velocity correlation R2, firing rate correlation fp-bps) compared to TensorFlow implementation across MC Maze, MC RTT, Area2 Bump, and DMFC RSG datasets
- Validation on NLB benchmark shows R2 values ranging from 0.32-0.54 and fp-bps values from 0.07-0.24 across datasets
- Modular implementation enables experimentation with data augmentation strategies and custom prior distributions without modifying core model code

## Why This Works (Mechanism)

### Mechanism 1
The modular augmentation stack enables easy experimentation with data augmentation strategies without modifying core model code. The AugmentationStack class allows users to compose multiple augmentation transformations and specify their application order on data batches, loss tensors, or both. Separate stacks are automatically applied during training and inference.

### Mechanism 2
Modular prior distributions allow users to experiment with different assumptions about latent input structure without modifying core model code. Users can implement custom priors by writing make_posterior and forward functions and passing the modules to the LFADS constructor. The implementation includes multivariate normal, autoregressive multivariate normal, and multivariate Student's t priors.

### Mechanism 3
Modular reconstruction models allow lfads-torch to handle diverse neural data modalities without modifying core model code. The implementation includes Poisson, Gaussian, Gamma, and ZeroInflatedGamma distributions for different data types. Users can implement new observation models by subclassing the abstract Reconstruction class.

## Foundational Learning

- Concept: Variational Autoencoders (VAEs)
  - Why needed here: LFADS is a VAE-based model that learns latent representations of neural data through a probabilistic framework.
  - Quick check question: What is the role of the KL divergence term in the VAE loss function?

- Concept: Recurrent Neural Networks (RNNs)
  - Why needed here: LFADS uses RNNs to model temporal dynamics in neural population activity.
  - Quick check question: How do RNNs differ from feedforward neural networks in terms of their ability to model sequential data?

- Concept: PyTorch Lightning and Hydra
  - Why needed here: These libraries provide the framework for modular model configuration and training in lfads-torch.
  - Quick check question: How does Hydra's composable configuration system differ from traditional command-line argument parsing?

## Architecture Onboarding

- Component map:
  AugmentationStack -> Modular priors -> Modular reconstruction -> PyTorch Lightning integration -> Hydra configuration

- Critical path:
  1. Data preprocessing and loading
  2. Model instantiation with specified configuration
  3. Training loop with augmentation and regularization
  4. Inference and evaluation

- Design tradeoffs:
  - Flexibility vs. complexity: Modular design allows more customization but may increase cognitive load
  - Performance vs. ease of use: Using dynamic computation graphs (PyTorch) vs. static graphs (TensorFlow) improves debugging but may have performance implications
  - Generality vs. specificity: Supporting multiple data modalities requires more complex code but increases applicability

- Failure signatures:
  - NaN values in loss: Could indicate issues with data preprocessing or numerical instability
  - Poor reconstruction quality: May suggest incorrect observation model or hyperparameters
  - Overfitting: Could indicate insufficient regularization or data augmentation

- First 3 experiments:
  1. Train a basic LFADS model on a simple neural dataset (e.g., MC Maze) to verify basic functionality
  2. Experiment with different data augmentation strategies using the AugmentationStack
  3. Try different prior distributions (e.g., MultivariateStudentT) to see their effect on latent space structure

## Open Questions the Paper Calls Out

### Open Question 1
How do different modular priors (e.g., MultivariateStudentT) impact LFADS performance across various neural data modalities and brain regions?
- Basis in paper: The paper explicitly mentions that users can experiment with custom priors by implementing make posterior and forward functions, and provides a MultivariateStudentT prior to encourage inference of heavy-tailed inputs.
- Why unresolved: The paper validates the implementation against previous versions but doesn't explore the impact of different priors on model performance across diverse datasets or brain regions.
- What evidence would resolve it: Systematic experiments comparing performance metrics (e.g., velocity correlation, firing rate correlation) of LFADS models with different modular priors across multiple neural datasets and brain regions.

### Open Question 2
How does the choice of modular reconstruction models (e.g., Gamma, ZeroInflatedGamma) affect LFADS performance for different neural recording modalities like EMG, calcium imaging, or ECoG?
- Basis in paper: The paper explicitly mentions that lfads-torch includes implementations of Poisson, Gaussian, Gamma, and ZeroInflatedGamma distributions for modular reconstruction, widening applicability to different data modalities.
- Why unresolved: While the paper introduces these reconstruction models, it doesn't provide empirical comparisons of their performance across different neural recording modalities.
- What evidence would resolve it: Empirical studies comparing LFADS model performance using different reconstruction models across EMG, calcium imaging, and ECoG datasets, measuring metrics like velocity correlation and firing rate correlation.

### Open Question 3
How does the new modular configuration system in lfads-torch impact the ease of experimentation and model development compared to the previous implementation?
- Basis in paper: The paper states that the flexible configuration process in lfads-torch gives users significant control over model and architecture configuration without needing to edit source code, and mentions that run configurations are built by composing a main YAML config file with selected model, datamodule, and callbacks configs.
- Why unresolved: The paper claims improved modularity and ease of use but doesn't provide quantitative or qualitative comparisons of the development experience between lfads-torch and previous implementations.
- What evidence would resolve it: User studies or developer surveys comparing the time and effort required to implement new architectures or experiment with different configurations in lfads-torch versus the previous TensorFlow implementation.

## Limitations
- The paper does not provide specific hyperparameter settings or random seed configurations used in the NLB benchmark validation, making exact reproduction challenging
- While modular design enables flexibility, the complexity of the implementation may create barriers for users unfamiliar with PyTorch Lightning and Hydra
- The validation only demonstrates parity with the original TensorFlow implementation on a limited set of benchmark datasets

## Confidence
- **High Confidence**: Core functionality claims (modular augmentation, priors, and reconstruction models) are well-supported by the implementation code and documentation
- **Medium Confidence**: Performance claims are validated against TensorFlow implementation but lack independent benchmark verification
- **Medium Confidence**: Claims about ease of use and rapid experimentation are supported by design principles but not empirically tested with end-users

## Next Checks
1. Reproduce training on MC Maze dataset with provided configuration to verify metric parity with TensorFlow implementation
2. Implement a custom prior distribution (e.g., Laplace prior) to test the modular prior interface
3. Apply multiple data augmentation strategies in sequence to verify the AugmentationStack functionality and ordering behavior