---
ver: rpa2
title: 'DeforestVis: Behavior Analysis of Machine Learning Models with Surrogate Decision
  Stumps'
arxiv_id: '2304.00133'
source_url: https://arxiv.org/abs/2304.00133
tags:
- decision
- surrogate
- stumps
- data
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DeforestVis, a visual analytics tool for
  interpreting complex machine learning models through surrogate decision stumps.
  The tool employs AdaBoost to generate one-level decision trees (stumps) that approximate
  complex models while maintaining interpretability.
---

# DeforestVis: Behavior Analysis of Machine Learning Models with Surrogate Decision Stumps

## Quick Facts
- arXiv ID: 2304.00133
- Source URL: https://arxiv.org/abs/2304.00133
- Reference count: 40
- Key outcome: Visual analytics tool using AdaBoost-generated decision stumps to interpret complex ML models while maintaining interpretability through linked views and complexity-fidelity trade-off visualization

## Executive Summary
DeforestVis is a visual analytics tool that addresses the challenge of interpreting complex machine learning models by generating surrogate decision stumps using AdaBoost. The system creates one-level decision trees that approximate complex model behavior while remaining interpretable. Users can explore the trade-off between model complexity and fidelity through an incremental generation approach, with multiple linked views enabling model selection, behavioral summarization, rule overriding, and test case analysis. Expert evaluations with data analysts and model developers confirmed the tool's effectiveness in making complex models more interpretable without significant accuracy loss.

## Method Summary
DeforestVis employs AdaBoost to generate one-level decision trees (stumps) that serve as surrogate models for complex ML systems. The tool incrementally increases the number of stumps to allow users to explore the complexity-fidelity trade-off, visualized through a lollipop plot. Each stump focuses on a single feature split, and weighted combinations preserve interpretability while approximating the target model's behavior. The system includes interactive views for model selection, behavioral summarization through UMAP projections, rule overriding with visual feedback, and test set analysis. Users can adjust stump thresholds and immediately see local and global impacts on training instance allocation.

## Key Results
- Expert evaluations confirmed the tool's effectiveness in making complex ML models interpretable
- The complexity-fidelity trade-off visualization successfully guides users to optimal stump configurations
- Manual rule overriding with visual feedback enables domain experts to inject knowledge without overfitting
- The system maintains good performance while significantly improving model interpretability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The tool improves interpretability by using one-level decision trees (stumps) instead of deep trees.
- Mechanism: AdaBoost trains multiple shallow stumps, each focusing on a single feature split. The stumps are weighted and combined, preserving interpretability while approximating complex model behavior.
- Core assumption: Users can understand and act on single-feature decision boundaries more easily than multi-feature, deep trees.
- Evidence anchors:
  - [abstract]: "one-level decision trees (stumps) generated with the Adaptive Boosting (AdaBoost) technique"
  - [section]: "DEFOREST VIS creates such stumps using the adaptive boosting (AdaBoost) method [62]"
  - [corpus]: Weak - corpus doesn't contain direct mentions of stumps or AdaBoost usage in this context.
- Break condition: If the complex model's behavior requires multi-feature interactions that a single stump cannot capture, fidelity will degrade rapidly.

### Mechanism 2
- Claim: The complexity-fidelity trade-off visualization guides users to select the optimal number of stumps.
- Mechanism: A lollipop plot maps fidelity vs. complexity, allowing users to see how adding stumps improves accuracy. Rounding threshold precision reduces cognitive load without large fidelity loss.
- Core assumption: Users can identify a "sweet spot" where added complexity yields diminishing returns in fidelity.
- Evidence anchors:
  - [abstract]: "explore the complexity versus fidelity trade-off by incrementally generating more stumps"
  - [section]: "DEFOREST VIS shows their training-prediction accuracy with a lollipop plot (y-axis: accuracy; x-axis: complexity)"
  - [corpus]: Weak - corpus lacks specific discussion of complexity-fidelity trade-off visualization techniques.
- Break condition: If the model's decision boundaries are too intricate, even many stumps may fail to capture them, making the trade-off curve flat or misleading.

### Mechanism 3
- Claim: Manual rule overriding with visual feedback enables domain experts to inject knowledge without overfitting.
- Mechanism: Users adjust thresholds in individual stumps and see immediate local/global impact via UMAP projections and segmented bar charts. This allows hypothesis testing before committing changes.
- Core assumption: Visual feedback on rule changes is sufficient for users to judge whether adjustments generalize.
- Evidence anchors:
  - [abstract]: "analyzing the impact of rule overriding on training instance allocation between one or more stumps"
  - [section]: "users can visually inspect both the local and global impact of a change in a rule"
  - [corpus]: Weak - corpus does not discuss rule overriding mechanisms or visual feedback loops.
- Break condition: If the surrogate model is too sensitive to small threshold changes, users may inadvertently overfit to training data.

## Foundational Learning

- Concept: AdaBoost algorithm mechanics
  - Why needed here: Understanding how AdaBoost combines weighted stumps is critical to interpreting the tool's outputs and making informed adjustments.
  - Quick check question: How does AdaBoost assign weights to stumps, and how are those weights used to compute final predictions?

- Concept: Gini impurity and decision tree splits
  - Why needed here: The tool highlights stumps with high impurity; knowing what impurity measures helps users identify problematic rules.
  - Quick check question: What does a high Gini impurity value indicate about a stump's class separation quality?

- Concept: UMAP dimensionality reduction
  - Why needed here: The projection view shows how samples move when rules change; understanding UMAP helps users trust or question cluster interpretations.
  - Quick check question: What does it mean if two points are close in UMAP space but belong to different decision stumps?

## Architecture Onboarding

- Component map: Frontend (Vue.js + D3.js + Plotly.js) -> Backend (Flask + Scikit-Learn) -> Data Flow (User selection -> AdaBoost generation -> Reactive visualizations)
- Critical path:
  1. User selects complexity level in lollipop plot
  2. Backend trains AdaBoost with n_estimators matching selection
  3. Stump weights and thresholds are computed
  4. Frontend renders linked views (summary, rule override, test results)
- Design tradeoffs:
  - Shallow stumps vs. deep trees: Better interpretability but potentially lower fidelity
  - Decimal precision vs. cognitive load: Fewer decimals ease understanding but may reduce accuracy
  - UMAP vs. t-SNE: UMAP scales better and preserves global structure, important for many stumps
- Failure signatures:
  - Lollipop plot flat: Model too complex for stumps to approximate
  - UMAP projection noisy: High-dimensional interactions not captured by single-feature splits
  - Test set accuracy drops after rule changes: Overfitting to training data
- First 3 experiments:
  1. Load a simple binary dataset (e.g., breast cancer), run with default complexity, verify lollipop plot shows increasing fidelity.
  2. Adjust a stump threshold in rule override view, observe changes in UMAP projection and test results.
  3. Toggle between performance and uniqueness modes in stump selection view, confirm visual differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed approach be extended to handle multi-class classification problems effectively?
- Basis in paper: [explicit] The paper mentions that multi-class classification is perhaps the most limiting factor when using AdaBoost as a surrogate model, but suggests a binary one-vs-rest strategy could be used.
- Why unresolved: The paper does not provide detailed implementation or evaluation of this approach for multi-class problems, and the effectiveness of this strategy for complex models remains untested.
- What evidence would resolve it: Empirical results comparing the performance of the proposed tool with multi-class extensions against other interpretable surrogate methods on benchmark datasets with 3+ classes.

### Open Question 2
- Question: What are the optimal precision thresholds for decision stump rules that balance interpretability and fidelity across different domains?
- Basis in paper: [inferred] The paper discusses rounding threshold values to reduce cognitive load but doesn't provide domain-specific guidelines or empirical studies on optimal precision levels.
- Why unresolved: The paper shows examples of rounding but doesn't systematically study how different precision levels affect model performance across various data types and domains.
- What evidence would resolve it: Comprehensive evaluation studies measuring fidelity loss versus interpretability gains at different precision levels across multiple domains and data types.

### Open Question 3
- Question: How can the scalability limitations regarding large numbers of features and training samples be effectively addressed?
- Basis in paper: [explicit] Expert feedback highlighted scalability issues with many features and samples, suggesting aggregation strategies but not providing concrete solutions.
- Why unresolved: While the paper mentions using AdaBoost to automatically generate stumps for important features, it doesn't provide systematic approaches for handling high-dimensional data or large sample sizes.
- What evidence would resolve it: Implementation and evaluation of specific dimensionality reduction or aggregation techniques within the tool, demonstrating maintained interpretability and performance on high-dimensional datasets.

## Limitations
- The tool may not scale well to high-dimensional datasets or datasets with a large number of training instances
- Visual encodings and interactions may be confusing or overwhelming for non-expert users
- The paper lacks quantitative evaluation of how well the visual feedback helps users avoid overfitting

## Confidence

**Major Uncertainties**
- The paper lacks specific details about how users are expected to interpret and act on the fidelity-complexity trade-off curve, particularly for non-experts
- No quantitative evaluation of how well the visual feedback in the rule overriding interface helps users avoid overfitting
- Limited discussion of failure cases when the surrogate model cannot capture complex decision boundaries

**Confidence Assessment**
- **High Confidence**: The AdaBoost-based surrogate generation mechanism and the general framework of using stumps for interpretability are well-established techniques
- **Medium Confidence**: The visualization design appears reasonable but lacks user study data to confirm effectiveness for non-expert users
- **Low Confidence**: The paper claims effectiveness through expert evaluations but provides minimal methodological details about these evaluations

## Next Checks

1. **Baseline Comparison**: Compare DeforestVis's fidelity-accuracy trade-off curves against alternative surrogate methods (LIME, SHAP) on the same datasets to quantify performance differences

2. **User Study Protocol**: Design a controlled experiment testing whether users can successfully identify and correct model biases using the rule override interface, measuring both accuracy and time-to-completion

3. **Failure Case Analysis**: Systematically test the tool on datasets with known complex interactions (e.g., XOR patterns, feature interactions) to identify when stump-based surrogates break down