---
ver: rpa2
title: Do Large Language Models Know about Facts?
arxiv_id: '2310.05177'
source_url: https://arxiv.org/abs/2310.05177
tags:
- llms
- factual
- language
- knowledge
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Pinocchio, a comprehensive benchmark with
  20K diverse factual questions to evaluate large language models'' (LLMs) factual
  knowledge and reasoning capabilities. The benchmark covers seven tasks: multifaceted,
  structural, adversarial, temporal, real-world, domain-specific, and multilingual.'
---

# Do Large Language Models Know about Facts?

## Quick Facts
- **arXiv ID**: 2310.05177
- **Source URL**: https://arxiv.org/abs/2310.05177
- **Reference count**: 40
- **Primary result**: Introduces Pinocchio benchmark with 20K factual questions to evaluate LLMs' factual knowledge and reasoning capabilities

## Executive Summary
This paper introduces Pinocchio, a comprehensive benchmark with 20K diverse factual questions designed to evaluate large language models' (LLMs) factual knowledge and reasoning capabilities. The benchmark covers seven distinct tasks including multifaceted, structural, adversarial, temporal, real-world, domain-specific, and multilingual questions. Through extensive experiments on 10 different LLMs using various prompting strategies, the authors demonstrate that existing models still struggle with factual accuracy despite advances in model size and training approaches.

The study reveals that even state-of-the-art models like GPT-3.5-Turbo achieve only 47.0% overall accuracy, with particular weaknesses in temporal reasoning, numerical calculations, and fine-grained distinctions. The research also highlights the importance of prompt-language alignment, showing that LLMs perform better when prompts match the language of the questions. Additionally, the authors find that while self-consistency and self-refinement methods can improve performance, increasing chain-of-thought complexity can paradoxically lead to decreased accuracy.

## Method Summary
The researchers constructed Pinocchio, a benchmark containing 20K factual questions across seven task categories sourced from established datasets including FEVER, FEVEROUS, and others. They evaluated 10 LLMs of varying sizes and types (including OPT, BLOOM, LLaMA, Alpaca, Vicuna, ChatGLM, Flan-T5, and GPT variants) using four prompting strategies: zero-shot, zero-shot with chain-of-thought, few-shot, and few-shot with chain-of-thought. Performance was measured using accuracy and F1 scores across all tasks and prompting conditions.

## Key Results
- GPT-3.5-Turbo achieved the best overall performance at 47.0% accuracy, but still struggled with temporal questions, numerical reasoning, and fine-grained labels
- LLMs perform better when using prompts in the same language as the questions, particularly for non-English tasks
- Self-consistency and self-refinement methods improve performance by mitigating hallucination and allowing error correction
- Increasing chain-of-thought complexity can lead to decreased performance due to difficulty in extracting generalized reasoning patterns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs exhibit performance improvements when using prompts in the same language as the questions.
- **Mechanism**: Same-language prompts leverage better multilingual representations and reduce language-switching overhead.
- **Core assumption**: LLMs have sufficiently balanced multilingual representations.
- **Evidence anchors**: Experimental results show improved performance with Chinese prompts versus English prompts.
- **Break condition**: If multilingual training data is heavily imbalanced.

### Mechanism 2
- **Claim**: Increasing chain-of-thought complexity can lead to decreased performance.
- **Mechanism**: Complex prompts introduce longer reasoning chains that overwhelm the model's capacity to maintain factual accuracy.
- **Core assumption**: LLMs have finite capacity for processing complex reasoning chains.
- **Evidence anchors**: Performance deterioration observed as CoT complexity increases.
- **Break condition**: If model is specifically trained on complex reasoning tasks.

### Mechanism 3
- **Claim**: Self-consistency and self-refinement methods improve performance.
- **Mechanism**: Self-consistency aggregates multiple judgments through majority voting, while self-refinement allows correction through feedback.
- **Core assumption**: LLMs can recognize and correct their own errors.
- **Evidence anchors**: Marked performance boosts observed with these methods.
- **Break condition**: If factual knowledge is severely limited or feedback mechanism misaligned.

## Foundational Learning

- **Concept**: Factual knowledge retrieval from LLMs
  - **Why needed here**: Understanding how LLMs store and retrieve factual information is essential for evaluating their factual knowledge capabilities.
  - **Quick check question**: How do LLMs store and retrieve factual knowledge, and what are the limitations compared to traditional knowledge bases?

- **Concept**: Chain-of-thought prompting
  - **Why needed here**: The paper investigates how different prompting strategies including chain-of-thought affect factual accuracy.
  - **Quick check question**: What is chain-of-thought prompting and how does it influence reasoning and factual accuracy?

- **Concept**: Multilingual representations in LLMs
  - **Why needed here**: The paper evaluates multilingual factual tasks, highlighting the importance of cross-lingual transfer.
  - **Quick check question**: How do LLMs represent multilingual information and what are the challenges for cross-lingual knowledge transfer?

## Architecture Onboarding

- **Component map**: Pinocchio benchmark (20K questions) -> 10 LLMs (various sizes/types) -> 4 prompting strategies (zero-shot, zero-shot with CoT, few-shot, few-shot with CoT) -> Evaluation metrics (accuracy, F1 score)

- **Critical path**: Construct Pinocchio benchmark → Select and prepare LLM models → Design prompt strategies → Run experiments → Analyze results

- **Design tradeoffs**: Question diversity vs. task specificity; Model size vs. computational efficiency; Prompt complexity vs. performance

- **Failure signatures**: Poor multilingual performance; Decreased performance with complex CoT; Inconsistent performance across tasks

- **First 3 experiments**:
  1. Evaluate small instruction-tuned LLM on Pinocchio subset using zero-shot prompts
  2. Compare prompt strategies on temporal questions
  3. Assess impact of model size on factual recall by comparing different sized LLMs

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal number of in-context learning examples for improving factual reasoning performance?
- **Basis in paper**: Performance improves beyond a certain threshold of examples but exact optimal number is not specified
- **Why unresolved**: Paper only notes performance improves beyond a point without specifying the threshold
- **What evidence would resolve it**: Experiments with varying numbers of examples to identify diminishing returns point

### Open Question 2
- **Question**: How do different prompting strategies affect factual accuracy across various languages?
- **Basis in paper**: Paper notes same-language prompts improve performance but doesn't explore multiple languages in detail
- **Why unresolved**: Limited exploration of prompting strategies across multiple target languages
- **What evidence would resolve it**: Testing multiple strategies across various languages and comparing impact

### Open Question 3
- **Question**: What are the underlying mechanisms causing LLMs to struggle with numerical reasoning and fine-grained distinctions?
- **Basis in paper**: LLMs exhibit deficiencies in numerical calculations and fine-grained labels
- **Why unresolved**: Paper doesn't delve into specific mechanisms behind these struggles
- **What evidence would resolve it**: Analyzing internal representations and decision processes for numerical and fine-grained tasks

## Limitations

- Experimental design focuses on comparing strategies without deeply investigating underlying mechanisms
- Multilingual evaluation limited to Chinese without exploring other languages or granular language-specific effects
- Does not investigate optimal number of in-context examples or specific complexity thresholds

## Confidence

**High Confidence**: GPT-3.5-Turbo's 47.0% accuracy performance (well-supported by experimental data)

**Medium Confidence**: Self-consistency and self-refinement methods improve performance (reported improvements but generalizability not fully established)

**Medium Confidence**: Increasing chain-of-thought complexity decreases performance (evidence provided but relationship may vary by task and model)

## Next Checks

1. **Cross-lingual Prompting Experiment**: Systematically compare cross-language versus same-language prompts across multiple target languages beyond Chinese to validate language-alignment hypothesis

2. **Complexity Threshold Analysis**: Conduct granular analysis of chain-of-thought prompt complexity at multiple levels to identify specific performance degradation thresholds

3. **Model Architecture Analysis**: Investigate whether performance patterns regarding prompt complexity vary across different model architectures (pre-training vs. instruction-tuned vs. RLHF)