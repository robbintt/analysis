---
ver: rpa2
title: Concept Bottleneck with Visual Concept Filtering for Explainable Medical Image
  Classification
arxiv_id: '2308.11920'
source_url: https://arxiv.org/abs/2308.11920
tags:
- concept
- concepts
- visual
- image
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of interpretability in medical
  image classification, specifically improving the Concept Bottleneck Model (CBM)
  framework. While CBMs enhance interpretability by using human-understandable concepts
  as intermediate targets, automatically generating these concepts using Large Language
  Models (LLMs) often results in non-visual concepts that hinder model performance.
---

# Concept Bottleneck with Visual Concept Filtering for Explainable Medical Image Classification

## Quick Facts
- arXiv ID: 2308.11920
- Source URL: https://arxiv.org/abs/2308.11920
- Authors: 
- Reference count: 17
- Key outcome: Proposed visual activation score improves Concept Bottleneck Model accuracy by up to 16.7% in 1-shot settings on HAM-10000 skin disease dataset

## Executive Summary
This paper addresses the challenge of interpretability in medical image classification by improving the Concept Bottleneck Model (CBM) framework. The key innovation is a visual activation score that filters out non-visual concepts automatically generated by Large Language Models (LLMs), which often degrade model performance. By measuring CLIP score variance across unlabeled images, the method identifies and removes concepts that don't capture visual information. Experimental results on the HAM-10000 skin disease dataset demonstrate consistent accuracy improvements over the baseline method across various few-shot settings, with particularly dramatic gains in low-shot scenarios.

## Method Summary
The method improves CBM interpretability by filtering non-visual concepts generated via LLM (GPT-3). It computes a visual activation score for each concept by measuring the standard deviation of CLIP similarity scores between the concept and unlabeled images. This score is incorporated into a submodular optimization framework that selects concepts based on discriminability, coverage, and visual activation. The filtered concept set is then used to train a CBM, where a concept weight matrix maps concept scores to final predictions. The approach requires no additional labeled data beyond what's needed for the CBM itself.

## Key Results
- Outperforms baseline LaBo method by up to 16.7% accuracy at 1-shot setting on HAM-10000
- Shows consistent improvements across 1, 2, 4, 8, and 16-shot settings
- Qualitative analysis confirms visual activation score effectively identifies and removes non-visual concepts
- Performance matches or exceeds linear probing in most few-shot scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual activation score filters non-visual concepts by measuring CLIP score variance across unlabeled images
- Mechanism: Concepts with high visual activation scores have high variance in CLIP similarity scores across different images, indicating they capture visual features that distinguish images. Non-visual concepts show low variance as they don't respond to visual differences.
- Core assumption: CLIP embeddings capture meaningful visual-semantic relationships that correlate with actual image content
- Evidence anchors:
  - [abstract]: "propose a visual activation score that measures whether the concept contains visual cues or not, which can be easily computed with unlabeled image data"
  - [section]: "a concept that is activated differently depending on the image is regarded as a concept containing visual cues"
  - [corpus]: Weak evidence - corpus neighbors focus on CBM variants but don't validate CLIP-based filtering specifically
- Break condition: If CLIP embeddings don't capture the relevant visual semantics for the medical domain, or if the variance metric fails to distinguish visual from non-visual concepts

### Mechanism 2
- Claim: Submodular optimization with visual activation score improves concept selection over discriminability score alone
- Mechanism: The submodular optimization framework selects concepts that maximize discriminability, coverage, and visual activation. The visual activation term specifically penalizes non-visual concepts that discriminability score might miss in few-shot settings.
- Core assumption: Submodular optimization effectively balances multiple objectives when selecting concepts
- Evidence anchors:
  - [abstract]: "Computed visual activation scores are then used to filter out the less visible concepts, thus resulting in a final concept set with visually meaningful concepts"
  - [section]: "By using an unlabeled image setX, the visual activation scoreV(c) encourages to filter non-visual concepts effectively, therefore outperforming LaBo by 16.5% at 1-shot"
  - [corpus]: Weak evidence - corpus neighbors discuss CBM variants but don't validate multi-objective submodular optimization specifically
- Break condition: If submodular optimization fails to balance objectives properly, or if visual activation score conflicts with discriminability in concept selection

### Mechanism 3
- Claim: Few-shot performance gains come from better concept quality rather than increased model capacity
- Mechanism: The proposed method filters non-visual concepts that provide noisy signals during training, allowing the concept weight matrix to learn more effectively even with limited labeled data. This improves accuracy without changing model architecture.
- Core assumption: Non-visual concepts introduce significant noise that degrades performance in few-shot settings
- Evidence anchors:
  - [abstract]: "at 1-shot, the LaBo achieved a low accuracy of 36.7%. By using an unlabeled image setX, the visual activation scoreV(c) encourages to filter non-visual concepts effectively, therefore outperforming LaBo by 16.5% at 1-shot"
  - [section]: "the proposed method is shown to even outperform performances of linear probing where the classification process is completely non-interpretable"
  - [corpus]: Weak evidence - corpus neighbors discuss CBM variants but don't specifically address few-shot performance improvements
- Break condition: If concept filtering doesn't significantly impact few-shot learning, or if other factors (like dataset size) dominate performance

## Foundational Learning

- Concept: CLIP model and its text-image embedding space
  - Why needed here: The method relies on CLIP embeddings to compute visual activation scores and concept similarities
  - Quick check question: What does the dot product between CLIP text and image embeddings measure, and why is this relevant for visual concept filtering?

- Concept: Submodular optimization and its properties
  - Why needed here: The method uses submodular optimization to select concepts based on discriminability, coverage, and visual activation scores
  - Quick check question: Why is submodular optimization appropriate for selecting a diverse set of concepts, and what are its key properties?

- Concept: Concept Bottleneck Models and their architecture
  - Why needed here: The proposed method extends CBMs by improving concept selection, so understanding CBM fundamentals is essential
  - Quick check question: How does a CBM differ from a standard image classifier, and what role do concept scores play in the prediction process?

## Architecture Onboarding

- Component map: GPT-3 -> CLIP model -> Submodular optimization -> CBM architecture -> HAM10000 dataset
- Critical path: 1. Generate candidate concepts using GPT-3 2. Compute visual activation scores using CLIP on unlabeled images 3. Run submodular optimization with modified score function 4. Train CBM with filtered concepts 5. Evaluate classification accuracy
- Design tradeoffs: Using unlabeled images for visual activation vs requiring labeled data; adding visual activation term to score function vs using discriminability alone; computational cost of CLIP embeddings for many concepts vs filtering effectiveness
- Failure signatures: Visual activation scores don't correlate with concept quality (many false positives/negatives); submodular optimization fails to balance multiple objectives; performance gains disappear in high-shot settings
- First 3 experiments: 1. Verify visual activation score correlates with concept quality by manually inspecting high/low scoring concepts 2. Test visual activation score effectiveness with different unlabeled image datasets (HAM10000, ImageNet, COCO as in paper) 3. Compare concept selection with and without visual activation term in submodular optimization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform when the target image set X contains out-of-distribution data or images from different domains than the training data?
- Basis in paper: [inferred] The paper states that the visual activation score does not require a domain-specific dataset and can be implemented using an arbitrary dataset. However, it only tests the method on three datasets (HAM10000, ImageNet, and COCO) without exploring out-of-distribution data.
- Why unresolved: The paper does not investigate the performance of the proposed method when the target image set X contains out-of-distribution data or images from different domains than the training data. This is an important aspect to consider, as the method's effectiveness may depend on the relevance of the target image set to the training data.
- What evidence would resolve it: Conducting experiments with out-of-distribution data or images from different domains than the training data and comparing the performance of the proposed method with the baseline would provide evidence to resolve this question.

### Open Question 2
- Question: How does the proposed method perform when the candidate concept set S is generated using different LLMs or prompt engineering techniques?
- Basis in paper: [inferred] The paper uses GPT-3 to generate the candidate concept set S, but it does not explore the impact of using different LLMs or prompt engineering techniques on the performance of the proposed method.
- Why unresolved: The paper does not investigate the performance of the proposed method when the candidate concept set S is generated using different LLMs or prompt engineering techniques. This is an important aspect to consider, as the quality of the generated concepts may vary depending on the LLM and prompt engineering technique used.
- What evidence would resolve it: Conducting experiments with different LLMs or prompt engineering techniques to generate the candidate concept set S and comparing the performance of the proposed method with the baseline would provide evidence to resolve this question.

### Open Question 3
- Question: How does the proposed method perform when the number of target classes Y is increased or decreased?
- Basis in paper: [inferred] The paper uses the HAM-10000 dataset, which has 7 target classes, but it does not explore the impact of changing the number of target classes on the performance of the proposed method.
- Why unresolved: The paper does not investigate the performance of the proposed method when the number of target classes Y is increased or decreased. This is an important aspect to consider, as the method's effectiveness may depend on the number of target classes.
- What evidence would resolve it: Conducting experiments with datasets having different numbers of target classes and comparing the performance of the proposed method with the baseline would provide evidence to resolve this question.

## Limitations
- Effectiveness depends heavily on CLIP's ability to capture relevant visual semantics for medical images
- Visual activation score's reliability as a filter for non-visual concepts remains uncertain without direct ablation studies
- Method requires significant computational resources for CLIP embeddings across many candidate concepts
- Performance gains in few-shot settings may not translate to scenarios with abundant labeled data

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Visual activation score can be computed and incorporated into concept selection | High |
| Visual activation score effectively identifies non-visual concepts and improves few-shot learning performance | Medium |
| Method will generalize to other medical imaging tasks and datasets beyond skin disease classification | Low |

## Next Checks

1. Conduct ablation studies comparing concept selection with and without the visual activation score to quantify its contribution to performance gains
2. Test the method on additional medical imaging datasets (e.g., chest X-rays, histopathology images) to assess generalizability
3. Evaluate the visual activation score's effectiveness by manually validating whether high-scoring concepts correspond to visual features and low-scoring concepts are truly non-visual