---
ver: rpa2
title: Distill Knowledge in Multi-task Reinforcement Learning with Optimal-Transport
  Regularization
arxiv_id: '2309.15603'
source_url: https://arxiv.org/abs/2309.15603
tags:
- policy
- learning
- task
- each
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using optimal transport (OT) regularization
  to improve knowledge distillation in multi-task reinforcement learning. The key
  idea is to use Sinkhorn distances to measure state-action distribution differences
  between tasks, then use this as an amortized reward to encourage sharing common
  behaviors.
---

# Distill Knowledge in Multi-task Reinforcement Learning with Optimal-Transport Regularization

## Quick Facts
- arXiv ID: 2309.15603
- Source URL: https://arxiv.org/abs/2309.15603
- Authors: 
- Reference count: 33
- Primary result: OT regularization improves multi-task RL by encouraging shared behaviors through geometrically-aware reward signals

## Executive Summary
This paper proposes using optimal transport (OT) regularization to improve knowledge distillation in multi-task reinforcement learning. The key idea is to use Sinkhorn distances to measure state-action distribution differences between tasks, then use this as an amortized reward to encourage sharing common behaviors. Experiments on grid-world navigation tasks show the proposed method outperforms separate training and is comparable to KL-based distillation. OT rewards are visualized to show they focus on shared regions like corridors in Zig-zag and Separated Mazes environments. The method accelerates learning and improves stability, especially when tasks share little common behavior.

## Method Summary
The method builds on Soft Actor-Critic (SAC) by adding optimal transport-based rewards to encourage knowledge sharing between tasks. Instead of maintaining a shared default policy, the approach randomly pairs trajectories from different tasks and computes Sinkhorn distances between their state-action distributions. These distances are converted to proxy rewards using an exponential decay function, scaled by parameter σ, and added to environment rewards. The proxy rewards are bounded between (0, σ) to prevent overwhelming task-specific learning signals. During training, each task policy collects trajectories, randomly selects another task's trajectory, computes the OT-based rewards, and updates using the augmented rewards. This creates an implicit sharing mechanism where policies learn from the behavioral distribution across all other tasks.

## Key Results
- OT-based regularization outperforms separate training and matches Distral performance on grid-world navigation tasks
- Method shows faster learning and improved stability, particularly when tasks share minimal common behavior
- Visualizations demonstrate OT rewards focus on shared regions like corridors in Zig-zag and Separated Mazes environments
- Random pairwise trajectory comparison provides sufficient knowledge distillation without requiring a dedicated shared policy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sinkhorn distance between state-action distributions provides a geometrically-aware reward proxy that guides task policies toward shared behaviors.
- Mechanism: The Sinkhorn distance computes an optimal transport plan between empirical state-action distributions of two trajectories. This plan is used to calculate per-state-action contributions to the total distance, which are then mapped through an exponential decay function to produce bounded rewards. Maximizing these rewards effectively minimizes the transport distance, encouraging policies to visit similar state-action regions.
- Core assumption: The Euclidean distance in normalized state-action space meaningfully captures behavioral similarity relevant to task performance.
- Evidence anchors:
  - [abstract] "By using the Sinkhorn mapping, we can approximate the Optimal transport distance between the state distribution of tasks. The distance is then used as an amortized reward to regularize the amount of sharing information."
  - [section] "The proxy reward of a state-action pair can be defined as a monotonically decreasing function of the contribution of that pair to the OT distance" and "maximizing si will lead to reducing the OT distance from two trajectories τi and τj."
  - [corpus] Weak correlation: Only 5 of 25 neighbor papers directly mention optimal transport in RL contexts, suggesting this is a relatively novel application.
- Break condition: If the state-action space normalization fails to align behaviorally similar states, the Euclidean metric will not capture meaningful similarity, breaking the reward proxy's effectiveness.

### Mechanism 2
- Claim: Random pairwise trajectory comparison between tasks provides sufficient knowledge distillation without requiring a dedicated shared policy.
- Mechanism: Instead of maintaining a separate default policy that must be trained, the method randomly selects another task's trajectory at each update step and computes the OT-based reward between the current task's trajectory and the selected one. This creates a dynamic, implicit sharing mechanism where each task policy learns from the distribution of behaviors across all other tasks.
- Core assumption: Random pairing of trajectories provides representative coverage of inter-task behavioral similarities over training time.
- Evidence anchors:
  - [section] "To bypass the need of this sharing policy, we propose an approach to circumvent this issue by directly comparing distances between random pairs of trajectories" and "From the Eq. 9, maximizing si will lead to reducing the OT distance from two trajectories τi and τj."
  - [abstract] "The distance is then used as an amortized reward to regularize the amount of sharing information."
  - [corpus] No direct evidence in corpus neighbors about random pairing strategies for OT-based distillation.
- Break condition: If task behaviors are too dissimilar or non-overlapping, random pairing may consistently produce low or uninformative OT distances, providing insufficient learning signal.

### Mechanism 3
- Claim: Bounded OT rewards with scaling and decay parameters provide stable learning signals that don't overwhelm environment rewards.
- Mechanism: The OT-based rewards are scaled by parameter σ and decayed by parameter β before being added to environment rewards. The exponential decay function ensures rewards are bounded between (0, σ), preventing large gradients that could destabilize learning while still providing meaningful signals for shared behaviors.
- Core assumption: The exponential decay function appropriately balances the influence of shared behavior rewards relative to task-specific rewards across different state distributions.
- Evidence anchors:
  - [section] "To obtain reliable distance signals across state configurations that do not interfere too much with the actual reward signals in an uncontrollable way, it is necessary to map the calculated OT reward to a bounded range" and "the bonus rewards are bounded between (0, σ)."
  - [abstract] "The results show that our added Optimal transport-based rewards are able to speed up the learning process of agents and outperforms several baselines on multi-task learning."
  - [corpus] No corpus evidence about reward scaling strategies for OT-based RL methods.
- Break condition: If σ is too small, shared behavior rewards become negligible; if too large, they dominate task rewards and prevent task-specific learning.

## Foundational Learning

- Concept: Optimal Transport and Sinkhorn Distance
  - Why needed here: Provides a geometrically-aware metric for comparing state-action distributions that captures behavioral similarity beyond simple statistical moments.
  - Quick check question: How does the entropy regularization in Sinkhorn distance make the computation tractable compared to exact Wasserstein distance?

- Concept: Multi-Task Reinforcement Learning with Knowledge Distillation
  - Why needed here: Understanding how policies can share information through regularization terms is crucial for implementing the OT-based reward augmentation.
  - Quick check question: What is the key difference between KL divergence-based regularization and OT-based regularization in terms of what behavioral patterns they encourage?

- Concept: Soft Actor-Critic (SAC) Algorithm
  - Why needed here: The method builds upon SAC as the underlying RL algorithm, so understanding its entropy maximization and off-policy learning mechanisms is essential.
  - Quick check question: How does SAC's maximum entropy objective interact with the additional OT-based rewards in the proposed method?

## Architecture Onboarding

- Component map: SAC agent -> Trajectory collection -> Random task pairing -> OT distance computation -> Proxy reward calculation -> Reward augmentation -> SAC update
- Critical path: Environment interaction → Trajectory collection → Random task pairing → OT distance computation → Proxy reward calculation → Reward augmentation → SAC update → Repeat
- Design tradeoffs: Using random pairing avoids the complexity of maintaining a shared policy but may provide noisier signals than a well-trained central policy. The OT approach captures geometric similarity but requires computing distances between trajectory batches, adding computational overhead compared to simpler regularization methods.
- Failure signatures: Poor performance across all tasks suggests the OT rewards are either too weak (σ too small) or too strong (σ too large). Asymmetric performance improvements suggest some task pairs have more compatible behaviors than others. Slow convergence may indicate β decay is too aggressive.
- First 3 experiments:
  1. Implement trajectory collection and random pairing mechanism with dummy OT distance function to verify data flow.
  2. Add Sinkhorn distance computation between trajectory pairs and verify reward proxy calculation matches expected values on simple test cases.
  3. Integrate OT rewards into SAC training loop and test on a single grid-world task with a dummy second task to verify reward augmentation works before scaling to full multi-task setup.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of β (decay factor) in the proxy reward formula affect the performance of the OT-based regularization method across different multi-task environments?
- Basis in paper: [explicit] The paper mentions using two different values for β (2 and 5) in the experiments and discusses the proxy reward formula in Eq. 9.
- Why unresolved: The paper does not provide a detailed analysis of how different β values impact the performance of the method in various environments.
- What evidence would resolve it: Systematic experiments varying β across different environments and tasks, comparing the performance metrics (e.g., convergence speed, final returns) for different β values.

### Open Question 2
- Question: Can the OT-based regularization method be extended to continuous state-action spaces, and what would be the computational implications?
- Basis in paper: [inferred] The paper focuses on grid-world environments with discrete states and actions, implying potential limitations for continuous spaces.
- Why unresolved: The paper does not address the scalability of the method to continuous state-action spaces or discuss computational challenges.
- What evidence would resolve it: Experiments applying the method to continuous control tasks (e.g., MuJoCo environments) and analysis of computational complexity compared to discrete cases.

### Open Question 3
- Question: How does the performance of OT-based regularization compare to other knowledge distillation methods in more complex, high-dimensional environments?
- Basis in paper: [explicit] The paper compares OT-based regularization with Distral and No-sharing baselines in grid-world environments.
- Why unresolved: The paper does not explore the method's effectiveness in more complex environments with higher-dimensional state and action spaces.
- What evidence would resolve it: Comparative experiments in environments like Atari games or continuous control tasks, measuring performance metrics against other knowledge distillation methods.

### Open Question 4
- Question: What is the impact of the number of tasks on the effectiveness of OT-based regularization, and is there an optimal number of tasks for knowledge sharing?
- Basis in paper: [inferred] The paper uses two-task environments, suggesting a need to explore the method's scalability with more tasks.
- Why unresolved: The paper does not investigate the performance of OT-based regularization with varying numbers of tasks or discuss potential limitations.
- What evidence would resolve it: Experiments with different numbers of tasks (e.g., 2, 4, 8 tasks) in the same or similar environments, analyzing performance trends and identifying potential optimal task numbers.

## Limitations
- Effectiveness of random pairwise trajectory comparison versus dedicated shared policies remains theoretical without empirical comparison
- Geometric assumptions underlying state-action space normalization are not thoroughly tested across diverse task distributions
- Scaling parameters β and σ are treated as fixed hyper-parameters without systematic sensitivity analysis

## Confidence

- **High Confidence**: The mathematical framework for Sinkhorn distance computation and its use as an amortized reward is sound and well-defined.
- **Medium Confidence**: Empirical results show performance improvements on grid-world tasks, though comparisons are limited to two baselines.
- **Low Confidence**: The claim that OT regularization is particularly beneficial when tasks share little common behavior is based on limited experimental evidence and requires more rigorous testing across task similarity spectra.

## Next Checks

1. **Ablation Study on Reward Scaling**: Systematically vary σ across multiple orders of magnitude to determine the optimal range and test whether the claimed bounded rewards (0, σ) consistently improve stability without overwhelming task-specific learning signals.

2. **Task Similarity Analysis**: Design experiments comparing OT regularization performance across tasks with varying degrees of behavioral overlap (from highly similar to completely divergent) to validate the claim about effectiveness for dissimilar tasks.

3. **Random Pairing vs. Shared Policy Comparison**: Implement a dedicated shared policy baseline and compare its performance against the random pairing approach to quantify the trade-offs between simplicity and potential information loss.