---
ver: rpa2
title: How Does Pruning Impact Long-Tailed Multi-Label Medical Image Classifiers?
arxiv_id: '2308.09180'
source_url: https://arxiv.org/abs/2308.09180
tags:
- pruning
- image
- class
- pies
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzes how network pruning affects long-tailed, multi-label
  medical image classifiers for chest X-ray disease diagnosis. Using two large-scale
  datasets, it finds that pruning disproportionately harms rare disease detection,
  with rare classes suffering earlier and more severe performance drops.
---

# How Does Pruning Impact Long-Tailed Multi-Label Medical Image Classifiers?

## Quick Facts
- arXiv ID: 2308.09180
- Source URL: https://arxiv.org/abs/2308.09180
- Reference count: 40
- Key outcome: Pruning disproportionately harms rare disease detection in long-tailed, multi-label medical image classifiers, with rare diseases suffering earlier and more severe performance drops.

## Executive Summary
This study examines how network pruning affects long-tailed, multi-label medical image classifiers for chest X-ray disease diagnosis. Using two large-scale datasets (NIH-CXR-LT and MIMIC-CXR-LT), the authors find that pruning disproportionately harms rare disease detection, with rare classes experiencing earlier and more severe performance drops. A human study of pruning-identified exemplars reveals that models prune images with rare diseases, multiple concurrent diseases, label noise, and low image quality. The findings highlight risks of deploying pruned models in clinical settings and suggest opportunities for improved pruning-aware training.

## Method Summary
The study uses ResNet50 with ImageNet pretraining to classify thorax diseases on chest X-rays. Two long-tailed datasets (NIH-CXR-LT and MIMIC-CXR-LT) are augmented with five rare disease findings. Models are trained using sigmoid cross-entropy loss, then pruned using global unstructured L1 pruning at sparsity ratios from 0 to 0.95. Performance is evaluated using average precision (AP) per class, and pruning-identified exemplars (PIEs) are analyzed through a radiologist study. Experiments are run across 30 model instances per dataset.

## Key Results
- Rare diseases experience significantly larger performance drops under pruning compared to frequent diseases
- Pruning-identified exemplars (PIEs) are more likely to contain rare diseases, multiple concurrent diseases, label noise, and low image quality
- Disease pairs with large frequency differences and low co-occurrence show more dissimilar forgettability curves under pruning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rare diseases are forgotten earlier and more severely under pruning due to their low training frequency and lack of strong co-occurrence patterns.
- Mechanism: Pruning removes weights that contribute least to overall model performance. Since rare diseases have fewer examples, their feature representations are less robust and more dependent on fewer neurons. When those neurons are pruned, rare diseases lose discriminative features faster than frequent diseases.
- Core assumption: Model weights encoding rare disease features are less distributed across the network and more localized, making them more vulnerable to removal.
- Evidence anchors:
  - [abstract] "pruning disproportionately harms rare disease detection, with rare classes suffering earlier and more severe performance drops."
  - [section] "rare diseases are forgotten earlier (Fig. 3, left) and are more severely impacted at high sparsity (Fig. 3, right)."
  - [corpus] Weak evidence; no direct mention of rare disease pruning vulnerability in corpus titles/abstracts.

### Mechanism 2
- Claim: Disease pairs with large frequency differences and low co-occurrence show more dissimilar forgettability curves under pruning.
- Mechanism: Pruning affects disease representations based on their mutual dependence in the network. Diseases that rarely co-occur and have large frequency gaps rely on distinct, sparse feature sets. Pruning disrupts these sets differently, leading to dissimilar degradation patterns.
- Core assumption: Network feature sharing between diseases is proportional to their co-occurrence frequency and relative prevalence.
- Evidence anchors:
  - [section] "Large differences in class frequency lead to dissimilar 'forgettability' behavior and stronger co-occurrence leads to more similar forgettability behavior."
  - [section] "We observe a statistically significant interaction effect between the difference in individual class frequency and class co-occurrence on FCD."
  - [corpus] No direct evidence; corpus focuses on long-tailed classification but not pruning-induced forgettability.

### Mechanism 3
- Claim: Pruning-identified exemplars (PIEs) are more likely to contain rare diseases, multiple concurrent diseases, label noise, and low image quality, making them difficult for pruned models.
- Mechanism: Pruned models lose the capacity to handle complex, ambiguous, or underrepresented cases. PIEs aggregate cases that were already challenging for the original model but become impossible for the pruned model due to reduced representational capacity.
- Core assumption: Complexity and ambiguity in input data require more model capacity to resolve, which pruning removes.
- Evidence anchors:
  - [abstract] "radiologists perceive PIEs as having more label noise, lower image quality, and higher diagnosis difficulty."
  - [section] "PIEs are more likely to contain rare diseases and more likely to contain 3+ simultaneous diseases when compared to non-PIEs."
  - [corpus] Weak evidence; corpus mentions multi-label classification but not pruning-specific exemplar identification.

## Foundational Learning

- Concept: Long-tailed class distribution
  - Why needed here: Understanding how class frequency imbalance affects model learning and pruning vulnerability.
  - Quick check question: Why do rare classes suffer more under pruning than frequent classes?

- Concept: Multi-label classification dynamics
  - Why needed here: Diseases can co-occur, affecting how their representations are learned and pruned.
  - Quick check question: How does disease co-occurrence influence the similarity of forgettability curves?

- Concept: Pruning and model capacity
  - Why needed here: Pruning reduces model capacity, impacting the ability to represent rare and complex cases.
  - Quick check question: What happens to model performance when pruning removes neurons encoding rare disease features?

## Architecture Onboarding

- Component map: Data pipeline -> Model (ResNet50) -> Pruning module -> Evaluation -> Human study analysis

- Critical path:
  1. Train baseline model on full dataset
  2. Apply pruning at multiple sparsity levels
  3. Evaluate overall and per-class performance
  4. Identify PIEs by comparing uncompressed vs. 90% pruned model predictions
  5. Analyze PIE characteristics and conduct radiologist study

- Design tradeoffs:
  - Global vs. local pruning: Global pruning ensures uniform sparsity but may harm rare classes more
  - Structured vs. unstructured pruning: Structured pruning preserves computational efficiency but may lose fine-grained features
  - Metric choice: AP vs. AUC; AP is more robust to class imbalance

- Failure signatures:
  - Sudden drop in AP for rare classes at low sparsity ratios
  - High dissimilarity in forgettability curves for disease pairs with large frequency differences
  - PIEs dominated by rare diseases and complex cases with label noise

- First 3 experiments:
  1. Train ResNet50 on NIH-CXR-LT and measure AP per class before pruning
  2. Apply 50% sparsity pruning and compare AP drop across frequent vs. rare classes
  3. Identify PIEs at 90% sparsity and analyze their disease composition and image quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the pruning impact vary across different neural network architectures beyond ResNet50 for long-tailed, multi-label medical image classification?
- Basis in paper: [explicit] The study acknowledges it only considers ResNet50 architecture and notes uncertainty about whether other architectures could mitigate adverse pruning effects on rare classes.
- Why unresolved: The paper's experimental scope was limited to a single architecture, leaving the generalizability of findings to other model architectures unknown.
- What evidence would resolve it: Comparative experiments testing pruning effects across multiple architectures (e.g., Vision Transformers, EfficientNet) on the same long-tailed, multi-label medical datasets.

### Open Question 2
- Question: Can pruning-identified exemplars (PIEs) be effectively used as a data cleaning tool to identify and remove mislabeled or low-quality images from training datasets?
- Basis in paper: [inferred] The study found PIEs contain more label noise, lower image quality, and higher diagnosis difficulty, suggesting they could serve as indicators of problematic examples.
- Why unresolved: While the paper identifies PIEs as having these characteristics, it doesn't experimentally test whether removing or re-weighting PIEs during training improves model performance or fairness.
- What evidence would resolve it: Controlled experiments comparing model performance when PIEs are removed, reweighted, or left unchanged during training, measuring impact on both overall and class-specific performance.

### Open Question 3
- Question: What is the optimal training strategy to mitigate the disproportionate impact of pruning on rare disease detection in long-tailed, multi-label medical image classification?
- Basis in paper: [explicit] The authors suggest future research could leverage PIEs for improved long-tailed learning but don't propose or test specific algorithmic solutions.
- Why unresolved: The study identifies the problem and characterizes vulnerable classes but stops short of developing or evaluating methods to address the imbalance in pruning vulnerability.
- What evidence would resolve it: Development and empirical evaluation of pruning-aware training algorithms that specifically protect rare classes, such as weighted loss functions, curriculum learning approaches, or pruning strategies that preserve rare class representations.

## Limitations
- Study only examines ResNet50 architecture, limiting generalizability to other model types
- Human study of PIEs involves relatively small radiologist sample (n=5)
- Analysis relies on correlation rather than causal evidence for why rare diseases are more vulnerable to pruning

## Confidence
- Pruning disproportionately harms rare disease detection: High
- Pruning vulnerability correlates with disease frequency and co-occurrence: Medium
- PIEs contain systematically different case types: High

## Next Checks
1. **Feature Attribution Analysis**: Apply integrated gradients or similar methods to compare feature importance distributions for rare versus frequent diseases, directly testing whether rare disease representations are more localized and thus more vulnerable to pruning.

2. **Controlled Co-occurrence Experiment**: Generate synthetic disease pairs with controlled frequency ratios and co-occurrence rates to isolate the effects of each factor on forgettability curves, moving beyond observational correlations in real data.

3. **Pruning-Aware Training Validation**: Implement class-balanced sampling or re-weighting during training of pruned models and measure whether this mitigates the disproportionate harm to rare diseases, testing whether the observed vulnerability is fundamental or can be addressed through training modifications.