---
ver: rpa2
title: Algorithm Evolution Using Large Language Model
arxiv_id: '2311.15249'
source_url: https://arxiv.org/abs/2311.15249
tags:
- algorithm
- node
- algorithms
- optimization
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel approach called Algorithm Evolution
  using Large Language Model (AEL) to automatically generate optimization algorithms
  via an evolutionary framework using a large language model (LLM). The key idea is
  to treat each algorithm as an individual and evolve them using standard evolutionary
  operators like selection, crossover, and mutation, but with LLM used to create and
  modify the algorithms.
---

# Algorithm Evolution Using Large Language Model

## Quick Facts
- arXiv ID: 2311.15249
- Source URL: https://arxiv.org/abs/2311.15249
- Authors: 
- Reference count: 40
- Key outcome: AEL reduces TSP50 average gap to optimal from 20% to 12% vs hand-crafted algorithms

## Executive Summary
This paper introduces Algorithm Evolution using Large Language Model (AEL), a novel approach that leverages LLM-guided evolution to automatically generate optimization algorithms. The method treats each algorithm as an individual in a population and applies standard evolutionary operators (selection, crossover, mutation) using LLM prompts to create and modify algorithms. Demonstrated on constructive heuristics for the Traveling Salesman Problem (TSP), AEL shows superior performance compared to hand-crafted greedy algorithms and direct LLM instruction, with excellent scalability across different problem sizes.

## Method Summary
AEL represents each algorithm as an individual with three components: a natural language description, an executable code block, and a fitness value. The evolutionary framework uses LLMs (GPT-3.5-turbo and GPT-4) to generate initial populations through prompts, then applies crossover and mutation operators to evolve the algorithms. Fitness is evaluated by executing the algorithms on benchmark TSP instances and measuring solution quality relative to optimal solutions obtained via Gurobi solver. The approach is demonstrated with population size N=10, generations Ng=10, crossover probability σ1=1.0, and mutation probability σ2=0.2.

## Key Results
- AEL outperforms simple hand-crafted greedy algorithms and algorithms generated by directly instructing LLM
- The method shows excellent scalability across TSP problem sizes (20-1000)
- Average gap to optimal solution reduced from 20% to approximately 12% on TSP50 instances

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AEL leverages LLM-guided evolution to create novel algorithms by treating each algorithm as an individual in a population
- Mechanism: The evolutionary framework selects, crosses over, and mutates algorithm individuals using LLM prompts, allowing exploration of algorithm space without model training
- Core assumption: LLMs can meaningfully manipulate algorithmic structure through prompt engineering without extensive training data
- Evidence anchors: [abstract] "It utilizes a large language model (LLM) to automatically generate optimization algorithms via an evolutionary framework." [section] "AEL does algorithm-level evolution without model training."
- Break condition: LLM fails to generate syntactically correct or semantically meaningful algorithmic modifications; prompt engineering becomes too brittle

### Mechanism 2
- Claim: Individual representation (algorithm description + code block + fitness) enables seamless integration with evolutionary operators
- Mechanism: Standardized prompt formats guide LLM to produce executable code snippets that can be evaluated on benchmark instances, with fitness providing selection pressure
- Core assumption: Code generation quality from LLM is sufficient for automated evaluation without human intervention
- Evidence anchors: [section] "The algorithm description comprises a few sentences in natural language that can be easily processed by LLM." [section] "The code block should follow a predefined format so that it can be identified and seamlessly integrated into our AEL framework."
- Break condition: Generated code fails to compile or execute; fitness evaluation becomes unreliable due to poor code quality

### Mechanism 3
- Claim: AEL achieves better generalization across problem sizes than domain-specific trained models
- Mechanism: By evolving algorithms that are not overfitted to a single problem size, AEL maintains performance on diverse TSP instances without retraining
- Core assumption: Algorithm-level evolution inherently promotes generalization compared to instance-specific model training
- Evidence anchors: [abstract] "Compared with other domain deep learning model-based algorithms, these methods exhibit excellent scalability across different problem sizes." [section] "AEL demonstrates significantly better generalization performance across various problem sizes when compared to the domain model."
- Break condition: Algorithm evolution converges to overly specialized solutions; generalization gap remains large across scales

## Foundational Learning

- Concept: Evolutionary algorithm design
  - Why needed here: AEL adapts standard EC concepts to algorithm-level evolution rather than solution-level
  - Quick check question: What is the difference between evolving solutions and evolving algorithms in an evolutionary framework?

- Concept: Prompt engineering for code generation
  - Why needed here: Effective prompts are critical for LLM to generate valid, novel algorithms that can be evaluated automatically
  - Quick check question: What are the key components that must be included in a prompt to generate executable algorithmic code?

- Concept: Combinatorial optimization heuristics
  - Why needed here: Understanding constructive heuristics is essential for representing TSP algorithms and evaluating their performance
  - Quick check question: How does a constructive heuristic for TSP build a solution step-by-step?

## Architecture Onboarding

- Component map: LLM interface → Prompt engineering module → Evolutionary operators (selection, crossover, mutation) → Fitness evaluator → Population manager
- Critical path: Prompt generation → LLM response parsing → Algorithm execution → Fitness calculation → Population update
- Design tradeoffs: Algorithm complexity vs. LLM response reliability; population diversity vs. convergence speed; prompt specificity vs. creativity
- Failure signatures: LLM generates invalid code; fitness evaluation fails consistently; population converges prematurely to poor solutions
- First 3 experiments:
  1. Test LLM's ability to generate a valid greedy TSP heuristic from a simple prompt
  2. Evaluate crossover operator by combining two hand-crafted heuristics and checking output validity
  3. Run AEL for 5 generations on a small TSP instance and verify fitness improvement trajectory

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is AEL when applied to multi-objective optimization problems compared to its performance on single-objective problems?
- Basis in paper: [explicit] The paper mentions that AEL can be extended to handle multi-objective optimization problems using multi-objective optimization algorithms
- Why unresolved: The paper only demonstrates AEL on single-objective TSP and does not provide empirical results or comparisons for multi-objective problems
- What evidence would resolve it: Experimental results comparing AEL's performance on various multi-objective optimization problems (e.g., multi-objective TSP, multi-objective scheduling) against state-of-the-art multi-objective algorithms

### Open Question 2
- Question: Can AEL effectively evolve algorithms for more complex optimization problems that are difficult for LLMs to comprehend, such as those with high-dimensional or dynamic characteristics?
- Basis in paper: [inferred] The paper acknowledges that complex problems pose challenges for AEL as they are difficult for LLMs to comprehend, even for humans
- Why unresolved: The paper only demonstrates AEL on the TSP, which is a relatively simple combinatorial optimization problem. It does not explore AEL's performance on more complex problem domains
- What evidence would resolve it: Empirical results showing AEL's performance on various complex optimization problems (e.g., high-dimensional continuous optimization, dynamic optimization) compared to specialized algorithms for those domains

### Open Question 3
- Question: How does the choice of LLM (e.g., GPT-3.5 vs GPT-4) affect AEL's ability to evolve novel and effective algorithms, and what are the trade-offs between different LLM models?
- Basis in paper: [explicit] The paper compares the performance of AEL using GPT-3.5-turbo and GPT-4, noting that GPT-4 is more powerful but can be overly innovative with excessive randomness
- Why unresolved: While the paper provides a comparison between two LLM models, it does not explore the impact of using other LLM models or the trade-offs in terms of computational cost, effectiveness, and ability to generate novel algorithms
- What evidence would resolve it: Systematic comparison of AEL's performance using various LLM models (e.g., GPT-3.5, GPT-4, Claude, LLaMA) on a range of optimization problems, considering factors such as solution quality, computational cost, and algorithm novelty

## Limitations

- The method's effectiveness heavily depends on LLM response quality, which may be inconsistent across different models and prompts
- The approach is currently limited to relatively simple optimization problems like TSP and may struggle with more complex domains
- Extensive prompt engineering and parameter tuning may be required to achieve good performance, limiting practical applicability

## Confidence

- **High confidence**: The conceptual framework of using LLMs for algorithm evolution is sound and the individual representation format (algorithm description + code block + fitness) is well-defined
- **Medium confidence**: The demonstrated performance improvements on TSP50 instances appear significant, but the comparison methodology and statistical significance are not fully detailed
- **Low confidence**: Claims about superior generalization across problem sizes compared to domain-specific models require more rigorous validation on diverse problem types

## Next Checks

1. Cross-validation of generalization claims: Test AEL-generated algorithms on TSP instances beyond the 20-1000 range used in the paper, including significantly larger problem sizes, to verify the claimed scalability
2. Statistical significance analysis: Perform multiple independent runs of AEL with different random seeds and analyze the variance in performance to establish confidence intervals for the reported improvements
3. Comparison with alternative algorithm generators: Implement and evaluate other automated algorithm generation methods (e.g., genetic programming) on the same TSP benchmarks to contextualize AEL's performance relative to established approaches