---
ver: rpa2
title: Stabilizing RLHF through Advantage Model and Selective Rehearsal
arxiv_id: '2309.10202'
source_url: https://arxiv.org/abs/2309.10202
tags:
- reward
- training
- arxiv
- score
- rehearsal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses instability issues in RLHF training, including
  reward hacking and catastrophic forgetting. The authors propose two techniques:
  Advantage Model (AM) and Selective Rehearsal (SR).'
---

# Stabilizing RLHF through Advantage Model and Selective Rehearsal

## Quick Facts
- arXiv ID: 2309.10202
- Source URL: https://arxiv.org/abs/2309.10202
- Authors: 
- Reference count: 10
- Key outcome: Achieves 15.78% win rate on main test set and 10.30% win rate on forget test set with AM-PPO-SR, compared to 12.72% and 16.87% for RM-PPO respectively

## Executive Summary
This paper addresses instability issues in Reinforcement Learning from Human Feedback (RLHF) training, specifically reward hacking and catastrophic forgetting. The authors propose two techniques: Advantage Model (AM) and Selective Rehearsal (SR). AM directly models advantage scores to regulate reward distributions and prevent reward hacking, while SR strategically selects data for PPO training and rehearsal to mitigate catastrophic forgetting. Experiments on public and proprietary datasets demonstrate that these techniques increase stability in RLHF training, achieving higher reward scores and win rates over the SFT model.

## Method Summary
The method introduces Advantage Model (AM) to replace traditional reward modeling by directly estimating advantage scores, which normalizes reward distributions across tasks and prevents reward hacking. The Selective Rehearsal (SR) mechanism clusters PPO training data and selects representative examples based on advantage scores, then incorporates rehearsal training to prevent catastrophic forgetting. The approach combines AM with PPO optimization and SR data selection, training on BLOOMZ models (7B for reward modeling, 176B for SFT and RLHF) using public datasets (HH-RLHF, COIG, firefly) and proprietary data.

## Key Results
- AM-PPO-SR achieves 15.78% win rate on main test set versus 12.72% for RM-PPO
- AM-PPO-SR achieves 10.30% win rate on forget test set versus 16.87% for RM-PPO
- AM provides significantly lower Expected Calibration Error (ECE) while maintaining ranking accuracy compared to Reward Model

## Why This Works (Mechanism)

### Mechanism 1: Advantage Model (AM) replaces reward model (RM)
AM computes advantage scores relative to expected rewards under the current policy, normalizing per-task reward disparities and reducing instability caused by task-dependent reward distributions.

### Mechanism 2: Selective Rehearsal (SR) mitigates catastrophic forgetting
SR clusters PPO training data and selects representative examples based on advantage scores, then performs joint training with NLL loss to preserve knowledge from SFT while preventing over-optimization.

### Mechanism 3: Advantage Model provides better calibration than Reward Model
AM yields better-calibrated scores (lower ECE) than RM due to its bounded and normalized formulation, leading to more stable score distributions and reliable reward signals for PPO.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: This paper builds on RLHF as the baseline training paradigm that suffers from instability issues.
  - Quick check question: What are the three stages of RLHF training, and which stage does PPO belong to?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Selective Rehearsal directly addresses this problem during PPO training.
  - Quick check question: What is the difference between catastrophic forgetting and interference in neural networks?

- Concept: Proximal Policy Optimization (PPO)
  - Why needed here: PPO is the RL algorithm used in RLHF, and the paper modifies its training process.
  - Quick check question: What is the purpose of the KL-divergence term in the PPO objective?

## Architecture Onboarding

- Component map: AM -> PPO training loop (modified) -> Model updates policy parameters
- Critical path: AM generates advantage scores → PPO training uses these scores with selective rehearsal → Model updates policy parameters
- Design tradeoffs: AM vs RM: AM provides better calibration but requires additional computation for expected reward estimation. SR vs no SR: SR prevents forgetting but adds complexity and hyperparameter tuning.
- Failure signatures: Reward hacking manifests as win rate drop despite increasing reward scores. Catastrophic forgetting shows as performance degradation on forget test set.
- First 3 experiments:
  1. Compare AM vs RM on calibration metrics (ECE, accuracy) on validation set.
  2. Run PPO with AM and measure win rate on main vs forget test sets.
  3. Test different numbers of clusters (c) in selective rehearsal and measure impact on forget test set performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of clusters (c) in Selective Rehearsal impact the performance of RLHF training across different datasets and domains?
- Basis in paper: The paper mentions that the number of clusters (c) is a hyperparameter in the Selective Rehearsal strategy, and it briefly studies the influence of c in Section 4.3, showing that test-set rewards vary by approximately 0.05 points across different cluster numbers.
- Why unresolved: The paper only briefly studies the influence of c on a specific dataset and does not provide a comprehensive analysis of its impact across different datasets and domains.
- What evidence would resolve it: A thorough analysis of the impact of c on various datasets and domains, including different task spectra and data distributions, would provide a better understanding of the optimal number of clusters for Selective Rehearsal.

### Open Question 2
- Question: Can the Advantage Model (AM) be further improved by incorporating additional factors, such as the complexity or difficulty of learning the reward function for specific prompts?
- Basis in paper: The paper mentions that the permitted margin (m(x)) for each prompt may have a connection with the complexity or difficulty involved in learning the reward function for prompts similar to x, but this aspect is left as a topic for future study and exploration.
- Why unresolved: The paper does not explore the potential benefits of incorporating additional factors, such as the complexity or difficulty of learning the reward function, into the Advantage Model.
- What evidence would resolve it: Experiments comparing the performance of the current Advantage Model with an enhanced version that incorporates additional factors, such as prompt complexity or difficulty, would help determine if this approach leads to further improvements in RLHF training stability and performance.

### Open Question 3
- Question: How can the trade-off between helpfulness and harmlessness objectives be better managed in RLHF training, particularly in datasets with a limited presence of harmful examples?
- Basis in paper: The paper mentions that the trade-off between helpfulness and harmlessness objectives is more pronounced in the HH-RLHF dataset, possibly due to the limited presence of harmful examples in the proprietary data used in the experiments.
- Why unresolved: The paper does not provide a clear solution for managing the trade-off between helpfulness and harmlessness objectives, particularly in datasets with a limited presence of harmful examples.
- What evidence would resolve it: Experiments exploring different strategies for balancing the helpfulness and harmlessness objectives, such as adjusting the weighting of these objectives during training or incorporating additional constraints, would help identify effective approaches for managing this trade-off in RLHF training.

## Limitations

- Data limitations: The paper relies on proprietary datasets whose specifics are not disclosed, limiting reproducibility and independent verification of results.
- Unexplained hyperparameters: Critical parameters such as the number of clusters (c) for selective rehearsal and the margin function (m(x)) for advantage modeling are not fully specified.
- Evaluation scope: Results focus on win rates and reward scores against a single SFT baseline without comparison to other RLHF stabilization methods or ablation studies.

## Confidence

- High confidence in the theoretical foundation: The concepts of advantage modeling for reward normalization and selective rehearsal for catastrophic forgetting are well-established in reinforcement learning literature.
- Medium confidence in experimental results: While the paper presents quantitative improvements, the proprietary nature of key datasets and limited ablation studies reduce confidence in the absolute magnitude of improvements.
- Medium confidence in mechanism claims: The paper provides theoretical justification and some empirical evidence for both AM and SR mechanisms, but lacks direct causal evidence isolating their individual contributions.

## Next Checks

1. Conduct controlled experiments isolating the effects of Advantage Model versus Selective Rehearsal to quantify their individual contributions to stability improvements.

2. Implement the full AM-PPO-SR pipeline using only public datasets and evaluate whether similar win rate improvements (15.78% main, 10.30% forget) are achievable.

3. Systematically vary the importance weight computation in Advantage Model to test sensitivity to policy distribution shifts and verify the claimed robustness to reward hacking.