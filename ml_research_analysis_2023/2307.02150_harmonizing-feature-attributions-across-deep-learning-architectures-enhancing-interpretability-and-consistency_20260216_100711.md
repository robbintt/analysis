---
ver: rpa2
title: 'Harmonizing Feature Attributions Across Deep Learning Architectures: Enhancing
  Interpretability and Consistency'
arxiv_id: '2307.02150'
source_url: https://arxiv.org/abs/2307.02150
tags:
- feature
- features
- architectures
- learning
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the harmonization of feature attributions
  across deep learning architectures to improve interpretability and consistency in
  machine learning models. The research explores the feasibility of using feature
  attribution methods as feature detectors and examines how these features can be
  harmonized across multiple models with distinct architectures trained on the same
  data distribution.
---

# Harmonizing Feature Attributions Across Deep Learning Architectures: Enhancing Interpretability and Consistency

## Quick Facts
- **arXiv ID**: 2307.02150
- **Source URL**: https://arxiv.org/abs/2307.02150
- **Reference count**: 0
- **Primary result**: Features generated by one neural architecture can be detected by other architectures trained on the same data, with accuracy and F1 scores remaining stable across different configurations.

## Executive Summary
This study investigates the harmonization of feature attributions across deep learning architectures to improve interpretability and consistency in machine learning models. The research explores the feasibility of using feature attribution methods as feature detectors and examines how these features can be harmonized across multiple models with distinct architectures trained on the same data distribution. The methodology involves generating feature attribution maps using the Soundness Saliency algorithm and Grad-CAM, extracting features from input images, and passing them to models with different architectures. The primary result shows that features generated by one neural architecture can be detected by other architectures trained on the same data, with accuracy and F1 scores remaining stable across different configurations.

## Method Summary
The study generates feature attribution maps using Soundness Saliency and Grad-CAM algorithms on pretrained models (EfficientNet-B7, EfficientNet-B6, EfficientNet-B5, and Vision Transformer). Features are extracted by applying Hadamard products between input image channels and attribution maps, then these extracted features are fed as input to other pretrained models. The performance is evaluated using accuracy, F1 score, and output probability distributions, comparing results when models receive feature-attribution-extracted inputs versus full images.

## Key Results
- Features generated by one neural architecture can be detected by other architectures trained on the same data.
- Accuracy and F1 scores remain stable across different architectural configurations when using feature inputs.
- Feature attribution maps encapsulate sufficient data distribution information, allowing for generalization and transferability of features across architectures.

## Why This Works (Mechanism)

### Mechanism 1
Feature attribution maps from one architecture can be used as input to other architectures trained on the same data distribution because the same underlying data distribution is learned in different ways by different architectures, but the features that matter are shared. When a feature attribution map from one model is used to filter the input for another model, the resulting performance is close to using the full input. This assumes that feature attribution maps capture enough of the data distribution to be generalizable across architectures. The break condition occurs if the two architectures learn fundamentally different representations or if the data distribution changes.

### Mechanism 2
Soundness Saliency and Grad-CAM generate attribution maps that preserve discriminative features across architectures by highlighting regions of the input image that contribute most to the model's prediction. When only those regions are passed to another model, the prediction remains stable because the discriminative features are still present. This assumes that the highlighted features by SS and GC are sufficient for accurate classification by other models. The break condition is if the attribution methods are sensitive to model-specific artifacts, the extracted features may not be generalizable.

### Mechanism 3
Similar accuracy and F1 scores across architectures when using feature inputs indicate feature harmonization because the consistency in performance metrics implies that the selected features contain the necessary information for classification, regardless of the architecture that processes them. This assumes that stable metrics across architectures mean that the features are meaningful and transferable. The break condition is if the metrics drop significantly when using features instead of full images, suggesting that the attribution method is not capturing all necessary information.

## Foundational Learning

- **Feature Attribution Methods**: Why needed - They provide the mechanism for extracting important features that can be harmonized across architectures. Quick check - What is the difference between local and global explanations in feature attribution?
- **Data Distribution Learning**: Why needed - Understanding that different architectures learn the same data distribution in different ways is key to why harmonization works. Quick check - Why would architectures trained on the same data learn different internal representations?
- **Cross-Architecture Generalization**: Why needed - The core hypothesis is that features learned by one architecture can be used by another; this requires understanding how generalization works. Quick check - What factors might prevent features from generalizing across architectures?

## Architecture Onboarding

- **Component map**: Pretrained models (ViT, EfficientNet-B7, EfficientNet-B6, EfficientNet-B5) -> Attribution methods (Soundness Saliency, Grad-CAM) -> Data (ImageNet validation subset) -> Evaluation metrics (Accuracy, F1 score, probability distributions)
- **Critical path**: Generate attribution maps from a source model -> Extract features using Hadamard product with attribution maps -> Feed features to target models -> Compare performance metrics with full image inputs
- **Design tradeoffs**: Using attribution maps reduces input dimensionality but may lose context; Different attribution methods may capture different aspects of features; Transformers vs. CNNs have different inductive biases affecting feature detection
- **Failure signatures**: Large drops in accuracy/F1 when using features instead of full images; Inconsistent performance across different attribution methods; Attribution maps that highlight irrelevant regions
- **First 3 experiments**: 1) Generate attribution maps with SS on EfficientNet-B7, use them as input to EfficientNet-B6 and B5, compare metrics. 2) Repeat with Grad-CAM attribution maps, compare stability across architectures. 3) Swap roles: use ViT as source model, EfficientNet models as targets, analyze cross-architecture transfer.

## Open Questions the Paper Calls Out

### Open Question 1
Can feature attribution methods serve as reliable feature detectors across all types of neural architectures beyond CNNs and transformers? This question is based on the study's suggestion that features generated by one neural architecture can be detected by other architectures trained on the same data, with accuracy and F1 scores remaining stable across different configurations. It remains unresolved because the experiments were limited to CNNs and transformers. Testing feature attribution harmonization across a broader range of neural architectures including RNNs, GNNs, and autoencoders would demonstrate generalizability.

### Open Question 2
How do different feature attribution methods compare in their ability to capture essential information for cross-architecture feature detection? This question is based on the study finding that Grad-CAM-generated features showed consistent accuracy across architectures, while Soundness Saliency features performed better with similar architecture building blocks. It remains unresolved because the study only compared two methods and didn't explore other attribution methods like Integrated Gradients or SHAP. Systematic comparison of multiple feature attribution methods across various architectures and datasets would identify the most effective approaches.

### Open Question 3
What are the theoretical foundations explaining why feature attributions can be harmonized across different architectures trained on the same data? This question is inferred from the study's assumption that if features are discriminative for one architecture, they should be for another when both are trained on the same data, but doesn't explain the underlying mechanism. It remains unresolved because the paper demonstrates empirical results but doesn't provide theoretical justification for why different architectures learn compatible feature representations. Mathematical analysis of feature space alignment and representation similarity metrics across architectures could explain the observed harmonization phenomenon.

## Limitations
- Cross-architecture transferability assumes similar feature importance, but attribution methods may encode model-specific artifacts rather than truly shared features.
- Soundness Saliency algorithm implementation details are not provided, making exact reproduction difficult.
- Experiments focus on image classification; results may not generalize to other domains like NLP or tabular data where feature interactions differ significantly.

## Confidence
- **High confidence**: Features generated by one architecture can be detected by others trained on the same data (supported by consistent accuracy/F1 scores).
- **Medium confidence**: Attribution maps capture sufficient data distribution information for cross-architecture generalization (evidence is correlative, not causative).
- **Low confidence**: The specific mechanism of feature harmonization is well-understood (implementation details of Soundness Saliency remain unclear).

## Next Checks
1. **Implementation verification**: Obtain or reimplement the Soundness Saliency algorithm and verify attribution maps produce expected patterns on sample images before proceeding with cross-architecture experiments.
2. **Capacity sensitivity analysis**: Test feature harmonization when transferring from smaller to larger models (B5â†’B7) to determine if performance drops indicate capacity-dependent feature learning.
3. **Attribution method comparison**: Conduct a controlled ablation comparing SS vs Grad-CAM features to determine if observed harmonization is method-dependent or architecture-dependent.