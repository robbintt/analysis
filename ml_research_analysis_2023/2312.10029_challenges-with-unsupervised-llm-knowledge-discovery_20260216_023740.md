---
ver: rpa2
title: Challenges with unsupervised LLM knowledge discovery
arxiv_id: '2312.10029'
source_url: https://arxiv.org/abs/2312.10029
tags:
- accuracy
- alice
- banana
- default
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper challenges the core claim of unsupervised knowledge
  elicitation methods like CCS: that knowledge is the most prominent feature in LLM
  activations satisfying consistency structure. The authors prove that arbitrary binary
  features satisfy CCS''s consistency loss equally well as knowledge, and experimentally
  demonstrate that CCS and similar methods often learn non-knowledge features like
  random words or simulated characters'' opinions instead.'
---

# Challenges with unsupervised LLM knowledge discovery

## Quick Facts
- arXiv ID: 2312.10029
- Source URL: https://arxiv.org/abs/2312.10029
- Reference count: 40
- Key outcome: Unsupervised knowledge elicitation methods like CCS cannot reliably distinguish knowledge from arbitrary binary features that satisfy consistency structure

## Executive Summary
This paper challenges the core claim of unsupervised knowledge elicitation methods like CCS that knowledge is the most prominent feature in LLM activations satisfying consistency structure. The authors prove that arbitrary binary features satisfy CCS's consistency loss equally well as knowledge, and experimentally demonstrate that CCS and similar methods often learn non-knowledge features like random words or simulated characters' opinions instead. Key findings include CCS probes learning inserted random words rather than review sentiment, accurately predicting simulated character opinions rather than ground truth, and showing high sensitivity to prompt variations with no principled reason to prefer one prompt over another. The paper concludes that unsupervised methods for discovering latent knowledge are insufficient and provides sanity checks for evaluating future approaches.

## Method Summary
The paper evaluates unsupervised knowledge discovery methods (CCS, PCA, k-means) by testing whether they can be fooled by non-knowledge features that satisfy the same consistency structure as knowledge. The methodology involves creating contrast pairs from binary questions, normalizing activations to remove end-of-text markers, training unsupervised probes on contrastive activations, and comparing their performance on ground truth versus inserted non-knowledge features (random words, simulated character opinions). The study uses datasets including IMDb, BoolQ, DBpedia, and TruthfulQA, extracting activations from T5-11B and Chinchilla-70B at layer 30. Performance is evaluated against supervised logistic regression baselines and random probe baselines across 50 random seeds.

## Key Results
- CCS probes can perfectly predict inserted random words (banana/shed) rather than review sentiment on IMDb
- CCS accurately predicts a simulated character's opinion rather than ground truth on IMDb
- CCS performance is highly sensitive to prompt variations with no principled reason to prefer one prompt over another

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrast-consistent search (CCS) fails because arbitrary binary features satisfy its consistency loss equally well as knowledge.
- Mechanism: The CCS loss function rewards consistency between a statement and its negation, but this property holds for any binary feature, not just knowledge. When probe outputs p(x+) and p(x-) are complementary (p(x+) + p(x-) = 1), the consistency loss is zero regardless of whether the probe is detecting knowledge or some other feature.
- Core assumption: Knowledge is not the only feature that exhibits negation-consistency in LLM activations.
- Evidence anchors:
  - [abstract] "arbitrary binary features satisfy the consistency structure of a particular leading unsupervised knowledge-elicitation method"
  - [section] "We prove that arbitrary binary features satisfy the CCS loss equally well"
  - [corpus] Weak evidence - corpus neighbors focus on feature discovery but not CCS-specific mechanisms
- Break condition: If CCS were modified to require additional constraints beyond consistency (e.g., probabilistic coherence with domain knowledge), this mechanism would break.

### Mechanism 2
- Claim: Unsupervised methods learn the most prominent feature in activations rather than knowledge.
- Mechanism: During training, probe parameters converge to detect whichever binary feature is most salient in the activation space. When multiple features could satisfy the loss, inductive biases favor the most prominent one. The experiments show that when distracting features (banana/shed, Alice's opinion) are introduced, unsupervised methods learn these instead of review sentiment.
- Core assumption: The activation space contains multiple binary features that could satisfy CCS loss, and the learning process has no principled way to distinguish knowledge from other features.
- Evidence anchors:
  - [abstract] "unsupervised methods on large language model (LLM) activations do not discover knowledge – instead they seem to discover whatever feature of the activations is most prominent"
  - [section] "Our results are shown in Figure 2a, displaying accuracy of each method...we see that for all unsupervised methods, default prompts...score highly on ground truth accuracy...However, for the banana/shed prompts we see 50%, random chance, on ground truth accuracy"
  - [corpus] Weak evidence - corpus neighbors discuss feature discovery but not prominence-based learning failures
- Break condition: If unsupervised methods incorporated mechanisms to evaluate feature relevance to knowledge (e.g., causal interventions), this mechanism would break.

### Mechanism 3
- Claim: Prompt sensitivity reveals unsupervised methods lack principled foundations.
- Mechanism: Different prompt templates that should be semantically equivalent produce different accuracy results, indicating that unsupervised methods are sensitive to superficial prompt variations rather than extracting stable knowledge representations. This sensitivity shows the methods are not discovering robust knowledge features.
- Core assumption: Knowledge representations should be invariant to irrelevant prompt variations.
- Evidence anchors:
  - [abstract] "we demonstrate the sensitivity of the methods to unimportant details of the prompt"
  - [section] "We find that a 'non-default' prompt gives the 'best performance' in the sense of the highest test-set accuracy. This highlights the reliance of unsupervised methods on implicit inductive biases which cannot be set in a principled way"
  - [corpus] Weak evidence - corpus neighbors don't discuss prompt sensitivity in knowledge discovery
- Break condition: If prompt sensitivity could be explained by genuine knowledge extraction variations (e.g., different prompts activating different knowledge subsets), this mechanism would break.

## Foundational Learning

- Concept: Contrast pairs and activation normalization
  - Why needed here: The paper's methodology relies on creating contrast pairs (x+, x-) and normalizing activations to remove the "Yes"/"No" signal. Understanding this preprocessing is essential to grasp how unsupervised methods operate on the data.
  - Quick check question: Why does normalization remove the "Yes"/"No" feature, and what does this tell us about the remaining activation space?

- Concept: Binary classification probes and sigmoid outputs
  - Why needed here: CCS and other methods use linear probes followed by sigmoid functions to produce probabilities. The theoretical results show that binary probes can perfectly satisfy CCS loss, which is crucial for understanding the identification problem.
  - Quick check question: How does the proof that binary probes achieve optimal CCS loss relate to the claim that CCS can't distinguish knowledge from arbitrary features?

- Concept: Inductive biases in unsupervised learning
  - Why needed here: The paper argues that unsupervised methods learn whatever feature is most prominent due to inductive biases. Understanding how these biases work is key to grasping why knowledge discovery fails.
  - Quick check question: What role do inductive biases play in determining which feature (knowledge vs. distraction) an unsupervised method learns?

## Architecture Onboarding

- Component map:
  - Data pipeline: Text prompts → Contrast pairs → LLM → Activation extraction → Normalization
  - Methods: CCS (consistency loss), PCA (principal components), K-means (clustering), Logistic regression (supervised ceiling), Random (baseline)
  - Evaluation: Accuracy on ground truth vs. accuracy on inserted features
  - Theoretical analysis: Loss function analysis, probe transformation proofs

- Critical path: Prompt template selection → Contrast pair generation → Activation extraction → Probe training → Accuracy evaluation
  - Failure at any stage (e.g., poor normalization, inappropriate prompt) can lead to learning non-knowledge features

- Design tradeoffs:
  - Single vs. multiple prompt templates: Multiple templates might average out prompt-specific effects but increase complexity
  - Layer selection in models: Different layers may encode knowledge differently; layer 30 was chosen empirically
  - Loss function design: CCS adds consistency constraints but as shown, these don't specifically target knowledge

- Failure signatures:
  - High accuracy on ground truth with default prompts but random chance with modified prompts (indicates learning prominent features rather than knowledge)
  - Prompt sensitivity where semantically equivalent prompts yield different accuracies
  - Bimodal distributions in CCS results suggesting some seeds learn knowledge while others learn distractions

- First 3 experiments:
  1. Random word insertion (banana/shed): Test whether unsupervised methods learn arbitrary binary features instead of knowledge
  2. Explicit opinion insertion (Alice's view): Test whether methods learn simulated character opinions rather than review sentiment
  3. Prompt template variation (TruthfulQA): Test sensitivity to superficial prompt differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can unsupervised methods reliably distinguish a model's knowledge from simulated character beliefs?
- Basis in paper: [explicit] The paper explicitly states "we hypothesise that the identification issues explored here, e.g. distinguishing a model's knowledge from that of a simulated character's, will persist for future unsupervised methods."
- Why unresolved: The paper demonstrates that current methods fail at this distinction and the authors speculate this will persist, but they don't prove it's impossible for future methods.
- What evidence would resolve it: A new unsupervised method that consistently outperforms current methods on datasets where knowledge vs. simulated beliefs differ, with theoretical justification for why it can distinguish these cases.

### Open Question 2
- Question: What properties of LLM activations make certain features more prominent than knowledge in unsupervised learning?
- Basis in paper: [explicit] The paper states that unsupervised methods "discover whatever feature of the activations is most prominent" rather than knowledge.
- Why unresolved: The paper identifies this as a problem but doesn't investigate the underlying properties that make non-knowledge features more prominent in activations.
- What evidence would resolve it: A detailed analysis of activation patterns showing why random words, character opinions, or other non-knowledge features dominate over knowledge features in contrastive activation spaces.

### Open Question 3
- Question: Are there alternative consistency structures beyond negation-consistency that could better identify knowledge in LLMs?
- Basis in paper: [explicit] The paper challenges CCS's consistency structure but acknowledges "Perhaps future unsupervised methods could leverage additional structure beyond negation-consistency."
- Why unresolved: The paper focuses on CCS's specific consistency structure but doesn't explore what other consistency properties knowledge might have.
- What evidence would resolve it: Development of a new unsupervised method using a different consistency structure that demonstrably outperforms CCS and PCA while maintaining theoretical guarantees about knowledge identification.

## Limitations
- Theoretical proof relies on idealized conditions that may not hold in practice
- Empirical demonstrations use relatively simple inserted features that may not represent full complexity
- Study focuses on sentiment classification and fact verification, leaving generalization to other knowledge types open
- Analysis uses only two large language models at a single layer, limiting model-to-model and layer-to-layer variation analysis

## Confidence

- **High confidence**: The core finding that CCS and similar methods can learn non-knowledge features satisfying the same consistency structure - supported by both theoretical proof and multiple experimental demonstrations across different datasets.
- **Medium confidence**: The claim about prompt sensitivity indicating lack of principled foundations - while demonstrated empirically, alternative explanations (such as different prompts activating different knowledge subsets) cannot be fully ruled out.
- **Medium confidence**: The general insufficiency of unsupervised methods for knowledge discovery - the paper provides strong evidence but doesn't explore whether hybrid approaches (combining unsupervised discovery with minimal supervision) might succeed where pure unsupervised methods fail.

## Next Checks
1. Test whether CCS can be made to learn more complex non-knowledge features (e.g., stylistic properties, syntactic patterns, or multi-token phrases) to determine if the vulnerability is specific to simple binary features or extends to arbitrary features.

2. Repeat the experiments across multiple layers within each model and across different model architectures to determine whether the failure of unsupervised knowledge discovery is consistent or varies with model depth and architecture.

3. Test whether adding minimal supervision (e.g., a small number of labeled examples or domain constraints) to CCS can recover knowledge discovery while maintaining the benefits of unsupervised initialization, potentially identifying a path forward for practical applications.