---
ver: rpa2
title: When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations
arxiv_id: '2310.19698'
source_url: https://arxiv.org/abs/2310.19698
tags:
- attention
- token
- prefix-tuning
- layer
- prefix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Context-based fine-tuning methods like prompting, soft prompting,
  and prefix-tuning have gained popularity for their ability to match full fine-tuning
  performance with fewer parameters. However, their theoretical limitations remain
  unclear.
---

# When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations

## Quick Facts
- arXiv ID: 2310.19698
- Source URL: https://arxiv.org/abs/2310.19698
- Reference count: 40
- Key outcome: Context-based fine-tuning methods like prompting, soft prompting, and prefix-tuning are strictly less expressive than full fine-tuning, even with the same number of learnable parameters.

## Executive Summary
This paper provides a theoretical analysis of context-based fine-tuning methods, demonstrating that techniques like prompting, soft prompting, and prefix-tuning are fundamentally limited in their expressiveness compared to full fine-tuning. The key finding is that these methods cannot change the relative attention pattern over content tokens, only biasing attention outputs in fixed directions. While the continuous embedding space offers more capacity than discrete tokens, this additional capacity is not fully utilized in practice. The results suggest that context-based fine-tuning can effectively elicit skills present in pretrained models but cannot learn novel tasks requiring new attention patterns.

## Method Summary
The paper combines theoretical constructions with empirical validation to analyze the expressiveness limitations of context-based fine-tuning. Theoretical work demonstrates that the continuous embedding space has exponentially more capacity than discrete tokens, but the transformer architecture cannot fully exploit this. The authors construct synthetic transformers and provide mathematical proofs showing that prefix-tuning can only bias attention outputs without changing attention patterns over content. Empirical validation includes experiments with small transformers on controlled synthetic tasks and analysis of attention patterns in larger pretrained models like LLaMA and GPT-2.

## Key Results
- Prefix-tuning cannot change relative attention patterns over content tokens but only adds constant bias to attention outputs
- Despite continuous embedding space being more expressive than discrete token space, soft prompting and prefix-tuning are strictly less expressive than full fine-tuning
- Context-based fine-tuning can elicit or combine skills from pretraining but cannot learn novel tasks requiring new attention patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prefix-tuning cannot change the relative attention pattern over content tokens but can only bias the attention block outputs in a fixed direction.
- Mechanism: Prefix-tuning adds a constant bias to the attention layer output that depends on the prefix but not on the input content, while the relative attention distribution across content tokens remains unchanged.
- Core assumption: The attention pattern over content tokens is determined by the pretrained model's query-key interactions and cannot be modified by prefix-tuning.
- Evidence anchors:
  - [abstract] "context-based fine-tuning cannot change the relative attention pattern over the content and can only bias the outputs of an attention layer in a fixed direction"
  - [section 4] "The numerator of Apt_ij is the same as in Equation (5), i.e., the prefix does not affect it. It only adds the term exp(T/√k x_i^⊤ H x_1) to the denominator."
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism.

### Mechanism 2
- Claim: Soft prompting and prefix-tuning have more capacity than prompting due to the continuous embedding space, but this capacity is not fully utilized in practice.
- Mechanism: While the continuous embedding space is uncountably infinite and can encode exponentially more behaviors than the discrete token space, the transformer architecture cannot fully exploit this capacity in practice.
- Core assumption: The transformer can indeed utilize the additional capacity of the embedding space, but practical limitations prevent full utilization.
- Evidence anchors:
  - [abstract] "despite the continuous embedding space being more expressive than the discrete token space, soft-prompting and prefix-tuning are strictly less expressive than full fine-tuning"
  - [section 3] "The constructions for the results in Section 3...are simply an algorithm that extracts the completion from a lookup table encoded in the virtual tokens."
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism.

### Mechanism 3
- Claim: Prefix-tuning can elicit or combine skills picked up during pretraining but cannot learn completely new tasks that require new attention patterns.
- Mechanism: The prefix-induced bias acts as a "task selector" in the residual stream, nudging the model to select the desired solution from the subspace corresponding to the desired task, but cannot create new attention patterns.
- Core assumption: The pretrained model has learned to solve different tasks in parallel and stores their solutions in different subspaces of the residual stream.
- Evidence anchors:
  - [abstract] "techniques like prompting, in-context learning, soft prompting, and prefix-tuning can effectively elicit skills present in the pretrained model, they may not be able to learn novel tasks that require new attention patterns"
  - [section 5] "This can be clearly seen from the activations of the attention layer at the last input position...The prefix-tuned activations for the same inputs are clustered as a result of the prefix-induced bias."
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism.

## Foundational Learning

- Concept: Transformer architecture and attention mechanism
  - Why needed here: Understanding how transformers work is crucial to understanding the limitations of prefix-tuning and why it cannot change attention patterns.
  - Quick check question: How does the attention mechanism in a transformer work, and what are the key components (query, key, value matrices) that determine the attention pattern?

- Concept: Parameter-efficient fine-tuning methods
  - Why needed here: Prefix-tuning is a type of parameter-efficient fine-tuning, so understanding other methods (like LoRA) and how they compare is important for context.
  - Quick check question: What are the main differences between prefix-tuning and other parameter-efficient fine-tuning methods like LoRA or adapter modules?

- Concept: Theory of universal approximation
  - Why needed here: The paper discusses whether prefix-tuning can be a universal approximator and compares its parameter efficiency to other methods.
  - Quick check question: What is the universal approximation theorem, and how does it relate to the expressiveness of neural networks like transformers?

## Architecture Onboarding

- Component map: Input embeddings -> Attention layers (query, key, value matrices) -> MLP layers -> Output layer (vocabulary-sized linear layer)

- Critical path: Input -> Embeddings -> Attention Layer 1 -> MLP 1 -> Attention Layer 2 -> ... -> Output
  - Prefix parameters are added to the input of each attention layer

- Design tradeoffs:
  - Expressiveness vs. parameter efficiency: Prefix-tuning is more parameter-efficient than full fine-tuning but less expressive
  - Capacity utilization: The continuous embedding space has more capacity than the discrete token space, but this capacity is not fully utilized in practice
  - Task compatibility: Prefix-tuning works well for tasks that can be solved by combining pretrained skills, but struggles with completely new tasks

- Failure signatures:
  - Inability to change attention patterns: If the task requires a different attention pattern than what the pretrained model has learned, prefix-tuning will fail
  - Limited subspace utilization: If the prefix-induced bias does not span the full subspace defined by the prefix length, the method will be less effective
  - Catastrophic forgetting: While less likely than with full fine-tuning, prefix-tuning can still lead to some forgetting of pretrained skills

- First 3 experiments:
  1. Implement a simple transformer and verify that prefix-tuning cannot change the attention pattern over content tokens
  2. Compare the performance of prefix-tuning and LoRA on a task that requires a new attention pattern
  3. Analyze the attention distribution over prefix positions for different inputs to see if the full subspace is utilized

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions can context-based fine-tuning methods learn novel tasks that require new attention patterns?
- Basis in paper: [inferred] The paper shows that prefix-tuning cannot change the relative attention pattern over content and can only bias the output in a fixed direction, making it unable to learn tasks requiring new attention patterns.
- Why unresolved: The paper demonstrates limitations but does not provide a comprehensive framework for when novel task learning might be possible.
- What evidence would resolve it: Empirical studies comparing prefix-tuning performance on tasks requiring new vs. existing attention patterns, along with theoretical analysis of the relationship between task structure and fine-tuning method expressiveness.

### Open Question 2
- Question: How does the expressiveness of suffix-tuning compare to prefix-tuning and full fine-tuning?
- Basis in paper: [explicit] The paper notes that their theoretical results hold for suffix-tuning but does not necessarily apply to suffixing with prompts or soft prompts, leaving the expressiveness comparison open.
- Why unresolved: The paper focuses on prefix-tuning but acknowledges that suffix-tuning's properties are not fully explored.
- What evidence would resolve it: Empirical comparisons of suffix-tuning vs. prefix-tuning and full fine-tuning on various tasks, along with theoretical analysis of suffixing's structural limitations.

### Open Question 3
- Question: Under what conditions can prefix-tuning be an efficient universal approximator?
- Basis in paper: [explicit] The paper argues that while prefix-tuning can be considered as learning a neural network, its representational capacity is severely limited and it is unlikely to be an efficient universal approximator.
- Why unresolved: The paper provides an example where prefix-tuning fails to be a universal approximator but does not provide a comprehensive analysis of the conditions under which it might be one.
- What evidence would resolve it: Formal analysis of the conditions under which prefix-tuning can be a universal approximator, along with empirical studies comparing its parameter efficiency to other fine-tuning methods on various tasks.

## Limitations

- The analysis assumes standard transformer architectures and may not generalize to architectures with different attention mechanisms
- The paper relies heavily on theoretical proofs with limited empirical validation on large-scale models
- The effectiveness of prefix-tuning is fundamentally tied to the quality and diversity of pretraining, which may not hold for all domains

## Confidence

**High Confidence**: The core theoretical claim that prefix-tuning cannot change relative attention patterns over content tokens is well-supported by mathematical proofs and has clear implications for expressiveness.

**Medium Confidence**: The claims about subspace utilization and task compatibility are supported by theoretical arguments but would benefit from more extensive empirical validation across diverse tasks and model scales.

**Low Confidence**: The assertions about catastrophic forgetting and failure modes are primarily theoretical and lack comprehensive empirical validation across different model families and pretraining strategies.

## Next Checks

**Validation Check 1**: Implement controlled experiments comparing prefix-tuning and full fine-tuning on tasks requiring novel attention patterns (e.g., sorting, hierarchical reasoning). Measure not just task performance but explicitly analyze the learned attention distributions to verify the theoretical claims about pattern invariance.

**Validation Check 2**: Conduct ablation studies on different transformer variants (multi-head attention configurations, attention-free transformers) to test the robustness of the theoretical limitations. Specifically examine whether modified attention mechanisms can circumvent the identified constraints.

**Validation Check 3**: Perform systematic experiments across different pretraining paradigms (causal language modeling vs. masked language modeling) to assess how pretraining objectives affect the subspace organization that prefix-tuning exploits. This would validate the dependency claims about pretrained skill availability.