---
ver: rpa2
title: 'FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation'
arxiv_id: '2310.03214'
source_url: https://arxiv.org/abs/2310.03214
tags:
- questions
- fresh
- prompt
- answer
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FRESHQA is a new benchmark designed to evaluate the factuality
  of large language models (LLMs) by asking them questions that require up-to-date
  world knowledge. We found that existing LLMs struggle on this benchmark, particularly
  on questions involving fast-changing knowledge and false premises.
---

# FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation

## Quick Facts
- arXiv ID: 2310.03214
- Source URL: https://arxiv.org/abs/2310.03214
- Reference count: 40
- Key outcome: FRESHQA benchmark reveals LLMs struggle with current knowledge; FRESH PROMPT method using search engine augmentation significantly improves performance

## Executive Summary
FreshLLMs introduces FRESHQA, a benchmark designed to evaluate the factuality of large language models on questions requiring up-to-date world knowledge. The benchmark reveals that existing LLMs struggle particularly with fast-changing knowledge and false-premise questions. To address this limitation, the authors propose FRESH PROMPT, a simple few-shot prompting method that incorporates relevant and up-to-date information from search engines into the model's prompt. This approach substantially boosts LLM performance on FRESHQA, outperforming competing search-augmented methods, with analysis showing that both the number and order of retrieved evidences significantly influence answer correctness.

## Method Summary
FRESH PROMPT is an in-context learning method that augments LLM prompts with retrieved search engine evidences. For each question, the method queries a search engine, retrieves relevant results including answer boxes, organic results, knowledge graph snippets, and related questions, then formats these evidences uniformly with source, date, title, and highlighted words. These evidences are incorporated into the prompt along with few-shot demonstrations showing how to reason over the evidence. The model learns to attend to the most relevant and recent information to generate accurate responses, with the approach specifically designed to handle both factual questions and those containing false premises.

## Key Results
- FRESH PROMPT significantly outperforms competing search-augmented approaches on the FRESHQA benchmark
- Increasing retrieved evidences from 1 to 15 improves accuracy by up to 16.2% under strict evaluation
- Sorting evidences from oldest to newest improves performance by encouraging focus on recent information
- Concise demonstration answers reduce hallucination compared to verbose demonstrations (2.6% accuracy improvement)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot in-context learning can teach LLMs to reason over retrieved search engine evidences and ground their responses in factual, up-to-date information.
- Mechanism: The model is provided with demonstrations that show how to process retrieved search results, extract relevant information, and generate an answer based on that evidence.
- Core assumption: The LLM can effectively learn the reasoning process over retrieved evidences through in-context learning without additional fine-tuning.
- Evidence anchors: [abstract], [section 4.1]

### Mechanism 2
- Claim: Incorporating more relevant and up-to-date evidences into the prompt improves the accuracy of LLM-generated answers.
- Mechanism: By retrieving and including a larger number of relevant search results, the model has access to a wider range of information, including conflicting answers.
- Core assumption: LLMs can effectively handle an increasing number of retrieved evidences and ground their responses accordingly.
- Evidence anchors: [abstract], [section 4.3]

### Mechanism 3
- Claim: Instructing the LLM to generate concise and direct answers reduces hallucination compared to encouraging more verbose answers.
- Mechanism: By providing demonstrations with concise answers and instructing the model to generate direct responses, the method limits the opportunity for the model to generate hallucinated or irrelevant information.
- Core assumption: LLMs are less likely to hallucinate when constrained to provide concise and direct answers.
- Evidence anchors: [abstract], [section 4.3]

## Foundational Learning

- Concept: Few-shot in-context learning
  - Why needed here: The method relies on providing the LLM with a few examples to learn the task of reasoning over retrieved evidences without fine-tuning the model.
  - Quick check question: Can the LLM effectively learn a new task by observing a few input-output exemplars in the prompt?

- Concept: Retrieval-augmented generation
  - Why needed here: The method augments the LLM's knowledge by incorporating relevant information retrieved from a search engine into the prompt.
  - Quick check question: Does incorporating external information into the prompt improve the LLM's ability to generate accurate responses?

- Concept: Hallucination in LLMs
  - Why needed here: The method aims to reduce hallucination by grounding the LLM's responses in factual and up-to-date information from search results.
  - Quick check question: How does the LLM's tendency to hallucinate impact its performance on tasks requiring current knowledge?

## Architecture Onboarding

- Component map: Search engine interface -> Evidence formatter -> Prompt generator -> LLM
- Critical path: 1. Receive a question. 2. Query the search engine and retrieve results. 3. Format the retrieved evidences. 4. Generate the input prompt with evidences and demonstrations. 5. Send the prompt to the LLM and obtain the answer.
- Design tradeoffs: Number of retrieved evidences vs. prompt length; Evidence order (date vs. search engine vs. random); Demonstration verbosity (concise vs. verbose)
- Failure signatures: Inaccurate or irrelevant search results; LLM failing to attend to evidences or demonstrations; Exceeding LLM context limit
- First 3 experiments: 1. Vary number of retrieved evidences (1, 5, 10, 15) and measure accuracy impact. 2. Compare sorting evidences by date vs. search engine order vs. random order. 3. Test effect of concise vs. verbose demonstration answers on accuracy and hallucination.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of search engine choice on the performance of FRESH PROMPT, particularly when the search engine does not provide features like answer boxes or knowledge graphs?
- Basis in paper: [inferred] The paper states that FRESH PROMPT interfaces with GOOGLE SEARCH and that it is unclear how it performs with other search engines that lack certain features.
- Why unresolved: The paper only evaluates FRESH PROMPT using GOOGLE SEARCH and does not explore its performance with other search engines that might not offer the same level of detail or features.
- What evidence would resolve it: Conducting experiments using FRESH PROMPT with different search engines, such as BING or DUCKDUCKGO, and comparing the results to those obtained with GOOGLE SEARCH.

### Open Question 2
- Question: How does the performance of FRESH PROMPT change when using multiple search queries per question, especially for complex questions that might require decomposition?
- Basis in paper: [inferred] The paper mentions that FRESH PROMPT relies on a single search query per question and suggests that its performance could be further improved by decomposing questions and performing multiple search queries.
- Why unresolved: The paper does not explore the effects of using multiple search queries or question decomposition on the performance of FRESH PROMPT.
- What evidence would resolve it: Conducting experiments with FRESH PROMPT using multiple search queries per question and comparing the results to those obtained with a single query.

### Open Question 3
- Question: What is the effect of different writing styles in the demonstration answers on the performance of FRESH PROMPT, particularly for complex questions?
- Basis in paper: [explicit] The paper explores the effect of verbose demonstration answers and finds that they may be helpful for complex questions but can also increase hallucination.
- Why unresolved: The paper only experiments with one style of demonstration answers and does not explore other writing styles or their impact on the performance of FRESH PROMPT.
- What evidence would resolve it: Conducting experiments with FRESH PROMPT using different styles of demonstration answers (e.g., concise, technical, conversational) and comparing the results.

## Limitations
- Performance relies heavily on quality and relevance of search engine results, which may vary by query and search engine
- Method's effectiveness may be influenced by specific LLM architectures and their capacity for in-context learning
- Study focuses on a curated benchmark, and real-world deployment may face additional challenges

## Confidence
- High: FRESHQA benchmark design and evaluation methodology
- Medium: Search engine augmentation effectiveness across diverse question types
- Medium: In-context learning generalization to other LLMs and domains

## Next Checks
1. **Ablation study on evidence selection**: Systematically test the impact of different search result filtering strategies and their relevance scores on final accuracy
2. **Cross-domain generalization**: Evaluate FRESH PROMPT on non-factoid question types and domains not represented in FRESHQA (e.g., creative writing, reasoning tasks)
3. **Resource efficiency analysis**: Measure the computational overhead and latency introduced by search engine queries compared to baseline approaches