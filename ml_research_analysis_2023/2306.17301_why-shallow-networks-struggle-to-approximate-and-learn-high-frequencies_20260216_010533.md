---
ver: rpa2
title: Why Shallow Networks Struggle to Approximate and Learn High Frequencies
arxiv_id: '2306.17301'
source_url: https://arxiv.org/abs/2306.17301
tags:
- function
- then
- neural
- which
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work presents a comprehensive numerical study analyzing why
  shallow neural networks struggle to approximate and learn high-frequency components
  in functions. The study focuses on three fundamental computational issues: the minimal
  numerical error achievable under finite precision, the computational cost required
  to attain a given accuracy, and the stability of the method with respect to perturbations.'
---

# Why Shallow Networks Struggle to Approximate and Learn High Frequencies

## Quick Facts
- arXiv ID: 2306.17301
- Source URL: https://arxiv.org/abs/2306.17301
- Reference count: 40
- Key outcome: Shallow neural networks act as low-pass filters due to rapid Gram matrix eigenvalue decay, limiting high-frequency approximation despite universal approximation capability.

## Executive Summary
This paper presents a comprehensive numerical study explaining why shallow neural networks struggle to approximate and learn high-frequency components in functions. The analysis focuses on three fundamental computational issues: minimal numerical error under finite precision, computational cost to achieve accuracy, and stability with respect to perturbations. Through spectral analysis of the Gram matrix, the study reveals that eigenvalues decay rapidly (O(k^-4) in 1D for ReLU), causing networks to act as low-pass filters that can only stably resolve leading eigenmodes. This limits the ability to capture high-frequency features even when theoretical universal approximation holds.

## Method Summary
The study analyzes two-layer neural networks with ReLU activation through spectral analysis of the Gram matrix, gradient flow-based learning dynamics, and Rashomon set characterization. The Gram matrix is constructed from activation function correlations, and its spectral properties are examined to understand approximation capabilities. Learning dynamics are studied through continuous-time gradient flow analysis, revealing stiffness due to ill-conditioning. The Rashomon set measure is characterized to understand the probability of finding good solutions. Numerical experiments in 1D and 2D validate the theoretical findings.

## Key Results
- Gram matrix eigenvalues decay rapidly (O(k^-4) in 1D) for ReLU activations, creating low-pass filter behavior
- Ill-conditioning causes stiffness in gradient flow dynamics, leading to slow convergence for high-frequency modes
- Rashomon set has exponentially small measure for oscillatory functions, making good solutions hard to find randomly

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Gram matrix spectral decay (O(k^-4) in 1D) acts as a low-pass filter that limits high-frequency resolution in shallow networks.
- Mechanism: Activation functions in two-layer networks are highly correlated, causing rapid eigenvalue decay in the Gram matrix. This means only low-frequency modes (leading eigenvectors) can be stably resolved within finite machine precision, effectively filtering out high-frequency components.
- Core assumption: The decay rate is sufficiently fast that high-frequency eigenvalues fall below machine precision thresholds.
- Evidence anchors:
  - [abstract] states "eigenvalues of the Gram matrix decay rapidly (as O(k^-4) in 1D)"
  - [section] explains "eigenvalues of the Gram matrix decay rapidly (as O(k^-4) in 1D), causing the network to act as a 'low-pass filter'"

### Mechanism 2
- Claim: Ill-conditioning of the Gram matrix causes stiffness in gradient flow dynamics, leading to slow convergence for high-frequency modes.
- Mechanism: The rapid eigenvalue decay creates large condition numbers in the Gram matrix, making the gradient flow system stiff. This stiffness causes gradient-based optimization to converge very slowly for high-frequency components, even when they are theoretically learnable.
- Core assumption: The optimization landscape's conditioning directly impacts learning speed through gradient flow dynamics.
- Evidence anchors:
  - [abstract] mentions "ill-conditioning of the Gram matrix leads to stiffness in the system, causing slow learning dynamics"
  - [section] shows "ill-conditioning of the Gram matrix leads to the stiffness of the learning dynamics"

### Mechanism 3
- Claim: Small Rashomon set measure for oscillatory functions makes finding good approximations probabilistically difficult.
- Mechanism: The set of parameters that achieve accurate approximations (Rashomon set) has exponentially small measure for high-frequency functions. This means random initialization or simple search methods have vanishing probability of finding good solutions for oscillatory targets.
- Core assumption: The measure of the Rashomon set directly correlates with the probability of finding good solutions through random or simple search methods.
- Evidence anchors:
  - [abstract] states "Rashomon set, the set of parameters where accurate approximations can be achieved, has a small measure for highly oscillatory functions"
  - [section] shows "Rashomon set, the set of parameters where accurate approximations can be achieved, for two-layer neural networks has a small measure for highly oscillatory functions"

## Foundational Learning

- Concept: Spectral analysis of integral operators and kernel methods
  - Why needed here: Understanding the Gram matrix as a discrete approximation to a continuous integral operator is crucial for analyzing the spectral properties that drive the low-pass filter behavior
  - Quick check question: Can you explain why the eigenvalues of the continuous Gram kernel G translate to the discrete Gram matrix eigenvalues with scaling n^2?

- Concept: Gradient flow and continuous-time optimization dynamics
  - Why needed here: The analysis of learning dynamics requires understanding how gradient descent behaves as a continuous-time dynamical system, particularly how ill-conditioning affects convergence rates
  - Quick check question: What is the relationship between the condition number of the Gram matrix and the stiffness of the gradient flow system?

- Concept: Probabilistic analysis and concentration inequalities
  - Why needed here: Characterizing the Rashomon set measure requires understanding how random parameter choices concentrate around good solutions, which relies on concentration inequalities
  - Quick check question: How does Hoeffding's inequality apply to bounding the probability of finding good parameters in the Rashomon set?

## Architecture Onboarding

- Component map: Two-layer neural network with fixed weights/biases in hidden layer -> Gram matrix construction from activation correlations -> Spectral analysis of eigenvalues -> Gradient flow dynamics on linear system -> Rashomon set characterization
- Critical path: 1) Fix hidden layer parameters (weights/biases), 2) Construct Gram matrix from activation function correlations, 3) Analyze spectral properties, 4) Study optimization dynamics on the resulting linear system
- Design tradeoffs: Fixed vs. trainable hidden layer parameters - fixed parameters enable spectral analysis but limit expressivity; trainable parameters increase complexity but may improve approximation
- Failure signatures: Slow convergence on high-frequency targets, error plateaus below machine precision despite increasing network width, sensitivity to initialization
- First 3 experiments:
  1. Replicate 1D spectrum analysis: compute eigenvalues of Gram matrix for ReLU functions with uniform and adaptive biases, verify O(k^-4) decay
  2. Test low-pass filter effect: approximate smooth vs. oscillatory functions with varying network widths, measure error convergence
  3. Verify learning dynamics: train on high-frequency targets with gradient descent, measure convergence rates and compare to theoretical predictions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the conditioning of the Gram matrix for two-layer ReLU networks change in higher dimensions?
- Basis in paper: [explicit] The paper states "The spectrum of the discrete Gram matrix has a similar behavior when wi ∈ Sd−1, bi ∈ [−1, 1] are uniformly distributed" and mentions that "the ill-conditioning of the Gram matrix may be less severe in higher dimensions since the dense sampling of wi and bi may not be affordable."
- Why unresolved: While the paper provides theoretical bounds on eigenvalue decay rates in higher dimensions, it does not provide numerical experiments or analysis of the actual conditioning of the Gram matrix in higher dimensions.
- What evidence would resolve it: Numerical experiments showing the spectrum of the Gram matrix and its conditioning for two-layer ReLU networks in 2D and 3D, along with analysis of how the conditioning scales with network width and dimension.

### Open Question 2
- Question: How does the training dynamics of two-layer ReLU networks change when using different activation functions?
- Basis in paper: [explicit] The paper states "The smoother the activation function is, the faster the Gram matrix spectrum decays" and mentions "the activation function plays a crucial role in the analysis" but does not provide a comprehensive study of different activation functions.
- Why unresolved: The paper focuses on ReLU activation functions and only briefly mentions other activation functions in the appendix. A comprehensive study of how different activation functions affect the training dynamics is missing.
- What evidence would resolve it: Numerical experiments comparing the training dynamics of two-layer ReLU networks using different activation functions (e.g., Leaky ReLU, Tanh, Sigmoid) for functions with high-frequency components, along with analysis of the corresponding Gram matrix spectra.

### Open Question 3
- Question: How does the Rashomon set for two-layer ReLU networks change when using different network architectures?
- Basis in paper: [explicit] The paper characterizes the Rashomon set for two-layer ReLU networks and states "the optimal set of parameters only occupies an extremely small measure of the parameter space for highly oscillatory functions" but does not explore other network architectures.
- Why unresolved: The paper only considers two-layer ReLU networks and does not explore how the Rashomon set changes for other network architectures (e.g., deep networks, networks with different activation functions).
- What evidence would resolve it: Analysis of the Rashomon set for different network architectures (e.g., deep ReLU networks, networks with different activation functions) and comparison of the measure of the Rashomon set for functions with high-frequency components.

## Limitations
- Spectral analysis relies on specific assumptions about activation function correlations that may not generalize to all function classes
- Theoretical framework assumes idealized conditions (infinite precision, perfect optimization) that may not hold in practical implementations
- Rashomon set measure characterization relies on assumptions about the geometry of good solutions that may not hold for all function classes

## Confidence
- High confidence: The spectral decay mechanism and its connection to low-pass filtering behavior
- Medium confidence: The learning dynamics analysis and practical implications
- Low confidence: The Rashomon set measure characterization and probabilistic analysis

## Next Checks
1. Test the spectral decay analysis with different activation functions (Leaky ReLU, tanh, polynomial) to verify if the low-pass filter behavior is universal or specific to ReLU
2. Extend the spectral analysis to 3D and higher dimensions to validate whether the O(k^-4) decay generalizes or follows different patterns in higher dimensions
3. Compare gradient flow dynamics with practical optimization algorithms (Adam, SGD with momentum) to determine if the stiffness issue persists and identify potential mitigation strategies