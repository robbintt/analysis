---
ver: rpa2
title: Preference Ranking Optimization for Human Alignment
arxiv_id: '2306.17492'
source_url: https://arxiv.org/abs/2306.17492
tags:
- human
- responses
- reward
- ranking
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Preference Ranking Optimization (PRO) is proposed as a replacement
  for reinforcement learning from human feedback (RLHF) to directly align language
  models with human preferences. The core idea is to extend the Bradley-Terry paired
  comparison to accommodate preference rankings of any length, and iteratively contrast
  the likelihood of generating responses to prioritize the best response while progressively
  ranking the remaining responses.
---

# Preference Ranking Optimization for Human Alignment

## Quick Facts
- arXiv ID: 2306.17492
- Source URL: https://arxiv.org/abs/2306.17492
- Reference count: 11
- Key outcome: PRO outperforms baseline algorithms, achieving comparable results to ChatGPT and human responses through various evaluations

## Executive Summary
Preference Ranking Optimization (PRO) introduces a direct method for aligning language models with human preferences by extending the Bradley-Terry paired comparison to handle preference rankings of any length. Instead of approximating a reward model as in RLHF, PRO directly optimizes the language model to rank generated responses according to human preferences. The method iteratively contrasts generation likelihoods to prioritize the best response while progressively ranking the remaining responses, effectively transforming human alignment into aligning the probability ranking of n responses with human preference rankings.

## Method Summary
PRO replaces RLHF's two-stage process (reward modeling + reinforcement learning) with direct optimization of the language model to match human preference rankings. The method extends Bradley-Terry paired comparison to accommodate rankings of arbitrary length, computing response scores as log-likelihoods from the LLM and using these to construct a contrastive loss. During training, the LLM generates multiple responses to a prompt, which are then ranked according to human preferences. PRO optimizes the model to align the probability ranking of generated responses with the human preference ranking through a contrastive loss that treats all worse responses as negative examples of better ones. The approach naturally inherits self-bootstrapping capability from RLHF by allowing sampled responses to be added to the response set and reranked based on reward scores.

## Key Results
- PRO outperforms baseline algorithms including SFT and RLHF on the HH-RLHF dataset
- Performance improves with longer, more diverse, and higher-quality preference ranking sequences
- PRO achieves comparable results to ChatGPT and human responses through automatic metrics, reward model scores, and human evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PRO transforms human alignment into aligning the probability ranking of n responses generated by LLM with the preference ranking of humans towards these responses.
- Mechanism: By iteratively contrasting generation likelihood, PRO teaches the LLM to prioritize the best response while progressively ranking the remaining responses. This is achieved through a recursive application of the Bradley-Terry comparison extended to preference rankings of arbitrary lengths.
- Core assumption: The Bradley-Terry model can be effectively extended from pairwise comparisons to rankings of arbitrary length while preserving its preference-ordering properties.
- Evidence anchors:
  - [abstract] "PRO effectively transforms human alignment into aligning the probability ranking of n responses generated by LLM with the preference ranking of humans towards these responses."
  - [section 3.1] "Based on this motivation, we propose the Preference Ranking Optimization (PRO) algorithm. Instead of optimizing the LLM to approximate the Reward Model, we propose directly training the LLM to reach Equation 5."
  - [corpus] Weak evidence - no direct corpus support found for the specific claim about recursive Bradley-Terry extension.

### Mechanism 2
- Claim: PRO achieves better data efficiency than RLHF by requiring only n-1 comparisons for a ranking of length n, while RLHF requires n(n-1)/2 pairwise comparisons.
- Mechanism: PRO treats all responses yi ≺ yk as negative examples of yk and applies the same penalty to them in each comparison, whereas RLHF only compares individual pairs.
- Core assumption: Treating all worse responses as negatives in a single comparison is statistically as effective as pairwise comparisons for learning preference orderings.
- Evidence anchors:
  - [section 3.1] "PRO only needs n − 1 comparisons and introduces more negative examples in each comparison compared to RLHF."
  - [section 3.1] "Therefore, PRO provides better and more stable score estimates since more negative examples enlarge the response space, making the ranking process for obtaining the desired response more aligned with human expectations."
  - [corpus] No direct corpus evidence found for the specific claim about comparison efficiency advantage.

### Mechanism 3
- Claim: PRO naturally inherits self-bootstrapping capability from RLHF by allowing sampled responses to be added to the response set and reranked based on reward scores.
- Mechanism: During training, responses sampled from the LLM can be added to the existing response set, reranked using the reward model, and then used for further optimization through PRO.
- Core assumption: The self-bootstrapping mechanism from RLHF can be effectively grafted onto PRO without disrupting its direct optimization approach.
- Evidence anchors:
  - [section 3.2] "Furthermore, it is worth noting that the length of sequences that PRO relies on is variable. In other words, there is no requirement for fixed sequences during training. This allows us to consider grafting the self-bootstrapping advantage of RLHF as a subset onto PRO."
  - [section 3.2] "Specifically, RLHF aims to continuously evaluate the model's responses during training by employing a reward model. Positive or negative rewards are provided to bootstrap the Language Model itself. Similarly, with PRO..."
  - [corpus] Weak evidence - the corpus mentions self-bootstrapping but doesn't provide strong evidence for its effectiveness when combined with PRO.

## Foundational Learning

- Concept: Bradley-Terry paired comparison model
  - Why needed here: Forms the theoretical foundation for both RLHF and PRO, and understanding its limitations motivates the need for PRO's extension
  - Quick check question: How does the Bradley-Terry model assign probabilities to preference orderings in pairwise comparisons?

- Concept: Contrastive learning principles
- Why needed here: PRO establishes a connection between human alignment and contrastive learning by maximizing the similarity between a query and its corresponding positive instance while creating distance from other negatives
  - Quick check question: How does PRO's treatment of multiple negative examples per comparison relate to standard contrastive learning formulations?

- Concept: Reinforcement learning vs supervised fine-tuning tradeoffs
  - Why needed here: Understanding why RLHF is more complex than SFT helps explain PRO's advantage in avoiding RL's non-differentiable optimization while maintaining alignment benefits
  - Quick check question: What are the key differences in optimization objectives between RLHF's reward maximization and PRO's ranking alignment?

## Architecture Onboarding

- Component map:
  - LLM backbone (e.g., LLaMA-7B) -> generates candidate responses
  - Contrastive loss module -> implements PRO objective (Equation 8)
  - SFT loss module -> maintains text quality through NLL loss
  - Temperature scaling module -> implements dynamic temperature for differentiated contrast (Equation 9-11)
  - Optional reward model -> for sorting expanded rankings and self-bootstrapping

- Critical path: Input prompt → LLM generates n responses → Compute generation likelihoods → Apply PRO contrastive loss → Backpropagate through LLM → Update parameters

- Design tradeoffs:
  - Ranking length vs. computational cost: Longer rankings provide more preference information but increase computation quadratically
  - SFT vs. PRO loss weighting: Balancing text quality maintenance with preference alignment
  - Temperature scaling vs. uniform contrast: Differentiated penalties may improve learning but add complexity

- Failure signatures:
  - Overfitting to reward model: BLEU scores drop significantly while reward scores improve
  - Degraded text quality: Generated responses become repetitive or lose coherence
  - Insufficient preference learning: Reward scores plateau early despite continued training

- First 3 experiments:
  1. Baseline comparison: Train PRO on HH-RLHFraw (ranking length 2) vs. SFT and RLHF, measuring BLEU and reward scores
  2. Ranking length ablation: Train PRO with ranking lengths 2, 3, 4, 5 on HH-RLHFAlpaca, measuring performance gains
  3. Temperature scaling ablation: Train PRO with and without dynamic temperature on HH-RLHFChatGPT, measuring impact on preference learning and text quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PRO scale with increasingly longer preference ranking sequences beyond length 5?
- Basis in paper: [explicit] The paper mentions that longer ranking sequences generally lead to improved performance, but only experiments with sequences up to length 5.
- Why unresolved: The paper does not provide data on how PRO performs with preference ranking sequences longer than 5, leaving uncertainty about the upper limits of its scalability.
- What evidence would resolve it: Experiments showing PRO's performance with preference ranking sequences of lengths 6, 10, 20, etc., would clarify how the algorithm scales with longer sequences.

### Open Question 2
- Question: How does PRO perform when fine-tuned on larger language models (e.g., LLaMA-13B or LLaMA-33B) compared to LLaMA-7B?
- Basis in paper: [inferred] The paper uses LLaMA-7B as the backbone model, but does not explore how PRO's performance changes with larger models.
- Why unresolved: The impact of model size on PRO's effectiveness is not addressed, leaving uncertainty about whether the algorithm's benefits scale with model capacity.
- What evidence would resolve it: Experiments comparing PRO's performance on LLaMA-7B, LLaMA-13B, and LLaMA-33B would reveal how model size affects the algorithm's effectiveness.

### Open Question 3
- Question: How does the quality of self-bootstrapped responses compare to responses generated by other high-quality LLMs (e.g., ChatGPT) when used to expand preference ranking sequences?
- Basis in paper: [explicit] The paper finds that self-bootstrapping improves performance on HH-RLHFChatGPT,3 but not on HH-RLHFraw or HH-RLHFAlpaca,3, suggesting that the quality of the underlying LLM matters.
- Why unresolved: The paper does not directly compare the quality of self-bootstrapped responses to those generated by other high-quality LLMs, leaving uncertainty about the relative effectiveness of these approaches.
- What evidence would resolve it: Experiments comparing the performance of PRO when using self-bootstrapped responses versus responses from other high-quality LLMs (e.g., ChatGPT) would clarify the relative effectiveness of these approaches.

## Limitations

- The effectiveness of treating multiple negative examples in a single comparison has not been rigorously validated through ablation studies
- The paper lacks detailed analysis of how ranking length affects convergence rates and final performance
- The self-bootstrapping capability is mentioned but not empirically validated for its benefits or potential instability

## Confidence

**High Confidence:** The core mechanism of using preference rankings to align language models (Mechanism 1) is well-founded and clearly explained. The mathematical formulation of the PRO loss is explicit and reproducible.

**Medium Confidence:** The data efficiency claims (Mechanism 2) are supported by the theoretical formulation but lack empirical validation through controlled experiments comparing PRO to RLHF on equivalent computational budgets.

**Low Confidence:** The self-bootstrapping mechanism (Mechanism 3) is mentioned but not empirically validated. The paper doesn't provide evidence that this capability actually improves performance or that it can be successfully implemented without introducing instability.

## Next Checks

1. **Ranking Length Sensitivity Analysis:** Conduct controlled experiments varying ranking lengths from 2 to 10 on a held-out validation set to measure how PRO performance scales with ranking complexity, and compare this to the computational cost increase.

2. **Direct Efficiency Comparison:** Design an experiment that measures both final performance and training time for PRO versus RLHF on equivalent datasets, accounting for the different numbers of comparisons required, to validate the claimed data efficiency advantage.

3. **Self-Bootstrapping Validation:** Implement and test the self-bootstrapping mechanism proposed in section 3.2, measuring whether adding sampled responses to rankings improves alignment quality over time without causing reward hacking or model collapse.