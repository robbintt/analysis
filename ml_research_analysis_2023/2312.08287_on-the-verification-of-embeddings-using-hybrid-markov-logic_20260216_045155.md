---
ver: rpa2
title: On the verification of Embeddings using Hybrid Markov Logic
arxiv_id: '2312.08287'
source_url: https://arxiv.org/abs/2312.08287
tags:
- verification
- embeddings
- property
- where
- specification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for verifying properties of deep
  neural network embeddings by integrating symbolic domain knowledge with sub-symbolic
  representations using Hybrid Markov Logic Networks (HMLNs). The key contribution
  is learning HMLN parameters from specification embeddings and verifying test embeddings
  by computing bounds on property probabilities via Mixed Integer Linear Programming
  (MILP).
---

# On the verification of Embeddings using Hybrid Markov Logic

## Quick Facts
- **arXiv ID**: 2312.08287
- **Source URL**: https://arxiv.org/abs/2312.08287
- **Reference count**: 23
- **Primary result**: Introduces a framework for verifying deep neural network embeddings by integrating symbolic domain knowledge with sub-symbolic representations using Hybrid Markov Logic Networks (HMLNs), demonstrating successful verification across GNNs, DKT, and ITS domains.

## Executive Summary
This paper presents a novel framework for verifying properties of deep neural network embeddings by integrating symbolic domain knowledge with sub-symbolic representations using Hybrid Markov Logic Networks (HMLNs). The key innovation is learning HMLN parameters from specification embeddings and verifying test embeddings by computing bounds on property probabilities via Mixed Integer Linear Programming (MILP). The framework is evaluated across three domains: semantic verification of Graph Neural Network (GNN) embeddings, invariance verification of Deep Knowledge Tracing (DKT) embeddings, and transferability verification of Intelligent Tutoring System (ITS) embeddings.

## Method Summary
The framework learns HMLN parameters from specification DNN embeddings using a Rao-Blackwellized likelihood function that reduces variance by marginalizing out sub-symbolic atom uncertainty. The HMLN is encoded as a Mixed Integer Linear Program (MILP) and solved using Gurobi to perform MAP inference for weight learning. To manage computational complexity, the embedding space is recursively partitioned into hypercubes with shared weights across each partition. Verification of test embeddings involves computing upper and lower bounds on property probabilities via MILP and comparing these bounds using Welch's T-Test to determine statistical similarity with specification embeddings.

## Key Results
- Successfully verified semantic preservation in GNN embeddings across Cora, Citeseer, and PubMed datasets with GCN, GAT, and GraphSage architectures
- Verified invariance to problem ordering in DKT embeddings across varying problem counts, student counts, and latent concepts
- Verified transferability of student knowledge embeddings across tasks in ITS using the Carnegie Learning MATHia 2019-20 dataset
- Learned HMLN parameters that capture domain-specific properties while maintaining computational tractability through hypercube partitioning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Rao-Blackwellized likelihood reduces variance in learned HMLN weights by marginalizing out sub-symbolic atom uncertainty before gradient computation.
- **Mechanism**: The objective function ℓRB(w, y, xs, xe) integrates out the assignments to sub-symbolic atoms (xs) from the log-likelihood. This creates a conditional distribution P RB w(Y = y|Xe) that is less sensitive to noise in specification embeddings. During gradient descent, the partial derivative uses expectations Ew,xs and Ew,xs,xe which are estimated via MAP inference, producing more stable weight updates.
- **Core assumption**: Sub-symbolic atom values from the specification DNN introduce noise that can be mitigated by marginalization; the MAP approximation of expectations is sufficiently accurate.
- **Evidence anchors**:
  - [abstract] "we develop a Rao-Blackwellized likelihood function that we optimize to learn the specification HMLN"
  - [section IV] "To reduce σ2( ˆw), we Rao-Blackwellize the log-likelihood by summing out sub-symbolic atoms from the function"
  - [corpus] Weak: No direct citations supporting Rao-Blackwellization in neural embedding verification contexts
- **Break condition**: If the MAP approximations have high variance or the sub-symbolic atoms are highly correlated with symbolic atoms in a way that marginalization destroys important signal, the variance reduction benefit disappears.

### Mechanism 2
- **Claim**: Mixed Integer Linear Programming (MILP) formulation enables scalable verification by encoding MAP inference over hybrid discrete-continuous spaces.
- **Mechanism**: The HMLN distribution is converted to a linear objective Xiwi si(x) where each property's contribution is weighted. Discrete variables encode truth values of symbolic atoms, continuous variables represent sub-symbolic atoms, and constraints encode logical relationships and soft inequalities/equalities. Gurobi solves this to obtain MAP assignments that approximate expectations needed for weight learning and bound computation.
- **Core assumption**: The piecewise linear relaxation of non-linear soft functions (equality/inequality) preserves the relative ordering of probabilities sufficiently for verification purposes.
- **Evidence anchors**:
  - [abstract] "we develop a verification method to test embeddings in this framework by encoding this task as a Mixed Integer Linear Program"
  - [section IV-A] "we formulate MAP inference as a Mixed Integer Linear Program (MILP)"
  - [corpus] Weak: No citations showing MILP effectiveness specifically for HMLN verification
- **Break condition**: When the piecewise linear approximation deviates significantly from true non-linear functions, or when the number of variables makes MILP intractable despite solver optimizations.

### Mechanism 3
- **Claim**: Weight sharing via hypercube projections reduces parameter count while preserving verification accuracy through structured partitioning of embedding space.
- **Mechanism**: The embedding space is recursively partitioned into disjoint hypercubes based on subsets of domains. Properties are projected onto each hypercube, and weights are shared across all groundings within a projection. This creates k*n learnable weights from potentially exponential groundings. Theorem 1 provides a bound on the log-likelihood difference between optimal and shared weights.
- **Core assumption**: The geometry, sparsity, and density of points vary across embedding space such that partitioning captures these variations and sharing within partitions is appropriate.
- **Evidence anchors**:
  - [section IV-B] "we share weights over subsets of property groundings defined as hypercubes"
  - [section IV-B] "we can show the following" referring to Theorem 1 about log-likelihood bounds
  - [corpus] Weak: No citations demonstrating hypercube weight sharing effectiveness in HMLN or neural verification contexts
- **Break condition**: If the hypercube partitioning is too coarse (underfitting) or too fine (overfitting), or if the embedding space topology doesn't align with hypercube structure, verification accuracy degrades.

## Foundational Learning

- **Markov Logic Networks (MLNs)**
  - Why needed here: HMLNs extend MLNs by incorporating continuous sub-symbolic atoms alongside symbolic atoms, enabling the framework to reason about neural embeddings within a probabilistic logical framework
  - Quick check question: How does a Hybrid Markov Logic Network differ from a standard Markov Logic Network in terms of the types of atoms it can represent?

- **Mixed Integer Linear Programming (MILP)**
  - Why needed here: MILP provides a scalable way to solve the MAP inference problem required for both weight learning and verification bounds computation in HMLNs with hybrid discrete-continuous variables
  - Quick check question: What are the two main types of variables in the MILP formulation for HMLN MAP inference, and what do they represent?

- **Rao-Blackwellization**
  - Why needed here: This technique reduces variance in parameter estimation by marginalizing out noisy sub-symbolic atom assignments before computing gradients, which is crucial when learning from imperfect neural embeddings
  - Quick check question: In the context of HMLN learning, what specific source of uncertainty does Rao-Blackwellization address?

## Architecture Onboarding

- **Component map**: Specification HMLN -> MILP Solver (Gurobi) -> Weight Learning Module -> Hypercube Partitioner -> Verification Engine

- **Critical path**:
  1. Learn specification HMLN from specification DNN embeddings using Rao-Blackwellized gradient descent with MILP-based MAP inference
  2. Partition embedding space into hypercubes for weight sharing
  3. For test DNN embeddings, compute MAP upper/lower bounds on property probabilities via MILP
  4. Sample groundings from each hypercube and estimate mean differences using Welch's T-Test
  5. Verify if differences are statistically significant (p ≤ 0.05)

- **Design tradeoffs**:
  - MILP vs. approximate inference: Exact MAP via MILP provides better quality estimates but is computationally expensive; approximate methods are faster but may introduce bias
  - Number of hypercubes: More hypercubes reduce approximation error in weight sharing (tighter Theorem 1 bound) but increase computational cost and risk of overfitting
  - Piecewise linear relaxation accuracy vs. MILP complexity: Finer piecewise approximations better capture non-linear soft functions but increase variable count and solve time

- **Failure signatures**:
  - Verification fails for semantically correct embeddings: Likely indicates insufficient hypercube partitioning or poor piecewise linear approximation of soft functions
  - Weight learning diverges or produces NaNs: May indicate ill-conditioned gradient estimates from MAP approximation or inappropriate learning rate
  - MILP solver reports infeasibility: Could result from conflicting constraints in the encoding of logical relationships or overly tight bounds on continuous variables

- **First 3 experiments**:
  1. **Sanity check on synthetic data**: Create a small HMLN with known weights and synthetic embeddings that perfectly satisfy the properties. Verify that the learned weights match the true weights and that verification passes.
  2. **Ablation on weight sharing**: Compare verification performance with different numbers of hypercubes on a simple GNN verification task to find the sweet spot between accuracy and computational cost.
  3. **Noise sensitivity analysis**: Add controlled Gaussian noise to specification embeddings and measure how verification performance degrades as noise increases, to establish the framework's robustness bounds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the threshold τ in the soft inequality function affect the verification results across different domains?
- Basis in paper: [explicit] The paper mentions τ as a threshold in the context of verifying semantic meaning in GNN embeddings, invariance in DKT, and transferability in ITS.
- Why unresolved: The paper does not provide an analysis of how varying τ impacts the verification outcomes or the robustness of the framework.
- What evidence would resolve it: Empirical results showing the effect of different τ values on verification accuracy and robustness across the three domains.

### Open Question 2
- Question: What is the impact of the number of hypercubes on the scalability and accuracy of the verification process?
- Basis in paper: [explicit] The paper discusses weight sharing across hypercubes and sets an upper limit on the number of hypercubes as 200.
- Why unresolved: The paper does not explore the trade-off between the number of hypercubes and the performance of the verification process.
- What evidence would resolve it: A study comparing verification results and computational efficiency with varying numbers of hypercubes.

### Open Question 3
- Question: How does the framework handle verification when the specification DNN and the test DNN are trained on different datasets or with different architectures?
- Basis in paper: [inferred] The paper implies that the framework can verify embeddings from different DNNs but does not address the scenario of differing datasets or architectures.
- Why unresolved: The paper focuses on verification within the same dataset and architecture, leaving open the question of cross-dataset or cross-architecture verification.
- What evidence would resolve it: Experimental results showing the framework's performance when verifying embeddings from DNNs trained on different datasets or architectures.

## Limitations

- The Rao-Blackwellization variance reduction is theoretical and lacks empirical validation
- The MILP formulation's scalability for large HMLNs is not demonstrated
- Hypercube weight sharing performance depends heavily on embedding space geometry which isn't characterized

## Confidence

- **High**: The HMLN framework can encode hybrid symbolic-subsymbolic verification problems
- **Medium**: MILP-based MAP inference provides bounds for verification (no comparison to alternatives)
- **Low**: Rao-Blackwellization meaningfully reduces variance in learned weights (theoretical only)

## Next Checks

1. Conduct ablation study comparing Rao-Blackwellized vs standard likelihood learning on synthetic data with known ground truth
2. Test verification performance as embedding dimensionality increases to establish scalability limits
3. Analyze the relationship between hypercube partitioning granularity and verification accuracy across different embedding space topologies