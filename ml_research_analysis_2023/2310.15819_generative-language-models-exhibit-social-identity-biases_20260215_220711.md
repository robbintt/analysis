---
ver: rpa2
title: Generative Language Models Exhibit Social Identity Biases
arxiv_id: '2310.15819'
source_url: https://arxiv.org/abs/2310.15819
tags:
- ingroup
- outgroup
- social
- language
- biases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models exhibit human-like ingroup solidarity and
  outgroup hostility biases similar to those found in human-written internet text.
  The authors found that most language models generate more positive sentences about
  their ingroup and more negative sentences about outgroups, with instruction-tuned
  models showing less bias than base models.
---

# Generative Language Models Exhibit Social Identity Biases

## Quick Facts
- arXiv ID: 2310.15819
- Source URL: https://arxiv.org/abs/2310.15819
- Authors: [List not provided in source]
- Reference count: 40
- Key outcome: Large language models exhibit human-like ingroup solidarity and outgroup hostility biases similar to those found in human-written internet text.

## Executive Summary
Large language models exhibit social identity biases comparable to human-written text, showing ingroup solidarity (more positive sentiment toward "we are") and outgroup hostility (more negative sentiment toward "they are"). The study analyzed 51 language models and found that instruction-tuned models show less bias than base models. Fine-tuning on partisan social media data amplified these biases, particularly outgroup hostility. Crucially, removing biased sentences from training data significantly reduced these biases, demonstrating that careful curation of training corpora is essential for mitigating social identity biases in language models.

## Method Summary
The researchers generated sentences using "We are" and "They are" prompts across 51 language models, including base and instruction-tuned versions. They filtered sentences for quality and diversity, then classified sentiment using RoBERTa and VADER. Logistic regression models predicted positive/negative sentiment based on ingroup/outgroup indicators while controlling for sentence length and type-to-token ratio. They also conducted fine-tuning experiments using partisan Twitter data and tested bias reduction by removing varying proportions of ingroup-positive and outgroup-negative sentences from training data.

## Key Results
- Base language models exhibit ingroup solidarity and outgroup hostility biases similar to human-written internet text
- Instruction-tuned models show significantly lower bias levels than base models
- Fine-tuning on partisan social media data increases both ingroup solidarity and outgroup hostility, with stronger effects on hostility
- Removing biased sentences from training data substantially reduces both forms of bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models inherit social identity biases from their training data.
- Mechanism: LLMs learn next-token probabilities from human-generated text; if training corpora contain ingroup-positive and outgroup-negative patterns, the models will reproduce these associations when prompted with "We are" or "They are".
- Core assumption: The distributional statistics of human-written sentences directly transfer to model-generated text without systematic distortion.
- Evidence anchors:
  - [abstract]: "Our findings suggest that modern language models exhibit fundamental social identity biases to a similar degree as humans"
  - [section]: "a comparison of LLM-generated sentences with human-written sentences on the internet reveals that these models exhibit similar level, if not greater, levels of bias than human text"
  - [corpus]: Weak - corpus evidence only shows related papers, not bias distribution in training data.
- Break condition: If pre-training data is aggressively filtered to remove ingroup-positive/outgroup-negative patterns, bias reduction should occur.

### Mechanism 2
- Claim: Instruction fine-tuning reduces social identity bias.
- Mechanism: Human feedback during instruction fine-tuning steers the model toward neutral or aligned responses, lowering the odds ratio of positive sentiment for ingroup vs. outgroup sentences.
- Core assumption: Human annotators consistently mark biased outputs as undesirable, creating a preference signal that overrides learned statistical patterns.
- Evidence anchors:
  - [abstract]: "consumer-facing large-language models...tend to exhibit lower degrees of ingroup-solidarity and outgroup hostility than base LLMs"
  - [section]: "Our results also show that alignment techniques such as instruction fine-tuning...are effective in reducing social identity bias"
  - [corpus]: Weak - no direct corpus evidence of fine-tuning data content.
- Break condition: If fine-tuning corpus contains strong ingroup-positive or outgroup-negative examples, bias may persist or even increase.

### Mechanism 3
- Claim: Fine-tuning on partisan social media data amplifies bias asymmetrically.
- Mechanism: Exposure to highly polarized language shifts the model's probability distributions, increasing both ingroup solidarity and outgroup hostility, with a stronger effect on the latter due to the prevalence of hostile outgroup language in such corpora.
- Evidence anchors:
  - [abstract]: "fine-tuning models on partisan social media data...resulted in the models exhibiting a marked increase in ingroup solidarity and an even greater increase in outgroup hostility"
  - [section]: "Language models, on average, become roughly five times more hostile toward a general (non-specific) outgroup after fine-tuning with US partisan social media data"
  - [corpus]: Weak - related papers discuss bias but not specific partisan corpus statistics.
- Break condition: If fine-tuning corpus is balanced or filtered to remove extreme partisan language, bias amplification should diminish.

## Foundational Learning

- Concept: Social identity theory and affective polarization
  - Why needed here: Provides theoretical framework for interpreting ingroup/outgroup sentiment differences in LLM outputs.
  - Quick check question: Can you explain the difference between ingroup solidarity and outgroup hostility in one sentence?

- Concept: Logistic regression with random effects
  - Why needed here: Used to quantify bias by modeling sentiment likelihood as a function of sentence type while controlling for data quality.
  - Quick check question: What does the odds ratio represent in the context of ingroup vs. outgroup sentiment classification?

- Concept: Fine-tuning vs. instruction fine-tuning
  - Why needed here: Distinguishes between domain adaptation (fine-tuning) and human-aligned behavior learning (instruction fine-tuning), both of which affect bias.
  - Quick check question: Which type of fine-tuning explicitly incorporates human preference labels?

## Architecture Onboarding

- Component map: Data pipeline -> Model loading -> Prompt generation -> Text generation -> Sentiment classification -> Statistical analysis -> Visualization
- Critical path: Prompt generation -> Text generation -> Sentiment classification -> Logistic regression -> Bias metric calculation
- Design tradeoffs: Larger models yield stronger bias signals but cost more to run; simpler prompts are faster but may miss nuanced bias patterns
- Failure signatures: Consistently neutral outputs (bias near zero), degenerate or repetitive generations (low type-to-token ratio), or crashes during model loading
- First 3 experiments:
  1. Run default prompt "We are" and "They are" on a small base model, verify sentiment classifier outputs and TTR thresholds
  2. Compare ingroup/outgroup sentiment distributions for one base model vs. its instruction-tuned counterpart
  3. Fine-tune a base model on a balanced corpus and measure bias reduction relative to the original

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent do real-world user interactions with LLMs reinforce or mitigate social identity biases compared to controlled experiments?
- Basis in paper: [explicit] The authors note that their study may not accurately replicate real-world LLM use and suggest that definitive measures require corporations' exclusive access to user interaction data.
- Why unresolved: Controlled experiments use artificial prompts, while actual user interactions involve diverse contexts and goals that could amplify or reduce biases differently.
- What evidence would resolve it: Large-scale anonymized logs of real LLM user queries showing sentiment patterns and bias reinforcement across different user demographics.

### Open Question 2
- Question: How does the magnitude of social identity bias in LLMs scale with model size, and does this relationship differ between base models and instruction-tuned models?
- Basis in paper: [explicit] The authors found a very small increase in outgroup hostility with model size but did not observe consistent patterns across all model families or types.
- Why unresolved: The relationship between model size and bias magnitude is complex and may depend on training data composition, instruction tuning, and task-specific fine-tuning.
- What evidence would resolve it: Systematic evaluation of social identity biases across multiple model sizes within each family, controlling for training data and fine-tuning methods.

### Open Question 3
- Question: Can targeted removal of specific types of biased training data (e.g., only outgroup-negative content) effectively reduce social identity biases without compromising model performance on other tasks?
- Basis in paper: [explicit] The authors found that removing either ingroup-positive or outgroup-negative sentences reduced both ingroup solidarity and outgroup hostility, but the trade-offs with model performance remain unclear.
- Why unresolved: The relationship between bias reduction and task performance is not well understood, and removing biased data might affect the model's ability to learn other useful patterns.
- What evidence would resolve it: Comparative evaluation of models fine-tuned on debiased data versus original data across a range of natural language understanding and generation tasks.

## Limitations

- The study uses artificial "We are"/"They are" prompts that may not generalize to real-world LLM usage patterns
- Sentiment classifiers may not fully capture nuanced social meanings in model-generated text
- The relationship between model size and bias magnitude is not consistently observed across all model families

## Confidence

**High confidence** in the core finding that base language models exhibit measurable ingroup solidarity and outgroup hostility biases similar to human-written text.

**Medium confidence** in the claim that instruction fine-tuning systematically reduces bias, though potential confounding factors exist.

**Medium confidence** in the asymmetric amplification of outgroup hostility during partisan fine-tuning, based on limited fine-tuning experiments.

## Next Checks

1. **Prompt generalization test**: Run the same bias analysis pipeline using varied prompt templates (e.g., "As members of this group, we..." or "People in this community...") to determine whether the observed biases are robust across different linguistic framings of social identity.

2. **Classifier validation study**: Manually annotate a stratified sample of model-generated sentences to validate the RoBERTa and VADER sentiment classifications, particularly focusing on cases where sentiment and social identity content may be misaligned.

3. **Long-term bias persistence experiment**: After fine-tuning on cleaned training data, continue pre-training the models on standard web corpora for multiple epochs, then retest for social identity biases to assess whether initial bias reduction is durable.