---
ver: rpa2
title: 'MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual
  Contexts'
arxiv_id: '2310.02255'
source_url: https://arxiv.org/abs/2310.02255
tags:
- reasoning
- visual
- question
- figure
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MATHVISTA, a benchmark designed to evaluate
  mathematical reasoning in visual contexts using foundation models. It comprises
  6,141 examples from 31 datasets, including 3 newly created ones.
---

# MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts

## Quick Facts
- arXiv ID: 2310.02255
- Source URL: https://arxiv.org/abs/2310.02255
- Reference count: 40
- Key outcome: MathVista benchmark reveals current multimodal models underperform humans (34.8% vs 60.3% accuracy) on visual mathematical reasoning tasks.

## Executive Summary
MathVista is a comprehensive benchmark designed to evaluate mathematical reasoning in visual contexts using foundation models. It includes 6,141 examples from 31 datasets, covering 7 mathematical reasoning types and 19 visual contexts. Evaluation of 11 foundation models shows that even the best-performing models achieve only 58% of human performance, highlighting significant gaps in both visual perception and mathematical reasoning capabilities. The benchmark provides fine-grained analysis of model failures, identifying hallucinations and calculation errors as primary issues.

## Method Summary
The MathVista benchmark was constructed by curating 6,141 examples from existing datasets and newly created ones, annotated with metadata including grade level, reasoning type, and visual context. Evaluation was performed using three setups: text-only LLMs, augmented LLMs with image captions and OCR, and LMMs. Models were tested using zero-shot and few-shot prompting with chain-of-thought and program-of-thought strategies. Answers were extracted using GPT-4 and scored based on correctness. The pipeline included response generation, answer extraction, and automated scoring.

## Key Results
- Multimodal Bard achieved 34.8% accuracy, 58% of human performance (60.3%).
- GPT-4V and other LMMs struggled with complex visual contexts and compositional reasoning.
- Augmented LLMs with captions and OCR performed better than text-only LLMs but still lagged behind human performance.
- 49.6% of responses contained hallucinations, primarily due to incorrect visual perception and textual reasoning.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MathVista benchmark isolates the gap between current multimodal models and human-level performance in visual mathematical reasoning.
- Mechanism: By curating 6,141 examples from 31 datasets, including 3 new ones, MathVista ensures coverage of diverse mathematical reasoning types (algebraic, arithmetic, geometry, etc.) and visual contexts (charts, diagrams, plots). This diversity forces models to handle both fine-grained visual perception and compositional reasoning.
- Core assumption: Current foundation models, even with visual augmentation, lack the integrated capability to interpret complex figures and perform rigorous reasoning in tandem.
- Evidence anchors:
  - [abstract]: "GPT-4V also struggles, highlighting the need for improved visual perception and mathematical reasoning in AI agents."
  - [section]: "Multimodal Bard achieves only 58% of human performance (34.8% vs 60.3%), indicating substantial room for improvement."
  - [corpus]: Corpus signals show related benchmarks like MathScape and Math-LLaVA also aim to fill this gap, but MathVista is unique in combining both math reasoning and visual contexts comprehensively.
- Break condition: If a model achieves near-human performance across all visual contexts and reasoning types, the benchmark's purpose would be fulfilled.

### Mechanism 2
- Claim: Augmented LLMs (e.g., GPT-4 with image captions and OCR) can approach LMM performance on MathVista.
- Mechanism: External visual tools (Bard captions, EasyOCR) compensate for the inherent visual reasoning limitations of LLMs by providing structured textual descriptions of images. This allows LLMs to leverage their strong language reasoning capabilities on interpreted visual data.
- Core assumption: The bottleneck for LLMs is not reasoning per se, but rather the inability to directly process visual inputs; thus, high-quality captions and OCR text can bridge this gap.
- Evidence anchors:
  - [abstract]: "With access to image captions and detected OCR text, we find that augmented LLMs perform better than text-only LLMs on MATHVISTA."
  - [section]: "2-shot GPT-4 with program-of-thought prompting with 33.9%."
  - [corpus]: Corpus neighbors include ConTextual and Math-LLaVA, which also explore augmenting models with external visual understanding tools.
- Break condition: If visual captions/OCR are inaccurate or hallucinated, the augmented LLMs' performance degrades, revealing dependency on external tool quality.

### Mechanism 3
- Claim: MathVista exposes specific failure modes in current models, guiding future research directions.
- Mechanism: Through qualitative analysis of model predictions, MathVista identifies two primary failure sources: incorrect calculations and hallucinations in visual perception and textual reasoning. This granular failure diagnosis informs targeted model improvements.
- Core assumption: By categorizing errors explicitly, researchers can prioritize fixes—e.g., hallucination mitigation vs. calculation accuracy.
- Evidence anchors:
  - [abstract]: "Further analysis indicates that model failures arise from incorrect calculations and hallucinations caused by visual perception and textual reasoning."
  - [section]: "49.6% of its responses contain hallucinations."
  - [corpus]: Corpus signals indicate ongoing research on hallucination in natural language generation (Ji et al., 2023) aligns with these findings.
- Break condition: If models eliminate hallucinations and calculation errors, MathVista would no longer reveal such clear failure signatures.

## Foundational Learning

- Concept: Visual context taxonomy (geometry diagrams, charts, plots, etc.)
  - Why needed here: Understanding the diversity of visual inputs is essential to grasp why MathVista poses challenges; different contexts require distinct perception skills.
  - Quick check question: Which visual context type requires the most complex perception—geometry diagrams or bar charts—and why?

- Concept: Mathematical reasoning types taxonomy
  - Why needed here: Recognizing the seven reasoning categories (algebraic, arithmetic, etc.) clarifies why MathVista evaluates both reasoning and perception, not just one or the other.
  - Quick check question: How does logical reasoning differ from arithmetic reasoning in the context of visual problems?

- Concept: Chain-of-thought and program-of-thought prompting
  - Why needed here: These prompting strategies are key to how LLMs are tested on MathVista; understanding them explains model performance differences.
  - Quick check question: What is the main difference between chain-of-thought and program-of-thought prompting, and how might that affect math problem solving?

## Architecture Onboarding

- Component map: Data collection (existing + new datasets) -> Metadata annotation (grade level, reasoning type, visual context) -> Benchmark construction -> Evaluation protocols (response generation, answer extraction, scoring) -> Model testing (text-only LLMs, augmented LLMs, LMMs)
- Critical path: Data curation → metadata annotation → benchmark construction → model evaluation → qualitative analysis
- Design tradeoffs: Including diverse visual contexts increases benchmark difficulty but may introduce noise; limiting dataset size per source ensures balance but reduces total examples per domain.
- Failure signatures: Incorrect calculations, hallucinations in visual captions/OCR, inability to resolve linguistic ambiguities using visual cues.
- First 3 experiments:
  1. Evaluate a baseline LLM (e.g., ChatGPT) on a subset of MathVista without visual augmentation to establish lower bound performance.
  2. Augment the same LLM with generated captions and OCR, then re-evaluate to measure impact of visual augmentation.
  3. Test an open-source LMM (e.g., LLaVA) on the full MathVista test set to compare zero-shot multimodal reasoning capability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the accuracy of visual perception in large multimodal models be significantly improved through enhanced training on math-specific visual contexts?
- Basis in paper: [inferred] The paper notes that models like GPT-4V and Multimodal Bard struggle with understanding complex figures and performing rigorous reasoning, suggesting limitations in current visual perception capabilities.
- Why unresolved: The paper highlights these challenges but does not explore potential training strategies or architectural improvements to address them.
- What evidence would resolve it: Comparative experiments showing improved accuracy on MATHVISTA after training models on a larger, more diverse dataset of math-specific visual contexts.

### Open Question 2
- Question: How does the quality of image captions and OCR text impact the mathematical reasoning performance of augmented large language models?
- Basis in paper: [explicit] The paper mentions that the gap in performance of augmented LLMs can be attributed to poor image captions and OCR text that do not adequately describe the math in visual contexts.
- Why unresolved: While the paper acknowledges this issue, it does not provide a detailed analysis of how caption and OCR quality specifically affects reasoning accuracy.
- What evidence would resolve it: A controlled study varying the quality of image captions and OCR text while measuring changes in reasoning accuracy on MATHVISTA tasks.

### Open Question 3
- Question: What are the most effective prompting strategies for eliciting mathematical reasoning in visual contexts from large language models?
- Basis in paper: [explicit] The paper compares zero-shot, few-shot, chain-of-thought, and program-of-thought prompting strategies but finds that even the best-performing model (Multimodal Bard) achieves only 58% of human performance.
- Why unresolved: The paper does not explore alternative prompting techniques or combinations that might better elicit reasoning capabilities.
- What evidence would resolve it: Experiments testing novel prompting approaches, such as step-by-step reasoning prompts or prompts that explicitly guide attention to relevant visual elements, and measuring their impact on accuracy.

## Limitations
- Benchmark evaluation relies heavily on GPT-4 for answer extraction and scoring, potentially introducing bias and circularity.
- Visual contexts, while diverse, may not fully capture real-world complexity, particularly for multi-step geometric reasoning.
- Comparison between augmented LLMs and LMMs assumes external tools can fully substitute for native visual processing.

## Confidence
- High confidence: Current multimodal models significantly underperform humans on visual mathematical reasoning (34.8% vs 60.3% accuracy).
- Medium confidence: Augmented LLMs can approach LMM performance, but this depends critically on external tool quality.
- Low confidence: MathVista uniquely exposes failure modes guiding future research, as similar patterns exist in other benchmarks.

## Next Checks
1. Conduct human evaluation of a random sample of model predictions to independently verify accuracy scores and identify scoring biases.
2. Expand visual context diversity by including more complex real-world examples, particularly multi-step geometric reasoning, and re-evaluate model performance.
3. Perform ablation studies on visual augmentation tool quality by systematically varying caption/OCR accuracy and measuring impact on augmented LLM performance.