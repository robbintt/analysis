---
ver: rpa2
title: 'LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigms'
arxiv_id: '2311.13133'
source_url: https://arxiv.org/abs/2311.13133
tags:
- lima
- finetuned
- dataset
- datasets
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores whether small, high-quality instruction datasets
  can effectively fine-tune large language models (LLMs) for both traditional NLP
  benchmarks and model-based evaluation. The authors fine-tune open-source MPT-7B
  and MPT-30B models on datasets of varying sizes (1k-60k samples) from Instruct-v1,
  Instruct-v3, and the LIMA dataset.
---

# LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigms

## Quick Facts
- **arXiv ID:** 2311.13133
- **Source URL:** https://arxiv.org/abs/2311.13133
- **Reference count:** 40
- **Primary result:** 1k-6k instruction samples sufficient for effective fine-tuning across evaluation paradigms

## Executive Summary
This paper investigates whether small, high-quality instruction datasets can effectively fine-tune large language models (LLMs) for both traditional NLP benchmarks and model-based evaluation. The authors fine-tune MPT-7B and MPT-30B models on datasets ranging from 1k to 60k samples from Instruct-v1, Instruct-v3, and the LIMA dataset. They demonstrate that subsets of 1k-6k instruction samples achieve good performance on both traditional NLP benchmarks and model-based evaluation, with optimal results when combining textbook-style and open-ended QA finetuning datasets.

## Method Summary
The authors fine-tune MPT-7B and MPT-30B base models on subsets of Instruct datasets (1k-60k samples) and LIMA dataset (1k samples) using next word prediction loss. They evaluate the fine-tuned models on two paradigms: MosaicML's Eval Gauntlet (34 benchmarks across 5 categories) for traditional NLP performance, and AlpacaEval's LIMA test set using GPT-4 as judge for model-based evaluation of open-ended generation.

## Key Results
- 1k-6k instruction finetuning samples are sufficient to achieve good performance on both traditional NLP benchmarks and model-based evaluation
- Combining textbook-style and open-ended QA finetuning datasets optimizes performance across both evaluation paradigms
- Finetuning on 1k-6k Instruct samples combined with LIMA dataset leads to improved performance on both evaluation paradigms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A small number of diverse, high-quality finetuning samples can achieve effective style alignment for LLMs
- Mechanism: Finetuning on diverse task types (both textbook-style and open-ended QA) helps models generalize across evaluation paradigms
- Core assumption: Dataset diversity, not size, is the key driver of downstream performance
- Evidence anchors: MPT-7B finetuned on 5k samples from Instruct-v1 achieved same performance as full Instruct finetuning

### Mechanism 2
- Claim: Model-based evaluation using GPT-4 as a judge correlates well with human preferences for open-ended generation tasks
- Mechanism: GPT-4 evaluates style and coherence, which aligns with human judgment for open-ended tasks
- Core assumption: GPT-4 preferences reflect human preferences for style and tone in responses
- Evidence anchors: MPT-30B finetuned on LIMA samples outperformed models finetuned on full Instruct-v3 when judged by GPT-4

### Mechanism 3
- Claim: Combining textbook-style and open-ended QA finetuning datasets optimizes performance on both evaluation paradigms
- Mechanism: The combined dataset provides coverage of both traditional NLP benchmarks and open-ended generation tasks
- Core assumption: A balanced dataset that covers both task types can achieve good performance on both evaluation paradigms
- Evidence anchors: Combining 1-5k samples from Instruct dataset with LIMA samples resulted in good performance on both paradigms

## Foundational Learning

- **Concept: Instruction finetuning**
  - Why needed here: Understanding how finetuning on instruction-response pairs can enable LLMs to perform general question-answering tasks
  - Quick check question: What is the difference between traditional finetuning and instruction finetuning?

- **Concept: Model-based evaluation**
  - Why needed here: Understanding how using another LLM (like GPT-4) as a judge can evaluate open-ended generation tasks
  - Quick check question: How does model-based evaluation differ from traditional NLP benchmark evaluation?

- **Concept: Dataset composition and diversity**
  - Why needed here: Understanding how the types of tasks included in a finetuning dataset can affect downstream performance on different evaluation paradigms
  - Quick check question: Why might a dataset with only textbook-style questions not perform well on open-ended generation tasks?

## Architecture Onboarding

- **Component map:** MPT-7B/30B models -> Instruct-v1/v3 subsets (1k-60k samples) and LIMA dataset (1k samples) -> Fine-tuning process -> Evaluation on Eval Gauntlet and AlpacaEval with GPT-4 judge
- **Critical path:** Fine-tuning base models on instruction datasets and evaluating their performance on both traditional NLP benchmarks and model-based evaluation
- **Design tradeoffs:** Tradeoff between dataset size and diversity - larger datasets may provide more coverage but introduce noise, while smaller, more diverse datasets may be more effective for style alignment
- **Failure signatures:** Poor performance on one evaluation paradigm if finetuning dataset is too homogeneous; unreliable results if evaluation paradigm doesn't align with human preferences
- **First 3 experiments:**
  1. Finetune MPT-7B on LIMA and evaluate on both traditional NLP benchmarks and model-based evaluation
  2. Finetune MPT-7B on a random subset of Instruct-v1 and evaluate on both traditional NLP benchmarks and model-based evaluation
  3. Finetune MPT-7B on a combination of Instruct-v1 subset and LIMA and evaluate on both traditional NLP benchmarks and model-based evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does finetuning on larger datasets than the ones tested (1k-6k samples) continue to improve performance on both traditional NLP benchmarks and model-based evaluation?
- Basis in paper: The paper tests finetuning datasets ranging from 1k to 60k samples, but does not explore the performance beyond 6k samples
- Why unresolved: The study focuses on demonstrating that small datasets are sufficient, so it does not investigate whether even larger datasets provide additional benefits
- What evidence would resolve it: Finetuning models on datasets larger than 6k samples and evaluating their performance on both traditional NLP benchmarks and model-based evaluation

### Open Question 2
- Question: How do the evaluation paradigms (traditional NLP benchmarks vs. model-based evaluation) correlate with human judgment of instruction-following model quality?
- Basis in paper: The paper uses GPT-4 as a judge for model-based evaluation, but does not compare the results with human judgments
- Why unresolved: While GPT-4 is widely used, its preferences may not always align with human preferences, and the study does not validate the correlation between the two evaluation paradigms and human judgment
- What evidence would resolve it: Conducting human evaluations on the same instruction-following tasks and comparing the results with those obtained from traditional NLP benchmarks and model-based evaluation

### Open Question 3
- Question: How does the composition of the finetuning dataset (e.g., mix of textbook-style and open-ended QA) impact the model's performance on specific downstream tasks?
- Basis in paper: The paper finds that combining textbook-style and open-ended QA finetuning datasets optimizes performance on both evaluation paradigms
- Why unresolved: The study does not investigate the effect of different dataset compositions on the model's performance on specific downstream tasks beyond the evaluation paradigms used
- What evidence would resolve it: Finetuning models on datasets with varying compositions and evaluating their performance on a wide range of downstream tasks to determine the impact of dataset composition on task-specific performance

## Limitations
- The minimal dataset size findings may not generalize to other instruction datasets with different quality profiles
- GPT-4 evaluation correlation with human preferences lacks comprehensive human validation across multiple domains
- The optimal balance between textbook-style and open-ended QA tasks is not systematically explored across different dataset ratios

## Confidence

- **High Confidence:** LIMA-style finetuning improves performance on open-ended generation tasks
- **Medium Confidence:** 1k-6k instruction samples are sufficient for good performance across both evaluation paradigms
- **Low Confidence:** The optimal balance between textbook-style and open-ended QA tasks is definitively established

## Next Checks
1. Conduct human evaluation studies to validate GPT-4's correlation with human preferences across diverse domains
2. Test the minimal dataset size hypothesis (1k-6k samples) across multiple instruction dataset types with varying quality profiles
3. Systematically explore different mixing ratios of textbook-style and open-ended QA tasks to identify the optimal balance for achieving consistent performance across evaluation paradigms