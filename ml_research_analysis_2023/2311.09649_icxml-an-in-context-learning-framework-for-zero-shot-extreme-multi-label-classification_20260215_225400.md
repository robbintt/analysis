---
ver: rpa2
title: 'ICXML: An In-Context Learning Framework for Zero-Shot Extreme Multi-Label
  Classification'
arxiv_id: '2311.09649'
source_url: https://arxiv.org/abs/2311.09649
tags:
- label
- labels
- input
- arxiv
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of zero-shot extreme multi-label
  classification (XMC), where the goal is to predict multiple relevant labels for
  each instance from an extremely large label space, without any supervision signals.
  The proposed method, ICXML, is a two-stage framework that addresses this issue by
  first generating a set of candidate labels through in-context learning and then
  reranking them.
---

# ICXML: An In-Context Learning Framework for Zero-Shot Extreme Multi-Label Classification

## Quick Facts
- **arXiv ID**: 2311.09649
- **Source URL**: https://arxiv.org/abs/2311.09649
- **Reference count**: 12
- **Primary result**: Advances state-of-the-art on two diverse public benchmarks, LF-Amazon-131K and LF-WikiSeeAlso-320K, achieving significant improvements in precision and recall metrics compared to existing baselines.

## Executive Summary
This paper tackles the challenge of zero-shot extreme multi-label classification (XMC), where the goal is to predict multiple relevant labels for each instance from an extremely large label space, without any supervision signals. The proposed method, ICXML, is a two-stage framework that addresses this issue by first generating a set of candidate labels through in-context learning and then reranking them. ICXML advances the state of the art on two diverse public benchmarks, LF-Amazon-131K and LF-WikiSeeAlso-320K, achieving significant improvements in precision and recall metrics compared to existing baselines. The framework leverages the capabilities of large language models to generate high-quality input-label pairs and perform multi-label classification, demonstrating the effectiveness of in-context learning for extreme classification tasks.

## Method Summary
ICXML is a two-stage framework that addresses zero-shot extreme multi-label classification by leveraging in-context learning. The first stage involves generating a set of candidate labels through in-context learning using pseudo demonstrations. The second stage re-ranks these candidates to produce the final predictions. The framework uses a large language model (GPT-3.5 or GPT-4) and a zero-shot retriever (TAS-B) to perform semantic matching and label identification. ICXML's demonstration generation strategies (content-based and label-centric) effectively capture the correlation between input text and labels, while the label space mapping and reranking steps transform raw language model outputs into structured labels and improve overall performance.

## Key Results
- ICXML significantly improves precision and recall metrics on LF-Amazon-131K and LF-WikiSeeAlso-320K datasets compared to existing baselines.
- The framework demonstrates the effectiveness of in-context learning for extreme multi-label classification tasks.
- ICXML's performance is influenced by the quality of the language model used, with GPT-4 showing better results than GPT-3.5, especially on larger label spaces.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICXML reduces extreme multi-label classification to a manageable candidate generation and reranking problem by leveraging in-context learning.
- Mechanism: The framework first generates a condensed set of candidate labels through in-context learning using pseudo demonstrations, then re-ranks these candidates to produce the final predictions. This two-stage approach addresses the challenge of an extremely large label space by focusing computational resources on a smaller, more relevant subset of labels.
- Core assumption: The language model can effectively generate high-quality pseudo demonstrations that capture the relationship between input instances and relevant labels, even without access to the actual label mappings.
- Evidence anchors:
  - [abstract] "We address this issue by introducing In-Context Extreme Multi-label Learning (ICXML), a two-stage framework that cuts down the search space by generating a set of candidate labels through in-context learning and then re-ranks them."
  - [section 4.1] Describes the process of generating pseudo demonstrations using the language model.
  - [corpus] Weak evidence. The corpus neighbors include papers on extreme multi-label classification, but there's no direct evidence of similar two-stage in-context learning approaches.
- Break condition: If the language model cannot generate meaningful pseudo demonstrations or if the label space mapping step fails to identify relevant candidates, the framework's effectiveness will be significantly reduced.

### Mechanism 2
- Claim: ICXML's demonstration generation strategies (content-based and label-centric) effectively capture the correlation between input text and labels.
- Mechanism: The content-based approach generates diverse demonstration inputs based on the test input content, then links them to highly correlated labels. The label-centric approach maps the test input to the label space, identifies highly correlated labels, and generates demonstration inputs based on these labels. Both strategies aim to create demonstrations that embody the inherent correlation between input text and labels.
- Core assumption: The language model can generate relevant and diverse demonstration inputs, and the zero-shot retriever can accurately identify labels with high correlation to these inputs.
- Evidence anchors:
  - [section 4.1] Details the content-based and label-centric demonstration generation strategies.
  - [section 6.4] Shows that ICXML outperforms baselines on LF-Amazon-131K dataset, suggesting effective demonstration generation.
  - [corpus] Weak evidence. The corpus includes papers on extreme multi-label classification, but there's no direct evidence of similar demonstration generation strategies.
- Break condition: If the demonstration generation strategies fail to capture the correlation between input text and labels, the in-context learning performance will degrade.

### Mechanism 3
- Claim: ICXML's label space mapping and reranking steps effectively transform raw language model outputs into structured labels and improve overall performance.
- Mechanism: After the language model generates a set of labels, the framework uses a zero-shot retriever to find the most semantically similar labels from the actual label space. This mapping step transforms raw outputs into structured labels. The reranking step then selects the most suitable set of labels from the shortlisted candidates, leveraging the language model's ability to handle multiple labels concurrently.
- Core assumption: The zero-shot retriever can accurately identify semantically similar labels, and the language model can effectively rerank the shortlisted candidates to produce the final predictions.
- Evidence anchors:
  - [section 4.2] Describes the label space mapping process.
  - [section 4.3] Explains the label reranking step.
  - [section 6.5] Shows that the reranking step improves performance compared to heuristic methods, validating the effectiveness of this mechanism.
  - [corpus] Weak evidence. The corpus includes papers on extreme multi-label classification, but there's no direct evidence of similar label space mapping and reranking approaches.
- Break condition: If the label space mapping step fails to identify relevant labels or if the reranking step does not improve the quality of the final predictions, the overall performance of ICXML will be negatively impacted.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: ICXML relies on in-context learning to generate pseudo demonstrations and perform label reranking without access to labeled training data.
  - Quick check question: What is the difference between in-context learning and traditional fine-tuning in terms of how the model learns from examples?

- Concept: Semantic similarity and retrieval
  - Why needed here: ICXML uses a zero-shot retriever to identify labels that are semantically similar to the generated labels, enabling effective label space mapping.
  - Quick check question: How does a zero-shot retriever differ from a traditional information retrieval system in terms of the types of queries it can handle?

- Concept: Extreme multi-label classification
  - Why needed here: ICXML is designed specifically for extreme multi-label classification tasks, where the label space is extremely large (e.g., 10^6 magnitude).
  - Quick check question: What are the key challenges in extreme multi-label classification compared to traditional multi-label classification?

## Architecture Onboarding

- Component map:
  - Language model (Φ) for in-context learning
  - Zero-shot retriever (θ) for semantic matching and label identification
  - Demonstration generation strategies (content-based and label-centric)
  - Label space mapping and reranking components

- Critical path:
  1. Generate pseudo demonstrations using the language model and demonstration generation strategies
  2. Perform in-context learning inference to generate a set of candidate labels
  3. Map the generated labels to the actual label space using the zero-shot retriever
  4. Rerank the shortlisted candidates using the language model to produce the final predictions

- Design tradeoffs:
  - Using a large language model for in-context learning provides flexibility but may be computationally expensive.
  - The two-stage approach (candidate generation + reranking) reduces the search space but adds complexity to the framework.
  - Relying on a zero-shot retriever for label space mapping avoids the need for labeled training data but may not capture all relevant labels.

- Failure signatures:
  - Poor performance on the LF-WikiSeeAlso-320K dataset with GPT-3.5, indicating potential issues with the label space mapping step for larger label spaces.
  - Lower precision@1 compared to recall@10, suggesting the framework may struggle to identify the most relevant labels.

- First 3 experiments:
  1. Evaluate the performance of ICXML on a small subset of the test data using GPT-3.5 to identify potential issues with the framework.
  2. Compare the performance of the content-based and label-centric demonstration generation strategies to determine which is more effective for different datasets.
  3. Analyze the impact of the label space mapping and reranking steps on the overall performance of ICXML to identify potential areas for improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ICXML perform on datasets with significantly larger label spaces, such as 1 million labels, compared to the 131K and 320K label spaces tested in the paper?
- Basis in paper: [explicit] The paper mentions that the underperformance on the LF-WikiSeeAlso-320K dataset might be due to the large label space, which is significantly larger than 131K. It also suggests that employing GPT-4 instead of GPT-3.5 for the reranking component could mitigate this underperformance.
- Why unresolved: The paper does not provide experimental results on datasets with label spaces of 1 million or more. The potential performance improvement with GPT-4 is only suggested, not demonstrated.
- What evidence would resolve it: Conducting experiments on datasets with 1 million or more labels and comparing the performance of ICXML using both GPT-3.5 and GPT-4 for the reranking component would provide concrete evidence of how the model scales with larger label spaces.

### Open Question 2
- Question: How does the performance of ICXML vary across different domains, such as image tagging or recommendation systems, compared to its performance on text classification tasks?
- Basis in paper: [explicit] The paper mentions that XMC finds applications in various domains, including text categorization, recommendation systems, and image tagging. However, the experiments are only conducted on text classification datasets.
- Why unresolved: The paper does not explore the performance of ICXML on non-text data or in domains other than text classification. The effectiveness of the framework in these areas remains untested.
- What evidence would resolve it: Evaluating ICXML on image tagging and recommendation system datasets, and comparing its performance to existing methods in these domains, would provide insights into its versatility and effectiveness across different types of data.

### Open Question 3
- Question: What is the impact of using different types of retrievers (e.g., dense retrievers) instead of the TAS-B model for the semantic matching component in ICXML?
- Basis in paper: [explicit] The paper uses the TAS-B model as the semantic matcher for its strong generalization capabilities. However, it does not explore the performance impact of using alternative retrievers.
- Why unresolved: The choice of TAS-B is justified by its performance, but the paper does not provide a comparative analysis with other retrievers, such as dense retrievers, which might offer different trade-offs in terms of accuracy and computational efficiency.
- What evidence would resolve it: Conducting experiments with different types of retrievers, including dense retrievers, and comparing their impact on ICXML's performance would clarify the role of the semantic matching component and potentially identify more efficient or accurate alternatives.

## Limitations

- The framework's performance on larger label spaces (e.g., LF-WikiSeeAlso-320K) is limited, particularly when using GPT-3.5 for reranking, suggesting potential scalability issues.
- The paper does not thoroughly investigate the robustness of the framework to noisy or ambiguous input text, which could affect demonstration quality and subsequent predictions.
- The evaluation is limited to two text classification datasets, raising questions about the framework's generalizability to other domains or types of data.

## Confidence

- **High confidence**: The two-stage framework design (generation + reranking) and its general approach to reducing the search space are well-supported by the experimental results and ablation studies.
- **Medium confidence**: The effectiveness of specific demonstration generation strategies (content-based vs. label-centric) is supported but could benefit from more extensive testing across diverse datasets.
- **Medium confidence**: The claim that ICXML advances the state of the art is supported by comparisons to baselines, but the evaluation is limited to two datasets and may not generalize to all XMC scenarios.

## Next Checks

1. **Label Space Scalability**: Test ICXML on a dataset with an even larger label space (e.g., 1M+ labels) to assess the framework's scalability and identify potential bottlenecks in the label space mapping step.
2. **Robustness Analysis**: Evaluate ICXML's performance on noisy or ambiguous input text to understand how well the framework handles real-world data imperfections and whether demonstration quality degrades.
3. **Cross-dataset Generalization**: Apply ICXML to a third, diverse XMC dataset (e.g., from a different domain like scientific literature or social media) to validate the framework's generalizability beyond the two evaluated datasets.