---
ver: rpa2
title: 'AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion
  Prediction'
arxiv_id: '2305.09620'
source_url: https://arxiv.org/abs/2305.09620
tags:
- survey
- questions
- data
- missing
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a new methodological framework that fine-tunes
  large language models with repeated cross-sectional surveys to predict missing survey
  responses. The method incorporates neural embeddings of survey questions, individual
  beliefs, and temporal contexts to personalize LLMs for opinion prediction.
---

# AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction

## Quick Facts
- **arXiv ID:** 2305.09620
- **Source URL:** https://arxiv.org/abs/2305.09620
- **Reference count:** 40
- **Key outcome:** Fine-tuned LLMs predict missing survey responses with AUC=0.87 (imputation), AUC=0.86 (retrodiction), and ρ=0.99 (imputation correlation)

## Executive Summary
This paper introduces a methodological framework that fine-tunes large language models (LLMs) with repeated cross-sectional surveys to predict missing survey responses. The approach incorporates neural embeddings of survey questions, individual beliefs, and temporal contexts to personalize LLMs for opinion prediction. The authors evaluate their method on 3,110 binarized opinions from 68,846 Americans in the General Social Survey (1972-2021), demonstrating strong performance in retrodiction and missing data imputation but more modest results for zero-shot prediction. The framework enables researchers to fill in missing survey trends with high confidence and identify when public attitudes changed, while also highlighting limitations around demographic biases and ethical concerns.

## Method Summary
The framework fine-tunes LLMs by incorporating three types of neural embeddings: semantic embeddings of survey questions (generated via pre-trained LLMs), individual belief embeddings (initialized randomly and optimized during training), and period embeddings (capturing temporal context). These embeddings are combined using a Deep Cross Network (DCN) architecture to capture interactions among them, with binary cross-entropy loss for training. The model first predicts individual opinions, then aggregates them at the population level using survey weights. The approach is evaluated using 10-fold cross-validation on binarized GSS data across three tasks: missing data imputation, retrodiction (predicting missing year-level responses), and zero-shot prediction (predicting entirely new questions).

## Key Results
- Best model (Alpaca-7b) achieves AUC=0.87 for missing data imputation and ρ=0.99 correlation
- Retrodiction performance is strong with AUC=0.86 for personal opinion prediction and ρ=0.98 for public opinion prediction
- Zero-shot prediction shows limited success with AUC=0.73 and ρ=0.67
- Model accuracy is lower for low-SES individuals, racial minorities, and non-partisans but higher for ideologically sorted opinions in contemporary periods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning LLMs with survey questions and responses improves prediction accuracy by incorporating semantic understanding of questions and capturing individual belief heterogeneity.
- **Mechanism:** The framework embeds survey questions into neural embeddings representing their semantic meaning, combining these with individual belief embeddings and temporal context embeddings. These embeddings are optimized during training so that questions with similar response patterns and individuals with similar beliefs are positioned closely in the embedding space.
- **Core assumption:** The semantic embedding of questions, individual belief embedding, and period embedding are jointly informative for predicting responses.
- **Evidence anchors:**
  - [abstract] "We present a new methodological framework that incorporates neural embeddings of survey questions, individual beliefs, and temporal contexts to personalize LLMs in opinion prediction."
  - [section] "Our model first predicts individuals’ opinions and then aggregates them at the population level using survey weights... We customize the architecture of LLMs to be suitable for predicting personalized responses to survey questions over time."
- **Break condition:** If embeddings fail to capture meaningful variation, the model will not outperform simpler matrix factorization approaches.

### Mechanism 2
- **Claim:** Retrodiction (predicting missing year-level responses) is more accurate than zero-shot prediction because the model leverages existing human responses during training.
- **Mechanism:** For retrodiction, the model is trained on most survey years and questions but omits a subset of years to predict. This allows the model to learn from real human response patterns, including temporal shifts in opinion. In zero-shot prediction, the model must predict responses to questions never asked before, relying only on semantic similarity and learned embedding relationships.
- **Core assumption:** Human responses in similar survey questions and periods are informative for predicting missing year-level data.
- **Evidence anchors:**
  - [abstract] "Our best models based on Alpaca-7b excels in retrodiction (AUC = 0.86 for personal opinion prediction, ρ = 0.98 for public opinion prediction). However, the models show limited performance in a zero-shot prediction task (AUC = 0.73, ρ = 0.67)."
- **Break condition:** If underlying patterns of human responses change dramatically across periods or if questions are too dissimilar to any previously asked, retrodiction accuracy will degrade.

### Mechanism 3
- **Claim:** Higher socioeconomic status (SES) individuals and strong partisans are more predictable because they have more organized belief systems.
- **Mechanism:** The model learns embeddings that group individuals with similar belief structures. Those with higher SES and strong partisan affiliation tend to have opinions that are more systematically linked to their ideology and demographics, making their embeddings more consistent and easier to predict.
- **Core assumption:** Opinion predictability correlates with the degree of ideological sorting and belief system coherence.
- **Evidence anchors:**
  - [abstract] "We find that the best models’ accuracy is lower for individuals with low socioeconomic status, racial minorities, and non-partisan affiliations but higher for ideologically sorted opinions in contemporary periods."
  - [section] "The lower predictability of racial minorities should remind us of the recent finding that the meaning of the terms 'liberal' and 'conservative' is unfamiliar to many black Americans."
- **Break condition:** If the assumption about ideological sorting is incorrect or if the embedding space fails to capture subtle belief differences, the observed SES and partisanship effects may not replicate.

## Foundational Learning

- **Concept:** Neural embeddings and similarity in high-dimensional space
  - **Why needed here:** The model relies on embedding survey questions, individuals, and periods into latent vector spaces so that similar items are close together, enabling predictions via nearest-neighbor reasoning.
  - **Quick check question:** Can you explain how a sentence embedding captures the semantic meaning of a survey question, and why two questions with similar response patterns would be mapped close in the embedding space?

- **Concept:** Cross-validation and evaluation metrics (AUC, correlation)
  - **Why needed here:** The paper uses 10-fold cross-validation to estimate model performance and AUC to measure how well the model ranks positive over negative responses. Understanding these ensures correct interpretation of reported results.
  - **Quick check question:** What does an AUC of 0.86 imply about the model's ability to rank positive responses higher than negative ones, and how is this different from accuracy?

- **Concept:** Missing data mechanisms (MCAR, MAR, MNAR)
  - **Why needed here:** The model's performance can vary depending on whether missingness is random or systematic. Understanding these mechanisms is crucial for assessing when the model's imputations are trustworthy.
  - **Quick check question:** How would the model's performance differ under MCAR versus MNAR conditions, and why does this matter for real-world survey applications?

## Architecture Onboarding

- **Component map:** Pre-trained LLM (frozen) -> Survey question embeddings -> Individual belief embeddings (optimized) -> Period embeddings (optimized) -> Deep Cross Network (DCN) -> Sigmoid output layer
- **Critical path:** The embeddings must be correctly initialized and optimized; the DCN must learn meaningful interactions; the final prediction must be aggregated with survey weights for population-level estimates.
- **Design tradeoffs:** Freezing LLM parameters avoids overfitting and reduces compute but may limit adaptation to survey-specific language. Binarizing responses simplifies the problem but loses granularity. Using embeddings trades interpretability for flexibility.
- **Failure signatures:** Low AUC or correlation indicates embeddings do not capture predictive signal; systematic bias across demographic groups suggests embedding bias or unmodeled confounders; poor retrodiction vs. imputation performance suggests overfitting to available responses.
- **First 3 experiments:**
  1. Train and evaluate the model on a small subset of questions with complete data to verify that embeddings and DCN can learn the basic task.
  2. Compare performance of frozen vs. fine-tuned LLM parameters on a validation set to confirm the benefit of freezing.
  3. Simulate missing data under MCAR, MAR, and MNAR to test robustness and identify conditions under which performance degrades.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can we validate the external validity of counterfactual trends generated by AI-augmented surveys, especially when applied to different nationally representative surveys?
  - **Basis in paper:** [explicit] The paper discusses this limitation in the Discussion section, noting that "some may cast doubt on the external validity of our model's ability to retrodict" and suggesting cross-survey validation as a potential solution.
  - **Why unresolved:** While the paper acknowledges this limitation and suggests cross-survey validation as a potential solution, it does not actually conduct such validation or provide empirical evidence of the model's performance on different survey datasets.
  - **What evidence would resolve it:** Empirical validation using multiple nationally representative surveys (e.g., comparing GSS-based predictions against ANES data or other national surveys) to demonstrate consistent performance across different datasets.

- **Open Question 2:** What are the specific cognitive and social processes underlying the predictability of opinions by LLMs, and how do these differ from traditional survey response processes?
  - **Basis in paper:** [explicit] The Discussion section notes that "the predictability of personal opinions highlights the inherently social nature of human beings" and raises questions about how LLMs extract relevant information from "extensive record of human history," but does not investigate the specific mechanisms.
  - **Why unresolved:** The paper demonstrates that opinions are predictable but does not investigate the underlying cognitive or social mechanisms that make this possible, nor does it compare these processes to traditional survey response formation.
  - **What evidence would resolve it:** Detailed cognitive studies comparing survey response processes with LLM prediction processes, potentially through think-aloud protocols, response time analysis, or experimental manipulation of information available to both humans and models.

- **Open Question 3:** How can AI-augmented survey methods be adapted to handle multi-class classification for survey responses with more than two response options?
  - **Basis in paper:** [explicit] The Discussion section explicitly identifies this as a limitation: "First, our current models have to binarize response options into positive and negative for the sake of an intuitive understanding of opinion" and suggests future work should "combine our models with multi-class classification layers or decoders."
  - **Why unresolved:** The paper acknowledges this limitation but does not implement or test alternative approaches for handling multi-category response options beyond binary transformation.
  - **What evidence would resolve it:** Implementation and testing of multi-class classification architectures (e.g., softmax output layers, ordinal regression models) and comparison of their performance against the current binary approach across different types of survey questions.

## Limitations
- Performance gap between retrodiction and zero-shot prediction suggests potential overfitting to historical patterns rather than genuine semantic understanding
- Lower accuracy for racial minorities and low-SES individuals raises concerns about systematic biases in embedding representations
- Binarization of responses discards potentially valuable ordinal information and limits applicability to nuanced survey questions

## Confidence
- **High Confidence:** The mechanism of using neural embeddings to capture semantic similarity between questions and beliefs is well-established and supported by strong empirical results (AUC = 0.86-0.87 for retrodiction and imputation).
- **Medium Confidence:** The claim about retrodiction outperforming zero-shot prediction is supported by reported metrics, but underlying reasons need further investigation to rule out overfitting.
- **Low Confidence:** The assertion that higher SES and strong partisans are more predictable is based on internal results without strong external validation, and the proposed explanation may be incomplete.

## Next Checks
1. **External validation:** Test the framework on a different survey dataset (e.g., European Social Survey) to assess generalizability beyond GSS.
2. **Bias audit:** Conduct a systematic analysis of embedding representations across demographic groups to identify and mitigate systematic biases affecting minority populations.
3. **Robustness testing:** Simulate various missing data mechanisms (MCAR, MAR, MNAR) to quantify how the model's performance degrades under different real-world scenarios.