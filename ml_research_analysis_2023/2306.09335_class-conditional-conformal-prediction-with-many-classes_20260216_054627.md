---
ver: rpa2
title: Class-Conditional Conformal Prediction with Many Classes
arxiv_id: '2306.09335'
source_url: https://arxiv.org/abs/2306.09335
tags:
- coverage
- data
- prediction
- conformal
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles the challenge of achieving class-conditional
  coverage in conformal prediction for classification tasks with many classes and
  limited data per class. Standard conformal methods only guarantee marginal coverage,
  which may lead to poor coverage for certain classes.
---

# Class-Conditional Conformal Prediction with Many Classes

## Quick Facts
- arXiv ID: 2306.09335
- Source URL: https://arxiv.org/abs/2306.09335
- Reference count: 40
- One-line primary result: Clustered conformal prediction improves class-conditional coverage for classification tasks with many classes and limited data per class.

## Executive Summary
This paper addresses the challenge of achieving class-conditional coverage in conformal prediction for classification tasks with many classes and limited data per class. Standard conformal methods only guarantee marginal coverage, which may lead to poor coverage for certain classes. The authors propose clustered conformal prediction, which groups similar classes based on their score distributions and performs conformal prediction at the cluster level. This approach balances the granularity of classwise methods with the data efficiency of standard methods.

The proposed method is evaluated on four large-scale image datasets with up to 1000 classes and three different score functions. The results show that clustered conformal prediction consistently outperforms standard and classwise methods in terms of class-conditional coverage and set size metrics, especially in the low-data regime where each class has between 20 to 75 examples. The authors also provide guidelines for choosing the appropriate conformal prediction method based on the amount of available data per class.

## Method Summary
The method clusters classes with similar score distributions and performs conformal prediction at the cluster level. For each class, quantiles of the scores at multiple levels (0.5, 0.6, 0.7, 0.8, 0.9, and 1-α) are computed to create a feature vector representing the score distribution shape. K-means clustering is then applied in this embedding space to group classes with similar distributions. The calibration data is split into clustering and proper calibration sets, with the fraction γ = K/(75+K) where K is the number of classes with at least max(nmin, nα) examples. Cluster-level quantiles are estimated from the proper calibration set and used for prediction. The method is evaluated on four image datasets with three score functions: softmax, APS, and RAPS.

## Key Results
- Clustered conformal prediction consistently outperforms standard and classwise methods in terms of class-conditional coverage gap and average set size metrics
- The method shows particular advantage in the low-data regime with 20-75 examples per class
- Performance is relatively insensitive to clustering parameters γ and M within reasonable ranges
- Clustered conformal prediction achieves better balance between coverage and set size compared to classwise methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clustered conformal prediction improves class-conditional coverage by grouping classes with similar score distributions and performing conformal prediction at the cluster level.
- Mechanism: By clustering classes with similar score distributions, the method pools calibration data across classes within each cluster, leading to more stable quantile estimates and better coverage for rare classes.
- Core assumption: Classes within the same cluster have sufficiently similar score distributions that a single quantile threshold can provide adequate coverage for all classes in the cluster.
- Evidence anchors:
  - [abstract]: "We propose a method called clustered conformal prediction that clusters together classes having 'similar' conformal scores and performs conformal prediction at the cluster level."
  - [section 2.2]: "We seek to cluster together classes with similar score distributions. To do so, we first summarize the empirical score distribution for each class via a vector of score quantiles..."
  - [corpus]: Weak - the corpus contains related papers on class-wise coverage but none specifically address the clustering mechanism or its theoretical justification.
- Break condition: If the clustering algorithm fails to group classes with truly similar score distributions, the cluster-level quantile estimates will be inappropriate for some classes, leading to poor coverage.

### Mechanism 2
- Claim: The quantile-based embedding approach effectively captures the similarity between score distributions of different classes.
- Mechanism: By computing quantiles of the scores at multiple levels (0.5, 0.6, 0.7, 0.8, 0.9, and 1-α), the method creates a feature vector that represents the score distribution shape, allowing k-means to group classes with similar distributions.
- Core assumption: The chosen quantile levels adequately capture the key characteristics of the score distributions that determine coverage performance.
- Evidence anchors:
  - [section 2.2]: "We compute quantiles of the scores {si}i∈Iy1 from class y at the levels T = {⌈(|Iy| + 1)τ⌉/|Iy| : τ ∈ {0.5, 0.6, 0.7, 0.8, 0.9} ∪ {1 − α}}"
  - [section 2.2]: "In this embedding space, a larger distance between two classes means their score distributions are more different."
  - [corpus]: Weak - the corpus mentions class-wise coverage but does not discuss the specific quantile-based embedding approach used here.
- Break condition: If the score distributions differ significantly in ways not captured by the chosen quantiles (e.g., multimodality, heavy tails), classes with different coverage needs might be incorrectly clustered together.

### Mechanism 3
- Claim: The data-splitting strategy (clustering data vs proper calibration data) balances the need for accurate clustering with sufficient data for quantile estimation.
- Mechanism: By reserving a fraction γ of the calibration data for clustering and the remainder for quantile estimation, the method ensures that clustering is based on sufficient data while still providing adequate data for computing cluster-level quantiles.
- Core assumption: The chosen fraction γ (K/(75 + K) where K is the number of classes with at least max(nmin, nα) examples) provides a good balance between clustering accuracy and quantile estimation precision.
- Evidence anchors:
  - [section 2.1]: "To begin, we randomly split the calibration data set into two parts: the clustering data set D1... and a proper calibration data set D2..."
  - [section 3.1]: "Define ˜n = max(nmin, nα) and let K be the number of classes with at least ˜n examples. We then set γ = K/(75 + K)..."
  - [corpus]: Weak - the corpus does not discuss data-splitting strategies for clustered conformal prediction.
- Break condition: If γ is set too low, insufficient data will be available for accurate clustering, leading to poor cluster assignments. If γ is set too high, insufficient data will remain for accurate quantile estimation within clusters.

## Foundational Learning

- Concept: Conformal prediction and its coverage guarantees
  - Why needed here: The entire paper builds on understanding how standard conformal prediction works and why it fails to provide class-conditional coverage
  - Quick check question: What is the difference between marginal coverage (P(Ytest ∈ C(Xtest)) ≥ 1-α) and class-conditional coverage (P(Ytest ∈ C(Xtest) | Ytest = y) ≥ 1-α)?

- Concept: Quantile estimation and its variance properties
  - Why needed here: The performance of both standard and clustered methods depends critically on the stability of quantile estimates, especially in low-data regimes
  - Quick check question: Given n calibration examples, what is the variance of the empirical (1-α)-quantile estimate, and how does this variance scale with n?

- Concept: Clustering algorithms and distance metrics
  - Why needed here: The effectiveness of the clustered method depends on the clustering algorithm's ability to group classes with similar score distributions
  - Quick check question: How does the choice of distance metric in the embedding space affect the quality of the resulting clusters, and what properties should this metric have?

## Architecture Onboarding

- Component map: Data preprocessing -> Score computation -> Clustering -> Quantile estimation -> Coverage evaluation
- Critical path: The clustering and quantile estimation steps are most critical, as errors here propagate to all downstream coverage calculations. The choice of γ and M parameters directly impacts clustering quality and subsequent coverage performance.
- Design tradeoffs: The method trades off between the granularity of classwise (good coverage but high variance) and the stability of standard (low variance but poor class-conditional coverage). The clustering approach aims to capture the benefits of both while mitigating their weaknesses.
- Failure signatures: Poor class-conditional coverage despite good marginal coverage indicates clustering failure. Excessively large prediction sets suggest quantile estimation issues within clusters. Large variance in coverage metrics across runs suggests sensitivity to random initialization or data splits.
- First 3 experiments:
  1. Verify that standard conformal prediction achieves marginal but not class-conditional coverage on a balanced dataset
  2. Test the quantile-based embedding approach by visualizing the embeddings and verifying that similar score distributions produce nearby points
  3. Evaluate the sensitivity of clustered coverage to the γ parameter by running with different values and comparing results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal clustering parameter (γ, M) for clustered conformal prediction across different data sets and score functions?
- Basis in paper: [explicit] The paper provides heuristics for choosing γ and M but acknowledges these are not universally optimal and performance depends on data characteristics and score function.
- Why unresolved: The paper performs sensitivity analysis showing that performance is not highly sensitive to these parameters within certain ranges, but does not determine the absolute optimal values.
- What evidence would resolve it: Systematic grid search experiments across multiple data sets and score functions to identify parameter combinations that consistently minimize CovGap or balance CovGap and AvgSize optimally.

### Open Question 2
- Question: How does clustered conformal prediction perform when classes have highly non-overlapping score distributions?
- Basis in paper: [inferred] The method assumes classes within clusters have similar score distributions. Proposition 3 provides an approximate coverage guarantee based on TV distance between score distributions.
- Why unresolved: The paper's empirical evaluation focuses on datasets with some class similarity (ImageNet, CIFAR-100, Places365, iNaturalist) but doesn't test extreme cases of non-overlapping distributions.
- What evidence would resolve it: Experiments on synthetic or real datasets where classes are designed to have very distinct score distributions, measuring how well clustered maintains class-conditional coverage.

### Open Question 3
- Question: Can the clustering methodology be extended to achieve group-conditional coverage beyond class labels?
- Basis in paper: [explicit] The discussion section suggests generalizing the approach to other group structures defined by input features or mixture distribution components.
- Why unresolved: The paper only implements and evaluates clustering based on class labels, leaving theoretical extensions to other grouping schemes untested.
- What evidence would resolve it: Implementation and evaluation of clustered conformal prediction where groups are defined by different attributes (e.g., demographic features, input feature clusters) and comparison to existing group-conditional methods.

## Limitations
- The theoretical justification for the clustering mechanism is limited, with no guarantees on cluster quality or its relationship to coverage performance
- Performance is sensitive to hyperparameter choices, particularly γ and M, though reasonable defaults are provided
- The quantile-based embedding may not capture all relevant aspects of score distribution similarity, particularly for complex, multimodal distributions

## Confidence
**High confidence**: The empirical demonstration that clustered conformal prediction improves class-conditional coverage compared to standard methods, particularly in low-data regimes. The experimental design and results are well-documented and reproducible.

**Medium confidence**: The effectiveness of the quantile-based embedding approach for capturing score distribution similarity. While the method is intuitive and works well empirically, there is no theoretical analysis of what score distribution characteristics this embedding captures or fails to capture.

**Medium confidence**: The data-splitting strategy with parameter γ = K/(75+K). The choice appears reasonable based on the authors' experiments, but the theoretical basis for this specific formula is not provided, and alternative splitting strategies may perform better in certain scenarios.

## Next Checks
1. **Cluster quality analysis**: For a representative dataset, compute the silhouette score or other cluster validity metrics to assess whether the k-means clustering successfully groups classes with similar score distributions. Visualize the quantile embeddings using dimensionality reduction to qualitatively assess cluster separation.

2. **Sensitivity analysis for γ**: Systematically vary the γ parameter (e.g., γ ∈ {0.1, 0.2, 0.3, 0.4, 0.5}) and evaluate the impact on class-conditional coverage and set size metrics. This would help determine whether the default γ = K/(75+K) is near-optimal or if there's significant room for improvement.

3. **Alternative clustering approaches**: Replace the quantile-based embedding and k-means with alternative clustering methods (e.g., hierarchical clustering, spectral clustering) and compare coverage performance. This would help determine whether the specific embedding approach is critical to the method's success or if the general clustering framework is the key innovation.