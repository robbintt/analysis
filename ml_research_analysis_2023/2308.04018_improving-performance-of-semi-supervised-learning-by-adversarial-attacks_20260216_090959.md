---
ver: rpa2
title: Improving Performance of Semi-Supervised Learning by Adversarial Attacks
arxiv_id: '2308.04018'
source_url: https://arxiv.org/abs/2308.04018
tags:
- data
- learning
- adversarial
- scar
- semi-supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a generalized framework, named SCAR, for improving
  the performance of semi-supervised learning algorithms. The core idea is to use
  adversarial attacks to select high-confident unlabeled data to be labeled with current
  predictions.
---

# Improving Performance of Semi-Supervised Learning by Adversarial Attacks

## Quick Facts
- arXiv ID: 2308.04018
- Source URL: https://arxiv.org/abs/2308.04018
- Reference count: 14
- Primary result: SCAR improves VAT, MixMatch, and FixMatch SSL accuracy on CIFAR10 by 1.54%, 3%, and 1.04% respectively.

## Executive Summary
This paper introduces SCAR, a generalized framework that improves semi-supervised learning (SSL) performance by leveraging adversarial attacks. The core idea is to use adversarial robustness as a filter to select high-confidence pseudo-labels from unlabeled data, which are then added to the training set for further fine-tuning. SCAR is applied to three state-of-the-art SSL algorithms (VAT, MixMatch, FixMatch) on CIFAR10, demonstrating significant accuracy improvements.

## Method Summary
SCAR operates in two stages: first, SSL models are pre-trained using VAT, MixMatch, or FixMatch on a small labeled set and a large unlabeled set. Second, adversarial attacks (PGD) are used to select unlabeled samples whose predicted class remains unchanged under attack; these are assumed to be correctly classified and are added to the labeled set with pseudo-labels. The model is then retrained on this expanded labeled set, improving accuracy.

## Key Results
- SCAR improves VAT accuracy by 1.54% on CIFAR10
- SCAR improves MixMatch accuracy by 3% on CIFAR10
- SCAR improves FixMatch accuracy by 1.04% on CIFAR10
- Sensitivity and specificity of sample selection vary with perturbation size ε

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial robustness filtering improves confidence calibration of pseudo-labels.
- Mechanism: SCAR uses adversarial attacks to test unlabeled samples; those whose predictions remain unchanged are considered robust and likely correctly classified.
- Core assumption: Adversarial robustness correlates with classification confidence and correctness in SSL.
- Evidence anchors:
  - [abstract] "adversarial attacks successfully select high-confident unlabeled data to be labeled with current predictions"
  - [section] "we can replace the criterion... with /BD {argmax_y∈Y pθθθ(y|xxxi) = argmax_y∈Y min_xxx′i∈Bp(xxxi,ε) pθθθ(argmax_y∈Y p˙θθθ(y|xxxi)|xxx′i)}"
- Break condition: If adversarial examples are too easy to generate (ε too large), robustness no longer correlates with confidence, leading to poor pseudo-label quality.

### Mechanism 2
- Claim: SCAR acts as a two-stage fine-tuning process that progressively improves model generalization.
- Mechanism: After initial SSL pre-training, SCAR selects robust unlabeled samples and augments the labeled set, allowing the model to retrain with more high-quality labeled data.
- Core assumption: Fine-tuning with high-quality pseudo-labels from robust samples improves generalization beyond initial SSL training.
- Evidence anchors:
  - [abstract] "On CIFAR10, three recent SSL algorithms with SCAR result in significantly improved image classification."
  - [section] "SCAR can be interpreted as a fine-tuning framework for selecting high-confident unlabeled data, which induces a two-step scheme for obtaining the final trained model."
- Break condition: If initial SSL model is poor, robust sample selection will be unreliable, degrading fine-tuning.

### Mechanism 3
- Claim: Trade-off between sensitivity and specificity in robust sample selection is tunable via ε.
- Mechanism: Larger ε increases sensitivity (more samples selected) but reduces specificity (more incorrectly labeled samples). SCAR can balance this by tuning ε to optimize the F1 score or downstream accuracy.
- Core assumption: There exists an optimal ε that maximizes accuracy by balancing true positives and false positives in sample selection.
- Evidence anchors:
  - [section] "sensitivity and specificity... means the rates of success of prediction about correctly and incorrectly pseudo-labeling... For large ε, the sensitivity is high, and the specificity is low"
  - [section] Table 2 shows sensitivity/specificity vary with ε across algorithms.
- Break condition: If ε is too small, almost no samples are selected; if too large, almost all are selected regardless of correctness.

## Foundational Learning

- Concept: Semi-supervised learning assumptions (cluster, low-density, manifold).
  - Why needed here: SCAR builds on these assumptions implicitly; robust samples are assumed to lie in low-density regions or on the data manifold.
  - Quick check question: If two samples are close in feature space under augmentation, what should their predictions be under cluster assumption?

- Concept: Adversarial example generation (PGD attacks).
  - Why needed here: SCAR uses adversarial attacks to test sample robustness; understanding PGD is essential to tune ε and interpret results.
  - Quick check question: In PGD, what does the projection ΠBp(xxx,ε)(·) enforce during iterative updates?

- Concept: Pseudo-label confidence thresholding.
  - Why needed here: SCAR replaces explicit confidence thresholds with adversarial robustness; understanding how pseudo-labels work is key to grasping the improvement.
  - Quick check question: Why might high-confidence pseudo-labels still be wrong in SSL?

## Architecture Onboarding

- Component map: SSL pre-training -> Adversarial robustness selector (PGD with ε) -> Labeled set augmenter -> Retraining loop

- Critical path:
  1. Train SSL model on initial labeled/unlabeled split.
  2. Generate adversarial examples for each unlabeled sample.
  3. Compare predictions before/after attack; keep if unchanged.
  4. Add robust samples to labeled set with pseudo-labels.
  5. Retrain model on expanded labeled set.

- Design tradeoffs:
  - ε size vs. sample quality (sensitivity/specificity).
  - Computational cost of adversarial attacks vs. SSL accuracy gains.
  - Model capacity: larger models may need more robust samples to improve.

- Failure signatures:
  - Accuracy plateaus or drops after SCAR fine-tuning.
  - Very few or very many samples survive adversarial robustness check.
  - High variance in sensitivity/specificity across runs.

- First 3 experiments:
  1.