---
ver: rpa2
title: OCR Language Models with Custom Vocabularies
arxiv_id: '2308.09671'
source_url: https://arxiv.org/abs/2308.09671
tags:
- vocabulary
- language
- words
- word
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving optical character
  recognition (OCR) accuracy for specialized domains such as medical prescriptions,
  checks, and price tags, where domain-specific language models can significantly
  outperform general-purpose ones. The core method introduces a runtime-customizable
  language model that combines a domain-specific vocabulary (both literal words and
  regular expressions) with a general character-based language model.
---

# OCR Language Models with Custom Vocabularies

## Quick Facts
- arXiv ID: 2308.09671
- Source URL: https://arxiv.org/abs/2308.09671
- Reference count: 32
- One-line primary result: Runtime-customizable OCR language models with domain vocabularies improve recognition accuracy for specialized domains.

## Executive Summary
This paper presents a method to improve optical character recognition (OCR) accuracy for specialized domains such as medical prescriptions, checks, and price tags by integrating domain-specific vocabularies into the OCR decoding process. The core innovation is a runtime-customizable language model that combines literal words and regular expressions with a general character-based language model using finite-state machines. A modified CTC beam search decoder is employed, maintaining dual scoring criteria to retain hypotheses that could complete vocabulary words. Experiments show substantial reductions in word error rates, with improvements ranging from 10% to over 80% for in-vocabulary words, and minimal computational overhead.

## Method Summary
The method involves compiling domain-specific vocabularies (literal words and regular expressions) into finite-state machines, which are then integrated into a modified CTC beam search decoder. The decoder uses dual scoring criteria (current and best possible) to retain hypotheses that could complete vocabulary words, improving recall for in-vocabulary terms. Vocabulary weights are tuned based on word length, empirical frequency, and baseline language model scores. The system supports fast runtime configuration and shows negligible computational overhead.

## Key Results
- Up to 83% reduction in word error rate for in-vocabulary words on medical prescription images.
- Jaccard index for medication recognition increased from 0.19 to 0.27.
- Improvements of 10% to 80% for in-vocabulary words across multiple datasets, with minimal impact on out-of-vocabulary words.

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific vocabularies reduce word error rates by providing prior probabilities for likely domain terms. The OCR decoder integrates a custom language model built from a finite-state machine representation of vocabulary words and regular expressions. This model is merged at runtime with the general character-based language model, allowing hypotheses containing domain terms to score higher.

### Mechanism 2
The dual criterion beam search keeps competing hypotheses alive longer, improving recall for in-vocabulary words. Instead of pruning solely on current score, the decoder retains hypotheses that have either a good current score or a promising best-possible future score, allowing partial word matches to survive until completion.

### Mechanism 3
Vocabulary weight tuning based on word length, empirical frequency, and language model scores optimizes domain coverage. The system assigns higher weights to longer, more frequent domain words that score poorly under the baseline language model, balancing recall and precision.

## Foundational Learning

- **Finite-state machine representation of language models**: Enables efficient encoding and fast runtime traversal of vocabulary words and regex patterns. *Quick check: How does a trie differ from a general finite-state machine when representing literal words?*
- **CTC (Connectionist Temporal Classification) decoding**: Allows alignment-free training of sequence-to-sequence OCR models, critical for variable-length character sequences. *Quick check: What role does the "blank" label play in CTC decoding?*
- **Beam search pruning strategies**: Controls search space size and runtime, directly affecting OCR speed and memory usage. *Quick check: What is the trade-off between beam width and hypothesis recall?*

## Architecture Onboarding

- **Component map**: Optical model -> Frame-level character probabilities -> Baseline language model -> Custom vocabulary FSM -> Dual-criterion beam search -> Best hypothesis
- **Critical path**: 1) Compile vocabulary into FSMs. 2) Load FSMs into decoder state. 3) For each frame: generate optical scores, update LM states, apply dual-criterion pruning. 4) Output best hypothesis at sequence end.
- **Design tradeoffs**: Vocabulary size vs. compilation time and memory footprint; Beam width vs. decoding speed and accuracy; Anchoring strategy (start vs. anywhere) vs. state vector size.
- **Failure signatures**: Slow startup → FSM compilation bottleneck; High memory usage → Vocabulary too large for efficient FSM representation; Degraded accuracy → Beam width too small or dual-criterion thresholds misconfigured.
- **First 3 experiments**: 1) Measure FSM compilation time for vocabularies of 100, 1k, 10k words. 2) Compare WER with and without dual-criterion beam search on a small synthetic dataset. 3) Vary vocabulary anchoring (start-only vs. anywhere) and measure impact on hypothesis state vector size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's performance scale with vocabulary size for in-vocabulary words versus out-of-vocabulary words, and what is the optimal vocabulary size for balancing coverage and accuracy?
- Basis in paper: [explicit] The paper presents tables showing the effect of vocabulary size on word error rate (WER) and Jaccard index for different datasets, indicating that there is a trade-off between including more words and maintaining accuracy.
- Why unresolved: The paper shows that increasing vocabulary size initially improves accuracy but may lead to a decrease in performance for out-of-vocabulary words, suggesting a need for further analysis to determine the optimal vocabulary size for different applications.
- What evidence would resolve it: Conducting experiments with varying vocabulary sizes on diverse datasets to identify the point at which the accuracy gains plateau or decline, and analyzing the impact on both in-vocabulary and out-of-vocabulary word recognition.

### Open Question 2
- Question: How can the scoring function for vocabulary weights be further optimized to account for coverage by the vocabulary of the target text and the confusability of vocabulary items?
- Basis in paper: [explicit] The paper mentions that the current scoring function uses factors like word length, frequency, and language model score, but suggests that future work could explore formulas that better account for coverage and confusability.
- Why unresolved: The current scoring function may not fully capture the nuances of vocabulary coverage and confusability, potentially leading to suboptimal vocabulary weights and reduced accuracy in certain applications.
- What evidence would resolve it: Developing and testing alternative scoring functions that incorporate coverage and confusability metrics, and evaluating their impact on recognition accuracy across various datasets and applications.

### Open Question 3
- Question: How can the dual criterion beam search be further optimized or adapted for different types of OCR tasks, and what are the conditions under which it provides the most benefit?
- Basis in paper: [explicit] The paper demonstrates that the dual criterion beam search improves accuracy in some applications, such as handwritten prescriptions, but not in others, like pay stubs, suggesting that its effectiveness may depend on the specific characteristics of the input data.
- Why unresolved: The paper does not provide a comprehensive analysis of the conditions under which the dual criterion beam search is most beneficial, leaving open the question of how to optimize or adapt it for different OCR tasks.
- What evidence would resolve it: Conducting experiments with various OCR tasks and datasets to identify the characteristics that influence the effectiveness of the dual criterion beam search, and developing guidelines for its application in different scenarios.

## Limitations
- The claims about runtime efficiency and accuracy gains rely heavily on controlled synthetic datasets and well-curated domain vocabularies.
- Evidence for finite-state machine compilation performance is largely inferred from related work, not directly measured here.
- The weight tuning formula is hand-crafted without systematic ablation, leaving the robustness of the approach to vocabulary quality and size unexplored.

## Confidence

- **Mechanism 1 (Domain vocabularies reduce WER)**: Medium confidence. Supported by experimental results, but the finite-state compilation overhead and scaling limits are not empirically tested.
- **Mechanism 2 (Dual-criterion beam search)**: Low confidence. The theoretical benefit is described, but no ablation study compares it to standard beam search under identical conditions.
- **Mechanism 3 (Weight tuning)**: Low confidence. No systematic evaluation of alternative weight functions or sensitivity analysis is provided.

## Next Checks

1. **Runtime Compilation Scaling**: Measure FSM compilation time and memory use for vocabularies of 100, 1k, and 10k words on a representative CPU to assess real-time feasibility.
2. **Dual-Criterion Ablation**: Run experiments with and without the dual-criterion beam search on a held-out test set to quantify its isolated impact on WER.
3. **Vocabulary Quality Sensitivity**: Test the system with noisy or incomplete vocabularies to see how performance degrades and whether overfitting occurs.