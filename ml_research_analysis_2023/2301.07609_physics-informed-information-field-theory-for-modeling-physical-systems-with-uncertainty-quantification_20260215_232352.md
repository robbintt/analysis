---
ver: rpa2
title: Physics-informed Information Field Theory for Modeling Physical Systems with
  Uncertainty Quantification
arxiv_id: '2301.07609'
source_url: https://arxiv.org/abs/2301.07609
tags:
- posterior
- which
- theory
- information
- physics-informed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents physics-informed information field theory (PIFT)
  for modeling physical systems with uncertainty quantification. PIFT extends IFT
  to encode known physical laws into functional priors, allowing inference over fields
  directly rather than parameterized approximations.
---

# Physics-informed Information Field Theory for Modeling Physical Systems with Uncertainty Quantification

## Quick Facts
- arXiv ID: 2301.07609
- Source URL: https://arxiv.org/abs/2301.07609
- Reference count: 15
- One-line primary result: Physics-informed Information Field Theory (PIFT) extends IFT to encode physical laws into functional priors, enabling inference over fields directly with uncertainty quantification

## Executive Summary
This paper introduces Physics-informed Information Field Theory (PIFT), a novel approach for modeling physical systems that combines Information Field Theory with physics-informed priors. PIFT extends traditional IFT by encoding known physical laws into functional priors, allowing inference over fields directly rather than using parameterized approximations. The method handles forward and inverse problems with unknown parameters, noisy data, or incomplete physics, and includes a mechanism to automatically detect and quantify model-form uncertainty. PIFT demonstrates the ability to capture multiple modes for ill-posed problems, identify unknown parameters, and balance physics and data contributions through an automatically inferred inverse temperature parameter β.

## Method Summary
PIFT extends Information Field Theory by defining physics-informed functional priors based on energy functionals derived from partial differential equations. The method uses Bayesian inference to combine these priors with measurement likelihoods, obtaining posteriors over fields and parameters. For numerical implementation, PIFT employs stochastic gradient Langevin dynamics and its variants to sample from the joint posterior. The approach treats the inverse temperature parameter β as a hyperparameter that can be inferred from data, allowing automatic adjustment of the balance between physics and data contributions. This framework enables uncertainty quantification, model-form uncertainty detection, and handling of ill-posed problems through multi-modal posterior distributions.

## Key Results
- PIFT successfully identifies unknown parameters in physical systems while quantifying uncertainty
- The method automatically detects model-form uncertainty by inferring the inverse temperature parameter β from data
- Numerical experiments show PIFT can capture multiple modes in ill-posed problems where traditional methods fail
- PIFT is shown to be related to PINNs and B-PINNs under certain conditions, with key differences in how physics is enforced through priors rather than observations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The physics-informed functional prior assigns higher probability to functions that minimize the energy functional U[φ], encoding known physics.
- Mechanism: The prior is defined as p(φ) ∝ exp{-βU[φ]}, where β acts as an inverse temperature scaling parameter that controls the strength of the physics enforcement.
- Core assumption: The physical system can be described by a PDE that can be reformulated as the minimization of an energy functional.
- Evidence anchors:
  - [abstract]: "PIFT extends IFT to encode known physical laws into functional priors, allowing inference over fields directly rather than parameterized approximations."
  - [section 4.1.1]: "The appropriate choice of the function space Φ is problem-specific. Assume that we have access to a PDE which describes our current state of knowledge about the field D[φ] = f..."
- Break condition: If the physical system cannot be expressed as a minimization of an energy functional, this mechanism fails.

### Mechanism 2
- Claim: The method can detect when the known physics are incorrect and automatically adjust the balance between physics and data contributions.
- Mechanism: The inverse temperature parameter β is treated as a hyperparameter that can be inferred from data. When the physics are wrong, the posterior over β concentrates on low values, effectively treating the problem as regression.
- Core assumption: The physics can be "trusted" to varying degrees, and this trust level can be learned from data.
- Evidence anchors:
  - [abstract]: "PIFT is equipped with a metric which allows the posterior to automatically quantify model-form uncertainty."
  - [section 5.2]: "We observe that PIFT yields β posteriors that concentrate on large values when the model is correct and to low values when the model is wrong."
- Break condition: If the physics are completely wrong and the data are insufficient, the method may fail to detect this.

### Mechanism 3
- Claim: The posterior remains independent of any numerical scheme, providing a true representation of the field over its entire domain.
- Mechanism: By using Information Field Theory, the method defines probability measures directly over the function space Φ, rather than over parameterized approximations of the field.
- Core assumption: The function space Φ is sufficiently expressive to capture the relevant scales of the physical field.
- Evidence anchors:
  - [abstract]: "The posteriors derived from this PIFT remain independent of any numerical scheme, and can capture multiple modes which allows for the solution of problems which are not well-posed."
  - [section 3]: "Typically in IFT, the functional priors are Gaussian random fields which encode known regularity constraints on the field."
- Break condition: If the function space Φ is not expressive enough or if the numerical approximation of the posterior introduces discretization errors.

## Foundational Learning

- Concept: Information Field Theory (IFT)
  - Why needed here: IFT provides the theoretical foundation for defining probability measures over function spaces, which is essential for the physics-informed approach.
  - Quick check question: What is the main advantage of using IFT over traditional parameterized approaches in physics-informed modeling?

- Concept: Bayesian Inference
  - Why needed here: The method uses Bayesian inference to combine the physics-informed prior with the likelihood from measurements to obtain the posterior over the field.
  - Quick check question: How does the posterior p(φ|d) relate to the prior p(φ) and the likelihood p(d|φ)?

- Concept: Stochastic Gradient Langevin Dynamics (SGLD)
  - Why needed here: SGLD is used to numerically sample from the posterior over the field and model parameters, especially when an analytical form is not available.
  - Quick check question: What are the key conditions that the learning rate series must satisfy in SGLD to ensure correct sampling?

## Architecture Onboarding

- Component map: Physics-informed functional prior (IFT) -> Likelihood function (measurement model) -> Posterior (Bayes' rule) -> Sampling (SGLD variants)
- Critical path: Define the physics-informed functional prior → Construct the likelihood function → Derive the posterior via Bayes' rule → Numerically sample from the posterior using SGLD
- Design tradeoffs: The choice of the function space Φ and the parameterization of the field affects the expressiveness and computational efficiency. A more expressive function space may capture the field better but at a higher computational cost.
- Failure signatures: If the physics are completely wrong and the data are insufficient, the method may fail to detect this. If the function space Φ is not expressive enough, the posterior may not capture the true field.
- First 3 experiments:
  1. Implement the analytic example from section 4.2 (Klein-Gordon equation) to verify the method's ability to derive analytical posteriors.
  2. Reproduce the results from section 5.1 (Example 1) to empirically verify the effect of β on the posterior variance.
  3. Implement the inverse problem from section 5.3 (Example 3a) to test the method's ability to identify energy parameters.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can PIFT be extended to time-dependent problems with stochastic state transitions?
- Basis in paper: [inferred] The paper mentions that "the theory presented here is developed only for deterministic problems without dynamics, and it would be beneficial to study time-dependent problems with stochastic state transitions."
- Why unresolved: The current formulation of PIFT is limited to steady-state problems and does not account for time evolution or stochastic dynamics.
- What evidence would resolve it: Development of a PIFT framework that incorporates time-dependent operators and stochastic processes, along with numerical experiments demonstrating its effectiveness on dynamic systems.

### Open Question 2
- Question: What are the limitations of using Fourier series as surrogate functions in PIFT, and how do other choices perform?
- Basis in paper: [inferred] The paper states that "Fourier series in 1D or 2D was the primary choice of surrogate functions in this paper, it may be interesting to study how other choices perform. There are certain applications for which the Fourier series is a bad choice."
- Why unresolved: The paper primarily uses Fourier series without extensively comparing it to other basis functions or surrogate models.
- What evidence would resolve it: Numerical experiments comparing PIFT performance using different basis functions (e.g., neural networks, wavelets) on various types of problems, including those with discontinuities or sharp features.

### Open Question 3
- Question: How can Feynman diagrams be applied to IFT for Gaussian random fields to derive analytic representations for non-quadratic operators?
- Basis in paper: [inferred] The paper mentions that "analytic representations of the functional priors and posteriors were able to be derived for quadratic operators. Ideally, this could be generalized to other operators. Doing so requires perturbation approximations of the path integrals that appear or through the rich theory of Feynman diagrams."
- Why unresolved: The current analytic results are limited to quadratic operators, and extending them to more general cases requires advanced mathematical techniques.
- What evidence would resolve it: Derivation of Feynman diagram rules for IFT with Gaussian random fields and application to specific non-quadratic operators, demonstrating the feasibility of obtaining analytic results in these cases.

## Limitations

- Implementation details for nested SGLD schemes in inverse problems are not fully specified
- Limited discussion on the impact of function space choice and discretization on results
- Performance on high-dimensional problems and with sparse, noisy data requires further validation
- Computational efficiency compared to PINNs needs comprehensive benchmarking

## Confidence

- High confidence in the theoretical framework and its relationship to existing methods (PINNs/B-PINNs)
- Medium confidence in the automatic model-form uncertainty detection mechanism, as empirical validation is limited to specific examples
- Medium confidence in the numerical stability and convergence properties of the proposed sampling schemes
- Low confidence in scalability to complex, high-dimensional physical systems

## Next Checks

1. **Reproduce parameter estimation**: Implement the Allen-Cahn equation example with HMCECS sampling and verify the joint posterior over fields and parameters converges correctly
2. **Test model-form uncertainty detection**: Systematically vary the degree of mismatch between true physics and assumed model, measuring the β posterior's ability to detect and quantify this uncertainty
3. **Benchmark computational efficiency**: Compare PIFT's computational cost against PINNs for equivalent problems, measuring both training time and prediction accuracy across multiple problem sizes and dimensionalities