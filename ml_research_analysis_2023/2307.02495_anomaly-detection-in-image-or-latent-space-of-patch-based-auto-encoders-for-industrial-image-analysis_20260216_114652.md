---
ver: rpa2
title: Anomaly detection in image or latent space of patch-based auto-encoders for
  industrial image analysis
arxiv_id: '2307.02495'
source_url: https://arxiv.org/abs/2307.02495
tags:
- images
- dans
- pour
- image
- thodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We study several methods for detecting anomalies in color images,
  constructed on patch-based auto-encoders. We compare the performance of three types
  of methods based, first, on the error between the original image and its reconstruction,
  second, on the support estimation of the normal image distribution in the latent
  space, and third, on the error between the original image and a restored version
  of the reconstructed image.
---

# Anomaly detection in image or latent space of patch-based auto-encoders for industrial image analysis

## Quick Facts
- arXiv ID: 2307.02495
- Source URL: https://arxiv.org/abs/2307.02495
- Reference count: 0
- Key outcome: Patch-based auto-encoders with 1000 patches per image and 17x17x16 latent dimension achieve comparable AUROC to state-of-the-art methods on MVTecAD wood and carpet classes, with only thousands of parameters versus hundreds of millions

## Executive Summary
This paper studies anomaly detection in color images using patch-based auto-encoders, focusing on two texture classes from the MVTecAD industrial image database: wood and carpet. The authors propose three detection methods: reconstruction error, latent space support estimation with one-class SVM, and restoration error using discretized latent space with auto-regressive modeling. Their lightweight models achieve competitive performance (AUPRO 0.83 for wood, 0.78 for carpet) compared to state-of-the-art methods with orders of magnitude more parameters, though performance varies significantly based on anomaly type and contrast levels.

## Method Summary
The method extracts 63x63 RGB patches from training images (only normal samples) and trains a patch-based auto-encoder with 4 convolutional blocks and a 17x17x16 latent space. Three anomaly detection approaches are evaluated: (1) reconstruction error between original and reconstructed patches, (2) one-class SVM on latent vectors to estimate normal distribution support, and (3) a hybrid method using discretized latent space with auto-regressive modeling for restoration before reconstruction. The models are evaluated on wood and carpet classes from MVTecAD, comparing against PaDiM and FastFlow baselines.

## Key Results
- Best patch-based method achieves AUPRO of 0.83 and AUPRC of 0.42 on wood class, comparable to state-of-the-art methods
- For carpet class, achieves AUPRO of 0.78 and AUPRC of 0.42
- Models contain only thousands of parameters versus hundreds of millions in competitive methods
- Performance varies significantly by anomaly type, with better results for high-contrast anomalies versus low-contrast or color-similar anomalies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Patch-based auto-encoders learn localized normal patterns that differ in reconstruction error from anomalous regions.
- Mechanism: Small image patches are encoded into a latent space and reconstructed; anomalies cause higher reconstruction error because the model never saw them during training.
- Core assumption: The training data contains only normal patches, so the model cannot accurately reconstruct anomalous patterns.
- Evidence anchors:
  - [abstract] "We study several methods for detecting anomalies in color images, constructed on patch-based auto-encoders."
  - [section] "L'utilisation de patchs permet d'augmenter artificiellement la taille de la base de données... qui peut limiter le sur-apprentissage."
  - [corpus] No direct corpus evidence; similar claims appear in related patch-based methods but with no comparative quantitative support here.
- Break condition: If anomalous patches share enough visual similarity with normal ones (e.g., low contrast), reconstruction error becomes unreliable.

### Mechanism 2
- Claim: The latent space of an auto-encoder trained on normal patches forms a compact support that can be bounded using one-class SVM.
- Mechanism: Latent vectors from normal patches are projected into a higher-dimensional space; a one-class SVM learns a hyperplane separating the origin from the data, flagging deviations as anomalies.
- Core assumption: Normal patches cluster tightly in latent space, leaving anomalous patches outside the learned support.
- Evidence anchors:
  - [section] "Une méthode alternative à l'erreur de reconstruction... consiste à estimer le support de la distribution de probabilité des patchs normaux dans l'espace latent."
  - [corpus] Weak; the referenced SVM-based method exists in literature but no specific performance metrics are given here.
- Break condition: If anomalies occupy latent regions also used by normal data (e.g., texture variations), the SVM boundary becomes ineffective.

### Mechanism 3
- Claim: Discrete latent representations allow autoregressive modeling, enabling restoration of anomalous patches toward the normal distribution before reconstruction.
- Mechanism: Latent vectors are quantized; an autoregressive model predicts each coordinate conditioned on previous ones; low-probability coordinates are resampled to match the normal distribution, then decoded.
- Core assumption: The discrete latent space captures enough structure that anomalies can be identified by low autoregressive likelihood and corrected by resampling.
- Evidence anchors:
  - [section] "Une méthode hybride... s'appuie sur la discrétisation de cet espace latent pour apprendre un modèle auto-régressif permettant de 'restaurer' les images déviant de la normalité."
  - [corpus] No corpus evidence; this appears to be a novel hybrid approach not directly supported by nearby literature.
- Break condition: If anomalies occupy high-probability discrete states or if the autoregressive model overfits to normal patterns, restoration will fail.

## Foundational Learning

- Concept: Patch-based representation learning
  - Why needed here: Enables training on small image regions, increasing effective dataset size and reducing model complexity.
  - Quick check question: How does using patches instead of full images affect the number of training samples and the risk of overfitting?

- Concept: One-class SVM for support estimation
  - Why needed here: Provides a discriminative boundary in latent space without requiring labeled anomalies.
  - Quick check question: What kernel and hyperparameters would you tune to maximize separation between normal and anomalous latent vectors?

- Concept: Auto-regressive modeling in discrete latent space
  - Why needed here: Allows sequential prediction and resampling of latent coordinates to "correct" anomalies before decoding.
  - Quick check question: Why does discretizing the latent space make autoregressive modeling feasible where continuous space would not?

## Architecture Onboarding

- Component map:
  Input: 63x63x3 RGB patches -> Encoder (4 conv blocks) -> Latent space (17x17x16) -> Decoder (symmetric) -> Output: Reconstruction
  Anomaly detectors: (1) Reconstruction MSE, (2) One-class SVM on latent vectors, (3) Autoregressive model + restoration + SSIM

- Critical path:
  1. Extract patches from training images (normal only)
  2. Train auto-encoder to minimize reconstruction loss
  3. For continuous model: train one-class SVM on latent vectors
  4. For discrete model: train autoregressive model on quantized latents
  5. At inference: compute chosen anomaly score per patch, aggregate to image map

- Design tradeoffs:
  - Patch size vs. context: Smaller patches reduce parameters but lose global context
  - Continuous vs. discrete latent: Continuous is simpler but discrete enables restoration
  - Reconstruction vs. latent modeling: Reconstruction is intuitive but latent modeling can be more robust to certain anomalies

- Failure signatures:
  - High false positives on low-contrast anomalies
  - Slow inference due to patch-wise processing
  - Poor performance when normal and anomalous patches overlap in latent space

- First 3 experiments:
  1. Train auto-encoder on wood patches, evaluate reconstruction MSE anomaly map, compare AUROC/AUPRC to baseline
  2. Replace reconstruction scoring with one-class SVM on latent vectors, measure impact on AUPRO
  3. Train discrete auto-encoder + autoregressive model, implement restoration, compare anomaly detection performance to continuous model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do patch-based autoencoders compare to full-image autoencoders for anomaly detection across different types of anomalies (e.g., high vs. low contrast, color vs. shape)?
- Basis in paper: [explicit] The authors state "Une étude plus approfondie montre une grande disparité de performances en fonction du type d'anomalie à détecter sur les images" and note that models perform better on anomalies with high contrast versus those with low contrast or similar color to normal patterns.
- Why unresolved: While the authors observe performance differences based on anomaly type, they do not provide a systematic comparison between patch-based and full-image approaches across various anomaly characteristics. The trade-offs between local detail capture (patch-based) and global context (full-image) are not fully explored.
- What evidence would resolve it: A controlled study comparing patch-based and full-image autoencoders on the same dataset, systematically varying anomaly types (contrast, color similarity, shape) and measuring performance metrics across these categories.

### Open Question 2
- Question: Can the performance of patch-based autoencoders on anomalies with low contrast or similar color to normal patterns be improved through architectural modifications or training strategies?
- Basis in paper: [explicit] The authors note "Les modèles semblent meilleurs lorsque l'anomalie à segmenter a un contraste différent des motifs normaux" and observe poor performance on carpet anomalies like metallic debris contamination that don't significantly alter color.
- Why unresolved: The paper identifies limitations in detecting certain anomaly types but does not explore potential solutions or modifications to address these weaknesses. The fundamental reasons for poor performance on low-contrast anomalies remain unexplored.
- What evidence would resolve it: Experiments testing architectural modifications (e.g., multi-scale features, attention mechanisms) or training strategies (e.g., data augmentation, contrastive learning) specifically designed to enhance detection of low-contrast anomalies, with performance comparisons before and after modifications.

### Open Question 3
- Question: What is the optimal balance between model complexity and performance for industrial anomaly detection, and how do patch-based approaches compare to state-of-the-art methods in terms of computational efficiency?
- Basis in paper: [explicit] The authors highlight that "La taille de l'auto-encodeur est de seulement quelques milliers de paramètres là où des modèles reposant sur de grands extracteurs de caractéristiques contiennent plusieurs dizaines voire centaines de millions de paramètres" but also note that "Un désavantage de l'approche par patch se trouve dans le temps d'inférence, généralement plus long."
- Why unresolved: While the authors present the lightweight nature of patch-based models as an advantage, they do not provide a comprehensive analysis of the trade-off between model size, inference speed, and detection performance. The computational efficiency of patch-based approaches relative to state-of-the-art methods is not fully quantified.
- What evidence would resolve it: A systematic comparison measuring inference time, memory usage, and training time for patch-based models versus state-of-the-art approaches (like PaDiM and FastFlow) across different hardware configurations, while correlating these metrics with detection performance across various datasets.

## Limitations
- Limited evaluation to only two texture classes from MVTecAD raises generalizability concerns
- Novel restoration method lacks direct quantitative comparison to other proposed methods
- Poor performance on anomalies with low contrast or similar color to normal patterns
- Slow inference due to patch-based processing compared to full-image approaches

## Confidence
- Medium Confidence: The effectiveness of patch-based auto-encoders for industrial anomaly detection, as supported by AUROC/AUPRC metrics on wood and carpet classes
- Medium Confidence: The comparative performance against PaDiM and FastFlow methods, though implementation details of these baselines are not fully specified
- Low Confidence: The novel restoration method's performance, as it lacks direct quantitative comparison to the other two methods

## Next Checks
1. Implement and evaluate all three proposed methods (reconstruction error, SVM in latent space, and restoration) on the same texture classes to establish relative performance rankings
2. Test the patch-based approach on additional MVTecAD categories beyond wood and carpet to assess generalizability across different anomaly types and textures
3. Compare inference speed and memory usage of the lightweight patch-based models against the state-of-the-art methods with hundreds of millions of parameters to quantify the claimed efficiency benefits