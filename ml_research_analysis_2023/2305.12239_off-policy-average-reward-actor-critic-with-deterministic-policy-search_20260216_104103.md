---
ver: rpa2
title: Off-Policy Average Reward Actor-Critic with Deterministic Policy Search
arxiv_id: '2305.12239'
source_url: https://arxiv.org/abs/2305.12239
tags:
- reward
- policy
- diff
- lemma
- assumption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of extending off-policy actor-critic
  methods to the average reward criterion, which is less explored compared to discounted
  reward settings. The core method introduces deterministic policy gradient theorems
  for both on-policy and off-policy settings under average reward, leading to an Average
  Reward Off-Policy Deep Deterministic Policy Gradient (ARO-DDPG) algorithm.
---

# Off-Policy Average Reward Actor-Critic with Deterministic Policy Search

## Quick Facts
- arXiv ID: 2305.12239
- Source URL: https://arxiv.org/abs/2305.12239
- Reference count: 40
- Primary result: Introduced ARO-DDPG algorithm with average reward optimization, achieving superior performance compared to state-of-the-art on-policy methods on MuJoCo environments

## Executive Summary
This paper addresses the challenge of extending off-policy actor-critic methods to the average reward criterion, which is crucial for continuing tasks where discounting is not appropriate. The authors present deterministic policy gradient theorems for both on-policy and off-policy settings under average reward, leading to the ARO-DDPG algorithm. The method uses target networks and double Q-learning for stability and provides both asymptotic convergence guarantees and finite-time analysis. Empirical results demonstrate significant improvements over existing average reward actor-critic algorithms on standard continuous control benchmarks.

## Method Summary
The ARO-DDPG algorithm extends deep deterministic policy gradient to average reward settings through a three-timescale stochastic approximation framework. The method maintains separate actor and critic networks with target networks for stability, uses double Q-learning to reduce overestimation bias, and employs a replay buffer for sample efficiency. The policy gradient is derived using the average reward differential value function, and the algorithm alternates between critic updates (using TD(0) with target networks) and actor updates (using deterministic policy gradient). The average reward estimator is updated separately to track the long-term performance of the current policy.

## Key Results
- ARO-DDPG outperforms state-of-the-art on-policy average reward methods (ATRPO and APO) on MuJoCo environments
- Finite-time analysis shows ε-optimal stationary policy with sample complexity Ω(ε^{-2.5})
- Asymptotic convergence guarantees via ODE-based analysis for the on-policy variant
- Superior average reward performance compared to discounted reward methods in continuing tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The average reward policy gradient theorem allows direct optimization of long-term average reward in continuing tasks.
- Mechanism: By deriving a deterministic policy gradient theorem for average reward, the algorithm optimizes the average reward directly rather than discounted reward, avoiding the need for discounting parameters and making the objective match evaluation metrics.
- Core assumption: The Markov process under the policy is ergodic (Assumption 2.1).
- Evidence anchors:
  - [abstract] "present both on-policy and off-policy deterministic policy gradient theorems for the average reward performance criterion"
  - [section 3.1] "Theorem 3.1. The gradient of ρ(π) with respect to the policy parameter θ is given as follows: ∇θρ(π) = ∫S dπ(s)∇aQπdiff(s,a)|a=π(s)∇θπ(s,θ)ds"
- Break condition: If the Markov process is not ergodic, the stationary distribution doesn't exist and the theorem doesn't apply.

### Mechanism 2
- Claim: Off-policy learning with replay buffer enables sample efficiency while maintaining convergence.
- Mechanism: The algorithm uses double Q-learning and target networks to stabilize learning from off-policy data stored in replay buffer, enabling reuse of past experiences while maintaining convergence guarantees.
- Core assumption: The Markov chain generated by the behavior policy satisfies uniform ergodicity (Assumption 3.3).
- Evidence anchors:
  - [abstract] "present an Average Reward Off-Policy Deep Deterministic Policy Gradient (ARO-DDPG) Algorithm"
  - [section 3.5] "We are using target estimators to ensure stability of the iterates of the algorithm"
- Break condition: If the replay buffer contains data from policies too different from current policy, the approximation error grows unbounded (Theorem 3.4).

### Mechanism 3
- Claim: Three-timescale stochastic approximation ensures asymptotic convergence.
- Mechanism: The algorithm uses different step sizes for actor, critic, and target network updates (Assumption 3.5), creating a three-timescale structure that allows the critic to track the optimal Q-value while the actor optimizes the policy.
- Core assumption: Step sizes satisfy αt = Cα/(1+t)σ, βt = Cβ/(1+t)u, γt = Cγ/(1+t)v with 0 < σ < u < v < 1.
- Evidence anchors:
  - [section 4.1] "Asymptotic convergence analysis using ODE-based method for on-policy algorithm"
  - [section 3.5] "αt is at the fastest timescale, βt is at slower timescale and γt is at the slowest timescale"
- Break condition: If step sizes don't satisfy the timescale separation conditions, convergence guarantees break down.

## Foundational Learning

- Concept: Markov Decision Processes and ergodicity
  - Why needed here: The average reward criterion requires the existence of a stationary distribution, which only exists for ergodic Markov processes
  - Quick check question: What condition must hold for a Markov process to have a unique stationary distribution?

- Concept: Bellman equations for average reward
  - Why needed here: The differential Q-value function and Poisson equation are central to the policy gradient derivations
  - Quick check question: How does the Bellman equation for average reward differ from the discounted reward case?

- Concept: Stochastic approximation theory
  - Why needed here: The convergence analysis relies on ODE-based methods and three-timescale stochastic approximation
  - Quick check question: What is the key requirement for a two-timescale stochastic approximation scheme to converge?

## Architecture Onboarding

- Component map:
  Actor network -> Critic network -> Average reward estimator -> Target networks (actor/critic) -> Replay buffer

- Critical path:
  1. Sample batch from replay buffer
  2. Update critic using TD(0) with target networks
  3. Update average reward estimator
  4. Update actor using deterministic policy gradient
  5. Update target networks using polyak averaging

- Design tradeoffs:
  - Three-timescale vs two-timescale: Three-timescale provides stronger convergence guarantees but may converge slower
  - Double Q-learning vs single Q: Double Q reduces overestimation bias but doubles computation
  - Replay buffer size vs memory: Larger buffer provides better sample efficiency but requires more memory

- Failure signatures:
  - Critic divergence: Q-values explode or become NaN
  - Actor collapse: Policy becomes deterministic too quickly or explores poorly
  - Target network lag: Actor and critic move too far ahead of target networks
  - Poor sample efficiency: Performance doesn't improve despite many environment steps

- First 3 experiments:
  1. Run on simple continuous control task (e.g., Pendulum) with small network to verify basic functionality
  2. Test with only on-policy updates (ignore replay buffer) to verify policy gradient implementation
  3. Run with doubled Q-networks but no target networks to observe instability without stabilization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would ARO-DDPG perform in sparse reward environments compared to dense reward environments?
- Basis in paper: [inferred] The paper focuses on MuJoCo environments with dense rewards and does not test sparse reward settings.
- Why unresolved: The algorithm's performance in environments with sparse rewards is not evaluated, which is a common challenge in RL.
- What evidence would resolve it: Empirical results comparing ARO-DDPG's performance in both dense and sparse reward environments.

### Open Question 2
- Question: Can the sample complexity of ARO-DDPG be further improved by using alternative optimization techniques like natural gradient descent?
- Basis in paper: [explicit] The paper mentions this as a future work direction in the conclusion.
- Why unresolved: The authors only implemented standard gradient descent and did not explore more advanced optimization methods.
- What evidence would resolve it: Comparative experiments between ARO-DDPG with standard gradient descent and natural gradient descent.

### Open Question 3
- Question: How does the choice of the behavior policy affect the approximation error in the off-policy setting?
- Basis in paper: [explicit] Theorem 3.4 shows that the approximation error depends on the difference between target and behavior policies.
- Why unresolved: The paper does not provide empirical analysis of how different behavior policies (e.g., random vs. near-optimal) impact performance.
- What evidence would resolve it: Experiments varying the behavior policy while keeping other factors constant and measuring the resulting approximation error.

### Open Question 4
- Question: Would using a multi-timescale update scheme (as opposed to the two-timescale scheme that gave best results) affect the convergence rate?
- Basis in paper: [explicit] The paper mentions that two-timescale performed better than three-timescale but does not explore other configurations.
- Why unresolved: The authors only tested two specific timescale configurations and did not explore the full parameter space.
- What evidence would resolve it: Systematic comparison of different timescale configurations (e.g., varying the relative update frequencies).

## Limitations

- Theoretical convergence guarantees rely on ergodicity assumptions that may not hold in practice for complex continuous control tasks
- Empirical evaluation is limited to relatively simple MuJoCo environments and may not generalize to more complex tasks
- Finite-time analysis shows theoretical existence of ε-optimal policy but doesn't guarantee practical performance or sample efficiency

## Confidence

- **High confidence**: The deterministic policy gradient theorem derivation for average reward is mathematically sound and the ODE-based asymptotic convergence analysis follows established stochastic approximation theory.
- **Medium confidence**: The practical implementation details and hyperparameter choices are sufficient for the algorithm to work, though exact specifications are unclear.
- **Medium confidence**: The empirical results showing performance improvements over baselines, as the methodology is sound but the scope is limited to specific environments.

## Next Checks

1. **Robustness across environment variations**: Test ARO-DDPG on a broader set of continuous control tasks including more challenging ones like Ant and Humanoid from MuJoCo, and assess performance with different reward scales and task complexities.

2. **Ablation study of key components**: Systematically evaluate the contribution of each stabilization mechanism (target networks, double Q-learning, three-timescale updates) by removing them individually and measuring impact on both convergence and final performance.

3. **Comparison with discounted reward baselines**: Include direct comparison with standard DDPG and TD3 using appropriately tuned discount factors to quantify the practical benefit of average reward optimization versus discounted reward approaches in continuing tasks.