---
ver: rpa2
title: 'PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback'
arxiv_id: '2307.14936'
source_url: https://arxiv.org/abs/2307.14936
tags:
- code
- language
- pangu-coder2
- arxiv
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RRTF (Rank Responses to align Test&Teacher
  Feedback), a new training framework for boosting code generation in large language
  models (Code LLMs). The RRTF framework addresses the limitations of existing reinforcement
  learning (RL) approaches, which often rely on complex RL algorithms and time-consuming
  test executions.
---

# PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback

## Quick Facts
- arXiv ID: 2307.14936
- Source URL: https://arxiv.org/abs/2307.14936
- Reference count: 11
- Achieves 62.20% pass@1 on HumanEval, outperforming all previously published Code LLMs

## Executive Summary
PanGu-Coder2 introduces RRTF (Rank Responses to align Test&Teacher Feedback), a novel training framework that replaces complex reinforcement learning with ranking-based alignment for code generation. The approach aligns language models with test results and teacher feedback without requiring expensive test execution during training. By combining ranking loss with cross-entropy fine-tuning, RRTF achieves state-of-the-art performance on major code generation benchmarks while maintaining computational efficiency.

## Method Summary
The RRTF framework operates through three stages: sampling responses from both teacher and student models, ranking these responses based on test results and heuristic preferences, and training using a combined rank loss and cross-entropy loss. Unlike traditional RL approaches, RRTF avoids direct test execution during training by using pass/fail indicators and ranking criteria. The method was applied to StarCoder 15B, using 68K instruction-solution pairs generated via the Evol-Instruct technique, achieving significant performance improvements while reducing inference time and memory usage through quantization.

## Key Results
- Achieves 62.20% pass@1 on HumanEval, setting a new state-of-the-art
- Reaches 38.26% pass@1 on CoderEval, outperforming previous models
- Maintains strong performance on LeetCode benchmarks while improving inference speed and memory efficiency through quantization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ranking-based alignment outperforms traditional RL reward maximization by avoiding noisy test execution feedback.
- Mechanism: Instead of executing tests during training, RRTF ranks candidate code responses based on compilation/runtime success and heuristic preferences, then trains the model to maximize the probability of higher-ranked responses.
- Core assumption: Test execution is expensive and noisy, while ranking based on pass/fail and source attribution provides sufficient signal for model improvement.
- Evidence anchors:
  - [abstract] "Instead, RRTF uses a ranking-based approach to align the model with test results and teacher feedback, making it more efficient and easier to implement."
  - [section] "Regarding the test results as a reward directly provides limited improvements to the base model."

### Mechanism 2
- Claim: Combining teacher and test feedback provides complementary signals that improve code generation quality.
- Mechanism: Teacher models (higher-quality generators) and unit tests provide dual ranking criteria, with teacher-generated responses always preferred when tied, ensuring the student learns from stronger exemplars.
- Core assumption: Teacher models reliably generate higher-quality code than the student, and test pass/fail provides objective correctness signals.
- Evidence anchors:
  - [section] "Meanwhile, we filter out data whose teachers' score is lower than the student model. For two samples that fall into the same situation, we always assign a higher rank to the sample from the teachers, since we prefer the student to learn from the teacher."

### Mechanism 3
- Claim: Rank loss with cross-entropy fine-tuning enables efficient alignment without complex RL.
- Mechanism: The model is trained with a combined loss: rank loss to prefer higher-ranked responses and cross-entropy loss to match teacher outputs, avoiding the instability of PPO while maintaining alignment.
- Core assumption: The combination of ranking feedback and supervised fine-tuning provides sufficient gradient signal for effective alignment.
- Evidence anchors:
  - [abstract] "Instead of aligning the model with human intents, the purpose of code generation is to improve generating correctness, so we replace the H (human) with T, which can be a combination of tests and teachers."
  - [section] "The training procedures of RRTF can be divided into 3 steps: sampling, ranking, and training."

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RRTF is inspired by RLHF but simplifies it for code generation by replacing human feedback with test and teacher feedback.
  - Quick check question: What are the three main steps in RLHF, and how does RRTF modify them?

- Concept: Ranking-based optimization
  - Why needed here: RRTF uses ranking of responses instead of absolute reward values, simplifying training and avoiding test execution costs.
  - Quick check question: How does ranking-based feedback differ from reward-based feedback in terms of training efficiency and signal quality?

- Concept: Evol-Instruct data augmentation
  - Why needed here: RRTF uses Evol-Instruct to generate diverse, high-quality instruction-code pairs for training.
  - Quick check question: What is the purpose of iteratively evolving programming problems in the Evol-Instruct technique?

## Architecture Onboarding

- Component map: Base Code LLM (StarCoder 15B) → RRTF training loop (sampling → ranking → training) → quantized deployment (CTranslate2/GPTQ) → inference.
- Critical path: Sampling teacher/student responses → parallel test execution → ranking assignment → rank + cross-entropy loss computation → parameter update.
- Design tradeoffs: Ranking feedback trades fine-grained reward signals for efficiency; teacher feedback adds bias toward known good solutions.
- Failure signatures: Training instability if rank loss dominates cross-entropy loss; poor performance if teacher models are weak or tests are too simple.
- First 3 experiments:
  1. Verify that teacher models generate consistently better code than the student on a small validation set.
  2. Test the ranking mechanism with synthetic pass/fail labels to ensure correct loss computation.
  3. Train on a small subset of data and evaluate pass@1 on HumanEval to confirm the training pipeline works.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PanGu-Coder2 compare to other models when using different decoding strategies, such as beam search or sampling with temperature?
- Basis in paper: [explicit] The paper mentions using greedy decoding and nucleus sampling with different temperatures, but does not explore other decoding strategies.
- Why unresolved: The paper only reports results using greedy decoding and nucleus sampling with different temperatures. It is unclear how the model would perform with other decoding strategies.
- What evidence would resolve it: Additional experiments comparing the performance of PanGu-Coder2 using different decoding strategies, such as beam search or sampling with different temperatures, would provide insights into the model's behavior and potential improvements.

### Open Question 2
- Question: How does the performance of PanGu-Coder2 change when trained on different datasets, such as those containing more complex or diverse programming problems?
- Basis in paper: [explicit] The paper mentions using the Evol-Instruct technique to construct the training dataset, but does not explore the impact of different datasets on the model's performance.
- Why unresolved: The paper only reports results using the dataset constructed with the Evol-Instruct technique. It is unclear how the model would perform with different datasets, especially those containing more complex or diverse programming problems.
- What evidence would resolve it: Additional experiments training PanGu-Coder2 on different datasets, such as those containing more complex or diverse programming problems, would provide insights into the model's generalization ability and potential improvements.

### Open Question 3
- Question: How does the performance of PanGu-Coder2 change when using different optimization techniques, such as different learning rates or regularization methods?
- Basis in paper: [explicit] The paper mentions using a global batch size of 512 and training for 6 epochs, but does not explore the impact of different optimization techniques on the model's performance.
- Why unresolved: The paper only reports results using a specific set of optimization techniques. It is unclear how the model would perform with different optimization techniques, such as different learning rates or regularization methods.
- What evidence would resolve it: Additional experiments training PanGu-Coder2 using different optimization techniques, such as different learning rates or regularization methods, would provide insights into the model's sensitivity to these techniques and potential improvements.

## Limitations
- The evaluation focuses primarily on benchmark performance without extensive ablation studies to isolate component contributions.
- Reliance on teacher models introduces potential bias if teacher outputs contain systematic errors.
- The paper does not address performance on out-of-distribution code or tasks requiring creative problem-solving.
- Computational requirements for parallel test execution during ranking may limit scalability.

## Confidence

- **High confidence**: The core mechanism of ranking-based alignment replacing traditional RL approaches is well-supported by the theoretical framework and benchmark results.
- **Medium confidence**: The claim that RRTF is more efficient than RL approaches is supported by the design, but actual computational cost comparisons would strengthen this claim.
- **Medium confidence**: The state-of-the-art performance claims are well-demonstrated on the reported benchmarks, but lack of comparisons with concurrent methods may limit generalizability.

## Next Checks

1. **Ablation study**: Conduct controlled experiments removing either the teacher feedback component or the ranking mechanism to quantify their individual contributions to overall performance gains.

2. **Robustness testing**: Evaluate model performance on out-of-distribution code generation tasks and compare against baseline models to assess generalization beyond standard benchmarks.

3. **Computational cost analysis**: Measure and compare the actual training time and resources required for RRTF versus traditional RLHF approaches on the same hardware to validate efficiency claims.