---
ver: rpa2
title: 'Selective Perception: Optimizing State Descriptions with Reinforcement Learning
  for Language Model Actors'
arxiv_id: '2307.11922'
source_url: https://arxiv.org/abs/2307.11922
tags:
- state
- blinder
- task
- descriptions
- actor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BLINDER, a method for automatically selecting
  concise, task-relevant state descriptions for LLM actors. By framing state description
  selection as a reinforcement learning problem, BLINDER learns to prune irrelevant
  information from exhaustive state feature sets, reducing context length by 6x and
  improving task success rates by up to 150%.
---

# Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors

## Quick Facts
- arXiv ID: 2307.11922
- Source URL: https://arxiv.org/abs/2307.11922
- Reference count: 26
- Key outcome: Reduces context length by 6x and improves task success rates by up to 150% for LLM actors through learned state description selection

## Executive Summary
BLINDER is a method that automatically selects concise, task-relevant state descriptions for large language model (LLM) actors. By framing state description selection as a reinforcement learning problem, BLINDER learns to prune irrelevant information from exhaustive state feature sets, significantly improving task success rates while reducing computational costs. The method demonstrates strong generalization across different tasks and LLM architectures, with learned descriptions that can transfer between models. Experiments on NetHack and robotic manipulation tasks show BLINDER outperforms manual and zero-shot baselines.

## Method Summary
BLINDER treats state description selection as a reinforcement learning problem where state descriptions are assembled feature-by-feature. It trains a value function (a finetuned LLM) to estimate the value of partial state descriptions based on expert action likelihood. The method uses expert trajectories to learn which state features are most relevant for task success. During inference, a state description selector uses the learned value function to construct optimal state descriptions for new tasks, which are then provided to the LLM actor for decision-making.

## Key Results
- Reduces context length by 6x compared to using full state descriptions
- Improves task success rates by up to 150% over manual and zero-shot baselines
- Generalizes well to unseen tasks and different LLM actors, including larger models like GPT3
- Learned descriptions are intuitive enough to transfer between different LLM models

## Why This Works (Mechanism)

### Mechanism 1
The value function learns to assign high rewards to state descriptions that maximize the likelihood of the expert action under the LLM actor. The reward function is defined as the expected log-probability of the expert action given a state description and task, directly incentivizing the value function to prefer descriptions that lead to the expert action being more probable. This assumes the expert action is optimal for the given state and task.

### Mechanism 2
The value function generalizes to new tasks and state features by leveraging the semantic understanding from its pre-training on a large text corpus. As a finetuned LLM, it can understand the meaning of state features and tasks, enabling it to generalize to unseen combinations. This assumes the pre-training provides sufficient semantic understanding of state features and tasks.

### Mechanism 3
The state description selection process is framed as a reinforcement learning problem, allowing for efficient exploration of the state space. The MDP formulation with deterministic transitions enables the value function to learn optimal state descriptions through efficient exploration. This assumes the MDP formulation accurately captures the problem of state description selection.

## Foundational Learning

- **Markov Decision Process (MDP)**: The state description selection is framed as an MDP where state is the current description and action is adding a new state feature. Quick check: What are the key components of an MDP, and how are they defined in the context of state description selection?

- **Reinforcement Learning (RL)**: The value function is learned using RL to explore the state space and identify optimal state descriptions. Quick check: What are the key components of an RL algorithm, and how are they applied to learn the value function in this context?

- **Large Language Models (LLMs)**: LLMs serve as both actors for decision-making and as the value function. Understanding LLM capabilities and limitations is crucial. Quick check: What are the key strengths and weaknesses of LLMs for decision-making tasks, and how do they impact BLINDER's performance?

## Architecture Onboarding

- **Component map**: Expert trajectories -> Value function (finetuned LLM) -> State description selector -> LLM actor
- **Critical path**: 1) Collect expert trajectories; 2) Train value function on expert trajectories; 3) Use value function to select state descriptions for new tasks; 4) Evaluate LLM actor performance with selected descriptions
- **Design tradeoffs**: Model size vs. generalization (larger LLMs generalize better but are more expensive); reward function design (impacts learned value function); exploration vs. exploitation (balance new vs. known good descriptions)
- **Failure signatures**: Poor performance on new tasks (value function didn't generalize well); inconsistent state descriptions (value function unstable or selector non-deterministic); high computational cost (LLM or value function too large or selector inefficient)
- **First 3 experiments**: 1) Ablation study on reward function (compare different reward formulations); 2) Analysis of state description diversity across tasks; 3) Evaluation of state description transferability between different LLM actors

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several remain unresolved:

1. How does BLINDER's performance compare when using different value function architectures?
2. Can BLINDER be extended to handle continuous state spaces or high-dimensional observations?
3. How does BLINDER's performance scale with the number of tasks and state features in the training set?
4. How sensitive is BLINDER to hyperparameter choices?
5. Can BLINDER be combined with other techniques like prompt engineering or retrieval methods?

## Limitations

- Generalization claims rely on experiments with only two environments (NetHack and Franka Panda robot), with limited diversity in task types
- Assumes expert actions represent optimal decisions without empirical validation of this assumption
- Transfer results showing descriptions can transfer between models are based on limited comparisons and need broader validation

## Confidence

- **High Confidence**: The core RL formulation and MDP design are sound; experimental setup is clearly described and reproducible; context length reduction claims are directly measurable
- **Medium Confidence**: Task success rate improvements are supported by experimental results, but baseline comparisons could be stronger
- **Low Confidence**: Claims about intuitive transferability of learned descriptions between models need more rigorous validation

## Next Checks

1. **Expert Demonstration Quality Analysis**: Systematically inject varying levels of noise or suboptimality into expert demonstrations and measure how this affects BLINDER's learned state descriptions and downstream task performance.

2. **Cross-Model Transfer Robustness**: Test the transferability of learned descriptions across a wider range of LLM sizes and architectures (multiple T5 sizes, GPT variants, LLaMA models) to determine whether transferability is robust or model-specific.

3. **Baseline Optimization Study**: Implement and compare against stronger baseline approaches for state description selection, such as learned attention mechanisms or explicit feature importance scoring, to better contextualize BLINDER's performance improvements.