---
ver: rpa2
title: Finding Foundation Models for Time Series Classification with a PreText Task
arxiv_id: '2311.14534'
source_url: https://arxiv.org/abs/2311.14534
tags:
- datasets
- time
- series
- task
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the overfitting challenge in deep learning
  for time series classification (TSC), especially in small datasets. The authors
  propose a novel approach using pre-trained domain foundation models via a pretext
  task: training a model to predict which dataset each time series sample originates
  from, creating shared convolution filters across datasets.'
---

# Finding Foundation Models for Time Series Classification with a PreText Task

## Quick Facts
- arXiv ID: 2311.14534
- Source URL: https://arxiv.org/abs/2311.14534
- Reference count: 30
- Primary result: Pre-trained H-InceptionTime (PHIT) model outperforms baseline H-InceptionTime and achieves SOTA performance comparable to non-DL ensembles like HydraMR and HC2 on UCR archive (88 datasets)

## Executive Summary
This paper addresses overfitting in deep learning for time series classification (TSC), particularly on small datasets. The authors propose a novel approach using domain foundation models trained via a pretext task: predicting which dataset each time series sample originates from. This creates shared convolution filters across datasets during multi-dataset pre-training. The method uses a Batch Normalization Multiplexer (BNM) layer to handle different data distributions across datasets. Experiments on the UCR archive show PHIT significantly outperforms the baseline H-InceptionTime and achieves state-of-the-art performance, particularly benefiting datasets with fewer than 1000 training samples.

## Method Summary
The approach uses a two-phase pipeline: first, a pre-trained model (first 3 Inception modules of H-Inception) is trained on a pretext task across multiple datasets to predict dataset origin, creating shared temporal features. The BNM layer routes each sample to dataset-specific batch normalization layers to handle different data distributions. Second, each dataset's classifier is fine-tuned independently by copying the pre-trained model and adding randomly initialized Inception modules. The method is tested on 88 UCR archive datasets grouped into 8 domain types, with performance evaluated using classification accuracy and statistical significance testing.

## Key Results
- PHIT model significantly outperforms baseline H-InceptionTime on UCR archive datasets
- Achieves state-of-the-art performance comparable to non-deep-learning ensembles (HydraMR, HC2)
- Particularly effective for datasets with fewer than 1000 training samples
- Demonstrates knowledge transfer from larger to smaller datasets within the same domain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training on dataset-origin prediction forces the model to learn general time series features that transfer to classification tasks.
- Mechanism: The pretext task (predicting which dataset a sample came from) compels the model to extract common patterns across multiple datasets, creating shared convolution filters that are useful for classification.
- Core assumption: Datasets from the same domain share underlying temporal structures that are useful for classification.
- Evidence anchors:
  - [abstract] "This task is designed to identify the originating dataset of each time series sample, with the goal of creating flexible convolution filters that can be applied across different datasets."
  - [section] "Specifically, the task is to predict the original dataset of each sample."
- Break condition: If datasets are too heterogeneous (different domains), the shared features learned may not transfer to specific classification tasks.

### Mechanism 2
- Claim: The Batch Normalization Multiplexer (BNM) solves the multi-distribution batch normalization problem.
- Mechanism: Instead of using one batch normalization layer for all datasets, BNM routes each sample to the correct dataset-specific batch normalization layer based on dataset metadata.
- Core assumption: Different datasets have different distributions, requiring separate batch normalization statistics.
- Evidence anchors:
  - [section] "The role of the Batch Normalization is to learn how to scale and shift the batch samples in order to get a zero mean and unit variance. This however may be problematic when samples in a same batch are generated from different distributions."
  - [section] "For this reason, while training the pre-trained model on the pretext task, we should define multiple Batch Normalization layers for each dataset to replace the one batch normalization layer usually used in modern CNN architectures."
- Break condition: If dataset distributions are too similar, the overhead of multiple batch normalization layers may not be justified.

### Mechanism 3
- Claim: Larger datasets improve performance on smaller datasets through knowledge transfer.
- Mechanism: The pre-trained model learns robust features from larger datasets, which then provide useful initialization when fine-tuning on smaller datasets, reducing overfitting.
- Core assumption: Features learned from larger datasets are generalizable to smaller datasets within the same domain.
- Evidence anchors:
  - [section] "This work is proposing a pretext task that consists of a pre-trained model to learn features on multiple datasets at the same time. As detailed at the beginning, the purpose of this pretext task is to enhance the performance of deep learners on TSC tasks when the datasets presents very few number of training samples."
  - [section] "On the one hand, in the case of ECG200 (left plot), almost no common areas exist between the filters of the three models. On the other hand, in the case of NonInvasiveFetalECGThorax1 (right plot) there exist many common areas between the filters of different colors."
- Break condition: If the source and target datasets are too dissimilar, knowledge transfer may degrade performance.

## Foundational Learning

- Concept: Domain foundation models
  - Why needed here: TSC suffers from overfitting when training data is scarce; domain foundation models provide pre-trained features that transfer across datasets.
  - Quick check question: Why can't we just use transfer learning from one dataset to another?

- Concept: Multi-dataset pretext tasks
  - Why needed here: Training on multiple datasets simultaneously allows the model to learn general temporal patterns rather than dataset-specific features.
  - Quick check question: What would happen if we combined all datasets and trained on the final classification task directly?

- Concept: Batch normalization in multi-dataset settings
  - Why needed here: Standard batch normalization assumes samples come from the same distribution, which is violated when training on multiple datasets.
  - Quick check question: How does the BNM differ from simply normalizing each dataset separately before training?

## Architecture Onboarding

- Component map:
  H-Inception backbone (6 Inception modules) -> Pre-trained model (first 3 modules + BNM) -> Add-on model (last 3 modules, randomly initialized) -> Dataset routing logic for BNM

- Critical path:
  1. Pre-training phase: Train pre-trained model on pretext task (predict dataset origin)
  2. Fine-tuning phase: Copy pre-trained model, add random add-on model, fine-tune on specific classification task

- Design tradeoffs:
  - Using first 3 Inception modules for pre-training vs. using different number of layers
  - BNM adds complexity but solves distribution mismatch; simpler alternative would be dataset-wise normalization
  - Ensemble of 5 models (as in original H-Inception) vs. single model

- Failure signatures:
  - Poor performance on small datasets: may indicate insufficient knowledge transfer
  - Instability during pre-training: could indicate BNM routing issues or incompatible dataset combinations
  - Fine-tuning degradation: may suggest the pre-trained features are too generic

- First 3 experiments:
  1. Pre-train on ECG datasets only, fine-tune on ECG200 vs. training from scratch
  2. Compare BNM vs. standard batch normalization on multi-dataset pre-training
  3. Test different numbers of pre-trained layers (2 vs. 3 vs. 4 Inception modules)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of varying the number of pre-training layers versus fine-tuning layers in the hybrid model?
- Basis in paper: [inferred] The paper uses a fixed division of the H-Inception architecture (first three modules for pre-training, last three for fine-tuning) without exploring alternative splits.
- Why unresolved: The paper does not experiment with different ratios of pre-training to fine-tuning layers, which could affect performance.
- What evidence would resolve it: Comparative experiments testing various layer splits (e.g., 2-4, 4-2) across multiple dataset types would show optimal layer allocation.

### Open Question 2
- Question: How does the proposed BNM layer affect training stability and convergence speed compared to standard batch normalization?
- Basis in paper: [explicit] The paper introduces BNM to handle multi-dataset training but only mentions it controls the multi-distribution problem without quantitative comparison.
- Why unresolved: No experiments directly compare BNM against standard batch normalization in terms of training metrics or final performance.
- What evidence would resolve it: Ablation studies measuring training time, convergence curves, and final accuracy with/without BNM across diverse datasets.

### Open Question 3
- Question: Would the pretext task remain effective if applied to larger-scale time series datasets beyond the UCR archive?
- Basis in paper: [inferred] The paper demonstrates effectiveness on UCR datasets but doesn't test scalability to larger or more diverse datasets.
- Why unresolved: All experiments are confined to the UCR archive, leaving uncertainty about performance on bigger datasets with more classes and samples.
- What evidence would resolve it: Replication of the methodology on larger benchmark datasets (e.g., UEA archive, real-world industrial datasets) with varying sample sizes.

## Limitations
- Dataset heterogeneity within domains may limit transfer effectiveness
- Approach is tightly coupled with H-Inception architecture
- Multi-dataset pre-training phase requires significant computational investment

## Confidence
- High Confidence: The effectiveness of BNM for handling multi-distribution batch normalization in multi-dataset settings
- Medium Confidence: The claim that pre-training on dataset-origin prediction creates useful general features for classification
- Medium Confidence: The assertion that the method achieves state-of-the-art performance comparable to non-deep-learning ensembles

## Next Checks
1. Test pre-training on progressively larger subsets of datasets to quantify how dataset diversity affects transfer performance, particularly for small target datasets
2. Evaluate performance when pre-training on one domain (e.g., ECG) and fine-tuning on a different domain (e.g., Sensors) to test the limits of domain transfer
3. Implement the pretext task and BNM approach using different backbone architectures (e.g., ResNet, Transformers) to verify the method generalizes beyond H-Inception