---
ver: rpa2
title: 'Deep Learning for Time Series Classification and Extrinsic Regression: A Current
  Survey'
arxiv_id: '2302.02515'
source_url: https://arxiv.org/abs/2302.02515
tags:
- time
- series
- classification
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey reviews deep learning for time series classification
  (TSC) and extrinsic regression (TSER), important tasks in time series analysis where
  deep learning can excel due to its ability to automatically extract relevant features
  from raw data. It categorizes deep learning models into multilayer perceptrons,
  CNNs, RNNs, and attention-based models, discussing their architectures and refinements
  for TSC/TSER.
---

# Deep Learning for Time Series Classification and Extrinsic Regression: A Current Survey

## Quick Facts
- arXiv ID: 2302.02515
- Source URL: https://arxiv.org/abs/2302.02515
- Reference count: 40
- Key outcome: This survey reviews deep learning for time series classification (TSC) and extrinsic regression (TSER), important tasks in time series analysis where deep learning can excel due to its ability to automatically extract relevant features from raw data.

## Executive Summary
This comprehensive survey examines deep learning approaches for time series classification and extrinsic regression, two critical tasks in time series analysis. The authors systematically categorize deep learning models into multilayer perceptrons, CNNs, RNNs, and attention-based models, providing detailed discussions of their architectures and refinements for TSC/TSER applications. The survey focuses on two key domains: human activity recognition (HAR) using wearable sensor data and satellite earth observation (EO) for land cover classification and environmental variable estimation. By reviewing recent developments and challenges in these areas, the paper offers a comprehensive overview of the current state of the art in deep learning for time series analysis.

## Method Summary
The survey employs a comprehensive literature review methodology, examining deep learning approaches across four main architectural categories: MLPs, CNNs, RNNs, and attention-based models. The authors analyze how each architecture can be adapted for TSC and TSER tasks, with particular emphasis on hybrid models that combine multiple architectural approaches. The review focuses on practical applications in HAR (using wearable sensor data) and satellite EO (using time series of satellite imagery), examining how different deep learning architectures perform across these domains. The methodology includes evaluation of recent developments, challenges, and emerging trends in the field.

## Key Results
- Deep learning models can automatically extract relevant features from raw time series data, eliminating the need for manual feature engineering
- Attention-based models show superior ability to capture long-range dependencies in time series compared to CNNs or RNNs alone
- Hybrid CNN-RNN architectures leverage complementary strengths of spatial and temporal feature extraction, improving classification performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention-based models capture long-range dependencies in time series better than CNNs or RNNs alone.
- Mechanism: Self-attention computes weighted representations of each time step by comparing it to all other time steps, allowing the model to focus on relevant temporal patterns regardless of distance.
- Core assumption: Long-range temporal dependencies exist and are important for TSC/TSER tasks.
- Evidence anchors:
  - [abstract] "attention models can capture long-range dependencies, and their broader receptive fields provide more contextual information"
  - [section] "attention mechanism aims to enhance a network's representation ability by focusing on essential features and suppressing unnecessary ones"
- Break Condition: If time series patterns are purely local or very short-range, the added computational cost of attention may not be justified.

### Mechanism 2
- Claim: Multiscale convolutional operations extract hierarchical features at different temporal resolutions, improving classification accuracy.
- Mechanism: Applying convolutions with varying kernel sizes captures both fine-grained and coarse-grained temporal patterns in parallel, then combines them for richer feature representations.
- Core assumption: Time series data contains patterns at multiple temporal scales that are relevant for classification.
- Evidence anchors:
  - [section] "Multi-scale Convolutional Neural Networks (MCNN) [81] and Time LeNet (t-LeNet [82]) were considered the first models that preprocess the input series to apply convolution on multi-scale series"
  - [corpus] Weak evidence for this mechanism in corpus - only general mention of "deep learning for time series analysis" without specifics.
- Break Condition: If the dataset only contains patterns at a single temporal scale, multiscale convolutions add unnecessary complexity.

### Mechanism 3
- Claim: Hybrid CNN-RNN architectures leverage the complementary strengths of spatial and temporal feature extraction.
- Mechanism: CNNs extract local spatial features from each time step, while RNNs model the temporal dependencies between extracted features, resulting in improved classification performance.
- Core assumption: Both spatial (within each time step) and temporal (across time steps) patterns are important for the task.
- Evidence anchors:
  - [section] "Many recent studies have focussed on hybrid models, combining both CNNs and RNNs... Combining the strengths of CNNs and RNNs makes it possible to learn spatial and temporal features"
  - [corpus] No direct evidence in corpus for this specific mechanism.
- Break Condition: If either spatial or temporal information alone is sufficient for the task, the hybrid approach adds unnecessary complexity.

## Foundational Learning

- Concept: Understanding of time series data structure (univariate vs multivariate, fixed vs variable length)
  - Why needed here: Different architectures handle these variations differently; choosing appropriate models depends on data characteristics.
  - Quick check question: Can your time series have varying lengths, and if so, how will your model handle padding or truncation?

- Concept: Knowledge of convolution operations and receptive fields
  - Why needed here: CNNs rely on convolution kernels to extract features; understanding kernel size and dilation is crucial for designing effective architectures.
  - Quick check question: What is the receptive field size of a 1D convolution with kernel size k and stride s after n layers?

- Concept: Understanding of sequence modeling and temporal dependencies
  - Why needed here: RNNs and attention mechanisms are designed to capture temporal relationships; knowing when these are necessary vs. when CNNs suffice is important.
  - Quick check question: When would you choose an RNN over a CNN for time series classification?

## Architecture Onboarding

- Component map: Input → Feature Extractor (CNN/RNN/Transformer) → Attention Module (optional) → Classifier/Regressor → Output
- Critical path: Data preprocessing → Model training → Evaluation → Hyperparameter tuning
- Design tradeoffs:
  - CNN: Fast, good for local patterns, limited long-range dependency capture
  - RNN: Good for temporal dependencies, slower, struggles with very long sequences
  - Transformer: Excellent for long-range dependencies, computationally expensive
  - Attention: Can be added to any architecture, improves performance but adds complexity
- Failure signatures:
  - Overfitting: High training accuracy but poor validation/test performance
  - Underfitting: Poor performance on both training and validation sets
  - Vanishing/exploding gradients: Especially in deep RNNs
  - Inefficient learning: Very slow convergence despite proper hyperparameter settings
- First 3 experiments:
  1. Baseline CNN with 1D convolutions and global average pooling
  2. CNN + LSTM hybrid with attention layer after LSTM
  3. Pure transformer architecture with positional encoding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the key architectural features that make InceptionTime consistently outperform ResNet on time series classification tasks?
- Basis in paper: [explicit] The paper states that InceptionTime explores much larger filters than any previously proposed network for TSC and consistently outperforms ResNet.
- Why unresolved: While the paper mentions that InceptionTime uses larger filters and residual connections, it doesn't provide a detailed analysis of why these specific architectural choices lead to better performance. A deeper investigation into the role of filter sizes, residual connections, and ensemble methods in InceptionTime's success is needed.
- What evidence would resolve it: Comparative studies isolating the impact of different architectural components (e.g., filter sizes, residual connections) in InceptionTime and ResNet on various time series classification datasets.

### Open Question 2
- Question: How do attention-based models compare to traditional deep learning models (CNNs, RNNs) in terms of capturing long-range dependencies in time series data?
- Basis in paper: [explicit] The paper discusses the success of attention models in NLP and their recent application to time series classification, highlighting their ability to capture long-range dependencies.
- Why unresolved: While the paper mentions the potential of attention models for capturing long-range dependencies, it doesn't provide a comprehensive comparison of their performance against traditional models on various time series tasks. A systematic evaluation of attention models' strengths and weaknesses in different scenarios is needed.
- What evidence would resolve it: Extensive benchmarking studies comparing attention models to CNNs, RNNs, and other deep learning architectures on diverse time series datasets, evaluating their performance on tasks requiring long-range dependencies.

### Open Question 3
- Question: What are the most effective strategies for handling missing or irregularly sampled data in time series classification and regression tasks using deep learning?
- Basis in paper: [inferred] The paper doesn't explicitly discuss this issue, but it's a common challenge in time series analysis. The success of deep learning models often relies on regular, complete data, which is not always available in real-world scenarios.
- Why unresolved: The paper doesn't address the problem of missing or irregularly sampled data, which is a significant challenge in many time series applications. Developing robust deep learning models that can handle such data effectively is an important open research question.
- What evidence would resolve it: Studies comparing different strategies for handling missing data (e.g., imputation, masking, attention mechanisms) in deep learning models for time series tasks, evaluating their performance on datasets with varying levels of missing data.

## Limitations
- The survey's broad scope across multiple architectures and applications may sacrifice depth for breadth
- Performance evaluations are often dataset-dependent, making generalization claims challenging
- The rapidly evolving nature of deep learning means some newer techniques may not be fully captured

## Confidence
- High Confidence: Claims about the general effectiveness of deep learning for TSC/TSER tasks
- Medium Confidence: Specific performance comparisons between architectures
- Low Confidence: Predictions about future trends and emerging architectures

## Next Checks
1. **Architecture-Specific Validation**: Implement and test the three core architectures (CNN-only, RNN-only, and hybrid CNN-RNN) on a standard time series dataset to verify the claimed performance differences and failure modes.
2. **Cross-Dataset Generalization**: Evaluate the same model architecture across multiple datasets (e.g., HAR datasets with different sensor configurations and EO datasets with varying temporal resolutions) to assess generalizability claims.
3. **Attention Mechanism Impact**: Conduct ablation studies to quantify the actual contribution of attention mechanisms in hybrid architectures, comparing models with and without attention layers while controlling for other variables.