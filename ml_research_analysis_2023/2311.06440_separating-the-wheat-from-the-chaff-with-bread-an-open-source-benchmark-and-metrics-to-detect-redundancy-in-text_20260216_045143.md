---
ver: rpa2
title: 'Separating the Wheat from the Chaff with BREAD: An open-source benchmark and
  metrics to detect redundancy in text'
arxiv_id: '2311.06440'
source_url: https://arxiv.org/abs/2311.06440
tags:
- data
- metrics
- bread
- distribution
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BREAD is the first multilingual benchmark for detecting repetitive
  boilerplate vs natural text. It provides human-labeled annotations across 360 languages,
  enabling evaluation of language-agnostic filtering methods.
---

# Separating the Wheat from the Chaff with BREAD: An open-source benchmark and metrics to detect redundancy in text

## Quick Facts
- arXiv ID: 2311.06440
- Source URL: https://arxiv.org/abs/2311.06440
- Reference count: 32
- Key outcome: BREAD provides human-labeled annotations across 360 languages for evaluating language-agnostic filtering methods

## Executive Summary
BREAD is the first multilingual benchmark for detecting repetitive boilerplate vs natural text. It provides human-labeled annotations across 360 languages, enabling evaluation of language-agnostic filtering methods. The paper introduces CRED scores—simple ngram-based metrics that operate on character ngram frequency distributions. Three metrics are explored: TTR (type-token ratio), moment score (measuring distribution peakiness), and Zipfianness (measuring distance from natural language distribution). When evaluated on BREAD, the Zipfianness metric achieves 85 F1 for detecting noise+boilerplate and 94 F1 for detecting repetition alone, outperforming token-based approaches. These interpretable, fast metrics show strong agreement with human judgments and are released as reference implementations. The work addresses data quality challenges in low-resource languages by providing both evaluation data and practical filtering tools based on surface-level ngram statistics.

## Method Summary
The paper introduces CRED scores—character n-gram based metrics that detect repetitive text through statistical properties of n-gram frequency distributions. The approach operates purely on character n-grams to remain language-agnostic, avoiding language-specific features or neural methods that require training data. Three metrics are developed: TTR measures lexical diversity, moment score captures distribution peakiness, and Zipfianness quantifies deviation from natural language's power-law distribution. The metrics use Laplace smoothing and length normalization to handle short texts. A grid search over hyperparameters (n-gram length, smoothing, nonlinearities) optimizes performance on the BREAD tuning set. The evaluation shows Zipfianness achieves 85 F1 for noise+boilerplate detection and 94 F1 for repetition detection, outperforming token-based approaches while remaining interpretable and fast.

## Key Results
- Zipfianness metric achieves 85 F1 for detecting noise+boilerplate and 94 F1 for detecting repetition alone
- Surface-level ngram statistics outperform token-based approaches for repetition detection
- Language-agnostic metrics show strong agreement with human judgments across 360 languages
- Simple, interpretable metrics provide practical filtering tools for low-resource language data quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Character n-gram frequency distributions differ systematically between natural text and repetitive boilerplate, enabling detection through simple statistical metrics.
- Mechanism: Repetitive text exhibits lower type-token ratios, more peaked frequency distributions, and deviations from Zipfian patterns compared to natural language. These differences are captured by three ngram-based metrics: TTR measures lexical diversity, moment score captures distribution peakiness, and Zipfianness quantifies distance from natural language's power-law distribution.
- Core assumption: The statistical properties of character n-grams in repetitive text are sufficiently distinct from natural language across diverse languages to enable reliable detection.
- Evidence anchors:
  - [abstract] "Three metrics are explored: TTR (type-token ratio), moment score (measuring distribution peakiness), and Zipfianness (measuring distance from natural language distribution)."
  - [section] "The difference between a paragraph of natural text and a long, repetitive list does not depend on the source language or the particular thing that is repeating."
  - [corpus] Weak evidence - corpus contains no direct frequency distribution comparisons across languages.
- Break condition: When text exhibits Zipfian distribution despite being repetitive (e.g., highly structured repetitive content that mimics natural language statistics), or when non-repetitive text has artificially low type-token ratios.

### Mechanism 2
- Claim: Language-agnostic detection is possible by ignoring textual features and operating purely on n-gram frequency distributions.
- Mechanism: Since repetitive patterns manifest similarly across languages in n-gram frequency space, metrics can be constructed without language-specific knowledge. The approach leverages the universality of repetition effects on frequency distributions rather than linguistic features.
- Core assumption: Repetitive patterns create statistically detectable signatures in n-gram distributions that transcend language boundaries.
- Evidence anchors:
  - [abstract] "We only consider metrics that are language-agnostic (meaning their performance doesn't depend on any particular language)."
  - [section] "The difference between a paragraph of natural text and a long, repetitive list does not depend on the source language or the particular thing that is repeating."
  - [corpus] Weak evidence - corpus contains no cross-linguistic validation studies.
- Break condition: When repetitive content follows language-specific patterns that don't manifest in universal frequency distributions, or when non-repetitive content has similar frequency signatures.

### Mechanism 3
- Claim: Simple, interpretable metrics can achieve high performance for noise detection, making them preferable to complex neural approaches for low-resource languages.
- Mechanism: Surface-level n-gram statistics provide sufficient signal for distinguishing repetitive from natural text, while avoiding the data requirements, interpretability issues, and potential biases of neural methods.
- Core assumption: The signal-to-noise ratio in n-gram frequency distributions is high enough that complex models are unnecessary.
- Evidence anchors:
  - [abstract] "we do not consider neural methods for the baselines... they rely on high-quality training data, and are therefore less reliable for low-resource languages."
  - [section] "neural metrics or complex ensembles... 1) are harder to interpret; 2) may filter on artifacts like domain, rather than quality; 3) will tend only to work for languages they have explicitly been trained on."
  - [corpus] Weak evidence - corpus contains no direct comparison of neural vs. non-neural approaches.
- Break condition: When noise patterns are too subtle to be captured by surface-level statistics, or when domain-specific filtering is actually beneficial.

## Foundational Learning

- Concept: N-gram frequency distributions and their statistical properties
  - Why needed here: The entire approach relies on analyzing how character n-grams are distributed in text to detect repetition and noise.
  - Quick check question: Given a text with 1000 characters containing 200 unique character 4-grams, what is the type-token ratio for 4-grams?

- Concept: Type-token ratio and its relationship to text diversity
  - Why needed here: TTR is one of the three core metrics, and understanding how it relates to text repetitiveness is fundamental to the approach.
  - Quick check question: If a document has 1000 characters and 800 unique character 3-grams, what is its 3-gram TTR?

- Concept: Power-law distributions and Zipf's law in natural language
  - Why needed here: The Zipfianness metric directly measures deviation from natural language's characteristic frequency distribution.
  - Quick check question: In a Zipfian distribution, if the most frequent word appears 1000 times, approximately how many times would the 10th most frequent word appear?

## Architecture Onboarding

- Component map:
  Data ingestion pipeline → Character n-gram extraction → Frequency distribution calculation → Metric computation (TTR, moment, Zipfianness) → Threshold-based classification → Output scoring
  Evaluation framework: BREAD benchmark → Grid search over hyperparameters → Performance measurement (F1 score) → Model selection

- Critical path:
  1. Extract character n-grams of specified lengths from input text
  2. Compute smoothed frequency distributions with Laplace smoothing
  3. Calculate metric scores (TTR, moment, Zipfianness) with length normalization
  4. Apply threshold from grid search to classify as repetitive/noisy vs. natural
  5. Return interpretable score and classification

- Design tradeoffs:
  - Character vs. token n-grams: Character n-grams are more language-agnostic but may capture less semantic information
  - N-gram length: Longer n-grams capture more specific patterns but require more data and may overfit
  - Smoothing parameters: More smoothing increases robustness but may reduce sensitivity to actual repetition
  - Ensemble vs. single metric: Ensembles slightly improve performance but add complexity

- Failure signatures:
  - False positives: High scores on non-repetitive but low-diversity text (e.g., technical documentation with limited vocabulary)
  - False negatives: Low scores on repetitive text that follows natural language statistics (e.g., repetitive marketing copy with varied vocabulary)
  - Performance degradation: Poor results on very short texts where frequency distributions are unreliable
  - Language-specific failures: Unexpected behavior on languages with non-standard character sets or writing systems

- First 3 experiments:
  1. Run all three metrics (TTR, moment, Zipfianness) on a small sample of BREAD test data with varying n-gram lengths to observe score distributions
  2. Compare metric performance on the BREAD-REPEAT vs. BREAD-NOISY splits to understand their different sensitivities
  3. Test length normalization by running metrics on texts of varying lengths but similar content to verify the normalization works correctly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Zipfianness metric generalize well to non-Latin scripts like Chinese, Japanese, or Arabic?
- Basis in paper: [inferred] The paper notes that non-Latin scripts may exhibit unique forms of noise or token distributions, and the authors caution that these metrics may not generalize well to all languages.
- Why unresolved: The BREAD benchmark primarily covers Latin-script languages, and the authors acknowledge that languages with distinct character sets may behave differently.
- What evidence would resolve it: Testing the Zipfianness metric on corpora of Chinese, Japanese, and Arabic text with known noise patterns would show whether the metric's performance degrades for non-Latin scripts.

### Open Question 2
- Question: What is the optimal n-gram length for the Zipfianness metric across different language families?
- Basis in paper: [explicit] The paper found that for Zipfianness, the peak performance was at 4-grams and 5-grams, but notes this may vary by language family.
- Why unresolved: The paper only tested on a mix of languages and found 4-5 grams optimal, but didn't systematically test across different language families.
- What evidence would resolve it: A controlled study testing Zipfianness across different language families (e.g., Indo-European, Sino-Tibetan, Afro-Asiatic) with varying n-gram lengths would identify optimal settings for each family.

### Open Question 3
- Question: How do these surface-level metrics compare to neural approaches for data quality assessment in low-resource languages?
- Basis in paper: [explicit] The paper deliberately excludes neural methods because they require high-quality training data and are less reliable for low-resource languages, but doesn't compare their effectiveness.
- Why unresolved: The authors chose not to include neural baselines due to data requirements, but this leaves an open question about whether neural approaches could outperform these interpretable metrics when sufficient data exists.
- What evidence would resolve it: A head-to-head comparison of surface-level metrics versus neural approaches (like those used in BICLEANER) on the same low-resource language corpora would reveal whether the trade-off in interpretability is worth the potential performance gain.

## Limitations

- Dataset composition limits generalization claims: Only 4-5 documents per language across 360 languages, with evaluation primarily on 49 highest-resource languages
- Non-Latin script performance unknown: The approach may not generalize well to languages with non-standard writing systems or character sets
- All repetition treated equally: Cannot distinguish between harmful repetitive content and acceptable repetition for downstream tasks

## Confidence

**High confidence**: The Zipfianness metric achieves strong performance on the BREAD benchmark (85-94 F1) and shows reasonable correlation with human judgments. The method's interpretability and language-agnostic design are well-demonstrated.

**Medium confidence**: Claims about low-resource language applicability are supported by validation on MADLAD-400 but face limitations due to dataset composition. The assertion that neural methods are less reliable for low-resource languages is reasonable but not directly tested against these specific metrics.

**Low confidence**: The claim that these metrics will work effectively across all 7000+ languages is not substantiated by the evaluation data, which only covers 49 languages with sufficient representation. The assumption that all repetitive content should be filtered may not hold for all downstream applications.

## Next Checks

1. **Cross-linguistic robustness test**: Apply the Zipfianness metric to a diverse sample of 20-30 languages with varying writing systems (including non-Latin scripts) and document types to assess performance beyond the BREAD benchmark languages.

2. **Noise type discrimination**: Design an experiment to test whether the metrics can distinguish between harmful repetitive content (spam, boilerplate) and acceptable repetition (legal disclaimers, technical specifications) to validate the assumption that all repetition should be filtered.

3. **Low-resource adaptation**: Test the metrics on truly low-resource languages (with <100 training documents) from the BREAD dataset to evaluate whether the strong performance on MADLAD-400's validation set translates to the extreme low-resource setting the paper targets.