---
ver: rpa2
title: Communication-Efficient Federated Learning via Regularized Sparse Random Networks
arxiv_id: '2309.10834'
source_url: https://arxiv.org/abs/2309.10834
tags:
- communication
- mask
- regularization
- networks
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses communication inefficiency in federated learning
  by training sparse random networks, where a binary mask is optimized instead of
  model weights, reducing communication to 1 bit per parameter. The core idea is adding
  a regularization term to the loss function that acts as a proxy for the entropy
  of transmitted masks, encouraging sparser sub-networks and eliminating redundant
  features.
---

# Communication-Efficient Federated Learning via Regularized Sparse Random Networks

## Quick Facts
- arXiv ID: 2309.10834
- Source URL: https://arxiv.org/abs/2309.10834
- Reference count: 15
- One-line primary result: Achieves up to 5× reduction in communication and memory costs compared to FedPM while maintaining accuracy

## Executive Summary
This paper addresses communication inefficiency in federated learning by training sparse random networks, where a binary mask is optimized instead of model weights, reducing communication to 1 bit per parameter. The core idea is adding a regularization term to the loss function that acts as a proxy for the entropy of transmitted masks, encouraging sparser sub-networks and eliminating redundant features. Experiments show significant improvements: up to 5× reduction in communication and memory costs compared to state-of-the-art FedPM, with minimal loss in accuracy—achieving 0.31-0.52 bpp gains on CIFAR datasets and 0.8 bpp on MNIST, while maintaining similar generalization performance.

## Method Summary
The method optimizes binary masks to characterize sparse sub-networks within an over-parameterized random network with fixed weights. Instead of transmitting model weights, devices transmit binary masks sampled from probability masks. A regularization term is added to the local loss function to encourage sparsity by penalizing the sum of sigmoid-transformed mask scores. The training proceeds with local optimization of mask scores using SGD with the regularized loss, followed by aggregation of binary masks to form a global probability mask. This approach reduces communication overhead to 1 bit per parameter while maintaining accuracy through careful regularization.

## Key Results
- Up to 5× reduction in communication and memory costs compared to FedPM
- Achieved 0.31-0.52 bits-per-parameter (bpp) gains on CIFAR datasets
- Maintained 0.8 bpp improvement on MNIST while preserving accuracy
- Demonstrated effectiveness across 4Conv, 6Conv, and 10Conv architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Regularization term in loss function encourages sparser sub-networks by penalizing the number of active weights.
- Mechanism: The regularization term adds a cost proportional to the sum of sigmoid-transformed scores, which biases the optimization toward deactivating weights with minimal impact on loss.
- Core assumption: Sigmoid transformation provides a smooth approximation of binary mask selection, enabling gradient-based optimization.
- Evidence anchors:
  - [abstract]: "we propose adding a regularization term to local objectives that encourages sparser solutions by eliminating redundant features across sub-networks."
  - [section III.B]: "our loss function imposes unstructured sparsity on the sub-networks independently discovered by each individual device, by accounting to the normalized average number of chosen parameters within the original over-parameterized network through a regularization term."
  - [corpus]: Weak or missing - no direct corpus support found.
- Break condition: If the regularization strength λ is too high, it may overly constrain the search space and prevent finding sufficiently accurate sub-networks.

### Mechanism 2
- Claim: Stochastic sub-network sampling in FedPM leads to redundant optimization of many sub-networks, increasing communication overhead.
- Mechanism: Each local iteration samples a new sub-network from the probability mask, and gradients are computed for that specific instance. This implicit averaging over many distinct sub-networks results in higher entropy of the final mask.
- Core assumption: The over-parameterized random network contains many sub-networks with similar generalization performance, making the sampling process redundant.
- Evidence anchors:
  - [section III.A.2]: "the local stochastic sub-network sampling step designed in FedPM implicitly promotes the minimization of the average loss of sub-networks sampled from the probability mask at each device"
  - [section III.B]: "The regularization term is introduced aiming at expediting the deactivation of weights with minimal impact on the current sampled network."
  - [corpus]: Weak or missing - no direct corpus support found.
- Break condition: If the number of local iterations is too small, the averaging effect may be insufficient to observe the redundancy.

### Mechanism 3
- Claim: Adding regularization narrows the search space for sub-networks, leading to more consistent and sparse solutions across devices.
- Mechanism: The regularization term constrains the optimization to favor solutions with fewer active weights, reducing the effective size of the search space. This leads to more similar sub-networks being found by different devices.
- Core assumption: There exists a sparse sub-network within the over-parameterized network that achieves comparable accuracy to the full network.
- Evidence anchors:
  - [section III.A.1]: "the objective of finding a sub-network within an over-parameterized random network by optimizing a mask via SGD using consistent loss functions [...] is not synonymous to solving (9), but equivalent to solving : min_S |sum(X∈S) X - wt|"
  - [section III.B]: "The training proceeds until a probability mask is found that can produce sub-networks with sparsity guided by the parameter λ, thereby ensuring both communication and memory efficiency, alongside achieving good generalization performance."
  - [corpus]: Weak or missing - no direct corpus support found.
- Break condition: If the over-parameterized network is not sufficiently large, there may not exist a sparse sub-network that achieves the desired accuracy.

## Foundational Learning

- Concept: Over-parameterized random networks and the lottery ticket hypothesis
  - Why needed here: The paper builds on the observation that sparse sub-networks can be found within over-parameterized random networks that generalize as well as smaller target networks.
  - Quick check question: What is the key insight from the lottery ticket hypothesis that this paper leverages?

- Concept: Stochastic gradient descent with binary masks
  - Why needed here: The paper uses SGD to optimize binary masks that characterize sparse sub-networks, rather than optimizing the weights directly.
  - Quick check question: How does the use of binary masks reduce communication overhead compared to traditional federated learning?

- Concept: Regularization in optimization
  - Why needed here: The paper introduces a regularization term to encourage sparsity in the sub-networks found, addressing a limitation of previous methods.
  - Quick check question: What is the role of the regularization parameter λ in the proposed loss function?

## Architecture Onboarding

- Component map:
  Over-parameterized random network -> Binary masks -> Local optimization with regularized loss -> Global probability mask aggregation

- Critical path:
  1. Initialize over-parameterized random network
  2. Initialize global probability mask (θ = 0.5)
  3. For each round:
     a. Send θ to devices
     b. For each device:
        i. Sample binary mask from θ
        ii. Optimize mask scores using SGD with regularized loss
        iii. Sample final binary mask
        iv. Send binary mask to server
     c. Aggregate binary masks to update global probability mask

- Design tradeoffs:
  - Regularization strength (λ) vs. sparsity vs. accuracy
  - Number of local iterations vs. redundancy in sub-network sampling
  - Size of over-parameterized network vs. likelihood of finding sparse sub-networks

- Failure signatures:
  - If accuracy degrades significantly with increased sparsity, the regularization may be too strong or the network not sufficiently over-parameterized.
  - If communication efficiency does not improve, the found sub-networks may not be sparse enough.

- First 3 experiments:
  1. Verify that the regularization term effectively reduces the entropy of the binary masks.
  2. Compare the accuracy-sparsity tradeoff curve for different values of λ.
  3. Test the algorithm on non-IID data distributions to assess robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical upper bound on sparsity achievable through regularization in sparse random networks for federated learning?
- Basis in paper: [explicit] The paper demonstrates empirical improvements in sparsity but does not establish theoretical limits.
- Why unresolved: The paper focuses on empirical validation rather than theoretical analysis of the maximum sparsity achievable.
- What evidence would resolve it: A formal proof or theoretical analysis showing the maximum achievable sparsity under the regularization framework.

### Open Question 2
- Question: How does the choice of regularization parameter λ affect the convergence rate of federated learning with sparse random networks?
- Basis in paper: [explicit] The paper shows λ affects sparsity and accuracy trade-off but doesn't analyze its impact on convergence speed.
- Why unresolved: The paper only examines the final accuracy and communication efficiency, not the training dynamics.
- What evidence would resolve it: Empirical studies comparing convergence rates across different λ values and theoretical analysis of the convergence properties.

### Open Question 3
- Question: Can the regularization approach be extended to non-IID scenarios with more severe data heterogeneity than tested?
- Basis in paper: [explicit] The paper tests non-IID settings with limited class heterogeneity (c = 2, 4) but doesn't explore more extreme cases.
- Why unresolved: The paper's non-IID experiments are limited in scope and don't test the method's robustness to severe data distribution skew.
- What evidence would resolve it: Experiments with more extreme non-IID settings (e.g., each client has completely different classes) and analysis of performance degradation.

## Limitations

- The paper's claims about communication efficiency improvements rely heavily on the assumption that over-parameterized random networks contain sufficiently sparse sub-networks that generalize well, without direct validation.
- Analysis focuses on bits-per-parameter reduction without explicitly quantifying total communication cost across all devices and rounds.
- The performance under highly non-IID data distributions is only briefly mentioned without detailed analysis.

## Confidence

**High Confidence**: The empirical results demonstrating reduced bpp (0.31-0.52 on CIFAR datasets, 0.8 on MNIST) and maintained accuracy are well-supported by the experiments.

**Medium Confidence**: The mechanism explanation regarding regularization encouraging sparsity is logically consistent but lacks direct empirical validation.

**Low Confidence**: The paper does not provide theoretical guarantees about the relationship between regularization strength (λ) and the trade-off between sparsity and accuracy.

## Next Checks

1. **Entropy Measurement**: Empirically measure the entropy of binary masks before and after applying the regularization term across different values of λ to directly validate the sparsity-inducing effect.

2. **Communication Cost Analysis**: Calculate the total communication cost (in bits) across all devices and rounds for both the proposed method and FedPM, considering the number of devices, rounds, and network size to provide a complete efficiency comparison.

3. **Non-IID Robustness**: Conduct extensive experiments on highly skewed non-IID data distributions to evaluate the algorithm's robustness and identify potential failure modes in realistic federated learning scenarios.