---
ver: rpa2
title: 'InterPrompt: Interpretable Prompting for Interrelated Interpersonal Risk Factors
  in Reddit Posts'
arxiv_id: '2311.12404'
source_url: https://arxiv.org/abs/2311.12404
tags:
- gpt-3
- fine-tuned
- mental
- health
- cues
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses early detection of mental health disorders
  by identifying interpersonal risk factors (Thwarted Belongingness and Perceived
  Burdensomeness) in Reddit posts. The authors introduce InterPrompt, a method that
  fine-tunes GPT-3 models with task-specific context to enhance interpretability and
  system-level explainability.
---

# InterPrompt: Interpretable Prompting for Interrelated Interpersonal Risk Factors in Reddit Posts

## Quick Facts
- arXiv ID: 2311.12404
- Source URL: https://arxiv.org/abs/2311.12404
- Reference count: 31
- One-line primary result: InterPrompt fine-tuned GPT-3 models achieve 87% accuracy in detecting interpersonal risk factors with explanation quality scores up to 79.89% ROUGE-1 and 78.46% BLEU

## Executive Summary
This paper addresses early detection of mental health disorders by identifying interpersonal risk factors (Thwarted Belongingness and Perceived Burdensomeness) in Reddit posts. The authors introduce InterPrompt, a method that fine-tunes GPT-3 models with task-specific context to enhance interpretability and system-level explainability. Their approach combines multi-task prompting with story completion to capture textual cues representing both IRFs. Experimental results show that InterPrompt-driven fine-tuned GPT-3 variants significantly outperform baseline methods, achieving up to 87% accuracy in classification and generating explanations with ROUGE-1 scores up to 79.89% and BLEU scores up to 78.46%. The work demonstrates improved performance in both classification and explanation generation, highlighting the importance of task-specific fine-tuning for mental health analysis.

## Method Summary
The InterPrompt method fine-tunes GPT-3 models using task-specific context to detect Thwarted Belongingness and Perceived Burdensomeness in Reddit posts. The approach constructs multi-task prompts combining labels and textual cues, then fine-tunes GPT-3 using a combined loss function balancing entity extraction and text generation. The method leverages four GPT-3 variants (ada, babbage, curie, davinci) trained on OpenAI servers with the IRF dataset containing 3,522 Reddit posts. The combined loss function incorporates three objectives: predicting TBe/PBu labels, extracting textual cues for TBe, and extracting textual cues for PBu.

## Key Results
- InterPrompt-driven fine-tuned GPT-3 models achieve up to 87% accuracy in classifying interpersonal risk factors
- Generated explanations achieve ROUGE-1 scores up to 79.89% and BLEU scores up to 78.46%
- N-shot learning approaches yield near-chance accuracy (40-60%), demonstrating the need for task-specific fine-tuning
- Fine-tuned GPT-3 models significantly outperform baseline methods in both classification and explanation generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: InterPrompt fine-tuning adjusts pre-trained weights to capture task-specific context, improving classification and explanation generation for IRFs.
- Mechanism: InterPrompt constructs multi-task prompts combining labels and textual cues, then fine-tunes GPT-3 using a combined loss function balancing entity extraction and text generation.
- Core assumption: GPT-3 can learn interconnected IRF patterns when trained on task-specific context with balanced loss weights.
- Evidence anchors:
  - [abstract]: "InterPrompt method to boost the attention mechanism by fine-tuning the GPT-3 model... Our model learns to detect usual patterns and underlying connections across both the IRFs"
  - [section]: "InterPrompt... effectively incorporates task-specific context to enhance the interpretability and System-level explainabilty of the model"
  - [corpus]: Weak evidence - no direct citation of this mechanism in neighbor papers
- Break condition: If loss weights are imbalanced or prompts don't capture IRF interconnections, model performance degrades significantly.

### Mechanism 2
- Claim: N-shot learning is inefficient for mental health analysis due to contextual sensitivity of IRFs.
- Mechanism: The paper demonstrates that zero-shot, one-shot, and few-shot prompting yield near-chance accuracy (40-60%), necessitating task-specific fine-tuning.
- Core assumption: IRFs require nuanced understanding of textual cues that cannot be captured through few examples alone.
- Evidence anchors:
  - [section]: "The decline in performance observed with N-shot learning can be attributed to the contextual sensitivity, underscoring the importance of task-specific fine-tuning"
  - [section]: "accuracy of GPT-3 classifiers for TBe and PBu tasks falls within the range of near chance, typically around 40% to 60%"
  - [corpus]: No direct evidence in neighbor papers about N-shot learning inefficiency
- Break condition: If IRFs were less context-dependent or more explicit in text, N-shot learning might become viable.

### Mechanism 3
- Claim: Combined loss function balancing classification and explanation generation improves both tasks simultaneously.
- Mechanism: The model optimizes for three objectives - predicting labels (TBe/PBu), extracting textual cues for TBe, and extracting textual cues for PBu - using weighted loss components.
- Core assumption: Joint optimization of classification and explanation tasks leads to better performance than sequential or separate training.
- Evidence anchors:
  - [section]: "combined loss function that incorporates both the structured extraction of entities and the generation of coherent text"
  - [section]: "ð¿ð‘ð‘œð‘šð‘ð‘–ð‘›ð‘’ð‘‘ = ðœ†1 Â·ð¿1 + ðœ†2 Â·ð¿2 + ðœ†3 Â·ð¿3"
  - [corpus]: No direct evidence in neighbor papers about combined loss approaches
- Break condition: If loss weights are poorly tuned, the model may optimize for one task at the expense of others.

## Foundational Learning

- Concept: Interpersonal Risk Factors (IRFs) - Thwarted Belongingness and Perceived Burdensomeness
  - Why needed here: Understanding these concepts is crucial for interpreting the model's task and evaluating its performance on mental health analysis
  - Quick check question: What are the two main IRFs studied in this paper, and how do they relate to suicide risk according to the Interpersonal-Psychological Theory of Suicide?

- Concept: Prompt engineering and few-shot learning with large language models
  - Why needed here: The paper compares N-shot learning approaches with its InterPrompt method, highlighting the limitations of few-shot prompting for this task
  - Quick check question: Why did the authors find that N-shot learning approaches yielded near-chance accuracy for IRF classification?

- Concept: Evaluation metrics for text generation (ROUGE, BLEU, Exact Match)
  - Why needed here: The paper evaluates explanation quality using these metrics, which are standard in text generation tasks
  - Quick check question: Which evaluation metrics were used to assess the quality of generated explanations, and why are they appropriate for this task?

## Architecture Onboarding

- Component map: Data preprocessing -> InterPrompt construction -> GPT-3 fine-tuning -> Inference -> Evaluation
- Critical path:
  1. Load and preprocess IRF dataset
  2. Construct InterPrompt templates for multi-task learning
  3. Fine-tune GPT-3 variants on OpenAI servers
  4. Generate predictions and explanations on test set
  5. Evaluate using classification metrics and text similarity scores
- Design tradeoffs:
  - Fine-tuning vs. prompting: Full fine-tuning offers better performance but requires more resources and access to OpenAI servers
  - Combined vs. separate losses: Joint optimization may improve both tasks but requires careful weight tuning
  - Prompt complexity: More detailed prompts may capture IRF interconnections better but could increase fine-tuning instability
- Failure signatures:
  - Poor classification performance: Indicates issues with prompt construction or loss weighting
  - Low explanation quality scores: Suggests problems with textual cue extraction or generation
  - Fine-tuning instability: May result from overly complex prompts or imbalanced loss weights
- First 3 experiments:
  1. Implement zero-shot prompting with GPT-3 to establish baseline performance
  2. Create InterPrompt templates and test with one GPT-3 variant on a small dataset
  3. Fine-tune all four GPT-3 variants with InterPrompt and compare performance across classification and explanation tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of InterPrompt-driven fine-tuned GPT-3 models vary across different mental health disorders beyond Interpersonal Risk Factors?
- Basis in paper: [explicit] The authors focus on Interpersonal Risk Factors but suggest broader applications in mental health analysis.
- Why unresolved: The paper specifically addresses IRFs in Reddit posts, limiting generalizability to other mental health conditions.
- What evidence would resolve it: Testing InterPrompt on datasets for depression, anxiety, PTSD, and other mental health conditions with varying textual characteristics.

### Open Question 2
- Question: What is the optimal balance between task-specific fine-tuning and preserving general language understanding capabilities in large language models?
- Basis in paper: [inferred] The authors fine-tune GPT-3 with task-specific context, implying a trade-off between specialization and general capabilities.
- Why unresolved: The paper does not explore how different fine-tuning approaches affect both IRF detection performance and general language capabilities.
- What evidence would resolve it: Comparative analysis of models with varying degrees of fine-tuning across multiple tasks, measuring both task-specific performance and general language abilities.

### Open Question 3
- Question: How do cultural and linguistic differences impact the effectiveness of InterPrompt for mental health analysis across different social media platforms?
- Basis in paper: [explicit] The authors use Reddit data but acknowledge the growing importance of mental health analysis on social media platforms.
- Why unresolved: The study focuses on English Reddit posts without exploring cross-cultural or cross-platform generalizability.
- What evidence would resolve it: Testing InterPrompt on multilingual datasets and different social media platforms to assess performance variations across cultural contexts.

## Limitations
- Limited transparency into fine-tuning process due to proprietary GPT-3 models
- Lack of ablation studies on prompt design and loss function weight sensitivity
- Evaluation relies primarily on automatic metrics without human validation of clinical relevance
- Focus on English Reddit data limits generalizability to other languages and platforms

## Confidence

**Major Uncertainties:**
The paper demonstrates strong performance improvements but relies heavily on proprietary GPT-3 models with limited transparency into the fine-tuning process. Key limitations include the lack of ablation studies on prompt design, loss function weight sensitivity, and the specific nature of textual cues used for TBe and PBu. The evaluation focuses primarily on automatic metrics without human validation of explanation quality or clinical relevance.

**Confidence Labels:**
- **High Confidence**: The overall performance improvement of InterPrompt over N-shot learning approaches (87% accuracy vs. 40-60%)
- **Medium Confidence**: The effectiveness of the combined loss function for joint classification and explanation generation
- **Low Confidence**: The clinical utility of generated explanations without human evaluation or expert validation

## Next Checks

1. Conduct ablation studies varying prompt complexity and loss function weights to identify optimal configurations
2. Perform human evaluation of generated explanations for coherence, relevance, and clinical utility
3. Test model generalization on out-of-domain mental health datasets to assess robustness beyond Reddit posts