---
ver: rpa2
title: 'Self-driven Grounding: Large Language Model Agents with Automatical Language-aligned
  Skill Learning'
arxiv_id: '2309.01352'
source_url: https://arxiv.org/abs/2309.01352
tags:
- robot
- door
- next
- ball
- discover
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Self-Driven Grounding (SDG), a framework
  that enables Large Language Models (LLMs) to autonomously interact with environments
  by automatically discovering and learning generalizable skills. SDG decomposes tasks
  into subgoals, verifies them through environment interaction, and clusters similar
  subgoals to train generalized skills via reinforcement learning.
---

# Self-driven Grounding: Large Language Model Agents with Automatical Language-aligned Skill Learning

## Quick Facts
- arXiv ID: 2309.01352
- Source URL: https://arxiv.org/abs/2309.01352
- Reference count: 40
- Primary result: Achieves 75.9% success rate on complex BabyAI tasks without human demonstrations

## Executive Summary
This paper introduces Self-Driven Grounding (SDG), a framework that enables Large Language Models to autonomously interact with environments by automatically discovering and learning generalizable skills. SDG decomposes tasks into subgoals, verifies them through environment interaction, and clusters similar subgoals to train generalized skills via reinforcement learning. In the BabyAI instruction-following benchmark, SDG achieves comparable performance to imitation learning methods requiring millions of expert demonstrations, solving up to 75.9% of the most complex tasks without human-labeled data.

## Method Summary
SDG employs a four-phase workflow: hypothesis generation using LLM to propose subgoals and check functions, verification through RL agent interaction with the environment, induction via clustering successful subgoals to discover generalizable skills, and deduction using few-shot learning to generate programs for complex tasks. The framework addresses the sparse reward problem by generating intrinsic rewards through LLM-created check functions, enabling more efficient RL training. Subgoals are clustered based on semantic similarity to learn skills that can achieve categories of related tasks rather than specific instructions.

## Key Results
- Solves 75.9% of the most complex BossLevel tasks in BabyAI
- Achieves comparable performance to imitation learning methods that require millions of demonstrations
- Demonstrates efficient skill acquisition through automatic subgoal clustering and RL training
- Shows progressive skill accumulation enabling increasingly complex task solving

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM's ability to generate intrinsic rewards through check functions alleviates the sparse reward problem in reinforcement learning.
- Mechanism: By decomposing tasks into subgoals and providing check functions, the framework creates dense, task-specific rewards that guide RL agents more efficiently than environment-level sparse rewards.
- Core assumption: The LLM can accurately generate check functions that correctly evaluate subgoal achievement and provide meaningful rewards for RL training.
- Evidence anchors:
  - [abstract] "SDG first employs the LLM to propose the hypothesis of sub-goals to achieve tasks and then verify the feasibility of the hypothesis via interacting with the underlying environment."
  - [section 3.2] "We produce intrinsic rewards based on LLM-generated subgoals and their check functions, which increase successful experiences by alleviating the sparse reward issue."
  - [corpus] Weak evidence - the cited papers focus on skill learning and grounding but don't directly address intrinsic reward generation through LLM check functions.

### Mechanism 2
- Claim: Clustering subgoals based on semantic similarity enables learning generalizable skills that can be applied to unseen tasks.
- Mechanism: By grouping semantically similar subgoals together and training RL policies on these clusters, the framework learns skills that can achieve a category of related tasks rather than being tied to specific instructions.
- Core assumption: Subgoals with similar semantic descriptions share underlying skill requirements and can be effectively learned through a single policy.
- Evidence anchors:
  - [abstract] "SDG can then learn generalized skills with the guidance of these successfully grounded subgoals."
  - [section 4.4] "We group the subgoals in successful hypotheses with similar semantics to train generalized skills reinforcement learning."
  - [corpus] Moderate evidence - papers like "Language-guided Skill Learning with Temporal Variational Inference" support skill discovery from demonstrations, but the clustering approach is distinct.

### Mechanism 3
- Claim: The iterative hypothesis-verification-induction cycle progressively builds a library of increasingly complex skills.
- Mechanism: Simple tasks are first decomposed and verified, providing basic skills. These skills are then used to solve more complex tasks, creating a virtuous cycle of skill accumulation and task complexity increase.
- Core assumption: Skills learned from simple tasks can be composed or leveraged to solve more complex tasks, creating a meaningful progression.
- Evidence anchors:
  - [abstract] "These skills can be further utilized to accomplish more complex tasks which fail to pass the verification phase."
  - [section 4.1] "Verified in BabyAI, SDG achieves comparable performance in the most difficult tasks compared with imitation learning methods that cost millions of demonstrations."
  - [corpus] Weak evidence - while papers discuss skill learning and composition, the specific progressive accumulation framework isn't well-supported in the corpus.

## Foundational Learning

- Concept: Reinforcement Learning with Intrinsic Rewards
  - Why needed here: The framework relies on RL agents learning from the dense rewards provided by LLM-generated check functions, which is distinct from standard RL with sparse environmental rewards.
  - Quick check question: How does the check function mechanism change the reward structure compared to standard RL in this environment?

- Concept: Semantic Clustering for Skill Discovery
  - Why needed here: The framework uses k-means clustering on LLM-generated API descriptions to group similar subgoals, which is essential for learning generalizable skills.
  - Quick check question: What happens to skill generalization if the semantic clustering incorrectly groups dissimilar subgoals?

- Concept: Few-shot In-context Learning for Program Generation
  - Why needed here: The high-level planner uses the LLM to generate programs for complex tasks using learned skills, requiring understanding of few-shot prompting and in-context learning.
  - Quick check question: How does the quality of the few-shot examples in the prompt affect the generated program's success rate?

## Architecture Onboarding

- Component map:
  LLM module (ChatGPT) for hypothesis generation, semantic embedding, and deduction -> RL training module (PPO) for verification and skill learning -> Environment interface for BabyAI -> Clustering module (k-means) for skill discovery -> Program generation/debugging module for complex task solving

- Critical path:
  1. Receive instruction
  2. LLM decomposes into subgoals + check functions
  3. RL agent verifies subgoals through environment interaction
  4. Successful subgoals are clustered by semantic similarity
  5. RL agents train skills on clustered subgoals
  6. LLM generates programs using learned skills for complex tasks
  7. Interactive debugging refines programs

- Design tradeoffs:
  - LLM choice vs. fine-tuning: Using ChatGPT avoids fine-tuning costs but may have inference latency and cost considerations
  - Clustering granularity: More clusters (higher k) may improve skill specificity but reduce generalization
  - Verification steps: More steps increase success rate but also training time and cost

- Failure signatures:
  - Low verification success rate: Indicates LLM is generating poor subgoal decompositions or check functions
  - Skill training plateaus early: Suggests clustering is grouping dissimilar subgoals or check functions are inadequate
  - Program generation failures: Points to issues with few-shot examples or skill coverage gaps

- First 3 experiments:
  1. Run verification phase on GoToLocal level with different Tverify thresholds to find optimal balance between success rate and efficiency
  2. Test skill clustering with different k values on PickupLoc subgoals to evaluate the effect on skill generalization
  3. Generate programs for SynthSeq tasks using 0, 1, and 5 interaction attempts to measure the impact of the debugging mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the clustering process in the induction phase affect the generality and performance of the learned skills, and what is the optimal number of clusters for different task types?
- Basis in paper: [explicit] The paper mentions that they use k-means clustering based on semantic similarity and choose k=9 based on Calinski-Harabasz scores, but also notes that unsupervised clustering may cause uncertainty in skill discovery.
- Why unresolved: The paper does not systematically explore how different clustering parameters (number of clusters, similarity metrics) affect skill quality and task performance.
- What evidence would resolve it: Experiments varying k values, clustering algorithms, and similarity metrics with systematic evaluation of resulting skill performance across different task types.

### Open Question 2
- Question: Can the self-driven grounding framework be extended to environments with non-textual observations, and what modifications would be needed to handle visual or multi-modal inputs?
- Basis in paper: [explicit] The paper acknowledges this limitation, stating "limited by the simple way of perceiving the environment status, SDG can only deal with tasks with textual descriptions" and suggests introducing multi-modal LLMs as a solution.
- Why unresolved: The current framework relies entirely on textual perception functions and check functions, making it incompatible with environments requiring visual processing.
- What evidence would resolve it: Successful implementation of SDG using multi-modal LLMs that can process visual observations and generate appropriate grounding strategies.

### Open Question 3
- Question: How would multiple cycles of hypothesis-verification-induction improve the framework's ability to learn more complex and diverse hierarchical skills?
- Basis in paper: [explicit] The paper mentions "SDG only contains a single cycle of hypothesis, verification, and induction" and suggests designing multiple cycles as a promising direction.
- Why unresolved: The current single-cycle approach may limit the depth and complexity of skills that can be learned, as each skill is trained independently rather than building upon previously learned skills.
- What evidence would resolve it: Implementation of a multi-cycle SDG framework showing improved performance on increasingly complex tasks through progressive skill refinement and hierarchical learning.

## Limitations
- The framework is limited to environments with textual descriptions, unable to handle visual or multi-modal inputs
- Single-cycle skill learning may constrain the depth and complexity of skills that can be acquired
- Reliance on ChatGPT introduces variability and potential cost/latency issues without fine-tuning alternatives

## Confidence

**Major Uncertainties:**
The framework's reliance on ChatGPT for all LLM operations introduces significant variability, as the authors don't specify prompt engineering details or version dependencies. The semantic clustering approach for skill discovery, while intuitive, lacks empirical validation of whether k-means on LLM embeddings truly captures functionally similar subgoals. Additionally, the progressive skill accumulation mechanism is theoretically sound but may face scalability challenges as task complexity increases beyond the BabyAI benchmark.

**Confidence Labels:**
- **High Confidence**: The core SDG architecture and its four-phase workflow (hypothesis, verification, induction, deduction) are well-defined and reproducible.
- **Medium Confidence**: The performance claims against baseline methods, while impressive, depend heavily on specific implementation choices not fully detailed in the paper.
- **Low Confidence**: The generalizability of the framework to more complex environments and tasks beyond BabyAI remains unproven.

## Next Checks
1. **Prompt Robustness Test**: Evaluate how sensitive the subgoal decomposition and check function generation are to different ChatGPT versions and prompt formulations.
2. **Clustering Validation**: Conduct ablation studies with different k values and alternative clustering methods to verify the optimal grouping of semantically similar subgoals.
3. **Cross-Environment Transfer**: Test the framework on a more complex, open-world environment to assess scalability and generalization beyond the constrained BabyAI setting.