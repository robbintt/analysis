---
ver: rpa2
title: Applying Large Language Models and Chain-of-Thought for Automatic Scoring
arxiv_id: '2312.03748'
source_url: https://arxiv.org/abs/2312.03748
tags:
- scoring
- automatic
- accuracy
- llms
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study demonstrates that large language models, specifically
  GPT-4, can effectively automate the scoring of student-written responses in science
  assessments with improved accuracy and explainability. By employing chain-of-thought
  prompting combined with contextual item stems and rubrics, the researchers achieved
  significant improvements in scoring accuracy, particularly when using few-shot learning.
---

# Applying Large Language Models and Chain-of-Thought for Automatic Scoring

## Quick Facts
- arXiv ID: 2312.03748
- Source URL: https://arxiv.org/abs/2312.03748
- Reference count: 12
- Large language models with Chain-of-Thought prompting can automate science assessment scoring with improved accuracy

## Executive Summary
This study demonstrates that large language models, specifically GPT-4, can effectively automate the scoring of student-written responses in science assessments with improved accuracy and explainability. By employing chain-of-thought prompting combined with contextual item stems and rubrics, the researchers achieved significant improvements in scoring accuracy, particularly when using few-shot learning. The findings indicate that GPT-4, especially with a single-call greedy sampling strategy, outperforms GPT-3.5 in automatic scoring tasks. This approach offers a promising solution for real-time, interpretable feedback in educational settings, addressing the challenges of accessibility and technical complexity in traditional AI-based assessment tools.

## Method Summary
The study used 1,650 student responses from six assessment tasks covering MS-PS1-4 NGSS performance expectations, with three binomial and three trinomial scoring rubrics. Six prompt engineering strategies were implemented, combining zero-shot or few-shot learning with Chain-of-Thought (CoT), alone or with item stems and rubrics. The researchers compared GPT-4 and GPT-3.5 performance using greedy sampling (temperature=0, top_p=0.01) versus nucleus sampling strategies. Scoring accuracy, precision, recall, F1 score, and balanced accuracy were evaluated across proficiency categories (Beginning, Developing, Proficient).

## Key Results
- Few-shot learning with CoT improved scoring accuracy by 12.6% compared to zero-shot learning
- GPT-4 with single-call greedy sampling outperformed GPT-3.5 by 8.64% in automatic scoring accuracy
- CoT paired with contextual item stems and rubrics increased scoring accuracy by up to 13.44% for zero-shot and 3.7% for few-shot learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot learning with Chain-of-Thought (CoT) improves scoring accuracy by 12.6% compared to zero-shot learning.
- Mechanism: Few-shot learning provides LLMs with exemplar reasoning patterns, allowing them to mimic human scoring behavior through guided CoT steps.
- Core assumption: The LLM can effectively learn from limited examples and generalize the reasoning pattern to unseen responses.
- Evidence anchors:
  - [abstract] "Results indicated that few-shot (acc = .67) outperformed zero-shot learning (acc = .60), with 12.6% increase."
  - [section] "Specifically, our results show an average increase of 17.8% in scoring accuracy from zero-shot with CoT (ZS CoT; M = .5532, SD = .102) to few-shot with CoT strategies (FS CoT; M = .6515, SD = .2047)."
- Break condition: If the provided examples are not representative of the task complexity, the LLM may fail to generalize correctly.

### Mechanism 2
- Claim: CoT paired with contextual item stems and rubrics increases scoring accuracy by up to 13.44% for zero-shot and 3.7% for few-shot.
- Mechanism: Domain-specific reasoning embedded in prompts helps LLMs align their scoring decisions with human rubric-based evaluations.
- Core assumption: The rubric structure is clear and the LLM can parse the reasoning steps required for each score category.
- Evidence anchors:
  - [abstract] "CoT prompting paired with contextual item stems and rubrics proved to be a significant contributor to scoring accuracy (13.44% increase for zero-shot; 3.7% increase for few-shot)."
  - [section] "CoT CR seems to increase the overall accuracy by balancing accuracy for all proficiency categories."
- Break condition: If the rubric is ambiguous or the CoT steps are not aligned with the scoring criteria, accuracy may decrease.

### Mechanism 3
- Claim: GPT-4 with single-call greedy sampling outperforms GPT-3.5 in automatic scoring accuracy by 8.64%.
- Mechanism: GPT-4's enhanced reasoning and broader knowledge base lead to more consistent and accurate scoring decisions compared to GPT-3.5.
- Core assumption: GPT-4's architecture and training data provide better domain-specific reasoning for educational tasks.
- Evidence anchors:
  - [abstract] "We also found that GPT-4 demonstrated superior performance over GPT -3.5 in various scoring tasks when combined with the single-call greedy sampling or ensemble voting nucleus sampling strategy, showing 8.64% difference."
  - [section] "Specifically, in greedy sampling (temperature = 0 and top p = 0.01), GPT-4 showed accuracy = .6975 which is higher than GPT-3.5 (accuracy = .6331) by 10.2%."
- Break condition: If the task requires nuanced understanding beyond GPT-4's training scope, performance may degrade.

## Foundational Learning

- Concept: Prompt Engineering
  - Why needed here: Effective prompts guide LLMs to produce accurate and explainable scores.
  - Quick check question: What are the key components of a prompt that includes role, context, rubric, and CoT?

- Concept: Chain-of-Thought (CoT) Reasoning
  - Why needed here: CoT helps LLMs articulate intermediate reasoning steps, improving alignment with human scoring.
  - Quick check question: How does CoT differ between zero-shot and few-shot learning approaches?

- Concept: Sampling Strategies (Greedy vs. Nucleus)
  - Why needed here: Sampling affects the consistency and reliability of LLM outputs in scoring tasks.
  - Quick check question: What is the difference between greedy sampling and nucleus sampling in terms of output diversity?

## Architecture Onboarding

- Component map: Construct prompt -> Call LLM API -> Process response -> Compare to human scores -> Calculate accuracy metrics
- Critical path: Construct prompt → Call LLM API → Process response → Compare to human scores → Calculate accuracy metrics
- Design tradeoffs: Using few-shot examples increases accuracy but requires more input data; greedy sampling ensures consistency but may lack creativity
- Failure signatures: Inconsistent scores across categories, failure to follow rubric criteria, or misclassification of response proficiency levels
- First 3 experiments:
  1. Test zero-shot vs. few-shot learning impact on scoring accuracy
  2. Evaluate CoT effectiveness with and without contextual rubrics
  3. Compare GPT-4 vs. GPT-3.5 performance using different sampling strategies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific conditions under which Chain-of-Thought (CoT) paired with contextual item stems and rubrics yields the most significant improvements in scoring accuracy?
- Basis in paper: [explicit] The study found that CoT paired with contextual item stems and rubrics significantly improved scoring accuracy (13.44% increase for zero-shot; 3.7% increase for few-shot).
- Why unresolved: The paper does not detail the specific conditions or contexts where this improvement is maximized.
- What evidence would resolve it: Further experiments testing various contexts and rubric complexities to identify optimal conditions for CoT effectiveness.

### Open Question 2
- Question: How do different sampling strategies (greedy vs. nucleus) interact with GPT model versions (GPT-3.5 vs. GPT-4) to affect the reliability and accuracy of automatic scoring?
- Basis in paper: [explicit] The study found that GPT-4 with greedy sampling outperformed other approaches, including ensemble voting strategies, and that the interaction between model capacity and voting strategy varies with different hyperparameters.
- Why unresolved: The paper does not fully explore the nuanced interactions between sampling strategies and model versions across diverse assessment tasks.
- What evidence would resolve it: Comparative studies across a wider range of tasks and sampling strategies to determine optimal pairings.

### Open Question 3
- Question: To what extent can prompt engineering techniques be standardized across different educational domains to improve the accessibility and effectiveness of automatic scoring systems?
- Basis in paper: [inferred] The study highlights the potential of prompt engineering to reduce the need for extensive data collection and programming skills, suggesting a transformative impact on automatic scoring paradigms.
- Why unresolved: The paper does not address the generalizability of these techniques across various educational subjects beyond science education.
- What evidence would resolve it: Testing prompt engineering techniques in diverse educational fields to assess their adaptability and effectiveness.

## Limitations
- The study focused specifically on six science assessment tasks aligned to MS-PS1-4 NGSS standards, limiting generalizability to other subjects
- The 1,650 student responses came from a specific dataset with pre-existing human scores, creating potential data dependency and bias concerns
- The finding that voting strategies were less effective than single-call approaches contradicts some literature, suggesting potential sensitivity to parameter choices

## Confidence

- **High confidence**: The finding that few-shot learning with CoT improves accuracy over zero-shot (12.6% increase) is well-supported by direct comparisons in the results
- **Medium confidence**: GPT-4's superiority over GPT-3.5 (8.64% difference) is demonstrated, but this could be influenced by specific implementation choices in prompt engineering
- **Low confidence**: The effectiveness of CoT with contextual item stems and rubrics showing 13.44% improvement for zero-shot may be overstated due to lack of controlled component isolation

## Next Checks
1. Test the CoT approach with responses from different subject areas (math, language arts, social studies) and varying rubric complexities to assess generalizability beyond the science domain
2. Systematically test the individual contributions of CoT, contextual stems, and rubrics by comparing accuracy when each component is removed
3. Evaluate scoring consistency across multiple iterations and over time to test for degradation or adaptation effects