---
ver: rpa2
title: Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization
arxiv_id: '2307.12851'
source_url: https://arxiv.org/abs/2307.12851
tags:
- data
- have
- xmin
- alignment
- neurons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the training dynamics of two-layer ReLU networks
  under small initialization and gradient flow, focusing on the early alignment phase.
  The key contribution is showing that during the early phase of training, neurons
  align with either the positive or negative data, depending on their second-layer
  weights.
---

# Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization

## Quick Facts
- arXiv ID: 2307.12851
- Source URL: https://arxiv.org/abs/2307.12851
- Reference count: 40
- One-line primary result: Analyzes early neuron alignment in two-layer ReLU networks with small initialization, showing alignment occurs within O(log n/√μ) time and leads to O(1/t) loss convergence.

## Executive Summary
This paper analyzes the training dynamics of two-layer ReLU networks under small initialization and gradient flow, focusing on the early alignment phase. The key contribution is showing that during the early phase of training, neurons align with either the positive or negative data, depending on their second-layer weights. The paper establishes an O(log n / √μ) upper bound on the time it takes for all neurons to achieve this alignment, where n is the number of data points and μ measures data separation. After alignment, the loss converges to zero at O(1/t) rate, and the first-layer weight matrix becomes approximately low-rank. The analysis relies on careful examination of neuron directional dynamics and provides a rigorous justification for the "align-then-fit" behavior observed in prior work.

## Method Summary
The paper studies binary classification using two-layer ReLU networks with small initialization, trained via gradient flow. The input data is assumed to be well-separated with specific correlation structure (Assumption 1). The network is initialized with small random weights for the first layer and balanced initialization for the second layer. The analysis tracks the directional dynamics of neurons, showing they converge to one of finitely many extreme directions determined by the dataset. After this early alignment phase, the loss converges to zero at O(1/t) rate, and the first-layer weight matrix becomes approximately low-rank.

## Key Results
- Neurons align with either positive or negative data centers within O(log n/√μ) time during early training
- After alignment, loss converges to zero at O(1/t) rate with approximately low-rank first-layer weights
- The analysis provides theoretical justification for the "align-then-fit" behavior observed in prior work

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neurons align with either positive or negative data centers early in training
- Mechanism: Under small initialization, gradient flow dynamics push neurons toward the signed combination of activated data points (xa(w)). The neuron direction evolution is approximately decoupled, with each neuron converging to one of finitely many extreme directions determined by the dataset.
- Core assumption: Balanced initialization is preserved under gradient flow, and data exhibits strong positive/negative correlation structure as specified in Assumption 1.
- Evidence anchors:
  - [abstract] "Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer."
  - [section 3.2] "First of all, it can be shown that S+, Sdead are trapping regions for all wj(t), j ∈ V+, that is, whenever wj(t) gets inside S+ (or Sdead), it never leaves S+ (or Sdead)."
  - [corpus] Weak evidence - related papers discuss early alignment but lack specific theoretical bounds.
- Break condition: Data correlation structure degrades (µ → 0), initialization scale is too large, or balancedness assumption is violated.

### Mechanism 2
- Claim: Directional convergence occurs within O(log n/√μ) time
- Mechanism: Neurons can only change activation patterns in limited ways due to the data correlation structure. Each neuron transitions through at most n activation pattern changes before reaching S+, S−, or Sdead. The analysis bounds the time per transition and sums across all transitions.
- Core assumption: Data separation parameter μ is bounded away from zero, and initialization scale is sufficiently small.
- Evidence anchors:
  - [abstract] "A careful analysis of the neurons' directional dynamics allows us to provide an O( log n√μ ) upper bound on the time it takes for all neurons to achieve good alignment with the input data"
  - [section 3.2] "Then, since it is possible to show that a neuron wj with j ∈ V+ that has cos(wj, x−) < 1... must change its activation pattern after O( 1/na √μ) time"
  - [corpus] Weak evidence - related work discusses alignment but doesn't provide explicit time bounds.
- Break condition: Data becomes poorly separated (μ → 0), number of neurons h becomes very small, or initialization scale increases.

### Mechanism 3
- Claim: After alignment, loss converges to zero at O(1/t) rate with low-rank first-layer weights
- Mechanism: Once neurons reach S+, S−, or Sdead, they remain there permanently. The network decomposes into two linear subnetworks (one for positive data, one for negative data) that converge at O(1/t) rate. The balanced initialization and convergence to S+ and S− creates low-rank structure in W.
- Core assumption: At least one neuron reaches S+ and one reaches S− by time t1.
- Evidence anchors:
  - [abstract] "After the early alignment phase, the loss converges to zero at a O( 1/t ) rate, and the weight matrix on the first layer is approximately low-rank."
  - [section D.3] "We use W+ = [W ]:,˜V+, W − = [W ]:,˜V− to denote submatrices... Given these notations, after t1 the loss is decomposed as L = L+ + L−"
  - [corpus] Moderate evidence - some related work discusses low-rank bias but not in the context of two-layer ReLU networks with small initialization.
- Break condition: Sdead contains all neurons, preventing any neuron from reaching S+ or S−, or balanced initialization is not preserved.

## Foundational Learning

- Concept: Gradient flow dynamics and differential inclusions
  - Why needed here: The paper analyzes continuous-time gradient descent (gradient flow) for training ReLU networks, requiring understanding of how weights evolve over time.
  - Quick check question: What is the difference between gradient descent and gradient flow, and why is gradient flow analysis useful for understanding training dynamics?

- Concept: Directional dynamics and activation patterns
  - Why needed here: The key insight is that neurons don't change magnitude much during early training, but their directions evolve to align with data. Understanding activation patterns (which data points activate each neuron) is crucial.
  - Quick check question: How does the activation pattern of a neuron evolve during training, and why does this matter for the "align-then-fit" behavior?

- Concept: Convex cones and geometric properties
  - Why needed here: The analysis uses convex cones (S+, S−, Sdead) to characterize regions where neurons converge. Understanding these geometric properties is essential for the proof.
  - Quick check question: What are the defining properties of the cones S+, S−, and Sdead, and how do they relate to the alignment behavior?

## Architecture Onboarding

- Component map: Preprocessed MNIST data (centered) -> h ReLU neurons with small random initialization -> Single neuron with weights v -> Exponential loss

- Critical path:
  1. Initialize W with small random weights, v with balanced initialization
  2. Compute gradient flow dynamics for both W and v
  3. Track neuron directions and activation patterns
  4. Detect when all neurons enter S+, S−, or Sdead
  5. Continue training until loss converges to zero
  6. Verify low-rank structure in final W

- Design tradeoffs:
  - Width h: Larger h increases probability of achieving convergence but adds computational cost
  - Initialization scale: Must be small enough for alignment phase but not so small that training is impractical
  - Data preprocessing: Centering helps satisfy Assumption 1 but may not be sufficient for all datasets

- Failure signatures:
  - Loss plateaus early: May indicate poor data separation or too large initialization
  - Neurons don't align: Could mean Assumption 1 is violated or initialization is unbalanced
  - Loss decreases slowly after alignment: May suggest insufficient neurons in S+ and S−

- First 3 experiments:
  1. Train on toy 2D dataset satisfying Assumption 1 with small h (e.g., 10 neurons) to verify alignment behavior
  2. Test with different initialization scales to find threshold where alignment breaks down
  3. Apply to MNIST digits 0 and 1 with preprocessing to verify practical relevance of theory

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between the initialization scale ϵ and the convergence time t1 for directional alignment?
- Basis in paper: [explicit] The paper shows t1 = O(log n / √μ) but doesn't provide a tight bound on how ϵ affects this.
- Why unresolved: The analysis only establishes an upper bound on t1 and shows it's independent of ϵ, but doesn't explore the exact dependency.
- What evidence would resolve it: A detailed analysis showing how varying ϵ affects the convergence time t1, possibly through numerical experiments or refined theoretical bounds.

### Open Question 2
- Question: How does the neural alignment behavior change when the data doesn't satisfy Assumption 1 exactly?
- Basis in paper: [inferred] The numerical experiments use preprocessed MNIST data that only approximately satisfies the assumption.
- Why unresolved: The theoretical analysis relies heavily on Assumption 1, but real-world data rarely satisfies it perfectly.
- What evidence would resolve it: A theoretical framework extending the analysis to approximately well-separated data, or extensive numerical experiments showing alignment behavior under various data conditions.

### Open Question 3
- Question: What happens to the alignment phase when using gradient descent instead of gradient flow?
- Basis in paper: [explicit] The paper analyzes gradient flow dynamics, not gradient descent.
- Why unresolved: Gradient flow is a continuous approximation that may not capture all aspects of discrete gradient descent updates.
- What evidence would resolve it: A rigorous analysis of the alignment phase under gradient descent dynamics, or numerical experiments comparing gradient flow and gradient descent behaviors.

## Limitations

- The analysis relies heavily on Assumption 1 regarding data correlation structure, which may not hold for real-world datasets beyond the carefully constructed MNIST experiments
- The requirement for extremely small initialization (O(1/√h)) could be practically limiting
- The theory assumes gradient flow rather than discrete gradient descent, and the transition between these regimes is not fully characterized

## Confidence

- Early alignment mechanism: High
- O(log n/√μ) convergence time: Medium
- Post-alignment O(1/t) convergence and low-rank bias: Medium

## Next Checks

1. Test the alignment behavior on synthetic datasets with controlled correlation structure, varying μ to verify the O(log n/√μ) scaling relationship.
2. Implement the same analysis for gradient descent with small learning rates to understand the discretization effects and compare with gradient flow predictions.
3. Apply the methodology to more challenging binary classification tasks (e.g., CIFAR-10 binary subsets) with appropriate preprocessing to assess robustness of the alignment phenomenon beyond MNIST.