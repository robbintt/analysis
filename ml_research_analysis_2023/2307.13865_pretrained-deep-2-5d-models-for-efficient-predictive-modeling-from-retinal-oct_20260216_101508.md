---
ver: rpa2
title: Pretrained Deep 2.5D Models for Efficient Predictive Modeling from Retinal
  OCT
arxiv_id: '2307.13865'
source_url: https://arxiv.org/abs/2307.13865
tags:
- transformer
- data
- pretrained
- learning
- pretraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of developing efficient deep
  learning models for predicting progression to wet age-related macular degeneration
  (AMD) from 3D retinal OCT scans. The proposed 2.5D approach uses 2D CNNs to extract
  features from individual B-scans, followed by either a BiLSTM or Transformer to
  aggregate features across the volume.
---

# Pretrained Deep 2.5D Models for Efficient Predictive Modeling from Retinal OCT

## Quick Facts
- arXiv ID: 2307.13865
- Source URL: https://arxiv.org/abs/2307.13865
- Authors: 
- Reference count: 22
- Key outcome: 2.5D CNN + BiLSTM with TINC pretraining achieved AUROC 0.766±0.012 on HARBOR dataset for predicting wet AMD progression

## Executive Summary
This study develops efficient deep learning models for predicting progression to wet age-related macular degeneration from 3D retinal OCT scans. The proposed 2.5D approach uses 2D CNNs to extract features from individual B-scans, followed by either BiLSTM or Transformer to aggregate features across the volume. Models were pretrained using in-domain TINC pretraining, which outperformed ImageNet pretraining in all cases. The CNN + BiLSTM model with only 34M parameters achieved strong performance on both HARBOR and external PINNACLE datasets, demonstrating the effectiveness of hybrid 2.5D architectures and in-domain pretraining for 3D medical imaging tasks with limited data.

## Method Summary
The method uses hybrid 2.5D architectures that process 32 central B-scans (224×224) from OCT volumes. A pretrained ResNet50 backbone extracts 2048-dimensional feature vectors from each B-scan, which are then aggregated by either a BiLSTM with attention or a 4-layer Transformer with 2 heads. The models are pretrained using TINC (Time-sensitive Non-contrastive) pretraining on unlabeled OCT data, which outperforms ImageNet pretraining. Training uses ADAM/SGD optimizers with cosine learning rate scheduling, batch size 20, and 4-fold cross-validation.

## Key Results
- CNN + BiLSTM with TINC pretraining achieved AUROC 0.766±0.012 on HARBOR dataset
- Same model achieved AUROC 0.646±0.019 on external PINNACLE dataset
- CNN + BiLSTM model has 34M parameters vs 108M for CNN + Transformer with similar FLOPs
- TINC pretraining outperformed ImageNet pretraining in all tested architectures
- 2.5D models outperformed 3D CNN and Transformer models on limited data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: 2.5D hybrid architectures outperform full 3D models in OCT-based progression prediction due to efficient use of pretrained 2D weights and explicit modeling of slice-level relationships.
- Mechanism: The model leverages pretrained 2D CNNs (ResNet50) for individual B-scan feature extraction, followed by sequential aggregation via BiLSTM or attention-based Transformers. This design reduces parameter count while maintaining spatial-temporal context.
- Core assumption: B-scan-level features contain sufficient information for progression prediction, and temporal/slice relationships are best modeled by sequential models.
- Evidence anchors:
  - [abstract] "The CNN + BiLSTM model, with only 34M parameters, achieved an AUROC of 0.766±0.012 on the HARBOR dataset"
  - [section] "The CNN + BiLSTM model has significantly fewer trainable parameters than the CNN + Transformer model (34M vs 108M) with a similar number of FLOPs (130G and 133G, respectively)"

### Mechanism 2
- Claim: In-domain TINC pretraining outperforms ImageNet pretraining for OCT-based tasks, especially under domain shift.
- Mechanism: TINC leverages temporal OCT data without labels, using time-sensitive non-contrastive similarity loss to learn representations that preserve progression-relevant temporal patterns.
- Core assumption: OCT-specific pretraining captures domain-relevant features better than natural image pretraining, particularly for longitudinal prediction tasks.
- Evidence anchors:
  - [abstract] "The proposed 2.5D approach uses 2D CNNs to extract features from individual B-scans, followed by either a BiLSTM or Transformer to aggregate features across the volume"
  - [section] "The performances of four distinct architectures... Each architecture was initialized either with ImageNet or TINC weights... TINC pretraining in terms of AUROC score in all cases"

### Mechanism 3
- Claim: Attention layers improve robustness by emphasizing subtle biomarkers while preventing their dilution through global pooling.
- Mechanism: The BiLSTM attention layer assigns higher weights to B-scans containing progression-relevant biomarkers, creating sparse but informative representations.
- Core assumption: Progression biomarkers are subtle and scarce, requiring selective attention rather than uniform aggregation.
- Evidence anchors:
  - [section] "The goal is to enforce B-scan level sparsity with attention such that subtle biomarkers do not blur out due to the global pooling operations"
  - [section] "we hypothesised that OCT biomarkers indicative of wet-AMD progression are subtle and scarce"

## Foundational Learning

- Concept: Transfer learning and pretraining strategies
  - Why needed here: Limited labeled OCT data requires leveraging pretrained weights to achieve good performance without overfitting
  - Quick check question: What's the key difference between contrastive and non-contrastive pretraining, and why might TINC's approach be advantageous for medical imaging?

- Concept: Hybrid 2.5D architectures and their tradeoffs
  - Why needed here: Understanding when to use 2.5D vs 3D models based on data efficiency, parameter count, and task requirements
  - Quick check question: How does the parameter efficiency of 2.5D models compare to 3D models, and what architectural components enable this efficiency?

- Concept: Bidirectional LSTM and Transformer architectures for sequential data
  - Why needed here: Both architectures aggregate slice-level features, but with different mechanisms for capturing temporal relationships
  - Quick check question: What's the key difference between BiLSTM and Transformer approaches for aggregating B-scan features, and when might each be preferable?

## Architecture Onboarding

- Component map: B-scans -> ResNet50 -> Feature vectors -> BiLSTM/Transformer -> Attention -> Classification
- Critical path: B-scans → ResNet50 → Feature vectors → BiLSTM/Transformer → Attention → Classification
- Design tradeoffs:
  - Parameter efficiency vs model capacity (34M vs 108M for CNN+BiLSTM vs CNN+Transformer)
  - Sequential modeling vs parallel attention mechanisms
  - In-domain vs out-of-domain pretraining effectiveness
  - Model complexity vs data availability
- Failure signatures:
  - Overfitting on small datasets (high training accuracy, low validation accuracy)
  - Poor generalization to external datasets (significant AUROC drop on PINNACLE)
  - Ineffective pretraining (minimal performance difference between ImageNet and TINC)
  - Attention layer not focusing on relevant B-scans (uniform attention weights)
- First 3 experiments:
  1. Baseline: CNN+BiLSTM with ImageNet pretraining on HARBOR dataset
  2. Ablation: Remove attention layer from CNN+BiLSTM to test its impact
  3. Domain adaptation: Test TINC pretraining on PINNACLE dataset with domain shift

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of hybrid 2.5D models compare to 3D models when trained on larger, more diverse datasets?
- Basis in paper: [inferred] The paper demonstrates the superiority of 2.5D models over 3D models on limited datasets, but does not explore performance on larger datasets.
- Why unresolved: The study focuses on limited data scenarios, leaving the question of scalability to larger datasets unanswered.
- What evidence would resolve it: Comparative studies using 2.5D and 3D models trained on large-scale medical imaging datasets with sufficient computational resources.

### Open Question 2
- Question: What specific B-scan features or biomarkers are most indicative of AMD progression, and how can they be extracted and interpreted by the model?
- Basis in paper: [explicit] The paper mentions that the attention layer in the CNN + BiLSTM model can provide attention scores over B-scans, potentially highlighting important features.
- Why unresolved: The study does not perform a detailed analysis of the attention scores or identify specific biomarkers.
- What evidence would resolve it: Detailed analysis of attention scores, feature importance rankings, and clinical validation of identified biomarkers.

### Open Question 3
- Question: How does the performance of hybrid 2.5D models with in-domain pretraining compare to models trained from scratch on the same task?
- Basis in paper: [explicit] The paper demonstrates the superiority of TINC pretraining over ImageNet pretraining for both 2.5D and 3D models.
- Why unresolved: The study does not include a baseline comparison with models trained from scratch on the downstream task.
- What evidence would resolve it: Experiments comparing the performance of hybrid 2.5D models with in-domain pretraining to models trained from scratch on the same task and dataset.

## Limitations
- Significant domain shift between HARBOR and PINNACLE datasets (13% AUROC drop) suggests limited generalizability
- TINC pretraining implementation details are not fully specified, making exact reproduction difficult
- Limited ablation analysis of attention mechanism's contribution to performance

## Confidence
- 2.5D architecture superiority: High
- TINC pretraining effectiveness: Medium
- Attention mechanism benefits: Medium

## Next Checks
1. Ablation study: Systematically remove the attention layer from CNN+BiLSTM to quantify its contribution to performance gains
2. Pretraining comparison: Test TINC pretraining on a third, independent OCT dataset to validate generalization of pretraining benefits
3. Hyperparameter sensitivity: Conduct grid search on key hyperparameters (learning rate, batch size, optimizer choice) to ensure robust performance across settings