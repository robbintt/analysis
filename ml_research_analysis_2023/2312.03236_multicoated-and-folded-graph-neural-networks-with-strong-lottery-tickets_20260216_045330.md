---
ver: rpa2
title: Multicoated and Folded Graph Neural Networks with Strong Lottery Tickets
arxiv_id: '2312.03236'
source_url: https://arxiv.org/abs/2312.03236
tags:
- gnns
- m-sup
- accuracy
- s-sup
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies multicoated supermasks (M-Sup) to graph neural
  networks (GNNs) and introduces adaptive thresholds for weight pruning. It demonstrates
  the existence of untrained recurrent subnetworks within deep GNNs and proposes multi-stage
  folding (MSF) and unshared masks to expand the search space.
---

# Multicoated and Folded Graph Neural Networks with Strong Lottery Tickets

## Quick Facts
- arXiv ID: 2312.03236
- Source URL: https://arxiv.org/abs/2312.03236
- Reference count: 40
- One-line primary result: M-Sup achieves high sparsity, competitive performance, and high memory efficiency in GNNs, suitable for energy-efficient graph processing.

## Executive Summary
This paper applies multicoated supermasks (M-Sup) to graph neural networks (GNNs) and introduces adaptive thresholds for weight pruning. It demonstrates the existence of untrained recurrent subnetworks within deep GNNs and proposes multi-stage folding (MSF) and unshared masks to expand the search space. Experiments on various datasets, including the Open Graph Benchmark (OGB), show that M-Sup outperforms previous methods in terms of accuracy, especially when sparsity is high. The proposed methods achieve significant memory reductions (up to 98.7%) while maintaining competitive performance compared to dense-weight learning baselines.

## Method Summary
The method applies multicoated supermasks (M-Sup) to GNNs with adaptive thresholds for weight pruning. It identifies high-performing subnetworks in random initial states through sequential pruning coats, each removing weaker weights. The approach includes multi-stage folding to convert deep GNNs into recurrent structures and unshared masks for per-iteration specialization. The method uses Signed Kaiming Constant initialization and operates without weight training, relying on binary mask selection to achieve sparse architectures that maintain competitive accuracy.

## Key Results
- M-Sup outperforms previous methods in accuracy, especially at high sparsity levels (80-90%)
- Memory reductions up to 98.7% achieved while maintaining competitive performance
- Strong Lottery Ticket Hypothesis validated for GNNs with untrained recurrent subnetworks matching trained feed-forward counterparts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multicoated supermasks enable untrained GNNs to match or exceed dense-weight training baselines.
- Mechanism: Multiple sparse pruning coats are applied sequentially, each removing weaker weights. This creates progressively sparser subnetworks that retain critical connectivity for task performance.
- Core assumption: Randomly initialized weights with signed Kaiming Constant initialization contain sufficient information to support multiple effective sparse subnetworks without training.
- Evidence anchors:
  - [abstract]: "M-Sup outperforms previous methods in terms of accuracy, especially when sparsity is high"
  - [section 3.2]: "By identifying high-performing subnetworks in their random initial states, this work can surpass the performance of single-coated supermask (S-Sup), e.g., UGT, and the accuracy of the dense-weight training baseline"
- Break condition: If weight score distributions become multimodal or dominated by extreme values, adaptive threshold calculation fails and coats become ineffective.

### Mechanism 2
- Claim: Adaptive threshold selection improves M-Sup accuracy by preventing ineffective coats.
- Mechanism: Thresholds are set using pre-trained model statistics (if available) or uniform sparsity distribution otherwise, avoiding coats that would prune all or no weights.
- Core assumption: Weight score distributions vary systematically with network width and depth, requiring dynamic threshold calibration.
- Evidence anchors:
  - [section 3.2]: "Experiments show its advantage in terms of accuracy" and describes adaptive method avoiding ineffective coats when stn ≥ α
  - [section 4.1]: Compares adaptive Linear threshold (α = 0.9996) against non-adaptive (α = 1.0), showing better accuracy at high sparsity
- Break condition: When pre-trained models are unavailable and weight distributions are highly skewed, uniform approach may still produce suboptimal sparsity patterns.

### Mechanism 3
- Claim: Folding deep GNNs into recurrent structures with shared/unshared masks reduces memory while maintaining accuracy.
- Mechanism: Residual blocks are converted to recurrent iterations with weight sharing; unshared masks allow per-iteration specialization, and multi-stage folding expands architectural search space.
- Core assumption: Deep GNNs contain untrained recurrent subnetworks that can approximate trained feed-forward behavior when appropriately masked.
- Evidence anchors:
  - [abstract]: "this research uncovers the existence of untrained recurrent networks, which exhibit performance on par with their trained feed-forward counterparts"
  - [section 3.3]: Describes folding ResGCNs and DyResGENs, and introduces Multi-stage folding and Unshared Masks
  - [section 4.2]: Shows memory reductions up to 98.7% while maintaining accuracy
- Break condition: If folding disrupts gradient flow or temporal dependencies too severely, performance degrades despite masking.

## Foundational Learning

- Concept: Graph Neural Networks and message passing
  - Why needed here: All methods operate on GNNs; understanding aggregation and weight sharing is essential for reasoning about pruning and folding
  - Quick check question: What is the difference between the aggregation step and the weight application step in a GCN layer?

- Concept: Lottery Ticket Hypothesis and its strong variant
  - Why needed here: The paper extends SLTH from CNNs to GNNs; knowing what SLTH claims (existence of untrained high-performing subnetworks) is fundamental
  - Quick check question: How does the Strong Lottery Ticket Hypothesis differ from the Weak version in terms of training requirements?

- Concept: Pruning and mask-based weight elimination
  - Why needed here: M-Sup and folding both rely on pruning via binary masks; understanding how masks interact with weight initialization and gradients is critical
  - Quick check question: What is the effect of a pruning mask on the computational graph during inference in a GNN?

## Architecture Onboarding

- Component map:
  - Input: Graph (A, X), randomly initialized weights Wrand
  - Mask generator: Scores S → binary masks via adaptive thresholds
  - Multi-coat application: Sequential mask application (M-Sup) or single coat (S-Sup)
  - Folding engine: Optional conversion of deep GNNs to recurrent structure (MSF or SSF)
  - Forward pass: Sparse matrix operations with masked weights
  - Output: Node or graph predictions

- Critical path:
  1. Initialize weights (SC or KN)
  2. Generate or load score matrix S
  3. Compute adaptive thresholds based on sparsity target
  4. Generate binary masks and apply to weights
  5. Perform forward pass (with folding if enabled)
  6. Evaluate accuracy and memory metrics

- Design tradeoffs:
  - Memory vs accuracy: More coats (M-Sup) improve accuracy but increase memory for masks
  - Shared vs unshared masks: Shared reduces parameters but unshared improves accuracy
  - Folding stages: More stages increase search space but add parameter overhead
  - Sparsity level: Higher sparsity saves memory but risks accuracy drop

- Failure signatures:
  - Accuracy plateaus or drops sharply at high sparsity → ineffective masks or poor threshold selection
  - Memory reduction much lower than expected → folding not properly implemented or masks not shared
  - Training instability → score updates destabilizing mask structure

- First 3 experiments:
  1. Reproduce 2-layer GCN on Cora with S-Sup and M-Sup, compare accuracy vs sparsity
  2. Implement adaptive threshold selection and verify it outperforms fixed thresholds on multimodal score distributions
  3. Apply single-stage folding to a 4-layer GCN and measure memory reduction vs accuracy trade-off

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed adaptive Linear threshold method compare to other threshold determination techniques (e.g., Uniform, entropy-based) in terms of accuracy and computational efficiency across diverse GNN architectures and datasets?
- Basis in paper: [explicit] The paper introduces an adaptive Linear threshold method and mentions the Linear and Uniform methods as baselines, noting that the Linear method proved less effective in some cases and the adaptive Linear threshold outperforms in high sparsity scenarios.
- Why unresolved: The paper does not provide a comprehensive comparison between the adaptive Linear threshold and other threshold determination techniques. It only mentions the Linear and Uniform methods and focuses on the performance of the adaptive Linear threshold relative to the non-adaptive Linear threshold.
- What evidence would resolve it: A comparative study evaluating the adaptive Linear threshold against various threshold determination techniques (e.g., entropy-based, gradient-based) across a wide range of GNN architectures (e.g., GCN, GAT, GIN, ResGCN, DyResGEN) and diverse datasets (e.g., Cora, Citeseer, PubMed, OGBN-Arxiv, OGBG-Molhiv, OGBG-Molbace) would provide insights into the effectiveness and generalizability of the proposed method.

### Open Question 2
- Question: What are the theoretical foundations and practical implications of the observed phenomenon where M-Sup maintains consistent accuracy without apparent drops even with high sparsity (80-90%)?
- Basis in paper: [explicit] The paper observes that M-Sup maintains consistent accuracy without apparent drops even with high sparsity (80-90%) and suggests that the projection of node representations learned by M-Sup maintains a distinguishable effect.
- Why unresolved: The paper does not provide a theoretical explanation for the observed phenomenon or explore its practical implications in detail. It only mentions the empirical observation and suggests that the distinguishable node representations learned by M-Sup might be a contributing factor.
- What evidence would resolve it: A theoretical analysis of the M-Sup method, including its impact on the optimization landscape, the distribution of weight scores, and the resulting node representations, would provide insights into the underlying mechanisms that enable M-Sup to maintain accuracy at high sparsity levels. Additionally, experiments investigating the practical implications of this phenomenon, such as its impact on model robustness, generalization, and energy efficiency, would further validate its significance.

### Open Question 3
- Question: How does the proposed folding method with shared and unshared supermasks impact the trade-off between model accuracy, memory efficiency, and computational complexity in deep GNNs?
- Basis in paper: [explicit] The paper introduces the folding method with shared and unshared supermasks to optimize deep GNNs, achieving significant memory reductions while maintaining comparable accuracy to baseline models. It also mentions that unshared masks and MSF enhance accuracy but result in an increase in both memory and parameters, creating a trade-off.
- Why unresolved: The paper does not provide a comprehensive analysis of the trade-offs between accuracy, memory efficiency, and computational complexity when using the proposed folding method with shared and unshared supermasks. It only presents the performance of different configurations on specific datasets and mentions the trade-off without exploring it in detail.
- What evidence would resolve it: A systematic evaluation of the proposed folding method with shared and unshared supermasks across a wide range of deep GNN architectures (e.g., ResGCN, DyResGEN) and diverse datasets, considering metrics such as accuracy, memory usage, parameter count, and computational complexity (e.g., inference time, MACs), would provide insights into the trade-offs and guide the selection of the most appropriate configuration for different applications and resource constraints.

## Limitations

- Adaptive threshold calibration lacks theoretical grounding and may fail on highly skewed weight distributions
- Parameter overhead from unshared masks may offset memory efficiency gains in very deep networks
- Method performance on datasets where random initialization is known to perform poorly remains untested

## Confidence

- **High**: The existence of untrained recurrent subnetworks within deep GNNs (supported by folding experiments showing competitive accuracy with 98.7% memory reduction)
- **Medium**: M-Sup's accuracy advantage over S-Sup at high sparsity (consistent across multiple datasets but threshold selection lacks theoretical grounding)
- **Low**: The universal applicability of adaptive thresholds without pre-trained models (only validated on Cora, Citeseer, and PubMed where weight distributions may be more predictable)

## Next Checks

1. Test adaptive threshold performance on synthetic GNN weight distributions with varying skew and multimodality to identify failure modes
2. Profile memory overhead of unshared masks across different folding stage counts to determine practical efficiency limits
3. Compare M-Sup accuracy against trained dense baselines on a dataset where random weight initialization is known to perform poorly (e.g., protein interaction networks with long-range dependencies)