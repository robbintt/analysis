---
ver: rpa2
title: Computing SHAP Efficiently Using Model Structure Information
arxiv_id: '2309.02417'
source_url: https://arxiv.org/abs/2309.02417
tags:
- order
- shap
- when
- shapley
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents efficient methods for computing Shapley Additive
  exPlanations (SHAP) for machine learning models. The authors develop three approaches
  based on model structure information: (1) when the functional decomposition is known,
  SHAP can be computed exactly by summing SHAP values of low-order components; (2)
  when the model order is known but decomposition is unknown, SHAP can be computed
  in polynomial time using derived formulas; (3) when the model order is unknown,
  an iterative approximation method is proposed.'
---

# Computing SHAP Efficiently Using Model Structure Information

## Quick Facts
- arXiv ID: 2309.02417
- Source URL: https://arxiv.org/abs/2309.02417
- Reference count: 13
- One-line primary result: Three efficient methods for computing SHAP values using model structure information, achieving polynomial-time computation for low-order models

## Executive Summary
This paper presents efficient methods for computing Shapley Additive exPlanations (SHAP) by exploiting model structure information. The authors develop three approaches: (1) exact computation using known functional decompositions, (2) polynomial-time formulas for known model orders, and (3) iterative approximation for unknown model orders. These methods are computationally efficient for low-order models typical in practice and outperform sampling-based approaches like Captum in both speed and accuracy, particularly when model order is low or high-order interactions are weak.

## Method Summary
The paper introduces three methods for efficient SHAP computation based on available model structure information. When functional decomposition is known, SHAP values can be computed exactly by summing the SHAP values of low-order components using an additive property. When model order is known but decomposition is unknown, polynomial-time formulas evaluate only subsets of small cardinality, dramatically reducing computation. When model order is unknown, an iterative approximation method progressively increases the assumed order until convergence, leveraging the observation that most real-world models are approximately low-order. All methods exploit the hierarchical nature of feature interactions to avoid exponential complexity.

## Key Results
- Proposed methods are computationally efficient when model order is not high, which is typically the case in practice
- Simulation studies show proposed methods are faster and more accurate than sampling-based approaches like Captum
- Iterative method converges quickly when high-order interactions are small
- Exact computation via decomposition reduces complexity from exponential to polynomial time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: When functional decomposition is known, SHAP can be computed exactly in polynomial time by summing SHAP values of low-order components
- Mechanism: The paper introduces an additive property for SHAP values. If a model can be decomposed into low-order components, the SHAP value for the entire model equals the sum of the SHAP values of each component that contains the feature
- Core assumption: The model satisfies additivity and dummy assumptions, and the functional decomposition is known and accurate
- Evidence anchors:
  - [abstract] "First, we show that Shapley values can be computed much more efficiently if the models have known structure"
  - [section] "Proposition 1. Assume that the model can be decomposed into lower order terms... then ùúôùëñ = ‚àë ùúôùëñ(ùëìùë£)ùëñ‚ààùë£"
- Break condition: The decomposition is unknown, inaccurate, or the model violates additivity/dummy assumptions

### Mechanism 2
- Claim: When model order is known but decomposition is unknown, SHAP can be computed in polynomial time using derived formulas
- Mechanism: The paper derives formulas that compute SHAP values by evaluating the cost function only on subsets with small cardinalities (near 0 or near p)
- Core assumption: The model order is known and relatively small, and the model satisfies Assumption 1
- Evidence anchors:
  - [abstract] "Second, when we do not have a functional decomposition of a model, but we know the order of the model, we derive formulas that can compute SHAP in polynomial time"
  - [section] "Theorem 1. Assume the model ùëì(ùíô) has order ùêæ and Assumption 1 holds, then... ùúôùëñ = ‚àë ùëéùëö(ùëëùëö + ùëëùëù‚àíùëö‚àí1)ùëûùëö=0..."
- Break condition: The model order is unknown, very high, or the model violates Assumption 1

### Mechanism 3
- Claim: When model order is unknown, an iterative approximation method can converge to accurate SHAP values
- Mechanism: The algorithm starts with order K=1, computes SHAP values, then increases K by 2 each iteration, comparing the difference between successive SHAP values
- Core assumption: The true underlying model is either low-order or approximately low-order with weak high-order interactions
- Evidence anchors:
  - [abstract] "Finally, if even the order of model is unknown, we propose an iterative way to approximate Shapley values"
  - [section] "When the order of the model is unknown... we propose an iterative way to approximate the Shapley values with low-order results"
- Break condition: The model contains strong high-order interactions that prevent convergence

## Foundational Learning

- Concept: Functional ANOVA (fANOVA) decomposition
  - Why needed here: This decomposition technique allows complex models to be expressed as sums of low-order components, enabling the additive property for SHAP computation
  - Quick check question: What orthogonality constraint must be satisfied for fANOVA decomposition to guarantee a unique representation?

- Concept: Shapley value properties (efficiency, symmetry, dummy, additivity)
  - Why needed here: These properties define what makes a fair attribution method and are used to validate the correctness of the proposed SHAP computation methods
  - Quick check question: Which Shapley value property ensures that the sum of all feature attributions equals the model prediction?

- Concept: Model order and interaction hierarchy
  - Why needed here: Understanding model order (maximum interaction order) is crucial for determining which computation method to apply and how many subsets need to be evaluated
  - Quick check question: If a model has order 4, what is the maximum number of features that can interact in any single component?

## Architecture Onboarding

- Component map:
  - Input layer: Feature vector and model prediction function
  - Decomposition analyzer: Checks if functional decomposition is available
  - Order detector: Determines model order if decomposition unavailable
  - SHAP calculator: Implements one of three methods based on available information
  - Convergence checker: For iterative method, monitors SHAP value changes
  - Output formatter: Returns SHAP values with metadata

- Critical path:
  1. Receive input features and model
  2. Check for functional decomposition availability
  3. If available, apply additive property to compute SHAP values
  4. If not, check for known model order
  5. If known, apply polynomial-time formula
  6. If unknown, run iterative approximation
  7. Return computed SHAP values

- Design tradeoffs:
  - Speed vs. accuracy: Sampling methods (like Captum) are faster but less accurate; exact methods are slower but precise
  - Memory vs. computation: Storing all subset evaluations vs. recomputing as needed
  - Generality vs. efficiency: Universal methods work for all models but may be slower than model-specific optimizations

- Failure signatures:
  - Extremely slow computation: Likely indicates high model order not properly accounted for
  - SHAP values not summing to prediction difference: Suggests violation of additivity assumption
  - Non-convergence in iterative method: Indicates strong high-order interactions
  - Large discrepancies between methods: Suggests model structure information is incorrect

- First 3 experiments:
  1. Implement the additive property for a known decomposition (e.g., main effects + pairwise interactions) and verify SHAP values sum correctly
  2. Test the order-K formula on a synthetic model with known order (e.g., max_depth=4 XGBoost) and compare with exact computation
  3. Run the iterative method on a model with weak high-order interactions and verify convergence behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the iterative approximation method be optimized to handle cases where only a few features or observations have not converged, while most have?
- Basis in paper: [explicit] The paper mentions that the algorithm can be further optimized to drop observations/features which have converged and focus on the ones which haven't, but doesn't provide details on how to implement this
- Why unresolved: The paper only briefly mentions this possibility without providing specific implementation details or performance comparisons
- What evidence would resolve it: A detailed algorithm showing how to identify and handle non-converged features/observations, along with simulation results comparing the optimized approach to the basic iterative method

### Open Question 2
- Question: How does the accuracy of the proposed methods scale with the number of features (p) when the model order is unknown and potentially high?
- Basis in paper: [inferred] The paper shows that methods are efficient when order is low, but doesn't thoroughly examine cases where order might be high relative to p, especially in the iterative approximation approach
- Why unresolved: The paper focuses on low-order models and doesn't provide extensive analysis of performance degradation when dealing with high-order models or large feature spaces
- What evidence would resolve it: Systematic simulations varying both model order and number of features, showing accuracy and computational time trends as these parameters increase

### Open Question 3
- Question: How do the proposed methods perform on real-world datasets with complex feature interactions compared to theoretical models?
- Basis in paper: [inferred] The paper only uses simulated data with known model structures, not testing on real-world datasets where feature relationships are unknown and potentially more complex
- Why unresolved: Real-world data often contains unknown interactions, non-linearities, and correlations that may not be captured in the simple simulated models used in the paper
- What evidence would resolve it: Application of the methods to benchmark datasets (e.g., from UCI repository or Kaggle) with comparison to existing SHAP approximation methods in terms of both accuracy and computational efficiency

## Limitations
- Methods require accurate knowledge of model structure (decomposition or order), which may not be available for black-box models
- Performance degrades significantly when high-order interactions are strong or model order is high
- The paper lacks comprehensive empirical validation on real-world datasets with complex feature interactions

## Confidence
- Confidence in the additive property approach: Medium-High (clear mathematical derivation and validation)
- Confidence in the polynomial-time formulas: Medium (lack of explicit coefficient derivations and limited empirical validation)
- Confidence in the iterative approximation method: Medium-Low (depends critically on assumption that real-world models are approximately low-order)

## Next Checks
1. **Structural Validation**: Test the additive property on a range of models with known decompositions (decision trees, generalized additive models, and polynomial expansions) to verify the claimed polynomial-time complexity and correctness of the summation approach

2. **Order Detection Robustness**: Evaluate the iterative approximation method across models with varying degrees of high-order interactions to quantify its convergence behavior and accuracy degradation when strong interactions are present

3. **Scalability Assessment**: Benchmark the polynomial-time formula against exact computation and sampling-based methods (Captum) on models with orders 2 through 5, measuring both computational time and approximation error across different sample sizes and feature dimensions