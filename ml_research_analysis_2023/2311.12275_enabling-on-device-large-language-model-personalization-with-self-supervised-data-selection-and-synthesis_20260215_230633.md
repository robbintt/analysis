---
ver: rpa2
title: Enabling On-Device Large Language Model Personalization with Self-Supervised
  Data Selection and Synthesis
arxiv_id: '2311.12275'
source_url: https://arxiv.org/abs/2311.12275
tags:
- data
- buffer
- dialogue
- framework
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses on-device personalization of large language\
  \ models (LLMs) using limited memory and sparse user annotations. The method uses\
  \ a small buffer and three unsupervised quality metrics\u2014embedding entropy,\
  \ domain relevance, and in-domain dissimilarity\u2014to select the most representative\
  \ data from streaming user-LLM interactions."
---

# Enabling On-Device Large Language Model Personalization with Self-Supervised Data Selection and Synthesis

## Quick Facts
- arXiv ID: 2311.12275
- Source URL: https://arxiv.org/abs/2311.12275
- Reference count: 23
- One-line primary result: Achieves up to 38% higher ROUGE-1 accuracy and faster learning than baselines for on-device LLM personalization.

## Executive Summary
This paper introduces a method for personalizing large language models on edge devices using limited memory and sparse user annotations. The approach uses a small buffer and three unsupervised quality metrics—embedding entropy, domain relevance, and in-domain dissimilarity—to select representative data from streaming user interactions. Selected data are annotated by the user and expanded with semantically similar synthetic pairs generated by the LLM to improve fine-tuning quality. Experiments across multiple datasets show significant improvements over baseline strategies.

## Method Summary
The method uses a small buffer to store dialogue sets from user-LLM interactions. Three unsupervised quality metrics—embedding entropy (EOE), domain-specific score (DSS), and in-domain dissimilarity (IDD)—evaluate data quality for selection. Selected dialogue sets are annotated by the user and expanded with 3 synthetic similar pairs generated by the LLM using a ROUGE-1 threshold filter. The expanded dataset fine-tunes Llama-3B with LoRA. The buffer replacement policy prioritizes data maximizing the minimum of EOE, DSS, and IDD scores.

## Key Results
- Achieves up to 38% higher ROUGE-1 accuracy compared to random, FIFO, and K-center baselines
- Demonstrates faster learning with limited annotations across multiple datasets
- Shows effective personalization across diverse domains including medical, emotion, and general dialogue

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The entropy of embedding (EOE) metric effectively measures the diversity of token information in a dialogue set, guiding the selection of high-quality data for fine-tuning.
- Mechanism: EOE calculates the Shannon entropy of token embeddings, normalized by sequence length, to quantify the amount of unique information each dialogue set contains.
- Core assumption: Higher entropy in token embeddings correlates with richer information content useful for model adaptation.
- Evidence anchors:
  - [abstract] "We propose to utilize embedding entropy, domain-related information, and embedding similarity to measure data quality from different perspectives in an unsupervised way."
  - [section] "For each input T, EOE aims to qualify and measure the information of embedding vector rE = f(T) generated by an end-to-end embedding function f(·)."
  - [corpus] Weak - The corpus does not provide direct evidence for this mechanism, but the abstract and section clearly explain its role.
- Break condition: If token embeddings are too uniform or if the embedding function fails to capture semantic diversity, EOE may not effectively distinguish high-quality data.

### Mechanism 2
- Claim: The domain-specific score (DSS) ensures selected data aligns with relevant text domains, improving model personalization.
- Mechanism: DSS calculates the overlap between dialogue tokens and domain-specific lexicons, averaged across multiple domains, to prioritize data relevant to user interests.
- Core assumption: Dialogue sets with higher token overlap to domain lexicons are more valuable for domain-specific personalization.
- Evidence anchors:
  - [abstract] "To tackle the challenges of sparse local annotations and limited buffer size for on-device LLM personalization, in this paper, we propose to utilize embedding entropy, domain-related information, and embedding similarity to measure data quality from different perspectives in an unsupervised way."
  - [section] "Given a dialogue set T containing n tokens, and a collection of lexicon set L = {l1, l2, ..., lm} from m different domains, the Domain Specific Score (DSS) can be calculated as: DSS(T, L) = 1/m Σ|i=1 m |T ∩ li| / n"
  - [corpus] Weak - The corpus does not provide direct evidence for this mechanism, but the abstract and section clearly explain its role.
- Break condition: If domain lexicons are incomplete or if the user's data does not align well with predefined domains, DSS may select suboptimal data.

### Mechanism 3
- Claim: In-domain dissimilarity (IDD) ensures the buffer contains diverse dialogue sets within the same domain, preventing redundancy and improving model generalization.
- Mechanism: IDD calculates the average cosine dissimilarity between a new dialogue set and existing sets in the buffer with the same dominant domain, favoring sets that introduce new information.
- Core assumption: Dialogue sets that are dissimilar to existing buffer contents within the same domain provide more valuable learning signals.
- Evidence anchors:
  - [abstract] "To tackle the challenges of sparse local annotations and limited buffer size for on-device LLM personalization, in this paper, we propose to utilize embedding entropy, domain-related information, and embedding similarity to measure data quality from different perspectives in an unsupervised way."
  - [section] "When a new dialogue set is considered, we identify all the dialogue sets already in the buffer that have the same dominant domain as the new set, and compute the dissimilarity between them, which will reflect the amount of new information that can be brought by the new set to the dominant domain."
  - [corpus] Weak - The corpus does not provide direct evidence for this mechanism, but the abstract and section clearly explain its role.
- Break condition: If the buffer size is too small or if the input data lacks diversity, IDD may struggle to find sufficiently dissimilar sets, leading to redundancy.

## Foundational Learning

- Concept: Shannon entropy and its application to text embeddings
  - Why needed here: EOE relies on Shannon entropy to quantify the information content of token embeddings, guiding data selection.
  - Quick check question: How does normalizing entropy by sequence length help compare dialogue sets of different lengths?
- Concept: Domain-specific lexicons and their role in text classification
  - Why needed here: DSS uses domain lexicons to measure the relevance of dialogue sets to specific topics, ensuring personalized model adaptation.
  - Quick check question: What challenges might arise if domain lexicons are too broad or too narrow?
- Concept: Cosine similarity and dissimilarity in high-dimensional spaces
  - Why needed here: IDD uses cosine dissimilarity to measure the diversity of dialogue sets within the same domain, preventing redundancy in the buffer.
  - Quick check question: How does cosine dissimilarity differ from Euclidean distance in measuring vector similarity?

## Architecture Onboarding

- Component map: Data buffer → Quality metrics (EOE, DSS, IDD) → Data selection policy → User annotation → LLM-based synthesis → Fine-tuning
- Critical path: Streaming dialogue sets → Quality score calculation → Buffer update decision → User annotation request → Data synthesis → Fine-tuning
- Design tradeoffs: Buffer size vs. memory constraints; quality metrics vs. computational overhead; data synthesis vs. annotation frequency
- Failure signatures: Poor ROUGE-1 scores despite high-quality data; buffer overflow/underflow; excessive user annotation requests
- First 3 experiments:
  1. Test EOE metric alone on a small dataset to verify its ability to rank dialogue sets by information content.
  2. Evaluate DSS metric's effectiveness in selecting domain-relevant data using a labeled dataset.
  3. Assess IDD metric's impact on buffer diversity by comparing ROUGE-1 scores with and without IDD-based filtering.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed framework perform under extreme memory constraints (e.g., buffer sizes below 100KB) on edge devices?
- Basis in paper: [inferred] The paper explores buffer sizes ranging from 176KB to 11MB, but does not evaluate performance at the lower extreme of on-device memory capabilities.
- Why unresolved: The paper focuses on demonstrating scalability and performance improvements across a range of buffer sizes, but does not address the limits of the framework under extreme memory constraints.
- What evidence would resolve it: Experimental results showing ROUGE-1 scores and learning curves for buffer sizes below 100KB, particularly for resource-constrained edge devices like microcontrollers or IoT devices.

### Open Question 2
- Question: How does the framework adapt to non-textual data streams, such as multimodal interactions (e.g., text + images or speech)?
- Basis in paper: [explicit] The framework is explicitly designed for text-based user-LLM interactions, with no mention of handling multimodal data.
- Why unresolved: The paper focuses solely on text-based personalization and does not explore how the framework might extend to multimodal data, which is increasingly relevant for edge devices.
- What evidence would resolve it: Experimental results or theoretical analysis demonstrating the framework's ability to process and integrate multimodal data streams, such as combining text with images or speech inputs.

### Open Question 3
- Question: What is the impact of varying the number of semantically similar pairs generated during data synthesis on the framework's performance and computational efficiency?
- Basis in paper: [explicit] The paper mentions generating three additional dialogue sets per original set but does not thoroughly explore the trade-offs between the number of generated pairs and performance.
- Why unresolved: While the paper briefly discusses the relationship between the number of generated pairs and ROUGE-1 scores, it does not provide a comprehensive analysis of how this affects computational efficiency or scalability.
- What evidence would resolve it: Detailed experimental results showing ROUGE-1 scores, training time, and memory usage for varying numbers of generated pairs (e.g., 1, 3, 6, 10) across different datasets and buffer sizes.

## Limitations

- The paper lacks ablation studies isolating the individual impact of each quality metric (EOE, DSS, IDD) on overall performance
- Synthetic data generation quality and its potential to introduce bias or noise is not thoroughly evaluated, particularly for edge cases or underrepresented domains
- The framework is designed specifically for text-based interactions and does not address multimodal data streams

## Confidence

- **High Confidence**: The buffer-based data selection framework and the use of unsupervised quality metrics for on-device personalization are well-justified by the problem context and demonstrated improvements over baseline methods.
- **Medium Confidence**: The specific formulations of EOE, DSS, and IDD metrics appear sound, but their individual effectiveness and robustness across different domains and user behaviors require further validation.
- **Low Confidence**: The synthetic data generation process's impact on fine-tuning quality and its potential to introduce noise or bias is not thoroughly evaluated, particularly for edge cases or underrepresented domains.

## Next Checks

1. Conduct experiments removing each quality metric (EOE, DSS, IDD) individually to quantify their individual contributions to the overall performance improvement.
2. Analyze the distribution of ROUGE-1 scores for synthetic pairs and investigate whether low-quality synthetic data negatively impacts fine-tuning, especially in domain-specific contexts.
3. Test the system's performance under varying user interaction patterns (e.g., rapid topic switching, infrequent engagement) to assess the buffer's ability to adapt to dynamic personalization needs.