---
ver: rpa2
title: Enhancing the Rationale-Input Alignment for Self-explaining Rationalization
arxiv_id: '2312.04103'
source_url: https://arxiv.org/abs/2312.04103
tags:
- rationale
- predictor
- input
- full
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a critical problem in self-explaining rationalization
  called rationale shift, where selected rationales may deviate semantically from
  the original input while still enabling accurate predictions, leading to misleading
  feedback to the generator. To address this issue, the authors propose DAR (Discriminatively
  Aligned Rationalization), which employs a pretrained predictor as a discriminator
  to align selected rationales with the full input.
---

# Enhancing the Rationale-Input Alignment for Self-explaining Rationalization

## Quick Facts
- arXiv ID: 2312.04103
- Source URL: https://arxiv.org/abs/2312.04103
- Reference count: 40
- Primary result: Proposed method DAR significantly improves rationale quality (up to 8.6% F1 score) by preventing "rationale shift" where selected rationales deviate from input semantics while still enabling accurate predictions.

## Executive Summary
This paper addresses a critical problem in self-explaining rationalization called "rationale shift," where selected rationales may deviate semantically from the original input while still enabling accurate predictions. This creates misleading feedback to the generator and undermines interpretability. The authors propose DAR (Discriminatively Aligned Rationalization), which employs a pretrained predictor as a discriminator to align selected rationales with the full input. Theoretical analysis shows that DAR ensures the predictor generalizes to full inputs, thus preventing rationale shift. Experiments on BeerAdvocate and HotelReview datasets demonstrate significant improvements in explanation quality compared to state-of-the-art methods.

## Method Summary
DAR addresses rationale shift by pretraining an auxiliary predictor on full input text and then freezing it as a discriminator during joint training of the generator and main predictor. The generator selects binary masks over input tokens, while the predictor makes predictions from the selected rationale. The training objective combines cross-entropy prediction loss on the rationale, alignment loss via the frozen auxiliary predictor, and sparsity regularization. This forces the selected rationale to preserve the same predictive information as the full input, preventing deviation. The method uses 200-dim GRU encoders/decoders with Gumbel-softmax for binarization, and GloVe embeddings.

## Key Results
- DAR achieves up to 8.6% F1 score improvement on BeerAdvocate and HotelReview datasets compared to state-of-the-art methods
- Synthetic experiments validate DAR's effectiveness in handling rationale shift
- Method shows strong robustness even with very sparse rationales or when generator/predictor is deliberately skewed during training
- Empirical results demonstrate significant improvements in explanation quality while maintaining prediction accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A predictor that generalizes to the full input enforces alignment between rationale and original text semantics.
- Mechanism: DAR pretrains a predictor on the full input, then fixes it as a discriminator to guide the generator. This forces the selected rationale to preserve the same predictive information as the full input, preventing deviation.
- Core assumption: The entropy of the label given the full input is lower than the entropy given a partial rationale (Lemma 1).
- Evidence anchors: [abstract]: "We theoretically illustrate how DAR accomplishes the desired alignment, thereby overcoming the rationale shift problem."

### Mechanism 2
- Claim: The vanilla cooperative game encourages predictors to overfit to rationales rather than the true input semantics.
- Mechanism: In RNP, the predictor is trained only on rationales. Because rationales contain fewer conditions, the predictor can more easily fit trivial patterns in them than the true semantics, leading to rationale shift.
- Core assumption: Conditioning on fewer variables (Z) reduces the complexity of the learning task compared to conditioning on the full input (X).
- Evidence anchors: [abstract]: "Rationale shift refers to a situation where the semantics of the selected rationale may deviate from the original input, but the predictor still produces accurate predictions based on the deviation."

### Mechanism 3
- Claim: DAR ensures the predictor's output is invariant to whether the input is the full text or the selected rationale.
- Mechanism: By pretraining the auxiliary predictor on full inputs and aligning its output distribution with that of the rationale-based predictor, DAR forces P(Ŷ|Z) = P(Ŷ|X), ensuring consistent predictions.
- Core assumption: If both predictors converge to the true label distribution P(Y|X), their outputs must match for inputs Z that preserve X's information.
- Evidence anchors: [abstract]: "We theoretically illustrate how DAR accomplishes the desired alignment, thereby overcoming the rationale shift problem."

## Foundational Learning

- Concept: Mutual information and entropy
  - Why needed here: Used to argue that the full input is more informative about the label than a partial rationale, justifying the need for alignment.
  - Quick check question: If I(X;Y) > I(Z;Y), what does that imply about the predictive difficulty of X vs Z?

- Concept: Kullback-Leibler divergence
  - Why needed here: Used to show that conditioning on fewer variables can reduce the divergence between true and predicted distributions, explaining why predictors overfit rationales.
  - Quick check question: How does DKL(P(Y|Z)||P(Ŷ|Z)) compare to DKL(P(Y|X)||P(Ŷ|X)) when Z is a strict subset of X?

- Concept: Adversarial training alignment
  - Why needed here: The core idea behind DAR—using a fixed pretrained discriminator to align distributions—comes from adversarial learning literature.
  - Quick check question: In GANs, what role does the fixed discriminator play compared to a trainable one?

## Architecture Onboarding

- Component map:
  Generator (fG) -> Predictor (fP) -> Predictions
  Pretrained Predictor (fPt) -> Alignment loss
  Full input -> Pretrained Predictor training

- Critical path:
  1. Pretrain fPt on full input → X→Y mapping
  2. Train generator and predictor jointly with loss: cross-entropy on rationale + cross-entropy via fPt on rationale + sparsity regularization
  3. At inference, only fG and fP are used; fPt is discarded

- Design tradeoffs:
  - Fixed vs trainable discriminator: Fixed avoids overfitting to rationale deviation but may be suboptimal if pretraining is poor
  - Sparsity regularization: Balances interpretability vs information preservation

- Failure signatures:
  - Low F1 despite high accuracy: predictor may be overfitting rationales
  - Pretrained predictor performs poorly on full input: pretraining data distribution mismatch
  - Generator selects near-random tokens: sparsity regularization too strong or generator not learning

- First 3 experiments:
  1. Verify that fPt achieves high accuracy on full input during pretraining
  2. Check that DAR's predictor matches fPt's output distribution on rationale vs full input
  3. Compare F1 and accuracy of DAR vs RNP on held-out test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can DAR be extended to other domains beyond text, such as computer vision or graph learning, while maintaining its effectiveness?
- Basis in paper: [explicit] The authors state that "Given the flexibility of the self-explaining rationalization framework, our proposed methods hold great potential for application to diverse fields such as computer vision and graph learning."
- Why unresolved: The paper focuses solely on text-based data and does not provide any experimental results or theoretical analysis for other domains.

### Open Question 2
- Question: What specific properties of pretrained language models cause them to perform poorly under the self-explaining rationalization framework, and can these issues be mitigated?
- Basis in paper: [explicit] The authors note that "The empirical results with pretrained models do not exhibit significant improvements compared to those achieved with GRUs" and acknowledge that "how to utilize pretrained language models in the self-explaining framework of rationalization is a big challenge."

### Open Question 3
- Question: How does the choice of the pretrained predictor in DAR affect the alignment between rationales and full inputs, and can this component be optimized?
- Basis in paper: [inferred] The effectiveness of DAR depends on the discriminative predictor, but the paper uses a simple predictor pretrained on full inputs without exploring alternative architectures or optimization strategies.

## Limitations

- The theoretical claims about entropy reduction and convergence guarantees lack rigorous proofs in the main text
- Empirical validation, while showing improvements on two datasets, doesn't systematically explore failure cases or ablations
- The paper doesn't investigate how different predictor architectures or training strategies might improve alignment quality
- No comprehensive analysis of why pretrained models struggle in the rationalization framework

## Confidence

- **High confidence**: The identification of rationale shift as a meaningful problem in self-explaining rationalization, and the basic observation that pretraining a predictor on full inputs before using it as a fixed discriminator is a reasonable approach
- **Medium confidence**: The synthetic experiments showing that DAR handles rationale shift better than baselines, though the synthetic setup's realism is unclear
- **Low confidence**: The theoretical analysis section's claims about entropy and the assertion that DAR guarantees prevention of rationale shift in all cases

## Next Checks

1. **Ablation study on the discriminator**: Remove the pretrained predictor entirely and compare performance to verify that the discriminative alignment component is essential for preventing rationale shift, not just improving overall accuracy

2. **Stress test on adversarial rationales**: Construct synthetic test cases where rationales are deliberately shifted from input semantics but still preserve class-indicative tokens, then verify that DAR's predictor performance degrades while baselines maintain high accuracy

3. **Pretraining sensitivity analysis**: Systematically vary the quality and quantity of pretraining data for the auxiliary predictor to determine the minimum requirements for effective alignment, and test whether poor pretraining still prevents rationale shift