---
ver: rpa2
title: Grey-box Bayesian Optimization for Sensor Placement in Assisted Living Environments
arxiv_id: '2309.05784'
source_url: https://arxiv.org/abs/2309.05784
tags:
- sensor
- placement
- sensors
- optimization
- dgbo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Distribution-Guided Bayesian Optimization
  (DGBO) to optimize motion sensor placement in indoor environments for activity recognition.
  The key idea is to leverage domain knowledge about the spatial distribution of activities
  by estimating the expected information gain for each sensor location and incorporating
  this into the Bayesian optimization acquisition function.
---

# Grey-box Bayesian Optimization for Sensor Placement in Assisted Living Environments

## Quick Facts
- **arXiv ID**: 2309.05784
- **Source URL**: https://arxiv.org/abs/2309.05784
- **Reference count**: 40
- **Primary result**: DGBO achieves superior activity recognition accuracy with 51.3% fewer expensive function queries compared to baselines

## Executive Summary
This paper introduces Distribution-Guided Bayesian Optimization (DGBO) for optimizing motion sensor placement in indoor environments to maximize activity recognition accuracy. The key innovation is incorporating domain knowledge about spatial activity distributions into the Bayesian optimization process through spatial credit assignment. DGBO estimates expected information gain for each sensor location based on prior observations and biases the acquisition function toward high-information regions. Experiments in simulated assisted living suites and a real-world testbed demonstrate that DGBO outperforms vanilla Bayesian optimization, genetic algorithms, and greedy methods while requiring significantly fewer expensive function evaluations.

## Method Summary
The method uses SIMsis to simulate motion sensor triggers and activities from ADL plans, then applies a random forest classifier to evaluate F1-scores for different sensor placements. DGBO iteratively selects query points by balancing expected information gain with expected improvement through a scalarized acquisition function. The algorithm maintains information profiles for each possible sensor location and uses these to guide the search toward regions likely to improve activity recognition. The approach is compared against vanilla BO, genetic algorithms, and greedy methods across different numbers of sensors and environment configurations.

## Key Results
- DGBO achieves 51.3% fewer expensive function queries on average compared to BO
- Superior F1-scores across all test environments and sensor configurations
- Consistent performance advantage across different activity classification models (RF, gradient boosting, KNN)
- Effective sample efficiency in both simulated and real-world testbed environments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: DGBO incorporates spatial distribution knowledge to improve exploration efficiency in sensor placement optimization.
- **Mechanism**: DGBO uses spatial credit assignment to estimate the expected information gain of each sensor location based on prior observations, then biases the acquisition function toward high-information regions.
- **Core assumption**: The spatial distribution of activities can be learned from noisy observations and meaningfully guides sensor placement.
- **Evidence anchors**: [abstract] "Our key technical contribution lies in capturing domain-specific knowledge about the spatial distribution of activities and incorporating it into the iterative selection of query points in Bayesian optimization."
- **Break condition**: If the spatial distribution of activities is highly uniform or if sensor placement has negligible impact on activity recognition accuracy, the information gain estimates become uninformative and the bias may misguide the search.

### Mechanism 2
- **Claim**: DGBO achieves better sample efficiency than vanilla BO by combining exploitation and exploration tendencies through scalarization.
- **Mechanism**: DGBO computes an acquisition function αDG(x) that balances expected information gain and expected improvement, then maximizes their sum to select the next query point.
- **Core assumption**: Both information gain and expected improvement are valid and complementary objectives for sensor placement optimization.
- **Evidence anchors**: [abstract] "DGBO achieves superb performance with respect to the activity recognition accuracy and better sample efficiency than BO across all test environments."
- **Break condition**: If the trade-off between exploitation and exploration is poorly tuned, the scalarization may either overly exploit or explore, degrading both sample efficiency and solution quality.

### Mechanism 3
- **Claim**: The SIMsis simulator provides realistic motion sensor triggers that enable accurate evaluation of sensor placement strategies.
- **Mechanism**: SIMsis generates synthetic datasets of sensor triggers and corresponding activities from ADL plans and building models, allowing evaluation of activity recognition models without requiring expensive real-world data collection.
- **Core assumption**: Simulated sensor triggers and activities closely approximate real-world distributions and patterns.
- **Evidence anchors**: [section] "To efficiently obtain motion sensor data for each candidate sensor placement, we use a simulator of smart indoor spaces (SIMsis) developed by Golestanet al. [19] ... generates a synthetic dataset with sensor readings and their corresponding activity for each of the N occupants."
- **Break condition**: If the simulator fails to capture critical dynamics (e.g., occlusions, sensor failures, non-stationary occupant behavior), the evaluation may not generalize to real-world deployments.

## Foundational Learning

- **Concept**: Bayesian Optimization
  - Why needed here: BO provides a sample-efficient way to optimize the expensive-to-evaluate black-box function f(x) that measures activity recognition accuracy for a given sensor placement.
  - Quick check question: What is the role of the surrogate model in BO, and how does it guide the selection of the next query point?

- **Concept**: Grey-box Optimization
  - Why needed here: Grey-box optimization allows incorporation of domain knowledge (spatial activity distribution) into the BO process, improving both solution quality and sample efficiency.
  - Quick check question: How does grey-box optimization differ from black-box optimization in terms of information used during the optimization process?

- **Concept**: Spatial Credit Assignment
  - Why needed here: Spatial credit assignment estimates the contribution of each sensor location to the overall activity recognition accuracy, enabling DGBO to prioritize high-information regions.
  - Quick check question: How does spatial credit assignment in DGBO relate to temporal credit assignment in reinforcement learning?

## Architecture Onboarding

- **Component map**: SIMsis (simulator) -> Activity Classifier (F1-score calculator) -> DGBO/BO/Greedy/GA (optimizer) -> Sensor Placement (output)
- **Critical path**: Sensor Placement -> SIMsis -> Activity Classifier -> F1-score -> Optimizer -> Next Sensor Placement
- **Design tradeoffs**:
  - High-fidelity simulation vs. computational cost
  - Exploitation vs. exploration in acquisition function design
  - Sensor granularity (ϵ) vs. search space size
- **Failure signatures**:
  - Poor activity recognition accuracy despite high F1-score in simulation
  - Optimizer converges to suboptimal sensor placements
  - Simulator fails to generate realistic sensor triggers
- **First 3 experiments**:
  1. Verify that SIMsis generates realistic sensor triggers by comparing simulated and real-world datasets.
  2. Test that the activity classifier achieves reasonable F1-scores on simulated data with known ground truth.
  3. Run DGBO and BO on a small, simple environment (e.g., 3×3 grid) to ensure basic functionality and compare sample efficiency.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Limited testing on environments larger than 8×8 meters, raising scalability concerns
- Reliance on simulated data may not capture all real-world complexities and sensor failure modes
- Performance impact when activity patterns are non-stationary or change over time is not investigated

## Confidence
- **High confidence**: DGBO achieves superior F1-scores compared to baseline methods (51.3% fewer queries on average)
- **Medium confidence**: The mechanism of incorporating spatial distribution knowledge improves sample efficiency
- **Medium confidence**: The SIMsis simulator provides realistic enough evaluation data for indoor environments

## Next Checks
1. **Validation check 1**: Conduct ablation studies removing the spatial credit assignment component to quantify its specific contribution to DGBO's performance gains.
2. **Validation check 2**: Test DGBO on real-world sensor data from multiple assisted living facilities to assess generalization beyond simulated environments.
3. **Validation check 3**: Evaluate DGBO's robustness to sensor failures and occlusions by introducing realistic noise patterns into the simulator.