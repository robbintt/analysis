---
ver: rpa2
title: Look-back Decoding for Open-Ended Text Generation
arxiv_id: '2305.13477'
source_url: https://arxiv.org/abs/2305.13477
tags:
- look-back
- decoding
- prefix
- generation
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of degeneration in open-ended text
  generation, where large language models often produce repetitive or incoherent text.
  The proposed method, Look-back, uses Kullback-Leibler divergence to track the distribution
  distance between the current and historical decoding steps, allowing it to automatically
  predict and avoid repetitive phrases and topic drifts.
---

# Look-back Decoding for Open-Ended Text Generation

## Quick Facts
- arXiv ID: 2305.13477
- Source URL: https://arxiv.org/abs/2305.13477
- Authors: 
- Reference count: 20
- One-line primary result: Look-back decoding significantly outperforms other methods on document continuation and story generation, achieving highest MAUVE and coherence scores while maintaining diversity.

## Executive Summary
This paper addresses degeneration problems in open-ended text generation, where large language models often produce repetitive or incoherent text. The proposed method, Look-back, uses Kullback-Leibler divergence to track the distribution distance between current and historical decoding steps, automatically predicting and avoiding repetitive phrases and topic drifts. The method demonstrates state-of-the-art performance on both automatic and human evaluations across document continuation and story generation tasks.

## Method Summary
Look-back decoding uses KL divergence to detect repetition risk by comparing probability distributions between consecutive decoding steps and coherence drift by comparing current step to the prefix. When repetition risk is detected (low KL divergence to historical steps), the method samples tokens according to their negative KL divergence to the prefix, biasing toward topic-relevant tokens. The algorithm takes prefix text C, a language model with vocabulary V, beam size k, and threshold α as inputs, computing KL divergence at each decoding step to decide between sampling and argmax selection.

## Key Results
- Look-back achieves highest MAUVE scores and coherence scores while maintaining similar diversity levels as human text
- Significantly outperforms strong baselines (nucleus, typical, η-sampling, SimCTG) on both WikiText-103 and WritingPrompts datasets
- Maintains superior performance in human evaluations for fluency and coherence

## Why This Works (Mechanism)

### Mechanism 1
KL divergence between current and prior decoding steps signals repetition risk. When probability distributions between consecutive decoding steps are very similar, the model is likely repeating tokens or phrases. Look-back detects this by computing the minimum KL divergence to any prior step and uses it as an alarm signal.

### Mechanism 2
KL divergence between current step and prefix signals coherence drift. If this divergence is large, the model is likely drifting off-topic. When repetition risk is low, tokens are sampled according to their negative KL divergence to the prefix, biasing toward topic-relevant tokens.

### Mechanism 3
Sampling according to softmax of negative KL divergence improves coherence while maintaining diversity. Instead of uniform sampling from top-k tokens when repetition risk is detected, Look-back samples according to softmax(-KL_t+1,v|C_min), biasing selection toward tokens whose next-step distribution is close to the prefix.

## Foundational Learning

- **Concept**: KL divergence as a measure of distribution similarity
  - Why needed here: Look-back relies on KL divergence to quantify how close current decoding steps are to historical steps or the prefix
  - Quick check question: What property of KL divergence makes it suitable for detecting repetition in text generation?

- **Concept**: Probability distribution space in language models
  - Why needed here: The method operates by comparing probability distributions over tokens, not just token embeddings or hidden states
  - Quick check question: How does the probability distribution over tokens differ from the hidden state representation in a transformer?

- **Concept**: Decoding algorithms and their degeneration problems
  - Why needed here: Understanding why greedy, nucleus, and other sampling methods fail helps contextualize Look-back's improvements
  - Quick check question: What are the two main degeneration problems in open-ended text generation that Look-back addresses?

## Architecture Onboarding

- **Component map**: Prefix C → KL divergence computation → repetition/coherence check → token selection → append to output → repeat
- **Critical path**: Prefix → KL divergence computation → repetition/coherence check → token selection → append to output → repeat
- **Design tradeoffs**: Using KL divergence vs. cosine similarity trades interpretability for applicability to models without accessible hidden states; threshold α balances avoiding repetition and maintaining diversity; sampling strategy trades coherence improvement for potential diversity loss
- **Failure signatures**: High KL_t_min consistently below threshold → excessive sampling, potential incoherence; KL_t|C_min consistently high → topic drift despite Look-back; model probability distributions poorly calibrated → KL divergence not reflective of actual repetition/coherence risk
- **First 3 experiments**:
  1. Implement KL divergence computation between current and prefix distributions on a small text sample
  2. Test KL_t_min detection on a repetitive text generation example
  3. Compare uniform vs softmax sampling when KL_t_min is below threshold on a coherence-sensitive task

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal measure for text output distributions to predict degeneration in open-ended text generation? The authors mention that KL divergence may not be the optimal measure and suggest investigating other measures in future work.

### Open Question 2
How can we explicitly distinguish between natural repetitions and undesired repetitions in open-ended text generation? Look-back penalizes tokens with low KL divergence to historical distributions but cannot explicitly distinguish if such tokens are natural or undesired repetitions.

### Open Question 3
What is the impact of using a more comprehensive evaluation setup on the performance of Look-back and other decoding algorithms? The authors suggest that adopting a more comprehensive evaluation setup in the future could better capture performance across various contexts.

### Open Question 4
How can we improve the development of NLG algorithms to better reflect the quality of generated text, considering the limitations of current automatic metrics? The authors discuss the limitations of automatic metrics like MAUVE scores in truthfully reflecting text quality.

## Limitations
- Limited external validation of core mechanisms, relying heavily on internal empirical observations rather than corpus-based validation
- Dataset-specific performance with no established generalization to other domains or languages
- Fixed threshold α = 4.0 used for all experiments without systematic sensitivity analysis

## Confidence
- **High confidence** in: The empirical performance improvements demonstrated by Look-back on the tested datasets
- **Medium confidence** in: The theoretical mechanism claims about KL divergence detecting repetition and coherence drift
- **Low confidence** in: The generalizability of the method across different domains, languages, and model architectures

## Next Checks
1. Test the hypothesis that low KL divergence between consecutive decoding steps indicates repetition by analyzing a corpus of human-written text for correlation between KL divergence values and actual repetitive patterns
2. Apply Look-back to at least two additional domains (e.g., technical documentation and conversational dialogue) with different lexical and semantic characteristics
3. Systematically vary the threshold α across multiple orders of magnitude on both datasets and measure the impact on all evaluation metrics