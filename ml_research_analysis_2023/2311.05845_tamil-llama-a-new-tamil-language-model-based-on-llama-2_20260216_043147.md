---
ver: rpa2
title: 'Tamil-Llama: A New Tamil Language Model Based on Llama 2'
arxiv_id: '2311.05845'
source_url: https://arxiv.org/abs/2311.05845
tags:
- tamil
- llama
- language
- text
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Tamil-LLaMA, a Tamil language model built
  by extending the LLaMA 2 architecture with 16,000 additional Tamil tokens and fine-tuning
  on a comprehensive Tamil corpus. The authors employed the LoRA methodology for efficient
  training and developed Tamil-translated versions of the Alpaca and OpenOrca datasets
  for instruction fine-tuning.
---

# Tamil-Llama: A New Tamil Language Model Based on Llama 2

## Quick Facts
- arXiv ID: 2311.05845
- Source URL: https://arxiv.org/abs/2311.05845
- Reference count: 4
- A Tamil language model based on LLaMA 2 architecture with 16,000 additional Tamil tokens, achieving GPT-4-assessed score of 71.17 on Tamil instruction tasks

## Executive Summary
This paper introduces Tamil-LLaMA, a Tamil language model built by extending the LLaMA 2 architecture with 16,000 additional Tamil tokens and fine-tuning on a comprehensive Tamil corpus. The authors employed the LoRA methodology for efficient training and developed Tamil-translated versions of the Alpaca and OpenOrca datasets for instruction fine-tuning. Their 13B model achieved an overall GPT-4-assessed score of 71.17 on Tamil instruction tasks, outperforming GPT-3.5-turbo's score of 61.33 and showing significant improvements in Tamil text generation and understanding compared to baseline LLaMA models. The work addresses the underrepresentation of Tamil in large language models and provides an open-source solution for Tamil language processing.

## Method Summary
The authors extended the LLaMA 2 tokenizer with 16,000 Tamil tokens using SentencePiece, creating a 48k vocabulary model. They pre-trained the extended model on 12GB of Tamil text from the CulturaX dataset using LoRA (rank 64) for 1 epoch. Instruction fine-tuning was performed on Tamil-translated versions of the Alpaca dataset (52k instructions) and OpenOrca subset (93k instructions) for 1-2 epochs. The LoRA adapters were inserted into attention layers, allowing efficient adaptation to Tamil-specific patterns while keeping most parameters frozen.

## Key Results
- Tamil-LLaMA 13B achieved a GPT-4-assessed score of 71.17 on Tamil instruction tasks
- Outperformed GPT-3.5-turbo's score of 61.33 on the same evaluation
- Achieved 81.3% accuracy on IndicSentiment-7B dataset and 80.12% on IndicGLUE Text Classification dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding 16,000 Tamil tokens directly improves token efficiency for Tamil text encoding.
- Mechanism: Tamil characters are typically encoded as 3-4 byte tokens in the original LLaMA tokenizer. By adding dedicated Tamil tokens, each Tamil character can be represented by a single token, reducing sequence length and improving processing speed.
- Core assumption: Token efficiency gains will translate into measurable performance improvements in text generation and comprehension.
- Evidence anchors: [abstract]: "This paper addresses this lacuna, enhancing the open-source LLaMA model with an addition of 16,000 Tamil tokens, aiming to achieve superior text generation and comprehension in the Tamil language." [section 3.3]: "In Figure 1, we can see that the Tamil LLaMA tokenizer needs only 20% to 25% of the tokens that the original LLaMA model uses to encode Tamil text."

### Mechanism 2
- Claim: LoRA-based fine-tuning allows efficient adaptation to Tamil language tasks without full model retraining.
- Mechanism: LoRA (Low-Rank Adaptation) modifies the weight matrices of the attention layers and MLPs by learning low-rank decompositions, allowing the model to adapt to Tamil-specific patterns while keeping most parameters frozen.
- Core assumption: The low-rank structure is sufficient to capture the nuances of Tamil language without requiring full parameter updates.
- Evidence anchors: [abstract]: "We strategically employ the LoRA methodology for efficient model training on a comprehensive Tamil corpus, ensuring computational feasibility and model robustness." [section 3.6.1]: "LoRA (Low-Rank Adapters) is a technique that offers an efficient pathway to fine-tuning large language models, as introduced by Hu et al. (2021)."

### Mechanism 3
- Claim: Instruction fine-tuning on Tamil-translated datasets improves the model's ability to follow task-oriented instructions in Tamil.
- Mechanism: By fine-tuning on translated versions of instruction datasets like Alpaca and OpenOrca, the model learns to map natural language instructions to appropriate responses in Tamil context.
- Core assumption: Translation of instruction datasets preserves the semantic structure needed for the model to learn the instruction-following behavior.
- Evidence anchors: [abstract]: "Moreover, we introduce a Tamil-translated version of the Alpaca dataset and a subset of the OpenOrca dataset tailored for instruction fine-tuning." [section 3.1.2]: "The 'Instruction Tuning' phase was a pivotal stage in refining LLaMA's proficiency in precisely adhering to textual instructions."

## Foundational Learning

- Concept: Tokenization and vocabulary design
  - Why needed here: Understanding how different tokenization strategies affect model performance is crucial for implementing the 16,000 Tamil token addition.
  - Quick check question: How does SentencePiece tokenization differ from byte-pair encoding, and why is it suitable for Tamil?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: LoRA modifies attention weights, so understanding how attention works in transformers is essential for grasping the fine-tuning approach.
  - Quick check question: What is the role of QKV matrices in transformer attention, and how does LoRA modify them?

- Concept: Instruction fine-tuning methodology
  - Why needed here: The model's instruction-following capability depends on understanding how fine-tuning on instruction datasets shapes model behavior.
  - Quick check question: What is the difference between standard fine-tuning and instruction fine-tuning, and why is it important for task-oriented models?

## Architecture Onboarding

- Component map: LLaMA 2 base model with expanded vocabulary (32k → 48k tokens) → LoRA adapters in attention layers → Fine-tuned weights for Tamil tasks → Modified tokenizer with Tamil characters → Resized model head for larger vocabulary

- Critical path: Tokenization → Pre-training on Tamil corpus with LoRA → Instruction fine-tuning on translated datasets → Evaluation. Each step depends on the successful completion of the previous one.

- Design tradeoffs: The choice of 16,000 Tamil tokens balances coverage of Tamil characters with computational efficiency. LoRA is chosen over full fine-tuning to reduce computational costs while still achieving good performance. The translation-based instruction fine-tuning approach is cost-effective but may introduce some semantic loss.

- Failure signatures: If the model produces garbled Tamil text, it suggests tokenization or pre-training issues. Poor instruction-following indicates problems with the fine-tuning phase. Unexpected behavior in translation tasks might indicate semantic loss during dataset translation.

- First 3 experiments:
  1. Test the new Tamil tokenizer on sample Tamil text to verify token efficiency improvements (should use 20-25% of original token count).
  2. Run pre-training for a small number of steps and check loss curves to ensure the model is learning from the Tamil corpus.
  3. Evaluate instruction-following capability on a small subset of translated instructions to verify the fine-tuning process is working.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of Tamil-LLaMA models change if trained on a significantly larger Tamil corpus beyond the current 12GB?
- Basis in paper: [explicit] The paper acknowledges computational constraints limited training to 12GB of Tamil text, stating "we believe that the true potential can be unlocked with access to a broader data spectrum"
- Why unresolved: The current models were trained on a limited dataset due to computational and cost constraints, but the impact of training on a larger corpus is not empirically tested
- What evidence would resolve it: Training the same model architecture on progressively larger Tamil corpora (e.g., 50GB, 100GB, 500GB) and comparing performance metrics across the same evaluation suite

### Open Question 2
- Question: What is the precise impact of translation loss on instruction fine-tuning performance, and how could it be mitigated?
- Basis in paper: [explicit] The authors note that "the instructional prompts used for fine-tuning the Tamil LLaMA base models are derived from English datasets translated into Tamil, [creating] a potential for nuanced inaccuracies—commonly referred to as translation loss"
- Why unresolved: While translation loss is acknowledged as a concern, the paper does not quantify its impact on model performance or propose specific mitigation strategies
- What evidence would resolve it: A controlled experiment comparing models fine-tuned with: 1) translated instructions, 2) native Tamil instructions, and 3) a mixed approach, measuring performance differences across all evaluation tasks

### Open Question 3
- Question: How does the performance gap between Tamil-LLaMA and GPT-4/gpt-3.5-turbo evolve for more complex reasoning tasks beyond those tested?
- Basis in paper: [explicit] The paper shows Tamil-LLaMA 13B outperforms gpt-3.5-turbo overall but notes "challenges in literature/entertainment and other areas can be attributed to data limitations" and "Mathematical reasoning presents a significant challenge for our models"
- Why unresolved: The evaluation suite covers a range of tasks but doesn't include more advanced reasoning challenges that might reveal deeper limitations of the Tamil-LLaMA models
- What evidence would resolve it: Testing both models on a comprehensive suite of complex reasoning tasks (mathematical proofs, logical puzzles, multi-step inference problems) and analyzing failure patterns to identify specific architectural or training limitations

## Limitations

- Translation quality impact: The instruction fine-tuning relies on translated versions of English datasets, and the impact of translation loss on model performance is not quantified.
- Dataset representativeness: The exact composition and representativeness of the 12GB Tamil subset from CulturaX is not specified, raising questions about potential bias toward specific text types.
- Evaluation methodology gaps: The primary evaluation uses GPT-4 assessment, but lacks transparency in evaluation prompts, scoring rubric, and inter-annotator agreement details.

## Confidence

- High Confidence: The token efficiency improvement claim (20-25% token reduction) is directly supported by empirical evidence and follows logically from the mechanics of adding dedicated Tamil tokens.
- Medium Confidence: The overall performance improvement (71.17 vs 61.33 GPT-4 score) is reported with specific numbers, but the evaluation methodology lacks transparency.
- Low Confidence: The claim about "significant improvements in Tamil text generation and understanding" is vague and not substantiated with specific examples or qualitative analysis.

## Next Checks

1. **Tokenizer Validation**: Test the Tamil tokenizer on diverse Tamil text samples (news articles, social media, literature) to verify the claimed 20-25% token reduction holds across different domains and text styles, and examine whether the new tokens are being used appropriately by the model during generation.

2. **Translation Quality Assessment**: Conduct a systematic comparison of model performance on original English instructions versus Tamil-translated instructions for a subset of tasks, and perform human evaluation of the translation quality to determine if semantic drift is affecting instruction-following capability.

3. **Output Quality Analysis**: Generate sample outputs for complex Tamil language tasks (creative writing, technical explanations, dialogue) and perform detailed qualitative analysis comparing Tamil-LLaMA outputs with baseline LLaMA and GPT-3.5-turbo, focusing on coherence, factual accuracy, and language appropriateness rather than just aggregate scores.