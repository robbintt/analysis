---
ver: rpa2
title: Global Convergence of SGD For Logistic Loss on Two Layer Neural Nets
arxiv_id: '2309.09258'
source_url: https://arxiv.org/abs/2309.09258
tags:
- neural
- loss
- nets
- convergence
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proves the first convergence of Stochastic Gradient Descent
  (SGD) to global minima for logistic loss on two-layer neural networks with any number
  of gates and smooth bounded activations (e.g., sigmoid, tanh), without assumptions
  on data or network width. The key idea is adding a constant amount of Frobenius
  norm regularization to the logistic loss, making it a "Villani function" which enables
  leveraging recent results on SGD convergence for such objectives.
---

# Global Convergence of SGD For Logistic Loss on Two Layer Neural Nets

## Quick Facts
- arXiv ID: 2309.09258
- Source URL: https://arxiv.org/abs/2309.09258
- Reference count: 40
- This work proves the first convergence of Stochastic Gradient Descent (SGD) to global minima for logistic loss on two-layer neural networks with any number of gates and smooth bounded activations (e.g., sigmoid, tanh), without assumptions on data or network width.

## Executive Summary
This paper establishes the first global convergence guarantees for Stochastic Gradient Descent (SGD) on logistic loss for two-layer neural networks with sigmoid, tanh, or SoftPlus activations. The key insight is that adding a constant amount of Frobenius norm regularization transforms the logistic loss into a "Villani function," which enables leveraging recent theoretical results on SGD convergence. The work demonstrates that SGD with constant step sizes converges to global minima in O(1/ϵ) steps for sigmoid and tanh activations, while SoftPlus achieves exponential convergence. Crucially, experiments show that the required regularization does not harm downstream classification accuracy.

## Method Summary
The method involves adding Frobenius norm regularization to the logistic loss function, transforming it into a "Villani function" that satisfies specific mathematical properties enabling convergence analysis. For sigmoid and tanh activations, SGD with constant step sizes converges to global minima in O(1/ϵ) steps using step-size O(ϵ). For SoftPlus activation, continuous-time SGD converges exponentially fast. The regularization threshold depends only on the activation function, input data norms, and outer layer weights, not on network width or data distribution specifics.

## Key Results
- SGD with constant step sizes converges to global minima for sigmoid/tanh activations in O(1/ϵ) steps using step-size O(ϵ)
- SoftPlus activation achieves exponential convergence in continuous-time SGD
- The required regularization amount does not harm downstream classification accuracy
- Results hold for any number of gates and network width without assumptions on data distribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding Frobenius norm regularization transforms logistic loss into a "Villani function" enabling SGD convergence to global minima.
- Mechanism: Regularization ensures the loss satisfies the dissipativity condition, making it a "confining function" that meets Villani function criteria. This allows leveraging existing convergence results for SGD on Villani functions.
- Core assumption: The threshold regularization amount depends only on activation function, input data norms, and outer layer weights, not on network width or data distribution specifics.
- Evidence anchors:
  - [abstract]: "Our key idea is to show the existence of Frobenius norm regularized logistic loss functions on constant-sized neural nets which are 'Villani functions' and thus be able to build on recent progress with analyzing SGD on such objectives."
  - [section 3]: "We note that the threshold values of regularization computed above, λc, do not explicitly depend on the training data or the neural architecture, consistent with observations in Anthony & Bartlett (2009); Zhang et al. (2021)."
  - [corpus]: Weak - no direct evidence found in corpus about Villani functions or their role in convergence.
- Break condition: If the regularization threshold cannot be achieved without affecting classification accuracy, or if the activation function doesn't satisfy the smoothness and boundedness conditions required for the Villani function property.

### Mechanism 2
- Claim: SGD on regularized logistic loss with sigmoid/tanh activations converges in O(1/ϵ) steps using step-size O(ϵ).
- Mechanism: The regularization makes the loss gradient-Lipschitz with a smoothness coefficient that depends on network parameters and regularization amount. This allows applying Shi et al.'s results to convert SGD guarantees to the continuous-time SDE framework, achieving convergence in O(1/ϵ) steps.
- Core assumption: The initial weights can be sampled from a distribution close enough to the stationary Gibbs measure for the convergence guarantee to hold.
- Evidence anchors:
  - [abstract]: "For nets with sigmoid or tanh gates, SGD with constant stepsizes converges to global minima in O(1/ϵ) steps using step-size O(ϵ)."
  - [section 3]: "We note that the 'time horizon' T above is a free parameter - which in turn parameterizes the choice of the step-size and the initial weight distribution."
  - [corpus]: Weak - no direct evidence found in corpus about O(1/ϵ) convergence rates for SGD on neural networks.
- Break condition: If the initial weight distribution cannot be controlled to be close enough to the stationary distribution, or if the gradient-Lipschitz constant is too large relative to the regularization amount.

### Mechanism 3
- Claim: For SoftPlus activation, continuous-time SGD converges exponentially fast to global minima.
- Mechanism: SoftPlus is Lipschitz continuous, making the regularized loss a Villani function. The SDE approximation of SGD converges to the global minimum at a linear rate due to the exponential mixing of the underlying dynamics.
- Core assumption: The SoftPlus activation maintains the necessary smoothness and boundedness properties for the Villani function construction.
- Evidence anchors:
  - [abstract]: "For SoftPlus activation, continuous-time SGD converges exponentially fast."
  - [section 3.1]: "The SoftPlus function is Lipschitz, hence using the same analysis as in (Appendix 5), we can claim that for λ > λc the loss function in Definition 1 with SoftPlus activations is a Villani function."
  - [corpus]: Weak - no direct evidence found in corpus about exponential convergence rates for SGD on neural networks with SoftPlus activation.
- Break condition: If the SoftPlus activation fails to satisfy the Lipschitz or smoothness conditions, or if the regularization amount is insufficient to ensure the Villani function property.

## Foundational Learning

- Concept: Villani functions and their role in SGD convergence
  - Why needed here: The core mechanism relies on showing the regularized logistic loss is a Villani function, which enables applying existing convergence results for SGD on such objectives.
  - Quick check question: What are the key properties that make a function a Villani function, and how does Frobenius norm regularization ensure these properties for the logistic loss?

- Concept: Poincaré-type inequalities and their relationship to convergence rates
  - Why needed here: The convergence rate guarantees depend on the Poincaré constant of the Gibbs measure induced by the regularized loss, which is determined by whether the loss is a Villani function.
  - Quick check question: How does the Poincaré inequality for the Gibbs measure translate to convergence guarantees for SGD, and what determines the value of the Poincaré constant?

- Concept: Gradient Lipschitzness and its impact on optimization dynamics
  - Why needed here: The smoothness coefficient of the regularized loss determines the step-size that can be used while maintaining convergence guarantees, as well as the approximation quality between SGD and its continuous-time SDE counterpart.
  - Quick check question: How does Frobenius norm regularization affect the gradient Lipschitzness of the logistic loss, and what is the relationship between the smoothness coefficient and the step-size that can be used?

## Architecture Onboarding

- Component map: Logistic loss -> Frobenius norm regularization -> SGD optimization -> Global minima convergence
- Critical path:
  1. Choose network architecture and activation function (sigmoid/tanh/SoftPlus)
  2. Compute regularization threshold λc based on activation function and network parameters
  3. Initialize weights from a distribution close to the stationary Gibbs measure
  4. Run SGD with step-size O(ϵ) for O(1/ϵ) steps
  5. Verify convergence to global minimum via downstream classification accuracy
- Design tradeoffs:
  - Regularization amount vs. classification accuracy: Too much regularization may harm generalization, while too little may prevent convergence guarantees
  - Network width vs. convergence guarantees: The results hold for any width, but wider networks may require larger regularization thresholds
  - Step-size vs. convergence rate: Larger step-sizes may lead to faster convergence but require tighter initial weight distributions
- Failure signatures:
  - Divergence of SGD iterates: Indicates insufficient regularization or inappropriate step-size
  - Slow convergence or getting stuck in local minima: Suggests the initial weight distribution is too far from the stationary Gibbs measure
  - Poor downstream classification accuracy: Implies the regularization amount is too large or the network architecture is not suitable for the task
- First 3 experiments:
  1. Verify convergence on a simple binary classification task with clearly separable data and a small network
  2. Test the impact of regularization amount on convergence rate and classification accuracy across different network widths
  3. Compare the convergence behavior of sigmoid, tanh, and SoftPlus activations on a fixed network architecture and dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SGD convergence to global minima be proven without regularization for logistic loss on two-layer neural networks?
- Basis in paper: The paper proves SGD convergence for logistic loss on two-layer neural networks only when adding a constant amount of Frobenius norm regularization. It states "it has remained an unresolved challenge to show convergence of SGD for logistic loss on any neural architecture with a constant number of gates while not constraining the distribution of the data to a specific functional form."
- Why unresolved: The authors explicitly note this as an open question and leave it for future work, suggesting current methods rely on regularization to make the loss function a "Villani function."
- What evidence would resolve it: A proof showing SGD convergence to global minima for logistic loss on two-layer neural networks without any regularization, for arbitrary data and network width.

### Open Question 2
- Question: Does an "optimal" regularization value exist for neural networks similar to the optimal ℓ2-regularizer found for linear regression?
- Basis in paper: The authors note that for linear regression, there exists an optimal ℓ2-regularizer value that makes the excess risk dimension-free. They state "it remains open to investigate if such a similar threshold of regularization also exists for nets."
- Why unresolved: The paper only establishes that a lower bound on regularization is needed for convergence, but doesn't identify if there's an optimal value that balances convergence and generalization.
- What evidence would resolve it: Analytical or experimental identification of an optimal regularization parameter for neural networks that minimizes a specific performance metric (e.g., generalization error).

### Open Question 3
- Question: Can the insights from this work be extended to more exotic loss functions and neural architectures?
- Basis in paper: The authors conclude by asking "if these insights could also resolve similar mysteries for more exotic loss functions and neural architectures that are in use."
- Why unresolved: The current work only addresses logistic loss on two-layer neural networks. The method of making the loss a "Villani function" through regularization may not directly apply to other architectures or loss functions.
- What evidence would resolve it: Successful application of the regularization-based convergence proof technique to other neural network architectures (e.g., deeper networks) or loss functions (e.g., cross-entropy).

## Limitations
- The theoretical guarantees rely on specific mathematical properties of activation functions that may not hold for all commonly used functions
- The derived regularization thresholds λc have not been empirically validated across diverse datasets
- The assumption that initial weights can be sampled from distributions close to the stationary Gibbs measure may be challenging to achieve in practice

## Confidence

- **High Confidence**: The theoretical framework connecting Frobenius norm regularization to Villani functions is well-established in the mathematical literature. The proof methodology following from existing results on SGD convergence for Villani functions is sound.
- **Medium Confidence**: The specific regularization thresholds λc derived for different activation functions are mathematically correct, but their practical achievability and impact on downstream performance requires empirical validation.
- **Low Confidence**: The exponential convergence rate claimed for SoftPlus activation has the weakest empirical support, as the continuous-time analysis may not accurately reflect discrete SGD behavior in practice.

## Next Checks

1. **Empirical Validation of Regularization Thresholds**: Systematically vary the regularization parameter λ across multiple datasets and network widths to empirically determine the minimum value required for convergence while maintaining classification accuracy. Compare observed thresholds with the theoretical λc values.

2. **Convergence Rate Verification**: Conduct controlled experiments measuring the actual number of iterations required to reach ε-accuracy for different step sizes and initial weight distributions. Compare empirical convergence rates with the theoretical O(1/ϵ) prediction, particularly for sigmoid and tanh activations.

3. **Cross-Architecture Generalization**: Test whether the convergence guarantees extend to architectures beyond two-layer networks, such as three-layer networks or networks with different activation functions (ReLU, Leaky ReLU). Investigate whether the Villani function framework can be generalized to these cases.