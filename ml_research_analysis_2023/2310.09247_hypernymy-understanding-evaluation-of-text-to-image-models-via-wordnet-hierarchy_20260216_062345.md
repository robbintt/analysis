---
ver: rpa2
title: Hypernymy Understanding Evaluation of Text-to-Image Models via WordNet Hierarchy
arxiv_id: '2310.09247'
source_url: https://arxiv.org/abs/2310.09247
tags:
- diffusion
- metrics
- image
- text-to-image
- synsets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes WordNet-based metrics to evaluate the hypernymy
  understanding of text-to-image models. It defines the In-Subtree Probability (ISP)
  and Subtree Coverage Score (SCS) metrics that leverage the WordNet hierarchy and
  ImageNet classifiers to measure the precision and diversity of generated images
  for a given prompt.
---

# Hypernymy Understanding Evaluation of Text-to-Image Models via WordNet Hierarchy

## Quick Facts
- arXiv ID: 2310.09247
- Source URL: https://arxiv.org/abs/2310.09247
- Reference count: 26
- The paper proposes WordNet-based metrics (ISP and SCS) to evaluate hypernymy understanding of text-to-image models.

## Executive Summary
This paper introduces two automatic metrics, In-Subtree Probability (ISP) and Subtree Coverage Score (SCS), to evaluate how well text-to-image models understand hypernymy relationships using WordNet hierarchy and ImageNet classifiers. The metrics measure both the precision of generated images (whether they fall within the correct semantic subtree) and their diversity (coverage of hyponyms). The authors evaluate popular models including GLIDE, Latent Diffusion, Stable Diffusion, and unCLIP, showing that the metrics correlate with human evaluation and provide granular insights into model performance. The analysis reveals connections between model performance, training data frequency, and language encoder capabilities.

## Method Summary
The method generates images for WordNet synsets using text-to-image models, classifies the generated images with a pretrained ImageNet classifier, and computes two metrics: ISP measures the probability that generated images fall within the correct semantic subtree, while SCS measures the diversity of hyponym coverage through KL divergence between average and individual sample distributions. The evaluation uses a prompt template "An image of a/an [lemma]." and generates 32 images per synset, resized to 224x224 for classification.

## Key Results
- ISP and SCS metrics correlate with human evaluation preferences for image quality and diversity
- Model performance on metrics shows significant correlations with concept frequency in training data
- Language encoder knowledge of hypernymy relationships significantly impacts overall model performance
- Classifier-free guidance trade-off: higher guidance improves precision (ISP) but reduces diversity (SCS)

## Why This Works (Mechanism)

### Mechanism 1
The proposed metrics effectively measure hypernymy understanding because they leverage WordNet's semantic hierarchy combined with ImageNet classifiers to evaluate whether generated images correspond to the correct concept and its hyponyms. For a given prompt synset, images are generated and classified using a pretrained ImageNet classifier. The ISP metric measures the probability that generated images fall within the classifiable subtree of the prompt, while SCS measures the diversity of hyponym coverage by computing KL divergence between the average hyponym distribution and individual sample distributions. Core assumption: ImageNet classifiers can accurately map generated images to WordNet synsets, and the WordNet hierarchy properly represents the "is-a" relationships relevant to image generation.

### Mechanism 2
The metrics correlate with human evaluation because they capture both precision (correct object generation) and diversity (coverage of hyponyms), which are fundamental aspects of what humans judge in image quality. Human evaluators were asked to compare image-caption similarity and sample diversity between models. The metrics show statistically significant correlations with these human preferences, particularly ISP for precision and SCS for diversity. Core assumption: Human judgments about image quality fundamentally align with the precision and diversity captured by the metrics.

### Mechanism 3
Model performance on ISP and SCS relates to training data frequency because models are more likely to generate concepts they've seen more often during training. The paper analyzes three popular datasets (LAION-400M, LAION-2B-en, COYO) and finds significant correlations between synset frequencies and metric values, particularly for weaker models. Core assumption: Training data frequency directly influences a model's ability to generate and understand specific concepts.

## Foundational Learning

- Concept: WordNet semantic hierarchy and synsets
  - Why needed here: The entire evaluation framework relies on WordNet's hypernymy relationships to define what constitutes correct image generation for a given prompt.
  - Quick check question: What is the difference between a hypernym and a hyponym in WordNet?

- Concept: Image classifier calibration and confidence calibration
  - Why needed here: The metrics depend on classifier probability outputs, which must be well-calibrated to provide meaningful measurements of whether generated images belong to the correct subtree.
  - Quick check question: Why is it important that the ImageNet classifier be well-calibrated for this evaluation?

- Concept: Diffusion model sampling and classifier-free guidance
  - Why needed here: The paper evaluates how different sampling parameters (steps, guidance scale) affect the metrics, requiring understanding of how these parameters influence generation quality.
  - Quick check question: How does increasing classifier-free guidance affect the trade-off between image precision and diversity?

## Architecture Onboarding

- Component map: Text-to-image generation models -> ImageNet classifier -> Evaluation pipeline (ISP/SCS computation)
- Critical path: prompt → image generation → image classification → metric computation. The classifier choice and prompt template selection are critical decision points that significantly impact results.
- Design tradeoffs: Using ImageNet classifiers limits evaluation to concepts present in ImageNet, but provides reliable classification. Alternative approaches like CLIP-based similarity would be more general but less precise for hierarchical evaluation.
- Failure signatures: Low ISP values indicate models don't understand the concept well; low SCS values suggest lack of diversity in generated hyponyms; unstable metrics across seeds indicate sensitivity to random initialization.
- First 3 experiments:
  1. Replicate the basic ISP/SCS calculation on a single model with a small subset of synsets to verify the pipeline works
  2. Test classifier calibration by evaluating on ImageNet validation set to ensure probability outputs are reliable
  3. Compare metric values across different classifier-free guidance scales to understand the precision-diversity trade-off

## Open Questions the Paper Calls Out

### Open Question 1
How do data-driven hierarchies (e.g., based on actual text-to-image model usage) compare to WordNet for evaluating hypernymy understanding? The authors suggest that future work might address the limitation of relying on WordNet and ImageNet by studying data-driven hierarchies based on actual use cases of text-to-image models. Why unresolved: The paper uses WordNet and ImageNet hierarchies, but these do not cover the entire concept hierarchy and may not reflect how users actually interact with text-to-image models. What evidence would resolve it: Conducting a study comparing the performance of text-to-image models using both WordNet/ImageNet and data-driven hierarchies, and analyzing the differences in results and insights gained.

### Open Question 2
How do models that explicitly leverage ImageNet data (e.g., CLIP-based models) compare to other models in terms of hypernymy understanding, and do they have an unfair advantage? The authors mention that models using the CLIP encoder might have an unfair advantage due to a smaller domain shift and thus obtain inflated ISP and SCS scores. Why unresolved: The paper does not directly compare the performance of CLIP-based models to other models or investigate the potential bias introduced by using ImageNet data. What evidence would resolve it: Conducting a comparative study of CLIP-based models and other models using the proposed metrics, and analyzing the differences in performance and potential biases.

### Open Question 3
How does the performance of text-to-image models on hypernymy understanding relate to the semantic capabilities of their language encoders? The authors find a high and significant correlation between average hyponym embedding similarities and metric values for a given synset, suggesting a connection between the knowledge of the hypernymy relationship of the encoder and the performance of the entire model. Why unresolved: While the paper establishes a correlation, it does not investigate the causal relationship or explore how improving the language encoder's hypernymy understanding might impact the overall model performance. What evidence would resolve it: Conducting experiments to improve the language encoder's hypernymy understanding (e.g., through fine-tuning or architectural modifications) and measuring the impact on the text-to-image model's performance using the proposed metrics.

## Limitations
- Reliance on ImageNet classifiers creates bottleneck where evaluation quality depends on alignment between ImageNet classes and WordNet synsets
- Analysis of training data frequency effects is correlational rather than causal, lacking mechanistic explanation
- Human evaluation component uses relatively small number of annotators (12) and limited synset pairs, reducing statistical power

## Confidence

High confidence: The core methodology of using WordNet hierarchy with ImageNet classifiers is sound and well-implemented. The mathematical definitions of ISP and SCS are clear and reproducible.

Medium confidence: The correlations between metrics and human evaluation, while statistically significant, show only moderate effect sizes (Spearman correlations around 0.3-0.5). The practical significance of these correlations for model selection is unclear.

Low confidence: The conclusions about training data frequency effects and their implications for model performance are speculative given the correlational nature of the evidence and lack of mechanistic explanation.

## Next Checks

1. Test the ImageNet classifier's calibration on a held-out validation set by computing expected calibration error. Verify that the classifier's confidence scores accurately reflect true accuracy rates, particularly for generated images which may differ systematically from ImageNet training distribution.

2. Evaluate the same models on a different image classification dataset (e.g., COCO or OpenImages) with different label ontologies to test whether the ISP/SCS metrics are robust to the choice of classifier and semantic hierarchy.

3. Manually verify the accuracy of WordNet-to-ImageNet class mappings for a random sample of 50 synsets. Measure the proportion of synsets where the primary ImageNet class correctly represents the WordNet concept, and assess how this affects overall metric reliability.