---
ver: rpa2
title: A Bayesian approach for prompt optimization in pre-trained language models
arxiv_id: '2312.00471'
source_url: https://arxiv.org/abs/2312.00471
tags:
- optimization
- prompt
- space
- bayesian
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formulates prompt optimization as a combinatorial optimization
  problem and proposes a Bayesian optimization method executed in a continuous embedding
  of the discrete token space. The method, called hard prompt tuning (HPT), directly
  searches for discrete tokens to be added to text inputs without requiring access
  to the large language model (LLM), making it suitable for black-box scenarios like
  Model as a Service (MaaS).
---

# A Bayesian approach for prompt optimization in pre-trained language models

## Quick Facts
- arXiv ID: 2312.00471
- Source URL: https://arxiv.org/abs/2312.00471
- Reference count: 0
- One-line primary result: Bayesian optimization in continuous token embeddings achieves competitive prompt tuning performance in black-box LLM settings

## Executive Summary
This paper presents a Bayesian optimization approach for discrete prompt optimization in pre-trained language models that operates entirely in a continuous embedding of the discrete token space. The method, called hard prompt tuning (HPT), directly searches for discrete tokens to prepend to inputs without requiring gradient access to the LLM, making it suitable for black-box scenarios like Model as a Service (MaaS). Using BoTorch for the Bayesian optimization framework, the approach achieves competitive performance across six classification benchmarks while enabling analysis of the tradeoff between search space size, accuracy, and computational efficiency.

## Method Summary
The method formulates prompt optimization as a combinatorial optimization problem and solves it using Bayesian optimization executed in a continuous embedding of the discrete token space. Hard prompt tuning directly searches for discrete tokens to add to text inputs without requiring access to the large language model's internals, making it applicable in black-box settings. The approach uses a continuous relaxation of discrete token indices, a Gaussian process surrogate model, and the Upper-Confidence-Bound acquisition function, with BoTorch providing the modular components for efficient implementation. Prompts are evaluated using the objective function (accuracy or F1), and the GP is updated iteratively to guide the search toward high-performing prompts.

## Key Results
- Bayesian optimization in continuous token embeddings achieves competitive performance across six classification benchmarks (MNLI, QQP, SST-2, MRPC, QNLI, RTE)
- The method enables analysis of the tradeoff between search space size, accuracy, and wall-clock time for prompt optimization
- Hard prompt tuning demonstrates effectiveness in black-box LLM scenarios without requiring gradient access to the model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bayesian optimization enables efficient exploration of a high-dimensional discrete prompt space without gradient access.
- Mechanism: By embedding discrete token indices into a continuous space, BO can use Gaussian process models to predict prompt performance and guide sampling toward high-performing prompts. The acquisition function balances exploration (high variance regions) and exploitation (high mean regions).
- Core assumption: The continuous relaxation of token indices preserves the relative ranking of prompt quality and the GP surrogate can model the objective function effectively.
- Evidence anchors:
  - [abstract]: "The high dimensionality of the token space compounded by the length of the prompt sequence requires a very efficient solution. In this paper we propose a Bayesian optimization method, executed in a continuous embedding of the combinatorial space."
  - [section]: "One is to embed structured inputs into a lower dimensional continuous space where BO is executed whose output is subsequently decoded back into the structured space."
  - [corpus]: Weak evidence - no direct mentions of BO on token embeddings in the corpus summaries.
- Break condition: If the continuous relaxation poorly preserves token semantics or if the GP model fails to capture the objective landscape, BO performance degrades significantly.

### Mechanism 2
- Claim: Hard prompt tuning avoids the need for gradient access to the LLM, making it applicable in black-box settings.
- Mechanism: HPT directly searches for discrete tokens to prepend to inputs, relying only on input-output pairs from the LLM. This contrasts with soft prompt tuning which requires backpropagating through the model.
- Core assumption: The LLM's output quality for a given task is sufficiently sensitive to prompt token choices to enable effective optimization.
- Evidence anchors:
  - [abstract]: "Hard prompt tuning (HPT) which directly searches for discrete tokens to be added to the text input without requiring access to the large language model (LLM) and can be used also when LLM is available only as a black-box."
  - [section]: "Hard prompt Tuning directly searches for discrete token to be added to text input, does not use internal knowledge about the pretrained LLM and only requires for the LLM to be accessible as a black box."
  - [corpus]: Moderate evidence - "Black-Box Tuning for Language-Model-as-a-Service" mentioned in baselines.
- Break condition: If prompt tokens have minimal impact on LLM outputs or if the black-box access is too limited, HPT becomes ineffective.

### Mechanism 3
- Claim: The BoTorch library provides modular components that enable flexible and efficient BO implementation.
- Mechanism: BoTorch offers a Gaussian process surrogate model, acquisition function optimization, and integration with PyTorch for scalable computation. This modularity allows easy experimentation with different BO components.
- Core assumption: The available BO modules in BoTorch are suitable for the prompt optimization task and integrate well with the computational workflow.
- Evidence anchors:
  - [abstract]: "In this paper we use BoTorch, a library for Bayesian optimization research built on top of pyTorch."
  - [section]: "BoTorch provides a modular and flexible interface for composing Bayesian optimization algorithms. BoTorch bridges the gap between research and production..."
  - [corpus]: Weak evidence - no corpus mentions of BoTorch specifically.
- Break condition: If BoTorch modules are incompatible with the specific prompt search space or computational requirements, the implementation fails.

## Foundational Learning

- Concept: Bayesian optimization basics (GP surrogate, acquisition functions)
  - Why needed here: BO is the core algorithmic framework for efficient prompt search in a black-box setting.
  - Quick check question: What is the role of the acquisition function in Bayesian optimization?

- Concept: Continuous relaxation of discrete spaces
  - Why needed here: Enables BO to operate on the combinatorial prompt space by converting it to a continuous optimization problem.
  - Quick check question: How does rounding continuous BO outputs back to discrete tokens affect the search process?

- Concept: Black-box optimization vs white-box optimization
  - Why needed here: HPT operates in a black-box setting, which is critical for Model-as-a-Service scenarios.
  - Quick check question: What are the key differences between hard and soft prompt tuning approaches?

## Architecture Onboarding

- Component map: Input (task specification, LLM access, dataset, acquisition function, objective function, initial prompts) -> BO loop (GP surrogate, acquisition optimization, prompt discretization) -> Output (top-performing prompt tokens) -> Dependencies (BoTorch, PyTorch, RoBERTa tokenizer)

- Critical path:
  1. Generate initial random prompts
  2. Evaluate prompts using the objective function
  3. Train GP surrogate on observed data
  4. Optimize acquisition function to select next prompt
  5. Evaluate new prompt and update GP
  6. Repeat until convergence
  7. Return best prompts

- Design tradeoffs:
  - Continuous relaxation vs direct discrete optimization: Relaxation enables efficient BO but may lose some token semantics
  - Acquisition function choice: Different functions balance exploration/exploitation differently
  - Prompt length: Longer prompts increase search space exponentially but may improve performance
  - GP kernel choice: Affects surrogate model accuracy and computational cost

- Failure signatures:
  - GP uncertainty remains high throughout optimization (poor surrogate modeling)
  - Acquisition function optimization gets stuck in local optima
  - Performance plateaus quickly (search space too small or poorly defined)
  - High variance in results across runs (insufficient exploration)

- First 3 experiments:
  1. Run BO with default settings on a simple task (e.g., SST-2) to verify basic functionality
  2. Compare BO performance against random search on the same task
  3. Test different acquisition functions (UCB vs EI) on a mid-complexity task (e.g., QQP)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal encoding method for high-dimensional discrete token spaces beyond the continuous relaxation approach used in this paper?
- Basis in paper: [explicit] The authors note that MNLI's large vocabulary (117,056 tokens) shows significantly worse performance with their continuous relaxation approach, suggesting a more sophisticated encoding might be better.
- Why unresolved: The paper only tests a "na√Øve relaxation" approach and acknowledges it may be suboptimal for very large vocabularies, but doesn't explore or compare alternative encoding methods.
- What evidence would resolve it: Comparative experiments testing multiple encoding strategies (e.g., hierarchical encoding, learned embeddings, or structured kernels) on datasets with varying vocabulary sizes and reporting performance differences.

### Open Question 2
- Question: How does prompt length affect the tradeoff between search space size, accuracy, and wall-clock time in Bayesian optimization for prompt tuning?
- Basis in paper: [explicit] The authors acknowledge they haven't given "proper consideration" to prompt length impact and provide only "partial and preliminary results" suggesting length doesn't significantly impact performance.
- Why unresolved: The paper provides limited analysis on prompt length effects, only testing a few specific lengths (25, 50, 75) on one dataset (MRPC), and doesn't systematically analyze the tradeoff.
- What evidence would resolve it: Comprehensive experiments varying prompt length across all datasets, measuring accuracy, optimization time, and search space cardinality to identify optimal length ranges for different task types.

### Open Question 3
- Question: Can Bayesian optimization methods be effectively extended to generative tasks and prompt robustness/fairness considerations?
- Basis in paper: [explicit] The authors identify these as promising areas for future work, noting that current work focuses on classification tasks and suggesting BO could be extended to "alternative spaces" and "prompt robustness" issues.
- Why unresolved: The paper doesn't implement or test these extensions, only speculating about their potential based on BO's versatility.
- What evidence would resolve it: Experimental results applying the proposed BO approach to generative tasks (e.g., text completion, summarization) and incorporating robustness/fairness metrics into the acquisition function to demonstrate effectiveness.

## Limitations

- The continuous relaxation mechanism for preserving token semantics during BO optimization is not empirically validated
- The computational efficiency claims are relative to other black-box methods but lack absolute performance benchmarks
- The experiments focus on RoBERTa with specific benchmarks, limiting generalizability to other PLMs or task types
- The method's sensitivity to hyperparameter choices (prompt length, vocabulary size, BO parameters) is not systematically explored

## Confidence

**High Confidence:** The core contribution of using Bayesian optimization for black-box prompt tuning is well-defined and the implementation using BoTorch is straightforward to reproduce. The distinction between hard and soft prompt tuning is clearly articulated.

**Medium Confidence:** The experimental results showing competitive performance across six benchmarks are credible, but the comparison baselines and absolute performance numbers require careful verification. The computational efficiency claims need independent validation.

**Low Confidence:** The effectiveness of the continuous relaxation mechanism for preserving token semantics during BO optimization is not empirically validated. The scalability claims for larger vocabularies and prompt lengths are speculative based on the current experimental scope.

## Next Checks

1. **Continuous Relaxation Validation:** Implement a systematic ablation study comparing BO performance with and without continuous relaxation across different prompt lengths (5, 10, 15 tokens) on SST-2. Measure the semantic drift by computing token similarity between continuous BO outputs and their rounded discrete counterparts using RoBERTa's embedding space.

2. **Acquisition Function Sensitivity:** Run parallel BO experiments on QQP using three different acquisition functions (UCB, Expected Improvement, Probability of Improvement) with identical initializations and random seeds. Compare convergence speed, final accuracy, and variance across 10 random seeds to quantify the impact of acquisition function choice.

3. **Black-Box vs White-Box Comparison:** Implement a soft prompt tuning baseline with gradient access to RoBERTa for SST-2. Compare wall-clock time per iteration, final accuracy, and required GPU memory between HPT (black-box) and soft tuning (white-box) across three prompt lengths. This validates the practical efficiency claims for black-box scenarios.