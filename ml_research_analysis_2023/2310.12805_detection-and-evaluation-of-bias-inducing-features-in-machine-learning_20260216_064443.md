---
ver: rpa2
title: Detection and Evaluation of bias-inducing Features in Machine learning
arxiv_id: '2310.12805'
source_url: https://arxiv.org/abs/2310.12805
tags:
- features
- feature
- swapping
- distance
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a systematic approach to detect and evaluate
  bias-inducing features in machine learning models, addressing the problem that current
  methods often require pre-identification of sensitive features and may miss other
  bias-inducing features. The authors propose using cause-to-effect analysis, including
  single and double feature swapping functions, to identify features that directly
  and indirectly introduce bias to the model.
---

# Detection and Evaluation of bias-inducing Features in Machine learning

## Quick Facts
- arXiv ID: 2310.12805
- Source URL: https://arxiv.org/abs/2310.12805
- Reference count: 40
- Key outcome: Systematic approach using cause-to-effect analysis with single and double feature swapping to detect bias-inducing features, showing these differ from feature importance measures

## Executive Summary
This paper addresses the limitation of current bias detection methods that require pre-identification of sensitive features and may miss other bias-inducing features. The authors propose a systematic approach using cause-to-effect analysis through single and double feature swapping functions to identify features that directly and indirectly introduce bias to machine learning models. The method evaluates the impact of swapping feature values on model predictions using various divergence measures. Empirical evaluation on four datasets demonstrates that the proposed approach can identify bias-inducing features and assess their importance, revealing that treating feature importance and bias-inducing features as separate is essential for building fairer ML models.

## Method Summary
The method employs single and double feature swapping functions to detect bias-inducing features. Single feature swapping modifies one feature while keeping others unchanged to identify controlled direct impact, while double feature swapping alters pairs of features including mediators to estimate total natural impact. The approach uses temporal priority ordering to determine mediator relationships and applies four divergence measures (Hellinger, Jensen-Shannon, Total Variation, Wasserstein) to quantify prediction distribution changes. SHAP values are used to compare feature importance with bias-inducing features. The method requires preprocessing to remove correlated features, model training (logistic regression in experiments), and systematic swapping with various swap percentages and maximum distortion constraints.

## Key Results
- Single feature swapping successfully isolates direct effects of individual features on model predictions
- Double feature swapping captures total natural impact through mediating variables when temporal ordering is correct
- Multiple divergence metrics provide complementary perspectives on bias impact, with consistent rankings across metrics
- Identified bias-inducing features differ significantly from SHAP-determined feature importance rankings

## Why This Works (Mechanism)

### Mechanism 1
Single feature swapping isolates the direct effect of each feature on model predictions by swapping values of a single feature while holding all others constant, then measuring divergence between original and swapped prediction distributions. This assumes features can be meaningfully binarized without losing critical causal information. Break condition occurs if features have complex, non-binary relationships or mediators exist that aren't accounted for.

### Mechanism 2
Double feature swapping captures total natural impact through mediating variables by swapping both the target feature and its associated mediators in sequence, measuring cumulative effect on predictions. This assumes temporal priority ordering correctly identifies which features act as mediators for others. Break condition occurs if temporal ordering is incorrect or important mediators are omitted.

### Mechanism 3
Divergence metrics (Hellinger, JS, TV, Wasserstein) quantify bias impact consistently by comparing prediction distributions before and after swapping, providing complementary perspectives on bias. This assumes chosen divergence metrics are appropriate for prediction distribution types. Break condition occurs if distributions are highly skewed or metrics disagree substantially.

## Foundational Learning

- Concept: Counterfactual approach to causal inference
  - Why needed here: The methodology relies on understanding "what would happen if" feature values were changed, which is fundamentally a counterfactual reasoning problem.
  - Quick check question: Can you explain the difference between controlled direct effect and natural direct effect in plain language?

- Concept: Mediation analysis and temporal priority
  - Why needed here: Correctly identifying mediating variables and their temporal order is essential for double feature swapping to capture indirect effects.
  - Quick check question: If feature A occurs before feature B in time, and B influences the outcome, what is B's role in the causal chain?

- Concept: Feature importance vs. bias-inducing features distinction
  - Why needed here: The paper argues these are separate concepts - a feature can be important for predictions but not bias-inducing, or vice versa.
  - Quick check question: Can you give an example where a feature is crucial for model accuracy but shouldn't be considered bias-inducing?

## Architecture Onboarding

- Component map: Data preprocessing (binning, correlation removal) -> Model training (logistic regression) -> Single feature swapping engine -> Double feature swapping engine with temporal ordering logic -> Divergence calculation module (4 metrics) -> SHAP integration for feature importance comparison -> Evaluation and ranking system

- Critical path: Preprocess → Train → Swap (single) → Measure divergence → Swap (double) → Measure divergence → Compare with SHAP → Rank features → Evaluate mitigation

- Design tradeoffs:
  - Binarization vs. preserving continuous information (tradeoff between causal clarity and information loss)
  - Single vs. double swapping computational cost (double is much more expensive)
  - Temporal ordering accuracy vs. automation (manual ordering more accurate but less scalable)

- Failure signatures:
  - Highly correlated features not removed → unstable rankings
  - Incorrect temporal ordering → misleading mediator identification
  - Features with many categories → excessive computational cost in swapping
  - Non-monotonic relationships → divergence measures may not capture true impact

- First 3 experiments:
  1. Run single feature swapping on a small synthetic dataset with known direct effects to verify the mechanism works as expected
  2. Compare single vs. double swapping results on a simple mediation structure to validate the indirect effect capture
  3. Test all four divergence metrics on a dataset with known bias patterns to see which metrics agree or disagree

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of temporal priority ordering affect the identification of bias-inducing features?
- Basis in paper: The paper discusses using both manually defined and statistically determined temporal orderings, but does not extensively explore the impact of different orderings on the results.
- Why unresolved: The authors mention that temporal priority ordering can significantly impact results, but don't provide detailed analysis of how different orderings affect bias-inducing feature identification.
- What evidence would resolve it: A comprehensive study comparing results using different temporal priority orderings and analyzing the impact on identified bias-inducing features.

### Open Question 2
How does the proposed method perform on non-tabular datasets, such as image or text data?
- Basis in paper: The authors mention the method is based on tabular data but don't explore its applicability to other data types.
- Why unresolved: The paper focuses on tabular datasets, leaving open how well the method generalizes to other data types commonly used in machine learning.
- What evidence would resolve it: An empirical evaluation of the proposed method on non-tabular datasets like image or text data to assess its performance and limitations.

### Open Question 3
How does the proposed method compare to other existing methods for bias detection and mitigation in terms of computational efficiency?
- Basis in paper: The authors mention their method is based on data swapping and divergence measures but don't provide detailed comparison of computational efficiency with other methods.
- Why unresolved: The paper doesn't discuss computational complexity or compare efficiency with other existing methods.
- What evidence would resolve it: A thorough analysis of the computational complexity of the proposed method and comparison with other existing methods for bias detection and mitigation, considering both accuracy and efficiency.

## Limitations

- The methodology assumes features can be meaningfully binarized without losing causal information, which may not hold for complex, continuous variables
- The double feature swapping mechanism depends critically on correct temporal ordering of features, yet the paper doesn't specify how this ordering is determined or validated
- Empirical evaluation is limited to logistic regression models on four datasets, raising questions about generalizability to other model types and domains

## Confidence

- **High confidence**: The fundamental concept that controlled direct effects and natural impacts can be separated using feature swapping is sound and well-grounded in causal inference literature
- **Medium confidence**: The empirical demonstration that bias-inducing features differ from important features is convincing for the specific datasets and models tested, but requires broader validation
- **Low confidence**: The scalability and robustness of the approach across different model types, feature distributions, and temporal ordering scenarios remains unproven

## Next Checks

1. Test the temporal priority ordering mechanism on synthetic datasets with known causal structures to verify it correctly identifies mediators and their sequences
2. Apply the methodology to non-linear models (e.g., gradient boosting, neural networks) to assess whether the causal interpretations hold beyond logistic regression
3. Evaluate the impact of different binning strategies on feature swapping results by comparing outcomes using various discretization approaches on the same datasets