---
ver: rpa2
title: 'Kosmos-2: Grounding Multimodal Large Language Models to the World'
arxiv_id: '2306.14824'
source_url: https://arxiv.org/abs/2306.14824
tags:
- kosmos
- image
- referring
- grounding
- bounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Kosmos-2 is a multimodal large language model that adds the ability
  to ground text to visual objects. It represents text spans as hyperlinks to bounding
  boxes, creating a large-scale dataset of grounded image-text pairs.
---

# Kosmos-2: Grounding Multimodal Large Language Models to the World

## Quick Facts
- arXiv ID: 2306.14824
- Source URL: https://arxiv.org/abs/2306.14824
- Reference count: 16
- Primary result: Kosmos-2 achieves strong zero-shot grounding performance while maintaining competitive language and vision-language task capabilities

## Executive Summary
Kosmos-2 is a multimodal large language model that adds the ability to ground text to visual objects by representing text spans as hyperlinks to bounding boxes. The model is trained on a large-scale dataset of grounded image-text pairs (GRIT) and achieves impressive zero-shot performance on multimodal grounding tasks like phrase grounding and referring expression comprehension. Kosmos-2 maintains competitive performance on language and vision-language tasks while gaining these new grounding capabilities, enabling it to both ground text to visual objects and generate referring expressions for given regions.

## Method Summary
Kosmos-2 represents object descriptions as Markdown hyperlinks with bounding box tokens, creating a unified format for learning language and spatial grounding. The model is trained on GRIT, a dataset of 91M images with 137M bounding boxes linked to 115M text spans, constructed by extracting noun phrases and referring expressions from image captions. A MAGNETO Transformer architecture with 1.6B parameters is trained using next-word prediction on interleaved multimodal data. Instruction tuning further improves the model's ability to generate and comprehend bounding box descriptions in response to user prompts.

## Key Results
- Achieves impressive zero-shot performance on phrase grounding, outperforming GRILL by a large margin
- Maintains competitive performance on language tasks compared to Kosmos-1 while adding grounding capabilities
- Shows strong performance on referring expression comprehension and generation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Representing object descriptions as Markdown hyperlinks with bounding box tokens enables joint learning of language and spatial grounding in a unified format
-