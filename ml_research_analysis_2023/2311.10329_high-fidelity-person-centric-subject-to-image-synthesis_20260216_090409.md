---
ver: rpa2
title: High-fidelity Person-centric Subject-to-Image Synthesis
arxiv_id: '2311.10329'
source_url: https://arxiv.org/abs/2311.10329
tags:
- generation
- scene
- image
- stage
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Face-diffuser is proposed to address the challenges of person-centric
  image generation by eliminating training imbalance and quality compromise. The core
  method involves developing two specialized diffusion models, Text-driven Diffusion
  Model (TDM) and Subject-augmented Diffusion Model (SDM), for scene and person generation
  respectively.
---

# High-fidelity Person-centric Subject-to-Image Synthesis

## Quick Facts
- arXiv ID: 2311.10329
- Source URL: https://arxiv.org/abs/2311.10329
- Authors: 
- Reference count: 40
- Key outcome: Face-diffuser addresses training imbalance and quality compromise in person-centric image generation through specialized diffusion models and Saliency-adaptive Noise Fusion (SNF)

## Executive Summary
Face-diffuser introduces a novel approach to person-centric subject-to-image synthesis by addressing the fundamental challenge of training imbalance between scene and person generation. The method employs two specialized diffusion models - Text-driven Diffusion Model (TDM) for scene generation and Subject-augmented Diffusion Model (SDM) for person generation - which collaborate through a novel Saliency-adaptive Noise Fusion mechanism. The sampling process is divided into three sequential stages: semantic scene construction, subject-scene fusion, and subject enhancement. Extensive experiments demonstrate that Face-diffuser achieves superior identity preservation and prompt consistency compared to state-of-the-art methods while generating high-fidelity images of multiple unseen persons in diverse contexts.

## Method Summary
Face-diffuser employs two specialized diffusion models based on Stable Diffusion v1-5: TDM for scene generation and SDM for person generation. Both models are fine-tuned on the FFHQ-face dataset, with SDM incorporating an additional reference image condition. The sampling process operates in three stages: TDM constructs the semantic scene for αT steps, TDM and SDM collaboratively infuse the person into the scene for (β-α)T steps using the Saliency-adaptive Noise Fusion (SNF) mechanism, and SDM enhances the person details for the remaining steps. SNF uses classifier-free guidance responses to automatically allocate generation responsibilities between models in a saliency-aware manner, allowing spatial blending of predicted noises from both models.

## Key Results
- Achieves improved identity preservation and prompt consistency compared to state-of-the-art methods
- Effectively eliminates training imbalance between scene and person generation through specialized models
- Demonstrates robustness in generating high-fidelity images of multiple unseen persons across diverse contexts
- Shows quantitative improvements in both identity preservation (measured by FaceNet similarity) and prompt consistency (measured by CLIP-L/14 similarity)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Current methods jointly fine-tuning a single diffusion model for both scene and person generation leads to catastrophic forgetting of semantic scene priors and irreconcilable training imbalance.
- Core assumption: Fine-tuning a single model for both tasks inherently compromises person generation quality and causes forgetting of semantic scenes.
- Evidence anchors:
  - [abstract]: "they learn the semantic scene and person generation by fine-tuning a common pre-trained diffusion, which involves an irreconcilable training imbalance."
  - [section]: "Precisely, to generate realistic persons, they need to sufficiently tune the pre-trained model, which inevitably causes the model to forget the rich semantic scene prior and makes scene generation over-fit to the training data."
  - [corpus]: Weak evidence - no directly comparable work found, but the concept of training imbalance is supported by general knowledge of catastrophic forgetting in neural networks.
- Break condition: If a single model could be designed to effectively learn both scene and person generation without compromising either, this mechanism would be invalidated.

### Mechanism 2
- Claim: Saliency-adaptive Noise Fusion (SNF) enables effective collaboration between two specialized diffusion models by automatically allocating regions for each model to synthesize based on classifier-free guidance responses.
- Core assumption: Classifier-free guidance responses can effectively evaluate the impact of conditions on each pixel and allocate generation responsibilities between models.
- Evidence anchors:
  - [abstract]: "In each time step, SNF leverages the unique strengths of each model and allows for the spatial blending of predicted noises from both models automatically in a saliency-aware manner."
  - [section]: "SNF leverages the unique strengths of each model and allows for the spatial blending of predicted noises from both models automatically in a saliency-aware manner, all of which can be seamlessly integrated into the DDIM sampling process."
  - [corpus]: Weak evidence - no directly comparable work found, but the concept of using classifier-free guidance responses for saliency evaluation is supported by the cited image editing study [32].
- Break condition: If classifier-free guidance responses do not accurately reflect saliency of generated images, or if there is no robust link between these responses and saliency, this mechanism would fail.

### Mechanism 3
- Claim: The three-stage sampling process allows for high-fidelity person and diverse semantic scene generation by leveraging specialized capabilities of each model.
- Core assumption: Dividing sampling into three stages, each focusing on a specific task, will lead to higher-quality generation than joint approaches.
- Evidence anchors:
  - [abstract]: "The sampling process is divided into three sequential stages, i.e., semantic scene construction, subject-scene fusion, and subject enhancement."
  - [section]: "Following this pipeline, Face-diffuser divides the sampling process into three consecutive stages: semantic scene construction by TDM, subject-scene fusion by collaboration between TDM and SDM, and subject enhancement by SDM."
  - [corpus]: Weak evidence - no directly comparable work found, but the concept of staged generation is supported by general knowledge of progressive refinement in image generation.
- Break condition: If division of tasks between stages does not lead to improved quality, or if collaboration between models in fusion stage is not effective, this mechanism would be invalidated.

## Foundational Learning

- Concept: Diffusion models and DDIM sampling process
  - Why needed here: Understanding how diffusion models work is crucial for understanding Face-diffuser architecture and components.
  - Quick check question: What is the role of the noise predictor in a diffusion model, and how does it contribute to the image generation process?

- Concept: Classifier-free guidance and its application in diffusion models
  - Why needed here: Classifier-free guidance is a key component of the Saliency-adaptive Noise Fusion mechanism.
  - Quick check question: How does classifier-free guidance modify the predicted noise in a diffusion model, and what is the effect of this modification on the generated image?

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Explains why jointly fine-tuning a single model for both scene and person generation leads to quality compromise.
  - Quick check question: What is catastrophic forgetting, and how does it manifest in the context of fine-tuning neural networks for multiple tasks?

## Architecture Onboarding

- Component map:
  - Text-driven Diffusion Model (TDM) -> Subject-augmented Diffusion Model (SDM) -> Saliency-adaptive Noise Fusion (SNF) -> Three-stage sampling process (semantic scene construction -> subject-scene fusion -> subject enhancement)

- Critical path:
  1. Fine-tune TDM and SDM on FFHQ-face dataset
  2. Use TDM to construct semantic scene for αT steps
  3. Use TDM and SDM to collaboratively infuse person into scene for (β-α)T steps using SNF
  4. Use SDM to enhance person details for remaining steps

- Design tradeoffs:
  - Using two specialized models increases architectural complexity but better handles training imbalance and quality compromise
  - Three-stage sampling adds computational overhead but enables fine-grained control and better model collaboration

- Failure signatures:
  - Generated images showing signs of catastrophic forgetting (unrealistic scenes or persons)
  - Inconsistencies between scene and person (mismatched lighting or styles)
  - Suboptimal results if classifier-free guidance responses don't accurately reflect saliency

- First 3 experiments:
  1. Ablation study: Remove each stage of sampling process individually to assess importance of each stage
  2. Replace SNF with direct addition of predicted noises from TDM and SDM to quantify SNF improvement
  3. Vary hyperparameters α and β to find optimal values for semantic scene construction and subject-scene fusion stages

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided text.

## Limitations

- The evaluation protocol uses a single benchmark dataset with 15 subjects and 30 prompts per subject, limiting generalizability
- FaceNet-based identity preservation metric has known limitations in capturing fine-grained identity features
- The study does not address potential biases in the FFHQ dataset or evaluate performance across diverse demographic groups
- Claims about irreconcilable training imbalance rely primarily on theoretical reasoning rather than empirical comparison with alternative approaches

## Confidence

**High Confidence**: The architectural design of using specialized diffusion models for scene and person generation is technically sound and builds on established diffusion model principles.

**Medium Confidence**: The effectiveness of the Saliency-adaptive Noise Fusion mechanism in improving collaboration between models. While theoretically plausible, empirical validation is limited to qualitative comparisons.

**Low Confidence**: The claim that joint fine-tuning inherently leads to catastrophic forgetting and quality compromise. This mechanism is stated as fact but lacks direct empirical comparison with alternative training strategies.

## Next Checks

1. Conduct an ablation study on the SNF mechanism by replacing it with simple noise averaging between TDM and SDM, then measure quantitative differences in identity preservation and prompt consistency to isolate SNF's contribution.

2. Evaluate Face-diffuser on an independent person-centric dataset (e.g., DeepFashion or LaPa) to assess whether improvements generalize beyond the FFHQ-based training distribution.

3. Perform a controlled experiment comparing SNF-based collaboration with ground-truth saliency maps (from human annotation or established saliency detection methods) to verify that guidance responses accurately predict saliency regions.