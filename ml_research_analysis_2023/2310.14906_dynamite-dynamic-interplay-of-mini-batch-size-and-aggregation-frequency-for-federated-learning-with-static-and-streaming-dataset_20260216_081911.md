---
ver: rpa2
title: 'DYNAMITE: Dynamic Interplay of Mini-Batch Size and Aggregation Frequency for
  Federated Learning with Static and Streaming Dataset'
arxiv_id: '2310.14906'
source_url: https://arxiv.org/abs/2310.14906
tags:
- data
- batch
- training
- size
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DYNAMITE, an adaptive federated learning
  algorithm that dynamically co-optimizes mini-batch size and aggregation frequency
  to balance convergence, cost, and completion time in resource-constrained edge environments.
  The method derives a novel error bound accounting for heterogeneous datasets and
  resource constraints, and proposes both offline closed-form solutions and an online
  adaptive control algorithm that adjusts parameters based on real-time system and
  data dynamics.
---

# DYNAMITE: Dynamic Interplay of Mini-Batch Size and Aggregation Frequency for Federated Learning with Static and Streaming Dataset

## Quick Facts
- **arXiv ID:** 2310.14906
- **Source URL:** https://arxiv.org/abs/2310.14906
- **Reference count:** 40
- **Primary result:** Adaptive algorithm that co-optimizes mini-batch size and aggregation frequency to improve federated learning accuracy and efficiency

## Executive Summary
DYNAMITE is a novel federated learning algorithm that dynamically optimizes mini-batch size and aggregation frequency to balance convergence, cost, and completion time in resource-constrained edge environments. The method derives a theoretical error bound that captures the trade-off between gradient variance reduction and local model drift, and proposes both offline closed-form solutions and online adaptive control algorithms. Extensive experiments demonstrate that DYNAMITE achieves up to 8.4% higher accuracy and 59.6% faster training compared to state-of-the-art methods while reducing cost by 58% under various edge network configurations.

## Method Summary
DYNAMITE jointly optimizes mini-batch size and aggregation frequency in federated learning by deriving a novel error bound that accounts for heterogeneous datasets and resource constraints. The method uses a closed-form solution based on Cauchy-Schwarz to allocate heterogeneous batch sizes across devices proportional to their data heterogeneity and computational capacity. For streaming data, an online adaptive control algorithm dynamically adjusts parameters by estimating model and system parameters each round, using reservoir sampling to manage limited storage. The algorithm solves a constrained optimization problem to minimize training error while satisfying cost and time constraints.

## Key Results
- Achieves up to 8.4% higher accuracy compared to state-of-the-art methods on static datasets
- Reduces training time by 59.6% while maintaining model quality
- Cuts training cost by 58% under various edge network configurations
- Demonstrates superior performance on both static and streaming data with different arrival patterns

## Why This Works (Mechanism)

### Mechanism 1
Co-optimizing batch size and aggregation frequency improves model accuracy and training efficiency by balancing gradient variance, local model drift, and resource constraints. Increasing batch size reduces gradient variance, which stabilizes updates, while increasing local update steps reduces communication overhead. However, too many local steps increase model drift from the global optimum. The algorithm jointly adjusts both parameters to find the optimal balance.

### Mechanism 2
Assigning heterogeneous batch sizes to clients based on their data heterogeneity and computational capacity improves convergence speed and final accuracy. Devices with more diverse data or larger datasets benefit from larger batch sizes to reduce gradient variance. Devices with higher computational capacity can handle larger batches without increasing straggler delays. The algorithm uses a closed-form solution based on Cauchy-Schwarz to allocate batches optimally.

### Mechanism 3
Online adaptive control dynamically adjusts batch size and aggregation frequency in response to data stream dynamics and system variability, maintaining convergence efficiency. The algorithm estimates model parameters and system parameters online each round. It uses reservoir sampling to handle limited storage and updates batch size and aggregation frequency to minimize a marginal error bound.

## Foundational Learning

- **Concept: Error bound derivation for federated learning**
  - Why needed here: The paper's core contribution relies on proving how batch size and aggregation frequency jointly affect convergence error.
  - Quick check question: What role does the parameter ρ play in the convergence bound, and why is it called "quadratic-continuous"?

- **Concept: Optimization under resource constraints**
  - Why needed here: The algorithm must balance accuracy, cost, and completion time, requiring constrained optimization techniques.
  - Quick check question: How does the constraint K(aτstot + b) ≤ R translate to a per-round budget for batch size allocation?

- **Concept: Online parameter estimation in distributed systems**
  - Why needed here: The adaptive algorithm requires real-time estimation of model and system parameters without centralized access to raw data.
  - Quick check question: Why does the algorithm use weighted averages (based on dataset sizes) when aggregating parameter estimates from clients?

## Architecture Onboarding

- **Component map:** Parameter Server -> Client Devices -> Communication Layer
- **Critical path:**
  1. PS initializes w(0), τ, s
  2. PS broadcasts w(t), τk, sk to clients
  3. Clients estimate ρi, βi, ci, Mi and perform local updates
  4. Clients send wi(t), pi, Dk i to PS
  5. PS aggregates gradients, estimates ρ, β, c, δ
  6. PS solves for τk+1, sk+1 using CoOptFL
  7. Repeat until convergence or budget exhausted
- **Design tradeoffs:**
  - Batch size vs. aggregation frequency: Larger batches reduce variance but increase computation per round; more local steps reduce communication but increase drift
  - Homogeneous vs. heterogeneous batch sizes: Uniform sizing simplifies implementation but may underutilize resources; heterogeneous sizing improves accuracy but requires more complex allocation
  - Static vs. dynamic data: Static data allows simpler optimization; streaming data requires online adaptation and buffer management
- **Failure signatures:**
  - Slow convergence or divergence: May indicate poor parameter estimates or inappropriate τ/s choices
  - High communication overhead: May suggest aggregation frequency too high or batch sizes too small
  - Buffer overflow/underflow: May indicate reservoir sampling or data arrival rate misestimation
- **First 3 experiments:**
  1. Verify Theorem 2 predictions: Run SVM on MNIST with varying K, τ, s; compare accuracy to theoretical predictions
  2. Test Algorithm 1 optimality: Compare CoOptFL batch allocation to uniform and no-straggler baselines on heterogeneous devices with non-i.i.d. data
  3. Validate online adaptation: Run DYNAMITE on streaming CIFAR-10 with burst arrival; measure accuracy vs. FedAvg and Dynamic-τ

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed error bound (11) change if we consider non-quadratic continuous loss functions (Assumption 1 is relaxed)? While the paper mentions that the algorithm can be applied to models that do not satisfy these assumptions, it does not provide theoretical analysis for the error bound when Assumption 1 is relaxed.

### Open Question 2
How does the performance of DYNAMITE change with different data stream configurations (I.I.D. stream vs. Continuous stream) and arrival patterns (Smooth, Burst, Random) in terms of convergence speed and final accuracy? While experimental results are provided, the paper does not offer a comprehensive theoretical analysis of how these factors affect convergence.

### Open Question 3
How does the choice of data sampling method (e.g., Reservoir Sampling, Random Sampling, FIFO Sampling) affect the performance of DYNAMITE in terms of convergence speed and final accuracy? The paper shows that Reservoir Sampling outperforms other methods but does not provide a theoretical analysis of why this is the case.

## Limitations
- The theoretical error bound relies on assumptions about smoothness and bounded gradients that may not hold for deep neural networks
- Online parameter estimation accuracy is not thoroughly validated, and estimation errors could accumulate over time
- The paper does not provide convergence guarantees for the adaptive control algorithm, only for the static optimization problem

## Confidence

- **High confidence** in the fundamental insight that batch size and aggregation frequency are coupled hyperparameters that should be optimized together
- **Medium confidence** in the effectiveness of heterogeneous batch sizing, as this depends on the degree of heterogeneity in the deployment scenario
- **Medium confidence** in the online adaptation mechanism, as the parameter estimation procedures are complex and their accuracy is not thoroughly validated
- **Low confidence** in the practical applicability to deep learning tasks, as the evaluation focuses on simpler models (logistic regression, SVM)

## Next Checks

1. **Convergence Analysis:** Implement the online parameter estimation and test its accuracy on a streaming dataset. Compare the estimated model parameters (ρ, β, c, δ) to ground truth values obtained from centralized training.

2. **Robustness Testing:** Evaluate DYNAMITE's performance under varying degrees of data heterogeneity (α from 0 to 1) and device heterogeneity (pi ratios from 1:1 to 1:10). Identify the point where the heterogeneous batch sizing strategy breaks down.

3. **Deep Learning Applicability:** Extend the evaluation to a deep CNN on CIFAR-10 with the streaming data setup. Compare DYNAMITE to FedAvg and Dynamic-τ in terms of accuracy, cost, and completion time. Verify if the theoretical insights from simpler models transfer to deep learning.