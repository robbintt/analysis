---
ver: rpa2
title: Using In-Context Learning to Improve Dialogue Safety
arxiv_id: '2302.00871'
source_url: https://arxiv.org/abs/2302.00871
tags:
- demonstrations
- responses
- safety
- response
- safe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using in-context learning for safe response
  generation from dialogue systems. The method retrieves safety demonstrations from
  similar dialogue contexts and uses them in-context to condition response generation.
---

# Using In-Context Learning to Improve Dialogue Safety
## Quick Facts
- arXiv ID: 2302.00871
- Source URL: https://arxiv.org/abs/2302.00871
- Reference count: 40
- Key outcome: In-context safety demonstrations increase response safeness from 68.24% to 87.18% while maintaining quality

## Executive Summary
This paper introduces an in-context learning approach for improving dialogue safety by retrieving demonstrations of safe responses to similar unsafe contexts and using them to condition response generation. The method leverages large pre-trained language models like OPT-13B without requiring additional training, making it computationally efficient compared to fine-tuning approaches. Automatic and human evaluations on ProsocialDialog and DiaSafety datasets demonstrate substantial improvements in response safeness while maintaining other response quality metrics.

The approach works by first retrieving safety demonstrations using BM25, DPR, or random selection methods, then including these demonstrations in-context to guide the generation of safe responses. The paper also proposes a re-ranking procedure that further improves response safeness by selecting responses most similar to the retrieved safety demonstrations. The method achieves competitive performance with training-based approaches while offering advantages in terms of computational efficiency and adaptability.

## Method Summary
The method retrieves safety demonstrations from similar dialogue contexts using BM25, DPR, or random selection, then includes these demonstrations in-context to condition response generation from large pre-trained language models. For a target unsafe context, K safety demonstrations are retrieved and formatted as input examples showing safe responses to similar contexts. The model generates candidate responses, which can optionally be re-ranked based on similarity to the retrieved demonstrations. The approach is evaluated on ProsocialDialog and DiaSafety datasets using automatic metrics (ROUGE, BLEU, METEOR, F1, DEB) and human evaluation, with a safety classifier trained on multiple datasets to assess response safeness.

## Key Results
- Using 10 safety demonstrations, OPT-13B generates safe responses to unsafe dialogue contexts from DiaSafety 87.18% of the time, compared to 68.24% without demonstrations
- Response safeness consistently increases with more in-context demonstrations while maintaining or improving other response quality metrics
- The approach performs competitively with training-based methods for safe response generation without requiring additional training
- Re-ranking responses based on similarity to safety demonstrations provides additional improvements in response safeness

## Why This Works (Mechanism)
### Mechanism 1
- Claim: In-context learning improves response safeness by providing examples of how to handle unsafe dialogue contexts
- Mechanism: The model retrieves demonstrations of safe responses to similar unsafe contexts and includes them in the prompt, conditioning the generation to follow similar patterns
- Core assumption: The retrieved demonstrations are semantically similar to the target context and correctly exemplify safe behavior
- Evidence anchors: [abstract] "To generate a response to an unsafe dialogue context, we retrieve demonstrations of safe responses to similar dialogue contexts."

### Mechanism 2
- Claim: Using in-context safety demonstrations does not significantly harm other response qualities
- Mechanism: The demonstrations guide the model towards safe responses while maintaining response coherence, engagingness, and relevance
- Core assumption: The model can balance safety guidance with other response quality metrics
- Evidence anchors: [abstract] "We find our method performs competitively with strong baselines which use fine-tuning."

### Mechanism 3
- Claim: Re-ranking responses based on similarity to safety demonstrations further improves response safeness
- Mechanism: Multiple candidate responses are generated and scored based on their similarity to the retrieved safety demonstrations, with the highest-scoring response selected
- Core assumption: Responses with higher similarity to safety demonstrations are more likely to be safe
- Evidence anchors: [abstract] "Finally, we also propose a straightforward re-ranking procedure which can further improve response safeness."

## Foundational Learning
- Concept: In-context learning
  - Why needed here: To leverage the model's ability to learn from examples without additional training
  - Quick check question: What is the key difference between in-context learning and fine-tuning?

- Concept: Retrieval-based methods
  - Why needed here: To find relevant safety demonstrations for the target context
  - Quick check question: How do BM25 and DPR differ in their approach to retrieval?

- Concept: Response generation metrics
  - Why needed here: To evaluate the quality of generated responses across multiple dimensions
  - Quick check question: What is the difference between ROUGE and BLEU metrics?

## Architecture Onboarding
- Component map: Retriever -> Generator -> Re-ranker -> Safety classifier
- Critical path: 1. Retrieve safety demonstrations 2. Generate candidate responses 3. (Optional) Re-rank responses 4. Evaluate and select final response
- Design tradeoffs: Number of demonstrations vs computational cost; BM25 vs DPR for retrieval speed vs accuracy; re-ranking for safety vs added computational overhead
- Failure signatures: Low safeness scores (poor retrieval or generation), decreased response quality (overfitting to safety), high computational cost (need for optimization)
- First 3 experiments: 1. Evaluate safeness with varying numbers of demonstrations 2. Compare different retrievers (BM25, DPR, random) 3. Test re-ranking effectiveness with different numbers of candidate responses

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does meta in-context learning affect the effectiveness of in-context safety demonstrations for dialogue systems?
- Basis in paper: The paper mentions that meta in-context learning has been shown to be effective for enabling models to adapt to novel examples at inference-time, but it does not investigate the potential benefits of meta in-context learning for enabling dialogue systems to adapt to novel unsafe target contexts.
- Why unresolved: The paper does not provide any experimental results or analysis on the impact of meta in-context learning on the performance of in-context safety demonstrations.
- What evidence would resolve it: Experimental results comparing the performance of in-context safety demonstrations with and without meta in-context learning on dialogue safety tasks.

### Open Question 2
- Question: How do demonstrations of undesirable model behavior compare to demonstrations of exemplary behavior in improving response safeness?
- Basis in paper: The paper mentions that alternative to retrieving demonstrations of exemplary model behavior, one could also retrieve demonstrations of undesirable model behavior, but it does not investigate approaches which utilize negative examples.
- Why unresolved: The paper does not provide any experimental results or analysis on the impact of using negative examples in in-context learning for dialogue safety.
- What evidence would resolve it: Experimental results comparing the performance of in-context safety demonstrations with positive examples versus negative examples on dialogue safety tasks.

### Open Question 3
- Question: To what extent are generated responses faithful to the retrieved demonstrations used to generate them?
- Basis in paper: The paper mentions that it does not investigate the extent to which generated responses are faithful to the retrieved demonstrations used to generate them.
- Why unresolved: The paper does not provide any analysis or metrics to measure the faithfulness of generated responses to the retrieved demonstrations.
- What evidence would resolve it: Metrics or experimental results that measure the similarity or faithfulness of generated responses to the retrieved demonstrations, such as BLEU scores or human evaluation.

## Limitations
- Implementation details for DPR retriever and safety classifier training are underspecified
- Generalizability to other dialogue domains and languages remains untested
- Computational overhead and scalability with larger numbers of demonstrations not thoroughly explored

## Confidence
- High Confidence: Core claim that in-context learning with safety demonstrations improves response safeness is well-supported by automatic and human evaluations
- Medium Confidence: Claim that method maintains response quality while improving safeness is supported but requires careful interpretation
- Low Confidence: Effectiveness of re-ranking procedure relies heavily on similarity metric correlation with actual safety

## Next Checks
1. Implement and test the DPR retriever with multiple query/document encoder configurations to establish baseline retrieval quality
2. Evaluate the approach on at least two additional dialogue datasets from different domains to assess performance consistency
3. Systematically evaluate computational cost and effectiveness trade-offs with varying numbers of demonstrations (1, 5, 10, 20) and response candidates