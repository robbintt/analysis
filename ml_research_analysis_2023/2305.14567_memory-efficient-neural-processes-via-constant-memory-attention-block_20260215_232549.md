---
ver: rpa2
title: Memory Efficient Neural Processes via Constant Memory Attention Block
arxiv_id: '2305.14567'
source_url: https://arxiv.org/abs/2305.14567
tags:
- constant
- memory
- datapoints
- context
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a constant memory attention block (CMAB) that
  can compute outputs and perform updates in constant memory and computation. The
  CMAB is used to construct CMANPs, a neural process variant that requires only constant
  memory for conditioning, querying, and updating phases.
---

# Memory Efficient Neural Processes via Constant Memory Attention Block

## Quick Facts
- arXiv ID: 2305.14567
- Source URL: https://arxiv.org/abs/2305.14567
- Authors: 
- Reference count: 14
- This paper proposes a constant memory attention block (CMAB) that can compute outputs and perform updates in constant memory and computation.

## Executive Summary
This paper introduces CMAB (Constant Memory Attention Block) and CMANPs (Constant Memory Attentive Neural Processes) to address the memory inefficiency of traditional attention-based neural processes. CMAB achieves constant memory complexity by splitting inputs into fixed-size batches and performing sequential updates, while maintaining competitive or superior performance on image completion and meta-regression tasks. The CMANP-AND variant enables scalable not-diagonal predictions with autoregressive capabilities.

## Method Summary
CMAB is a novel attention mechanism that computes outputs and updates in constant memory regardless of input size. It achieves this through batch processing of cross-attention operations and rolling average calculations for updates. CMANPs use stacked CMAB blocks to implement the conditioning, querying, and updating phases of neural processes while maintaining constant memory complexity. The CMANP-AND variant adds autoregressive not-diagonal prediction capabilities with block size BQ=5.

## Key Results
- CMANPs achieve state-of-the-art results on image completion tasks (EMNIST, CelebA at 32x32, 64x64, 128x128 resolutions)
- CMANPs significantly outperform prior methods in memory efficiency while maintaining competitive log-likelihood scores
- CMANP-AND variant demonstrates scalable not-diagonal predictions with constant memory usage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CMAB can compute its output in constant memory regardless of input size by splitting the input into fixed-size batches and updating sequentially.
- Mechanism: The cross-attention between BEMB and INPUT is split into batches of size BC. After computing each batch's output, the memory is freed before processing the next batch, enabling constant memory usage through rolling updates.
- Core assumption: The update operation can reuse memory space after each batch computation, and numerical stability is maintained during the rolling average calculation.
- Evidence anchors:
  - [abstract] "CMAB can compute its output in constant memory regardless of the number of inputs"
  - [section 3.1] "Computing CrossAttention(BEMB, DN/BC) requires O(LBBC) constant memory. After its computation, the memory can be freed up"
  - [corpus] No direct evidence in corpus neighbors
- Break condition: If BC is too small relative to input size, computational overhead increases significantly. If numerical stability fails during rolling updates, the method breaks down.

### Mechanism 2
- Claim: CMAB performs constant computation updates per new datapoint through a rolling average calculation.
- Mechanism: When new datapoints DU arrive, the updated cross-attention output can be computed using a rolling average formula that requires O(|DU|) operations per new datapoint, rather than recomputing from scratch.
- Core assumption: The query matrix Q from BEMB remains constant while only the key/value matrices from INPUT need updating.
- Evidence anchors:
  - [section 3.1] "CMAB can compute its updated output in O(|DU |), i.e., a constant amount of computation per datapoint"
  - [appendix A.2] Detailed proof showing "emb′j = exp(log(C) − log(C ′)) × embj + ..." requiring only O(|DU|) computation
  - [corpus] No direct evidence in corpus neighbors
- Break condition: If the rolling average becomes numerically unstable with very large or very small values, or if the assumption about constant Q fails.

### Mechanism 3
- Claim: CMANPs achieve state-of-the-art results while being more memory efficient than prior attention-based neural processes.
- Mechanism: By using CMAB blocks instead of standard attention mechanisms, CMANPs reduce memory complexity from linear/quadratic to constant for conditioning, querying, and updating phases while maintaining competitive or superior performance.
- Core assumption: The bottleneck size (LI, LB) is sufficient to capture the essential information while keeping memory constant.
- Evidence anchors:
  - [abstract] "CMANPs achieve state-of-the-art results on meta-regression and image completion tasks while being significantly more memory efficient"
  - [section 4.1] "CMANP-AND achieves clear state-of-the-art results on CelebA (32x32), CelebA (64x64), and CelebA (128x128)"
  - [section 4.2] "CMANP-AND outperforms all baselines except for TNP-ND by a significant margin"
- Break condition: If the bottleneck size is too small to capture necessary information, performance degrades despite memory efficiency.

## Foundational Learning

- Concept: Attention mechanisms and their computational complexity
  - Why needed here: Understanding why standard attention mechanisms scale quadratically with input size, while CMAB achieves constant memory complexity
  - Quick check question: What is the computational complexity of standard self-attention with N input elements and d model dimension?

- Concept: Neural Processes and their three phases (conditioning, querying, updating)
  - Why needed here: CMANPs build upon this framework by replacing attention mechanisms with CMAB blocks while maintaining the same three-phase structure
  - Quick check question: In neural processes, what is the purpose of the conditioning phase and how does it differ from the querying phase?

- Concept: Rolling average and numerical stability techniques
  - Why needed here: The constant computation updates rely on rolling averages and log-sum-exp tricks to maintain numerical stability
  - Quick check question: Why is computing log(C) and log(C') preferred over computing C and C' directly in the rolling average update?

## Architecture Onboarding

- Component map: Input data (INPUT) -> CMAB block (CrossAttention(BEMB, INPUT) → SelfAttention → CrossAttention(IEMB, output) → SelfAttention) -> Next CMAB block or Predictor module

- Critical path: Input data flows through CMAB blocks; each CMAB performs: CrossAttention(BEMB, INPUT) → SelfAttention → CrossAttention(IEMB, output) → SelfAttention; output latents from one CMAB become input latents to the next; final latents are used by predictor for predictions

- Design tradeoffs:
  - Bottleneck size (LI, LB) vs. performance: Larger bottlenecks improve performance but increase memory usage
  - Block size (BQ) vs. computation time: Smaller blocks improve autoregressive performance but require more sequential computation
  - Fixed learned latents vs. adaptive mechanisms: CMAB uses fixed BEMB latents for simplicity and efficiency

- Failure signatures:
  - Performance degradation when bottleneck size is too small
  - Numerical instability in rolling average calculations
  - Memory leaks if batch size BC is not properly managed
  - Degraded autoregressive performance with large BQ values

- First 3 experiments:
  1. Verify constant memory behavior: Test CMAB with increasing input sizes while monitoring memory usage
  2. Validate constant computation updates: Add new datapoints to an existing CMAB and verify O(|DU|) update time
  3. Benchmark against standard attention: Compare CMANP performance and memory usage against baseline attention-based methods on image completion task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical upper bound on the number of latent vectors (LI and LB) that can be used in CMABs before diminishing returns or overfitting occurs?
- Basis in paper: [inferred] The paper mentions that increasing the size of the latent bottleneck (LI and LB) considerably improves model performance, but does not specify an upper limit or point of diminishing returns.
- Why unresolved: The paper only states that increasing LI and LB improves performance without quantifying the extent or optimal values, and does not provide a theoretical framework for determining an upper bound.
- What evidence would resolve it: Experimental results showing performance vs. LI and LB values, including points of diminishing returns, overfitting, or computational inefficiency.

### Open Question 2
- Question: How does the performance of CMANPs compare to other NP variants when the number of context datapoints exceeds the maximum used during training (e.g., more than 800 for CelebA experiments)?
- Basis in paper: [explicit] The paper mentions that CMANPs were trained on tasks with a maximum of 800 context datapoints and evaluates generalization to up to 2000 context datapoints, but does not provide a comprehensive comparison with other NP variants under these conditions.
- Why unresolved: The paper only provides limited analysis of CMANPs' generalization ability beyond training distribution and does not compare it to other NP variants in this regime.
- What evidence would resolve it: Comparative experiments between CMANPs and other NP variants (e.g., TNPs, LBANPs) when conditioning on context datapoints exceeding the maximum used during training.

### Open Question 3
- Question: What is the impact of varying the embedding sizes for the learned latent values (LEMB0 and BEMB) on the performance and efficiency of CMANPs?
- Basis in paper: [explicit] The paper mentions that CMANPs allow for varying embedding sizes for the learned latent values (LEMB0 and BEMB) and sets them to 64 for simplicity, but does not explore the impact of different embedding sizes on performance or efficiency.
- Why unresolved: The paper only provides a single embedding size (64) without exploring the impact of different sizes on performance or efficiency, leaving the question of optimal embedding sizes unanswered.
- What evidence would resolve it: Experimental results showing the performance and efficiency of CMANPs with varying embedding sizes for LEMB0 and BEMB, including comparisons between different sizes and their impact on memory usage and computation time.

## Limitations

- Bottleneck size selection uncertainty: The paper assumes fixed bottleneck sizes are sufficient but doesn't provide systematic analysis of how performance scales with different bottleneck dimensions
- Numerical stability assumptions: While theoretically sound, the practical numerical stability of rolling updates under extreme conditions needs empirical validation
- Memory measurement methodology: Performance comparisons are clear, but memory efficiency claims relative to TNPs and LBANPs could be more detailed

## Confidence

**High Confidence**: CMAB can compute outputs in constant memory through batch processing - The mathematical framework and implementation details are clearly specified; CMANPs achieve competitive performance on image completion and meta-regression tasks - Empirical results show consistent improvement over baseline methods

**Medium Confidence**: Constant computation updates per new datapoint - While theoretically sound, practical numerical stability needs verification; Memory efficiency claims relative to TNPs and LBANPs - Performance comparisons are clear, but memory measurement methodology could be more detailed

## Next Checks

1. Memory profiling under varying conditions: Measure actual GPU memory usage of CMANPs vs baseline methods across different input sizes and bottleneck dimensions to verify constant memory scaling claims.

2. Numerical stability stress test: Implement extreme value tests (very large/small inputs) to verify that rolling average calculations maintain numerical stability and don't produce NaNs or infinities.

3. Ablation study on bottleneck sizes: Systematically vary bottleneck dimensions (LI, LB) and measure the trade-off between performance and memory usage to identify optimal configurations and validate the assumption that small bottlenecks suffice.