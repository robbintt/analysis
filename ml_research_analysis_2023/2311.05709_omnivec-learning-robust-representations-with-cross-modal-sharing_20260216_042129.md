---
ver: rpa2
title: 'OmniVec: Learning robust representations with cross modal sharing'
arxiv_id: '2311.05709'
source_url: https://arxiv.org/abs/2311.05709
tags:
- arxiv
- omniv
- tasks
- modalities
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OmniVec introduces a unified architecture for learning robust representations
  across multiple modalities and tasks. It uses modality-specific encoders, a shared
  transformer backbone, and task-specific heads, enabling cross-modal knowledge sharing.
---

# OmniVec: Learning robust representations with cross modal sharing

## Quick Facts
- **arXiv ID**: 2311.05709
- **Source URL**: https://arxiv.org/abs/2311.05709
- **Reference count**: 40
- **Key outcome**: OmniVec achieves state-of-the-art or competitive results on 22 benchmarks spanning classification, segmentation, and summarization tasks across multiple modalities.

## Executive Summary
OmniVec introduces a unified architecture for learning robust representations across multiple modalities (image, video, audio, text, 3D) and tasks. The network uses modality-specific encoders, a shared transformer backbone, and task-specific heads to enable cross-modal knowledge sharing. Through masked autoencoding pretraining and sequential multi-task fine-tuning, OmniVec achieves strong performance on diverse benchmarks and demonstrates generalization to unseen datasets. The approach balances flexibility in handling heterogeneous data with the benefits of shared representations.

## Method Summary
OmniVec employs a unified architecture with modality-specific encoders that project inputs into a common embedding space via a shared transformer backbone. The network is first pretrained using masked autoencoding across multiple modalities, then fine-tuned sequentially on grouped tasks (simple vs dense). Modality mixing during training encourages cross-modal knowledge transfer. Task-specific heads enable adaptation to diverse downstream tasks while maintaining shared representations learned during pretraining.

## Key Results
- Achieves state-of-the-art or competitive results on 22 public benchmarks
- Demonstrates strong generalization to unseen datasets and tasks
- Ablation studies confirm effectiveness of task grouping and modality mixing
- Shows robust performance across classification, segmentation, and summarization tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The shared transformer backbone acts as a bottleneck that forces cross-modal information fusion, leading to generalized embeddings.
- **Mechanism**: Different modality encoders output distinct representations that are projected into a common embedding space via the transformer backbone. This bottleneck forces the network to find a unified representation that captures commonalities across modalities.
- **Core assumption**: The transformer architecture is expressive enough to map heterogeneous inputs into a shared latent space while preserving modality-specific details necessary for downstream tasks.
- **Evidence anchors**:
  - [abstract]: "The proposed network is composed of task specific encoders, a common trunk in the middle, followed by task specific prediction heads."
  - [section]: "The transformer network is the common part of the framework and is in effect a ‘bottleneck‘ block."
- **Break condition**: If the transformer backbone cannot adequately represent certain modalities (e.g., highly structured 3D point clouds vs. continuous audio spectrograms), the shared space may lose modality-specific discriminative features.

### Mechanism 2
- **Claim**: Sequential multi-task training with task grouping and modality mixing improves generalization by preventing catastrophic forgetting and encouraging cross-task regularization.
- **Mechanism**: The network is trained iteratively on simple and dense tasks, with mini-batches constructed by mixing samples from different modalities. This creates a curriculum that balances task complexity and forces the network to maintain representations useful across diverse tasks.
- **Core assumption**: The order and grouping of tasks matters - alternating between simple and dense tasks helps the network maintain both high-level semantic features and detailed local information.
- **Evidence anchors**:
  - [abstract]: "we train the network in a sequential manner, i.e. we train on different tasks, one after another."
  - [section]: "we propose to group the tasks based on the extent of information exploited by the task across different modalities."
- **Break condition**: If tasks are too dissimilar or the training order is poorly chosen, the network may overfit to certain task groups or fail to learn meaningful cross-task representations.

### Mechanism 3
- **Claim**: Masked pretraining across multiple modalities provides a strong self-supervised initialization that improves downstream task performance and cross-modal knowledge transfer.
- **Mechanism**: The network learns to reconstruct masked patches from multiple modalities using the same transformer backbone, forcing it to learn general-purpose representations that transfer well to supervised tasks.
- **Core assumption**: The reconstruction objective encourages the network to capture both modality-specific and modality-agnostic features that are useful for downstream tasks.
- **Evidence anchors**:
  - [abstract]: "we first pre-train it by self-supervised masked training, followed by sequential training for the different tasks."
  - [section]: "Motivated by empirical observations and previous works indicating that self-supervised pretraining helps networks in better exploiting multiple modalities."
- **Break condition**: If the masking strategy is not well-suited for certain modalities (e.g., too aggressive for text vs. too conservative for images), the pretraining may not provide meaningful initialization.

## Foundational Learning

- **Concept**: Masked autoencoding as a self-supervised learning paradigm
  - Why needed here: Provides a way to pretrain the network on large amounts of unlabeled data from multiple modalities without requiring task-specific labels
  - Quick check question: What is the difference between masked autoencoding and contrastive learning approaches?

- **Concept**: Transformer architecture and attention mechanisms
  - Why needed here: The shared backbone relies on transformer layers to process and fuse information from different modalities
  - Quick check question: How does multi-head attention in transformers help capture different aspects of the input data?

- **Concept**: Multi-task learning and catastrophic forgetting
  - Why needed here: The sequential training approach needs to balance learning multiple tasks without forgetting previously learned representations
  - Quick check question: What are the main strategies to prevent catastrophic forgetting in sequential multi-task learning?

## Architecture Onboarding

- **Component map**: Modality-specific encoders → Meta tokens → Projection layer → Shared transformer backbone → Vectorizer → Task-specific heads
- **Critical path**: Input modality → Encoder → Projection → Transformer → Vectorizer → Task head → Output
- **Design tradeoffs**: Using separate encoders per modality provides flexibility but increases computational cost; shared backbone enables cross-modal knowledge transfer but may limit modality-specific optimization
- **Failure signatures**: Poor performance on certain modalities may indicate inadequate encoder design; inconsistent results across tasks may suggest issues with task grouping or training order
- **First 3 experiments**:
  1. Test each modality encoder independently on its native task to verify basic functionality
  2. Evaluate the shared transformer backbone with two modalities on a simple classification task
  3. Test the full pipeline with task grouping but without modality mixing to isolate the effects of each component

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of modality-specific encoders impact cross-modal knowledge sharing and overall performance compared to using a single shared encoder?
- Basis in paper: [explicit] The authors mention using modality-specific encoders and compare to methods like Omnivore which uses a common encoder, noting that their approach facilitates cross-modal knowledge sharing through sequential training.
- Why unresolved: While the paper demonstrates strong performance with modality-specific encoders, it doesn't provide a direct comparison to a unified encoder architecture within the same framework, making it difficult to quantify the exact impact of this design choice on knowledge sharing.
- What evidence would resolve it: A controlled experiment within OmniVec comparing performance and cross-modal transfer capabilities using modality-specific encoders versus a single shared encoder would provide quantitative insights.

### Open Question 2
- Question: What is the optimal strategy for task grouping and modality mixing during training to maximize cross-modal knowledge transfer and generalization?
- Basis in paper: [explicit] The authors propose task grouping (simple vs. dense tasks) and modality mixing as key training strategies, but acknowledge that the optimal approach depends on the complexity of tasks and their relationships.
- Why unresolved: The paper provides empirical evidence that these strategies improve performance, but doesn't explore the full space of possible task groupings or mixing strategies, leaving open questions about their relative importance and optimal configuration.
- What evidence would resolve it: Systematic ablation studies varying task grouping criteria, mixing ratios, and training schedules would reveal the impact of these choices on performance and knowledge transfer across different modalities and tasks.

### Open Question 3
- Question: How does the proposed masked pretraining approach generalize to new modalities not seen during pretraining, and what are the limitations of this approach?
- Basis in paper: [inferred] The authors demonstrate pretraining on multiple modalities (image, video, audio, text, 3D) and show generalization to unseen datasets, but don't explicitly test the framework's ability to adapt to entirely new modalities.
- Why unresolved: While the paper shows strong performance across diverse modalities, it doesn't investigate the framework's extensibility to modalities outside the pretraining distribution, which is crucial for real-world applications with evolving data types.
- What evidence would resolve it: Experiments pretraining on a subset of modalities and evaluating the framework's ability to learn from and transfer knowledge to completely new modalities (e.g., LiDAR, SAR, EEG) would reveal the generalization limits of the masked pretraining approach.

## Limitations

- Limited corpus validation for proposed mechanisms due to lack of directly comparable related work
- Missing critical hyperparameters for faithful reproduction of pretraining and fine-tuning procedures
- Task grouping criteria and iterative training schedule lack precise definitions in the paper

## Confidence

- **Mechanism 1**: Medium - Weak corpus validation and limited experimental isolation of the bottleneck effect
- **Mechanism 2**: Medium - Empirical support but no rigorous validation of catastrophic forgetting prevention
- **Mechanism 3**: Medium - Strong empirical results but masking strategy details are underspecified

## Next Checks

1. **Ablation study on backbone capacity**: Test the shared transformer backbone with varying numbers of layers and attention heads to determine the minimum capacity needed for effective cross-modal fusion.

2. **Task similarity analysis**: Systematically vary the task grouping strategy (by modality, by complexity, by supervision type) to quantify the impact of task similarity on cross-task knowledge transfer.

3. **Cross-modal generalization stress test**: Evaluate the model on truly unseen combinations of modalities (e.g., audio-video without text) and tasks to assess the limits of its generalization capabilities.