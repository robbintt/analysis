---
ver: rpa2
title: 'HowkGPT: Investigating the Detection of ChatGPT-generated University Student
  Homework through Context-Aware Perplexity Analysis'
arxiv_id: '2305.18226'
source_url: https://arxiv.org/abs/2305.18226
tags:
- perplexity
- dataset
- text
- threshold
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces HowkGPT, a tool to detect AI-generated university
  homework using perplexity analysis. It leverages a dataset of academic assignments
  and metadata to compute perplexity scores for student and ChatGPT-generated responses.
---

# HowkGPT: Investigating the Detection of ChatGPT-generated University Student Homework through Context-Aware Perplexity Analysis

## Quick Facts
- arXiv ID: 2305.18226
- Source URL: https://arxiv.org/abs/2305.18226
- Reference count: 39
- Primary result: HowkGPT uses category-specific perplexity thresholds to detect AI-generated homework, with improved accuracy when filtering out math and code responses.

## Executive Summary
HowkGPT is a detection tool designed to identify ChatGPT-generated university homework by leveraging perplexity analysis. The system computes perplexity scores for student and AI-generated responses using a pre-trained GPT-2 model and establishes category-specific thresholds based on metadata. Experiments show that filtering out math and code-related questions improves detection performance, and categorization by knowledge and cognitive dimensions yields better accuracy than global thresholds. The tool is available as a web application for real-time assessment.

## Method Summary
HowkGPT detects AI-generated homework by computing perplexity scores using a pre-trained GPT-2 model. The method involves filtering the dataset based on metadata, computing optimal perplexity thresholds using F1 score and AUC metrics for each category, and evaluating classification accuracy. The system leverages a dataset of student and ChatGPT-generated responses with metadata categorization to tailor thresholds to specific academic contexts.

## Key Results
- Category-specific perplexity thresholds improve detection accuracy over global thresholds.
- Filtering out math and code-related questions enhances performance.
- Knowledge and cognitive process dimension categorization yields better accuracy than overall thresholds.
- Optimal perplexity thresholds vary by category and dataset flavor, with F1 score and AUC providing different optimal values.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Category-specific perplexity thresholds improve detection accuracy compared to a single global threshold.
- Mechanism: Academic assignments vary in linguistic structure and complexity across knowledge and cognitive dimensions. By computing optimal perplexity thresholds for each category, HowkGPT tailors its decision boundary to the specific characteristics of that subset of data.
- Core assumption: Within each category, the distribution of perplexity values for student-authored and AI-generated texts is sufficiently distinct to allow for effective thresholding.
- Evidence anchors:
  - [abstract]: "Given the specificity and contextual nature of academic work, HowkGPT further refines its analysis by defining category-specific thresholds derived from the metadata, enhancing the precision of the detection."
  - [section]: "In order to better understand the functionality of the proposed algorithm we provide detailed step by step intermediate results for the use cases mentioned in Table III."
  - [corpus]: Weak evidence - no direct corpus comparison of category-level detection rates.
- Break condition: If categories overlap significantly in perplexity distribution, category-specific thresholds lose discriminative power and may reduce overall accuracy.

### Mechanism 2
- Claim: Filtering out math and code-related questions improves detection performance.
- Mechanism: Math and code responses follow rigid syntactic patterns that LLMs can reproduce closely, making perplexity less discriminative. Removing these items reduces noise in the perplexity distribution.
- Core assumption: Pattern-based responses (math/code) produce perplexity scores that are not informative for source attribution.
- Evidence anchors:
  - [abstract]: "Experiments show that filtering out math and code-related questions improves performance."
  - [section]: "Considering the fundamental principles of the perplexity metric, such short texts do not provide adequate context for the algorithm to provide a meaningful value."
  - [corpus]: Weak evidence - corpus does not provide quantitative before/after filtering performance metrics.
- Break condition: If math/code items are highly context-dependent and semantically rich, removing them may reduce dataset diversity and harm model generalizability.

### Mechanism 3
- Claim: Perplexity computed using a pre-trained GPT-2 model can distinguish between ChatGPT and student-generated text.
- Mechanism: Perplexity reflects how well a language model predicts a sequence. Student writing often deviates from predictable patterns, yielding higher perplexity, whereas ChatGPT outputs tend to be more fluent and predictable, resulting in lower perplexity.
- Core assumption: GPT-2 and ChatGPT share similar generative behavior, so GPT-2-based perplexity is a valid proxy for ChatGPT-generated text.
- Evidence anchors:
  - [abstract]: "These scores then assist in establishing a threshold for discerning the origin of a submitted assignment."
  - [section]: "Hashimoto et al. [16] demonstrated the potential of discerning between human-written and AI-generated texts based on model likelihood."
  - [corpus]: No corpus evidence for GPT-2 perplexity vs. ChatGPT text comparison.
- Break condition: If ChatGPT uses substantially different architecture or training data than GPT-2, the proxy relationship breaks down.

## Foundational Learning

- Concept: Perplexity as a measure of text predictability.
  - Why needed here: HowkGPT uses perplexity scores to distinguish AI-generated from human-authored text.
  - Quick check question: If a text has low perplexity under GPT-2, what does that imply about its likelihood of being AI-generated?
- Concept: Receiver Operating Characteristic (ROC) curves and AUC.
  - Why needed here: The paper uses ROC/AUC to select optimal perplexity thresholds and evaluate classifier performance.
  - Quick check question: What does a high AUC indicate about a classifier's ability to separate two classes?
- Concept: Dataset filtering and flavor creation.
  - Why needed here: Different dataset flavors (e.g., excluding math/code) are used to test robustness and improve detection accuracy.
  - Quick check question: Why might excluding math and code responses improve perplexity-based detection?

## Architecture Onboarding

- Component map: Dataset ingestion and metadata extraction -> Perplexity computation pipeline (GPT-2 model + tokenizer) -> Threshold optimization (F1 vs. AUC methods) -> Category classification (taxonomy-based) -> Web application frontend and backend API
- Critical path: Data -> Preprocessing -> Perplexity Scoring -> Category Assignment -> Threshold Lookup -> Classification Decision
- Design tradeoffs:
  - Use of GPT-2 (accessible) vs. ChatGPT model (inaccessible)
  - Single global threshold vs. category-specific thresholds
  - Filtering out math/code for cleaner distributions vs. reduced dataset size
- Failure signatures:
  - High false positives/negatives when perplexity distributions overlap heavily
  - Poor performance on short or highly formulaic responses
  - Degradation if categories are not well-defined or balanced
- First 3 experiments:
  1. Compute perplexity on a small sample of student vs. ChatGPT responses to confirm expected score ranges.
  2. Apply category-based thresholding on a held-out test set and compare to global threshold accuracy.
  3. Test detection performance before and after filtering out math/code questions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of HowkGPT vary when using different pretrained language models (e.g., GPT-2 vs. GPT-3.5) for perplexity calculation, given that the paper uses GPT-2 due to the unavailability of ChatGPT's underlying models?
- Basis in paper: [explicit] The paper mentions using GPT-2 due to the unavailability of GPT-3.5 and GPT-4, but does not explore the performance impact of using different models.
- Why unresolved: The paper does not provide a comparison of HowkGPT's performance using different pretrained language models, which could affect the accuracy of AI-generated text detection.
- What evidence would resolve it: Experimental results comparing the detection accuracy of HowkGPT using various pretrained language models, such as GPT-2, GPT-3.5, and potentially others, on the same dataset.

### Open Question 2
- Question: Can the knowledge and cognitive process dimension categorizations be further refined or expanded to improve the accuracy of AI-generated text detection, and how would this impact the perplexity threshold values?
- Basis in paper: [inferred] The paper shows that categorization using knowledge and cognitive process dimensions improves detection accuracy, but does not explore the potential for further refinement or expansion of these categories.
- Why unresolved: The paper does not investigate the effects of refining or expanding the categorization dimensions on detection accuracy or perplexity threshold values.
- What evidence would resolve it: Experimental results demonstrating the impact of refined or expanded categorization dimensions on detection accuracy and perplexity threshold values, potentially using a larger and more diverse dataset.

### Open Question 3
- Question: How does the performance of HowkGPT change when applied to texts from different domains or disciplines, and are there domain-specific adjustments needed for optimal detection accuracy?
- Basis in paper: [explicit] The paper mentions that HowkGPT was tested on a dataset of academic assignments from various courses but does not explore its performance across different domains or disciplines.
- Why unresolved: The paper does not provide insights into how HowkGPT's performance varies across different domains or disciplines, which could affect its generalizability and effectiveness.
- What evidence would resolve it: Experimental results comparing the detection accuracy of HowkGPT when applied to texts from different domains or disciplines, potentially requiring domain-specific adjustments to the categorization and perplexity threshold values.

## Limitations

- Dataset dependency: The performance of HowkGPT is highly dependent on the specific dataset used, with no public dataset available for replication.
- Model alignment uncertainty: The paper uses GPT-2 for perplexity computation but does not validate whether GPT-2-based scores are truly reflective of ChatGPT-generated text.
- No real-world validation: The paper does not report on any real-world deployment or blind testing with unlabeled student submissions.

## Confidence

- High confidence: Category-specific thresholds improve detection over global thresholds.
- Medium confidence: Filtering out math and code improves performance.
- Low confidence: GPT-2 perplexity reliably distinguishes ChatGPT-generated text.

## Next Checks

1. **Dataset replication test**: Replicate the perplexity scoring on an independently sourced dataset of student and ChatGPT-generated academic responses to test generalizability.
2. **Model alignment check**: Compare perplexity scores using both GPT-2 and a smaller ChatGPT-based model (if accessible) to quantify the impact of model choice on detection accuracy.
3. **Threshold stability assessment**: Evaluate the stability of category-specific perplexity thresholds over multiple time periods or assignment cohorts to assess temporal robustness.