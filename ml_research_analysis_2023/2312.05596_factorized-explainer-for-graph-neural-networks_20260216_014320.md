---
ver: rpa2
title: Factorized Explainer for Graph Neural Networks
arxiv_id: '2312.05596'
source_url: https://arxiv.org/abs/2312.05596
tags:
- graph
- explanation
- methods
- neural
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a fundamental limitation of existing GNN
  explanation methods based on the graph information bottleneck (GIB) principle, showing
  they admit trivial solutions that fail to provide meaningful explanations. The authors
  propose a modified GIB formulation and introduce a novel factorized explainer (K-FactExplainer)
  that combines multiple local explainers to avoid locality and lossy aggregation
  issues.
---

# Factorized Explainer for Graph Neural Networks

## Quick Facts
- arXiv ID: 2312.05596
- Source URL: https://arxiv.org/abs/2312.05596
- Reference count: 30
- Key outcome: This paper identifies a fundamental limitation of existing GNN explanation methods based on the graph information bottleneck (GIB) principle, showing they admit trivial solutions that fail to provide meaningful explanations. The authors propose a modified GIB formulation and introduce a novel factorized explainer (K-FactExplainer) that combines multiple local explainers to avoid locality and lossy aggregation issues. The method is theoretically analyzed and shown to outperform state-of-the-art baselines on both synthetic and real-world datasets, achieving significant improvements in explanation faithfulness (AUC-ROC) across multiple benchmarks including BA-Shapes, BA-Community, Tree-Circles, Tree-Grid, BA-2motifs, and MUTAG. The K-FactExplainer demonstrates consistent gains by addressing the fundamental limitations of existing GIB-based approaches while maintaining comparable inference efficiency.

## Executive Summary
This paper identifies a fundamental limitation in existing GNN explanation methods based on the graph information bottleneck (GIB) principle, showing they can produce trivial solutions that fail to provide meaningful explanations. The authors propose a modified GIB formulation and introduce K-FactExplainer, a novel approach that combines multiple local explainers to address locality and lossy aggregation issues in parametric explainers. The method is theoretically analyzed and shown to outperform state-of-the-art baselines on both synthetic and real-world datasets, achieving significant improvements in explanation faithfulness across multiple benchmarks.

## Method Summary
The paper proposes K-FactExplainer, a factorized explainer that combines multiple local MLPs with a global MLP to mitigate locality and aggregation issues in existing GIB-based methods. The approach modifies the GIB principle by using cross-entropy between true and predicted labels instead of mutual information, preventing trivial solutions in statistically degraded classification tasks. A bootstrapping algorithm estimates the optimal number of MLPs (k) by analyzing explanation graphs from a pre-trained explainer, providing an upper bound on necessary local explainers. The method is theoretically analyzed and shown to outperform baselines on synthetic and real-world datasets including BA-Shapes, BA-Community, Tree-Circles, Tree-Grid, BA-2motifs, and MUTAG.

## Key Results
- K-FactExplainer achieves significant improvements in explanation faithfulness (AUC-ROC) across multiple synthetic benchmarks including BA-Shapes, BA-Community, Tree-Circles, and Tree-Grid
- On multi-motif tasks (BA-2motifs), K-FactExplainer with k=3 demonstrates superior coverage rate compared to baseline methods
- On real-world MUTAG dataset, K-FactExplainer outperforms state-of-the-art methods in explanation faithfulness while maintaining comparable inference efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The factorized explainer avoids trivial solutions in GIB-based explanation methods by using cross-entropy between true and predicted labels instead of mutual information.
- Mechanism: The modified GIB objective G* = arg min_{PG'|G:E(|G'|)≤γ} I(G,G') + αCE(Y,Y') prevents the explainer from producing subgraphs that only signal the predicted label but are independent of the input graph.
- Core assumption: The original GIB formulation admits trivial solutions in statistically degraded classification tasks where G ↔ h(G) ↔ Y holds.
- Evidence anchors:
  - [abstract]: "we show analytically that for a large class of explanation tasks, conventional approaches, which are based on the principle of graph information bottleneck (GIB), admit trivial solutions that do not align with the notion of explainability."
  - [section]: "We propose a modified version of the GIB principle that avoids this trivial solution and is applicable in constructing GNN explanation methods."
  - [corpus]: Weak evidence - no direct mention of this specific mechanism in related papers.
- Break condition: If the classification task is not statistically degraded (as defined in Definition 1), the modified GIB may not provide additional benefits over the original formulation.

### Mechanism 2
- Claim: The K-FactExplainer mitigates locality issues in parametric explainers by combining multiple local explainers.
- Mechanism: By using k MLPs (Ψt) and a global weighting MLP (Ψ0), the method creates a factorized explanation that can capture multiple motifs in the input graph, avoiding the limitations of single local explainers.
- Core assumption: Local explanation methods are suboptimal in multi-motif classification tasks because they cannot capture all relevant subgraphs simultaneously.
- Evidence anchors:
  - [section]: "we propose a new framework to unify existing parametric methods and show that their suboptimality is caused by their locality property and the lossy aggregation step in GNNs."
  - [section]: "Theorem 2 implies that local explanation methods are not optimal in multi-motif classification tasks."
  - [corpus]: Weak evidence - related papers mention locality issues but don't discuss the specific factorization approach.
- Break condition: If the input graph contains only a single motif or the GNN has lossless aggregation (high-dimensional layers), the benefits of factorization may be minimal.

### Mechanism 3
- Claim: The bootstrapping algorithm provides an effective way to determine the optimal number of MLPs (k) without prior knowledge of motifs.
- Mechanism: The algorithm estimates k by analyzing the minimal cover of explanation graphs produced by a pre-trained explainer, providing an upper bound on the number of necessary local explainers.
- Core assumption: The explanation graphs produced by a pre-trained explainer (e.g., PGExplainer) can be used as a proxy for the true motifs in the data.
- Evidence anchors:
  - [section]: "We provide an approximate solution, where instead of finding the minimal cover for Ge, we use a bootstrapping method in which we find the minimal cover for the explanation graphs produced by another pre-trained explainer."
  - [section]: "It takes the GNN model to be explained f, a set of training input graphs G, and a post-hoc explainer Ψ as input."
  - [corpus]: Weak evidence - no direct mention of this specific bootstrapping approach in related papers.
- Break condition: If the pre-trained explainer fails to capture the true motifs, the bootstrapping algorithm may produce an incorrect estimate of k.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and message-passing mechanisms
  - Why needed here: Understanding how GNNs aggregate information from neighboring nodes is crucial for comprehending the limitations of local explanation methods and the benefits of factorization.
  - Quick check question: How does the aggregation function in a GNN layer affect the ability of local explainers to capture multi-motif explanations?

- Concept: Information Bottleneck Principle and its application to graph data
  - Why needed here: The GIB principle forms the theoretical foundation for many GNN explanation methods, and understanding its limitations is key to appreciating the proposed modifications.
  - Quick check question: What is the difference between the original GIB formulation and the modified version proposed in this paper?

- Concept: Motifs and their role in graph classification tasks
  - Why needed here: The paper focuses on explaining GNN predictions in terms of important subgraphs (motifs), so understanding how motifs relate to graph labels is essential.
  - Quick check question: In the context of graph classification, what is the relationship between motifs and the predicted label?

## Architecture Onboarding

- Component map:
  Input graph G = (V, E; Z, A) and trained GNN model f -> Node embeddings Zi, i ∈ [n] -> Edge embeddings Zi,j = (Zi, Zj) -> Graph embedding Z = (Zi, i ∈ [n]) -> k MLPs (Ψt) operating on edge embeddings -> Global MLP (Ψ0) assigning weights -> Edge probability vector Ω = (Σ_{t=1}^k PK(t)ωt_{i,j})_{i,j∈[n]} -> Sampled explanation graph G* -> Output explanation graph G* and cross-entropy loss CE(Y', Ŷ)

- Critical path:
  1. Compute node embeddings from input graph using trained GNN
  2. Generate edge and graph embeddings
  3. Pass graph embedding through global MLP to obtain weights
  4. Pass each edge embedding through k local MLPs
  5. Combine outputs of local MLPs using weights from global MLP
  6. Sample explanation graph from combined edge probabilities
  7. Compute cross-entropy loss and update model parameters

- Design tradeoffs:
  - Number of MLPs (k): Higher k allows capturing more motifs but increases computational complexity
  - Dimensionality of embeddings: Higher dimensions reduce lossy aggregation but increase memory usage
  - Choice of pre-trained explainer for bootstrapping: Affects the accuracy of k estimation

- Failure signatures:
  - Poor explanation quality: May indicate incorrect choice of k or issues with the pre-trained explainer used for bootstrapping
  - High computational cost: Could be due to excessive number of MLPs or high-dimensional embeddings
  - Instability during training: Might result from improper weighting between local and global explainers

- First 3 experiments:
  1. Evaluate explanation quality on BA-Shapes dataset with varying k values to observe the impact of factorization on single-motif tasks
  2. Test the bootstrapping algorithm on BA-2motifs dataset to verify its ability to estimate k for multi-motif tasks
  3. Compare explanation faithfulness (AUC-ROC) against state-of-the-art methods on MUTAG dataset to demonstrate overall performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact theoretical relationship between the number of motifs in a dataset and the optimal value of k for the K-FactExplainer?
- Basis in paper: [explicit] The paper states "Particularly, if the classifier to be explained, f(·), is a GNN with ℓ layers, and ℓ is greater than or equal to the largest geodisc diameter of the motifs gi, i ∈ [s], then k can be upper-bounded by s" (Proposition 2)
- Why unresolved: The proof of Proposition 2 provides an upper bound but doesn't prove this is the optimal value of k. The paper only empirically shows that performance improves with larger k values in multi-motif scenarios.
- What evidence would resolve it: A theoretical proof that the upper bound from Proposition 2 is actually the optimal k value, or extensive empirical testing showing the exact relationship between number of motifs and optimal k.

### Open Question 2
- Question: How does the K-FactExplainer perform on datasets with overlapping motifs or motifs that share common substructures?
- Basis in paper: [inferred] The paper analyzes single-motif and multi-motif scenarios separately but doesn't address cases where motifs overlap or share substructures, which would violate the assumption that "the geodisc distance between gi and gj is greater than r for all i ≠ j" used in Theorem 2
- Why unresolved: Real-world datasets often contain overlapping motifs, and the current theoretical analysis assumes disjoint motifs
- What evidence would resolve it: Experiments on datasets with overlapping motifs and theoretical analysis extending Theorem 2 to handle overlapping motifs.

### Open Question 3
- Question: What is the impact of the K-FactExplainer's bootstrapping algorithm on datasets where the initial explainer (e.g., PGExplainer) performs poorly?
- Basis in paper: [explicit] The paper describes a bootstrapping algorithm that "finds the minimal cover for the explanation graphs produced by another pre-trained explainer, e.g., a PGExplainer" but doesn't analyze how this affects performance when the initial explainer is inaccurate
- Why unresolved: The quality of the K-FactExplainer depends on the quality of the initial explainer used for bootstrapping, but this dependency is not characterized
- What evidence would resolve it: Experiments comparing K-FactExplainer performance when initialized with different baseline explainers, and theoretical analysis of how errors in the initial explainer propagate through the bootstrapping process.

## Limitations
- The effectiveness of the modified GIB formulation depends on the assumption that classification tasks are "statistically degraded" - this requires careful validation across diverse real-world datasets
- The factorization approach assumes that multi-motif tasks benefit from multiple local explainers, which may not hold for all graph classification problems
- The bootstrapping algorithm's performance depends heavily on the quality of the pre-trained explainer used as a proxy for true motifs

## Confidence
- High confidence: The empirical results demonstrating improved AUC-ROC scores across multiple benchmarks (BA-Shapes, BA-Community, Tree-Circles, Tree-Grid, BA-2motifs, MUTAG) are robust and well-supported by the experimental data
- Medium confidence: The theoretical analysis of GIB limitations and the modified formulation is sound, but requires further validation on datasets with varying statistical properties to confirm generalizability
- Medium confidence: The factorization approach and bootstrapping algorithm show promise, but their effectiveness may be dataset-dependent and requires additional testing on real-world graphs with unknown motif structures

## Next Checks
1. **Statistical degradation analysis**: Systematically vary the statistical properties of synthetic datasets (e.g., noise levels, motif complexity) to determine the boundary conditions under which the modified GIB formulation provides benefits over the original approach

2. **Motif diversity evaluation**: Create benchmark datasets with varying numbers and types of motifs (e.g., 1-5 motifs with different structural properties) to quantify how factorization benefits scale with motif complexity and diversity

3. **Real-world application testing**: Apply K-FactExplainer to real-world molecular datasets (beyond MUTAG) and social network graphs to evaluate performance on datasets with unknown and potentially overlapping motif structures, comparing results against domain-expert annotated explanations where available