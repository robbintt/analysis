---
ver: rpa2
title: Modular Deep Learning
arxiv_id: '2302.11529'
source_url: https://arxiv.org/abs/2302.11529
tags:
- learning
- modules
- routing
- conference
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Modular deep learning is a framework where neural network components
  (modules) are specialized for distinct tasks and can be dynamically routed to and
  aggregated. The core idea is to decompose a model into computation functions, a
  routing function, and an aggregation function.
---

# Modular Deep Learning

## Quick Facts
- arXiv ID: 2302.11529
- Source URL: https://arxiv.org/abs/2302.11529
- Reference count: 40
- Primary result: Modular deep learning framework enables parameter-efficient fine-tuning, prevents negative interference, and supports zero-shot transfer through specialized module composition

## Executive Summary
Modular deep learning decomposes neural networks into specialized components (modules) that can be dynamically routed and aggregated for specific tasks. The framework separates computation, routing, and aggregation functions, allowing models to be fine-tuned efficiently by updating only relevant modules rather than entire architectures. This approach enables positive transfer between tasks, systematic generalization, and compositionality across domains including NLP, speech processing, computer vision, and reinforcement learning. Empirical results demonstrate that modular methods like LoRA and mixture-of-experts outperform standard fine-tuning in both efficiency and generalization.

## Method Summary
The survey provides a unified taxonomy of modular deep learning architectures, categorizing existing methods into computation functions (parameter, input, or function composition), routing mechanisms (fixed or learned, hard or soft), and aggregation strategies (parameter, representation, input, or function aggregation). While the paper describes modular design principles and categorizes research threads, it does not provide specific implementation details or training procedures for a single model. The framework emphasizes parameter-efficient fine-tuning, zero-shot transfer through module recombination, and systematic task generalization. Applications span multiple domains but lack standardized evaluation benchmarks.

## Key Results
- Modular approaches achieve superior parameter efficiency compared to full model fine-tuning
- Zero-shot transfer is enabled through composition and recombination of specialized modules
- Mixture-of-experts and adapter-based methods demonstrate strong empirical performance across diverse tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular deep learning prevents negative interference by allocating distinct functions to different dedicated modules.
- Mechanism: Each module specializes for a unique purpose and is reused consistently, enabling separation of computation from routing and local updates.
- Core assumption: Modules correspond to independent and reusable physical mechanisms that don't interfere with each other.
- Evidence anchors:
  - [abstract]: "These properties enable positive transfer and systematic generalisation by separating computation from routing and updating modules locally."
  - [section]: "In other words, each module is specialised for a unique purpose, for which it is reused consistently."
  - [corpus]: "Found 25 related papers (using 8). Average neighbor FMR=0.398, average citations=1.4." (Weak corpus evidence)
- Break Condition: If modules become entangled or start sharing parameters, negative interference can occur.

### Mechanism 2
- Claim: Modular deep learning achieves parameter efficiency by only updating a small subset of parameters for each task.
- Mechanism: Modules are implemented as parameter-efficient components (sparse subnetworks, low-rank adapters, input composition) that can be added to a frozen base model.
- Core assumption: Only a small number of parameters are relevant for a particular task, and similar tasks share similar sub-networks.
- Evidence anchors:
  - [abstract]: "The main advantages of modular neural architectures are positive transfer, compositionality, and parameter efficiency."
  - [section]: "In this framework, fine-tuning a model on a specific task only requires storing a modular adapter rather than a separate copy of the entire (typically large) model."
  - [corpus]: "Found 25 related papers (using 8). Average neighbor FMR=0.398, average citations=1.4." (Weak corpus evidence)
- Break Condition: If the base model parameters need to be updated frequently, parameter efficiency is lost.

### Mechanism 3
- Claim: Modular deep learning enables zero-shot transfer by recombining specialized modules for new tasks.
- Mechanism: Modules representing different skills or features can be composed together and updated locally, without affecting the rest of the network.
- Core assumption: Skills required for complex tasks are a superset of those of simpler tasks, and modules can be recombined in novel ways.
- Evidence anchors:
  - [abstract]: "This modular approach enables parameter-efficient fine-tuning, prevents negative interference, and supports zero-shot transfer by recombining specialized modules."
  - [section]: "For instance, while modules for the Guaraní language and for dependency parsing can only be trained separately due to the lack of annotated data for dependency parsing in Guaraní, they can be composed to perform inference on this unobserved task–language combination."
  - [corpus]: "Found 25 related papers (using 8). Average neighbor FMR=0.398, average citations=1.4." (Weak corpus evidence)
- Break Condition: If modules cannot be composed or recombined effectively, zero-shot transfer fails.

## Foundational Learning

- Concept: Neural network layers and their parameters
  - Why needed here: Understanding how modules modify the base model through parameter composition, input composition, or function composition requires knowledge of neural network layers.
  - Quick check question: What is the difference between a linear layer and a transformer layer in terms of their parameters and computation?

- Concept: Routing and aggregation functions
  - Why needed here: Modular deep learning relies on routing functions to select active modules and aggregation functions to combine their outputs.
  - Quick check question: How does soft routing differ from hard routing in terms of module selection and efficiency?

- Concept: Transfer learning and fine-tuning
  - Why needed here: Modular deep learning is often used for parameter-efficient fine-tuning of pre-trained models on new tasks.
  - Quick check question: What are the advantages of using modular adapters over full model fine-tuning in terms of parameter efficiency and preventing negative interference?

## Architecture Onboarding

- Component map:
  - Base model (frozen parameters)
  - Modules (parameter-efficient components)
  - Routing function (selects active modules)
  - Aggregation function (combines module outputs)
  - Training settings (joint, continual, or post-hoc)

- Critical path:
  - Define the base model and its parameters
  - Design the module architecture (parameter, input, or function composition)
  - Implement the routing function (fixed, learned, hard, or soft)
  - Choose the aggregation function (parameter, representation, input, or function aggregation)
  - Select the training setting (joint, continual, or post-hoc)

- Design tradeoffs:
  - Parameter efficiency vs. model capacity
  - Modularity vs. entanglement
  - Fixed routing vs. learned routing
  - Soft routing vs. hard routing
  - Parameter aggregation vs. representation aggregation

- Failure signatures:
  - Negative interference between modules
  - Poor zero-shot transfer performance
  - Inefficient parameter usage
  - Unstable or collapsed routing

- First 3 experiments:
  1. Implement a simple modular architecture with LoRA adapters and fixed routing on a small NLP task.
  2. Compare the performance and parameter efficiency of modular fine-tuning vs. full model fine-tuning on a larger NLP benchmark.
  3. Evaluate the zero-shot transfer capabilities of the modular architecture on a new task or language.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can learned routing methods be made more effective to match or surpass fixed routing in modular deep learning architectures?
- Basis in paper: [explicit] Section 4.2.1 discusses challenges of learned routing including training instability, module collapse, and overfitting, and states that learned routing often underperforms fixed routing.
- Why unresolved: The paper identifies challenges but does not provide definitive solutions to overcome them, leaving the effectiveness of learned routing an open question.
- What evidence would resolve it: Empirical studies comparing learned and fixed routing across diverse tasks and architectures, demonstrating learned routing's superiority in efficiency or performance.

### Open Question 2
- Question: How do different computation functions (parameter composition, input composition, function composition) influence the nature of learned modular representations?
- Basis in paper: [inferred] Section 3 discusses three types of computation functions and their trade-offs, but does not explore how they affect the learned representations.
- Why unresolved: The paper focuses on functional equivalence but does not investigate the impact of computation functions on the properties of modular representations.
- What evidence would resolve it: Analysis of modular representations learned with different computation functions, examining their structure, interpretability, and generalization capabilities.

### Open Question 3
- Question: How can modular deep learning be standardized for evaluation across diverse applications and architectures?
- Basis in paper: [explicit] Section 9.1 mentions the lack of standardized evaluation benchmarks and metrics for modular models.
- Why unresolved: The paper highlights the need for standardization but does not propose specific benchmarks or evaluation criteria.
- What evidence would resolve it: Development and adoption of standardized benchmarks, metrics, and evaluation protocols specifically designed for modular deep learning models.

## Limitations
- Weak empirical grounding with only 25 related papers and modest citation counts (average 1.4)
- Strong assumptions about module independence that may not hold in practice
- Limited systematic validation across the proposed framework

## Confidence
- High confidence: Parameter efficiency claims for specific methods like LoRA and adapters, which have substantial empirical validation in the literature
- Medium confidence: Zero-shot transfer capabilities, as these are demonstrated in specific cases but not systematically validated across the framework
- Low confidence: The claim that modular architectures inherently prevent negative interference, which is theoretically appealing but not rigorously proven

## Next Checks
1. Implement a controlled experiment testing negative interference between modules on a multi-task benchmark, measuring performance degradation when modules are combined versus trained jointly.

2. Conduct ablation studies on routing strategies (hard vs. soft) across different task distributions to quantify efficiency-accuracy tradeoffs and identify failure conditions.

3. Evaluate zero-shot transfer capabilities systematically by training modules on disjoint task subsets and measuring performance on held-out combinations, comparing against non-modular baselines.