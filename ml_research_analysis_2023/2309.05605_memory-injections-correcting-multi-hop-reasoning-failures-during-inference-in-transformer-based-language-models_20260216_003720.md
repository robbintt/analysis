---
ver: rpa2
title: 'Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference
  in Transformer-Based Language Models'
arxiv_id: '2309.05605'
source_url: https://arxiv.org/abs/2309.05605
tags:
- multi-hop
- memory
- prompt
- layer
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multi-hop reasoning in transformer-based
  language models, where models struggle to consistently answer questions requiring
  synthesis of information from diverse sources. The authors propose a novel approach
  called "memory injections" to pinpoint and rectify these failures during inference.
---

# Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models

## Quick Facts
- **arXiv ID**: 2309.05605
- **Source URL**: https://arxiv.org/abs/2309.05605
- **Reference count**: 37
- **One-line primary result**: Memory injections can increase the probability of the desired next token in multi-hop tasks by up to 424%.

## Executive Summary
This paper addresses the challenge of multi-hop reasoning in transformer-based language models, where models struggle to consistently answer questions requiring synthesis of information from diverse sources. The authors propose a novel approach called "memory injections" to pinpoint and rectify these failures during inference. By analyzing per-layer activations of GPT-2 models and injecting pertinent prompt-specific information at critical locations, the technique enhances the quality of multi-hop prompt completions. The primary result shows that a simple, efficient, and targeted memory injection into a key attention layer can significantly increase the probability of the desired next token in multi-hop tasks.

## Method Summary
The paper analyzes per-layer activations of GPT-2 models in response to single and multi-hop prompts, identifying attention heads as the primary mechanism for memory retrieval. The authors then propose injecting pertinent prompt-specific information (referred to as "memories") at critical LLM locations during inference. This involves adding embedded memories into the outputs of specific attention heads at selected layers during the inference pass. The method is tested on two GPT-2 models (Small and Large) using curated and programmatically generated datasets of single and multi-hop prompt pairs, measuring the increase in next token prediction probability as the success metric.

## Key Results
- Attention heads, not MLPs, are responsible for retrieving memories critical to successful model predictions
- Multi-hop prompts fail because attention heads cannot resolve implicit subjects to explicit ones
- Memory injections at specific attention layers can increase next token prediction probability by up to 424%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Attention heads retrieve memories critical to correct predictions during inference.
- **Mechanism**: The model's multi-headed self-attention layers act as key-value stores, where specific attention heads retrieve relevant factual associations needed to complete prompts.
- **Core assumption**: The information needed for multi-hop reasoning is already encoded in the model's weights, but certain attention heads fail to retrieve it when the subject is stated implicitly.
- **Evidence anchors**:
  - [abstract] "We find that in transformer-based models it is attention heads, rather than multi-layer perceptrons, that are responsible for retrieving memories critical to successful model predictions"
  - [section 4.1] "we leverage the model's unembedding matrix to study the internal mechanism of each attention head" and "we hypothesize that attention layers play an important role in retrieving memories relevant to the 'hop' in a given prompt"
- **Break condition**: If the needed memory is not encoded in the model's weights during training, or if the model uses a different architecture that doesn't rely on attention heads for memory retrieval.

### Mechanism 2
- **Claim**: Multi-hop prompts fail because attention heads cannot resolve implicit subjects to explicit ones.
- **Mechanism**: When a prompt references its subject indirectly (e.g., "the largest coral reef" instead of "the Great Barrier Reef"), the attention mechanism fails to make the necessary inference to retrieve relevant information, leading to incorrect predictions.
- **Core assumption**: The model can successfully complete single-hop prompts where the subject is explicit, indicating the information is present but the retrieval mechanism fails for implicit subjects.
- **Evidence anchors**:
  - [abstract] "LLMs often fail to recall relevant memories when attempting to answer a prompt that requires multiple 'hops' of reasoning, rather than lacking knowledge of the memories altogether"
  - [section 2.1] Definition of multi-hop prompts requiring an additional "hop" or inference step to identify the implicit subject
- **Break condition**: If the model's architecture doesn't use attention heads for retrieval, or if the implicit subject resolution requires reasoning beyond simple memory retrieval.

### Mechanism 3
- **Claim**: Injecting curated memories at specific attention layers can correct retrieval failures.
- **Mechanism**: By adding the missing hop information (e.g., "The Great Barrier Reef") directly into the attention head outputs at critical layers, the model can bypass the retrieval failure and make correct predictions.
- **Core assumption**: There exist specific attention layers where the injection will be most effective, and the magnitude of injection needs to be tuned.
- **Evidence anchors**:
  - [abstract] "We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as 'memories,' at critical LLM locations during inference"
  - [section 4.2] "To inject a memory at the attention layer of layer ℓ, add the embedded memory into the outputs of the attention heads during the inference pass"
- **Break condition**: If the injection magnitude is too high (causing interference) or too low (insufficient correction), or if the injection is applied to the wrong attention layer.

## Foundational Learning

- **Concept**: Transformer attention mechanism
  - **Why needed here**: Understanding how attention heads retrieve information is crucial for identifying why multi-hop reasoning fails and how memory injections can correct it
  - **Quick check question**: What is the role of the softmax operation in the attention calculation, and how does it affect which tokens receive the most weight?

- **Concept**: Residual connections and layer stacking
  - **Why needed here**: The memory injection technique relies on adding information to the residual stream at specific layers, so understanding how residual connections work is essential
  - **Quick check question**: How does the residual connection formula Rℓ+1 = Rℓ + rℓ+1 affect the propagation of injected memories through subsequent layers?

- **Concept**: Token embedding and unembedding
  - **Why needed here**: The memory injection technique converts memories from vocabulary space to embedding space and back, requiring understanding of this transformation
  - **Quick check question**: How does the embedding matrix WE map tokens to vectors, and how does its transpose WUT perform the reverse operation?

## Architecture Onboarding

- **Component map**: Embedding layer -> L residual blocks (each containing MHSA and MLP) -> Unembedding layer
- **Critical path**: For a token at position i: embedding → residual block 1 → ... → residual block L → unembedding. Memory injection intervenes at attention head outputs within this path.
- **Design tradeoffs**: Trades computational overhead during inference against model retraining needs; requires identifying optimal injection locations and magnitudes
- **Failure signatures**: If injections don't improve performance, possible causes include wrong injection layer, incorrect magnitude, irrelevant memory, or failure due to factors other than memory retrieval
- **First 3 experiments**:
  1. Verify attention heads behave differently for single-hop vs multi-hop prompts using the unembedding matrix technique
  2. Test memory injections with τ=1 at various layers to find optimal layer for specific prompt type
  3. Compare curated vs random memory injections at optimal (layer, magnitude) pair to validate technique's effectiveness

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several remain unanswered based on the analysis:
- How do memory injections perform on other transformer-based models beyond GPT-2?
- How do memory injections affect model performance on other types of reasoning tasks beyond multi-hop reasoning?
- How do memory injections impact the model's ability to generalize to unseen prompts or prompts from different domains?
- How do memory injections affect the model's computational efficiency and memory usage?

## Limitations
- Relies on post-hoc analysis rather than formal guarantees about memory retrieval mechanisms
- Assumes needed information exists in model weights but wasn't retrieved, without systematic verification
- Effectiveness depends heavily on specific (layer, magnitude) pair, suggesting limited generalizability

## Confidence
- **High Confidence**: Attention heads are responsible for memory retrieval (strong empirical evidence with 424% increase)
- **Medium Confidence**: Multi-hop failures primarily stem from inability to resolve implicit subjects to explicit ones (plausible but not definitively proven)
- **Low Confidence**: Memory injections represent a generally applicable solution for multi-hop reasoning failures (demonstrated success on specific prompt types only)

## Next Checks
1. **Cross-architecture validation**: Test memory injections on models with different attention mechanisms (e.g., sparse attention, linear attention) to verify whether the technique depends on specific architectural features of standard transformer attention.

2. **Memory presence verification**: Systematically probe whether the required memories for multi-hop prompts are actually encoded in the model's weights before injection, to distinguish between retrieval failures and knowledge gaps.

3. **Generalization testing**: Evaluate the memory injection technique on prompts requiring more than two reasoning hops and on tasks requiring integration of multiple distinct knowledge pieces, to assess whether the approach scales beyond the demonstrated simple cases.