---
ver: rpa2
title: Sample Size in Natural Language Processing within Healthcare Research
arxiv_id: '2309.02237'
source_url: https://arxiv.org/abs/2309.02237
tags:
- sample
- page
- size
- data
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a simulation study to determine appropriate
  sample sizes for training NLP classifiers in healthcare research. The authors trained
  models on MIMIC-III data using varying sample sizes (200-5000), class proportions
  (50/50 to 90/10), and nine different classifiers to classify medical records by
  diagnosis codes.
---

# Sample Size in Natural Language Processing within Healthcare Research

## Quick Facts
- arXiv ID: 2309.02237
- Source URL: https://arxiv.org/abs/2309.02237
- Reference count: 0
- Primary result: Simulation study provides guidelines for selecting sample sizes and class proportions for NLP classifiers in healthcare based on classifier performance

## Executive Summary
This paper presents a simulation study to determine appropriate sample sizes for training NLP classifiers in healthcare research. The authors trained models on MIMIC-III data using varying sample sizes (200-5000), class proportions (50/50 to 90/10), and nine different classifiers to classify medical records by diagnosis codes. They found that larger sample sizes (>1000) generally produced better performance across classifiers, with specific algorithms performing best under certain conditions: KNN excelled with smaller samples and imbalanced classes, while SVM, LinearSVC, and BERT performed better with larger samples. The study provides guidelines for selecting sample sizes and class proportions based on expected classifier performance, with the methodology being generalizable to other datasets.

## Method Summary
The authors extracted clinical notes from MIMIC-III database for patients with ICD-9 codes 4019 (Unspecified Essential Hypertension) and 25000 (Diabetes Mellitus without complication). They preprocessed text data using NLTK (lowercase, stopword removal, lemmatization, tokenization) and split into train/test/validation sets (60/20/20). Nine classifiers were trained and evaluated: Logistic Regression, Naive Bayes, Random Forest, Decision Tree, SVC, LinearSVC, SGD, KNN, and BERT. The simulation varied sample sizes (200-5000) and class proportions (50/50 to 90/10), measuring performance using F1-score and AUC-ROC metrics.

## Key Results
- Sample sizes >1000 generally produced better performance across all classifiers
- KNN performed better with smaller samples (800 and below) and imbalanced class proportions
- BERT showed best performance with imbalanced classes due to transfer learning benefits
- Classifiers with larger sample sizes showed better performance in general
- The study provides guidelines for selecting sample sizes based on classifier type and expected class proportions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger sample sizes (>1000) generally produce better performance across classifiers.
- Mechanism: Increasing training data reduces overfitting, stabilizes model parameters, and improves generalization.
- Core assumption: The underlying data distribution is stable and additional samples represent the same population.
- Evidence anchors:
  - [abstract] "Overall, a sample size larger than 1000 was sufficient to provide decent performance metrics."
  - [section] "As expected, classifiers built with larger sample sizes showed better performance in general."
- Break condition: If data contains high variance or class imbalance not addressed by sampling, additional data may not help.

### Mechanism 2
- Claim: KNN performs better with smaller sample sizes and imbalanced class proportions.
- Mechanism: KNN's distance-based decision boundaries are less affected by high-dimensional sparsity in smaller datasets; it can leverage local density in imbalanced settings.
- Core assumption: Local neighborhood structure remains informative even when global sample size is small.
- Evidence anchors:
  - [abstract] "Smaller sample sizes resulted in better results when using a K-nearest neighbours classifier."
  - [section] "Smaller samples (800 and below) resulted in better performance by the KNN classifier and more frequently with imbalanced class proportions such as 90/10, 80/20 and 70/30."
- Break condition: If the dataset becomes too sparse or the nearest neighbors are no longer representative of the class structure.

### Mechanism 3
- Claim: BERT performs best with imbalanced classes due to transfer learning.
- Mechanism: Pre-trained embeddings encode general language knowledge, allowing BERT to generalize from limited examples even when class proportions are skewed.
- Core assumption: Transfer learning provides robust representations that are less dependent on class balance in the target task.
- Evidence anchors:
  - [abstract] "Imbalanced classes performed best most frequently with the BERT model."
  - [section] "This could be because the model utilises transfer learning and being pre-trained on a large corpus of language, its architecture produces pre-trained context-dependent embeddings which encode aspects of general language."
- Break condition: If the domain vocabulary is highly specialized and not well-covered by the pre-training corpus.

## Foundational Learning

- Concept: Sample size calculation principles in statistical studies.
  - Why needed here: Provides baseline understanding of why sample size matters for precision and representativeness.
  - Quick check question: Why does increasing sample size generally improve model performance?

- Concept: Supervised machine learning workflow (train/validation/test split).
  - Why needed here: The paper uses 60/20/20 splits; understanding this helps interpret results correctly.
  - Quick check question: What is the purpose of the validation set in model training?

- Concept: Class imbalance and its effects on classifier performance.
  - Why needed here: The study varies class proportions (90/10, 80/20, etc.) and observes performance differences.
  - Quick check question: How might a 90/10 class split affect precision and recall metrics?

## Architecture Onboarding

- Component map: SQL extraction -> NLTK preprocessing -> scikit-learn/HuggingFace model training -> F1/AUC evaluation
- Critical path: Extract MIMIC-III data → Preprocess text → Train classifiers with varying sample sizes/proportions → Evaluate and compare
- Design tradeoffs: Smaller samples reduce computation but may hurt generalization; imbalanced classes may bias models but reflect real-world data
- Failure signatures: Low AUC approaching 0.5 indicates random performance; wide confidence intervals suggest unstable estimates
- First 3 experiments:
  1. Replicate HTN classification with 1000-sample, 50/50 split using Logistic Regression
  2. Test KNN on 200-sample, 90/10 split to observe small-sample advantage
  3. Evaluate BERT on 5000-sample, 90/10 split to confirm transfer learning benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific text features (e.g., vocabulary size, document length, semantic similarity between classes) affect the minimum sample size required for effective NLP classification in healthcare?
- Basis in paper: [inferred] The authors mention plans to investigate "if such text features as size of the underlying vocabulary, number of words per document, or similarity of the descriptive words for the positive and negative classes, affect the minimum training sample size required for an NLP model."
- Why unresolved: This question was identified as future work but not addressed in the current study.
- What evidence would resolve it: Empirical results showing classification performance across varying text features and sample sizes from different healthcare datasets.

### Open Question 2
- Question: How transferable are the sample size recommendations across different healthcare domains (e.g., critical care vs. primary care vs. specialty clinics)?
- Basis in paper: [explicit] "Since the results obtained in this paper were from data of a critical care unit based in the United States, there will be differences in the structure and content of the textual data when compared to other sources of health data."
- Why unresolved: The study only used MIMIC-III critical care data; transferability to other healthcare settings remains untested.
- What evidence would resolve it: Replication of the same simulation methodology on multiple healthcare datasets from different specialties and care settings.

### Open Question 3
- Question: How does inter-annotator agreement (IAA) influence the required sample size for reliable NLP classification in healthcare?
- Basis in paper: [explicit] "This work does not account for IAA scores, since we are using the diagnosis code as the label... while this work has focussed on sample sizes, and not addressed the issues of IAA, they are both important factors that might determine the performance of an NLP classifier."
- Why unresolved: The study used diagnosis codes as gold standard labels without considering the variability in human annotation.
- What evidence would resolve it: Comparative studies measuring performance differences when using labels with varying IAA scores across different sample sizes.

## Limitations

- The study uses only two diagnosis codes (HTN and Diabetes) from a single hospital system, limiting generalizability to other clinical conditions or healthcare settings
- Results may not transfer to other NLP tasks like named entity recognition or relation extraction that use different data structures
- Class imbalance experiments used synthetic proportions rather than naturally occurring distributions

## Confidence

- **High confidence**: Larger sample sizes (>1000) improve classifier performance across most algorithms
- **Medium confidence**: KNN performs better with smaller samples and imbalanced classes, as this contradicts typical ML expectations and needs further validation
- **Medium confidence**: BERT benefits from transfer learning in imbalanced settings, though this depends heavily on the pre-training corpus matching the domain vocabulary

## Next Checks

1. Test the sample size recommendations using different diagnosis codes or clinical note types to verify generalizability
2. Evaluate model performance on a multi-site dataset to assess robustness across different documentation styles
3. Conduct experiments with naturally occurring class imbalances rather than synthetic splits to better reflect real-world conditions