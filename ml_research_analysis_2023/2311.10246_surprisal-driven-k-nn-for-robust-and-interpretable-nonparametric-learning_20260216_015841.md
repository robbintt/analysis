---
ver: rpa2
title: Surprisal Driven $k$-NN for Robust and Interpretable Nonparametric Learning
arxiv_id: '2311.10246'
source_url: https://arxiv.org/abs/2311.10246
tags:
- lightgbm
- data
- distance
- mean
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work proposes a robust and interpretable nonparametric learning\
  \ framework based on k-NN, enhanced with information-theoretic concepts. The key\
  \ innovations include a novel \u0141ukaszyk\u2013Karmowski distance metric tailored\
  \ for Laplace distributions to avoid zero distances, Inverse Residual Weighting\
  \ to transform distances into surprisal space, and the introduction of \"conviction\"\
  \ metrics (familiarity, similarity, residual) to quantify data point importance\
  \ and model uncertainty."
---

# Surprisal Driven $k$-NN for Robust and Interpretable Nonparametric Learning

## Quick Facts
- arXiv ID: 2311.10246
- Source URL: https://arxiv.org/abs/2311.10246
- Reference count: 40
- Key outcome: This work proposes a robust and interpretable nonparametric learning framework based on k-NN, enhanced with information-theoretic concepts. The key innovations include a novel Łukaszyk–Karmowski distance metric tailored for Laplace distributions to avoid zero distances, Inverse Residual Weighting to transform distances into surprisal space, and the introduction of "conviction" metrics (familiarity, similarity, residual) to quantify data point importance and model uncertainty. The framework enables interpretable feature contributions and data point influence weights without explicit model training. Experimental results show the method achieves near or above state-of-the-art performance on 90 PMLB classification/regression datasets, outperforming algorithms like LightGBM, KNN, Random Forests, and SVMs. It also demonstrates strong anomaly detection performance across 20 ODDS datasets, achieving the highest F1 scores in 12 out of 20 datasets using a simple threshold on conviction values.

## Executive Summary
This paper introduces a novel nonparametric learning framework that enhances k-Nearest Neighbors with information-theoretic concepts to improve robustness and interpretability. The key innovations include the Łukaszyk–Karmowski distance metric, which avoids zero distances in high dimensions by integrating over Laplace distributions, and Inverse Residual Weighting, which transforms feature differences into surprisal space to make feature scales irrelevant. The framework also introduces conviction metrics—familiarity, similarity, and residual—that quantify data point importance and model uncertainty without requiring explicit model training. Experimental results demonstrate strong performance across classification, regression, and anomaly detection tasks on benchmark datasets.

## Method Summary
The method enhances k-NN by incorporating information-theoretic concepts to improve robustness and interpretability. It uses the Łukaszyk–Karmowski distance metric, which avoids zero distances by integrating over Laplace distributions, and Inverse Residual Weighting (IRW), which transforms feature differences into surprisal space. The framework also introduces conviction metrics—familiarity, similarity, and residual—that quantify data point importance and model uncertainty. The approach achieves strong performance across classification, regression, and anomaly detection tasks on benchmark datasets.

## Key Results
- Achieves near or above state-of-the-art performance on 90 PMLB classification/regression datasets, outperforming algorithms like LightGBM, KNN, Random Forests, and SVMs.
- Demonstrates strong anomaly detection performance across 20 ODDS datasets, achieving the highest F1 scores in 12 out of 20 datasets using a simple threshold on conviction values.
- Introduces interpretable conviction metrics (familiarity, similarity, residual) to quantify data point importance and model uncertainty without requiring explicit model training.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inverse Residual Weighting (IRW) converts raw distances into a common surprisal space, making feature scales irrelevant and improving model interpretability.
- Mechanism: By dividing each feature's residual by its mean absolute deviation, IRW transforms distances into information content measured in bits. This normalization ensures that features with different units or ranges contribute proportionally to the model's decision based on their predictability.
- Core assumption: Residuals follow an exponential distribution (maximum entropy under first moment constraint).
- Evidence anchors:
  - [abstract] "we use a novel formulation of surprisal (amount of information required to explain the difference between the observed and expected result)"
  - [section] "Inverse Residual Weighting (IRW), a maximum entropy method of transforming these each feature difference into surprisal space so that the entire distance itself becomes expected surprisal."
- Break condition: If feature residuals are not representative of uncertainty (e.g., highly non-exponential distributions), the surprisal interpretation fails and IRW weights become misleading.

### Mechanism 2
- Claim: Łukaszyk-Karmowski distance with Laplace distributions prevents zero distances, enabling meaningful comparisons in high dimensions.
- Mechanism: LK distance integrates over probability density functions of feature differences, ensuring non-zero values even when raw differences are zero. The Laplace assumption with local error bounds (from residuals) gives a closed-form expression that smoothly penalizes similarity.
- Core assumption: Local data around each point can be modeled with Laplace distributions sharing a common scale parameter.
- Evidence anchors:
  - [section] "we derived the Minkowski distance as p → 0 expressed over the feature set... If any of the differences are zero, the entire distance metric will become zero. In order to solve this problem, we use the Łukaszyk–Karmowski metric as a distance term rather than absolute error."
  - [appendix] Complete derivation of LK distance with Laplace distributions is provided.
- Break condition: If the true data distribution is not approximately Laplacian or if scale parameters vary wildly, the LK distance may not reflect true dissimilarity.

### Mechanism 3
- Claim: Conviction metrics (familiarity, similarity, residual) quantify model uncertainty and data point importance without requiring retraining.
- Mechanism: Conviction is defined as the ratio of expected surprisal to observed surprisal. This ratio highlights cases where predictions deviate from model expectations, flagging anomalies or uncertain regions.
- Core assumption: The exponential distribution of distances (or residuals) provides a valid probabilistic model for computing expected surprisal.
- Evidence anchors:
  - [abstract] "we introduce the concept of conviction: a ratio of expected surprisal to observed surprisal. This is further broken down into familiarity conviction, similarity conviction, and residual conviction."
  - [section] "conviction, π, can be expressed as π(x) = E[I(X)] / I(x)" and subsequent definitions.
- Break condition: If the exponential distribution assumption fails or if the local neighborhood is not representative, conviction values become unreliable.

## Foundational Learning

- Concept: Information theory basics (entropy, surprisal, Kullback-Leibler divergence)
  - Why needed here: The entire framework is built on information-theoretic measures to replace ad-hoc distance scaling.
  - Quick check question: What is the surprisal of an event with probability 0.01?

- Concept: Maximum entropy distributions (exponential, Laplace)
  - Why needed here: These distributions justify the probabilistic interpretation of residuals and distances used in IRW and LK metrics.
  - Quick check question: Which distribution maximizes entropy given a fixed mean absolute deviation?

- Concept: Distance metrics in high-dimensional spaces and the curse of dimensionality
  - Why needed here: The work explicitly addresses high-dimensional issues by using fractional norms and LK distance.
  - Quick check question: Why do traditional Euclidean distances become less discriminative in high dimensions?

## Architecture Onboarding

- Component map: Data preprocessing -> Residual calculation (leave-one-out) -> Feature weighting (IRW) -> Distance computation (LK) -> Neighbor search (k-NN) -> Prediction (inverse distance weighting) -> Conviction calculation -> Output
- Critical path: Residual calculation -> IRW weighting -> Distance computation -> Neighbor search. Any failure here breaks the entire pipeline.
- Design tradeoffs:
  - Accuracy vs interpretability: The framework sacrifices some raw predictive power for transparent uncertainty quantification.
  - Memory vs scale: k-NN requires all data in memory; sampling strategies are needed for large datasets.
- Failure signatures:
  - Zero or near-zero residuals -> IRW weights explode -> numerical instability.
  - Highly skewed residuals -> exponential assumption invalid -> conviction metrics misleading.
  - Degenerate neighborhoods (k=1) -> conviction values become noisy.
- First 3 experiments:
  1. Verify residual calculation: run leave-one-out on a small dataset and check MAE stability.
  2. Test IRW weighting: compare feature weights before and after IRW on a dataset with mixed scales.
  3. Validate LK distance: compute distances with and without LK on a dataset with duplicate points and confirm non-zero outputs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the method scale to very large datasets (millions of points)?
- Basis in paper: [explicit] "The present methodology, while promising, exhibits certain limitations in terms of the scale of data it can effectively handle... As an extension of the foundational k-Nearest Neighbors framework, this approach necessitates that the data used for model fitting is constrained to the memory capacity of a machine."
- Why unresolved: The paper acknowledges this as a limitation but does not provide experimental data or analysis on performance with large-scale datasets.
- What evidence would resolve it: Experiments demonstrating runtime, memory usage, and accuracy on datasets with millions of points, or algorithmic optimizations that enable handling such scales.

### Open Question 2
- Question: What is the optimal threshold for conviction values in anomaly detection across different domains?
- Basis in paper: [explicit] "In practice we would recommend tuning this threshold per dataset, but here we show that even a naive threshold can outperform commonly used methods."
- Why unresolved: The paper uses a fixed threshold of 0.7 for all datasets without systematic analysis of how to determine optimal thresholds for different data characteristics.
- What evidence would resolve it: Analysis of threshold sensitivity across various datasets, guidelines for threshold selection based on data properties, or adaptive thresholding methods.

### Open Question 3
- Question: How does the method perform against inference-time adversarial attacks?
- Basis in paper: [explicit] "We have also begun to probe the robustness of our method against inference-time adversarial attacks. Early results have shown significant promise across tabular as well as image datasets..."
- Why unresolved: The paper mentions this is part of future work and only provides "early results" without detailed analysis or comparison to other robust methods.
- What evidence would resolve it: Comprehensive adversarial attack experiments comparing robustness to other ML methods, analysis of which conviction metrics are most robust, and defensive strategies.

## Limitations
- **Implementation Complexity**: The Łukaszyk-Karmowski distance derivation and Inverse Residual Weighting scheme are novel and require careful implementation, with the paper lacking detailed pseudocode.
- **Distribution Assumptions**: The framework relies heavily on exponential and Laplace distributions for residuals and distances, which real-world data often violates, particularly in high-dimensional spaces.
- **Computational Scalability**: The additional complexity of LK distance and IRW weighting may create computational bottlenecks for large datasets, with the paper not addressing approximate nearest neighbor methods or optimizations.

## Confidence
- **High Confidence**: The theoretical framework connecting surprisal, maximum entropy distributions, and distance metrics is sound and well-established in information theory.
- **Medium Confidence**: The experimental results show strong performance across benchmarks, but the lack of detailed implementation guidance and hyperparameter tuning procedures creates uncertainty about achieving identical results.
- **Low Confidence**: Claims about interpretability improvements are largely qualitative, lacking systematic user studies or quantitative measures of interpretability gains.

## Next Checks
1. **Distribution Validation**: Apply the method to synthetic datasets with known residual distributions (exponential, Gaussian, heavy-tailed) to quantify performance degradation when core assumptions are violated.
2. **Hyperparameter Sensitivity**: Systematically vary k, the LK distance parameter α, and the conviction threshold to understand their impact on accuracy, interpretability, and computational cost.
3. **Benchmark Completeness**: Replicate the full experimental pipeline on a subset of PMLB datasets with public code to verify the claimed performance improvements and identify any implementation-specific factors.