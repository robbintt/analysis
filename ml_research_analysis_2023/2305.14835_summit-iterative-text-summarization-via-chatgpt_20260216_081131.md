---
ver: rpa2
title: 'SummIt: Iterative Text Summarization via ChatGPT'
arxiv_id: '2305.14835'
source_url: https://arxiv.org/abs/2305.14835
tags:
- summary
- summarization
- arxiv
- summaries
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SummIt, a novel iterative text summarization
  framework leveraging large language models like ChatGPT. Unlike traditional one-shot
  summarization, SummIt allows the model to refine summaries iteratively through self-evaluation
  and feedback, closely mimicking human drafting and revision processes.
---

# SummIt: Iterative Text Summarization via ChatGPT

## Quick Facts
- arXiv ID: 2305.14835
- Source URL: https://arxiv.org/abs/2305.14835
- Authors: Multiple
- Reference count: 12
- Primary result: Iterative refinement via self-evaluation improves summary quality over one-shot generation on CNN/DM, XSum, and NewTS datasets.

## Executive Summary
This paper introduces SummIt, a novel iterative text summarization framework that leverages large language models like ChatGPT to refine summaries through self-evaluation and feedback. Unlike traditional one-shot summarization, SummIt mimics human drafting processes by allowing the model to progressively improve its output across multiple iterations. The framework incorporates knowledge and topic extractors to enhance faithfulness and controllability, demonstrating improved performance across multiple benchmark datasets.

## Method Summary
SummIt employs an iterative refinement process where a summarizer generates initial output, an evaluator provides feedback rationale, and the summarizer revises based on this feedback. The system uses in-context learning with exemplar pairs to guide both summarization and evaluation without fine-tuning. Knowledge extractors (OpenIE triplets) constrain edits for faithfulness, while topic extractors guide relevance. The process continues until a stopping criterion is met, producing refined summaries that outperform one-shot generation baselines.

## Key Results
- Improved ROUGE and GPT-Eval scores compared to one-shot generation on CNN/DM, XSum, and NewTS datasets
- Enhanced faithfulness measured by FactCC scores when using knowledge extractors
- Human evaluation shows successful refinement based on evaluator feedback, though bias toward model's own criteria observed

## Why This Works (Mechanism)

### Mechanism 1: Iterative Self-Evaluation
The model progressively improves summary quality through self-evaluation cycles. Each iteration applies targeted edits based on feedback rationales addressing redundancy, missing information, or unclear text. This assumes the LLM can generate useful self-feedback and act upon it effectively.

### Mechanism 2: In-Context Learning
The framework leverages few-shot prompting with exemplar source-summary pairs to guide both summarizer and evaluator behavior without fine-tuning. This relies on the LLM's strong few-shot capabilities to mimic desired summarization patterns.

### Mechanism 3: Knowledge and Topic Constraints
OpenIE triplets extracted from source documents constrain summarizer edits to maintain factual consistency, while topic extractors ensure summary relevance. This assumes extracted knowledge accurately captures document facts and topic relevance can be quantified.

## Foundational Learning

- **Conditional language modeling**: Autoregressive generation conditioned on input text forms the basis of both summarizer and evaluator. *Quick check*: What is the probability formula for generating the next token given input and previous tokens? *Answer*: p(yt | y<t, x, θ) as shown in Equation (1).

- **In-context learning**: The framework uses exemplar pairs in prompts to guide summarization behavior without fine-tuning. *Quick check*: How does the model receive context in ICL? *Answer*: By concatenating exemplar source-summary pairs and the new document in the prompt.

- **FactCC-style faithfulness evaluation**: This measures whether generated summaries are factually consistent with source documents, particularly important for evaluating iterative refinement quality. *Quick check*: What does FactCC measure in summarization? *Answer*: It evaluates factual consistency between summaries and source documents.

## Architecture Onboarding

- **Component map**: Document → Knowledge Extractor → Triplets → Summarizer → Summary → Evaluator → Feedback rationale → Summarizer (loop) → Stop condition → Output

- **Critical path**: Document → Summarizer → Evaluator → Feedback → Summarizer (loop) → Stop condition → Output

- **Design tradeoffs**: LLM-only approach avoids costly supervised training but depends heavily on prompt quality; OpenIE provides interpretable constraints but may miss implicit relations; single model reduces complexity but may conflate roles.

- **Failure signatures**: Over-correction where evaluator biases toward its own criteria; stalling when evaluator fails to produce STOP; hallucination when extractors miss key facts and summarizer fabricates details.

- **First 3 experiments**:
  1. Ablation on in-context learning: Compare zero-shot vs. few-shot performance on XSum
  2. Knowledge extractor impact: Compare FactCC scores with and without OpenIE guidance
  3. Stopping criteria test: Vary max iterations and measure quality vs. over-correction

## Open Questions the Paper Calls Out

### Open Question 1
What specific mechanisms could reduce the over-correction bias observed in SummIt's iterative refinement? The paper identifies this issue where LLM evaluation criteria may not align with human judgment and suggests human-in-the-loop feedback as a potential solution, but lacks concrete implementation details or empirical evidence of effectiveness.

### Open Question 2
How does SummIt compare to other state-of-the-art models using metrics beyond ROUGE and GPT evaluation scores? The paper's evaluation is limited to specific metrics, leaving uncertainty about performance relative to other models when considering broader assessment criteria.

### Open Question 3
What is the impact of different knowledge extraction methods on SummIt's performance? The paper uses OpenIE but doesn't explore alternative methods or compare their effectiveness on summary faithfulness and quality.

## Limitations
- Lack of ablation studies on critical components like prompt design and stopping criteria
- Potential bias toward model's own evaluation criteria rather than human judgment
- Vulnerability to prompt quality and exemplar selection without supervised training

## Confidence

- **High Confidence**: The basic iterative mechanism is well-defined and reproducible; empirical ROUGE and GPT-Eval improvements are clearly demonstrated
- **Medium Confidence**: Knowledge extractor benefits are supported by FactCC improvements, but impact of different extractor types remains unclear
- **Low Confidence**: Claims about mimicking human drafting processes are largely intuitive with limited direct evidence from human writer studies

## Next Checks

1. **Ablation on stopping criteria**: Systematically vary maximum iteration count and stopping threshold to identify optimal trade-offs between refinement quality and over-correction

2. **Human-in-the-loop refinement**: Conduct user studies where human evaluators provide feedback instead of automated evaluator to test generalization beyond self-evaluation

3. **Prompt sensitivity analysis**: Test multiple prompt variants for both summarizer and evaluator to quantify impact of prompt engineering on final summary quality