---
ver: rpa2
title: A Watermark for Large Language Models
arxiv_id: '2301.10226'
source_url: https://arxiv.org/abs/2301.10226
tags:
- watermark
- text
- tokens
- language
- truncated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a watermarking framework for proprietary language
  models that embeds signals into generated text, making it algorithmically detectable
  without access to model parameters or API. The watermark selects a randomized set
  of "whitelist" tokens before a word is generated and promotes their use during sampling.
---

# A Watermark for Large Language Models

## Quick Facts
- **arXiv ID:** 2301.10226
- **Source URL:** https://arxiv.org/abs/2301.10226
- **Reference count:** 40
- **Primary result:** Proposes a watermarking framework for proprietary language models that embeds detectable signals into generated text using whitelist token promotion and statistical testing.

## Executive Summary
This paper introduces a watermarking framework for proprietary language models that embeds detectable signals into generated text without requiring access to model parameters or API. The method works by selecting a randomized set of "whitelist" tokens before each word is generated and promoting their use during sampling through logit boosting. Detection is performed using a statistical z-test that counts whitelist tokens and compares against expected distributions. The watermark demonstrates robustness against removal attacks due to cascading effects when tokens are modified, and maintains text quality across various sampling strategies.

## Method Summary
The watermarking framework operates by computing a hash of previous tokens to seed a random number generator, which partitions the vocabulary into whitelist and blacklist tokens. During generation, logits for whitelist tokens are boosted by a constant δ before applying softmax, increasing their probability of selection. Detection involves counting whitelist tokens in the text and computing a z-statistic based on the expected count under the null hypothesis that text is human-generated. The method uses statistical hypothesis testing with interpretable p-values and demonstrates robustness through cascading blacklist violations when tokens are modified.

## Key Results
- Watermark detection achieves high statistical significance (z > 4 threshold) with interpretable p-values
- Robust against simple removal attacks due to cascading blacklist effects when tokens are modified
- Minimal quality degradation across multinomial, greedy, and beam search sampling strategies
- Effective without requiring access to model parameters or API when using public mode

## Why This Works (Mechanism)

### Mechanism 1: Whitelist Token Promotion
The watermark works by selecting a randomized set of "whitelist" tokens before each word is generated and softly promoting their use during sampling. Before generating each token, the algorithm computes a hash of previous tokens to seed a random number generator, which partitions the vocabulary into whitelist and blacklist sets. The logits for whitelist tokens are boosted by δ before applying softmax, increasing their selection probability during sampling.

### Mechanism 2: Statistical Detection
The watermark is detectable using a statistical z-test with interpretable p-values without access to model parameters. Detection involves counting whitelist tokens and computing a z-statistic based on the expected count under the null hypothesis of human-generated text. The z-score formula is z = (|s|_w - γT) / √(Tγ(1-γ)), where T is total tokens and |s|_w is whitelist token count.

### Mechanism 3: Attack Resistance Through Cascading Effects
The watermark is robust against removal attacks because modifying tokens has downstream effects on blacklist assignments. When a token is modified, it changes the hash seed for subsequent tokens, potentially causing multiple downstream tokens to be blacklisted. This creates cascading effects where each modified token can cause 2+ additional blacklist violations, making watermark removal expensive.

## Foundational Learning

- **Statistical hypothesis testing and z-scores**: Understanding z-scores and p-values is essential for interpreting detection results and evaluating watermark strength. Quick check: For watermarked text with 160 whitelist tokens out of 200 total tokens (γ=0.5), the z-score would be (160-100)/√(200×0.5×0.5) = 60/√50 ≈ 8.49, well above the z>4 detection threshold.

- **Language model tokenization and sampling**: Understanding how tokens work and how different sampling methods (multinomial, greedy, beam search) affect output is crucial for implementing and testing the watermark. Quick check: Increasing temperature in sampling increases entropy by flattening the probability distribution, making token selection more random and potentially reducing watermark detection strength.

- **Cryptographic hash functions and pseudo-random number generation**: The watermark uses hashes of previous tokens to seed RNGs for whitelist generation. Understanding reproducible but unpredictable sequence generation is key to the watermark's security. Quick check: It's important that the same hash input always produces the same whitelist because detection requires reproducibility; if this property didn't hold, the same text could be detected differently in different runs.

## Architecture Onboarding

- **Component map**: Tokenization layer → Hash generator → Whitelist generator → Logit booster → Softmax layer → Sampler → Detector
- **Critical path**: 1) Tokenization of input text, 2) Hash computation of previous tokens, 3) Whitelist generation via seeded RNG, 4) Logit boosting by δ, 5) Token sampling from modified distribution, 6) Detection by counting whitelist tokens and computing z-score
- **Design tradeoffs**: 
  - γ vs δ: Larger γ requires smaller δ to maintain quality; smaller γ requires larger δ for detection strength
  - Hard vs soft watermark: Hard watermark provides stronger detection but degrades quality on low-entropy text
  - Public vs private mode: Public mode allows open detection but is more vulnerable to attacks
- **Failure signatures**: Low z-scores despite watermarked generation (low entropy text), high false positive rate (γ too low or threshold too lenient), poor text quality with watermark (δ too high)
- **First 3 experiments**: 1) Generate 100 samples with γ=0.5, δ=2.0 using multinomial sampling and compute empirical whitelist fraction vs theoretical bound, 2) Test detection sensitivity by measuring type-II error rate at different z-score thresholds, 3) Implement simple attack by randomly replacing tokens and measure replacements needed to reduce z-score below threshold

## Open Questions the Paper Calls Out

- **Optimal window size h**: The paper discusses trade-offs between window size h and attack amplification but doesn't provide specific experimental results to determine optimal h for different threat models. Resolution would require empirical studies comparing different window sizes across various attack scenarios.

- **Robustness to adversarial prompting**: The paper mentions generative attacks that prompt models to change output language predictably but lacks detailed methods or results on designing resistant watermarking schemes. Resolution would require development and evaluation of watermarking schemes robust to adversarial prompting attacks.

- **Relationship between γ and entropy**: While the paper discusses how detectability depends on text entropy and provides theoretical bounds, it doesn't comprehensively analyze how γ affects the trade-off between detection sensitivity and text quality across different entropy levels. Resolution would require experimental studies examining γ's impact across various entropy levels and text generation methods.

## Limitations

- Implementation dependency on specific hash function and random number generator choices affects security and reproducibility
- Quality degradation assessment relies on perplexity comparisons without clearly defined acceptable thresholds
- Evaluation focuses primarily on simple token substitution attacks, not comprehensive attack landscape including beam search optimization or adversarial prompting

## Confidence

**High Confidence**: Statistical detection mechanism using z-scores is mathematically sound; soft blacklist approach preserves quality better than hard watermarking; watermark detectable without model access in public mode

**Medium Confidence**: Robustness against simple removal attacks due to cascading effects; method generalizes across sampling strategies; minimal quality impact across diverse domains

**Low Confidence**: Resistance to sophisticated attacks like beam search optimization; reliable performance across all text types and model architectures; acceptable false positive rates in real-world deployment

## Next Validation Checks

1. **Implement beam search-based watermark removal attacks**: Test watermark resistance by implementing beam search optimization that explicitly searches for high-quality token sequences minimizing whitelist token usage, then measure quality degradation required for removal compared to simple substitution attacks.

2. **Cross-domain quality assessment**: Generate watermarked text across diverse domains (technical, creative, conversational) using multiple OPT model sizes, then compare perplexity and human evaluation scores between watermarked and non-watermarked text to identify domains where watermarking causes most severe quality degradation.

3. **Parameter sensitivity analysis**: Systematically vary γ and δ parameters across a wider range than tested, measuring detection accuracy, false positive rate, and quality degradation for each configuration to identify optimal parameter settings for different use cases and identify failure modes.