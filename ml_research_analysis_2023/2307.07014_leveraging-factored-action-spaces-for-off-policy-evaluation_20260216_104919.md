---
ver: rpa2
title: Leveraging Factored Action Spaces for Off-Policy Evaluation
arxiv_id: '2307.07014'
source_url: https://arxiv.org/abs/2307.07014
tags:
- state
- policy
- right
- action
- down
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of high variance and bias in
  off-policy evaluation (OPE) when dealing with large combinatorial action spaces.
  The proposed method leverages factored action spaces by decomposing each action
  into independent sub-actions from smaller spaces, enabling finer-grained analysis
  of action effects.
---

# Leveraging Factored Action Spaces for Off-Policy Evaluation

## Quick Facts
- **arXiv ID**: 2307.07014
- **Source URL**: https://arxiv.org/abs/2307.07014
- **Reference count**: 40
- **Key outcome**: Decomposed importance sampling estimators provably reduce variance while maintaining zero bias in off-policy evaluation for large combinatorial action spaces.

## Executive Summary
This paper addresses the high variance problem in off-policy evaluation (OPE) when dealing with large combinatorial action spaces. The authors propose a novel approach that leverages factored action spaces by decomposing each action into independent sub-actions from smaller spaces. This decomposition enables finer-grained analysis of action effects and leads to a new family of "decomposed" importance sampling estimators. The key insight is that by analyzing actions at a more granular level, the overlap between behavior and evaluation policies improves in each sub-action space, reducing variance while preserving zero bias under specific structural assumptions.

## Method Summary
The method involves factoring the action space A = ⊗D d=1 Ad into D independent sub-action spaces, along with corresponding factored policies π = ∏D d=1 πd. The Q-function is additively decomposed into D factors Q = ΣD d=1 Qd, and importance sampling weights are also factored according to the policy decompositions. This allows the construction of decomposed IS estimators (DecIS, DecPDIS, DecPDWIS) that maintain the same zero bias property as their non-decomposed counterparts while provably having lower variance under certain structural assumptions. The approach is evaluated through simulations on two synthetic MDPs with varying numbers of trajectories, trajectory lengths, discount factors, and policy divergences.

## Key Results
- Decomposed IS estimators provably have lower variance than non-decomposed counterparts while maintaining zero bias under Theorem 1 conditions
- Experiments show decomposed estimators achieve 10-30% lower variance and similar bias compared to baseline OPE methods
- Effective sample size (ESS) is improved by decomposed estimators, indicating better data sample efficiency
- Results hold across varying trajectory lengths, discount factors, and policy divergences in synthetic experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposed estimators achieve lower variance by ensuring better overlap between behavior and evaluation policies in each sub-action space
- Mechanism: The action space is factored into smaller independent sub-action spaces. In each sub-space, the probability distributions of the behavior and evaluation policies overlap more than in the full action space. This increased overlap reduces the importance sampling weights' variance.
- Core assumption: Factored action spaces exist such that the MDP and policies satisfy the conditions in Theorem 1, and the rewards and IS ratios in different factored action spaces are uncorrelated (Assumption 3)
- Evidence anchors:
  - [abstract] "Given certain assumptions on the underlying problem structure, we prove that the decomposed IS estimators have less variance than their original non-decomposed versions"
  - [section] "Theorem 3. The decomposed IS and PDIS estimators are guaranteed to have at most the variance of their respective non-decomposed equivalent estimators i.e. Vπb[ ˆQDecISπe ] ≤ Vπb[ ˆQISπe ] and Vπb[ ˆQDecPDISπe ] ≤ Vπb[ ˆQP DISπe ], provided that Theorem 1 and the conditions in Assumption 3 hold."
- Break condition: If Assumption 3 is violated, there is no guarantee on the variance of the decomposed estimators, as interactions between rd_t and ρd across t and d can lead to higher variance.

### Mechanism 2
- Claim: Decomposed estimators maintain zero bias under specific structural conditions
- Mechanism: The Q-function can be additively decomposed into factors corresponding to each sub-action space (Equation 2). The importance sampling weights are also factored according to the policy decompositions (Equation 5). This allows unbiased estimation of each factor, and their sum gives an unbiased estimate of the overall Q-function.
- Core assumption: The MDP and policies satisfy the conditions in Theorem 1, ensuring the validity of the additive decomposition of the Q-function and the factorization of policies
- Evidence anchors:
  - [abstract] "Given certain assumptions on the underlying problem structure, we prove that the decomposed IS estimators have less variance than their original non-decomposed versions, while preserving the property of zero bias."
  - [section] "Theorem 2. When the assumptions in Theorem 1 hold, the decomposed IS estimator ˆQDecISπe and decomposed PDIS estimator ˆQDecPDISπe are unbiased estimators of the true Q-function Qπe."
- Break condition: If the conditions in Theorem 1 are not satisfied, the additive decomposition of the Q-function may not hold, leading to biased estimates.

### Mechanism 3
- Claim: Decomposed estimators improve data sample efficiency as measured by effective sample size (ESS)
- Mechanism: ESS is inversely proportional to the variance of the estimator. Since decomposed estimators have lower variance than their non-decomposed counterparts, they have higher ESS, meaning they use available data samples more efficiently
- Core assumption: The variance reduction from decomposition is significant enough to outweigh any potential computational overhead
- Evidence anchors:
  - [abstract] "decomposed estimators have lower variance and similar bias to existing OPE baselines, while having better data sample efficiency, as measured by effective sample size (ESS)."
  - [section] "An important result of Theorem 3 relates to the effective sample size (ESS) of an IS (or PDIS or PDWIS) estimator ˆQISπe, which is a measure of data sample efficiency: ESS [ ˆQISπe ] = N × Vπe[ ˆQ on policyπe ] / Vπb[ ˆQISπe ]"
- Break condition: If the computational overhead of decomposition is significant, the practical benefits of increased ESS may be offset.

## Foundational Learning

- Concept: Importance Sampling (IS) in Off-Policy Evaluation
  - Why needed here: IS is the core technique used to correct for the difference between the behavior policy (which generated the data) and the evaluation policy (which we want to evaluate). The paper builds on this by introducing decomposed IS estimators.
  - Quick check question: What is the key challenge in using IS for OPE when dealing with large action spaces, and how does the paper propose to address it?

- Concept: Factored Action Spaces in MDPs
  - Why needed here: The paper leverages the idea that action spaces can be decomposed into smaller, independent sub-action spaces. This allows for a more granular analysis of action effects and improves the efficiency of OPE.
  - Quick check question: What are the sufficient conditions (from Theorem 1) for a Q-function to be additively decomposed based on factored action spaces?

- Concept: Variance and Bias in Estimators
  - Why needed here: Understanding how variance and bias affect the quality of OPE estimators is crucial. The paper aims to reduce variance without increasing bias, which is a key contribution.
  - Quick check question: How do the decomposed IS estimators achieve lower variance while maintaining zero bias, according to the theoretical results?

## Architecture Onboarding

- Component map: MDP with factored action space (S, A = ⊗D d=1 Ad, p, r, d1, γ, T) -> Factored policies (π = ∏D d=1 πd) -> Decomposed IS estimators (DecIS, DecPDIS, DecPDWIS) -> Non-decomposed baselines (IS, PDIS, PDWIS) -> Evaluation metrics (bias, variance, MSE, ESS)

- Critical path:
  1. Factor the action space into independent sub-action spaces
  2. Verify that the MDP and policies satisfy the conditions in Theorem 1
  3. Implement the decomposed IS estimators using the factored action spaces and policies
  4. Compare the performance of the decomposed estimators against non-decomposed baselines using the evaluation metrics

- Design tradeoffs:
  - Decomposition vs. computational overhead: Decomposing the action space and policies may introduce additional computational complexity
  - Factorization quality vs. estimator performance: The effectiveness of the decomposed estimators depends on how well the action space can be factored and whether the conditions in Theorem 1 are satisfied
  - Bias-variance tradeoff: While the decomposed estimators aim to reduce variance without increasing bias, there may be cases where the factorization assumptions are not perfectly met, leading to some bias

- Failure signatures:
  - High variance in decomposed estimators: This could indicate that the factorization assumptions are not well-satisfied, or that the sub-action spaces are not sufficiently independent
  - Non-zero bias in decomposed estimators: This suggests that the conditions in Theorem 1 are not fully met, and the additive decomposition of the Q-function may not hold
  - Poor performance compared to non-decomposed baselines: This could be due to an inappropriate factorization of the action space or policies, or the presence of strong interactions between sub-action spaces that are not captured by the decomposition

- First 3 experiments:
  1. Implement the decomposed IS estimators and compare their variance and bias against non-decomposed baselines on a simple MDP with a known factorization
  2. Investigate the impact of violating the conditions in Theorem 1 on the performance of the decomposed estimators by introducing dependencies between sub-action spaces
  3. Evaluate the data sample efficiency of the decomposed estimators by measuring the effective sample size (ESS) and comparing it to non-decomposed baselines on a range of MDPs with varying degrees of action space factorization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we derive the state abstraction, policy factorisation, and reward factorisation for general OPE problems using only offline data?
- Basis in paper: [explicit] The paper states that future work should investigate "using offline data to design the state abstraction, policy and reward factorisations based on action factorisations for general and/or specific OPE problems."
- Why unresolved: The paper acknowledges this as an important avenue of future research but does not provide a methodology for automatically deriving these factorisations from offline data.
- What evidence would resolve it: A method that can automatically derive the state abstraction, policy factorisation, and reward factorisation from a given offline dataset, and demonstrate its effectiveness on multiple OPE problems.

### Open Question 2
- Question: Under what conditions does the variance of decomposed estimators scale more slowly than non-decomposed estimators with increasing trajectory length or policy mismatch?
- Basis in paper: [explicit] The paper proves that decomposed estimators have at most the variance of non-decomposed ones, but notes there's no guarantee they always scale slower, especially when assumptions don't hold.
- Why unresolved: The theoretical analysis shows the decomposed estimator can have lower variance, but doesn't characterize when this leads to better scaling behavior with problem parameters.
- What evidence would resolve it: A theoretical analysis or empirical study characterizing the relationship between variance scaling rates and problem parameters (e.g., trajectory length, policy mismatch) for both decomposed and non-decomposed estimators.

### Open Question 3
- Question: How does the performance of decomposed estimators compare to other methods for handling large action spaces in OPE, such as action pruning or clustering?
- Basis in paper: [inferred] The paper focuses on decomposing action spaces but doesn't compare to other approaches for handling large action spaces in OPE.
- Why unresolved: While the paper shows decomposed estimators can improve OPE, it doesn't provide a comparison to alternative methods that also aim to address the large action space challenge.
- What evidence would resolve it: An empirical comparison of decomposed estimators to other action space reduction techniques (e.g., action pruning, clustering) on a set of OPE problems with large action spaces.

## Limitations

- Theoretical guarantees depend on strict structural assumptions (Theorem 1, Assumption 3) that may not hold in practice for complex real-world MDPs
- Empirical evaluation limited to synthetic MDPs with known factorizations, leaving open questions about performance on real-world problems
- No comparison to alternative methods for handling large action spaces in OPE, such as action pruning or clustering

## Confidence

- **High Confidence**: Variance reduction claims (supported by Theorem 3 and empirical results across multiple experiments)
- **Medium Confidence**: Zero bias guarantees (theoretical but dependent on strict structural assumptions that may not hold in practice)
- **Medium Confidence**: Sample efficiency improvements (empirically demonstrated but only on synthetic domains)

## Next Checks

1. Test decomposed estimators on MDPs where Assumption 3 is intentionally violated to quantify performance degradation
2. Evaluate performance on a real-world problem with naturally factored action spaces (e.g., recommender systems with categorical features)
3. Compare computational overhead of decomposition against variance reduction benefits across varying problem sizes