---
ver: rpa2
title: 'Unifying the Perspectives of NLP and Software Engineering: A Survey on Language
  Models for Code'
arxiv_id: '2311.07989'
source_url: https://arxiv.org/abs/2311.07989
tags:
- code
- pages
- language
- conference
- software
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically reviews the recent advancements in code
  processing with language models, covering over 70 models, 40 evaluation tasks, and
  180 datasets. It breaks down code language models into general models (e.g., GPT
  family) and specialized models pretrained on code with tailored objectives.
---

# Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code

## Quick Facts
- **arXiv ID**: 2311.07989
- **Source URL**: https://arxiv.org/abs/2311.07989
- **Reference count**: 40
- **Key outcome**: Comprehensive survey of 70+ code language models, 40+ evaluation tasks, and 180+ datasets, integrating NLP and SE perspectives

## Executive Summary
This survey systematically reviews recent advancements in code processing with language models, covering over 70 models, 40 evaluation tasks, and 180 datasets. It categorizes code language models into general models (e.g., GPT family) and specialized models pretrained on code with tailored objectives. The paper highlights the transition from statistical models and RNNs to pretrained Transformers and LLMs, paralleling NLP's evolution. It discusses code-specific features like AST, CFG, and unit tests, and identifies key challenges such as comprehensive benchmarks, high-quality data acquisition, and integrating code features into language models.

## Method Summary
The survey provides a comprehensive overview of code language models through systematic review of recent literature, categorizing models by architecture (encoder-only, encoder-decoder, decoder-only, UniLM, diffusion) and training objectives. It analyzes evaluation tasks and benchmarks including HumanEval, MBPP, and CodeXGLUE, while identifying open challenges in the field. The authors integrate software engineering perspectives with natural language processing techniques, examining how advanced NLP methods like instruction tuning and reinforcement learning are applied to code processing.

## Key Results
- Code language models benefit from pretraining objectives that leverage structured code representations (AST, CFG, IR) providing additional semantic signals
- Decoder-only models with fill-in-the-middle (FIM) objectives outperform CLM-only models on code completion tasks by better matching real coding scenarios
- Instruction tuning and reinforcement learning significantly improve practical task performance through alignment with human intent and compiler feedback

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Code language models benefit from pretraining objectives that leverage the structured nature of code (AST, CFG, IR).
- **Mechanism**: These structured representations provide additional semantic signals beyond raw tokens, allowing models to learn deeper code understanding.
- **Core assumption**: The structured representations accurately capture semantic information that correlates with downstream task performance.
- **Evidence anchors**:
  - [abstract]: "We also discuss code-specific features such as AST, CFG, and unit tests, along with their application in training code language models"
  - [section]: "Unlike natural languages, the nature of code makes it easy to extract auxiliary information from alternative views, and to utilize interpreter and unit tests for automatic feedback."
- **Break Condition**: If the structured representations don't capture meaningful semantic information or introduce noise that degrades model performance.

### Mechanism 2
- **Claim**: Instruction tuning and reinforcement learning significantly improve code model performance on practical tasks.
- **Mechanism**: These methods align models with human intent and provide feedback through compilation/execution results.
- **Core assumption**: The instruction data and reward signals are representative of real-world usage patterns and task requirements.
- **Evidence anchors**:
  - [abstract]: "We also discuss the latest techniques adapted from NLP, such as instruction tuning, reinforcement learning"
  - [section]: "CodeRL (Le et al., 2022) is one such model, which defines four levels of rewards for each generated program (viz. compile error, runtime error, unit test failure, pass) as well as fine-grained token-level reward estimated by a critic model."
- **Break Condition**: If the instruction data becomes stale or the reward signals don't generalize to unseen code patterns.

### Mechanism 3
- **Claim**: Decoder-only models with fill-in-the-middle (FIM) objectives outperform CLM-only models on code completion tasks.
- **Mechanism**: FIM allows models to fill gaps in code with context on both sides, better matching real coding scenarios.
- **Core assumption**: Real coding tasks often require completing partial code with available surrounding context.
- **Evidence anchors**:
  - [abstract]: "We break down code processing models into general language models represented by the GPT family and specialized models that are specifically pretrained on code, often with tailored objectives."
  - [section]: "Incoder (Fried et al., 2023), SantaCoder (Allal et al., 2023), and StarCoder (Li et al., 2023) are all trained with fill-in-the-middle (FIM) objective... These objectives also lead to higher performance on downstream tasks when compared with CLM-only models such as CodeGen."
- **Break Condition**: If the bidirectional context doesn't provide meaningful information or if the task distribution shifts away from completion scenarios.

## Foundational Learning

- **Concept: Abstract Syntax Tree (AST)**
  - Why needed here: AST provides a structured representation of code that captures syntax and relationships between code elements.
  - Quick check question: How does an AST differ from the raw source code representation?

- **Concept: Control Flow Graph (CFG)**
  - Why needed here: CFG represents the possible paths of execution through a program, capturing runtime behavior.
  - Quick check question: What information does a CFG provide that a simple token sequence cannot?

- **Concept: Intermediate Representation (IR)**
  - Why needed here: IR is a compiler-generated representation that abstracts away language-specific details, useful for cross-language tasks.
  - Quick check question: How can IR help with code translation between different programming languages?

## Architecture Onboarding

- **Component map**: Pretraining modules (CLM, MLM, FIM) → Structured feature extractors (AST, CFG, IR) → Instruction tuning components → Reinforcement learning modules with compiler feedback
- **Critical path**: Pretraining → Feature extraction → Downstream task finetuning → Evaluation on benchmarks like HumanEval
- **Design tradeoffs**: Balancing model size vs. pretraining data, structured features vs. raw code, instruction diversity vs. task specificity
- **Failure signatures**: Poor performance on HumanEval indicates issues with pretraining or instruction tuning; high variance in compilation results suggests reward signal problems
- **First 3 experiments**:
  1. Train a small decoder-only model on code with CLM objective and evaluate on HumanEval.
  2. Add AST feature extraction and pretrain with both CLM and AST edge prediction.
  3. Apply instruction tuning using synthetically generated examples and compare performance on practical coding tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively integrate control flow graphs (CFGs) and data flow graphs (DFGs) into large language models for code without severely limiting cross-task generalization and scalability?
- Basis in paper: [explicit] The paper identifies this as a key challenge, noting that current approaches like GraphCodeBERT modify attention masks for code and variable segments, which "severely limits their cross-task generalization and scaling ability."
- Why unresolved: CFGs and DFGs contain semantic information not captured by static representations like ASTs, but their integration requires architectural changes that conflict with the generalizability of pre-trained models.
- What evidence would resolve it: A successful approach would demonstrate improved performance on downstream tasks (e.g., code summarization, defect detection) using CFG/DFG information while maintaining comparable or better performance on general NLP tasks compared to models without CFG/DFG integration.

### Open Question 2
- Question: What are the fundamental limitations of current evaluation benchmarks like HumanEval and MBPP, and what characteristics should a comprehensive benchmark have to reflect production-level software development requirements?
- Basis in paper: [explicit] The paper states that "HumanEval...does not exactly reflect real-world behaviors" and "the community is eager for a new standard benchmark after HumanEval to further boost the progress of Code LLMs to the next stage."
- Why unresolved: Current benchmarks focus primarily on program synthesis with limited unit tests, neglecting other critical aspects of software development like requirement engineering, testing, deployment, and operations.
- What evidence would resolve it: A comprehensive benchmark would include diverse software engineering tasks, realistic test cases with edge cases, repository-level evaluation, and metrics that capture practical utility beyond pass@1 scores.

### Open Question 3
- Question: What are the long-term consequences of training large language models on synthetic data generated by other AI models, and how can we mitigate potential risks like data contamination and model collapse?
- Basis in paper: [explicit] The paper mentions that "synthetic training data becomes widespread" and researchers "should proceed with caution about such practice, as the consequence of training AI models with AI generated data is yet to be investigated at scale."
- Why unresolved: The paper notes this is an emerging trend with limited research on long-term effects, while the potential for data contamination, hallucinations, and reduced model diversity is significant.
- What evidence would resolve it: Long-term studies comparing models trained on synthetic vs. human-generated data would show differences in performance, robustness, and potential degradation over multiple generations of synthetic data.

## Limitations

- The survey's comprehensiveness is constrained by the rapidly evolving nature of code language models, with new models emerging monthly
- Integration of structured code features (AST, CFG, IR) shows promise but lacks systematic ablation studies across diverse model architectures
- The evaluation landscape remains fragmented with inconsistent benchmarks, making cross-model comparisons challenging

## Confidence

- **High Confidence**: Overall categorization of code language models into general vs. specialized families; decoder-only models with FIM objectives outperform CLM-only models for code completion
- **Medium Confidence**: Effectiveness of instruction tuning and reinforcement learning with compiler feedback; practical benefits of structured code features (AST, CFG, IR)
- **Low Confidence**: Specific performance claims for individual models without standardized benchmarks; generalizability of current techniques to emerging programming paradigms

## Next Checks

1. Conduct systematic ablation studies comparing models with and without structured feature extraction (AST, CFG, IR) across multiple model architectures and programming languages to quantify their contribution.
2. Implement standardized evaluation protocols using consistent benchmarks (HumanEval, MBPP, CodeXGLUE) to enable fair cross-model performance comparisons, particularly for instruction-tuned and RL-finetuned models.
3. Analyze the distribution of training data across programming languages and paradigms to assess potential biases and limitations in current code language models' generalization capabilities.