---
ver: rpa2
title: Bi-level Contrastive Learning for Knowledge-Enhanced Molecule Representations
arxiv_id: '2306.01631'
source_url: https://arxiv.org/abs/2306.01631
tags:
- graph
- knowledge
- molecular
- prediction
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of molecular representation learning
  for property prediction tasks, proposing a novel approach called GODE that integrates
  graph representations of individual molecular structures with multi-domain information
  from biomedical knowledge graphs (KGs). GODE pre-trains two graph neural networks
  (GNNs) on different graph structures - one on molecular graphs and another on KG
  substructures - and employs contrastive learning to fuse molecular structures with
  their corresponding KG substructures.
---

# Bi-level Contrastive Learning for Knowledge-Enhanced Molecule Representations

## Quick Facts
- arXiv ID: 2306.01631
- Source URL: https://arxiv.org/abs/2306.01631
- Reference count: 40
- The proposed GODE framework significantly outperforms existing benchmarks, achieving an average ROC-AUC improvement of 12.7% for classification tasks and an average RMSE/MAE improvement of 34.4% for regression tasks.

## Executive Summary
This paper introduces GODE, a novel approach for molecular representation learning that leverages both molecular graphs and biomedical knowledge graphs (KGs). GODE pre-trains two separate graph neural networks (GNNs) - one on molecular structures and another on KG substructures - and uses contrastive learning to fuse their representations. The method significantly improves molecular property prediction performance across 11 chemical property tasks, demonstrating the value of integrating chemical and biological information. GODE achieves state-of-the-art results, surpassing the current leading model with improvements of 2.2% in classification and 7.2% in regression tasks.

## Method Summary
GODE employs a bi-level self-supervised pre-training framework where two GNNs are trained independently on molecular graphs and KG substructures. The molecular GNN (M-GNN) learns from atom/bond structures using masking and motif prediction tasks, while the KG GNN (K-GNN) processes 3-hop sub-graphs with node/edge masking and molecule classification. Contrastive learning aligns these embeddings in a shared latent space, and the combined model is fine-tuned on property prediction tasks. The approach leverages 11 million molecules from ZINC15 and ChEMBL, plus biomedical triples from PubChemRDF, UMLS, and PrimeKG.

## Key Results
- Average ROC-AUC improvement of 12.7% for classification tasks compared to existing benchmarks
- Average RMSE/MAE improvement of 34.4% for regression tasks
- Surpasses current leading model in property prediction with 2.2% classification and 7.2% regression improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training two separate GNNs on molecular graphs and knowledge graphs, then fusing them with contrastive learning, yields more robust molecular representations than using either source alone.
- Mechanism: The molecular GNN learns fine-grained structural features of atoms and bonds, while the KG GNN learns relational semantics across biochemical entities. Contrastive learning aligns embeddings of the same molecule across these views, encouraging mutual reinforcement.
- Core assumption: Molecule-level and KG-level representations capture complementary information and can be meaningfully aligned in a shared latent space.
- Evidence anchors:
  - [abstract]: "GODE integrates individual molecular graph representations with multi-domain biochemical data from knowledge graphs... employing contrastive learning to fuse molecular structures with their corresponding knowledge graph substructures."
  - [section]: "We use a contrastive learning approach to learn the joint embedding between molecule-level and KG-level embeddings... encourages the embeddings to be close to the same molecule and far apart for different molecules."
- Break condition: If the alignment space is misaligned or the contrastive loss is poorly scaled, the joint embedding may collapse or lose modality-specific nuance.

### Mechanism 2
- Claim: Pre-training with self-supervised tasks (masking, motif prediction, edge/node prediction) enables the GNNs to learn rich priors without labeled data.
- Mechanism: Self-supervision forces the GNN to infer missing attributes or detect structural patterns, building generalizable feature extractors that improve downstream fine-tuning efficiency.
- Core assumption: The chosen self-supervised objectives are semantically relevant to the downstream property prediction tasks.
- Evidence anchors:
  - [section]: "We propose a bi-level self-supervised pre-training framework... that utilizes both molecular graph and knowledge graph to pre-train two GNNs."
  - [section]: "We use node and edge masking prediction tasks for KG-level pre-training... This loss function encourages the GNN to predict the correct values of the masked attribute and edge label."
- Break condition: If self-supervision tasks are too easy or irrelevant, the learned representations may not transfer well to real property prediction.

### Mechanism 3
- Claim: Initializing KG embeddings with pre-trained KGE models (e.g., TuckER) provides a strong semantic prior for the KG GNN.
- Mechanism: KGE methods capture relational patterns in the KG that inform edge and node embeddings, reducing the burden on the GNN to rediscover basic relational semantics.
- Core assumption: The KG structure is sufficiently rich and consistent that KGE embeddings are meaningful for the downstream task.
- Evidence anchors:
  - [section]: "We use knowledge graph embedding (KGE) methods... to initialize the node and edge embeddings... These vectors capture the semantic meanings and relationships between entities and relations."
  - [section]: "We select TuckER's entity and relation embeddings to initialize the node and edge embeddings of the K-GNN, respectively."
- Break condition: If the KG contains noise or inconsistent relations, KGE initialization may mislead rather than help.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: The core model architecture processes both molecular graphs and knowledge graphs, so understanding message passing and aggregation is essential.
  - Quick check question: What is the difference between node-wise and graph-wise pooling in GNNs?

- Concept: Contrastive Learning
  - Why needed here: The joint embedding step relies on aligning molecule-level and KG-level embeddings via InfoNCE-style loss.
  - Quick check question: How does temperature scaling in InfoNCE affect embedding similarity distributions?

- Concept: Knowledge Graph Embeddings (KGE)
  - Why needed here: KG embeddings initialize the KG GNN and provide semantic priors for relational reasoning.
  - Quick check question: What distinguishes translational (e.g., TransE) from tensor factorization (e.g., TuckER) KGE approaches?

## Architecture Onboarding

- Component map:
  - Molecular GNN (M-GNN) -> KG GNN (K-GNN) -> Contrastive Learner -> Fine-tuning Head

- Critical path:
  1. Extract 3-hop KG sub-graphs for each molecule
  2. Pre-train M-GNN and K-GNN independently with their respective self-supervised tasks
  3. Apply contrastive learning to align embeddings
  4. Fine-tune the combined model on labeled property datasets

- Design tradeoffs:
  - Separate pre-training vs joint pre-training: separate allows specialized task objectives but adds complexity
  - KG embedding initialization: provides semantic priors but may introduce noise if KG quality is low
  - 3-hop sub-graph depth: deeper captures more context but increases computational cost

- Failure signatures:
  - Joint embeddings collapse to trivial vectors → check contrastive loss scaling and negative sampling
  - M-GNN outperforms combined model → KG may not add value; revisit KG quality or relevance
  - Poor fine-tuning → check that pre-training objectives align with downstream tasks

- First 3 experiments:
  1. Ablation: remove KG GNN, compare M-GNN alone vs. combined model on BBBP dataset
  2. Ablation: remove contrastive learning, compare joint embeddings from simple concatenation vs. contrastive fusion
  3. Scale test: increase KG sub-graph depth from 2-hop to 4-hop and measure impact on classification accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed GODE framework compare to other state-of-the-art models in molecular property prediction tasks?
- Basis in paper: [explicit] The paper mentions that GODE significantly outperforms existing benchmarks, achieving an average ROC-AUC improvement of 12.7% for classification tasks and an average RMSE/MAE improvement of 34.4% for regression tasks. Additionally, GODE surpasses the current leading model in property prediction, with advancements of 2.2% in classification and 7.2% in regression tasks.
- Why unresolved: The paper provides the performance improvements of GODE compared to existing benchmarks and the leading model, but it does not provide a direct comparison of GODE's performance with other state-of-the-art models in the field.
- What evidence would resolve it: A comprehensive comparison of GODE's performance with other state-of-the-art models in molecular property prediction tasks would provide a clearer understanding of its relative performance and potential advantages.

### Open Question 2
- Question: How does the pre-training of the M-GNN and K-GNN separately, rather than using the molecule-level information as node features in the KG, benefit the overall performance of the GODE framework?
- Basis in paper: [explicit] The paper mentions that the two graphs contain different structural information, and pre-training two different GNNs allows each to learn from those input graphs independently. Integrating the representations of the same molecule from both levels can potentially improve prediction performance.
- Why unresolved: The paper provides a general explanation of the benefits of separate pre-training, but it does not provide specific examples or detailed analysis of how this approach contributes to the overall performance of the GODE framework.
- What evidence would resolve it: Detailed experiments comparing the performance of GODE with separate pre-training of M-GNN and K-GNN to GODE with molecule-level information as node features in the KG would provide insights into the benefits of the proposed approach.

### Open Question 3
- Question: What are the potential improvements that could be made to the GODE framework to achieve better results in cases where it does not secure the top position in molecular property prediction tasks?
- Basis in paper: [inferred] The paper mentions that while GODE achieves competitive performance on some datasets, it does not secure the top position. It suggests that further optimization of hyperparameters or refining the knowledge graph construction process could potentially lead to better results.
- Why unresolved: The paper does not provide specific details on the potential improvements or their expected impact on the performance of GODE in cases where it does not secure the top position.
- What evidence would resolve it: Detailed analysis and experiments exploring the impact of hyperparameter optimization and knowledge graph construction refinement on the performance of GODE in cases where it does not secure the top position would provide insights into potential improvements.

## Limitations

- The approach assumes meaningful alignment between molecular and KG representations, which may not hold if KGs contain noise or inconsistent relations
- Effectiveness depends on the relevance of self-supervised pre-training tasks to downstream property prediction tasks
- Performance is sensitive to KG quality and the semantic relevance of pre-trained KGE embeddings

## Confidence

- High Confidence: Claims about effectiveness are supported by empirical evidence and consistent with literature
- Medium Confidence: Claims about specific mechanisms are supported by evidence but sensitive to implementation details
- Low Confidence: Claims about general applicability to other domains are speculative

## Next Checks

1. **Ablation Study**: Remove the KG GNN and compare the performance of the M-GNN alone vs. the combined model on a benchmark dataset (e.g., BBBP)
2. **Hyperparameter Sensitivity**: Vary the hyperparameters of the self-supervised pre-training tasks and assess their impact on downstream performance
3. **Generalization**: Evaluate the proposed approach on a diverse set of molecular property prediction tasks and compare its performance to other state-of-the-art methods