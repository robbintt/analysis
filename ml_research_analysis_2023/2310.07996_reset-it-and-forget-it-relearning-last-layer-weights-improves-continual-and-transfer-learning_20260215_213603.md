---
ver: rpa2
title: 'Reset It and Forget It: Relearning Last-Layer Weights Improves Continual and
  Transfer Learning'
arxiv_id: '2310.07996'
source_url: https://arxiv.org/abs/2310.07996
tags:
- transfer
- learning
- zapping
- accuracy
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies zapping\u2014repeatedly resetting the weights\
  \ in the last layer of a neural network during training\u2014as a simple yet effective\
  \ mechanism for improving continual and transfer learning. Zapping was originally\
  \ designed for meta-learning but proves effective beyond those settings."
---

# Reset It and Forget It: Relearning Last-Layer Weights Improves Continual and Transfer Learning

## Quick Facts
- **arXiv ID**: 2310.07996
- **Source URL**: https://arxiv.org/abs/2310.07996
- **Reference count**: 40
- **One-line primary result**: Zapping (resetting last-layer weights) during pre-training improves both continual and transfer learning, often matching state-of-the-art meta-learning performance without expensive higher-order gradients.

## Executive Summary
This paper introduces zapping—repeatedly resetting the weights in the last layer of a neural network during training—as a simple yet effective mechanism for improving continual and transfer learning. Zapping was originally designed for meta-learning but proves effective beyond those settings. The method involves reinitializing classifier weights before training on each new class, which forces the model to relearn mappings while backpropagating errors through upstream layers. Experiments show that zapping consistently improves transfer accuracy in both continual learning (sequential transfer) and standard transfer learning settings across three datasets (Omniglot, Mini-ImageNet, and OmnImage).

## Method Summary
The method involves pre-training a convolutional neural network with repeated resetting of the last fully-connected layer weights ("zapping") before training on each new class. This is combined with an Alternating Sequential and Batch (ASB) learning pattern where the model learns classes sequentially before batch updates. After pre-training, the model is transferred to new classes either sequentially (continual learning) or in batches (standard transfer learning). The key insight is that zapping forces the feature extractor to learn representations that can rapidly adapt to newly initialized classifiers, improving generalization to new tasks.

## Key Results
- Zapping achieves 40.3% final accuracy on Omniglot continual learning vs 28.5% without zapping
- Standard transfer learning accuracy improves from 69.0% to 78.4% on Omniglot with zapping
- Zapping+ASB often matches or exceeds state-of-the-art meta-learning performance without expensive higher-order gradients

## Why This Works (Mechanism)

### Mechanism 1
Resetting last-layer weights during pre-training forces the model to learn more generalizable features that can rapidly adapt to new classifiers. When last-layer weights are reset, the model must relearn how to map existing features to classes. This forces the feature extractor to learn representations that are useful across many possible classifier mappings, rather than overfitting to a specific classifier configuration.

### Mechanism 2
Zapping acts as a form of regularization that prevents co-adaptation between feature extractor and classifier layers. By periodically disrupting the classifier weights, zapping prevents the feature extractor from becoming too specialized to a particular classifier configuration. This maintains more flexible, generalizable features.

### Mechanism 3
The ASB learning pattern combined with zapping creates a meta-learning-like effect without expensive higher-order gradients. Sequential learning on individual classes followed by batch updates on all classes, combined with zapping, creates a training pattern that mimics the transfer learning scenario where a pre-trained model must quickly adapt to new classes.

## Foundational Learning

- **Catastrophic forgetting in neural networks**: Why needed here: The paper addresses continual learning where models must learn new classes without forgetting old ones. Quick check: What happens to a neural network's performance on previously learned tasks when it's trained on new tasks without special techniques?

- **Transfer learning and feature generalization**: Why needed here: The core mechanism relies on learning features that generalize across different classification tasks. Quick check: Why might a model trained on one set of classes perform poorly when the classifier layer is replaced and trained on a completely different set of classes?

- **Meta-learning and higher-order gradients**: Why needed here: The paper compares zapping+ASB to meta-learning approaches that use expensive second-order gradients. Quick check: What distinguishes meta-learning from standard supervised learning, and why are higher-order gradients typically needed?

## Architecture Onboarding

- **Component map**: Feature extractor (ConvNet) -> Classifier (FC layer) -> Zapping reset -> Sequential learning -> Batch updates
- **Critical path**: Pre-training → Sequential learning on individual classes → Batch updates on all classes → Zapping reset → Transfer to new classes
- **Design tradeoffs**: Zapping frequency vs. training stability, Sequential vs. batch learning ratio, ConvNet depth vs. computational cost, Resetting all vs. partial classifier weights
- **Failure signatures**: Overfitting to pre-training classes (no improvement on transfer), Unstable training (accuracy fluctuates wildly), No benefit from zapping (transfer performance same with/without), Degraded performance compared to standard i.i.d. training
- **First 3 experiments**:
  1. Compare i.i.d. pre-training vs. ASB pre-training on Omniglot with zapping enabled
  2. Test different zapping frequencies (reset all weights every epoch vs. every few epochs) on Mini-ImageNet
  3. Evaluate cross-domain transfer from Mini-ImageNet to Omniglot with different pre-training methods

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the optimal zapping frequency and class reset schedule for i.i.d. pre-training? The paper discusses sweeping over different numbers of classes to reset (K) and epochs between resets (E), finding that resetting all last layer weights at the beginning of every epoch (K=all, E=1) often performed best, but suggests that even more frequent resetting (E<1) could be beneficial.

- **Open Question 2**: How does zapping interact with other regularization techniques like dropout in continual and transfer learning? The paper mentions that both zapping and dropout improve continual and transfer learning, suggesting a potential relationship in their mechanisms, but the interaction between the two is not explored.

- **Open Question 3**: Does the benefit of zapping scale to larger, more complex neural network architectures and datasets? The paper uses relatively small convolutional networks and datasets, and while zapping shows consistent benefits, it's unclear if these gains hold for larger models like ResNet or Vision Transformers on larger-scale datasets like ImageNet.

## Limitations
- The exact optimal zapping frequency and timing remains unclear
- The mechanism's relationship to catastrophic forgetting needs more characterization
- Generalizability to non-image domains and larger-scale architectures is unknown

## Confidence
- **High Confidence**: Zapping consistently improves transfer learning accuracy, ASB pre-training provides benefits beyond standard i.i.d. training, Zapping acts as a computationally cheaper alternative to meta-learning
- **Medium Confidence**: The mechanism involves forcing the feature extractor to learn more generalizable representations, Zapping prevents co-adaptation between feature and classifier layers, The combination of zapping and ASB creates a meta-learning effect
- **Low Confidence**: Exact optimal zapping frequency and timing, Generalizability to non-image domains and larger-scale architectures, Relationship between zapping and other continual learning techniques

## Next Checks
1. **Ablation Study on Zapping Frequency**: Systematically vary the zapping reset frequency (every epoch, every N epochs, random intervals) across Omniglot and Mini-ImageNet to identify optimal schedules and understand the mechanism's sensitivity to timing.

2. **Cross-Domain Transfer Evaluation**: Test transfer from natural image domains (Mini-ImageNet) to sketch/symbol domains (Omniglot) and vice versa, measuring both accuracy gains and catastrophic forgetting to validate the feature generalization claims.

3. **Architectural Scaling Test**: Evaluate zapping on deeper architectures (ResNet, Vision Transformer) and larger-scale datasets to assess whether the benefits scale with model complexity and dataset size.