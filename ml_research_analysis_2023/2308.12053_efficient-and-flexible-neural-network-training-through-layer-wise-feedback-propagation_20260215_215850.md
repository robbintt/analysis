---
ver: rpa2
title: Efficient and Flexible Neural Network Training through Layer-wise Feedback
  Propagation
arxiv_id: '2308.12053'
source_url: https://arxiv.org/abs/2308.12053
tags:
- feedback
- training
- gradient
- learning
- sign
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Layer-wise Feedback Propagation (LFP), a
  novel gradient-free training paradigm for neural networks that leverages explainability
  methods to decompose a task reward into individual neuron contributions. Instead
  of optimizing via gradients, LFP distributes a feedback signal throughout the model
  and updates parameters based on their current contribution to solving the task.
---

# Efficient and Flexible Neural Network Training through Layer-wise Feedback Propagation

## Quick Facts
- arXiv ID: 2308.12053
- Source URL: https://arxiv.org/abs/2308.12053
- Reference count: 40
- LFP achieves comparable performance to gradient descent while preserving parameter sign changes during transfer learning and enabling training of non-differentiable models like SNNs.

## Executive Summary
This paper introduces Layer-wise Feedback Propagation (LFP), a novel gradient-free training paradigm for neural networks that leverages explainability methods to decompose task rewards into individual neuron contributions. Instead of optimizing via gradients, LFP distributes a feedback signal throughout the model and updates parameters based on their current contribution to solving the task. The authors establish theoretical convergence of LFP for ReLU models with non-negative logits, and demonstrate empirically that LFP can achieve comparable performance to gradient descent on various models and datasets. Two key applications showcase LFP's advantages: training Spiking Neural Networks (SNNs) without requiring differentiable activations, and efficient transfer learning where LFP preserves parameter sign changes while achieving similar accuracy to gradient descent.

## Method Summary
LFP replaces gradient computation with explainability-based feedback distribution. During training, the model makes predictions as usual, then a task-specific feedback signal is computed for each output neuron. This feedback is distributed backward through the network using Layer-wise Relevance Propagation (LRP) rules, assigning credit to each connection based on its contribution to the task. Parameters are then updated directly from these distributed credits rather than gradients. The method supports multiple LRP variants (LFP-0, LFP-ε, LFP-αβ, LFP-z+z−) to address numerical stability and exploding feedback issues. LFP can train models with non-differentiable activations like step functions used in SNNs, and shows particular strength in transfer learning scenarios where it preserves parameter sign changes while maintaining accuracy.

## Key Results
- LFP achieves comparable test accuracy to gradient descent on MNIST, CIFAR-10, and ImageNet subsets
- LFP enables training of Spiking Neural Networks without requiring differentiable activations
- LFP-based fine-tuning preserves parameter sign changes while achieving similar accuracy to gradient descent, with superior pruning performance (90% weight removal vs 70% for gradient-based methods)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LFP can train neural networks without gradients by leveraging explainability methods to decompose task rewards into individual neuron contributions.
- Mechanism: LFP distributes a feedback signal through the network using Layer-wise Relevance Propagation (LRP) rules. Instead of computing gradients, it assigns credit to each connection based on its contribution to solving the task, then updates parameters directly from these credits.
- Core assumption: The feedback signal can be meaningfully decomposed through the network layers and still provide useful update directions.
- Evidence anchors:
  - [abstract] "LFP distributes a reward signal throughout the model without the need for gradient computations."
  - [section] "Instead of an initial relevance value, a feedback is given to each output neuron evaluating how well it performed w.r.t. a given task."
  - [corpus] Weak - corpus papers focus on neural network optimization and pruning but don't directly discuss LFP methodology.
- Break condition: If the feedback decomposition fails to provide meaningful credit assignments or if the feedback signal becomes too sparse (especially in SNNs with sparse activations).

### Mechanism 2
- Claim: LFP can train models with non-differentiable activations like Spiking Neural Networks (SNNs) where gradient-based methods fail.
- Mechanism: By avoiding gradients entirely and using reward-based feedback, LFP can train models with step functions or other non-differentiable activations that would normally make gradient descent impossible.
- Core assumption: The reward signal can propagate effectively through networks with discrete or non-differentiable activation functions.
- Evidence anchors:
  - [abstract] "training models with no meaningful derivatives, e.g., step-function activated Spiking Neural Networks (SNNs)"
  - [section] "LFP provides a suitable training paradigm for SNNs that circumvents this limitation by propagating rewards directly through the non-differentiable spiking activation."
  - [corpus] Weak - corpus papers discuss neural network optimization but don't specifically address training with non-differentiable activations.
- Break condition: If the sparse nature of SNN activations prevents effective feedback propagation, leading to poor training performance.

### Mechanism 3
- Claim: LFP can preserve existing knowledge during transfer learning by changing parameter signs minimally while maintaining accuracy.
- Mechanism: LFP updates parameters based on their current contribution to the task, which tends to decay obstructive connections toward zero rather than changing their sign. This preserves the qualitative meaning of connections while fine-tuning their quantitative contributions.
- Core assumption: The sign of a parameter encodes meaningful information about its qualitative role in the network.
- Evidence anchors:
  - [abstract] "efficient transfer learning where LFP preserves parameter sign changes while achieving similar accuracy to gradient descent"
  - [section] "parameter sign can be interpreted as the qualitative meaning of the corresponding connection (i.e., inhibitory or excitatory)"
  - [corpus] Weak - corpus papers discuss neural network pruning and optimization but don't specifically address parameter sign preservation during transfer learning.
- Break condition: If the network requires significant sign changes to adapt to the new task, LFP's conservative approach may lead to suboptimal performance.

## Foundational Learning

- Concept: Gradient-based optimization and backpropagation
  - Why needed here: LFP is presented as an alternative to gradient-based training, so understanding how gradient descent works helps contextualize LFP's innovations and differences.
  - Quick check question: What are the key limitations of gradient-based training that LFP aims to address?

- Concept: Layer-wise Relevance Propagation (LRP) and XAI methods
  - Why needed here: LFP directly builds upon LRP methodology to decompose feedback signals through the network, making understanding LRP essential to grasping how LFP works.
  - Quick check question: How does LRP decompose relevance through network layers, and how is this adapted in LFP?

- Concept: Neural network pruning and lottery ticket hypothesis
  - Why needed here: LFP's transfer learning application relates to pruning concepts and the lottery ticket hypothesis about subnetworks within larger models, as LFP seems to extract and fine-tune contributing subnets.
  - Quick check question: How does LFP's approach to parameter updates relate to the concept of identifying and training subnetworks?

## Architecture Onboarding

- Component map:
  Forward pass -> Feedback computation -> LRP-based backward distribution -> Parameter update

- Critical path:
  1. Model makes prediction during forward pass
  2. Task-specific feedback is computed for each output neuron
  3. Feedback is distributed backward through network using LRP rules
  4. Parameters are updated based on distributed feedback credits
  5. Process repeats for training iterations

- Design tradeoffs:
  - LFP vs gradient descent: LFP avoids gradient computation but may require careful hyperparameter tuning; gradient descent is well-established but struggles with non-differentiable activations
  - LFP rule selection: LFP-ε offers stability but may cause exploding feedback in deep models; LFP-z+z− prevents exploding feedback but may underperform in shallow models
  - Transfer learning vs random initialization: LFP excels at fine-tuning pretrained models but struggles with training from scratch

- Failure signatures:
  - Training instability or divergence (often due to exploding feedback or inappropriate feedback functions)
  - Poor performance on randomly initialized models (LFP works best with existing knowledge to leverage)
  - Excessive sparsity preventing effective updates (especially in SNNs with high threshold settings)
  - Numerical instability in deep networks (requiring careful choice of LFP rule and parameters)

- First 3 experiments:
  1. Train a simple MLP on MNIST using LFP-ε and compare performance to gradient descent with varying learning rates
  2. Test LFP on a model with non-differentiable activations (e.g., Heaviside step function) to verify gradient-free training capability
  3. Apply LFP to fine-tune a pretrained VGG-16 on a subset of ImageNet classes and measure parameter sign changes compared to gradient descent

## Open Questions the Paper Calls Out

The paper acknowledges several limitations and calls for future work. The authors note that experiments were restricted to simple problems and models, and that "application to more complex settings may require additional steps and optimizations that are subject to future work." They also acknowledge that while they demonstrate LFP's viability on standard benchmarks, the choice of feedback function was not the main focus and that "other feedback functions may perform similarly well or better." The paper explicitly states that scaling LFP to state-of-the-art problems remains unexplored, and that systematic exploration of different feedback formulations across multiple tasks would be valuable future work.

## Limitations

- LFP performs poorly on randomly initialized models, working best with existing knowledge to leverage rather than training from scratch
- The method requires careful selection of LFP rules and parameters to prevent numerical instability, especially in deep networks
- Empirical validation is limited to simple models and datasets, with scaling to complex architectures and large-scale problems remaining unexplored

## Confidence

High confidence: The theoretical convergence proof for ReLU models with non-negative logits provides solid foundation.

Medium confidence: Empirical results showing LFP's competitiveness with gradient descent on standard benchmarks are promising but would benefit from more extensive hyperparameter tuning and larger-scale experiments.

Medium confidence: SNN training results demonstrate LFP's unique capability for non-differentiable models, though experimental setup differs significantly from standard benchmarks.

Medium confidence: Transfer learning results showing superior pruning performance are interesting but focus on specific pruning methodology rather than comprehensive transfer learning scenarios.

## Next Checks

1. Benchmark LFP on larger-scale vision tasks beyond the 20-class ImageNet subset to assess scalability to complex architectures and datasets.

2. Compare LFP's performance on randomly initialized models versus pretrained models to quantify its dependence on existing knowledge and identify scenarios where it may underperform gradient descent.

3. Test LFP with various non-differentiable activation functions beyond step functions to evaluate its general applicability to gradient-free optimization scenarios across different model architectures.