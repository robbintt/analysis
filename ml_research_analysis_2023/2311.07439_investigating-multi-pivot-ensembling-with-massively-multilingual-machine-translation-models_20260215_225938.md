---
ver: rpa2
title: Investigating Multi-Pivot Ensembling with Massively Multilingual Machine Translation
  Models
arxiv_id: '2311.07439'
source_url: https://arxiv.org/abs/2311.07439
tags:
- translation
- language
- linguistics
- association
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates multi-pivot ensembling for massively multilingual
  machine translation, focusing on improving translation quality for low-resource
  language pairs. While previous approaches averaged probability distributions across
  multiple pivot languages, the authors find this can exacerbate hallucinations and
  underperform single-pivot translation.
---

# Investigating Multi-Pivot Ensembling with Massively Multilingual Machine Translation Models

## Quick Facts
- arXiv ID: 2311.07439
- Source URL: https://arxiv.org/abs/2311.07439
- Reference count: 11
- Primary result: MaxEns ensembling strategy reduces hallucinations and improves translation quality for low-resource language pairs compared to simple averaging, though still underperforms single English pivot

## Executive Summary
This paper investigates multi-pivot ensembling strategies for massively multilingual machine translation, focusing on improving translation quality for low-resource language pairs. The authors find that simple averaging of probability distributions across multiple pivot languages (MultiAvg) can worsen hallucinations and underperform single-pivot translation. They propose MaxEns, a novel strategy that selects the most confident prediction across pivots at each decoding step. Experiments on 20 low-resource language pairs from FLORES benchmark show MaxEns reduces hallucinations and improves translation quality compared to both direct translation and simple averaging, though still lags behind using English as a single pivot.

## Method Summary
The study compares direct translation, single-pivot translation, and multi-pivot ensembling strategies for translating between low-resource language pairs. The multi-pivot approach uses English, Spanish, and French as pivot languages, generating translations through each pivot path and combining results using either simple averaging (MultiAvg) or the proposed MaxEns strategy that selects the most confident prediction at each decoding step. The experiments use two massively multilingual models (M2M100 and SMaLL100) and evaluate translation quality using spBLEU and hallucination rates using ChrF < 20 as a proxy metric.

## Key Results
- MaxEns outperforms MultiAvg on both translation quality (spBLEU) and hallucination reduction across 20 low-resource language pairs
- Simple averaging (MultiAvg) exacerbates hallucinations and underperforms single-pivot translation
- Multi-pivot strategies still lag behind using English as a single pivot due to training data distribution
- The optimal pivoting strategy varies significantly across different language pairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-pivot ensembling with simple averaging (MultiAvg) exacerbates hallucinations because it combines probability distributions where hallucinations are "sticky" across pivots
- Mechanism: When multiple pivot paths independently generate similar hallucinations, averaging increases their joint probability, making them more likely in the final output
- Core assumption: Hallucinations in low-resource translation are systematic and tend to appear similarly across different pivot paths due to model training biases
- Evidence anchors:
  - [abstract] "we find that this performs worse than using a single pivot, and exacerbates the hallucination problem because the same hallucinations can be probable across different paths"
  - [section 3] "we find that a simple averaging is suboptimal and may increase the number of hallucinations in the output, a typical failure case in low-resource settings"
  - [corpus] Weak - no direct evidence of hallucination stickiness across pivots in related papers

### Mechanism 2
- Claim: MaxEns works by selecting the most confident prediction at each decoding step, which tends to avoid low-confidence hallucinations
- Mechanism: Confidence scores correlate inversely with hallucination likelihood; by always picking the highest probability token across pivots, the method avoids uncertain outputs
- Core assumption: Model confidence is a reliable heuristic for hallucination detection - confident predictions are less likely to be hallucinations
- Evidence anchors:
  - [abstract] "MaxEns, a novel combination strategy that makes the output biased towards the most confident predictions, hypothesising that confident predictions are less prone to be hallucinations"
  - [section 4.2] "MaxEns performs better in both respects, but still lags behind using only English as pivot" and "reduces the occurrence of hallucinations"
  - [corpus] Weak - while related papers discuss hallucination detection, none directly validate confidence as a hallucination filter

### Mechanism 3
- Claim: Multi-pivot strategies underperform single English pivot because most training data pairs with English, giving it higher quality representations
- Mechanism: The multilingual model has learned better cross-lingual mappings through English due to higher training volume, making direct English pivot more effective than multi-pivot averaging
- Core assumption: Training data distribution (more English-aligned pairs) creates asymmetric quality across translation directions
- Evidence anchors:
  - [abstract] "On average, multi-pivot strategies still lag behind using English as a single pivot language"
  - [section 1] "English has the largest amount of bitext overall in the training data of M2M100"
  - [section 4.2] "MaxEns method performs a little worse on average compared to English pivoting, as most of the parallel sentences of pre-training data for M2M100 are paired with English"
  - [corpus] Weak - no direct comparison of pivot language effectiveness in related papers

## Foundational Learning

- Concept: Probability distribution ensembling
  - Why needed here: The paper compares averaging vs. max-selection across probability distributions from multiple pivot paths
  - Quick check question: If three pivot paths produce probabilities [0.3, 0.4, 0.5] for a token, what does MultiAvg output vs. MaxEns?

- Concept: Hallucination detection metrics
  - Why needed here: The paper uses ChrF < 20 as a proxy for hallucination detection
  - Quick check question: Why might ChrF be a better hallucination metric than BLEU for this task?

- Concept: Pivot translation vs. direct translation
  - Why needed here: The paper compares these as baseline strategies for low-resource languages
  - Quick check question: In what scenarios would direct translation outperform pivot translation despite having less training data?

## Architecture Onboarding

- Component map: Source text -> Pivot selection (English/Spanish/French) -> Path generation (sourceâ†’pivot) -> Probability computation -> Ensembling (MultiAvg/MaxEns) -> Final translation

- Critical path:
  1. Generate pivot translations
  2. Compute token probabilities for each pivot
  3. Apply ensembling strategy (MultiAvg or MaxEns)
  4. Generate final output sequence

- Design tradeoffs:
  - MultiAvg is computationally simpler but risks hallucination amplification
  - MaxEns requires tracking max probabilities but better handles hallucinations
  - Adding more pivots increases coverage but also computational cost and hallucination risk

- Failure signatures:
  - High hallucination rates (>25%) indicate MultiAvg failure
  - No improvement over direct translation suggests poor pivot selection
  - Performance degradation with additional pivots indicates overfitting to hallucination patterns

- First 3 experiments:
  1. Implement MultiAvg baseline and verify it outperforms direct translation on FLORES-20
  2. Add MaxEns implementation and compare hallucination reduction vs. MultiAvg
  3. Test English-only pivot vs. multi-pivot strategies to confirm data distribution effect

## Open Questions the Paper Calls Out
The paper raises the question of how to identify the best pivoting strategy for a given translation direction, noting that the optimal strategy differs across translation directions. It suggests that future work could explore better methods for selecting pivot languages based on language pair characteristics.

## Limitations
- The hypothesized mechanisms for hallucination stickiness and confidence-based filtering lack direct empirical validation
- The study only tests on 20 low-resource language pairs and two specific multilingual models, limiting generalizability
- The ChrF < 20 hallucination metric is novel but not widely validated in the literature

## Confidence
- **High confidence**: Experimental methodology and implementation of MultiAvg and MaxEns strategies are clearly specified and reproducible
- **Medium confidence**: Observation that MaxEns outperforms MultiAvg and reduces hallucinations is well-supported by experimental results
- **Low confidence**: Hypothesized mechanisms for why hallucinations occur and how MaxEns addresses them lack direct empirical validation

## Next Checks
1. Conduct controlled experiments to verify whether hallucinations are systematic and "sticky" across different pivot paths
2. Test the reliability of confidence scores as hallucination detectors by comparing against human-annotated hallucination labels
3. Run experiments using models trained with balanced pivot language distributions to isolate the effect of training data imbalance