---
ver: rpa2
title: Learning Active Subspaces and Discovering Important Features with Gaussian
  Radial Basis Functions Neural Networks
arxiv_id: '2307.05639'
source_url: https://arxiv.org/abs/2307.05639
tags:
- feature
- importance
- none
- grbf-nn
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of building predictive models
  that are both accurate and interpretable, a task complicated by the conflicting
  objectives of performance and transparency. The authors propose a modification of
  the Radial Basis Function Neural Network (RBF-NN) by introducing a learnable precision
  matrix into the Gaussian kernel.
---

# Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks

## Quick Facts
- **arXiv ID:** 2307.05639
- **Source URL:** https://arxiv.org/abs/2307.05639
- **Reference count:** 40
- **Primary result:** A modified RBF-NN with learnable precision matrix achieves competitive predictive performance while extracting active subspaces and feature importance rankings.

## Executive Summary
This paper presents a modification to the Radial Basis Function Neural Network (RBF-NN) by introducing a learnable precision matrix into the Gaussian kernel. This enables the model to not only make accurate predictions but also extract meaningful information about the data's latent structure through the eigenvalues and eigenvectors of the precision matrix. The approach provides both competitive predictive performance and valuable interpretability by revealing active subspaces and identifying important input features.

## Method Summary
The method extends classical RBF-NNs by replacing the fixed-width Gaussian kernel with one that includes a learnable precision matrix M. This matrix is upper-triangular and optimized during training along with the network weights. The model uses Adam optimization for up to 10,000 epochs with hyperparameter tuning via grid search. The key innovation is that the eigenvalues and eigenvectors of M can be used to extract the active subspace and feature importance rankings, providing interpretability alongside predictive performance.

## Key Results
- The GRBF-NN achieves competitive predictive performance compared to popular ML models and state-of-the-art deep learning-based feature selection techniques
- Eigenvalues and eigenvectors of the precision matrix reveal the active subspace and important input features
- The model provides meaningful insights into its behavior through estimated feature importance rankings
- The approach works effectively across regression, classification, and feature selection tasks on 20 real-world datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learnable precision matrix enables extraction of latent structure in data.
- Mechanism: Gaussian RBF kernel with learnable precision matrix M projects input space into a latent space Z. Eigenvalues of M reveal directions of maximum variability; eigenvectors define the active subspace.
- Core assumption: Second derivatives of the Gaussian kernel argument along eigenvector directions are captured by eigenvalues.
- Evidence anchors: [abstract] "eigenvectors explain the directions of maximum sensitivity of the model revealing the active subspace"; [section] "d2 M(x) = (xi − xj)T M(xi − xj) = (zi − zj)T Γ (zi − zj)" and "eigenvalues provide valuable information regarding the second derivative"

### Mechanism 2
- Claim: Eigenvectors of precision matrix provide feature importance ranking.
- Mechanism: Jacobian of linear transformation from latent to input space (∂x/∂z = V) quantifies how each latent variable influences input features. Scaled by eigenvalues, this yields importance vector.
- Core assumption: Importance scales with both eigenvector direction and eigenvalue magnitude.
- Evidence anchors: [abstract] "eigenvectors highlight the relationship in terms of absolute variation between the input and the latent variables"; [section] "Jacobian of the linear transformation...we can assess how a change in a particular latent variable affects the input variables"

### Mechanism 3
- Claim: Regularization of precision matrix controls smoothness more effectively than weight regularization.
- Mechanism: Penalizing ||u||2 (precision matrix entries) enforces flatness of Gaussian kernel, limiting rapid variations; weight regularization only limits amplitude.
- Core assumption: Flatness of kernel basis functions is more critical for generalization than weight magnitude.
- Evidence anchors: [section] "regularizers have the responsibility to force the Gaussian kernel to be as flat as possible, penalizing large values of the entries of the precision matrix M"; [section] "numerical results suggest that a stronger role is played by the regularizer of the precision matrix rather than the one that controls the magnitude of the weights"

## Foundational Learning

- Concept: Radial Basis Function Neural Networks
  - Why needed here: The model extends classical RBF-NN by adding a learnable precision matrix to the Gaussian kernel.
  - Quick check question: How does a learnable precision matrix differ from a fixed-width Gaussian kernel in standard RBF-NNs?

- Concept: Active Subspace Methods
  - Why needed here: Eigenvalues/eigenvectors of the precision matrix define the active subspace, enabling dimensionality reduction.
  - Quick check question: What is the relationship between the eigenvalues of the precision matrix and the second derivatives of the Gaussian kernel?

- Concept: Feature Importance Ranking via Jacobians
  - Why needed here: The model uses the Jacobian of the latent-to-input transformation to rank input features by their contribution.
  - Quick check question: Why is the absolute value of Jacobian entries used when computing feature importance?

## Architecture Onboarding

- Component map: Input → Precision matrix M (learnable, DxD) → Gaussian kernel → RBF centers (fixed or learnable) → Linear combination → Output
- Critical path: 1. Initialize precision matrix M as upper triangular U. 2. Compute Gaussian kernel with M. 3. Forward pass through RBF-NN. 4. Backpropagate to update w, u (and c if supervised centers). 5. Regularize M and weights.
- Design tradeoffs: More centers → higher capacity but more parameters. Stronger λu → smoother model but risk underfitting. Unsupervised vs supervised centers: speed vs accuracy.
- Failure signatures: Large eigenvalues → sharp kernels, overfitting. Very small eigenvalues → nearly singular M, unstable gradients. High variance in cross-validation → optimizer stuck in local minima.
- First 3 experiments: 1. Train on synthetic dataset where ground truth active subspace is known; compare recovered vs true subspace. 2. Vary λu while fixing λw=0; observe effect on test RMSE and eigenvalue decay. 3. Compare feature importance ranking from model vs gradient-based importance on a regression task with known relevant features.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between the regularizers λw and λu for different types of datasets?
- Basis in paper: [explicit] The paper discusses the impact of these regularizers on the performance of the GRBF-NN model, but does not provide a definitive answer on the optimal balance.
- Why unresolved: The optimal balance likely depends on the specific characteristics of the dataset and the task at hand. Further experimentation and analysis are needed to determine general guidelines.
- What evidence would resolve it: Empirical studies comparing the performance of GRBF-NN with different balances of λw and λu on a wide range of datasets and tasks would help establish optimal settings.

### Open Question 2
- Question: How does the GRBF-NN model perform on high-dimensional datasets with a large number of irrelevant features?
- Basis in paper: [inferred] The paper does not explicitly test the model on high-dimensional datasets with many irrelevant features. The synthetic datasets used for feature selection evaluation contain a moderate number of features.
- Why unresolved: The effectiveness of the model's feature selection and dimensionality reduction capabilities may vary with the dimensionality and complexity of the dataset.
- What evidence would resolve it: Testing the model on real-world high-dimensional datasets with a large number of irrelevant features and comparing its performance to other feature selection methods would provide insights into its scalability and robustness.

### Open Question 3
- Question: Can the GRBF-NN model be extended to handle structured data, such as graphs or sequences?
- Basis in paper: [inferred] The paper focuses on tabular data and does not discuss potential extensions to structured data. The Gaussian kernel used in the model is not inherently designed for non-Euclidean data structures.
- Why unresolved: Handling structured data requires specialized kernels or modifications to the model architecture, which are not explored in the paper.
- What evidence would resolve it: Developing and evaluating extensions of the GRBF-NN model for structured data, such as using graph kernels or recurrent architectures, would demonstrate its applicability to a wider range of data types.

## Limitations
- The feature importance ranking mechanism relies on the Jacobian of the latent-to-input transformation, but empirical validation against ground truth importances is limited to synthetic datasets
- The claim that precision matrix regularization is more effective than weight regularization is based on numerical results rather than theoretical analysis
- The paper does not provide error bars or statistical significance testing for performance comparisons across datasets

## Confidence
- **High confidence**: Predictive performance claims (RMSE, accuracy metrics) are supported by extensive experiments across 20 real-world datasets
- **Medium confidence**: The active subspace extraction mechanism is theoretically sound but lacks rigorous mathematical proof of convergence properties
- **Medium confidence**: Feature importance rankings are demonstrated on synthetic data but not validated on real-world datasets with known feature relevance

## Next Checks
1. **Theoretical validation**: Prove convergence guarantees for the optimization of the precision matrix M under the proposed regularization scheme
2. **Real-world feature importance**: Apply the model to a real-world dataset with known feature relevance (e.g., gene expression data) and compare the extracted importances to established biological knowledge
3. **Ablation study**: Systematically vary the regularization strengths (λu, λw) across multiple datasets to quantify the relative importance of precision matrix vs. weight regularization