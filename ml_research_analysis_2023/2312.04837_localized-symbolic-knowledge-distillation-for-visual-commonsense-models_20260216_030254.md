---
ver: rpa2
title: Localized Symbolic Knowledge Distillation for Visual Commonsense Models
arxiv_id: '2312.04837'
source_url: https://arxiv.org/abs/2312.04837
tags:
- image
- answer
- visual
- question
- localized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Localized Symbolic Knowledge Distillation (LSKD) trains vision-language
  models to accept region references as input by distilling localized commonsense
  knowledge from a large language model (LLM). The method generates a diverse corpus
  of 1M localized commonsense statements over 250K images by prompting an LLM with
  global and local image descriptions, then filtering outputs with a supervised critic
  model.
---

# Localized Symbolic Knowledge Distillation for Visual Commonsense Models

## Quick Facts
- arXiv ID: 2312.04837
- Source URL: https://arxiv.org/abs/2312.04837
- Reference count: 40
- Key outcome: LSKD achieves state-of-the-art zero-shot performance on three localized visual reasoning benchmarks (VCR, Sherlock, VisualCOMET), improving accuracy by 5.4-10.2% over strong baselines.

## Executive Summary
Localized Symbolic Knowledge Distillation (LSKD) trains vision-language models to accept region references as input by distilling localized commonsense knowledge from a large language model (LLM). The method generates a diverse corpus of 1M localized commonsense statements over 250K images by prompting an LLM with global and local image descriptions, then filtering outputs with a supervised critic model. LSKD achieves state-of-the-art zero-shot performance on three localized visual reasoning benchmarks (VCR, Sherlock, VisualCOMET), improving accuracy by 5.4-10.2% over strong baselines. Human evaluation shows LSKD-trained generative models produce more informative answers than the LLM teacher. The approach enables intuitive user interaction by allowing "pointing" to image regions instead of writing referring expressions, advancing visual commonsense reasoning capabilities.

## Method Summary
LSKD generates localized commonsense knowledge by prompting ChatGPT with global descriptors (CLIP-ViTL concepts + OFA-Huge narratives), local descriptors (BLIP-2 region captions), and dynamic descriptors (ChatGPT-generated questions). This produces 18 QAR triples per image across 250K images. A supervised critic model (BLIP-2 + ViT-G) filters the 4.5M instances to ~300K high-quality examples using a threshold of 0.8. The filtered corpus is then used to fine-tune BLIP-2 Q-Former with contrastive, matching, and grounding objectives, enabling zero-shot region-based visual reasoning.

## Key Results
- 5.4-10.2% accuracy improvements on VCR, Sherlock, and VisualCOMET benchmarks
- Outperforms LLaVA-instruct by 6.2% on VisualCOMET
- Human evaluation shows LSKD models produce more informative answers than the LLM teacher
- Effective transfer to non-localized tasks like AOKVQA and SNLI-VE

## Why This Works (Mechanism)

### Mechanism 1
The LLM teacher can connect literal visual descriptions to commonsense inferences about specific regions. By providing both global and local image descriptions as context, the LLM learns to ground abstract commonsense knowledge in concrete visual elements. The corpus contains 1M localized commonsense statements over 250K images, showing the LLM can generate diverse, region-specific inferences at scale.

### Mechanism 2
The critic model effectively filters out hallucinations and inconsistent statements from the LLM teacher. A supervised critic model trained on human-annotated data learns to distinguish between visually coherent and incoherent statements. Only 45% of instances were labeled as accepted by human annotators before filtering, indicating the need for aggressive filtering.

### Mechanism 3
Training on localized commonsense knowledge improves zero-shot performance on region-grounded visual reasoning tasks. The distilled knowledge provides models with richer, region-specific understanding that transfers to downstream tasks requiring localized reasoning. Improvement on three localized visual reasoning benchmarks (VCR, Sherlock, VisualCOMET) with 5.4-10.2% accuracy gains demonstrates effective transfer.

## Foundational Learning

- Concept: Knowledge distillation from large language models
  - Why needed here: The approach relies on transferring commonsense reasoning capabilities from an LLM teacher to a vision-language student model
  - Quick check: Can you explain how knowledge distillation differs from standard supervised learning?

- Concept: Multimodal grounding and reference resolution
  - Why needed here: The system needs to connect abstract commonsense knowledge to specific regions within images
  - Quick check: How would you evaluate whether a model has successfully grounded a statement to a specific image region?

- Concept: Supervised filtering and quality control
  - Why needed here: Raw LLM outputs contain hallucinations that must be filtered before training student models
  - Quick check: What are the key considerations when designing a critic model for filtering synthetic data?

## Architecture Onboarding

- Component map: CLIP-ViTL concepts + OFA-Huge narratives → ChatGPT teacher → Critic model → Filtered corpus → BLIP-2 Q-Former fine-tuning
- Critical path: Image → Verbalization → LLM Generation → Critic Filtering → Student Training → Zero-shot Evaluation
- Design tradeoffs:
  - Quality vs. quantity in corpus generation (aggressive filtering improves quality but reduces dataset size)
  - Model capacity vs. training efficiency (freezing visual and language models speeds training but may limit adaptation)
  - Generic vs. task-specific knowledge (broad corpus helps generalization but may miss task-specific patterns)
- Failure signatures:
  - Student model performs worse than zero-shot baseline → likely filtering too aggressively or LLM teacher quality is poor
  - No improvement on localized tasks but improvement on non-localized → localized knowledge may not be effectively transferred
  - Critic model accuracy plateaus at low threshold → human annotation may be inconsistent or criteria too strict
- First 3 experiments:
  1. Evaluate zero-shot performance of BLIP-2 baseline on localized benchmarks to establish performance floor
  2. Train student model on unfiltered LLM outputs to measure impact of critic filtering
  3. Ablate different descriptor components (global, local, dynamic) to identify most important knowledge sources

## Open Questions the Paper Calls Out

### Open Question 1
How would increasing the threshold for the critic model beyond 0.8 affect downstream task performance? The paper states that higher thresholds tend to yield superior quality generations but doesn't test thresholds beyond 0.8. Conducting experiments with thresholds beyond 0.8 would provide necessary evidence.

### Open Question 2
How does the performance of LSKD compare to other knowledge distillation approaches that use different teacher models or data sources? The paper only compares LSKD to LLaVA-instruct, so it's unclear how it would perform against other potential methods using different teachers or data sources.

### Open Question 3
How would the performance of LSKD change if it were applied to other vision-language tasks beyond localized reasoning? The paper only tests LSKD on a limited set of tasks, so it's unclear how it would perform on other vision-language tasks like image captioning or general visual question answering.

## Limitations

- The approach relies heavily on the quality and diversity of ChatGPT's commonsense inferences, which may be constrained by the prompting strategy
- The filtering mechanism may inadvertently remove valid but unconventional reasoning patterns that could be valuable for certain tasks
- Performance gains are demonstrated primarily on three localized visual reasoning benchmarks, with unclear generalizability to other domains or task types

## Confidence

- **High Confidence**: The zero-shot performance improvements on VCR, Sherlock, and VisualCOMET (5.4-10.2% accuracy gains) are well-supported by experimental results
- **Medium Confidence**: The claim that LSKD enables more intuitive user interaction through region references would benefit from direct user studies
- **Medium Confidence**: The assertion that the critic model effectively filters hallucinations relies on the quality and representativeness of human-annotated training data

## Next Checks

1. **Cross-dataset generalization test**: Evaluate the distilled model on additional localized visual reasoning datasets beyond the three reported benchmarks to assess generalizability to different domains and reasoning types.

2. **Human evaluation of interaction efficiency**: Conduct user studies comparing task completion time and accuracy when using region-based versus referring-expression-based interaction with the LSKD-trained model.

3. **Knowledge diversity analysis**: Analyze the semantic diversity and coverage of the filtered corpus using clustering or embedding-based methods to quantify how well it captures the full space of localized commonsense knowledge.