---
ver: rpa2
title: 'CTP: Towards Vision-Language Continual Pretraining via Compatible Momentum
  Contrast and Topology Preservation'
arxiv_id: '2308.07146'
source_url: https://arxiv.org/abs/2308.07146
tags:
- learning
- continual
- image
- data
- image-text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of Vision-Language Continual
  Pretraining (VLCP), where models must learn from a non-stationary data stream of
  image-text pairs across multiple tasks. The authors propose a new algorithm called
  Compatible Momentum Contrast with Topology Preservation (CTP) to tackle the unique
  challenges of VLCP, including fixed-dimensional embedding, missing contrast samples,
  and multi-modal/task optimization.
---

# CTP: Towards Vision-Language Continual Pretraining via Compatible Momentum Contrast and Topology Preservation

## Quick Facts
- **arXiv ID**: 2308.07146
- **Source URL**: https://arxiv.org/abs/2308.07146
- **Reference count**: 40
- **Primary result**: CTP achieves superior performance in Vision-Language Continual Pretraining with over 1M product image-text pairs across 9 industries

## Executive Summary
This paper addresses the challenge of Vision-Language Continual Pretraining (VLCP), where models must learn from non-stationary data streams across multiple tasks. The authors propose CTP (Compatible Momentum Contrast with Topology Preservation), a novel algorithm that tackles the unique challenges of VLCP including fixed-dimensional embedding, missing contrast samples, and multi-modal/task optimization. CTP employs a compatible momentum model that absorbs knowledge from both current and previous-task models, while preserving topology knowledge across tasks. Experiments on a newly created P9D benchmark demonstrate that CTP achieves superior performance compared to baselines in both cross-modal and multi-modal retrieval tasks without significant training burden.

## Method Summary
CTP addresses VLCP by introducing a compatible momentum model that synchronously absorbs parameters from the current model and previous-task model, allowing smooth feature adjustment while maintaining old knowledge. The method also preserves topology relationships across tasks by constraining mini-batch sample relationships between current and previous-task models using cross-entropy between similarity distributions. The multi-modal fusion feature demonstrates strong anti-forgetting ability compared to cross-modal alignment. The approach uses ViT-B/16 for image encoding, BERT-base for text encoding, and a cross-attention multi-modal encoder with ALBEF architecture. Training involves sequential task learning with compatible momentum contrastive losses and topology preservation terms.

## Key Results
- CTP outperforms other baselines in both cross-modal and multi-modal retrieval tasks on P9D benchmark
- Compatible momentum update (θc ← m · θc + (1-m)/2 · θt-1 + (1-m)/2 · θt) provides smooth knowledge integration
- Multi-modal fusion features show stronger anti-forgetting ability compared to cross-modal alignment
- Topology preservation maintains consistent sample relationships across tasks while allowing feature adjustment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compatible momentum contrast enables flexible adaptation by absorbing knowledge from both current and previous-task models
- Mechanism: The momentum model Mc updates parameters through synchronous absorption from Mt and Mt-1, allowing smooth feature adjustment while maintaining old knowledge
- Core assumption: The model can effectively integrate knowledge from both models without catastrophic forgetting or instability
- Break condition: If momentum coefficient m is too high, model may rely too heavily on old knowledge and fail to adapt to new tasks

### Mechanism 2
- Claim: Topology preservation maintains consistent sample relationships across tasks, preventing catastrophic forgetting
- Mechanism: Constrains mini-batch sample relationships of current and previous-task models to be consistent using cross-entropy between similarity distributions
- Core assumption: Preserving embedding space topology is crucial for maintaining knowledge across tasks
- Break condition: If similarity distributions diverge significantly, model may fail to preserve topology and suffer catastrophic forgetting

### Mechanism 3
- Claim: Multi-modal fusion feature has strong anti-forgetting ability compared to cross-modal alignment
- Mechanism: Multi-modal encoder fuses information from both image and text encoders, creating more robust representation less susceptible to forgetting
- Core assumption: Redundancy and complementarity of multi-modal information help resist forgetting of class attributes
- Break condition: If multi-modal encoder fails to effectively fuse information, model may suffer catastrophic forgetting

## Foundational Learning

- **Vision-Language Pretraining (VLP)**: Foundation for learning transferable image-text embeddings crucial for continual learning task. Quick check: What are the two main paradigms of VLP and how do they differ in terms of modal interaction and retrieval efficiency?

- **Continual Learning**: Core challenge addressed by paper. Quick check: What are the three main categories of continual learning methods and how do they differ in terms of their approach to preventing catastrophic forgetting?

- **Momentum Contrast**: Key component of proposed solution. Quick check: How does momentum contrast differ from traditional contrastive learning, and what are its benefits in terms of stability and efficiency?

## Architecture Onboarding

- **Component map**: Image Encoder (ViT-B/16) -> Text Encoder (BERT-base) -> Multi-modal Encoder (cross-attention) -> Compatible Momentum Model -> Topology Preservation Module

- **Critical path**:
  1. Load and preprocess image-text pairs
  2. Encode images and texts using respective encoders
  3. Fuse features using multi-modal encoder
  4. Apply compatible momentum update
  5. Preserve topology relationships
  6. Compute loss and update parameters

- **Design tradeoffs**: Memory vs. Performance (memory buffer improves performance but increases storage costs); Complexity vs. Efficiency (proposed solution adds complexity but aims to achieve better performance without significant training burden)

- **Failure signatures**: If model fails to converge, check momentum coefficient m and temperature parameter τ; if suffers from catastrophic forgetting, verify topology preservation module is working correctly; if performance degrades over time, examine compatible momentum update mechanism

- **First 3 experiments**:
  1. Implement basic VLP model without continual learning mechanisms and evaluate performance on single task
  2. Add compatible momentum update mechanism and evaluate impact on performance and stability
  3. Incorporate topology preservation module and assess effect on preventing catastrophic forgetting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CTP performance compare to other continual learning methods when task order is reversed?
- Basis: Paper mentions check experiment with reversed task order showing CTP consistently outperforms other methods
- Why unresolved: Paper doesn't provide detailed performance metrics for CTP and other methods under reversed task order
- What evidence would resolve it: Detailed performance metrics (e.g., TR@1, IR@1, Rm, mAP@1) for CTP and other methods under reversed task order

### Open Question 2
- Question: How does choice of momentum parameter (m) affect CTP performance in different continual learning stages?
- Basis: Paper mentions model is robust to momentum parameter selection but lacks detailed analysis
- Why unresolved: Paper doesn't provide comprehensive analysis of impact of different m values, especially in different continual learning stages
- What evidence would resolve it: Detailed analysis of CTP performance with different momentum parameters at different continual learning stages

### Open Question 3
- Question: How does topology preservation mechanism contribute to CTP's superior performance compared to other continual learning methods?
- Basis: Paper mentions topology preservation keeps consistent sample relationships across tasks
- Why unresolved: Paper doesn't provide detailed analysis of how topology preservation specifically contributes to superior performance
- What evidence would resolve it: Detailed analysis of impact of topology preservation mechanism on CTP performance through ablation studies or comparisons with methods not using topology preservation

## Limitations
- Evaluation relies entirely on single proprietary dataset (P9D) that is not publicly available, limiting external validation
- Lacks ablation studies on relative importance of compatible momentum contrast versus topology preservation components
- Computational requirements (4 A100 GPUs for 3 days) and memory overhead may limit practical deployment

## Confidence
- **High confidence**: Core algorithmic framework (compatible momentum update and topology preservation) is clearly specified with mathematical formulations
- **Medium confidence**: Performance claims on cross-modal retrieval based on internal evaluation with no comparison to established benchmarks
- **Low confidence**: Claims about CTP's advantages over traditional continual learning methods in other domains not empirically validated

## Next Checks
1. Implement and validate the compatible momentum update mechanism independently on a public vision-language dataset like COCO with task-splitting
2. Recreate the topology preservation evaluation by implementing cross-modal and same-modal topology preservation losses and measuring KL divergence between similarity distributions across task transitions
3. Test multi-modal fusion anti-forgetting claim by designing experiment comparing catastrophic forgetting rates between cross-modal alignment and multi-modal fusion features on public continual learning benchmark