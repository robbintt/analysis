---
ver: rpa2
title: 'TIM: Teaching Large Language Models to Translate with Comparison'
arxiv_id: '2307.04408'
source_url: https://arxiv.org/abs/2307.04408
tags:
- translation
- language
- data
- output
- comparison
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes TIM, a novel framework to fine-tune open-source
  large language models for translation tasks using examples in comparison. TIM constructs
  output comparison and preference comparison data to guide the model's learning,
  and introduces a preference loss as regularization.
---

# TIM: Teaching Large Language Models to Translate with Comparison

## Quick Facts
- arXiv ID: 2307.04408
- Source URL: https://arxiv.org/abs/2307.04408
- Reference count: 8
- Primary result: TIM outperforms existing instruction-tuning methods on WMT2022 translation tasks

## Executive Summary
TIM (Translation with Comparison) is a novel framework for fine-tuning open-source large language models for translation tasks. The key innovation is using comparison-based learning, where the model is trained on pairs of correct and incorrect translations with explicit error annotations. TIM introduces a preference loss as regularization, which helps the model learn to distinguish good translations from bad ones. Experiments show TIM achieves state-of-the-art quality estimation without references and demonstrates better zero-shot translation performance and stability compared to standard instruction tuning.

## Method Summary
TIM fine-tunes LLMs using three types of comparison data: dictionary-guided pairs showing multiple correct translations, error-guided pairs with explicit error annotations, and output comparison pairs. The framework combines a standard language modeling loss with a preference loss that computes token-level rewards to penalize incorrect translations. TIM is evaluated with different parameter-efficient tuning strategies (LoRA, FixEmb, Full) and shows consistent improvements across BLEU and COMET metrics on WMT2022 test sets.

## Key Results
- TIM achieves state-of-the-art quality estimation without references on WMT2022 test sets
- Demonstrates better zero-shot translation performance compared to standard instruction tuning
- Shows improved stability in prompt choice across different parameter tuning strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The preference loss regularizes the model by explicitly penalizing incorrect translations relative to correct ones.
- Mechanism: The preference loss computes a scalar reward for each token in a pair of outputs (correct vs. incorrect) and encourages the model to assign higher reward to the correct translation.
- Core assumption: The reward model can accurately distinguish correct from incorrect translations based on context and dictionary guidance.
- Evidence anchors:
  - [abstract] "We introduce an additional preference loss during fine-tuning, which is used to learn reward models, as regularization to penalize unexpected outputs."
  - [section 3.3] "The overall loss function for tuning the model is L = Llm + λLpl, where λ is a coefficient of the preference learning loss."
- Break condition: If the reward model cannot reliably discriminate correct vs. incorrect translations, the preference loss will not effectively guide learning.

### Mechanism 2
- Claim: Dictionary-guided data helps the model understand valid lexical choices by showing multiple correct translations for the same source sentence.
- Mechanism: By providing word alignments from a bilingual dictionary, the model learns that different lexical choices can be correct depending on context.
- Core assumption: The word alignments and multiple references are accurate and cover the variability in correct translations.
- Evidence anchors:
  - [section 3.2] "We use the word alignments as a note added to the input. As shown in Figure 2, for the same input sentence..., with the note containing different word alignments, the outputs of Example 1 and Example 2 are different."
  - [section 5.3] "Moreover, the results of ① ② and ④ show that output comparison is more crucial than preference comparison."
- Break condition: If the dictionary alignments are noisy or the references are too similar, the model may not learn meaningful distinctions.

### Mechanism 3
- Claim: Error-guided data teaches the model to recognize and avoid specific translation errors by showing explicit error annotations.
- Mechanism: By comparing a correct translation with an error-annotated incorrect one, the model learns patterns that lead to errors and avoids them.
- Core assumption: The error annotations are accurate and cover common translation error types.
- Evidence anchors:
  - [section 3.2] "In addition, inspired by Jiao et al. (2023), we introduce translations with error annotations... For correct input-output pairs, the added notes indicate no mistakes in the references, while the notes of incorrect input-output pairs indicate detailed translation errors."
  - [section 5.3] "In particular, the removal of error-guided data (i.e., ③) results in a greater performance drop than the removal of dictionary-guided data (i.e., ②)."
- Break condition: If error annotations are sparse, incorrect, or do not match real-world error patterns, the model may not generalize well.

## Foundational Learning

- Concept: Cross-entropy loss for language modeling
  - Why needed here: TIM still uses the standard language modeling loss Llm = −1/|y| Σ log p(y_i | c, x) as the base objective before adding preference loss.
  - Quick check question: What is the difference between the language modeling loss and the preference loss in TIM?

- Concept: Token-level reward modeling
  - Why needed here: The preference loss computes a reward at each token position to guide the model's generation.
  - Quick check question: How does the preference loss use the hidden states of tokens to compute rewards?

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: TIM experiments with LoRA as a parameter-efficient tuning strategy.
  - Quick check question: What is the main advantage of LoRA compared to full fine-tuning in terms of memory and storage?

## Architecture Onboarding

- Component map:
  Base LLM (BLOOMZ-7b-mt or LLaMA-7b) -> Training data (standard + dictionary-guided + error-guided) -> Preference loss head -> Optimizer (AdamW, lr=2e-5)

- Critical path:
  1. Load and preprocess training data (add dictionary/error notes)
  2. Forward pass through LLM to generate output
  3. Compute language modeling loss (standard cross-entropy)
  4. Compute preference loss using reward model on token pairs
  5. Backpropagate combined loss (λ=0.5)
  6. Update model parameters (LoRA, FixEmb, or Full)

- Design tradeoffs:
  - LoRA vs. FixEmb vs. Full: memory usage vs. performance
  - Preference loss coefficient λ: too high may dominate; too low may be ineffective
  - Dictionary quality: noisy alignments may confuse the model

- Failure signatures:
  - Model diverges or produces low-quality translations: check preference loss scaling and reward model stability
  - No improvement over baselines: verify dictionary/error annotations are accurate and diverse
  - Overfitting to training data: try smaller λ or more regularization

- First 3 experiments:
  1. Train with standard translation data only (baseline TIM without comparisons)
  2. Add dictionary-guided data and measure improvement on De⇒En
  3. Add error-guided data and compare against dictionary-only version

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the size of the language model affect the performance of TIM in zero-shot translation tasks?
- Basis in paper: [explicit] The paper mentions that as the size of the language model increases, the translation performance of the models fine-tuned with TIM gradually improves, with more pronounced improvement in smaller models.
- Why unresolved: The paper provides a general observation but lacks a detailed analysis of the relationship between model size and performance in zero-shot translation.
- What evidence would resolve it: Conducting a comprehensive study with models of varying sizes and analyzing their performance in zero-shot translation tasks.

### Open Question 2
- Question: What is the impact of different types of translation errors on the performance of TIM?
- Basis in paper: [explicit] The paper introduces error-guided data to provide comparison signals for model learning, but the specific impact of different types of errors is not explored.
- Why unresolved: The paper does not provide a detailed analysis of how different types of translation errors affect the performance of TIM.
- What evidence would resolve it: Conducting experiments with different types of translation errors and analyzing their impact on TIM's performance.

### Open Question 3
- Question: How does the quality of the reference translations used in dictionary-guided data affect the performance of TIM?
- Basis in paper: [explicit] The paper mentions that the quality of reference translations can affect the performance of TIM, but the specific relationship is not explored.
- Why unresolved: The paper does not provide a detailed analysis of how the quality of reference translations affects TIM's performance.
- What evidence would resolve it: Conducting experiments with reference translations of varying quality and analyzing their impact on TIM's performance.

## Limitations
- Evaluation limited to specific language pairs (EN⇔DE, EN⇔ZH) which may not generalize to all language combinations
- Performance depends heavily on quality of external resources (bilingual dictionaries and human-annotated errors)
- Limited validation of reward model robustness across different error types and semantic nuances

## Confidence

**High Confidence**: The core claim that TIM outperforms existing instruction-tuning baselines on WMT2022 test sets is well-supported by the experimental results, with clear improvements in BLEU and COMET scores across multiple language pairs and parameter-efficient tuning strategies.

**Medium Confidence**: The mechanism claims regarding how dictionary-guided and error-guided data contribute to learning are supported by ablation studies, but the paper could benefit from more detailed analysis of which specific error types or dictionary patterns drive the improvements.

**Low Confidence**: The assertion that TIM achieves state-of-the-art results on quality estimation without references needs more rigorous validation, as this capability is mentioned but not extensively tested across diverse evaluation scenarios.

## Next Checks

1. **Cross-lingual generalization test**: Evaluate TIM on additional language pairs beyond EN⇔DE and EN⇔ZH, particularly low-resource language combinations, to assess the framework's broader applicability.

2. **Ablation on error type coverage**: Systematically test how TIM performs when the error-guided data covers different distributions of error types (e.g., lexical errors vs. syntactic errors) to understand which error patterns are most beneficial for learning.

3. **Robustness to dictionary quality**: Conduct controlled experiments where dictionary alignments are intentionally degraded (e.g., by adding noise or reducing coverage) to quantify how sensitive TIM's performance is to the quality of the comparison data.