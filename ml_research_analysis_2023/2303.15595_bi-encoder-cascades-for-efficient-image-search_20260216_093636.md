---
ver: rpa2
title: Bi-Encoder Cascades for Efficient Image Search
arxiv_id: '2303.15595'
source_url: https://arxiv.org/abs/2303.15595
tags:
- search
- cascade
- image
- images
- cascades
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high computational cost of neural encoders
  in text-image retrieval (TIR) systems. The authors propose a cascade approach that
  uses progressively larger and more expensive encoders to progressively filter and
  rerank images.
---

# Bi-Encoder Cascades for Efficient Image Search

## Quick Facts
- arXiv ID: 2303.15595
- Source URL: https://arxiv.org/abs/2303.15595
- Reference count: 27
- Key outcome: Reduces lifetime image encoding costs by over 3x while maintaining search quality (Recall@10)

## Executive Summary
This paper addresses the high computational cost of neural encoders in text-image retrieval (TIR) systems by proposing a cascade approach that uses progressively larger and more expensive encoders to progressively filter and rerank images. The authors introduce a cascading algorithm that leverages a "small-world search scenario" to reduce lifetime image encoding costs by over 3x while maintaining search quality. The algorithm works by pre-computing embeddings for all images with a cheap encoder, then using more expensive encoders to rerank the top results for each query. Experiments show the cascaded model achieves up to 6x speedup over uncascaded models with no reduction in Recall@10.

## Method Summary
The paper proposes a cascade approach using progressively larger and more expensive encoders to filter and rerank images in text-image retrieval systems. The method involves pre-computing embeddings for all images with a cheap encoder, then using more expensive encoders to rerank the top results for each query. The authors use CLIP's ViT-B/16 as baseline and create cascades with ViT-B/32 as the cheap encoder and ViT-B/16 or ViT-L/14 as the expensive encoder. They evaluate on MSCOCO validation (5k samples) and Flickr30k (32k samples) datasets, measuring Recall@1, Recall@5, Recall@10, lifetime speedup for 2-level cascades, and query speedup for deeper cascades.

## Key Results
- Cascaded models achieve up to 6x speedup over uncascaded models with no reduction in Recall@10
- The approach reduces lifetime image encoding costs by over 3x while maintaining search quality
- Deeper cascades can mitigate increased latency of early queries, though they offer no reduced lifetime costs over 2-level cascades

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cascading cheaper encoders first reduces the lifetime cost of expensive encoders.
- Mechanism: Precompute embeddings for all images with a cheap encoder (ğ¼ğ‘ ), then use more expensive encoders only on the top-m results from the cheap encoder.
- Core assumption: A small fraction (ğ‘“ â‰ª 1) of images are ever included in search results over the lifetime of the search engine.
- Evidence anchors:
  - [abstract] "This paper addresses the high computational cost of neural encoders in text-image retrieval (TIR) systems... reducing lifetime image encoding costs by over 3x while maintaining search quality"
  - [section] "In practice, it is possible for over 90% of all documents in D to never be included in any search result over the lifetime of a large-scale search engine"
- Break condition: If the lifetime return fraction ğ‘“ approaches 1 (i.e., nearly all images are frequently searched), the computational savings disappear.

### Mechanism 2
- Claim: Cascading encoders can improve search quality by using coarse-grained filtering followed by fine-grained reranking.
- Mechanism: The cheap encoder (ğ¼ğ‘ ) uses coarser image tiling (32x32 vs 16x16), potentially better at approximate filtering, followed by the expensive encoder (ğ¼1) providing more precise reranking.
- Core assumption: Coarse-grained embeddings can effectively filter search results, improving the quality of the top-m images selected for reranking.
- Evidence anchors:
  - [abstract] "Surprisingly, the cascaded model achieves at the same time consistently higher Recall@ ğ‘˜ than the uncascaded model"
  - [section] "One explanation may be that ViT-B/32 initially processes input images into 32x32 tiles... it may offer superior approximate filtering of search results"
- Break condition: If the cheap encoder's filtering is too coarse and misses relevant images, or if the expensive encoder's reranking doesn't sufficiently improve the top results.

### Mechanism 3
- Claim: Deeper cascades can reduce query latency for expensive final encoders.
- Mechanism: Use intermediate encoders to progressively narrow the search space, so the most expensive encoder only processes a small subset of images.
- Core assumption: The speedup factor (ğ‘¡ğ‘  + ğ‘“ ğ‘¡1)/ğ‘¡1 exceeds 1 for a 2-level cascade to be cheaper than a 1-level cascade.
- Evidence anchors:
  - [abstract] "The authors also investigate deeper cascades to mitigate increased latency of early queries"
  - [section] "Hence, deep cascades can mitigate the increased latency of early queries in 2-level cascades"
- Break condition: If the latency reduction from skipping expensive encoders on most queries is outweighed by the overhead of running multiple encoders on some queries.

## Foundational Learning

- Concept: Text-Image Retrieval (TIR) systems and the role of neural encoders
  - Why needed here: Understanding how TIR systems work and why neural encoders are computationally expensive is fundamental to grasping the problem being solved
  - Quick check question: What is the main difference between bi-encoders and cross-encoders in TIR systems, and why does this paper focus on bi-encoders?

- Concept: Model cascading and computational cost tradeoffs
  - Why needed here: The paper's core contribution is a cascading algorithm that trades off computational cost against search quality, so understanding these tradeoffs is essential
  - Quick check question: How does the paper define "lifetime computational cost" and why is this metric more relevant than single-query latency for large-scale search engines?

- Concept: Recall@ğ‘˜ metric and its significance in information retrieval
  - Why needed here: The paper uses Recall@ğ‘˜ to measure search quality, so understanding what this metric measures and why it's important is crucial for evaluating the results
  - Quick check question: What does Recall@10 measure in the context of this paper, and why might small increases in k significantly improve this metric?

## Architecture Onboarding

- Component map: Text query ğ‘ -> Cheap encoder ğ¼ğ‘  -> Top-m images -> Expensive encoder ğ¼1 -> Reranked top-k images
- Critical path:
  1. Precompute ğ‘½D with ğ¼ğ‘  (build time)
  2. Query ğ¼ğ‘  with ğ‘ to get top-m results
  3. Recompute ğ‘½Dğ‘š with ğ¼1 (query time)
  4. Rerank top-m results with ğ‘½Dğ‘š and return top-k
- Design tradeoffs:
  - Encoder choice: Balancing encoding speed vs. embedding quality
  - m value: Larger m improves recall but increases computation
  - Number of cascade levels: More levels can reduce latency but increase implementation complexity
- Failure signatures:
  - Recall@ğ‘˜ drops significantly when using cascade vs. single expensive encoder
  - Query latency increases despite cascade due to expensive encoders being run too frequently
  - Memory usage spikes from storing multiple sets of embeddings
- First 3 experiments:
  1. Implement 2-level cascade with ViT-B/32 as cheap encoder and ViT-B/16 as expensive encoder, measure Recall@ğ‘˜ and lifetime speedup vs. baseline
  2. Vary the m value (top-m images selected by cheap encoder) and measure its impact on Recall@ğ‘˜ and computational cost
  3. Implement 3-level cascade and measure query latency vs. 2-level cascade to validate the latency reduction claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal depth and configuration of cascades for maximizing the tradeoff between lifetime cost reduction and query latency?
- Basis in paper: [explicit] The paper investigates 2-level and 3-level cascades but notes that deeper cascades offer no reduced lifetime costs over 2-level cascades, though they may speed up individual queries
- Why unresolved: The paper only experiments with up to 3-level cascades and provides theoretical analysis but doesn't explore the full space of cascade depths and configurations
- What evidence would resolve it: Empirical evaluation of cascades with varying depths (4+ levels) and different configurations of encoder selections and cutoff values (m_j) to identify optimal configurations for different use cases

### Open Question 2
- Question: Why do cascaded models achieve higher Recall@ğ‘˜ than uncascaded models despite using cheaper initial encoders?
- Basis in paper: [explicit] The authors note that cascaded models consistently achieve higher Recall@ğ‘˜ than uncascaded models, speculating that the coarser 32x32 tiling of ViT-B/32 might offer superior approximate filtering
- Why unresolved: The authors acknowledge this observation but don't provide definitive explanation or experimental validation of the hypothesis
- What evidence would resolve it: Controlled experiments comparing the filtering behavior of different encoders, ablation studies isolating the effects of different encoder architectures, and analysis of how early-stage filtering impacts final retrieval quality

### Open Question 3
- Question: How does the lifetime return fraction (f) vary across different search scenarios and what impact does this have on achievable speedups?
- Basis in paper: [explicit] The authors state that their observations rely on Assumption 1 (f â‰ˆ 0.1) and acknowledge that different search scenarios likely vary in f, achieving accordingly different speedups
- Why unresolved: The paper provides only anecdotal evidence for their choice of f = 10% without comprehensive analysis of how this parameter varies in real-world scenarios
- What evidence would resolve it: Large-scale analysis of search logs from diverse image search engines to empirically measure f across different domains, time periods, and user populations, along with corresponding measurements of achieved speedups

## Limitations

- The paper's computational cost reduction relies heavily on the assumption that only a small fraction of images are ever retrieved over a search engine's lifetime, which may not hold for specialized image collections
- The empirical results are based on limited datasets (MSCOCO and Flickr30k) and may not generalize to diverse real-world image collections
- The analysis focuses on CPU time and doesn't account for potential GPU acceleration benefits or memory constraints from storing multiple embedding sets

## Confidence

- **High Confidence**: The basic mechanism of cascading cheaper encoders first to reduce lifetime computational costs is well-established and theoretically sound
- **Medium Confidence**: The empirical results showing improved Recall@10 with cascading are promising but based on limited datasets
- **Low Confidence**: The claim about coarse-grained embeddings providing superior approximate filtering requires further investigation

## Next Checks

1. **Validate the Lifetime Return Fraction Assumption**: Analyze the actual retrieval frequency distribution on MSCOCO and Flickr30k datasets to empirically determine if the 10% lifetime return fraction holds.

2. **Ablation Study on Coarse vs Fine Embeddings**: Implement a controlled experiment comparing ViT-B/32 (32x32 tiles) against a modified ViT-B/16 with forced 32x32 tiling to isolate whether the filtering improvement comes from tile size or the encoder architecture itself.

3. **Memory and Storage Cost Analysis**: Measure the total memory footprint of storing pre-computed embeddings for all images with multiple encoders across different dataset sizes, comparing this against the computational savings to determine the true cost-benefit tradeoff.