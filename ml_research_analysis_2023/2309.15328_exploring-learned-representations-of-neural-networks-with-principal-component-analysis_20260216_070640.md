---
ver: rpa2
title: Exploring Learned Representations of Neural Networks with Principal Component
  Analysis
arxiv_id: '2309.15328'
source_url: https://arxiv.org/abs/2309.15328
tags:
- neural
- representations
- k-nn
- figure
- learned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We analyze learned feature representations of a ResNet-18 trained
  on CIFAR-10 by projecting activations through PCA and evaluating k-NN, NCC, and
  SVM classifiers on varying numbers of principal components. We find that SVM performance
  consistently matches or exceeds the other two models, and the first ~100 PCs determine
  maximum accuracy for k-NN and NCC.
---

# Exploring Learned Representations of Neural Networks with Principal Component Analysis

## Quick Facts
- arXiv ID: 2309.15328
- Source URL: https://arxiv.org/abs/2309.15328
- Authors: 
- Reference count: 14
- We analyze learned feature representations of a ResNet-18 trained on CIFAR-10 by projecting activations through PCA and evaluating k-NN, NCC, and SVM classifiers on varying numbers of principal components.

## Executive Summary
This paper investigates learned feature representations in neural networks through PCA-based analysis of ResNet-18 activations across CIFAR-10. By evaluating k-NN, nearest class center (NCC), and SVM classifiers on varying numbers of principal components, the authors reveal that SVM consistently outperforms the other two models and that only 20-40% of variance is needed for 90% classification accuracy in later layers. The results support the phenomenon of intermediate neural collapse and demonstrate that simpler linear models can robustly exploit learned representations when properly analyzed through variance decomposition.

## Method Summary
The authors extract activations from each residual block of a pretrained ResNet-18 on CIFAR-10, standardize them, and apply PCA to project onto varying numbers of principal components. They then train and evaluate three classifier probes (k-NN with k=10, NCC, and SVM) on these projected representations, measuring classification accuracy as a function of PC count. This process is repeated across all residual blocks to track how representational properties evolve through the network. The minimum number of PCs achieving 90% of maximum accuracy and the variance explained by those PCs are computed and analyzed.

## Key Results
- SVM performance consistently matches or exceeds k-NN and NCC across all layers
- In later layers, only 20-40% of variance is needed for 90% classification accuracy, indicating feature collapse
- Low-variance components contain meaningful information for SVM classification
- Intermediate neural collapse is supported by behavioral transitions between residual blocks 4 and 5

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PCA can reveal meaningful low-dimensional structures in DNN feature spaces that align with classification boundaries
- Mechanism: Principal components capture directions of maximum variance in activation patterns. The first ~100 PCs fully determine k-NN and NCC performance because these classifiers rely on distances and centroids in the feature space, which are dominated by high-variance directions. SVM leverages both high and low-variance components because its decision boundary optimization is less sensitive to variance magnitude
- Core assumption: The classification-relevant information in intermediate layers is predominantly aligned with the top principal components, and low-variance components still contain discriminative information for linear classifiers
- Evidence anchors:
  - [abstract] "We show that in certain layers, as little as 20% of the intermediate feature-space variance is necessary for high-accuracy classification"
  - [section] "In the latter half of the network, the PCs necessary for 90% of the classification accuracy account for only 20%-40% of the variance"
  - [corpus] Weak evidence - corpus papers focus on neural collapse but don't directly address PCA-based variance analysis

### Mechanism 2
- Claim: Intermediate neural collapse manifests as a transition in classifier behavior between residual blocks 4 and 5
- Mechanism: As representations collapse toward class means, the variance becomes concentrated in directions between class centers. This causes k-NN and NCC to achieve near-perfect accuracy with few PCs, while SVM's ability to use low-variance components becomes less relevant in fully collapsed states
- Core assumption: The transition to "full collapse" creates a sharp change in the geometry of feature space that can be detected by comparing classifier performance
- Evidence anchors:
  - [abstract] "We relate our findings to neural collapse and provide partial evidence for the related phenomenon of intermediate neural collapse"
  - [section] "all three models exhibit identical profiles for blocks 5 through 8 and that, excluding block 5, they require only 2-3 PCs to attain 90% of the accuracy of the DNN"
  - [corpus] Weak evidence - corpus papers discuss neural collapse but don't analyze intermediate layers with PCA

### Mechanism 3
- Claim: SVM's superior performance across all layers indicates that learned representations maintain meaningful linear separability
- Mechanism: Even as k-NN and NCC performance saturates with the first ~100 PCs, SVM continues to improve because it can exploit information in low-variance subspaces. This suggests the feature space has structure that linear models can leverage beyond what distance-based classifiers capture
- Core assumption: The learned representations contain linearly separable structure that persists across multiple scales of variance
- Evidence anchors:
  - [abstract] "We show that the SVM matches or outperforms the k-NN and NCC across the network"
  - [section] "The SVM (Figure 2c) performs similarly to the k-NN for the first âˆ¼100 PCs, but continues to improve in accuracy as the number of PCs increases"
  - [corpus] No direct evidence - corpus focuses on neural collapse phenomena rather than classifier comparison

## Foundational Learning

- Concept: Principal Component Analysis and variance decomposition
  - Why needed here: Understanding how PCA captures variance structure is essential to interpreting why certain numbers of components determine classifier performance
  - Quick check question: What percentage of total variance is typically captured by the first 100 PCs in a 2000-dimensional layer activation space?

- Concept: Neural collapse and its geometric implications
  - Why needed here: The paper relates observations to neural collapse, so understanding how class means and within-class variance change across layers is crucial
  - Quick check question: What are the four properties of neural collapse as defined in the literature?

- Concept: Classifier probe methodology
  - Why needed here: The experimental approach uses different classifiers to probe learned representations, requiring understanding of how each classifier's decision mechanism interacts with feature space geometry
  - Quick check question: How does k-NN decision boundary differ from SVM decision boundary in terms of reliance on feature space variance?

## Architecture Onboarding

- Component map: CIFAR-10 data -> ResNet-18 -> Layer activations -> Standardization -> PCA -> Projected representations -> Classifier probes -> Performance analysis

- Critical path:
  1. Load pretrained ResNet-18 activations for each residual block
  2. Standardize training activations (zero mean, unit variance)
  3. Compute PCA on standardized activations
  4. Project activations to varying numbers of PCs
  5. Train and evaluate each classifier
  6. Analyze performance patterns across layers and PC counts

- Design tradeoffs:
  - PCA vs. non-linear dimensionality reduction: PCA provides interpretable variance-based analysis but may miss curved manifold structure
  - Number of PC thresholds: 90% variance vs. 90% accuracy - the paper shows these can diverge significantly
  - Classifier selection: Using multiple classifiers reveals different aspects of representation quality

- Failure signatures:
  - k-NN accuracy decreasing with more PCs indicates overfitting to noise
  - All classifiers requiring similar PC counts suggests collapse has occurred
  - SVM not improving beyond certain PC count suggests representation has reached its representational capacity

- First 3 experiments:
  1. Reproduce the PCA-based analysis on a different CNN architecture (e.g., VGG-16) to test generalizability
  2. Compare PCA analysis with non-linear dimensionality reduction methods (t-SNE, UMAP) on the same data
  3. Apply the same classifier probe methodology to a network trained with different objectives (contrastive learning, self-supervised)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the partially collapsed subspace identified by k-NN and NCC performance correlate with specific architectural features or training dynamics in ResNet-18?
- Basis in paper: [explicit] The paper observes that k-NN and NCC performance is determined by the first ~100 PCs, while SVM performance continues to improve with more PCs, suggesting a partially collapsed subspace.
- Why unresolved: The paper provides evidence of this subspace but does not investigate its relationship to network architecture or training process.
- What evidence would resolve it: Experiments varying ResNet architecture (e.g., number of layers, width) or training hyperparameters while tracking k-NN/NCC performance and PCA dimensionality would clarify this relationship.

### Open Question 2
- Question: Is the low-variance subspace containing meaningful information for SVM classification task-specific or a general property of learned representations?
- Basis in paper: [explicit] The paper notes that low-variance components improve SVM performance, indicating they contain meaningful information beyond noise.
- Why unresolved: The experiments are conducted only on CIFAR-10, so it's unclear if this phenomenon generalizes to other datasets or tasks.
- What evidence would resolve it: Testing the same PCA and SVM approach on diverse datasets (e.g., different image domains, text, or structured data) would determine if this is a universal property.

### Open Question 3
- Question: What is the theoretical explanation for why affine-linear models like SVM outperform non-linear models like k-NN and NCC on learned representations in later network layers?
- Basis in paper: [explicit] The paper observes that SVM consistently matches or exceeds k-NN and NCC performance, particularly in later layers.
- Why unresolved: The paper presents this as an empirical finding without providing theoretical justification for this behavior.
- What evidence would resolve it: Developing a mathematical framework that connects neural collapse properties to classifier performance, or conducting ablation studies on network weights and activation distributions, could explain this phenomenon.

## Limitations

- The analysis is based on a single pretrained ResNet-18 model on CIFAR-10, limiting generalizability to other architectures and datasets
- PCA assumes linear variance structure which may miss non-linear manifold characteristics of learned representations
- The comparison of classifier performance relies on specific hyperparameter choices that could affect observed patterns
- Evidence for intermediate neural collapse is indirect and based on behavioral patterns rather than direct geometric measurements

## Confidence

- **High confidence**: The observation that SVM consistently outperforms or matches k-NN and NCC across layers is directly observable from the experimental results and has clear mechanistic explanation through SVM's ability to leverage low-variance components.
- **Medium confidence**: The claim about intermediate neural collapse between residual blocks 4 and 5 is supported by behavioral transitions but lacks direct measurement of the geometric properties that define neural collapse.
- **Low confidence**: The generalization of these specific variance-based patterns to other architectures, datasets, or training objectives remains untested and represents an extrapolation beyond the presented evidence.

## Next Checks

1. **Architecture generalization test**: Apply the same PCA-based classifier probe methodology to VGG-16 and DenseNet-121 on CIFAR-10 to verify whether the observed patterns of SVM superiority and variance collapse are architecture-specific or general phenomena.

2. **Direct neural collapse measurement**: Complement the behavioral analysis with direct geometric measurements of class means and within-class covariance matrices across layers to provide quantitative evidence for intermediate neural collapse rather than relying solely on classifier performance patterns.

3. **Adversarial robustness analysis**: Test whether the representations that perform well under PCA-based analysis maintain their properties under adversarial perturbations, which would validate whether the observed variance structure captures truly meaningful classification-relevant features.