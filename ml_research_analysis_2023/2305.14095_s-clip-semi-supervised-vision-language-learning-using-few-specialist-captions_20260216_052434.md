---
ver: rpa2
title: 'S-CLIP: Semi-supervised Vision-Language Learning using Few Specialist Captions'
arxiv_id: '2305.14095'
source_url: https://arxiv.org/abs/2305.14095
tags:
- learning
- clip
- image
- images
- s-clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces S-CLIP, a semi-supervised learning method
  for training CLIP models using limited paired image-text data and additional unpaired
  images. The key innovation lies in two pseudo-labeling strategies: caption-level
  pseudo-labels derived from optimal transport between unpaired and paired images,
  and keyword-level pseudo-labels using partial label learning with candidate keyword
  sets.'
---

# S-CLIP: Semi-supervised Vision-Language Learning using Few Specialist Captions

## Quick Facts
- **arXiv ID:** 2305.14095
- **Source URL:** https://arxiv.org/abs/2305.14095
- **Reference count:** 40
- **Primary result:** S-CLIP achieves 10% better zero-shot classification accuracy and 4% better image-text retrieval on remote sensing benchmarks compared to supervised CLIP fine-tuning while using 3× fewer image-text pairs.

## Executive Summary
S-CLIP introduces a semi-supervised learning framework for training CLIP models in specialist domains using limited paired image-text data combined with additional unpaired images. The method employs two pseudo-labeling strategies: caption-level pseudo-labels derived from optimal transport between unpaired and paired images, and keyword-level pseudo-labels using partial label learning with candidate keyword sets. This approach significantly improves CLIP performance across multiple specialist domains including remote sensing, fashion, scientific figures, and comics, while requiring substantially less labeled data than traditional supervised fine-tuning.

## Method Summary
S-CLIP combines a pre-trained CLIP model with two pseudo-labeling strategies to leverage unpaired images in specialist domains. The caption-level strategy uses optimal transport to compute soft probability distributions over paired images for each unpaired image, creating pseudo-labels that prevent model collapse. The keyword-level strategy extracts keywords from the nearest paired image's caption and treats them as candidate labels using partial label learning. The final training objective combines the standard CLIP loss on paired data with both pseudo-label losses, weighted equally. This semi-supervised approach enables effective training with 3× fewer image-text pairs than supervised CLIP while maintaining or improving performance.

## Key Results
- S-CLIP achieves 10% better zero-shot classification accuracy and 4% better image-text retrieval on remote sensing benchmarks compared to supervised CLIP fine-tuning
- The method consistently outperforms supervised CLIP across four specialist domains (remote sensing, fashion, scientific figures, comics) while using 3× fewer image-text pairs
- Combining caption-level and keyword-level pseudo-labels yields better results than either approach alone, with caption-level pseudo-labels more beneficial for retrieval and keyword-level more beneficial for classification

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Caption-level pseudo-labels prevent model collapse by distributing learning weight across multiple similar images instead of concentrating on a single nearest neighbor.
- **Mechanism:** Optimal transport (OT) computes a soft probability distribution over all paired images based on their visual similarity, then uses this distribution as a pseudo-label target. This balances the flow of information and avoids overfitting to the nearest caption.
- **Core assumption:** Visually similar images share overlapping semantic content, and their captions can be combined to approximate the semantics of an unlabeled image.
- **Evidence anchors:**
  - [abstract] "The caption-level pseudo-label is given by a combination of captions of paired images, obtained by solving an optimal transport problem between unpaired and paired images."
  - [section 4.1] "We define the caption-level pseudo-label as a probability distribution over the labeled images... obtained by solving an optimal transport (OT) [24] problem."
  - [corpus] Weak correlation - related work focuses on remote sensing datasets but doesn't directly address caption-level OT.
- **Break condition:** If the optimal transport solver is given insufficient Sinkhorn iterations, the pseudo-labels collapse toward hard nearest-neighbor assignments, losing the regularization benefit.

### Mechanism 2
- **Claim:** Keyword-level pseudo-labels enable fine-grained local understanding by targeting individual words rather than full captions.
- **Mechanism:** For each unlabeled image, the nearest labeled image is found via OT assignments, and its caption keywords are used as a candidate set. Partial label learning then treats this candidate set as the supervision target, allowing the model to learn specific object or attribute concepts.
- **Core assumption:** Visually similar images share common keywords even when their full captions differ, enabling keyword-level alignment without requiring exact caption matches.
- **Evidence anchors:**
  - [abstract] "The keyword-level pseudo-label is given by a keyword in the caption of the nearest paired image, trained through partial label learning that assumes a candidate set of labels for supervision instead of the exact one."
  - [section 4.2] "We define the keyword-level pseudo-label as one of the keywords in the nearest paired image to an unlabeled image. This approach creates a candidate set of target keywords instead of a single exact one."
  - [corpus] No direct evidence - this specific keyword-level PLL approach for CLIP appears novel.
- **Break condition:** If keyword extraction fails to capture domain-relevant terms (e.g., extracting only generic words), the candidate sets become uninformative and training stalls.

### Mechanism 3
- **Claim:** Combining caption-level and keyword-level objectives leverages complementary global and local language understanding for both retrieval and classification.
- **Mechanism:** Caption-level loss optimizes for holistic text-image alignment (beneficial for retrieval), while keyword-level loss optimizes for component-level understanding (beneficial for classification). Their combination yields superior overall performance.
- **Core assumption:** Global caption semantics and local keyword semantics capture different aspects of visual understanding that are both necessary for strong vision-language models.
- **Evidence anchors:**
  - [abstract] "Combining them achieves the best of both worlds (Section 5.4)."
  - [section 5.4] "(a) Training objective. Caption-level pseudo-labels are more beneficial for image-text retrieval, while keyword-level pseudo-labels are more beneficial for zero-shot classification. Combining both yields the best results."
  - [corpus] Weak correlation - no corpus evidence directly supports this dual-objective combination claim.
- **Break condition:** If one objective dominates training (e.g., keyword loss overwhelms caption loss), the complementary benefit disappears and performance regresses toward single-objective results.

## Foundational Learning

- **Concept:** Optimal transport and Sinkhorn iteration
  - Why needed here: OT provides a principled way to compute soft alignments between unpaired and paired images, avoiding the hard nearest-neighbor collapse that naive pseudo-labeling suffers from.
  - Quick check question: What happens to the pseudo-label distribution when the number of Sinkhorn iterations is set to zero?

- **Concept:** Partial label learning (PLL)
  - Why needed here: PLL allows training on candidate keyword sets rather than exact labels, which is necessary because unlabeled images only share some keywords with their nearest labeled images.
  - Quick check question: How does PLL differ from standard multi-label classification in terms of the loss function?

- **Concept:** Contrastive learning with cross-modal alignment
  - Why needed here: CLIP's core mechanism relies on matching image and text embeddings in a shared space, and both pseudo-label strategies must respect this contrastive objective while incorporating unpaired data.
  - Quick check question: In CLIP's contrastive loss, what is the role of the temperature parameter τ?

## Architecture Onboarding

- **Component map:** CLIP base loss on paired data + caption-level pseudo-loss on unpaired data + keyword-level pseudo-loss on unpaired data. OT solver runs per batch to compute caption-level targets. Keyword extraction runs once per dataset to build candidate sets.
- **Critical path:** Embedding forward → OT cost matrix → Sinkhorn iteration → caption-level pseudo-labels → keyword extraction → partial label loss → total loss → backward pass. OT and keyword extraction are bottlenecks if not cached properly.
- **Design tradeoffs:** Caption-level OT provides soft regularization but adds computational overhead; keyword-level PLL adds granularity but depends on quality of keyword extraction. Using class names vs. YAKE keywords trades consistency for coverage.
- **Failure signatures:** Overfitting to training captions (validation loss diverges while training loss decreases), poor generalization to unseen datasets, keyword candidate sets dominated by stop words or irrelevant terms.
- **First 3 experiments:**
  1. Run with only caption-level OT (no keywords) to verify OT prevents collapse vs. hard nearest-neighbor baseline.
  2. Run with only keyword-level PLL (no OT) to verify keyword learning improves classification vs. caption-only baseline.
  3. Run full S-CLIP vs. supervised CLIP with 10% pairs to measure generalization gains on held-out specialist datasets.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How would S-CLIP perform with different keyword extraction methods beyond YAKE, such as supervised keyword extraction or transformer-based methods?
- **Basis in paper:** [explicit] The authors compare YAKE keywords to class names and find both work well, but class names provide more consistent improvement. They note YAKE provides both nouns and adjectives which help understand diverse semantics.
- **Why unresolved:** The paper only tests two keyword extraction methods (class names and YAKE). There are many other keyword extraction approaches that could potentially work better.
- **What evidence would resolve it:** Systematic comparison of S-CLIP performance using various keyword extraction methods including supervised approaches, transformer-based methods, and domain-specific keyword extraction techniques.

### Open Question 2
- **Question:** What is the optimal batch size for S-CLIP to balance computational efficiency with pseudo-label quality?
- **Basis in paper:** [inferred] The authors mention that the current pseudo-labeling approach assumes captions in a batch can capture semantics of unlabeled images, which may not hold for fine-grained contexts. They suggest increasing batch size could help.
- **Why unresolved:** The paper uses a fixed batch size of 32 paired and 32 unpaired images but doesn't explore how batch size affects performance or what the optimal size would be.
- **What evidence would resolve it:** Systematic experiments varying batch size to find the point where pseudo-label quality plateaus relative to computational cost, particularly for fine-grained classification tasks.

### Open Question 3
- **Question:** How does S-CLIP scale to extremely large specialist datasets where the distribution shift between labeled and unlabeled data becomes more severe?
- **Basis in paper:** [explicit] The authors demonstrate robustness to distribution shifts in several experiments but note that techniques from robust semi-supervised learning could further improve OT formulation when shifts are severe.
- **Why unresolved:** The paper tests distribution shifts but doesn't explore extreme cases where the shift might be so large that the OT formulation breaks down or requires additional regularization.
- **What evidence would resolve it:** Experiments on datasets with progressively larger distribution shifts, testing whether the OT formulation remains effective or requires modifications like domain adaptation techniques.

### Open Question 4
- **Question:** Can the keyword-level pseudo-labeling be extended to work with semantic embeddings of keywords rather than discrete keyword sets?
- **Basis in paper:** [inferred] The current keyword-level approach uses discrete keyword sets and partial label learning. The authors suggest future work could incorporate word hierarchies and related concepts.
- **Why unresolved:** The paper uses a discrete keyword approach but doesn't explore whether using semantic embeddings of keywords could provide richer supervision.
- **What evidence would resolve it:** Implementation and comparison of keyword-level pseudo-labeling using semantic embeddings (e.g., from word2vec or contextual embeddings) versus discrete keyword sets.

## Limitations

- **Computational overhead:** The optimal transport-based caption-level pseudo-labeling introduces significant computational cost through the Sinkhorn-Knopp iterations required per batch, potentially limiting practical deployment in resource-constrained settings.
- **Dataset-specific performance variability:** While S-CLIP shows consistent improvements across four specialist domains, the magnitude of gains varies considerably, and the paper doesn't provide statistical significance testing or analysis of which domain characteristics benefit most from each pseudo-labeling strategy.
- **Evaluation scope limitations:** The evaluation focuses on zero-shot classification and image-text retrieval but doesn't test transfer to downstream tasks like object detection, segmentation, or region-level understanding, leaving the effectiveness of keyword-level pseudo-labels on these tasks unexplored.

## Confidence

**High confidence:** The core finding that S-CLIP achieves comparable or superior performance to supervised CLIP while using 3× fewer image-text pairs is well-supported by systematic evaluation across multiple specialist domains. The ablation studies showing individual contributions of caption-level and keyword-level pseudo-labels provide strong evidence for the dual-objective approach.

**Medium confidence:** The claim that optimal transport prevents model collapse by distributing learning weight across multiple similar images is mechanistically sound but relies on qualitative reasoning rather than quantitative analysis of the pseudo-label distributions or empirical comparison with hard nearest-neighbor baselines.

**Low confidence:** The assertion that combining caption-level and keyword-level objectives achieves "the best of both worlds" lacks direct empirical support. While Table 1 shows combined performance is best, there's no analysis of whether the gains are truly complementary or simply additive, nor is there evidence that each objective targets the specific downstream tasks (retrieval vs. classification) as claimed.

## Next Checks

1. **Ablation on Sinkhorn iterations:** Systematically vary the number of Sinkhorn iterations in the OT solver to empirically verify that the soft pseudo-label distribution provides regularization benefits over hard nearest-neighbor assignments, and identify the point where computational cost outweighs performance gains.

2. **Cross-domain keyword generalization:** Evaluate S-CLIP's performance when the unpaired image distribution differs significantly from the paired training data, particularly focusing on whether keyword extraction remains effective for creating meaningful candidate sets when visual domains don't align perfectly.

3. **Downstream task transfer analysis:** Test S-CLIP-pretrained models on fine-grained downstream tasks like object detection, semantic segmentation, and region-level image-text retrieval to validate whether the keyword-level pseudo-labels indeed improve local understanding capabilities as mechanistically predicted.