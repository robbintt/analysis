---
ver: rpa2
title: 'CIDR: A Cooperative Integrated Dynamic Refining Method for Minimal Feature
  Removal Problem'
arxiv_id: '2312.08157'
source_url: https://arxiv.org/abs/2312.08157
tags:
- feature
- minimal
- cidr
- problem
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CIDR addresses the minimal feature removal problem in NLP by proposing
  a method to detect feature interactions and refine minimal feature sets. The core
  idea involves designing Cooperative Integrated Gradients (CIG) to detect feature
  interactions and transforming the problem into a knapsack problem.
---

# CIDR: A Cooperative Integrated Dynamic Refining Method for Minimal Feature Removal Problem

## Quick Facts
- arXiv ID: 2312.08157
- Source URL: https://arxiv.org/abs/2312.08157
- Reference count: 40
- Primary result: CIDR outperforms baselines in identifying more interpretable minimal feature sets across various models and datasets, with improvements in comprehensiveness and log-odds metrics

## Executive Summary
CIDR addresses the minimal feature removal problem in NLP by proposing a method to detect feature interactions and refine minimal feature sets. The core idea involves designing Cooperative Integrated Gradients (CIG) to detect feature interactions and transforming the problem into a knapsack problem. CIDR introduces an auxiliary Minimal Feature Refinement algorithm to determine the minimal feature set from numerous candidate sets. The method demonstrates superior performance in identifying interpretable minimal feature sets compared to existing baselines.

## Method Summary
CIDR works by first computing Cooperative Integrated Gradients (CIG) scores for all word pairs to detect feature interactions. These scores are then used to transform the minimal feature removal problem into a knapsack problem, where word pairs are items with weights equal to their CIG scores. A dynamic programming algorithm is used to find candidate minimal feature sets. Finally, an auxiliary refinement algorithm filters out false positive features by selecting only those that appear consistently across multiple perturbed runs.

## Key Results
- Outperforms baselines in identifying more interpretable minimal feature sets
- Improves comprehensiveness and log-odds metrics across various models and datasets
- Demonstrates effectiveness on SST-2, IMDB, and Rotten Tomatoes datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CIDR detects feature interactions by computing cooperative integrated gradients (CIG) that measure both individual and joint contributions of word pairs.
- Mechanism: CIG extends integrated gradients by including terms that capture the cooperative effect of two features while also accounting for their individual contributions when the other is removed.
- Core assumption: The cooperative effect can be approximated by adding individual gradients and a weighted term that captures their joint influence.
- Evidence anchors: [abstract] "Specifically, we design Cooperative Integrated Gradients (CIG) to detect interactions between features." [section 4.1] "We present the solution to model the feature interactions, namely CIG."
- Break condition: If the cooperative term does not reflect true interaction (e.g., when features are conditionally independent), the method will misidentify interactions.

### Mechanism 2
- Claim: The minimal feature removal problem is transformed into a knapsack problem where word pairs are items with weights equal to their CIG scores.
- Mechanism: By treating Sp_pos \ Sp_min as the knapsack and CIG scores as weights, CIDR uses a dynamic programming algorithm to find candidate minimal feature sets.
- Core assumption: The minimal feature set can be approximated by maximizing the number of high-weight word pairs under an upper bound constraint.
- Evidence anchors: [section 4.2] "we leverage Proposition 2 and 3 to approximate the minimum feature problem as a knapsack problem." [section 4.2] "we utilize CIG to assign scores for word pairs to detect feature interactions, then we regard the word pairs as items and CIG scores as corresponding weights in the knapsack problem."
- Break condition: If the upper bound U1 + U2 is too loose, the knapsack solution may include too many features, failing to minimize cardinality.

### Mechanism 3
- Claim: The refinement algorithm filters out "false positive" minimal features by selecting only those shared across multiple perturbed candidate sets.
- Mechanism: By sampling perturbation variables vi,j from a Gaussian distribution and running the knapsack solver multiple times, CIDR collects candidate sets and keeps only features that appear in more than a threshold fraction of them.
- Core assumption: True minimal features will consistently appear across different perturbations, while spurious ones will not.
- Evidence anchors: [section 4.2] "We assume that candidate sets comprise two distinct types of features: 'true positive' features and 'false positive' features." [section 4.2] "By focusing on the widely shared features, we can effectively filter out the 'false positive' minimal features."
- Break condition: If the perturbation introduces too much variance, even true minimal features may not consistently appear, reducing recall.

## Foundational Learning

- Concept: Cooperative game theory and Shapley values
  - Why needed here: The CIG formulation is inspired by cooperative game theory, where the Shapley value measures a feature's contribution to all possible coalitions.
  - Quick check question: How does the CIG score differ from a standard Shapley value computation?

- Concept: Knapsack problem and dynamic programming
  - Why needed here: The transformation to a knapsack problem allows CIDR to use efficient combinatorial optimization to approximate the minimal feature set.
  - Quick check question: What is the time complexity of solving the 0-1 knapsack problem using dynamic programming?

- Concept: Integrated gradients and completeness axiom
  - Why needed here: CIDR builds on integrated gradients, using its completeness axiom to ensure that the sum of attributions equals the difference in model output between the input and baseline.
  - Quick check question: What is the completeness axiom in the context of integrated gradients?

## Architecture Onboarding

- Component map: Feature extraction -> CIG computation -> Knapsack transformation -> Candidate generation -> Refinement filtering
- Critical path: The most time-consuming step is computing CIG scores for all word pairs, which is O(n²) where n is the number of words. The knapsack solver is O(nW) where W is the upper bound, and refinement is O(niter × |Sp|).
- Design tradeoffs: CIDR trades off computational cost (O(n²) for CIG) for richer interaction modeling. The perturbation-based refinement adds robustness but increases runtime linearly with the number of iterations.
- Failure signatures: Poor FMS scores indicate that the refinement step is not filtering false positives effectively. Low comprehensiveness or high log-odds suggest the CIG scores are not capturing true feature importance.
- First 3 experiments:
  1. Run CIDR on a small dataset (e.g., SST-2) with default hyperparameters and inspect the top word pairs identified.
  2. Compare CIDR's FMS score against a baseline that does not use CIG (e.g., simple gradient-based attribution) to validate the interaction modeling.
  3. Vary the refinement threshold ε and observe its effect on FMS and cardinality of the minimal feature set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CIDR vary with different values of the coefficient β in the CIG formula?
- Basis in paper: [explicit] The paper mentions that β ∈ R+ is a coefficient balancing individual and cooperative contributions in CIG, with values ranging from 0 to 1.
- Why unresolved: The paper does not provide empirical results showing how CIDR's performance changes with different β values.
- What evidence would resolve it: An ablation study varying β across its range (0 to 1) and measuring CIDR's performance on the metrics used in the paper.

### Open Question 2
- Question: How sensitive is CIDR to the choice of the numeric threshold t used in the feature essence property?
- Basis in paper: [explicit] The paper sets the numeric threshold t = 0.5 but does not explore how varying t affects CIDR's performance.
- Why unresolved: The paper does not investigate the impact of different threshold values on CIDR's ability to satisfy the feature essence property.
- What evidence would resolve it: Experiments varying t and measuring CIDR's performance on the LO and Comp metrics, as well as the feature minimality score.

### Open Question 3
- Question: How does CIDR compare to other methods for detecting feature interactions, such as hierarchical clustering algorithms or graph neural networks?
- Basis in paper: [inferred] The paper mentions that prior studies have used hierarchical clustering algorithms or GNNs to detect feature interactions, but it does not directly compare CIDR to these methods.
- Why unresolved: The paper only compares CIDR to model-agnostic feature importance-based methods and does not include baselines that explicitly model feature interactions.
- What evidence would resolve it: Experiments comparing CIDR to hierarchical clustering algorithms and GNNs on the same datasets and using the same evaluation metrics.

## Limitations
- The CIG formulation's effectiveness depends heavily on the approximation of cooperative effects through gradient-based methods, which may not capture all types of feature interactions.
- The perturbation-based refinement assumes that true minimal features will consistently appear across different runs, but this may not hold for features with context-dependent importance.
- The knapsack approximation introduces an upper bound constraint that may not always reflect the true minimal feature set cardinality.

## Confidence
- CIG formulation accuracy: Medium confidence
- Refinement algorithm effectiveness: Medium confidence
- Overall method performance: High confidence

## Next Checks
1. Create synthetic datasets with known feature interactions (both cooperative and competitive) and verify that CIG scores correctly identify these relationships with high precision and recall.
2. Systematically vary the number of perturbation iterations and refinement threshold ε to identify the sensitivity of FMS scores to these hyperparameters, establishing guidelines for parameter selection.
3. Implement a simpler baseline that uses only individual feature importance (no CIG) and compare FMS scores to quantify the contribution of interaction modeling versus individual feature attribution alone.