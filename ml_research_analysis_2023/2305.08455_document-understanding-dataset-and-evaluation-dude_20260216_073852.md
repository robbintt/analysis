---
ver: rpa2
title: Document Understanding Dataset and Evaluation (DUDE)
arxiv_id: '2305.08455'
source_url: https://arxiv.org/abs/2305.08455
tags:
- document
- question
- pages
- answer
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DUDE, a new large-scale, multi-page, multi-domain,
  multi-industry Document Visual Question Answering (DocVQA) benchmark. The dataset
  addresses the need for more practical and challenging benchmarks in the Document
  AI (DocAI) community, focusing on real-world scenarios where models must handle
  diverse document types, layouts, and question formats.
---

# Document Understanding Dataset and Evaluation (DUDE)

## Quick Facts
- arXiv ID: 2305.08455
- Source URL: https://arxiv.org/abs/2305.08455
- Authors: [Multiple authors]
- Reference count: 40
- Key outcome: Introduces DUDE, a large-scale multi-page, multi-domain, multi-industry Document Visual Question Answering benchmark with comprehensive evaluation metrics

## Executive Summary
This paper introduces DUDE, a new large-scale Document Visual Question Answering (DocVQA) benchmark designed to address limitations in existing datasets. DUDE features multi-page, multi-domain, and multi-industry documents with diverse layouts, question types (extractive, abstractive, list-based, non-answerable), and visual elements. The dataset aims to push the boundaries of current DocAI methods by requiring compositional reasoning, layout navigation, and handling of complex document structures. The paper presents a comprehensive evaluation setup using ANLS, ECE, and AURC metrics to assess model performance, calibration, and confidence.

## Method Summary
DUDE is a multi-page, multi-domain, multi-industry document dataset with OCR-extracted text and question-answer pairs. The evaluation uses three metrics: ANLS (Average Normalized Levenshtein Similarity) for answer accuracy allowing partial matches, ECE (Expected Calibration Error) for confidence calibration, and AURC (Area Under Risk Coverage Curve) for selective classification performance. Models are fine-tuned on the training set and evaluated on test set documents spanning various industries, domains, and time periods. The dataset includes documents from books, web pages, and reports with questions requiring different reasoning types and document navigation skills.

## Key Results
- Current SOTA models show significantly degraded performance on DUDE compared to existing DocVQA benchmarks
- Models struggle particularly with abstractive questions, multi-hop reasoning, and non-answerable questions
- The dataset reveals limitations in current models' ability to handle diverse layouts, visual elements, and multi-page document reasoning
- ANLS, ECE, and AURC metrics provide comprehensive evaluation beyond simple accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-page, multi-domain, and multi-industry document diversity improves model generalization by forcing reasoning over varied layouts, text arrangements, and visual elements.
- Mechanism: The dataset includes documents spanning different industries, time periods, and layouts (textual, visual, tabular). This forces models to learn representations robust to domain shifts and document heterogeneity, reducing overfitting to single-layout patterns.
- Core assumption: The distribution shift between training and unseen domains is non-trivial and not fully captured by existing DocVQA datasets.
- Evidence anchors:
  - [abstract] "We present a new dataset with novelties related to types of questions, answers, and document layouts based on multi-industry, multi-domain, and multi-page VRDs of various origins, and dates."
  - [section] "The dataset covers a wide range of document types, sources and dates...covers a broad range of domains, including medical, legal, technical, and financial, among others."
  - [corpus] Weak. Found no directly related work with multi-industry, multi-time-period focus; only single-domain benchmarks.
- Break condition: If the domain shift is minimal or if the test set contains documents too similar to the training set, the generalization benefit disappears.

### Mechanism 2
- Claim: Including complex question types (multi-hop, layout-navigating, non-answerable, list) pushes models to develop compositional reasoning rather than shallow extraction.
- Mechanism: Questions require the model to navigate document structure, combine evidence across pages, perform arithmetic or counting, and handle cases with no valid answer. This simulates real-world scenarios where answers are not trivially extractive.
- Core assumption: Current models rely too heavily on extractive reasoning and fail when answers require deeper semantic understanding or navigation across pages.
- Evidence anchors:
  - [abstract] "Moreover, we are pushing the boundaries of current methods by creating multi-task and multi-domain evaluation setups that more accurately simulate real-world situations..."
  - [section] "We assert that various layouts and visual elements must be comprehended semantically...multi-hop questions that indicate a model's robustness to sequential reasoning..."
  - [corpus] Weak. No corpus evidence of related datasets requiring multi-hop reasoning or non-answerable questions; existing datasets mostly extractive.
- Break condition: If the evaluation metric (ANLS) does not properly distinguish between shallow and deep reasoning, the mechanism's impact cannot be measured.

### Mechanism 3
- Claim: Using ANLS, ECE, and AURC metrics together captures not just accuracy but also calibration and selective classification performance, aligning evaluation with practical deployment needs.
- Mechanism: ANLS generalizes accuracy to allow partial matches and multiple answer variants; ECE measures calibration of predicted confidences; AURC evaluates selective classification performance (risk vs. coverage trade-off).
- Core assumption: Calibration and selective classification are critical for real-world applications where wrong predictions must be triaged by confidence thresholds.
- Evidence anchors:
  - [abstract] "Moreover, we are pushing the boundaries of current methods by creating multi-task and multi-domain evaluation setups...We report (next to ANLS) two additional metrics, Expected Calibration Error (ECE) and Area-Under-Risk-Coverage-Curve (AURC)..."
  - [section] "Calibration requires that the probability a model assigns to its predictions equals their true likelihood of being correct..."
  - [corpus] Weak. Found no directly related evaluation methodologies combining ANLS, ECE, and AURC in the context of DocVQA; most works use only accuracy.
- Break condition: If the metrics are not implemented consistently or if confidence estimates are unreliable, the calibration signal is lost.

## Foundational Learning

- Concept: Document layout analysis and visual element comprehension
  - Why needed here: The dataset includes questions requiring understanding of tables, charts, figures, lists, checkboxes, and spatial text arrangements, which are not present in text-only QA datasets.
  - Quick check question: Can you explain the difference between layout-navigating and extractive questions?

- Concept: Multi-hop reasoning and compositional question answering
  - Why needed here: Questions often require combining evidence across multiple pages or document elements (e.g., "If the checkbox on page 1 section 3a indicates that the company is incorporated, how much yearly revenue did it generate in 2022 (given the table on page 5)?")
  - Quick check question: What distinguishes a multi-hop question from a simple extractive one?

- Concept: Confidence calibration and selective classification
  - Why needed here: Models must provide confidence estimates for practical triage, and ECE/AURC metrics measure how well these estimates reflect true likelihood and risk.
  - Quick check question: Why is calibration important in real-world document understanding systems?

## Architecture Onboarding

- Component map: Document ingestion -> OCR/text extraction -> Layout parsing -> Question answering model -> Confidence estimation -> ANLS/ECE/AURC evaluation
- Critical path: OCR/text extraction -> Layout parsing -> Question answering model -> Evaluation
- Design tradeoffs: Tradeoff between sequence length (processing long documents) and model capacity; tradeoff between extractive vs. generative answer formats; tradeoff between accuracy and calibration.
- Failure signatures: Poor ANLS on abstractive questions -> model cannot generate non-extractive answers; high ECE -> model over/underconfident; low AURC -> model cannot rank confidence effectively.
- First 3 experiments:
  1. Evaluate baseline BERT/T5 on extractive vs. abstractive questions to confirm generative models needed.
  2. Compare Concat vs. Max Conf strategies to identify best page aggregation method.
  3. Test T5-2D with varying sequence lengths to measure impact of long document processing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of models on DUDE compare to their performance on other Document Visual Question Answering (DocVQA) benchmarks?
- Basis in paper: [explicit] The paper presents a comprehensive evaluation of various models on DUDE and compares their performance to human baselines.
- Why unresolved: The paper does not provide a direct comparison of DUDE results with other DocVQA benchmarks, making it difficult to assess the relative difficulty and practical relevance of DUDE.
- What evidence would resolve it: A detailed comparison of model performance on DUDE and other DocVQA benchmarks, including metrics like ANLS, ECE, and AURC.

### Open Question 2
- Question: How does the performance of models on DUDE vary across different document types and domains?
- Basis in paper: [explicit] The paper mentions that DUDE includes documents from various industries, domains, and time periods, but does not provide a detailed analysis of model performance across these different categories.
- Why unresolved: Understanding how well models generalize to different document types and domains is crucial for assessing their practical applicability.
- What evidence would resolve it: A breakdown of model performance on DUDE based on document type, industry, and domain, highlighting strengths and weaknesses in different categories.

### Open Question 3
- Question: How do different model architectures and pre-training strategies affect performance on DUDE?
- Basis in paper: [explicit] The paper evaluates various model architectures, including encoder-based, decoder-based, and multi-modal models, on DUDE.
- Why unresolved: While the paper provides some insights into the strengths and weaknesses of different models, a more systematic analysis of the impact of architecture and pre-training on DUDE performance is needed.
- What evidence would resolve it: A comprehensive study comparing the performance of different model architectures and pre-training strategies on DUDE, including ablation studies and analysis of key factors influencing performance.

### Open Question 4
- Question: How can the calibration and confidence estimation of models on DUDE be improved?
- Basis in paper: [explicit] The paper introduces metrics like ECE and AURC to evaluate model calibration and confidence, but does not provide solutions for improving these aspects.
- Why unresolved: Accurate calibration and confidence estimation are crucial for real-world applications of DUDE models, as they allow for better decision-making and risk assessment.
- What evidence would resolve it: Novel techniques and strategies for improving model calibration and confidence estimation on DUDE, including methods for post-hoc calibration, confidence regularization, and uncertainty quantification.

## Limitations

- Limited comparison with existing DocVQA benchmarks makes it difficult to assess DUDE's relative difficulty and novelty
- Reliance on OCR text extraction may introduce noise and errors that affect model performance
- Calibration metrics assume reliable confidence estimation, but the paper doesn't thoroughly investigate the trustworthiness of these estimates

## Confidence

**High Confidence**: The dataset successfully introduces novel question types and document layouts not present in existing benchmarks. The multi-page, multi-industry document collection represents genuine progress in dataset diversity. The baseline model performance results (showing SOTA models struggle) are well-supported by the experimental evidence.

**Medium Confidence**: The mechanism claims about multi-domain diversity improving generalization and complex question types requiring compositional reasoning are plausible but would benefit from ablation studies isolating these effects. The calibration evaluation's practical relevance is supported by the metrics' theoretical properties, but real-world deployment validation is absent.

**Low Confidence**: The assertion that DUDE specifically addresses all limitations of existing DocVQA benchmarks cannot be fully evaluated without direct comparisons under identical conditions. The claims about calibration metrics capturing deployment-relevant performance characteristics lack empirical validation beyond metric computation.

## Next Checks

1. **Domain Shift Validation**: Conduct controlled experiments comparing model performance on DUDE vs. single-domain benchmarks (e.g., DocVQA, InfographicVQA) when models are trained on their respective training sets. This would isolate whether DUDE's multi-domain design genuinely improves generalization or simply provides more training data.

2. **Reasoning Depth Validation**: Design diagnostic experiments that systematically vary question complexity (simple extraction, single-hop reasoning, multi-hop reasoning) across both extractive and abstractive formats. This would validate whether ANLS and other metrics properly distinguish between shallow and deep reasoning capabilities.

3. **Calibration Reliability Validation**: Implement cross-validation experiments where models are evaluated on both in-distribution and out-of-distribution document types, comparing predicted confidence scores with actual accuracy. This would test whether ECE and AURC metrics capture meaningful calibration properties or simply reflect model uncertainty on unfamiliar document types.