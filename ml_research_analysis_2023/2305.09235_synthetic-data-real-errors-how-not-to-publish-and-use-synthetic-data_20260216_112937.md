---
ver: rpa2
title: 'Synthetic data, real errors: how (not) to publish and use synthetic data'
arxiv_id: '2305.09235'
source_url: https://arxiv.org/abs/2305.09235
tags:
- data
- synthetic
- generative
- naive
- downstream
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses a critical gap in synthetic data usage: how
  errors in the generative process impact downstream machine learning tasks. The authors
  show that the naive approach of treating synthetic data as real leads to poor model
  training, unreliable evaluation, and underestimated uncertainty.'
---

# Synthetic data, real errors: how (not) to publish and use synthetic data

## Quick Facts
- **arXiv ID:** 2305.09235
- **Source URL:** https://arxiv.org/abs/2305.09235
- **Reference count:** 40
- **Primary result:** Deep Generative Ensemble (DGE) significantly outperforms naive synthetic data usage for downstream ML tasks by training multiple generative models and using their outputs to train and evaluate downstream models.

## Executive Summary
This paper addresses a critical gap in synthetic data usage: how errors in the generative process impact downstream machine learning tasks. The authors show that the naive approach of treating synthetic data as real leads to poor model training, unreliable evaluation, and underestimated uncertainty. To address this, they introduce Deep Generative Ensemble (DGE), which generates multiple synthetic datasets using different generative model parameters and uses them to train and evaluate downstream models. DGE improves model performance, evaluation, and uncertainty quantification, particularly for minority classes and low-density regions where generative uncertainty is highest. The method significantly outperforms the naive approach, achieving near-oracle performance on real data in downstream tasks.

## Method Summary
The authors introduce Deep Generative Ensemble (DGE), a method that trains multiple generative models with different random initializations on real data, generates synthetic datasets from each, and uses these datasets to train and evaluate downstream models. Unlike the naive approach of using a single synthetic dataset, DGE leverages the diversity of multiple generative models to capture and reduce errors in the synthetic data generation process. The method involves training K generative models, generating K synthetic datasets, training K downstream models (one on each synthetic dataset), and using ensemble predictions or cross-dataset evaluation for improved performance, evaluation, and uncertainty quantification.

## Key Results
- DGE improves downstream model training, achieving near-oracle performance on real data compared to the naive approach
- DGE provides more reliable model evaluation and selection by reducing data leakage bias
- DGE enables better uncertainty quantification by capturing generative uncertainty that single-model approaches miss
- The improvements are particularly significant for minority classes and low-density regions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Deep Generative Ensemble (DGE) improves downstream model performance by reducing the variance caused by generative model errors.
- **Mechanism:** DGE trains multiple generative models with different random initializations and uses their outputs to train downstream models, effectively averaging out errors from individual models. This approach implicitly approximates the posterior distribution over generative model parameters, capturing uncertainty in the generation process.
- **Core assumption:** Individual generative models make uncorrelated errors, so averaging their outputs reduces overall error.
- **Evidence anchors:**
  - [abstract]: "DGE improves downstream model training, evaluation, and uncertainty quantification, vastly outperforming the naive approach on average."
  - [section 4.1]: "Training the downstream models on an ensemble of synthetic datasets achieves almost Dr-model performance on real data. In contrast, the naive baseline is often a few percent lower."
  - [corpus]: Weak evidence; related papers discuss generative model ensembles but not specifically for downstream task improvement.
- **Break condition:** If generative models make correlated errors (e.g., systematic underfitting), the ensemble approach won't help and might even amplify shared biases.

### Mechanism 2
- **Claim:** DGE provides more reliable model evaluation and selection by reducing bias from using a single synthetic dataset.
- **Mechanism:** Instead of evaluating a downstream model on the same synthetic data it was trained on (which introduces bias), DGE evaluates models using synthetic test sets from different generative models. This reduces data leakage and provides a more accurate estimate of real-world performance.
- **Core assumption:** Different generative models produce synthetic data with diverse characteristics, making them suitable for cross-evaluation.
- **Evidence anchors:**
  - [section 4.2.1]: "The naive approach overestimates the model performance. This is due to a synthetic data variant of data leakage: overfitting in the generative model is copied by the trained model, but is also reflected in the synthetic test set."
  - [section 4.2.2]: "The naive approach consistently selects more complex models that can copy the generative model's pθ(Y|X), but which generalise poorly to real data."
  - [corpus]: Weak evidence; related papers discuss synthetic data evaluation but not specifically the ensemble-based approach for cross-validation.
- **Break condition:** If all generative models consistently overfit in the same way, cross-evaluation won't reveal the true performance on real data.

### Mechanism 3
- **Claim:** DGE enables better uncertainty quantification by capturing generative uncertainty that single-model approaches miss.
- **Mechanism:** By training multiple generative models and using their outputs to train downstream models, DGE captures the variance in predictions that arises from uncertainty in the generative process itself. This provides a more complete picture of model uncertainty compared to approaches that only consider predictive uncertainty.
- **Core assumption:** The variance between predictions from different generative models reflects the true uncertainty in the data generation process.
- **Evidence anchors:**
  - [section 4.3]: "DGE provides uncertainty on the generative level, which the naive approaches cannot. It is thus essential to ensure individual synthetic datasets in the DGE are published separately."
  - [section 4.3]: "We see that the naive approaches lead to poor diversity between different models within the ensemble. Since they cannot capture the generative uncertainty, these approaches provide little insight into how the model may do on real data."
  - [corpus]: Weak evidence; related papers discuss uncertainty quantification but not specifically the role of generative model ensembles.
- **Break condition:** If the generative models are too similar (e.g., same architecture, same training data), the ensemble won't capture meaningful generative uncertainty.

## Foundational Learning

- **Concept:** Bayesian inference and posterior approximation
  - **Why needed here:** DGE aims to approximate the posterior distribution over generative model parameters, which is fundamental to understanding how it improves downstream tasks.
  - **Quick check question:** Why can't we compute the exact posterior distribution over generative model parameters in most deep learning settings?

- **Concept:** Deep Ensembles and uncertainty quantification
  - **Why needed here:** DGE is inspired by Deep Ensembles, which use multiple models to quantify uncertainty. Understanding this connection is crucial for grasping DGE's mechanism.
  - **Quick check question:** How does Deep Ensembles' approach to uncertainty quantification differ from traditional Bayesian methods?

- **Concept:** Synthetic data generation and evaluation
  - **Why needed here:** The paper's core contribution relies on understanding how synthetic data is generated and how its quality affects downstream tasks.
  - **Quick check question:** What are the main challenges in evaluating the quality of synthetic data, and why is this particularly difficult for deep generative models?

## Architecture Onboarding

- **Component map:** Real data → Generative model training (K times with different seeds) → K synthetic datasets → Downstream model training (K times) → Ensemble predictions
- **Critical path:** 1. Generate multiple synthetic datasets using different generative model initializations 2. Train downstream models on each synthetic dataset 3. Ensemble predictions by averaging outputs 4. For evaluation: use cross-dataset evaluation (train on some synthetic datasets, test on others)
- **Design tradeoffs:** More generative models (larger K) → better uncertainty capture but higher computational cost; Sharing parameters across generative models → reduced computational cost but potentially less diverse synthetic data; Different generative model architectures → potentially better coverage but more complex implementation
- **Failure signatures:** If downstream performance doesn't improve with larger K, the generative models might be making correlated errors; If model evaluation becomes unstable with larger K, the synthetic datasets might be too diverse or the generative models might be underfitting; If uncertainty quantification doesn't improve, the generative models might not be diverse enough or might be making systematic errors
- **First 3 experiments:** 1. Generate K=5 synthetic datasets using different random seeds, train a simple classifier on each, and compare ensemble performance to a single synthetic dataset approach 2. Evaluate model performance using cross-dataset evaluation (train on 4 synthetic datasets, test on the 5th) and compare to traditional train-test splits on a single synthetic dataset 3. Compare uncertainty quantification (e.g., confidence intervals) between DGE and a single synthetic dataset approach on a simple dataset with known ground truth

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal number of generative models (K) to use in Deep Generative Ensemble (DGE) for different downstream tasks and data characteristics?
- **Basis in paper:** [explicit] The paper shows that even K=5 provides significant improvements over the naive approach, but doesn't systematically explore the optimal K for different scenarios.
- **Why unresolved:** The paper only tests K=5, 10, and 20, showing that performance improves with larger K but doesn't identify the optimal trade-off point between performance gains and computational costs.
- **What evidence would resolve it:** Systematic experiments varying K across different dataset sizes, task complexities, and computational constraints to identify performance-accuracy trade-offs.

### Open Question 2
- **Question:** How does DGE perform under strict privacy constraints when differential privacy guarantees are required?
- **Basis in paper:** [explicit] The paper mentions that generating multiple synthetic datasets requires scaling the privacy budget for each generator but doesn't explore this experimentally.
- **Why unresolved:** The paper only briefly mentions privacy considerations in the limitations section without empirical evaluation of DGE under differential privacy.
- **What evidence would resolve it:** Experiments measuring the trade-off between privacy budget allocation across K generators and downstream model performance.

### Open Question 3
- **Question:** What alternative methods beyond Deep Ensembles could better approximate the posterior distribution over generative model parameters?
- **Basis in paper:** [explicit] The paper acknowledges that Deep Ensembles are a crude Bayesian approximation and suggests future work could explore other UQ methods.
- **Why unresolved:** The paper only implements Deep Ensembles and doesn't compare against other posterior approximation methods like MC dropout, variational inference, or hyperparameter ensembles.
- **What evidence would resolve it:** Comparative experiments testing multiple posterior approximation methods for generative uncertainty quantification.

### Open Question 4
- **Question:** How does data leakage between synthetic train and test sets affect the reliability of DGE evaluation and what are the best practices to mitigate it?
- **Basis in paper:** [explicit] The paper acknowledges that all synthetic datasets are derived from the same real data, creating potential data leakage, but doesn't systematically study its impact.
- **Why unresolved:** The paper uses disjoint synthetic datasets for train/test but doesn't explore the bias introduced by all datasets sharing the same underlying real data source.
- **What evidence would resolve it:** Experiments comparing DGE evaluation when generative models are trained on disjoint subsets versus the full dataset.

### Open Question 5
- **Question:** How does DGE performance vary across different types of underrepresented groups beyond simple feature-level minorities?
- **Basis in paper:** [explicit] The paper examines underrepresented groups defined by minority categories of individual features but doesn't explore more complex intersectional or high-dimensional minority definitions.
- **Why unresolved:** The analysis only considers single-feature minority subsets and doesn't examine how DGE handles complex combinations of minority characteristics.
- **What evidence would resolve it:** Experiments defining underrepresented groups using intersectional features and measuring DGE's ability to improve performance for these complex minority definitions.

## Limitations

- The empirical validation relies on synthetic data generation from a single algorithm (CTGAN), limiting generalizability to other generative models
- The computational cost of training multiple generative models scales linearly with ensemble size, creating practical barriers for large-scale applications
- The assumption that uncorrelated errors across generative models drive the ensemble benefit is theoretically plausible but not empirically verified

## Confidence

- **High confidence:** The core observation that naive synthetic data usage leads to data leakage and biased evaluation is well-supported by experimental evidence. The improved performance of DGE over naive approaches is demonstrated across multiple datasets.
- **Medium confidence:** The mechanism explaining how DGE reduces variance through uncorrelated errors is reasonable but relies on unverified assumptions about error correlation structure. The specific performance improvements (near-oracle real data performance) are impressive but may be dataset-dependent.
- **Low confidence:** Claims about uncertainty quantification improvements are supported but the evaluation methodology for uncertainty quality (diversity metrics, calibration) could be more rigorous.

## Next Checks

1. **Error Correlation Analysis:** Compute and report the correlation matrix between generative model predictions on the same inputs to verify the uncorrelated errors assumption underlying DGE's mechanism.
2. **Cross-Algorithm Validation:** Repeat the main experiments using different generative models (e.g., VAEs, normalizing flows) to assess whether DGE benefits generalize beyond CTGAN.
3. **Computational Efficiency Trade-off:** Systematically vary the number of generative models (K) in DGE and measure the marginal performance gain per additional model to establish optimal ensemble sizing for different dataset sizes.