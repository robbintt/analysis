---
ver: rpa2
title: 'NAIST-SIC-Aligned: an Aligned English-Japanese Simultaneous Interpretation
  Corpus'
arxiv_id: '2304.11766'
source_url: https://arxiv.org/abs/2304.11766
tags:
- data
- corpus
- alignment
- simultaneous
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the lack of large-scale training corpora for
  simultaneous machine translation (SiMT) by introducing NAIST-SIC-Aligned, an automatically
  aligned English-Japanese simultaneous interpretation corpus. The authors propose
  a two-stage alignment approach: coarse alignment using vecAlign to map source and
  target sequences, followed by fine-grained alignment involving intra- and inter-sentence
  filtering to improve data quality.'
---

# NAIST-SIC-Aligned: an Aligned English-Japanese Simultaneous Interpretation Corpus

## Quick Facts
- arXiv ID: 2304.11766
- Source URL: https://arxiv.org/abs/2304.11766
- Reference count: 9
- The corpus achieves 80% alignment accuracy on the S-rank subset

## Executive Summary
This work addresses the lack of large-scale training corpora for simultaneous machine translation (SiMT) by introducing NAIST-SIC-Aligned, an automatically aligned English-Japanese simultaneous interpretation corpus. The authors propose a two-stage alignment approach: coarse alignment using vecAlign to map source and target sequences, followed by fine-grained alignment involving intra- and inter-sentence filtering to improve data quality. The corpus was validated both quantitatively and qualitatively, with the S-rank subset achieving 80% alignment accuracy. The authors also manually curated a small test set (SI-TEST) comprising 383 sentence pairs for evaluation. Models trained on this SI data demonstrated significant improvements in translation quality and latency compared to baselines, highlighting the value of SI data for SiMT.

## Method Summary
The authors constructed NAIST-SIC-Aligned by applying a two-stage alignment approach to the non-parallel NAIST-SIC corpus of TED talks interpreted by professional interpreters. The first stage used vecAlign for coarse many-to-many sentence mapping, filtering pairs with high alignment costs. The second stage applied intra-sentence filtering to remove meaningless chunks and inter-sentence filtering based on content word coverage, length ratios relative to Google Translate outputs, and semantic similarity using BLEURT. This produced a high-quality aligned corpus with 80% accuracy on the S-rank subset.

## Key Results
- vecAlign coarse alignment achieved 88% accuracy on the S-rank subset
- Fine-grained alignment with filtering improved S-rank accuracy to 80%
- A-rank subset achieved 72% accuracy with the same filtering approach
- Manual validation confirmed the quality of the SI-TEST evaluation set with 383 sentence pairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Coarse alignment using vecAlign effectively groups minimal source and target sentence pairs that are translations of each other.
- Mechanism: vecAlign leverages sentence embeddings (LASER) and dynamic programming to perform many-to-many mapping, tolerating under-translation and mis-translation common in SI data.
- Core assumption: LASER embeddings capture semantic similarity well enough for SI data where exact one-to-one sentence alignment is rare.
- Evidence anchors:
  - [abstract]: "coarse alignment where we perform a many-to-many mapping between source and target sentences"
  - [section 4.1.1]: "vecAlign (Thompson and Koehn, 2019), which suits the above purposes, to perform the alignment"
- Break condition: If LASER embeddings fail to capture semantic similarity in distant language pairs like English-Japanese, vecAlign's alignment quality degrades.

### Mechanism 2
- Claim: Intra-sentence filtering removes meaningless chunks from target sentences, improving alignment quality.
- Mechanism: Filters out target chunks containing no content words (NOUN, PROPN, PRON, VERB, NUM) at the beginning or end of sentences.
- Core assumption: Interpreters often add fillers or incomplete thoughts at sentence boundaries that don't contribute to meaning.
- Evidence anchors:
  - [section 4.1.2]: "we decide to filter out the beginning and last chunk of a target sentence if it does not carry any substantial meaning"
  - [section 4.1.2]: "Manual inspection of the output from this step on SAMPLE shows that this simple heuristic accurately detects and removes chunks that contain no content words"
- Break condition: If interpreters consistently use meaningful words in these positions, filtering removes valuable content.

### Mechanism 3
- Claim: Inter-sentence filtering improves alignment by considering content word coverage, length ratio, and semantic similarity.
- Mechanism: Filters pairs based on content word alignment ratio, length ratio to Google Translate output, and BLEURT semantic similarity score.
- Core assumption: The combination of syntactic (content words), structural (length), and semantic (BLEURT) measures effectively identifies poor alignments.
- Evidence anchors:
  - [section 4.1.2]: "we develop the following surface-level and semantic-level rules to filter out pairs"
  - [section 4.1.2]: "As T is the correct translation of E, high η implies high coverage of E byF"
- Break condition: If Google Translate produces poor translations or BLEURT fails to capture meaning differences, filtering becomes unreliable.

## Foundational Learning

- Concept: Sentence alignment techniques for parallel corpora
  - Why needed here: The corpus construction relies on aligning SI data which has unique characteristics (under-translation, mis-translation, non-one-to-one relationships)
  - Quick check question: What are the main differences between aligning SI data and standard MT parallel corpora?

- Concept: Semantic similarity metrics for evaluation
  - Why needed here: Validation requires measuring how well automatically aligned pairs match manual alignments using semantic similarity
  - Quick check question: How does BLEURT differ from traditional metrics like BLEU in measuring translation quality?

- Concept: Dynamic programming for sequence alignment
  - Why needed here: vecAlign uses dynamic programming to find optimal alignment paths through sentence embedding space
  - Quick check question: What is the time complexity of dynamic programming approaches for sentence alignment?

## Architecture Onboarding

- Component map: NAIST-SIC corpus -> vecAlign coarse alignment -> Intra-sentence filtering -> Inter-sentence filtering -> NAIST-SIC-Aligned corpus + SI-TEST evaluation set

- Critical path: vecAlign -> Intra-filtering -> Inter-filtering -> Validation -> Dataset split

- Design tradeoffs: Coarse alignment prioritizes recall over precision, accepting noisy pairs that are filtered in later stages; simple heuristics for filtering avoid complex model training

- Failure signatures: Low alignment accuracy (below 80%), high proportion of filtered pairs (>50%), manual validation shows systematic errors in specific talk types

- First 3 experiments:
  1. Run vecAlign on a small subset and measure accuracy against true-align using Equation 1
  2. Apply intra-sentence filtering to the coarse-aligned pairs and manually validate the filtering decisions
  3. Test different thresholds for content word coverage ratio and observe impact on corpus size and quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal threshold for the fine-grained alignment filtering metrics (α, γ, η) across different talks in the SI corpus?
- Basis in paper: [inferred] The authors mention that the optimal values for these metrics vary by talk and leave automatically learning optimal values per talk for future work.
- Why unresolved: The paper manually validated each filtering step but did not develop an automated method to determine optimal thresholds for different talks.
- What evidence would resolve it: A systematic study comparing model performance using different threshold values for each metric across various talks would help determine optimal settings.

### Open Question 2
- Question: How does the quality of SI data (e.g., S-rank vs. A-rank interpreters) impact the downstream performance of simultaneous machine translation systems?
- Basis in paper: [explicit] The authors note that SISrank_RAW has better data quality than SIArank_RAW, and the aligned pairs from the former have higher quality.
- Why unresolved: While the paper observes differences in data quality, it does not conduct a thorough analysis of how these differences affect SiMT model performance.
- What evidence would resolve it: Training and evaluating SiMT models using data from different interpreter ranks and comparing their performance would provide insights into the impact of data quality.

### Open Question 3
- Question: Can the proposed two-stage alignment approach be effectively extended to other language pairs beyond English-Japanese?
- Basis in paper: [explicit] The authors state that they expect their approach to be extendable to other language pairs.
- Why unresolved: The paper only demonstrates the approach on English-Japanese data and does not validate its effectiveness on other language pairs.
- What evidence would resolve it: Applying the alignment approach to SI corpora in different language pairs and evaluating the quality of the resulting aligned data would determine its generalizability.

## Limitations

- The alignment methodology relies heavily on automatic metrics that may not fully capture semantic equivalence in simultaneous interpretation data
- Manual validation was limited to a subset of the S-rank data, leaving uncertainty about the quality of the A-rank subset
- The approach assumes Google Translate outputs provide reliable reference lengths, which may introduce bias

## Confidence

**High confidence** in the overall methodology and validation results for the S-rank subset, given the 80% alignment accuracy and systematic manual validation approach.

**Medium confidence** in the generalizability of the corpus quality to the A-rank subset and to other simultaneous interpretation domains beyond TED talks.

**Low confidence** in the optimality of the automatic filtering thresholds, as these were set empirically without exhaustive parameter search.

## Next Checks

1. **Cross-subset validation**: Apply the alignment pipeline to the A-rank subset and conduct systematic manual validation to verify that the 80% accuracy benchmark holds across both quality tiers.

2. **Threshold sensitivity analysis**: Systematically vary the content word coverage (α), length ratio (γ), and semantic similarity (η) thresholds to identify optimal values and assess the robustness of corpus quality to parameter changes.

3. **Domain transfer evaluation**: Test the aligned corpus on simultaneous interpretation tasks from different domains (e.g., business meetings, medical consultations) to evaluate the robustness of the corpus quality and the learned alignment heuristics across diverse interpretation contexts.