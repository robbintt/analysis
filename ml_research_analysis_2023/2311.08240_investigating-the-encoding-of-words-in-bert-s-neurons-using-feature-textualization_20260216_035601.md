---
ver: rpa2
title: Investigating the Encoding of Words in BERT's Neurons using Feature Textualization
arxiv_id: '2311.08240'
source_url: https://arxiv.org/abs/2311.08240
tags:
- neurons
- activation
- words
- word
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study adapts activation maximization from computer vision to
  NLP for neuron-level interpretability in BERT. Using feature textualization, it
  generates dense vector representations optimized to maximize individual neuron activations,
  then evaluates their semantic similarity to word embeddings.
---

# Investigating the Encoding of Words in BERT's Neurons using Feature Textualization

## Quick Facts
- arXiv ID: 2311.08240
- Source URL: https://arxiv.org/abs/2311.08240
- Reference count: 7
- Key outcome: Individual neurons in BERT do not encode words, but groups of 250-450 neurons together show semantic alignment with target words.

## Executive Summary
This study adapts activation maximization from computer vision to NLP for neuron-level interpretability in BERT. Using feature textualization, the researchers generate dense vector representations optimized to maximize individual neuron activations, then evaluate their semantic similarity to word embeddings. Results show that single neurons do not reliably encode words—optimized inputs for individual neurons are dissimilar to word embeddings and only utilize ~3% of a neuron's activation potential when triggered by actual words. Groups of 250-450 neurons together show improved semantic alignment with target words, with 67% of cases yielding closest embeddings matching the intended word. These findings indicate that distributed neuron groups, rather than individual neurons, are required for symbolic language representations in BERT.

## Method Summary
The study employs activation maximization to synthesize inputs that maximize individual neuron activations in BERT. The approach uses gradient ascent to iteratively adjust a continuous input vector (in the 30,522-dimensional vocabulary space) to maximize a target neuron's activation while keeping model weights frozen. For single neurons, the method runs optimization for 5000 steps with a learning rate of 1001 and input length of 3 tokens. The researchers then evaluate semantic alignment by computing cosine similarity between optimized inputs and closest word embeddings, and comparing activation strength between optimized inputs and most activating words. For neuron groups, they identify relevant neurons using word activation patterns, optimize average activation across groups, and assess semantic alignment to target words.

## Key Results
- Single neurons in BERT do not reliably encode words; optimized inputs for individual neurons show average cosine similarity of 0.124 with closest word embeddings
- Actual words only trigger ~3% of a neuron's activation potential compared to optimized inputs
- Groups of 250-450 neurons together show improved semantic alignment, with 67% of cases yielding closest embeddings matching target words

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Activation maximization adapted to NLP can generate dense vector representations optimized to maximize individual neuron activations in BERT.
- Mechanism: Gradient ascent is used to iteratively adjust a continuous input vector (in the embedding space) to maximize a target neuron's activation while keeping model weights frozen. This synthesizes an interpretable representation of the neuron's encoded information.
- Core assumption: The input vector that maximally activates a neuron visualizes the kind of information encoded in that neuron, similar to feature visualization in computer vision.
- Evidence anchors: [abstract] "Activation maximization is used to synthesize inherently interpretable visual representations of the information encoded in individual neurons."

### Mechanism 2
- Claim: Individual neurons in BERT do not reliably encode words, as optimized inputs for single neurons are dissimilar to word embeddings and only utilize ~3% of a neuron's activation potential when triggered by actual words.
- Mechanism: By comparing the cosine similarity between optimized inputs and closest word embeddings, and measuring activation strength, it's shown that words activate neurons weakly compared to their full potential. Optimized inputs for single neurons are semantically unrelated to words.
- Core assumption: If neurons encoded words, optimized inputs should be similar to word embeddings and achieve high activation strength with words as inputs.
- Evidence anchors: [abstract] "Results show that single neurons do not reliably encode words—optimized inputs for individual neurons are dissimilar to word embeddings and only utilize ~3% of a neuron's activation potential when triggered by actual words."

### Mechanism 3
- Claim: Groups of 250-450 neurons together show improved semantic alignment with target words, with 67% of cases yielding closest embeddings matching the intended word.
- Mechanism: By optimizing inputs for meaningful groups of neurons identified through word activation patterns, the resulting vectors become more semantically similar to words. This indicates distributed encoding of words across neuron groups.
- Core assumption: Information in neural networks is distributed, so individual neurons alone cannot encode complex symbolic units like words, but groups can.
- Evidence anchors: [abstract] "Groups of 250-450 neurons together show improved semantic alignment with target words, with 67% of cases yielding closest embeddings matching the intended word."

## Foundational Learning

- Concept: Activation maximization
  - Why needed here: It's the core technique used to generate interpretable representations of individual neurons by synthesizing inputs that maximize their activation.
  - Quick check question: What is the main difference between activation maximization in computer vision and the proposed feature textualization for NLP?

- Concept: Distributed representations in neural networks
  - Why needed here: Understanding that information is encoded across groups of neurons, not in single neurons, is crucial for interpreting why individual neurons don't encode words.
  - Quick check question: Why might it be problematic to interpret the knowledge encoded in a single neuron as a symbolic unit like a word?

- Concept: Cosine similarity for semantic relatedness
  - Why needed here: Used to quantitatively evaluate how semantically similar the optimized inputs are to actual word embeddings, indicating whether neurons encode words.
  - Quick check question: If the cosine similarity between an optimized input and the closest word is 0.124, what does this suggest about their semantic relatedness?

## Architecture Onboarding

- Component map: Input tokens -> Embedding layer (768-dim) -> BERT encoder layers (12 transformer blocks) -> Output activations at neuron positions
- Critical path: 1) Select target neuron(s) and input length, 2) Initialize random input vector(s), 3) Forward pass through BERT, 4) Compute gradient of neuron activation w.r.t. input, 5) Update input vector(s) via gradient ascent, 6) Repeat 3-5 for fixed iterations, 7) Analyze optimized input(s)
- Design tradeoffs: Optimizing in input vector space (30,522-dim) vs. embedding space (768-dim) - allows focusing on meaning but requires more dimensions; single neuron vs. group optimization - groups increase semantic alignment but computational cost; regularized vs. unregularized optimization - regularization may improve interpretability but bias results
- Failure signatures: Optimized inputs in separate subspace from word embeddings; little semantic improvement as neuron group size increases; high activation for optimized inputs but very low activation for actual words
- First 3 experiments: 1) Apply activation maximization to single neuron in first BERT layer and visualize optimized input, 2) Compare activation strength of optimized input vs. most activating word for same neuron, 3) Optimize inputs for groups of 10, 100, and 1000 neurons and evaluate semantic similarity to target words

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the interpretability of neuron activations in BERT vary across different model architectures (e.g., GPT, RoBERTa, T5)?
- Basis in paper: [inferred] The study focuses on BERT and discusses distributed nature of information encoding, suggesting results may vary with architecture.
- Why unresolved: Paper doesn't compare BERT with other architectures; unique characteristics of each model may influence neuron interpretability.
- What evidence would resolve it: Comparative studies applying feature textualization across multiple model architectures.

### Open Question 2
- Question: Can the introduction of regularization techniques improve the semantic alignment of optimized inputs with word embeddings?
- Basis in paper: [explicit] Paper mentions non-regularized optimal inputs are dissimilar from words and suggests regularization could pull optimized inputs closer to word embedding space.
- Why unresolved: Initial experiments with regularization mentioned but detailed results not provided.
- What evidence would resolve it: Systematic experimentation with various regularization techniques to quantify improvements in semantic alignment.

### Open Question 3
- Question: What is the impact of input length on the interpretability of neuron activations in BERT?
- Basis in paper: [inferred] Study focuses on single-word inputs and mentions future work on contextualized inputs, implying input length could affect interpretability.
- Why unresolved: Paper doesn't explore effects of longer input sequences on neuron activation interpretability.
- What evidence would resolve it: Experiments with varying input lengths to determine how context influences semantic alignment and interpretability.

## Limitations

- Conceptual ambiguity: Assumes optimal activation input represents neuron's encoded information, but this premise isn't definitively established for NLP models
- Evaluation methodology limitations: Uses nearest-neighbor word embeddings as sole semantic alignment metric, potentially missing contextual variations or multi-word expressions
- Neuron selection sensitivity: Choice between absolute and relative activation measures for selecting relevant neurons could significantly impact results, but sensitivity not extensively explored

## Confidence

- High confidence: Single neurons don't reliably encode words (strong evidence of minimal semantic similarity and low activation potential utilization)
- Medium confidence: Groups of 250-450 neurons show improved semantic alignment (compelling evidence but specific threshold may depend on unexplored factors)
- Low confidence: Broader implications about symbolic vs. distributed representations (extends beyond empirical evidence, needs additional validation)

## Next Checks

- Validation Check 1: Test robustness of neuron group findings by varying optimization iterations and regularization strength to confirm 250-450 neuron range consistently shows optimal semantic alignment
- Validation Check 2: Apply methodology to other transformer-based models (RoBERTa, GPT-2, smaller BERT variants) to determine if single neurons encoding little information while groups encode words is architecture-specific or general
- Validation Check 3: Investigate whether optimized inputs for neuron groups correspond to meaningful linguistic units beyond single words (phrases, syntactic patterns) by evaluating semantic similarity to phrase embeddings and activation patterns across sentence contexts