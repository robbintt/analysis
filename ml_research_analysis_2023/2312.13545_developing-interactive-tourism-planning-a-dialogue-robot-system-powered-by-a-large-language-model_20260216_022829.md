---
ver: rpa2
title: 'Developing Interactive Tourism Planning: A Dialogue Robot System Powered by
  a Large Language Model'
arxiv_id: '2312.13545'
source_url: https://arxiv.org/abs/2312.13545
tags:
- system
- dialogue
- phase
- tourist
- spots
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a dialogue robot system for interactive tourism
  planning using a large language model (GPT-4) in the Dialogue Robot Competition
  2023. The system implements a scenario-based approach with five phases: introduction/ice
  breaker, inquiry, course/spot selection, schedule proposal, and confirmation/closing.'
---

# Developing Interactive Tourism Planning: A Dialogue Robot System Powered by a Large Language Model

## Quick Facts
- arXiv ID: 2312.13545
- Source URL: https://arxiv.org/abs/2312.13545
- Reference count: 4
- This paper presents a dialogue robot system for interactive tourism planning using GPT-4 in the Dialogue Robot Competition 2023

## Executive Summary
This paper presents a dialogue robot system for interactive tourism planning using a large language model (GPT-4) in the Dialogue Robot Competition 2023. The system implements a scenario-based approach with five phases: introduction/ice breaker, inquiry, course/spot selection, schedule proposal, and confirmation/closing. Key innovations include dividing the complex travel planning task into subtasks, using different prompts for each phase, and leveraging external APIs for route information and tourist spot data. The system achieved 4th place in the competition preliminaries, scoring 0.85 in plan evaluation and 4.41 average satisfaction across nine metrics (with high variance of 3.44).

## Method Summary
The system uses a scenario-based approach with five phases: introduction/ice breaker, inquiry, course/spot selection, schedule proposal, and confirmation/closing. Each phase has specific objectives and uses different prompts for GPT-4 response generation. The system integrates external APIs (Rurubu for tourist spots, NAVITIME for route information) to provide real-time data, which is incorporated into prompts to enable informed schedule proposals. Phase transitions occur either automatically when the LLM outputs an [END] signal or manually after a maximum number of turns. A streaming API with punctuation-based TTS chunking enables real-time speech output, while a web-based viewer displays tourist spot information with a 4-spot limit.

## Key Results
- Achieved 4th place in Dialogue Robot Competition 2023 preliminaries
- Plan evaluation score of 0.85 (out of 1.0)
- Average satisfaction score of 4.41 across nine metrics with high variance of 3.44

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dividing the travel planning task into five distinct phases improves task completion and system controllability
- Mechanism: The system uses a scenario-based approach with five phases (introduction/ice breaker, inquiry, course/spot selection, schedule proposal, confirmation/closing), each with specific objectives and prompts. This decomposition allows the LLM to focus on manageable subtasks rather than the entire complex planning problem.
- Core assumption: Complex tasks can be more effectively handled when broken into smaller, sequential subtasks with clear success criteria
- Evidence anchors:
  - [abstract] "Furthermore, we propose a method that divides the complex task of a travel agency into multiple subtasks, managing each as a separate phase to effectively accomplish the task."
  - [section] "The scenario consists of five phases as shown in the table, each aimed at accomplishing a subtask in tourist guidance."
- Break condition: If the system cannot reliably detect when a phase is complete, forcing transitions after a maximum number of turns may interrupt natural conversation flow and reduce user satisfaction.

### Mechanism 2
- Claim: Using different prompts for each phase enables phase-specific response generation while maintaining task structure
- Mechanism: The system employs GPT-4 with different prompt configurations for each of the five phases, allowing the model to generate responses appropriate to the current subtask while following the overall scenario structure.
- Core assumption: Large language models can maintain coherent task progression when guided by phase-specific instructions and dialogue history
- Evidence anchors:
  - [abstract] "We aimed to construct a system that not only leverages the flexible conversational abilities of LLMs but also their advanced planning capabilities"
  - [section] "A large-scale language model is employed for response generation, and different types of prompts are used in each phase."
- Break condition: If the LLM generates responses that don't align with phase objectives, the task structure may break down, requiring more rigid control mechanisms.

### Mechanism 3
- Claim: Integrating external APIs (Rurubu for tourist spots, NAVITIME for route information) provides task-relevant data that enhances planning quality
- Mechanism: The system uses external APIs to fetch real-time tourist spot information and route guidance, which are then incorporated into prompts to enable informed schedule proposals and answer customer queries about timing and logistics.
- Core assumption: LLM performance can be significantly enhanced when provided with accurate, up-to-date external data sources
- Evidence anchors:
  - [section] "By using the Rurubu API, we continuously display images of tourist spots, receive customer reactions, and lead the dialogue up to the decision of course preference and the spots they liked within it."
  - [section] "In order to create a schedule to visit two tourist spots, it is necessary to provide accurate route guidance that takes into account the latest traffic information. We achieve this by adding the latest route information obtained using the NAVITIME API"
- Break condition: If API responses are delayed or contain errors, the system's ability to provide accurate information and maintain conversation flow will be compromised.

## Foundational Learning

- Concept: Scenario-based dialogue system design
  - Why needed here: The system relies on predefined scenarios to structure the conversation flow and ensure all necessary subtasks are completed
  - Quick check question: What are the five phases in the system's scenario, and what is the objective of each phase?

- Concept: Prompt engineering for task decomposition
  - Why needed here: Different prompts are used for each phase to guide the LLM's response generation appropriately for the current subtask
  - Quick check question: How does the system handle phase transitions, and what is the purpose of the [END] termination signal?

- Concept: Integration of external APIs with LLMs
  - Why needed here: The system uses Rurubu and NAVITIME APIs to provide real-time tourist spot and route information, which is then incorporated into the LLM's responses
  - Quick check question: What type of information does the system retrieve from each API, and how is this information presented to the user?

## Architecture Onboarding

- Component map:
  - Dialogue management system: Controls conversation flow through five phases
  - GPT-4 API: Generates responses using phase-specific prompts
  - Rurubu API: Provides tourist spot information and images
  - NAVITIME API: Supplies route information and traffic data
  - TTS system: Converts LLM responses to speech in real-time
  - Image/Map viewer: Displays tourist spot information on monitors

- Critical path: Customer utterance → Phase detection → Prompt construction with API data → GPT-4 response generation → TTS conversion → Speech output + Display update

- Design tradeoffs:
  - Flexibility vs. control: Using a single prompt for multiple subtasks (course introduction and spot selection) reduces complexity but may require more sophisticated prompt engineering
  - Real-time performance vs. response quality: Streaming API with punctuation-based TTS chunking improves perceived responsiveness but may sacrifice natural speech rhythm
  - API dependency vs. self-containment: External APIs provide accurate information but introduce potential failure points and latency

- Failure signatures:
  - Phase progression issues: System gets stuck in a phase or skips necessary phases
  - Response relevance problems: Generated responses don't address user queries or deviate from phase objectives
  - API integration failures: Missing or incorrect information in displayed content, or system errors when APIs are unavailable

- First 3 experiments:
  1. Test phase transition logic by simulating conversations that should trigger both automatic ([END]) and forced (max turns) transitions
  2. Validate prompt effectiveness by running each phase in isolation with test inputs and checking if outputs align with phase objectives
  3. Verify API integration by checking that tourist spot information and route data are correctly formatted and incorporated into LLM responses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the system handle unexpected or out-of-scope user requests during the dialogue that fall outside the predefined phases?
- Basis in paper: [explicit] The paper mentions using a scenario-based approach with five phases but does not discuss how the system handles unexpected user behavior or requests that don't fit the predefined scenarios.
- Why unresolved: The system's robustness to unexpected inputs is critical for real-world deployment, but the paper focuses only on the planned scenarios without addressing edge cases or error handling.
- What evidence would resolve it: Detailed analysis of how the system performs when users ask about topics outside tourism planning, make requests that span multiple phases, or express contradictory preferences.

### Open Question 2
- Question: What is the optimal number and duration of phases for different types of travel planning scenarios (e.g., short weekend trips vs. extended vacations)?
- Basis in paper: [inferred] The paper describes a five-phase approach but does not explore whether this structure is optimal or how it might need to be adapted for different planning complexities.
- Why unresolved: The current five-phase structure may not be efficient for all planning scenarios, and the system's performance across different trip types is unknown.
- What evidence would resolve it: Comparative studies testing the system with different phase structures and durations across various trip types and complexities.

### Open Question 3
- Question: How does the system's performance vary across different cultural contexts or when dealing with international tourists?
- Basis in paper: [explicit] The system was tested only in Japanese travel agencies in Fukuoka and Nagoya, with a focus on Kyoto tourism, with no mention of cross-cultural testing or international user scenarios.
- Why unresolved: Tourism planning often involves international travelers, but the system's ability to handle different cultural expectations, language nuances, or international travel scenarios is untested.
- What evidence would resolve it: Evaluation data from international users, testing with diverse cultural contexts, and analysis of how well the system adapts to different cultural norms in tourism planning.

## Limitations

- The evaluation metrics are based on a single competition context with limited sample size, making generalization difficult
- The paper lacks detailed technical specifications for critical components, particularly prompt engineering and transition criteria
- The system's performance is heavily dependent on external APIs, introducing potential failure points not adequately addressed in evaluation

## Confidence

- Scenario-based decomposition improves task completion: Medium confidence - The claim is supported by the system's competitive performance (4th place) but lacks comparative data against non-decomposed approaches or ablation studies showing the specific contribution of this design choice.
- Phase-specific prompts maintain task structure while enabling flexible responses: Medium confidence - The mechanism is theoretically sound and supported by competition results, but the paper does not provide detailed examples of prompt effectiveness or demonstrate how well the system handles edge cases and deviations from expected conversation patterns.
- External API integration enhances planning quality: Medium confidence - The paper describes the integration approach but does not provide quantitative evidence of how API-derived information improves plan quality or user satisfaction compared to systems without such integration.

## Next Checks

1. **Phase transition reliability test**: Conduct systematic testing of phase transitions across 100 simulated conversations covering edge cases (early completion, maximum turns, unexpected user inputs) to measure the frequency of correct automatic transitions versus forced transitions and identify patterns in transition failures.

2. **API dependency stress test**: Simulate API failures (timeouts, incorrect responses, rate limiting) and measure system response quality and conversation continuity to quantify the system's robustness and identify critical failure points in the API integration pipeline.

3. **Prompt effectiveness ablation study**: Implement controlled variations of the prompt engineering approach (single prompt vs. phase-specific prompts, different transition criteria) and measure their impact on task completion rates, user satisfaction, and response relevance to isolate the contribution of the proposed scenario-based approach.