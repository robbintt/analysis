---
ver: rpa2
title: Debiasing Algorithm through Model Adaptation
arxiv_id: '2310.18913'
source_url: https://arxiv.org/abs/2310.18913
tags:
- bias
- gender
- token
- language
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method called DAMA to mitigate gender
  bias in large language models. The authors perform causal analysis to identify that
  mid-upper feed-forward layers are most prone to convey bias.
---

# Debiasing Algorithm through Model Adaptation

## Quick Facts
- arXiv ID: 2310.18913
- Source URL: https://arxiv.org/abs/2310.18913
- Authors: 
- Reference count: 40
- Primary result: DAMA reduces stereotypical gender association by 0.13-0.21 while maintaining model performance

## Executive Summary
This paper introduces DAMA (Debiasing Algorithm through Model Adaptation), a novel method for mitigating gender bias in large language models. The authors perform causal analysis to identify that mid-upper feed-forward layers are most prone to convey bias, then intervene by applying linear projections to these layers' weight matrices. Experiments on LLaMA models show DAMA significantly decreases gender bias as measured by various metrics while maintaining model performance on downstream tasks.

## Method Summary
The DAMA method works by first using causal tracing to identify which model components store gender bias, finding that mid-upper feed-forward layers (18-25 for LLaMA 7B) are the primary mediators. The algorithm then computes stereotypical gender representations and applies Partial Least Squares regression to identify the subspace of these representations that correlates with gendered pronoun predictions. Finally, DAMA applies orthogonal projection matrices to the feed-forward weight matrices, removing the stereotypical gender subspace while preserving other semantic information. This approach changes neither the model's architecture nor parameter count.

## Key Results
- Reduces stereotypical gender association coefficient by 0.13-0.21
- Increases gender-neutral pronoun prediction by 5-10% on average
- Maintains model performance with minimal perplexity increase (0.5-1.5) and <1% downstream task degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mid-upper feed-forward layers store stereotypical gender associations that can be removed without harming language understanding
- Mechanism: Layers 18-25 contain gender bias mediators mapping profession names to gendered pronouns. Orthogonal projection matrix P = I - Pc removes stereotypical signal while preserving other correlations
- Core assumption: Gender bias is stored in specific MLP layers and can be separated from general language understanding through mathematical projection
- Evidence anchors: Causal tracing identifies layers 18-25 as bias mediators; PLS identifies stereotypical subspace
- Break condition: If gender bias is distributed across multiple component types rather than concentrated in MLP layers

### Mechanism 2
- Claim: PLS can identify stereotypical gender subspace that maximally correlates with gendered pronoun predictions
- Mechanism: PLS finds linear mapping B1 maximizing correlation between stereotyped keys and gendered values, then projects onto orthogonal complement
- Core assumption: Stereotypical gender subspace can be identified through correlation analysis and removed via orthogonal projection
- Evidence anchors: PLS algorithm applied to compute stereotypical keys and gendered values; projection matrix P = I - B1(BT1 B1)^-1 B1^T
- Break condition: If stereotypical gender subspace overlaps significantly with other semantic information

### Mechanism 3
- Claim: Weight editing through linear projection preserves architecture and parameter count while reducing bias
- Mechanism: DAMA multiplies feed-forward matrix WFF,l by projection matrix P, changing neither architecture nor parameter sizes
- Core assumption: Weight editing through projection can reduce bias without requiring architectural changes
- Evidence anchors: DAMA applies projections to feed-forwards without changing model architecture; orthogonal projection preserves dimensionality
- Break condition: If projection introduces numerical instability or edited weights cannot be effectively used during inference

## Foundational Learning

- Concept: Causal tracing methodology for identifying bias mediators
  - Why needed here: The paper relies on causal tracing rather than correlation-based methods to identify bias-storing components
  - Quick check question: What is the difference between observing correlation between model activations and using causal tracing to identify bias mediators?

- Concept: Partial Least Squares (PLS) regression
  - Why needed here: PLS is used to identify the stereotypical gender subspace that maximally correlates with gendered pronoun predictions
  - Quick check question: How does PLS differ from regular linear regression when identifying subspaces in high-dimensional data?

- Concept: Orthogonal projection matrices
  - Why needed here: The debiasing mechanism relies on applying orthogonal projections to remove stereotypical gender representations
  - Quick check question: What mathematical property ensures that applying an orthogonal projection matrix preserves the dimensionality of the original weight matrix?

## Architecture Onboarding

- Component map: Input text → embedding layer → causal attention blocks → MLP layers → output projection → next token prediction. Bias intervention occurs in mid-upper MLP layers (18-25 for LLaMA 7B).

- Critical path: The bias mediation occurs specifically in feed-forward layers after causal attention. DAMA intervenes by multiplying the feed-forward weight matrices by projection matrices.

- Design tradeoffs: The method trades some language modeling performance (slight perplexity increase) for significant bias reduction. Projection dimensionality (256 for LLaMA 7B) balances bias removal effectiveness against preserving language capabilities.

- Failure signatures:
  1. Perplexity increase > 5% indicates over-aggressive bias removal
  2. Downstream task performance drop > 2% suggests loss of general language understanding
  3. Bias metrics showing minimal improvement indicates incorrect layer selection or projection parameters

- First 3 experiments:
  1. Apply DAMA to only the last 2 layers (22-23) and measure bias reduction vs performance impact
  2. Vary projection dimensionality (32, 128, 512) while keeping layer count fixed at 9 layers
  3. Apply DAMA to attention layers instead of MLP layers to test if bias mediation occurs there

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of DAMA vary across different languages, particularly morphologically rich languages with more ubiquitous grammatical gender than English?
- Basis in paper: The authors mention interest in generalizing to morphologically rich languages with more ubiquitous grammatical gender
- Why unresolved: Current study focuses on English language models only
- What evidence would resolve it: Applying DAMA to models trained on morphologically rich languages and comparing effectiveness to English results

### Open Question 2
- Question: What is the optimal range of layers and dimensionality for applying DAMA to maximize bias reduction while minimizing performance impact?
- Basis in paper: Authors acknowledge their hyperparameter search is "coarse" and suggest more extensive investigation could be beneficial
- Why unresolved: Current hyperparameter search is limited and doesn't fully characterize the relationship between parameters and effectiveness
- What evidence would resolve it: Conducting more exhaustive hyperparameter search across wider range of model sizes and types

### Open Question 3
- Question: How does DAMA perform on other types of bias beyond gender bias, such as racial or cultural biases?
- Basis in paper: Authors state method can be easily generalized to other types of bias but only evaluate on gender bias
- Why unresolved: Current study only evaluates effectiveness on gender bias
- What evidence would resolve it: Applying DAMA to models trained on datasets containing various types of bias and evaluating effectiveness in reducing those biases

## Limitations

- Layer specificity uncertainty: The identified mid-upper MLP layers as bias mediators may be architecture-specific and not generalize to other model architectures
- Generalizability constraints: Methodology designed for binary gender bias may not extend effectively to intersectional biases, racial bias, or non-binary gender representations
- Performance-tradeoff tension: Relationship between bias removal intensity and performance preservation is not fully characterized, particularly regarding projection dimensionality sensitivity

## Confidence

**High Confidence**: Identification of mid-upper MLP layers as bias mediators through causal tracing is well-supported and the orthogonal projection approach is mathematically sound.

**Medium Confidence**: Effectiveness across different LLaMA model sizes is demonstrated, but generalization to other architectures and bias types remains uncertain. Performance preservation claims are supported but would benefit from broader testing.

**Low Confidence**: Claims about easy extension to other bias types beyond gender are speculative. Effectiveness for intersectional biases or non-binary representations is not addressed.

## Next Checks

1. **Architecture Transfer Validation**: Apply DAMA to alternative transformer architectures (OPT, BLOOM, GPT-Neo) to verify whether the same mid-upper layer pattern holds for bias mediation.

2. **Bias Type Extension**: Modify methodology to target racial bias or intersectional biases by creating appropriate stereotyped key sets and gendered value representations, then measure whether projection approach remains effective.

3. **Projection Dimensionality Sensitivity**: Systematically vary projection dimensionality (32, 64, 128, 256, 512) across different model sizes to establish optimal parameters and measure relationship between dimensionality, bias reduction, and performance preservation.