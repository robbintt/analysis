---
ver: rpa2
title: Discrete Prompt Compression with Reinforcement Learning
arxiv_id: '2308.08758'
source_url: https://arxiv.org/abs/2308.08758
tags:
- prompt
- prompts
- compression
- policy
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a discrete prompt compression method using
  reinforcement learning to address the limitations of existing prompt compression
  techniques that rely on trained embeddings. The approach employs a computationally
  efficient policy network that directly edits prompts to reduce non-contributory
  tokens, achieving an average 24.6% reduction in token count while maintaining performance.
---

# Discrete Prompt Compression with Reinforcement Learning

## Quick Facts
- arXiv ID: 2308.08758
- Source URL: https://arxiv.org/abs/2308.08758
- Reference count: 22
- Key outcome: Discrete prompt compression method using RL achieves 24.6% token reduction while maintaining performance across different language model architectures.

## Executive Summary
This paper introduces PCRL, a discrete prompt compression method using reinforcement learning to address limitations of existing gradient-based compression techniques. The approach employs a computationally efficient policy network that directly edits prompts to remove non-contributory tokens without requiring gradient access to language models or labeled data. Through comprehensive experiments, PCRL achieves an average 24.6% reduction in token count while maintaining output quality, outperforming heuristic approaches and demonstrating transferability to larger models including LLaMa2-7B and FLAN-T5-XXL-11B.

## Method Summary
PCRL trains a policy network using reinforcement learning to make binary include/exclude decisions for each token in a prompt. The method uses a DistilBERT backbone with MLP layers to process prompts and output token-level decisions, with action masking to preserve required statement tokens. Training employs a reward function balancing ROUGE-L score preservation (measuring output quality similarity) with compression ratio, using policy gradient algorithms with SCST baseline and entropy regularization. The policy is trained on the Alpaca+ dataset and evaluated across multiple language models including GPT2-XL, FLAN-T5-XL, and transfer models LLaMa2-7B, Falcon-7B, and FLAN-T5-XXL-11B.

## Key Results
- Achieves 24.6% average reduction in token count while maintaining performance
- Outperforms heuristic methods in terms of faithfulness to original prompts
- Successfully transfers learned policy to larger language models (47.3% win rate on GPT2-XL, 45.8% on FLAN-T5-XL)
- Identifies token categories commonly removed: stopwords, punctuation, and word endings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The RL-based policy can compress prompts without requiring gradient access to the generation LM.
- Mechanism: The reward function uses ROUGE-L scores computed from LM outputs rather than gradients, enabling training via policy gradient methods.
- Core assumption: ROUGE-L scores provide a reliable proxy for output quality similarity between original and compressed prompts.
- Evidence anchors: [abstract] "can be trained without gradient access to LMs or labeled data"; [section 3.1] "trained by a reward function that balances both the faithfulness... and their reduced length, through a policy gradient algorithm."

### Mechanism 2
- Claim: The policy learns to identify and remove non-contributory tokens while preserving semantic content.
- Mechanism: Through iterative RL training, the policy learns token importance patterns, preferentially removing stopwords, punctuation, and word endings.
- Core assumption: Certain token categories (stopwords, punctuation, endings) are consistently less important for maintaining output quality.
- Evidence anchors: [section 4.3] "categories of the eliminated tokens primarily belong to three main groups: stopword, punctuation, and ending"; [section 4.1] "significantly outperformed heuristic methods in terms of faithfulness."

### Mechanism 3
- Claim: The learned policy generalizes across different LM architectures and can be transferred to larger models.
- Mechanism: Discrete prompt editing creates model-agnostic compressed prompts that can be applied to any LM using the same tokenizer.
- Core assumption: Token-level compression decisions transfer across models with similar language understanding capabilities.
- Evidence anchors: [abstract] "learned policy can be transferred to larger LMs"; [section 4.2] "LLaMa2 demonstrated a successful transfer with a win rate of 47.3%."

## Foundational Learning

- Concept: Reinforcement Learning with Policy Gradient
  - Why needed here: Enables training without gradient access to the generation LM, using only output-based rewards
  - Quick check question: How does the SCST baseline help stabilize policy gradient training?

- Concept: ROUGE-L Score as Evaluation Metric
  - Why needed here: Provides a sentence-level similarity measure that correlates with output quality preservation
  - Quick check question: Why might ROUGE-L be more appropriate than ROUGE-1/2 for this task?

- Concept: Token Classification and Masking
  - Why needed here: Allows the policy to make binary decisions about token inclusion while preserving required prompt structure
  - Quick check question: What purpose do the action masks serve in the policy network?

## Architecture Onboarding

- Component map: Alpaca+ dataset → Policy network (DistilBERT + MLP) → Token decisions → Compressed prompt → Generation LM → Output responses → ROUGE-L calculation → Reward computation → Policy update
- Critical path: 1. Sample prompt from pool 2. Policy network processes prompt → token decisions 3. Compressed prompt fed to generation LM 4. ROUGE-L scores computed between original and compressed outputs 5. Reward calculated (ROUGE-L + compression ratio) 6. Policy parameters updated via policy gradient
- Design tradeoffs: Single-step vs multi-step decision making: Single-step chosen for computational efficiency; SCST vs other baselines: SCST provides stable training but can lead to near-zero gradients when rewards are similar; Entropy term: Added to encourage exploration and prevent premature convergence
- Failure signatures: Policy consistently removes all tokens → reward threshold too high or compression ratio weight too strong; No compression achieved → reward structure may not properly incentivize compression; Poor transfer to new LMs → tokenization mismatch or fundamental architectural differences
- First 3 experiments: 1. Verify basic functionality: Run single prompt through trained policy and confirm compression occurs 2. Ablation study: Test with and without entropy term to measure exploration impact 3. Transfer test: Apply policy trained on GPT2-XL to FLAN-T5-XL and measure performance drop

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the PCRL method's performance compare to other prompt compression techniques that use gradient-based optimization?
- Basis in paper: [inferred] The paper mentions that existing methods rely heavily on training embeddings using gradient descent, but does not provide a direct comparison between PCRL and these methods.
- Why unresolved: The paper does not provide a direct comparison between PCRL and gradient-based optimization techniques, making it difficult to determine the relative effectiveness of PCRL.
- What evidence would resolve it: A direct comparison between PCRL and gradient-based optimization techniques using the same datasets and evaluation metrics would provide insight into the relative effectiveness of each method.

### Open Question 2
- Question: How does the PCRL method's performance scale with larger language models and more complex prompts?
- Basis in paper: [explicit] The paper mentions that the learned policy can be transferred to larger LMs, but does not provide detailed information on how the performance scales with model size or prompt complexity.
- Why unresolved: The paper does not provide a comprehensive analysis of how the PCRL method's performance changes as the size of the language model or the complexity of the prompts increases.
- What evidence would resolve it: A systematic study of the PCRL method's performance on various language model sizes and prompt complexities would provide valuable insights into its scalability.

### Open Question 3
- Question: How does the PCRL method's performance compare to human-written summaries of prompts?
- Basis in paper: [inferred] The paper mentions that the ROUGE-L score is used to measure the similarity between the original and compressed prompts, but does not provide a comparison to human-written summaries.
- Why unresolved: The paper does not provide a direct comparison between the PCRL method's performance and human-written summaries of prompts, making it difficult to determine the quality of the compression.
- What evidence would resolve it: A comparison between the PCRL method's compressed prompts and human-written summaries using human evaluation or other relevant metrics would provide insight into the quality of the compression.

## Limitations
- Transferability limitations: Performance across diverse LM architectures and domains remains untested
- Evaluation metric reliability: ROUGE-L may not fully capture semantic preservation or task-specific performance
- Token importance generalizability: Observed patterns may be dataset-specific to Alpaca+ instruction set

## Confidence
**High Confidence**: The RL-based policy can compress prompts without gradient access; achieves 24.6% token reduction while maintaining performance; learns to remove non-contributory tokens; outperforms heuristic methods
**Medium Confidence**: Policy generalizes across LM architectures; method can be trained without labeled data; SCST baseline provides stable training
**Low Confidence**: Long-term transferability to vastly different architectures; performance consistency across diverse domains; compression ratios generalize to all prompt types

## Next Checks
1. Cross-Domain Transfer Test: Apply trained policy to prompts from medical, legal, and creative writing domains to validate generalizability beyond instruction-following tasks
2. Human Evaluation Validation: Conduct human studies to verify ROUGE-L scores accurately predict output quality preservation, particularly for edge cases
3. Architecture Diversity Test: Test policy on encoder-decoder vs decoder-only models and models with different tokenization schemes to quantify transferability limits and identify architectural constraints