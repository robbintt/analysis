---
ver: rpa2
title: How Many Validation Labels Do You Need? Exploring the Design Space of Label-Efficient
  Model Ranking
arxiv_id: '2312.01619'
source_url: https://arxiv.org/abs/2312.01619
tags:
- ensemble
- z-score
- learning
- soft
- hard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LEMR (Label-Efficient Model Ranking), a novel
  framework that significantly reduces the need for costly annotations in model selection
  by strategically annotating instances from an unlabeled validation set. The framework
  employs an ensemble method to generate pseudo-labels, uses uncertainty sampling
  to acquire ground-truth labels for high-uncertainty instances, and applies a Z-score
  mechanism to dynamically adjust the model committee.
---

# How Many Validation Labels Do You Need? Exploring the Design Space of Label-Efficient Model Ranking

## Quick Facts
- arXiv ID: 2312.01619
- Source URL: https://arxiv.org/abs/2312.01619
- Reference count: 40
- Primary result: Reduces labeling costs by up to 90% while maintaining high model ranking accuracy

## Executive Summary
This paper introduces LEMR (Label-Efficient Model Ranking), a framework that dramatically reduces the need for costly annotations in model selection tasks. By strategically annotating instances from an unlabeled validation set, LEMR employs ensemble methods for pseudo-label generation, uncertainty sampling for ground-truth label acquisition, and Z-score mechanisms for dynamic model committee adjustment. The framework is evaluated on the comprehensive MoraBench Benchmark across weak supervision, semi-supervised learning, and prompt selection tasks.

The key finding is that LEMR can achieve comparable model ranking accuracy with only 10% of the labeling budget typically required. For instance, in the semi-supervised learning scenario (AGNews task), LEMR needs just 387 labeled samples compared to the conventional 2000 samples. The framework consistently outperforms random acquisition strategies and demonstrates that uncertainty sampling is superior across all tested scenarios. The Z-score-based model committee selection method also shows better performance than selecting all available models.

## Method Summary
LEMR operates through a four-step process: pseudo-label generation using ensemble methods (hard or soft), active label acquisition through uncertainty sampling, model committee selection via Z-score analysis, and final model ranking. The framework uses an ensemble of models to generate initial pseudo-labels on an unlabeled validation set, then strategically selects high-uncertainty instances for ground-truth labeling. Models are evaluated based on their accuracy against current pseudo-label and ground-truth sets, with Z-scores determining committee membership. The framework is tested across 23 different NLP tasks in the MoraBench Benchmark, covering weak supervision, semi-supervised learning, and prompt selection scenarios.

## Key Results
- Achieves up to 90% reduction in labeling costs while maintaining high model ranking accuracy
- Labels only 387 samples sufficient for model selection in AGNews semi-supervised learning task (vs. conventional 2000 samples)
- Z-score-based model committee selection outperforms All-model strategy across all datasets
- Uncertainty sampling strategies consistently outperform random acquisition across all tasks and scenarios

## Why This Works (Mechanism)

### Mechanism 1
LEMR reduces labeling costs by leveraging pseudo-labels generated from model ensembles, selectively acquiring ground-truth labels for high-uncertainty instances. The framework generates pseudo-labels using ensemble methods, identifies high-uncertainty instances through uncertainty sampling, and dynamically adjusts the model committee based on updated labels. The core assumption is that ensemble-generated pseudo-labels are sufficiently accurate to guide uncertainty sampling and committee selection.

### Mechanism 2
Z-score-based model committee selection outperforms all-model strategies by filtering out underperforming models. Models are evaluated based on their accuracy against current pseudo-label and ground-truth label sets, Z-scores are calculated, and models exceeding a threshold are selected for the committee. The core assumption is that model performance differences are detectable and meaningful through Z-score analysis.

### Mechanism 3
Uncertainty sampling strategies consistently outperform random acquisition across all tasks and scenarios. Uncertainty, margin, and entropy sampling strategies identify the most informative instances for labeling based on the model's confidence in its predictions. The core assumption is that the most uncertain predictions are indeed the most informative for improving model selection accuracy.

## Foundational Learning

- Concept: Ensemble methods for pseudo-label generation
  - Why needed here: Provides a mechanism to generate pseudo-labels without requiring ground-truth labels for the entire validation set
  - Quick check question: What are the key differences between hard ensemble and soft ensemble approaches for pseudo-label generation?

- Concept: Active learning and uncertainty sampling
  - Why needed here: Enables strategic selection of which instances to label, maximizing information gain while minimizing labeling costs
  - Quick check question: How do uncertainty, margin, and entropy sampling strategies differ in their approach to identifying informative instances?

- Concept: Z-score analysis for model performance evaluation
  - Why needed here: Provides a statistical framework for selecting high-performing models while filtering out underperformers
  - Quick check question: What assumptions are made when using Z-scores to evaluate model performance in this context?

## Architecture Onboarding

- Component map:
  - Model ensemble module (hard/soft ensemble) -> Uncertainty sampling module (uncertainty, margin, entropy, random) -> Z-score calculation module -> Model committee selection module -> Label acquisition management module -> Model ranking module

- Critical path:
  1. Initialize with model set M and unlabeled validation set DV
  2. Generate pseudo-labels using ensemble method
  3. Apply uncertainty sampling to select instances for labeling
  4. Acquire ground-truth labels and update label sets
  5. Calculate Z-scores and select model committee
  6. Rank models based on final label sets

- Design tradeoffs:
  - Hard ensemble vs. soft ensemble: Hard ensemble is more robust when models perform well, soft ensemble is better when models perform poorly
  - Uncertainty sampling vs. random sampling: Uncertainty sampling is more efficient but requires more computation
  - Z-score vs. all-model: Z-score is more precise but may reduce diversity

- Failure signatures:
  - High optimal gap values despite labeling efforts
  - Unstable model rankings across different budget ratios
  - Poor correlation between pseudo-labels and ground-truth labels

- First 3 experiments:
  1. Baseline comparison: Run LEMR with random sampling strategy vs. uncertainty sampling strategy on a simple dataset
  2. Ensemble method comparison: Compare hard ensemble vs. soft ensemble performance on datasets with varying model quality
  3. Budget efficiency test: Measure optimal gap reduction as labeling budget increases from 0% to 100% of validation set size

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of LEMR vary with different model committee sizes? The paper discusses model committee selection methods but does not explicitly explore the impact of varying committee sizes. What evidence would resolve it: Experimental results showing model ranking accuracy and labeling cost for different committee sizes.

### Open Question 2
What is the impact of different ensemble methods on the quality of pseudo-labels generated in LEMR? While the paper mentions both hard and soft ensemble methods, it does not provide a detailed comparison of their impact on pseudo-label quality. What evidence would resolve it: Detailed analysis of pseudo-label quality metrics for both hard and soft ensemble methods across various tasks.

### Open Question 3
How does LEMR perform in low-resource scenarios with extremely limited annotation budgets? The paper demonstrates LEMR's effectiveness in reducing labeling costs but does not explore its performance in scenarios with very low annotation budgets. What evidence would resolve it: Experimental results showing LEMR's performance with annotation budgets significantly lower than those tested in the paper.

## Limitations
- The framework relies on the assumption that model predictions follow a normal distribution for Z-score calculations, which may not hold for all model ensembles
- The Z-score threshold (τ) is set to 0.3 based on empirical observation without theoretical justification
- Evaluation is limited to the MoraBench Benchmark, which may not fully represent all real-world model selection scenarios

## Confidence

- High confidence: The general framework architecture and the superiority of uncertainty sampling over random acquisition strategies
- Medium confidence: The specific Z-score threshold of 0.3 for model committee selection and the claimed 90% reduction in labeling costs
- Low confidence: The generalization of results across all NLP tasks beyond the MoraBench Benchmark scope

## Next Checks

1. **Z-score threshold sensitivity analysis**: Systematically vary τ from 0.1 to 0.5 in increments of 0.05 and measure the impact on optimal gap and ranking correction across all datasets to determine if 0.3 is truly optimal or dataset-dependent.

2. **Distribution assumption validation**: Test whether model prediction distributions actually follow normality assumptions by applying statistical tests (e.g., Shapiro-Wilk) to prediction accuracy distributions before applying Z-scores, and develop alternative selection methods if normality is violated.

3. **Cross-domain generalization test**: Apply LEMR to at least two new domains not covered in MoraBench (e.g., biomedical NLP and legal text processing) to verify whether the 90% cost reduction claim holds across substantially different vocabulary, domain knowledge requirements, and evaluation metrics.