---
ver: rpa2
title: 'Battle of the Large Language Models: Dolly vs LLaMA vs Vicuna vs Guanaco vs
  Bard vs ChatGPT -- A Text-to-SQL Parsing Comparison'
arxiv_id: '2310.10190'
source_url: https://arxiv.org/abs/2310.10190
tags:
- language
- datasets
- large
- performance
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic evaluation of six popular large
  language models (Dolly, LLaMA, Vicuna, Guanaco, Bard, and ChatGPT) on nine benchmark
  Text-to-SQL datasets using five distinct prompting strategies. The evaluation covers
  both zero-shot and few-shot scenarios.
---

# Battle of the Large Language Models: Dolly vs LLaMA vs Vicuna vs Guanaco vs Bard vs ChatGPT -- A Text-to-SQL Parsing Comparison

## Quick Facts
- arXiv ID: 2310.10190
- Source URL: https://arxiv.org/abs/2310.10190
- Reference count: 40
- Key outcome: Open-source models significantly underperform closed-source models like GPT-3.5 across most Text-to-SQL datasets, with performance highly sensitive to few-shot examples.

## Executive Summary
This paper presents a systematic evaluation of six popular large language models (Dolly, LLaMA, Vicuna, Guanaco, Bard, and ChatGPT) on nine benchmark Text-to-SQL datasets using five distinct prompting strategies. The evaluation covers both zero-shot and few-shot scenarios. The results show that open-source models significantly underperform closed-source models like GPT-3.5 across most Text-to-SQL datasets. While LLMs can generate syntactically valid SQL statements, they often struggle to produce semantically accurate queries. The performance of LLMs is highly sensitive to the examples used for few-shot learning, and there is no universal prompting strategy that works well across all models.

## Method Summary
The study evaluates six large language models on nine Text-to-SQL benchmark datasets using five prompting strategies (Informal Schema, API Docs, Select 3, 1-Shot Learning, 5-Shot Learning) in both zero-shot and few-shot settings. Models are assessed on execution accuracy (EX) and test suite accuracy (TS) metrics. The evaluation includes generating SQL queries from natural language questions, validating SQL syntax, executing queries, and comparing results to gold answers. The authors also analyze the proportion of valid SQL statements and sensitivity to example styles.

## Key Results
- Open-source models significantly underperform closed-source models like GPT-3.5 across most Text-to-SQL datasets.
- LLMs can generate syntactically valid SQL statements but often struggle to produce semantically accurate queries.
- Few-shot learning performance is highly sensitive to the examples provided in the prompt, with no universal prompting strategy working well across all models.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Large language models can generate syntactically valid SQL statements even without instruction fine-tuning.
- **Mechanism:** The autoregressive generation capability of decoder-based LLMs allows them to produce grammatically correct SQL queries by leveraging learned language patterns from large-scale text corpora.
- **Core assumption:** The training data contained sufficient SQL-like structured language patterns for the models to internalize SQL syntax rules.
- **Evidence anchors:**
  - [abstract] "While LLMs demonstrate proficiency in generating syntactically valid SQL statements, they often struggle to produce semantically accurate queries."
  - [section] "LLaMA also demonstrates the ability to generate valid SQL statements, even though it was not specifically fine-tuned on instruction datasets."
- **Break condition:** If the training data lacks SQL-like structured language patterns, the models may fail to generate syntactically valid SQL.

### Mechanism 2
- **Claim:** Few-shot learning performance is highly sensitive to the examples provided in the prompt.
- **Mechanism:** LLMs adapt their output style and structure based on the examples in the prompt, including adopting canonicalized SQL formats.
- **Core assumption:** The model can recognize and replicate patterns from the provided examples, including SQL formatting conventions.
- **Evidence anchors:**
  - [section] "When large language models are presented with examples from classical datasets, they start to generate SQL in a style similar to the canonicalized format described in Finegan-Dollak et al. (2018)."
  - [section] "LLaMA stands out among all the models, as it consistently appends the term 'alias' to over 86% of the generated SQL statements."
- **Break condition:** If the examples are too dissimilar from the target domain or lack clear patterns, the model may not effectively adapt.

### Mechanism 3
- **Claim:** Closed-source models significantly outperform open-source models on Text-to-SQL tasks.
- **Mechanism:** Closed-source models like GPT-3.5 have been trained on larger datasets and with more computational resources, resulting in better performance on complex reasoning tasks like Text-to-SQL.
- **Core assumption:** The performance gap is primarily due to differences in model size, training data quality, and fine-tuning methods rather than fundamental architectural differences.
- **Evidence anchors:**
  - [abstract] "Open-source models demonstrate notably inferior performance compared to closed-source models across a majority of Text-to-SQL datasets."
  - [section] "GPT-3.5 is the leading model, surpassing the second-place Bard model by 17.8% in execution accuracy (EX) and by 14.1% in test suite accuracy (TS)."
- **Break condition:** If open-source models receive equivalent training resources and fine-tuning methods, the performance gap may narrow.

## Foundational Learning

- **Concept:** Text-to-SQL parsing
  - **Why needed here:** Understanding the task of converting natural language questions into SQL queries is essential for evaluating LLMs' capabilities in this domain.
  - **Quick check question:** What is the primary challenge in Text-to-SQL parsing that LLMs often struggle with, despite generating valid SQL?

- **Concept:** Prompt engineering
  - **Why needed here:** Different prompting strategies (zero-shot, few-shot, schema descriptions) significantly impact LLM performance on Text-to-SQL tasks.
  - **Quick check question:** How does the inclusion of database schema information in prompts affect LLM performance on Text-to-SQL tasks?

- **Concept:** Model evaluation metrics
  - **Why needed here:** Understanding execution accuracy (EX) and test suite accuracy (TS) is crucial for interpreting the results of LLM evaluations on Text-to-SQL tasks.
  - **Quick check question:** Why is execution accuracy preferred over exact match accuracy for evaluating SQL generation tasks?

## Architecture Onboarding

- **Component map:** LLM inference engine (Dolly, LLaMA, Vicuna, Guanaco, Bard, GPT-3.5) -> Prompt generation module (IS, AD, S3, 1SL, 5SL strategies) -> Evaluation framework (execution accuracy, test suite accuracy) -> Data preprocessing pipeline (schema extraction, example selection)

- **Critical path:** 1. Load database schema and example data 2. Generate prompts using selected strategy 3. Send prompts to LLM for inference 4. Parse and validate generated SQL 5. Execute SQL and compare results with gold answers 6. Calculate evaluation metrics

- **Design tradeoffs:**
  - Model size vs. inference speed (larger models may perform better but are slower)
  - Prompt length vs. context window limitations (longer prompts may provide more information but risk truncation)
  - Zero-shot vs. few-shot learning (few-shot may improve performance but requires careful example selection)

- **Failure signatures:**
  - Consistently low execution accuracy across all models and strategies
  - High proportion of invalid SQL statements
  - Significant performance degradation with certain prompting strategies (e.g., Select 3)

- **First 3 experiments:**
  1. Compare zero-shot performance across all models using Informal Schema (IS) prompts
  2. Evaluate the impact of 1-shot learning on the best-performing zero-shot model
  3. Test the sensitivity of model performance to different example selection strategies in few-shot prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do performance trends differ across various prompt strategies (IS, AD, S3, 1SL, 5SL) when models are trained with larger datasets?
- Basis in paper: [inferred] The paper evaluates performance using five distinct prompt strategies and highlights that LLMs are highly sensitive to the examples utilized for few-shot learning, but it does not explore the impact of training dataset size on performance across different prompt strategies.
- Why unresolved: The paper focuses on the sensitivity of LLMs to examples and prompt styles but does not investigate whether increasing the training dataset size could mitigate performance gaps across different prompting strategies.
- What evidence would resolve it: Systematic experiments comparing model performance across varying training dataset sizes while maintaining consistent prompt strategies would reveal whether larger datasets improve generalization across different prompting approaches.

### Open Question 2
- Question: What is the impact of instruction-tuning on open-source models' ability to generate semantically accurate SQL queries?
- Basis in paper: [explicit] The paper notes that open-source models significantly underperform closed-source models like GPT-3.5, and while they can generate syntactically valid SQL statements, they often struggle to produce semantically accurate queries.
- Why unresolved: The study does not isolate the effect of instruction-tuning on the semantic accuracy of SQL queries, leaving open the question of whether instruction-tuning specifically improves semantic understanding.
- What evidence would resolve it: Comparing the semantic accuracy of SQL queries generated by models with and without instruction-tuning, while controlling for other variables, would clarify the impact of instruction-tuning on semantic accuracy.

### Open Question 3
- Question: How does the inclusion of database schema details in prompts affect the performance of LLMs on cross-domain Text-to-SQL tasks?
- Basis in paper: [inferred] The paper evaluates different prompting strategies, including those with varying levels of schema detail, but does not specifically address cross-domain performance or the role of schema detail in such contexts.
- Why unresolved: The study does not explore whether detailed schema information in prompts enhances or hinders model performance in cross-domain scenarios, which is a critical aspect of real-world Text-to-SQL applications.
- What evidence would resolve it: Experiments comparing model performance on cross-domain Text-to-SQL tasks with varying levels of schema detail in prompts would reveal the impact of schema information on cross-domain generalization.

### Open Question 4
- Question: What are the long-term effects of data contamination on the evaluation of zero-shot and few-shot learning capabilities in LLMs?
- Basis in paper: [explicit] The paper raises concerns about potential data contamination, questioning whether evaluations are truly zero-shot or few-shot given the exposure of models to evaluation data.
- Why unresolved: The study highlights the issue but does not provide a comprehensive analysis of how data contamination affects long-term model evaluation and development.
- What evidence would resolve it: Longitudinal studies tracking model performance over time with varying degrees of data contamination would provide insights into the long-term effects of data leakage on evaluation accuracy.

## Limitations

- The study focuses on English language SQL tasks, limiting generalizability to multilingual or domain-specific SQL tasks.
- Static prompting strategies are used without exploring dynamic or adaptive approaches that could potentially improve model performance.
- Resource constraints may have affected the evaluation of open-source models, as the paper does not specify equivalent computational resources for all models.

## Confidence

- **High Confidence:** The systematic comparison methodology and clear performance gap between closed-source and open-source models are well-supported by evaluation metrics.
- **Medium Confidence:** The sensitivity of few-shot learning to example selection is demonstrated, but the underlying reasons for this sensitivity are not explored in detail.
- **Low Confidence:** The claim that no universal prompting strategy works well across all models is based on specific strategies tested and may not hold for alternative approaches.

## Next Checks

1. Test the performance of evaluated models on SQL tasks from different domains (e.g., healthcare, legal) to assess generalizability beyond benchmark datasets.
2. Implement and evaluate adaptive prompting strategies that adjust based on model responses to determine if this can reduce the performance gap between open-source and closed-source models.
3. Re-run the evaluation of open-source models with guaranteed equivalent computational resources to determine if hardware limitations influenced the observed performance differences.