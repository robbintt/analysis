---
ver: rpa2
title: Diffusion-based Visual Counterfactual Explanations -- Towards Systematic Quantitative
  Evaluation
arxiv_id: '2308.06100'
source_url: https://arxiv.org/abs/2308.06100
tags:
- counterfactual
- classifier
- class
- target
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a systematic framework for evaluating visual
  counterfactual explanations (VCE) generated by diffusion models. It introduces key
  metrics including target accuracy (TA), oracle target accuracy (OTA), LPIPS for
  closeness, and FID for realism.
---

# Diffusion-based Visual Counterfactual Explanations -- Towards Systematic Quantitative Evaluation

## Quick Facts
- arXiv ID: 2308.06100
- Source URL: https://arxiv.org/abs/2308.06100
- Reference count: 33
- Key outcome: Diffusion-based VCEs achieve high target accuracy but low oracle target accuracy, suggesting most generated examples are adversarial rather than genuine counterfactuals

## Executive Summary
This paper introduces a systematic framework for quantitatively evaluating visual counterfactual explanations (VCE) generated by diffusion models. The authors propose key metrics including target accuracy (TA), oracle target accuracy (OTA), LPIPS for closeness, and FID for realism, and conduct extensive experiments on ImageNet across seven classifiers. Their findings reveal that while diffusion-based VCEs achieve high TA, the OTA remains low across most classifiers, suggesting generated examples often act as adversarial rather than genuine counterfactuals. The study identifies cone-projection and x0-prediction as crucial design choices for producing valid counterfactuals, while showing that SIMCLR guidance performs poorly. The paper provides a complete codebase and calls for rigorous quantitative evaluation practices in future VCE research.

## Method Summary
The method uses diffusion probabilistic models (DDPMs) with classifier guidance to generate counterfactual images that would cause a target classifier to predict a different class. The approach incorporates two key design choices: x0-prediction, which uses denoised image approximations as classifier inputs, and cone-projection, which projects gradients from the target classifier onto a 30° cone around robust classifier gradients. The framework evaluates generated counterfactuals using four metrics: target accuracy (whether the classifier changes its prediction), oracle target accuracy (whether multiple classifiers agree on the target class), LPIPS (perceptual similarity), and FID (distributional realism). Experiments compare seven different classifiers with various combinations of design choices.

## Key Results
- Diffusion-based VCEs achieve high target accuracy (TA) but consistently low oracle target accuracy (OTA) across classifiers
- Cone-projection and x0-prediction are identified as crucial design choices for producing valid counterfactuals
- SIMCLR guidance shows notably poor performance with low TA and OTA scores
- Generating counterfactuals for non-ideal target classes often fails completely
- Generated examples often act as adversarial examples rather than genuine counterfactuals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cone-projection using robust classifier gradients improves validity of counterfactuals by reducing noise in gradient directions
- Mechanism: The method replaces the gradients from the classifier being explained (∇x log pϕ) with those from a robust classifier (∇x log pr) projected onto a 30° cone around the original gradients. This projection reduces gradient noise while maintaining semantic direction toward the target class.
- Core assumption: Robust classifiers have less noisy gradients than non-robust classifiers, and a 30° cone projection preserves sufficient directional information while filtering noise
- Evidence anchors:
  - [abstract]: "The cone-projection and x0-prediction are identified as crucial for producing valid counterfactuals"
  - [section]: "One difficulty in using the classifier guidance of [8] for VCEs of an arbitrary classifier pϕ is that pϕ cannot be expected to have been trained on noisy images"
  - [corpus]: Weak evidence - corpus papers discuss counterfactual methods but don't specifically address cone-projection or robust classifier guidance
- Break condition: If the robust classifier's feature space differs significantly from the target classifier, or if the 30° cone is too restrictive, the projection may lose critical directional information

### Mechanism 2
- Claim: x0-prediction approach enables counterfactual generation for classifiers not trained on noisy images by providing denoised inputs to the classifier
- Mechanism: During diffusion sampling, instead of using the noisy xt directly as classifier input, the method uses the denoised approximation ˆx0 = xθ(xt, t). This allows the classifier to operate on meaningful image content rather than noise
- Core assumption: The denoising function xθ provides a sufficiently accurate approximation of the original image at each diffusion step, and the classifier can provide meaningful gradients on these denoised inputs
- Evidence anchors:
  - [abstract]: "We explore the effects of certain crucial design choices in the latest diffusion-based generative models for VCEs of natural image classification (ImageNet)"
  - [section]: "To tackle this issue, the authors in both the DVCE and DiME paper propose to replace the noisy example xt in the input of the classifier by an approximation of the denoised image"
  - [corpus]: No direct evidence in corpus about x0-prediction specifically, though diffusion-based counterfactual methods are mentioned
- Break condition: If the denoising function introduces artifacts or the approximation is poor, classifier gradients may be meaningless or misleading

### Mechanism 3
- Claim: The framework's comprehensive evaluation methodology reveals that diffusion-based VCEs often produce adversarial examples rather than genuine counterfactuals, exposing classifier weaknesses
- Mechanism: By using multiple metrics (TA, OTA, LPIPS, FID) across various classifiers and ablation conditions, the framework quantifies the gap between classifier behavior and true semantic changes, revealing when generated examples fool classifiers without representing true class changes
- Core assumption: Oracle classifiers (OTA) can serve as reasonable proxies for human assessment of whether examples belong to target classes, and that multiple metrics provide complementary insights into VCE quality
- Evidence anchors:
  - [abstract]: "Our findings suggest multiple directions for future advancements and improvements of VCE methods"
  - [section]: "We systematically and quantitatively evaluate diffusion-generated VCEs (see Section 2.3) across a suite of classifiers"
  - [corpus]: Weak evidence - corpus contains related counterfactual explanation papers but lacks systematic quantitative frameworks
- Break condition: If oracle classifiers share biases with the target classifier or if the metric suite fails to capture important aspects of counterfactual quality

## Foundational Learning

- Concept: Diffusion probabilistic models and denoising processes
  - Why needed here: The entire counterfactual generation mechanism relies on understanding how DDPMs progressively add and remove noise to generate images
  - Quick check question: What is the relationship between the forward diffusion process (adding noise) and the reverse process (denoising) in DDPMs?

- Concept: Classifier guidance in generative models
  - Why needed here: The method adapts classifier gradients to steer the diffusion process toward specific target classes
  - Quick check question: How does scaling the classifier gradients (parameter s) affect the trade-off between sample diversity and class consistency?

- Concept: Counterfactual explanation vs adversarial examples
  - Why needed here: The framework specifically distinguishes between examples that fool classifiers (adversarial) versus those that truly represent target classes (counterfactual)
  - Quick check question: What key criterion distinguishes counterfactual explanations from adversarial examples according to the paper's framework?

## Architecture Onboarding

- Component map:
  Pre-trained DDPM model (OpenAI guided-diffusion implementation) -> Target classifier(s) to be explained (7 different architectures) -> Robust classifier for cone-projection (Madry classifier) -> Oracle classifiers for validity assessment (multiple classifiers) -> Evaluation metrics pipeline (TA, OTA, LPIPS, FID calculations) -> GPU parallelization infrastructure (DataParallel across 4 A100 GPUs)

- Critical path:
  1. Load pre-trained DDPM and target classifier
  2. For each source image, initialize diffusion sampling from xT/2
  3. At each sampling step: denoise, apply classifier guidance (with/without x0-prediction), apply cone-projection (with/without)
  4. Generate counterfactual image
  5. Evaluate using target classifier and oracle classifiers
  6. Calculate metrics (TA, OTA, LPIPS, FID)
  7. Aggregate results across all source-target pairs

- Design tradeoffs:
  - Memory vs computation: Using x0-prediction requires backpropagating through DDPM, increasing memory usage significantly
  - Quality vs diversity: Higher gradient scaling produces more class-consistent samples but reduces diversity
  - Speed vs thoroughness: Running all 28 ablation experiments (4x7 classifiers) provides comprehensive results but requires ~2 days on limited hardware

- Failure signatures:
  - Zero TA with non-zero OTA: Method introduces changes but classifier doesn't flip class (likely adversarial examples)
  - High LPIPS but low OTA: Large perceptual changes that don't achieve semantic target class
  - High FID for certain classifiers: Generated examples poorly match target class distribution
  - Extremely low TA for SIMCLR: Self-supervised nature incompatible with guidance mechanism

- First 3 experiments:
  1. Baseline evaluation: Run with x0-prediction and cone-projection for Madry classifier on all source-target pairs to establish reference performance
  2. Ablation 1: Disable x0-prediction for Madry classifier to verify minimal impact on robust classifier
  3. Ablation 2: Disable cone-projection for Madry classifier to confirm it maintains validity (since projection has no effect on Madry's own gradients)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can VCE methods be improved to generate diverse counterfactuals for a single input image, and what are the optimal strategies for ensuring meaningful diversity?
- Basis in paper: [explicit] The paper notes that VCE methods rarely address the ability to generate multiple valid counterfactuals for a single analyzed image and suggests evaluating diversity using pairwise LPIPS, but acknowledges the computational challenges in doing so.
- Why unresolved: The study did not conduct experiments to evaluate diversity due to hardware limitations and computational time constraints.
- What evidence would resolve it: Experimental results comparing multiple counterfactuals generated for the same input using different random seeds or alternative diversity-inducing techniques, along with quantitative metrics like pairwise LPIPS.

### Open Question 2
- Question: What are the most effective strategies for selecting "ideal" target classes for counterfactual generation in natural image classification, and how can semantic similarity between source and target classes be quantified?
- Basis in paper: [explicit] The paper questions the arbitrary selection of "ideal" source-target pairs based on WordNet similarity and suggests that other pairs may be required in practice, noting that some target classes are more suitable for counterfactual generation than others.
- Why unresolved: The study used predefined source-target pairs from prior work and did not systematically explore the impact of different target class selections or develop a methodology for identifying optimal pairs.
- What evidence would resolve it: Comparative experiments showing the performance of VCE methods across a wide range of source-target pairs, along with a proposed framework for quantifying semantic similarity between classes in the context of counterfactual generation.

### Open Question 3
- Question: How can VCE methods be adapted to handle self-supervised classifiers like SIMCLR, and what are the underlying reasons for their poor performance in generating valid counterfactuals?
- Basis in paper: [explicit] The study found that SIMCLR guidance shows notably poor performance, with low TA and OTA scores, and speculates that this may be due to the self-supervised nature of SIMCLR and the resulting mismatch between its hidden features and the classification task.
- Why unresolved: The study did not conduct a detailed investigation into the specific challenges posed by self-supervised classifiers or explore potential modifications to VCE methods to address these challenges.
- What evidence would resolve it: Experiments comparing the performance of VCE methods on self-supervised classifiers with their performance on supervised classifiers, along with analysis of the feature representations learned by self-supervised models and their impact on counterfactual generation.

## Limitations

- Limited generalizability to domains beyond ImageNet and classification tasks, with uncertain applicability to medical imaging, satellite imagery, or other modalities
- Oracle classifiers may not perfectly represent human judgment of counterfactual validity, potentially overestimating the gap between TA and OTA
- Computational constraints limited the study to 7 classifiers and 17 source-target pairs rather than comprehensive ImageNet coverage

## Confidence

- Empirical performance measurements and metric calculations: High confidence
- Importance of x0-prediction and cone-projection: High confidence
- Generalization to other domains/tasks: Low confidence
- Theoretical understanding of why certain methods fail: Medium confidence

## Next Checks

1. **Cross-domain validation**: Test the framework on non-ImageNet datasets (e.g., medical imaging or satellite imagery) to assess generalizability beyond natural images.

2. **Human evaluation study**: Conduct user studies comparing oracle classifier judgments against human assessments of counterfactual validity to validate the OTA metric's reliability.

3. **Alternative guidance mechanisms**: Experiment with other forms of classifier guidance (e.g., feature space guidance, CLIP-based guidance) to determine if the performance gaps persist across different steering approaches.