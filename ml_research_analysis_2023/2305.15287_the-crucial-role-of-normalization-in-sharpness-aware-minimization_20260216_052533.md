---
ver: rpa2
title: The Crucial Role of Normalization in Sharpness-Aware Minimization
arxiv_id: '2305.15287'
source_url: https://arxiv.org/abs/2305.15287
tags:
- usam
- sign
- normalization
- theorem
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the role of normalization in Sharpness-Aware
  Minimization (SAM), a recently proposed optimizer that has achieved impressive empirical
  performance. The authors theoretically and empirically investigate the impact of
  the normalization factor in SAM updates, comparing it with an unnormalized variant
  (USAM).
---

# The Crucial Role of Normalization in Sharpness-Aware Minimization

## Quick Facts
- arXiv ID: 2305.15287
- Source URL: https://arxiv.org/abs/2305.15287
- Reference count: 40
- One-line primary result: Normalization in SAM stabilizes training and enables drift along manifolds of minima, improving robustness to hyperparameters.

## Executive Summary
This paper investigates the theoretical and empirical role of normalization in Sharpness-Aware Minimization (SAM), comparing it with an unnormalized variant (USAM). The authors show that normalization prevents divergence in strongly convex and smooth loss landscapes, whereas USAM can diverge even with lower learning rates. Additionally, normalization enables continuous drift along a manifold of minima, improving generalization. Experiments on matrix sensing, single-neuron networks, and neural networks validate these findings, demonstrating that normalization makes SAM more robust to hyperparameter choices, particularly the perturbation radius ρ.

## Method Summary
The paper compares SAM and USAM optimization algorithms on various toy and real-world problems. SAM updates parameters by normalizing the ascent direction, while USAM uses the unnormalized gradient. The authors analyze stability in strongly convex and smooth losses, study drifting behavior near manifolds of minima, and validate findings through experiments on matrix sensing, single-neuron networks, and neural networks on CIFAR-10 and CIFAR-100. They measure stability, convergence behavior, test accuracy, and bias in ReLU units.

## Key Results
- Normalization stabilizes SAM, preventing divergence under the same learning rate that allows GD to converge, while USAM can diverge even with a lower learning rate.
- Normalization enables SAM to drift along a continuum of minima, while USAM gets trapped at non-zero points on the manifold, requiring careful tuning to avoid divergence.
- Normalization makes SAM robust to hyperparameter choices, particularly the perturbation radius ρ, supporting its practicality in real-world applications.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Normalization prevents aggressive ascent steps that would otherwise destabilize training.
- **Mechanism:** Without normalization, the perturbation term ρ∇L(wt) can have unbounded magnitude when ∥∇L(wt)∥ is large. This leads to wt+ρ∇L(wt) being far from wt, amplifying the subsequent gradient step and causing divergence. Normalization bounds the ascent direction to unit length, so the perturbation magnitude stays at ρ regardless of gradient scale.
- **Core assumption:** The loss landscape is smooth enough that bounded perturbations preserve gradient quality; i.e., ∥∇L(wt+ρ·)∥ ≤ β∥wt+ρ·−wt∥.
- **Evidence anchors:**
  - [abstract] "Normalization helps stabilize the algorithm. For strongly convex and smooth losses, SAM does not diverge under the same learning rate that allows GD to converge, whereas USAM can diverge even with a lower learning rate."
  - [section 2.1] Theorem 1 showing GD's convergence bound η < 2/β implies SAM's non-divergence, but USAM diverges when η > 2/(β+ρβ²).
  - [corpus] Weak: no direct mention of β-smoothness bounds in cited works.
- **Break condition:** If β is infinite or very large (e.g., loss with non-Lipschitz gradients), normalization cannot bound the ascent step's effect.

### Mechanism 2
- **Claim:** Normalization enables continuous drift along a manifold of minima, improving generalization.
- **Mechanism:** Near a flat minimum, both wt and ∇L(wt) shrink toward zero. Without normalization, the ascent step wt+ρ∇L(wt) collapses to wt, so gradient updates vanish and the iterate stops moving. With normalization, the ascent direction remains a unit vector perpendicular to the manifold, keeping the descent gradient at a point far enough away to maintain non-zero updates and enable drift.
- **Core assumption:** The loss admits a connected set of global minima (a manifold) and iterates can approach it without immediate convergence to a single point.
- **Evidence anchors:**
  - [abstract] "Normalization enables the algorithm to drift along a continuum (manifold) of minima... SAM keeps moving towards the origin regardless of hyper-parameter choices, while USAM gets trapped at non-zero points on the manifold."
  - [section 3.1.1] Theorem 4 showing SAM approaches the origin while USAM gets stuck at a non-zero lower bound.
  - [corpus] No direct evidence; related works focus on sharpness, not manifold drift.
- **Break condition:** If the minimum set is discrete (isolated points), there is no manifold to drift along, so the benefit disappears.

### Mechanism 3
- **Claim:** Normalization decouples robustness to the perturbation radius ρ from learning-rate tuning.
- **Mechanism:** By bounding the ascent direction, normalization makes the effective step size depend mainly on η, not ρ. This isolates ρ's role to shaping the geometry of the perturbed point, not the magnitude of the update. Thus, once η is chosen for stability, ρ can be tuned independently for generalization benefits.
- **Core assumption:** The learning rate η is already in a stability regime where normalization's bounding effect dominates.
- **Evidence anchors:**
  - [abstract] "These two properties of normalization make SAM robust against the choice of hyper-parameters, supporting the practicality of SAM."
  - [section 4] Case study showing SAM maintains negative bias in ReLU units across different η values, while USAM requires careful joint tuning.
  - [corpus] No direct citations; practical tuning claims are based on Foret et al. (2021) guidelines.
- **Break condition:** If ρ becomes so large that the perturbation moves the iterate far outside the region where β-smoothness holds, normalization's bound no longer guarantees stability.

## Foundational Learning

- **Concept:** β-smoothness and strong convexity
  - Why needed here: These conditions give explicit bounds on gradient growth, which are essential for proving stability theorems and for understanding when normalization matters.
  - Quick check question: For a β-smooth loss, what is the maximum step size η that guarantees GD's iterates do not diverge?
- **Concept:** Manifold of minima and PL condition
  - Why needed here: Explains why algorithms can get stuck near minima and how normalization affects drift along flat regions.
  - Quick check question: In a PL loss, what is the relationship between gradient norm and distance to the nearest minimum?
- **Concept:** Edge-of-stability in deep learning
  - Why needed here: Provides context for why large learning rates can still find good minima despite theoretical instability bounds.
  - Quick check question: How does the sharpness of the nearest minimum affect the maximum stable learning rate in theory vs. practice?

## Architecture Onboarding

- **Component map:** Gradient computation -> (Optional) Normalization of ascent direction -> Perturbation step -> Second gradient computation -> Parameter update
- **Critical path:** Compute ∇L(wt), normalize if using SAM, form wt+ρ·, compute ∇L(wt+ρ·), apply descent step
- **Design tradeoffs:** Normalizing increases compute slightly (norm calculation, division) but buys stability and robustness to ρ. Removing normalization saves ops but risks divergence.
- **Failure signatures:** Divergence (loss spikes), premature stopping near a non-optimal minimum, sensitivity to ρ choice
- **First 3 experiments:**
  1. Run GD, SAM, and USAM on a β-smooth strongly convex quadratic with η near 2/β; compare convergence vs. divergence.
  2. Initialize all three on a 2D loss with a line of minima; track trajectory to see if USAM gets stuck while SAM drifts.
  3. Fix η, sweep ρ for SAM vs. USAM on a neural net; measure test accuracy variance to quantify robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the normalization factor in SAM specifically contribute to its stability compared to unnormalized methods, and can this be quantified for different types of loss landscapes?
- Basis in paper: [explicit] The paper states that normalization helps stabilize the algorithm and ensures SAM's non-divergence under the same learning rate that allows GD to converge, whereas USAM can diverge even with a lower learning rate.
- Why unresolved: The paper provides theoretical and empirical evidence but does not provide a quantitative measure of the stabilization effect for different loss landscapes.
- What evidence would resolve it: Experiments comparing the stability of SAM and USAM across a wide range of loss functions with varying degrees of smoothness and convexity, measuring the divergence rates and the critical learning rates for each method.

### Open Question 2
- Question: How does the normalization factor in SAM enable the algorithm to drift along a continuum of minima, and what are the implications of this behavior for generalization performance?
- Basis in paper: [explicit] The paper states that normalization enables the algorithm to drift along a continuum of minima, which is identified as a key property for better performance in recent theoretical works.
- Why unresolved: The paper provides theoretical and empirical evidence but does not provide a detailed explanation of the mechanism behind this drifting behavior or its direct impact on generalization.
- What evidence would resolve it: A theoretical analysis of the dynamics of SAM near a manifold of minima, quantifying the drift rate and its relationship to the sharpness of the loss landscape, and experiments comparing the generalization performance of SAM with and without normalization.

### Open Question 3
- Question: How robust is the performance of SAM to the choice of hyper-parameters, particularly the perturbation radius ρ, and can this robustness be explained by the normalization factor?
- Basis in paper: [explicit] The paper states that normalization makes SAM robust to the choice of hyper-parameters, particularly the perturbation radius ρ, supporting its practicality in real-world applications.
- Why unresolved: The paper provides empirical evidence but does not provide a theoretical explanation for the robustness of SAM to hyper-parameter choices or a quantitative measure of this robustness.
- What evidence would resolve it: A theoretical analysis of the sensitivity of SAM to hyper-parameter choices, quantifying the impact of normalization on this sensitivity, and experiments comparing the performance of SAM with different hyper-parameter settings, measuring the robustness of the algorithm to these choices.

## Limitations
- The theoretical analysis focuses on simple convex/non-convex cases, while real neural networks exhibit more complex loss landscapes and edge-of-stability dynamics that could alter normalization's impact.
- Empirical robustness claims have low-medium confidence since they rely on a limited set of experiments without comprehensive hyperparameter sweeps or ablation studies.
- The mechanism explaining how normalization enables manifold drift is medium confidence: the proof in the single-neuron case is clear, but the extension to general manifolds is not fully justified.

## Confidence
- **High:** Stability results for SAM vs USAM based on well-established convexity and smoothness assumptions.
- **Medium:** Mechanism explaining how normalization enables manifold drift; proof is clear for single-neuron case but extension to general manifolds is not fully justified.
- **Low-Medium:** Empirical robustness claims rely on limited experiments without comprehensive hyperparameter sweeps or ablation studies.

## Next Checks
1. Run SAM and USAM on a ResNet-18/CIFAR-10 setup with systematic sweeps of both η and ρ; measure test accuracy variance and loss stability to quantify robustness claims.
2. Implement a synthetic loss with a 2D manifold of minima (e.g., ℓ(x,y) = x² + y² with manifold along x=0) and track trajectories of SAM vs USAM to confirm or refute the drift mechanism.
3. Extend the stability analysis to non-Lipschitz losses (e.g., ReLU-based networks with unbounded gradients) to test whether normalization still guarantees bounded updates.