---
ver: rpa2
title: Multitask Learning Can Improve Worst-Group Outcomes
arxiv_id: '2312.03151'
source_url: https://arxiv.org/abs/2312.03151
tags:
- group
- learning
- task
- data
- worst-group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multitask learning (MTL) is widely used to improve average performance
  but its effect on worst-group outcomes is underexplored. This paper investigates
  whether and how MTL can be adapted to improve robustness to spurious correlations.
---

# Multitask Learning Can Improve Worst-Group Outcomes

## Quick Facts
- arXiv ID: 2312.03151
- Source URL: https://arxiv.org/abs/2312.03151
- Authors: [Not specified in input]
- Reference count: 27
- Primary result: Regularized MTL with L1 penalty on shared representations consistently improves worst-group accuracy over ERM and JTT

## Executive Summary
This paper investigates how multitask learning (MTL) can be adapted to improve robustness to spurious correlations and enhance worst-group outcomes. The authors propose a simple yet effective modification: multitask the end task with the pre-training objective (MLM for text, MIM for images) while applying L1 regularization to the shared, pre-prediction layer activations. Through extensive experiments on computer vision and NLP benchmarks, they demonstrate that this regularized MTL approach consistently outperforms standard ERM and JTT, especially when group annotations are unavailable. The method shows up to 2% improvement over ERM and 1% over JTT on worst-group accuracy.

## Method Summary
The method involves fine-tuning a pre-trained BERT or ViT model with two auxiliary objectives: the end task and a pre-training objective (MLM/MIM) constructed from the end-task data itself. L1 regularization is applied to the shared representation (CLS embedding) before task-specific heads. Training uses a task-heterogeneous batching scheme where each batch contains examples for either the end task or the auxiliary task. The combined loss includes the end task loss, auxiliary task loss weighted by α_aux, and L1 regularization weighted by α_reg.

## Key Results
- Regularized MTL consistently improves worst-group accuracy over ERM and JTT across multiple benchmarks
- The approach is especially effective when group annotations are unavailable for training
- Pre-training is critical for success - from-scratch training fails to improve worst-group outcomes
- Neither multitasking without regularization nor regularization without multitasking consistently improves performance; only their combination does

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Regularizing the shared representation space encourages the model to rely more on core features and less on spurious features.
- **Mechanism**: By applying ℓ1 regularization to the shared, pre-prediction layer activations, the model's capacity is restricted. This forces it to prioritize features that are useful for both the end task and the auxiliary reconstruction task, which are the core features.
- **Core assumption**: The auxiliary reconstruction task places higher weight on core features when spurious features have lower variance (σ²_spur ≤ σ²_core), as shown in synthetic data experiments.
- **Evidence anchors**: Synthetic data experiments show that under restricted capacity (low ℓ1-norm), the model assigns more weight to core features for the auxiliary reconstruction task, which then translates to improved worst-group accuracy on the end task.

### Mechanism 2
- **Claim**: Multitasking with the pre-training objective (e.g., MLM or MIM) improves robustness because these objectives are reconstruction tasks that inherently prefer core features.
- **Mechanism**: Pre-training objectives like masked language modeling and masked image modeling are reconstruction tasks. When multitask learning is applied with these objectives, the model learns to reconstruct inputs, which biases it toward using core features over spurious ones, especially when model capacity is constrained.
- **Core assumption**: Pre-training objectives are reconstruction tasks and can be effectively performed on end-task data without introducing external data.
- **Evidence anchors**: The paper shows that multitasking with MLM/MIM improves worst-group accuracy, and this improvement is conditional on sufficient prior pre-training.

### Mechanism 3
- **Claim**: The combination of multitasking with reconstruction objectives and ℓ1 regularization on shared representations consistently improves both average and worst-group accuracy.
- **Mechanism**: Multitasking with a reconstruction objective biases the model toward core features, while ℓ1 regularization ensures that the shared representation space does not overfit to spurious features. Together, they create a robust representation that generalizes well across groups.
- **Core assumption**: Both the auxiliary task and the regularization are necessary; neither alone is sufficient to consistently improve worst-group accuracy.
- **Evidence anchors**: Ablation studies show that neither multitasking without regularization nor regularization without multitasking consistently improves performance; only their combination does.

## Foundational Learning

- **Concept**: Spurious correlations and group robustness
  - **Why needed here**: The paper addresses how models can learn to rely on spurious features that correlate with labels in training data but do not generalize to minority groups. Understanding this is key to grasping why worst-group error occurs.
  - **Quick check question**: In a dataset where images of birds on water backgrounds are labeled as "water birds," what feature might the model incorrectly learn to rely on, and why would this hurt performance on birds on land?

- **Concept**: Multitask learning and auxiliary objectives
  - **Why needed here**: The method relies on multitasking the end task with a pre-training objective (e.g., MLM or MIM). Knowing how multitask learning works and what auxiliary objectives are helps understand the approach.
  - **Quick check question**: What is the difference between multitasking with an auxiliary task and multi-task learning with multiple end tasks of equal importance?

- **Concept**: Regularization techniques (ℓ1, ℓ2)
  - **Why needed here**: The method applies ℓ1 regularization to the shared representation space to constrain model capacity and force reliance on core features. Understanding regularization is essential to grasp how the method works.
  - **Quick check question**: How does ℓ1 regularization differ from ℓ2 in terms of its effect on model weights, and why might ℓ1 be preferred here?

## Architecture Onboarding

- **Component map**: Shared base model (BERT/ViT) -> [CLS] embedding -> ℓ1 regularization -> end-task-specific head (MLP) and auxiliary task head (MLM/MIM)
- **Critical path**: 
  1. Forward pass through shared base model to get [CLS] embedding
  2. Apply ℓ1 regularization to the embedding
  3. Pass regularized embedding to both end-task and auxiliary task heads
  4. Compute losses for both tasks
  5. Backpropagate combined loss (end task + α_aux * auxiliary task + α_reg * ℓ1 penalty)
- **Design tradeoffs**: 
  - Multitasking vs. single-task: Multitasking can improve robustness but adds complexity and computational cost
  - Choice of auxiliary task: Must be a reconstruction task that favors core features; not all auxiliary tasks are equally effective
  - Regularization strength: Too weak → spurious features still used; too strong → underfitting
- **Failure signatures**: 
  - No improvement in worst-group accuracy: Likely due to insufficient regularization or poor choice of auxiliary task
  - Worse average accuracy: Likely due to over-regularization or poor hyperparameter tuning
  - Instability during training: Could be due to improper weighting of auxiliary and regularization terms
- **First 3 experiments**:
  1. Reproduce baseline ERM and JTT results on Waterbirds to confirm poor worst-group performance
  2. Implement multitask learning with MLM/MIM auxiliary task without regularization; compare worst-group accuracy to baseline
  3. Add ℓ1 regularization to the multitask setup; tune α_aux and α_reg to maximize worst-group accuracy

## Open Questions the Paper Calls Out
The paper identifies several open questions for future research:
1. How to select or design auxiliary tasks that provably improve worst-group outcomes beyond reconstruction objectives
2. The precise relationship between pre-training quality, task capacity, and the effectiveness of regularized MTL in mitigating worst-group error
3. Whether multitask learning can be dynamically adapted during training (e.g., via meta-learning) to better target worst-group performance

## Limitations
- The effectiveness depends critically on the auxiliary task being a reconstruction objective that inherently favors core features
- The paper assumes group annotations are unavailable in practice, making comparisons to methods requiring group labels somewhat artificial
- The choice of L1 regularization over other forms is not empirically justified

## Confidence
- **High confidence**: The empirical observation that standard multitask learning without regularization does not consistently improve worst-group accuracy, and that combining MTL with L1 regularization shows consistent improvements across multiple benchmarks
- **Medium confidence**: The mechanism explanation that reconstruction tasks inherently prefer core features when capacity is constrained - this is theoretically plausible but primarily validated on synthetic data
- **Medium confidence**: The claim that this approach works especially well when group annotations are unavailable - while demonstrated empirically, the comparison to JTT (which requires group labels) may not reflect realistic deployment scenarios

## Next Checks
1. Test the robustness of the approach when the auxiliary task is not a reconstruction objective (e.g., using a classification auxiliary task) to verify that the reconstruction mechanism is essential
2. Conduct experiments where group annotations are available during training to compare against state-of-the-art group DRO methods rather than just ERM
3. Perform ablation studies comparing L1 regularization against L2 regularization and group sparsity regularization on the shared representation to validate the specific choice of L1