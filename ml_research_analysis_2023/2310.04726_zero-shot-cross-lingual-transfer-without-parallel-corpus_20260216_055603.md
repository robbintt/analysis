---
ver: rpa2
title: Zero-shot Cross-lingual Transfer without Parallel Corpus
arxiv_id: '2310.04726'
source_url: https://arxiv.org/abs/2310.04726
tags:
- language
- data
- labels
- cross-lingual
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a novel framework for zero-shot cross-lingual
  transfer without relying on parallel corpora or translation models. The approach
  consists of two main components: Bilingual Task Fitting and Self-training.'
---

# Zero-shot Cross-lingual Transfer without Parallel Corpus

## Quick Facts
- arXiv ID: 2310.04726
- Source URL: https://arxiv.org/abs/2310.04726
- Reference count: 7
- Key outcome: Achieves new state-of-the-art results on sentiment classification and NER tasks with up to 3.61 F1-score improvement over baseline multilingual BERT

## Executive Summary
This paper introduces a novel framework for zero-shot cross-lingual transfer that eliminates the need for parallel corpora or translation models. The approach combines Bilingual Task Fitting (BTF) with a self-training component that uses both soft and hard labels. The method achieves state-of-the-art results on sentiment classification and named entity recognition tasks across multiple languages including German, French, Japanese, Spanish, and Dutch.

## Method Summary
The framework consists of two main components: Bilingual Task Fitting and Self-training. BTF continues pre-training a multilingual model on task-related unlabeled data from both source and target languages using masked language modeling. The self-training component iteratively generates pseudo-labels for unlabeled target language data using both soft (predicted probabilities) and hard (discrete class) labels. An automatic confidence threshold selection mechanism and ensemble voting through a Voters Module improve the robustness and quality of pseudo-labels.

## Key Results
- Achieves new state-of-the-art results on sentiment classification and NER tasks
- Improves F1-score by up to 3.61 points compared to baseline multilingual BERT
- Demonstrates effectiveness across multiple languages: German, French, Japanese, Spanish, and Dutch
- Shows consistent improvements across different zero-shot transfer scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-specific bilingual pre-training improves cross-lingual transfer by aligning representations with task-specific semantic structures in both source and target languages.
- Mechanism: Bilingual Task Fitting (BTF) continues pre-training a multilingual model using masked language modeling on task-related unlabeled data from both source and target languages.
- Core assumption: Task-specific unlabeled data in both languages provides relevant semantic patterns that improve the model's ability to transfer knowledge across languages.

### Mechanism 2
- Claim: Self-training with soft and hard labels improves cross-lingual transfer by creating high-quality pseudo-labeled data in the target language.
- Mechanism: The self-training component generates soft labels (predicted probabilities) and hard labels (discrete class predictions) for unlabeled target language data.
- Core assumption: The model can generate reliable pseudo-labels that are accurate enough to improve performance when used for training.

### Mechanism 3
- Claim: Automatic confidence threshold selection and ensemble voting improve self-training stability and quality.
- Mechanism: The method automatically selects thresholds based on performance on source language validation data, using ensemble learning with multiple feature encoders.
- Core assumption: The relationship between confidence thresholds and accuracy/recall is similar between source and target languages.

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: BTF uses MLM to continue pre-training the multilingual model on task-related data, which helps align representations with task-specific semantics
  - Quick check question: How does MLM help a model learn contextual representations of words?

- Concept: Cross-entropy loss and mean squared error loss
  - Why needed here: The method uses cross-entropy loss for hard labels and MSE loss for soft labels during self-training
  - Quick check question: What's the difference between using cross-entropy loss versus MSE loss for training with labels?

- Concept: Ensemble learning and confidence thresholds
  - Why needed here: The Voter Module uses ensemble learning to improve robustness, and confidence thresholds are used to filter pseudo-labels
  - Quick check question: How does ensemble learning typically improve model robustness to noisy data?

## Architecture Onboarding

- Component map: Base multilingual model → BTF (task alignment) → Soft label fine-tuning → Hard label fine-tuning → Voter Module → Output predictions

- Critical path: Base model → BTF (task alignment) → Soft label fine-tuning → Hard label fine-tuning → Voter Module → Output predictions

- Design tradeoffs:
  - Using task-related unlabeled data in BTF improves task alignment but requires finding relevant data
  - Combining soft and hard labels in self-training balances stability and performance but adds complexity
  - Automatic threshold selection removes manual tuning but assumes similar accuracy-recall relationships between languages

- Failure signatures:
  - Poor performance on target language: BTF may not have found relevant task data, or self-training confidence thresholds are poorly chosen
  - Unstable training: Voter Module conflicts or inconsistent predictions across voters
  - Overfitting: Too many self-training iterations or insufficient diversity in pseudo-labeled data

- First 3 experiments:
  1. Test BTF alone: Apply only the Bilingual Task Fitting to see if task-specific pre-training improves baseline performance
  2. Test self-training alone: Apply self-training to the base model without BTF to measure the impact of pseudo-labeling
  3. Test threshold selection: Compare automatic threshold selection vs. fixed thresholds on a validation set to verify the effectiveness of the automatic approach

## Open Questions the Paper Calls Out

- How does the proposed method perform when transferring knowledge from multiple source languages simultaneously?
- What is the impact of different threshold selection strategies on the performance of the self-training component?
- How does the proposed method perform on languages with different linguistic properties?

## Limitations

- The Voters Module architecture and decision policy are not fully specified, making it difficult to assess its exact contribution
- The quality and relevance of task-related unlabeled data collected from Wikipedia is not thoroughly evaluated
- The method assumes similar accuracy-recall relationships between source and target languages for automatic threshold selection

## Confidence

**High Confidence**: The core claim that combining Bilingual Task Fitting with self-training improves cross-lingual transfer performance is well-supported by experimental results.

**Medium Confidence**: The claim about automatic confidence threshold selection being effective relies on the assumption that accuracy-recall relationships are similar across languages.

**Low Confidence**: The specific contribution of the Voters Module to overall performance improvement is difficult to assess due to limited architectural details.

## Next Checks

1. **Ablation study on Voters Module**: Remove the Voters Module from the pipeline and measure performance degradation to quantify its specific contribution.

2. **Pseudo-label quality analysis**: Conduct a detailed analysis of the accuracy and distribution of generated pseudo-labels across different confidence thresholds and languages.

3. **Threshold selection robustness test**: Test the automatic threshold selection method on language pairs with larger distribution shifts than those used in the paper.