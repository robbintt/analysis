---
ver: rpa2
title: 'Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning'
arxiv_id: '2312.08901'
source_url: https://arxiv.org/abs/2312.08901
tags:
- reasoning
- examples
- prompt
- tickets
- math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CoT-Max improves LLM mathematical reasoning by pruning redundant
  examples and tokens, allowing more informative few-shot Chain-of-Thought prompts
  within the context window. It uses a coarse-to-fine pruner to first select crucial
  examples and then prune unimportant tokens, trained via reinforcement learning on
  a diverse math dataset.
---

# Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning

## Quick Facts
- arXiv ID: 2312.08901
- Source URL: https://arxiv.org/abs/2312.08901
- Reference count: 40
- Primary result: CoT-Max improves LLM mathematical reasoning by pruning redundant examples and tokens

## Executive Summary
CoT-Max is a coarse-to-fine pruner that enhances LLM mathematical reasoning by selectively pruning redundant Chain-of-Thought examples and tokens. The approach addresses context window limitations by first identifying crucial examples and then removing unimportant tokens, enabling more informative few-shot prompts within the same context budget. Trained via reinforcement learning on a diverse math dataset, CoT-Max achieves up to 4.55% accuracy gains across multiple LLaMA2 models without requiring fine-tuning of the target LLMs.

## Method Summary
The method employs a two-stage reinforcement learning approach where a coarse-to-fine pruner first selects the most helpful Chain-of-Thought examples from a large batch, then prunes unimportant tokens within those examples. The pruner uses BERT-Large embeddings as input and makes binary decisions on which examples and tokens to keep. A multi-objective reward function balances LLM performance with token length constraints, and the REINFORCE algorithm trains the policy network. The MRD3 dataset, created with GPT-4 evolution, provides diverse training examples across different difficulty levels.

## Key Results
- CoT-Max achieves up to 4.55% accuracy gains on mathematical reasoning tasks
- Outperforms TopK+LLMLingua by 7.58% on GSM8K dataset
- Achieves 4.28× token compression while maintaining reasoning performance
- Improves LLaMA2-13B from 27.82% to 32.37% accuracy on GSM8K

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning redundant CoT examples and tokens frees up context window space for more informative examples, improving reasoning performance.
- Mechanism: The coarse-to-fine pruner first selects crucial CoT examples from a large batch, then prunes unimportant tokens within those examples to fit within the original LLM context window. This results in denser, more useful content for the LLM to solve math problems.
- Core assumption: Redundant tokens in natural language input, including numerical and format tokens, can be pruned without affecting the LLM's ability to comprehend the context for solving math problems.
- Evidence anchors:
  - [abstract] "CoT-Max addresses the challenges of the selection of useful examples and limited number of examples due to restricted context window length. Inspired by our observation that natural language inputs contain many redundancy, we propose a coarse-to-fine pruner as a plug-and-play module for LLMs, which first identifies crucial CoT examples from a large batch and then further prunes unimportant tokens to fit within the original LLM context window."
  - [section 3, Observation 3] "A CoT example contains redundant tokens for math reasoning, which can be pruned to free up space for more informative content."

### Mechanism 2
- Claim: Using reinforcement learning with a multi-objective reward function effectively trains the pruner to identify the most crucial CoT examples and useful tokens for math problem solving.
- Mechanism: The reward function measures both the effectiveness of the input CoT tokens for math reasoning and the token length constraints. REINFORCE is used to maximize this reward and train the two-stage policy network.
- Core assumption: The LLM loss gradient cannot be backpropagated through the tokenizer to update the pruner, so reinforcement learning is necessary.
- Evidence anchors:
  - [section 4.4] "We employ reinforcement learning to maximize the reward and train the two-stage policy network. According to REINFORCE (Williams, 1992), the network parameters are updated by the gradients: R · ∇ θ log π (a shot |s shot )π (a token |s token )"
  - [section 4.4] "To tackle this, we propose a multi-objective reward and leverage reinforcement learning for training"

### Mechanism 3
- Claim: The CoT dataset with diverse difficulty levels and reasoning steps enables the pruner to generalize across a wide range of math problems.
- Mechanism: The MRD3 dataset is created by merging existing CoT datasets and using GPT-4 and Evol-Instruct to generate diverse CoT examples with varying difficulty and reasoning steps. This dataset is used to train the pruner.
- Core assumption: A diverse dataset with varying difficulty levels and reasoning steps is necessary for the pruner to learn to identify the most crucial CoT examples and useful tokens for different types of math problems.
- Evidence anchors:
  - [section 4.1] "We employ GPT-4 (OpenAI, 2023) and Evol-Instruct (Xu et al., 2023) to create a math reasoning dataset, called MRD3. With problems of varying difficulty and reasoning steps, MRD 3 enables CoT-Max to generalize across a wide range of math problems."
  - [section 5.2] "We evaluate its effectiveness against two baselines: (1) MRD 3 without evolution, excluding GPT-4 evolved examples, and (2) the human-labeled GSM8K training set. Testing CoT-Max pruner on these datasets, as shown in Table 5, reveals that both GPT-4 generated and evolved CoT examples are vital for enhancing LLM reasoning performance."

## Foundational Learning

- Concept: Reinforcement learning
  - Why needed here: To train the pruner to maximize the multi-objective reward function when the LLM loss gradient cannot be backpropagated through the tokenizer.
  - Quick check question: What is the role of the REINFORCE algorithm in training the pruner?

- Concept: Text embeddings
  - Why needed here: To extract features from long inputs that exceed the LLM context window, enabling the pruner to make decisions about which CoT examples and tokens to prune.
  - Quick check question: How does the BERT-Large model extract sentence-level embeddings for the pruner?

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: To provide step-by-step reasoning paths for the LLM to solve math problems, which the pruner then optimizes by selecting and pruning examples and tokens.
  - Quick check question: What is the purpose of the CoT examples in the input prompt for the LLM?

## Architecture Onboarding

- Component map:
  BERT-Large embeddings -> Coarse-to-fine pruner (Shot Pruner -> Token Pruner) -> LLM input

- Critical path:
  1. Extract text embeddings from CoT examples using BERT-Large
  2. Prune examples using shot pruner, then prune tokens within examples using token pruner
  3. Compute reward based on LLM performance and token length constraints
  4. Update pruner parameters using REINFORCE algorithm

- Design tradeoffs:
  - Pruning more examples vs. pruning more tokens within examples to optimize context usage
  - Training with easy vs. hard questions to stabilize learning vs. achieving maximum performance
  - Using GPT-4 for dataset creation vs. human-labeled examples for cost vs. quality

- Failure signatures:
  - Pruned examples or tokens are too crucial, leading to degraded reasoning performance
  - Pruner overfits to the training dataset and does not generalize well to new problems
  - Reinforcement learning training is unstable or does not converge to effective policies

- First 3 experiments:
  1. Test the pruner on a held-out dataset to evaluate generalization across different math problems
  2. Compare the performance of the pruner with different reward function weights and formulations
  3. Analyze the impact of pruning different numbers of examples and tokens on reasoning performance and context efficiency

## Open Questions the Paper Calls Out

Open research questions:

1. **Generalization to other domains**: How well does CoT-Max generalize to other domains beyond math reasoning? The paper primarily focuses on math reasoning, but it would be interesting to explore its performance in other areas like natural language understanding or code generation.

2. **Impact of CoT dataset quality**: How does the quality of the CoT dataset (MRD3) affect the performance of CoT-Max? The paper mentions that GPT-4 is used to generate and evolve the CoT examples, but it would be valuable to investigate the impact of using different datasets or different methods for generating CoT examples.

3. **Comparison with other context window extension methods**: The paper compares CoT-Max with prompt retrieval and compression baselines, but it would be beneficial to compare it with other context window extension methods like positional interpolation or external memory modules.

4. **Transferability across different LLMs**: How well does CoT-Max transfer across different LLMs with varying capabilities and architectures? The paper evaluates it on LLaMA2 models, but it would be interesting to see its performance on other LLMs like GPT-3 or PaLM.

5. **Computational efficiency**: While CoT-Max introduces a negligible increase in inference latency and memory, it would be valuable to further optimize its computational efficiency, especially for larger models or when dealing with extremely long prompts.

6. **Interpretability of pruning decisions**: The paper provides some insights into the pruning decisions made by CoT-Max, but it would be beneficial to develop more interpretable methods for understanding why certain examples or tokens are pruned.

7. **Handling of out-of-distribution examples**: How does CoT-Max handle out-of-distribution examples that are significantly different from the training data? It would be valuable to investigate its robustness to such examples.

8. **Long-term memory and context retention**: The paper focuses on short-term context pruning, but it would be interesting to explore how CoT-Max can be extended to handle long-term memory and context retention for more complex tasks.

## Limitations

- **Dataset dependence**: Performance heavily relies on the MRD3 dataset created using GPT-4, with limited evidence of generalization to non-mathematical reasoning tasks or different domains
- **Reward function sensitivity**: The multi-objective reward function design choices (weights, formulation) could significantly impact pruning effectiveness, but ablation studies are limited
- **Structural integrity concerns**: The pruning approach may accidentally remove critical structural elements (mathematical operators, formatting) that could confuse the LLM, with rule-based repair mechanisms not thoroughly validated

## Confidence

**High Confidence (Likelihood >80%)**
- The coarse-to-fine pruning architecture is technically sound and implementable
- The use of BERT embeddings for example similarity is appropriate
- The overall framework of using reinforcement learning for token selection is valid

**Medium Confidence (Likelihood 50-80%)**
- The specific performance gains reported on GSM8K and other datasets
- The effectiveness of the MRD3 dataset for training the pruner
- The generalization capability to other mathematical reasoning tasks

**Low Confidence (Likelihood <50%)**
- Performance on non-mathematical reasoning tasks
- Robustness to different LLM architectures and tokenization schemes
- Long-term stability of the trained pruner across diverse data distributions

## Next Checks

1. **Cross-Domain Generalization Test**: Apply CoT-Max to a non-mathematical reasoning dataset (such as CommonsenseQA or logical reasoning benchmarks) to evaluate whether the pruning strategy generalizes beyond math problems. Measure both accuracy improvements and any degradation in performance compared to baseline CoT prompting.

2. **Ablation Study on Reward Function**: Systematically vary the weights in the multi-objective reward function and test the impact on pruning effectiveness and final reasoning performance. Include tests with alternative reward formulations (such as separate optimization stages or different token length constraints) to identify the most critical components of the reward design.

3. **Edge Case Analysis**: Create targeted test cases where pruning might remove critical structural elements (nested parentheses in equations, specific mathematical notation, or formatting tokens). Evaluate whether the pruner correctly preserves these elements and whether the LLM's reasoning performance degrades when these are accidentally pruned.