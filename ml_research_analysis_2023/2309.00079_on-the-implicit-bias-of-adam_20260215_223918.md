---
ver: rpa2
title: On the Implicit Bias of Adam
arxiv_id: '2309.00079'
source_url: https://arxiv.org/abs/2309.00079
tags:
- adam
- ijek
- gradient
- proof
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uses backward error analysis to study implicit regularization
  in Adam and RMSProp optimizers. The authors prove that these adaptive methods either
  penalize the perturbed one-norm of loss gradients or hinder its decrease, depending
  on hyperparameters and training stage, which differs from the two-norm penalization
  seen in standard gradient descent.
---

# On the Implicit Bias of Adam

## Quick Facts
- **arXiv ID**: 2309.00079
- **Source URL**: https://arxiv.org/abs/2309.00079
- **Reference count**: 40
- **Primary result**: Adam and RMSProp penalize the perturbed one-norm of loss gradients or hinder its decrease, unlike standard gradient descent which penalizes the two-norm

## Executive Summary
This paper investigates implicit regularization in adaptive optimization methods Adam and RMSProp using backward error analysis. The authors prove that these optimizers either penalize or hinder the decrease of the perturbed one-norm of loss gradients, depending on hyperparameters and training stage. This differs from standard gradient descent, which implicitly regularizes by penalizing the two-norm of gradients. The analysis provides theoretical results for both mini-batch and full-batch settings, showing that Adam and RMSProp trajectories approximate modified ODEs with additional bias terms. Numerical experiments on ResNet-50/101 with CIFAR-10/100 datasets confirm these findings and demonstrate how hyperparameters affect the perturbed one-norm behavior during training.

## Method Summary
The authors use backward error analysis to study implicit regularization in Adam and RMSProp optimizers. They derive continuous ODE approximations that capture the behavior of discrete update rules, identifying additional bias terms that act as regularization penalties on the perturbed one-norm of loss gradients. The analysis assumes the loss function has bounded partial derivatives up to fourth order and focuses on the "stable oscillation regime" where hyperparameters β and ρ are close but not too far apart. The theoretical results are validated through numerical experiments training ResNet architectures on CIFAR datasets, tracking training loss, perturbed one-norm, and test accuracy.

## Key Results
- Adam and RMSProp trajectories approximate ODEs with bias terms that either penalize or hinder the decrease of perturbed one-norm of loss gradients
- The implicit regularization behavior depends critically on the relationship between β and ρ hyperparameters
- The perturbed one-norm exhibits a "rising and falling" phenomenon during training, with hill height depending on how much the bias term hinders norm reduction
- Numerical experiments confirm theoretical predictions and show how hyperparameter choices affect generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adam and RMSProp implicitly regularize by penalizing the perturbed one-norm of loss gradients under certain hyperparameter conditions.
- Mechanism: Backward error analysis shows that discrete Adam/RMSProp trajectories approximate ODEs with additional bias terms that act like regularization penalties on ∥∇E(θ)∥₁,ε.
- Core assumption: The loss function has bounded partial derivatives up to fourth order (Assumption SA-2.2).
- Evidence anchors:
  - [abstract] "terms appearing in the ODEs penalize the two-norm of the loss gradients"
  - [section] "the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients"
  - [corpus] No direct evidence in corpus for perturbed one-norm regularization in Adam; corpus neighbors focus on other implicit bias aspects.
- Break condition: If ε is large relative to gradient components, the regularization switches from one-norm to two-norm penalization.

### Mechanism 2
- Claim: Adam's implicit bias depends critically on the relationship between β and ρ hyperparameters.
- Mechanism: When β > ρ and ε is small, the bias term penalizes the perturbed one-norm; when ρ > β, it hinders the decrease of this norm.
- Core assumption: Training occurs in the "stable oscillation regime" where ρ and β are close but not too far apart.
- Evidence anchors:
  - [abstract] "existence of similar implicit regularization in RMSProp and Adam depends on their hyperparameters"
  - [section] "the bias term either penalizes the perturbed one-norm of the loss gradients or, on the contrary, hinders its decrease (depending on hyperparameters and the training stage)"
  - [corpus] Weak evidence; corpus neighbors discuss hyperparameter effects but not specifically β vs ρ dynamics.
- Break condition: In the "spike regime" where ρ >> β, the ODE approximation breaks down and the implicit bias mechanism changes.

### Mechanism 3
- Claim: The perturbed one-norm exhibits a "rising and falling" phenomenon during training that affects generalization.
- Mechanism: During training, ∥∇E(θ)∥₁,ε first decreases, then rises (forms a "hill"), then decreases again, with hill height depending on how much the bias term hinders norm reduction.
- Core assumption: The training process remains stable enough for the ODE approximation to hold throughout training.
- Evidence anchors:
  - [abstract] "we notice a phenomenon of rising and falling norm in our experiments"
  - [section] "the perturbed norm rises, even though the training loss and test accuracy behave as expected: continue to decrease and increase respectively"
  - [corpus] No direct evidence in corpus; this appears to be an original experimental observation.
- Break condition: If training becomes unstable (spike regime) or reaches edge-of-stability, the ODE approximation fails and the rising/falling pattern may not hold.

## Foundational Learning

- Concept: Backward error analysis
  - Why needed here: This paper uses backward error analysis to derive ODEs that approximate Adam/RMSProp trajectories and identify implicit regularization terms.
  - Quick check question: What is the key difference between forward error analysis and backward error analysis in numerical integration?

- Concept: Implicit regularization
  - Why needed here: The paper investigates how finite step sizes in Adam/RMSProp create implicit regularization effects that differ from standard gradient descent.
  - Quick check question: How does implicit regularization in Adam differ from the two-norm penalization seen in standard gradient descent?

- Concept: Perturbed one-norm
  - Why needed here: The paper introduces ∥v∥₁,ε = ∑ᵢ √vᵢ² + ε as a key quantity whose behavior under Adam depends on hyperparameters.
  - Quick check question: What happens to the perturbed one-norm when ε becomes large compared to gradient components?

## Architecture Onboarding

- Component map: Adam/RMSProp update rules -> Backward error analysis -> Modified ODE derivation -> Implicit regularization identification -> Numerical validation
- Critical path: The core theoretical contribution is the derivation of modified ODEs (Theorem 3.1 for Adam, Theorem 4.2 for RMSProp) that capture the implicit bias effects.
- Design tradeoffs: The analysis assumes bounded fourth-order derivatives of the loss, which may not hold for ReLU networks; small ε values may cause the ODE approximation to break down in later training stages.
- Failure signatures: If the training enters the spike regime (ρ >> β), the ODE approximation becomes invalid; if ε is too small, the constants in error bounds blow up.
- First 3 experiments:
  1. Train Resnet-50 on CIFAR-10 with full-batch Adam varying ρ while keeping β fixed to observe perturbed one-norm behavior.
  2. Repeat experiment 1 but vary β while keeping ρ fixed to test the opposite hyperparameter effect.
  3. Train on CIFAR-100 with Resnet-101 to verify the rising/falling norm phenomenon across datasets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what precise conditions does the implicit regularization in Adam penalize the perturbed one-norm versus hindering its decrease?
- Basis in paper: [explicit] The paper explicitly states this depends on hyperparameters β, ρ and training stage, with two extreme cases described in Table 1
- Why unresolved: The paper provides theoretical bounds but doesn't give a complete characterization of when each regime occurs during actual training
- What evidence would resolve it: Systematic empirical studies varying β, ρ across different training stages and datasets to map out the transition between regimes

### Open Question 2
- Question: Does penalizing the perturbed one-norm of gradients actually improve generalization in deep learning tasks?
- Basis in paper: [inferred] The authors conjecture this based on analogy with two-norm penalization, noting there's currently no theory supporting this
- Why unresolved: The paper only provides preliminary numerical experiments showing correlation, not causation
- What evidence would resolve it: Controlled experiments comparing generalization with explicit one-norm regularization versus no regularization across multiple architectures and datasets

### Open Question 3
- Question: What causes the "rising and falling" phenomenon of the perturbed one-norm during Adam training?
- Basis in paper: [explicit] The authors observe this in Figure 6 and note it hasn't been previously documented, suggesting it may relate to the bias term's effect on the norm
- Why unresolved: The paper identifies the phenomenon but doesn't provide a complete theoretical explanation for its occurrence and relationship to hyperparameters
- What evidence would resolve it: Detailed analysis of how the bias term's sign and magnitude change during training, and experiments showing how modifying the bias term affects the phenomenon

## Limitations

- The analysis assumes the loss function has bounded fourth-order derivatives, which may not hold for ReLU networks
- The theoretical results are asymptotic and may not capture finite-time behavior accurately, particularly in later training stages
- The paper focuses on full-batch settings, and the extension to mini-batches requires additional assumptions about gradient variance

## Confidence

- **High Confidence**: The backward error analysis methodology and the derivation of modified ODEs for Adam and RMSProp are mathematically rigorous and well-established techniques.
- **Medium Confidence**: The claims about perturbed one-norm behavior and its relationship to hyperparameters are supported by theoretical analysis but require careful experimental validation, especially regarding the "rising and falling" phenomenon.
- **Low Confidence**: The generalization implications drawn from perturbed one-norm behavior are speculative and not directly proven by the theoretical analysis.

## Next Checks

1. **Cross-validate the perturbed one-norm calculations**: Implement independent numerical verification of the perturbed one-norm calculations using multiple gradient computation methods to ensure the theoretical predictions match empirical observations.

2. **Test stability boundaries**: Systematically explore the boundary between stable oscillation and spike regimes by varying β and ρ ratios, measuring when the ODE approximation breaks down and the implicit bias mechanisms change.

3. **Extend to mini-batch settings**: Conduct experiments with mini-batch Adam/RMSProp to verify whether the theoretical predictions about implicit regularization extend beyond the full-batch case, particularly examining how gradient variance affects the perturbed one-norm behavior.