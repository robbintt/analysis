---
ver: rpa2
title: On the Weight Dynamics of Deep Normalized Networks
arxiv_id: '2306.00700'
source_url: https://arxiv.org/abs/2306.00700
tags:
- learning
- training
- gradient
- layers
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the dynamics of effective learning rates (ELRs)
  across layers in deep normalized networks during early training. It introduces a
  theoretical model based on gradient flow and discrete-time stepping to describe
  how ELRs evolve, particularly in the presence of exploding gradients at initialization.
---

# On the Weight Dynamics of Deep Normalized Networks

## Quick Facts
- arXiv ID: 2306.00700
- Source URL: https://arxiv.org/abs/2306.00700
- Reference count: 40
- Primary result: Effective learning rates in deep normalized networks converge toward uniformity under gradient flow but diverge with large constant learning rates due to discretization effects; constraining layer-wise ELRs improves training stability and accuracy.

## Executive Summary
This paper analyzes how effective learning rates (ELRs) evolve across layers in deep normalized networks during early training. The authors develop a theoretical model based on gradient flow dynamics that predicts ELR convergence toward uniformity under continuous updates but divergence with large fixed learning rates due to discretization effects. They demonstrate that warm-up schedules, residual connections, and momentum reduce ELR spread by effectively lowering the ODE's step size. Experiments confirm these predictions and show a strong correlation between high ELR spread and poor trainability. The paper proposes a hyper-parameter-free method that constrains layer-wise ELRs by normalizing gradients, which improves training stability and accuracy—particularly in very deep networks without skip connections—and performs competitively with or better than standard training protocols.

## Method Summary
The authors study ELR dynamics in deep normalized networks by modeling weight evolution as gradient flow. They derive discrete and continuous models predicting ELR evolution based on orthogonality and scale invariance of normalization layers. The core method involves constraining layer-wise ELRs by normalizing gradients before weight updates: gradients are scaled such that each layer's ELR equals a target value Egoal. This normalization is applied per layer independently, with optional weight norm normalization to prevent unbounded growth. The approach is tested across various architectures (ResNets, Transformers) and datasets (CIFAR-10/100, CINIC-10, ImageNet, Multi30k) using SGD with and without momentum, comparing against standard training protocols with warm-up schedules.

## Key Results
- ELR spread correlates strongly with trainability, with high spread (>0.1) indicating potential training instability
- Gradient flow dynamics drive ELRs toward uniformity over time in normalized networks
- Large constant learning rates cause ELR spread divergence due to discretization-induced overshoot
- Warm-up schedules, residual connections, and momentum effectively reduce ELR spread
- Gradient normalization to constrain ELRs improves training stability and accuracy, especially in very deep networks without skip connections

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Effective learning rates (ELRs) in deep normalized networks evolve predictably due to gradient flow dynamics.
- Mechanism: Scale invariance of normalization layers causes gradients to scale inversely with input magnitudes. This leads to auto rate-tuning where weight norms grow fastest where gradients are largest, dampening those gradients in future steps. Under gradient flow, this drives ELRs toward uniformity over time.
- Core assumption: Initial gradient norms grow exponentially with depth (e.g., α^(L-ℓ) for feed-forward networks) and remain approximately constant during early training.
- Evidence anchors:
  - [abstract] "We formalize how these disparities evolve over time by modeling weight dynamics (evolution of expected gradient and weight norms) of networks with normalization layers, predicting the evolution of layer-wise ELR ratios."
  - [section 3.1] Derives discrete and continuous models predicting ELR evolution based on orthogonality and scale invariance.
  - [corpus] Weak evidence; neighboring work discusses trainability but not this specific gradient flow mechanism.
- Break condition: If gradient magnitudes change rapidly (e.g., due to changing loss landscape or adaptive optimizers), the constant-gradient assumption fails.

### Mechanism 2
- Claim: Large constant learning rates cause ELR spread divergence due to discretization effects.
- Mechanism: The ODE governing ELR evolution is stiff; large fixed step sizes (learning rates) cause weight norm overshoot, leading to vanishing gradients in early layers and persistent ELR disparities.
- Core assumption: The ODE dσ²/dt = λc²/σ² is stiff, with worst sensitivity at initialization.
- Evidence anchors:
  - [abstract] "Using large LRs is analogous to applying an explicit solver to a stiff non-linear ODE, causing overshooting and vanishing gradients in lower layers after the first step."
  - [section 3.5] Analyzes stability constraints and explains how large λ leads to divergence.
  - [corpus] Weak; neighboring work does not discuss discretization-induced ELR divergence.
- Break condition: If step size is adaptively reduced (e.g., warm-up or adaptive learning rate methods), the stiff ODE can be solved stably.

### Mechanism 3
- Claim: Warm-up, residual connections, and momentum reduce ELR spread by effectively lowering the ODE's step size or damping oscillations.
- Mechanism: Warm-up gradually increases λ, staying within stability bounds; residual connections dampen gradient explosion; momentum smooths gradient updates, all reducing the effective step size and thus the ELR disparity.
- Core assumption: These techniques modify the effective step size or gradient accumulation in ways that stabilize the ODE.
- Evidence anchors:
  - [abstract] "Our model also reveals that warm-up learning rate schedulers, residual connections, and, under stronger assumptions, momentum can help lowering the ELR spread."
  - [section 4.3] Derives momentum effects and shows empirical ELR spread reduction.
  - [corpus] Weak; neighboring work does not detail this connection.
- Break condition: If momentum is too high or warm-up too aggressive, the effective step size may still exceed stability bounds.

## Foundational Learning

- Concept: Gradient flow and discretization of ODEs
  - Why needed here: The paper models ELR evolution as an ODE and analyzes its discrete-time stepping behavior to explain training dynamics.
  - Quick check question: What is the stability condition for the Euler method applied to dy/dt = f(y)?

- Concept: Scale invariance in neural networks
  - Why needed here: Normalization layers make the network invariant to input scaling, causing gradients to scale inversely with weights—a key driver of auto rate-tuning.
  - Quick check question: Why does ∂N/∂(γW)(x, γW) = (1/γ) ∂N/∂W(x, W) hold for scale-invariant layers?

- Concept: Effective learning rate (ELR) as a scale-invariant metric
  - Why needed here: ELR = ||∇W||F / ||W||F measures relative parameter movement, accounting for scale invariance in normalized networks.
  - Quick check question: How does ELR differ from the nominal learning rate λ in a batch-normalized network?

## Architecture Onboarding

- Component map:
  Input → Conv → BN → ReLU → ... → Conv → BN → ReLU → Output
  Optional residual blocks: Input + F(input) where F includes BN
  Key: BN placed after linear (Conv) layers, not before

- Critical path:
  Forward: Conv → BN (normalize features) → ReLU (nonlinearity)
  Backward: Gradients pass through BN, scaled by 1/γ due to scale invariance
  Auto rate-tuning occurs per layer independently

- Design tradeoffs:
  BN placement: After linear layers ensures scale invariance and auto rate-tuning; before linear layers breaks it
  Skip connections: Reduce gradient explosion, lowering ELR spread but add parameters
  Momentum: Smooths gradients, reduces ELR spread but adds optimizer complexity

- Failure signatures:
  High ELR spread (> 0.1) correlates with training instability or low accuracy
  Very deep networks without skips or warm-up often diverge early
  Sudden gradient vanishing in early layers suggests overshoot due to large λ

- First 3 experiments:
  1. Train ResNet56 with and without BN, measure ELR spread over first 10 steps; expect higher spread without BN
  2. Apply warm-up scheduler to ResNet110, compare ELR spread vs constant LR; expect lower spread with warm-up
  3. Constrain gradients by ELR before update (eq. 14), train ResNet110 NoShort, measure accuracy vs baseline; expect improved stability

## Open Questions the Paper Calls Out

- Question: What is the precise relationship between effective learning rate spread and final model accuracy across different architectures and datasets?
- Basis in paper: [explicit] The authors observe "a strong correlation between high ELR spread and low trainability" and note that "very high ELR seems to correlate with low test accuracies"
- Why unresolved: The paper provides qualitative observations but lacks a quantitative analysis of the relationship between ELR spread and accuracy across various experimental conditions
- What evidence would resolve it: Systematic experiments measuring ELR spread and accuracy for multiple architectures (CNNs, Transformers), datasets, and training configurations, with statistical analysis of the correlation

## Limitations

- The theoretical model assumes constant gradient norms during early training, which may break down in later phases or with adaptive optimizers
- The claim that ELR spread above 0.1 reliably predicts poor trainability is supported empirically but lacks rigorous threshold justification
- The gradient normalization method's benefits in very deep networks without skip connections are demonstrated but may not generalize to architectures with different normalization placements or optimization objectives

## Confidence

- **High confidence**: ELR spread reduction improves trainability; scale invariance causes inverse gradient scaling; discretization effects cause ELR divergence with large constant learning rates
- **Medium confidence**: The ODE model accurately predicts ELR evolution; warm-up, residual connections, and momentum effectively reduce ELR spread through step size control
- **Low confidence**: The proposed gradient normalization method's performance gains are robust across all architectures and datasets; ELR spread > 0.1 is a universal threshold for trainability issues

## Next Checks

1. Test the gradient normalization method on architectures with LayerNorm in different positions (e.g., pre-activation ResNets) to verify the claimed improvements hold beyond post-activation BN placement
2. Evaluate ELR spread dynamics under adaptive optimizers (Adam, AdamW) to determine if the theoretical model extends beyond SGD
3. Measure trainability and ELR spread across a wider range of initializations and network depths to validate the 0.1 ELR spread threshold claim