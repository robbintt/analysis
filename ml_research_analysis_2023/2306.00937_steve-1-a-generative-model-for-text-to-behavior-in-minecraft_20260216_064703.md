---
ver: rpa2
title: 'STEVE-1: A Generative Model for Text-to-Behavior in Minecraft'
arxiv_id: '2306.00937'
source_url: https://arxiv.org/abs/2306.00937
tags:
- text
- visual
- agent
- goal
- dirt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STEVE-1 is an instruction-following Minecraft agent trained for
  $60 using a novel unCLIP-based approach that avoids expensive human annotations.
  The method fine-tunes a pretrained VPT model to achieve MineCLIP-embedded visual
  goals via self-supervised behavioral cloning with packed hindsight relabeling, then
  trains a CVAE prior to map text to visual embeddings.
---

# STEVE-1: A Generative Model for Text-to-Behavior in Minecraft

## Quick Facts
- arXiv ID: 2306.00937
- Source URL: https://arxiv.org/abs/2306.00937
- Reference count: 40
- One-line primary result: STEVE-1 achieves 75× more dirt, 4.9× more wood, and travels 4.3× further in Minecraft using a $60 self-supervised training approach

## Executive Summary
STEVE-1 is an instruction-following Minecraft agent trained using a novel unCLIP-based approach that avoids expensive human annotations. The method fine-tunes a pretrained VPT model to achieve MineCLIP-embedded visual goals via self-supervised behavioral cloning with packed hindsight relabeling, then trains a CVAE prior to map text to visual embeddings. This enables STEVE-1 to follow short-horizon text and visual instructions with low-level controls. It outperforms prior baselines, collecting 75× more dirt, 4.9× more wood, and traveling 4.3× further when prompted. Classifier-free guidance and prompt chaining further improve performance on longer tasks. All model weights and code are publicly released.

## Method Summary
STEVE-1 uses a two-step unCLIP-style approach: first fine-tuning the pretrained VPT model with packed hindsight relabeling to follow commands in MineCLIP's latent space, then training a CVAE prior to map text to visual embeddings. The fine-tuning uses self-supervised data generated by VPT agents, with goals determined by MineCLIP embeddings of achieved states. Classifier-free guidance is applied during inference to improve performance. The entire training process costs approximately $60 in compute, leveraging pretrained models to avoid expensive human annotations.

## Key Results
- Outperforms VPT baseline by 75× on dirt collection, 4.9× on wood collection, and 4.3× on distance traveled
- Achieves 12/13 short-horizon tasks robustly with classifier-free guidance (λ=1.5)
- Successfully completes longer-horizon tasks using prompt chaining strategies
- Training cost approximately $60 using self-supervised data instead of human annotations

## Why This Works (Mechanism)

### Mechanism 1
The VPT foundation model's extensive pretraining enables STEVE-1 to perform tasks that would be impossible without such prior knowledge. VPT was trained on 70k hours of Minecraft gameplay, providing a rich behavioral prior. Fine-tuning this pretrained model allows STEVE-1 to leverage this knowledge while learning to follow instructions. The core assumption is that behaviors learned during pretraining are transferable to instruction-following tasks.

### Mechanism 2
The unCLIP approach decomposes the instruction-following problem into two models: a policy that achieves visual goals and a prior that maps text to visual embeddings. The policy is fine-tuned to follow commands in MineCLIP's latent space, while the prior model is trained to predict latent codes from text. This allows STEVE-1 to follow text and visual instructions. The core assumption is that MineCLIP's latent space provides a meaningful representation of Minecraft goals that can be achieved by the policy.

### Mechanism 3
Classifier-free guidance allows STEVE-1 to trade-off between mode-coverage and sample fidelity, improving performance. During inference, STEVE-1 computes logits for the policy conditioned on the goal and for the unconditional policy. A combination of these logits using a λ parameter encourages the policy to follow actions more likely when conditioned on the goal. The core assumption is that the conditional logits provide a meaningful signal for guiding the policy towards the desired goal.

## Foundational Learning

### Concept: Video Pretraining (VPT) foundation model
Why needed here: VPT provides the behavioral prior that STEVE-1 builds upon. Understanding VPT is crucial for grasping how STEVE-1 works.
Quick check question: What is the main purpose of pretraining VPT on 70k hours of Minecraft gameplay?

### Concept: MineCLIP latent space
Why needed here: MineCLIP's latent space is used to represent Minecraft goals, which the policy is fine-tuned to achieve. Understanding this space is essential for understanding how STEVE-1 follows instructions.
Quick check question: How does MineCLIP's latent space relate to Minecraft goals, and why is it useful for instruction-following?

### Concept: Classifier-free guidance
Why needed here: Classifier-free guidance is a technique used to improve the performance of STEVE-1 by trading off between mode-coverage and sample fidelity. Understanding this technique is crucial for understanding how STEVE-1 achieves its results.
Quick check question: How does classifier-free guidance work, and why is it beneficial for STEVE-1's performance?

## Architecture Onboarding

### Component map
VPT foundation model -> MineCLIP model -> Policy (fine-tuned VPT) -> Prior (CVAE) -> Classifier-free guidance

### Critical path
1. Fine-tune VPT to follow commands in MineCLIP's latent space using self-supervised behavioral cloning and hindsight relabeling
2. Train the prior model to map text to visual embeddings
3. Combine the policy and prior models to create STEVE-1
4. Use classifier-free guidance during inference to improve performance

### Design tradeoffs
- Using a pretrained VPT model vs. training from scratch: Pretraining provides a rich behavioral prior but may introduce biases
- Fine-tuning on self-supervised data vs. human-labeled data: Self-supervised data is cheaper to obtain but may be noisier
- Using classifier-free guidance vs. not: Guidance can improve performance but requires tuning the λ parameter

### Failure signatures
- If STEVE-1 fails to follow instructions, it may be due to issues with the policy, prior, or classifier-free guidance
- If the performance is poor, it may be due to insufficient pretraining, poor fine-tuning, or inadequate classifier-free guidance

### First 3 experiments
1. Evaluate STEVE-1's performance on a set of short-horizon tasks with text and visual goals
2. Experiment with prompt chaining for longer-horizon tasks
3. Analyze the scaling properties of STEVE-1 with more data and compute

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several important ones emerge from the methodology and results. The most significant open questions relate to generalization beyond the 13 evaluated tasks, scaling properties with increased compute and data, and comparison to other state-of-the-art instruction-following methods in Minecraft and similar environments.

## Limitations
- Self-supervised training data may introduce distribution shifts between fine-tuning and evaluation tasks
- Claims about cost-effectiveness don't account for infrastructure and engineering time beyond compute
- Reliance on MineCLIP embeddings assumes they capture meaningful task-relevant information without full validation

## Confidence
High Confidence: The core two-step training methodology is technically sound and well-documented, with robust empirical results showing STEVE-1 outperforming baselines on short-horizon tasks.

Medium Confidence: Claims about classifier-free guidance and prompt chaining effectiveness are supported by experiments but would benefit from more extensive ablation studies. Scalability claims are theoretically justified but not empirically validated.

Low Confidence: The assertion that this approach generalizes to a wide range of open-ended instructions beyond tested tasks is speculative, lacking sufficient evidence for truly novel or complex multi-step instructions without extensive prompt engineering.

## Next Checks
1. Ablation of prompt chaining components: Systematically test the contribution of each prompt chaining strategy and their optimal combinations for longer-horizon tasks.

2. Distribution shift analysis: Quantify how STEVE-1's performance degrades on tasks out-of-distribution from VPT pretraining data and fine-tuning dataset.

3. Latent space interpretability: Analyze whether MineCLIP embeddings used as goals actually correspond to semantically meaningful Minecraft states, or if STEVE-1 is simply learning to navigate a continuous embedding space without true task understanding.