---
ver: rpa2
title: 'One for All: Towards Training One Graph Model for All Classification Tasks'
arxiv_id: '2310.00149'
source_url: https://arxiv.org/abs/2310.00149
tags:
- graph
- node
- tasks
- learning
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents One for All (OFA), the first general framework
  to train a single graph model for all classification tasks. OFA unifies different
  graph data by describing nodes and edges with natural language and using language
  models to encode the diverse text attributes into the same embedding space.
---

# One for All: Towards Training One Graph Model for All Classification Tasks

## Quick Facts
- arXiv ID: 2310.00149
- Source URL: https://arxiv.org/abs/2310.00149
- Reference count: 39
- Key outcome: OFA is the first general framework to train a single graph model for all classification tasks across different graph domains.

## Executive Summary
The paper presents One for All (OFA), the first general framework to train a single graph model for all classification tasks. OFA unifies different graph data by describing nodes and edges with natural language and using language models to encode the diverse text attributes into the same embedding space. It also introduces the concept of nodes-of-interest to standardize different tasks and a novel graph prompting paradigm for in-context learning. OFA is trained using graph data from multiple domains and evaluated on supervised, few-shot, and zero-shot learning scenarios.

## Method Summary
OFA proposes text-attributed graphs (TAGs) to unify heterogeneous graph data from different domains. Nodes and edges are described using natural language, which are then encoded into fixed-length vectors by a language model. The framework introduces Nodes-of-Interest (NOI) subgraphs and NOI prompt nodes to standardize various task types (node, link, and graph-level). A graph prompting paradigm (GPP) appends task-specific prompt graphs to the input graph, enabling in-context learning without fine-tuning. The model is trained using a combination of datasets from multiple domains.

## Key Results
- OFA achieves great results on cross-domain and cross-task scenarios, demonstrating strong potential as a future foundation model on the graph.
- The model performs well across different tasks, including node classification, link prediction, and graph classification, under supervised, few-shot, and zero-shot learning scenarios.
- OFA outperforms or matches the performance of task-specific models, showcasing its ability to generalize across diverse graph domains.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-attributed graphs (TAGs) enable cross-domain learning by unifying graph data into a common embedding space.
- Mechanism: Nodes and edges from diverse graph domains are described using natural language, then encoded into the same embedding space by a language model, allowing a single model to process heterogeneous data.
- Core assumption: Language models can effectively capture and embed the semantic meaning of graph features described in natural language, regardless of domain.
- Evidence anchors:
  - [abstract]: "OFA proposes text-attributed graphs to unify different graph data by describing nodes and edges with natural language and uses language models to encode the diverse and possibly cross-domain text attributes to feature vectors in the same embedding space."
  - [section 3.1]: "We can apply an LLM to encode all text features into a fixed-length vector as the final input feature of all nodes/edges."
  - [corpus]: Weak - no direct corpus evidence, relies on LLM general capability.
- Break condition: If the language model fails to encode semantic meaning of graph features accurately, or if graph domains are too disparate for effective embedding.

### Mechanism 2
- Claim: The Nodes-of-Interest (NOI) subgraph and NOI prompt node unify different task types.
- Mechanism: For each task, a subgraph centered around the task-relevant nodes is constructed, and a prompt node containing the task description is connected to these nodes, allowing a single model to handle node, link, and graph-level tasks.
- Core assumption: A unified subgraph representation can effectively capture the relevant information for diverse task types.
- Evidence anchors:
  - [abstract]: "OFA introduces the concept of nodes-of-interest to standardize different tasks with a single task representation."
  - [section 3.2]: "The NOI prompt node is associated with a task prompt text... The NOI prompt node connects to all nodes in NOI... the NOI prompt node summarizes information in the NOI and the task description."
  - [corpus]: Weak - no direct corpus evidence, relies on subgraph GNN general capability.
- Break condition: If the NOI subgraph fails to capture relevant task information, or if the prompt node cannot effectively summarize and direct the model's attention.

### Mechanism 3
- Claim: Graph prompting paradigm (GPP) enables in-context learning for graph tasks.
- Mechanism: Task-specific information is embedded into the graph structure via prompt nodes and edges, allowing the model to adapt to new tasks without fine-tuning.
- Core assumption: Adding task information to the graph structure allows the model to condition its predictions on the task context.
- Evidence anchors:
  - [abstract]: "OFA introduces a novel graph prompting paradigm that appends prompting substructures to the input graph, which enables it to address varied tasks without fine-tuning."
  - [section 3.3]: "GPP adds edges between the NOI prompt node and every node in NOI... the second node type in the prompt graph is called the class node... The prompted graph fed to the subsequent graph model is the combination of the input graph and prompt graph."
  - [corpus]: Weak - no direct corpus evidence, relies on prompting general capability.
- Break condition: If the prompt graph fails to provide sufficient task context, or if the model cannot effectively utilize the embedded task information.

## Foundational Learning

- Concept: Text-attributed graphs (TAGs)
  - Why needed here: TAGs allow unification of heterogeneous graph data from different domains into a common embedding space, enabling cross-domain learning.
  - Quick check question: Can you explain how describing nodes and edges with natural language enables a language model to encode diverse graph features into the same space?

- Concept: Nodes-of-Interest (NOI) subgraph
  - Why needed here: NOI subgraphs provide a unified representation for different task types (node, link, graph-level) by focusing on the relevant nodes and their neighborhood.
  - Quick check question: How does the NOI subgraph differ for node-level, link-level, and graph-level tasks, and why is this important?

- Concept: Graph prompting paradigm (GPP)
  - Why needed here: GPP enables in-context learning by embedding task-specific information into the graph structure, allowing the model to adapt to new tasks without fine-tuning.
  - Quick check question: What are the two types of nodes in the prompt graph, and how do they contribute to task-specific predictions?

## Architecture Onboarding

- Component map:
  Input (text-attributed graph) -> Language Model Encoder -> Graph Model (GNN) -> Prompt Graph -> Output (task predictions)

- Critical path:
  1. Convert graph nodes/edges to text descriptions
  2. Encode text descriptions using language model
  3. Construct NOI subgraph and prompt graph
  4. Process the combined graph using GNN
  5. Extract class node embeddings and make predictions

- Design tradeoffs:
  - Using language models for encoding introduces dependency on their quality and capability
  - The NOI subgraph approach may not capture all relevant information for complex tasks
  - Prompt graph construction requires careful design to ensure task-specific information is effectively conveyed

- Failure signatures:
  - Poor performance on cross-domain tasks: Indicates issues with TAG encoding or model generalization
  - Inability to handle new task types: Suggests problems with NOI subgraph construction or prompt graph design
  - In-context learning failures: Points to issues with GPP or the model's ability to utilize task context

- First 3 experiments:
  1. Verify TAG encoding: Test the language model's ability to encode diverse graph features into the same space by visualizing embeddings.
  2. Validate NOI subgraph: Construct NOI subgraphs for different task types and verify they capture relevant information.
  3. Test GPP effectiveness: Add task-specific prompts to a simple graph and check if the model's predictions align with the expected task context.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the OFA framework be extended to handle regression tasks, which are currently beyond its scope?
- Basis in paper: [explicit] The paper acknowledges that OFA is limited to classification tasks and cannot perform regression tasks due to the unbounded nature of regression targets.
- Why unresolved: The paper suggests a potential approach of specifying a target range in the NOI prompt node task description, but notes that reasoning about math concepts is difficult even for advanced LLMs, making this approach unreliable with the current sentence encoder.
- What evidence would resolve it: Developing a method to effectively encode and process regression tasks within the OFA framework, potentially by improving the mathematical reasoning capabilities of the language model or by designing a new prompting strategy for regression tasks.

### Open Question 2
- Question: How can the training data for the OFA framework be expanded to improve its generality and performance across diverse graph domains?
- Basis in paper: [explicit] The paper mentions that the current training data for OFA is limited compared to that of LLMs, which could restrict its ability to generalize to unseen tasks and domains.
- Why unresolved: While the paper suggests collecting more TAG data from wide domains and designing additional tasks for OFA training, it does not provide a concrete strategy for achieving this.
- What evidence would resolve it: Implementing a systematic approach to collect and integrate diverse graph datasets into the OFA framework, along with evaluating the model's performance on these expanded datasets to demonstrate improved generalization.

### Open Question 3
- Question: What training techniques beyond supervised learning can be incorporated into the OFA framework to enhance its modeling capabilities and generalization to unseen tasks?
- Basis in paper: [explicit] The paper notes that LLMs explore training techniques beyond supervised learning, such as auto-regressive training and contrastive learning, which significantly improve their ability to model data and generalize to unseen tasks.
- Why unresolved: The paper suggests that exploring such techniques without the need for labels is an important research direction to improve OFA, but does not provide specific methods or experiments to achieve this.
- What evidence would resolve it: Designing and implementing novel training strategies, such as self-supervised learning or contrastive learning, within the OFA framework, and evaluating their impact on the model's performance and ability to generalize to new tasks and domains.

## Limitations
- The fixed-length vectors from LLM encoders may lose important graph-specific information compared to raw features.
- The approach requires careful construction of natural language descriptions for all graph elements, which may not be scalable or accurate for complex graphs.
- Performance on zero-shot tasks is promising but not thoroughly validated across diverse scenarios.

## Confidence
- Cross-domain learning via TAGs: Medium confidence. Relies heavily on language model generalization capabilities.
- Task unification via NOI subgraphs: Medium confidence. Effectiveness depends on accurate subgraph construction for diverse tasks.
- GPP for in-context learning: Medium confidence. Success may vary based on task description quality and representation.

## Next Checks
1. Cross-domain robustness test: Evaluate OFA on graphs from completely unseen domains (e.g., social networks, biological interaction networks) to assess true generalization capabilities.
2. Prompt sensitivity analysis: Systematically vary the task descriptions in NOI prompt nodes to measure how sensitive performance is to the quality and specificity of natural language prompts.
3. Ablation on encoding methods: Compare OFA's performance using different encoding strategies (LLM vs. domain-specific encoders) to quantify the impact of the language model choice on cross-domain learning.