---
ver: rpa2
title: 'Okapi: Instruction-tuned Large Language Models in Multiple Languages with
  Reinforcement Learning from Human Feedback'
arxiv_id: '2307.16039'
source_url: https://arxiv.org/abs/2307.16039
tags:
- language
- languages
- llms
- instruction
- instructions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Okapi is the first framework for instruction-tuned LLMs in multiple
  languages using reinforcement learning from human feedback (RLHF). Unlike existing
  open-source multilingual LLMs which rely solely on supervised fine-tuning, Okapi
  introduces RLHF to improve alignment with human expectations.
---

# Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback

## Quick Facts
- arXiv ID: 2307.16039
- Source URL: https://arxiv.org/abs/2307.16039
- Reference count: 23
- Okapi introduces RLHF for multilingual instruction tuning, achieving up to 2.5% average improvement over SFT

## Executive Summary
Okapi is the first framework for instruction-tuned large language models (LLMs) in multiple languages using reinforcement learning from human feedback (RLHF). Unlike existing multilingual LLMs that rely solely on supervised fine-tuning, Okapi introduces RLHF to improve alignment with human expectations across 26 diverse languages. The authors generate a large English instruction dataset and translate it into multiple languages, including several understudied low-resource languages. Experiments with BLOOM and LLaMA base models demonstrate that RLHF consistently outperforms supervised fine-tuning across languages and tasks, with particularly strong results on ARC and HellaSwag datasets. The authors release all data, code, and models to support future research.

## Method Summary
The Okapi framework consists of three main phases: instruction generation and translation, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF). First, 158K English instructions are generated using the Self-Instruct method and translated into 26 target languages using ChatGPT. Base models (BLOOM or LLaMA 7B) are then fine-tuned using SFT on the translated instructions for 3 epochs. Next, response ranking data is created by having ChatGPT rank model outputs for 42K instructions. A reward model is trained using this ranked data with binary cross-entropy loss for 2 epochs. Finally, RLHF fine-tuning is performed using Proximal Policy Optimization (PPO) for 5 epochs with a KL divergence penalty to prevent excessive deviation from the base policy.

## Key Results
- RLHF consistently outperforms SFT across all 26 languages tested, with average improvements up to 2.5%
- Improvements are especially pronounced on ARC and HellaSwag datasets, which better align with the generated instruction data
- RLHF demonstrates significant advantages for low-resource languages, addressing a critical gap in multilingual instruction tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translating instructions into multiple languages improves multilingual instruction-following capabilities compared to English-only tuning.
- Mechanism: English instruction generation followed by translation preserves consistent instruction content while enabling broader language coverage through existing translation systems.
- Core assumption: Translation systems (e.g., ChatGPT) can accurately convey instruction semantics across languages without significant information loss.
- Evidence anchors:
  - [abstract] "generate a large English instruction dataset and translate it into 26 diverse languages"
  - [section 2.2] "We utilize ChatGPT to translate the 158K English instructions into 26 target languages"
- Break condition: Translation introduces significant semantic drift or fails to preserve task-specific details (e.g., code examples).

### Mechanism 2
- Claim: RLHF outperforms SFT for multilingual instruction tuning by learning from ranked responses rather than just positive demonstrations.
- Mechanism: RLHF uses contrastive learning from response rankings to train reward models, which then guide fine-tuning beyond the scope of supervised examples.
- Core assumption: Ranked response data effectively captures human preferences across multiple languages.
- Evidence anchors:
  - [abstract] "Experiments with BLOOM and LLaMA base models show RLHF consistently outperforms supervised fine-tuning across languages and tasks"
- Break condition: Reward model fails to generalize across languages or ranking quality is insufficient to guide RLHF.

### Mechanism 3
- Claim: Using ChatGPT for both instruction translation and response ranking enables scalable multilingual RLHF development.
- Mechanism: ChatGPT's instruction-following capabilities allow automated translation and ranking without requiring extensive human annotation across 26 languages.
- Core assumption: ChatGPT can provide reliable translations and rankings that approximate human preferences across diverse language pairs.
- Evidence anchors:
  - [section 2.2] "We utilize ChatGPT to translate the 158K English instructions into 26 target languages"
- Break condition: ChatGPT's performance degrades significantly for low-resource languages or complex instruction types.

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: SFT provides the initial instruction-following capability that serves as the base for RLHF fine-tuning
  - Quick check question: What distinguishes SFT from standard pre-training in the context of instruction tuning?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF enables learning from human preferences beyond positive examples, improving alignment with human expectations
  - Quick check question: How does RLHF differ from standard reinforcement learning in the context of language model fine-tuning?

- Concept: Reward Model Training
  - Why needed here: Reward models score response quality to guide RLHF optimization, requiring contrastive learning from ranked data
  - Quick check question: What type of loss function is typically used to train reward models for RLHF?

## Architecture Onboarding

- Component map: English instruction generation → Translation to 26 languages → SFT fine-tuning → Response generation and ranking → Reward model training → RLHF fine-tuning → Evaluation
- Critical path: Instruction generation → Translation → SFT → Response ranking → Reward model → RLHF → Evaluation
- Design tradeoffs: Translation-based approach vs. native instruction generation for each language (scalability vs. potential quality loss)
- Failure signatures: Low performance on low-resource languages, inconsistent improvements across language groups, poor generalization beyond training languages
- First 3 experiments:
  1. Generate 158K English instructions using Self-Instruct method and verify instruction diversity
  2. Translate instructions to 26 languages using ChatGPT and evaluate translation quality for a subset
  3. Fine-tune BLOOM 7B using SFT on translated instructions and measure baseline performance on ARC/HellaSwag datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RLHF performance vary across different language families and writing systems (e.g., Indo-European vs Afro-Asiatic languages, or alphabetic vs logographic scripts)?
- Basis in paper: [explicit] The paper notes RLHF improves performance across language groups but doesn't analyze performance variations by language family or script type.
- Why unresolved: The paper only categorizes languages by resource availability, not by linguistic properties that might affect RLHF effectiveness.
- What evidence would resolve it: Comparative RLHF performance analysis grouped by language families and writing systems.

### Open Question 2
- Question: What is the optimal balance between English-generated instructions and target language-specific instructions for multilingual instruction tuning?
- Basis in paper: [inferred] The paper uses English instruction generation and translation, but notes potential limitations of this approach.
- Why unresolved: The paper doesn't explore or compare the effectiveness of instruction generation directly in target languages versus translation from English.
- What evidence would resolve it: Head-to-head comparison of models trained on translated vs natively-generated instructions in each language.

### Open Question 3
- Question: How does RLHF performance scale with instruction dataset size in multilingual settings compared to monolingual settings?
- Basis in paper: [explicit] The paper uses 158K instructions but notes that BLOOMZ uses 78M multilingual instructions and still underperforms.
- Why unresolved: The paper doesn't investigate whether RLHF benefits from larger instruction datasets differently than SFT in multilingual contexts.
- What evidence would resolve it: Performance comparison of RLHF vs SFT across multiple instruction dataset sizes for each language.

## Limitations

- The approach relies heavily on ChatGPT for translation and ranking, which may introduce quality issues for low-resource languages
- Evaluation is limited to three benchmark datasets (ARC, HellaSwag, MMLU), leaving uncertainty about performance on other task types
- The translation-based approach may not fully capture cultural and linguistic nuances that affect instruction-following across languages

## Confidence

- **High Confidence**: The overall finding that RLHF outperforms SFT for multilingual instruction tuning across multiple languages and models (BLOOM and LLaMA)
- **Medium Confidence**: The specific magnitude of improvement (up to 2.5% average gain) and the claim that improvements are "especially pronounced" on ARC and HellaSwag compared to MMLU
- **Low Confidence**: The scalability of this approach to additional languages beyond the 26 tested, and the generalizability to languages with significantly different typological features

## Next Checks

1. Conduct human evaluation of a random sample of translated instructions across all 26 languages, focusing on semantic preservation and task clarity, to quantify translation quality and identify languages where the approach may be less effective.

2. Test the final RLHF-tuned models on additional multilingual instruction-following benchmarks not used in training (such as FLORES or XTREME-R) to assess generalization beyond the three evaluation datasets.

3. Perform ablation studies removing ChatGPT-based translation/ranking and replacing with either human annotation for a subset of languages or alternative translation systems to isolate the contribution of ChatGPT to the observed improvements.