---
ver: rpa2
title: 'FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions'
arxiv_id: '2310.15421'
source_url: https://arxiv.org/abs/2310.15421
tags:
- belief
- information
- conversation
- questions
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FANTOM, a benchmark designed to stress-test
  machine theory of mind (ToM) capabilities in conversational interactions. Unlike
  existing benchmarks that rely on passive narratives, FANTOM uses information-asymmetric
  multiparty conversations where characters join and leave discussions, creating distinct
  mental states.
---

# FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions

## Quick Facts
- **arXiv ID:** 2310.15421
- **Source URL:** https://arxiv.org/abs/2310.15421
- **Reference count:** 23
- **Primary result:** LLMs perform significantly worse than humans on ToM reasoning tasks, even with chain-of-thought reasoning or fine-tuning

## Executive Summary
This paper introduces FANTOM, a benchmark designed to stress-test machine theory of mind (ToM) capabilities in conversational interactions. Unlike existing benchmarks that rely on passive narratives, FANTOM uses information-asymmetric multiparty conversations where characters join and leave discussions, creating distinct mental states. The benchmark includes 10,000 questions across 256 conversations, testing models' ability to track beliefs and knowledge among multiple characters. The authors evaluate state-of-the-art large language models on FANTOM and find that they perform significantly worse than humans, even with chain-of-thought reasoning or fine-tuning. Models show illusory ToM by correctly answering some question types but failing on others requiring the same reasoning. The benchmark highlights the challenges of developing models with coherent ToM reasoning in interactive settings.

## Method Summary
FANTOM uses LLM-generated multiparty conversations with information asymmetry, where characters join and leave at different times, creating distinct knowledge states. The benchmark includes 256 conversations with 10,000 questions across four types: belief questions (who believes what), answerability questions (can a character answer), info access questions (what information is accessible), and fact questions (basic information retrieval). Each question type has multiple formats (free-response, multiple-choice, yes/no) to identify illusory ToM capabilities. The evaluation tests zero-shot performance, chain-of-thought reasoning, and fine-tuning on subsets of the data, comparing model performance against human baselines.

## Key Results
- LLMs perform significantly worse than humans on FANTOM, even with chain-of-thought reasoning or fine-tuning
- Models exhibit illusory ToM by correctly answering some question types while failing on others requiring identical reasoning
- Chain-of-thought reasoning selectively improves performance only in specific scenarios, primarily reducing false positive errors
- Information retrieval from full conversation contexts remains challenging for models, with performance varying across question formats

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Information asymmetry in conversations creates distinct mental states necessary for non-merging ToM evaluation
- Mechanism: By having characters join and leave conversations at different times, the model can observe information flows that are not equally accessible to all participants, creating natural "knowledge gaps"
- Core assumption: Models cannot simply rely on word correlations to solve these tasks because the correct answers depend on tracking which character has access to which information
- Evidence anchors:
  - [abstract]: "Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs)"
  - [section 2.2]: "We design multiparty conversations where specific information is inaccessible to certain characters"
  - [corpus]: FMR scores show related works (0.52-0.63) indicating this is a recognized challenge area
- Break condition: If models learn to identify conversation structure patterns (like "coffee break" as signal for information gap) rather than actual reasoning about knowledge states

### Mechanism 2
- Claim: Multiple question formats for same underlying reasoning reveal illusory ToM capabilities
- Mechanism: By converting the same factual question into different formats (free-response, multiple-choice, yes/no), the benchmark can distinguish between models that have genuine ToM reasoning versus those using superficial pattern matching
- Core assumption: Different question formats require the same underlying ToM reasoning but will be solved differently by models with illusory capabilities
- Evidence anchors:
  - [abstract]: "We formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs"
  - [section 3.3]: "Although their formats are different, all questions in FANT OM fundamentally aim to ascertain the same underlying reasoning"
  - [corpus]: Related work "Clever Hans or Neural Theory of Mind?" suggests this is a recognized issue
- Break condition: If models achieve similar performance across all formats, suggesting they've developed a unified approach to ToM reasoning

### Mechanism 3
- Claim: Chain-of-thought reasoning selectively improves performance only in specific scenarios
- Mechanism: CoT helps models in some ToM reasoning tasks but not others, revealing the limitations of current reasoning approaches
- Core assumption: CoT provides step-by-step reasoning that helps with certain types of inference but doesn't fundamentally solve the ToM reasoning problem
- Evidence anchors:
  - [abstract]: "We show that FANT OM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning"
  - [section 4.2]: "CoT primarily helps the model in reducing the false positive error rates, but the reduction in false negative error rates is not consistent across models"
  - [corpus]: Related work on CoT prompting suggests this is an active research area
- Break condition: If CoT consistently improves performance across all question types and models

## Foundational Learning

- Concept: Information asymmetry
  - Why needed here: The entire benchmark design relies on creating scenarios where different characters have access to different information
  - Quick check question: Can you explain how Kailey's absence creates information asymmetry in the sample conversation?

- Concept: Theory of Mind (ToM) reasoning
  - Why needed here: The benchmark is specifically designed to test whether models can reason about others' mental states
  - Quick check question: What's the difference between first-order and second-order ToM beliefs?

- Concept: Reporting bias in narratives
  - Why needed here: The paper argues that traditional narrative-based ToM tasks suffer from reporting bias, which conversations avoid
  - Quick check question: Why might a narrative that explicitly states "Carlos did not see this" be problematic for testing genuine ToM?

## Architecture Onboarding

- Component map: Conversation generator → Fact QA generator → ToM QA constructor → Evaluation engine → Analysis pipeline
- Critical path: Generate conversations with information asymmetry → Extract inaccessible information → Generate fact QAs → Construct ToM QAs → Evaluate model responses → Analyze error patterns
- Design tradeoffs: Conversation generation using LLMs vs. human-authored (speed/scale vs. quality control); multiple question formats (comprehensive testing vs. complexity)
- Failure signatures: High performance on BELIEF Q[CHOICE] but low on ANSWERABILITY Q[LIST] suggests illusory ToM; CoT helping with false positives but not false negatives suggests selective reasoning capabilities
- First 3 experiments:
  1. Test model on BELIEF Q[CHOICE] vs BELIEF Q[DIST] to identify illusory ToM patterns
  2. Compare performance on ANSWERABILITY Q[Y/N] vs INFOACCESS Q[Y/N] to measure reasoning complexity effects
  3. Evaluate short vs full conversation contexts to test information retrieval capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do models perform on FANTOM when evaluated on conversations with different relationship dynamics between participants (e.g., family, friends, co-workers) compared to the current small talk scenarios?
- Basis in paper: Explicit - The authors note that FANTOM is currently limited to small talk conversations on specific topics and suggest that social reasoning could become more dynamic when relationship variables are introduced.
- Why unresolved: The current benchmark does not include conversations with varying relationship dynamics, limiting understanding of how these factors impact ToM reasoning in models.
- What evidence would resolve it: Evaluating models on FANTOM with conversations containing different relationship dynamics and comparing performance metrics to the current small talk scenarios would provide insights into how relationship factors affect ToM reasoning.

### Open Question 2
- Question: How would multi-modal models perform on FANTOM compared to language-only models, given that ToM extends beyond a single modality?
- Basis in paper: Explicit - The authors state that evaluation is limited to language-based models and note that ToM extends beyond a single modality, citing that the well-known Sally-Anne test is typically conducted as a face-to-face experiment where visual cues affect performance.
- Why unresolved: FANTOM only evaluates language-based models, missing the potential impact of visual or other modalities on ToM reasoning capabilities.
- What evidence would resolve it: Evaluating multi-modal models (e.g., models that process both text and images) on FANTOM and comparing their performance to language-only models would reveal the importance of different modalities for ToM reasoning.

### Open Question 3
- Question: What is the impact of training models on interactive learning paradigms compared to pretraining alone for improving ToM reasoning capabilities?
- Basis in paper: Explicit - The authors suggest that combining pretraining with interactive learning could help address the issue of models' tendency to overly rely on information they are conditioned on, rather than distinguishing between accessible and inaccessible information for different agents.
- Why unresolved: The current evaluation does not test models trained with interactive learning paradigms, leaving the potential benefits of this approach unexplored.
- What evidence would resolve it: Training models using interactive learning paradigms and evaluating them on FANTOM would provide evidence for whether this approach improves their ability to distinguish between accessible and inaccessible information for different agents.

## Limitations
- Benchmark relies on LLM-generated conversations, which may introduce bias in data distribution and conversation quality
- Limited to English-language conversations, restricting generalizability to other languages and cultures where ToM reasoning might manifest differently
- Does not evaluate multi-modal models that could potentially leverage visual or other cues for ToM reasoning

## Confidence
- **High Confidence**: The core finding that current LLMs perform significantly worse than humans on ToM reasoning tasks, even with chain-of-thought prompting or fine-tuning
- **Medium Confidence**: The claim that FANTOM effectively identifies illusory ToM capabilities through multiple question formats
- **Medium Confidence**: The assertion that information asymmetry in conversations creates more robust ToM evaluation than narrative-based benchmarks

## Next Checks
1. **Cross-linguistic validation**: Test FANTOM on non-English conversations generated by multilingual models to assess whether information asymmetry and ToM reasoning patterns generalize across languages

2. **Human-human consistency**: Conduct human studies where participants answer FANTOM questions about conversations they did not participate in, measuring inter-rater reliability and identifying any systematic biases in human ToM reasoning that the benchmark might be capturing

3. **Adversarial evaluation**: Create a subset of FANTOM conversations with subtle information cues that could be exploited by models using surface-level pattern matching rather than genuine ToM reasoning, testing whether the benchmark effectively distinguishes between true and illusory ToM capabilities