---
ver: rpa2
title: Deep Reinforcement Learning with Plasticity Injection
arxiv_id: '2305.15555'
source_url: https://arxiv.org/abs/2305.15555
tags:
- plasticity
- injection
- learning
- agent
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies plasticity loss in deep RL, where neural networks
  gradually lose the ability to learn from new data. It introduces plasticity injection,
  which increases network plasticity without changing trainable parameters or affecting
  predictions.
---

# Deep Reinforcement Learning with Plasticity Injection

## Quick Facts
- arXiv ID: 2305.15555
- Source URL: https://arxiv.org/abs/2305.15555
- Reference count: 40
- Primary result: Plasticity injection improves RL performance on 57 Atari games by up to 20% compared to alternatives

## Executive Summary
This paper investigates plasticity loss in deep reinforcement learning, where neural networks gradually lose the ability to learn from new data. The authors introduce plasticity injection, a method that increases network plasticity without changing trainable parameters or affecting predictions. The technique works by freezing part of the network and adding a residual correction component, effectively providing a "clean slate" for learning new patterns while preserving existing knowledge. Experiments show significant performance improvements on Atari benchmarks and suggest plasticity loss is influenced by learning rate, replay ratio, and network size.

## Method Summary
Plasticity injection addresses the phenomenon where neural networks in reinforcement learning gradually lose their ability to learn from new data. The method freezes the original network's parameters and introduces a new residual branch with randomly initialized weights. The output is computed as frozen_output + new_trained_output - new_frozen_output, ensuring predictions remain unchanged initially. As training progresses, only the new_trained_output adapts, providing fresh capacity for learning while preserving existing representations. This approach requires no changes to the number of trainable parameters and can be applied at any point during training.

## Key Results
- Plasticity injection improves performance on 57 Atari games by up to 20% compared to parameter resets and width scaling
- The method enables dynamic network growth, saving approximately 20 hours of wallclock time on an A100 GPU
- Plasticity loss occurs at different times across environments (25M-100M frames), with learning rate and replay ratio significantly affecting its onset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Plasticity injection restores learning ability by adding a fresh, untrained component while keeping predictions stable
- Mechanism: Freezes the original network and introduces a new residual branch with randomly initialized weights. The output is computed as frozen_output + new_trained_output - new_frozen_output. Initially, new_trained_output equals new_frozen_output, so predictions remain unchanged. As training proceeds, only the new_trained_output adapts, providing a "clean slate" to learn new patterns without disrupting existing knowledge.
- Core assumption: The frozen original network retains useful representations that the residual branch can build upon
- Evidence anchors:
  - [abstract] "plasticity injection, a minimalistic intervention that increases the network plasticity without changing the number of trainable parameters or biasing the predictions"
  - [section 4] "After plasticity injection the predictions of the neural network remain unaltered. As learning progresses, θ′1 deviates from θ′2 and hθ(x) − hθ′2(x) serves as a bias term for predictions."
  - [corpus] Evidence is strong: related papers discuss similar mechanisms for addressing plasticity loss through residual learning or parameter resets

### Mechanism 2
- Claim: Freezing most of the network prevents interference between old and new learning, reducing catastrophic forgetting
- Mechanism: By keeping the encoder and most of the network frozen, the method preserves previously learned features while only allowing the newly added head to adapt. This isolates new learning from interfering with established representations.
- Core assumption: The frozen encoder contains valuable, generalizable features that should not be disturbed
- Evidence anchors:
  - [section 4] "we apply the intervention to only a subset of the parameters" and "we do not stop the gradient propagation from any of the components of the output"
  - [section 5.2] Performance improvements in environments like Phoenix suggest preserved representations were beneficial
  - [corpus] Related work supports the idea that freezing earlier layers while allowing later layers to adapt can preserve useful features while enabling adaptation

### Mechanism 3
- Claim: The residual architecture allows efficient computation by only training a small additional component rather than the entire network
- Mechanism: Instead of retraining the full network from scratch, plasticity injection only trains the new residual branch (θ′1), which has fewer parameters than the full network. This reduces computational cost compared to complete network retraining.
- Core assumption: The computational savings from training fewer parameters outweigh the overhead of maintaining the frozen components
- Evidence anchors:
  - [section 5.3] "plasticity injection can be used to improve the computational efficiency of RL training" and "it saves about 20 hours of wallclock time on an A100 GPU"
  - [section 4] "the intervention enables careful analysis of the plasticity loss phenomenon in RL while keeping other confounding factors aside"
  - [corpus] Evidence is mixed: while related papers discuss computational efficiency, specific timing comparisons are limited

## Foundational Learning

- Concept: Reinforcement Learning with function approximation
  - Why needed here: The paper studies plasticity loss specifically in deep RL agents that use neural networks to approximate value functions or policies
  - Quick check question: What distinguishes RL from supervised learning in terms of the data distribution the agent learns from?

- Concept: Catastrophic forgetting and continual learning
  - Why needed here: Plasticity loss is related to catastrophic forgetting, where neural networks lose the ability to learn new tasks or patterns over time
  - Quick check question: How does plasticity loss differ from catastrophic forgetting in terms of what aspects of learning are affected?

- Concept: Network capacity and representational power
  - Why needed here: The paper discusses how network size affects plasticity loss, with larger networks maintaining plasticity longer
  - Quick check question: Why might a larger network maintain plasticity longer than a smaller one during training?

## Architecture Onboarding

- Component map: The original network consists of an encoder (first k layers) and a head (remaining layers). After plasticity injection, there are three components: frozen original head, trained new head, and frozen copy of new head. The output is computed as encoder_output + trained_head_output - frozen_head_output.

- Critical path: During inference, input flows through the frozen encoder, then through all three heads in parallel, with their outputs combined according to the residual formula. During training, only the trained new head receives gradients.

- Design tradeoffs: The method trades increased memory usage (maintaining frozen copies) for improved learning ability and computational efficiency. Alternative designs could freeze different components or use different architectures for the residual branch.

- Failure signatures: If plasticity injection doesn't improve performance, it may indicate that plasticity loss isn't the primary bottleneck, or that the frozen components have become irrelevant. If performance degrades, it may suggest interference between components.

- First 3 experiments:
  1. Apply plasticity injection to a baseline DQN agent on a single Atari game and compare learning curves before and after injection
  2. Vary the injection timing (e.g., 25M, 50M, 100M frames) to identify when plasticity loss occurs in different environments
  3. Compare plasticity injection against alternatives like parameter resets and width scaling on multiple Atari games to validate computational efficiency claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the fundamental mechanisms causing plasticity loss in deep reinforcement learning?
- Basis in paper: [explicit] The paper states that "the precise mechanisms causing loss of plasticity in RL are not well understood" and identifies this as a key challenge
- Why unresolved: The paper focuses on diagnosing and mitigating plasticity loss through plasticity injection rather than identifying root causes. The complex relationship between plasticity, exploration, and performance makes causal analysis difficult
- What evidence would resolve it: Systematic ablation studies varying network architectures, learning rates, replay ratios, and other hyperparameters while measuring plasticity proxies and performance could reveal which factors most strongly influence plasticity loss

### Open Question 2
- Question: Can plasticity loss be completely eliminated rather than just mitigated?
- Basis in paper: [inferred] The paper demonstrates that plasticity injection can mitigate but not eliminate plasticity loss, as performance still degrades over time even with intervention. The authors ask "Can we solve the problem of plasticity loss completely?"
- Why unresolved: Current methods like plasticity injection only temporarily restore learning capacity. The paper shows that even with multiple injections or larger networks, plasticity still degrades, suggesting fundamental limitations in current approaches
- What evidence would resolve it: Developing a method that maintains constant learning capacity throughout training, validated across diverse environments and network architectures, would demonstrate complete elimination of plasticity loss

### Open Question 3
- Question: Which properties of newly initialized networks enable high plasticity, and can these properties be engineered without full re-initialization?
- Basis in paper: [explicit] The paper concludes by asking "Which properties of newly initialized networks enable high plasticity?" and suggests this is key for training truly intelligent agents
- Why unresolved: While plasticity injection works by adding newly initialized parameters, the paper doesn't investigate what specific characteristics of these parameters (weight distribution, normalization, architecture features) contribute to their high plasticity
- What evidence would resolve it: Comparative studies testing different initialization schemes, weight constraints, and architectural modifications on newly added parameters while measuring learning efficiency and long-term plasticity would identify the critical properties

## Limitations

- The exact timing for optimal plasticity injection varies significantly across environments, requiring environment-specific tuning
- The method increases memory usage by maintaining frozen copies of network components
- The fundamental causes of plasticity loss remain unclear, limiting understanding of when and why the intervention is needed

## Confidence

- **High confidence**: The plasticity injection mechanism works as described and improves performance on Atari benchmarks
- **Medium confidence**: Plasticity loss is a real phenomenon affecting deep RL agents, and learning rate/replay ratio affect it
- **Medium confidence**: The method serves as an effective diagnostic tool for identifying plasticity-related performance plateaus

## Next Checks

1. Conduct ablation studies varying the fraction of frozen parameters to determine the optimal balance between preserving old knowledge and enabling new learning
2. Test plasticity injection on non-Atari domains (e.g., continuous control tasks) to verify generalizability beyond the arcade setting
3. Analyze weight norm dynamics before and after injection across multiple training runs to quantify the extent of plasticity recovery