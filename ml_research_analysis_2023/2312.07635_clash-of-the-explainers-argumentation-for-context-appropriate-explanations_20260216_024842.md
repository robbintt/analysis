---
ver: rpa2
title: 'Clash of the Explainers: Argumentation for Context-Appropriate Explanations'
arxiv_id: '2312.07635'
source_url: https://arxiv.org/abs/2312.07635
tags:
- explanation
- explanations
- system
- argumentation
- stakeholder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of selecting context-appropriate
  explanations in explainable AI (XAI) by proposing a modular reasoning system that
  uses argumentation techniques. The core idea is to formalize the characteristics
  of explanation techniques and map them to stakeholder needs, allowing for transparent
  selection of the most suitable explainer.
---

# Clash of the Explainers: Argumentation for Context-Appropriate Explanations

## Quick Facts
- arXiv ID: 2312.07635
- Source URL: https://arxiv.org/abs/2312.07635
- Reference count: 37
- Primary result: Working example demonstrating selection between LIME and counterfactual explanations for housing price prediction use case

## Executive Summary
This paper addresses the challenge of selecting context-appropriate explanations in explainable AI (XAI) by proposing a modular reasoning system that uses argumentation techniques. The core idea is to formalize the characteristics of explanation techniques and map them to stakeholder needs, allowing for transparent selection of the most suitable explainer. The system employs Dung's Abstract Argumentation Framework and Gorgias Preference-based Argumentation Framework to reason over available explanation techniques and prioritize the best one for the given context.

## Method Summary
The proposed method implements a modular reasoning system consisting of four components: a mental model of the stakeholder, a multi-explainer component, a reasoner component using argumentation, and the ML system itself. The reasoner component uses argumentation frameworks to formalize supporting premises and inferences that map stakeholder characteristics to explanation technique properties. By creating attack relations between arguments and using admissibility-based semantics, the system selects the most appropriate explainer for the given context. The method was demonstrated using a housing price prediction use case comparing LIME and counterfactual explanations.

## Key Results
- Successfully demonstrated selection of counterfactual explanations over LIME for housing price prediction use case
- Showed how argumentation frameworks can formalize the selection process between multiple explanation techniques
- Demonstrated transparency in explanation selection through visible reasoning steps
- Identified computational efficiency as a key preference criterion in the argumentation process

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The system uses argumentation frameworks to resolve conflicts between multiple explanation techniques based on stakeholder needs.
- Mechanism: Dung's Abstract Argumentation Framework and Gorgias Preference-based Argumentation Framework are employed to formalize supporting premises and inferences that map stakeholder characteristics to explanation technique properties. The framework creates attack relations between arguments (e.g., "method X is computationally cheap" vs "method X is not trustworthy") and uses admissibility-based semantics to select the most appropriate explainer.
- Core assumption: Stakeholder characteristics and explanation technique properties can be accurately formalized as arguments with clear attack relations and preferences.
- Evidence anchors:
  - [abstract] "Due to the transparency they afford, we propose employing argumentation techniques to reach an agreement over the most suitable explainers from a given set of possible explainers."
  - [section 4.1] "Using syntax from the Gorgias framework, we can represent knowledge of rules, conflicts, and preferences using predicate symbols."

### Mechanism 2
- Claim: The modular system allows flexible selection of explanation techniques by maintaining separate components for stakeholder modeling, explanation generation, and reasoning.
- Mechanism: The system consists of four components: a mental model of the stakeholder (capturing their knowledge and needs), a multi-explainer component (containing various XAI techniques), a reasoner component (using argumentation to select the best explainer), and the ML system itself. This separation allows each component to be independently developed and modified while maintaining system coherence.
- Core assumption: The mental model of the stakeholder accurately represents their knowledge state and needs, and the explainers can provide sufficient information about their own characteristics.
- Evidence anchors:
  - [abstract] "In this paper, we propose a modular reasoning system consisting of a given mental model of the relevant stakeholder, a reasoner component that solves the argumentation problem generated by a multi-explainer component, and an AI model that is to be explained suitably to the stakeholder of interest."
  - [section 4] "One critical component is a given mental model of the relevant stakeholder. Here, we will assume that the mental model is already defined and provided."

### Mechanism 3
- Claim: Transparency is achieved by making the reasoning steps visible to stakeholders, allowing them to understand why a particular explanation technique was selected.
- Mechanism: The argumentation framework explicitly shows the rules, conflicts, and preferences used in the selection process. By presenting this information to stakeholders, they can see the chain of reasoning that led to the final decision, making the system's choices contestable and interpretable.
- Core assumption: Stakeholders can understand and benefit from seeing the formal argumentation structure and reasoning steps.
- Evidence anchors:
  - [abstract] "By formalising supporting premises—and inferences—we can map stakeholder characteristics to those of explanation techniques. This allows us to reason over the techniques and prioritise the best one for the given context, while also offering transparency into the selection decision."
  - [section 4.1] "Visualisations of the AA framework as a graph support the value of transparency, offering a representation of the argumentation flow that is often easily digestible for humans."

## Foundational Learning

- Concept: Dung's Abstract Argumentation Framework
  - Why needed here: Provides the formal structure for representing arguments and attack relations between explanation techniques
  - Quick check question: What makes an argument set "admissible" in Dung's framework?

- Concept: Gorgias Preference-based Argumentation Framework
  - Why needed here: Extends basic argumentation with preference relations to handle situations where multiple explanations might be valid but some are more suitable based on stakeholder priorities
  - Quick check question: How does Gorgias handle situations where argument A has higher priority than argument B?

- Concept: Mental models in human-computer interaction
  - Why needed here: Forms the basis for understanding stakeholder needs and knowledge states, which drives the explanation selection process
  - Quick check question: What are the key components of a mental model according to Carroll and Olson?

## Architecture Onboarding

- Component map:
  - Mental Model Component: Stores stakeholder characteristics, needs, and knowledge state
  - Multi-Explainer Component: Contains various XAI techniques with their characteristic descriptions
  - Reasoner Component: Contains Knowledge Base and Argumentation Solver
  - ML System Component: Contains dataset, trained model, and metadata

- Critical path: Mental Model → Reasoner (KB + Solver) → Multi-Explainer → ML System
  The mental model provides stakeholder information to the reasoner, which queries explainers about their characteristics and uses argumentation to select the best one for the given context.

- Design tradeoffs:
  - Flexibility vs. complexity: Adding more explainers increases flexibility but also increases the complexity of the argumentation framework
  - Transparency vs. simplicity: Detailed argumentation steps increase transparency but may overwhelm stakeholders
  - Accuracy vs. efficiency: More sophisticated mental models and preference systems improve accuracy but require more computational resources

- Failure signatures:
  - Empty solution set (E = ∅) from the solver indicates conflicting requirements with no viable explanation
  - Incorrect selection of explainer that doesn't match stakeholder needs
  - System becomes too slow due to large number of explainers and complex argumentation

- First 3 experiments:
  1. Implement the housing price prediction use case with LIME and counterfactual explanations, verifying the system selects counterfactual explanations as shown in the paper
  2. Test the system with conflicting stakeholder preferences (e.g., computational efficiency vs. trustworthiness) to ensure proper argumentation handling
  3. Evaluate the transparency by having users review the argumentation steps and verify they can understand why a particular explainer was selected

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can stakeholder mental models be effectively constructed and updated in real-time during interactive explanations?
- Basis in paper: [explicit] The paper mentions that "Neurosymbolic techniques can therefore be employed to extract symbolic rules from the user's natural language prompts" and discusses the need for "continuous updates to the mental model of the stakeholder" but doesn't specify implementation details.
- Why unresolved: The paper identifies this as future work but doesn't provide concrete methodology for how to implement real-time mental model updates during interactive explanations.
- What evidence would resolve it: A working implementation showing how natural language prompts are converted to symbolic rules and how these updates affect the explanation selection process in real-time.

### Open Question 2
- Question: What is the optimal balance between computational efficiency and explanation quality when selecting between multiple XAI techniques?
- Basis in paper: [inferred] The paper discusses preferences for computationally cheap methods and mentions computational costs as a characteristic to consider, but doesn't quantify the trade-offs or provide guidelines for balancing these factors.
- Why unresolved: While the paper identifies computational cost as a factor in the argumentation framework, it doesn't provide empirical data on how this impacts explanation quality or user satisfaction.
- What evidence would resolve it: Empirical studies comparing user comprehension and satisfaction across different computational efficiency levels for various XAI techniques.

### Open Question 3
- Question: How can the argumentation framework be extended to handle conflicting stakeholder preferences within the same context?
- Basis in paper: [explicit] The paper mentions "reasoning over multiple explanations can be used to map explainers to explainee characteristics" and discusses preferences, but doesn't address how to resolve conflicts when different stakeholders have opposing requirements.
- Why unresolved: The paper focuses on single-stakeholder scenarios and doesn't explore how to handle situations where multiple stakeholders with different preferences need to be served by the same explanation system.
- What evidence would resolve it: A modified argumentation framework that incorporates conflict resolution mechanisms and empirical validation showing how it handles competing stakeholder requirements.

## Limitations

- The system relies heavily on the accuracy of the mental model of stakeholders, which is assumed to be provided but not specified how to construct
- Current implementation is limited to a single use case with only two explanation techniques
- Lacks empirical validation through human-subject studies to verify effectiveness

## Confidence

- **High**: The core mechanism of using argumentation frameworks for explanation selection is well-founded and technically sound
- **Medium**: The modular architecture design and its theoretical benefits are plausible but not yet empirically validated
- **Low**: The practical effectiveness of the approach in real-world scenarios with diverse stakeholders and explanation techniques is not yet demonstrated

## Next Checks

1. Conduct a user study comparing stakeholder satisfaction and understanding when using the argumentation-based explanation selection versus random or expert-chosen selections
2. Implement the system with a broader range of explanation techniques (beyond LIME and counterfactual) to test scalability and handling of more complex argumentation scenarios
3. Develop and test methods for automatically constructing and updating stakeholder mental models based on interaction data and feedback