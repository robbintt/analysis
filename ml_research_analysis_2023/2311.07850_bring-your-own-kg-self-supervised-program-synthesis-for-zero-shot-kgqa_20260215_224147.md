---
ver: rpa2
title: 'Bring Your Own KG: Self-Supervised Program Synthesis for Zero-Shot KGQA'
arxiv_id: '2311.07850'
source_url: https://arxiv.org/abs/2311.07850
tags:
- question
- join
- byokg
- tropical
- exploration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces BYOKG, a universal question-answering (QA)
  system that can operate on any knowledge graph (KG), requires no human-annotated
  training data, and can be ready to use within a day. The key idea is to mechanize
  the human tendency to explore and understand new KGs by using an LLM-backed symbolic
  agent to generate diverse query-program exemplars.
---

# Bring Your Own KG: Self-Supervised Program Synthesis for Zero-Shot KGQA

## Quick Facts
- arXiv ID: 2311.07850
- Source URL: https://arxiv.org/abs/2311.07850
- Reference count: 22
- F1-score of 27.89 on GrailQA zero-shot split

## Executive Summary
BYOKG is a universal knowledge graph question answering (KGQA) system that requires no human-annotated training data and can be deployed on any KG within a day. The system mechanizes human exploration behavior through a symbolic graph-based random walk procedure that generates diverse executable programs, which are then converted to natural language questions using an LLM. A bottom-up reasoning module with compositional generalization and inverse-consistency re-ranking predicts programs for arbitrary questions, achieving dramatic gains over zero-shot baselines.

## Method Summary
BYOKG operates through three main stages: (1) Symbolic graph exploration that uses random walks over KG nodes to enumerate diverse programs guaranteed to be executable, (2) LLM-based question generation using least-to-most prompting with schema descriptions and inverse-consistency re-ranking to improve generation quality, and (3) Bottom-up reasoning with case-based retrieval that iteratively synthesizes programs for test questions using the exploration corpus, combined with candidate pruning and inverse-consistency re-ranking to recover from exploration coverage gaps.

## Key Results
- BYOKG achieves 27.89 F1 on the zero-shot split of GrailQA and 58.02 F1 on MetaQA
- Outperforms a supervised in-context learning method and state-of-the-art fine-tuned model by 7.08 F1 on GrailQA zero-shot split
- Shows consistent gains with increasing exploration budget, suggesting room for further improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BYOKG's symbolic graph exploration generates a diverse set of executable programs that provide approximate coverage of query patterns supported by the KG.
- Mechanism: The symbolic agent uses random walks over KG nodes combined with graph operations (e.g., COUNT, ARGMAX, >=) to enumerate diverse programs. The agent extends sub-programs by randomly selecting reachable schema items until the desired complexity is achieved, then optionally applies program functions.
- Core assumption: Diverse program generation through random exploration approximates the distribution of real-world queries without requiring curated training data.
- Evidence anchors:
  - [abstract]: "BYOKG draws inspiration from the remarkable ability of humans to comprehend information present in an unseen KG through exploration — starting at random nodes, inspecting the labels of adjacent nodes and edges, and combining them with their prior world knowledge."
  - [section 3.1]: "Our approach uses a symbolic, graph-based random walk procedure, which enumerates a diverse set of programs guaranteed to be executable."
  - [corpus]: Weak evidence - the corpus doesn't directly discuss the exploration mechanism, but shows related work on KGQA that uses graph traversal methods.
- Break condition: If the KG contains query patterns that require specific semantic knowledge or reasoning that random exploration cannot discover, coverage will be insufficient.

### Mechanism 2
- Claim: Inverse-consistency re-ranking improves zero-shot question generation quality by scoring outputs based on the likelihood of the inverse task (program synthesis).
- Mechanism: For each candidate question generation, compute the log-probability of re-generating the original program from the question using a separate inverse task prompt. Re-rank candidates using this score combined with the original generation score.
- Core assumption: The inverse task (program synthesis) provides complementary information to the forward task (question generation) that helps identify semantically accurate outputs.
- Evidence anchors:
  - [abstract]: "We develop a novel inverse-consistency re-ranking method for LLM generation, which computes scores for generated queries based on the likelihood of the query re-generating the program."
  - [section 3.2.2]: "To re-rank ycands, we now construct the following inverse task: T^-1 := x | I^-1, D^-1, y, i.e. the task of predicting the query sequence x given an output sequence y from T."
  - [corpus]: Weak evidence - the corpus mentions related work on KGQA and language models but doesn't specifically discuss inverse-consistency re-ranking.
- Break condition: If the inverse task prompt construction is poor or the LLM cannot accurately perform program synthesis, the re-ranking will not improve accuracy.

### Mechanism 3
- Claim: BYOKG's bottom-up reasoning with compositional generalization and candidate pruning enables accurate program synthesis even with limited exploration coverage.
- Mechanism: Start with topic entities from the test question, iteratively extend sub-programs using schema items, prune candidates based on similarity with the question, and use inverse-consistency re-ranking to recover from exploration coverage gaps.
- Core assumption: Even with incomplete exploration, the combination of compositional generalization (combining seen sub-programs into novel constructs) and inverse-consistency re-ranking can produce accurate programs for unseen query patterns.
- Evidence anchors:
  - [abstract]: "With the original motivation of designing a QA system that can work on any KG, we opt for a semi-parametric, case-based reasoning approach instead of KG-specific fine-tuning."
  - [section 3.3]: "Therefore, an important desiderata for our reasoning module is the ability to generalize compositionally, i.e. combine seen sub-programs into novel constructs."
  - [corpus]: Weak evidence - the corpus mentions related work on KGQA reasoning methods but doesn't specifically discuss BYOKG's compositional generalization approach.
- Break condition: If the exploration coverage is too sparse relative to the query distribution, or if the LLM cannot effectively combine sub-programs compositionally, accuracy will degrade significantly.

## Foundational Learning

- Concept: Knowledge Graph Structure and Query Languages
  - Why needed here: BYOKG operates on KGs and generates programs in SPARQL or s-expression format that can be executed against the KG.
  - Quick check question: Can you write a SPARQL query to find all movies starring a specific actor in a KG with movie, actor, and starred_actors relations?

- Concept: Large Language Model Prompting and In-Context Learning
  - Why needed here: BYOKG uses LLMs for question generation, least-to-most prompting, and inverse-consistency re-ranking, all of which rely on effective prompting strategies.
  - Quick check question: How would you structure a prompt to generate a question from a logical program using schema descriptions of relations and classes?

- Concept: Random Walks and Graph Traversal Algorithms
  - Why needed here: The symbolic exploration uses random walks over the KG to generate diverse programs, requiring understanding of graph traversal concepts.
  - Quick check question: What is the difference between depth-first and breadth-first search, and how might each affect the diversity of programs generated during KG exploration?

## Architecture Onboarding

- Component map: Symbolic Graph Explorer -> LLM Question Generator -> Inverse-Consistency Re-ranker -> Bottom-Up Reasoner -> Candidate Pruner -> KG Query Engine
- Critical path: Exploration → Question Generation → Reasoning → Answer Retrieval
  - Exploration must complete before reasoning can begin
  - Question generation depends on successful exploration
  - Reasoning requires the exploration corpus for in-context demonstrations
- Design tradeoffs:
  - Exploration budget vs. coverage: More programs improve coverage but increase setup time
  - Model size vs. cost: Larger LLMs improve accuracy but increase inference costs
  - Pruning aggressiveness vs. recall: Aggressive pruning improves efficiency but may miss correct programs
- Failure signatures:
  - Low exploration coverage → poor reasoning performance, especially on compositional and zero-shot queries
  - Poor inverse-consistency re-ranking → generation quality doesn't improve despite additional computation
  - Inefficient candidate scoring → prohibitively long inference times per question
- First 3 experiments:
  1. Run exploration on a small KG (e.g., MoviesKG) with a limited budget (1K programs) and verify program diversity and executability
  2. Generate questions for the explored programs using a small LLM (e.g., MPT-7B) and evaluate quality with automatic metrics (ROUGE, BLEU, BERTscore)
  3. Test reasoning on a small set of questions using the exploration corpus, measuring accuracy and inference time with different pruning thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal exploration budget for BYOKG in terms of maximizing QA accuracy while minimizing computational costs?
- Basis in paper: The paper mentions that they set the exploration budget to 10K programs to meet the goal of readying a QA system within a day, but also notes that BYOKG shows consistent gains with increasing exploration budget and suggests room for further improvement.
- Why unresolved: The paper does not provide a detailed analysis of how different exploration budgets affect QA accuracy and computational costs.
- What evidence would resolve it: A study comparing QA accuracy and computational costs for different exploration budgets (e.g., 5K, 10K, 20K, 50K programs) on various KG datasets.

### Open Question 2
- Question: How does the inverse-consistency re-ranking method compare to other re-ranking techniques, such as PMI-based scoring or contrastive learning, in terms of improving zero-shot generation accuracy?
- Basis in paper: The paper introduces inverse-consistency re-ranking and shows it improves generation quality, but also mentions related work using PMI scoring and contrastive learning for re-ranking.
- Why unresolved: The paper does not directly compare inverse-consistency re-ranking to other re-ranking techniques.
- What evidence would resolve it: An empirical comparison of inverse-consistency re-ranking, PMI-based scoring, and contrastive learning methods on the same question generation tasks and datasets.

### Open Question 3
- Question: Can BYOKG be extended to handle other types of structured data beyond knowledge graphs, such as databases or spreadsheets?
- Basis in paper: The paper focuses on KGQA, but the authors mention that reasoning in BYOKG can be seen as iteratively constructing a plan to navigate the KG, which is similar to other reasoning tasks over structured data.
- Why unresolved: The paper does not explore the applicability of BYOKG to other types of structured data.
- What evidence would resolve it: A study applying BYOKG to question answering over databases or spreadsheets and comparing its performance to existing methods for these tasks.

## Limitations
- Exploration coverage may be insufficient for specialized domains requiring specific semantic knowledge
- Inverse-consistency re-ranking effectiveness depends on LLM's program synthesis capabilities
- Compositional generalization claims lack comprehensive empirical validation of boundaries

## Confidence
- High Confidence: Core architectural components (graph exploration, LLM-based question generation, bottom-up reasoning) are well-defined and empirically validated
- Medium Confidence: Inverse-consistency re-ranking mechanism shows consistent improvements across experiments
- Low Confidence: Claims about compositional generalization boundaries and exploration coverage sufficiency lack rigorous empirical validation

## Next Checks
1. Systematically measure coverage of explored programs against full distribution of GrailQA test queries, specifically quantifying fraction of compositional and zero-shot patterns discovered
2. Conduct ablation study comparing question generation quality with and without inverse-consistency re-ranking across different LLM sizes
3. Design targeted experiments testing compositional generalization limits by systematically varying complexity and novelty of test queries beyond exploration corpus coverage