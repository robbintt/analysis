---
ver: rpa2
title: 'Enabling Resource-efficient AIoT System with Cross-level Optimization: A survey'
arxiv_id: '2309.15467'
source_url: https://arxiv.org/abs/2309.15467
tags:
- memory
- training
- aiot
- inference
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys cross-level optimization techniques for resource-efficient
  AIoT systems, addressing the challenge of deploying deep learning models on resource-constrained
  devices. It proposes a taxonomy of optimization methods spanning resource-friendly
  algorithms, model-adaptive system scheduling, and context-aware controllers.
---

# Enabling Resource-efficient AIoT System with Cross-level Optimization: A survey

## Quick Facts
- arXiv ID: 2309.15467
- Source URL: https://arxiv.org/abs/2309.15467
- Reference count: 40
- Key outcome: This paper surveys cross-level optimization techniques for resource-efficient AIoT systems, addressing the challenge of deploying deep learning models on resource-constrained devices. It proposes a taxonomy of optimization methods spanning resource-friendly algorithms, model-adaptive system scheduling, and context-aware controllers. The paper covers both on-device and distributed DL inference and training tasks, presenting enabling techniques such as model compression, computation graph optimization, memory scheduling, and hardware instruction optimization. It also discusses enabling engines and applications for AIoT, including smart homes, cities, and industries, and identifies open issues and future directions for research.

## Executive Summary
This survey comprehensively examines cross-level optimization techniques for resource-efficient AIoT systems, addressing the fundamental challenge of deploying deep learning models on resource-constrained devices. The paper proposes a taxonomy spanning three optimization levels: resource-friendly algorithms, model-adaptive system scheduling, and context-aware controllers. It covers both on-device and distributed DL inference and training tasks, presenting enabling techniques such as model compression, computation graph optimization, memory scheduling, and hardware instruction optimization.

The survey identifies key challenges in AIoT DL deployment, including memory limitations, computational constraints, and dynamic resource availability. It presents enabling engines and applications for AIoT, including smart homes, cities, and industries, while also identifying open issues and future research directions. The cross-level optimization framework aims to jointly optimize algorithm design and system implementation to achieve better runtime resource availability and performance.

## Method Summary
The paper employs a comprehensive survey methodology, systematically reviewing existing literature on cross-level optimization for AIoT systems. It synthesizes techniques across three optimization levels: resource-friendly algorithms (model compression, NAS), model-adaptive system scheduling (compiler optimization, memory scheduling), and context-aware controllers (intra-device, inter-device). The survey covers both on-device and distributed DL inference and training tasks, providing a taxonomy of enabling techniques and discussing applications in smart homes, cities, and industries. The method involves analyzing state-of-the-art approaches, identifying gaps, and proposing future research directions for automated cross-level optimization in dynamic AIoT environments.

## Key Results
- Cross-level optimization techniques enable efficient deployment of deep learning models on resource-constrained AIoT devices by jointly optimizing algorithms and system scheduling
- Distributed resource aggregation improves both inference and training efficiency by leveraging spatial and temporal correlations in device capabilities and data
- Near-memory and in-memory computing reduce data movement costs, significantly improving latency and energy efficiency for AIoT devices
- Context-aware controllers using reinforcement learning enable automatic adaptation to dynamic AIoT environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-level co-design propagates runtime system feedback into algorithm design, enabling fine-grained adaptation to dynamic AIoT contexts.
- Mechanism: Runtime monitoring captures resource supply (memory, compute, battery) and performance outcomes (latency, accuracy). This feedback loop updates algorithm-level model structures and compression strategies in real-time, rather than relying solely on offline training.
- Core assumption: The algorithm-system interface can be instrumented to provide actionable performance metrics without excessive overhead.
- Evidence anchors:
  - [abstract] "The algorithm-system co-design that jointly optimizes the resource-friendly DL models and model-adaptive system scheduling improves the runtime resource availability"
  - [section] "Prior efforts in cross-level algorithm-system co-design like PCONV [346], PatDNN [344], and CoCoPIE [345] simultaneously optimize the model compression algorithms and runtime operator/memory scheduling mechanisms"
  - [corpus] No direct evidence found in corpus for runtime feedback propagation to algorithm design
- Break Condition: If runtime monitoring introduces unacceptable overhead or if the algorithm cannot adapt quickly enough to transient resource changes.

### Mechanism 2
- Claim: Distributed AIoT resource aggregation improves both inference and training efficiency by leveraging spatial and temporal correlations in device capabilities and data.
- Mechanism: Devices with complementary resources and data are dynamically grouped based on their current state and the task requirements. This grouping enables parallel processing and reduces the burden on individual devices.
- Core assumption: The networked AIoT system can accurately assess device capabilities and data correlations in real-time to form effective collaboration groups.
- Evidence anchors:
  - [abstract] "Distributed AIoT Resource Aggregation in DL inference/training Tasks"
  - [section] "Efficiently aggregating memory and computing resources within the networked AIoT system for seamless communication and the provision of complex services"
  - [corpus] No direct evidence found in corpus for spatial and temporal correlation-based resource aggregation
- Break Condition: If device heterogeneity leads to significant synchronization overhead or if data privacy concerns prevent effective collaboration.

### Mechanism 3
- Claim: Near-memory and in-memory computing reduce data movement costs, significantly improving latency and energy efficiency for AIoT devices.
- Mechanism: Compute units are physically relocated closer to memory arrays, allowing for direct processing of data without the need for extensive data transfers. This is particularly beneficial for data-intensive operations like ReLU and other activation functions.
- Core assumption: The benefits of reduced data movement outweigh the potential limitations in computing power and flexibility of near-memory units.
- Evidence anchors:
  - [abstract] "Memory-computation Feedback and Joint-optimization"
  - [section] "Near- and in-memory computing. The underlying memory schedule techniques have not kept up with computation optimization advances in latency and energy reduction over the years"
  - [corpus] No direct evidence found in corpus for near-memory and in-memory computing in AIoT
- Break Condition: If the compute units near memory cannot support the required operations or if the cost of hardware redesign outweighs the benefits.

## Foundational Learning

- Concept: Dynamic Resource Monitoring
  - Why needed here: AIoT devices have highly variable resource availability due to other tasks, battery drain, and OS scheduling.
  - Quick check question: What are the key resources affecting DL model deployment in AIoT devices, and how do they vary over time?

- Concept: Cross-Level Algorithm-System Co-Design
  - Why needed here: Algorithm-level techniques alone cannot fully exploit the capabilities of AIoT hardware; system-level optimization is needed for runtime efficiency.
  - Quick check question: How does runtime system feedback influence algorithm-level model design in AIoT systems?

- Concept: Distributed Resource Aggregation
  - Why needed here: Single AIoT devices often lack the resources to handle complex DL tasks; aggregating resources from multiple devices improves performance.
  - Quick check question: What are the challenges and benefits of aggregating resources from distributed AIoT devices for DL tasks?

## Architecture Onboarding

- Component Map: Algorithm Layer (DL models, compression techniques, NAS) -> System Layer (Compiler, runtime engine, memory scheduling, hardware instructions) -> Controller Layer (Intra-device and inter-device controllers for adaptive optimization)
- Critical Path: Algorithm design → System scheduling → Runtime execution → Performance monitoring → Algorithm adaptation
- Design Tradeoffs:
  - Memory vs. Computation: Techniques like recomputation save memory but increase computation time.
  - Accuracy vs. Resource Efficiency: Model compression reduces resource usage but may decrease accuracy.
  - On-Device vs. Distributed Processing: On-device processing offers privacy but limited resources; distributed processing offers more resources but potential communication overhead.
- Failure Signatures:
  - Memory exhaustion during DL training
  - Significant accuracy drop due to data drift
  - Excessive latency due to synchronization overhead in distributed systems
  - Resource contention with other tasks on the device
- First 3 Experiments:
  1. Deploy a basic DL model on an MCU and measure memory usage and latency.
  2. Implement a simple memory swapping mechanism for DL training on a GPU and compare memory usage and training time.
  3. Test a distributed DL inference scenario with two devices and evaluate the impact of model partitioning on latency and accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can runtime resource-efficient algorithms propagate feedback from system execution to algorithm design in AIoT systems?
- Basis in paper: [explicit] "The challenge is how to propagate the feedback of the runtime system execution to the algorithm design" and "State-of-the-art on algorithm-system co-design like PCONV [346], PatDNN [344], and CoCoPIE [345] simultaneously optimize the model compression algorithms and runtime operator/memory scheduling mechanisms."
- Why unresolved: Existing works focus on manual optimization of partial system levels. The paper identifies the need for automated feedback loops between algorithm design and runtime execution, but no clear solution is presented.
- What evidence would resolve it: A working prototype that demonstrates automated adjustment of DNN model architecture based on real-time performance metrics collected during inference, with quantifiable improvements in latency or energy efficiency compared to static models.

### Open Question 2
- Question: What are the optimal strategies for resource allocation between training and inference tasks on resource-constrained AIoT devices?
- Basis in paper: [explicit] "Making accurate decisions on resource allocation and performance assessment is challenging in advance DL training" and "The best decision at the current training may become suboptimal for future training phases."
- Why unresolved: The paper identifies the tradeoff between training speed and inference performance, but doesn't provide concrete allocation strategies. The dynamic nature of AIoT context adds complexity to finding optimal solutions.
- What evidence would resolve it: A comprehensive framework that dynamically allocates resources between training and inference tasks based on real-time accuracy requirements, energy constraints, and latency demands, with demonstrated performance improvements across multiple AIoT scenarios.

### Open Question 3
- Question: How can distributed AIoT devices effectively collaborate for DL training when datasets are temporally correlated?
- Basis in paper: [explicit] "The temporal correlation of the distributed data at diverse sub-clusters of AIoT devices varies" and "The data distribution collected by AIoT devices is clearly more diverse and challenging."
- Why unresolved: The paper discusses the need for different aggregation mechanisms based on temporal patterns, but doesn't provide specific solutions for handling asynchronous datasets and varying device capabilities.
- What evidence would resolve it: A distributed training protocol that leverages temporal correlations between datasets from physically proximate devices, demonstrating improved convergence speed and accuracy compared to traditional federated learning approaches.

## Limitations

- Low confidence in runtime feedback mechanisms: The survey claims that runtime system feedback propagates into algorithm design for fine-grained adaptation, but provides limited concrete evidence of how this feedback loop is implemented or validated.
- Unclear distributed resource aggregation evidence: While the paper discusses distributed AIoT resource aggregation for improving inference and training efficiency, the specific mechanisms for leveraging spatial and temporal correlations are not well-detailed.
- Limited hardware-specific validation: The claims about near-memory and in-memory computing benefits lack specific AIoT hardware implementations or measurements showing actual latency and energy efficiency improvements.

## Confidence

- High confidence: The taxonomy organization and survey comprehensiveness covering cross-level optimization techniques for AIoT systems.
- Medium confidence: The theoretical framework connecting algorithm, system, and controller layers, though practical implementation details remain sparse.
- Low confidence: The claimed mechanisms for runtime feedback propagation and distributed resource aggregation without empirical validation or specific implementation examples.

## Next Checks

1. **Runtime Feedback Implementation**: Develop a prototype AIoT system demonstrating how runtime performance metrics (latency, memory usage, battery level) directly influence model compression and scheduling decisions in real-time.

2. **Distributed Resource Aggregation Test**: Implement a distributed DL inference scenario across heterogeneous AIoT devices, measuring the actual benefits and overheads of resource aggregation compared to single-device processing.

3. **Near-Memory Computing Benchmark**: Create a benchmark comparing AIoT workloads using traditional memory access versus near-memory computing implementations on actual AIoT hardware (e.g., ARM-based devices with integrated accelerators).