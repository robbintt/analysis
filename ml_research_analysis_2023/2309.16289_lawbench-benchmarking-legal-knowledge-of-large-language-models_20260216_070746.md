---
ver: rpa2
title: 'LawBench: Benchmarking Legal Knowledge of Large Language Models'
arxiv_id: '2309.16289'
source_url: https://arxiv.org/abs/2309.16289
tags:
- legal
- llms
- tasks
- arxiv
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces LawBench, a comprehensive benchmark for
  evaluating large language models'' (LLMs) legal knowledge under the Chinese civil
  law system. The benchmark includes 20 diverse tasks covering 3 cognitive levels:
  memorization, understanding, and applying legal knowledge.'
---

# LawBench: Benchmarking Legal Knowledge of Large Language Models

## Quick Facts
- arXiv ID: 2309.16289
- Source URL: https://arxiv.org/abs/2309.16289
- Reference count: 40
- Primary result: GPT-4 significantly outperforms other LLMs on Chinese legal knowledge tasks, with legal-specific fine-tuning providing incremental but insufficient improvements

## Executive Summary
LawBench introduces a comprehensive benchmark for evaluating large language models' legal knowledge under the Chinese civil law system. The benchmark includes 20 diverse tasks covering three cognitive levels (memorization, understanding, applying) and five task types (single-label classification, multi-label classification, regression, extraction, and generation). Extensive evaluations of 51 LLMs reveal that while GPT-4 remains the best-performing model in legal domains, legal-specific fine-tuning brings only modest improvements. The study provides a structured framework for assessing legal reasoning capabilities and highlights the gap between current models and practical legal applications.

## Method Summary
The benchmark evaluates 51 LLMs (20 multilingual, 22 Chinese-oriented, and 9 legal-specific) on 20 tasks using zero-shot and one-shot inference via the OpenCompass platform. Task-specific answer extraction rules standardize model outputs for comparison, employing metrics including accuracy, F1 variants, nLog-distance, and Rouge-L. The evaluation framework maps tasks to Bloom's cognitive taxonomy and covers five task types across three skill levels. Results are aggregated to compare model performance across different categories and cognitive dimensions.

## Key Results
- GPT-4 significantly outperforms all other models on legal tasks, maintaining a substantial lead even over specialized legal LLMs
- Legal-specific fine-tuning provides measurable but limited improvements, failing to close the performance gap with general-purpose models
- Model performance scales with size in one-shot settings but shows inconsistent patterns in zero-shot scenarios, with ChatLaw as an outlier
- Task difficulty correlates with cognitive level, with application tasks proving most challenging for all models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark's three cognitive dimensions (memorization, understanding, applying) align with how humans acquire legal expertise, allowing models to be assessed in a structured progression.
- Mechanism: By mapping tasks into Bloom's taxonomy, the benchmark enforces prerequisite learning—models must demonstrate lower-level skills before excelling in higher-level tasks.
- Core assumption: Legal reasoning follows a hierarchical skill acquisition similar to human learning.
- Evidence anchors:
  - [abstract] "We divided these tasks into 3 skill levels according to widely accepted Bloom's cognitive models"
  - [section 3.1] "Bloom's Taxonomy divides learning objectives in the cognitive domain into six levels... we simplified Bloom's cognitive hierarchy model and kept the first three categories"
- Break condition: If legal tasks require non-linear reasoning (e.g., memorization and application occurring simultaneously), the hierarchical structure may misrepresent true capabilities.

### Mechanism 2
- Claim: The answer extraction rules standardize model outputs, enabling fair comparison across diverse LLM architectures.
- Mechanism: Task-specific regular expressions and parsing rules transform unstructured LLM predictions into comparable formats for metric computation.
- Core assumption: LLMs can be guided to produce structured outputs through instruction formatting and post-processing rules.
- Evidence anchors:
  - [section 3.3] "we define a set of task-specific rules to extract the answer from the model prediction"
  - [section 3.3] "We employ OpenCompass [13] to perform model inference... tailor the prompt using prefixes and suffixes specific to each model"
- Break condition: If extraction rules fail to capture valid but differently phrased answers, the evaluation may unfairly penalize certain models.

### Mechanism 3
- Claim: Fine-tuning on legal-specific data improves task performance but cannot close the gap with general-purpose models pre-trained on broader data.
- Mechanism: Legal-specific fine-tuning provides domain knowledge, but foundation models with larger, more diverse training corpora maintain superiority.
- Core assumption: General-purpose models have richer parametric knowledge from broader training data.
- Evidence anchors:
  - [abstract] "While fine-tuning LLMs on legal specific text brings certain improvements, we are still a long way from obtaining usable and reliable LLMs in legal tasks"
  - [section 4.4] "Legal specific fine-tuning is helpful... there is a consistent enhancement of model scores and reduction of abstention rates"
- Break condition: If legal-specific training data quality or quantity improves significantly, the performance gap may narrow.

## Foundational Learning

- Concept: Bloom's Taxonomy and cognitive skill hierarchies
  - Why needed here: The benchmark design explicitly uses Bloom's taxonomy to structure legal tasks; understanding this framework is essential for interpreting results.
  - Quick check question: What are the three cognitive levels used in LawBench, and how do they relate to legal skill acquisition?

- Concept: Regular expressions and text parsing for answer extraction
  - Why needed here: The evaluation methodology relies on task-specific rules to extract answers from LLM outputs; engineers need this skill to reproduce or modify the benchmark.
  - Quick check question: How does the benchmark handle prison term extraction differently from named-entity recognition?

- Concept: Legal document structure and terminology (Chinese civil law system)
  - Why needed here: The benchmark is specific to Chinese civil law; understanding the legal context is crucial for task design and result interpretation.
  - Quick check question: What distinguishes the Chinese civil law system from common law systems in terms of judicial decision-making?

## Architecture Onboarding

- Component map: Data collection → Task formulation → Prompt engineering → LLM generation → Answer extraction → Metric computation → Result aggregation
- Critical path: Prompt → LLM generation → Answer extraction → Metric computation → Result storage
- Design tradeoffs:
  - Using task-specific extraction rules vs. relying on model's ability to format answers correctly
  - Choosing between multiple-choice and generation tasks based on answer verifiability
  - Balancing task difficulty to assess different cognitive levels without making the benchmark too narrow
- Failure signatures:
  - High abstention rates indicating instruction-following issues
  - Low scores despite high model capability, suggesting extraction rule failures
  - Inconsistent performance across similar tasks, indicating potential data contamination
- First 3 experiments:
  1. Reproduce extraction rules on sample predictions to verify they correctly parse expected answers
  2. Test model performance on a subset of tasks with and without one-shot examples to measure instruction-following ability
  3. Compare scores when using different answer extraction methods (e.g., exact match vs. soft F1) to assess sensitivity

## Open Questions the Paper Calls Out

- Question: How does the abstention rate of legal-specific LLMs compare to general-purpose models when handling complex legal tasks?
  - Basis in paper: [explicit] The paper shows that GPT-4 and ChatGPT have low abstention rates across tasks, while some legal-specific LLMs show higher abstention rates, particularly in one-shot settings.
  - Why unresolved: The analysis focuses on abstention rates for individual tasks but doesn't provide a comprehensive comparison across all legal-specific models or investigate the root causes of higher abstention in these models.
  - What evidence would resolve it: A systematic comparison of abstention rates across all 51 models on the full benchmark, with analysis of task types where legal-specific models most frequently abstain.

- Question: What is the relationship between model size and performance on legal reasoning tasks in zero-shot versus one-shot settings?
  - Basis in paper: [explicit] Figure 4 shows that scaling up model size generally improves performance in one-shot cases but results are mixed for zero-shot scenarios, with ChatLaw being an outlier.
  - Why unresolved: The paper doesn't explore why some larger models underperform or what specific aspects of legal reasoning benefit most from increased scale.
  - What evidence would resolve it: Controlled experiments varying model size while keeping other factors constant, with detailed analysis of performance on different cognitive levels (memorization, understanding, application).

- Question: Why does including legal article content in prompts often decrease performance on prison term prediction tasks?
  - Basis in paper: [explicit] Figure 5 shows that most models, including GPT-4, experience decreased performance on tasks 3-4 and 3-5 when article content is provided, contrary to expectations.
  - Why unresolved: The paper hypothesizes that simple retrieval enhancement methods may not work but doesn't investigate the underlying reasons for this phenomenon.
  - What evidence would resolve it: Analysis of model attention patterns and internal representations when processing legal articles, comparison with human performance on similar tasks with and without reference materials.

## Limitations

- Data contamination risk from widely available legal texts in web-scale training data may inflate model performance scores
- Chinese legal system specificity limits generalizability of findings to other jurisdictions
- Answer extraction rules may fail to capture valid but differently phrased responses, potentially penalizing models unfairly

## Confidence

- High Confidence: Claims about GPT-4 outperforming other models and the general observation that legal-specific fine-tuning provides incremental improvements are well-supported by the empirical results presented.
- Medium Confidence: The assertion that there is "a long way from obtaining usable and reliable LLMs in legal tasks" is supported by the performance data but depends on subjective thresholds for what constitutes "usable" in legal contexts.
- Low Confidence: Claims about the hierarchical relationship between cognitive levels (memorization → understanding → application) are theoretically grounded in Bloom's taxonomy but not empirically validated within this study.

## Next Checks

1. **Data Contamination Audit**: Perform a systematic search of the training corpora of top-performing models to identify potential overlap with LawBench's legal cases and knowledge points. Quantify the fraction of benchmark examples that appear in publicly available training data sources.

2. **Cross-Jurisdictional Validation**: Adapt a subset of LawBench tasks to equivalent legal concepts in common law systems and evaluate whether the same model performance patterns hold. This would test whether the benchmark captures universal legal reasoning skills or system-specific knowledge.

3. **Extraction Rule Robustness Test**: Conduct a human evaluation study where legal experts assess model outputs against extracted answers to measure false negatives/positives introduced by the parsing rules. Calculate the correlation between extraction-based scores and expert judgments.