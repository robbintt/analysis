---
ver: rpa2
title: 'When "A Helpful Assistant" Is Not Really Helpful: Personas in System Prompts
  Do Not Improve Performances of Large Language Models'
arxiv_id: '2311.10054'
source_url: https://arxiv.org/abs/2311.10054
tags:
- roles
- role
- social
- prompt
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how incorporating social roles in system
  prompts affects the performance of large language models (LLMs) on objective tasks.
  The authors curate a dataset of 162 social roles spanning interpersonal relationships
  and occupations, and evaluate three popular LLMs on 2,457 factual questions from
  the MMLU dataset.
---

# When "A Helpful Assistant" Is Not Really Helpful: Personas in System Prompts Do Not Improve Performances of Large Language Models

## Quick Facts
- arXiv ID: 2311.10054
- Source URL: https://arxiv.org/abs/2311.10054
- Reference count: 11
- Adding social roles to system prompts consistently improves LLM performance on factual tasks, but predicting optimal roles remains challenging

## Executive Summary
This paper investigates how incorporating social roles in system prompts affects the performance of large language models (LLMs) on objective tasks. The authors curate a dataset of 162 social roles spanning interpersonal relationships and occupations, and evaluate three popular LLMs on 2,457 factual questions from the MMLU dataset. They find that adding social roles in prompts consistently improves model performance compared to a control setting with no role specified. Interpersonal roles like "friend" and gender-neutral roles tend to lead to higher performance than occupational roles. However, predicting which specific role will lead to the best performance for a given question remains challenging.

## Method Summary
The study uses three LLMs (FLAN-T5-XXL, LLaMA2-7b-chat, and OPT-instruct) to generate responses to 2,457 factual questions from the MMLU dataset. For each question, responses are generated using 162 different social roles applied through three prompt templates: Role Prompt, Audience Prompt, and Interpersonal Prompt. The performance is evaluated by comparing accuracy across different role categories and prompt templates, with additional analysis of potential mechanisms through correlation with word frequency, prompt-question similarity, and prompt perplexity.

## Key Results
- Adding social roles in prompts consistently improves performance compared to control (no role specified)
- Interpersonal roles like "friend" and gender-neutral roles lead to higher performance than occupational roles
- Audience prompts (talking to a {role}) yield higher accuracy than Role prompts (being a {role}) or Interpersonal prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Specifying an audience role improves performance more than specifying the model's own role.
- Mechanism: Audience prompts frame the model as an expert responding to a specific stakeholder, which may increase task focus and reduce irrelevant output.
- Core assumption: The model interprets "talking to a {role}" as a directive to tailor answers for that role's presumed knowledge level.
- Evidence anchors:
  - Figure 6 shows "Audience Prompt" yields higher accuracy than "Role Prompt" and "Interpersonal Prompt."
  - Interpersonal prompts like "You are talking to your mom" perform worst, possibly due to ambiguity in relational context.
- Break condition: If the model overfits to the audience role and generates overly simplified or domain-inappropriate responses.

### Mechanism 2
- Claim: Gender-neutral roles produce better performance than gendered roles.
- Mechanism: Neutral terms avoid implicit bias toward stereotypical knowledge associated with male or female roles.
- Core assumption: The model's training corpus contains gendered associations that influence reasoning quality.
- Evidence anchors:
  - Figure 5 shows gender-neutral roles outperform both male and female roles in accuracy.
  - The effect persists across domains, suggesting a consistent bias pattern.
- Break condition: If the gender distinction is context-dependent and not universally beneficial.

### Mechanism 3
- Claim: High similarity between prompt and question increases accuracy.
- Mechanism: Semantic alignment reduces ambiguity in task interpretation, leading to more relevant outputs.
- Core assumption: The model's attention mechanisms benefit from contextual coherence between instruction and query.
- Evidence anchors:
  - Figure 9b shows a moderate positive correlation between prompt-question similarity and accuracy for FLAN-T5 and LLaMA2.
  - Related work confirms persona assignment affects behavior, but not necessarily accuracy.
- Break condition: If the model's performance is dominated by domain expertise rather than prompt alignment.

## Foundational Learning

- Concept: Role-based prompting
  - Why needed here: Understanding how social roles shape model output is central to the study.
  - Quick check question: What is the difference between a role prompt and an audience prompt?
- Concept: Prompt engineering strategies
  - Why needed here: Different prompt templates lead to different performance outcomes.
  - Quick check question: How does adding "Imagine" to a prompt affect model performance?
- Concept: Corpus-based similarity metrics
  - Why needed here: Measuring semantic similarity helps explain performance differences.
  - Quick check question: What method is used to compute prompt-question similarity?

## Architecture Onboarding

- Component map:
  - Data pipeline: MMLU question sampling → role assignment → prompt generation
  - Model interface: FLAN-T5, LLaMA2, OPT with role-based prompts
  - Evaluation: Accuracy scoring and statistical analysis
- Critical path:
  - Generate all role-prompt combinations → run inference → aggregate accuracy → analyze correlations
- Design tradeoffs:
  - Larger models yield higher accuracy but increase compute cost
  - More roles improve coverage but risk redundancy
  - Similarity-based selection is effective but computationally expensive
- Failure signatures:
  - Low accuracy across all roles → model capacity or prompt mismatch
  - High variance in role performance → overfitting to specific prompts
  - Inconsistent gender effects → bias artifacts in training data
- First 3 experiments:
  1. Run baseline control prompt vs. audience prompt for a subset of questions.
  2. Compare male, female, and gender-neutral roles on the same question set.
  3. Test similarity-based role selection vs. random selection for accuracy improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do different large language models (LLMs) exhibit varying performance when prompted with social roles, and if so, what factors contribute to these differences?
- Basis in paper: The authors observe that different models (FLAN-T5, LLaMA2, OPT) perform differently across datasets when prompted with social roles, suggesting differences in model capacities and task complexities.
- Why unresolved: The paper does not delve into the specific factors that cause these performance differences between models. It is unclear whether the differences are due to model architecture, training data, or other factors.
- What evidence would resolve it: Further experiments comparing the performance of different LLMs on a variety of tasks and datasets, along with detailed analysis of model architectures and training data, could provide insights into the factors contributing to performance differences.

### Open Question 2
- Question: How does the effect of social roles on LLM performance vary across different domains and question types?
- Basis in paper: The authors find that interpersonal roles tend to lead to higher performance compared to occupational roles, but the effect depends on the specific question and dataset. They also observe that in-domain roles do not significantly outperform out-domain roles.
- Why unresolved: The paper does not provide a comprehensive analysis of how the effect of social roles varies across different domains and question types. It is unclear whether certain domains or question types are more sensitive to the effect of social roles.
- What evidence would resolve it: Conducting a detailed analysis of LLM performance when prompted with social roles across a wide range of domains and question types, and comparing the results to identify patterns and trends, could provide insights into the variation of the effect across different contexts.

### Open Question 3
- Question: Can the performance of LLMs be further improved by optimizing the selection of social roles based on specific tasks or datasets?
- Basis in paper: The authors experiment with automatic role-searching strategies but find that predicting the best role remains challenging. They suggest that optimal roles can meaningfully improve performance, but more work is needed to automatically identify these.
- Why unresolved: The paper does not explore advanced techniques for optimizing the selection of social roles based on specific tasks or datasets. It is unclear whether more sophisticated methods, such as reinforcement learning or meta-learning, could be used to improve the selection process.
- What evidence would resolve it: Developing and testing advanced techniques for optimizing the selection of social roles based on specific tasks or datasets, and comparing the results to baseline methods, could provide insights into the potential for further performance improvements.

## Limitations

- The investigation focused on factual question-answering from MMLU, which may not generalize to other task types or domains
- The analysis of mechanisms behind role effects remains correlational rather than causal, with weak explanatory power from word frequency, similarity, and perplexity metrics
- The study did not explore interaction effects between different role types or investigate temporal dynamics in model responses

## Confidence

- **High Confidence**: The finding that adding social roles improves performance over control prompts is robust across multiple models and role types
- **Medium Confidence**: The relative performance differences between interpersonal, gender-neutral, and occupational roles, while consistent, show substantial variation that limits predictive utility
- **Low Confidence**: The proposed mechanisms (similarity, frequency, perplexity) have weak correlations with performance improvements and require further validation

## Next Checks

1. **Mechanistic Validation**: Conduct controlled experiments isolating individual prompt components (role vs. framing vs. audience specification) to establish causal relationships between prompt structure and performance gains.

2. **Generalization Testing**: Apply the same experimental framework to open-ended generation tasks and subjective domains to determine whether role effects extend beyond factual question-answering.

3. **Bias Investigation**: Systematically analyze whether specific social roles introduce systematic biases in model outputs, particularly examining whether gender-neutral roles truly mitigate bias or simply redistribute it differently.