---
ver: rpa2
title: Modeling Uncertainty and Using Post-fusion as Fallback Improves Retrieval Augmented
  Generation with LLMs
arxiv_id: '2308.12574'
source_url: https://arxiv.org/abs/2308.12574
tags:
- answer
- arxiv
- passages
- question
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores methods for effectively integrating retrieved
  passages with large language models (LLMs) for open-domain question answering. It
  identifies two key challenges: "unknown" responses when feeding LLMs with concatenated
  passages, and erroneous majority voting when using a Post-Fusion approach.'
---

# Modeling Uncertainty and Using Post-fusion as Fallback Improves Retrieval Augmented Generation with LLMs

## Quick Facts
- arXiv ID: 2308.12574
- Source URL: https://arxiv.org/abs/2308.12574
- Reference count: 9
- Primary result: Concat + PF approach (concatenation with post-fusion fallback) achieves best performance on open-domain QA

## Executive Summary
This paper addresses challenges in retrieval-augmented generation (RAG) with large language models (LLMs) for open-domain question answering. The authors identify two key problems: "unknown" responses when concatenating retrieved passages and erroneous majority voting in post-fusion approaches. They propose four improved strategies including two single-round methods (Pruning Prompt, Summary Prompt) and two multi-round methods (Concat + PF, PF + Concat). Through comprehensive experiments on three datasets, they demonstrate that their proposed strategies significantly outperform simple concatenation and post-fusion methods, with Concat + PF achieving the best overall performance.

## Method Summary
The authors evaluate four strategies for integrating retrieved passages with LLMs: simple concatenation, post-fusion, pruning prompt (chain-of-thought reasoning to eliminate irrelevant passages), and summary prompt (generating passage summaries). They also propose two multi-round approaches: Concat + PF (concatenation with post-fusion fallback for "unknown" responses) and PF + Concat (post-fusion with concatenation as distiller). The methods are evaluated on Natural Questions, TriviaQA, and SQuAD-Open using gpt-3.5-turbo-16k with top-5 retrieved passages from a Wikipedia corpus.

## Key Results
- Concat + PF approach achieves highest exact match scores: 42.9% on NQ, 55.9% on TriviaQA, and 60.6% on SQuAD
- Simple concatenation often produces "unknown" responses even when correct passages are retrieved
- Post-fusion approaches suffer from erroneous majority voting
- Pruning Prompt and Summary Prompt strategies effectively improve passage selection and answer generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM uncertainty estimation improves answer quality when integrated with RAG
- Mechanism: By modeling uncertainty in retrieved passages, the LLM can better distinguish between relevant and irrelevant information, reducing "unknown" responses
- Core assumption: LLMs can effectively leverage uncertainty signals to improve decision-making in RAG scenarios
- Evidence anchors: Abstract states "Modeling Uncertainty... Improves Retrieval Augmented Generation," section 1 notes "unknown" outputs even with correct documents retrieved
- Break condition: If uncertainty estimation is too conservative, it may increase "unknown" responses

### Mechanism 2
- Claim: Post-fusion as fallback improves performance when concatenation fails
- Mechanism: Using post-fusion as fallback for "unknown" responses allows majority voting to select correct answers from candidate pools
- Core assumption: Correct answer is more likely in majority when multiple passages considered
- Evidence anchors: Abstract identifies Concat + PF as best approach, section 3.2 describes fallback mechanism
- Break condition: If majority voting consistently selects incorrect answers, fallback becomes ineffective

### Mechanism 3
- Claim: Chain-of-thought reasoning improves passage identification
- Mechanism: Prompting step-by-step reasoning helps LLM eliminate irrelevant passages and focus on answer-containing ones
- Core assumption: LLMs perform better reasoning when explicitly prompted to do so
- Evidence anchors: Abstract mentions pruning and summarization strategies, section 3.1 describes elimination process
- Break condition: If chain-of-thought increases token usage without quality improvements, may not be cost-effective

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: RAG is the fundamental approach used to integrate retrieved passages with LLMs
  - Quick check question: What are the two main stages of a typical RAG pipeline?

- Concept: Uncertainty estimation in LLMs
  - Why needed here: Understanding how LLMs estimate uncertainty is crucial for proposed mechanisms
  - Quick check question: How might an LLM's uncertainty estimation be used to improve performance in RAG settings?

- Concept: Chain-of-thought reasoning
  - Why needed here: Paper proposes chain-of-thought strategies to improve passage processing
  - Quick check question: What is the primary benefit of using chain-of-thought reasoning in retrieval-augmented question answering?

## Architecture Onboarding

- Component map: Retriever -> LLM (with uncertainty estimation) -> Answer
- Critical path: Question → Retriever → LLM (with uncertainty estimation) → Answer
- Design tradeoffs:
  - Token usage vs. answer quality: Complex strategies may improve accuracy but increase token consumption
  - Speed vs. thoroughness: Multi-round approaches may be more accurate but slower
- Failure signatures:
  - High percentage of "unknown" responses despite relevant passages being retrieved
  - Majority voting in post-fusion consistently selecting incorrect answers
  - Chain-of-thought reasoning leading to irrelevant passage selection
- First 3 experiments:
  1. Compare concatenation vs. post-fusion strategies on small dataset to identify initial performance differences
  2. Implement and test Concat + PF approach to evaluate fallback mechanism effectiveness
  3. Assess impact of chain-of-thought reasoning on answer quality and token usage across different datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different LLM architectures perform when integrating retrieved passages for open-domain QA?
- Basis in paper: Authors acknowledge testing only on ChatGPT (GPT-3.5-turbo-16k) and plan to extend evaluation to other LLMs
- Why unresolved: Results may be specific to GPT-3.5 architecture, performance could vary with different LLM designs
- What evidence would resolve it: Conducting same experiments with multiple LLM architectures (GPT-4, Falcon, LLaMA) and comparing performance

### Open Question 2
- Question: How does effectiveness change with varying retrieval quality?
- Basis in paper: Authors consider two settings - top-k retrieved passages and gold passage included
- Why unresolved: Current results focus on scenarios with gold passage included, but real-world applications have varying retrieval quality
- What evidence would resolve it: Evaluating proposed methods across range of retrieval quality scenarios, including cases where gold passage is ranked lower or missing

### Open Question 3
- Question: What is the impact of increasing retrieved passages beyond top-5 on performance and efficiency tradeoff?
- Basis in paper: Authors analyze token usage for top-5 passages but don't explore beyond top-5
- Why unresolved: Paper provides token usage analysis for top-5 but doesn't investigate how performance scales with more passages
- What evidence would resolve it: Systematically testing proposed methods with increasing numbers of retrieved passages and analyzing both performance improvements and token usage costs

## Limitations
- Claims about uncertainty modeling remain largely theoretical with limited empirical evidence
- Multi-round strategies lack detailed implementation specifications
- Evaluation focuses on EM and F1 without analyzing cost-benefit tradeoff of increased token usage
- Only tested on gpt-3.5-turbo-16k, limiting generalizability across different LLM architectures

## Confidence
- High confidence: Core observation that simple concatenation produces "unknown" responses is well-supported by experimental data
- Medium confidence: Effectiveness of Concat + PF as best-performing strategy is demonstrated, though implementation details remain unclear
- Medium confidence: Proposed chain-of-thought strategies show improvements, but lack analysis of token efficiency compared to simpler approaches

## Next Checks
1. Conduct ablation studies to quantify specific contribution of uncertainty estimation versus other factors to performance improvements
2. Measure and compare token usage across all strategies to establish cost-effectiveness of complex approaches
3. Test proposed strategies with different LLM architectures (e.g., Claude, Llama) to assess generalizability beyond gpt-3.5-turbo-16k