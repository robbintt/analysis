---
ver: rpa2
title: Post-processing Private Synthetic Data for Improving Utility on Selected Measures
arxiv_id: '2305.15538'
source_url: https://arxiv.org/abs/2305.15538
tags:
- data
- synthetic
- utility
- real
- privacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a post-processing method to improve the utility
  of private synthetic data on user-selected measures while preserving differential
  privacy guarantees. The approach involves resampling from the synthetic data using
  optimal weights calculated via a stochastic first-order algorithm, which finds a
  distribution that aligns the selected utility measures with noisy real data.
---

# Post-processing Private Synthetic Data for Improving Utility on Selected Measures

## Quick Facts
- arXiv ID: 2305.15538
- Source URL: https://arxiv.org/abs/2305.15538
- Reference count: 25
- Primary result: Introduces a post-processing method that improves utility of private synthetic data on selected measures while preserving differential privacy guarantees.

## Executive Summary
This paper presents a novel post-processing technique for private synthetic data that improves utility on user-selected measures without compromising differential privacy guarantees. The method uses optimal resampling weights computed via a stochastic first-order algorithm to filter synthetic samples, aligning selected utility measures with noisy real data. Experiments demonstrate consistent utility improvements across multiple benchmark datasets and state-of-the-art DP mechanisms, with particular effectiveness in aligning correlation matrices to aid feature selection and model accuracy.

## Method Summary
The approach involves computing utility measures from real data with differential privacy guarantees, denoising the answers, and then solving a dual optimization problem to find optimal resampling weights. These weights are used to resample from the synthetic data, creating post-processed data that better aligns with the selected utility measures. The method is model-agnostic, scalable, and preserves privacy through composition rules. The stochastic first-order algorithm efficiently solves the dual problem using mini-batch estimates of partial derivatives.

## Key Results
- Consistently improves utility on selected measures across multiple benchmark datasets
- Effectively aligns correlation matrices between synthetic and real data
- Maintains differential privacy guarantees through composition with original DP mechanism
- Improves downstream model performance without degrading accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Resampling synthetic data using exponential tilting weights improves utility on selected measures without expanding support set
- Core assumption: Utility measures can be expressed as queries with optimal distribution having closed-form exponential tilting expression
- Evidence: Theorem 1 provides closed-form solution Ppost*(x) ∝ Psyn(x) exp(−∑Kk=1 λ*k(qk(x) − ak))
- Break condition: Infeasibility of optimization problem or failure of exponential tilting assumption

### Mechanism 2
- Post-processing preserves differential privacy guarantees through composition with original DP mechanism
- Core assumption: Utility measure computation from real data is a DP mechanism, and post-processing preserves DP
- Evidence: Proposition 1 states post-processed data satisfy (ϵ + ϵpost, δ + δpost)-DP
- Break condition: Violation of DP in utility measure computation or support expansion in post-processing

### Mechanism 3
- Stochastic first-order algorithm efficiently solves dual optimization problem for scalability
- Core assumption: Partial derivative of dual objective has closed-form expression estimable using mini-batches
- Evidence: Algorithm estimates partial derivative using mini-batch ratios of expectations
- Break condition: Excessive gradient estimation noise or failure of convergence guarantees

## Foundational Learning

- Concept: Differential Privacy (DP)
  - Why needed here: Framework relies on maintaining DP guarantees while improving utility
  - Quick check question: What are the two key properties of DP that make this post-processing approach valid?

- Concept: Information Projection and KL-divergence
  - Why needed here: Optimization problem formulated as minimizing KL-divergence between distributions
  - Quick check question: Why is minimizing KL-divergence between synthetic and post-processed distributions a reasonable objective?

- Concept: Stochastic Compositional Optimization
  - Why needed here: Dual optimization involves logarithmic function composed with expectation
  - Quick check question: What makes the dual objective function challenging to optimize using standard SGD?

## Architecture Onboarding

- Component map: Real data → Utility measures (noisy) → Denoising → Dual optimization → Resampling weights → Post-processed synthetic data

- Critical path:
  1. Compute noisy utility measures from real data
  2. Denoise answers to ensure feasibility
  3. Solve dual optimization problem using stochastic first-order method
  4. Resample synthetic data using optimal weights
  5. Validate utility improvement on selected measures

- Design tradeoffs:
  - Privacy budget allocation between synthetic generation and utility measure computation
  - Choice of γ parameter balancing utility improvement vs. distortion
  - Batch size vs. convergence speed in stochastic algorithm
  - Number of utility measures vs. computational complexity

- Failure signatures:
  - Poor utility improvement from misaligned or poorly chosen utility measures
  - Degraded downstream performance from excessive distribution alteration
  - Convergence issues from incorrect stochastic algorithm implementation
  - Suboptimal weights from mismatched feature scaling

- First 3 experiments:
  1. Verify DP preservation: Check that post-processed data maintains (ϵ + ϵpost, δ + δpost)-DP guarantees
  2. Test scalability: Measure runtime on datasets of increasing size and dimensionality
  3. Validate utility improvement: Compare selected utility measures before and after post-processing on benchmark datasets

## Open Questions the Paper Calls Out

### Open Question 1
- How does performance vary when applied to different synthetic data generation mechanisms beyond those tested?
- Basis: Method is model-agnostic but only tested on GAN-based, marginal-based, and workload-based methods
- Why unresolved: Experiments only cover limited set of state-of-the-art mechanisms
- What evidence would resolve it: Experiments on synthetic data from Bayesian networks or decision trees with performance comparisons

### Open Question 2
- What is the impact on privacy budget when utility measures use more complex or higher-dimensional queries?
- Basis: Paper mentions additional privacy budget may be needed for utility measure computation
- Why unresolved: Does not explore how budget scales with query complexity or dimensionality
- What evidence would resolve it: Analysis of privacy budget consumption for various query complexities and dimensions

### Open Question 3
- How does choice of violation tolerance parameter γ affect balance between utility and distribution preservation?
- Basis: Paper mentions users can select various γ values to navigate trade-off
- Why unresolved: Does not provide systematic study on γ impact on utility and distribution fidelity
- What evidence would resolve it: Sensitivity analysis showing effects of varying γ on utility measures and distributional properties

## Limitations

- Effectiveness depends heavily on quality of initial synthetic data generated by DP mechanisms
- Choice of utility measures requires domain expertise and poor selection may lead to suboptimal results
- Exponential tilting mechanism assumes optimization problem feasibility which may not hold for all measure combinations
- Additional privacy budget required for computing utility measures from real data

## Confidence

- **High Confidence**: Privacy preservation guarantees (Proposition 1) - follows from composition properties of DP
- **Medium Confidence**: Utility improvement on selected measures - supported by experiments but depends on measure selection
- **Medium Confidence**: Scalability claims - stochastic approach should scale but performance may vary with characteristics

## Next Checks

1. **Robustness to Measure Selection**: Systematically test how choice and number of utility measures affect both utility improvement and downstream model performance across different datasets.

2. **Privacy Budget Allocation Tradeoffs**: Experiment with varying allocation of privacy budget between synthetic data generation and utility measure computation to identify optimal tradeoffs.

3. **Convergence Verification**: Implement convergence diagnostics for stochastic algorithm to verify consistent finding of near-optimal weights across different problem instances and noise levels.