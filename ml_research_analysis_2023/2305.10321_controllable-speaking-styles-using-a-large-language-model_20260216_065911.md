---
ver: rpa2
title: Controllable Speaking Styles Using a Large Language Model
arxiv_id: '2305.10321'
source_url: https://arxiv.org/abs/2305.10321
tags:
- prosody
- style
- target
- speech
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of controlling prosody in text-to-speech
  (TTS) synthesis to achieve appropriate speaking styles and emotions, which is critical
  for natural and effective spoken communication. The core method idea is to use a
  large language model (InstructGPT) to suggest context-appropriate modifications
  to pitch, energy, and duration for a controllable TTS model, without requiring a
  reference utterance or prompt-labelled speech corpus.
---

# Controllable Speaking Styles Using a Large Language Model

## Quick Facts
- arXiv ID: 2305.10321
- Source URL: https://arxiv.org/abs/2305.10321
- Reference count: 0
- Primary result: LLM-suggested prosody modifications rated most appropriate in 50% of cases vs 31% for baseline

## Executive Summary
This paper presents a novel approach for controlling prosody in text-to-speech synthesis without requiring reference utterances or prompt-labeled speech corpora. The method uses a large language model (InstructGPT) to suggest context-appropriate modifications to pitch, energy, and duration for a controllable TTS model. By prompting the LLM with target text and optional contextual information, the system generates expressive and natural-sounding speech that adapts to different speaking styles and dialogue contexts.

## Method Summary
The approach modifies a FastSpeech-2 TTS model by replacing variance adapters with low-level prosody predictors and Gaussian upsampling modules. Instead of using reference utterances or prompt-conditioned encoders, the system prompts InstructGPT to generate relative modifications to F0, energy, and duration parameters based on the target text and contextual information. The LLM predictions are applied at both global and local levels to modify the baseline acoustic features predicted by the TTS model, with the final speech generated using HiFi-GAN vocoder.

## Key Results
- LLM-generated prosody modifications were rated as most appropriate in 50% of A/B/C preference tests compared to 31% for the baseline model
- The method successfully generated context-appropriate prosody for both reading-style and dialogue-style speech without requiring reference utterances
- Listener preference ratings showed significant improvement over the baseline TTS model across multiple speaking style conditions

## Why This Works (Mechanism)

### Mechanism 1
LLM can predict appropriate prosody modifications without reference utterances or labeled speech data by using contextual information from target text and optional style/dialogue context to generate relative modifications to F0, energy, and duration parameters.

### Mechanism 2
Relative parameter modifications are more effective than absolute value predictions because predicting relative changes to baseline prosody is a simpler task for LLMs than predicting absolute values, working with the model's initial predictions.

### Mechanism 3
Chain-of-thought and few-shot prompting improve LLM consistency by including validation examples and intermediate reasoning steps in the prompt, helping the LLM generate more consistent and appropriate modifications.

## Foundational Learning

- **Contextual Word Embeddings (CWEs)**: Encode semantic and syntactic word relations within context, essential for understanding text that informs prosody prediction. *Quick check: How do CWEs differ from traditional word embeddings in capturing context-dependent meaning?*

- **Transformer architecture**: The FastSpeech-2 model uses transformer blocks, crucial for understanding how modifications flow through the model. *Quick check: What are the key components of a transformer block and how do they process sequential data?*

- **Prosodic features and their acoustic correlates**: Understanding F0, energy, and duration as fundamental prosodic features is essential for interpreting how LLM suggestions modify speech synthesis. *Quick check: How do pitch, energy, and duration relate to perceived speaking style and emotion in speech?*

## Architecture Onboarding

- **Component map**: LLM (InstructGPT) -> FastSpeech-2 TTS model -> Low-level prosody predictor -> Gaussian upsampling module -> HiFi-GAN vocoder

- **Critical path**: Text → LLM prompt → LLM output → Parameter modification → Low-level prosody predictor → Gaussian upsampling → Mel-spectrogram decoder → HiFi-GAN → Audio

- **Design tradeoffs**: Using relative modifications vs. absolute values, balancing global vs. local prosodic control, tradeoff between prompt complexity and LLM consistency, choosing between reference-based and LLM-based control

- **Failure signatures**: Inconsistent LLM parameter outputs, audio artifacts from parameter modifications, inappropriate prosody for given context, speaker leakage when using reference-based comparisons

- **First 3 experiments**: 1) Test LLM with minimal prompt to establish baseline performance, 2) Evaluate effect of adding chain-of-thought examples to prompt, 3) Compare global vs. local modification effectiveness on simple text samples

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important questions remain unresolved regarding the method's performance on languages other than English, its handling of highly expressive speaking styles not represented in training data, and its performance on longer dialogues with more complex contextual information.

## Limitations

- The method's effectiveness depends heavily on the quality of instruction prompts and the LLM's consistency, which isn't quantified across multiple generations
- Evaluation only compares against a single baseline model rather than a comprehensive set of state-of-the-art approaches
- The approach is evaluated only on English data using an English-trained LLM, limiting conclusions about generalizability to other languages

## Confidence

- **High Confidence**: The fundamental approach of using LLM-suggested relative modifications to prosody parameters is technically sound and well-grounded
- **Medium Confidence**: The effectiveness of chain-of-thought and few-shot prompting in improving LLM consistency is supported by qualitative observations but lacks systematic ablation studies
- **Low Confidence**: The generalizability of the method to diverse speaking styles and languages beyond English remains uncertain

## Next Checks

1. **Cross-LLM Validation**: Test the same prompting approach with different LLM architectures (e.g., Claude, LLaMA) to verify that the method isn't specific to InstructGPT

2. **Prompt Sensitivity Analysis**: Systematically vary prompt complexity, example selection, and formatting to quantify how sensitive the LLM's output consistency is to prompt design choices

3. **Long-Form Content Evaluation**: Evaluate the method on longer passages (paragraph-level or multi-turn dialogue) rather than individual sentences to test whether the LLM can maintain consistent speaking style across extended contexts