---
ver: rpa2
title: Can I Trust Your Answer? Visually Grounded Video Question Answering
arxiv_id: '2309.01327'
source_url: https://arxiv.org/abs/2309.01327
tags:
- video
- grounding
- visual
- temporal
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines the grounding capabilities of vision-language
  models (VLMs) in video question answering (VideoQA). It finds that while VLMs achieve
  high QA accuracy, their predictions are often not visually grounded in relevant
  video content, relying instead on language bias or spurious visual correlations.
---

# Can I Trust Your Answer? Visually Grounded Video Question Answering

## Quick Facts
- arXiv ID: 2309.01327
- Source URL: https://arxiv.org/abs/2309.01327
- Reference count: 40
- Key outcome: VLMs achieve high QA accuracy but predictions are often not visually grounded, relying on language bias or spurious visual correlations

## Executive Summary
This paper investigates the visual grounding capabilities of vision-language models (VLMs) in video question answering (VideoQA). Despite achieving strong QA performance, the authors find that these models frequently fail to ground their answers in relevant video content, instead relying on language shortcuts and spurious visual correlations. To address this limitation, the paper introduces NExT-GQA, a dataset extension of NExT-QA with 10.5K temporal grounding labels, and proposes a Gaussian mask optimization method with cross-modal learning to improve grounding quality. The approach demonstrates that enhancing grounding capabilities not only provides more reliable visual evidence but also improves QA performance, outperforming post-hoc grounding methods.

## Method Summary
The method employs a weakly-supervised temporal grounding approach using Gaussian mask optimization. A Gaussian distribution parameterized by mean (μ) and standard deviation (σ) is applied across video frame indices, learned end-to-end by optimizing for correct QA predictions and enforcing cross-modal alignment between questions and localized video segments. Cross-modal self-supervision further improves grounding by pulling the representation of the localized video segment closer to the question embedding while pushing it away from other questions through contrastive learning. The approach is evaluated on the NExT-GQA dataset using QA accuracy, Acc@GQA (accuracy with visual grounding IoP≥0.5), mIoP, and mIoU metrics.

## Key Results
- VLMs achieve high QA accuracy but predictions are often not visually grounded in relevant video content
- Gaussian mask optimization with cross-modal learning improves both grounding and QA performance
- The proposed approach outperforms post-hoc grounding methods on NExT-GQA dataset

## Why This Works (Mechanism)

### Mechanism 1
Weakly-supervised temporal grounding via Gaussian mask optimization aligns visual attention with answer-relevant moments without requiring ground truth segment labels. A Gaussian distribution parameterized by mean (μ) and standard deviation (σ) is applied across video frame indices. The model learns these parameters end-to-end by optimizing for correct QA predictions and enforcing cross-modal alignment between questions and localized video segments.

### Mechanism 2
Cross-modal self-supervision improves grounding by enforcing semantic alignment between questions and localized video content. For each question, the model pulls the representation of the localized video segment (using predicted Gaussian mask) closer to the question embedding while pushing it away from other questions. This creates a contrastive objective that regularizes the model toward visually grounded reasoning.

### Mechanism 3
Post-hoc attention analysis reveals that strong QA performance does not imply strong visual grounding, exposing spurious correlation bias. By examining temporal attention patterns after QA training, the paper shows that models achieve high accuracy by relying on language shortcuts and irrelevant visual context rather than truly grounding answers in relevant video segments.

## Foundational Learning

- Concept: Temporal localization in videos
  - Why needed here: The task requires identifying start and end timestamps of video segments relevant to answering questions, which demands understanding of temporal relationships in video data.
  - Quick check question: Can you describe how you would determine the temporal span of a specific action in a video without manual annotations?

- Concept: Cross-modal representation alignment
  - Why needed here: The method needs to align visual features (localized video segments) with textual features (questions/answers) in a shared embedding space for contrastive learning.
  - Quick check question: How would you measure similarity between a video segment representation and a question embedding?

- Concept: Gaussian distribution parameterization
  - Why needed here: The grounding mechanism uses a Gaussian distribution to represent the probability of relevant content at each time step, requiring understanding of how to parameterize and optimize such distributions.
  - Quick check question: What parameters define a 1D Gaussian distribution and how would you optimize them for temporal localization?

## Architecture Onboarding

- Component map: Vision encoder (CLIP/BiLM/ViT) → Temporal Transformer → Gaussian Mask Prediction Head → QA Prediction Head + Cross-modal Contrastive Loss
- Critical path: Video frames → Vision encoder → Temporal attention → Gaussian-weighted tokens → Answer prediction
- Design tradeoffs: Single Gaussian mask vs multiple masks (better grounding but higher computational cost and potential QA accuracy drop), dual transformer vs stacked transformer (grounding performance varies)
- Failure signatures: High QA accuracy but low grounding metrics (IoP/IoU), attention concentrated on irrelevant frames, poor performance on questions requiring fine-grained temporal reasoning
- First 3 experiments:
  1. Run post-hoc attention analysis on a pre-trained VQA model to measure baseline grounding quality
  2. Implement Naive Gaussian (NG) baseline with single Gaussian mask and compare to post-hoc results
  3. Add cross-modal contrastive loss (NG+) and measure improvements in both grounding and QA metrics

## Open Questions the Paper Calls Out

### Open Question 1
How does the effectiveness of the proposed Gaussian mask optimization approach vary with the number of Gaussian masks used? The paper only briefly mentions that using multiple Gaussian masks can hurt QA accuracy, but doesn't provide a detailed analysis of the impact of different numbers of masks.

### Open Question 2
To what extent does the proposed approach generalize to other VideoQA datasets beyond NExT-GQA? The experiments are conducted solely on NExT-GQA, leaving open the question of how well the approach would perform on other datasets with different characteristics.

### Open Question 3
What is the impact of the proposed approach on the interpretability of VideoQA models? While the paper mentions the importance of interpretability, it doesn't provide a quantitative or qualitative analysis of how the proposed approach affects the interpretability of the models.

### Open Question 4
How does the proposed approach compare to other methods for improving grounding in VideoQA? The paper only compares the proposed approach to post-hoc analysis and a naive Gaussian approach, but doesn't provide a comprehensive comparison to other grounding methods.

## Limitations
- The study relies on IoP/IoU metrics computed via post-hoc attention analysis, which may not perfectly capture true visual grounding quality
- The effectiveness of contrastive alignment assumes semantic similarity in embedding space corresponds to temporal proximity in video content
- Using a single Gaussian distribution to represent relevant temporal segments assumes unimodal, contiguous grounding regions

## Confidence

- **High Confidence**: Claims about weak grounding despite strong QA performance in existing VLMs are supported by multiple post-hoc analyses and align with established literature on language bias in multimodal models.
- **Medium Confidence**: The effectiveness of Gaussian mask optimization for temporal grounding is demonstrated through controlled experiments, but the approach may not generalize to questions requiring complex temporal reasoning or discontinuous grounding.
- **Low Confidence**: Claims about the superiority of dual-transformer architecture over stacked transformer for grounding performance are based on limited ablation studies and may be architecture-specific rather than generally applicable.

## Next Checks

1. Conduct human evaluation studies to compare post-hoc attention-based grounding metrics against human annotations, particularly for questions requiring fine-grained temporal reasoning or discontinuous evidence.

2. Systematically vary the temperature parameter and negative sampling strategy in the contrastive loss to determine optimal settings for grounding improvement without sacrificing QA accuracy.

3. Test the Gaussian mask approach on questions requiring multimodal temporal evidence (e.g., "what happened first" vs "what happened last") to evaluate representational capacity limits and identify scenarios where multiple masks or alternative distributions would be beneficial.