---
ver: rpa2
title: Description-Based Text Similarity
arxiv_id: '2305.12517'
source_url: https://arxiv.org/abs/2305.12517
tags:
- descriptions
- sentences
- sentence
- description
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of retrieving sentences based on
  abstract descriptions, where the goal is to find sentences that exemplify the abstract
  scenario described in a query, rather than sentences that are semantically similar
  to the query. The authors argue that existing sentence similarity methods are inadequate
  for this task, as they tend to retrieve sentences that are similar to the query
  description rather than sentences that instantiate it.
---

# Description-Based Text Similarity

## Quick Facts
- arXiv ID: 2305.12517
- Source URL: https://arxiv.org/abs/2305.12517
- Reference count: 15
- Key outcome: New model significantly outperforms sentence similarity baselines in retrieving sentences that instantiate abstract descriptions rather than semantically similar ones

## Executive Summary
This paper introduces the task of retrieving sentences based on abstract descriptions, where the goal is to find sentences that exemplify the scenario described rather than sentences semantically similar to the query. The authors propose a novel contrastive learning approach using dual encoders that significantly outperforms existing sentence similarity methods in human evaluation, with crowdworkers finding more than twice as many relevant sentences in the top-5 results. The model is trained on a GPT-3 generated dataset of 165,960 Wikipedia sentences with corresponding valid and misleading descriptions.

## Method Summary
The authors fine-tune a pre-trained MPNet-based sentence embedding model using contrastive learning with a combination of triplet loss and InfoNCE loss functions. Two distinct instances of the model are used - one as a sentence encoder and one as a description encoder - allowing for efficient similarity search through pre-indexing sentences. The training data consists of 165,960 Wikipedia sentences with 5 valid and 5 misleading descriptions each, generated by GPT-3. The model is evaluated against strong sentence similarity baselines using human evaluation on MTurk, where crowdworkers rate the relevance of retrieved sentences to query descriptions.

## Key Results
- Model retrieves sentences that instantiate abstract descriptions rather than semantically similar ones
- Human evaluation shows more than 2x as many relevant sentences in top-5 compared to best baseline
- Model successfully retrieves relevant sentences for diverse abstract queries including historical events, scientific discoveries, and cultural phenomena

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning aligns descriptions and sentences in embedding space
- Mechanism: By maximizing similarity between descriptions and their corresponding sentences while pushing apart mismatched pairs, the model learns a semantic space where descriptions and instantiated sentences are close together
- Core assumption: Descriptions and sentences that describe the same scenario share latent semantic structure that can be captured through contrastive loss
- Evidence anchors: [abstract] "propose an alternative model that significantly improves when used in standard nearest neighbor search"; [section] "we utilize a pre-trained sentence embedding model and fine-tune it with contrastive learning"
- Break condition: If the contrastive objective fails to capture the abstract-to-instance mapping, the model will retrieve sentences similar to descriptions rather than their instantiations

### Mechanism 2
- Claim: GPT-3 can generate diverse, abstract descriptions that serve as training supervision
- Mechanism: Large language models can produce multiple valid and invalid descriptions for sentences, providing rich training data without manual annotation
- Core assumption: GPT-3's ability to generate abstract descriptions captures the human concept of "describing the essence" of text
- Evidence anchors: [abstract] "The model is trained using positive and negative pairs sourced through prompting a LLM"; [section] "We use GPT-3 (text-davinci-003) to generate positive and misleading descriptions for sentences"
- Break condition: If GPT-3's descriptions are too literal or miss the abstract nature, the model won't learn the intended behavior

### Mechanism 3
- Claim: Dual encoders enable efficient similarity search at scale
- Mechanism: Separate sentence and description encoders allow pre-indexing of sentences while encoding queries on-the-fly for nearest neighbor search
- Core assumption: The semantic relationship between descriptions and sentences is symmetric enough to work in both directions
- Evidence anchors: [abstract] "These vector encodings can then be used in a standard similarity-based retrieval setting"; [section] "Given a query q, we represent it with the query encoder and perform exact nearest-neighbor search under cosine distance"
- Break condition: If the encoders drift apart during training, similarity search will fail

## Foundational Learning

- Concept: Contrastive learning objective
  - Why needed here: To learn alignment between abstract descriptions and concrete sentence instantiations
  - Quick check question: What loss function ensures that descriptions are closer to their corresponding sentences than to other sentences in embedding space?

- Concept: Dual encoder architecture
  - Why needed here: To enable scalable similarity search by pre-encoding the corpus
  - Quick check question: Why use separate encoders for descriptions and sentences instead of a single encoder?

- Concept: Negative sampling strategies
  - Why needed here: To provide informative training signals by contrasting positive pairs with negative examples
  - Quick check question: How do in-batch negatives differ from pre-generated misleading descriptions as training negatives?

## Architecture Onboarding

- Component map:
  GPT-3 data generator -> Training dataset -> MPNet-based dual encoder model with contrastive loss -> FAISS nearest neighbor search -> MTurk evaluation pipeline

- Critical path:
  1. Generate training data with GPT-3
  2. Train dual encoder with contrastive loss
  3. Index sentences with sentence encoder
  4. Encode query with description encoder
  5. Retrieve nearest neighbors

- Design tradeoffs:
  - Single vs dual encoders: Dual encoders enable indexing but may learn different spaces
  - Triplet vs InfoNCE: Both used for better performance, though increases complexity
  - GPT-3 vs human annotations: Automated but potentially noisier data

- Failure signatures:
  - Model retrieves sentences semantically similar to query description
  - Retrieval results show poor diversity across queries
  - Human evaluation shows low relevance scores

- First 3 experiments:
  1. Train with only triplet loss vs only InfoNCE loss to verify combined approach helps
  2. Test retrieval quality with description encoder vs sentence encoder on queries
  3. Compare performance on abstract vs concrete descriptions to understand model behavior

## Open Questions the Paper Calls Out
- How does the proposed model handle the trade-off between abstractness and specificity in descriptions?
- Can the model be extended to handle multi-sentence or document-level descriptions?
- How does the model perform on non-English languages or multilingual retrieval?

## Limitations
- Model can sometimes produce unfaithful retrievals that don't match the query intent
- Limited to English language; no evaluation on other languages or cross-lingual retrieval
- Relies on GPT-3 generated data which may contain biases from training data

## Confidence

| Claim | Confidence |
|-------|------------|
| Description-based task formulation | High |
| Contrastive learning effectiveness | Medium |
| GPT-3 data generation quality | Medium |
| Human evaluation methodology | Medium |

## Next Checks
1. Conduct ablation studies removing either the triplet loss or InfoNCE components to quantify their individual contributions
2. Test model performance on descriptions from different domains (e.g., technical writing, creative writing) to assess generalizability
3. Implement an automatic evaluation metric that measures whether retrieved sentences instantiate rather than merely resemble descriptions