---
ver: rpa2
title: 'DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining'
arxiv_id: '2305.12074'
source_url: https://arxiv.org/abs/2305.12074
tags:
- disco
- data
- student
- uni00000013
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DisCo, a semi-supervised learning (SSL) framework
  for fine-tuning a cohort of small student models generated from a large pre-trained
  language model (PLM) using knowledge distillation. The key idea is to share complementary
  knowledge among distilled student cohorts to promote their SSL effectiveness.
---

# DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining

## Quick Facts
- arXiv ID: 2305.12074
- Source URL: https://arxiv.org/abs/2305.12074
- Reference count: 34
- 7.6× smaller and 4.8× faster student models than baseline PLMs while maintaining comparable performance

## Executive Summary
DisCo introduces a semi-supervised learning framework that co-trains multiple small student models distilled from a large pre-trained language model. The key innovation is leveraging both model-view diversity (through different distillation strategies from teacher layers) and data-view diversity (through various input augmentations) to enable knowledge sharing among students. Experimental results demonstrate that this approach produces compact student models that are significantly faster and smaller than baseline PLMs while maintaining competitive performance on both text classification and extractive summarization tasks.

## Method Summary
DisCo creates a cohort of student models through knowledge distillation from BERT-base using two strategies: separated-layer distillation (SKD) and connected-layer distillation (CKD). Each student model is paired with a supervised copy, and the framework employs co-training with consistency constraints across model views (different distillation architectures) and data views (augmented inputs via adversarial attack, token shuffling, cutoff, and dropout). The training objective combines supervised cross-entropy loss with unsupervised consistency MSE loss, optimized using Adam with a ramp-up function for the consistency weight.

## Key Results
- Student models achieve 7.6× reduction in parameters and 4.8× faster inference compared to baseline PLMs
- DisCo outperforms state-of-the-art SSL methods like UDA, Mean Teacher, and Noisy Student on multiple text classification and summarization tasks
- Performance gains are consistent across different label scarcity scenarios (10, 30, 100 labels per class)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Co-training with model views allows students to learn complementary representations from different teacher layers
- Mechanism: By distilling from alternate vs continuous layers (SKD vs CKD), each student captures different knowledge views. The co-training framework then encourages them to teach each other through consistency constraints
- Core assumption: Different teacher layers encode complementary information that can be leveraged through peer learning
- Evidence anchors: [abstract] "Our key insight is to share complementary knowledge among distilled student cohorts"; [section 2.2.1] "Model view encoding diversifies the individual student by leveraging different knowledge of the teacher"

### Mechanism 2
- Claim: Data augmentation creates diverse views that improve model robustness through consistency training
- Mechanism: Adversarial attack, token shuffling, cutoff, and dropout create perturbed inputs. Students must maintain consistent predictions across these views, which regularizes their learning
- Core assumption: Models can learn to be invariant to semantically-meaningful input perturbations
- Evidence anchors: [abstract] "data views produced by various input augmentations"; [section 2.2.2] "Our intuition is that advanced data augmentation can introduce extra inductive biases"

### Mechanism 3
- Claim: The dual student architecture with shared parameters enables effective knowledge transfer
- Mechanism: Each student shares parameters with its supervised copy while being trained on unlabeled data. This allows the unsupervised training to benefit from supervised learning progress
- Core assumption: Parameter sharing between supervised and unsupervised copies creates an effective teacher-student relationship
- Evidence anchors: [section 2.2.3] "Each student model concurrently shares the parameters of its copied one, which is trained by supervised learning"

## Foundational Learning

- Concept: Knowledge distillation from PLMs
  - Why needed here: DisCo relies on creating smaller student models from BERT through distillation
  - Quick check question: What are the key differences between task-agnostic and task-specific distillation?

- Concept: Semi-supervised learning consistency training
  - Why needed here: The core of DisCo is using unlabeled data through consistency constraints between students
  - Quick check question: How does consistency training differ from pseudo-labeling approaches?

- Concept: Co-training framework fundamentals
  - Why needed here: DisCo extends classic co-training to student models rather than traditional classifiers
  - Quick check question: What are the key assumptions of co-training that DisCo must satisfy?

## Architecture Onboarding

- Component map: Teacher BERT (fixed, large) -> Two student models (different architectures) -> Data augmentation pipeline (4 methods) -> Supervised training module -> Unsupervised consistency training module -> Parameter sharing mechanism
- Critical path: Teacher distillation → Student initialization → Supervised fine-tuning → Co-training loop
- Design tradeoffs: Model view diversity vs training complexity; Data augmentation strength vs semantic preservation; Number of students vs computational cost
- Failure signatures: Students diverge rather than converge; Performance worse than single student; Training instability with parameter sharing
- First 3 experiments: 1) Single student baseline with AD augmentation; 2) Dual students with model views only (no data views); 3) Dual students with data views only (no model views)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would DisCo perform if the student models had different numbers of layers (e.g., one 4-layer and one 6-layer model)?
- Basis in paper: [explicit] The paper mentions that prior works have shown larger-size students can better simulate the teacher and that the student cohort can contain models with different numbers of layers.
- Why unresolved: The current implementation uses student models with the same architecture. The effect of heterogeneous student architectures on performance and knowledge sharing is unexplored.
- What evidence would resolve it: Experimental results comparing DisCo with heterogeneous student architectures (different numbers of layers) against the current homogeneous setup on various tasks.

### Open Question 2
- Question: How does the choice of data augmentation strategy (e.g., back-translation vs. adversarial attack) affect DisCo's performance compared to other SSL methods?
- Basis in paper: [explicit] The paper discusses the performance of UDA/FLiText with back-translation vs. adversarial attack and mentions that back-translation is more suitable for these methods.
- Why unresolved: While the paper shows DisCo outperforms other SSL methods with its chosen augmentation strategies, a comprehensive comparison across different augmentation strategies for all methods is missing.
- What evidence would resolve it: A study comparing the performance of DisCo and other SSL methods using various data augmentation strategies on multiple tasks.

### Open Question 3
- Question: Can DisCo be effectively extended to other model architectures like TextCNN or MLP-Mixer, or other language models like RoBERTa or GPT?
- Basis in paper: [explicit] The paper mentions that DisCo currently uses Transformer-based student models from BERT and that it would be useful to evaluate its generalization to other model architectures and language models.
- Why unresolved: The current implementation is limited to BERT-based models. The applicability and effectiveness of DisCo on other architectures and models remain untested.
- What evidence would resolve it: Experimental results applying DisCo to different model architectures (e.g., TextCNN, MLP-Mixer) and language models (e.g., RoBERTa, GPT) on various tasks.

## Limitations

- The theoretical justification for why model-view diversity specifically improves SSL performance remains under-specified and lacks rigorous theoretical grounding
- Several critical implementation details are underspecified, including exact adversarial attack parameters and random seed management for data augmentation
- The framework's effectiveness across different PLM architectures (beyond BERT-base), text domains, and languages remains untested

## Confidence

**High Confidence**: The empirical results showing improved performance over single student baselines and competitive results with full supervision are well-supported by the experimental evidence across multiple datasets and tasks.

**Medium Confidence**: The effectiveness of the co-training mechanism itself is reasonably supported, though the relative contribution of model views versus data views is not clearly isolated.

**Low Confidence**: Claims about the framework's generalizability to other PLMs, text domains, and languages are speculative without supporting experiments.

## Next Checks

**Validation Check 1**: Conduct an ablation study isolating the contribution of model views versus data views by training single students with only one type of augmentation.

**Validation Check 2**: Implement the framework using alternative PLM architectures (e.g., RoBERTa, DistilBERT) as teachers to test architectural generalizability.

**Validation Check 3**: Measure and report computational overhead during training (GPU hours, memory usage) for different student configurations, including sensitivity analysis of key hyperparameters.