---
ver: rpa2
title: 'RankAug: Augmented data ranking for text classification'
arxiv_id: '2311.04535'
source_url: https://arxiv.org/abs/2311.04535
tags: []
core_contribution: This paper proposes a data ranking method called RankAug to improve
  text classification performance by selecting high-quality augmented data. The method
  combines semantic similarity (using BERTScore) and diversity (using self-Levenshtein
  distance) to rank paraphrases generated from a Pegasus model.
---

# RankAug: Augmented data ranking for text classification

## Quick Facts
- arXiv ID: 2311.04535
- Source URL: https://arxiv.org/abs/2311.04535
- Authors: Not specified in source
- Reference count: 12
- Key outcome: Data ranking method using semantic similarity and diversity improves text classification accuracy by up to 35% for underrepresented classes.

## Executive Summary
This paper introduces RankAug, a text-ranking approach that detects and filters out low-quality augmented texts by combining semantic similarity (BERTScore) and diversity (self-Levenshtein distance). The method is designed to improve text classification performance, particularly for imbalanced datasets where underrepresented classes can benefit from quality-ensured augmentation. Experiments demonstrate consistent improvements across English and German datasets, validating the approach's effectiveness and generalizability.

## Method Summary
RankAug generates paraphrases using either a Pegasus model (for English) or a back-translation pipeline (for German), then ranks them using a harmonic mean of BERTScore (semantic similarity) and self-Levenshtein distance (diversity). The top-ranked paraphrases are selected and added to the training set. This filtering process is applied selectively to underrepresented classes in imbalanced datasets to enrich them with high-quality, diverse examples while avoiding noise from low-quality augmentations.

## Key Results
- Achieves up to 35% accuracy gains for underrepresented classes in ATIS intent classification
- Consistently outperforms other filtering methods (BLEU, METEOR, BERTScore-only) across all datasets
- Demonstrates effectiveness on both English and German datasets, showing cross-lingual applicability
- Reduces overfitting and improves generalization by ensuring lexical diversity in augmented data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RankAug improves classification accuracy by filtering out low-quality augmented data using a combined semantic similarity and diversity score.
- Mechanism: RankAug computes a harmonic mean of BERTScore (semantic similarity) and self-Levenshtein distance (diversity) to rank generated paraphrases. Only top-ranked samples are retained for training, reducing noise from low-quality augmentations.
- Core assumption: High-quality augmented data must be both semantically similar to the original and lexically diverse to improve model robustness.
- Evidence anchors:
  - [abstract] "proposes RankAug, a text-ranking approach that detects and filters out the top augmented texts in terms of being most similar in meaning with lexical and syntactical diversity."
  - [section] "We propose RankAug, a ranking method that accounts for both similarity and diversity to filter high quality augmentations."
- Break condition: If the augmented data contains paraphrases that are semantically similar but lexically too close to the original, diversity ranking may not help and the model may overfit.

### Mechanism 2
- Claim: Filtering augmented data improves performance especially on imbalanced datasets by enriching underrepresented classes.
- Mechanism: RankAug is applied only to underrepresented class samples in ATIS and Hate Speech datasets, generating paraphrases that are filtered and added back to the training set, increasing the effective sample size without introducing noise.
- Core assumption: Imbalanced datasets benefit more from quality-ensured augmentation than from indiscriminate data addition.
- Evidence anchors:
  - [abstract] "judicious selection of filtering techniques can yield a substantial improvement of up to 35% in classification accuracy for under-represented classes."
  - [section] "For the ATIS and Hate Speech datasets, which are imbalanced datasets, we generate paraphrases for each underrepresented data point."
- Break condition: If the original dataset is already balanced or sufficiently large, the marginal benefit of filtering may be minimal.

### Mechanism 3
- Claim: RankAug generalizes across languages and tasks by leveraging language-agnostic semantic similarity metrics.
- Mechanism: RankAug uses BERTScore (contextual embeddings) and self-Levenshtein distance, both of which can be computed on multilingual embeddings (e.g., multilingual BERT), enabling application to German reviews.
- Core assumption: Multilingual contextual embeddings preserve semantic similarity across languages, making the ranking method transferable.
- Evidence anchors:
  - [section] "we also extend our work on a balanced but a low resource German dataset. Our filtering method outperforms all other methods for both settings indicating that RankAug is adaptable to other languages as well."
  - [section] "For German text augmentation, we use a pivot-based back translation pipeline."
- Break condition: If multilingual embeddings poorly represent the target language's semantics, the filtering effectiveness may degrade.

## Foundational Learning

- Concept: Semantic similarity metrics (e.g., BERTScore)
  - Why needed here: To ensure augmented paraphrases retain the meaning of the original sentence.
  - Quick check question: How does BERTScore differ from BLEU in evaluating semantic similarity?

- Concept: Diversity metrics (e.g., Levenshtein distance)
  - Why needed here: To avoid overfitting by ensuring augmented data introduces lexical variation.
  - Quick check question: What does a low Levenshtein distance indicate about two texts?

- Concept: Data augmentation for imbalanced datasets
  - Why needed here: To boost performance on underrepresented classes without introducing noise.
  - Quick check question: Why might adding unfiltered augmented data harm model performance?

## Architecture Onboarding

- Component map:
  - Text generation: Pegasus model (English) or back-translation pipeline (German)
  - Filtering: RankAug (BERTScore + self-Levenshtein ranking)
  - Downstream model: BERT-base (English) or multilingual BERT (German)
  - Evaluation: Classification accuracy on test set

- Critical path:
  1. Generate paraphrases from source data
  2. Compute BERTScore and self-Levenshtein distance for each paraphrase
  3. Rank paraphrases by harmonic mean of scores
  4. Select top-n paraphrases per source
  5. Train downstream classifier on filtered augmented data
  6. Evaluate performance

- Design tradeoffs:
  - High filtering strictness (small n) may discard useful augmentations but ensures quality.
  - Low filtering strictness (large n) may include noisy samples, hurting performance.
  - BERTScore computation is resource-intensive; consider caching embeddings.

- Failure signatures:
  - Accuracy drops when using unfiltered augmentations but improves with RankAug filtering.
  - Performance gain is minimal if the original dataset is balanced or large.
  - If BERTScore embeddings are unavailable, semantic ranking fails.

- First 3 experiments:
  1. Run Pegasus paraphrasing on ATIS underrepresented classes, apply RankAug-5, train classifier, compare accuracy to baseline.
  2. Repeat with n=3 in RankAug, observe trade-off between quality and quantity.
  3. Apply RankAug to German reviews using back-translation pipeline, compare multilingual BERT performance to English-only settings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RankAug's performance scale with larger amounts of augmented data, and is there an optimal point where additional data becomes detrimental?
- Basis in paper: [explicit] The paper mentions that using "nearly half the total augmented data" yields the best performance, but doesn't explore the impact of varying this parameter or using the full augmented dataset.
- Why unresolved: The paper only evaluates RankAug using a fixed number of top-ranked paraphrases (n=3,5) without exploring the impact of varying this parameter or using the full augmented dataset.
- What evidence would resolve it: Experiments varying the number of selected paraphrases (n) and comparing performance to using the full augmented dataset would provide insights into the optimal data selection strategy.

### Open Question 2
- Question: Can RankAug be effectively applied to other text generation tasks beyond classification, such as question answering or summarization?
- Basis in paper: [inferred] The paper mentions that the method is "independent of the training model" and can be "extended to any data augmentation model for evaluation," but only evaluates on classification tasks.
- Why unresolved: The paper only demonstrates RankAug's effectiveness on classification tasks (ATIS intent classification, hate speech detection, German product reviews), leaving its applicability to other tasks unexplored.
- What evidence would resolve it: Applying RankAug to augment data for other NLP tasks like question answering, summarization, or named entity recognition and evaluating the impact on model performance would determine its generalizability.

### Open Question 3
- Question: How does RankAug compare to human judgment in evaluating the quality of augmented data?
- Basis in paper: [explicit] The paper mentions that "there is no golden standard" for evaluating augmented data and that "the real value of the generated data can be only evaluated through downstream tasks."
- Why unresolved: The paper relies on automated metrics and downstream task performance to evaluate augmented data quality, but doesn't compare these results to human judgment.
- What evidence would resolve it: Conducting a human evaluation study where annotators rate the quality of augmented data selected by RankAug versus other methods would provide insights into how well the automated approach aligns with human perception of data quality.

## Limitations
- Computational cost of BERTScore is substantial, potentially limiting scalability for large datasets
- Evaluation confined to text classification tasks, leaving generalizability to other NLP tasks unexplored
- Does not address potential bias amplification when filtering augmented data, which could be problematic in sensitive applications

## Confidence

**Confidence Labels:**
- High confidence in the core mechanism: The combination of semantic similarity and diversity metrics for filtering augmented data is well-supported by experimental results across multiple datasets and languages.
- Medium confidence in generalizability: While results transfer to German reviews, the claim of cross-task and cross-domain robustness requires more diverse validation.
- Medium confidence in the 35% improvement figure: This maximum gain comes from a specific subset (ATIS intent detection), and performance varies across datasets and filtering strictness.

## Next Checks

1. Benchmark RankAug against recent text generation models (e.g., T5, BART) to assess whether the filtering advantage persists with stronger generators.
2. Conduct an ablation study varying filtering strictness (n=1, 3, 5, 10) systematically across all datasets to identify optimal trade-offs.
3. Test RankAug in a low-resource multilingual setting beyond German (e.g., Spanish or Arabic) to verify cross-lingual robustness claims.