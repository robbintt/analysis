---
ver: rpa2
title: Improving Performance in Continual Learning Tasks using Bio-Inspired Architectures
arxiv_id: '2308.04539'
source_url: https://arxiv.org/abs/2308.04539
tags:
- learning
- continual
- accuracy
- data
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a bio-inspired neural network architecture
  for online continual learning without catastrophic forgetting. The key ideas are:
  (1) a feature extraction layer using sparse projection and activity sparsity to
  highlight salient input features, (2) a neuromodulated learning layer using local
  learning rules (like MSE and Oja''s rule) modulated by ground truth labels, and
  (3) a novel inelastic learning rule that stabilizes weights over time.'
---

# Improving Performance in Continual Learning Tasks using Bio-Inspired Architectures

## Quick Facts
- arXiv ID: 2308.04539
- Source URL: https://arxiv.org/abs/2308.04539
- Reference count: 13
- One-line primary result: Outperforms memory-free methods and matches memory-intensive replay-based approaches in continual learning without catastrophic forgetting.

## Executive Summary
This paper presents a bio-inspired neural network architecture designed for online continual learning without catastrophic forgetting. The architecture decouples feature extraction from learning using sparse projection and local learning rules modulated by ground truth labels. Experiments demonstrate strong performance across multiple benchmark datasets, achieving up to 99.56% accuracy on Split-MNIST while maintaining competitive results on more challenging datasets like CIFAR-100.

## Method Summary
The proposed architecture consists of two main components: a feature extraction layer using sparse projection and activity sparsity to highlight salient input features, and a neuromodulated learning layer that employs local learning rules (MSE, Oja's rule) modulated by ground truth labels. A novel inelastic learning rule (INEL) further stabilizes weights over time by freezing those that deviate significantly from the layer mean. The architecture is designed for online learning without requiring replay buffers or global gradient computations, making it more biologically plausible and computationally efficient.

## Key Results
- Achieves 99.56% accuracy on Split-MNIST task-incremental learning
- Matches memory-intensive replay-based approaches with 94.43% accuracy on Split-CIFAR-10
- Demonstrates 83.17% accuracy on more challenging Split-CIFAR-100 task-incremental setting
- Outperforms memory-free methods across all benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The sparse projection layer in the feature extraction component improves continual learning by reducing the number of active neurons, thus limiting interference between tasks.
- Mechanism: The sparse projection uses dynamic thresholding (ReLU with activity sparsity) to retain only the most salient features. This forces the learning layer to operate on a compressed, high-signal representation, reducing cross-task confusion.
- Core assumption: Activity sparsity preserves discriminative information while filtering noise.
- Evidence anchors:
  - [abstract] "sparse projection and activity sparsity to highlight salient input features"
  - [section] "The α-dimensional input (ut) is projected into a much larger κ-dimensional space (xe) through the sparse projection matrix We... only the most salient features are included in the representation"
- Break condition: If the sparse projection threshold is too aggressive, critical features may be dropped, harming performance.

### Mechanism 2
- Claim: Local learning rules (MSE, Oja, INEL) with neuromodulation reduce catastrophic forgetting by allowing task-specific weight stabilization.
- Mechanism: Local rules update weights using only current input, output, and label (modulatory signal) without requiring global error gradients. The inelastic rule (INEL) further stabilizes weights by freezing those that deviate significantly from the layer mean, preserving learned representations.
- Core assumption: Local updates suffice to capture task-relevant patterns when combined with feature sparsity.
- Evidence anchors:
  - [abstract] "local learning rules (like MSE and Oja's rule) modulated by ground truth labels"
  - [section] "INEL... augments the MSE rule by introducing memory effects on the synaptic plasticity... introduces a simple mechanism for memory consolidation"
- Break condition: If the local rules cannot capture complex dependencies, performance may degrade compared to global gradient methods.

### Mechanism 3
- Claim: The decoupling of feature extraction from the learning layer enables better task separation and reduces interference.
- Mechanism: The feed-forward map F and hidden states St are separated, with plasticity occurring only in the learning layer. This allows the feature extraction layer to remain fixed or change slowly, providing a stable representation space.
- Core assumption: Fixed or slowly changing features reduce catastrophic forgetting.
- Evidence anchors:
  - [abstract] "decoupling our networks in a feature extraction and a learning component"
  - [section] "Our architecture comprises two components, the feed-forward map F and the hidden states St"
- Break condition: If feature extraction is not adapted to new tasks, representational capacity may be insufficient.

## Foundational Learning

- Concept: Sparsity and thresholding in neural networks
  - Why needed here: Enables efficient feature selection and reduces interference between tasks.
  - Quick check question: How does dynamic thresholding in the sparse layer affect the number of active neurons during training?

- Concept: Local vs global learning rules
  - Why needed here: Local rules avoid the need for global error signals, making the architecture more biologically plausible and suitable for online learning.
  - Quick check question: What is the main difference between MSE local rule updates and standard SGD weight updates?

- Concept: Catastrophic forgetting and its mitigation
  - Why needed here: The core problem being addressed; understanding its causes helps in evaluating the proposed solutions.
  - Quick check question: How does the inelastic learning rule (INEL) help prevent catastrophic forgetting?

## Architecture Onboarding

- Component map: Input -> Sparse Projection Layer -> Neuromodulated Learning Layer -> Output

- Critical path: For a new task: Input -> sparse projection -> local weight update (MSE/OJA/INEL) -> prediction. The sparse projection and local update must be fast; no replay buffer or global gradient computation.

- Design tradeoffs:
  - Sparse projection reduces interference but may lose information if threshold is too high.
  - Local rules are fast and online-friendly but may converge slower than global SGD.
  - Fixed feature extractor speeds learning but may limit adaptability to new domains.

- Failure signatures:
  - Accuracy plateaus early: sparsity threshold too aggressive.
  - Forgetting occurs rapidly: learning rate too high or inelasticity not active.
  - Slow convergence: local rule learning rate too low or sparse projection too lossy.

- First 3 experiments:
  1. Train on MNIST with MSE rule, measure accuracy after 1 epoch.
  2. Switch to INEL rule, measure accuracy and forgetting on Split-MNIST.
  3. Integrate sparse projection, compare accuracy to baseline without it.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implications for future work emerge from the limitations discussed.

## Limitations
- Specific implementation details of the sparse projection layer and inelastic learning rule are not fully specified, potentially impacting reproducibility.
- The architecture's scalability to real-world, high-dimensional data (e.g., video, audio) is not demonstrated.
- Computational efficiency gains from using local rules are not quantified or compared to baseline methods.

## Confidence

- High: The sparse projection layer effectively reduces interference by filtering salient features.
- Medium: Local learning rules with neuromodulation can match memory-based replay methods in task-incremental settings.
- Low: The architecture generalizes well to class-incremental and domain-incremental learning without further modifications.

## Next Checks

1. Implement and test the sparse projection layer with different thresholding mechanisms to assess the impact on feature retention and classification accuracy.
2. Compare the performance of the INEL rule against other local learning rules (e.g., MSE, OJA) on larger datasets (e.g., CIFAR-100, Tiny ImageNet) to evaluate scalability.
3. Conduct ablation studies to quantify the individual contributions of sparsity, neuromodulation, and inelasticity to the overall performance.