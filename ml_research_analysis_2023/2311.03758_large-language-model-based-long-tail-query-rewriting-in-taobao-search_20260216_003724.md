---
ver: rpa2
title: Large Language Model based Long-tail Query Rewriting in Taobao Search
arxiv_id: '2311.03758'
source_url: https://arxiv.org/abs/2311.03758
tags:
- query
- queries
- rewriting
- search
- taobao
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BEQUE is a three-stage framework designed to improve long-tail
  query rewriting in e-commerce search by bridging semantic gaps. It uses rejection
  sampling to construct a high-quality rewriting dataset, combines it with auxiliary
  tasks, and fine-tunes a large language model (LLM) through supervised fine-tuning.
---

# Large Language Model based Long-tail Query Rewriting in Taobao Search

## Quick Facts
- arXiv ID: 2311.03758
- Source URL: https://arxiv.org/abs/2311.03758
- Reference count: 40
- Primary result: BEQUE significantly improves GMV, #Trans, and UV for long-tail queries in Taobao search via a three-stage LLM fine-tuning and alignment framework.

## Executive Summary
BEQUE is a three-stage framework that improves long-tail query rewriting in e-commerce search by bridging semantic gaps. It uses rejection sampling to construct a high-quality rewriting dataset, combines it with auxiliary tasks, and fine-tunes a large language model (LLM) through supervised fine-tuning. Offline feedback is provided by simulating the Taobao search engine to rank candidate rewrites, and a contrastive learning method is applied to align the model with search objectives. Offline experiments showed significant improvements in retrieval performance, and online A/B tests demonstrated substantial boosts in gross merchandise volume (GMV), number of transactions (#Trans), and unique visitors (UV) for long-tail queries. BEQUE has been deployed on Taobao since October 2023.

## Method Summary
BEQUE is a three-stage framework designed to improve long-tail query rewriting in e-commerce search. It starts by constructing a high-quality rewriting dataset via rejection sampling with relevance and increment filters. This dataset is mixed with auxiliary task data (quality classification, product title prediction, and chain of thought) and used to fine-tune a large language model (LLM) through supervised fine-tuning. Offline feedback is simulated using the Taobao search engine to rank candidate rewrites, and a Bradley-Terry based contrastive learning method is applied to align the model with online search objectives. The framework has been deployed on Taobao since October 2023, showing significant improvements in both offline and online metrics.

## Key Results
- Offline: BEQUE achieved 5.71% relative improvement in relevance, 12.05% in increment, and 2.67% in hitrate over baseline.
- Online A/B test: BEQUE increased GMV by 1.55%, #Trans by 2.48%, and UV by 1.28% for long-tail queries.
- BEQUE was deployed on Taobao in October 2023 and continues to serve production traffic.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rejection sampling with relevance and increment filters constructs a high-quality query rewriting dataset that aligns with e-commerce retrieval objectives.
- Mechanism: The method starts with a large initial set of query-rewrite pairs from an existing policy, then applies rejection sampling twice: first to filter out rewrites that do not retrieve semantically relevant products, and second to keep only those that meaningfully expand the original query's retrieval set.
- Core assumption: Semantic similarity between query and rewrite is not sufficient; what matters is the overlap of retrieved product sets, which can be measured via relevance and increment scores.
- Evidence anchors:
  - [section] "To prevent the model from retrieving products that are completely different from the user’s original intent, we introduce the relevance score. ... we need to ensure that the rewrite can expand the semantic meaning of the original query ... we introduce the increment score."
  - [abstract] "We first construct a rewriting dataset based on rejection sampling and auxiliary tasks mixing to fine-tune our large language model (LLM) in a supervised fashion."
  - [corpus] Weak evidence; related works focus on embedding-based retrieval or generative rewriting but do not describe rejection sampling with product overlap metrics.
- Break condition: If the relevance threshold is too strict or the increment threshold too high, the dataset becomes too small to train effectively; if too loose, the rewrites will drift semantically and hurt retrieval performance.

### Mechanism 2
- Claim: A Bradley-Terry based contrastive learning method learns a partial order over candidate rewrites from offline feedback, improving alignment with online search objectives.
- Mechanism: Multiple candidate rewrites are generated for each query and scored offline via the search engine. These scores induce a partial order, which is used in a contrastive loss that pushes the model to assign higher generation probabilities to rewrites with better retrieval outcomes.
- Core assumption: The offline search engine's ranking of rewrites' retrieval quality is a reliable proxy for the online reward signal; the partial order is stable enough to learn from.
- Evidence anchors:
  - [section] "Leveraging the partial order of candidate rewrites, we introduce a Bradley-Terry based contrastive learning method to highlight the distinctions between rewrites and align the model with the Taobao online objectives."
  - [abstract] "Leveraging the partial order of rewrites, we introduce a contrastive learning method to highlight the distinctions between rewrites, and align the model with the Taobao online objectives."
  - [corpus] Weak evidence; related methods use RL with reward models, but the specific Bradley-Terry contrastive approach is not well documented in the corpus.
- Break condition: If the offline simulation does not accurately reflect online ranking, the learned partial order may be misleading; if candidate diversity is too low, contrastive learning will not effectively differentiate rewrites.

### Mechanism 3
- Claim: Mixing auxiliary tasks (quality classification, product title prediction, CoT) with query rewriting data in multi-instruction SFT improves the LLM's comprehension of long-tail e-commerce queries.
- Mechanism: Auxiliary tasks are constructed from online logs and human annotation, then combined with the query rewriting dataset. The model is trained on this mixed data, encouraging it to learn domain-specific language patterns and reasoning steps alongside rewriting.
- Core assumption: Auxiliary tasks that mimic real-world query understanding and reasoning in e-commerce provide signal that helps the LLM generalize to long-tail rewriting, not just copying existing rewrites.
- Evidence anchors:
  - [section] "To further enhance LLMs’ ability to comprehend long-tail queries, we have gathered three high-related task datasets ... quality classification, product title prediction, and CoT."
  - [abstract] "We combine these <query, rewrite> pairs with data from quality classification, product title prediction and chain of thought (CoT) tasks to construct the multi-instruction rewriting dataset for fine-tuning our LLM."
  - [corpus] Weak evidence; related works mention auxiliary data or task mixing but do not detail the specific combination or show results of its effect on long-tail rewriting.
- Break condition: If auxiliary tasks are not well-aligned with rewriting or the data quality is low, the model may learn irrelevant patterns, diluting its rewriting ability.

## Foundational Learning

- Concept: Rejection sampling for dataset curation
  - Why needed here: Standard rewriting datasets are biased toward popular queries; rejection sampling allows filtering for long-tail relevance and increment, directly addressing the "few-recall" problem.
  - Quick check question: What two criteria does BEQUE use to filter the initial rewriting dataset via rejection sampling?
- Concept: Contrastive learning with partial order
  - Why needed here: Instead of explicit reward modeling, learning a partial order over rewrites from offline retrieval scores is more stable and avoids reward model bias.
  - Quick check question: Which statistical model underlies the contrastive learning method used in BEQUE?
- Concept: Multi-instruction fine-tuning
  - Why needed here: A single-task fine-tuning on query rewrites alone may not generalize well; mixing related tasks improves semantic understanding and reasoning, especially for long-tail cases.
  - Quick check question: Name the three auxiliary tasks mixed with query rewriting data in BEQUE.

## Architecture Onboarding

- Component map: Rejection sampling data pipeline → Multi-instruction SFT stage → Offline search simulation for candidate scoring → Bradley-Terry contrastive alignment → Online key-value graph serving
- Critical path: SFT training → Candidate generation → Offline scoring → Contrastive alignment → Deployment as key-value lookup
- Design tradeoffs: Larger candidate sets improve relevance but reduce increment/hitrate due to SFT loss weighting; more auxiliary tasks can improve semantic understanding but increase training complexity and risk of task interference.
- Failure signatures: Poor relevance/increment in offline metrics suggests rejection sampling thresholds or auxiliary task quality is off; online GMV drop indicates misalignment between offline feedback and real-world retrieval.
- First 3 experiments:
  1. Ablation: Train with and without rejection sampling to quantify impact on long-tail query performance.
  2. Ablation: Train with and without auxiliary tasks to measure effect on semantic understanding.
  3. Ablation: Vary candidate set size (2, 3, 4, 5) in contrastive alignment to find optimal tradeoff between relevance and increment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does BEQUE's rejection sampling method for dataset construction compare to other query rewriting dataset curation techniques in terms of quality and diversity?
- Basis in paper: [explicit] The paper describes BEQUE's use of rejection sampling to construct a high-quality rewriting dataset and compares it with auxiliary tasks mixing.
- Why unresolved: While the paper mentions rejection sampling and auxiliary tasks, it does not provide a direct comparison with other dataset curation techniques.
- What evidence would resolve it: Comparative studies of BEQUE's rejection sampling method against other dataset curation techniques, such as active learning or human annotation, would provide insights into its effectiveness and potential improvements.

### Open Question 2
- Question: What is the impact of using different large language models (LLMs) as base models in BEQUE on its performance in query rewriting tasks?
- Basis in paper: [explicit] The paper compares the performance of different LLMs (ChatGLM, ChatGLM2.0, Baichuan, Qwen) as base models for BEQUE.
- Why unresolved: Although the paper provides a comparison, it does not explore the impact of using other potential base models or the reasons behind the observed differences in performance.
- What evidence would resolve it: Further experiments with a broader range of base models, including newer or domain-specific LLMs, and an analysis of their impact on BEQUE's performance would provide a more comprehensive understanding of the role of base models.

### Open Question 3
- Question: How does BEQUE's approach to aligning with online objectives through contrastive learning compare to other alignment methods, such as reinforcement learning or supervised learning?
- Basis in paper: [explicit] The paper describes BEQUE's use of contrastive learning for aligning with online objectives and mentions the limitations of reinforcement learning in the context of query rewriting.
- Why unresolved: While the paper discusses contrastive learning and mentions reinforcement learning, it does not provide a direct comparison with other alignment methods.
- What evidence would resolve it: Comparative studies of BEQUE's contrastive learning approach against other alignment methods, such as reinforcement learning or supervised learning, would provide insights into the effectiveness and potential trade-offs of different alignment strategies.

## Limitations

- Offline Feedback Fidelity: The offline search engine simulation is treated as a ground truth signal, but its exact implementation and alignment with online ranking are not disclosed. If the offline simulator does not accurately reflect real-world retrieval performance, the learned partial order from contrastive learning may be misleading, leading to degraded online performance despite good offline metrics.
- Rejection Sampling Thresholds: The choice of thresholds is critical for dataset quality but not clearly justified. Too strict thresholds may result in an insufficient dataset, limiting the model's ability to learn; too loose thresholds may allow semantic drift.
- Auxiliary Task Impact: While the paper claims that mixing auxiliary tasks improves semantic understanding, the actual contribution of each task to long-tail rewriting performance is not clearly quantified. It is possible that the benefits are marginal or that task interference occurs.

## Confidence

- Rejection Sampling for Dataset Curation: High
- Bradley-Terry Contrastive Learning for Partial Order Alignment: Medium
- Multi-Instruction Fine-Tuning with Auxiliary Tasks: Low

## Next Checks

1. **Ablation Study on Rejection Sampling**: Conduct an ablation study where the model is trained with and without rejection sampling, and compare performance on long-tail query rewriting metrics (relevance, increment, hitrate).
2. **Auxiliary Task Contribution Analysis**: Perform an ablation study to measure the contribution of each auxiliary task (quality classification, product title prediction, CoT) to the overall model performance.
3. **Offline-Online Alignment Validation**: Design a validation study to assess the alignment between offline feedback and online performance. This could involve a small-scale online A/B test where a subset of queries is routed to models trained with different offline feedback configurations, and their online GMV, #Trans, and UV are compared.