---
ver: rpa2
title: 'Black-Box Analysis: GPTs Across Time in Legal Textual Entailment Task'
arxiv_id: '2309.05501'
source_url: https://arxiv.org/abs/2309.05501
tags:
- legal
- gpt-4
- entailment
- gpt-3
- japanese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzes GPT-3.5 and GPT-4 performance on the COLIEE
  Task 4 dataset for Japanese legal textual entailment from 2006-2021. The analysis
  reveals that GPT-4 outperforms GPT-3.5 on more recent years (H29, H30, R01, R02,
  R03) while GPT-3.5 excels on earlier years (H18, H19, H26).
---

# Black-Box Analysis: GPTs Across Time in Legal Textual Entment Task

## Quick Facts
- arXiv ID: 2309.05501
- Source URL: https://arxiv.org/abs/2309.05501
- Reference count: 28
- Key outcome: GPT-4 outperforms GPT-3.5 on more recent years while GPT-3.5 excels on earlier years in Japanese legal textual entailment task

## Executive Summary
This study analyzes GPT-3.5 and GPT-4 performance on the COLIEE Task 4 dataset for Japanese legal textual entailment from 2006-2021. The analysis reveals that GPT-4 outperforms GPT-3.5 on more recent years (H29, H30, R01, R02, R03) while GPT-3.5 excels on earlier years (H18, H19, H26). Both models struggle with certain years like H22 and H29, suggesting data distribution significantly impacts performance. GPT-4 shows an average accuracy of 0.7670 for English and 0.7843 for Japanese, compared to GPT-3.5's 0.7109 and 0.6412 respectively. The results indicate GPT-4 has improved Japanese language processing capabilities and narrowed the performance gap between English and Japanese data.

## Method Summary
The study uses black-box API calls to GPT-3.5 and GPT-4 models with formatted prompts for binary classification (YES/NO) on the COLIEE Task 4 dataset containing legal questions and relevant Japanese statute law articles from Heisei 18 (2006) to Reiwa 3 (2021). The method involves setting up API access, formatting prompts in both English and Japanese, executing API calls for all dataset instances, and computing per-year and overall accuracy for both models and languages.

## Key Results
- GPT-4 outperforms GPT-3.5 on recent years (H29, H30, R01, R02, R03) while GPT-3.5 excels on earlier years (H18, H19, H26)
- Both models struggle with certain years like H22 and H29, suggesting data distribution significantly impacts performance
- GPT-4 shows average accuracy of 0.7670 for English and 0.7843 for Japanese, compared to GPT-3.5's 0.7109 and 0.6412 respectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 outperforms GPT-3.5 on more recent years while GPT-3.5 excels on earlier years
- Mechanism: The performance shift reflects the models' training data cutoff dates. GPT-4 was trained on more recent data, giving it better context for newer legal statutes and language patterns. GPT-3.5's stronger performance on older data suggests it retains better coverage of earlier legal language norms.
- Core assumption: The models' training data distributions align with the COLIEE dataset years, creating a temporal alignment effect on performance.
- Evidence anchors:
  - [abstract] "The analysis reveals that GPT-4 outperforms GPT-3.5 on more recent years (H29, H30, R01, R02, R03) while GPT-3.5 excels on earlier years (H18, H19, H26)."
  - [section] "GPT-4 achieved an average accuracy of 0.7670 in English and 0.7843 in Japanese, while GPT-3.5's average accuracy was 0.7109 for English and 0.6412 for Japanese."
- Break condition: If training data distributions were uniform across all years, this temporal performance pattern would not emerge.

### Mechanism 2
- Claim: Both models struggle with certain years like H22 and H29, suggesting data distribution significantly impacts performance
- Mechanism: Specific years present unique linguistic or conceptual challenges that don't align well with either model's training distribution. This creates systematic performance drops regardless of model version.
- Core assumption: The difficulty in H22 and H29 represents genuine dataset distribution issues rather than random noise or individual question complexity.
- Evidence anchors:
  - [abstract] "Both models struggle with certain years like H22 and H29, suggesting data distribution significantly impacts performance."
  - [section] "Interestingly, both models consistently struggle with some years such as H22 and H29, which may have a more complicated set of legal questions or a misaligned distribution of data compared to the models' training data."
- Break condition: If the struggles were due to random factors rather than systematic distribution issues, we would expect to see similar patterns across other years as well.

### Mechanism 3
- Claim: GPT-4 shows improved Japanese language processing capabilities and narrowed the performance gap between English and Japanese data
- Mechanism: GPT-4's training included more balanced multilingual data or better Japanese-specific fine-tuning, reducing the typical performance disparity between languages seen in earlier models.
- Core assumption: The narrowing performance gap reflects genuine improvement in Japanese processing rather than just overall accuracy gains that affect both languages equally.
- Evidence anchors:
  - [abstract] "The results indicate GPT-4 has improved Japanese language processing capabilities and narrowed the performance gap between English and Japanese data."
  - [section] "From the accuracy plots shown in Figures 7 and 8, it is evident that both GPT-3.5 and GPT-4 models perform better with English data compared to Japanese data. However, the performance gap between the two languages is significantly reduced with GPT-4, potentially indicating its improved ability to handle Japanese language processing."
- Break condition: If the gap reduction were simply due to English performance degradation rather than Japanese improvement, we would expect to see different patterns in absolute performance metrics.

## Foundational Learning

- Concept: Temporal alignment between training data and test data
  - Why needed here: Understanding why model performance varies by year requires recognizing that models learn patterns from their training data distribution, which may not align with test data from different time periods.
  - Quick check question: If a model was trained only on data from 2015-2020, would you expect it to perform equally well on data from 2006 and 2021? Why or why not?

- Concept: Language-specific model capabilities
  - Why needed here: The paper shows different performance patterns between English and Japanese, indicating that models may have varying proficiency across languages based on their training data composition.
  - Quick check question: Why might a model trained primarily on English data struggle more with Japanese legal text compared to English legal text?

- Concept: Zero-shot vs. few-shot learning limitations
  - Why needed here: The paper uses direct prompting without extensive fine-tuning, so understanding the limitations of these approaches helps explain why models struggle with certain data distributions.
  - Quick check question: How might the performance differ if these models were fine-tuned specifically on COLIEE data versus using zero-shot prompting?

## Architecture Onboarding

- Component map: API access → Prompt formatting → Binary classification → Response collection → Accuracy calculation → Temporal analysis by year and language
- Critical path: Data preparation → Prompt formatting → API call → Response collection → Accuracy calculation → Performance analysis by year and language
- Design tradeoffs: Using API calls limits control over model internals but enables rapid experimentation. Binary classification simplifies evaluation but loses nuance. Year-by-year analysis provides granular insights but may miss broader patterns.
- Failure signatures: Consistent struggles with specific years suggest systematic distribution issues. Language performance gaps indicate multilingual capability limitations. Temporal performance shifts reveal training data cutoff effects.
- First 3 experiments:
  1. Test model performance on a single year with balanced English and Japanese data to isolate language effects
  2. Compare few-shot prompting with additional examples versus zero-shot prompting for difficult years
  3. Analyze the distribution of training data availability across years to correlate with performance patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the distribution of training data across different years affect the performance of GPT models on legal textual entailment tasks?
- Basis in paper: [explicit] The study observes that both GPT-3.5 and GPT-4 perform differently across years, with notable fluctuations in accuracy, and suggests that the distribution of the dataset might play a critical role in their performance.
- Why unresolved: The paper does not provide a detailed analysis of the training data distribution or its direct impact on model performance across different years.
- What evidence would resolve it: Detailed analysis of the training data distribution for each year and its correlation with model performance could clarify the impact of data distribution on accuracy.

### Open Question 2
- Question: What specific characteristics of the legal questions in years H22 and H29 contribute to the consistent struggle of both GPT models?
- Basis in paper: [explicit] The paper notes that both models consistently struggle with the years H22 and H29, suggesting these years may have a more complicated set of legal questions or a misaligned distribution of data compared to the models' training data.
- Why unresolved: The paper does not provide an in-depth analysis of the question types or data characteristics that make these years particularly challenging.
- What evidence would resolve it: A detailed examination of the legal questions and data distribution in H22 and H29 compared to other years could identify specific factors contributing to the models' difficulties.

### Open Question 3
- Question: How do the models' limitations in handling Japanese language processing affect their overall performance on legal textual entailment tasks?
- Basis in paper: [explicit] The paper highlights that both GPT-3.5 and GPT-4 perform better with English data compared to Japanese data, but the performance gap is reduced with GPT-4, indicating its improved ability to handle Japanese language processing.
- Why unresolved: The paper does not explore the specific limitations of the models in processing Japanese legal text or how these limitations impact their overall performance.
- What evidence would resolve it: Comparative analysis of the models' performance on Japanese versus English legal texts, focusing on specific language processing challenges, could provide insights into their limitations and areas for improvement.

## Limitations

- The black-box nature of the analysis prevents direct examination of training data composition and model internals
- The study assumes performance variations across years directly reflect training data distribution without empirical verification
- Binary classification may oversimplify the nuanced nature of legal entailment, potentially masking important performance patterns

## Confidence

- **High Confidence**: The overall accuracy differences between GPT-3.5 and GPT-4 are well-established through direct API testing
- **Medium Confidence**: The interpretation that temporal performance patterns reflect training data cutoff dates is plausible but not directly verified
- **Low Confidence**: The specific attribution of struggles with H22 and H29 to data distribution misalignment is speculative without analysis of the actual content

## Next Checks

1. **Training Data Analysis**: Obtain and analyze the actual training data distributions of GPT-3.5 and GPT-4 to verify the correlation between training data availability by year and model performance patterns.
2. **Question Complexity Control**: Conduct a controlled experiment comparing performance on questions from H22 and H29 with matched complexity questions from other years to determine if the struggles are truly distribution-related.
3. **Fine-tuning Experiment**: Test whether fine-tuning both models on COLIEE data from specific years eliminates the temporal performance patterns, helping distinguish between model limitations and data distribution effects.