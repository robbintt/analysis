---
ver: rpa2
title: 'Human Choice Prediction in Language-based Persuasion Games: Simulation-based
  Off-Policy Evaluation'
arxiv_id: '2305.10361'
source_url: https://arxiv.org/abs/2305.10361
tags:
- simulation
- data
- games
- game
- hotel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We address the problem of predicting human decisions in off-policy
  evaluation (OPE) for language-based persuasion games, where an expert influences
  a decision-maker (DM) through natural language messages. Unlike previous on-policy
  studies, our OPE framework evaluates models on interactions with experts not seen
  during training.
---

# Human Choice Prediction in Language-based Persuasion Games: Simulation-based Off-Policy Evaluation

## Quick Facts
- arXiv ID: 2305.10361
- Source URL: https://arxiv.org/abs/2305.10361
- Reference count: 33
- Key outcome: Simulation-based approach improves prediction accuracy in off-policy evaluation for language-based persuasion games, achieving 7.1% improvement on top 15% hardest-to-predict decisions

## Executive Summary
This paper addresses the challenge of predicting human decisions in off-policy evaluation (OPE) for language-based persuasion games, where an expert influences a decision-maker through natural language messages. Unlike previous on-policy studies, this work evaluates models on interactions with experts not seen during training. The authors propose a simulation-based approach that generates synthetic data from interactions between simulated decision-makers and the entire space of expert strategies. By training prediction models using a mixture of real human-bot interactions and simulated data, they achieve significant OPE gains, particularly for challenging cases where human behavior is difficult to predict.

## Method Summary
The method involves collecting a large dataset of 87K human decisions from 244 participants playing 10-round games against 12 different rule-based expert bots. A simulation framework generates synthetic data by creating simulated decision-makers with three strategies (Trustful, Text-based, Random) that improve over time, interacting with experts spanning the entire strategy space. The authors train LSTM and Transformer models using a mixture of real and simulated data, with simulated data serving as pre-training and periodic fine-tuning during epochs. This approach aims to create a more robust model that is not overfitted to the idiosyncrasies of the bots in the human-bot interaction training set.

## Key Results
- Simulation-based data significantly improves OPE prediction accuracy by exposing models to unseen bot strategies during training
- LSTM model achieves 7.1% improvement in prediction accuracy for the top 15% hardest-to-predict decisions
- Mixing real and simulated data during training yields better performance than either alone, with the optimal mixing ratio being crucial for avoiding overfitting to synthetic patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simulation-based data improves OPE prediction accuracy by exposing models to unseen bot strategies during training.
- Mechanism: The simulation generates synthetic interactions across the entire strategy space, including bots not present in the human-bot training data. This diversity regularizes the model, preventing overfitting to the limited set of training bots.
- Core assumption: The simulated DMs follow a learning dynamics model that reflects real human improvement over time, making synthetic data representative of unseen scenarios.
- Break condition: If the exogenous DM model's learning assumptions don't match real human learning patterns, simulation data becomes misleading rather than helpful.

### Mechanism 2
- Claim: Mixing real and simulated data during training yields better performance than either alone.
- Mechanism: Real data provides ground truth human behavior patterns, while simulated data provides diversity and coverage of strategy space. The combined training allows the model to learn both specific human patterns and generalizable strategies.
- Core assumption: The proportion of simulated to real data is balanced such that the model doesn't overfit to synthetic patterns.
- Break condition: If the simulated data distribution significantly differs from real human behavior, the mixed training could degrade performance rather than improve it.

### Mechanism 3
- Claim: LSTM architecture performs best for this OPE task due to its ability to capture sequential dependencies in repeated game interactions.
- Mechanism: LSTM cells maintain hidden states across game rounds and games, allowing the model to learn how human decision patterns evolve over time and across multiple interactions with the same bot.
- Core assumption: Human decision-making in this persuasion game context exhibits temporal dependencies that sequential models can capture better than non-sequential ones.
- Break condition: If human decision patterns in this context don't exhibit meaningful temporal dependencies, simpler models might perform equally well or better.

## Foundational Learning

- Concept: Off-Policy Evaluation (OPE)
  - Why needed here: The paper addresses predicting human behavior with unseen bots, which is fundamentally an OPE problem where we evaluate policies (bot strategies) not present in training data.
  - Quick check question: In OPE, what's the key difference between training data and test data that makes this problem challenging?

- Concept: Reinforcement Learning with Non-Cooperative Games
  - Why needed here: Unlike typical RL problems focused on maximizing rewards, this work predicts behavior in non-zero-sum games where there's no optimal strategy.
  - Quick check question: How does the non-cooperative nature of this persuasion game differ from standard RL reward maximization problems?

- Concept: Simulation-Based Learning
  - Why needed here: The paper uses synthetic data generation to overcome the limitation of limited real human-bot interaction data, especially for unseen bot strategies.
  - Quick check question: What's the core challenge that simulation-based learning addresses in this context, and how does the exogenous DM model help?

## Architecture Onboarding

- Component map:
  - Data Collection Layer: Mobile app for human-bot interactions
  - Feature Extraction Layer: Hand-crafted features from reviews + strategic features
  - Model Layer: LSTM (primary), Transformer, XGBoost
  - Simulation Layer: Exogenous DM model with learning dynamics
  - Training Pipeline: Mixed training with real and simulated data

- Critical path:
  1. Collect human-bot interaction data
  2. Generate simulated data using exogenous DM model
  3. Extract features from both datasets
  4. Train LSTM with mixed data (simulation for initialization, then alternating epochs)
  5. Evaluate on unseen bot-human pairs

- Design tradeoffs:
  - LSTM vs Transformer: LSTM captures temporal dependencies better but Transformer can handle longer contexts
  - Simulation complexity vs. data diversity: More complex DM models generate more diverse data but require more parameters
  - Real data quantity vs. simulation quantity: More real data reduces reliance on potentially imperfect simulations

- Failure signatures:
  - Performance degrades on unseen bots: Indicates overfitting to training bot set
  - No improvement from simulation: Suggests simulation doesn't match real human learning patterns
  - Poor performance on challenging cases: Indicates model struggles with uncertainty in predictions

- First 3 experiments:
  1. Train LSTM with only real data vs. only simulated data to establish baseline performance levels
  2. Test different mixing ratios of real to simulated data to find optimal balance
  3. Compare performance across different DM improvement rate parameters (η) in the simulation model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed simulation-based approach for off-policy evaluation (OPE) in language-based persuasion games compare to other OPE methods, such as importance sampling or model-based approaches?
- Basis in paper: The authors mention that their approach contributes to OPE methods in the offline RL literature but do not compare their method to other OPE techniques.
- Why unresolved: The paper focuses on demonstrating the effectiveness of their simulation-based approach within the specific context of language-based persuasion games, without comparing it to other OPE methods.
- What evidence would resolve it: Conducting experiments comparing the simulation-based approach to other OPE methods on the same dataset and game setup would provide insights into its relative performance and advantages.

### Open Question 2
- Question: How does the performance of the proposed approach scale with larger strategy spaces and more complex games?
- Basis in paper: The authors mention that their strategy space is rigorously defined but note that considering more involved strategies and extending the study to other language-based games is a natural extension.
- Why unresolved: The experiments are conducted on a specific strategy space and game setup, and it is unclear how the approach would perform in more complex scenarios.
- What evidence would resolve it: Evaluating the approach on games with larger strategy spaces and more complex game dynamics would provide insights into its scalability and generalization capabilities.

### Open Question 3
- Question: How sensitive is the proposed approach to the choice of hyperparameters, such as the nature vector and temperament vector parameters?
- Basis in paper: The authors mention that the nature vector and temperament vector are hyperparameters and that the DM improvement parameter (η) is also a hyperparameter.
- Why unresolved: The paper does not provide a sensitivity analysis of the approach to different hyperparameter choices.
- What evidence would resolve it: Conducting experiments with different hyperparameter settings and analyzing their impact on the approach's performance would provide insights into its sensitivity and robustness.

## Limitations
- The exogenous DM model's learning assumptions may not perfectly capture real human behavior patterns, potentially limiting the simulation data's effectiveness
- The LSTM model's superior performance is based on comparisons with only one other neural architecture (Transformer), suggesting the need for broader architecture evaluation
- The long-term effectiveness of the simulation approach across different persuasion game domains beyond the hotel review context remains untested

## Confidence
- High confidence: The core methodology of combining real and simulated data for OPE is well-founded and the improvement on challenging cases is statistically significant
- Medium confidence: The assumption that LSTM is optimal for this task, based on comparison with only one other neural architecture
- Medium confidence: The long-term effectiveness of the simulation approach across different persuasion game domains beyond the hotel review context

## Next Checks
1. Conduct ablation studies to quantify the specific contribution of simulated data versus real data at different mixing ratios
2. Test the approach with additional neural architectures (e.g., BERT, GRU) to verify LSTM's superiority claims
3. Validate the simulation model's assumptions by comparing predicted learning curves against longitudinal human data across multiple game sessions