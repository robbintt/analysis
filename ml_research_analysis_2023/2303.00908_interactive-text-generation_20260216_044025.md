---
ver: rpa2
title: Interactive Text Generation
arxiv_id: '2303.00908'
source_url: https://arxiv.org/abs/2303.00908
tags:
- text
- user
- oracle
- generation
- edits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new task, Interactive Text Generation,
  which trains generation models to interact with users or user simulators, allowing
  models to be guided by user edits toward a target text. The task addresses the limitations
  of non-interactive text generation, where models must produce complete outputs without
  user input.
---

# Interactive Text Generation

## Quick Facts
- arXiv ID: 2303.00908
- Source URL: https://arxiv.org/abs/2303.00908
- Reference count: 19
- Models trained to interactively generate text with user guidance, achieving 7% absolute improvement in BLEU over non-interactive baselines

## Executive Summary
This paper introduces Interactive Text Generation, a new task where generation models learn to interact with users through edits rather than producing complete outputs in one shot. The approach leverages large language models' inferential capabilities between user edits, allowing them to focus on difficult parts of text while inferring easier components. The authors propose training these models using imitation learning, where models learn to imitate an expert policy that constructs a trajectory from current draft to target document. Experiments on CNN/DailyMail summaries show interactive models outperform non-interactive counterparts by 7%, 4%, and 5% absolute improvements in BLEU, BLEU-RT, and BARTScore respectively, even with the same number of user edits.

## Method Summary
The authors propose a token editing model architecture that iteratively edits text through three actions: insert, substitute, or delete words at specific locations. Training uses imitation learning with an expert policy that always produces the target document, sampling from a mixture of expert and current policy to address distributional shift. The model predicts edit location, operation type, and token vocabulary through separate MLP heads on top of a BART transformer. During inference, beam search is used for the autoregressive model while depth decoding with top-k action sampling is used for the token editing model. The approach is evaluated on CNN/DailyMail summaries with sentences limited to 64 tokens.

## Key Results
- Interactive models outperform non-interactive counterparts by 7%, 4%, and 5% (absolute) in BLEU, BLEU-RT, and BARTScore respectively
- Token editing model achieves best performance among interactive approaches
- Interactive models maintain superiority even when given identical total number of user edits as non-interactive models
- Performance improvements are consistent across multiple evaluation metrics including CHR F and ROUGE-1

## Why This Works (Mechanism)

### Mechanism 1
Interactive models outperform non-interactive ones because they can leverage large language models' inferential capabilities between user edits. The model generates text incrementally, using user-provided edits as context to infer and fill in missing details, rather than generating everything in one shot. Core assumption: User edits provide meaningful, high-quality signals that guide the model toward the target document. Evidence anchors: [abstract] "Interactive text generation leverages LLM's capability to infer, e.g., 'spacecraft' from 'NASA' and allows it to focus on parts of the text (e.g., 'Monday') that are more difficult to predict correctly". Break condition: If user edits are noisy, irrelevant, or too sparse to provide useful guidance, the inferential advantage disappears.

### Mechanism 2
The token editing model architecture is better suited for this task than autoregressive models because it can make targeted changes without regenerating entire documents. The model operates by selecting locations, operations (insert, substitute, delete), and tokens, allowing it to efficiently modify existing text rather than generating from scratch. Core assumption: The edit space (insert, substitute, delete) is sufficient to transform any draft into the target document. Evidence anchors: [section] "we instead propose the following token editing model. This model iteratively edits text by taking actions where an action can be: inserting a word, substituting a word for another, or deleting a word". Break condition: If the target document requires complex structural changes or word movements that cannot be captured by simple edits, the model's effectiveness degrades.

### Mechanism 3
Imitation learning with expert policy (always producing the target document) provides an effective training signal without requiring reward engineering. The model learns to imitate an expert that can always produce the target document, using a mixture of expert and current policy for state sampling during training. Core assumption: The expert policy (producing the target document) is learnable and provides useful gradients for the interactive task. Evidence anchors: [section] "Instead of trying to express this goal with a reward function, we propose to use the following simple policy: in any given state, try to produce d*". Break condition: If the expert policy produces gradients that are too sparse or the mixture sampling doesn't provide sufficient exploration, learning becomes ineffective.

## Foundational Learning

- Concept: Markov Decision Process formulation
  - Why needed here: Provides the theoretical framework for modeling the interactive text generation as a sequential decision problem with states, actions, and transitions
  - Quick check question: What are the state and action spaces in this MDP formulation, and how do they differ from standard text generation tasks?

- Concept: Imitation Learning (specifically DAgger algorithm)
  - Why needed here: Allows training without reward engineering by having the model learn from expert demonstrations, crucial for avoiding the complexity of designing appropriate rewards for interactive text generation
  - Quick check question: How does the mixture policy (λπE + (1-λ)πθ) help address the distributional shift problem in imitation learning?

- Concept: Non-autoregressive generation and editing models
  - Why needed here: Enables targeted modifications to existing text rather than generating from scratch, which is essential for the interactive nature of the task where the model builds on previous drafts
  - Quick check question: What are the key differences between the token editing model's action space and that of a standard autoregressive sequence-to-sequence model?

## Architecture Onboarding

- Component map: Input draft → Alignment computation → Oracle edit generation → Model prediction (location, operation, token) → Updated draft → Repeat until stopping condition

- Critical path: Input draft → Alignment computation → Oracle edit generation → Model prediction (location, operation, token) → Updated draft → Repeat until stopping condition

- Design tradeoffs:
  - Non-autoregressive vs autoregressive: Edit-based model allows targeted changes but requires more complex action space and decoding
  - Oracle complexity: More sophisticated oracles provide better guidance but may create unrealistic expectations for real users
  - Training noise level: Higher noise improves robustness but may slow convergence

- Failure signatures:
  - Model gets stuck in loops of inserting and deleting the same words
  - Model stops editing prematurely despite significant quality gaps
  - Model consistently misses important edits from the oracle
  - Model generates grammatically incorrect intermediate drafts

- First 3 experiments:
  1. Compare interactive vs non-interactive models with identical oracle edit budgets to verify interactivity benefit
  2. Test different oracle heuristics (unrestricted vs adjacent vs contiguous) to understand impact on model learning
  3. Evaluate different noise levels during training to find optimal denoising strategy

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of interactive text generation models scale with document length beyond the 64-token limit studied in this paper? The authors acknowledge that their work is limited to 64-token documents for practical reasons and plan to extend to paragraph-level and multi-paragraph texts in future work. The paper only evaluates on single sentences (filtered to <64 tokens), so the effectiveness of interactivity on longer documents remains unknown. Experiments comparing interactive vs. non-interactive models on progressively longer documents (paragraphs, multi-paragraph texts) using the same evaluation framework and metrics would resolve this.

### Open Question 2
How do real users actually behave in AI-assisted text editing compared to the simulated user oracles used in this work? The authors acknowledge that training with simulated users is necessary due to cost, but recognize that real user behavior in AI-user collaborative settings may differ from their simulators. The paper relies entirely on simulated oracles for training and evaluation, with no real user studies conducted to validate the realism of these simulations. User studies where real users edit AI-generated text in collaborative settings, comparing their edit patterns, strategies, and preferences to those of the simulated oracles would resolve this.

### Open Question 3
What are the specific human-computer interaction benefits of interactivity that cannot be quantified by the current evaluation framework? The authors note that while their framework can show quantitative improvements from interactivity, there may be additional benefits from a human-computer interaction standpoint that they cannot quantify. The current evaluation framework only measures text quality metrics (BLEU, ROUGE, BARTScore) and edit efficiency, not user experience factors like satisfaction, cognitive load, or collaborative flow. User studies measuring subjective experience, task completion time, error correction efficiency, and user satisfaction when editing with interactive vs. non-interactive systems across various document types and editing scenarios would resolve this.

## Limitations

- Oracle simulator implementation is critical but only described at a high level, creating uncertainty about exact training details
- Results may not generalize to real user interactions as the simulated oracles represent idealized users
- Comparison between autoregressive and token editing models may be confounded by architectural differences beyond just interactivity

## Confidence

**High confidence**: The core claim that interactive text generation is a useful task formulation and that the proposed token editing architecture is well-suited to this task. The experimental setup is clearly described, and the performance improvements over non-interactive baselines are substantial and consistent across multiple metrics.

**Medium confidence**: The specific mechanisms by which interactivity improves generation quality (LLM inferential capabilities between edits). While the paper provides intuitive examples, the empirical evidence is indirect, and alternative explanations (such as architectural advantages of the editing model) are plausible.

**Low confidence**: The generalization of results to real user interactions. The oracle simulator, while sophisticated, represents an idealized user that may not capture the variability, inconsistency, or limited expertise of actual human editors.

## Next Checks

1. **Oracle ablation study**: Evaluate interactive models with progressively simpler oracle heuristics (unrestricted → adjacent → contiguous → no oracle) to quantify how much of the performance gain depends on oracle sophistication versus the interactive architecture itself.

2. **Real user study**: Conduct a small-scale experiment with human users providing edits to drafts generated by both interactive and non-interactive models, measuring whether humans can achieve similar quality improvements with comparable edit budgets.

3. **Noise robustness evaluation**: Systematically vary the noise level during training (randomly editing or deleting words in oracle edits) and measure the impact on final performance to determine whether the models truly learn to handle imperfect user input or are overfitting to oracle quality.