---
ver: rpa2
title: Harnessing GPT-3.5-turbo for Rhetorical Role Prediction in Legal Cases
arxiv_id: '2310.17413'
source_url: https://arxiv.org/abs/2310.17413
tags:
- context
- examples
- legal
- task
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores using GPT-3.5-turbo for predicting rhetorical
  roles in legal case sentences, a task requiring textual context. It systematically
  evaluates prompting strategies including zero/few-shot examples, label definitions,
  and context enrichment.
---

# Harnessing GPT-3.5-turbo for Rhetorical Role Prediction in Legal Cases

## Quick Facts
- arXiv ID: 2310.17413
- Source URL: https://arxiv.org/abs/2310.17413
- Reference count: 26
- Key outcome: GPT-3.5-turbo achieves 72% weighted F1 on legal rhetorical role prediction, outperforming BERT but falling short of specialized systems (86%).

## Executive Summary
This paper investigates using GPT-3.5-turbo for rhetorical role prediction in legal case sentences, a task requiring understanding of textual context and legal discourse patterns. The authors systematically evaluate prompting strategies including zero/few-shot examples, label definitions, context enrichment, and Chain-of-Thought reasoning. They find that providing labelled examples from direct textual context significantly improves performance over supervised BERT classifiers (72% vs 66% weighted F1), though still below the best LegalEval 2023 system (86%). The study demonstrates the potential of one-stage prompting approaches while highlighting remaining gaps to fully optimized architectures.

## Method Summary
The study uses GPT-3.5-turbo to classify legal sentences into 13 rhetorical roles using various one-stage prompting strategies. The prompt template consists of four components: PREAMBLE (task description), EXAMPLES-or-CoT (few-shot examples or Chain-of-Thought reasoning), INPUT (target sentence), and REQUEST (output format). Experiments compare zero-shot, few-shot, label definition, context enrichment, and specific reasoning question approaches on Indian legal data from the LegalEval 2023 shared task.

## Key Results
- Labelled context examples significantly outperform random examples and supervised BERT classifiers
- Label definitions perform better than examples alone for this task
- Specific reasoning questions about textual context improve performance over general prompts
- Following context provides better performance than preceding context for this task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing labelled examples from direct textual context improves performance by leveraging semantic similarity and local coherence cues.
- Mechanism: The model uses in-context learning to match the target sentence's semantic and rhetorical role based on examples that share the same local context window.
- Core assumption: Local context is sufficiently representative of rhetorical role, and examples from that context carry more predictive signal than random examples.
- Evidence anchors:
  - [abstract]: "prompting with a few labelled examples from direct context can lead the model to a better performance than a supervised fined-tuned multi-class classifier"
  - [section]: "By adding unlabelled sentences from the context (context[−+]\d in Table 2), we can see that the performance is deteriorating"
- Break condition: If local context is too short or dissimilar from target sentence, benefit diminishes.

### Mechanism 2
- Claim: Label definitions provide clearer semantic boundaries than examples alone.
- Mechanism: Explicit definitions reduce confusion between similar labels by providing precise semantic constraints.
- Core assumption: Model can effectively parse and apply formal label definitions to guide classification.
- Evidence anchors:
  - [abstract]: "definition of labels... have a positive influence on the performance"
  - [section]: "our results indicated a higher performance when employing the label definitions... than providing just the examples"
- Break condition: If definitions are too complex or overlapping, model may struggle to differentiate between labels.

### Mechanism 3
- Claim: Specific reasoning questions improve performance by forcing attention to relevant features.
- Mechanism: When asked specific questions about influential sentences and key terms, model focuses on identifying actual rhetorical cues rather than generating generic reasoning.
- Core assumption: Model can process and act on specific contextual questions to improve classification accuracy.
- Evidence anchors:
  - [abstract]: "specific questions about this context have a positive influence on the performance"
  - [section]: "When we targeted questions about the best prompt with context (labelled context+8), we achieved higher performance"
- Break condition: If specific questions are poorly formulated or irrelevant, they may confuse the model.

## Foundational Learning

- Concept: In-context learning and few-shot prompting
  - Why needed here: Study relies on GPT-3.5-turbo's ability to learn from examples without parameter updates.
  - Quick check question: What is the difference between zero-shot, one-shot, and few-shot prompting, and how does each affect model performance?

- Concept: Semantic similarity and local coherence in legal texts
  - Why needed here: Understanding why examples from same document context work better than random examples.
  - Quick check question: How do consecutive sentences in legal documents typically relate to each other in terms of rhetorical roles?

- Concept: Rhetorical role classification and annotation guidelines
  - Why needed here: Task involves classifying sentences into specific legal rhetorical roles.
  - Quick check question: What are the key differences between "Analysis" and "Argument by Petitioner" rhetorical roles in legal documents?

## Architecture Onboarding

- Component map: Prompt template -> GPT-3.5-turbo API -> JSON response -> Evaluation metrics -> Results comparison

- Critical path: 1) Prepare prompt template with examples/definitions/context, 2) Send prompt to GPT-3.5-turbo API, 3) Parse JSON response, 4) Calculate evaluation metrics, 5) Compare results across strategies

- Design tradeoffs:
  - More examples → better performance but higher cost and token limits
  - Including context sentences → improved accuracy but increased prompt complexity
  - Specific reasoning questions → higher performance but requires careful prompt engineering

- Failure signatures:
  - Low macro F1 scores indicate confusion between rhetorical roles
  - Performance drops when adding unlabelled context suggest context importance
  - Poor results with general reasoning prompts indicate need for specific questions

- First 3 experiments:
  1. Zero-shot prompting with empty EXAMPLES-or-CoT section
  2. Two-shot prompting with self-generated examples
  3. Labelled context+8 prompting with 8 labelled following sentences and specific reasoning questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different methods of selecting examples for few-shot prompting affect GPT-3.5's performance?
- Basis in paper: [explicit] "For one and more shot prompting, we left the problem of selecting significant examples for future work."
- Why unresolved: Authors used self-generated examples but did not explore systematic methods for selecting more effective examples.
- What evidence would resolve it: Comparative experiments testing various example selection strategies against self-generated examples.

### Open Question 2
- Question: Why does GPT-3.5 perform better with label definitions versus examples, and how can this improve prompt engineering?
- Basis in paper: [explicit] "our results indicated a higher performance when employing the label definitions... than providing just the examples."
- Why unresolved: Finding is significant but underlying mechanisms and hybrid approaches are unexplored.
- What evidence would resolve it: Controlled experiments isolating effects of definitions versus examples, and exploring hybrid approaches.

### Open Question 3
- Question: What is the optimal context size and direction for improving rhetorical role prediction?
- Basis in paper: [explicit] Found that "following context compared to preceding context" performed better.
- Why unresolved: While various configurations were tested, systematic exploration of context characteristics by role type is lacking.
- What evidence would resolve it: Detailed analysis of context effectiveness by role type, and experiments varying context size and direction more granularly.

## Limitations

- Performance gap: 72% F1 still falls short of specialized systems (86%), suggesting prompting strategies don't match optimized architectures
- Dataset specificity: Results based on single LegalEval 2023 dataset, limiting generalizability to other legal corpora
- Context dependency: Heavy reliance on labelled examples from context rather than learning generalizable patterns

## Confidence

- High Confidence: Labelled context examples outperforming random examples is well-supported by consistent performance gains
- Medium Confidence: Label definitions working better than examples is supported but mechanism explanation is speculative
- Medium Confidence: Specific reasoning questions improving performance over general prompts is supported but needs more systematic testing

## Next Checks

1. Cross-dataset validation: Test best prompting strategy on different legal corpus to verify generalizability
2. Ablation study on context size: Systematically vary number of preceding/following sentences to determine optimal context window
3. Comparison with newer models: Replicate experiments using GPT-4 or other contemporary LLMs to assess prompting strategy transfer and performance gaps