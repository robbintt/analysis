---
ver: rpa2
title: A Theory of Link Prediction via Relational Weisfeiler-Leman on Knowledge Graphs
arxiv_id: '2302.02209'
source_url: https://arxiv.org/abs/2302.02209
tags:
- rawl
- graph
- knowledge
- node
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies graph neural networks for link prediction on
  knowledge graphs. The authors introduce conditional message passing neural networks
  (C-MPNNs) that compute pairwise node representations conditioned on a source node
  and a query relation.
---

# A Theory of Link Prediction via Relational Weisfeiler-Leman on Knowledge Graphs

## Quick Facts
- arXiv ID: 2302.02209
- Source URL: https://arxiv.org/abs/2302.02209
- Reference count: 40
- Key outcome: Introduces C-MPNNs for link prediction on KGs with theoretical characterization via relational WL tests

## Executive Summary
This paper introduces conditional message passing neural networks (C-MPNNs) for link prediction on knowledge graphs, which compute pairwise node representations conditioned on both a source node and query relation. The authors theoretically characterize C-MPNN expressive power through a novel relational Weisfeiler-Leman algorithm (rawl2) and show equivalence to a guarded first-order logic fragment. Empirical validation on WN18RR and FB15k-237 demonstrates that while initialization and history function choices have minimal impact, aggregation and message function choices significantly affect performance.

## Method Summary
C-MPNNs extend traditional R-MPNNs by conditioning each node's representation on both the source node and query relation, enabling computation of binary invariants over node pairs rather than just node representations. The framework iteratively updates representations through message passing, with four key design choices: initialization function δ, aggregation function ψ, message function θr, and history function f. The theoretical analysis connects C-MPNNs to the relational asymmetric local 2-WL test (rawl2), showing this characterizes their distinguishing power, and establishes a logical characterization in terms of rGFO3_cnt guarded first-order logic.

## Key Results
- C-MPNNs strictly outperform traditional R-MPNNs by computing pairwise invariants conditioned on source node and query relation
- The choice of initialization and history functions does not significantly impact performance, contrary to intuition
- Aggregation and message function choices substantially affect performance, with sum aggregation often matching or outperforming PNA
- Theoretical characterization via rawl2 is validated through empirical experiments on WN18RR and FB15k-237

## Why This Works (Mechanism)

### Mechanism 1
- Claim: C-MPNNs compute pairwise node representations by conditioning on a source node and query relation, which allows them to distinguish node pairs that traditional R-MPNNs cannot.
- Mechanism: The conditional message passing framework initializes each node's representation based on whether it is the source node, then iteratively updates using relation-specific messages. This creates a binary invariant function over pairs of nodes.
- Core assumption: The initialization function satisfies "target node distinguishability" (δ(u,u,q) ≠ δ(u,v,q) for all u≠v).
- Evidence anchors:
  - [abstract]: "every node representation is conditional on a source node and a query relation, which allows for computing pairwise node representations"
  - [section 4]: "This framework strictly contains NBFNets and allows for a systematic treatment of various other models"
  - [corpus]: Weak - corpus papers focus on hyper-relational KGs and few-shot scenarios, not the conditioning mechanism specifically
- Break condition: If initialization fails target node distinguishability, the model cannot compute meaningful binary invariants and loses its link prediction advantage.

### Mechanism 2
- Claim: The expressive power of C-MPNNs is characterized by the relational asymmetric local 2-WL test (rawl2), which is strictly more powerful than traditional R-MPNN expressive power.
- Mechanism: rawl2 operates on pairs of nodes by only considering neighbors of the second node in each pair, creating a one-directional neighborhood that captures the conditional nature of C-MPNNs.
- Core assumption: rawl2 correctly captures the distinguishing power of conditional message passing without requiring full higher-order WL tests.
- Evidence anchors:
  - [section 5.1]: "The expressive power of C-MPNNs is characterized via a corresponding relational Weisfeiler-Leman algorithm"
  - [Theorem 5.1]: "rawl(t)2 ⪯ h(t)q and rawl(t)2 ≡ h(t)q" showing the tight characterization
  - [corpus]: Weak - corpus neighbors discuss various GNN approaches but don't address the specific WL characterization
- Break condition: If the asymmetry in rawl2 doesn't align with how C-MPNNs process information, the theoretical characterization breaks.

### Mechanism 3
- Claim: The choice of history function (f(t) = t vs f(t) = 0) does not significantly impact model performance, contrary to intuition about path-based vs traditional message passing.
- Mechanism: Both history function choices result in models that can express the same binary invariants as measured by rawl2, making them theoretically equivalent despite different computational paths.
- Core assumption: rawl2's expressive power is independent of how node representations accumulate history, as long as the final representation can distinguish node pairs.
- Evidence anchors:
  - [section 5.1]: "The expressive power of C-MPNNs is independent of the history function"
  - [experimental Q1]: "There is no significant difference between the models with different history functions"
  - [corpus]: Weak - corpus papers don't discuss history function tradeoffs in C-MPNNs
- Break condition: If certain architectural patterns require specific history accumulation that rawl2 doesn't capture, the equivalence could break down in practice.

## Foundational Learning

- Concept: Relational Weisfeiler-Leman algorithms
  - Why needed here: The paper's core theoretical contribution relies on characterizing GNN expressive power through relational WL tests, which extend the classic WL test to knowledge graphs with multiple relation types.
  - Quick check question: What's the key difference between rwl1 and rawl2 in how they process node pairs?

- Concept: First-order logic and guarded fragment
  - Why needed here: The logical characterization connects C-MPNNs to formal logic, showing they capture exactly the rGFO3_cnt fragment, which has well-studied properties.
  - Quick check question: How does rGFO3_cnt differ from standard first-order logic in terms of variable usage and counting?

- Concept: Graph neural network design space
  - Why needed here: Understanding the tradeoffs between initialization, aggregation, message functions, and history functions is crucial for implementing and experimenting with C-MPNNs.
  - Quick check question: What property must initialization functions satisfy to ensure C-MPNNs compute binary invariants rather than just node representations?

## Architecture Onboarding

- Component map: Initialization function δ → iterative message passing (φ, ψ, θr) → final pairwise representations → decoder for link prediction
- Critical path: δ → iterative message passing (φ, ψ, θr) → final pairwise representations → decoder for link prediction
- Design tradeoffs:
  - Initialization: δ1 (simple 1/0) vs δ2 (learnable query vector) vs δ3 (random perturbation) - impacts inductive capability
  - Aggregation: sum (simple, robust) vs PNA (more sophisticated but not always better)
  - Message: θ1r (query-dependent, more parameters) vs θ2r (simpler, fewer parameters) vs θ3r (RGCN-style, basis decomposition)
  - History: f(t)=t (standard) vs f(t)=0 (NBFNet-style) - theoretically equivalent but may differ in optimization
- Failure signatures:
  - Poor performance across all query relations suggests initialization doesn't satisfy target node distinguishability
  - Strong performance on some relations but not others suggests message function parameters aren't learning relation-specific patterns
  - Overfitting on training data but poor generalization suggests insufficient regularization or too many parameters
- First 3 experiments:
  1. Implement basic C-MPNN with δ2 initialization, sum aggregation, θ1r message function, f(t)=t - verify it matches NBFNet performance
  2. Compare δ1 vs δ2 initialization on a small dataset to confirm the importance of query-specific initialization
  3. Test θ2r vs θ3r message functions on FB15k-237 to observe the impact of parameter regularization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of history function affect the expressiveness of C-MPNNs beyond theoretical equivalence?
- Basis in paper: [explicit] "There is no significant difference between the models with different history functions... This may appear as a subtle point, but it is very important for informing future work."
- Why unresolved: While the paper shows theoretical equivalence, practical differences in runtime efficiency or convergence behavior are not explored.
- What evidence would resolve it: Comparative runtime analysis and convergence curves for different history functions on larger datasets.

### Open Question 2
- Question: Does the initialization function impact model performance when the target node distinguishability property does not hold?
- Basis in paper: [explicit] "To validate the impact of different initialization regimes... we conduct a further experiment... we also experiment with a very simple function δ0 which assigns 0 to all nodes."
- Why unresolved: The experiments only show one counterexample (δ0), and the theoretical necessity of target node distinguishability is not fully validated.
- What evidence would resolve it: Systematic ablation studies varying degrees of distinguishability in initialization.

### Open Question 3
- Question: How does the choice of aggregation function interact with different message functions in practice?
- Basis in paper: [explicit] "On FB15k237, there seems to be an intricate interplay between aggregation and message functions... PNA tends to result in slightly better-performing models... but PNA may not be necessary."
- Why unresolved: The paper identifies patterns but does not explain the underlying reasons for the interaction effects.
- What evidence would resolve it: Theoretical analysis of how different aggregation functions affect the expressive power when combined with various message functions.

## Limitations

- Theoretical characterization via rawl2 may not capture all practical distinctions between different C-MPNN design choices
- Evaluation limited to standard KGs (WN18RR, FB15k-237) without node features, leaving performance on feature-rich KGs untested
- Results use fixed hyperparameters (6 layers, 32 dims), with potential sensitivity to depth, width, and learning rate unexplored

## Confidence

- **High confidence**: The theoretical characterization via rawl2 and its comparison to R-MPNNs; the logical characterization in terms of rGFO3_cnt; the empirical finding that aggregation choice matters more than history function
- **Medium confidence**: The practical implications of theoretical equivalence between f(t)=t and f(t)=0; the sufficiency of sum aggregation versus PNA across all scenarios
- **Low confidence**: Whether the theoretical insights generalize to KGs with rich node features or non-standard link prediction tasks (e.g., multi-hop reasoning, few-shot learning)

## Next Checks

1. **Hyperparameter stress test**: Systematically vary depth (2-8 layers), width (16-128 dims), and learning rate to identify regimes where history function choices show measurable performance differences
2. **Feature-rich KG evaluation**: Test C-MPNNs on benchmark KGs with node features (e.g., NELL-995, YAGO3-10) to assess how initialization functions handle heterogeneous node attributes
3. **Logic-to-practice alignment**: Design synthetic KG patterns that should be distinguishable under rawl2 but not under R-MPNN WL tests, then verify C-MPNNs can learn these patterns while traditional R-MPNNs cannot