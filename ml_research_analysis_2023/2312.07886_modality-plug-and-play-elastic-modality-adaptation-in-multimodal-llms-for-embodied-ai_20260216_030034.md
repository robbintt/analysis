---
ver: rpa2
title: 'Modality Plug-and-Play: Elastic Modality Adaptation in Multimodal LLMs for
  Embodied AI'
arxiv_id: '2312.07886'
source_url: https://arxiv.org/abs/2312.07886
tags:
- mpnp-llm
- modality
- arxiv
- training
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficient multimodal adaptation
  in Large Language Models (LLMs) for embodied AI, particularly on resource-constrained
  edge devices. The proposed method, mPnP-LLM, enables elastic runtime modality adaptation
  by connecting unimodal encoders to a flexible set of last LLM blocks, with trainable
  latent connections.
---

# Modality Plug-and-Play: Elastic Modality Adaptation in Multimodal LLMs for Embodied AI

## Quick Facts
- arXiv ID: 2312.07886
- Source URL: https://arxiv.org/abs/2312.07886
- Reference count: 40
- Key outcome: mPnP-LLM achieves up to 3.7× FLOPs reduction and 30% GPU memory usage reduction while maintaining on-par accuracy with existing schemes for multimodal adaptation in embodied AI

## Executive Summary
This paper addresses the challenge of efficient multimodal adaptation in Large Language Models (LLMs) for embodied AI on resource-constrained edge devices. The proposed mPnP-LLM method enables elastic runtime modality adaptation by connecting unimodal encoders to a flexible set of last LLM blocks with trainable latent connections. This approach allows automated runtime modality switching between RGB camera and LiDAR inputs for autonomous driving applications while significantly reducing computational overhead. Experiments on the nuScenes-QA dataset demonstrate that mPnP-LLM achieves substantial computational savings while maintaining or improving accuracy compared to existing adaptation schemes.

## Method Summary
The mPnP-LLM method connects unimodal encoders (ViT for RGB, RangeViT for LiDAR) to the last N blocks of an LLM via trainable latent connections with sigmoid weighting. The approach uses Key & Value Aligners with non-linear projection to integrate multimodal information, and applies LoRA adapters to K,V projectors for parameter-efficient fine-tuning. During runtime adaptation, the method connects encoders to the last LLM blocks, allowing flexible switching between modalities while freezing earlier LLM layers to minimize backpropagation cost. The architecture supports automated modality switching based on environmental contexts and task requirements, with the number of connected blocks (N) serving as a tunable parameter for balancing accuracy and computational cost.

## Key Results
- Achieves up to 3.7× FLOPs reduction and 30% GPU memory usage reduction compared to existing schemes
- Maintains on-par accuracy with existing adaptation methods while providing runtime flexibility
- Improves task accuracy by up to 4% compared to best existing schemes under the same compute budget
- Enables effective adaptation with only hundreds of training samples (659 samples reported)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: mPnP-LLM achieves significant computational savings by connecting unimodal encoders to the last LLM blocks rather than the input layer, reducing the backpropagation cost.
- Mechanism: By freezing all LLM layers except the last N blocks, mPnP-LLM minimizes the activation gradient computation during training. The FLOPs model shows that backpropagation cost scales with N, allowing flexible tradeoff between accuracy and runtime cost.
- Core assumption: The last LLM blocks contain sufficient representation power to integrate multimodal information effectively when combined with trainable latent connections.
- Evidence anchors:
  - [abstract] "by connecting unimodal encoders to a flexible set of last LLM blocks and making such latent connections fully trainable at runtime"
  - [section] "the training cost at runtime depends on the value of N" and provides FLOPs calculation formula
  - [corpus] Weak - no direct corpus evidence for this specific architectural choice

### Mechanism 2
- Claim: The trainable latent connections with sigmoid weighting allow efficient cross-modal interaction by controlling information flow intensity.
- Mechanism: The weighting module (αj = sigmoid(wj/T)) modulates how much multimodal information flows into each connected LLM block, enabling the model to learn optimal information integration patterns.
- Core assumption: Continuous weighting provides better cross-modal interaction than binary hard-coded connections.
- Evidence anchors:
  - [abstract] "making such latent connections fully trainable at runtime"
  - [section] "we use a lightweight post-weighting module to optimize the amount of multimodal information that flows to each LLM block"
  - [corpus] Weak - no direct corpus evidence for this specific weighting mechanism

### Mechanism 3
- Claim: mPnP-LLM's design enables data-efficient modality adaptation, requiring only hundreds of samples for effective training.
- Mechanism: The combination of pre-trained aligners, trainable latent connections, and LoRA adapters on K,V projectors allows rapid adaptation to new modalities with minimal training data.
- Core assumption: The pre-training phase provides sufficient foundation for quick adaptation to new modalities with limited data.
- Evidence anchors:
  - [abstract] "Experiments over the nuScenes-QA dataset show that mPnP-LLM can achieve up to 3.7 × FLOPs reduction while retaining on-par accuracy"
  - [section] "the amount of runtime training samples is 659" and shows effective adaptation with reduced samples
  - [corpus] Weak - no direct corpus evidence for this specific data efficiency claim

## Foundational Learning

- Concept: Transformer architecture and multi-head attention mechanism
  - Why needed here: Understanding how mPnP-LLM inserts multimodal tokens as new K-V pairs into MHA modules
  - Quick check question: How does the MHA module compute attention scores using K, Q, and V sequences?

- Concept: Parameter-efficient fine-tuning techniques (LoRA, adapters)
  - Why needed here: Understanding how mPnP-LLM applies LoRA to K,V projectors and uses parameter-efficient methods for modality adaptation
  - Quick check question: What is the computational advantage of using LoRA compared to full fine-tuning?

- Concept: Backpropagation and computational complexity analysis
  - Why needed here: Understanding how mPnP-LLM reduces training cost by connecting to last LLM blocks and the FLOPs model
  - Quick check question: How does freezing earlier LLM layers reduce the backpropagation computational cost?

## Architecture Onboarding

- Component map: Unimodal encoders (ViT/RangeViT) → Key & Value Aligners → Trainable latent connections → LLM blocks → Output
- Critical path: Encoder → Aligner → Latent Connection → LLM Block → Output
  - Bottleneck: Aligner computation and latent connection training
  - Optimization target: Minimizing aligner parameters while maintaining accuracy
- Design tradeoffs:
  - Number of connected LLM blocks (N) vs. accuracy and training cost
  - Continuous vs. binary weighting for latent connections
  - Pre-training comprehensiveness vs. adaptation flexibility
- Failure signatures:
  - Accuracy drops when switching modalities indicate insufficient representation power
  - High training cost suggests need for fewer connections or simpler aligners
  - Memory issues indicate encoder input size problems
- First 3 experiments:
  1. Validate baseline accuracy with full LLM fine-tuning
  2. Test adaptation with varying N values (e.g., N=4, 10, 16) to find optimal tradeoff
  3. Compare continuous vs. binary weighting for latent connections on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal method for quantifying the importance of different input modalities at runtime to balance accuracy and computational cost in mPnP-LLM?
- Basis in paper: [explicit] The paper mentions that it may be necessary to rank modalities' importance to fit device constraints but does not provide a method for doing so.
- Why unresolved: The paper does not explore or propose a method for dynamically assessing and ranking modality importance during runtime adaptation.
- What evidence would resolve it: A proposed algorithm or metric for evaluating modality importance, along with experimental results demonstrating its effectiveness in optimizing runtime performance.

### Open Question 2
- Question: How can mPnP-LLM be extended to non-transformer-based generative AI backbones, such as stable diffusion models, while maintaining efficiency?
- Basis in paper: [inferred] The paper suggests that mPnP-LLM's design principles could apply to non-transformer-based models but notes that specific structure designs would be required.
- Why unresolved: The paper does not explore or provide examples of mPnP-LLM's application to non-transformer-based models.
- What evidence would resolve it: A detailed design and experimental validation of mPnP-LLM applied to a non-transformer-based generative model, demonstrating efficiency and effectiveness.

### Open Question 3
- Question: What are the trade-offs between using a shared set of connections versus separate sets of connections for different input modalities in terms of accuracy and computational cost?
- Basis in paper: [explicit] The paper discusses the option of using separate sets of connections for each modality, noting the trade-off between flexibility and extra computing cost.
- Why unresolved: The paper does not provide experimental results or analysis comparing the performance and cost of shared versus separate connection sets.
- What evidence would resolve it: Comparative experiments showing the impact on accuracy and computational cost when using shared versus separate connection sets for different modalities.

### Open Question 4
- Question: How does the runtime adaptation of the number of connections between input modality encoders and the LLM affect the overall performance and efficiency of mPnP-LLM?
- Basis in paper: [explicit] The paper mentions that the number of connections can be adjusted at runtime but does not explore the effects of such adjustments.
- Why unresolved: The paper does not investigate or report on the impact of dynamically changing the number of connections during runtime.
- What evidence would resolve it: Experimental results showing how varying the number of connections at runtime influences task accuracy and computational efficiency.

### Open Question 5
- Question: What are the challenges and solutions for collecting and preparing modality-aligned training data for runtime modality adaptation in practical embodied AI applications?
- Basis in paper: [explicit] The paper suggests preparing multimodal training data offline but acknowledges the difficulty of collecting such data at runtime.
- Why unresolved: The paper does not address the specific challenges or propose solutions for data collection and preparation in real-world scenarios.
- What evidence would resolve it: A discussion of practical challenges in data collection and preparation, along with proposed solutions or strategies to overcome these challenges, supported by experimental validation.

## Limitations

- The approach relies on the assumption that last LLM blocks contain sufficient representation power for effective multimodal integration, which may not generalize to all LLM architectures
- Data efficiency claims lack sufficient empirical validation with systematic studies on performance scaling with training data size
- Evaluation is limited to a single autonomous driving QA task, raising questions about generalizability to other multimodal scenarios

## Confidence

**High Confidence:** The mechanism for reducing computational cost by connecting to last LLM blocks rather than input layers is well-grounded and the FLOPs analysis is mathematically sound. The architectural design choices are clearly specified and reproducible.

**Medium Confidence:** The accuracy improvements over baseline methods are demonstrated on the specific nuScenes-QA task, but the generalizability to other domains and LLM sizes remains uncertain. The trade-off between N (number of connected blocks) and performance is well-characterized for the tested range but may not extend beyond.

**Low Confidence:** The data efficiency claims lack sufficient empirical validation. The paper does not provide systematic studies showing how performance degrades with fewer training samples or how the method compares to other parameter-efficient adaptation techniques in low-data regimes.

## Next Checks

1. **Architecture Transferability Test:** Evaluate mPnP-LLM on a different multimodal task (e.g., visual question answering on VQA dataset) using a different LLM backbone (e.g., LLaMA) to assess generalizability beyond the nuScenes-QA + OPT/BLOOMZ combination.

2. **Data Efficiency Scaling Study:** Systematically vary the number of training samples during runtime adaptation (e.g., 100, 200, 500, 1000 samples) and measure the degradation in accuracy to empirically validate the claimed data efficiency and identify minimum sample requirements.

3. **Extreme Compute Budget Analysis:** Test mPnP-LLM under more severe computational constraints (e.g., 10× or 20× FLOPs reduction) to determine the practical limits of the approach and identify failure modes when pushed beyond the reported 3.7× reduction.