---
ver: rpa2
title: 'DoRA: Domain-Based Self-Supervised Learning Framework for Low-Resource Real
  Estate Appraisal'
arxiv_id: '2309.00855'
source_url: https://arxiv.org/abs/2309.00855
tags:
- real
- estate
- dora
- features
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DoRA, a self-supervised learning framework
  designed for real estate appraisal in low-resource scenarios. The core method uses
  an intra-sample geographic prediction task combined with inter-sample contrastive
  learning to learn robust domain representations from unlabeled real estate data.
---

# DoRA: Domain-Based Self-Supervised Learning Framework for Low-Resource Real Estate Appraisal

## Quick Facts
- arXiv ID: 2309.00855
- Source URL: https://arxiv.org/abs/2309.00855
- Reference count: 31
- Primary result: DoRA achieves 7.6% MAPE improvement, 11.59% MAE reduction, and 3.34% HR10% increase in few-shot real estate appraisal

## Executive Summary
DoRA is a self-supervised learning framework designed for real estate appraisal in low-resource scenarios where labeled transaction data is scarce. The method uses an intra-sample geographic prediction pretext task combined with inter-sample contrastive learning to learn robust domain representations from unlabeled real estate data. By pre-training on large unlabeled datasets and fine-tuning on limited labeled examples, DoRA significantly outperforms traditional supervised models and existing SSL methods in house price prediction tasks. The framework has been deployed in production at E.SUN Bank for automating mortgage-related real estate valuations.

## Method Summary
DoRA uses a two-stage approach: pre-training and fine-tuning. During pre-training, the model learns from unlabeled real estate data through an intra-sample geographic prediction task (predicting the located town from real estate metadata features) and inter-sample contrastive learning (pulling together embeddings from the same town while pushing apart embeddings from different towns). The model leverages heterogeneous features including real estate metadata, PoI features, and economic/geographical indicators. Four parallel embedders process different feature types before passing through a shared encoder. During fine-tuning, the pre-trained model is adapted to predict house prices using limited labeled examples for specific property types.

## Key Results
- Outperforms baselines including supervised models, SSL methods, and graph-based approaches
- Achieves at least 7.6% MAPE improvement, 11.59% MAE reduction, and 3.34% HR10% increase in few-shot settings
- Especially effective in rural and newly developed areas with sparse transaction data
- Successfully deployed in production at E.SUN Bank for mortgage-related real estate valuations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DoRA's intra-sample geographic prediction pretext task equips the model with domain-specific geographic knowledge that improves downstream house price prediction in low-resource settings.
- Mechanism: The pretext task requires the model to predict the located town from real estate metadata features (excluding the town label itself). This forces the model to learn fine-grained patterns in how geographic, economic, and property features co-vary with location, encoding domain knowledge directly into the embeddings.
- Core assumption: Geographic location is a strong signal for real estate value, and the model can learn meaningful geographic patterns from metadata without explicit location features.
- Evidence anchors:
  - [abstract] "pre-trained with an intra-sample geographic prediction as the pretext task based on the metadata of the real estate for equipping the real estate representations with prior domain knowledge"
  - [section 4.2] "predict the located town of the given real estate... the model is equipped with fine-grained domain knowledge to distinguish what might be the composition of real estate for each city, which benefits downstream tasks with limited transactions"
  - [corpus] Weak - no direct citations found in corpus neighbors
- Break condition: If the geographic patterns in metadata are not strongly correlated with property values, or if the model cannot learn these patterns from limited feature sets, the pretext task would fail to improve downstream performance.

### Mechanism 2
- Claim: Inter-sample contrastive learning improves representation robustness by pulling together embeddings from the same town and pushing apart embeddings from different towns.
- Mechanism: By treating properties from the same town as positive pairs and those from different towns as negative pairs, the contrastive loss encourages the model to learn representations that are both discriminative and robust to noise, which is especially valuable when labeled data is scarce.
- Core assumption: Properties within the same geographic area share more similar characteristics and value determinants than those in different areas.
- Evidence anchors:
  - [abstract] "inter-sample contrastive learning is employed to generalize the representations to be robust for limited transactions of downstream tasks"
  - [section 4.2] "inter-sample contrastive learning... to consider the similarities and discrepancies between both inter- and cross-cities of real estate"
  - [corpus] Weak - no direct citations found in corpus neighbors
- Break condition: If the contrastive pairs are not well-formed (e.g., properties from the same town are actually quite different), or if the temperature parameter is poorly tuned, the contrastive learning could introduce noise rather than improve robustness.

### Mechanism 3
- Claim: Pre-training on a large unlabeled dataset from multiple property types improves generalization and reduces the need for labeled data in downstream tasks.
- Mechanism: By learning from a diverse set of unlabeled transactions across different property types, the model develops transferable representations that capture common real estate valuation factors, which can then be fine-tuned on limited labeled examples for specific property types.
- Core assumption: There are shared valuation factors across different property types that can be learned from unlabeled data and transferred to improve performance on specific types.
- Evidence anchors:
  - [abstract] "DoRA is pre-trained on large unlabeled datasets and fine-tuned on limited labeled examples for house price prediction"
  - [section 4.3] "During the pre-training stage, we use all property types of unlabeled sets to learn a pre-trained model to enforce the generalizability and then fine-tune based on various property type datasets"
  - [section 5.3] "only using the unlabeled set of the corresponding type (building type in row 1) significantly degenerates the downstream performance, which signifies that DoRA is able to leverage the unlabeled set from various types to improve model performance on the house price prediction"
- Break condition: If the valuation factors across property types are too dissimilar, or if the unlabeled data is not representative of the target domain, pre-training could lead to negative transfer and hurt downstream performance.

## Foundational Learning

- Concept: Self-supervised learning in tabular data
  - Why needed here: Real estate data is inherently tabular, and traditional SSL methods designed for images or text don't apply directly. Understanding how to design pretext tasks and contrastive learning for tabular domains is crucial for DoRA's approach.
  - Quick check question: How does DoRA's geographic prediction pretext task differ from traditional reconstruction-based pretext tasks used in tabular SSL?

- Concept: Geographic knowledge in real estate valuation
  - Why needed here: The core innovation of DoRA is incorporating geographic knowledge into SSL for real estate. Understanding how location affects property values and how this can be encoded into representations is key to the method's success.
  - Quick check question: Why might geographic location be more important than other features in real estate valuation, and how does DoRA leverage this?

- Concept: Contrastive learning theory and application
  - Why needed here: DoRA uses contrastive learning to improve representation robustness. Understanding the theory behind contrastive learning and how it applies to tabular data is necessary to grasp why this component improves performance.
  - Quick check question: How does DoRA's inter-sample contrastive learning differ from standard instance discrimination approaches used in vision SSL?

## Architecture Onboarding

- Component map: Input features (real estate metadata, PoI features, economic/geographical features) → Embedder (4 parallel embedders for different feature types) → Encoder (MLP with 6 layers) → Pre-trained predictor (for pretext task) or House price predictor (for fine-tuning) → Output (town prediction or price prediction)
- Critical path: The most critical path is from input features through the embedder and encoder to the predictor, as this is where the learned representations are used for both pretext tasks and final predictions.
- Design tradeoffs: DoRA trades off between learning domain-specific geographic knowledge (through pretext task) and general robustness (through contrastive learning). The embedder design allows for different feature types but adds complexity compared to a single unified embedding approach.
- Failure signatures: Poor pretext task performance (low accuracy on town prediction) indicates issues with feature encoding or domain knowledge incorporation. High contrastive loss or poor downstream performance suggests issues with representation quality or contrastive pair selection.
- First 3 experiments:
  1. Ablation study: Remove the geographic prediction pretext task and measure impact on downstream performance to verify its importance.
  2. Hyperparameter sensitivity: Vary the weight α between pretext task and contrastive loss to find the optimal balance.
  3. Contrastive learning analysis: Compare performance with and without contrastive learning, and with different positive/negative pair definitions to understand its impact.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- The geographic prediction pretext task's effectiveness depends heavily on the assumption that real estate metadata features contain sufficient information to predict location without explicit coordinates.
- The contrastive learning formulation assumes properties within the same town are semantically similar, which may not hold for diverse property types within the same geographic area.
- The hyperparameter α balancing pretext and contrastive tasks is fixed at 0.5 without sensitivity analysis showing optimal values for different scenarios.

## Confidence
- **High confidence**: The overall methodology (pre-training + fine-tuning framework), the effectiveness of pre-training on unlabeled data, and the relative ranking of baselines vs DoRA are well-supported by experimental results.
- **Medium confidence**: The specific mechanisms of how geographic prediction and contrastive learning contribute to improvements, as these are primarily justified through ablation studies rather than direct ablation or controlled experiments.
- **Low confidence**: The claim that DoRA's geographic prediction is fundamentally different from reconstruction-based pretext tasks in tabular SSL, as this comparison is not directly addressed in the paper.

## Next Checks
1. **Ablation of PoI features**: Train DoRA without PoI features to quantify their contribution to geographic prediction accuracy and downstream performance.
2. **Contrastive pair quality analysis**: Conduct a controlled study varying the definition of positive/negative pairs (e.g., same district vs same town) to understand the sensitivity of contrastive learning to pair selection.
3. **Hyperparameter sensitivity analysis**: Systematically vary α and other key hyperparameters (e.g., temperature τ, encoder depth) to identify optimal settings and understand robustness to hyperparameter choices.