---
ver: rpa2
title: A Robust Negative Learning Approach to Partial Domain Adaptation Using Source
  Prototypes
arxiv_id: '2309.03531'
source_url: https://arxiv.org/abs/2309.03531
tags:
- source
- target
- domain
- adaptation
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel partial domain adaptation framework
  that addresses the negative transfer problem by integrating robust target-supervision
  strategies, ensemble learning, and complementary label feedback. The approach improves
  pseudo-label refinement and employs explicit objectives to optimize intra-class
  compactness and inter-class separation using inferred source prototypes and highly-confident
  target samples in a domain-invariant fashion.
---

# A Robust Negative Learning Approach to Partial Domain Adaptation Using Source Prototypes

## Quick Facts
- arXiv ID: 2309.03531
- Source URL: https://arxiv.org/abs/2309.03531
- Reference count: 38
- Primary result: Outperforms existing state-of-the-art PDA methods on benchmark datasets through robust target-supervision strategies, ensemble learning, and complementary label feedback

## Executive Summary
This paper introduces a novel partial domain adaptation framework that addresses the negative transfer problem by integrating robust target-supervision strategies, ensemble learning, and complementary label feedback. The approach improves pseudo-label refinement and employs explicit objectives to optimize intra-class compactness and inter-class separation using inferred source prototypes and highly-confident target samples in a domain-invariant fashion. The framework ensures source data privacy by eliminating the need to access source data during the adaptation phase through a priori inference of source prototypes.

## Method Summary
The proposed method addresses partial domain adaptation by first inferring source prototypes during the source training phase using cross-entropy and complement entropy objectives. These prototypes are then used to align target samples without accessing source data during adaptation. The approach employs an ensemble of target classifiers trained on disjoint complementary label sets to refine pseudo-labels and reduce the impact of incorrect feedback. Confident target samples are filtered using a CAC metric, and explicit objectives are applied to optimize intra-class compactness and inter-class separation in the latent space.

## Key Results
- Outperforms state-of-the-art PDA methods on benchmark datasets (Office-31, Office-Home, VisDA 2017)
- Demonstrates enhanced robustness and generalization through ensemble learning and complementary label feedback
- Ensures source data privacy by eliminating the need to access source data during the adaptation phase

## Why This Works (Mechanism)

### Mechanism 1
Robust target-supervision strategy reduces negative transfer by filtering out incorrect pseudo-labels during adaptation. The approach uses an ensemble of target classifiers trained on disjoint complementary label sets. This diversity in complementary feedback reduces the impact of incorrect labels, especially in early training stages when the classifier is biased towards source data.

### Mechanism 2
Source prototype inference enables source data privacy and computational efficiency by avoiding source data access during adaptation. The source classifier's weights are interpreted as class prototypes, estimated during the source training phase using cross-entropy and complement entropy objectives. These prototypes are then used to align target samples without needing access to the source dataset.

### Mechanism 3
Maximizing intra-class compactness and inter-class separation beyond first-order moments improves classification accuracy. The approach uses explicit objectives (Lintra and Linter) to align samples from the same class and separate samples from different classes in the latent space, using highly-confident target samples filtered by the CAC metric.

## Foundational Learning

- **Domain adaptation and domain shift**: Understanding the domain shift problem is crucial for grasping why standard supervised learning fails when training and testing data come from different distributions. Quick check: What is the difference between domain adaptation and domain generalization?

- **Negative transfer in partial domain adaptation**: Recognizing how irrelevant source classes can harm target classification performance is essential for understanding the need for robust target-supervision strategies. Quick check: How does negative transfer differ from positive transfer in domain adaptation?

- **Ensemble learning and complementary label training**: These concepts are central to the approach's robustness, as they help refine pseudo-labels and reduce the impact of incorrect feedback. Quick check: What is the benefit of using complementary labels in ensemble training?

## Architecture Onboarding

- **Component map**: Feature encoder (E) -> Source classifier (Cs) -> Ensemble of target classifiers ({Cm_t}) -> Source prototype inference -> Confident target sample filtration -> Intra/inter-class distribution optimization

- **Critical path**: 1) Train source classifier on source data to infer prototypes, 2) Initialize target classifiers with source classifier weights, 3) Filter confident target samples using CAC metric, 4) Refine pseudo-labels using ensemble learning and complementary labels, 5) Optimize intra/inter-class distributions using filtered target samples

- **Design tradeoffs**: Using ensemble classifiers increases computational cost but improves robustness. Inferring source prototypes ensures privacy but may be less effective if the source domain shifts significantly.

- **Failure signatures**: Low classification accuracy may indicate insufficient ensemble diversity or incorrect confidence threshold. High computational cost may suggest inefficient filtering or optimization.

- **First 3 experiments**: 1) Test the effect of varying the number of ensemble classifiers (ne) on classification accuracy, 2) Evaluate the impact of different confidence thresholds (τ) on target sample filtration, 3) Assess the performance of the approach with and without intra/inter-class distribution optimization objectives

## Open Questions the Paper Calls Out

- How does the performance of the proposed method change when using different backbone architectures instead of ResNet-50? The paper focuses on ResNet-50 and does not provide a comparative analysis with other backbones.

- What is the impact of varying the number of ensemble classifiers (ne) on the overall classification accuracy and computational efficiency? The paper mentions using an ensemble of classifiers but does not provide a detailed analysis of the impact of varying the number of classifiers.

- How does the proposed method handle scenarios where the target domain contains classes not present in the source domain (open set domain adaptation)? The paper focuses on partial domain adaptation where the target label space is a subset of the source label space.

## Limitations

- Insufficient details on ensemble learning implementation, particularly how complementary label sets are generated and diversified
- Reliance on pre-inferred source prototypes may become less effective if the source domain experiences significant shift
- CAC metric for confident sample filtration lacks detailed specification, raising questions about optimal threshold determination

## Confidence

- **High Confidence**: The general framework architecture and core objectives (intra-class compactness, inter-class separation) are well-defined and theoretically sound
- **Medium Confidence**: The effectiveness of source prototype inference and its ability to capture sufficient class information without source data access during adaptation
- **Low Confidence**: The specific implementation details of ensemble learning with complementary labels and the CAC metric calculation

## Next Checks

1. Systematically vary the number of ensemble classifiers (ne) and measure the impact on classification accuracy to validate the importance of ensemble diversity in mitigating negative transfer

2. Conduct experiments with different confidence thresholds (τ) to determine their effect on target sample filtration quality and subsequent classification performance

3. Evaluate the approach's performance when the source domain experiences controlled shifts to assess the robustness of pre-inferred source prototypes