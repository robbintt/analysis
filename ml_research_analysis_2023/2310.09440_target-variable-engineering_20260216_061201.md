---
ver: rpa2
title: Target Variable Engineering
arxiv_id: '2310.09440'
source_url: https://arxiv.org/abs/2310.09440
tags:
- regression
- target
- performance
- classification
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of target variable formulation
  (numeric regression vs. binary classification) on machine learning performance and
  computational efficiency.
---

# Target Variable Engineering

## Quick Facts
- arXiv ID: 2310.09440
- Source URL: https://arxiv.org/abs/2310.09440
- Reference count: 11
- Primary result: Regression tasks require 2-3x more HPO iterations and are more sensitive to randomness, data size, and model selection compared to classification tasks

## Executive Summary
This paper investigates whether formulating ML problems as binary classification rather than numeric regression can reduce computational resources and improve replicability. Through systematic experiments on three datasets with ten numeric targets each, the authors compare regression models trained to predict continuous values versus classification models trained on binarized versions of those targets. The results show that classification consistently converges faster to optimal performance, requires fewer HPO iterations, and is less sensitive to hyperparameter choices, model selection, and training data size. These findings have important implications for sustainable AI development by highlighting how problem formulation choices can significantly impact computational efficiency.

## Method Summary
The study compares regression and classification performance across three tabular datasets (Airbnb, Kickstarter, Yelp) with 30,000 instances each, 10 numeric targets per dataset, and 2,000 engineered features per domain. Targets are binarized at the mean value after z-score normalization. The experimental pipeline involves: (1) feature engineering and target binarization, (2) splitting data into 10,000 training, 5,000 validation, and 15,000 test instances, (3) performing HPO with 400 iterations using Optuna with random search and TPE algorithms, and (4) evaluating XGBoost, Linear, and ResNet models for both regression (R2) and classification (AUC). The study uses 15 random initializations per target variable and examines learning curves with up to 20,000 training instances.

## Key Results
- Regression tasks require 2-3x more HPO iterations to converge to optimal performance compared to classification
- Classification consistently achieves 8 percentage points higher average test performance (0.96 vs 0.88 AUC)
- Regression is significantly more sensitive to randomness, training data size, and model selection choices
- The performance gap between best and worst model families is 245.79% for regression versus only 6.34% for classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Regression tasks are more sensitive to hyperparameter optimization budget than classification tasks.
- Mechanism: Regression models require more tuning iterations to converge to optimal performance because they are more sensitive to hyperparameter choices and model selection, leading to overfitting and higher variance in test performance.
- Core assumption: The only difference between regression and classification tasks is the target variable formulation (numeric vs. binarized).
- Evidence anchors:
  - [abstract]: "regression requires significantly more computational effort to converge upon the optimal performance, and is more sensitive to both randomness and heuristic choices in the training process."
  - [section]: "Figure 1 plots the normalized cumulative maximum validation and test performance for each numeric target variable vs. its binarized counterpart across 400 HPO budgets."
  - [corpus]: Weak - corpus neighbors do not directly address the regression vs. classification HPO sensitivity comparison.

### Mechanism 2
- Claim: Classification tasks are less sensitive to the amount of training data compared to regression tasks.
- Mechanism: Classification models achieve higher relative performance with fewer training instances because the binarization of the target variable simplifies the problem, reducing the data requirements for optimal performance.
- Core assumption: Binarization of the target variable simplifies the underlying problem without losing essential predictive information.
- Evidence anchors:
  - [abstract]: "regression is more sensitive not only to HPO budget, but to all of the heuristic choices across the ML pipeline that we investigate."
  - [section]: "Figure 3 shows the normalized progress to the maximum performance averaged across 30 target variables and 30 random draws for each."
  - [corpus]: Weak - corpus neighbors do not directly address the training data sensitivity comparison between regression and classification.

### Mechanism 3
- Claim: The choice of model family has a larger impact on regression performance compared to classification performance.
- Mechanism: Regression tasks are more sensitive to model selection because the continuous nature of the target variable makes the model's ability to capture complex relationships more critical, whereas classification tasks are less affected by model choice due to the simplified problem structure.
- Core assumption: The performance gap between different model families is larger for regression tasks than for classification tasks.
- Evidence anchors:
  - [abstract]: "The selection of models included in model selection are more consequential. Model selection can be significantly shortcut for classification because the best model is closer in performance to the worst and/or default model."
  - [section]: "Figure 6 compares the average best tuned test performance of Linear and ResNet models relative to XGBoost. The differences between the best and worst-performing model families for classification are significantly less than those associated with regression."
  - [corpus]: Weak - corpus neighbors do not directly address the model family impact comparison between regression and classification.

## Foundational Learning

- Concept: Hyperparameter Optimization (HPO)
  - Why needed here: The paper extensively compares regression and classification performance as a function of HPO budget, highlighting the importance of tuning hyperparameters for optimal model performance.
  - Quick check question: What is the primary goal of hyperparameter optimization in machine learning?

- Concept: Learning Curves
  - Why needed here: The paper uses learning curves to demonstrate how the amount of training data affects the performance of regression and classification models differently.
  - Quick check question: What does a steep learning curve indicate about a model's performance with respect to training data?

- Concept: Model Selection
  - Why needed here: The paper investigates the impact of choosing different model families (e.g., XGBoost, Linear, ResNet) on regression and classification performance, emphasizing the importance of model selection in the ML pipeline.
  - Quick check question: Why is model selection particularly important for regression tasks compared to classification tasks, according to the paper?

## Architecture Onboarding

- Component map: Data Preprocessing -> Model Training -> Evaluation
- Critical path:
  1. Load and preprocess data (feature engineering, target variable binarization)
  2. Split data into training, validation, and test sets
  3. Perform HPO for each model family and task type (regression/classification)
  4. Evaluate and compare performance across task types and model families
  5. Analyze results (learning curves, sensitivity to HPO budget, model selection impact)
- Design tradeoffs:
  - Regression vs. Classification: Choosing between numeric regression and binary classification based on the problem context and computational resources
  - HPO Budget: Balancing the computational cost of HPO with the need for optimal model performance
  - Model Family Selection: Considering the performance trade-offs between different model families (e.g., XGBoost, Linear, ResNet) for regression and classification tasks
- Failure signatures:
  - High variance in test performance across different HPO runs or model initializations
  - Large performance gap between regression and classification tasks
  - Significant impact of model selection on regression performance
- First 3 experiments:
  1. Replicate the learning curve analysis to confirm that classification tasks are less sensitive to training data size compared to regression tasks.
  2. Perform HPO with varying budgets to verify that regression tasks require more tuning iterations to converge to optimal performance.
  3. Compare the performance of different model families (e.g., XGBoost, Linear, ResNet) for regression and classification tasks to confirm the larger impact of model selection on regression performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance advantage of classification over regression persist when using alternative binarization thresholds beyond the mean value?
- Basis in paper: [explicit] The authors binarized numeric targets at the mean value (threshold = 0 for z-score normalized data) and found classification outperformed regression, but did not explore other threshold values
- Why unresolved: The study only examined one binarization strategy (mean threshold), which may not be optimal for all target variables and could potentially change the relative performance between regression and classification
- What evidence would resolve it: Systematic experiments comparing different threshold values (median, percentiles, domain-specific values) across various datasets to determine if the classification advantage holds across different binarization strategies

### Open Question 2
- Question: How do the resource efficiency differences between regression and classification scale with dataset size, particularly for large-scale industrial applications?
- Basis in paper: [inferred] The authors note their datasets are "medium-sized" and acknowledge preliminary experiments with larger datasets suggest their results may be conservative, but did not fully explore this scaling relationship
- Why unresolved: The study used datasets with ~30,000 instances and ~2,000 features, which may not represent the scale of modern ML applications that use millions of instances or features
- What evidence would resolve it: Comprehensive experiments across datasets of varying scales (small, medium, large) measuring computational resources, convergence rates, and performance differences between regression and classification

### Open Question 3
- Question: Under what conditions might regression provide advantages over classification that justify the additional computational resources?
- Basis in paper: [explicit] The authors acknowledge situations where regression and classification could be "interchangeable" but do not systematically explore when regression might be preferable despite higher resource costs
- Why unresolved: The study focuses on demonstrating classification's efficiency but doesn't explore scenarios where regression's continuous predictions might be necessary for downstream applications
- What evidence would resolve it: Empirical studies comparing regression and classification across different application domains and downstream tasks to identify specific conditions where regression's continuous outputs provide meaningful advantages

### Open Question 4
- Question: How do different hyperparameter optimization algorithms compare in their ability to reduce the performance gap between regression and classification?
- Basis in paper: [explicit] The authors compared random search and TPE (Tree-Structured Parzen Estimator) and found TPE helped regression more than classification, but did not explore other optimization methods
- Why unresolved: Only two optimization algorithms were tested, and newer methods like Bayesian optimization with acquisition functions, gradient-based methods, or meta-learning approaches could potentially narrow the performance gap
- What evidence would resolve it: Systematic comparison of multiple HPO algorithms (Bayesian optimization variants, evolutionary algorithms, gradient-based methods) measuring their effectiveness at reducing computational requirements for regression relative to classification

## Limitations

- The findings are based on three specific tabular datasets and may not generalize to other data types like images, text, or time series
- The study only explores one binarization strategy (mean threshold), which may not be optimal for all target variables
- The results focus exclusively on XGBoost, Linear, and ResNet models, potentially limiting conclusions about other model families

## Confidence

- **High Confidence**: The observation that classification tasks consistently require fewer HPO iterations to converge (based on 30 target variables Ã— 15 random initializations = 450 experiments)
- **Medium Confidence**: The claim that regression is more sensitive to model selection, given the 245.79% vs 6.34% performance gap between worst and best models
- **Medium Confidence**: The learning curve analysis showing classification's reduced sensitivity to training data, though this depends on the specific datasets and binarization approach used

## Next Checks

1. Test the core findings on non-tabular datasets (images, text, or time series) to assess generalizability beyond the three tabular datasets used
2. Experiment with alternative binarization thresholds (not just the mean) to verify that the computational efficiency advantage of classification persists across different class balance scenarios
3. Include additional model families (e.g., neural networks beyond ResNet, ensemble methods) to determine if the model selection sensitivity finding holds across a broader range of algorithms