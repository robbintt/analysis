---
ver: rpa2
title: Adversarial Purification of Information Masking
arxiv_id: '2311.15339'
source_url: https://arxiv.org/abs/2311.15339
tags:
- adversarial
- image
- perturbations
- attack
- defense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel adversarial purification method called
  IMPure, which aims to extensively eliminate adversarial perturbations in images.
  The key idea is to use a Regional Intersection AutoEncoder (RIAE) to reconstruct
  masked image patches and resist same-position perturbations, while a Random Combination
  Module (RCM) encourages the model to defend against content-similar perturbations.
---

# Adversarial Purification of Information Masking

## Quick Facts
- **arXiv ID**: 2311.15339
- **Source URL**: https://arxiv.org/abs/2311.15339
- **Reference count**: 40
- **Primary result**: IMPure achieves 74.42% accuracy on adversarial images and 87.54% on clean images across three target networks

## Executive Summary
This paper introduces IMPure, a novel adversarial purification method that combines regional patch masking with reconstruction and random combination strategies to defend against adversarial attacks. The approach uses a Regional Intersection AutoEncoder (RIAE) to reconstruct masked patches while resisting same-position perturbations, along with a Random Combination Module (RCM) to improve defense against content-similar perturbations. IMPure achieves state-of-the-art results on ImageNet, demonstrating strong robustness across nine different attack methods and three target networks.

## Method Summary
IMPure employs a two-pronged defense strategy: (1) RIAE masks and reconstructs image patches to eliminate localized adversarial perturbations, and (2) RCM randomly combines purified and adversarial images during training to improve robustness against content-similar perturbations. The method uses joint pixel loss and perceptual loss constraints to ensure both low-level information consistency and high-level feature representation quality. The approach is evaluated across multiple attack methods and target networks on the ImageNet dataset.

## Key Results
- Achieves 87.54% classification accuracy on clean images across three target networks
- Defends against nine adversarial attack methods with 74.42% accuracy on adversarial images
- Demonstrates 86.12% accuracy on clean images and 72.42% on adversarial images under agnostic attacks
- Outperforms state-of-the-art defense methods in both clean and adversarial image classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RIAE mitigates same-position adversarial perturbations by masking patches and reconstructing them from intact patches
- Mechanism: Input image is divided into patches; subset of patches is masked and replaced with mask tokens; remaining intact patches are used to reconstruct the masked patches in parallel, preventing same-position perturbations from directly influencing reconstruction
- Core assumption: Adversarial perturbations are localized and can be effectively eliminated by reconstructing masked patches using information from remaining intact patches
- Evidence anchors: [abstract] "To obtain an adversarial sample, we first mask part of the patches information, then reconstruct the patches to resist adversarial perturbations from the patches"
- Break condition: If adversarial perturbations are not localized or if remaining intact patches do not contain sufficient information to reconstruct masked patches accurately, RIAE may fail to effectively eliminate same-position perturbations

### Mechanism 2
- Claim: RCM encourages network to improve defense capabilities against content-similar perturbations
- Mechanism: During training, RCM randomly combines purified image with adversarial image, intentionally preserving some adversarial perturbations, forcing reconstruction network to generate images with stronger defense capabilities
- Core assumption: Adversarial perturbations in similar regions exhibit similarity, and simulating this risk during training can enhance model's robustness against such perturbations
- Evidence anchors: [abstract] "Then, in order to protect the purified samples against potential similar regional perturbations, we simulate this risk by randomly mixing the purified samples with the input samples before inputting them into the feature extraction network"
- Break condition: If preserved adversarial perturbations are too strong or if reconstruction network cannot effectively learn to defend against them, RCM may not improve model's defense capabilities

### Mechanism 3
- Claim: Joint constraints of pixel loss and perceptual loss promote consistency between low-level information and high-level representation
- Mechanism: Pixel loss ensures purified image is close to clean image at pixel level, while perceptual loss ensures feature representations of purified and clean images are similar in feature extraction network
- Core assumption: Both pixel-level and feature-level consistency are important for generating high-quality purified images that are robust against adversarial attacks
- Evidence anchors: [abstract] "Finally, we establish a combined constraint of pixel loss and perceptual loss to augment the model's reconstruction adaptability"
- Break condition: If balance between pixel loss and perceptual loss is not optimal, or if feature extraction network is not well-suited for task, joint constraints may not effectively promote consistency between low-level and high-level information

## Foundational Learning

- **Concept: Adversarial attacks and defenses**
  - Why needed here: Understanding nature of adversarial attacks and different defense mechanisms is crucial for designing effective adversarial purification methods
  - Quick check question: What are key differences between model-specific and model-agnostic defense methods?

- **Concept: Generative models and image reconstruction**
  - Why needed here: Proposed method relies on generative models to reconstruct clean images from adversarial samples; understanding principles and techniques of image reconstruction is essential
  - Quick check question: How do generative models learn to reconstruct images, and what are common loss functions used in image reconstruction tasks?

- **Concept: Feature extraction and perceptual loss**
  - Why needed here: Perceptual loss is used to ensure purified image has similar feature representations to clean image; understanding how feature extraction networks work and how perceptual loss is calculated is important
  - Quick check question: What is role of feature extraction networks in calculating perceptual loss, and how does choice of network affect effectiveness of loss?

## Architecture Onboarding

- **Component map**: Input Image -> RIAE (mask & reconstruct patches) -> RCM (random combination) -> Feature Extraction -> Loss Calculation (pixel + perceptual) -> Model Update

- **Critical path**:
  1. Input adversarial image
  2. RIAE masks and reconstructs patches
  3. RCM combines purified image with adversarial image
  4. Feature extraction network extracts features
  5. Pixel loss and perceptual loss are calculated
  6. Model parameters are updated based on overall loss

- **Design tradeoffs**:
  - Masking vs. denoising: Masking patches and reconstructing them from intact patches can be more effective than denoising entire image, but may lead to some loss of information
  - Pixel loss vs. perceptual loss: Balancing pixel loss and perceptual loss is important for generating high-quality purified images that are both visually similar to clean images and robust against adversarial attacks
  - Model complexity vs. training efficiency: Using more complex model may improve defense performance, but may also increase training time and computational cost

- **Failure signatures**:
  - Poor reconstruction quality: If RIAE fails to accurately reconstruct masked patches, purified image may have visible artifacts or distortions
  - Overfitting to training data: If model is too complex or if training data is not diverse enough, model may overfit to training data and fail to generalize well to new adversarial samples
  - Sensitivity to hyperparameter settings: Performance of method may be sensitive to choice of hyperparameters, such as patch size, number of transformer blocks, and weights of loss functions

- **First 3 experiments**:
  1. Evaluate defense performance of method against different adversarial attack methods on ImageNet dataset
  2. Compare proposed method with other state-of-the-art adversarial purification methods in terms of classification accuracy and defense robustness
  3. Analyze impact of different information masking methods (e.g., image occlusion, image noise, feature noise) on defense performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Assumes adversarial perturbations are localized and can be effectively eliminated through patch reconstruction, which may not hold for all attack types
- Computational overhead introduced by Regional Intersection AutoEncoder and Random Combination Module could impact real-time applications
- Performance on non-localized, global perturbations remains uncertain

## Confidence
- **High confidence**: Basic premise that masking and reconstructing image patches can reduce adversarial effects is well-supported by experimental results showing improved accuracy on adversarial images
- **Medium confidence**: Effectiveness of Random Combination Module in simulating content-similar perturbations and improving defense capabilities requires further validation across diverse attack scenarios
- **Medium confidence**: Combined pixel and perceptual loss approach for training stability is supported by results, but optimal balance between these losses may vary depending on specific dataset and attack methods

## Next Checks
1. Test IMPure's performance against advanced adversarial attacks that combine local and global perturbations to evaluate method's robustness limits
2. Conduct ablation studies to quantify individual contributions of RIAE and RCM to overall defense performance, helping identify potential redundancies or optimization opportunities
3. Evaluate method's computational efficiency and memory requirements on resource-constrained devices to assess practical deployment feasibility