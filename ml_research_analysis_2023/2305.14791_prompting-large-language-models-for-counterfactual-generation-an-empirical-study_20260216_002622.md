---
ver: rpa2
title: 'Prompting Large Language Models for Counterfactual Generation: An Empirical
  Study'
arxiv_id: '2305.14791'
source_url: https://arxiv.org/abs/2305.14791
tags:
- sentence
- shot
- llms
- words
- counterfactual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the ability of large language models (LLMs)
  to generate counterfactuals for data augmentation. It proposes a comprehensive evaluation
  framework covering various natural language understanding tasks and factors affecting
  counterfactual generation.
---

# Prompting Large Language Models for Counterfactual Generation: An Empirical Study

## Quick Facts
- arXiv ID: 2305.14791
- Source URL: https://arxiv.org/abs/2305.14791
- Reference count: 40
- Large language models can generate counterfactuals that improve small language models' performance, but struggle with complex tasks like NER and RE

## Executive Summary
This paper investigates large language models' (LLMs) ability to generate counterfactuals for data augmentation across four natural language understanding tasks: sentiment analysis, natural language inference, named entity recognition, and relation extraction. The study proposes a comprehensive evaluation framework examining how different prompt designs affect counterfactual generation quality. Experiments reveal that while LLMs can generate promising counterfactuals for simpler tasks, they struggle with more complex tasks due to self-limitations and lack of logical guidance. The research demonstrates that accurate task definitions and detailed step-by-step instructions are crucial for effective counterfactual generation by LLMs.

## Method Summary
The study uses GPT-3.5 as the LLM to generate counterfactual data for training smaller language models (SLMs) like BERT. A quadruplet prompt structure (Task Definition, Instruction, Demonstration, Input Sample) guides the LLM's counterfactual generation. The research evaluates performance across four tasks using standard datasets (SST-2, IMDB, SNLI, MNLI, CoNLL2003, OntoNotesV5, SemEval2010, TACRED) under few-shot settings (5, 10, 20, 50 shots) for both IID and OOD scenarios. The method compares SLM performance when trained on original data versus data augmented with LLM-generated counterfactuals, and benchmarks against existing methods like AutoCAD, CFGen, and CoCo.

## Key Results
- LLMs show strong correlation between their own task performance and counterfactual generation quality
- Accurate task definitions and detailed step-by-step instructions are foundational for effective counterfactual generation
- Demonstrations primarily regulate output format rather than provide task semantics
- LLMs struggle with complex tasks like NER and RE due to self-limitations and lack of logical guidance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate counterfactuals that improve SLMs' performance when the LLM itself performs well on the target task
- Mechanism: LLMs leverage their pre-trained knowledge to create diverse samples that mitigate spurious correlations, enhancing SLM generalization
- Core assumption: The quality of generated counterfactuals is positively correlated with the LLM's own performance on the task
- Evidence anchors:
  - [abstract]: "While LLMs show promising enhancements in various settings, they struggle in complex tasks like NER and RE due to their self-limitations and the lack of logical guidance to produce counterfactuals that align with commonsense."
  - [section]: "We find a strong correlation between the counterfactual generation capability of LLMs and their own performance in performing each task directly."
- Break condition: When LLM's task performance is low, generated counterfactuals may degrade SLM performance instead of improving it

### Mechanism 2
- Claim: Accurate task definitions and detailed step-by-step instructions are crucial for effective counterfactual generation by LLMs
- Mechanism: LLMs require clear task definitions and structured instructions to understand the goal and constraints of counterfactual generation
- Core assumption: LLMs' ability to comprehend instructions and learn in context enables them to follow detailed prompts for counterfactual generation
- Evidence anchors:
  - [abstract]: "Our analysis reveals the pivotal role of providing accurate task definitions and detailed step-by-step instructions to LLMs in generating counterfactuals."
  - [section]: "Providing accurate task definition and detailed step-by-step instructions to LLMs plays a foundational role in generating counterfactuals."
- Break condition: If instructions are vague or missing, LLMs may generate counterfactuals that don't align with the intended task or commonsense

### Mechanism 3
- Claim: Demonstrations primarily serve to regulate the output format rather than provide additional task details for counterfactual generation
- Mechanism: LLMs can generate reasonable counterfactuals even with unreasonable demonstrations, indicating that demonstrations mainly establish formatting constraints
- Core assumption: LLMs' ability to learn from demonstrations is primarily for understanding output structure, not task semantics
- Evidence anchors:
  - [abstract]: "We also find that LLMs can generate reasonable counterfactuals even with unreasonable demonstrations, which illustrates that demonstrations are primarily to regulate the output format."
  - [section]: "Even using unreasonable demonstrations in the prompt can guide LLMs to generate reasonable counterfactuals. This further validates that the demonstration only serve for regulating the output format during the process of generating counterfactuals via LLMs."
- Break condition: If demonstrations are not only unreasonable but also contradict the task definition or instructions, LLMs may fail to generate appropriate counterfactuals

## Foundational Learning

- Concept: Causal inference theory
  - Why needed here: Counterfactual generation is grounded in causal inference theory, which is essential for understanding cause-and-effect relationships in text
  - Quick check question: How does causal inference theory relate to the generation of counterfactuals in NLP tasks?

- Concept: In-context learning
  - Why needed here: LLMs use in-context learning to understand instructions and learn from demonstrations, which is crucial for generating counterfactuals
  - Quick check question: What is in-context learning, and how does it enable LLMs to generate counterfactuals based on prompts?

- Concept: Data augmentation
  - Why needed here: Counterfactual generation is a form of data augmentation used to improve model performance by mitigating spurious correlations
  - Quick check question: How does counterfactual data augmentation differ from other data augmentation techniques in NLP?

## Architecture Onboarding

- Component map: Prompt -> LLM (GPT-3.5) -> Counterfactual samples -> SLM (BERT) -> Enhanced model
- Critical path: 1. Design prompt with task definition, instructions, and demonstration 2. Input prompt to LLM to generate counterfactual samples 3. Filter counterfactual samples (optional) 4. Combine original and counterfactual data 5. Train SLM on augmented dataset 6. Evaluate SLM performance
- Design tradeoffs: Tradeoff between prompt complexity and LLM's ability to understand instructions; balance between diversity of counterfactual samples and quality (common-sense alignment); choice between using filtering mechanism or accepting all generated counterfactuals
- Failure signatures: Poor SLM performance improvement or degradation after data augmentation; LLMs generate counterfactuals that don't align with commonsense or task constraints; LLMs fail to understand instructions or demonstrations in the prompt
- First 3 experiments: 1. Test LLM's ability to generate counterfactuals for a simple task (e.g., sentiment analysis) with a well-designed prompt 2. Evaluate the impact of different prompt components (e.g., task definition, instructions, demonstration) on counterfactual quality 3. Compare SLM performance when trained on original data vs. data augmented with LLM-generated counterfactuals

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can structured knowledge be effectively incorporated into LLMs to improve their counterfactual generation capabilities for complex tasks like relation extraction?
- Basis in paper: [explicit] The paper discusses that LLMs lack structured knowledge and logical guidance, particularly for complex tasks like relation extraction. It suggests that incorporating structured knowledge based on human experience could potentially enhance LLMs' ability to generate reasonable counterfactuals.
- Why unresolved: While the paper identifies the need for structured knowledge, it does not provide specific methods or evidence for how to effectively incorporate such knowledge into LLMs.
- What evidence would resolve it: Experiments demonstrating improved counterfactual generation performance in complex tasks after incorporating structured knowledge into LLMs would provide evidence to resolve this question.

### Open Question 2
- Question: What is the impact of instruction tuning and reinforcement learning from human feedback on LLMs' counterfactual generation ability?
- Basis in paper: [explicit] The paper mentions that alignment techniques like instruction tuning and reinforcement learning from human feedback may potentially enhance the counterfactual generation ability of LLMs, but does not provide detailed analysis or experimental results.
- Why unresolved: The paper only briefly mentions the potential impact of these techniques without providing concrete evidence or analysis of their effects on counterfactual generation.
- What evidence would resolve it: Comparative experiments showing the performance of LLMs with and without instruction tuning or reinforcement learning from human feedback in counterfactual generation tasks would help resolve this question.

### Open Question 3
- Question: How does the performance of LLMs in counterfactual generation vary across different model sizes and architectures?
- Basis in paper: [explicit] The paper notes that simply increasing the parameter size of LLMs does not yield desired improvements in counterfactual generation, but does not explore the effects of different model sizes or architectures.
- Why unresolved: The paper focuses on a specific LLM (GPT-3.5) and does not investigate how counterfactual generation performance might differ with other model sizes or architectures.
- What evidence would resolve it: Comparative studies examining the counterfactual generation performance of LLMs with varying sizes and architectures would provide insights into this question.

## Limitations

- The study relies on GPT-3.5 as the specific LLM, which may not generalize to other LLMs with different architectures or training approaches
- The self-limiting performance of LLMs on complex tasks like NER and RE is identified but the underlying causes remain unclear
- The filtering mechanism's effectiveness is demonstrated but the methodology for determining "low-quality" counterfactuals could introduce bias

## Confidence

- **High confidence**: The empirical observation that LLM performance on the target task correlates with counterfactual generation quality; the finding that task definitions and step-by-step instructions are crucial for effective counterfactual generation; the claim that demonstrations primarily regulate output format rather than task semantics
- **Medium confidence**: The generalization of findings across different tasks and datasets; the assumption that LLM-generated counterfactuals will consistently improve SLM performance across diverse domains; the specific design tradeoffs between prompt complexity and generation quality
- **Low confidence**: The scalability of this approach to more complex tasks or larger datasets; the long-term stability of improvements when using LLM-generated counterfactuals; the potential for LLMs to introduce subtle biases through counterfactual generation

## Next Checks

1. **Cross-LLM validation**: Repeat experiments with different LLMs (e.g., Claude, LLaMA) to assess whether the observed correlations between LLM task performance and counterfactual generation quality hold across models with different architectures and training approaches

2. **Human evaluation benchmark**: Conduct systematic human evaluation of LLM-generated counterfactuals to establish ground truth quality metrics and identify patterns in which types of counterfactuals are most beneficial versus potentially harmful for model training

3. **Ablation study on prompt components**: Design controlled experiments that systematically vary individual prompt components (task definition, instructions, demonstrations) to quantify their specific contributions to counterfactual quality and identify potential redundancy or conflicts between components