---
ver: rpa2
title: Few Shot Part Segmentation Reveals Compositional Logic for Industrial Anomaly
  Detection
arxiv_id: '2312.13783'
source_url: https://arxiv.org/abs/2312.13783
tags:
- segmentation
- anomaly
- images
- detection
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PSAD, a novel anomaly detection method for
  industrial images that leverages few-shot part segmentation. PSAD uses a segmentation
  model trained on few labeled images and unlabeled images sharing logical constraints,
  combined with three memory banks for class histograms, component composition embeddings,
  and patch-level representations.
---

# Few Shot Part Segmentation Reveals Compositional Logic for Industrial Anomaly Detection

## Quick Facts
- arXiv ID: 2312.13783
- Source URL: https://arxiv.org/abs/2312.13783
- Reference count: 11
- Achieves 98.1% AUROC on MVTec LOCO AD for logical anomaly detection

## Executive Summary
This paper introduces PSAD, a novel industrial anomaly detection method that leverages few-shot part segmentation to identify logical and structural anomalies. The method addresses the challenge of annotating industrial components by using limited labeled images combined with unlabeled images that share logical constraints. By constructing three specialized memory banks and applying adaptive scaling, PSAD achieves state-of-the-art performance on the MVTec LOCO AD dataset, particularly excelling at detecting logical anomalies that existing methods struggle with.

## Method Summary
PSAD operates in two stages: first training a part segmentation model using few labeled images and many unlabeled images with histogram matching loss, then building three memory banks (class histograms, component composition embeddings, and patch-level representations) for anomaly detection. The segmentation model uses a WideResNet101 backbone with coordinate features and is trained with supervised losses plus histogram matching loss on unlabeled data. Anomaly scores from the three memory banks are combined using an adaptive scaling strategy that normalizes scores based on training data statistics. This approach enables accurate component classification even with limited annotations and robust detection of both logical and structural anomalies.

## Key Results
- Achieves 98.1% AUROC for logical anomaly detection on MVTec LOCO AD
- Significantly outperforms existing methods (89.6% AUROC from best competitor)
- Demonstrates effectiveness of few-shot segmentation for industrial applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot part segmentation enables accurate differentiation of similar components in industrial images.
- Mechanism: The method leverages limited labeled images combined with many unlabeled images, using histogram matching loss to ensure consistent pixel counts per class across unlabeled samples, thereby overcoming the challenge of segmenting similar textures.
- Core assumption: Logical constraints shared across normal images allow consistent segmentation without full supervision.
- Evidence anchors:
  - [abstract]: "curation of pixel-level annotations for semantic segmentation is both time-consuming and expensive... We introduce a novel component segmentation model for LA detection that leverages a few labeled samples and unlabeled images sharing logical constraints."
  - [section]: "We propose a new part segmentation model that can segment components in various industrial images using only few labeled samples... We utilize positional features for prediction and minimize a histogram matching loss for unlabeled images, ensuring each image maintains a consistent number of pixels per class."
- Break condition: If the logical constraints are not shared across normal images or if components are too visually similar, segmentation accuracy would degrade.

### Mechanism 2
- Claim: Three memory banks capture different aspects of anomaly patterns, enabling robust detection of both logical and structural anomalies.
- Mechanism: Class histogram memory bank captures component counts and arrangement, component composition memory bank captures valid combinations, and patch-level memory bank captures fine-grained local features; combining these with adaptive scaling integrates their strengths.
- Core assumption: Different types of anomalies manifest differently at these three levels, and combining them improves detection.
- Evidence anchors:
  - [abstract]: "we propose to enhance both local and global sample validity detection by capturing key aspects from visual semantics via three memory banks: class histograms, component composition embeddings and patch-level representations."
  - [section]: "We introduce a novel AD method that involves constructing 3 distinct memory banks based on the segmentation... To generate a unified anomaly score from varying scales of anomaly scores, we introduce an adaptive scaling strategy."
- Break condition: If one memory bank dominates or if adaptive scaling fails to normalize scores properly, overall performance could suffer.

### Mechanism 3
- Claim: Adaptive scaling normalizes anomaly scores from different memory banks, enabling reliable comparison and combination.
- Mechanism: Anomaly scores from each memory bank are scaled using statistics derived from training data, treating each training sample as a test case, which avoids manual hyperparameter tuning.
- Core assumption: The distributions of anomaly scores differ across memory banks and can be effectively normalized using training data statistics.
- Evidence anchors:
  - [abstract]: "For effective LA detection, we propose an adaptive scaling strategy to standardize anomaly scores from different memory banks in inference."
  - [section]: "To obtain a unified anomaly score from the different scaled outputs of the memory banks, we propose an adaptive strategy to re-scale anomaly scores using scores from training data."
- Break condition: If the training data does not represent the score distributions well, scaling could be inaccurate.

## Foundational Learning

- Concept: Semantic segmentation vs instance segmentation
  - Why needed here: The method requires semantic segmentation to classify components by type, not just identify instances, since component classes are predefined by manufacturers.
  - Quick check question: Can you explain the difference between semantic and instance segmentation and why semantic is needed for detecting logical anomalies?

- Concept: Memory banks for density estimation
  - Why needed here: The method stores normal sample features in memory banks to compute anomaly scores by comparing test samples to nearest neighbors, a density estimation approach.
  - Quick check question: How does a memory bank-based density estimation approach differ from reconstruction-based anomaly detection?

- Concept: Few-shot learning with transductive inference
  - Why needed here: The method uses a limited number of labeled images and unlabeled images sharing logical constraints, updating the model during inference using these samples.
  - Quick check question: What is the difference between inductive and transductive few-shot learning, and why is transductive useful when logical constraints are shared across images?

## Architecture Onboarding

- Component map: Segmentation model (feature extractor + pixel classifier) → Three memory banks (class histogram, component composition, patch-level) → Adaptive scaling → Unified anomaly score
- Critical path: Segmentation model training → Memory bank construction → Anomaly score computation → Adaptive scaling → Final decision
- Design tradeoffs: Using three memory banks increases detection capability but adds complexity; adaptive scaling avoids manual hyperparameter tuning but depends on training data representativeness.
- Failure signatures: Poor segmentation leads to inaccurate memory banks; incorrect adaptive scaling causes score aggregation errors; insufficient labeled data limits segmentation quality.
- First 3 experiments:
  1. Train segmentation model with only LCE and LDice on labeled data, evaluate segmentation accuracy.
  2. Add LH and Lhist, evaluate improvement in segmentation and consistency across unlabeled images.
  3. Construct memory banks and test anomaly detection performance with and without adaptive scaling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed PSAD method scale when applied to datasets with significantly more object types or classes than the MVTec LOCO AD dataset?
- Basis in paper: [explicit] The paper mentions handling datasets with multiple types of products and suggests using nearest neighbor search in latent space for type classification, but does not extensively evaluate scalability.
- Why unresolved: The scalability of PSAD to larger datasets with more object types or classes is not tested or discussed in depth, leaving uncertainty about its performance and computational efficiency in such scenarios.
- What evidence would resolve it: Testing PSAD on a larger dataset with more object types and classes, comparing performance and computational efficiency with state-of-the-art methods.

### Open Question 2
- Question: What is the impact of using different segmentation models on the anomaly detection performance of PSAD?
- Basis in paper: [explicit] The paper compares PSAD using different FSS models and notes that accurate segmentation correlates with better AD performance, but does not explore the impact of using other types of segmentation models.
- Why unresolved: The paper does not provide a comprehensive analysis of how different segmentation models, especially those not based on FSS, might affect PSAD's performance.
- What evidence would resolve it: Evaluating PSAD with various segmentation models, including those not based on FSS, and analyzing their impact on AD performance.

### Open Question 3
- Question: How does the adaptive scaling strategy in PSAD perform compared to other normalization techniques for aggregating anomaly scores from different memory banks?
- Basis in paper: [explicit] The paper introduces an adaptive scaling strategy to normalize and aggregate anomaly scores from different memory banks, but does not compare it with other normalization techniques.
- Why unresolved: The effectiveness of the adaptive scaling strategy compared to other normalization methods is not explored, leaving questions about its relative performance and robustness.
- What evidence would resolve it: Comparing the adaptive scaling strategy with other normalization techniques in terms of AD performance and robustness across various datasets.

## Limitations
- Limited evaluation scope: Only tested on MVTec LOCO AD with 5 categories, not validated on diverse industrial settings
- Memory bank scalability: Computational and memory requirements for large-scale industrial applications remain unclear
- Adaptive scaling assumptions: Effectiveness depends on training data being representative of test distributions

## Confidence

- **High confidence**: The core mechanism of using few-shot segmentation with histogram matching loss for component classification is well-supported by experimental results (98.1% AUROC on LA detection).
- **Medium confidence**: The three-memory-bank architecture's contribution to performance improvements is demonstrated but not fully isolated.
- **Low confidence**: The claim that PSAD generalizes to arbitrary industrial settings is not empirically supported.

## Next Checks
1. Conduct ablation study on memory banks by removing each individually to quantify their individual contributions to the 98.1% AUROC score.
2. Evaluate PSAD on a different industrial anomaly detection dataset (e.g., magnetic tile or printed circuit board datasets) to assess performance when logical constraints differ from MVTec LOCO AD.
3. Measure memory usage and inference time as image resolution and number of components increase to identify practical limits for industrial deployment.