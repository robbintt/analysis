---
ver: rpa2
title: 'Towards Effective Ancient Chinese Translation: Dataset, Model, and Evaluation'
arxiv_id: '2308.00240'
source_url: https://arxiv.org/abs/2308.00240
tags:
- chinese
- ancient
- translation
- erya
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Erya for ancient Chinese translation, which
  includes dataset, model, and evaluation. The authors collect, clean, and classify
  ancient Chinese materials from various sources, forming the largest ancient Chinese
  resource to date.
---

# Towards Effective Ancient Chinese Translation: Dataset, Model, and Evaluation

## Quick Facts
- arXiv ID: 2308.00240
- Source URL: https://arxiv.org/abs/2308.00240
- Reference count: 38
- Key outcome: Erya model achieves +12.0 BLEU over GPT-3.5 models and better human evaluation than ERNIE Bot in zero-shot ancient Chinese translation

## Executive Summary
This paper presents Erya, a comprehensive system for ancient Chinese to modern Chinese translation that addresses the critical bottleneck of limited training data and evaluation benchmarks. The authors collect and clean the largest ancient Chinese corpus to date (88.8M sentences, 1.94B characters), propose a novel multi-task training methodology combining disyllabic aligned substitution and dual masked language modeling, and establish a new benchmark for evaluating ancient Chinese translation quality across five domains. Erya demonstrates remarkable zero-shot performance, achieving significant improvements over both large language models like GPT-3.5 and specialized ancient Chinese models, with further gains through fine-tuning.

## Method Summary
Erya employs a multi-task pre-training approach that combines disyllabic aligned substitution (DAS) and dual masked language modeling (DMLM) to address the representation gap between ancient and modern Chinese. DAS replaces monosyllabic ancient characters with their disyllabic modern equivalents during training to create more aligned representations, while DMLM applies bidirectional masking to both encoder and decoder for improved bidirectional context understanding. The model is trained using a weighted combination of these objectives (μ=0.3) on the Erya dataset, then fine-tuned on parallel ancient-modern Chinese sentences. The final system achieves strong zero-shot translation performance across five evaluation domains.

## Key Results
- Zero-shot BLEU score of 28.8 on the Erya benchmark, +12.0 BLEU over GPT-3.5 models
- Outperforms specialized models (AnchiBERT, Guwen-UNILM) by significant margins
- Better human evaluation results than ERNIE Bot across faithfulness, expressiveness, elegance, and overall quality metrics
- Fine-tuning provides additional +6.2 BLEU improvement, demonstrating strong transfer capability
- Achieves state-of-the-art performance across all five evaluation domains (medicine, poetry, history, Taoism, Buddhism)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Disyllabic aligned substitution (DAS) improves translation quality by reducing representation mismatch between ancient and modern Chinese words.
- Mechanism: DAS replaces monosyllabic ancient characters with their disyllabic modern equivalents in the source sentence before translation, creating more aligned representations between ancient and modern text during training.
- Core assumption: Most ancient Chinese monosyllabic characters expand to disyllabic modern equivalents in translation, and aligning these during training reduces semantic distance.
- Evidence anchors:
  - [section] "In ancient Chinese translation, disyllabic expansion [4] is a widespread method, i.e., a monosyllabic ancient character is often translated into a disyllabic word (e.g., 解 → 理解 in Figure 1)."
  - [section] "We explore our parallel dataset and find that 99.8% of the pairs contain such alignment, and an average of 62.5% of characters in source sentences can be aligned."

### Mechanism 2
- Claim: Dual masked language modeling (DMLM) with bidirectional decoder improves both ancient and modern Chinese representations.
- Mechanism: DMLM applies bidirectional masking to both encoder and decoder, allowing the model to learn better contextual representations for both ancient and modern Chinese simultaneously.
- Core assumption: Masked language modeling on both ancient and modern Chinese text during training improves the model's ability to understand and generate both language forms.
- Evidence anchors:
  - [section] "We randomly mask tokens at both sides independently with a dynamic probability... As for the pair (X, Y), we denote the masked-token subset as (Xmask, Ymask). Hence, the DMLM objective can be formulated as..."
  - [abstract] "We design two jointly-working tasks: disyllabic aligned substitution (DAS) and dual masked language model (DMLM)."

### Mechanism 3
- Claim: Multi-task training combining DAS and DMLM with balanced loss weights achieves optimal performance.
- Mechanism: The model is trained using a weighted combination of DAS and DMLM objectives, with specific weights (μ = 0.3) determined through ablation studies to balance representation alignment and generation quality.
- Core assumption: The combination of DAS for representation alignment and DMLM for bidirectional context understanding creates synergistic effects that improve translation performance.
- Evidence anchors:
  - [section] "Combining the above objectives 1 and 2, the final training loss is: L = (1 − μ)LDAS + μLDM LM, where μ is a weight to balance two objectives."
  - [section] "From the results shown in Table 5, we set μ as 0.3 for better overall performance."

## Foundational Learning

- Concept: Ancient Chinese to modern Chinese translation differences
  - Why needed here: Understanding the linguistic evolution between ancient and modern Chinese is crucial for designing effective translation models
  - Quick check question: Why do ancient Chinese characters often need to expand to disyllabic words in modern Chinese translation?

- Concept: Masked language modeling objectives
  - Why needed here: DMLM is a core component of the training methodology, requiring understanding of how masking affects language model learning
  - Quick check question: How does bidirectional masking in the decoder differ from standard autoregressive decoding?

- Concept: Parallel corpus construction and alignment
  - Why needed here: The quality and structure of the parallel data directly impacts the effectiveness of DAS and DMLM training tasks
  - Quick check question: What percentage of ancient characters typically have aligned disyllabic modern equivalents in this dataset?

## Architecture Onboarding

- Component map:
  - Data pipeline: Ancient corpus collection → Cleaning → Classification → Benchmark creation
  - Model architecture: Pre-trained base (CPT) → DAS training → DMLM training → Translation fine-tuning
  - Evaluation pipeline: Automatic metrics (BLEU, BERTScore) → Human evaluation → Ablation studies

- Critical path: Dataset preparation → Multi-task pre-training → Zero-shot evaluation → Fine-tuning → Final evaluation

- Design tradeoffs:
  - Model size vs. performance: Erya (145M) vs. GPT-3.5 (175B) shows parameter efficiency
  - Zero-shot vs. fine-tuning: Multi-task training enables strong zero-shot performance but fine-tuning still provides additional gains
  - Masking ratio: Dynamic vs. fixed ratios affect learning effectiveness

- Failure signatures:
  - Poor BLEU scores indicate DAS alignment issues or insufficient training
  - Low BERTScore suggests representation quality problems
  - Human evaluation showing low faithfulness indicates translation accuracy issues

- First 3 experiments:
  1. Test DAS effectiveness by comparing translation with and without disyllabic substitution
  2. Evaluate different masking ratios for DMLM to find optimal balance
  3. Run ablation study on the multi-task loss weight (μ) to determine best combination

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Erya compare to other specialized ancient Chinese translation models when trained on different sizes of parallel corpora?
- Basis in paper: [explicit] The authors mention that Erya outperforms existing models like AnchiBERT and Guwen-UNILM, but they don't explore how model performance scales with varying amounts of training data.
- Why unresolved: The paper doesn't investigate the relationship between training data size and model performance, which is crucial for understanding the efficiency of the Erya model.
- What evidence would resolve it: Conducting experiments with varying sizes of parallel corpora and analyzing the resulting model performance would provide insights into the scalability and data efficiency of the Erya model.

### Open Question 2
- Question: Can the Erya model be effectively applied to translate other ancient languages or historical texts with similar characteristics to ancient Chinese?
- Basis in paper: [inferred] The authors design the Erya model specifically for ancient Chinese translation, but the underlying techniques (DAS and DMLM) might be applicable to other ancient languages with similar linguistic features.
- Why unresolved: The paper doesn't explore the generalizability of the Erya model to other ancient languages or historical texts, which could be valuable for expanding the model's applicability.
- What evidence would resolve it: Testing the Erya model on translation tasks for other ancient languages or historical texts with similar characteristics would demonstrate its potential for broader application.

### Open Question 3
- Question: How does the Erya model perform in translating ancient Chinese texts with complex grammatical structures or rare vocabulary?
- Basis in paper: [explicit] The authors mention that ancient Chinese texts have unique grammatical structures and vocabulary, but they don't specifically address how the model handles these challenges.
- Why unresolved: The paper doesn't provide a detailed analysis of the model's performance on ancient Chinese texts with complex grammatical structures or rare vocabulary, which is important for assessing its practical utility.
- What evidence would resolve it: Evaluating the Erya model on a dataset of ancient Chinese texts with varying levels of grammatical complexity and vocabulary rarity would reveal its strengths and limitations in handling such texts.

## Limitations
- Limited Ancient Data: The Erya dataset, while largest available, still represents a small fraction of ancient Chinese textual corpus
- Domain Specificity: Evaluation focuses on five specific domains, potentially missing broader translation scenarios
- Evaluation Methodology: Human evaluation lacks detailed demographic information and sample size specifications

## Confidence
- High Confidence: Multi-task training methodology is technically sound with statistically significant improvements over baselines
- Medium Confidence: Claims about dataset size and human evaluation results require additional verification
- Low Confidence: Comparison with ERNIE Bot needs larger, more diverse human evaluator samples

## Next Checks
1. Cross-domain validation: Test Erya on additional ancient Chinese domains not included in the original benchmark to assess generalizability beyond the five evaluated domains
2. Long-form translation evaluation: Evaluate Erya's performance on longer ancient Chinese texts to assess its capability for handling complex discourse structures and context-dependent translation challenges
3. Comparative ablation study: Conduct a more extensive ablation study comparing Erya's DAS and DMLM components against alternative training strategies to isolate the specific contributions of each mechanism to overall performance