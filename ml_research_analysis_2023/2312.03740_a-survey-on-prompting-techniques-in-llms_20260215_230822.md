---
ver: rpa2
title: A Survey on Prompting Techniques in LLMs
arxiv_id: '2312.03740'
source_url: https://arxiv.org/abs/2312.03740
tags:
- prompting
- language
- prompts
- llms
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of prompting techniques
  in autoregressive large language models (LLMs), offering a taxonomy that categorizes
  these methods based on human effort and prompt objectives. The paper addresses the
  challenge of harnessing the full potential of LLMs, which have emerged as powerful
  tools for natural language processing but are difficult to fine-tune due to their
  large size.
---

# A Survey on Prompting Techniques in LLMs

## Quick Facts
- arXiv ID: 2312.03740
- Source URL: https://arxiv.org/abs/2312.03740
- Reference count: 40
- Primary result: Comprehensive taxonomy of prompting techniques in autoregressive LLMs based on human effort and prompt objectives

## Executive Summary
This paper provides a systematic survey of prompting techniques for autoregressive large language models (LLMs), addressing the challenge of harnessing their full potential without extensive fine-tuning. The authors present a taxonomy that categorizes prompting methods based on human effort (hand-crafted vs. automated) and prompt objectives (task-based, generate-auxiliary, and resource/tools augmented). The survey covers a wide range of techniques including chain of thought prompting, continuous prompt optimization, and resource-augmented approaches like program-aided language models. The paper identifies several open problems in the field including handling structured data, answer engineering, and prompt injection vulnerabilities.

## Method Summary
The paper presents a comprehensive survey of existing literature on prompting techniques in autoregressive LLMs, organizing the findings into a systematic taxonomy. The authors categorize techniques based on human effort (hand-crafted vs. automated prompts) and prompt objectives (task-based, generate-auxiliary, and resource/tools augmented). The survey covers discrete and continuous prompt optimization methods, generate-auxiliary approaches like Chain-of-Thought prompting, and resource-augmented techniques such as program-aided language models. The paper also identifies open problems in the field and suggests directions for future research.

## Key Results
- Presents a comprehensive taxonomy of prompting techniques in autoregressive LLMs
- Identifies three main categories of prompts: task-based, generate-auxiliary, and resource/tools augmented
- Surveys automated prompting methods including discrete and continuous optimization techniques
- Highlights open problems including structured data handling, answer engineering, and prompt injection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompting replaces fine-tuning for LLMs by leveraging their emergent in-context learning ability.
- Mechanism: Autoregressive LLMs can generate correct outputs when provided with a task description and/or demonstration examples, bypassing the need to update model parameters for each new task.
- Core assumption: The LLM has been pre-trained on sufficiently large and diverse data to contain relevant knowledge and reasoning capabilities.
- Evidence anchors:
  - [abstract] "LLMs have shown great promise for a variety of downstream tasks owing to their vast parameters and huge datasets that they are pre-trained on."
  - [section II.A] "With the introduction of Transformers [1], researchers were able to train more deeper networks... These PLMs are often then fine-tuned for specific downstream tasks with human-annotated data."
  - [corpus] Weak - neighbors discuss prompt engineering surveys but not the specific mechanism of in-context learning replacing fine-tuning.
- Break condition: If the LLM lacks the scale or diversity of pre-training data to exhibit emergent abilities like in-context learning.

### Mechanism 2
- Claim: Automated prompting methods can discover more effective prompts than manual hand-crafted ones.
- Mechanism: Techniques like discrete and continuous prompt optimization search the space of possible prompts to find ones that yield better performance, often by treating parts of the prompt as learnable parameters.
- Core assumption: There exist prompts that are more effective than those created by intuition alone, and these can be found through systematic search or optimization.
- Evidence anchors:
  - [section IV.A.2.a] "Wallace et al. [35] proposed a gradient-based search technique to find discrete prompts... Other approaches score the prompt using another LM."
  - [section IV.A.2.b] "Prefix Tuning [41] involves the addition of task-specific prefixes... Then, the log-likelihood objective is optimized, with the LLM parameters frozen while only updating the prefix parameters."
  - [corpus] Weak - neighbors discuss prompt engineering but do not provide specific evidence for the superiority of automated over manual methods.
- Break condition: If the search space is too large or the optimization objective is poorly defined, leading to local optima or ineffective prompts.

### Mechanism 3
- Claim: Generate-auxiliary prompts like Chain-of-Thought (CoT) elicit reasoning abilities in LLMs to solve complex tasks.
- Mechanism: By prompting the LLM to generate intermediate reasoning steps before the final answer, the model can decompose complex problems and leverage its knowledge more effectively.
- Core assumption: LLMs have latent reasoning capabilities that can be activated by appropriate prompting strategies, even if these capabilities are not apparent with standard prompting.
- Evidence anchors:
  - [section IV.B.2.a] "Wei et al. [20] popularized the term 'Chain of thought prompting'... Chain of thought prompting finds its usefulness in arithmetic reasoning, commonsense reasoning, and symbolic reasoning tasks."
  - [section IV.B.2.a] "Zero-shot CoT [21] can be considered the zero-shot version of CoT... 'Let's think step by step' is added to the prompt, and with sufficiently large language models, we get a series of reasoning steps leading to correct answers."
  - [corpus] Weak - neighbors discuss prompt engineering but do not provide specific evidence for the reasoning capabilities unlocked by CoT.
- Break condition: If the LLM does not have the scale or training to support the kind of reasoning the prompt is trying to elicit.

## Foundational Learning

- Concept: Transformer architecture and self-attention
  - Why needed here: Understanding how LLMs process input and generate output is crucial for designing effective prompts and interpreting their behavior.
  - Quick check question: How does the self-attention mechanism allow a transformer to capture long-range dependencies in a sequence?

- Concept: Pre-training and fine-tuning paradigm
  - Why needed here: Prompting emerged as an alternative to fine-tuning, so understanding the limitations of fine-tuning large models motivates the need for prompting.
  - Quick check question: Why is fine-tuning a 175B parameter model like GPT-3 infeasible for many practitioners?

- Concept: In-context learning and emergent abilities
  - Why needed here: The core premise of prompting is that LLMs can learn from the context provided in the prompt without parameter updates.
  - Quick check question: What evidence is there that in-context learning is an emergent ability that arises only in sufficiently large models?

## Architecture Onboarding

- Component map:
  LLM -> Prompt Template -> Prompt Optimization Method (optional) -> External Tools/Resources (optional)

- Critical path:
  1. Define the task and desired output format
  2. Design an initial prompt template (hand-crafted or automated)
  3. If using automated prompting, optimize the prompt on a validation set
  4. Evaluate the final prompt on a held-out test set
  5. Iterate on the prompt design or optimization method as needed

- Design tradeoffs:
  - Hand-crafted vs. automated prompts: Manual effort vs. potential performance gains
  - Discrete vs. continuous prompts: Interpretability vs. flexibility and potential performance
  - Task-based vs. generate-auxiliary vs. resource-augmented: Simplicity vs. capability for complex tasks

- Failure signatures:
  - Sub-optimal outputs: The prompt is not effectively guiding the LLM
  - Mode collapse: The LLM is generating repetitive or nonsensical outputs
  - High variance across runs: The LLM's output is highly sensitive to the prompt or random seed

- First 3 experiments:
  1. Evaluate a few-shot prompt on a simple classification task (e.g., sentiment analysis) and compare to a zero-shot prompt
  2. Implement a discrete prompt optimization method (e.g., paraphrasing) and evaluate its impact on a few-shot prompt
  3. Design a Chain-of-Thought prompt for a reasoning task (e.g., arithmetic word problems) and compare to a standard few-shot prompt

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively handle structured data (e.g., tables, graphs) in prompting techniques for autoregressive LLMs?
- Basis in paper: [explicit] The paper identifies handling structured data as an open problem, noting that many downstream NLP tasks involve structured formats beyond plain text.
- Why unresolved: Existing approaches like GraphPrompt involve pre-training with graph structures, which is often infeasible, and other methods are tailored for masked language models rather than autoregressive LLMs.
- What evidence would resolve it: Development and demonstration of a prompting technique that effectively handles structured data in autoregressive LLMs without requiring pre-training or being limited to specific model types.

### Open Question 2
- Question: How can we mitigate the issue of sub-optimal prompts in large-scale LLMs, especially when using continuous prompts that require training a significant number of parameters?
- Basis in paper: [explicit] The paper identifies addressing sub-optimal prompts as a significant challenge, particularly with continuous prompts that become resource-intensive as LLMs grow in size.
- Why unresolved: Continuous prompts, while effective, require training a large number of parameters, which is costly and often infeasible due to resource limitations, especially for closed-source LLMs.
- What evidence would resolve it: A solution that effectively addresses sub-optimal prompts without requiring extensive parameter training or being limited to open-source LLMs.

### Open Question 3
- Question: How can we develop a universal approach to answer engineering that works across various downstream tasks without being task-specific or requiring instruction tuning of the LLM?
- Basis in paper: [explicit] The paper identifies answer engineering as an open problem, noting that existing techniques are often task-specific and not universally applicable.
- Why unresolved: Current approaches to answer engineering, such as exact matching or using regular expressions, are limited in their applicability and often require instruction tuning of the LLM, which is not always feasible.
- What evidence would resolve it: A universal approach to answer engineering that can effectively extract meaningful answers from LLM-generated text across various downstream tasks without requiring instruction tuning or being task-specific.

## Limitations

- Focus on autoregressive LLMs may overlook prompting techniques specific to other model architectures
- Taxonomy may not capture all emerging prompting paradigms, particularly those involving multi-modal applications
- Limited analysis of computational overhead and latency introduced by more complex prompting strategies

## Confidence

- **High Confidence**: The survey's taxonomy of prompting techniques based on human effort and prompt objectives is well-grounded in the literature and provides a clear organizational framework.
- **Medium Confidence**: The claims regarding the effectiveness of generate-auxiliary prompts like Chain-of-Thought are based on promising results but may be sensitive to the specific tasks and model scales examined.
- **Low Confidence**: The assertion that prompting universally replaces fine-tuning for all LLM applications is overstated, as certain specialized tasks may still benefit from parameter updates.

## Next Checks

1. Conduct a controlled experiment comparing hand-crafted few-shot prompts against optimized discrete prompts on at least three diverse tasks (e.g., classification, reasoning, generation) using a consistent LLM.
2. Implement and evaluate a continuous prompt optimization method (e.g., prefix tuning) on a structured data task to assess its effectiveness compared to traditional fine-tuning approaches.
3. Design a benchmark suite that tests prompting techniques across varying model scales (e.g., 1B, 10B, 100B+ parameters) to identify at which scales different prompting strategies become effective or ineffective.