---
ver: rpa2
title: 'SqueezeLLM: Dense-and-Sparse Quantization'
arxiv_id: '2306.07629'
source_url: https://arxiv.org/abs/2306.07629
tags:
- quantization
- arxiv
- squeezellm
- values
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Language Models (LLMs) face deployment challenges due to
  their massive memory requirements, especially for generative tasks. SqueezeLLM addresses
  this by using quantization to reduce model size and improve inference speed.
---

# SqueezeLLM: Dense-and-Sparse Quantization

## Quick Facts
- arXiv ID: 2306.07629
- Source URL: https://arxiv.org/abs/2306.07629
- Authors: [not provided]
- Reference count: 40
- Key outcome: Achieves up to 2.1x better perplexity than state-of-the-art methods at the same memory cost with 3-bit quantization, and speeds up inference by up to 2.3x on an A6000 GPU

## Executive Summary
SqueezeLLM addresses the memory bottleneck in LLM inference by introducing sensitivity-based non-uniform quantization and Dense-and-Sparse decomposition. The method uses Fisher information to identify sensitive weight values and allocates more bits to them during quantization, while extracting outliers into a sparse format. This approach enables ultra-low 3-bit quantization with minimal performance loss, achieving significant improvements in both perplexity and inference speed compared to existing methods.

## Method Summary
SqueezeLLM combines sensitivity-based non-uniform quantization with Dense-and-Sparse decomposition to achieve efficient LLM inference. The method computes Fisher information to identify weight sensitivity, then performs weighted k-means clustering for non-uniform quantization. Outliers and sensitive values are extracted into sparse FP16 matrices, reducing the quantization range for the dense part. Custom CUDA kernels handle the hybrid dense-sparse matrix multiplication, enabling efficient inference with compressed weights while maintaining full-precision activations.

## Key Results
- Up to 2.1x better perplexity than state-of-the-art methods at the same memory cost
- Inference speedup of up to 2.3x on A6000 GPU compared to baseline methods
- Achieves ultra-low 3-bit quantization with minimal performance degradation
- Extracts only 0.45% of weight values as sparse components for significant accuracy gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SqueezeLLM reduces memory bandwidth bottleneck by quantizing only the weights while keeping activations in full precision.
- Mechanism: In generative LLM inference, the arithmetic intensity is extremely low (ratio of compute to memory operations is just 2), making memory bandwidth the primary bottleneck rather than compute. By reducing weight precision from 16-bit to 3-bit, memory traffic for loading weights is reduced by ~5x, directly improving inference speed without needing to accelerate computation.
- Core assumption: The performance bottleneck is indeed memory bandwidth, not compute operations, specifically for single-batch generative inference.
- Evidence anchors:
  - [abstract] "the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference"
  - [section] "The results are illustrated in Fig. 2, where we plot the runtime for different bit precisions used for the weight values... the runtime goes down linearly as we reduce the bit precision"
  - [corpus] Weak - no direct corpus evidence supporting this specific mechanism
- Break condition: If the arithmetic intensity were higher (e.g., with larger batch sizes or encoder-only models), compute would become the bottleneck and weight-only quantization would not yield the same speedups.

### Mechanism 2
- Claim: Sensitivity-based non-uniform quantization improves quantization accuracy by allocating more bits to sensitive weight values using second-order information.
- Mechanism: Instead of uniform quantization where quantization bins are equally spaced, SqueezeLLM uses sensitivity-based weighted k-means clustering. Each weight value is weighted by its Fisher information (diagonal approximation of Hessian), which measures how much perturbation affects the final loss. This pulls quantization centroids closer to sensitive values, reducing quantization error where it matters most.
- Core assumption: The Fisher information (second-order derivative) accurately captures the sensitivity of each weight to the final model performance.
- Evidence anchors:
  - [abstract] "sensitivity-based non-uniform quantization, which searches for the optimal bit precision assignment based on second-order information"
  - [section] "The objective of quantizing a model is to represent the model weights with low-bit precision while ensuring minimal perturbation in the model output... we need to minimize the overall perturbation with respect to the final loss term"
  - [corpus] Weak - no direct corpus evidence supporting this specific mechanism
- Break condition: If the diagonal approximation of Fisher information is poor (i.e., if cross-weight interactions are significant), the sensitivity weighting would be inaccurate and the quantization would not achieve the claimed improvements.

### Mechanism 3
- Claim: Dense-and-Sparse decomposition handles outliers by extracting them into a sparse full-precision format, improving quantization resolution for the dense part.
- Mechanism: Weight matrices in LLMs contain outliers that significantly increase the quantization range. SqueezeLLM decomposes weights into a dense matrix (excluding outliers) and a sparse matrix (containing outliers). The dense part, now with a much smaller range, can be quantized more accurately. The sparse part is stored in efficient formats (e.g., CSR) with minimal overhead since only ~0.45% of values are extracted as outliers.
- Core assumption: The number of outlier values is small enough that the sparse storage overhead is negligible compared to the accuracy gains from reducing the quantization range.
- Evidence anchors:
  - [abstract] "the Dense-and-Sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format"
  - [section] "we observe that weight matrices in many LLMs contain significant outliers, making low-bit precision quantization extremely challenging... by only extracting 0.45% of the weight values as the sparse component, we further improve the perplexity"
  - [corpus] Weak - no direct corpus evidence supporting this specific mechanism
- Break condition: If the percentage of outliers were much higher (e.g., >5%), the sparse storage overhead would become significant and potentially negate the benefits of improved quantization accuracy.

## Foundational Learning

- Concept: Memory wall problem in deep learning
  - Why needed here: Understanding why memory bandwidth, not compute, is the bottleneck in LLM inference is crucial for grasping why SqueezeLLM's approach of weight-only quantization is effective
  - Quick check question: What is the arithmetic intensity of generative LLM inference, and why does this make it memory-bound rather than compute-bound?

- Concept: Non-uniform quantization and k-means clustering
  - Why needed here: SqueezeLLM uses sensitivity-based non-uniform quantization, which requires understanding how k-means clustering can be used to find optimal quantization levels when the weight distribution is non-uniform
  - Quick check question: How does sensitivity-based weighted k-means clustering differ from standard k-means in the context of quantization?

- Concept: Second-order optimization and Fisher information
  - Why needed here: The sensitivity metric in SqueezeLLM is based on Fisher information (diagonal approximation of Hessian), which requires understanding second-order derivatives and their role in measuring parameter importance
  - Quick check question: Why does SqueezeLLM use Fisher information instead of computing the full Hessian, and how is Fisher information calculated?

## Architecture Onboarding

- Component map: Sensitivity analysis module -> Non-uniform quantization module -> Dense-and-Sparse decomposition module -> Custom CUDA kernels -> Integration layer
- Critical path: Sensitivity analysis → Non-uniform quantization → Dense-and-Sparse decomposition → Custom kernel deployment → Inference
- Design tradeoffs:
  - Non-uniform vs uniform quantization: Non-uniform provides better accuracy but requires lookup tables and dequantization overhead
  - Sparsity level selection: Higher sparsity (0.45%) gives better accuracy but adds latency overhead from sparse kernels
  - Group size in quantization: SqueezeLLM avoids grouping for simplicity and efficiency, unlike GPTQ/AWQ which use grouping with permutation
- Failure signatures:
  - If sensitivity analysis produces incorrect Fisher information, quantization accuracy will degrade significantly
  - If outlier detection thresholds are poorly chosen, either too many values will be extracted (wasting memory) or too few (leaving quantization range too large)
  - If custom kernels have poor memory access patterns, the latency benefits of quantization will be lost
- First 3 experiments:
  1. Run sensitivity analysis on a small LLaMA-7B layer and verify that the top-20 sensitive values (by Fisher information) are correctly identified
  2. Perform 3-bit non-uniform quantization with and without sensitivity weighting on a single layer and compare perplexity impact
  3. Test Dense-and-Sparse decomposition by quantizing a layer with and without outlier extraction, measuring both perplexity and memory usage

## Open Questions the Paper Calls Out
None explicitly stated in the provided input.

## Limitations
- The method's effectiveness on encoder-only or encoder-decoder architectures is not thoroughly evaluated
- Performance variation with sequence lengths beyond the tested 128 and 1024 tokens is not explored
- Different grouping strategies and their comparison with Dense-and-Sparse decomposition are not extensively investigated

## Confidence

**High confidence**: The fundamental premise that memory bandwidth is the bottleneck for single-batch generative LLM inference is well-supported by empirical evidence showing linear runtime reduction with bit precision. The Dense-and-Sparse decomposition concept is also well-validated through ablation studies showing consistent perplexity improvements.

**Medium confidence**: The sensitivity-based non-uniform quantization approach shows promise through ablation studies, but the reliance on Fisher information and weighted k-means clustering introduces complexity that may not generalize perfectly to all model architectures. The optimal sparsity level of 0.45% appears effective for tested models but may require tuning for others.

**Low confidence**: Claims about the negligible overhead of sparse storage and the universal effectiveness of the sensitivity metric across different model families lack comprehensive validation. The paper doesn't explore scenarios where the number of outliers might be substantially higher or where Fisher information might poorly capture weight importance.

## Next Checks

1. **Sensitivity metric robustness test**: Apply SqueezeLLM to a diverse set of LLM architectures (BERT, OPT, GPT-NeoX) and measure whether the Fisher information-based sensitivity ranking consistently identifies the most impactful weights for quantization across different model families and tasks.

2. **Outlier distribution analysis**: Systematically vary the outlier extraction threshold from 0.1% to 5% and measure the trade-off between perplexity degradation and sparse storage overhead across multiple model sizes to determine the sensitivity of performance to outlier percentage.

3. **Hardware portability evaluation**: Deploy SqueezeLLM on multiple GPU architectures (A100, H100, RTX 4090) and measure whether the reported 2.3x speedup is reproducible or if performance varies significantly with memory bandwidth characteristics and CUDA core configurations.