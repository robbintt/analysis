---
ver: rpa2
title: 'ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using
  Floating-Point Formats'
arxiv_id: '2307.09782'
source_url: https://arxiv.org/abs/2307.09782
tags:
- quantization
- activation
- arxiv
- weight
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores floating-point (FP) quantization for post-training
  large language models (LLMs), motivated by limitations of integer quantization in
  handling activation outliers and NVIDIA's H100 hardware support for FP8. It compares
  FP8/FP4 quantization against INT8/INT4 for both weight and activation across LLaMA
  and OPT model families (1-30B parameters), using GPTQ and LoRC methods.
---

# ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats

## Quick Facts
- arXiv ID: 2307.09782
- Source URL: https://arxiv.org/abs/2307.09782
- Reference count: 39
- This paper shows FP8 activation quantization consistently outperforms INT8, with larger gains in bigger models, and FP4 weights are comparable or better than INT4.

## Executive Summary
This paper explores floating-point (FP) quantization for post-training large language models (LLMs), motivated by limitations of integer quantization in handling activation outliers and NVIDIA's H100 hardware support for FP8. It compares FP8/FP4 quantization against INT8/INT4 for both weight and activation across LLaMA and OPT model families (1-30B parameters), using GPTQ and LoRC methods. The key finding is that FP8 activation consistently outperforms INT8, with larger gains in bigger models, and FP4 weights are comparable or better than INT4. LoRC significantly reduces quantization error, especially in smaller models. The paper also proposes power-of-2 scaling constraints to optimize hardware efficiency, showing negligible quality loss. Overall, FP quantization offers better accuracy than integer methods, with practical advantages on FP8-capable hardware.

## Method Summary
The paper implements post-training quantization using the GPTQ method with optional Low Rank Compensation (LoRC) for LLaMA and OPT models. It compares floating-point formats (FP8 and FP4) against integer formats (INT8 and INT4) for both weight and activation quantization. The evaluation uses perplexity scores on Wikitext-2, PTB, and C4 datasets. Key methodological innovations include LoRC for error compensation and power-of-2 scaling constraints for hardware efficiency.

## Key Results
- FP8 activation quantization consistently outperforms INT8 across all model sizes tested
- LoRC significantly reduces quantization error, particularly benefiting smaller models
- Power-of-2 scaling constraints show negligible quality loss while enabling hardware efficiency
- Larger models (>1B parameters) show the most significant improvements from FP8 activation

## Why This Works (Mechanism)

### Mechanism 1
FP8 activation quantization preserves more information than INT8 because floating-point formats can dynamically adjust the decimal point to handle outliers. In FP8, the exponent bits allow dynamic range scaling, so extreme values don't skew the quantization of the main data distribution. In INT8, fixed step sizes cause outlier values to compress the effective resolution of the majority of activations. Core assumption: The activation distributions in LLMs have significant skewness and outliers that would otherwise degrade INT8 quantization.

### Mechanism 2
LoRC reduces quantization error by reconstructing the low-rank structure of the quantization error matrix and adding it back to the quantized weights. After initial quantization, LoRC computes the error matrix, performs SVD, approximates it with two low-rank matrices, and adds this approximation to the quantized weights, effectively compensating for lost information. Core assumption: The quantization error has a low-rank structure that can be captured and compensated.

### Mechanism 3
Power-of-2 scaling constraints enable efficient hardware casting from FP4 to FP8 by replacing multiplications with bit-shifts. By constraining scale factors to powers of 2, the hardware can implement scaling operations as simple bit-shifts rather than general multiplications, reducing computational overhead during FP4-to-FP8 conversion. Core assumption: Hardware implementations of FP4-to-FP8 casting are significantly optimized when scale factors are powers of 2.

## Foundational Learning

- Concept: Floating-point representation (ExMy format)
  - Why needed here: Understanding how FP8/FP4 differ from INT8/INT4 is essential to grasp why they handle outliers better
  - Quick check question: In FP8-E4M3 format, how many bits are allocated to exponent and mantissa respectively?

- Concept: Quantization error and its sources
  - Why needed here: The paper's improvements rely on reducing quantization error through better formats and compensation methods
  - Quick check question: What are the two main sources of quantization error in post-training quantization?

- Concept: Low-rank matrix approximation
  - Why needed here: LoRC's mechanism depends on capturing quantization error structure using SVD and low-rank reconstruction
  - Quick check question: Why might quantization error matrices be approximately low-rank in neural network weights?

## Architecture Onboarding

- Component map: Pre-trained weights -> GPTQ quantization -> (Optional: Power-of-2 scaling constraint) -> (Optional: LoRC) -> Hardware casting layer -> Evaluation pipeline

- Critical path:
  1. Load pre-trained weights
  2. Apply GPTQ with chosen precision (W4A8, W8A8, etc.)
  3. If W4A8, apply power-of-2 scaling constraint
  4. Apply LoRC if enabled
  5. Evaluate on benchmark datasets

- Design tradeoffs:
  - FP8 vs INT8: Better accuracy vs potentially higher hardware cost
  - LoRC vs no LoRC: Lower error vs increased computation during quantization
  - Power-of-2 scaling vs arbitrary scaling: Hardware efficiency vs minimal quality loss
  - Model size impact: Larger models benefit more from FP8 activation than smaller ones

- Failure signatures:
  - Quality degradation when LoRC dimension is too small
  - Unexpected perplexity increases when applying power-of-2 constraints without LoRC
  - No performance gain from FP8 activation when activation distributions are already symmetric

- First 3 experiments:
  1. Compare W16A16 (baseline) vs W8A8-INT vs W8A8-FP on LLaMA-7B to verify FP8 activation advantage
  2. Enable LoRC on W4A8 configuration and measure improvement, particularly for smaller models
  3. Apply power-of-2 scaling constraint with and without LoRC to assess quality impact on LLaMA-13B

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of FP8 activation quantization scale with model size beyond 30 billion parameters? The paper shows FP8 activation consistently outperforms INT8, with larger gains in models beyond one billion parameters, but only tests up to 30B parameters. Empirical evaluation of FP8 vs INT8 activation quantization on models larger than 30B parameters (e.g., 70B+ parameter models) with perplexity measurements would resolve this.

### Open Question 2
What is the optimal low-rank dimension for the LoRC method across different model families and sizes? The paper uses different LoRC dimensions for LLaMA (8) vs OPT (16, 32, 40, 56) but states dimensions above 8 don't significantly impact quantization error. Systematic comparison of LoRC performance across a range of low-rank dimensions for various model sizes and families would resolve this.

### Open Question 3
How do power-of-2 scaling constraints affect inference latency in practice on H100 hardware? The paper proposes power-of-2 scaling constraints (M1 and M2) and shows negligible quality loss, but doesn't measure actual latency impact. Benchmark measurements of inference latency with and without power-of-2 scaling constraints on actual H100 hardware across various model sizes would resolve this.

## Limitations
- Limited model diversity: Experiments focus primarily on LLaMA and OPT architectures
- Dataset scope: Only three evaluation datasets used despite claims about robustness across diverse tasks
- Hardware specificity: Results may be highly dependent on NVIDIA H100's specific FP8 implementation

## Confidence
- **High Confidence**: FP8 activation consistently outperforms INT8 across model sizes
- **Medium Confidence**: LoRC effectiveness, particularly for smaller models
- **Low Confidence**: Power-of-2 scaling constraints providing negligible quality loss

## Next Checks
1. Test FP quantization across diverse model architectures (BLOOM, Falcon, Mistral) to verify generalizability beyond LLaMA/OPT
2. Evaluate on task-specific benchmarks (SuperGLUE, BBH) to confirm downstream task performance matches perplexity improvements
3. Profile actual H100 hardware performance to measure real-world benefits of power-of-2 scaling constraints versus theoretical claims