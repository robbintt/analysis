---
ver: rpa2
title: Investigation of Self-supervised Pre-trained Models for Classification of Voice
  Quality from Speech and Neck Surface Accelerometer Signals
arxiv_id: '2308.03226'
source_url: https://arxiv.org/abs/2308.03226
tags:
- speech
- features
- voice
- glottal
- signal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the classification of voice quality (breathy,
  modal, and pressed) using features derived from self-supervised pre-trained models
  (wav2vec2-BASE, wav2vec2-LARGE, and HuBERT) and two types of input signals: raw
  speech and neck surface accelerometer (NSA) signals. The study compares these pre-trained
  model-based features with conventional features (spectrogram, mel-spectrogram, MFCCs,
  i-vector, and x-vector) and evaluates two classifiers: SVM and CNN.'
---

# Investigation of Self-supervised Pre-trained Models for Classification of Voice Quality from Speech and Neck Surface Accelerometer Signals

## Quick Facts
- arXiv ID: 2308.03226
- Source URL: https://arxiv.org/abs/2308.03226
- Reference count: 40
- Primary result: Pre-trained model-based features (especially HuBERT) outperform conventional features for voice quality classification, with NSA signals providing better performance than speech signals.

## Executive Summary
This study investigates voice quality classification (breathy, modal, and pressed) using features derived from self-supervised pre-trained models (wav2vec2-BASE, wav2vec2-LARGE, and HuBERT) and two input modalities: raw speech and neck surface accelerometer (NSA) signals. The research compares these pre-trained model features with conventional features (spectrogram, mel-spectogram, MFCCs, i-vector, and x-vector) and evaluates two classifiers: SVM and CNN. Glottal source waveforms are estimated from both signal types using quasi-closed phase (QCP) glottal inverse filtering and zero frequency filtering (ZFF) methods. Results show NSA signals provide better classification performance than speech signals, and pre-trained model-based features outperform conventional features for both input types. HuBERT features showed an absolute accuracy improvement of 3%-6% compared to conventional features.

## Method Summary
The study used simultaneously recorded speech and NSA signals from 31 female speakers producing five vowels in three voice qualities (1395 utterances total). Features were extracted from three pre-trained models (wav2vec2-BASE, wav2vec2-LARGE, HuBERT) and five conventional features. Two glottal source estimation methods (QCP and ZFF) were applied to both signal types. Classification was performed using SVM (RBF kernel) and CNN (3 conv layers + dense layers) with leave-one-speaker-out cross-validation. The dataset was downsampled to 16 kHz and features were extracted for both raw signals and glottal source waveforms.

## Key Results
- NSA signals provided higher classification accuracy than speech signals across all feature types
- Pre-trained model-based features outperformed conventional features for both speech and NSA inputs
- HuBERT features performed better than wav2vec2-BASE and wav2vec2-LARGE features
- Initial layers of pre-trained models showed better performance than later layers for voice quality classification
- The two classifiers (SVM and CNN) performed equally well for all pre-trained model-based features

## Why This Works (Mechanism)

### Mechanism 1
- Pre-trained self-supervised models outperform conventional hand-crafted features for voice quality classification because they were trained on large-scale unsupervised audio data, learning rich, generic speech representations containing phoneme-related and speaker-specific information useful for downstream tasks like voice quality classification.

### Mechanism 2
- NSA signals provide better classification performance than acoustic speech signals because NSA sensors capture direct vibrations of the vocal folds below the glottis, making them less affected by vocal tract resonances and more directly reflective of glottal source characteristics.

### Mechanism 3
- Features from initial layers of pre-trained models perform better than later layers for voice quality classification because initial layers capture generic, low-level acoustic features broadly useful for tasks like voice quality classification, while later layers are fine-tuned for more specific tasks like phoneme identity in ASR.

## Foundational Learning

- Concept: Self-supervised learning and pre-trained models (wav2vec2, HuBERT)
  - Why needed here: The study compares these models' feature extraction ability to conventional features for voice quality classification
  - Quick check question: What is the main difference between wav2vec2 and HuBERT in terms of their training objectives and how might that affect their suitability for voice quality tasks?

- Concept: Voice quality and phonation types (breathy, modal, pressed)
  - Why needed here: The classification task is defined over these three voice qualities
  - Quick check question: What are the key acoustic differences between breathy, modal, and pressed phonation types, and how do they relate to glottal source characteristics?

- Concept: Glottal source estimation methods (QCP, ZFF)
  - Why needed here: The study uses these methods to extract glottal source waveforms from both speech and NSA signals
  - Quick check question: How do QCP and ZFF differ in their approach to estimating the glottal source waveform, and what are the trade-offs between them?

## Architecture Onboarding

- Component map: Raw signal → (optional) glottal source estimation → feature extraction → classification → evaluation
- Critical path: Raw signal → (optional) glottal source estimation → feature extraction → classification → evaluation
- Design tradeoffs:
  - Raw signal vs. glottal source: Glottal source may be more discriminative but involves estimation error; raw signal is noisier but direct
  - Pre-trained models vs. conventional features: Pre-trained models are more adaptable but require careful layer selection; conventional features are interpretable but less generalizable
  - SVM vs. CNN: SVM is simpler and may generalize better with small data; CNN can learn complex patterns but is more data-hungry
- Failure signatures:
  - Low accuracy on NSA despite theoretical advantages: NSA signal quality issues or mismatch between NSA and model expectations
  - Later layers outperform initial layers: Voice quality is more linguistically or phonetically defined than assumed
  - Conventional features outperform pre-trained models: Dataset too small or domain mismatch for pre-trained models
- First 3 experiments:
  1. Replicate the main result: Compare HuBERT features on NSA-QCP vs. x-vector on NSA-QCP for the same classifier to quantify the claimed 3-6% improvement
  2. Layer-wise analysis: Train classifiers using only the first layer, middle layers, and final layer of HuBERT on NSA signals to confirm initial layers are best
  3. Modality ablation: Train on speech only, NSA only, and both modalities concatenated to see if NSA truly adds unique information beyond speech

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance and generalization capabilities of self-supervised pre-trained models (wav2vec2 and HuBERT) compare to other pre-trained models or traditional feature extraction methods for voice quality classification tasks beyond the three investigated (breathy, modal, and pressed)?
- Basis in paper: The paper compares wav2vec2 and HuBERT to conventional features but does not explore other pre-trained models or voice quality classes
- Why unresolved: The study is limited to a specific set of pre-trained models and voice quality classes
- What evidence would resolve it: Comparative studies involving additional pre-trained models and a wider range of voice quality classes would provide insight into generalizability

### Open Question 2
- Question: How do the glottal source waveforms estimated by the QCP and ZFF methods compare in terms of their ability to capture subtle differences in voice quality across different speakers and speaking styles?
- Basis in paper: The paper uses both QCP and ZFF methods but does not provide a detailed comparison of their performance across diverse speaker populations
- Why unresolved: The study focuses on a specific dataset and does not explore robustness across different speakers and speaking styles
- What evidence would resolve it: Comparative analyses using diverse datasets and perceptual evaluations would help determine strengths and limitations of each method

### Open Question 3
- Question: How does the use of NSA signals as input compare to other non-acoustic modalities (e.g., electroglottography, ultrasound) in terms of voice quality classification accuracy and robustness to noise and artifacts?
- Basis in paper: The paper compares NSA signals to acoustic speech signals but does not explore other non-acoustic modalities
- Why unresolved: The study focuses on a single non-acoustic modality
- What evidence would resolve it: Comparative studies using multiple non-acoustic modalities with evaluations of robustness to noise would provide insights into relative strengths and limitations

## Limitations

- Results are based on a relatively small, controlled dataset (31 female speakers producing five vowels) which may not capture full variability of natural speech
- The NSA signal processing pipeline and glottal source estimation methods are complex and their performance directly impacts downstream classification accuracy
- The study does not investigate robustness of models to noise or speaker variability beyond the cross-validation scheme
- Perceptual relevance of learned features is not validated
- Trade-offs between model complexity and classification performance are not explored in detail

## Confidence

**High Confidence**: The claim that NSA signals provide better classification performance than speech signals is supported by clear quantitative results across multiple feature types and classifiers with direct experimental isolation of modality effect.

**Medium Confidence**: The superiority of pre-trained model-based features over conventional features is well-demonstrated, but the study does not explore why this is the case or whether this advantage holds for other voice quality datasets or downstream tasks.

**Low Confidence**: The specific performance gains (3%-6% absolute improvement for HuBERT features) are tied to the exact experimental setup and may not generalize to other voice quality datasets, speaker populations, or classification tasks.

## Next Checks

1. Replicate the main result: Compare the classification accuracy of HuBERT features on NSA-QCP signals versus x-vector features on the same NSA-QCP signals using the same SVM classifier to quantify the claimed 3-6% improvement in a new, independent dataset or a different train-test split.

2. Layer-wise ablation study: Systematically train classifiers using features from only the first layer, middle layers, and final layer of HuBERT on NSA signals to confirm that initial layers consistently outperform later layers across different classification tasks and feature normalization strategies.

3. Modality fusion analysis: Train classifiers on speech only, NSA only, and both modalities concatenated (early fusion) to determine if NSA signals add unique information beyond speech and to quantify the potential gains from multi-modal fusion for voice quality classification.