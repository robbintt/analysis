---
ver: rpa2
title: 'LowResource at BLP-2023 Task 2: Leveraging BanglaBert for Low Resource Sentiment
  Analysis of Bangla Language'
arxiv_id: '2311.12735'
source_url: https://arxiv.org/abs/2311.12735
tags:
- task
- banglabert
- sentiment
- bangla
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes the system of the LowResource Team for Task
  2 of BLP-2023, which involves conducting sentiment analysis on a dataset composed
  of public posts and comments from diverse social media platforms. Our primary aim
  is to utilize BanglaBert, a BERT model pre-trained on a large Bangla corpus, using
  various strategies including fine-tuning, dropping random tokens, and using several
  external datasets.
---

# LowResource at BLP-2023 Task 2: Leveraging BanglaBert for Low Resource Sentiment Analysis of Bangla Language

## Quick Facts
- arXiv ID: 2311.12735
- Source URL: https://arxiv.org/abs/2311.12735
- Reference count: 15
- Primary result: Achieved 3rd place in Test Set among 30 teams with a micro-F1 score of 0.718

## Executive Summary
This paper presents the LowResource team's approach to Task 2 of BLP-2023, a sentiment analysis challenge on Bangla social media data. The team leveraged BanglaBERT, a monolingual BERT model pre-trained on Bangla text, and explored various strategies including fine-tuning, random token dropping, external dataset integration, and model ensembling. Their final submission combined three variations of BanglaBERT and achieved third place in the competition. The paper also discusses approaches that showed promise but didn't improve results, such as task-adaptive pre-training and paraphrasing using BanglaT5.

## Method Summary
The approach centers on fine-tuning BanglaBERT and BanglaBERT-Large models using various strategies. The team employed random token dropping as a data augmentation technique, integrated external sentiment datasets through two-stage fine-tuning, and combined the best three models through ensemble voting. Text preprocessing included normalization using BanglaBERT's built-in normalizer, URL/username replacement with tags, and tokenization with a maximum length of 128 tokens. The models were trained with learning rate 2e-5, batch size 16, 3 epochs, weight decay 0.01, and classifier dropout 0.1.

## Key Results
- Achieved 3rd place overall in the competition with a micro-F1 score of 0.718
- Random token dropping and external datasets improved micro-F1 scores by 0.006-0.01
- Task-adaptive pre-training and paraphrasing approaches showed promise but didn't improve results
- Monolingual BanglaBERT models outperformed multilingual models on this task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning a monolingual BERT model (BanglaBERT) outperforms multilingual models for monolingual sentiment analysis.
- Mechanism: Pre-trained models have already learned general language representations. Fine-tuning adapts these representations to the specific task while preserving the language-specific knowledge encoded in the monolingual model.
- Core assumption: Monolingual models have richer language-specific representations than multilingual models for the target language.
- Evidence anchors:
  - [abstract] "Our primary aim is to utilize BanglaBert, a BERT model pre-trained on a large Bangla corpus"
  - [section 4.1] "We don't explore much on multi-lingual models, since we have found that monolingual models are more used than multi-lingual models on monolingual-specific tasks (Muhammad et al., 2023) due to high scores."
  - [corpus] Weak - no direct comparison evidence provided in corpus
- Break condition: If the pre-trained model lacks sufficient language coverage or the task requires cross-lingual knowledge.

### Mechanism 2
- Claim: Random token dropping during training improves model generalization.
- Mechanism: By randomly masking tokens during training, the model learns to rely on context rather than memorizing specific word sequences, leading to better generalization.
- Core assumption: The task benefits from learning context-dependent representations rather than exact word matching.
- Evidence anchors:
  - [section 4.4] "First, instead of dropping random words (Bayer et al., 2022), we drop random tokens(RTD) since dropping words might change the meaning."
  - [section 6] "Utilization of random token drop and external datasets has benefited our systems by improving micro-f1 scores around 0.006 to 0.01."
  - [corpus] Weak - no direct evidence from corpus about token dropping effectiveness
- Break condition: If the dataset is very large or the task requires precise word-level understanding.

### Mechanism 3
- Claim: Two-stage fine-tuning with external datasets improves performance.
- Mechanism: Pre-training on external data helps the model learn domain-specific patterns before fine-tuning on the target task, leading to better convergence and performance.
- Core assumption: External datasets contain relevant patterns that transfer to the target task.
- Evidence anchors:
  - [section 4.3] "In the first stage, we fine-tune BanglaBert using the external data only."
  - [section 6] "Utilization of random token drop and external datasets has benefited our systems by improving micro-f1 scores around 0.006 to 0.01."
  - [corpus] Weak - no direct evidence from corpus about two-stage fine-tuning
- Break condition: If external datasets are too dissimilar from the target task or contain conflicting labels.

## Foundational Learning

- Concept: Fine-tuning vs. Feature Extraction
  - Why needed here: Understanding when to update all model parameters vs. just the classification head
  - Quick check question: When would you choose feature extraction over fine-tuning for a pre-trained model?
- Concept: Tokenization and Subword Units
  - Why needed here: BanglaBERT uses specific tokenization that must be understood for proper preprocessing
  - Quick check question: How does subword tokenization handle out-of-vocabulary words differently from word-level tokenization?
- Concept: Ensemble Methods
  - Why needed here: Understanding how to combine multiple model predictions for improved performance
  - Quick check question: What voting strategy would you use if you have an odd number of models vs. an even number?

## Architecture Onboarding

- Component map: Pre-trained models (BanglaBERT, BanglaBERT-Large) -> Tokenization pipeline with Bangla-specific normalization -> Fine-tuning trainer with mixed precision training -> External dataset preprocessing and label mapping -> Ensemble voting mechanism
- Critical path: Data preprocessing → Model fine-tuning → Ensemble prediction
- Design tradeoffs: Monolingual vs. multilingual models, single model vs. ensemble, simple fine-tuning vs. two-stage fine-tuning
- Failure signatures: Inconsistent test vs. validation performance, poor generalization to neutral class, ineffective data augmentation
- First 3 experiments:
  1. Baseline: Fine-tune BanglaBERT on training data only
  2. Random token dropping: Add RTD during training of baseline
  3. External data: Try two-stage fine-tuning with external datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does Task-Adaptive Pre-Training (TAPT) improve performance for BanglaBERT on sentiment analysis tasks?
- Basis in paper: [explicit] The paper states that TAPT did not improve results and actually decreased scores by 0.039 compared to simple fine-tuning.
- Why unresolved: The paper only conducted a few experiments with TAPT, so more extensive testing is needed to conclusively determine its effectiveness for this specific task and language.
- What evidence would resolve it: Conducting a larger number of experiments with different datasets, hyperparameters, and training strategies to thoroughly evaluate TAPT's impact on BanglaBERT's performance.

### Open Question 2
- Question: Are multilingual pre-trained models like XLM-Roberta-Large effective for low-resource language sentiment analysis tasks compared to monolingual models?
- Basis in paper: [explicit] The paper found that XLM-Roberta-Large scored lower than BanglaBERT on the development test set, even with enhancements.
- Why unresolved: The paper only tested one multilingual model, so it's unclear if this result generalizes to other multilingual models or if there are specific scenarios where they outperform monolingual models.
- What evidence would resolve it: Testing a variety of multilingual models on the same task and comparing their performance to monolingual models across different datasets and languages.

### Open Question 3
- Question: What is the optimal approach for handling label mismatches between external datasets and the target task in low-resource sentiment analysis?
- Basis in paper: [explicit] The paper had to map different labels from external datasets to the three labels used in the target task, which could introduce noise or bias.
- Why unresolved: The paper only used a simple label mapping strategy, and it's unclear if more sophisticated approaches (e.g., multi-label classification, label smoothing) would yield better results.
- What evidence would resolve it: Experimenting with different label mapping strategies and evaluating their impact on model performance across various datasets and tasks.

## Limitations

- Lack of direct comparison between monolingual and multilingual models on the same dataset
- Limited ablation studies to quantify individual contributions of techniques
- No testing on independent Bangla sentiment datasets beyond the competition data
- Incomplete investigation into why certain promising approaches (TAPT, BanglaT5 paraphrasing) failed

## Confidence

**High Confidence Claims:**
- The overall methodology of fine-tuning monolingual BERT models for sentiment analysis is sound and well-established
- The use of ensemble methods to combine multiple model predictions is a valid approach that improved their ranking
- The technical implementation details (tokenization, preprocessing, training parameters) are sufficiently specified for reproduction

**Medium Confidence Claims:**
- The assertion that monolingual models outperform multilingual models for this specific task, as this is based on external literature rather than direct comparison
- The effectiveness of random token dropping and two-stage fine-tuning, as these improvements are reported but not rigorously validated through ablation studies
- The ranking achievement (3rd place) is valid but doesn't necessarily prove the approach is optimal

**Low Confidence Claims:**
- The reasons why TAPT and BanglaT5 paraphrasing didn't perform well, as these are stated as observations without systematic investigation
- The claim that specific improvements (0.006-0.01 micro-F1) are directly attributable to particular techniques without controlled experiments

## Next Checks

1. **Direct Monolingual vs. Multilingual Comparison**: Conduct a controlled experiment training both BanglaBERT and a multilingual model (e.g., mBERT) on the exact same dataset with identical hyperparameters to verify the claimed superiority of monolingual models.

2. **Ablation Study on Data Augmentation**: Systematically remove random token dropping and two-stage fine-tuning from the best-performing model to quantify their individual contributions to the final score, rather than assuming the reported improvements.

3. **Cross-Dataset Generalization Test**: Evaluate the trained ensemble on an independent Bangla sentiment analysis dataset not used in training or competition to assess whether the approach generalizes beyond the specific competition data distribution.