---
ver: rpa2
title: On the Equivalence of Graph Convolution and Mixup
arxiv_id: '2310.00183'
source_url: https://arxiv.org/abs/2310.00183
tags:
- graph
- mixup
- neural
- networks
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the relationship between graph convolution
  and Mixup techniques, revealing that graph convolution can be viewed as a specialized
  form of Mixup under two mild conditions: Homophily Relabel (assigning the target
  node''s label to all its neighbors) and Test-Time Mixup (mixing features during
  testing). The authors establish this equivalence mathematically and empirically
  by training an MLP using these conditions to achieve comparable performance to graph
  neural networks.'
---

# On the Equivalence of Graph Convolution and Mixup

## Quick Facts
- arXiv ID: 2310.00183
- Source URL: https://arxiv.org/abs/2310.00183
- Reference count: 35
- Primary result: Graph convolution can be viewed as a specialized form of Mixup under Homophily Relabel and Test-Time Mixup conditions, with empirical validation on citation networks.

## Executive Summary
This paper establishes a theoretical and empirical equivalence between graph convolution and Mixup data augmentation techniques. Under two mild conditions - Homophily Relabel (assigning target node labels to neighbors) and Test-Time Mixup (neighbor feature mixing during inference) - graph convolution can be expressed as a specialized form of Mixup. The authors demonstrate this equivalence mathematically and validate it empirically by training an MLP with these conditions to achieve performance comparable to GCNs on citation networks like Cora, CiteSeer, and PubMed.

## Method Summary
The authors investigate the relationship between graph convolution and Mixup by proposing two conditions: Homophily Relabel (relabeling neighbors with the target node's label) and Test-Time Mixup (mixing neighbor features during inference). They mathematically prove that one-layer GCN is equivalent to input Mixup, two-layer GCN is a hybrid of input and manifold Mixup, and SGC is equivalent to input Mixup. The proposed methods, HMLP (MLP with Homophily Relabel) and TMLP (MLP with Test-Time Mixup), are trained and evaluated on citation networks. The key insight is that by relabeling neighbors under the homophily assumption, GCN's neighbor aggregation becomes mathematically equivalent to Mixup's interpolation.

## Key Results
- One-layer GCN is mathematically equivalent to input Mixup under Homophily Relabel
- Two-layer GCN is a hybrid of input Mixup (first layer) and manifold Mixup (second layer)
- HMLP and TMLP achieve comparable performance to GCNs on Cora, CiteSeer, and PubMed (83.57% and 84.06% vs GCN's 83.89%)
- The equivalence holds empirically across various training ratios (10-90%) on citation networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph convolution can be expressed as a weighted average of neighbor features, mathematically equivalent to Mixup when labels are reassigned.
- Mechanism: By relabeling each neighbor node with the target node's label (Homophily Relabel), the GCN's neighbor aggregation becomes a convex combination of features with matching labels, identical to Mixup's interpolation.
- Core assumption: Graph nodes are homophilous (connected nodes tend to share labels), so relabeling neighbors is a reasonable approximation.
- Evidence anchors:
  - [abstract] "Our investigation reveals that, under two mild conditions, graph convolution can be viewed as a specialized form of Mixup..."
  - [section] "if we assign the yi to all the neighbors of node ni in set Ni"
  - [corpus] No direct match; weak evidence, likely an original contribution.
- Break condition: On graphs with low homophily or heterophily, relabeling neighbors introduces incorrect supervision, breaking equivalence.

### Mechanism 2
- Claim: Two-layer GCN is a hybrid of input Mixup (first layer) and manifold Mixup (second layer).
- Mechanism: The first layer aggregates raw neighbor features (input Mixup). The second layer mixes the resulting hidden representations across neighbors (manifold Mixup).
- Core assumption: Stacking Mixup-like operations preserves the equivalence to deeper GCNs.
- Evidence anchors:
  - [section] "two-layer GCN can be expressed as a hybrid of input and manifold Mixup"
  - [section] "˜ai · H is the multiple-sample Mixup of the hidden representation of the neighbors"
  - [corpus] Weak evidence; similar concept appears in related Mixup works but not GCN equivalence.
- Break condition: If hidden representations become too dissimilar across neighbors, manifold Mixup no longer approximates the second GCN layer.

### Mechanism 3
- Claim: Test-Time Mixup (neighbor aggregation during inference) allows an MLP trained without graph info to match GCN performance.
- Mechanism: The MLP learns feature representations without topology. At inference, neighbor features are mixed to refine predictions, effectively applying GCN-like smoothing post-hoc.
- Core assumption: Test-time neighbor mixing can compensate for lack of graph awareness during training.
- Evidence anchors:
  - [section] "perform feature mixing on the nodes and then use the mixed feature for the inference"
  - [section] "TMLP... achieves comparable performance to GNNs"
  - [corpus] Weak evidence; similar inference-time augmentation appears in some works but not exact equivalence.
- Break condition: If neighbor features are highly heterogeneous or if the graph structure is too complex, test-time mixing may not recover GCN performance.

## Foundational Learning

- Concept: Graph Neural Networks and neighbor aggregation
  - Why needed here: Understanding how GCNs aggregate neighbor features is essential to see the Mixup equivalence.
  - Quick check question: In a GCN, what operation is applied to neighbor features before mixing?

- Concept: Mixup data augmentation
  - Why needed here: Mixup interpolates between samples; here it is generalized to graph-structured data.
  - Quick check question: In standard Mixup, how are the mixing coefficients constrained?

- Concept: Homophily assumption in graphs
  - Why needed here: Homophily justifies relabeling neighbors in HMLP and underpins the theoretical claim.
  - Quick check question: What happens to the equivalence claim if the graph has low homophily?

## Architecture Onboarding

- Component map:
  MLP backbone -> Homophily Relabel preprocessing -> Test-Time Mixup inference

- Critical path:
  1. Preprocess: Relabel graph nodes with Homophily Relabel
  2. Train: Standard MLP on node features and relabeled targets
  3. Infer: Aggregate neighbor features via weighted averaging, feed to MLP

- Design tradeoffs:
  - HMLP: Efficient training (no graph info needed), but test-time neighbor access required
  - TMLP: Fast training (MLP only), but inference involves neighbor aggregation (potentially slow)
  - GCN: Both training and inference use graph; potentially slower but no approximation

- Failure signatures:
  - Large drop in accuracy on heterophilous graphs (HMLP/TMLP fail to generalize)
  - Slow inference on large graphs (neighbor aggregation expensive)
  - Over-smoothing when many Mixup steps applied (feature collapse)

- First 3 experiments:
  1. Verify HMLP matches GCN accuracy on Cora with 60% train ratio.
  2. Compare TMLP inference time vs GCN on a medium graph (e.g., Citeseer).
  3. Test HMLP/TMLP on a heterophilous graph (e.g., Texas) to observe accuracy drop.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the equivalence between graph convolution and Mixup hold for attention-based GNNs like GAT?
- Basis in paper: [explicit] The paper explicitly states that "certain complex GNN architectures, such as attention-based graph convolution (Veličković et al., 2018; Xu et al., 2018; Javaloy et al., 2023), may not lend themselves to a straightforward transformation into a Mixup formulation due to the intricacies of their convolution mechanisms."
- Why unresolved: The paper acknowledges this limitation but does not provide a detailed analysis of why attention-based GNNs cannot be transformed into Mixup formulations or whether there are any conditions under which they could be.
- What evidence would resolve it: A formal proof showing that attention-based GNNs cannot be expressed as Mixup under any conditions, or a specific example of an attention-based GNN that can be expressed as Mixup under certain conditions.

### Open Question 2
- Question: How does the performance of HMLP and TMLP compare to other efficient GNN methods like GraphSAINT or FastGCN?
- Basis in paper: [inferred] The paper discusses the practical potential of HMLP and TMLP for efficient training and inference, but does not compare their performance to other efficient GNN methods.
- Why unresolved: The paper does not provide a direct comparison between HMLP/TMLP and other efficient GNN methods, leaving their relative performance unclear.
- What evidence would resolve it: Experimental results comparing the performance of HMLP and TMLP to GraphSAINT, FastGCN, and other efficient GNN methods on standard benchmark datasets.

### Open Question 3
- Question: Can the equivalence between graph convolution and Mixup be extended to other graph tasks beyond node classification, such as link prediction or graph classification?
- Basis in paper: [explicit] The paper focuses on node classification and does not discuss the applicability of the equivalence to other graph tasks.
- Why unresolved: The paper does not explore whether the equivalence between graph convolution and Mixup can be generalized to other graph tasks, leaving this question open.
- What evidence would resolve it: Experimental results showing that HMLP and TMLP achieve comparable performance to GNNs on link prediction or graph classification tasks, or a theoretical proof of the equivalence for these tasks.

### Open Question 4
- Question: What are the theoretical implications of the equivalence between graph convolution and Mixup for the expressiveness of GNNs?
- Basis in paper: [explicit] The paper mentions that HMLP has the "theoretical potential to understand the expressiveness of graph neural network from the Mixup perspective."
- Why unresolved: The paper does not provide a detailed analysis of how the equivalence between graph convolution and Mixup affects the expressiveness of GNNs, leaving this question open.
- What evidence would resolve it: A theoretical analysis showing how the equivalence between graph convolution and Mixup relates to the universal approximation theorem for GNNs or other results on the expressiveness of GNNs.

## Limitations
- The equivalence heavily depends on the homophily assumption, which may not hold for heterophilous graphs
- Implementation details of Homophily Relabel and Test-Time Mixup are not fully specified, making reproduction challenging
- Results are validated only on citation networks; effectiveness on other graph types remains untested

## Confidence
- Theoretical equivalence claim: Medium
- Empirical validation on Cora/CiteSeer/PubMed: Medium
- General applicability to other graph types: Low

## Next Checks
1. **Heterophily Test**: Evaluate HMLP/TMLP on a known heterophilous graph (e.g., Texas) to measure performance degradation and validate the homophily dependency.

2. **Runtime Analysis**: Compare inference time of TMLP (with neighbor aggregation) versus GCN on a medium-sized graph to quantify the efficiency tradeoff.

3. **Robustness Check**: Vary the Mixup interpolation coefficient (α) in HMLP/TMLP to determine sensitivity to this hyperparameter and its impact on the equivalence to GCN.