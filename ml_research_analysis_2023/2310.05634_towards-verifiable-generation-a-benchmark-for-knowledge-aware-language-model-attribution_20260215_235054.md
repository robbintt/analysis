---
ver: rpa2
title: 'Towards Verifiable Generation: A Benchmark for Knowledge-aware Language Model
  Attribution'
arxiv_id: '2310.05634'
source_url: https://arxiv.org/abs/2310.05634
tags:
- knowledge
- evaluation
- question
- gentileschi
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new task called Knowledge-aware Language
  Model Attribution (KaLMA) to address the limitations of existing language attribution
  methods that rely solely on unstructured documents. The authors propose extending
  attribution sources to structured Knowledge Graphs (KGs), incorporating a "Conscious
  Incompetence" setting to handle incomplete knowledge repositories, and proposing
  a comprehensive automatic evaluation metric.
---

# Towards Verifiable Generation: A Benchmark for Knowledge-aware Language Model Attribution

## Quick Facts
- arXiv ID: 2310.05634
- Source URL: https://arxiv.org/abs/2310.05634
- Reference count: 33
- Key outcome: Introduces KaLMA task, BioKaLMA dataset, and evaluation framework for knowledge-aware language model attribution with Conscious Incompetence setting

## Executive Summary
This paper introduces Knowledge-aware Language Model Attribution (KaLMA) as a new task to improve language model attribution beyond traditional document-based methods. The authors propose extending attribution sources from unstructured documents to Knowledge Graphs (KGs), incorporating a "Conscious Incompetence" setting to handle incomplete knowledge repositories, and developing a comprehensive automatic evaluation metric. They construct the BioKaLMA dataset in the biography domain using an evolutionary question generation strategy and evaluate baseline models including GPT-4, ChatGPT, and LLaMA variants. The results demonstrate the importance of KG-based attribution and highlight opportunities for improvement in citation generation.

## Method Summary
The paper proposes a retrieval-augmented generation pipeline for KaLMA that uses Knowledge Graphs as the primary knowledge source. The method involves entity recognition using spaCy, SPARQL-based graph retrieval from WikiData, re-ranking of retrieved subgraphs, and generation with attribution. A novel "Conscious Incompetence" setting allows models to identify knowledge gaps using [NA] tokens when supporting knowledge is missing from the KG. The BioKaLMA dataset is constructed using an evolutionary question generation strategy that iteratively improves question quality. The evaluation framework combines automatic metrics including GPTScore for text quality, NLI-based alignment for text-citation consistency, and precision/recall metrics for citation quality.

## Key Results
- KG-based attribution significantly improves citation quality over document-based methods
- The "Conscious Incompetence" setting enables models to identify knowledge gaps, enhancing trustworthiness
- Automatic evaluation metrics correlate well with human judgment for text and citation quality assessment
- Retrieval accuracy is critical for citation generation performance, with room for improvement in LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extending attribution source from unstructured documents to Knowledge Graphs (KGs) improves both attribution performance and working scenarios.
- Mechanism: KGs efficiently organize world knowledge in a structured manner, unifying various data formats and enabling fine-grained entity-level attribution.
- Core assumption: Knowledge Graphs contain the necessary information to support generated text and can be reliably retrieved.
- Evidence anchors:
  - [abstract] "we extend attribution source from unstructured texts to Knowledge Graph (KG), whose rich structures benefit both the attribution performance and working scenarios."
  - [section] "To address the first challenge, we utilize knowledge graph (KG) as a reliable source for attribution, namely Knowledge-aware Language Model Attribution (KaLMA)."
- Break condition: If the KG does not contain the required knowledge for answering the question, the attribution fails.

### Mechanism 2
- Claim: The "Conscious Incompetence" setting allows models to identify knowledge gaps and enhances trustworthiness.
- Mechanism: Models can flag sentences requiring supporting knowledge not present in the KG using [NA] tokens, enabling users to verify uncertain claims.
- Core assumption: Models can accurately identify when knowledge is missing from the provided KG.
- Evidence anchors:
  - [abstract] "we propose a new 'Conscious Incompetence' setting considering the incomplete knowledge repository, where the model identifies the need for supporting knowledge beyond the provided KG."
  - [section] "We introduce a new setting 'Conscious Incompetence' (Curtiss and Warren, 1974), which is the psychological stage that one is aware of the knowledge gap."
- Break condition: If the model incorrectly flags knowledge as missing when it is present in the KG, or vice versa.

### Mechanism 3
- Claim: The comprehensive automatic evaluation metric enables reliable assessment without human-annotated ground truth.
- Mechanism: Evaluation encompasses text quality, citation quality, and text-citation alignment using automatic metrics like GPTScore and NLI.
- Core assumption: Automatic metrics can accurately evaluate the quality of generated text and citations.
- Evidence anchors:
  - [abstract] "we propose a comprehensive automatic evaluation metric encompassing text quality, citation quality, and text citation alignment."
  - [section] "Our benchmark includes measurements for both the generated text and citations."
- Break condition: If automatic metrics fail to correlate with human judgments of quality and alignment.

## Foundational Learning

- Concept: Knowledge Graphs and SPARQL queries
  - Why needed here: KGs are the primary source of knowledge for attribution, and SPARQL is used to retrieve relevant subgraphs.
  - Quick check question: What is the difference between a node and an edge in a Knowledge Graph?

- Concept: Natural Language Inference (NLI)
  - Why needed here: NLI models are used to evaluate the alignment between generated text and citations.
  - Quick check question: What is the difference between entailment and contradiction in NLI?

- Concept: Automatic evaluation metrics for text generation
  - Why needed here: Automatic metrics are used to evaluate the quality of generated text and citations without human-annotated ground truth.
  - Quick check question: What are the advantages and disadvantages of using GPTScore for text evaluation?

## Architecture Onboarding

- Component map:
  - Data Construction: Name Pair Extraction → Name Disambiguation → Evolutionary Question Generation
  - Model Pipeline: NER + Graph Retrieval → Re-ranking → Generation (with [NA] for Conscious Incompetence)
  - Evaluation: Text Quality (GPTScore) → Citation Quality (Correctness, Precision, Recall, F1) → Text-Citation Alignment (NLI) → Conscious Incompetence ([NA] Precision/Recall)

- Critical path:
  - Question → Retrieval (NER + SPARQL) → Re-ranking → Generation (with [NA]) → Evaluation (GPTScore + NLI + [NA] metrics)

- Design tradeoffs:
  - KG vs. unstructured documents: KGs enable fine-grained entity-level attribution but may have coverage issues.
  - [NA] tokens: Enhance trustworthiness by flagging knowledge gaps but may reduce fluency if overused.
  - Automatic vs. human evaluation: Automatic evaluation is scalable but may not fully capture quality nuances.

- Failure signatures:
  - Low citation correctness/recall: Retrieval or KG coverage issues.
  - Low text-citation alignment: Generation or NLI model issues.
  - Low [NA] precision/recall: Model struggles to identify knowledge gaps accurately.

- First 3 experiments:
  1. Evaluate baseline retrieval accuracy on a small subset of BioKaLMA.
  2. Test generation with and without [NA] tokens on a sample of questions.
  3. Assess correlation between automatic and human evaluation on a sample of generated text-citation pairs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of language models in generating accurate and thorough citations vary across different knowledge graph structures (e.g., simple triples vs. document nodes)?
- Basis in paper: [inferred] The paper mentions that the current work only investigates a simple form of knowledge graph and plans to explore more complicated forms in future work.
- Why unresolved: The paper does not provide experimental results comparing the performance of language models on different knowledge graph structures.
- What evidence would resolve it: Conducting experiments with language models using knowledge graphs of varying complexity (e.g., simple triples vs. document nodes) and comparing the accuracy and thoroughness of generated citations.

### Open Question 2
- Question: To what extent does the use of ChatGPT as the model for text quality evaluation introduce bias in the evaluation results, given that ChatGPT might prefer the text style it generates?
- Basis in paper: [explicit] The paper acknowledges the limitation of using ChatGPT for text quality evaluation, stating that it could potentially have a bias if the model prefers the text style generated by itself.
- Why unresolved: The paper does not provide any empirical evidence or analysis to quantify the extent of this potential bias.
- What evidence would resolve it: Conducting experiments where the text quality evaluation is performed using different models (e.g., GPT-4) and comparing the results to those obtained using ChatGPT.

### Open Question 3
- Question: How does the inclusion of the "Conscious Incompetence" setting impact the overall trustworthiness and reliability of attributed language models in real-world applications?
- Basis in paper: [explicit] The paper introduces the "Conscious Incompetence" setting to allow models to identify knowledge gaps and enable users to verify uncertain claims, which enhances trustworthiness.
- Why unresolved: The paper does not provide user studies or real-world applications to demonstrate the impact of this setting on the trustworthiness and reliability of attributed language models.
- What evidence would resolve it: Conducting user studies or deploying attributed language models with the "Conscious Incompetence" setting in real-world applications to assess their impact on user trust and the reliability of generated content.

## Limitations

- The SPARQL retrieval accuracy directly impacts all downstream metrics, creating a potential bottleneck if KG retrieval fails to extract relevant subgraphs.
- The BioKaLMA dataset construction relies heavily on evolutionary question generation, but exact prompt templates and demonstration examples are not fully specified, creating potential reproducibility issues.
- The automatic evaluation metrics (GPTScore, NLI-based alignment) are presented as alternatives to human judgment, but their correlation with human evaluations is not empirically validated within the paper.

## Confidence

- **High confidence**: The core claim that extending attribution sources from unstructured documents to Knowledge Graphs provides benefits for fine-grained entity-level attribution is well-supported by the mechanism description and aligns with established KG advantages.
- **Medium confidence**: The assertion that the comprehensive automatic evaluation metric enables reliable assessment without human-annotated ground truth is reasonable given the use of multiple complementary metrics, but lacks empirical validation of metric correlation with human judgment.
- **Medium confidence**: The claim that the "Conscious Incompetence" setting enhances trustworthiness by allowing models to identify knowledge gaps is theoretically sound, but the practical implementation and accuracy of gap identification require further validation.

## Next Checks

1. **Retrieval Accuracy Validation**: Implement a controlled experiment measuring the baseline SPARQL retrieval accuracy on a small, manually verified subset of BioKaLMA questions to establish the lower bound of citation quality and identify whether retrieval failures or generation failures are the primary bottleneck.

2. **Human Evaluation Correlation Study**: Conduct a small-scale human evaluation (e.g., 50 samples) comparing human judgments of text quality, citation quality, and alignment against the automatic metrics (GPTScore, NLI) to empirically validate the proposed evaluation framework's reliability.

3. **Knowledge Gap Identification Test**: Design a diagnostic experiment testing the model's ability to correctly identify knowledge gaps across a diverse set of questions where some entities are present in the KG and others are not, measuring precision and recall of [NA] token usage against ground truth.