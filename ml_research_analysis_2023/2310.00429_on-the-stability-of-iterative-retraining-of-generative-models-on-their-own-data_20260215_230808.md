---
ver: rpa2
title: On the Stability of Iterative Retraining of Generative Models on their own
  Data
arxiv_id: '2310.00429'
source_url: https://arxiv.org/abs/2310.00429
tags:
- data
- generative
- retraining
- iterative
- pdata
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the stability of iterative retraining generative
  models on mixed datasets of real and synthetic data. The authors develop a theoretical
  framework and prove that iterative retraining is stable if the initial model is
  close enough to the true data distribution and enough real data is used.
---

# On the Stability of Iterative Retraining of Generative Models on their own Data

## Quick Facts
- arXiv ID: 2310.00429
- Source URL: https://arxiv.org/abs/2310.00429
- Reference count: 33
- This paper studies the stability of iterative retraining generative models on mixed datasets of real and synthetic data.

## Executive Summary
This paper investigates the stability of iteratively retraining generative models on datasets containing both real and synthetic data. The authors develop a theoretical framework proving that iterative retraining is stable when the initial model is close to the true data distribution and sufficient real data is used. They validate these theoretical results empirically using diffusion models on CIFAR-10 and FFHQ datasets, showing that retraining remains stable when the proportion of synthetic data is small but diverges otherwise.

## Method Summary
The authors analyze iterative retraining by first establishing conditions for the existence and uniqueness of fixed points in the parameter space, then proving stability through Jacobian analysis. They use maximum likelihood estimation with a regularization term from synthetic data, retraining models iteratively on mixed datasets. The theoretical framework is validated empirically by retraining diffusion models (DDPM, EDM, CFM) on CIFAR-10 and FFHQ datasets with varying proportions of synthetic data, measuring stability through FID scores, precision, and recall.

## Key Results
- Iterative retraining is stable when the initial model approximates the data distribution well and sufficient real data is used
- Model collapse occurs when retraining exclusively on synthetic data due to vanishing variance
- With high probability, iterative retraining remains within a neighborhood of the optimal generative model in parameter space when working in the stable regime

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative retraining remains stable if the initial model is close enough to the true data distribution and a sufficient proportion of real data is used.
- Mechanism: The model converges to a fixed point θ* where the gradient of the loss with respect to parameters is zero. The fixed point exists when the synthetic data acts as a regularizer that doesn't push the parameters too far from θ*.
- Core assumption: The Hessian of the log-likelihood at θ* is negative definite and the model class can approximate the true distribution well enough.
- Break condition: If the proportion of synthetic data exceeds a threshold where λ(1 + Lε/α) ≥ 1/2, the Jacobian operator norm exceeds 1 and training diverges.

### Mechanism 2
- Claim: Model collapse occurs when retraining exclusively on synthetic data due to vanishing variance in parameter estimates.
- Mechanism: When sampling only from the model distribution, the empirical variance of the samples decreases exponentially with each retraining iteration, causing the model to collapse to a single point.
- Core assumption: The model is trained on finite samples, not the true distribution.
- Break condition: Using at least some real data in each retraining iteration prevents the variance from collapsing.

### Mechanism 3
- Claim: With high probability, iterative retraining remains within a neighborhood of the optimal generative model in parameter space when working in the stable regime.
- Mechanism: The error between iteratively retrained parameters and the fixed point can be decomposed into optimization error, statistical error, and iterative retraining error, all of which remain bounded under appropriate conditions.
- Core assumption: There exists a generalization bound for the model class with a vanishing term as sample size increases.
- Break condition: If the statistical error term a/√n · √log b/δ grows too large due to insufficient real data, the neighborhood constraint is violated.

## Foundational Learning

- Concept: Wasserstein distance as a measure of distribution similarity
  - Why needed here: Used to quantify how close the optimal generative model pθ* is to the true data distribution pdata, which determines stability conditions
  - Quick check question: What property of the Wasserstein distance makes it suitable for measuring distances between probability distributions in this context?

- Concept: Fixed point analysis in iterative algorithms
  - Why needed here: The stability of iterative retraining depends on whether the update operator has a fixed point and whether iterations converge to it
  - Quick check question: What condition on the Jacobian of the update operator at the fixed point ensures local convergence of the iterative process?

- Concept: Implicit Function Theorem for characterizing solutions
  - Why needed here: Used to prove the uniqueness of the local maximum likelihood solution and characterize the fixed point operator
  - Quick check question: How does the Implicit Function Theorem help establish that the solution to the optimization problem is locally unique?

## Architecture Onboarding

- Component map: Generative model -> Data sampling (real + synthetic) -> Optimization loop -> Stability evaluation -> Retraining
- Critical path: Sample from current model → Mix with real data → Retrain model → Evaluate stability → Repeat
- Design tradeoffs: More real data improves stability but reduces the amount of synthetic data that can be used; more expressive model classes may reduce approximation error but increase computational cost
- Failure signatures: Increasing FID scores over iterations, decreasing precision with increasing recall, visible collapse to single modes in density visualizations
- First 3 experiments:
  1. Implement the simple Gaussian collapse experiment from Section 2.2 to verify the theoretical predictions about model collapse
  2. Run the DDPM retraining experiment on CIFAR-10 with λ = 0.1 to verify stability claims
  3. Test the stability boundary by varying λ from 0.1 to 0.5 on FFHQ-64 with EDM to observe the transition from stable to unstable behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the necessary conditions for iterative retraining stability, beyond the sufficient conditions provided in Theorems 1 and 2?
- Basis in paper: The authors note that their sufficient conditions may not be necessary, and there is a gap to close between the necessity of λ < +∞ and the sufficiency λ(1 + Lε/α) < 1 for local stability.
- Why unresolved: The paper focuses on proving sufficient conditions for stability but does not explore whether these conditions are also necessary.
- What evidence would resolve it: Proving lower bounds on λ or ε that are required for stability, or demonstrating cases where the sufficient conditions are not necessary.

### Open Question 2
- Question: How does the finite sampling aspect affect iterative retraining stability, and can we quantify its impact more precisely?
- Basis in paper: The authors mention that the collapse in the Gaussian case only happens in the finite sample case, suggesting that finite sampling plays a crucial role in stability.
- Why unresolved: The paper provides some theoretical and empirical evidence but does not fully characterize the impact of finite sampling on stability.
- What evidence would resolve it: Developing a more refined analysis of the statistical error term in Theorem 2 or conducting experiments with varying sample sizes to observe the effect on stability.

### Open Question 3
- Question: Can we weaken Assumption 3 (generalization bound) and still prove similar stability results?
- Basis in paper: The authors acknowledge that Assumption 3 is relatively strong and local, and they suggest that weakening it could be a rich research direction.
- Why unresolved: The current stability proof relies on a specific generalization bound assumption, which may be too restrictive in practice.
- What evidence would resolve it: Proving stability results under weaker assumptions on the generalization error, such as using different types of bounds or relaxing the locality requirement.

### Open Question 4
- Question: How do the trends in precision and recall (diversity vs. fidelity) observed in the experiments relate to the theoretical stability results?
- Basis in paper: The authors note that for EDM models, precision decreases while recall increases with the number of retraining iterations, which is an interesting empirical observation.
- Why unresolved: The theoretical results focus on stability and parameter convergence, but do not directly address the trade-off between precision and recall observed in practice.
- What evidence would resolve it: Analyzing how the iterative retraining procedure affects the model's ability to generate diverse samples while maintaining fidelity, and relating this to the theoretical stability guarantees.

## Limitations

- The theoretical analysis relies on strong assumptions about model class approximation quality and fixed point existence
- Results are primarily derived for maximum likelihood frameworks and may not generalize to adversarial training
- Generalization bounds depend on model complexity, which is difficult to characterize for deep neural networks

## Confidence

**High Confidence**: The theoretical framework for stability analysis is sound, with rigorous proofs based on fixed point theorems and Jacobian analysis. The model collapse result for Gaussian distributions is mathematically proven and experimentally verified.

**Medium Confidence**: The stability conditions for complex models like diffusion networks are theoretically derived but rely on assumptions about approximation quality that are difficult to verify empirically. The empirical validation on CIFAR-10 and FFHQ provides supporting evidence but with limited hyperparameter exploration.

**Low Confidence**: The generalization bounds and their implications for practical model selection are not fully explored. The relationship between the theoretical λ threshold and practical performance boundaries needs more systematic investigation.

## Next Checks

1. **Systematic Stability Boundary Mapping**: Conduct a comprehensive grid search over λ values (0.01 to 0.99) and retraining iteration counts to precisely map the transition from stable to unstable regimes for different model architectures.

2. **Approximation Error Analysis**: Quantify the gap between the theoretical stability conditions (which assume perfect model class approximation) and empirical performance by measuring the actual approximation error of diffusion models to CIFAR-10 and FFHQ distributions.

3. **Alternative Objective Functions**: Test whether the stability results extend to alternative training objectives like adversarial losses or score matching, which are commonly used in practice but not covered by the current theoretical framework.