---
ver: rpa2
title: 'Retrieval-Pretrained Transformer: Long-range Language Modeling with Self-retrieval'
arxiv_id: '2306.13421'
source_url: https://arxiv.org/abs/2306.13421
tags:
- chunks
- chunk
- retriever
- language
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Retrieval-Pretrained Transformer (RPT),
  a retrieval-augmented language model where the retriever is jointly trained from
  scratch as a native component of the model. RPT computes query representations from
  the LM itself (self-retrieval), retrieves semantically relevant chunks from the
  past, and fuses retrieved chunks back into the LM through chunked cross-attention
  to improve next-word prediction.
---

# Retrieval-Pretrained Transformer: Long-range Language Modeling with Self-retrieval

## Quick Facts
- arXiv ID: 2306.13421
- Source URL: https://arxiv.org/abs/2306.13421
- Authors: 
- Reference count: 19
- Key outcome: RPT improves perplexity and retrieval quality across four long-range language modeling tasks (books, code, and mathematical writing) compared to strong baselines.

## Executive Summary
This paper introduces the Retrieval-Pretrained Transformer (RPT), a retrieval-augmented language model where the retriever is jointly trained from scratch as a native component of the model. RPT computes query representations from the LM itself (self-retrieval), retrieves semantically relevant chunks from the past, and fuses retrieved chunks back into the LM through chunked cross-attention to improve next-word prediction. Experiments on four long-range language modeling tasks show that RPT improves perplexity and retrieval quality across all tasks compared to strong baselines.

## Method Summary
RPT jointly trains a transformer decoder with a retriever component that retrieves relevant chunks from the document. The retriever uses representations computed by the lower decoder layers (self-retrieval) to find semantically relevant chunks, which are then fused into the upper decoder layers through chunked cross-attention. The retriever is trained with a semantic objective using a reference LM to supervise retrieval quality. The model uses sliding window attention with 2048-token windows and 1024-token strides, processes documents as 64-token chunks, and employs neighbor gating to control information flow from retrieved chunks.

## Key Results
- RPT consistently outperforms strong baselines on perplexity across all four long-range datasets
- Jointly training retriever and LM from scratch improves performance compared to fixed retrievers
- Semantic supervision (using scoring LM) outperforms lexical supervision (BM25 retrieval)
- Retrieval quality metrics (Precision@2/Recall@10/NDCG@20) show RPT retrieves more relevant chunks than baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint training of retriever and LM from scratch enables them to adapt to each other's representations and improve perplexity.
- Mechanism: The retriever learns to retrieve chunks that increase the probability of generating the next chunk according to a reference LM. This supervision signal is stronger than lexical-only signals because it captures semantic relevance.
- Core assumption: The scoring LM can accurately identify chunks that are semantically relevant for predicting the next chunk.
- Evidence anchors:
  - [abstract] "We train the retriever component with a semantic objective, where the goal is to retrieve chunks that increase the probability of the next chunk, according to a reference LM."
  - [section 3.2] "We compute a score st (¯cj) that reflects whether the information in ¯cj is more useful for decoding ct compared to chunks that are close to cq."
  - [corpus] Weak - the corpus does not directly validate the semantic relevance of retrieved chunks.
- Break condition: If the scoring LM is not well-calibrated or does not capture semantic relevance accurately, the supervision signal will be weak and the retriever will not learn useful representations.

### Mechanism 2
- Claim: Chunked cross-attention (CCA) layers allow deep fusion of retrieved information into the LM representations, improving performance compared to shallow fusion methods.
- Mechanism: CCA applies cross-attention between the query chunk representations and the retrieved chunk representations, allowing the LM to attend to relevant information from retrieved chunks when predicting the next chunk.
- Core assumption: The retrieved chunks contain relevant information that can improve the LM's predictions.
- Evidence anchors:
  - [abstract] "Information from retrieved chunks is fused into the LM representations to predict the next target chunk."
  - [section 3.1] "The top nlayers2 layers (upper decoder) use Chunked Cross-Attention (CCA) to fuse information from the top-K neighbor chunks retrieved by the retriever back into the LM."
  - [corpus] Weak - the corpus does not directly validate the effectiveness of CCA.
- Break condition: If the retrieved chunks are not relevant or the CCA layers are not effective at fusing the information, the performance improvement will be minimal.

### Mechanism 3
- Claim: Self-retrieval, where the retriever uses representations computed by the LM itself, leads to better adaptation compared to using external representations.
- Mechanism: The lower decoder computes representations that are used by the retriever to retrieve relevant chunks. These representations are then used by the upper decoder to make predictions. This allows the retriever to learn representations that are specifically useful for the LM's predictions.
- Core assumption: The representations computed by the LM are informative for retrieving relevant chunks.
- Evidence anchors:
  - [abstract] "RPT computes query representations from the LM itself (self-retrieval)"
  - [section 3.1] "input representations for the retriever are computed from the LM representations themselves (which we dubself-retrieval)"
  - [corpus] Weak - the corpus does not directly validate the effectiveness of self-retrieval.
- Break condition: If the LM's representations are not informative for retrieval, self-retrieval will not lead to better performance.

## Foundational Learning

- Concept: Attention mechanisms
  - Why needed here: Attention mechanisms are used throughout the model, including self-attention in the decoder layers and cross-attention in the CCA layers.
  - Quick check question: What is the difference between self-attention and cross-attention?

- Concept: Retrieval and indexing
  - Why needed here: The retriever needs to efficiently retrieve relevant chunks from the document. Understanding retrieval techniques and indexing is crucial for implementing the retriever.
  - Quick check question: How does BM25 retrieval work and how is it different from dense retrieval?

- Concept: Language modeling and perplexity
  - Why needed here: The model is trained to minimize perplexity, which is a measure of how well the model predicts the next word. Understanding language modeling concepts is essential for training and evaluating the model.
  - Quick check question: What is perplexity and how is it calculated?

## Architecture Onboarding

- Component map:
  - Input sequence -> Chunks (64 tokens each) -> Lower decoder (standard Transformer layers) -> Retriever (computes query representations) -> Retrieved chunks -> Upper decoder (CCA layers) -> Next chunk prediction

- Critical path:
  1. Input sequence is divided into chunks
  2. Lower decoder computes representations for each chunk
  3. Retriever computes query representations and retrieves relevant chunks
  4. CCA layers fuse retrieved information into upper decoder representations
  5. Upper decoder predicts next chunk

- Design tradeoffs:
  - Chunk size: Larger chunks may contain more information but are more computationally expensive to process
  - Number of retrieved chunks: More chunks provide more context but increase computation and memory usage
  - Training signal: Semantic supervision (using scoring LM) vs. lexical supervision (using BM25)

- Failure signatures:
  - High perplexity: Indicates the model is not predicting the next chunk well
  - Low retrieval quality: Indicates the retriever is not finding relevant chunks
  - Unstable training: Indicates issues with the training procedure or hyperparameters

- First 3 experiments:
  1. Train a baseline Transformer-XL model to establish a performance baseline
  2. Train RETRO with BM25 retrieval to compare with a fixed retriever
  3. Train RPT-Sem with semantic supervision to evaluate the impact of joint training and semantic retrieval

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RPT scale with larger models and increased training data compared to non-retrieval baselines?
- Basis in paper: [inferred] The paper shows RPT outperforms strong baselines on 4 datasets, but does not explore scaling behavior or compare to larger non-retrieval models.
- Why unresolved: The experiments use a fixed model size (12 layers, 1024 hidden dim) and training budget. Scaling laws for retrieval-augmented models remain largely unexplored.
- What evidence would resolve it: Systematic experiments varying model size, dataset size, and comparing to similarly sized/powered non-retrieval models across diverse tasks.

### Open Question 2
- Question: Can the retriever be made more efficient for long documents without sacrificing retrieval quality, perhaps through sparse attention or hierarchical indexing?
- Basis in paper: [explicit] The paper notes that scoring every chunk w.r.t. all preceding chunks is quadratic and computationally difficult, and uses BM25 to get candidate chunks first.
- Why unresolved: The current approach still requires computing scores between query chunks and all previous chunks in the document. More efficient architectures or retrieval strategies could enable longer contexts.
- What evidence would resolve it: Experiments showing comparable retrieval quality with reduced computational complexity, such as using sparse attention patterns or hierarchical document representations.

### Open Question 3
- Question: How well does RPT generalize to domains outside of books, code, and mathematical writing, and can the retrieval mechanism adapt to different types of long-range dependencies?
- Basis in paper: [explicit] The paper evaluates on 4 specific domains (books, code, math papers, and PG19 books) and shows consistent improvements, but does not test on other domains.
- Why unresolved: Different domains may have different patterns of long-range dependencies (e.g., dialogue, news articles, scientific papers). The effectiveness of RPT's retrieval mechanism may vary across domains.
- What evidence would resolve it: Experiments on a diverse set of long-document domains, analyzing retrieval quality and perplexity improvements, and potentially adapting the retrieval training signal to domain-specific characteristics.

## Limitations
- The paper relies heavily on perplexity as the primary evaluation metric, which may not fully capture the semantic quality of retrieved chunks
- Several critical implementation details are underspecified, including tokenization parameters and scoring LM implementation
- Experiments are limited to four specific long-range language modeling tasks, with no evaluation on downstream applications

## Confidence
- High confidence: The core architectural contribution of RPT (jointly training retriever and LM from scratch with semantic supervision) is well-supported by the experimental results showing consistent perplexity improvements across all four datasets
- Medium confidence: The claim that self-retrieval leads to better adaptation than using external representations is supported by ablation studies but limited to specific implementation choices
- Low confidence: The assertion that semantic supervision is superior to lexical supervision is based on perplexity improvements alone, without direct evaluation of semantic quality

## Next Checks
- Check 1: Implement qualitative analysis of retrieved chunks by sampling predictions from RPT and manually evaluating semantic relevance
- Check 2: Conduct ablation studies varying chunk size (32, 64, 128 tokens) to understand impact on retrieval quality and perplexity
- Check 3: Evaluate RPT on a downstream task such as question answering or summarization to demonstrate practical utility beyond language modeling