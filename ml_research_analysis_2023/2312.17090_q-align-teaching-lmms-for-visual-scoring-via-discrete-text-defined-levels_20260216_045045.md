---
ver: rpa2
title: 'Q-Align: Teaching LMMs for Visual Scoring via Discrete Text-Defined Levels'
arxiv_id: '2312.17090'
source_url: https://arxiv.org/abs/2312.17090
tags:
- quality
- image
- training
- visual
- lign
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of teaching large multi-modality
  models (LMMs) to predict visual scores aligned with human opinions. The core method
  idea is to emulate the subjective process of human raters by teaching LMMs with
  text-defined rating levels instead of direct scores.
---

# Q-Align: Teaching LMMs for Visual Scoring via Discrete Text-Defined Levels

## Quick Facts
- arXiv ID: 2312.17090
- Source URL: https://arxiv.org/abs/2312.17090
- Reference count: 15
- Primary result: Achieves SOTA performance on image quality assessment, image aesthetic assessment, and video quality assessment using discrete text-defined levels

## Executive Summary
Q-Align addresses the challenge of teaching large multi-modality models (LMMs) to predict visual scores aligned with human opinions. The core insight is that LMMs naturally respond with qualitative adjectives rather than numerical values when evaluating visual content. By training LMMs to output discrete text-defined rating levels (excellent, good, fair, poor, bad) instead of direct scores, Q-Align leverages the LMMs' existing linguistic strengths while emulating the subjective human rating process. The approach achieves state-of-the-art performance across three visual scoring tasks and introduces OneAlign, a unified model that handles all tasks simultaneously.

## Method Summary
Q-Align converts continuous MOS values from visual scoring datasets into five discrete text-defined rating levels using equidistant intervals. These levels are formatted into instruction-response pairs for visual instruction tuning of LMMs. During training, the model learns to predict the probability distribution over rating levels using cross-entropy loss. For inference, the model extracts log probabilities for each level, applies softmax pooling to obtain probabilities, and calculates a weighted average of level scores to produce the final predicted score. The unified OneAlign model combines datasets from image quality assessment, image aesthetic assessment, and video quality assessment through multitask learning.

## Key Results
- Achieves SOTA performance on IQA, IAA, and VQA benchmarks
- Demonstrates superior out-of-distribution generalization compared to score-based approaches
- OneAlign successfully unifies three visual scoring tasks in a single model
- Discrete-level training shows 1-3% SRCC/PLCC improvements over direct score regression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Teaching LMMs with discrete text-defined rating levels aligns better with how LMMs naturally respond and how human raters operate
- Core assumption: LMMs' language understanding capabilities are better suited to handling categorical qualitative labels than continuous numerical predictions
- Evidence anchors: Human raters use discrete levels in subjective studies; LMMs predominantly respond with qualitative adjectives
- Break condition: If LMMs could be easily trained to output accurate numerical scores directly

### Mechanism 2
- Claim: Equidistant interval conversion preserves sufficient information while simplifying the learning task
- Core assumption: The ordinal relationship between rating levels is preserved in equidistant interval mapping
- Evidence anchors: Adjacent levels in human rating are inherently equidistant; converted rating levels are sufficiently accurate as training labels
- Break condition: If equidistant mapping significantly distorts ordinal relationships or precision loss is too detrimental

### Mechanism 3
- Claim: Weighted average with softmax pooling accurately simulates human opinion collection
- Core assumption: LMM probability distribution over levels reflects confidence, and weighted average appropriately aggregates information
- Evidence anchors: MOS calculation mirrors weighted average of converted scores and frequencies; LMM-predicted probabilities substitute for human frequencies
- Break condition: If LMM probability distribution doesn't accurately reflect confidence or weighted average doesn't appropriately aggregate information

## Foundational Learning

- Concept: Human visual scoring processes
  - Why needed here: Understanding human rating stages (discrete levels → MOS calculation) informs LMM training and inference design
  - Quick check question: What are the three stages of human visual scoring, and how does each stage inform the LMM approach?

- Concept: LMM behavior and capabilities
  - Why needed here: Knowing LMMs naturally respond with adjectives guides training target selection and evaluation
  - Quick check question: How do LMMs typically respond when asked to evaluate visual content, and why is this relevant to the proposed approach?

- Concept: Multitask learning and dataset mixing
  - Why needed here: Combining IQA, IAA, and VQA datasets requires understanding multitask learning principles and domain mixing challenges
  - Quick check question: What are the potential benefits and challenges of mixing datasets from different visual scoring tasks?

## Architecture Onboarding

- Component map: Visual encoder -> Visual abstractor (1024→64 tokens) -> LLM + Tokenizer -> Cross-entropy loss
- Critical path: 1) Input encoded into embeddings 2) Abstractor reduces token count 3) LLM generates rating level response 4) Cross-entropy loss calculated
- Design tradeoffs: Pre-trained LLM vs. scratch training; token reduction vs. visual detail preservation; individual task models vs. unified multitask approach
- Failure signatures: Low SRCC/PLCC scores, high prediction variance, poor cross-dataset performance
- First 3 experiments: 1) Train on single IQA dataset (KonIQ) 2) Train on multiple IQA datasets for cross-dataset evaluation 3) Train unified model on IQA, IAA, VQA for multitask capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does discrete-level syllabus compare to other alignment strategies like reinforcement learning or direct score regression?
- Basis in paper: Paper mentions discrete-level proves far better than scores as learning targets
- Why unresolved: No comparison with reinforcement learning or other alignment strategies
- What evidence would resolve it: Experiments comparing discrete-level approach with reinforcement learning and direct score regression on same tasks

### Open Question 2
- Question: How does performance vary with different numbers of text-defined rating levels?
- Basis in paper: Uses five levels but doesn't explore impact of varying level count
- Why unresolved: No investigation of how level count affects performance
- What evidence would resolve it: Experiments with 3, 5, 7 levels comparing performance and efficiency

### Open Question 3
- Question: How does Q-Align performance compare to human raters?
- Basis in paper: Aims to align with human opinions but doesn't directly compare to human performance
- Why unresolved: Focuses on machine learning comparisons rather than human rater comparison
- What evidence would resolve it: Experiments comparing Q-Align with human raters on consistency, bias, and subtle quality differences

## Limitations
- Evidence for LMMs' "innate ability" to respond with adjectives is observational rather than experimental
- Equidistant interval mapping assumes uniform human perception of quality differences
- Performance improvements from multitask learning are modest (1-3% gains) and may come from increased data rather than knowledge transfer

## Confidence

**High confidence**: Core methodology of discrete-level conversion and weighted averaging is well-defined and reproducible; SOTA benchmark results are robust

**Medium confidence**: Claims about LMMs' natural tendency toward qualitative responses have reasonable support but lack rigorous comparative analysis

**Low confidence**: Claims that discrete-level training superiority is fundamentally tied to LMM behavior patterns are speculative and not well-supported

## Next Checks

1. Implement and train identical LMM architecture with direct numerical score regression to compare performance impact of discrete-level approach

2. Systematically vary discrete levels (3, 7, 9) and interval mapping strategies (equidistant vs. perceptually-weighted) to quantify design choice impacts

3. Conduct ablation studies on unified model to determine if multitask gains come from genuine knowledge transfer or increased capacity and training data