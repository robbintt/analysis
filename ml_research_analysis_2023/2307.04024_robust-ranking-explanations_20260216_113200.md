---
ver: rpa2
title: Robust Ranking Explanations
arxiv_id: '2307.04024'
source_url: https://arxiv.org/abs/2307.04024
tags:
- ranking
- features
- r2et
- thickness
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of making saliency map explanations\
  \ robust against adversarial attacks that manipulate gradient-based explanations.\
  \ The authors propose a new metric called \"explanation thickness\" that measures\
  \ the stability of the top-ranked salient features, which is more aligned with human\
  \ perception than existing \u2113p-norm based metrics."
---

# Robust Ranking Explanations

## Quick Facts
- arXiv ID: 2307.04024
- Source URL: https://arxiv.org/abs/2307.04024
- Reference count: 7
- Primary result: R2ET algorithm improves explanation robustness against stealthy attacks while maintaining accuracy

## Executive Summary
This paper addresses the critical problem of making saliency map explanations robust against adversarial attacks that manipulate gradient-based explanations. The authors propose a novel metric called "explanation thickness" that measures the stability of top-ranked salient features, which aligns better with human perception than traditional ℓp-norm based metrics. They develop an efficient algorithm called R2ET that optimizes surrogate bounds of explanation thickness to anchor top salient features, connecting it theoretically to adversarial training. Extensive experiments across various datasets and neural network architectures demonstrate that R2ET achieves superior explanation robustness under stealthy attacks while retaining model accuracy compared to state-of-the-art baselines.

## Method Summary
The paper introduces ranking explanation thickness as a metric for measuring saliency map stability, then derives surrogate bounds for efficient optimization. R2ET maximizes these bounds while minimizing Hessian norm to anchor top salient features. The algorithm is connected to adversarial training through a min-max optimization framework. The method is evaluated on tabular (Bank, Adult, COMPAS), image (CIFAR-10, MNIST), and graph (BP, ADHD) datasets using gradient-based saliency (SimpleGrad) and tested against ERAttack and MSE attacks.

## Key Results
- R2ET achieves higher P@k scores under ERAttack (stealthy attack) compared to Vanilla, WD, SP, Est-H, and AT baselines
- R2ET maintains model accuracy (AUC) while improving explanation robustness
- Explanation thickness correlates with the number of attack iterations needed to swap top features
- R2ET demonstrates consistent performance across diverse datasets and architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explanation thickness directly correlates with robustness of top-k salient features against adversarial manipulation
- Mechanism: Thickness measures the probability that the relative ranking of feature pairs remains unchanged under small perturbations, making it harder for attacks to swap top features
- Core assumption: Human perception focuses on top-k features, so stabilizing their ranking is more important than overall explanation similarity
- Evidence anchors:
  - [abstract] "We define explanation thickness for measuring salient features ranking stability"
  - [section] "the ℓp norm considers the importance of all features equally. Such inconsistency between the ℓp metric and the modus operandi of human perception can lead to mistrust of the model"
  - [corpus] "Found 25 related papers" - suggests active research area but no direct evidence of thickness metric in corpus
- Break condition: If perturbations can be large enough to overcome the thickness-bound relationship, or if human perception actually cares about feature importance beyond just ranking order

### Mechanism 2
- Claim: Optimizing the surrogate bound of thickness is computationally efficient compared to direct thickness optimization
- Mechanism: The surrogate bound depends only on the original input and Hessian information, avoiding costly sampling of perturbed points and line integrals
- Core assumption: The surrogate bound is tight enough that maximizing it effectively maximizes true thickness
- Evidence anchors:
  - [section] "Directly optimizing Θ in Eq. (4) requires M1 × M2 × 2 backward propagations... To avoid sampling x′ and x(t), we derive a lower bound of Θ"
  - [section] "We reveal another motivation for minimizing Hessian norm: rather than smoothing the curvature, we aim to tighten the bounds of thickness"
  - [corpus] Weak - no direct evidence of this computational efficiency claim in corpus
- Break condition: If the surrogate bound becomes loose (e.g., when Hessian norm cannot be sufficiently reduced), the optimization may not effectively improve true thickness

### Mechanism 3
- Claim: R2ET achieves connection between explanation robustness and adversarial training without expensive inner-loop optimization
- Mechanism: The thickness regularizer implicitly performs a min-max optimization similar to adversarial training but uses a regularization approach that avoids solving the inner maximization for each sample
- Core assumption: The regularization term in Eq. (6) effectively approximates the effect of adversarial training
- Evidence anchors:
  - [section] "we prove a connection between R2ET and adversarial training. Theoretically, we derive surrogate bounds of ranking explanation thickness"
  - [section] "Proposition 3.5. The optimization problem in Eq. (6) is equivalent to the following min-max problem"
  - [corpus] "Towards Faithful Explanations for Text Classification with Robustness Improvement and Explanation Guided Training" - related work but no direct evidence of R2ET connection
- Break condition: If the equivalence between R2ET and adversarial training breaks down (e.g., with non-standard loss functions or architectures), or if the regularization term cannot effectively approximate the adversarial objective

## Foundational Learning

- Concept: Local Pairwise Ranking Thickness
  - Why needed here: Forms the theoretical foundation for measuring explanation robustness
  - Quick check question: What does a thickness value of 1 indicate about feature rankings?

- Concept: Hessian-based curvature analysis
  - Why needed here: Critical for understanding why existing defenses fail and how R2ET improves upon them
  - Quick check question: How does Hessian norm relate to the bounds on explanation thickness?

- Concept: Adversarial training and min-max optimization
  - Why needed here: Essential for understanding the theoretical connection between R2ET and existing robust training methods
  - Quick check question: What is the computational bottleneck in standard adversarial training that R2ET avoids?

## Architecture Onboarding

- Component map:
  Model training with R2ET regularizer -> Explanation generation (SimpleGrad) -> Adversarial attack evaluation (ERAttack, MSE) -> Performance metrics (P@k, thickness, AUC)

- Critical path:
  1. Train model with R2ET regularizer (thickness and Hessian norm terms)
  2. Generate explanations for clean and attacked inputs
  3. Evaluate robustness using P@k and thickness metrics
  4. Compare against baselines on multiple datasets

- Design tradeoffs:
  - R2ET vs. R2ET\H: Tradeoff between computational efficiency and robustness (R2ET\H may perform better on small feature sets)
  - k value selection: Balancing between computational cost and coverage of top features
  - Regularization weights (λ1, λ2): Balancing accuracy vs. robustness

- Failure signatures:
  - Low P@k under ERAttack indicates vulnerability to ranking-based attacks
  - High P@k under MSE attack but low under ERAttack indicates MSE attack ineffectiveness
  - Significant drop in clean AUC indicates robustness-accuracy tradeoff issues

- First 3 experiments:
  1. Compare P@k under ERAttack between R2ET and baseline models on tabular datasets
  2. Measure correlation between thickness and number of attack iterations to first feature swap
  3. Visualize saliency maps before/after attack for R2ET vs. Vanilla models on MNIST

## Open Questions the Paper Calls Out
The paper mentions plans to consider R2ET in NLP problems in the future but does not provide any results or specific research questions about NLP applications.

## Limitations
- Primary validation relies on synthetic attacks rather than real-world adversarial scenarios
- Connection between explanation thickness and human perception remains theoretical without user studies
- Computational overhead of R2ET compared to standard training is not fully characterized

## Confidence

- **High Confidence**: The theoretical derivation of explanation thickness and its surrogate bounds is mathematically sound. The connection between R2ET and adversarial training is rigorously proven.
- **Medium Confidence**: Experimental results show consistent improvements across multiple datasets and architectures, but the evaluation relies heavily on synthetic attack metrics.
- **Low Confidence**: The claim that explanation thickness better aligns with human perception than ℓp-norm metrics lacks empirical validation through human studies or real-world attack scenarios.

## Next Checks

1. **Human Perception Validation**: Conduct user studies comparing human judgment of explanation quality between R2ET and baseline methods, particularly focusing on whether humans value top-k feature stability as the paper assumes.

2. **Real-World Attack Evaluation**: Test R2ET's robustness against black-box attacks and transfer-based attacks that weren't part of the training process, to assess practical security guarantees.

3. **Computational Overhead Analysis**: Measure and report the exact computational overhead of R2ET during training (additional epochs, training time, memory usage) and at inference time, comparing against standard adversarial training methods.