---
ver: rpa2
title: 'Multilingual Language Models are not Multicultural: A Case Study in Emotion'
arxiv_id: '2307.01370'
source_url: https://arxiv.org/abs/2307.01370
tags:
- emotion
- english
- cultural
- feel
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether widely-used multilingual language
  models reflect cultural variation in emotion, a critical factor for emotionally
  sensitive applications. The authors examine two aspects: emotion embeddings and
  language generation.'
---

# Multilingual Language Models are not Multicultural: A Case Study in Emotion

## Quick Facts
- **arXiv ID:** 2307.01370
- **Source URL:** https://arxiv.org/abs/2307.01370
- **Reference count:** 18
- **Key outcome:** Multilingual language models are Anglocentric and fail to capture cultural variation in emotion, leading to inappropriate responses in emotionally sensitive applications.

## Executive Summary
This paper investigates whether widely-used multilingual language models reflect cultural variation in emotion, a critical factor for emotionally sensitive applications. The authors examine two aspects: emotion embeddings and language generation. They find that multilingual embeddings are Anglocentric, failing to preserve the embedding space of monolingual non-English models. Additionally, GPT-3 completion probabilities do not reflect known cultural differences in emotion expression between the US and Japan for Pride and Shame. The key finding is that multilingual language models do not successfully learn culturally appropriate nuances of emotion, anchoring non-English emotion embeddings to English and generating text that does not align with users' expected cultural norms. This highlights the need for future research on more culturally-aware multilingual models.

## Method Summary
The authors evaluate multilingual language models' ability to capture cultural variation in emotion by comparing embeddings from monolingual and multilingual RoBERTa models, projecting emotion embeddings into the Valence-Arousal plane, and analyzing GPT-3 completion probabilities for Pride and Shame-related scenarios in English and Japanese. They also conduct a user study to assess the cultural appropriateness of GPT-3.5 and GPT-4 responses to culturally sensitive prompts in English and native languages.

## Key Results
- Multilingual embeddings are Anglocentric, failing to preserve the embedding space of monolingual non-English models.
- GPT-3 completion probabilities do not reflect known cultural differences in emotion expression between the US and Japan for Pride and Shame.
- Multilingual language models fail to provide culturally-appropriate emotional responses, especially in Eastern languages, as revealed by a user study.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Implicit and explicit alignment in multilingual LMs inappropriately anchors non-English emotion embeddings to English.
- **Mechanism:** Training data with parallel corpora (e.g., Wikipedia) causes implicit alignment by mapping similar concepts across languages. Explicit alignment via knowledge distillation forces English and translated sentences to share identical embedding vectors, directly anchoring non-English emotions to their English counterparts.
- **Core assumption:** The more similar two models are, the more similarly they embed the same phrases in embedding space; thus, measuring similarity between monolingual and multilingual emotion embeddings reveals alignment effects.
- **Evidence anchors:**
  - [abstract] "embeddings obtained from LMs (e.g., XLM-RoBERTa) are Anglocentric"
  - [section] "XLM-RoBERTa was trained on text that includes parallel and comparable corpora (e.g., Wikipedia)... causes implicit alignment in training. Worse, XLM-RoBERTa variants trained via multilingual knowledge distillation... enforce English sentences and their translations to map to the same point in embedding space"
  - [corpus] "Neighbor FMR=0.473" indicates moderate relatedness to existing work on multilingual LM alignment.

### Mechanism 2
- **Claim:** Multilingual LMs fail to preserve the embedding space structure of monolingual non-English models for emotions.
- **Mechanism:** Distance-based similarity analysis shows that multilingual embeddings for non-English emotions are more similar to English emotions than to their monolingual non-English counterparts, indicating loss of language-specific emotion representation structure.
- **Core assumption:** Lower distance-based similarity between monolingual and multilingual embeddings indicates that the multilingual model has altered the emotion representation space.
- **Evidence anchors:**
  - [abstract] "multilingual embeddings are Anglocentric, failing to preserve the embedding space of monolingual non-English models"
  - [section] "lower similarities for non-English languages indicate that XLM-RoBERTa embeds non-English emotions differently compared to monolingual models"
  - [corpus] "Average neighbor FMR=0.473" suggests related work exists on multilingual LM embeddings.

### Mechanism 3
- **Claim:** GPT-3 completion probabilities do not reflect known cultural differences in emotion expression between the US and Japan for Pride and Shame.
- **Mechanism:** Analysis of GPT-3 log probabilities for Pride and Shame-related words following culturally relevant scenarios shows no consistent pattern where Pride words are more likely in English or Shame words in Japanese, contrary to cultural expectations.
- **Core assumption:** If GPT-3 had learned appropriate cultural norms, we would expect to see higher probabilities for culturally appropriate emotion words in each language.
- **Evidence anchors:**
  - [abstract] "generative LMs (e.g., ChatGPT) reflect Western norms, even when responding to prompts in other languages"
  - [section] "we do not see any consistent evidence that Pride is more likely to be expressed in English or Shame is more likely to be expressed in Japanese"
  - [corpus] "Neighbor FMR=0.473" indicates some relatedness to work on LM cultural alignment.

## Foundational Learning

- **Concept:** Cultural variation in emotion
  - **Why needed here:** The paper's core premise is that emotions are experienced and expressed differently across cultures, and multilingual LMs must reflect this variation to be effective for emotionally sensitive tasks.
  - **Quick check question:** Can you name two emotions that are expressed differently in Western versus Eastern cultures according to the paper?
- **Concept:** Distance-based similarity for comparing embedding spaces
  - **Why needed here:** The paper uses this method to compare emotion embeddings between monolingual and multilingual models without needing to align their embedding spaces directly.
  - **Quick check question:** How does the distance-based similarity metric determine if two models embed emotions similarly?
- **Concept:** Valence-Arousal plane for visualizing emotion embeddings
  - **Why needed here:** The paper projects emotion embeddings onto this plane to visualize and compare cultural differences in Pride and Shame between the US and Japan.
  - **Quick check question:** What are the two dimensions of the circumplex model of affect used to project emotions?

## Architecture Onboarding

- **Component map:** Monolingual RoBERTa models -> Multilingual RoBERTa models -> GPT-3/GPT-3.5/GPT-4 -> Distance-based similarity metrics -> Valence-Arousal plane projection -> User study
- **Critical path:** 1) Select and load monolingual and multilingual RoBERTa models, 2) Generate emotion embeddings using contextualized phrases, 3) Compute distance-based similarity between models, 4) Project embeddings onto Valence-Arousal plane, 5) Analyze GPT-3/GPT-3.5/GPT-4 completions for cultural appropriateness.
- **Design tradeoffs:** Using machine translation for non-English emotion phrases introduces potential inaccuracies; relying on user studies with small sample sizes may not capture full cultural variation; focusing on only four high-resource languages limits generalizability.
- **Failure signatures:** Low distance-based similarity between monolingual and multilingual emotion embeddings indicates Anglocentric anchoring; GPT completions not reflecting expected cultural norms suggest model bias; inconclusive Valence-Arousal projections indicate embeddings not capturing cultural variation.
- **First 3 experiments:**
  1. Compute distance-based similarity between monolingual and multilingual RoBERTa emotion embeddings for all 271 emotions.
  2. Project English and Japanese Pride/Shame embeddings onto the Valence-Arousal plane and compare their positions.
  3. Analyze GPT-3 log probabilities for Pride and Shame words following culturally relevant scenarios in English and Japanese.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do multilingual language models perform on low-resource languages compared to high-resource languages in capturing cultural nuances of emotion?
- **Basis in paper:** [inferred] The authors analyzed four high-resource languages (English, Spanish, Chinese, and Japanese) and acknowledged that their analysis could have benefited from more languages, especially low-resource ones.
- **Why unresolved:** The paper only focused on high-resource languages, leaving the performance of multilingual models on low-resource languages unexplored.
- **What evidence would resolve it:** Conducting a similar study with a diverse set of low-resource languages and comparing the results with those obtained for high-resource languages would provide insights into the performance of multilingual models on capturing cultural nuances of emotion across different language resource levels.

### Open Question 2
- **Question:** Do monolingual language models in non-English languages successfully encode their respective culture's norms and values?
- **Basis in paper:** [inferred] The authors encouraged future work to investigate whether state-of-the-art monolingual models in non-English languages succeed in encoding the respective culture's norms.
- **Why unresolved:** The paper primarily focused on multilingual models and did not extensively explore the performance of monolingual models in non-English languages.
- **What evidence would resolve it:** Conducting a comprehensive study on the cultural awareness of monolingual models in various non-English languages, using benchmarks that measure cultural awareness, would provide insights into their ability to encode cultural norms and values.

### Open Question 3
- **Question:** How do multilingual language models perform in capturing cultural nuances of emotions beyond Pride and Shame?
- **Basis in paper:** [inferred] The authors acknowledged that their analysis of cultural differences in emotion was specific to Pride and Shame and that analyzing other differences could provide stronger results.
- **Why unresolved:** The paper focused primarily on Pride and Shame, leaving the performance of multilingual models on other emotions unexplored.
- **What evidence would resolve it:** Conducting a similar study on a broader range of emotions, such as joy, anger, fear, and disgust, across different cultures, would provide insights into the performance of multilingual models in capturing cultural nuances of emotions beyond Pride and Shame.

## Limitations
- The analysis of cultural differences in emotion is inherently subjective, and the paper's findings may be limited by the specific emotions and scenarios chosen for evaluation.
- The use of machine translation for non-English emotion phrases introduces potential inaccuracies in the embeddings and user study responses.
- The focus on only four high-resource languages limits the generalizability of the findings to low-resource languages and cultures.

## Confidence
- **High Confidence**: The finding that multilingual language models fail to preserve the embedding space structure of monolingual non-English models for emotions, as evidenced by lower distance-based similarity between embeddings.
- **Medium Confidence**: The conclusion that multilingual language models are Anglocentric, based on the observation that they inappropriately anchor non-English emotion embeddings to English. This finding is supported by the alignment techniques used in training, but the specific mechanisms may vary across models.
- **Low Confidence**: The interpretation of inconclusive results when projecting emotion embeddings into the Valence-Arousal plane. While the embeddings may not capture cultural variation in Pride and Shame, the lack of consistent patterns could also be due to limitations in the visualization method or the specific emotions chosen for analysis.

## Next Checks
1. **Expand language coverage**: Evaluate the Anglocentricity of multilingual language models for a more diverse set of languages, including low-resource languages, to assess the generalizability of the findings.
2. **Investigate alternative alignment methods**: Explore the effects of different alignment techniques, such as adversarial training or contrastive learning, on the preservation of non-English emotion embedding spaces.
3. **Conduct larger-scale user studies**: Recruit a more diverse and representative sample of participants for user studies to capture a wider range of cultural variation in emotion expression and improve the reliability of the findings.