---
ver: rpa2
title: Indonesian Automatic Speech Recognition with XLSR-53
arxiv_id: '2308.11589'
source_url: https://arxiv.org/abs/2308.11589
tags:
- data
- speech
- language
- voice
- indonesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed an Indonesian Automatic Speech Recognition
  (ASR) system using the XLSR-53 pre-trained model, which is based on cross-lingual
  speech representations. The goal was to reduce the need for large training datasets
  in non-English languages while achieving competitive performance.
---

# Indonesian Automatic Speech Recognition with XLSR-53

## Quick Facts
- arXiv ID: 2308.11589
- Source URL: https://arxiv.org/abs/2308.11589
- Authors: Multiple authors
- Reference count: 25
- One-line primary result: Achieved 20% WER without LM, reduced to 12% with 5-gram KenLM on Indonesian ASR

## Executive Summary
This study developed an Indonesian Automatic Speech Recognition system using the XLSR-53 pre-trained model, which leverages cross-lingual speech representations to reduce the need for large Indonesian-specific training datasets. The researchers fine-tuned XLSR-53 on a combined dataset of 24 hours, 18 minutes, and 1 second from three sources: TITML-IDN, Magic Data, and Common Voice. The system achieved a Word Error Rate of 20% without a language model, which improved to 12% when using a 5-gram KenLM language model, demonstrating the effectiveness of language modeling in improving ASR accuracy.

## Method Summary
The method involved fine-tuning the XLSR-53 pre-trained model on a combined Indonesian speech dataset totaling 24 hours, 18 minutes, and 1 second. The datasets were preprocessed by converting audio to 16kHz .wav format and normalizing transcriptions to lowercase alphabetic characters. The model was trained using CTC loss with a learning rate of 3e-4, batch size of 12, and 20 epochs. A 5-gram KenLM language model was trained on transcriptions from multiple sources and integrated with the ASR system to improve accuracy. The evaluation was performed on the Common Voice test split using Word Error Rate as the primary metric.

## Key Results
- Achieved 20% WER without language model on Indonesian ASR task
- Reduced WER to 12% (approximately 8% improvement) using 5-gram KenLM
- Demonstrated effectiveness of cross-lingual transfer learning with limited data
- Outperformed previous wav2vec 2.0 based models on Indonesian ASR

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual pre-training on XLSR-53 enables effective learning of Indonesian speech representations with limited data.
- Mechanism: XLSR-53 is pre-trained on 53 languages including Indonesian, allowing it to share latent speech representations across languages. This cross-lingual transfer learning means the model can leverage speech features from other languages to improve performance on Indonesian without needing extensive Indonesian-specific data.
- Core assumption: Indonesian speech features are sufficiently similar to features in other languages used during pre-training.
- Evidence anchors: [abstract] "XLSR stands for cross-lingual speech representations. The use of this XLSR-53 pre-trained model is to significantly reduce the amount of training data in non-English languages required to achieve a competitive Word Error Rate (WER)." [section] "The experiment conducted by Conneau, et al. [7] showed that cross-language pre-training significantly outperformed monolingual pre-training."

### Mechanism 2
- Claim: Combining multiple Indonesian datasets improves ASR performance by increasing data diversity and coverage.
- Mechanism: The study combines TITML-IDN, Magic Data, and Common Voice datasets to create a larger, more diverse training corpus. This approach exposes the model to varied speakers, accents, recording conditions, and vocabulary, leading to better generalization.
- Core assumption: The combined datasets are sufficiently representative of the Indonesian language's variability.
- Evidence anchors: [abstract] "The total amount of data used in this study is 24 hours, 18 minutes, and 1 second: (1) TITML-IDN 14 hours and 31 minutes; (2) Magic Data 3 hours and 33 minutes; and (3) Common Voice 6 hours, 14 minutes, and 1 second." [section] "In addition, this study also tries to contribute to a new type of recipe when using a pre-trained model, where before entering the fine-tuning step, it first combines the dataset first, then divides it into training, validation, and testing."

### Mechanism 3
- Claim: Language modeling significantly reduces WER by providing contextual word prediction.
- Mechanism: A 5-gram KenLM language model is trained on a large text corpus (including OSCAR, TITML-IDN, Magic Data, and Common Voice transcriptions) and integrated with the ASR system. This model predicts likely word sequences based on context, helping correct recognition errors.
- Core assumption: The language model's training corpus is sufficiently large and representative of the Indonesian language to provide accurate predictions.
- Evidence anchors: [abstract] "WER can be decreased by around 8% using a language model, resulted in WER from 20% to 12%." [section] "The results of making the language model are saved in the form of an .arpa file... The resulting language model format consists of \data\ which gives multiple entries for each N-gram."

## Foundational Learning

- Concept: Cross-lingual speech representation learning
  - Why needed here: Understanding how XLSR-53 leverages features from multiple languages to improve performance on Indonesian without extensive Indonesian-specific training data.
  - Quick check question: What is the key advantage of cross-lingual pre-training compared to monolingual pre-training for low-resource languages?

- Concept: Connectionist Temporal Classification (CTC) loss
  - Why needed here: CTC is the core training algorithm used for sequence-to-sequence mapping in ASR, allowing the model to align variable-length audio with text output.
  - Quick check question: How does CTC handle the alignment problem between audio frames and text characters in speech recognition?

- Concept: N-gram language modeling
  - Why needed here: Understanding how the 5-gram KenLM model provides contextual predictions to improve ASR accuracy.
  - Quick check question: What is the relationship between n-gram order and the model's ability to capture longer-range dependencies in language?

## Architecture Onboarding

- Component map:
  Feature Encoder (7-layer CNN) -> Quantization Module (Product Quantization) -> Context Network (Transformer Encoder) -> CTC Classifier -> Language Model (KenLM 5-gram)

- Critical path:
  1. Audio preprocessing (16kHz sampling, normalization)
  2. Feature extraction via CNN
  3. Quantization to discrete units
  4. Transformer encoding with context
  5. CTC decoding to characters
  6. Optional LM integration for word-level correction

- Design tradeoffs:
  - Model size vs. performance: BASE vs. LARGE XLSR-53
  - Language model complexity vs. inference speed
  - Dataset diversity vs. potential domain mismatch
  - Fine-tuning vs. full training for resource constraints

- Failure signatures:
  - High WER on test data: Possible overfitting, insufficient data diversity, or domain mismatch
  - Poor LM integration: Mismatch between LM training corpus and ASR domain
  - Inconsistent results across datasets: Speaker or environmental variability not captured in training

- First 3 experiments:
  1. Baseline: Fine-tune XLSR-53 on single dataset (TITML-IDN) without LM, measure WER
  2. Multi-dataset: Fine-tune on combined datasets without LM, measure WER improvement
  3. LM integration: Apply 5-gram KenLM to best-performing multi-dataset model, measure WER reduction

## Open Questions the Paper Calls Out
- How does the performance of XLSR-53 compare to other pre-trained multilingual models like mSLAM or mBART when applied to Indonesian ASR tasks?
- What is the optimal balance between dataset size and language model complexity for Indonesian ASR using XLSR-53?
- How does XLSR-53 perform on Indonesian dialects and regional languages compared to standard Indonesian?

## Limitations
- Evaluation based on single test set (Common Voice) without comparison to alternative benchmarks
- Language model training corpus details are sparse, particularly regarding OSCAR subset used
- Lack of ablation studies to isolate contributions of dataset combination versus language modeling

## Confidence
- Claims about cross-lingual transfer and LM effectiveness: Medium
- Claims about dataset combination benefits: Low
- Claims about outperforming wav2vec 2.0: Low

## Next Checks
1. Replicate the experiments using alternative Indonesian speech datasets to verify robustness
2. Conduct ablation studies comparing single vs. combined datasets with and without LM to quantify individual contributions
3. Test the model on spontaneous or conversational Indonesian speech to assess real-world applicability