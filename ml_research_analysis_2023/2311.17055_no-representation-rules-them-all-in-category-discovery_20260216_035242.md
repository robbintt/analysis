---
ver: rpa2
title: No Representation Rules Them All in Category Discovery
arxiv_id: '2311.17055'
source_url: https://arxiv.org/abs/2311.17055
tags:
- clevr-4
- learning
- category
- which
- discovery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the Generalized Category Discovery (GCD) problem,
  where the goal is to classify all images in a dataset using categories from a partially
  labeled subset, or by discovering new categories. The key challenge is extrapolating
  the desired taxonomy from the labeled data rather than simply clustering based on
  natural groupings.
---

# No Representation Rules Them All in Category Discovery

## Quick Facts
- **arXiv ID**: 2311.17055
- **Source URL**: https://arxiv.org/abs/2311.17055
- **Reference count**: 40
- **Key outcome**: µGCD achieves 92.5% clustering accuracy on Clevr-4 and sets state-of-the-art on Semantic Shift Benchmark

## Executive Summary
This paper addresses the Generalized Category Discovery (GCD) problem where models must classify images using categories from a partially labeled subset while discovering new categories. The key insight is that existing benchmarks don't adequately test whether algorithms truly learn the desired taxonomy rather than just natural image groupings. To address this, the authors introduce Clevr-4, a synthetic dataset with four independent taxonomies (shape, texture, color, count), revealing that even strong unsupervised models fail to perform consistently across all taxonomies. The authors propose µGCD, a mean-teacher inspired method that substantially outperforms existing GCD approaches on Clevr-4 and achieves state-of-the-art results on real-world benchmarks.

## Method Summary
µGCD is a two-phase method for Generalized Category Discovery. First, it trains a backbone feature extractor using a standard GCD baseline loss on the labeled subset. Second, it appends a cosine classifier and fine-tunes using a mean-teacher setup where a teacher network (with EMA-updated parameters) provides pseudo-labels for a student network. The student uses strong augmentations while the teacher uses weak ones, with losses including supervised classification, pseudo-label consistency, and entropy regularization to encourage exploration of all categories.

## Key Results
- µGCD achieves 92.5% clustering accuracy on Clevr-4, significantly outperforming existing GCD methods
- Sets new state-of-the-art on Semantic Shift Benchmark across all four datasets (CUB, Stanford Cars, FGVC-Aircraft, Herbarium19)
- Demonstrates particular effectiveness on Herbarium19 (55.8% accuracy), the most challenging dataset with long-tailed distribution and 19 classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mean-teacher setup improves pseudo-label quality for new categories in GCD
- Mechanism: Teacher parameters are updated via EMA of student, making them less sensitive to noisy pseudo-labels, thus producing cleaner labels for novel classes
- Core assumption: Slow teacher updates provide more stable pseudo-labels than directly using student predictions
- Evidence anchors:
  - [abstract]: "The slowly updated teacher is less affected by the noisy pseudo-labels which it produces, allowing clean pseudo-labels to be produced for new categories"
  - [section 5]: "The teacher is an identical architecture with parameters updated with the Exponential Moving Average (EMA) of the student"
- Break condition: If teacher becomes too stale and diverges from current student state, pseudo-labels may become irrelevant

### Mechanism 2
- Claim: Cosine classifier with entropy regularization prevents collapse to labeled categories
- Mechanism: Normalized weight vectors and entropy maximization encourage balanced predictions across all categories, preventing the model from ignoring novel class vectors
- Core assumption: Without weight normalization and entropy regularization, classification vectors for novel categories will shrink to zero magnitude
- Evidence anchors:
  - [section 7.1]: "we can see that the norms of vectors which are not supervised by ground truth labels (indices 101-200) fall substantially"
  - [section 7.1]: "if we further remove the entropy regularization term... the magnitudes of the 'Old' class vectors... increases dramatically"
- Break condition: If regularization strength is too high, it may over-penalize confident predictions even for correct classes

### Mechanism 3
- Claim: Strong data augmentation for student network improves generalization in GCD
- Mechanism: Aggressive cropping and other strong augmentations prevent overfitting to specific image features, encouraging learning of more generalizable representations
- Core assumption: Stronger augmentations provide better regularization than weaker ones in the GCD context
- Evidence anchors:
  - [section 5]: "we pass different views of the same instance to the student and teacher networks. We generate a strong augmentation which is passed to the student network"
  - [section B.5]: "we find that an aggressive cropping strategy, as well as a strong base augmentation, is critical for strong performance"
- Break condition: If augmentations are too strong, they may destroy semantic content needed for category discovery

## Foundational Learning

- **Concept**: Exponential Moving Average (EMA) for teacher networks
  - Why needed here: Provides stable target distribution for student training in mean-teacher framework
  - Quick check question: What happens to teacher parameters if EMA momentum is set to 1.0?

- **Concept**: Cosine similarity for classification
  - Why needed here: Prevents weight vector magnitude from dominating classification decisions, ensuring balanced treatment of all categories
  - Quick check question: How does weight normalization affect the decision boundary between classes?

- **Concept**: Entropy regularization in classification
  - Why needed here: Encourages exploration of all class categories rather than collapsing predictions to a few dominant classes
  - Quick check question: What would happen to prediction distribution if entropy regularization coefficient is set to zero?

## Architecture Onboarding

- **Component map**: 
  Backbone feature extractor (Φ) -> Classification head (g) -> Student network (fθS = g ◦ Φ) with strong augmentations -> Teacher network (fθT = g ◦ Φ) with weak augmentations and EMA updates

- **Critical path**:
  1. Initialize backbone with GCD baseline loss training
  2. Append and initialize classification head
  3. Generate strong/weak augmentations for student/teacher
  4. Compute pseudo-labels from teacher, predictions from student
  5. Calculate losses and update student parameters
  6. Update teacher parameters via EMA

- **Design tradeoffs**:
  - Strong vs weak augmentations: Strong for student improves generalization but may destroy semantics; weak for teacher ensures stable pseudo-labels
  - EMA momentum: Higher values provide more stable teachers but may lag behind student learning; lower values track student better but with more noise
  - Temperature scaling: Lower teacher temperature produces sharper pseudo-labels but may reduce diversity

- **Failure signatures**:
  - Performance collapse on specific taxonomies (like shape in Clevr-4) indicates local optima in classification head initialization
  - Very high accuracy on old classes but poor on new classes suggests pseudo-label quality issues
  - Degraded performance with stronger augmentations indicates semantic content destruction

- **First 3 experiments**:
  1. Compare with/without teacher EMA updates to validate stability benefit
  2. Test different EMA momentum schedules (constant vs cosine decay)
  3. Evaluate impact of strong vs weak augmentations on student network performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can category discovery algorithms effectively handle datasets with multiple, statistically independent taxonomies, like Clevr-4, while maintaining performance across all taxonomies?
- Basis in paper: [explicit] The paper introduces Clevr-4 as a benchmark dataset with four equally valid taxonomies (shape, texture, color, count) to better study the GCD problem.
- Why unresolved: Existing GCD benchmarks typically only contain labels for a single clustering of the data, making it difficult to evaluate performance across multiple taxonomies. The paper demonstrates that even strong unsupervised models fail on Clevr-4, highlighting the challenge of handling multiple independent taxonomies.
- What evidence would resolve it: Developing and evaluating GCD algorithms on Clevr-4 and other datasets with multiple independent taxonomies, comparing their performance across all taxonomies and identifying strategies to improve performance on each taxonomy.

### Open Question 2
- Question: What are the key factors that contribute to the biases observed in pre-trained models when used for category discovery, and how can these biases be mitigated or leveraged to improve performance?
- Basis in paper: [explicit] The paper investigates the limitations of large-scale pre-training for category discovery and finds that pre-trained representations develop certain biases that limit their performance. For instance, most models are strongly biased towards shape, while MAE exhibits a color bias.
- Why unresolved: Understanding the factors contributing to these biases and how to effectively mitigate or leverage them is crucial for improving the performance of category discovery algorithms. The paper highlights the importance of carefully selecting the initialization for a given GCD task and points to the utility of Clevr-4 for doing so.
- What evidence would resolve it: Analyzing the impact of different pre-training objectives, architectures, and datasets on the biases observed in pre-trained models. Developing strategies to mitigate or leverage these biases, such as domain adaptation techniques or task-specific fine-tuning, and evaluating their effectiveness on GCD benchmarks.

### Open Question 3
- Question: How can category discovery algorithms effectively handle long-tailed distributions of classes, where some categories have significantly fewer examples than others, while maintaining performance across all classes?
- Basis in paper: [inferred] The paper evaluates the proposed µGCD method on the Herbarium19 dataset, which is highly challenging due to its long-tailed nature and large number of classes. The results demonstrate the potential of µGCD to handle long-tailed distributions.
- Why unresolved: Handling long-tailed distributions is a common challenge in machine learning, and category discovery is no exception. Developing algorithms that can effectively handle long-tailed distributions while maintaining performance across all classes is an important open question.
- What evidence would resolve it: Evaluating category discovery algorithms on long-tailed datasets with varying degrees of imbalance, comparing their performance across different class distributions, and identifying strategies to improve performance on tail classes, such as class-balanced sampling or meta-learning techniques.

## Limitations

- The Clevr-4 dataset, while useful for controlled experiments, may not fully capture the complexity and noise present in real-world scenarios
- The study focuses primarily on image classification tasks, and the generalizability to other data modalities remains unexplored
- The paper doesn't extensively explore the impact of different pre-training objectives on category discovery performance

## Confidence

- **High confidence**: The mechanism of EMA-based teacher updates improving pseudo-label stability is well-established in semi-supervised learning literature and supported by the empirical results
- **Medium confidence**: The specific formulation of cosine classifier with entropy regularization appears novel and effective, but could benefit from ablation studies on different regularization strengths
- **Medium confidence**: The Clevr-4 dataset provides valuable controlled experiments, but the artificial nature of the taxonomies may limit real-world applicability

## Next Checks

1. Conduct ablation studies on the entropy regularization coefficient to determine optimal values and understand its interaction with the EMA teacher
2. Test the method on additional real-world datasets with known taxonomy shifts to validate generalizability beyond the SSB benchmark
3. Implement a version of µGCD without the two-phase training (combining representation learning and fine-tuning in a single end-to-end process) to assess the necessity of the current approach