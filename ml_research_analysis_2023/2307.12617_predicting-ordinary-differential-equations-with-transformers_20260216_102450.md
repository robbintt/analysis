---
ver: rpa2
title: Predicting Ordinary Differential Equations with Transformers
arxiv_id: '2307.12617'
source_url: https://arxiv.org/abs/2307.12617
tags:
- nsode
- proged
- nsode-eps
- sindy
- pysr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a transformer-based method for inferring symbolic
  ordinary differential equations from noisy and irregularly sampled data. The approach
  uses a sequence-to-sequence model trained on a large corpus of synthetic ODEs, employing
  a two-hot encoding for constants to avoid rounding errors.
---

# Predicting Ordinary Differential Equations with Transformers

## Quick Facts
- arXiv ID: 2307.12617
- Source URL: https://arxiv.org/abs/2307.12617
- Reference count: 40
- Primary result: Transformer model directly predicts symbolic ODEs from noisy data without derivative estimation, outperforming existing symbolic regression methods in accuracy, robustness, and inference speed.

## Executive Summary
This work introduces a transformer-based method for inferring symbolic ordinary differential equations from noisy and irregularly sampled data. The approach uses a sequence-to-sequence model trained on a large corpus of synthetic ODEs, employing a two-hot encoding for constants to avoid rounding errors. The model directly predicts the symbolic form of the governing ODE without requiring derivative estimation. Empirical evaluations show that it outperforms most existing symbolic regression methods in accuracy, robustness to noise, and inference speed, especially when trained on noisy data.

## Method Summary
The method employs a transformer-based sequence-to-sequence model that maps observed trajectories to symbolic ODE expressions. Trajectories are encoded as IEEE-754 floating-point representations fed into a transformer encoder. The decoder generates mathematical expressions in prefix notation using a vocabulary that includes operators, variables, and constants. Constants are represented using two-hot encoding to preserve precision without rounding errors. The model is pretrained on millions of synthetic ODEs and their numerical solutions, enabling efficient one-shot inference through beam search decoding and numerical integration-based model selection.

## Key Results
- Outperforms most existing symbolic regression methods in accuracy and robustness to noise
- Achieves comparable or superior performance to best baselines while being significantly faster
- Direct prediction without derivative estimation maintains precision on irregularly sampled data

## Why This Works (Mechanism)

### Mechanism 1
Transformer models trained on large synthetic ODE datasets can directly map noisy, irregularly sampled trajectories to symbolic ODE forms without needing derivative estimation. The model treats each observed (t,y) pair as a sequence of IEEE-754 encoded floats fed into an encoder. The decoder outputs a sequence of tokens representing mathematical operators, variables, and constants in prefix notation. Two-hot encoding allows exact representation of real-valued constants without rounding, preserving precision. Core assumption: The underlying ODE is in canonical form dy/dt = f(y), autonomous, scalar, and first-order, and its symbolic form can be represented within the training distribution of operators and constant ranges.

### Mechanism 2
Pretraining on a large, diverse synthetic ODE dataset enables efficient one-shot inference without retraining per equation. By generating >3M ODEs and >63M numerical solutions, the transformer learns a general mapping from solution trajectories to symbolic forms. At inference, a single forward pass with beam search yields candidate expressions; model selection chooses the best fit by numerical integration. Core assumption: The training distribution is broad enough to cover the space of target ODEs in test sets; the pretraining data is sufficiently representative.

### Mechanism 3
Two-hot encoding of constants avoids loss of precision and enables exact symbolic representation, improving accuracy over one-hot with rounding. Constants are represented as αxi + βxi+1 where xi, xi+1 are adjacent tokens in a fixed grid. This creates a continuous interpolation space without discretization error, while still being compatible with cross-entropy loss. Core assumption: The fixed grid spacing is fine enough to capture required precision for target ODEs; the decoder can learn to interpolate effectively.

## Foundational Learning

- **IEEE-754 floating-point encoding**: Allows the model to handle the wide dynamic range of ODE solutions without numerical instability in the encoder. Quick check: How does IEEE-754 encoding help when a solution spans many orders of magnitude?

- **Two-hot encoding for continuous values**: Avoids rounding errors inherent in one-hot encoding of a finite constant vocabulary; preserves exactness needed for symbolic math. Quick check: Why is two-hot encoding preferable to one-hot for representing real-valued constants?

- **Beam search for decoding symbolic expressions**: Balances exploration of multiple candidate expressions with computational efficiency; model selection picks the best via numerical integration. Quick check: How does beam search help when multiple plausible symbolic forms exist for the same trajectory?

## Architecture Onboarding

- **Component map**: Trajectory (t,y) pairs → IEEE-754 encoding → linear projection → 6-layer encoder → context vectors → 6-layer decoder → token probabilities → beam search → numerical integration → R² model selection

- **Critical path**: 1. Encode trajectory (t,y) pairs as IEEE-754 bit patterns → linear layer → encoder; 2. Decoder autoregressively generates symbolic tokens; 3. Decode constants via two-hot interpolation; 4. Select best candidate by integrating predicted ODE and comparing to observations

- **Design tradeoffs**: Fixed operator/variable set vs. model flexibility (limits scope to autonomous scalar ODEs but simplifies learning); Two-hot constant encoding vs. one-hot (higher precision but larger vocabulary); Pretraining on synthetic data vs. few-shot learning (efficient inference but requires broad training coverage)

- **Failure signatures**: Poor R² despite syntactically correct expression (constants poorly estimated or trajectory mismatch); Decoder produces invalid syntax (decoding issue or training data bias); Runtime errors during integration (predicted ODE unstable or ill-posed)

- **First 3 experiments**: 1. Train on small synthetic dataset (1000 ODEs) with one-hot constants; compare R² and inference time to baseline; 2. Replace two-hot with one-hot constant encoding; measure precision loss in recovered constants; 3. Evaluate on held-out ODEs outside training operator set; document failure modes

## Open Questions the Paper Calls Out

- **Higher-order and multi-dimensional systems**: How well does the proposed model scale to higher-order and multi-dimensional systems of ODEs, such as those commonly found in real-world scientific applications? The paper acknowledges this as a key limitation, stating that the current model is restricted to "the arguably most simple class of differential equations: explicit autonomous scalar first-order ODEs." This remains unresolved as the paper does not provide experimental results or theoretical analysis on extending the model to more complex ODE systems.

- **Domain knowledge incorporation**: Can the model be adapted to incorporate domain knowledge or physical constraints into the ODE inference process, and how would this impact performance and interpretability? The paper mentions this as an interesting direction for future work but does not explore it in the current study.

- **Multiple trajectory inference**: How does the model's performance and robustness change when inferring ODEs from multiple observed trajectories of the same system, and can it effectively leverage this additional information? The paper explicitly states that the model currently cannot profit from multiple observations of the same process, as it can only be applied to each trajectory individually.

## Limitations
- Restricted to autonomous scalar first-order ODEs, limiting applicability to coupled or higher-order systems
- Reliance on pretraining on synthetic data raises questions about generalization to real-world datasets
- Two-hot encoding precision limited by grid resolution and vocabulary size constraints

## Confidence
- **High Confidence**: Transformer architecture can effectively map solution trajectories to symbolic ODE forms (demonstrated by superior performance metrics across multiple test sets and baselines)
- **Medium Confidence**: Two-hot encoding approach provides meaningful precision improvements over one-hot encoding (though quantitative comparisons with alternative constant representation methods would strengthen this claim)
- **Medium Confidence**: Pretraining on large synthetic datasets enables efficient one-shot inference (though the breadth of training distribution required for robust generalization remains unclear)

## Next Checks
1. **Distribution Coverage Analysis**: Systematically evaluate model performance on ODEs outside the training operator set and with qualitatively different dynamics (e.g., stiff systems, chaotic dynamics) to quantify the limits of pretraining generalization.

2. **Precision Scaling Study**: Compare two-hot encoding against one-hot encoding across varying grid resolutions and constant ranges, measuring the trade-off between precision, vocabulary size, and model performance.

3. **Real-World Application Test**: Apply the method to experimental datasets from physics or biology (e.g., pendulum motion, population dynamics) to assess performance on noisy, irregularly sampled real-world data beyond synthetic benchmarks.