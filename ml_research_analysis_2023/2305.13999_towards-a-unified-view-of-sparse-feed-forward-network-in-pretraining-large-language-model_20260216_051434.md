---
ver: rpa2
title: Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large
  Language Model
arxiv_id: '2305.13999'
source_url: https://arxiv.org/abs/2305.13999
tags:
- memory
- block
- dense
- baseline
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a unified framework to analyze sparse feed-forward
  networks (S-FFN) in language model pretraining, encompassing both sparse neural
  memory and mixture-of-experts (MoE) approaches. The framework decomposes S-FFN along
  two dimensions: memory block size and memory block selection method.'
---

# Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model

## Quick Facts
- arXiv ID: 2305.13999
- Source URL: https://arxiv.org/abs/2305.13999
- Reference count: 40
- Key outcome: Proposed Avg-K achieves 2.16 lower perplexity than dense baseline with <1% additional FLOPs

## Executive Summary
This paper presents a unified framework for analyzing sparse feed-forward networks (S-FFN) in transformer-based language model pretraining, covering both sparse neural memory and mixture-of-experts approaches. The authors decompose S-FFN architectures along two dimensions: memory block size and selection method. Through systematic experiments, they demonstrate that smaller memory block sizes consistently improve perplexity with minimal computational overhead, while selection methods using gating functions provide better efficiency trade-offs. Based on these insights, they propose Avg-K, a new routing method that achieves superior perplexity-FLOPs trade-offs compared to existing approaches.

## Method Summary
The authors systematically study S-FFN architectures by varying memory block sizes (g ∈ {1, 64, 256, 1024, 2048, 4096}) and selection methods (VanillaM direct, RandHash indirect, Avg-K hybrid). They pretrain models on a 453GB corpus for 60B tokens using 32 V100 GPUs, replacing FFN at every 6th layer of a 24-layer transformer. The proposed Avg-K method uses mean aggregated hidden states for block selection, achieving better perplexity-FLOPs trade-offs than both dense baselines and existing MoE approaches like Switch Transformer.

## Key Results
- Smaller memory block sizes consistently improve perplexity with minimal additional FLOPs
- Selection methods using gating functions achieve better FLOPs-perplexity trade-offs than direct methods
- Avg-K achieves 2.16 lower perplexity than dense baseline (16.96) with <1% additional FLOPs
- Avg-K outperforms Switch Transformer even without load balancing constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smaller memory block sizes improve perplexity with minimal computational cost increase
- Mechanism: Smaller block sizes increase model capacity by allowing more combinations of memory cells to be activated together
- Core assumption: Reducing block size increases the number of unique memory cell combinations that can be activated
- Evidence anchors:
  - [abstract]: "smaller memory block sizes lead to improved perplexity with minimal additional computational cost"
  - [section]: "smaller block size can keep improving the perplexity with little incurred extra FLOPs"
  - [corpus]: Weak - no corpus evidence found for this specific mechanism
- Break condition: If communication costs dominate due to all-to-all operations when block size becomes too small

### Mechanism 2
- Claim: Selection methods using gating functions achieve better FLOPs-perplexity trade-offs than direct methods
- Mechanism: Gating functions can leverage learned parameters to make more informed selection decisions, improving efficiency
- Core assumption: Gating functions can effectively learn to route tokens to appropriate memory blocks
- Evidence anchors:
  - [abstract]: "selection methods using gating functions achieve better FLOPs-perplexity trade-offs compared to direct methods"
  - [section]: "the selection method through a gating function, in general, improves the FLOPs-Perplexity trade-off"
  - [corpus]: Weak - no corpus evidence found for this specific mechanism
- Break condition: If gating function parameters are not expressive enough or if load balancing issues become severe

### Mechanism 3
- Claim: Avg-K achieves superior performance by using mean aggregated hidden states for selection
- Mechanism: Avg-K efficiently selects memory blocks based on mean aggregated hidden states, providing better routing decisions than existing methods
- Core assumption: Mean aggregated hidden states provide sufficient information for effective block selection
- Evidence anchors:
  - [abstract]: "Avg-K achieves 2.16 lower perplexity than a dense baseline (16.96) with less than 1% additional FLOPs"
  - [section]: "proposes a new routing method — Avg-K— as a hybrid design choice between sparse neural memory methods and mixture-of-experts methods"
  - [corpus]: Weak - no corpus evidence found for this specific mechanism
- Break condition: If negative dot products become too prevalent in smaller block sizes, affecting the averaging process

## Foundational Learning

- Concept: Sparse neural memory and mixture-of-experts (MoE) architectures
  - Why needed here: Understanding these architectures is crucial for grasping the unified framework proposed in the paper
  - Quick check question: What are the two main approaches to achieving sparse feed-forward networks discussed in the paper?

- Concept: Memory block size and selection method as key design dimensions
  - Why needed here: These dimensions form the basis of the unified framework for analyzing S-FFN architectures
  - Quick check question: How does the paper characterize different S-FFN methods along these two dimensions?

- Concept: Load balancing in MoE models
  - Why needed here: Understanding load balancing is important for interpreting the results and limitations of the proposed Avg-K method
  - Quick check question: Why do conventional MoE models like Switch Transformer enforce load balancing constraints?

## Architecture Onboarding

- Component map: Token embedding -> Self-attention -> S-FFN selection -> Memory block computation -> Output generation
- Critical path: 1) Token embedding through self-attention, 2) S-FFN selection of memory blocks, 3) Computation using selected memory blocks, 4) Output generation
- Design tradeoffs: Smaller block size improves perplexity but may increase communication costs; gating functions provide better efficiency but require learned parameters; Avg-K balances efficiency and performance but may have load balancing issues
- Failure signatures: Degraded performance due to poor memory block selection; communication bottlenecks when block size is too small; mode collapse when load balancing is not enforced
- First 3 experiments: 1) Implement Dense Baseline transformer for comparison, 2) Test different block sizes with RandHash selection method, 3) Compare Avg-K with other S-FFN architectures on perplexity and FLOPs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal block size for sparse feed-forward networks that balances performance and computational efficiency?
- Basis in paper: [explicit] The authors systematically study memory block sizes ranging from 1 to 4096, finding that smaller block sizes consistently improve perplexity while only marginally increasing FLOPs. However, they note that practical implementation constraints (communication cost, GPU load balancing) may limit how small blocks can be made.
- Why unresolved: The paper identifies a trend toward better performance with smaller blocks but doesn't pinpoint an optimal size. The authors acknowledge implementation challenges with very small blocks but don't explore what might be the sweet spot between 1 and 4096.
- What evidence would resolve it: A systematic study of perplexity, FLOPs, and wall-clock time across various block sizes (e.g., 1, 2, 4, 8, 16, 32, 64, 128, 256) with identical memory capacity and different model parallelism configurations would reveal the optimal trade-off point.

### Open Question 2
- Question: How can load balancing be effectively incorporated into the Avg-K architecture without sacrificing its performance advantages?
- Basis in paper: [explicit] The authors note that Avg-K performs comparably to Switch Transformer even without load balancing constraints, which is surprising given that load balancing is essential in large-scale MoE training. They hypothesize that Avg-K's performance suggests it learns a good representation despite potential mode collapse.
- Why unresolved: While the authors demonstrate Avg-K works well without load balancing, they don't explore how to incorporate it. The paper acknowledges that their scale is relatively small and doesn't use model parallelism, so the load balancing problem isn't pronounced.
- What evidence would resolve it: Experiments comparing Avg-K with and without load balancing loss across different scales (including model parallelism setups), measuring both perplexity and expert utilization, would show whether and how load balancing affects performance.

### Open Question 3
- Question: Are there alternative aggregation functions to averaging that could improve the performance of sparse feed-forward networks?
- Basis in paper: [explicit] The authors note that their Avg-K method "heavily depends on dot product information" and that "a better alternative may exist." They also tested other aggregators (Min, Max, Avg(|·|)) in VanillaM and found that average performed best, but acknowledge this might not generalize to all settings.
- Why unresolved: The paper settles on averaging based on limited experimentation with one baseline architecture. The authors suggest that "future work might want to focus on finding a better metric than dot product and other aggregation methods than average."
- What evidence would resolve it: A comprehensive comparison of different aggregation functions (weighted averages, learned aggregations, attention-based aggregations, geometric means, etc.) across multiple sparse FFN architectures and tasks would identify whether alternatives to simple averaging can provide performance gains.

## Limitations

- The proposed Avg-K method does not implement load balancing constraints, making direct comparison with Switch Transformer problematic
- Analysis of gating function optimality is based primarily on Switch Transformer without exploring alternative gating designs
- Results are based on a single experiment configuration (FFN replacement every 6th layer) without exploring optimal placement strategies

## Confidence

**High Confidence**: The observation that smaller memory block sizes generally improve perplexity with minimal additional computational cost is well-supported by systematic experiments across multiple selection methods.

**Medium Confidence**: The claim that gating function selection methods provide better FLOPs-perplexity trade-offs than direct methods is supported but has limitations, as analysis is based primarily on comparing RandHash with Switch Transformer.

**Low Confidence**: The assertion that current MoE gating functions are suboptimal for small block sizes and that Avg-K represents a better design choice requires more extensive validation beyond the empirical results on a single benchmark.

## Next Checks

1. **Load Balancing Implementation**: Implement load balancing constraints for Avg-K to enable fair comparison with Switch Transformer, then re-evaluate the perplexity and FLOPs trade-offs to determine if the performance gap persists under equivalent conditions.

2. **Gating Function Ablation**: Systematically test alternative gating function architectures (different activation functions, additional hidden layers, attention-based gating) with small block sizes to determine whether the gating function design itself is limiting performance rather than the fundamental approach.

3. **Architecture Placement Analysis**: Experiment with different layer replacement strategies (every 3rd layer vs every 6th layer) and varying the number of S-FFN layers to determine whether the reported Avg-K performance is dependent on the specific placement strategy used in the experiments.