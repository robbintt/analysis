---
ver: rpa2
title: Fundamental Limitations of Alignment in Large Language Models
arxiv_id: '2304.11082'
source_url: https://arxiv.org/abs/2304.11082
tags:
- behavior
- prompt
- alignment
- component
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a theoretical framework called Behavior Expectation
  Bounds (BEB) to analyze fundamental limitations of alignment in large language models
  (LLMs). The key insight is that for any behavior that an LLM exhibits with non-zero
  probability, there exist adversarial prompts that can trigger the model to exhibit
  that behavior with probability that increases with the length of the prompt.
---

# Fundamental Limitations of Alignment in Large Language Models

## Quick Facts
- arXiv ID: 2304.11082
- Source URL: https://arxiv.org/abs/2304.11082
- Reference count: 40
- Primary result: Introduces Behavior Expectation Bounds (BEB) framework showing that any non-zero probability undesired behavior can be triggered by sufficiently long adversarial prompts

## Executive Summary
This paper presents a theoretical framework called Behavior Expectation Bounds (BEB) that reveals fundamental limitations in aligning large language models (LLMs). The core insight is that for any behavior that an LLM exhibits with non-zero probability, there exist adversarial prompts that can trigger that behavior with probability increasing with prompt length. The framework decomposes the LLM's probability distribution into well-behaved and ill-behaved components, then analyzes how these interact under prompting. A key finding is that alignment methods like RLHF may paradoxically make LLMs more susceptible to adversarial prompts by sharpening the distinction between desired and undesired behaviors.

## Method Summary
The BEB framework defines a ground truth behavior scoring function B: Σ* → [-1, 1] and measures how well an LLM distribution P aligns with this behavior through the expected value of B under P. The method decomposes P into mixture components (P- for ill-behaved and P+ for well-behaved) and defines β-distinguishability between these components using KL divergence bounds. Theoretical results prove that if any ill-behaved component has non-zero prior probability, adversarial prompts of length O(log 1/ε, log 1/α, 1/β) can trigger that behavior with probability approaching 1-ε.

## Key Results
- Any non-zero probability undesired behavior can be adversarially triggered with increasing probability as prompt length grows
- RLHF alignment can paradoxically increase vulnerability to adversarial prompts by increasing β-distinguishability between desired and undesired behaviors
- Persona-based jailbreaking can be more efficient than direct adversarial prompting when the persona is well-captured but has low prior probability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial prompts can trigger any non-zero probability behavior to appear with increasing probability as prompt length grows
- Mechanism: The LLM's probability distribution decomposes into well-behaved and ill-behaved components. When prompted, the conditional distribution amplifies the ill-behaved component if it is sufficiently distinguishable from the well-behaved one
- Core assumption: The ill-behaved component is β-distinguishable from the well-behaved component, meaning the KL divergence between them remains bounded below by β for any initial prompt
- Break condition: If the ill-behaved component is not distinguishable (β=0) or has zero prior (α=0), adversarial prompts cannot reliably trigger the undesired behavior

### Mechanism 2
- Claim: RLHF alignment can paradoxically make LLMs more vulnerable to adversarial prompts by increasing distinguishability between desired and undesired behaviors
- Mechanism: RLHF training sharpens the separation between desired and undesired behaviors in the LLM's representation space, increasing β-distinguishability. While this reduces the prior α of undesired behaviors, it simultaneously makes them easier to trigger via longer prompts
- Break condition: If RLHF does not increase distinguishability (β remains low), or if it eliminates the undesired behavior entirely (α=0), the vulnerability does not manifest

### Mechanism 3
- Claim: Prompting an LLM to imitate a well-captured persona can be more efficient than directly eliciting undesired behavior, as persona-specific prompts require shorter lengths
- Mechanism: When an LLM captures a persona well during pretraining, it has high distinguishability β relative to other personas but low prior w in the mixture. The prompt length required to invoke such a persona scales as O(log 1/w / β), which can be shorter than direct elicitation if β is sufficiently high
- Break condition: If the persona is not well-captured (low β) or has high prior w, direct elicitation becomes more efficient than persona imitation

## Foundational Learning

- Concept: Probability decomposition into mixture components
  - Why needed here: The BEB framework relies on decomposing the LLM's distribution into distinguishable components (well-behaved vs. ill-behaved) to analyze alignment limitations
  - Quick check question: Given a distribution P = 0.3P- + 0.7P+ where BP- = -0.8 and BP+ = 0.9, what is BP?

- Concept: KL divergence and distinguishability
  - Why needed here: Distinguishability (β) is defined using KL divergence between conditional distributions, which determines how easily one component can be amplified over another via prompting
  - Quick check question: If two distributions have constant KL divergence β=2 for any prompt, what happens to their distinguishability after n prompt sentences?

- Concept: Sub-martingale concentration
  - Why needed here: The framework uses sub-martingale arguments to prove that prompt sequences can amplify ill-behaved components, requiring understanding of concentration inequalities
  - Quick check question: Why is the log likelihood ratio M_n = log P_-(s_0⊕q_1⊕...⊕q_n) / P+(s_0⊕q_1⊕...⊕q_n) a sub-martingale when P- is distinguishable from P+?

## Architecture Onboarding

- Component map: Define behavior scoring -> Decompose distribution -> Check distinguishability -> Calculate adversarial prompt length -> Test experimentally
- Critical path: Define behavior scoring → Decompose distribution → Check distinguishability → Calculate adversarial prompt length → Test experimentally
- Design tradeoffs: Simpler two-component decomposition vs. more expressive multi-persona decomposition; theoretical guarantees vs. empirical validation; prompt length bounds vs. practical adversarial attack detection
- Failure signatures: Incorrect behavior scoring (misaligned expectations); poor decomposition (non-distinguishable components); numerical instability in KL divergence calculations; overestimation of adversarial prompt requirements
- First 3 experiments:
  1. Implement behavior scoring for "harmlessness" and test on sample sentences to validate BEB framework assumptions
  2. Create a synthetic LLM distribution with known mixture components and verify that adversarial prompts of calculated length trigger undesired behavior
  3. Test persona invocation efficiency by comparing prompt lengths for direct elicitation vs. persona imitation on a controlled dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does RLHF training actually increase the β-distinguishability between ill-behaved and well-behaved components in practice?
- Basis in paper: Conjecture 1 states that alignment loss that increases the likelihood of desired sentences and minimizes the likelihood of undesired ones increases the β-distinguishability of resulting aligned LLMs
- Why unresolved: The authors acknowledge this as a conjecture and leave it as an open question for follow-up work
- What evidence would resolve it: Empirical measurement of KL divergence between ill- and well-behaved components before and after RLHF training across multiple models

### Open Question 2
- Question: How does the effectiveness of persona-based jailbreaking compare to direct adversarial prompting across different model architectures?
- Basis in paper: Corollary 1 shows that invoking a persona for bad behavior requires prompts that are asymptotically shorter than ones for invoking general bad behavior, but this depends on the distinguishability scaling super-logarithmically with prior
- Why unresolved: The theoretical framework assumes certain distinguishability properties that may not hold uniformly across models
- What evidence would resolve it: Systematic comparison of jailbreak prompt lengths needed when using persona impersonation vs. direct prompts across multiple LLMs

### Open Question 3
- Question: What is the relationship between conversation length guardrails and the robustness of alignment against adversarial attacks?
- Basis in paper: Theorem 1 shows that the more aligned the model is to begin with, the longer the adversarial prompt required to elicit undesired behaviors, suggesting limited interaction length could serve as a safety measure
- Why unresolved: The paper establishes theoretical bounds but doesn't explore practical implications for conversation length limits
- What evidence would resolve it: Empirical testing of alignment robustness as a function of conversation length with varying degrees of initial alignment

## Limitations
- The framework assumes perfect knowledge of behavior scoring functions and mixture decomposition, which is rarely available in practice
- Theoretical guarantees rely on idealized prompting conditions that may not hold under practical constraints
- Connection between theoretical bounds and empirical observations is largely correlational rather than causal

## Confidence

- **Major Uncertainties (Low-Medium)**: The theoretical framework assumes perfect knowledge of behavior scoring and mixture decomposition. The distinguishability parameter β may be difficult to estimate accurately for real LLMs.
- **Key Assumptions (Medium)**: The core mechanism relies on the assumption that β-distinguishability remains bounded away from zero during prompting sequences. This may not hold for all LLM architectures or training regimes.
- **Theoretical-Experimental Gap (Low)**: While the framework provides theoretical bounds, the connection to empirical observations is largely correlational rather than causal.

## Next Checks

1. **Empirical β Estimation**: Systematically measure distinguishability parameters across different LLM families and alignment methods to validate the predicted relationship between alignment strength and adversarial vulnerability.

2. **Prompt Efficiency Testing**: Compare the predicted prompt length scaling O(log 1/ε) with empirical measurements of adversarial prompt effectiveness across varying ε values and behavior categories.

3. **Multi-Component Validation**: Extend the two-component decomposition to realistic multi-behavior scenarios and test whether the framework's predictions hold when multiple ill-behaved components compete during prompting.