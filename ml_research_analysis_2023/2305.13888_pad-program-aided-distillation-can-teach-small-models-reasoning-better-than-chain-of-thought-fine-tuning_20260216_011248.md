---
ver: rpa2
title: 'PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than
  Chain-of-thought Fine-tuning'
arxiv_id: '2305.13888'
source_url: https://arxiv.org/abs/2305.13888
tags:
- reasoning
- data
- small
- arxiv
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Program-aided Distillation (PaD), a method
  that improves small model reasoning by distilling large language models (LLMs) with
  program-aided reasoning data. PaD synthesizes reasoning programs from LLMs and uses
  a Python interpreter to automatically filter out faulty reasoning steps, ensuring
  high-quality training data.
---

# PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning

## Quick Facts
- arXiv ID: 2305.13888
- Source URL: https://arxiv.org/abs/2305.13888
- Reference count: 12
- This paper introduces Program-aided Distillation (PaD), a method that improves small model reasoning by distilling large language models (LLMs) with program-aided reasoning data.

## Executive Summary
This paper introduces Program-aided Distillation (PaD), a method that improves small model reasoning by distilling large language models (LLMs) with program-aided reasoning data. PaD synthesizes reasoning programs from LLMs and uses a Python interpreter to automatically filter out faulty reasoning steps, ensuring high-quality training data. The method separates reasoning from computation, simplifying the task for small models and improving their focus on logical steps. Experiments on arithmetic reasoning tasks show that PaD significantly outperforms previous baselines and even surpasses certain LLMs (e.g., LLaMA 13B, PaLM 60B) with a model size 10x smaller and 3x less data. PaD achieves a 10% improvement over prior methods on GSM8K and demonstrates higher training efficiency. Data pruning analysis reveals that PaD can achieve performance comparable to GPT-3.5-turbo using only half the data. However, PaD trades general capabilities for improved reasoning, as evidenced by a drop in performance on general benchmarks like BBH. Overall, PaD offers a resource-efficient and effective approach to enhancing small model reasoning.

## Method Summary
PaD synthesizes reasoning programs from LLMs using in-context learning with manually constructed examples, then filters out incorrect samples via Python interpreter. Small models (CodeT5) are fine-tuned on the filtered dataset using seq2seq with cross-entropy loss. The method applies self-distillation for iterative improvement, with learning rate set to 6e-5 and max sequence lengths to 128 (encoder) and 256 (decoder). The approach leverages program-aided reasoning data to simplify the prediction space for small models while ensuring high-quality training data through automated error checking.

## Key Results
- PaD achieves 10% improvement over prior methods on GSM8K arithmetic reasoning tasks
- PaD surpasses LLaMA 13B and PaLM 60B with a model size 10x smaller and 3x less data
- PaD can achieve performance comparable to GPT-3.5-turbo using only half the data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Program-aided reasoning simplifies the prediction space for small models by enforcing Python syntax compliance.
- Mechanism: By generating reasoning programs instead of natural language rationales, the model's output space is constrained to syntactically valid Python code. This reduces the complexity of the generation task and allows the model to focus on logical reasoning rather than language generation.
- Core assumption: Small models can effectively learn to generate syntactically correct Python code that represents reasoning steps.
- Evidence anchors:
  - [abstract] "program-aided reasoning data and automatically filtering faulty reasoning"
  - [section 3.1] "synthetic program can be compiled and executed automatically by an additional interpreter"
  - [corpus] "Small Models Struggle to Learn from Strong Reasoners" suggests small models benefit from structured formats
- Break condition: If the small model cannot learn to generate syntactically correct Python code, the distillation process will fail.

### Mechanism 2
- Claim: Automated error checking via Python interpreter ensures high-quality training data by filtering out faulty reasoning steps.
- Mechanism: The Python interpreter compiles and executes the generated reasoning programs, identifying and removing samples with syntax errors or incorrect results. This process guarantees that only correct reasoning steps are used for fine-tuning.
- Core assumption: The Python interpreter can reliably detect all faulty reasoning steps through compilation and execution.
- Evidence anchors:
  - [abstract] "synthesizes reasoning programs from LLMs and uses a Python interpreter to automatically filter out faulty reasoning steps"
  - [section 3.1] "additional Python interpreter automatically filters data and removes samples that are either unable to compile or have incorrect results"
  - [corpus] "Program-Aided Reasoners (better) Know What They Know" suggests program-aided methods improve reasoning accuracy
- Break condition: If the Python interpreter fails to catch certain types of errors or introduces new errors during execution.

### Mechanism 3
- Claim: Separation of reasoning and computation through program execution allows small models to focus solely on logical steps.
- Mechanism: The small model generates reasoning programs, but the actual computation is delegated to the Python interpreter. This separation ensures that correct reasoning steps always produce accurate answers, eliminating evaluation errors.
- Core assumption: Delegating computation to an external interpreter does not introduce new sources of error or complexity.
- Evidence anchors:
  - [abstract] "separates reasoning from computation, simplifying the task for small models"
  - [section 3.1] "program-aided distillation also separates the processes of reasoning and computation by delegating the computational tasks to an additional interpreter"
  - [corpus] "Mentor-KD: Making Small Language Models Better Multi-step Reasoners" explores similar separation of reasoning and computation
- Break condition: If the interaction between the model-generated code and the interpreter introduces new errors or complexities.

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: Understanding CoT is crucial as PaD is positioned as an alternative to CoT-based distillation methods.
  - Quick check question: What are the main limitations of CoT prompting that PaD aims to address?

- Concept: Knowledge distillation
  - Why needed here: PaD is a form of knowledge distillation, transferring reasoning capabilities from LLMs to smaller models.
  - Quick check question: How does program-aided distillation differ from traditional knowledge distillation approaches?

- Concept: Python programming basics
  - Why needed here: The method relies on generating and executing Python code, so understanding Python syntax is essential.
  - Quick check question: What are the key elements of Python syntax that the small model needs to learn to generate?

## Architecture Onboarding

- Component map:
  LLM (e.g., gpt-3.5-turbo) -> Python interpreter -> Small model (e.g., CodeT5) -> Training data pipeline

- Critical path:
  1. Data synthesis: LLM generates reasoning programs from questions
  2. Error checking: Python interpreter filters out faulty programs
  3. Fine-tuning: Small model is trained on filtered data
  4. Self-distillation: Iterative refinement of the small model

- Design tradeoffs:
  - Python syntax vs. natural language rationales: Simpler for small models but may limit expressiveness
  - Automated filtering vs. manual curation: More efficient but may miss nuanced errors
  - Self-distillation vs. teacher-student setup: More scalable but may converge to local optima

- Failure signatures:
  - Low accuracy on arithmetic reasoning tasks
  - High rate of syntax errors in generated programs
  - Degradation in general capabilities (e.g., BBH performance)

- First 3 experiments:
  1. Data synthesis quality check: Generate programs for a small set of GSM8K questions and manually verify correctness
  2. Error filtering effectiveness: Run generated programs through Python interpreter and measure the percentage of filtered samples
  3. Fine-tuning baseline: Fine-tune CodeT5 on unfiltered data and compare performance to PaD

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the PaD approach generalize beyond arithmetic reasoning to more complex domains like logical reasoning or natural language inference?
- Basis in paper: [explicit] The paper states PaD is evaluated on arithmetic reasoning, symbolic reasoning, and general ability (BBH), but focuses heavily on arithmetic results
- Why unresolved: The paper demonstrates strong performance on arithmetic tasks but doesn't thoroughly test generalization to other reasoning domains that may have different structural properties
- What evidence would resolve it: Systematic evaluation of PaD on diverse reasoning benchmarks (e.g., logical reasoning, common sense reasoning, natural language inference) showing consistent performance gains

### Open Question 2
- Question: What is the exact trade-off relationship between reasoning specialization and general capability degradation in PaD?
- Basis in paper: [explicit] The paper notes that PaD "trades general capabilities for improved reasoning" and shows BBH performance drops, but doesn't quantify the relationship
- Why unresolved: The paper observes a correlation between improved reasoning and decreased general ability, but doesn't establish whether this is an inherent limitation or a parameter/optimization issue
- What evidence would resolve it: Controlled experiments varying model size, training data, and hyperparameters to map the precise relationship between reasoning improvement and general capability loss

### Open Question 3
- Question: How does PaD's data efficiency advantage scale with increasing problem complexity?
- Basis in paper: [explicit] The paper demonstrates PaD achieves comparable performance to GPT-3.5-turbo with half the data, but only tests this on GSM8K problems
- Why unresolved: The paper shows data efficiency gains on relatively simple arithmetic problems but doesn't investigate whether this advantage persists for more complex multi-step reasoning problems
- What evidence would resolve it: Scaling experiments comparing PaD and baseline methods across problem sets of increasing complexity while measuring data efficiency and performance trade-offs

## Limitations

- Lack of transparency in manually constructed context examples used for in-context learning
- Trade-off between reasoning specialization and general capability degradation (BBH benchmark drops)
- Limited evaluation scope focused primarily on arithmetic reasoning tasks

## Confidence

High confidence in the core methodology: The separation of reasoning and computation through program execution is technically sound and well-supported by the Python interpreter's ability to validate code.

Medium confidence in the performance claims: While the 10% improvement over baselines on GSM8K is substantial, the comparison is primarily against older methods, and the sample size for some comparisons is limited.

Low confidence in the scalability claims: The assertion that PaD can achieve performance comparable to GPT-3.5-turbo using only half the data needs further validation across diverse datasets and problem types.

## Next Checks

1. **Syntax Generation Analysis**: Analyze a sample of 100 synthesized programs from CodeT5 models to measure the actual percentage of syntactically correct Python code generated, comparing this to the assumed high success rate in the paper.

2. **Error Type Classification**: Run the Python interpreter on the filtered dataset and classify the types of errors caught (syntax errors, logical errors, execution errors) to validate that the automated filtering is catching the intended failure modes.

3. **Generalization Assessment**: Test PaD-trained models on a broader set of reasoning tasks beyond arithmetic word problems, including logical reasoning and commonsense reasoning benchmarks, to verify that the performance gains are not task-specific.