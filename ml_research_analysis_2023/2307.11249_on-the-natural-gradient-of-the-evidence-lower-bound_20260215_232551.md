---
ver: rpa2
title: On the Natural Gradient of the Evidence Lower Bound
arxiv_id: '2307.11249'
source_url: https://arxiv.org/abs/2307.11249
tags:
- latexit
- gradient
- sha1
- base64
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the Fisher-Rao gradient (natural gradient)
  of the evidence lower bound (ELBO), a central quantity in generative machine learning
  models like variational autoencoders. The authors reveal that the gap between the
  evidence and its ELBO bound has a vanishing natural gradient under unconstrained
  optimization, meaning maximizing the ELBO is equivalent to minimizing the Kullback-Leibler
  divergence from a target distribution.
---

# On the Natural Gradient of the Evidence Lower Bound

## Quick Facts
- arXiv ID: 2307.11249
- Source URL: https://arxiv.org/abs/2307.11249
- Reference count: 11
- Key outcome: The natural gradient of the ELBO matches the natural gradient of the original KL-divergence objective under unconstrained optimization, with cylindrical models preserving this equivalence when optimization is constrained.

## Executive Summary
This paper investigates the Fisher-Rao gradient (natural gradient) of the evidence lower bound (ELBO), a central quantity in generative machine learning models like variational autoencoders. The authors reveal that the gap between the evidence and its ELBO bound has a vanishing natural gradient under unconstrained optimization, meaning maximizing the ELBO is equivalent to minimizing the Kullback-Leibler divergence from a target distribution. They derive conditions under which this equivalence holds even when optimization is constrained to a model, formalizing this through the notion of a cylindrical model. The paper provides a geometric characterization of when the natural gradient of the ELBO aligns with the natural gradient of the original KL-divergence objective, highlighting that any deviation arises from model constraints rather than the ELBO itself.

## Method Summary
The paper employs information geometry to analyze the Fisher-Rao gradient of the ELBO. It compares the natural gradient of the ELBO with the natural gradient of the original KL-divergence objective under different optimization scenarios: unconstrained optimization in the full probability space P, and constrained optimization within a model M. The analysis introduces the concept of cylindrical models, which have a special tangent space decomposition property that preserves gradient equivalence. The authors derive conditions for when the ELBO's natural gradient matches the original objective's gradient, both in unconstrained settings and within cylindrical model constraints.

## Key Results
- The natural gradient of the ELBO is mathematically equivalent to the natural gradient of the original KL-divergence objective in unconstrained optimization
- Cylindrical models preserve this gradient equivalence when optimization is constrained to the model
- Any deviation between ELBO and KL-divergence gradients arises from model constraints rather than the ELBO formulation itself

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The natural gradient of the ELBO aligns with the natural gradient of the original KL-divergence objective under unconstrained optimization.
- Mechanism: When optimization occurs in the full space P (without restricting to a lower-dimensional model), the gradient of the ELBO is mathematically equivalent to the gradient of the original KL-divergence objective. This happens because the ELBO upper bounds the log-evidence, and its gradient matches the gradient of the true objective in the ambient space.
- Core assumption: The model space M equals the full probability space P, meaning no constraints are imposed on the optimization.
- Evidence anchors:
  - [abstract] "the gap between the evidence and its lower bound, the ELBO, has essentially a vanishing natural gradient within unconstrained optimization"
  - [section] "it is remarkable that in this case, the ELBO leads to the same gradient field as the original objective function, the Kullback-Leibler divergence from a target distribution"
- Break condition: When optimization is constrained to a lower-dimensional model M that is not cylindrical, the equivalence breaks down.

### Mechanism 2
- Claim: Cylindrical models preserve the equivalence between ELBO and KL-divergence gradients.
- Mechanism: A cylindrical model M has a tangent space that can be decomposed orthogonally into horizontal and vertical components. This structure ensures that when the gradient of the ELBO is projected onto the tangent space of M, it matches the gradient of the original KL-divergence objective.
- Core assumption: The model M is cylindrical, meaning it has the orthogonal decomposition property where TpM = (TpM ∩ Hp) ⊕ (TpM ∩ Vp).
- Evidence anchors:
  - [abstract] "we derive a condition under which this equivalence persists even when optimization is constrained to a model. This condition yields a geometric characterization, which we formalize through the notion of a cylindrical model"
  - [section] "According to the invariance (15), these two gradients are equivalent, if M is cylindrical"
- Break condition: When the model is not cylindrical, the projection of the ELBO gradient onto the model tangent space no longer matches the KL-divergence gradient.

### Mechanism 3
- Claim: The ELBO optimization is "exact" in the sense that it doesn't alter the original optimization trajectory when evaluated in the full space.
- Mechanism: The ELBO is constructed as an upper bound on the negative log-evidence, and its gradient in the full space P matches the gradient of the original objective. This means that maximizing the ELBO is mathematically equivalent to minimizing the KL-divergence in the ambient space.
- Core assumption: The optimization is performed in the full probability space P rather than a restricted model.
- Evidence anchors:
  - [abstract] "maximization of the ELBO is equivalent to minimization of the Kullback-Leibler divergence from a target distribution"
  - [section] "it is remarkable that in this case, the ELBO leads to the same gradient field as the original objective function"
- Break condition: When restricting to a lower-dimensional model, the equivalence no longer holds.

## Foundational Learning

- Concept: Fisher-Rao metric and natural gradient
  - Why needed here: The paper relies on the Fisher-Rao metric to define natural gradients, which are central to understanding the equivalence between ELBO and KL-divergence optimization
  - Quick check question: What is the difference between a standard gradient and a natural gradient in the context of probability distributions?

- Concept: Kullback-Leibler divergence and its properties
  - Why needed here: The KL-divergence is the original objective function, and understanding its properties is crucial for grasping why the ELBO behaves as it does
  - Quick check question: Why is the KL-divergence asymmetric, and what does this mean for optimization?

- Concept: Evidence Lower Bound (ELBO) and its derivation
  - Why needed here: The ELBO is the central object of study, and understanding how it's derived from the log-evidence is essential
  - Quick check question: How is the ELBO derived from the log-evidence using Jensen's inequality?

## Architecture Onboarding

- Component map:
  - Full probability space P (ambient space)
  - Model M (typically a subset of P, possibly cylindrical)
  - Visible units V and hidden units H
  - Marginalization map πV
  - Fisher-Rao metric and natural gradient computation
  - KL-divergence objective function
  - ELBO bound construction

- Critical path: Compute natural gradient of ELBO in full space P → Check if model M is cylindrical → If cylindrical, equivalence holds; if not, analyze projection effects

- Design tradeoffs: Using cylindrical models preserves gradient equivalence but may limit model expressiveness; using full space preserves equivalence but may be computationally expensive

- Failure signatures: When the natural gradient of the ELBO does not match the natural gradient of the KL-divergence, this indicates either a non-cylindrical model or a constraint-induced deviation

- First 3 experiments:
  1. Implement ELBO computation and verify that its natural gradient matches the KL-divergence gradient in the full space P
  2. Construct a simple cylindrical model and verify that the gradient equivalence is preserved
  3. Construct a non-cylindrical model and measure the deviation between ELBO and KL-divergence gradients

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions do cylindrical models maintain the natural gradient invariance property when optimizing constrained variational autoencoders with complex latent structures?
- Basis in paper: [explicit] The paper defines cylindrical models and shows that the natural gradient invariance holds for these models, but notes that the requirement for a model to be cylindrical is quite restrictive.
- Why unresolved: While the paper provides a definition of cylindrical models and proves invariance properties, it acknowledges that determining when real-world models (especially deep neural network architectures) satisfy these conditions requires further investigation.
- What evidence would resolve it: Empirical verification across different VAE architectures showing which maintain gradient invariance under the cylindrical model condition.

### Open Question 2
- Question: How does the natural gradient behavior of the ELBO differ between overparameterized models versus models with specific structural constraints?
- Basis in paper: [inferred] The paper contrasts the behavior of natural gradients in the unconstrained case (M = P) versus constrained models, noting that deviations from invariance arise from model constraints.
- Why unresolved: The paper establishes theoretical conditions but doesn't explore the practical differences in gradient behavior between different types of constrained models or varying levels of parameterization.
- What evidence would resolve it: Comparative analysis of natural gradient trajectories across models with varying structural constraints and parameter counts.

### Open Question 3
- Question: Can the geometric characterization of cylindrical models be extended to continuous latent spaces or non-parametric distributions?
- Basis in paper: [explicit] The paper focuses on finite sets and parametric models, with the main example being the full model M = P on finite sets.
- Why unresolved: The current theory is developed for finite state spaces and parametric models, but modern generative models often use continuous latent spaces or non-parametric approaches.
- What evidence would resolve it: Extension of the cylindrical model framework to continuous spaces and demonstration of gradient invariance properties in these settings.

## Limitations

- The paper's theoretical framework is developed for finite state spaces and parametric models, which may not directly translate to continuous or non-parametric settings common in modern generative models
- The concept of cylindrical models, while mathematically elegant, lacks concrete examples in the corpus and may be difficult to verify in practical implementations
- The analysis assumes infinite-dimensional probability spaces in some arguments, which may not translate directly to finite-dimensional implementations common in machine learning

## Confidence

- High confidence: The theoretical derivation of natural gradient equivalence in unconstrained optimization
- Medium confidence: The characterization of cylindrical models and their role in preserving gradient equivalence
- Low confidence: Practical implications and numerical verification of the theoretical results

## Next Checks

1. Implement a numerical example comparing natural gradients of ELBO and KL-divergence in a simple probabilistic model to verify the unconstrained equivalence
2. Construct specific examples of cylindrical and non-cylindrical models to test when gradient equivalence is preserved or broken
3. Analyze the computational complexity and feasibility of using Fisher-Rao natural gradients in practical variational inference scenarios