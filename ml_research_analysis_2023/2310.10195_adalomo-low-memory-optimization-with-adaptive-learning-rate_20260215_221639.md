---
ver: rpa2
title: 'AdaLomo: Low-memory Optimization with Adaptive Learning Rate'
arxiv_id: '2310.10195'
source_url: https://arxiv.org/abs/2310.10195
tags:
- adalomo
- memory
- training
- adamw
- lomo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the high memory cost of training large language
  models by introducing AdaLomo, a low-memory optimization method with adaptive learning
  rates. AdaLomo improves upon LOMO by incorporating second-moment estimation to provide
  parameter-specific adaptive learning rates, while maintaining memory efficiency
  through non-negative matrix factorization of the optimizer state.
---

# AdaLomo: Low-memory Optimization with Adaptive Learning Rate

## Quick Facts
- arXiv ID: 2310.10195
- Source URL: https://arxiv.org/abs/2310.10195
- Reference count: 40
- The paper introduces AdaLomo, a low-memory optimization method that achieves AdamW-level performance on LLM training while using memory comparable to LoRA through non-negative matrix factorization and grouped update normalization.

## Executive Summary
AdaLomo addresses the high memory cost of training large language models by introducing an adaptive learning rate optimizer that maintains memory efficiency. The method builds on LOMO by incorporating second-moment estimation to provide parameter-specific adaptive learning rates, while using non-negative matrix factorization to keep memory usage low. Through empirical analysis, the authors demonstrate that second-order moment estimation is more critical for convergence than momentum, and that their NMF approach achieves memory savings comparable to LoRA while maintaining performance on instruction-tuning and further pre-training tasks.

## Method Summary
AdaLomo is a low-memory optimization method that combines adaptive learning rates with memory-efficient second-moment estimation. It uses non-negative matrix factorization (NMF) to compress the second-order moment matrix into two vectors per parameter matrix, reducing memory from O(mn) to O(m+n). The method employs grouped update normalization to stabilize convergence and includes a fused backward pass that computes gradients and updates parameters simultaneously while clearing previous gradients to minimize memory footprint.

## Key Results
- Achieves AdamW-level performance on instruction-tuning tasks while using memory comparable to LoRA
- Reduces memory consumption to O(m+n) per parameter matrix through NMF factorization
- Demonstrates stable convergence across LLaMA models from 7B to 65B parameters
- Shows second-moment estimation is more critical for convergence than momentum in adaptive optimizers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive learning rates per parameter improve convergence compared to non-adaptive methods like SGD.
- Mechanism: By incorporating second-moment estimation of gradients (as in Adam), each parameter receives an individualized learning rate that scales inversely with the historical variance of that parameter's gradients.
- Core assumption: The second-order moment of gradients is the primary driver of improved convergence, more so than momentum (first-order moment).
- Evidence anchors:
  - [abstract] "Through empirical analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap."
  - [section 2.2] "Through our ablation study on Adam, we found that its second-order moment estimation has a significantly greater impact on its convergence than the first-order moment estimation."
  - [corpus] Weak/no direct evidence; neighboring papers focus on memory efficiency rather than adaptive learning rate mechanisms.
- Break condition: If gradient statistics are not representative (e.g., highly non-stationary), second-moment estimates may become stale and harm convergence.

### Mechanism 2
- Claim: Non-negative matrix factorization (NMF) of second-moment estimates enables memory-efficient adaptive optimization.
- Mechanism: The second-moment matrix vt,i ∈ Rm×n is factorized into rt,i ∈ Rm×1 and ct,i ∈ R1×n such that vt,i = rt,ict,i, reducing memory from O(mn) to O(m+n).
- Core assumption: The second-moment matrix is approximately rank-1 or low-rank, making NMF a good approximation.
- Evidence anchors:
  - [abstract] "To maintain memory efficiency, we employ non-negative matrix factorization for the second-order moment estimation in the optimizer state."
  - [section 3.1] "In our pursuit of further memory efficiency, we applied non-negative matrix factorization to the second-order moment, inspired by Adafactor."
  - [corpus] Weak evidence; neighboring papers mention memory-efficient factorizations but do not validate NMF specifically for AdaLomo.
- Break condition: If the true second-moment matrix has high rank, NMF approximation degrades, hurting optimization performance.

### Mechanism 3
- Claim: Grouped update normalization stabilizes convergence without requiring global gradient normalization.
- Mechanism: Each parameter's update is scaled by its own root-mean-square (RMS) value and the parameter's magnitude, avoiding a single global scaling factor that could mismatch parameter update needs.
- Core assumption: Different parameters have heterogeneous gradient magnitudes, so per-parameter normalization is more effective than global normalization.
- Evidence anchors:
  - [abstract] "Additionally, we suggest the use of a grouped update normalization to stabilize convergence."
  - [section 3.2] "Group update normalization ensures that each parameter's update is meaningful and not overshadowed by large gradient values from other parameters."
  - [corpus] No direct evidence; neighboring papers do not discuss grouped update normalization.
- Break condition: If all parameters have similar gradient scales, per-parameter normalization adds unnecessary complexity without benefit.

## Foundational Learning

- Concept: Second-order moment estimation in adaptive optimizers
  - Why needed here: The core novelty of AdaLomo is using second-moment estimates to provide per-parameter adaptive learning rates, distinguishing it from non-adaptive LOMO.
  - Quick check question: What is the mathematical form of the second-moment estimate in Adam, and how does it influence the effective learning rate?

- Concept: Non-negative matrix factorization for low-rank approximation
  - Why needed here: AdaLomo uses NMF to compress second-moment matrices, reducing memory usage from O(mn) to O(m+n) per parameter matrix.
  - Quick check question: Given a matrix vt,i, how are rt,i and ct,i computed in Adafactor's NMF approach, and what constraints ensure non-negativity?

- Concept: Grouped update normalization vs. global normalization
  - Why needed here: AdaLomo employs grouped update normalization to maintain stable training without the memory overhead of full gradient normalization.
  - Quick check question: How does grouped update normalization differ from global normalization in terms of scaling factors and computational cost?

## Architecture Onboarding

- Component map:
  Forward pass -> Backward pass (fused gradient computation and parameter update) -> Optimizer state update (rt,i, ct,i) -> Parameter update with grouped normalization

- Critical path:
  1. Compute gradient gt,i during backpropagation
  2. Update optimizer states: rt,i = βrt−1,i + (1−β)g²t,i1n and ct,i = βct−1,i + (1−β)1Tmg²t,i
  3. Compute vt,i = rt,ict,i for adaptive learning rate
  4. Apply grouped update normalization and parameter update
  5. Clear gt,i−1 to free memory

- Design tradeoffs:
  - Memory vs. accuracy: NMF approximation trades some precision for large memory savings
  - Computational overhead: Additional matrix multiplications for vt,i computation
  - Stability vs. speed: Grouped update normalization adds stability but may slow convergence slightly

- Failure signatures:
  - Poor convergence: likely due to inaccurate NMF approximation or inappropriate β decay rate
  - Memory bloat: unexpected if NMF factorization fails or optimizer states grow incorrectly
  - Training instability: may arise if grouped update normalization parameters are misconfigured

- First 3 experiments:
  1. Ablation: Run with and without second-moment estimation to confirm its impact on convergence.
  2. Memory profiling: Measure actual memory usage vs. theoretical O(m+n) bound for different model sizes.
  3. Normalization comparison: Test grouped update normalization against global normalization on a small benchmark.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does AdaLomo's performance scale with increasingly larger model sizes beyond 65B parameters?
- Basis in paper: [inferred] The paper only evaluates up to LLaMA-65B models, but suggests AdaLomo could be used for training large language models.
- Why unresolved: The authors do not test models larger than 65B parameters to demonstrate the method's scalability.
- What evidence would resolve it: Experimental results showing AdaLomo's performance and memory efficiency on models larger than 65B parameters, such as LLaMA-175B or GPT-3 scale models.

### Open Question 2
- Question: What is the impact of AdaLomo's grouped update normalization on convergence speed compared to global update normalization?
- Basis in paper: [explicit] The paper states that grouped update normalization is suggested to stabilize convergence, but does not provide a direct comparison to global update normalization.
- Why unresolved: The authors do not provide experimental results comparing the convergence speed and stability of AdaLomo with and without grouped update normalization.
- What evidence would resolve it: Experiments comparing AdaLomo's training loss curves and convergence speed with and without grouped update normalization on various tasks.

### Open Question 3
- Question: How does AdaLomo's computational overhead affect training time for very large-scale models?
- Basis in paper: [explicit] The paper mentions that AdaLomo introduces additional computational overhead during parameter updates compared to LOMO, but does not quantify this overhead's impact on overall training time.
- Why unresolved: The authors do not provide experiments measuring the total training time of AdaLomo compared to other methods for large-scale models.
- What evidence would resolve it: Experiments measuring the wall-clock time to train large models to convergence using AdaLomo versus AdamW and LOMO, accounting for the computational overhead.

## Limitations

- The evaluation is limited to LLaMA model family and specific instruction-tuning/further pre-training tasks, without testing generalization to other model architectures or diverse downstream applications.
- The ablation study focuses on second-moment importance but lacks exploration of NMF rank sensitivity and β2 decay rate impact on convergence and memory efficiency.
- The paper proposes grouped update normalization without thorough comparison to alternative normalization strategies or sensitivity analysis of its hyperparameters.

## Confidence

- **High confidence**: Memory efficiency claims - The paper provides clear theoretical analysis and empirical measurements showing memory usage reduction to levels comparable to LoRA. The O(m+n) memory complexity for NMF factorization is well-established and verified through experiments.
- **Medium confidence**: Convergence performance claims - While the paper demonstrates comparable results to AdamW on tested tasks, the ablation study provides limited insight into which components contribute most to the performance. The claim that second-moment estimation is more critical than momentum is supported by internal ablation but lacks comparison with other adaptive methods.
- **Low confidence**: Generalization claims - The paper's results are based on a limited set of models (LLaMA family) and tasks (instruction-tuning and further pre-training). The claim that AdaLomo generalizes well to diverse LLMs and training scenarios requires further validation.

## Next Checks

1. **Cross-architecture validation**: Test AdaLomo on different transformer architectures (e.g., GPT, OPT) and model sizes to verify that the memory efficiency and convergence benefits generalize beyond LLaMA models.

2. **Rank sensitivity analysis**: Conduct experiments varying the rank of the NMF factorization to quantify the tradeoff between memory savings and approximation accuracy, identifying optimal rank for different model scales.

3. **Long-term stability evaluation**: Run extended training sessions (beyond the reported 3-5 epochs) to assess whether AdaLomo maintains stable convergence and memory efficiency over longer training horizons, particularly for tasks requiring many optimization steps.