---
ver: rpa2
title: On the Expressivity of Multidimensional Markov Reward
arxiv_id: '2307.12184'
source_url: https://arxiv.org/abs/2307.12184
tags:
- reward
- markov
- policies
- function
- rewards
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates when a set of desired behaviors (acceptable
  policies) can be characterized by scalar or multidimensional Markov reward functions
  in a Markov Decision Process (MDP). The key contribution is to show that characterizing
  a set of acceptable policies is equivalent to separating their occupancy measures
  with hyperplanes: if the convex hull of the occupancy measures of acceptable policies
  does not intersect the occupancy measures of unacceptable policies, then a scalar
  reward function suffices.'
---

# On the Expressivity of Multidimensional Markov Reward

## Quick Facts
- arXiv ID: 2307.12184
- Source URL: https://arxiv.org/abs/2307.12184
- Authors: 
- Reference count: 13
- One-line primary result: Characterizing a set of acceptable policies in an MDP is equivalent to separating their occupancy measures with hyperplanes: if convex hulls of good and bad policies do not intersect, a scalar reward suffices; otherwise, multidimensional rewards are necessary.

## Executive Summary
This paper investigates when a set of desired behaviors (acceptable policies) can be characterized by scalar or multidimensional Markov reward functions in a Markov Decision Process. The key contribution is to show that characterizing a set of acceptable policies is equivalent to separating their occupancy measures with hyperplanes: if the convex hull of the occupancy measures of acceptable policies does not intersect the occupancy measures of unacceptable policies, then a scalar reward function suffices. Otherwise, multidimensional rewards are necessary. Specifically, for a consistent SOAP (disjoint sets of good and bad policies), a scalar reward exists if and only if the convex hulls of good and bad policies' occupancy measures do not intersect. Otherwise, a multidimensional reward with at most as many dimensions as the number of bad policies can always characterize the SOAP. The paper also shows that for any consistent SOAP with deterministic policies, a multidimensional reward always exists.

## Method Summary
The paper uses convex hull analysis and linear programming to determine when a set of acceptable policies can be characterized by scalar or multidimensional Markov reward functions. The method involves computing occupancy measures for all policies in the SOAP, checking consistency, and then determining separability of convex hulls for scalar rewards or extreme point separation for multidimensional rewards.

## Key Results
- Scalar Markov rewards suffice to characterize acceptable policies if and only if the convex hulls of good and bad policies' occupancy measures do not intersect.
- For any consistent SOAP with deterministic policies, a multidimensional reward function with at most |ΠB| dimensions always exists.
- Characterizing a SOAP with multidimensional rewards is equivalent to polyhedral separability of the policy occupancy sets.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scalar Markov rewards suffice to characterize acceptable policies if and only if the convex hulls of good and bad policies' occupancy measures do not intersect.
- Mechanism: The scalar reward function induces a hyperplane in the occupancy measure space. If the convex hulls of good and bad policies are disjoint, a single hyperplane can separate them, making all good policies optimal while keeping bad policies suboptimal.
- Core assumption: The occupancy measures of good policies form a convex set, as do those of bad policies, enabling convex hull analysis.
- Evidence anchors:
  - [abstract] "if the convex hull of the occupancy measures of acceptable policies does not intersect the occupancy measures of unacceptable policies, then a scalar reward function suffices."
  - [section] "there exists c ∈ R, and R : S × A → R that realizes ⟨E, ΠG, ΠB⟩ iff conv(PG) ∩ conv(PB) = ∅."
  - [corpus] Weak evidence - neighbors focus on multi-objective settings, not convex hull separability.
- Break condition: If conv(PG) ∩ conv(PB) ≠ ∅, no scalar reward can separate the two sets; a higher-dimensional reward is required.

### Mechanism 2
- Claim: For any consistent SOAP with deterministic policies, a multidimensional reward function with at most |ΠB| dimensions always exists.
- Mechanism: Each bad policy's occupancy measure is an extreme point in the policy space. Since the SOAP is consistent, these extreme points are outside conv(PG), allowing each to be separated by a dedicated hyperplane, thus constructing a d-dimensional reward with d ≤ |ΠB|.
- Core assumption: Consistency ensures all policy occupancy measures are distinct, and deterministic policies correspond to extreme points.
- Evidence anchors:
  - [abstract] "for every non-degenerate set of deterministic policies, there exists a multidimensional Markov reward function that characterizes it."
  - [section] "Consistency of the SOAP ensures that PG and PB are disjoint... ρπb ̸∈ conv(PG), which implies conv (PG) ∩ PB = ∅."
  - [corpus] No direct evidence; neighbors do not discuss SOAP or extreme point arguments.
- Break condition: If the SOAP is inconsistent, some bad policy shares an occupancy measure with a good policy, making separation impossible.

### Mechanism 3
- Claim: Characterizing a SOAP with multidimensional rewards is equivalent to polyhedral separability of the policy occupancy sets.
- Mechanism: Each dimension of the reward function defines a linear inequality (hyperplane) in occupancy measure space. Feasibility is defined as lying within the intersection of these halfspaces, i.e., a polyhedron. Thus, reward design reduces to finding a polyhedron that contains all good policies and excludes all bad ones.
- Core assumption: Reward feasibility is defined via linear inequalities on occupancy measures, enabling geometric interpretation.
- Evidence anchors:
  - [abstract] "asking if there exists d-dimensional ⟨R, c⟩ that realizes ⟨E, ΠG, ΠB⟩ is equivalent to asking if there exists a polyhedron using d hyperplanes that separates PG and PB."
  - [section] "a policy π is feasible iff Rρπ ≥ c. In other words, a policy π is feasible iff it is within the polyhedron {x ∈ R|S|×|A||Rx ≥ c}."
  - [corpus] No direct evidence; neighbors do not discuss polyhedral separability in this context.
- Break condition: If no polyhedron with d hyperplanes can separate PG from PB, the given SOAP cannot be characterized by a d-dimensional reward.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formalism
  - Why needed here: The paper's analysis is built on the MDP framework, especially occupancy measures and policy optimality.
  - Quick check question: In an MDP, how is the value of a policy Vπ(s) related to the reward function R and the occupancy measure ρπ?

- Concept: Convex hulls and polyhedral separability
  - Why needed here: The main results hinge on whether convex hulls of policy occupancy measures intersect, which determines the existence of separating hyperplanes.
  - Quick check question: What does it mean geometrically if conv(PG) ∩ conv(PB) = ∅ in the space of discounted expected state-action visitations?

- Concept: Consistent vs inconsistent SOAP
  - Why needed here: Consistency ensures all policies in the SOAP have distinct occupancy measures, a prerequisite for separability.
  - Quick check question: Why does the paper require SOAPs to be consistent, and what goes wrong if ρπa = ρπb for some πa ∈ ΠG and πb ∈ ΠB?

## Architecture Onboarding

- Component map:
  - Input: Markov environment E, SOAP ⟨ΠG, ΠB⟩
  - Core engine: Convex hull computation for PG and PB, separability check (scalar) or extreme point analysis (multidimensional)
  - Output: Existence (and optionally construction) of scalar/multidimensional reward function R and threshold/c vector c
  - Supporting: Linear programming (for separability), occupancy measure calculation, consistency check

- Critical path:
  1. Compute occupancy measures for all policies in ΠG and ΠB
  2. Check SOAP consistency (all ρπa ≠ ρπb)
  3. For scalar reward: check if conv(PG) ∩ conv(PB) = ∅ (LP separability test)
  4. For multidimensional reward: for each πb ∈ ΠB, check if ρπb ∉ conv(PG); if so, a d ≤ |ΠB| reward exists
  5. (Optional) Construct explicit R and c via LP or hyperplane placement

- Design tradeoffs:
  - Scalar vs multidimensional: Scalar rewards are simpler and interpretable but may not exist for all SOAPs; multidimensional rewards are more expressive but harder to design and may overfit.
  - Explicit vs implicit construction: Constructing R and c allows direct use but is computationally heavier; proving existence is lighter but less actionable.
  - Consistency requirement: Excludes degenerate cases but may reject valid specifications; relaxing this could broaden applicability but complicates analysis.

- Failure signatures:
  - Scalar reward fails when conv(PG) ∩ conv(PB) ≠ ∅ (e.g., when some bad policy's occupancy measure is in the convex hull of good policies)
  - Multidimensional reward fails when SOAP is inconsistent (ρπa = ρπb for some πa ∈ ΠG, πb ∈ ΠB)
  - Computational failure if occupancy measure calculation or convex hull computation is numerically unstable

- First 3 experiments:
  1. Implement occupancy measure computation for deterministic policies in a small MDP (e.g., Figure 1a/b) and verify against known examples.
  2. Code convex hull computation for PG and PB, and implement LP-based separability test for scalar rewards; run on synthetic SOAPs.
  3. Extend the implementation to construct d-dimensional rewards for consistent SOAPs with deterministic policies, using hyperplane separation for each bad policy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the computational complexity of determining whether a given SOAP can be realized by a scalar or multidimensional Markov reward function?
- Basis in paper: [explicit] The paper mentions that an algorithm should be developed based on ideas from [4] and the computational complexity should be analyzed.
- Why unresolved: The paper states that while conditions for the existence of such reward functions are provided, no algorithm is given to check these conditions.
- What evidence would resolve it: A formal algorithm that checks the separability conditions and an analysis of its time and space complexity.

### Open Question 2
- Question: Can the expressivity results be extended to general preferences among policies and trajectories, beyond the binary classification of acceptable vs. unacceptable policies?
- Basis in paper: [explicit] The paper mentions that a natural direction is to extend results to general preferences among policies and trajectories.
- Why unresolved: The current work only considers SOAPs, which are a specific type of preference specification.
- What evidence would resolve it: Formal theorems and proofs extending the expressivity results to general preference structures, along with examples demonstrating the increased expressivity.

### Open Question 3
- Question: How does the bounded rationality of the reward designer affect the expressivity of Markov rewards?
- Basis in paper: [explicit] The paper mentions exploring the connection between bounded rationality and expressivity.
- Why unresolved: The current work assumes the designer can specify any SOAP, but in practice, designers may have limited ability to specify preferences.
- What evidence would resolve it: A formal model of bounded rationality for reward design, along with analysis of how it limits the expressivity of rewards.

## Limitations
- The convex hull separability analysis may not scale well for large state/action spaces due to computational intensity.
- The strong SOAP consistency assumption excludes many realistic scenarios where good and bad policies may share occupancy measures.
- The paper proves existence of multidimensional rewards but does not provide efficient construction algorithms for arbitrary MDPs.

## Confidence
- High confidence in the convex hull separability characterization for scalar rewards.
- Medium confidence in the extreme point argument for multidimensional rewards.
- Low confidence in the practical applicability without computational experiments.

## Next Checks
1. Implement and test the convex hull separability check on synthetic MDPs with known good/bad policy separations.
2. Verify the extreme point argument by explicitly constructing d-dimensional rewards for SOAPs with deterministic policies in small MDPs.
3. Conduct scalability experiments to measure the computational cost of convex hull computations and LP separation tests as state/action space grows.