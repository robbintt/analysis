---
ver: rpa2
title: Multiclass Online Learning and Uniform Convergence
arxiv_id: '2303.17716'
source_url: https://arxiv.org/abs/2303.17716
tags:
- online
- multiclass
- learnability
- learning
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of characterizing online learnability
  in multiclass classification, particularly when the number of classes (or labels)
  is unbounded. The main result proves that any multiclass concept class is agnostically
  learnable if and only if its Multiclass Littlestone dimension (MCLdim) is finite,
  thus solving an open problem.
---

# Multiclass Online Learning and Uniform Convergence

## Quick Facts
- arXiv ID: 2303.17716
- Source URL: https://arxiv.org/abs/2303.17716
- Reference count: 4
- Primary result: Characterizes online multiclass learnability via Multiclass Littlestone dimension (MCLdim), showing it's necessary and sufficient even for unbounded label spaces

## Executive Summary
This paper resolves a fundamental open problem in online multiclass learning by proving that any multiclass concept class is agnostically learnable if and only if its Multiclass Littlestone dimension (MCLdim) is finite, even when the label space is unbounded. The authors extend the Littlestone dimension to the multiclass setting and develop an algorithm using the multiplicative weights algorithm with experts defined by executions of the Standard Optimal Algorithm (SOA) on subsequences of size MCLdim. A key contribution is establishing a separation between online learnability and online uniform convergence by exhibiting a class whose sequential Rademacher complexity is unbounded despite being easy to learn.

## Method Summary
The paper's approach uses the Standard Optimal Algorithm (SOA) in a black-box fashion to construct a finite set of experts, then runs the Randomized Exponentially Weighted Algorithm (REWA) over these experts. The SOA algorithm updates a set of hypotheses based on observed examples and predicts using the label maximizing MCLdim of remaining hypotheses. The expert set is constructed by running SOA on all possible subsequences of indices with size at most MCLdim. This approach achieves a regret bound of d + √(2dT ln(T)) where d is the Multiclass Littlestone dimension, proving that finite MCLdim is sufficient for online learnability.

## Key Results
- Any multiclass concept class is agnostically learnable if and only if its Littlestone dimension is finite
- The learning algorithm achieves regret at most MCLdim relative to the best concept in the class
- A separation is proven between online learnability and online uniform convergence using the Natarajan example
- The result holds even when the label space is countable but unbounded

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The finiteness of the Multiclass Littlestone dimension (MCLdim) is necessary and sufficient for online learnability even when the label space is unbounded.
- Mechanism: By extending the Littlestone dimension to the multiclass setting, the paper defines a binary tree shattered by a hypothesis class H. If MCLdim(H) is finite, the sequential Rademacher complexity vanishes, guaranteeing online learnability. The algorithm uses the Standard Optimal Algorithm (SOA) on subsequences of size MCLdim to construct a set of experts, ensuring that the best expert has regret at most MCLdim relative to the best concept in H.
- Core assumption: The Littlestone dimension-based characterization holds for multiclass classification when the label space is countable but unbounded.
- Evidence anchors:
  - [abstract]: "any multiclass concept class is agnostically learnable if and only if its Littlestone dimension is finite"
  - [section]: "Rakhlin et al. [2015] show that the minimax value of an online game is upper-bounded by the sequential Rademacher complexity of the loss class ℓ ◦ H. If Ldim(ℓ ◦ H) < ∞, the sequential Rademacher complexity of ℓ ◦ H vanishes."
  - [corpus]: Weak evidence; no direct mentions of Littlestone dimension in corpus titles/abstracts, but related papers exist on online learnability and classification.

### Mechanism 2
- Claim: The Multiclass Littlestone dimension continues to characterize online learnability even when the label space is unbounded.
- Mechanism: The paper shows that the SOA algorithm remains a realizable, mistake-bound online learner when the label space is unbounded. Using SOA in a black-box fashion, the algorithm constructs a finite set of experts and runs the Randomized Exponentially Weighted Algorithm (REWA) over them. This ensures that the best expert has regret at most MCLdim relative to the best concept in the class.
- Core assumption: The SOA algorithm can be used as a black-box to construct a finite set of experts that approximately covers the optimal hypothesis in hindsight.
- Evidence anchors:
  - [section]: "Recall the SOA algorithm from Daniely et al. [2011]... Using this fact, we use SOA in a black-box fashion to construct an agnostic online learner."
  - [abstract]: "Our learning algorithm uses the multiplicative weights algorithm, with a set of experts defined by executions of the Standard Optimal Algorithm on subsequences of size Littlestone dimension."
  - [corpus]: Weak evidence; no direct mentions of SOA or expert construction in corpus titles/abstracts, but related papers exist on online learning algorithms.

### Mechanism 3
- Claim: There is a separation between online learnability and online uniform convergence.
- Mechanism: The paper exhibits a hypothesis class whose sequential Rademacher complexity is unbounded, despite being easy to learn. This shows that the finiteness of the Littlestone dimension of the loss class (Ldim(ℓ ◦ H)) is not necessary for online learnability, providing a separation between online learnability and online uniform convergence.
- Core assumption: The example constructed by Natarajan [1989] can be used to show the separation between online learnability and online uniform convergence.
- Evidence anchors:
  - [abstract]: "Additionally, the paper proves a separation between online learnability and online uniform convergence by exhibiting a class whose sequential Rademacher complexity is unbounded, despite being easy to learn."
  - [section]: "We show in Section 3 that this is not the case... Our proof of this Theorem involves a beautiful example studied by Natarajan [1989] to show that the Vapnik-Chervonenkis (VC) dimension of the loss class is not necessary for multiclass learnability in the batch setting."
  - [corpus]: Weak evidence; no direct mentions of uniform convergence or separation in corpus titles/abstracts, but related papers exist on learning theory and complexity.

## Foundational Learning

- Concept: Littlestone dimension
  - Why needed here: The Littlestone dimension characterizes the online learnability of binary hypothesis classes and is extended to the multiclass setting in this paper.
  - Quick check question: What is the definition of the Littlestone dimension for binary hypothesis classes, and how is it extended to the multiclass setting?

- Concept: Multiclass Littlestone dimension (MCLdim)
  - Why needed here: MCLdim is used to characterize the online learnability of multiclass hypothesis classes, even when the label space is unbounded.
  - Quick check question: How does the definition of MCLdim differ from the Littlestone dimension for binary classes, and what is its significance in characterizing online learnability?

- Concept: Sequential Rademacher complexity
  - Why needed here: The sequential Rademacher complexity is used to bound the minimax value of an online game and is related to the finiteness of the Littlestone dimension.
  - Quick check question: What is the relationship between the sequential Rademacher complexity and the Littlestone dimension, and how does it relate to online learnability?

## Architecture Onboarding

- Component map:
  - Multiclass Littlestone dimension (MCLdim) -> Characterizes online learnability
  - Standard Optimal Algorithm (SOA) -> Constructs set of experts
  - Randomized Exponentially Weighted Algorithm (REWA) -> Runs over expert set
  - Sequential Rademacher complexity -> Bounds minimax value of online game

- Critical path:
  1. Calculate MCLdim for the hypothesis class
  2. Use SOA to construct a set of experts
  3. Run REWA over the set of experts
  4. Achieve regret bound based on MCLdim

- Design tradeoffs:
  - Using SOA as a black-box may limit flexibility in algorithm design
  - Constructing a finite set of experts may be computationally expensive for large hypothesis classes
  - The regret bound depends on the finiteness of MCLdim, which may not hold for all hypothesis classes

- Failure signatures:
  - If MCLdim is infinite, the sequential Rademacher complexity is unbounded, implying the hypothesis class is not online learnable
  - If the SOA algorithm cannot be used as a black-box or the expert construction fails, the regret bound may not be achieved
  - If the example constructed by Natarajan [1989] cannot be used to show the separation between online learnability and online uniform convergence, the claim is invalid

- First 3 experiments:
  1. Verify the finiteness of MCLdim for a given hypothesis class
  2. Implement the SOA algorithm and use it to construct a set of experts
  3. Run REWA over the set of experts and measure the regret bound achieved

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is there a way to characterize online multiclass learnability using sequential Rademacher complexity instead of Multiclass Littlestone dimension?
- Basis in paper: [explicit] The authors prove that there exists an easy-to-learn class whose sequential Rademacher complexity is unbounded, despite being easy to learn.
- Why unresolved: The paper shows that the sequential Rademacher complexity is not necessary for online learnability, but it does not explore whether it could be sufficient or provide an alternative characterization.
- What evidence would resolve it: A proof that sequential Rademacher complexity is either sufficient or provides an alternative characterization of online multiclass learnability, or a counterexample demonstrating it is not.

### Open Question 2
- Question: Can the regret bound of the learning algorithm be improved beyond O(d + √(dT log T))?
- Basis in paper: [explicit] The authors' algorithm achieves a regret bound of d + √(2dT log T), where d is the Multiclass Littlestone dimension.
- Why unresolved: While the authors show that this bound is sufficient for online learnability, it remains open whether it is tight or if a better bound is achievable.
- What evidence would resolve it: A lower bound proving that the regret must be at least Ω(d + √(dT log T)), or an algorithm achieving a strictly better regret bound.

### Open Question 3
- Question: How does the online multiclass learnability characterization extend to structured output prediction problems?
- Basis in paper: [inferred] The paper focuses on standard multiclass classification with a finite label space. However, many real-world problems involve structured outputs (e.g., sequences, trees).
- Why unresolved: The paper does not address how the characterization might extend to structured output settings, where the label space is typically infinite and the loss function is more complex.
- What evidence would resolve it: A generalization of the Multiclass Littlestone dimension to structured output settings, along with a proof that it characterizes online learnability in that context.

### Open Question 4
- Question: Is there a more efficient algorithm for online multiclass learning that does not rely on the multiplicative weights algorithm?
- Basis in paper: [explicit] The authors use the multiplicative weights algorithm with a set of experts defined by executions of the Standard Optimal Algorithm (SOA) on subsequences of size Littlestone dimension.
- Why unresolved: While the algorithm is shown to be effective, it may not be the most computationally efficient approach, especially for large Littlestone dimensions.
- What evidence would resolve it: A new algorithm for online multiclass learning that achieves the same regret bound but with better computational efficiency, or a lower bound proving that the multiplicative weights approach is optimal in terms of computational complexity.

## Limitations

- The extension of Littlestone dimension to unbounded label spaces relies on countable label space assumption
- The separation result depends on the Natarajan example, which requires verification of unbounded sequential Rademacher complexity
- The expert construction approach may face computational challenges for large or complex hypothesis classes

## Confidence

- High confidence: Necessity direction (infinite MCLdim implies non-learnability)
- Medium confidence: Sufficiency direction (finite MCLdim implies learnability) due to black-box use of SOA
- Medium confidence: Separation result between online learnability and uniform convergence

## Next Checks

1. Implement a concrete verification that the Natarajan example has unbounded sequential Rademacher complexity while being online learnable, as this is the critical separation result.
2. Test the expert construction approach on a specific hypothesis class with small MCLdim to verify that the regret bound of MCLdim is achieved in practice.
3. Verify the computational tractability of the algorithm by implementing it on a multiclass classification problem with a known finite MCLdim, measuring both regret and computational cost.