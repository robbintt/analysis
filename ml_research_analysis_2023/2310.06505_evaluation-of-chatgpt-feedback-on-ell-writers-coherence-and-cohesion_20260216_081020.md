---
ver: rpa2
title: Evaluation of ChatGPT Feedback on ELL Writers' Coherence and Cohesion
arxiv_id: '2310.06505'
source_url: https://arxiv.org/abs/2310.06505
tags:
- feedback
- essay
- chatgpt
- essays
- ideas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated ChatGPT\u2019s ability to provide feedback\
  \ on English Language Learners\u2019 (ELLs) essay coherence and cohesion without\
  \ task-specific tuning. Researchers generated feedback for 50 argumentative essays\
  \ using ChatGPT and assessed its accuracy and usability using a two-step approach:\
  \ linguistic analysis of feedback type (e.g., positive reinforcement, problem statement)\
  \ and evaluation of accuracy and usefulness."
---

# Evaluation of ChatGPT Feedback on ELL Writers' Coherence and Cohesion

## Quick Facts
- arXiv ID: 2310.06505
- Source URL: https://arxiv.org/abs/2310.06505
- Reference count: 21
- Primary result: ChatGPT's feedback on ELL essay coherence and cohesion was found to be generic, inaccurate, and of low usability without task-specific training.

## Executive Summary
This study evaluated ChatGPT's ability to provide feedback on English Language Learners' (ELLs) essay coherence and cohesion without task-specific tuning. Researchers generated feedback for 50 argumentative essays using ChatGPT and assessed its accuracy and usability using a two-step approach: linguistic analysis of feedback type (e.g., positive reinforcement, problem statement) and evaluation of accuracy and usefulness. Results showed that most feedback was highly abstract and generic, lacking concrete suggestions for improvement. ChatGPT struggled to detect major problems like repetitive ideas and inaccurate cohesive device usage, often relying on superficial linguistic features. The feedback was found to be inaccurate and of low usability, failing to provide actionable guidance for students. Therefore, ChatGPT, without task-specific training, does not offer effective feedback on ELLs' coherence and cohesion.

## Method Summary
The study used 50 argumentative essays from the ELLIPSE Corpus (grades 8-12) with cohesion scores ranging from 1-5. ChatGPT (GPT-4) generated feedback using a zero-shot approach with a task definition and ELLIPSE rubrics. Feedback was classified by type (positive reinforcement, problem statement, explicit examples, indirect suggestions) and evaluated for accuracy and usability. Expert-generated feedback served as a benchmark. The evaluation involved linguistic analysis and comparison with human expert feedback.

## Key Results
- Most feedback sentences were highly abstract and generic, failing to provide concrete suggestions for improvement.
- ChatGPT struggled to detect major problems like repetitive ideas and inaccurate cohesive device usage, relying on superficial linguistic features.
- Feedback accuracy and usability were low, with no meaningful improvement across different essay quality levels.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ChatGPT's feedback accuracy depends heavily on the surface-level linguistic features (e.g., presence of transitional words) rather than deeper semantic coherence.
- **Mechanism:** The model matches input patterns to rubric-based templates. When it sees connectives like "first" or "second," it assumes logical flow, even if ideas are unrelated or contradictory.
- **Core assumption:** Coherence can be approximated by keyword presence rather than idea progression.
- **Evidence anchors:**
  - [abstract] "The accuracy in detecting major problems, such as repetitive ideas and the inaccurate use of cohesive devices, depended on superficial linguistic features and was often incorrect."
  - [section 5] "It appeared to be sensitive to the presence or absence of transitional words, but it could not distinguish inefficient and inaccurate usage from efficient and accurate usage."
  - [corpus] Weak: only 5 related papers, none directly on this mechanism.
- **Break condition:** If rubric language is replaced with deeper semantic evaluation, or if the model is fine-tuned to detect logical flow instead of connective tokens.

### Mechanism 2
- **Claim:** ChatGPT generates feedback that mirrors rubric language verbatim, providing little task-specific insight.
- **Mechanism:** Without task-specific tuning, the model defaults to regurgitating rubric text, resulting in generic problem statements that do not reflect the essay's actual issues.
- **Core assumption:** The rubric text is sufficient to guide meaningful feedback without adaptation.
- **Evidence anchors:**
  - [abstract] "most feedback sentences were highly abstract and generic, failing to provide concrete suggestions for improvement."
  - [section 4.6] "ChatGPT used almost exclusively the rubric's language and offered not much new information."
  - [corpus] Weak: no direct evidence on rubric mirroring in corpus.
- **Break condition:** If feedback is required to include specific, actionable examples or if the rubric is adapted to include corrective guidance.

### Mechanism 3
- **Claim:** ChatGPT struggles to identify the central idea in lower-scoring essays, leading to misaligned feedback.
- **Mechanism:** The model evaluates coherence based on organizational markers but cannot detect whether the essay has a coherent controlling idea.
- **Core assumption:** Organizational markers (intro, body, conclusion) imply coherence even if the central idea is unclear.
- **Evidence anchors:**
  - [abstract] "ChatGPT failed to recognize the presence of a central idea in these two low-scoring essays."
  - [section 4.5] "ChatGPT failed to recognize the presence of a central idea in these two low-scoring essays. This failure led to distinct differences in the feedback provided by the writing experts and ChatGPT."
  - [corpus] Weak: corpus does not contain direct evidence on central idea detection.
- **Break condition:** If the model is trained to explicitly identify and assess controlling ideas before evaluating coherence.

## Foundational Learning

- **Concept:** ELLIPSE Rubric Scoring
  - Why needed here: The study uses this rubric to define coherence/cohesion criteria; understanding it is critical to interpreting feedback.
  - Quick check question: What is the highest score in the ELLIPSE rubric, and what does it signify?
- **Concept:** Prompt Engineering for LLMs
  - Why needed here: The study tests ChatGPT's feedback without task-specific tuning; understanding prompt effects is key to evaluating results.
  - Quick check question: What parameter was set to zero to prevent randomness in the GPT-4 output?
- **Concept:** Types of Corrective Feedback (Direct, Indirect, Positive Reinforcement, Problem Statement, Explicit Example, Indirect Suggestion)
  - Why needed here: The study classifies feedback into these types to evaluate accuracy and usefulness.
  - Quick check question: Which feedback type was most frequent in ChatGPT's output, and why might that be?

## Architecture Onboarding

- **Component map:**
  - Data Source (ELLIPSE Corpus) -> LLM (GPT-4) -> Prompt (Task + Rubric) -> Evaluation (Linguistic Analysis + Expert Review) -> Feedback Classification

- **Critical path:**
  1. Select essays by cohesion score
  2. Generate feedback with GPT-4
  3. Classify each sentence by feedback type
  4. Evaluate accuracy and usability
  5. Compare with expert feedback

- **Design tradeoffs:**
  - No prompt tuning vs. task-specific performance: Higher external validity but lower accuracy.
  - Rubric mirroring vs. novel feedback: Simpler implementation but less actionable guidance.
  - Sample essays in prompt vs. cost: Slight accuracy improvement but higher token usage.

- **Failure signatures:**
  - Feedback relies on rubric text verbatim (low specificity).
  - Feedback focuses on connectives rather than idea progression (low accuracy).
  - Low-scoring essays receive no positive reinforcement (biased scoring).
  - Repeated sentences across essays (lack of personalization).

- **First 3 experiments:**
  1. Add explicit corrective suggestions to the rubric and regenerate feedback.
  2. Include sample essays in the prompt and measure changes in accuracy.
  3. Manually annotate essays for central idea presence and correlate with feedback type.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does task-specific fine-tuning of ChatGPT significantly improve the accuracy and usability of feedback on ELL students' coherence and cohesion?
- **Basis in paper:** [explicit] The authors explicitly state that their conclusions are restricted to ChatGPT without training for the coherence/cohesion feedback task and suggest that prompts optimized for coherence/cohesion feedback generation might achieve higher accuracy and enhanced usability.
- **Why unresolved:** The study only evaluated ChatGPT's performance without task-specific training, leaving open the question of how fine-tuning might affect performance.
- **What evidence would resolve it:** Conducting a study comparing ChatGPT's performance with and without task-specific fine-tuning on a large dataset of ELL essays would provide evidence of the impact of fine-tuning.

### Open Question 2
- **Question:** What specific types of training data and prompt engineering would be most effective in improving ChatGPT's ability to detect and provide feedback on major coherence and cohesion issues?
- **Basis in paper:** [inferred] The paper discusses ChatGPT's limitations in detecting major problems like repetitive ideas and inaccurate cohesive device usage, suggesting that improved detection requires better training data and prompts.
- **Why unresolved:** The study identified problems but did not test specific training data or prompt engineering solutions.
- **What evidence would resolve it:** Experiments testing different training datasets and prompt structures to optimize ChatGPT's detection of coherence/cohesion issues would provide insights into effective approaches.

### Open Question 3
- **Question:** How does ChatGPT's feedback accuracy compare to human expert feedback across different proficiency levels of ELL students?
- **Basis in paper:** [explicit] The authors mention that their evaluation compared ChatGPT feedback to writing experts' feedback but note that the study was limited in scope.
- **Why unresolved:** The study had a small sample size and did not comprehensively compare ChatGPT's performance across all proficiency levels.
- **What evidence would resolve it:** A large-scale study comparing ChatGPT's feedback to human expert feedback across essays from all proficiency levels would provide a comprehensive assessment of relative accuracy.

## Limitations

- ChatGPT's feedback accuracy depends on superficial linguistic features rather than deeper semantic coherence.
- The model generates feedback that mirrors rubric language verbatim, providing little task-specific insight.
- ChatGPT struggles to identify the central idea in lower-scoring essays, leading to misaligned feedback.

## Confidence

**High:** ChatGPT's feedback is generic and relies on superficial features.
**Medium:** ChatGPT struggles to identify central ideas in low-scoring essays.
**Medium:** Feedback accuracy is significantly lower without task-specific tuning.

## Next Checks

1. Test ChatGPT with a modified rubric that includes explicit corrective suggestions to assess if feedback specificity improves.
2. Compare feedback accuracy when sample essays are included in the prompt versus when they are omitted.
3. Conduct a human expert evaluation of feedback usefulness in a real classroom setting with ELL students.