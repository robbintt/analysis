---
ver: rpa2
title: Benchmarking Jetson Edge Devices with an End-to-end Video-based Anomaly Detection
  System
arxiv_id: '2307.16834'
source_url: https://arxiv.org/abs/2307.16834
tags:
- jetson
- system
- docker
- nvidia
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks an end-to-end video-based anomaly detection
  system on multiple NVIDIA Jetson edge devices (Nano, AGX Xavier, Orin Nano), incorporating
  the RTFM model for weakly supervised anomaly detection and TensorRT for performance
  optimization. The system processes surveillance videos directly on edge hardware
  using Docker containers and a PyTorch-based pipeline.
---

# Benchmarking Jetson Edge Devices with an End-to-end Video-based Anomaly Detection System

## Quick Facts
- arXiv ID: 2307.16834
- Source URL: https://arxiv.org/abs/2307.16834
- Reference count: 2
- Key outcome: Jetson Orin Nano delivers best performance (47.56 FPS, 3.11 GB RAM) with 15% higher throughput and 50% lower energy than AGX Xavier

## Executive Summary
This study benchmarks an end-to-end video-based anomaly detection system on multiple NVIDIA Jetson edge devices using the RTFM model for weakly supervised anomaly detection and TensorRT for performance optimization. The system processes surveillance videos directly on edge hardware using Docker containers and a PyTorch-based pipeline. Experimental results show the Jetson Orin Nano delivers the best performance with 47.56 FPS inference speed and 3.11 GB RAM usage, achieving 15% higher throughput and 50% lower energy consumption compared to the AGX Xavier, while the Nano version runs at 1.55 FPS due to hardware limitations.

## Method Summary
The study deploys an RTFM-based anomaly detection system on three NVIDIA Jetson edge devices (Nano, AGX Xavier, Orin Nano) using Docker containers with specific L4T PyTorch and Torch-TensorRT versions. The RTFM model, which uses video-level labels and temporal feature magnitude for weakly supervised anomaly detection, is fine-tuned on UCF-Crime and UIT-VNAnomaly datasets. The system processes video streams from a Logitech C920 camera through data preprocessing, ResNet50-I3D non-local feature extraction, and RTFM anomaly detection. Performance is measured across FPS, RAM usage, and power consumption metrics.

## Key Results
- Jetson Orin Nano achieves 47.56 FPS with 3.11 GB RAM usage, outperforming other devices
- Jetson AGX Xavier provides 15% lower throughput and 50% higher energy consumption than Orin Nano
- RTFM model achieves 84.39% AUC on UCF-Crime and 88.4% AUC on UIT-VNAnomaly datasets
- Jetson Nano runs at 1.55 FPS due to hardware limitations, demonstrating resource constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TensorRT optimization significantly improves inference speed on Jetson devices.
- Mechanism: TensorRT uses techniques like reduced precision (FP16/INT8), layer fusion, and dynamic memory allocation to reduce computational load and memory bandwidth, enabling faster inference.
- Core assumption: The hardware-specific optimizations provided by TensorRT are compatible with the RTFM model and its feature extractor (ResNet50-I3D non-local).
- Evidence anchors:
  - [abstract] "The system processes surveillance videos directly on edge hardware using Docker containers and a PyTorch-based pipeline. Experiments show the Jetson Orin Nano delivers the best performance with 47.56 FPS inference speed and 3.11 GB RAM usage..."
  - [section 3.2] "TensorRT dramatically improves the performance of end-to-end anomaly detection systems, enabling us to deploy and run on multiple resource-limited Jetson Edge Devices efficiently."
- Break condition: If the RTFM model or its dependencies are not supported by the installed TensorRT version, the optimizations will fail or degrade performance.

### Mechanism 2
- Claim: Weakly supervised learning with RTFM reduces the need for expensive frame-level annotations.
- Mechanism: RTFM uses video-level labels to identify anomalous snippets by learning the temporal feature magnitude, selecting k snippets with the highest scores for training.
- Core assumption: The temporal feature magnitude is a reliable indicator of anomaly presence, and the selected snippets adequately represent both normal and anomalous behaviors.
- Evidence anchors:
  - [abstract] "the RTFM model achieves 84.39% AUC on UCF-Crime and 88.4% AUC on UIT-VNAnomaly datasets..."
  - [section 3.3] "Each video is divided into a bag of T snippet clips... The low feature magnitude scores set the normal snippets, while the high feature magnitude denotes the anomalous snippets."
- Break condition: If the video-level labels are too coarse or the anomalies are too subtle, the feature magnitude may not effectively separate normal and anomalous snippets.

### Mechanism 3
- Claim: Edge deployment minimizes latency and bandwidth usage compared to cloud-based processing.
- Mechanism: By processing video streams directly on the Jetson edge devices, the system avoids uploading large amounts of data to the cloud, reducing network latency and bandwidth requirements.
- Core assumption: The Jetson devices have sufficient computational power to process the video streams in real-time and meet the latency requirements for anomaly detection.
- Evidence anchors:
  - [abstract] "The application of Edge AI (Edge computing-based Artificial Intelligent) meets the strict latency requirements for security."
  - [introduction] "The traditional approach requires uploading large amounts of visual surveillance data to the cloud for analysis, leading to extensive bandwidth and inevitable network latency."
- Break condition: If the Jetson device cannot maintain the required frame rate or if the network connection is unreliable, the edge deployment advantage is lost.

## Foundational Learning

- Concept: Deep Learning Model Optimization
  - Why needed here: To deploy complex models like RTFM on resource-constrained edge devices while maintaining real-time performance.
  - Quick check question: What are the key optimization techniques provided by TensorRT, and how do they reduce computational load?

- Concept: Weakly Supervised Learning
  - Why needed here: To train anomaly detection models without requiring expensive frame-level annotations for each video.
  - Quick check question: How does RTFM use video-level labels to identify anomalous snippets, and what is the role of temporal feature magnitude?

- Concept: Edge Computing Deployment
  - Why needed here: To minimize latency and bandwidth usage by processing video streams directly on the Jetson edge devices.
  - Quick check question: What are the advantages of edge deployment over cloud-based processing for real-time anomaly detection, and what are the potential limitations?

## Architecture Onboarding

- Component map:
  Surveillance camera (Logitech C920) -> Jetson edge device (Nano/AGX Xavier/Orin Nano) -> RTFM anomaly detection model -> Output prediction
  Key components: Docker container, PySlowFast framework, TensorRT optimization, ResNet50-I3D non-local feature extractor
- Critical path: Video capture -> Data preprocessing -> Feature extraction -> Anomaly detection -> Output prediction
- Design tradeoffs:
  - Performance vs. resource usage: Higher frame rates require more computational power and memory.
  - Accuracy vs. annotation effort: Weakly supervised learning reduces annotation effort but may impact accuracy.
  - Edge vs. cloud deployment: Edge deployment minimizes latency but may have limited computational resources.
- Failure signatures:
  - Low frame rate or dropped frames: Indicates computational bottleneck or insufficient resources.
  - High memory usage or crashes: Indicates memory limitations or inefficient memory management.
  - Low accuracy or false positives/negatives: Indicates issues with model training, feature extraction, or anomaly detection.
- First 3 experiments:
  1. Test camera integration and video capture on Jetson device.
  2. Verify data preprocessing pipeline and feature extraction with sample video.
  3. Run RTFM model on Jetson device with pre-extracted features and measure inference time and accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific components from TensorRT v1.3.0 need to be manually integrated into TensorRT v1.0.0 for Jetson Nano compatibility?
- Basis in paper: [explicit] The paper mentions that Jetson Nano is compatible with TensorRT v1.0.0 but this version only partially supports functionalities of the chosen anomaly detection model. The authors note that they need to examine functionalities in TensorRT v1.1.1 and manually add scripts not present in v1.0.0 to ensure operational accuracy.
- Why unresolved: The paper identifies the need for manual integration but doesn't specify which exact components or scripts are required, only mentioning that this examination and integration process is necessary.
- What evidence would resolve it: A detailed list of the specific TensorRT v1.1.1 scripts/components that were identified as necessary and successfully integrated into the v1.0.0 codebase for Jetson Nano compatibility.

### Open Question 2
- Question: How does the RTFM model performance scale with different video snippet lengths and frame counts beyond the tested configuration of 32 snippets with 16 frames each?
- Basis in paper: [inferred] The paper describes using 32 snippets with 16 frames each as the specific configuration, but doesn't explore how performance changes with different snippet lengths or frame counts. This represents an unexplored parameter space for optimization.
- Why unresolved: The authors chose a specific configuration but didn't conduct experiments varying these parameters to determine optimal settings for different use cases or hardware constraints.
- What evidence would resolve it: Systematic experiments varying snippet lengths (e.g., 16, 32, 64 snippets) and frame counts (e.g., 8, 16, 32 frames) with corresponding AUC and FPS measurements to identify optimal configurations.

### Open Question 3
- Question: What is the impact of different Docker base images on the system's performance and compatibility across Jetson devices?
- Basis in paper: [inferred] While the paper discusses using L4T PyTorch Docker images and mentions version compatibility issues, it doesn't explore alternative base images or optimization strategies for Docker deployment.
- Why unresolved: The authors settled on L4T PyTorch images but didn't investigate whether other base images might offer better performance, smaller footprint, or easier deployment across different Jetson devices.
- What evidence would resolve it: Comparative performance benchmarks using different Docker base images (e.g., L4T base, minimal PyTorch images, multi-architecture images) measuring RAM usage, FPS, and deployment ease across all three Jetson devices.

## Limitations

- Hardware constraints limit Jetson Nano to 1.55 FPS, demonstrating significant performance gaps between devices
- Model training requires 16GB memory for ResNet50-I3D non-local feature extractor, which may not represent typical edge deployment scenarios
- Lack of statistical significance testing or confidence intervals for reported AUC scores (84.39% and 88.4%) reduces confidence in performance claims

## Confidence

- **High Confidence**: The Jetson Orin Nano delivering the best performance metrics (47.56 FPS, 3.11 GB RAM usage) - This claim is directly supported by experimental measurements in the results section with specific hardware configurations and clear methodology.
- **Medium Confidence**: TensorRT optimization providing 15% higher throughput and 50% lower energy consumption compared to AGX Xavier - While the relative improvement is reported, the baseline measurements and energy consumption methodology lack sufficient detail for full verification.
- **Low Confidence**: The weakly supervised RTFM model achieving effective anomaly detection on resource-constrained edge environments - The reported AUC scores appear promising but lack comparative analysis with fully supervised approaches or other state-of-the-art methods to establish true effectiveness.

## Next Checks

1. **Reproduce benchmark metrics**: Deploy the RTFM model with TensorRT optimization on the three Jetson devices using the specified Docker configurations and measure FPS, RAM usage, and power consumption under identical conditions to verify the reported 47.56 FPS on Orin Nano, 3.11 GB RAM usage, and energy efficiency claims.

2. **Statistical validation of accuracy**: Perform statistical significance testing (t-tests or ANOVA) on the AUC scores (84.39% UCF-Crime, 88.4% UIT-VNAnomaly) using cross-validation or multiple runs to establish confidence intervals and determine if the performance differences between devices are statistically significant.

3. **Alternative model comparison**: Benchmark at least one alternative anomaly detection model (e.g., fully supervised CNN or another weakly supervised approach) on the same Jetson devices to establish whether the RTFM model's performance advantages are model-specific or representative of general edge deployment capabilities.