---
ver: rpa2
title: Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing
  with Non-Learnable Primitives
arxiv_id: '2308.02066'
source_url: https://arxiv.org/abs/2308.02066
tags:
- task
- tasks
- shared
- features
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multi-task learning (MTL) suffers from task interference, limiting
  performance gains. This paper introduces ETR-NLP, a method combining non-learnable
  primitives (NLPs) and explicit task routing (ETR) to mitigate interference.
---

# Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives

## Quick Facts
- arXiv ID: 2308.02066
- Source URL: https://arxiv.org/abs/2308.02066
- Reference count: 40
- Primary result: ETR-NLP achieves 61.49 mIoU on Cityscapes semantic segmentation, a +3.56 improvement over prior methods

## Executive Summary
This paper addresses task interference in multi-task learning by introducing ETR-NLP, a method that combines non-learnable primitives (NLPs) and explicit task routing (ETR). NLPs extract diverse, task-agnostic features without learnable parameters, while ETR partitions network parameters into shared and task-specific branches. The method significantly outperforms state-of-the-art baselines on both image-level classification (CelebA) and pixel-level dense prediction tasks (Cityscapes, NYU-v2) while using fewer learnable parameters and maintaining similar FLOPs.

## Method Summary
ETR-NLP mitigates task interference by using non-learnable primitives to extract diverse, task-agnostic features, then recombining them through a group-wise 1×1 convolution before routing through an explicit task routing module. This module separates parameters into shared and task-specific branches, providing fine-grained control over parameter sharing. The approach leverages NLPs' lack of learnable parameters to reduce gradient conflicts while using ETR's explicit partitioning to further isolate task-specific learning. Training uses a "steady-state" strategy for image-level tasks and "synchronized" strategy for pixel-level tasks.

## Key Results
- On Cityscapes, ETR-NLP achieves 61.49 mIoU for semantic segmentation (+3.56 over prior methods)
- On CelebA, ETR-NLP improves average relative performance by 2.12% over baselines
- ETR-NLP uses fewer learnable parameters than comparison methods while maintaining similar FLOPs

## Why This Works (Mechanism)

### Mechanism 1
Non-learnable primitives (NLPs) mitigate task interference by extracting task-agnostic features without learnable parameters, preventing conflicting gradients from affecting each task. Since NLPs lack learnable parameters, the extracted features aren't optimized for any specific task, reducing the impact of conflicting gradients that typically cause interference.

### Mechanism 2
Explicit task routing (ETR) mitigates task interference by partitioning parameters into shared and task-specific branches, providing precise control over parameter sharing. ETR separates network parameters into a shared branch (common to all tasks) and task-specific branches (exclusive to each task), allowing each task to learn task-specific features without interfering with others.

### Mechanism 3
The combination of NLPs and ETR works synergistically to further reduce task interference. NLPs provide diverse, task-agnostic features that reduce gradient conflicts, while ETR provides explicit control over parameter sharing. Together, they allow each task to adaptively select the most relevant features and parameters for its specific needs, minimizing interference.

## Foundational Learning

- Concept: Multi-task learning (MTL) and task interference
  - Why needed here: Understanding the fundamental challenge that ETR-NLP addresses - how learning multiple tasks simultaneously can lead to performance degradation due to conflicting gradients
  - Quick check question: What is task interference in multi-task learning, and why does it occur?

- Concept: Parameter sharing strategies in neural networks
  - Why needed here: Understanding different approaches to sharing parameters across tasks (hard sharing, soft sharing, parameter partitioning) and their limitations
  - Quick check question: What are the differences between hard parameter sharing, soft parameter sharing, and explicit parameter partitioning?

- Concept: Non-parametric and weight-agnostic operations
  - Why needed here: Understanding types of operations that can be used as non-learnable primitives (NLPs) and how they differ from standard learnable layers
  - Quick check question: What are some examples of non-parametric and weight-agnostic operations that could be used as NLPs?

## Architecture Onboarding

- Component map: Input → NLP extraction → feature recombination → ETR routing → task-specific predictions
- Critical path: Input → NLP extraction → feature recombination → ETR routing → task-specific predictions
- Design tradeoffs:
  - Number and types of NLPs vs. model complexity and computational cost
  - Ratio of shared to task-specific parameters in ETR vs. performance and interference
  - Diversity of NLP features vs. relevance to specific tasks
- Failure signatures:
  - Performance degradation when increasing number of tasks
  - One task dominating shared branch, leading to poor performance on other tasks
  - Insufficient diversity in NLP features leading to poor task performance
- First 3 experiments:
  1. Compare standard MTL (hard sharing) vs. MTL with only NLPs vs. MTL with only ETR on simple multi-task dataset
  2. Evaluate effect of different NLP configurations (number and types of NLPs) on MTL performance
  3. Test impact of different ETR sharing ratios (gamma values) on MTL performance

## Open Questions the Paper Calls Out

### Open Question 1
How do different combinations of NLPs affect task interference in MTL beyond the specific configuration studied? The paper evaluates 31 possible NLP combinations but only presents results for the optimal configuration, leaving questions about how other combinations might perform across different task domains.

### Open Question 2
What is the optimal sharing ratio (γ) for explicit task routing across different types of MTL problems? The paper states γ=0.9 is optimal but only tested on two specific datasets, without exploring whether this ratio generalizes to other MTL scenarios or task combinations.

### Open Question 3
How does the training strategy for ETR-NLP (steady-state vs. synchronized) interact with different learning rates, batch sizes, or optimization algorithms? The paper compares training strategies but only reports results for specific configurations, without exploring whether observed differences persist under different optimization settings.

## Limitations

- Scalability to very large-scale vision tasks with dozens of tasks or high-resolution inputs remains untested
- Limited exploration of the parameter space for optimal ETR partitioning configurations
- The synergistic interaction between NLPs and ETR could be more thoroughly characterized

## Confidence

- NLP diversity claim: **High** (supported by systematic ablation in Figure 8)
- ETR partitioning effectiveness: **Medium** (well-demonstrated but limited parameter space exploration)
- Synergistic effect: **Medium-High** (supported by ablation studies but specific interaction effects could be more characterized)

## Next Checks

1. Test ETR-NLP on a dataset with 5+ diverse tasks to evaluate scalability and identify potential interference that emerges with task complexity
2. Implement a variant where NLP feature diversity is reduced (e.g., using only 2-3 primitive types) to quantify the minimum diversity threshold needed for interference mitigation
3. Measure and compare the gradient variance across tasks in standard MTL vs. ETR-NLP to directly validate the claimed gradient conflict reduction mechanism