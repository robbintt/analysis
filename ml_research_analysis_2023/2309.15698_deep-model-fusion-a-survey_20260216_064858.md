---
ver: rpa2
title: 'Deep Model Fusion: A Survey'
arxiv_id: '2309.15698'
source_url: https://arxiv.org/abs/2309.15698
tags:
- learning
- arxiv
- fusion
- which
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive survey of deep model fusion
  techniques, which combine multiple deep learning models to improve performance and
  robustness. The authors categorize existing methods into four-fold: mode connectivity,
  alignment, weight average, and ensemble learning.'
---

# Deep Model Fusion: A Survey

## Quick Facts
- **arXiv ID:** 2309.15698
- **Source URL:** https://arxiv.org/abs/2309.15698
- **Reference count:** 40
- **Key outcome:** Comprehensive survey of deep model fusion techniques that combine multiple deep learning models to improve performance and robustness, categorizing methods into mode connectivity, alignment, weight average, and ensemble learning.

## Executive Summary
This survey provides a systematic overview of deep model fusion techniques, which combine multiple deep learning models to improve performance and robustness. The authors categorize existing methods into four main approaches: mode connectivity (connecting solutions via low-loss paths in weight space), alignment (matching units between networks), weight average (directly averaging model weights), and ensemble learning (combining model outputs). The survey covers theoretical foundations, representative approaches, advantages, disadvantages, and applications across federated learning, fine-tuning, distillation, and large language models.

## Method Summary
Deep model fusion combines multiple trained deep learning models through four primary approaches. Mode connectivity connects solutions in weight space via low-loss paths to provide better initialization for fusion. Alignment matches units between neural networks to create better fusion conditions. Weight average directly averages model weights, assuming models share similar training trajectories. Ensemble learning combines model outputs through various aggregation strategies. The survey emphasizes that these techniques can improve accuracy, diversity, and robustness while potentially reducing computational costs and storage requirements compared to maintaining multiple separate models.

## Key Results
- Four main categories of deep model fusion: mode connectivity, alignment, weight average, and ensemble learning
- Mode connectivity enables merging of solutions in weight space without increasing loss
- Alignment handles heterogeneous models by matching units through permutation optimization
- Weight average provides a simple but effective approach when models are similar
- Applications span federated learning, fine-tuning, distillation, and large language models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mode connectivity enables merging of solutions in weight space without increasing loss, providing better initialization for model fusion.
- Mechanism: Solutions found by gradient-based optimization lie on a low-loss manifold. By connecting these solutions via continuous paths (connectors) in weight space, we can find intermediate models that are better suited for fusion.
- Core assumption: Local minima obtained by SGD can be connected by a path with non-increasing loss.
- Evidence anchors:
  - [abstract] "Mode connectivity connects the solutions in weight space via a path of non-increasing loss, in order to obtain better initialization for model fusion"
  - [section] "The solutions obtained by gradient-based optimization can be connected in weight space by a path (connector) with no obstacles, which is referred to as mode connectivity"
  - [corpus] Weak - no direct evidence in corpus neighbors
- Break condition: If the loss barrier between two models is too high, indicating disconnected basins, mode connectivity fails.

### Mechanism 2
- Claim: Alignment matches units between neural networks to create better conditions for fusion by reducing differences between models.
- Mechanism: By finding optimal permutations of units between models, we can minimize the cost function (e.g., Euclidean distance, correlation) between corresponding activations or weights, making the models more similar.
- Core assumption: Permutation symmetry exists in neural networks, allowing equivalent solutions through unit reordering.
- Evidence anchors:
  - [abstract] "Alignment matches units between neural networks to create better conditions for fusion"
  - [section] "Alignment matches the units of multiple models and average the models to obtain the final model"
  - [corpus] Weak - no direct evidence in corpus neighbors
- Break condition: If combinatorial optimization for finding permutations is intractable for large models, alignment becomes impractical.

### Mechanism 3
- Claim: Weight average combines multiple weights of networks for the final model with better performance by reducing variance and finding flatter minima.
- Mechanism: Directly averaging weights of multiple similar models (in the same basin) produces solutions closer to the optimal point in regions of low loss, improving generalization.
- Core assumption: Models being averaged share part of their training trajectory or are located in the same basin of the loss landscape.
- Evidence anchors:
  - [abstract] "Weight average, a classical model fusion method, averages the weights of multiple models to obtain more accurate results closer to the optimal solution"
  - [section] "WA [227] is the most direct and efficient way to fuse several parent networks into a single network"
  - [corpus] Weak - no direct evidence in corpus neighbors
- Break condition: If models have large structural differences or were trained on very different data, simple weight averaging may perform poorly.

## Foundational Learning

- Concept: Loss landscape geometry and mode connectivity
  - Why needed here: Understanding how solutions in weight space are connected is crucial for mode connectivity and alignment methods
  - Quick check question: Can two independently trained models always be connected by a low-loss path in weight space?

- Concept: Permutation symmetry in neural networks
  - Why needed here: Alignment methods rely on the fact that unit permutations don't change network functionality
  - Quick check question: Does swapping two hidden units in a neural network always preserve the network's function?

- Concept: Bayesian model averaging and ensemble methods
  - Why needed here: Weight averaging and ensemble learning are both ways to combine multiple models, but with different approaches
  - Quick check question: What's the difference between averaging model weights versus averaging model predictions?

## Architecture Onboarding

- Component map: Four main fusion approaches (mode connectivity -> alignment -> weight average -> ensemble learning) with applications in federated learning -> fine-tuning -> distillation -> large language models

- Critical path: For a new engineer, the critical path is understanding when to use each fusion method, implementing the chosen method, and evaluating the fused model's performance compared to individual models

- Design tradeoffs: Mode connectivity offers better initialization but is computationally expensive. Alignment handles heterogeneous models but requires complex optimization. Weight average is simple but assumes model similarity. Ensemble learning is flexible but requires maintaining multiple models

- Failure signatures: Mode connectivity fails when loss barriers are too high. Alignment fails when permutation search is intractable. Weight average fails when models are too dissimilar. Ensemble learning fails when models are too similar (no diversity)

- First 3 experiments:
  1. Implement simple linear mode connectivity between two models trained on CIFAR-10 and measure loss barrier
  2. Apply weight averaging to two models fine-tuned from the same pre-trained model and compare accuracy
  3. Implement ensemble learning with snapshot ensembles and compare to single model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of deep model fusion techniques scale with model size, particularly for large language models with billions of parameters?
- Basis in paper: [explicit] "Although the development of deep model fusion has brought many technical breakthroughs, it also produces a series of challenges, such as high computational load, model heterogeneity, and slow speed of alignment via combinatorial optimization [133, 204], etc."
- Why unresolved: The paper acknowledges challenges with large-scale models but doesn't provide empirical data on how fusion techniques perform as model size increases
- What evidence would resolve it: Comprehensive benchmarking of fusion techniques across models of varying sizes, from small networks to LLMs with billions of parameters, measuring both performance gains and computational overhead

### Open Question 2
- Question: What is the theoretical relationship between mode connectivity and the generalization performance of fused models?
- Basis in paper: [explicit] "In a flat basin, the solutions tend to demonstrate good performance. Conversely, points in narrow regions are easily accessible to energy-barriers, resulting in increased losses [167]."
- Why unresolved: While the paper discusses mode connectivity and its relation to loss landscapes, it doesn't establish a clear theoretical link between connectivity properties and generalization
- What evidence would resolve it: Rigorous mathematical proofs or extensive empirical studies showing how properties of mode connectivity (e.g., barrier height, path curvature) correlate with generalization performance across various model architectures and datasets

### Open Question 3
- Question: How can deep model fusion techniques be optimized for resource-constrained environments, such as edge devices or mobile applications?
- Basis in paper: [inferred] "WA is expected to reduce the storage space and computing overhead of the model on resource-constrained devices by implementing network compression on the terminal devices [250]."
- Why unresolved: The paper mentions potential applications in resource-constrained settings but doesn't provide specific strategies for optimizing fusion techniques for these environments
- What evidence would resolve it: Development and validation of fusion algorithms specifically designed for low-resource settings, demonstrating improved performance while maintaining efficiency on edge devices

## Limitations
- Effectiveness of mode connectivity heavily depends on loss landscape geometry, which varies significantly across architectures and datasets
- Current alignment methods struggle with scalability to large models due to combinatorial optimization challenges
- Weight averaging assumes model similarity, but this assumption breaks down when models are trained on different distributions or have structural differences

## Confidence

- **High Confidence**: Basic weight averaging and ensemble learning approaches have extensive empirical validation across multiple domains and are well-established in the literature
- **Medium Confidence**: Mode connectivity theory is well-supported mathematically, but practical implementation details and hyperparameter sensitivity need more systematic evaluation
- **Low Confidence**: Advanced alignment methods (graph matching, optimal transport-based approaches) show promise theoretically but lack comprehensive benchmarking against simpler alternatives

## Next Checks

1. Conduct controlled experiments comparing mode connectivity versus weight averaging on models with varying degrees of similarity to quantify when each approach is optimal
2. Implement scalability benchmarks for alignment methods on transformer-based architectures to identify practical limits and computational bottlenecks
3. Design ablation studies to isolate the impact of different fusion hyperparameters (path complexity for mode connectivity, permutation algorithms for alignment) on final model performance