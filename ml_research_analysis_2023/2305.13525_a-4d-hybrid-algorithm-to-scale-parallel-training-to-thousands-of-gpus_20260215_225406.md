---
ver: rpa2
title: A 4D Hybrid Algorithm to Scale Parallel Training to Thousands of GPUs
arxiv_id: '2305.13525'
source_url: https://arxiv.org/abs/2305.13525
tags:
- gpus
- communication
- parallel
- tensor
- parallelism
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Tensor3D, a three-dimensional (3D) hybrid tensor
  and data parallel framework for scaling the training of large neural networks to
  thousands of GPUs. The framework introduces an intelligent parameter distribution
  that eliminates communication at layer boundaries and a novel overdecomposition
  scheme that enables overlap of communication with computation.
---

# A 4D Hybrid Algorithm to Scale Parallel Training to Thousands of GPUs

## Quick Facts
- arXiv ID: 2305.13525
- Source URL: https://arxiv.org/abs/2305.13525
- Reference count: 40
- Key outcome: Tensor3D achieves 26% improvement over Megatron-LM and attains 57% of theoretical peak FLOP/s when training an 80-billion parameter GPT on 1024 GPUs

## Executive Summary
This paper presents Tensor3D, a three-dimensional hybrid tensor and data parallel framework designed to scale large neural network training to thousands of GPUs. The framework addresses the communication bottleneck in distributed training by introducing intelligent parameter distribution that eliminates communication at layer boundaries and a novel overdecomposition scheme that enables overlap of communication with computation. When training an 80-billion parameter GPT on 1024 GPUs of Perlmutter, Tensor3D achieves a 26% improvement over Megatron-LM and attains 57% of theoretical peak FLOP/s (182 PFLOP/s total). For U-Net CNNs with 3.5B-28B parameters on 32-256 GPUs, Tensor3D improves training time by 18-61% compared to Megatron-LM and reduces communication volume by up to 80%.

## Method Summary
Tensor3D implements a hybrid 3D tensor and data parallelism scheme that decomposes the neural network across GPUs in three dimensions: data parallelism groups (Gdata), row-wise tensor parallelism (Gr), and column-wise tensor parallelism (Gc). The framework introduces intelligent parameter distribution that transposes weight matrices to eliminate communication at layer boundaries, and an overdecomposition scheme that splits each GPU's batch shard into two parts to enable overlap of communication with computation. A communication model analytically determines optimal Gr, Gc, and Gdata values based on neural network architecture parameters to minimize total communication volume. The framework was implemented and evaluated on Perlmutter and Polaris supercomputers using A100 GPUs, comparing performance against Megatron-LM and Colossal-AI-3D baselines.

## Key Results
- Achieves 26% improvement over Megatron-LM when training 80B parameter GPT on 1024 GPUs
- Attains 57% of theoretical peak FLOP/s (182 PFLOP/s total) on Perlmutter
- Improves U-Net training time by 18-61% and reduces communication volume by up to 80% compared to Megatron-LM
- Achieves GPU utilization of 29.95% of peak for the largest U-Net model (28B parameters)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intelligent parameter distribution eliminates communication at layer boundaries
- Mechanism: By transposing the weight matrices for alternating layers instead of the input tensors, the data layout stays consistent across layers, avoiding costly all-reduce operations at every layer boundary
- Core assumption: The computational cost of transposing parameters once at training start is less than the cumulative communication cost of repeated all-reduces
- Evidence anchors:
  - [abstract] "eliminates communication required for satisfying data dependencies of individual layers"
  - [section 4.1] "To alleviate this problem we propose a simple solution that involves transposing the parameter matrix of the second layer (say W′) instead of the input"

### Mechanism 2
- Claim: Overdecomposition enables overlap of communication with computation
- Mechanism: By splitting each GPU's batch shard into two parts and scheduling their computation and communication in a round-robin fashion, communication of one shard overlaps with computation of the other
- Core assumption: The computational time for one batch shard exceeds the communication time for the other, making overlap beneficial
- Evidence anchors:
  - [abstract] "proposed a novel overdecomposition scheme for tensor parallelism which makes it possible to achieve overlap of computation with communication"
  - [section 4.2] "With this overdecomposition scheme, a GPU can overlap the computation of X′ with the communication of X′′, and vice versa"

### Mechanism 3
- Claim: Communication-optimal GPU decomposition minimizes total communication volume
- Mechanism: The communication model analytically determines optimal Gr, Gc, and Gdata values based on neural network architecture parameters (k, n dimensions), minimizing the communication volume function V = 2B/G(n × (Gr - 1) + k × (Gc - 1))
- Core assumption: The analytical model accurately captures the communication characteristics of the specific neural network architecture
- Evidence anchors:
  - [abstract] "develop an analytical model to identify high-performing configurations"
  - [section 5] "Our communication model assumes that the underlying all-reduce algorithm is able to achieve this lower bound"

## Foundational Learning

- Concept: 3D tensor and data parallelism
  - Why needed here: Understanding how to decompose a neural network across GPUs in three dimensions (data parallelism, row-wise tensor parallelism, column-wise tensor parallelism) is fundamental to Tensor3D's approach
  - Quick check question: If you have 128 GPUs and want to train a transformer with hidden size 8192, what would be the optimal decomposition of GPUs into data parallel groups, row groups, and column groups according to the paper's model?

- Concept: All-reduce operations and their communication costs
  - Why needed here: Tensor3D's performance hinges on minimizing all-reduce operations and their communication volume, which scales with the number of participating GPUs and data size
  - Quick check question: For an all-reduce operation with 16 GPUs and a buffer size of 256MB per GPU, what is the minimum communication volume per GPU according to the paper's communication model?

- Concept: Overdecomposition and stream-based execution
  - Why needed here: The asynchronous communication backend relies on splitting work into multiple streams to enable overlap between computation and communication
  - Quick check question: In CUDA programming, why can't computation and communication overlap if they're scheduled on the same stream?

## Architecture Onboarding

- Component map: Data parallelism layer (Gdata groups) -> Tensor parallelism layer (Gr × Gc grid within each group) -> Communication overlap mechanism (two streams per GPU) -> Analytical model for configuration optimization

- Critical path: Forward pass -> All-reduce column-wise -> Backward pass -> All-reduce row-wise -> Parameter update (data parallel all-reduce) -> Next iteration

- Design tradeoffs: The framework trades parameter memory (storing transposed weights) for communication bandwidth savings; it also requires careful configuration of parallelism dimensions to minimize communication

- Failure signatures: Poor scaling when Gr or Gc is too large (communication dominates); suboptimal performance when Gdata is too small (insufficient parallelism); runtime errors when batch splitting causes memory issues

- First 3 experiments:
  1. Run a small transformer (e.g., 100M parameters) on 4 GPUs with varying (Gr, Gc, Gdata) configurations to validate the communication model predictions
  2. Compare time per iteration with and without the asynchronous communication backend on a medium-sized model (e.g., 1B parameters) to measure overlap benefits
  3. Profile communication volume for different layer types (FC vs convolution) to validate the analytical model for non-transformer architectures

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored based on the results presented. The framework's performance under different network topologies and GPU interconnect technologies is not investigated. The scalability beyond 256 GPUs is not discussed, nor is the communication model's accuracy across a wider range of neural network architectures. The paper also does not address potential bottlenecks or limitations of the current implementation for extreme-scale training.

## Limitations
- Scalability claims beyond 1024 GPUs remain unverified as results only extend to this scale
- Communication model assumes all-reduce algorithms achieve lower bounds that may not hold in real-world implementations
- Performance heavily dependent on optimal configuration of three parallelism dimensions requiring accurate network and model characterization

## Confidence

**High Confidence**: The fundamental mechanisms of intelligent parameter distribution and overdecomposition are well-grounded in established parallel computing principles. The 26% improvement over Megatron-LM on GPT training and 18-61% improvement for U-Net models are supported by specific experimental results with clear methodology.

**Medium Confidence**: The analytical communication model provides useful guidance but may have limited accuracy when network conditions deviate from ideal assumptions. The reported GPU utilization of 29.95% for the largest U-Net model, while impressive, may not be achievable in all deployment scenarios due to varying hardware characteristics and network conditions.

## Next Checks

1. **Scalability Validation**: Test the framework on larger GPU counts (2048+) to verify whether the communication model's predictions and performance improvements scale linearly or if new bottlenecks emerge at extreme scales.

2. **Network Condition Robustness**: Evaluate the framework's performance under varying network conditions, including network congestion, packet loss, and heterogeneous network topologies, to assess the communication model's accuracy and the framework's robustness.

3. **Configuration Sensitivity Analysis**: Conduct a systematic study of how sensitive the framework's performance is to variations in the three parallelism dimensions (Gr, Gc, Gdata) for different model architectures, identifying the stability region of optimal configurations.