---
ver: rpa2
title: 'FakeGPT: Fake News Generation, Explanation and Detection of Large Language
  Models'
arxiv_id: '2310.05046'
source_url: https://arxiv.org/abs/2310.05046
tags:
- news
- fake
- chatgpt
- prompt
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ChatGPT can generate, explain, and detect fake news effectively.
  We explored four prompting methods for generation and found the samples to be of
  high quality.
---

# FakeGPT: Fake News Generation, Explanation and Detection of Large Language Models

## Quick Facts
- **arXiv ID:** 2310.05046
- **Source URL:** https://arxiv.org/abs/2310.05046
- **Reference count:** 40
- **Primary result:** ChatGPT can generate, explain, and detect fake news effectively, achieving over 70% accuracy on 8 out of 11 datasets with reason-aware prompts.

## Executive Summary
This study explores ChatGPT's capabilities in generating, explaining, and detecting fake news. The researchers developed four prompt methods to generate fake news samples, which were evaluated as high quality through both self-assessment and human evaluation. By analyzing ChatGPT's explanations of fake news samples, nine key features characterizing fake news were identified. A reason-aware prompt method was proposed to enhance ChatGPT's detection capability, significantly improving performance across multiple datasets.

## Method Summary
The study employed a multi-phase approach: first, four prompt engineering methods were used to generate fake news samples from ChatGPT. Second, ChatGPT was asked to explain why given samples were fake news, from which nine features were extracted and analyzed across multiple datasets. Third, a reason-aware prompt incorporating these features was developed to enhance ChatGPT's detection capability. The detection performance was evaluated across 11 different datasets using custom metrics (Acc-1, Acc-2, Acc-3) designed to address ChatGPT's consistency issues.

## Key Results
- ChatGPT generated fake news of high quality using four prompting methods, with self-assessment achieving 72.5% accuracy and human evaluation achieving 54.8% accuracy
- Nine features were identified that characterize fake news based on ChatGPT's explanations, with "lack of evidence or credible sources" being most prevalent
- Reason-aware prompts improved ChatGPT's detection performance, achieving over 70% accuracy on 8 out of 11 datasets
- The maximum improvement in detection accuracy was 19.7% on the KAGGLE dataset

## Why This Works (Mechanism)

### Mechanism 1
ChatGPT can generate fake news of high quality using specific prompting strategies. Four prompt methods (altering text meaning, inventing stories, creating imaginary text, multiple prompts) enable ChatGPT to bypass its safety filters and generate fake news that closely resembles real-world news. The core assumption is that ChatGPT's generation capabilities are strong enough that, with the right prompts, it can produce text indistinguishable from authentic news to both human evaluators and self-assessment.

### Mechanism 2
ChatGPT can identify nine features that characterize fake news through explanation analysis. By asking ChatGPT to explain why given samples are fake news, researchers can extract consistent patterns and features that define fake news across multiple datasets. The core assumption is that ChatGPT's understanding of fake news characteristics is consistent enough across different datasets to identify common explanatory patterns.

### Mechanism 3
A reason-aware prompt method can enhance ChatGPT's fake news detection capability by 70%+ accuracy on most datasets. Incorporating summarized features of fake news into the prompt template subconsciously guides ChatGPT to increase its inclination toward predicting samples as fake news, improving detection performance. The core assumption is that ChatGPT's detection performance can be improved by providing it with explicit knowledge about the distinguishing characteristics of fake news.

## Foundational Learning

- **Concept:** Prompt engineering for large language models
  - **Why needed here:** The study relies heavily on different prompting strategies to generate, explain, and detect fake news, making understanding prompt engineering crucial for replicating and extending the work.
  - **Quick check question:** How do different prompt structures (single vs. multiple prompts, content guidance vs. open-ended) affect ChatGPT's output in terms of safety filter bypassing and content quality?

- **Concept:** Fake news characterization and detection metrics
  - **Why needed here:** The study introduces novel metrics (Acc-1, Acc-2, Acc-3) and analyzes fake news features, requiring understanding of both traditional detection approaches and the new methodology presented.
  - **Quick check question:** What are the key differences between Acc-1, Acc-2, and Acc-3 metrics, and how do they address the challenges of evaluating ChatGPT's detection performance?

- **Concept:** Large language model limitations and consistency
  - **Why needed here:** The study identifies ChatGPT's inconsistency in detection results and explores ways to mitigate this, making understanding LLM limitations essential for interpreting results.
  - **Quick check question:** Why does ChatGPT exhibit inconsistency in fake news detection across multiple tests, and what factors contribute to this variability?

## Architecture Onboarding

- **Component map:** Prompt generation module → Explanation extraction module → Detection evaluation module → Additional information analysis module
- **Critical path:** Prompt → Generation/Explanation/Detection → Evaluation → Analysis → Improvement
  The core workflow involves generating appropriate prompts, obtaining responses from ChatGPT, evaluating those responses, and iteratively improving the approach based on analysis.
- **Design tradeoffs:**
  - Single prompt vs. multiple prompts: Simpler but less effective vs. more complex but better at bypassing filters
  - Original prompt vs. reason-aware prompt: No performance improvement vs. significant improvement but requires feature extraction
  - 2-class vs. 3-class detection: Simpler evaluation vs. more nuanced but introduces "unclear" category challenges
- **Failure signatures:**
  - High inconsistency rates (>30%) across multiple test runs indicate reliability issues
  - Low accuracy on specific datasets suggests dataset-specific challenges or prompt ineffectiveness
  - High proportion of "unclear" predictions indicates insufficient information or ambiguous samples
- **First 3 experiments:**
  1. Test all four prompt methods on a small sample of fake news generation to compare quality and safety filter bypassing effectiveness
  2. Apply original prompt detection on one dataset and measure consistency across 10 test runs to establish baseline performance
  3. Implement reason-aware prompt on the same dataset and compare accuracy improvements against the baseline

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does ChatGPT's fake news generation quality compare across the four prompting methods in terms of believability and factual consistency?
- **Basis in paper:** The paper explores four prompting methods for generating fake news using ChatGPT and evaluates their quality through self-assessment and human evaluation.
- **Why unresolved:** While the paper mentions that the generated samples are of high quality, it does not provide a detailed comparison of the four methods in terms of believability and factual consistency.
- **What evidence would resolve it:** A detailed analysis of the generated fake news samples from each method, assessing their believability and factual consistency using human evaluation or fact-checking tools.

### Open Question 2
- **Question:** What is the impact of incorporating external knowledge, such as a knowledge graph or knowledge base, on ChatGPT's fake news detection performance?
- **Basis in paper:** The paper mentions that ChatGPT lacks external knowledge to accurately assess news authenticity and suggests that incorporating additional information could enhance its detection capabilities.
- **Why unresolved:** The paper does not provide experimental results on how incorporating external knowledge affects ChatGPT's fake news detection performance.
- **What evidence would resolve it:** Experimental results comparing ChatGPT's fake news detection performance with and without the incorporation of external knowledge, using a diverse set of datasets and evaluation metrics.

### Open Question 3
- **Question:** How does ChatGPT's fake news detection consistency vary across different types of news (e.g., political, health, entertainment) and topics?
- **Basis in paper:** The paper evaluates ChatGPT's consistency in detecting fake news across nine datasets but does not provide a detailed analysis of consistency across different types of news or topics.
- **Why unresolved:** The paper does not provide a breakdown of ChatGPT's detection consistency for different types of news or topics, which could reveal potential biases or weaknesses in its detection capabilities.
- **What evidence would resolve it:** An analysis of ChatGPT's detection consistency for different types of news and topics, using a diverse set of datasets and evaluating consistency using metrics such as accuracy and F1 score.

## Limitations

- The generation methods rely on prompt engineering to bypass safety filters, which may not be sustainable as OpenAI updates its moderation systems
- The feature extraction from explanations is qualitative and may not capture all nuances of fake news characteristics
- Human evaluation of generated fake news quality (54.8% accuracy) indicates that even humans struggle to distinguish ChatGPT-generated fake news from real news

## Confidence

- **High Confidence:** ChatGPT can generate fake news samples of high quality using prompt engineering techniques
- **Medium Confidence:** The nine features extracted from ChatGPT's explanations accurately characterize fake news across multiple datasets
- **Medium Confidence:** Reason-aware prompts improve ChatGPT's detection performance to over 70% accuracy on most datasets

## Next Checks

1. **Consistency Validation:** Conduct 20+ test runs on the same dataset to quantify the variability in ChatGPT's detection performance and determine if the observed inconsistency is systematic or random
2. **Cross-LLM Comparison:** Test the same prompt engineering methods with other large language models (e.g., Claude, Llama) to determine if the results are specific to ChatGPT or represent a broader LLM capability
3. **Longitudinal Safety Filter Test:** Replicate the generation experiments after 3-6 months to assess whether OpenAI's safety filter updates have made the prompt engineering approaches ineffective, testing the sustainability of the generation methods