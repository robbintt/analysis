---
ver: rpa2
title: Efficiently Adapting Pretrained Language Models To New Languages
arxiv_id: '2311.05741'
source_url: https://arxiv.org/abs/2311.05741
tags:
- language
- data
- training
- tokenizer
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of adapting large language models
  to low-resource languages, where models trained on English-dominated data perform
  poorly. The authors propose improving tokenizer efficiency by replacing infrequent
  tokens with new language tokens and mitigating catastrophic forgetting by mixing
  training data from both the original and target languages.
---

# Efficiently Adapting Pretrained Language Models To New Languages
## Quick Facts
- arXiv ID: 2311.05741
- Source URL: https://arxiv.org/abs/2311.05741
- Reference count: 40
- Primary result: Proposes a method to adapt English LLMs to low-resource languages by improving tokenizer efficiency and mitigating catastrophic forgetting, achieving better performance than open-source models on target languages

## Executive Summary
This paper addresses the challenge of adapting large language models to low-resource languages where English-dominated training data leads to poor performance. The authors propose a two-pronged approach: improving tokenizer efficiency by replacing infrequent tokens with new language tokens, and mitigating catastrophic forgetting by mixing training data from both languages. Their experiments on adapting English models to Hungarian and Thai demonstrate significant improvements in tokenization efficiency and model performance on target languages while retaining English capabilities, outperforming state-of-the-art open-source models on the target languages.

## Method Summary
The method involves replacing the least frequent tokens in a base English tokenizer with tokens from the target language to improve encoding efficiency, while maintaining the same vocabulary size by reinitializing the corresponding embeddings. To prevent catastrophic forgetting of the original language, training data from both languages is mixed at the sample level, ensuring every batch contains text from both languages. The adapted model is then trained using a two-stage pipeline: adaptive pretraining followed by instruction tuning on the mixed data.

## Key Results
- Replacing infrequent tokens in the base tokenizer with target language tokens significantly improves tokenizer efficiency
- Mixing training data from both languages effectively mitigates catastrophic forgetting
- Adapted models outperform state-of-the-art open-source models on target languages (Hungarian and Thai) while maintaining minimal regression on English benchmarks

## Why This Works (Mechanism)
### Mechanism 1: Tokenizer Efficiency
Replacing infrequent tokens in the base tokenizer with target language tokens improves tokenizer efficiency and model performance on the target language. The original tokenizer has poor fertility (average tokens per word) for the target language. By replacing the least frequent tokens with new tokens from the target language, the tokenizer can encode the target language more efficiently. This assumes the least frequent tokens in the base tokenizer are the least important for encoding the target language.

### Mechanism 2: Catastrophic Forgetting Mitigation
Mixing training data from both the original and target languages mitigates catastrophic forgetting. When training a model on a new language, it can forget the knowledge it learned from the original language. By mixing training data from both languages, the model continues to see examples from the original language during training, helping it retain that knowledge. This assumes that mixing the training data at the sample level is sufficient to mitigate catastrophic forgetting.

### Mechanism 3: Performance Superiority
The proposed recipe can reach better performance than open source models on the target language, with minimal regressions on English. By improving the tokenizer efficiency and mitigating catastrophic forgetting, the adapted model can learn the target language more effectively while retaining its English capabilities. This leads to better performance on the target language compared to open source models, with minimal loss of English performance. This assumes the improvements from the tokenizer and data mixing are sufficient to outperform open source models.

## Foundational Learning
- **Catastrophic forgetting**: Why needed - to understand why mixing training data from both languages is necessary. Quick check - What happens when a model is trained on a new task without seeing any examples from the original task?
- **Byte Pair Encoding (BPE)**: Why needed - to understand how the tokenizer works and how replacing tokens can improve efficiency. Quick check - How does BPE tokenization work, and why is it commonly used in LLMs?
- **Fertility**: Why needed - to understand how replacing tokens can improve tokenizer efficiency. Quick check - What is fertility in the context of tokenization, and how is it calculated?

## Architecture Onboarding
- **Component map**: Base model (English-centric LLM) -> Tokenizer (BPE tokenizer with 50k vocabulary) -> Training data (English and target language data) -> Training stages (Pretraining and instruction tuning)
- **Critical path**: 1) Replace infrequent tokens in base tokenizer with target language tokens, 2) Mix training data from both languages, 3) Pretrain adapted model on mixed data, 4) Instruction tune adapted model on mixed instruction data, 5) Evaluate on target and original language benchmarks
- **Design tradeoffs**: Replacing more tokens improves target language efficiency but increases risk of forgetting original language; mixing more target language data improves target language performance but increases risk of forgetting original language; larger vocabulary size improves target language efficiency but increases model size and training time
- **Failure signatures**: Poor target language performance (Tokenizer not efficiently encoding target language), poor original language performance (Catastrophic forgetting occurring), slow training/inference (Tokenizer not efficiently encoding either language)
- **First 3 experiments**: 1) Replace 5k tokens in base tokenizer with Hungarian tokens, pretrain and evaluate on Hungarian and English benchmarks, 2) Replace 5k tokens in base tokenizer with Thai tokens, pretrain and evaluate on Thai and English benchmarks, 3) Mix 50% English and 50% target language data during pretraining and instruction tuning, evaluate on both languages

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What is the optimal tokenizer replacement strategy for balancing fertility improvements with model adaptation difficulty?
- Basis in paper: The paper discusses replacing least frequent tokens with new language tokens but notes this increases adaptation difficulty due to more randomly re-initialized embeddings.
- Why unresolved: The paper chose 10% replacement based on fertility gains but didn't explore other replacement strategies or thresholds that might better balance efficiency and adaptation.
- What evidence would resolve it: Comparative experiments testing different replacement percentages (5%, 15%, 20%) and strategies (random vs frequency-based) measuring both fertility gains and model performance.

### Open Question 2
- Question: How does the optimal data mixing ratio vary across different language pairs and model sizes?
- Basis in paper: The paper found 25-50% English data works well for Hungarian/Thai but didn't test other language pairs or model sizes.
- Why unresolved: The optimal mixing ratio likely depends on language similarity, data availability, and model capacity - factors not explored in the study.
- What evidence would resolve it: Systematic experiments varying mixing ratios across diverse language pairs (e.g., Spanish/English, Japanese/English) and model sizes (1B, 7B, 13B parameters).

### Open Question 3
- Question: What is the minimum instruction tuning data required from a new language to achieve acceptable performance?
- Basis in paper: The paper shows 1% Hungarian IT data provides most benefits but didn't explore lower bounds or other languages.
- Why unresolved: The 1% threshold may be specific to Hungarian or the particular tasks used - different languages or tasks might require different amounts.
- What evidence would resolve it: Experiments testing progressively smaller percentages of new language IT data across multiple languages and task types, measuring performance degradation.

## Limitations
- The paper does not fully specify how tokens are selected for replacement or how their embeddings are handled
- The exact mixing ratios and their impact on catastrophic forgetting are not thoroughly analyzed
- Performance comparisons are limited to two low-resource languages and unspecified open-source models

## Confidence
**Tokenizer Efficiency Claims (High Confidence)**: The mechanism of replacing infrequent tokens with target language tokens to improve fertility is technically sound and well-supported by the literature on tokenization efficiency.

**Catastrophic Forgetting Claims (Medium Confidence)**: The data mixing approach is a standard technique with theoretical support, but the paper lacks rigorous ablation studies to confirm its effectiveness compared to other mitigation strategies.

**Performance Claims (Medium Confidence)**: The reported improvements in target language performance with minimal English regression are supported by experimental results, but the comparison to open-source models lacks transparency about specific baselines used.

## Next Checks
1. **Tokenizer Replacement Ablation**: Systematically vary the number of tokens replaced (e.g., 2k, 5k, 10k) and measure the impact on tokenization efficiency and model performance to determine optimal replacement strategy.

2. **Data Mixing Ratio Sensitivity**: Conduct controlled experiments with different mixing ratios during pretraining (e.g., 30-70, 50-50, 70-30) and instruction tuning to quantify the relationship between data composition and catastrophic forgetting.

3. **Cross-Lingual Transfer Analysis**: Test the adapted models on additional language pairs beyond Hungarian and Thai to evaluate the generalizability of the approach and identify any language-specific limitations in the tokenization or training methodology.