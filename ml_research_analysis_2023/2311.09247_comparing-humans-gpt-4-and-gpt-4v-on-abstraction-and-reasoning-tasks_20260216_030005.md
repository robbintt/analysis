---
ver: rpa2
title: Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks
arxiv_id: '2311.09247'
source_url: https://arxiv.org/abs/2311.09247
tags:
- tasks
- gpt-4
- reasoning
- gpt-4v
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares the abstract reasoning capabilities of text-only
  and multimodal versions of GPT-4 on the ConceptARC benchmark, which evaluates understanding
  and reasoning with core-knowledge concepts. The authors extend previous work by
  evaluating GPT-4 on one-shot prompting with text versions of tasks and evaluating
  GPT-4V, the multimodal version, on image versions of tasks.
---

# Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks

## Quick Facts
- arXiv ID: 2311.09247
- Source URL: https://arxiv.org/abs/2311.09247
- Authors: 
- Reference count: 26
- Primary result: GPT-4V performs worse than text-only GPT-4 on visual reasoning tasks despite being multimodal

## Executive Summary
This paper compares the abstract reasoning capabilities of text-only GPT-4 and multimodal GPT-4V on the ConceptARC benchmark, which evaluates understanding of core-knowledge concepts through visual grid transformations. The study finds that GPT-4's accuracy improves from 25% to 33% with detailed one-shot prompting but remains far below human performance of 91%. Surprisingly, GPT-4V performs worse (23-25%) than text-only GPT-4 (65-69%) on visual tasks, suggesting it cannot effectively translate visual grids into text representations needed for reasoning. These results indicate that neither model has developed human-like abstraction abilities despite their scale.

## Method Summary
The study evaluates GPT-4 and GPT-4V on 480 ConceptARC tasks using zero-shot and one-shot prompting with temperature settings of 0 and 0.5. Text-only GPT-4 receives encoded task descriptions, while GPT-4V processes image versions of the tasks. The authors compare performance against human baselines (~91% accuracy) and analyze differences between minimal (48 tasks) and non-minimal task sets. One-shot prompting includes detailed instructions and worked examples to improve model performance.

## Key Results
- GPT-4 accuracy improved from 25% to 33% with one-shot prompting but remained below human performance (91%)
- GPT-4V scored only 23-25% on visual tasks, substantially worse than text-only GPT-4's 65-69%
- Neither model achieved human-level abstraction abilities on core-knowledge reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal LLMs fail to effectively translate visual grid representations into text representations needed for abstract reasoning.
- Mechanism: GPT-4V receives image input but cannot reliably convert the visual grid (colors, positions) into the text encoding format that would allow it to leverage its learned reasoning patterns from text-only training.
- Core assumption: GPT-4V's visual processing does not integrate seamlessly with its text-based reasoning architecture for grid-based abstract reasoning tasks.
- Evidence anchors:
  - [abstract] "The multimodal model's performance suggests it does not effectively translate visual grids to text representations or leverage visual information for abstract reasoning."
  - [section] "When presented with an image, GPT-4V was unable to consistently translate the visual grid to a text representation, including both color names and numeric encodings."
- Break condition: If GPT-4V could be prompted to generate intermediate text representations or if the model were trained with more visual-grid reasoning examples.

### Mechanism 2
- Claim: Text-only LLMs perform better on visual reasoning tasks when provided with more detailed, structured prompts that include solved examples.
- Mechanism: The one-shot prompt format provides explicit instructions and a worked example, helping the model understand the task structure and reducing ambiguity in how to process the demonstration grids.
- Core assumption: The model can leverage the example to generalize the transformation pattern, even though it only sees the task in text form.
- Evidence anchors:
  - [abstract] "Using a more detailed, one-shot prompt, GPT-4's accuracy on text versions of tasks improved from 25% to 33%"
  - [section] "our more detailed, one-shot prompting method resulted in a higher accuracy overall, 0.33 for both temperature settings, than the simple zero-shot method used in [10], which reported 0.19 for temperature zero and 0.25 for temperature 0.5"
- Break condition: If the model relied heavily on pattern matching rather than true abstraction, it might fail on novel task types not covered by the example.

### Mechanism 3
- Claim: The gap between human and LLM performance on ConceptARC tasks indicates that current models lack robust abstraction abilities despite their training scale.
- Mechanism: Humans use innate core knowledge systems (objectness, numerosity, basic geometry) to flexibly reason about novel grid transformations, while LLMs rely on learned associations that don't generalize as well to unseen concepts.
- Core assumption: The ConceptARC benchmark successfully isolates abstract reasoning from language-based pattern matching by using visual grids and core spatial concepts.
- Evidence anchors:
  - [abstract] "The results reinforce the conclusion that neither version of GPT-4 has developed robust abstraction abilities at humanlike levels."
  - [section] "Our experimental results support the conclusion that neither version of GPT-4 has developed robust abstraction abilities at humanlike levels."
  - [corpus] Weak - the corpus provides related work but doesn't directly test this mechanism.
- Break condition: If future models showed significantly better generalization across ConceptARC concept groups, suggesting they had developed more robust abstraction mechanisms.

## Foundational Learning

- Concept: Zero-shot vs one-shot prompting
  - Why needed here: The paper directly compares performance improvements when moving from simple zero-shot prompts to more detailed one-shot prompts with examples.
  - Quick check question: What is the key difference between zero-shot and one-shot prompting, and why might one-shot improve performance on abstract reasoning tasks?

- Concept: Multimodal vs unimodal model evaluation
  - Why needed here: The paper evaluates both text-only GPT-4 and multimodal GPT-4V to test whether visual input improves performance on visual reasoning tasks.
  - Quick check question: Why is it important to compare human performance (visual input) with multimodal LLMs rather than just text-only models when evaluating visual reasoning tasks?

- Concept: Abstraction and core knowledge systems
  - Why needed here: The ConceptARC benchmark is designed to test understanding of core-knowledge concepts like objectness, numerosity, and basic geometry that are hypothesized to be innate in humans.
  - Quick check question: What are core knowledge systems, and why does the ConceptARC benchmark focus on them rather than language-based reasoning?

## Architecture Onboarding

- Component map: Image/text input → Prompt formatting → Model inference → Output parsing → Accuracy calculation
- Critical path: Input → Prompt formatting → Model inference → Output parsing → Accuracy calculation
- Design tradeoffs:
  - Text vs image input: Text is more reliable for GPT-4 but less natural for visual tasks; images are more natural but GPT-4V struggles with conversion
  - Prompt detail level: More detailed prompts improve performance but may introduce biases or overfitting to examples
  - Temperature settings: Higher temperature increases creativity but reduces consistency
- Failure signatures:
  - Text-only model: Struggles with visual reasoning, may misinterpret grid encodings
  - Multimodal model: Fails to translate images to text representations, may include irrelevant details from examples
  - Both models: Performance significantly below human baseline, especially on novel concept instantiations
- First 3 experiments:
  1. Test text-only GPT-4 with zero-shot, one-shot, and few-shot prompts on a subset of ConceptARC tasks to isolate prompting effect
  2. Test GPT-4V with minimal tasks using different image layouts (single image vs multiple images) to find optimal visual presentation
  3. Compare human performance on text vs image versions of ConceptARC tasks to validate fairness of multimodal comparison

## Open Questions the Paper Calls Out
- Would different prompting methods or task representations significantly improve GPT-4 and GPT-4V's performance on ConceptARC tasks?
- Why does GPT-4V perform worse than the text-only GPT-4 on ConceptARC tasks, despite being a multimodal model?
- How would GPT-4V perform on more complex ConceptARC tasks beyond the minimal tasks tested in the paper?

## Limitations
- The exact prompt formatting details remain unspecified, which could significantly impact model performance
- The image preprocessing pipeline for GPT-4V input is not fully described, making it difficult to assess whether the multimodal model's poor performance stems from architectural limitations or suboptimal visual input preparation
- The evaluation focuses only on minimal tasks (48 out of 480 total), potentially limiting generalizability to the full ConceptARC benchmark

## Confidence
- Medium confidence: GPT-4V underperforms GPT-4 on text versions of visual reasoning tasks (robust across multiple prompt variations but depends on unverified image processing pipeline)
- High confidence: Both models perform substantially below human levels (consistent 60-70% gap across experimental conditions)
- Medium confidence: GPT-4V's inability to translate visual grids to text representations (plausible mechanism but lack of direct evidence)

## Next Checks
1. Test GPT-4V's ability to accurately transcribe individual grid cells from images to text, isolating the translation problem from full task reasoning
2. Compare model performance across the full 480-task ConceptARC benchmark to assess whether minimal task results generalize
3. Implement and test alternative image encoding schemes for GPT-4V (e.g., different aspect ratios, preprocessing methods) to determine if the visual input format contributes to poor performance