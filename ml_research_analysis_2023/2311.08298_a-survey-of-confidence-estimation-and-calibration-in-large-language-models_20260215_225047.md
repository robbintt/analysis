---
ver: rpa2
title: A Survey of Confidence Estimation and Calibration in Large Language Models
arxiv_id: '2311.08298'
source_url: https://arxiv.org/abs/2311.08298
tags:
- language
- confidence
- calibration
- computational
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive survey of confidence estimation
  and calibration techniques for large language models (LLMs). The authors categorize
  confidence estimation methods into white-box and black-box approaches for generative
  LLMs, and logit-based, ensemble-based, Bayesian, density-based, and confidence learning
  methods for discriminative LLMs.
---

# A Survey of Confidence Estimation and Calibration in Large Language Models

## Quick Facts
- arXiv ID: 2311.08298
- Source URL: https://arxiv.org/abs/2311.08298
- Reference count: 36
- Key outcome: This paper presents a comprehensive survey of confidence estimation and calibration techniques for large language models (LLMs).

## Executive Summary
This paper provides a comprehensive survey of confidence estimation and calibration techniques for large language models (LLMs). The authors categorize confidence estimation methods into white-box and black-box approaches for generative LLMs, and logit-based, ensemble-based, Bayesian, density-based, and confidence learning methods for discriminative LLMs. They also summarize calibration methods for both types of LLMs, including in-training and post-hoc approaches. The paper highlights the unique challenges in confidence estimation and calibration for generative LLMs, such as the exponential growth of output space and the need for varied confidence granularity across tasks. The authors suggest future research directions, including hallucination detection, learning to explain and clarify, multi-modal LLMs, and calibration on human variation.

## Method Summary
The paper provides a comprehensive survey of confidence estimation and calibration techniques for large language models (LLMs). It categorizes confidence estimation methods into white-box and black-box approaches for generative LLMs, and logit-based, ensemble-based, Bayesian, density-based, and confidence learning methods for discriminative LLMs. The paper also summarizes calibration methods for both types of LLMs, including in-training and post-hoc approaches. While specific datasets are not provided, the paper mentions various tasks like nature language inference, paraphrase detection, commonsense reasoning, extractive question answering, machine translation, text summarization, and dialogue generation. The paper discusses various calibration measures like Expected Calibration Error (ECE), Brier Score, and reliability diagrams, emphasizing the importance of calibration for mitigating risks and enabling LLMs to produce better generations.

## Key Results
- Categorizes confidence estimation methods into white-box and black-box approaches for generative LLMs, and logit-based, ensemble-based, Bayesian, density-based, and confidence learning methods for discriminative LLMs.
- Highlights unique challenges in confidence estimation and calibration for generative LLMs, such as exponential output space growth and varied confidence granularity across tasks.
- Suggests future research directions, including hallucination detection, learning to explain and clarify, multi-modal LLMs, and calibration on human variation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The survey categorizes confidence estimation methods into white-box and black-box approaches for generative LLMs, and logit-based, ensemble-based, Bayesian, density-based, and confidence learning methods for discriminative LLMs.
- Mechanism: By organizing methods into these categories, the survey provides a structured framework for understanding the different approaches to confidence estimation, making it easier for researchers to identify relevant techniques for their specific use case.
- Core assumption: The categorization accurately reflects the fundamental differences between the various confidence estimation methods and their applicability to different types of LLMs.
- Evidence anchors:
  - [abstract]: "The authors categorize confidence estimation methods into white-box and black-box approaches for generative LLMs, and logit-based, ensemble-based, Bayesian, density-based, and confidence learning methods for discriminative LLMs."
  - [section]: "Generally, methods for estimating uncertainty in generative LMs are divided into white-box and black-box approaches. This categorization is also driven by the increasing prevalence of closed-source language models."
- Break condition: If new methods emerge that do not fit neatly into the existing categories, or if the distinction between white-box and black-box approaches becomes less relevant as more LLMs become open-source.

### Mechanism 2
- Claim: The survey highlights the unique challenges in confidence estimation and calibration for generative LLMs, such as the exponential growth of output space and the need for varied confidence granularity across tasks.
- Mechanism: By identifying these challenges, the survey draws attention to the specific difficulties faced when working with generative LLMs, encouraging researchers to develop targeted solutions and adapt existing techniques accordingly.
- Core assumption: The challenges identified are indeed unique to generative LLMs and significantly impact the effectiveness of confidence estimation and calibration methods.
- Evidence anchors:
  - [abstract]: "The paper highlights the unique challenges in confidence estimation and calibration for generative LLMs, such as the exponential growth of output space and the need for varied confidence granularity across tasks."
  - [section]: "In contrast to discriminative models that easily yield probability scores for specific categories, generative LMs face an exponential expansion of their output space with increasing sentence length."
- Break condition: If future research demonstrates that these challenges are not as significant as currently believed, or if new techniques emerge that effectively address these challenges without requiring significant adaptation.

### Mechanism 3
- Claim: The survey suggests future research directions, including hallucination detection, learning to explain and clarify, multi-modal LLMs, and calibration on human variation.
- Mechanism: By proposing these research directions, the survey provides a roadmap for future work in the field, guiding researchers towards areas that are likely to yield significant advancements in confidence estimation and calibration for LLMs.
- Core assumption: The suggested research directions are indeed promising and have the potential to address current limitations in the field.
- Evidence anchors:
  - [abstract]: "The authors suggest future research directions, including hallucination detection, learning to explain and clarify, multi-modal LLMs, and calibration on human variation."
  - [section]: "Based on our summary of the trends in research on uncertainty in existing language models, we cautiously propose several research directions in this field:"
- Break condition: If subsequent research fails to make significant progress in the suggested areas, or if new challenges and opportunities emerge that are not captured by the proposed directions.

## Foundational Learning

- Concept: Confidence and uncertainty in machine learning
  - Why needed here: Understanding the concepts of confidence and uncertainty is crucial for comprehending the importance and methods of confidence estimation and calibration in LLMs.
  - Quick check question: What is the difference between aleatoric and epistemic uncertainty, and how do they relate to a model's confidence?
- Concept: Calibration in machine learning
  - Why needed here: Calibration is a key aspect of confidence estimation, ensuring that a model's predicted probabilities align with the actual probabilities of the events it predicts.
  - Quick check question: What is the expected calibration error (ECE), and how is it calculated?
- Concept: Generative vs. discriminative models
  - Why needed here: Understanding the differences between generative and discriminative models is essential for appreciating the unique challenges and approaches in confidence estimation and calibration for LLMs.
  - Quick check question: What are the main differences between generative and discriminative models, and how do these differences impact confidence estimation?

## Architecture Onboarding

- Component map: The survey provides a comprehensive overview of the field, including:
  - Confidence estimation methods for discriminative and generative LLMs
  - Calibration techniques for both types of models
  - Challenges and future research directions
- Critical path: To effectively use the survey, a new engineer should:
  1. Understand the basic concepts of confidence, uncertainty, and calibration
  2. Familiarize themselves with the different categories of confidence estimation methods
  3. Identify the challenges specific to generative LLMs
  4. Explore the suggested future research directions
- Design tradeoffs: The survey focuses on providing a broad overview rather than in-depth technical details, which may require additional resources for a complete understanding of specific methods.
- Failure signatures: If a new engineer struggles to understand the survey's organization or fails to grasp the unique challenges of generative LLMs, they may need to review the foundational concepts or seek additional resources.
- First 3 experiments:
  1. Implement a simple confidence estimation method (e.g., temperature scaling) on a pre-trained LLM and evaluate its performance using calibration metrics.
  2. Compare the performance of different confidence estimation methods (e.g., ensemble-based vs. density-based) on a specific task or dataset.
  3. Investigate the impact of varying the granularity of confidence estimation (e.g., token-level vs. sentence-level) on the performance of a generative LLM.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can confidence estimation methods be effectively combined across different probing positions (logits, internal state, semantics) in generative LLMs to improve overall uncertainty quantification?
- Basis in paper: [explicit] The paper discusses different probing positions for confidence estimation and mentions that combining methods from different positions (e.g., logits-based and generated semantics) has shown promising results.
- Why unresolved: While the paper acknowledges the potential of combining methods from different probing positions, it does not provide a comprehensive framework or empirical evidence for how to effectively combine these methods across various generative LLM tasks.
- What evidence would resolve it: Empirical studies comparing the performance of individual probing position methods versus their combinations across a range of generative LLM tasks, along with a proposed framework for combining these methods effectively.

### Open Question 2
- Question: What are the most effective calibration techniques for addressing the unique challenges of generative LLMs, such as the exponential growth of output space and the need for varied confidence granularity across tasks?
- Basis in paper: [explicit] The paper highlights the unique challenges of generative LLMs and discusses various calibration techniques, but does not provide a definitive answer on the most effective methods for these specific challenges.
- Why unresolved: The paper reviews existing calibration techniques but does not conduct a comprehensive comparison or provide guidance on the most suitable methods for addressing the unique challenges of generative LLMs.
- What evidence would resolve it: A large-scale empirical study comparing the effectiveness of different calibration techniques on generative LLM tasks with varying output spaces and confidence granularity requirements, along with recommendations for the most suitable methods based on task characteristics.

### Open Question 3
- Question: How can hallucination detection and mitigation techniques be improved to provide fine-grained truthfulness assessment in open-ended generation tasks?
- Basis in paper: [explicit] The paper discusses the potential of confidence-related methods for detecting hallucinations in generative LLMs but notes that current methods are coarse-grained and emphasizes the importance of advancing towards fine-grained truthfulness assessment.
- Why unresolved: While the paper acknowledges the need for fine-grained hallucination detection, it does not provide specific techniques or empirical evidence for achieving this goal in open-ended generation tasks.
- What evidence would resolve it: The development and evaluation of novel hallucination detection techniques that can provide fine-grained truthfulness assessment at the token, phrase, or sentence level in open-ended generation tasks, along with empirical results demonstrating their effectiveness compared to existing methods.

## Limitations
- The categorization of methods, while useful, may oversimplify the nuanced differences between various approaches.
- The unique challenges identified for generative LLMs are well-founded but may not fully capture the complexity of real-world applications.
- The suggested future research directions, while promising, are based on current trends and may not anticipate emerging challenges or opportunities.

## Confidence
- The categorization of confidence estimation methods: Medium
- The identification of unique challenges for generative LLMs: High
- The suggested future research directions: Medium

## Next Checks
1. Conduct a systematic review of recent literature to identify any new confidence estimation or calibration methods that may not fit neatly into the existing categories, and propose potential modifications to the framework.

2. Design and execute experiments to quantify the impact of the identified challenges (e.g., exponential output space growth) on the performance of confidence estimation and calibration methods for generative LLMs, using a variety of tasks and model architectures.

3. Implement and evaluate a subset of the suggested future research directions (e.g., hallucination detection or multi-modal LLM calibration) to assess their feasibility and potential impact on the field.