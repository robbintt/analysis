---
ver: rpa2
title: 'WiCE: Real-World Entailment for Claims in Wikipedia'
arxiv_id: '2303.01432'
source_url: https://arxiv.org/abs/2303.01432
tags:
- entailment
- claim
- sentences
- wice
- claims
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WICE, a new textual entailment dataset for
  fact verification built on natural claims and evidence extracted from Wikipedia.
  To address the complexity of real-world claims, the authors propose CLAIM-SPLIT,
  an automatic claim decomposition strategy using GPT-3.5 that breaks claims into
  simpler sub-claims.
---

# WiCE: Real-World Entailment for Claims in Wikipedia

## Quick Facts
- arXiv ID: 2303.01432
- Source URL: https://arxiv.org/abs/2303.01432
- Reference count: 40
- Key outcome: Introduces WICE dataset with fine-grained entailment annotations and demonstrates CLAIM-SPLIT improves performance on multiple datasets

## Executive Summary
This paper introduces WICE, a new textual entailment dataset built on natural claims and evidence extracted from Wikipedia articles. The dataset features fine-grained annotations at the sub-sentence level, including supporting sentences and unsupported tokens. To address the complexity of real-world claims, the authors propose CLAIM-SPLIT, an automatic claim decomposition strategy using GPT-3.5 that breaks claims into simpler sub-claims. Experiments show that CLAIM-SPLIT improves entailment model performance across multiple datasets, and that real claims in WICE involve challenging verification and retrieval problems that existing models struggle with.

## Method Summary
The authors construct the WICE dataset by extracting claims from Wikipedia sentences and gathering evidence from cited web articles. They employ CLAIM-SPLIT, an automatic method using GPT-3.5 to decompose complex claims into simpler sub-claims. The dataset provides fine-grained entailment annotations at the sub-sentence level, including which parts of a claim are supported or not supported by evidence. The authors benchmark various entailment approaches on WICE, including T5 models fine-tuned on NLI datasets, and evaluate the impact of CLAIM-SPLIT on model performance. They also explore supporting sentence retrieval strategies to improve classification accuracy.

## Key Results
- WICE contains 1,260 claims with fine-grained annotations showing 33% claim-level support vs 56% sub-claim support
- CLAIM-SPLIT improves entailment model performance across multiple datasets when aggregating sub-claim scores
- Real claims in WICE require multi-sentence reasoning and complex inferences, presenting challenges for existing models
- Retrieval-based aggregation strategy performs worse than max-based aggregation in most cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing complex claims into sub-claims improves entailment model performance by reducing the complexity of the reasoning task.
- Mechanism: CLAIM-SPLIT uses GPT-3 to automatically decompose a complex claim into simpler, independent sub-claims that cover all information in the original claim. This simplifies both the annotation process and the final entailment prediction task for automatic models.
- Core assumption: Breaking down complex claims into simpler sub-claims reduces the complexity of the entailment task, making it easier for models to process and evaluate.
- Evidence anchors:
  - [abstract] "To support this, we propose an automatic claim decomposition strategy using GPT-3.5 which we show is also effective at improving entailment models' performance on multiple datasets at test time."
  - [section 5.2] "Table 5 outlines our results. It shows that for a majority of model-dataset pairs, using CLAIM-SPLIT and aggregating sub-claim scores improves performance on the entailment classification task."

### Mechanism 2
- Claim: Fine-grained annotation at the sub-sentence level provides a more detailed view of entailment, enabling better model training and evaluation.
- Mechanism: The WICE dataset provides entailment judgments over sub-sentence units of the claim, along with supporting sentence indices and unsupported tokens. This fine-grained annotation allows for a more detailed analysis of which parts of a claim are supported or not supported by evidence.
- Core assumption: Fine-grained annotation captures more nuanced information about entailment, which is beneficial for model training and evaluation.
- Evidence anchors:
  - [abstract] "Our annotations are over sub-sentence units of the hypothesis, decomposed automatically by GPT-3, each of which is labeled with a subset of evidence sentences from the source document."
  - [section 4.1] "At the sub-claim level, we observe that roughly 56% of the sub-claims are supported by the evidence sentences. However, this percentage is much lower at the claim level (33%) since all sub-claims must be supported."

### Mechanism 3
- Claim: Using real-world claims and evidence from Wikipedia provides a more challenging and diverse dataset compared to existing entailment datasets.
- Mechanism: The WICE dataset is built on natural claim and evidence pairs extracted from Wikipedia, which involves real-world claims and evidence that require multi-sentence reasoning and complex inferences.
- Core assumption: Real-world claims and evidence present more diverse and challenging entailment problems compared to synthetic or simplified datasets.
- Evidence anchors:
  - [abstract] "We show that real claims in our dataset involve challenging verification and retrieval problems, and we benchmark existing approaches on this dataset."
  - [section 4.2] "Table 4 compared the types of verification problems in FEVER, VitaminC and our WICE dataset, as manually annotated by us. We can see that natural claims in WICE often require inference and involve difficult entailment classification problems, with the majority of cases requiring some kind of inference, even at the sub-claim level."

## Foundational Learning

- Concept: Textual entailment
  - Why needed here: The paper focuses on textual entailment, which is the task of determining whether a hypothesis is entailed by a given premise or evidence.
  - Quick check question: What is the difference between textual entailment and natural language inference (NLI)?

- Concept: Fine-grained annotation
  - Why needed here: The WICE dataset provides fine-grained annotation at the sub-sentence level, which is crucial for capturing nuanced information about entailment.
  - Quick check question: How does fine-grained annotation differ from traditional sentence-level annotation in entailment tasks?

- Concept: Multi-sentence reasoning
  - Why needed here: The WICE dataset involves real-world claims and evidence that often require reasoning across multiple sentences, which is a challenging aspect of the task.
  - Quick check question: What are some examples of multi-sentence reasoning in natural language understanding tasks?

## Architecture Onboarding

- Component map: CLAIM-SPLIT -> Fine-grained annotation -> Entailment model training -> Supporting sentence retrieval
- Critical path: Decompose complex claims into sub-claims using CLAIM-SPLIT → Annotate sub-claims with entailment judgments, supporting sentences, and unsupported tokens → Train entailment models on the fine-grained annotated data → Evaluate models on the WICE dataset → Perform supporting sentence retrieval
- Design tradeoffs: Using GPT-3 for claim decomposition vs. manual decomposition; Fine-grained annotation vs. sentence-level annotation; Training models on real-world data vs. synthetic data
- Failure signatures: CLAIM-SPLIT fails to generate complete or correct sub-claims; Fine-grained annotation introduces too much complexity or noise; Models struggle with multi-sentence reasoning or complex inferences
- First 3 experiments:
  1. Evaluate the performance of different entailment models on the WICE dataset without fine-tuning.
  2. Fine-tune entailment models on the WICE dataset and evaluate their performance.
  3. Perform supporting sentence retrieval and evaluate its impact on entailment classification accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance of the CLAIM-SPLIT method be improved to reduce the number of errors in decomposing claims into sub-claims?
- Basis in paper: [explicit] The paper discusses intrinsic evaluation of CLAIM-SPLIT, finding that 7.7% of claims fail the completeness criterion and 2.3% fail the correctness criterion.
- Why unresolved: While the paper identifies types of errors made by CLAIM-SPLIT, it does not provide a comprehensive solution to address these errors or improve the method's performance.
- What evidence would resolve it: Developing and testing improved prompts, incorporating additional context, or using alternative decomposition strategies could potentially reduce errors and improve the method's performance.

### Open Question 2
- Question: How can the retrieval of supporting sentences be improved to enhance the overall performance of entailment classification in the WICE dataset?
- Basis in paper: [explicit] The paper discusses the RETRIEVAL strategy, which retrieves top-k sentences and concatenates them to construct a new premise/evidence for entailment classification. However, the results show that this strategy does not perform as well as the MAX strategy.
- Why unresolved: The paper does not provide a detailed analysis of the reasons behind the retrieval strategy's lower performance or potential improvements to the retrieval process.
- What evidence would resolve it: Investigating alternative retrieval methods, incorporating more sophisticated ranking algorithms, or exploring the impact of context on retrieval performance could help improve the retrieval of supporting sentences.

### Open Question 3
- Question: How can the detection of unsupported tokens in partially supported sub-claims be improved to provide more accurate and fine-grained entailment judgments?
- Basis in paper: [explicit] The paper mentions that the detection of unsupported tokens is not explored in the experiments, but it acknowledges the potential difficulty of this task with existing methods.
- Why unresolved: The paper does not provide a detailed analysis of the challenges in detecting unsupported tokens or propose potential solutions to address this issue.
- What evidence would resolve it: Developing and evaluating new methods for detecting unsupported tokens, such as incorporating more advanced natural language processing techniques or leveraging additional context, could improve the accuracy of fine-grained entailment judgments.

## Limitations
- The dataset size (1,260 claims) may limit generalization to larger-scale fact-checking scenarios
- Manual annotation process may introduce inter-annotator variability that isn't fully quantified
- The paper lacks detailed ablation studies to isolate the contribution of CLAIM-SPLIT decomposition from the aggregation strategy

## Confidence
- High Confidence: Dataset construction methodology and annotation pipeline are well-documented and reproducible
- Medium Confidence: Effectiveness of CLAIM-SPLIT in improving model performance across multiple datasets, but benefits vary significantly across different model-dataset combinations
- Low Confidence: Assertion that fine-grained sub-sentence annotation provides clear benefits over traditional sentence-level approaches lacks direct empirical comparison within the paper

## Next Checks
1. Conduct ablation studies to isolate the contribution of CLAIM-SPLIT decomposition from the aggregation strategy by comparing performance with and without decomposition on the same models and datasets.
2. Perform cross-dataset generalization experiments by training models on WICE and testing on other entailment datasets (and vice versa) to assess whether the fine-grained annotation approach generalizes beyond the Wikipedia domain.
3. Implement inter-annotator agreement analysis using the 30% overlap in manual annotations to quantify the consistency and reliability of the fine-grained labeling process, particularly for partial support cases and unsupported token identification.