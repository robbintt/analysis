---
ver: rpa2
title: Grokking Group Multiplication with Cosets
arxiv_id: '2312.06581'
source_url: https://arxiv.org/abs/2312.06581
tags:
- group
- coset
- cosets
- fourier
- circuit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the mechanisms used by a one-hidden-layer neural
  network trained to perform group multiplication on permutation groups S5 and S6.
  The authors find that the network learns to decompose group multiplication into
  operations on cosets of conjugate subgroups, a "coset circuit" that they reverse-engineer
  using the group Fourier transform.
---

# Grokking Group Multiplication with Cosets

## Quick Facts
- **arXiv ID:** 2312.06581
- **Source URL:** https://arxiv.org/abs/2312.06581
- **Reference count:** 40
- **Primary result:** Neural networks trained on group multiplication learn to decompose operations into coset circuits, which can be reverse-engineered using group Fourier transforms

## Executive Summary
This paper analyzes a one-hidden-layer neural network trained to perform group multiplication on permutation groups S5 and S6. The authors discover that the model learns to decompose group multiplication into operations on cosets of conjugate subgroups, a mechanism they reverse-engineer using the group Fourier transform. Through ablation studies and causal interventions, they validate that the network's generalization capability stems from this "coset circuit" approach rather than the previously claimed mechanism. The work highlights both the power of mechanistic interpretability techniques and the difficulty of correctly identifying neural network mechanisms.

## Method Summary
The authors train one-hidden-layer neural networks with separate left and right embeddings, a linear layer with ReLU activation, and an unembedding layer to perform group multiplication on S5 and S6. They analyze the trained models using group Fourier transforms to identify weight concentration patterns on specific irreps and cosets. The methodology includes causal interventions (swapping embeddings) and ablation studies (removing identified circuits) to validate their hypothesis that the model learns coset-based decomposition of group multiplication.

## Key Results
- The model achieves perfect generalization on group multiplication for S5 and S6
- Fourier analysis reveals weight concentration on specific irreps corresponding to coset membership
- Ablation studies confirm the identified coset circuits are responsible for the model's generalization
- Causal interventions demonstrate that left and right embeddings store coset membership information
- The coset circuit mechanism differs from previously claimed mechanisms for similar tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The model decomposes group multiplication into operations on cosets of conjugate subgroups.
- **Mechanism:** In the left and right embedding layers, the model stores information about membership in dozens of left and right cosets. In the linear layer, the embeddings are queried to get the coset membership for σl and σr. The linear layer combines this information to get the probable coset membership of their product σlσr. In the unembedding layer, the identity of σlσr is determined as the only permutation in Sn that satisfies all of the different coset memberships computed by the linear layer.
- **Core assumption:** Coset multiplication is well-defined for specific pairs of conjugate subgroups.
- **Evidence anchors:**
  - [abstract] "The models discover the true subgroup structure of the full group and converge on circuits that decompose the group multiplication into the multiplication of the group's conjugate subgroups."
  - [section] "The model learns to identify those subgroups using the fact that the coset multiplication is well-defined for specific pairs of conjugate subgroups."
  - [corpus] Weak evidence; the corpus papers focus on grokking and modular arithmetic, not group multiplication.
- **Break condition:** If coset multiplication is not well-defined for the chosen conjugate subgroups, the model cannot perfectly generalize.

### Mechanism 2
- **Claim:** The linear layer uses a ReLU activation function to create a perfect 0-1 coset membership switch.
- **Mechanism:** Each neuron in the linear layer has pre-activations that take on one of three values: -2x, 0, or 2x. The ReLU activation function cuts off the pre-activations if they are -2x and otherwise passes them along unchanged to the unembedding layer. This creates a binary signal indicating coset membership.
- **Core assumption:** The pre-activations are symmetric about the origin and the sign of the pre-activations does not matter, only whether or not the pre-activation is equal to zero.
- **Evidence anchors:**
  - [section] "The ReLU activation function cuts off the pre-activations if they are −2x and otherwise passes them along unchanged to the unembedding layer."
  - [section] "The pre-activations are symmetric about the origin and the sign of the pre-activations does not matter, only whether or not the pre-activations is equal to zero."
  - [corpus] No direct evidence in the corpus; this is specific to the coset circuit mechanism.
- **Break condition:** If the pre-activations are not symmetric or the ReLU is replaced with a different activation function, the coset membership switch may not work.

### Mechanism 3
- **Claim:** The unembedding layer decodes permutations by aggregating coset membership information from multiple neurons.
- **Mechanism:** Each element of the unembedding input vector can be thought of as an on/off switch keyed to a specific coset. If the activation is positive then the product σlσr is certainly not in that coset, but if it is zero it is only probably in the coset. Putting aside this ambiguity for a moment, we can see that with enough of these circuits the model can uniquely encode every permutation σ ∈ Sn and perfectly solve the group multiplication.
- **Core assumption:** The coset circuits are redundant and can compensate for individual neuron noise.
- **Evidence anchors:**
  - [section] "The unembedding layer receives as input a vector of activations from that encode the coset membership of of the product σlσr."
  - [section] "The model compensates for this by having multiple neurons dedicated to a single coset."
  - [corpus] No direct evidence in the corpus; this is specific to the coset circuit mechanism.
- **Break condition:** If the coset circuits are not redundant or the unembedding layer cannot aggregate the information correctly, the model may not perfectly solve the group multiplication.

## Foundational Learning

- **Concept:** Group theory, specifically the symmetric group Sn and its subgroups.
  - Why needed here: The model learns to decompose group multiplication into operations on cosets of conjugate subgroups of Sn.
  - Quick check question: What is the relationship between the cosets of a normal subgroup and the cosets of its conjugate subgroups?

- **Concept:** Fourier analysis, specifically the group Fourier transform over Sn.
  - Why needed here: The authors use the group Fourier transform to reverse engineer the model's mechanisms and confirm their theory.
  - Quick check question: How does the group Fourier transform over Sn differ from the classical Fourier transform over real numbers?

- **Concept:** Mechanistic interpretability, specifically causal interventions and ablation studies.
  - Why needed here: The authors use causal interventions and ablation studies to validate their coset circuit hypothesis.
  - Quick check question: What is the difference between a causal intervention and an ablation study in the context of mechanistic interpretability?

## Architecture Onboarding

- **Component map:** One-hot vectors of left and right permutations -> Separate embeddings for left and right permutations -> Concatenated embeddings passed through linear layer with ReLU -> Activations transformed into logits by unembedding layer -> Logits for each permutation in Sn

- **Critical path:**
  1. Embed left and right permutations separately.
  2. Concatenate embeddings and pass through linear layer with ReLU.
  3. Use unembedding layer to transform activations into logits.
  4. Apply softmax to logits to get probability distribution over permutations.

- **Design tradeoffs:**
  - Using separate embeddings for left and right permutations allows the model to store different information (left and right coset membership).
  - Using a ReLU activation function in the linear layer creates a binary signal for coset membership.
  - Using multiple neurons per coset allows the model to compensate for individual neuron noise.

- **Failure signatures:**
  - If the model cannot perfectly solve the group multiplication, it may indicate that the coset circuits are not working correctly.
  - If the unembedding layer cannot correctly decode the permutation from the coset membership information, it may indicate a problem with the coset circuits or the unembedding layer.

- **First 3 experiments:**
  1. Ablation study: Remove the identified coset circuits and observe the change in accuracy.
  2. Causal intervention: Switch the left and right embeddings and observe the change in accuracy.
  3. Ablation study: Remove all neurons except those forming the coset circuits and observe the change in accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the choice of experimental setup (e.g., learning rate, initialization) influence which algorithm a neural network converges on for group multiplication?
- **Basis in paper:** [inferred] The authors cite Zhang et al. [58] showing that the learned algorithm can be sensitive to experimental details.
- **Why unresolved:** The authors focus on a specific setup and do not explore the sensitivity of their results to variations in hyperparameters or initialization.
- **What evidence would resolve it:** Systematic experiments varying learning rate, initialization, and other relevant hyperparameters to observe how the learned algorithm changes.

### Open Question 2
- **Question:** Can the coset circuit mechanism be generalized to other non-abelian groups beyond permutation groups?
- **Basis in paper:** [explicit] The authors discuss the application of Fourier analysis over groups and the relationship between coset membership and irreps.
- **Why unresolved:** The authors only analyze the specific case of permutation groups and do not explore the broader applicability of their findings.
- **What evidence would resolve it:** Applying the coset circuit analysis to other non-abelian groups (e.g., matrix groups, quaternion group) and comparing the results to those obtained for permutation groups.

### Open Question 3
- **Question:** What is the relationship between the spectral properties of the weights and the generalization capabilities of the model?
- **Basis in paper:** [explicit] The authors discuss the concentration of weights on specific irreps in the Fourier basis and its connection to the coset circuit.
- **Why unresolved:** The authors observe this relationship but do not provide a theoretical explanation for why it leads to better generalization.
- **What evidence would resolve it:** Theoretical analysis of the Fourier spectrum of functions that can be learned by neural networks and empirical studies correlating spectral properties with generalization performance.

## Limitations

- The findings are specific to group multiplication tasks and may not generalize to other mathematical operations
- The analysis relies heavily on group Fourier transforms, which become computationally intractable for larger groups
- The perfect generalization achieved may be an artifact of the specific experimental setup rather than a general property of neural networks on group-theoretic tasks

## Confidence

- Coset circuit hypothesis: Medium-High
- Fourier transform analysis: High
- Causal intervention results: Medium-High
- Generalization to other tasks: Low

## Next Checks

1. Train models on larger permutation groups (S7, S8) to test scalability of the coset circuit mechanism
2. Replace ReLU activation with alternative nonlinearities to verify the 0-1 coset membership switch mechanism
3. Apply the same analytical framework to other group-theoretic tasks (e.g., group division or exponentiation) to test generalizability