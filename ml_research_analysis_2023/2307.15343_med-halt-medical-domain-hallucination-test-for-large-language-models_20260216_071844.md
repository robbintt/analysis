---
ver: rpa2
title: 'Med-HALT: Medical Domain Hallucination Test for Large Language Models'
arxiv_id: '2307.15343'
source_url: https://arxiv.org/abs/2307.15343
tags:
- correct
- medical
- answer
- title
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces Med-HALT, a new benchmark and dataset for
  evaluating and mitigating hallucinations in large language models (LLMs) in the
  medical domain. The dataset is derived from medical examinations across multiple
  countries and includes two categories of tests: reasoning hallucination tests and
  memory-based hallucination tests.'
---

# Med-HALT: Medical Domain Hallucination Test for Large Language Models

## Quick Facts
- arXiv ID: 2307.15343
- Source URL: https://arxiv.org/abs/2307.15343
- Reference count: 5
- Key outcome: Introduces Med-HALT benchmark showing current LLMs perform poorly on medical hallucination tests, with accuracy below acceptable levels on reasoning tasks.

## Executive Summary
Med-HALT introduces a comprehensive benchmark for evaluating hallucinations in large language models within medical domains. The dataset combines medical entrance exams from India, Spain, US, and Taiwan with PubMed literature, creating two categories of tests: reasoning hallucination tests (FCT, NOTA, Fake Questions) and memory-based hallucination tests (abstract-to-link, PMID-to-title, etc.). The study evaluates leading LLMs including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing that while some models perform better than others, none reach acceptable accuracy levels on reasoning tests, highlighting the fundamental challenge of medical hallucination detection for current models.

## Method Summary
The study evaluates LLMs on a newly created Med-HALT dataset using zero-shot and few-shot approaches with structured JSON output prompts. The dataset combines medical entrance exams from multiple countries with PubMed literature, creating reasoning tests (False Confidence, None of the Above, Fake Questions) and memory-based tests (abstract-to-link, PMID-to-title mappings). Models are evaluated using temperature settings of 0, 0.5, and 1, with results aggregated by accuracy and pointwise scores. The evaluation framework requires models to output structured JSON responses that are parsed for correctness, with error handling for malformed outputs.

## Key Results
- GPT-3.5 achieved highest accuracy (44.3%) on NOTA tests among evaluated models, but no model reached acceptable performance levels on reasoning tasks
- LlaMa-2-7B performed worst on reasoning tests but achieved 80% accuracy on memory-based PMID-to-Title test
- GPT-3.5's performance degraded with higher temperatures, while Falcon-40B showed improved reasoning performance with increased temperature
- Models consistently struggled with Fake Questions test, designed to detect fabricated information generation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Med-HALT improves LLM reliability in healthcare by forcing models to explicitly reason through medical multiple-choice questions with structured feedback.
- **Mechanism**: The dataset uses four reasoning hallucination tests (FCT, NOTA, Fake Questions, and memory-based retrieval tests) to measure whether LLMs can avoid generating incorrect medical information. Each test includes structured prompts requiring explicit justification of correct/incorrect answers.
- **Core assumption**: LLMs will improve reasoning reliability when prompted to justify their answers rather than simply selecting them.
- **Evidence anchors**:
  - [abstract] "Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMs's problem-solving and information retrieval abilities."
  - [section] "Reasoning Hallucination Tests (RHTs) assess how accurately the language model performs reasoning over the medical input data and whether it generates logically coherent and factually accurate output, without creating fake information."
- **Break condition**: If models continue to hallucinate despite structured prompts, it suggests inherent limitations in their reasoning capabilities rather than prompt design.

### Mechanism 2
- **Claim**: Memory hallucination tests evaluate LLMs' ability to retrieve accurate biomedical information without fabrication.
- **Mechanism**: Tests like Abstract-to-Link, PMID-to-Title, Title-to-Link, and Link-to-Title assess whether LLMs can accurately map between different representations of medical literature without generating incorrect information.
- **Core assumption**: LLMs trained on PubMed data should be able to retrieve accurate mappings between abstracts, titles, PMIDs, and URLs.
- **Evidence anchors**:
  - [abstract] "memory-based hallucination tests assess the models' ability to retrieve accurate information from their encoded training data."
  - [section] "Memory Hallucination Tests (MHTs) investigate the language model's ability to recall and generate accurate factual information."
- **Break condition**: If models consistently fail to retrieve correct mappings even for existing papers, it indicates fundamental limitations in their retrieval capabilities.

### Mechanism 3
- **Claim**: Multinational dataset diversity improves generalization of hallucination detection across different medical education systems.
- **Mechanism**: Med-HALT combines datasets from India, Spain, US, and Taiwan, exposing models to diverse question formats, medical terminology, and examination patterns.
- **Core assumption**: Models trained on diverse medical datasets will generalize better to different hallucination scenarios.
- **Evidence anchors**:
  - [abstract] "Med-HALT provides a diverse multinational dataset derived from medical examinations across various countries"
  - [section] "The Med-HALT dataset incorporates a diverse set of medical entrance exams from various countries, allowing for a rich, multicultural examination of medical knowledge and practice."
- **Break condition**: If performance varies significantly across different country datasets, it suggests the need for country-specific fine-tuning.

## Foundational Learning

- **Concept**: Hallucination in LLMs
  - **Why needed here**: Understanding what constitutes hallucination is essential for evaluating and mitigating it in medical contexts.
  - **Quick check question**: What distinguishes a hallucination from a simple error in LLM outputs?

- **Concept**: Medical domain specificity
  - **Why needed here**: Medical applications require higher accuracy standards than general text generation.
  - **Quick check question**: Why are hallucinations more dangerous in medical applications than in casual conversation?

- **Concept**: Prompt engineering and instruction following
  - **Why needed here**: The study shows that prompt framing significantly affects model performance on hallucination tests.
  - **Quick check question**: How does prompt specificity affect the likelihood of LLM hallucinations?

## Architecture Onboarding

- **Component map**: Dataset ingestion pipeline (multiple country sources) -> Test generation engine (reasoning and memory tests) -> LLM evaluation framework (structured JSON output parsing) -> Result aggregation and analysis system

- **Critical path**:
  1. Load and preprocess medical exam datasets
  2. Generate structured prompts for each test type
  3. Send prompts to target LLMs via API
  4. Parse JSON responses and validate format
  5. Calculate accuracy and pointwise scores
  6. Analyze results by test type and model

- **Design tradeoffs**:
  - Structured JSON output vs. natural language responses
  - Temperature settings for controlled vs. diverse generation
  - API-based vs. local model evaluation
  - Comprehensive vs. representative sampling

- **Failure signatures**:
  - Malformed JSON responses indicating instruction-following issues
  - Consistent hallucinations across multiple test types
  - Performance degradation with temperature adjustments
  - Country-specific performance variations

- **First 3 experiments**:
  1. Run FCT test on a small sample to verify prompt structure and JSON parsing
  2. Test temperature sensitivity on GPT-3.5 with reasoning questions
  3. Compare performance across country datasets on NOTA test

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different fine-tuning strategies beyond instruction tuning affect the hallucination control in medical LLMs?
- Basis in paper: [inferred]
- Why unresolved: The paper only briefly mentions instruction tuning's detrimental effect on hallucination control and suggests exploring other methods like adding external knowledge or setting specific training objectives, but doesn't provide empirical results for these alternatives.
- What evidence would resolve it: Comparative experiments testing various fine-tuning approaches (knowledge integration, contrastive learning, etc.) on the Med-HALT benchmark with detailed analysis of hallucination reduction effectiveness.

### Open Question 2
- Question: What is the optimal temperature range for minimizing hallucinations across different types of medical reasoning tasks?
- Basis in paper: [explicit]
- Why unresolved: The paper shows temperature has minimal effect on GPT-3.5's hallucination rate but doesn't systematically explore temperature optimization across different task types or models.
- What evidence would resolve it: Grid search experiments varying temperature parameters across all Med-HALT tasks for multiple models, identifying task-specific optimal ranges.

### Open Question 3
- Question: How does model architecture influence hallucination susceptibility in medical domains compared to general knowledge domains?
- Basis in paper: [explicit]
- Why unresolved: The paper compares multiple models on Med-HALT but doesn't analyze architectural differences (attention mechanisms, parameter counts, etc.) that might explain varying hallucination tendencies.
- What evidence would resolve it: Ablation studies comparing models with similar parameters but different architectures, or systematic analysis correlating architectural features with hallucination rates across domains.

## Limitations
- Current LLMs show fundamental limitations in medical reasoning, with accuracy below acceptable levels even on structured tests
- Performance degradation with temperature adjustments suggests sensitivity to decoding parameters that may affect reliability
- Memory-based tests reveal retrieval limitations, with models failing to accurately map between different medical literature representations

## Confidence
- **High Confidence**: The methodology for creating structured reasoning tests and memory-based hallucination evaluations is clearly specified and reproducible.
- **Medium Confidence**: The reported performance differences between models are reliable, but the absolute accuracy levels suggest fundamental limitations in current LLM capabilities for medical reasoning.
- **Low Confidence**: The effectiveness of prompt engineering approaches in improving model reliability, as the study shows limited success in reducing hallucinations through structured prompting.

## Next Checks
1. Test temperature sensitivity systematically across all reasoning tasks to determine if hallucination rates correlate with decoding parameters.
2. Conduct ablation studies on prompt structure to identify which elements most effectively reduce hallucination rates.
3. Evaluate model performance on cross-country datasets to determine if hallucination patterns vary by medical education system and terminology.