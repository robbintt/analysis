---
ver: rpa2
title: 'Silver Retriever: Advancing Neural Passage Retrieval for Polish Question Answering'
arxiv_id: '2309.08469'
source_url: https://arxiv.org/abs/2309.08469
tags:
- polish
- datasets
- passage
- passages
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SilverRetriever, a neural passage retrieval
  model for Polish. The model is trained on a diverse collection of manually and weakly
  labeled datasets, including existing and newly created datasets.
---

# Silver Retriever: Advancing Neural Passage Retrieval for Polish Question Answering

## Quick Facts
- arXiv ID: 2309.08469
- Source URL: https://arxiv.org/abs/2309.08469
- Reference count: 0
- Primary result: SilverRetriever achieves state-of-the-art retrieval performance on Polish QA tasks using a diverse training corpus with hard negatives

## Executive Summary
This paper introduces SilverRetriever, a neural passage retrieval model for Polish question answering. The model is trained on a diverse collection of manually and weakly labeled datasets, including existing and newly created datasets. SilverRetriever is evaluated on three passage retrieval tasks and achieves significant improvements over existing Polish models and competes favorably with larger multilingual models. The model achieves the best average accuracy (92.45%) and NDCG (66.72%) across all tasks, outperforming other models including E5-Base and BM25. The paper also open-sources five new passage retrieval datasets with over 500,000 questions, providing valuable resources for improving the accuracy and efficiency of open-domain question answering systems in Polish.

## Method Summary
SilverRetriever is a dense passage retriever based on the DPR architecture, using a fine-tuned HerBERT Base model as a bi-encoder. The model is trained on a diverse corpus of nine datasets (four existing, five newly created) with both manual and weak labels. Key training techniques include hard negative mining using BM25 and cross-encoders, denoising and filtering of training pairs, and aggregation of multiple weakly labeled datasets. The model is evaluated on three Polish QA datasets using Accuracy@10 and NDCG@10 metrics.

## Key Results
- SilverRetriever achieves the best average accuracy (92.45%) and NDCG (66.72%) across all tasks
- Outperforms existing Polish models and competes favorably with larger multilingual models like E5-Base
- Open-sources five new passage retrieval datasets with over 500,000 questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SilverRetriever achieves superior performance by training on a large, diverse collection of manually and weakly labeled datasets.
- Mechanism: The model benefits from a broader and more varied training signal than previous Polish models, which were trained on fewer or narrower datasets. The inclusion of both manually curated data (PolQA, PolEval Pairs) and automatically generated/weakly labeled data (MAUPQA, GPT-3.5-CC, GPT-3.5-Wiki, MS MARCO translations) increases the coverage of question types and domains.
- Core assumption: Training on a mixture of high-quality manual annotations and large-scale weak labels improves generalization across diverse retrieval tasks.
- Evidence anchors: [abstract] "trained on a diverse collection of manually or weakly labeled datasets", [section] "We use a collection of four existing and five newly created datasets for training"
- Break condition: If the weak labels are too noisy or unrepresentative, model performance may degrade despite dataset size.

### Mechanism 2
- Claim: The inclusion of hard negatives mined from the training corpus significantly improves retrieval quality.
- Mechanism: Hard negatives are irrelevant passages that are lexically or semantically similar to relevant ones, making them challenging for the model to distinguish. By mining and including such pairs, the model learns finer-grained distinctions, reducing false positives in retrieval.
- Core assumption: Hard negatives force the model to learn more discriminative representations rather than relying on simple lexical overlap.
- Evidence anchors: [section] "We follow a similar strategy to Ren et al. (2021) for mining hard negatives... score them using the mMiniLM-L6-v2 cross-encoder and keep only the irrelevant passages"
- Break condition: If hard negatives are not sufficiently challenging (e.g., too dissimilar from relevant passages), the benefit diminishes.

### Mechanism 3
- Claim: Denoising and filtering of training pairs improves the quality of learned representations.
- Mechanism: By removing low-quality or mislabeled pairs, the model focuses on learning from clean, informative examples, which leads to better generalization and higher accuracy on downstream tasks.
- Core assumption: Removing noisy pairs improves model robustness more than increasing dataset size.
- Evidence anchors: [section] "We apply several steps of filtering noisy pairs... We discard 14% of the relevant question-passage pairs"
- Break condition: Over-filtering may remove useful borderline cases, hurting robustness.

## Foundational Learning

- Concept: Dense passage retrieval (DPR) architecture
  - Why needed here: SilverRetriever uses a DPR-based bi-encoder model to learn dense representations of questions and passages for efficient similarity search.
  - Quick check question: What are the two main components of a DPR system, and how do they interact during retrieval?

- Concept: Hard negative mining in contrastive learning
  - Why needed here: Hard negatives are essential for training effective dense retrievers, as they prevent the model from relying on superficial lexical matches.
  - Quick check question: How does hard negative mining differ from random negative sampling, and why is it more effective for retrieval?

- Concept: Dataset construction and quality control for retrieval
  - Why needed here: The performance of neural retrievers depends heavily on the quality and diversity of training data. Understanding how to construct, filter, and augment datasets is critical for replicating or extending this work.
  - Quick check question: What are the trade-offs between manually annotated and automatically generated training data for retrieval tasks?

## Architecture Onboarding

- Component map: Input text -> HerBERT Base bi-encoder (question encoder + passage encoder) -> Dense vector representations -> Similarity scoring

- Critical path:
  1. Load and preprocess training data from all datasets
  2. Apply denoising filters (length, relevance, similarity thresholds)
  3. Mine hard negatives for each question
  4. Fine-tune HerBERT bi-encoder on filtered, augmented data
  5. Evaluate on PolQA, Allegro FAQ, and Legal Questions

- Design tradeoffs:
  - Dataset size vs. quality: Larger weakly labeled datasets improve coverage but may introduce noise; denoising mitigates this at the cost of data loss.
  - Model size vs. efficiency: HerBERT Base is smaller and faster than multilingual models but may sacrifice some multilingual generalization.
  - Hard negative mining cost: More thorough mining improves quality but increases preprocessing time.

- Failure signatures:
  - Low accuracy but high NDCG: Model retrieves relevant passages but ranks them poorly.
  - High lexical overlap but low semantic match: Model overfits to surface forms; consider more diverse negatives.
  - Degraded performance on long passages: Check for tokenization or truncation issues in the bi-encoder.

- First 3 experiments:
  1. Train a baseline HerBERT bi-encoder on PolQA only; compare accuracy/NDCG to SilverRetriever.
  2. Add hard negatives to the PolQA-only model; measure impact on accuracy and NDCG.
  3. Apply denoising to the full dataset; evaluate change in performance and data retention rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SilverRetriever perform on domains outside of trivia, FAQ, and legal questions?
- Basis in paper: [inferred] The paper evaluates SilverRetriever on three specific domains (trivia, FAQ, and legal questions) but does not explore its performance on other domains.
- Why unresolved: The paper does not provide any data or analysis on the model's performance on other domains, such as news, scientific literature, or social media.
- What evidence would resolve it: Testing SilverRetriever on a diverse set of domains and comparing its performance to other models would provide insights into its generalization capabilities.

### Open Question 2
- Question: What is the impact of using different types of hard negatives on the performance of SilverRetriever?
- Basis in paper: [explicit] The paper mentions that hard negatives are crucial for training robust neural retrievers but does not explore the impact of using different types of hard negatives.
- Why unresolved: The paper does not provide any analysis on how different types of hard negatives (e.g., lexical, semantic, or syntactic) affect the model's performance.
- What evidence would resolve it: Conducting experiments with different types of hard negatives and comparing their impact on the model's performance would provide insights into the importance of hard negatives.

### Open Question 3
- Question: How does SilverRetriever's performance compare to human performance on the evaluated tasks?
- Basis in paper: [inferred] The paper evaluates SilverRetriever's performance on three tasks but does not compare it to human performance.
- Why unresolved: The paper does not provide any data or analysis on how SilverRetriever's performance compares to human performance on the evaluated tasks.
- What evidence would resolve it: Conducting a human evaluation of the same tasks and comparing the results to SilverRetriever's performance would provide insights into the model's effectiveness.

## Limitations

- The effectiveness of the denoising pipeline is not fully validated with ablation studies
- Hard negative mining relies on a cross-encoder without analysis of its impact on retrieval quality
- Claims about scalability and generalization to other languages are limited by testing only one multilingual baseline

## Confidence

- **High**: Claims about dataset diversity and the use of both manual and weak labels for training.
- **Medium**: Claims about the impact of hard negatives and denoising on model performance, due to limited ablation evidence.
- **Low**: Claims about scalability and generalization to other languages, as only one multilingual baseline is tested.

## Next Checks

1. **Ablation Study on Hard Negatives**: Remove hard negative mining from the training pipeline and evaluate the change in accuracy and NDCG on the three test datasets to quantify its contribution.
2. **Denoising Pipeline Analysis**: Report the percentage of data filtered out by each denoising step and conduct an ablation study to measure the impact of denoising on retrieval performance.
3. **Multilingual Scalability Test**: Evaluate SilverRetriever on a non-Polish language (e.g., Czech or Slovak) using the same architecture and training strategy to assess its generalization beyond Polish.