---
ver: rpa2
title: Automatic Disfluency Detection from Untranscribed Speech
arxiv_id: '2311.00867'
source_url: https://arxiv.org/abs/2311.00867
tags:
- disfluency
- detection
- speech
- these
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of detecting and categorizing\
  \ speech disfluencies\u2014such as filled pauses, repetitions, and revisions\u2014\
  at the frame level, with applications in clinical assessment and downstream NLP\
  \ tasks. The authors compare language-based, acoustic-based, and multimodal approaches,\
  \ evaluating the impact of different ASR systems on disfluency detection performance."
---

# Automatic Disfluency Detection from Untranscribed Speech

## Quick Facts
- **arXiv ID**: 2311.00867
- **Source URL**: https://arxiv.org/abs/2311.00867
- **Reference count**: 40
- **Primary result**: Multimodal fusion of acoustic and language representations significantly improves automatic disfluency detection and categorization over unimodal approaches.

## Executive Summary
This paper addresses the challenge of detecting and categorizing speech disfluencies—such as filled pauses, repetitions, and revisions—at the frame level, with applications in clinical assessment and downstream NLP tasks. The authors compare language-based, acoustic-based, and multimodal approaches, evaluating the impact of different ASR systems on disfluency detection performance. They find that disfluency detection performance is largely limited by the quality of ASR transcripts and alignments, and that acoustic-only approaches can outperform language-based ones when ASR errors are high. Multimodal architectures that fuse acoustic and language representations, especially using a BLSTM, improve detection performance over unimodal approaches. WavLM fine-tuned for acoustic disfluency detection and Whisper-FT for ASR both show strong results. Overall, the study demonstrates that combining acoustic and language modalities can significantly enhance automatic disfluency detection and categorization.

## Method Summary
The study evaluates unimodal and multimodal approaches to frame-level disfluency detection using the Switchboard corpus. Unimodal models include fine-tuned BERT on ASR transcripts and fine-tuned WavLM on acoustic features. Multimodal models concatenate frame-level WavLM and BERT embeddings and fuse them using Perceptron, BLSTM, or Transformer layers. The authors compare ASR systems (W2V2, HuBERT, WavLM, Whisper-FT, Azure) for transcript quality and downstream disfluency detection performance. Models are trained with Adam optimizer, binary cross-entropy loss, and evaluated using frame-level F1, recall, and UAR. The best-performing multimodal approach combines WavLM acoustic features with Whisper-FT transcripts via BLSTM fusion.

## Key Results
- Acoustic-only approaches (WavLM) outperform language-only approaches (BERT) when ASR transcript quality is low, due to robust prosodic feature learning.
- Multimodal BLSTM fusion of WavLM and BERT embeddings yields the highest frame-level F1 scores, especially for repetitions, revisions, and partial words.
- Fine-tuned Whisper (Whisper-FT) for verbatim disfluency transcription outperforms off-the-shelf Whisper and Azure ASR, but still underperforms manual transcripts in downstream detection.
- Disfluency detection performance is strongly correlated with ASR transcription quality, particularly for disfluency-rich segments.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Acoustic representations generalize better to disfluency detection than language representations from ASR, especially when ASR transcript quality is low.
- **Mechanism**: WavLM pre-trained with denoising tasks learns robust speech features that capture prosodic and acoustic cues (pauses, pitch changes) associated with disfluencies, bypassing the need for accurate transcripts.
- **Core assumption**: Disfluencies have distinct acoustic signatures that are learnable from unlabeled speech data.
- **Evidence anchors**:
  - [abstract]: "We find that an acoustic-based approach that does not require transcription as an intermediate step outperforms the ASR language approach."
  - [section]: "We compare the use of W2V2, HuBERT, and WavLM as acoustic-based disfluency detection models... WavLM achives lower WER than HuBERT. WavLM also outperforms BERT on non-ASR tasks, such as speech separation and speaker diarization [10]."
  - [corpus]: "We evaluate the use of W2V2, HuBERT, and WavLM... We find that WavLM performs best in terms of F1 and recall score macros."

### Mechanism 2
- **Claim**: Multimodal fusion improves disfluency detection by combining language semantics with acoustic prosody.
- **Mechanism**: Concatenating frame-level acoustic features (WavLM) with language embeddings (BERT) provides complementary signals; the BLSTM fusion layer models local dependencies across modalities.
- **Core assumption**: Language and acoustic modalities capture disjoint aspects of disfluencies; their combination yields more robust predictions.
- **Evidence anchors**:
  - [abstract]: "Multimodal architectures that fuse acoustic and language representations... improve detection performance over unimodal approaches."
  - [section]: "We hypothesize that a multimodal fusion model will improve detection over the unimodal approaches... A BLSTM, which processes inputs sequentially forward and backward, may better model the frame-level representations."
  - [corpus]: "The BLSTM fusion approach outperformed the unimodal approaches... Compared to the unimodal models, the BLSTM fusion model yielded the largest improvements for F1 score of repetitions, revisions, and partial words."

### Mechanism 3
- **Claim**: Fine-tuning Whisper for verbatim disfluency transcription yields higher downstream disfluency detection recall than off-the-shelf Whisper.
- **Mechanism**: Fine-tuning on Switchboard verbatim disfluency data teaches Whisper to transcribe filler words and partial utterances instead of omitting them.
- **Core assumption**: Whisper's pre-training on diverse web data includes sufficient disfluency examples to fine-tune effectively.
- **Evidence anchors**:
  - [abstract]: "We find that disfluency detection performance is largely limited by the quality of ASR transcripts and alignments... Whisper-FT for ASR both show strong results."
  - [section]: "Whisper-OTS does include a decoding flag to detect disfluencies, but disfluencies are only transcribed as “*”... We hypothesize that if Whisper is fine-tuned with verbatim disfluent speech, its transcripts can be used in a language-based disfluency detection model with a higher average recall score."
  - [corpus]: "When comparing manually transcribed text versus Whisper-FT text as input to the BERT disfluency detection model, we found that performance drops considerably... Whisper-FT performed better than Azure in transcribing disfluent speech."

## Foundational Learning

- **Concept**: Frame-level labeling and alignment
  - Why needed here: Models predict disfluency per frame (25ms), but transcripts and labels are at word or token level. Accurate alignment is critical for training and evaluation.
  - Quick check question: How do you map word-level disfluency labels to frame-level labels given start/end timestamps?

- **Concept**: Fine-tuning vs. off-the-shelf ASR
  - Why needed here: ASR models pre-trained on fluent speech may omit disfluencies; fine-tuning adapts them to verbatim transcription needed for disfluency detection.
  - Quick check question: What metrics distinguish verbatim vs. intended speech transcription quality?

- **Concept**: Multimodal fusion architectures
  - Why needed here: Acoustic and language representations operate at different time scales and capture different cues; fusion must handle alignment and complementary signal integration.
  - Quick check question: Why might a BLSTM fusion layer outperform a simple perceptron or transformer in this context?

## Architecture Onboarding

- **Component map**:
  - Audio input -> WavLM-FT (acoustic features) + ASR transcript -> Whisper-FT (timestamps) -> BERT (language embeddings) -> frame-level alignment -> concatenate features -> BLSTM fusion -> frame-level disfluency predictions

- **Critical path**:
  1. Input audio → ASR transcription + timestamps (or skip for acoustic-only)
  2. ASR transcripts → BERT embeddings (word-level)
  3. Upsample both modalities to frame-level
  4. Concatenate and feed to fusion network
  5. Output frame-level disfluency class probabilities

- **Design tradeoffs**:
  - ASR-only: Scalable but limited by transcription errors; best for intended speech.
  - Acoustic-only: No transcription bottleneck; better for acoustic-rich disfluencies; may miss semantic cues.
  - Multimodal: Best accuracy but higher compute and dependency on ASR.

- **Failure signatures**:
  - High FER-D (disfluent frame error rate) → acoustic model missing prosody cues.
  - Low recall on revisions/restarts → semantic context insufficient in acoustic or language stream.
  - Fusion not outperforming unimodal → modality misalignment or feature redundancy.

- **First 3 experiments**:
  1. Compare WavLM-FT vs. Whisper-FT vs. Azure-OTS transcript quality on Switchboard verbatim disfluency task (WER, FER-D).
  2. Evaluate acoustic-only WavLM vs. language-only BERT (with Whisper-FT) on frame-level disfluency detection.
  3. Implement BLSTM fusion and compare F1/recall against unimodal baselines across disfluency classes.

## Open Questions the Paper Calls Out

- **Open Question 1**: How would fine-tuning Whisper with a larger and more diverse dataset of verbatim disfluent speech affect its performance in transcribing disfluencies compared to current results?
  - Basis in paper: [explicit] The paper notes that Whisper-OTS performs well for transcribing nondisfluent words but poorly for disfluencies, transcribing them only as "*". It hypothesizes that fine-tuning Whisper with verbatim disfluent speech could improve performance.
  - Why unresolved: The paper only fine-tuned Whisper on the Switchboard dataset, which is relatively small and limited in diversity. The impact of training on a larger, more varied dataset of disfluent speech remains untested.
  - What evidence would resolve it: Fine-tuning Whisper on a significantly larger and more diverse dataset of verbatim disfluent speech (e.g., multiple datasets, multiple speakers, various disfluency types) and comparing its WER-D, FER-D, and downstream disfluency detection performance to the current fine-tuned model.

- **Open Question 2**: What is the impact of incorporating prosodic features (e.g., pitch, intensity, duration) into the multimodal BLSTM architecture for disfluency detection and categorization?
  - Basis in paper: [inferred] Previous work (e.g., [14], [46], [47], [60]) has shown that combining language and prosody features can improve disfluency detection. The paper presents a multimodal approach combining language and acoustic representations but does not explicitly explore prosodic features.
  - Why unresolved: The paper focuses on combining BERT language representations with WavLM acoustic representations, which primarily capture spectral and temporal information. The potential benefits of explicitly incorporating prosodic features are not investigated.
  - What evidence would resolve it: Modifying the multimodal BLSTM architecture to include prosodic features (extracted from the audio signal) alongside the language and acoustic representations, and comparing its performance to the current BLSTM on disfluency detection and categorization metrics.

- **Open Question 3**: How does the performance of the proposed multimodal BLSTM model generalize to spontaneous speech datasets beyond Switchboard, such as those with different disfluency patterns or speaker demographics?
  - Basis in paper: [inferred] The paper evaluates the models on the Switchboard dataset, which is a well-established corpus of telephone conversations. However, the performance of the models on other spontaneous speech datasets with potentially different characteristics is unknown.
  - Why unresolved: The Switchboard dataset may not fully represent the diversity of spontaneous speech in terms of disfluency patterns, speaker demographics, or recording conditions. The generalizability of the proposed models to other datasets is untested.
  - What evidence would resolve it: Evaluating the multimodal BLSTM model (and comparing it to unimodal approaches) on other spontaneous speech datasets with different characteristics (e.g., different disfluency patterns, speaker demographics, recording conditions) and reporting performance metrics.

## Limitations

- The study's conclusions are based on a single dataset (Switchboard), limiting generalizability to other spontaneous speech domains.
- Alignment quality between frame-level acoustic features and word-level ASR timestamps is not explicitly evaluated for its impact on fusion performance.
- Despite fine-tuning, Whisper-FT still underperforms manual transcripts in downstream disfluency detection, suggesting fundamental limitations in verbatim disfluency transcription.

## Confidence

- **High Confidence**: The finding that WavLM outperforms language-only BERT when ASR transcript quality is poor is well-supported by the experimental evidence and aligns with established findings about acoustic models' robustness to transcription errors.
- **Medium Confidence**: The claim that BLSTM fusion improves over unimodal approaches is supported by reported metrics but lacks ablation studies showing which component (BLSTM vs concatenation vs feature choice) drives improvements.
- **Low Confidence**: The assertion that Whisper fine-tuning yields "strong results" is questionable given the substantial performance gap between manually transcribed and Whisper-FT transcripts in downstream detection tasks.

## Next Checks

1. **Alignment Stress Test**: Systematically introduce controlled timestamp jitter (1-25ms) between acoustic and language modalities and measure degradation in multimodal F1 scores to quantify sensitivity to alignment quality.

2. **Domain Generalization Study**: Evaluate the same multimodal architecture on at least two out-of-domain datasets (e.g., clinical interviews and conversational speech from non-native speakers) to test robustness of the fusion approach.

3. **Feature Ablation Analysis**: Remove the BLSTM fusion layer and compare direct concatenation + classification versus other fusion methods (transformer, attention, late fusion) to isolate the specific contribution of the BLSTM architecture.