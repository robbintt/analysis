---
ver: rpa2
title: Statistical Knowledge Assessment for Large Language Models
arxiv_id: '2305.10519'
source_url: https://arxiv.org/abs/2305.10519
tags:
- knowledge
- karr
- prompts
- assessment
- relation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes KaRR, a statistical approach to assess factual
  knowledge in generative language models. KaRR measures the ratio of the likelihood
  of a model generating correct answers to varying prompts versus random chances.
---

# Statistical Knowledge Assessment for Large Language Models

## Quick Facts
- arXiv ID: 2305.10519
- Source URL: https://arxiv.org/abs/2305.10519
- Reference count: 21
- Key outcome: KaRR method achieves 0.43 Kendall's τ correlation with human assessment for GLM knowledge evaluation

## Executive Summary
This paper introduces KaRR (Knowledge Assessment Risk Ratio), a statistical approach for evaluating factual knowledge in generative language models. The method measures the ratio of likelihood of generating correct answers versus random chances across varying prompts. Unlike previous methods that use single templates, KaRR uses multiple text aliases for entities and relations to reduce spurious correlations. The authors evaluate 14 GLMs including LLaMA, Alpaca, and OPT, finding that knowledge retention follows scaling laws within architectures while instruction-tuning may compromise factual reliability.

## Method Summary
KaRR computes a ratio comparing the likelihood of a model generating correct entity text given a subject-relation pair versus generating it by chance. It uses a Bayesian network with latent variables for subject, relation, and object, along with their observable text forms (aliases). The method aggregates probabilities over diverse aliases rather than relying on single templates, reducing prompt-specific biases. For each fact, KaRR calculates two components - the impact of specifying the subject (KaRRs) and the impact of specifying the relation (KaRRr) - then combines them using a geometric mean. The approach is evaluated on a large-scale assessment suite with 994,123 entities, 600 relations, and 1,395,905 text aliases from T-REx and Wikidata knowledge graphs.

## Key Results
- KaRR achieves 0.43 Kendall's τ correlation with human assessment
- Knowledge retention in models with same architecture follows scaling law
- Instruction-tuning can compromise factual reliability in GLMs
- Method is robust to prompt variation and less affected by spurious correlations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: KaRR computes a ratio comparing the likelihood of generating correct entity text given a subject-relation pair versus generating it by chance, isolating knowledge reliability from spurious correlations.
- **Mechanism**: Uses Bayesian network with latent variables for subject, relation, and object, and observable text forms. Decomposes joint probability into conditional probabilities over text aliases, estimating P(o|s,r) versus P(o|s) or P(o|r) using MCMC sampling to approximate the denominator.
- **Core assumption**: Probability of generating an object given subject and relation can be accurately estimated by summing over all relation aliases and their associated object aliases.
- **Evidence anchors**: [abstract] KaRR measures "the ratio of the likelihood of a model generating correct answers to varying prompts versus random chances"; [section] "We introduce three latent variables to represent subject, object and relation, and a Bayesian network to represent the causal dependency among them and generated text."

### Mechanism 2
- **Claim**: Multiple text aliases reduce impact of prompt-specific biases and spurious correlations.
- **Mechanism**: Aggregates probabilities over large set of aliases for each entity and relation rather than relying on single template, smoothing out idiosyncrasies of individual surface forms.
- **Core assumption**: Alias set is sufficiently diverse and representative that averaging cancels out prompt-specific biases.
- **Evidence anchors**: [section] "We use our method to evaluate 14 GLMs of various sizes... Our assessment suite contains a comprehensive set of 994,123 entities and 600 relations, with 1,395,905 text aliases."

### Mechanism 3
- **Claim**: Geometric mean of KaRRs and KaRRr captures joint impact of subject and relation on object generation, providing balanced measure of knowledge reliability.
- **Mechanism**: KaRRs measures impact of specifying subject, KaRRr measures impact of specifying relation; geometric mean combines these effects into single score.
- **Core assumption**: Product of two ratios is meaningful measure of overall knowledge reliability, and geometric mean appropriately balances their contributions.
- **Evidence anchors**: [section] "To effectively represent the combined impact of KaRRs and KaRRr, we calculate the joint influence as the geometric mean of them, and name it Knowledge Assessment Risk Ratio (KaRR)."

## Foundational Learning

- **Concept**: Bayesian networks and probabilistic graphical models
  - Why needed here: KaRR relies on Bayesian network to represent causal dependencies among latent variables (subject, relation, object) and observable text forms. Understanding how to construct and use such models is essential.
  - Quick check question: How does a Bayesian network differ from simple probabilistic model, and why is it useful for representing knowledge assessment?

- **Concept**: Markov Chain Monte Carlo (MCMC) sampling
  - Why needed here: KaRR uses MCMC sampling to approximate denominators in ratio calculations, as computing over all possible text forms is intractable. Knowing how MCMC works and its limitations is crucial.
  - Quick check question: What are key assumptions of MCMC sampling, and how might they affect accuracy of KaRR?

- **Concept**: Spurious correlations and bias in language models
  - Why needed here: KaRR designed to be robust to spurious correlations that can inflate probing results in other methods. Understanding what spurious correlations are and how they arise is important.
  - Quick check question: What are common sources of spurious correlations in language model probing, and how does KaRR mitigate them?

## Architecture Onboarding

- **Component map**: Knowledge graph (T-REx, Wikidata) → Symbolic facts (triplets) → Alias datastore → Multiple text forms for entities and relations → Graphical model → Latent variables (S, R, O) and observable text forms → MCMC sampling → Approximate denominators in KaRR calculation → GLM → Generate probabilities over text forms → Evaluation suite → Compute KaRR scores and compare to human assessment

- **Critical path**: 1. Sample fact (s, r, o) from knowledge graph 2. Retrieve all aliases for s, r, and o from datastore 3. For each alias combination, generate model's probability distribution over object aliases 4. Use MCMC sampling to approximate denominators in KaRR calculation 5. Compute KaRRr and KaRRs for fact 6. Take geometric mean to get final KaRR score 7. Repeat for all facts and aggregate to get overall score for GLM

- **Design tradeoffs**: Alias diversity vs computational cost (more aliases improves robustness but increases computation); MCMC sampling accuracy vs speed (more samples give better approximations but take longer); geometric mean vs arithmetic mean (geometric mean balances ratios but may be less intuitive)

- **Failure signatures**: Low variance across prompts but poor correlation with human assessment (method not capturing right aspects of knowledge reliability); high sensitivity to few aliases (alias set not diverse enough); MCMC sampling not converging (model's probability distribution too complex or sampling parameters not well-tuned)

- **First 3 experiments**: 1. Verify alias retrieval process (check number and diversity of aliases retrieved matches expectations) 2. Test MCMC sampling (run on small set of facts, check approximations are stable and close to true values) 3. Compare KaRR to human assessment on small scale (manually assess knowledge reliability on few facts using human annotation, check if KaRR scores align with human judgments)

## Open Questions the Paper Calls Out

- **Open Question 1**: How does KaRR method perform when applied to larger and more diverse knowledge graphs beyond T-REx and Wikidata?
  - Basis in paper: [explicit] Paper mentions KaRR implemented based on T-REx and Wikidata but does not explore performance on other knowledge graphs.
  - Why unresolved: Paper does not provide information on how KaRR would perform on different knowledge graphs, limiting generalizability.
  - What evidence would resolve it: Evaluating KaRR on variety of knowledge graphs and comparing performance would provide insights into effectiveness across different domains and data sources.

- **Open Question 2**: How does KaRR method handle ambiguous or multi-faceted knowledge entities and relations?
  - Basis in paper: [inferred] Paper mentions use of aliases and templates but does not explicitly discuss how KaRR handles ambiguity or multi-faceted knowledge.
  - Why unresolved: Paper does not address how KaRR deals with complex or ambiguous knowledge, which could impact accuracy and reliability.
  - What evidence would resolve it: Conducting experiments with ambiguous or multi-faceted knowledge entities and relations would help determine how well KaRR handles such cases.

- **Open Question 3**: How does KaRR method compare to other knowledge assessment methods in terms of computational efficiency and scalability?
  - Basis in paper: [inferred] Paper mentions KaRR can be time-consuming due to need for GLM inference on various prompts but does not provide detailed comparison with other methods.
  - Why unresolved: Paper does not provide comprehensive analysis of KaRR's computational efficiency and scalability compared to other knowledge assessment methods.
  - What evidence would resolve it: Benchmarking KaRR against other knowledge assessment methods in terms of computational resources, time, and scalability would provide insights into efficiency and practical applicability.

## Limitations

- Alias quality and coverage not fully characterized, which could systematically skew KaRR scores
- MCMC sampling with K=4 samples per fact may not provide sufficient accuracy for stable estimates
- Geometric mean choice assumes subject and relation ratios are independent and equally important

## Confidence

**High Confidence Claims**:
- Bayesian network framework provides principled approach to decompose knowledge assessment
- Multiple aliases reduce prompt-specific biases compared to single-template methods
- Scaling law finding for knowledge retention within model architectures is supported

**Medium Confidence Claims**:
- KaRR's 0.43 Kendall's τ correlation with human assessment demonstrates validity
- Instruction-tuning's negative impact on factual reliability is reliably detected
- Method's robustness to spurious correlations is practically significant

**Low Confidence Claims**:
- KaRR is universally superior to all existing knowledge assessment methods
- Geometric mean is optimal way to combine subject and relation ratios
- Specific MCMC sampling parameters (K=4) are optimal across all model scales

## Next Checks

1. **Alias quality audit**: Manually examine stratified sample of 100 entity/relation alias sets to assess diversity, relevance, and potential biases. Check if certain alias types (e.g., Wikipedia titles vs colloquial names) dominate.

2. **MCMC convergence analysis**: For subset of 50 facts, run MCMC sampling with increasing samples (K=4, 8, 16, 32) and measure stability of KaRR scores. Determine if K=4 provides sufficient convergence or if larger values needed.

3. **Geometric mean sensitivity test**: Compute alternative aggregation methods (arithmetic mean, weighted average) for subject and relation ratios on same evaluation set. Compare resulting model rankings and human correlation to assess if geometric mean choice materially affects conclusions.