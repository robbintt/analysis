---
ver: rpa2
title: Playing repeated games with Large Language Models
arxiv_id: '2305.16867'
source_url: https://arxiv.org/abs/2305.16867
tags:
- games
- player
- other
- option
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Language Models (LLMs) are increasingly used in applications
  where they interact with humans and other agents. We propose to use behavioural
  game theory to study LLM's cooperation and coordination behaviour.
---

# Playing repeated games with Large Language Models

## Quick Facts
- arXiv ID: 2305.16867
- Source URL: https://arxiv.org/abs/2305.16867
- Reference count: 11
- Key outcome: LLMs show distinct behavioral patterns in repeated games - excelling at self-interested games but struggling with coordination tasks

## Executive Summary
This paper explores how Large Language Models (LLMs) behave in repeated strategic interactions using behavioral game theory. The authors test GPT-3, GPT-3.5, and GPT-4 across various 2×2 games, including Prisoner's Dilemma and Battle of the Sexes. The study reveals that LLMs perform particularly well in games favoring self-interest but struggle with coordination tasks. The authors also demonstrate that GPT-4's behavior can be modulated through prompt engineering, specifically using a "social chain-of-thought" strategy that improves coordination with human players.

## Method Summary
The researchers used prompt-chaining to simulate repeated game interactions, where LLMs received game descriptions and histories through in-context learning without fine-tuning. They tested different LLM models (GPT-3, GPT-3.5, GPT-4) across 2×2 games including Prisoner's Dilemma variants and coordination games. Games were played for 10 rounds with temperature=0 for deterministic outputs. The study compared LLM performance against human-like strategies and actual human players, measuring normalized performance scores and analyzing behavioral patterns.

## Key Results
- GPT-4 outperforms smaller models in self-interested games like Prisoner's Dilemma
- All LLMs struggle with coordination games like Battle of the Sexes, failing to achieve optimal outcomes
- Social chain-of-thought prompting improves GPT-4's coordination behavior and performance with human players
- Behavioral signatures are stable across robustness checks including different framings and payoff variations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can learn to cooperate and coordinate through repeated interactions in 2x2 games via in-context learning
- Mechanism: The paper uses prompt-chaining to simulate repeated game interactions, where the LLM updates its strategy based on the history of previous rounds within the same prompt
- Core assumption: LLMs can effectively use in-context learning to adapt their behavior based on the provided history of interactions
- Evidence anchors:
  - [abstract] "We let different LLMs (GPT-3, GPT-3.5, and GPT-4) play finitely repeated games with each other and with other, human-like strategies."
  - [section] "We let two LLMs interact via prompt-chaining... all integration of evidence and learning about past interactions happens as in-context learning"
  - [corpus] Weak - no direct evidence in related papers about in-context learning for repeated games
- Break condition: If the prompt length exceeds the context window or if the LLM fails to maintain coherence across multiple rounds

### Mechanism 2
- Claim: GPT-4 exhibits more strategic behavior than smaller models in competitive games
- Mechanism: GPT-4's larger parameter size and training data allow it to better understand and execute optimal strategies in games like the Prisoner's Dilemma
- Core assumption: Model size correlates with strategic reasoning ability in game-theoretic contexts
- Evidence anchors:
  - [abstract] "Our results show that LLMs generally perform well in such tasks and also uncover persistent behavioral signatures."
  - [section] "Analyzing their performance across different families of games, we find that they perform remarkably well in games that value pure self-interest"
  - [corpus] Weak - no direct evidence in related papers about size-performance correlation in game theory
- Break condition: If smaller models with different architectures outperform GPT-4 in similar tasks

### Mechanism 3
- Claim: LLMs can be influenced by prompt engineering to improve coordination behavior
- Mechanism: By explicitly asking the LLM to predict the opponent's actions before making its own choice, the LLM engages in a form of "social chain-of-thought" that improves coordination
- Core assumption: LLMs can simulate theory of mind through explicit prompting strategies
- Evidence anchors:
  - [abstract] "we show how GPT-4's behaviour can be modified by providing further information about the other player as well as by asking it to predict the other player's actions before making a choice."
  - [section] "Thus, we implemented a version of this reasoning through actions by asking LLMs to imagine the possible actions and their outcomes before making a decision."
  - [corpus] Weak - no direct evidence in related papers about prompt engineering for coordination
- Break condition: If the LLM ignores the prediction step or if coordination does not improve despite following the prompt structure

## Foundational Learning

- Concept: Game Theory
  - Why needed here: The paper relies on understanding strategic interactions, Nash equilibria, and different game types (e.g., Prisoner's Dilemma, Battle of the Sexes)
  - Quick check question: What is the difference between a zero-sum game and a non-zero-sum game?

- Concept: Reinforcement Learning
  - Why needed here: The paper explores how LLMs learn optimal strategies through repeated interactions, similar to RL agents
  - Quick check question: How does in-context learning in LLMs compare to traditional RL approaches in terms of learning efficiency?

- Concept: Theory of Mind
  - Why needed here: The paper investigates whether LLMs can reason about other agents' beliefs and intentions, which is crucial for coordination
  - Quick check question: What are the key components of theory of mind, and how might they manifest in LLM behavior?

## Architecture Onboarding

- Component map:
  Game Simulator -> Prompt Generator -> LLM Interface -> Analysis Module

- Critical path:
  1. Generate game prompt with history
  2. Send prompt to LLM
  3. Receive and parse LLM response
  4. Update game state and scores
  5. Repeat for next round

- Design tradeoffs:
  - Prompt length vs. context window limitations
  - Number of rounds vs. computational cost
  - Model size vs. response time

- Failure signatures:
  - Inconsistent responses across rounds
  - Failure to maintain game state
  - Suboptimal strategies that don't improve over time

- First 3 experiments:
  1. Test basic 2x2 game functionality with GPT-4
  2. Compare GPT-4 vs. GPT-3.5 performance in Prisoner's Dilemma
  3. Implement and test the "social chain-of-thought" prompt engineering technique

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the underlying mechanisms that lead LLMs to perform poorly in coordination games like the Battle of the Sexes?
- Basis in paper: [explicit] The paper shows that GPT-4 fails to coordinate with a simple alternating strategy in the Battle of the Sexes, but it can predict the alternating pattern when explicitly asked
- Why unresolved: The paper does not investigate the specific cognitive mechanisms or training processes that cause this inability to coordinate
- What evidence would resolve it: Experiments that manipulate the LLM's training data, architecture, or prompting to isolate the factors contributing to poor coordination performance

### Open Question 2
- Question: Can the behavioral signatures observed in LLMs playing repeated games generalize to other social interaction paradigms?
- Basis in paper: [inferred] The paper focuses on a specific set of 2x2 games and does not explore whether the observed behavioral patterns (e.g., unforgivingness in Prisoner's Dilemma, inability to coordinate) extend to other types of social interactions
- Why unresolved: The paper's scope is limited to the studied games, and it does not investigate the broader applicability of the findings
- What evidence would resolve it: Testing LLMs in a wider range of social interaction paradigms, such as multi-agent environments, negotiations, or social dilemmas with different payoff structures

### Open Question 3
- Question: How can we develop LLM training objectives that encourage cooperative and coordinated behavior in social interactions?
- Basis in paper: [explicit] The paper suggests that prompting GPT-4 to make predictions about the other player's actions can improve its coordination, but it does not explore more fundamental ways to shape LLM behavior
- Why unresolved: The paper focuses on understanding LLM behavior rather than developing new training methods or objectives
- What evidence would resolve it: Designing and testing new LLM training objectives that explicitly reward cooperative and coordinated behavior, and evaluating their effectiveness in improving LLM performance in social interactions

## Limitations

- The study relies exclusively on in-context learning without fine-tuning, which may not capture the full range of LLM behavioral capabilities
- The experimental design is limited to 2x2 games, which may not generalize to more complex strategic scenarios
- The paper does not investigate the underlying mechanisms that cause coordination failures in games like Battle of the Sexes

## Confidence

- GPT-4's superior performance in self-interested games: High confidence
- LLMs' coordination failures in Battle of the Sexes: Medium confidence
- Effectiveness of social chain-of-thought prompting: Medium confidence

## Next Checks

1. **Robustness Testing**: Replicate the experiments with a wider variety of game types (e.g., 3x3 games, games with more than two players) to test the generalizability of the observed behavioral patterns

2. **Fine-tuning Impact**: Conduct experiments comparing in-context learning results with fine-tuned models to determine if the observed behavioral signatures persist under different learning paradigms

3. **Prompt Engineering Variations**: Systematically test alternative prompt engineering techniques beyond social chain-of-thought to identify the most effective methods for improving LLM coordination behavior across different game types