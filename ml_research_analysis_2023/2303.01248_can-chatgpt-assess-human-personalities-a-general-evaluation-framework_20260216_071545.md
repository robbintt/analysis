---
ver: rpa2
title: Can ChatGPT Assess Human Personalities? A General Evaluation Framework
arxiv_id: '2303.01248'
source_url: https://arxiv.org/abs/2303.01248
tags:
- uni00000026
- uni00000013
- uni00000014
- uni0000003a
- uni00000052
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores the feasibility of assessing human personalities\
  \ using Large Language Models (LLMs) such as ChatGPT. A general evaluation framework\
  \ is proposed, which reformulates Myers\u2013Briggs Type Indicator (MBTI) questions\
  \ into unbiased prompts, subject-replaced queries, and correctness-evaluated instructions\
  \ to enable flexible personality assessments."
---

# Can ChatGPT Assess Human Personalities? A General Evaluation Framework

## Quick Facts
- arXiv ID: 2303.01248
- Source URL: https://arxiv.org/abs/2303.01248
- Reference count: 40
- Key outcome: This paper explores the feasibility of assessing human personalities using Large Language Models (LLMs) such as ChatGPT. A general evaluation framework is proposed, which reformulates Myers–Briggs Type Indicator (MBTI) questions into unbiased prompts, subject-replaced queries, and correctness-evaluated instructions to enable flexible personality assessments. Three quantitative metrics are introduced to measure consistency, robustness, and fairness of LLM assessments. Experiments on state-of-the-art LLMs (ChatGPT, InstructGPT) show that both can independently assess human personalities. ChatGPT achieves higher consistency and fairness with less gender bias compared to InstructGPT, though its results are more sensitive to prompt biases. The study demonstrates LLMs' potential in personality analysis and suggests implications for developing human-centered, trustworthy AI.

## Executive Summary
This paper investigates whether Large Language Models (LLMs) like ChatGPT can reliably assess human personalities. The authors propose a general evaluation framework that reformulates MBTI questions to minimize bias and enable group-level personality inference. Three key innovations are introduced: unbiased prompts with randomized answer options, subject-replaced queries for flexible target group assessment, and correctness-evaluated instructions to elicit clearer responses. The framework is validated using two state-of-the-art LLMs, demonstrating that ChatGPT achieves higher consistency and fairness compared to InstructGPT, albeit with greater sensitivity to prompt biases. The study highlights both the promise and challenges of using LLMs for personality assessment and lays the groundwork for future human-centered AI systems.

## Method Summary
The method involves reformulating MBTI personality questions to create unbiased prompts by randomly permuting answer options and averaging results over multiple independent testings. Subject-replaced queries are used to assess personality traits of target groups (e.g., "Men" or "Women") by replacing the generic "You" with specific subjects. Correctness-evaluated instructions convert self-neutralizing prompts into analyzable outputs by mapping "agree/disagree" to "correct/wrong". The LLM outputs are parsed and mapped to MBTI personality types, and three quantitative metrics—consistency, robustness, and fairness—are calculated to evaluate the quality of personality assessments.

## Key Results
- ChatGPT achieves higher consistency and fairness in personality assessments compared to InstructGPT, with less gender bias.
- Both ChatGPT and InstructGPT can independently assess human personalities using the proposed framework.
- The results are more sensitive to prompt biases in ChatGPT than in InstructGPT.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unbiased prompts with randomly permuted options and averaging results reduce LLM response variability.
- Mechanism: By randomizing the order of MBTI answer options in each prompt and averaging across multiple independent testings, the influence of prompt bias (sensitivity to option order) is mitigated, leading to more consistent and impartial personality assessments.
- Core assumption: LLM response distributions are sensitive to the lexical order of options, and averaging across randomized orders smooths out these biases.
- Evidence anchors:
  - [abstract]: "we first devise unbiased prompts by randomly permuting options in MBTI questions and adopt the average testing result to encourage more impartial answer generation."
  - [section 3.1]: "we propose to design unbiased prompts for the input questions. In particular, for each question in an independent testing...we randomly permute all available options...and adopt the average results of multiple independent testings as the final result."
  - [corpus]: Weak; no direct evidence of permuted-option prompting improving consistency in related works.
- Break condition: If LLM's output is invariant to option ordering, or if averaging over small N yields unstable results.

### Mechanism 2
- Claim: Subject-replaced queries enable LLMs to infer personality traits of target human groups rather than self-report.
- Mechanism: By replacing the generic "You" subject in MBTI statements with a specific group label (e.g., "Men", "Barbers"), the LLM is prompted to analyze the inferred MBTI responses of that group based on its knowledge base.
- Core assumption: LLMs encode stereotypical or aggregated knowledge about social groups that can be surfaced via conditional prompting.
- Evidence anchors:
  - [section 3.2]: "we propose the subject-replaced query (SRQ) by converting the original subject...into a specific subject-of-interest...to enable flexible queries and assessments from LLMs."
  - [section 5.1]: "ChatGPT achieves more consistent and fairer assessments in spite of lower robustness against prompt biases compared with InstructGPT."
  - [corpus]: Weak; related papers mention personality evaluation but not subject-replaced querying for group inference.
- Break condition: If the LLM lacks coherent group-level knowledge, or if group stereotypes are too noisy to yield interpretable personality types.

### Mechanism 3
- Claim: Correctness-evaluated instructions convert self-neutralizing prompts into analyzable outputs.
- Mechanism: By reformulating MBTI instructions from "Do you agree/disagree..." to "Is it correct/wrong...", the LLM is guided to give a determinate stance on the statement rather than defaulting to neutrality.
- Core assumption: LLMs trained to avoid self-assertion will default to neutral responses unless the instruction frame forces a correctness judgment.
- Evidence anchors:
  - [section 3.3]: "we propose to convert the original agreement-measured instruction...into correctness-evaluated instruction (CEI)...to obtain clearer responses."
  - [section 5.1]: "some assessment results from ChatGPT are close to our intuition: (1) Accountants are assessed as 'Logistician'..."
  - [corpus]: Weak; no corpus evidence of correctness-based reformulation improving clarity in personality tasks.
- Break condition: If the correctness framing introduces new bias or if the LLM still defaults to neutrality.

## Foundational Learning

- Concept: MBTI personality dimensions and scoring
  - Why needed here: The framework relies on MBTI's 5 dimensions (E/I, N/S, T/F, J/P, A/T) to map LLM outputs to interpretable personality types.
  - Quick check question: How many binary dimensions does the MBTI framework use, and what is the resulting number of possible personality types?

- Concept: LLM prompt sensitivity and prompt engineering
  - Why needed here: Understanding how subtle changes in prompt wording or option ordering affect LLM outputs is critical for designing unbiased assessments.
  - Quick check question: What are two documented causes of variability in LLM outputs when given the same semantic prompt with different surface forms?

- Concept: Evaluation metrics for consistency, robustness, and fairness
  - Why needed here: The proposed metrics (Euclidean similarity, robustness against permuted options, gender fairness comparison) operationalize the quality of LLM personality assessments.
  - Quick check question: How is the consistency score computed from multiple independent testings in this framework?

## Architecture Onboarding

- Component map:
  1. Unbiased prompt generator (randomizes MBTI option order)
  2. Subject-replaced query engine (substitutes "You" with target group)
  3. Correctness-evaluated instruction formatter (maps agree/disagree → correct/wrong)
  4. LLM query interface (submits prompts, collects raw answers)
  5. Answer parser (maps raw LLM text to MBTI options)
  6. Scoring aggregator (averages over N independent testings)
  7. Metric calculator (consistency, robustness, fairness)

- Critical path:
  Prompt construction → LLM query → Answer parsing → Averaging → Metric computation

- Design tradeoffs:
  - More testings (larger N) → higher consistency but higher latency/cost
  - Randomizing option order → reduces bias but increases variance per run
  - Subject choice → broader groups reduce noise but may oversimplify

- Failure signatures:
  - High variance across independent runs → prompt bias not fully mitigated
  - All "neither correct nor wrong" responses → correctness framing ineffective
  - Disproportionate gender bias in fairness scores → underlying LLM bias

- First 3 experiments:
  1. Run baseline MBTI prompt without unbiased design → measure option distribution variance
  2. Run subject-replaced query for "Men" vs "Women" → compare fairness scores
  3. Vary N (number of independent testings) → observe convergence of consistency score

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do personality assessments of LLMs vary across different cultural contexts and languages?
- Basis in paper: [inferred] The paper briefly mentions that ChatGPT may generate more biased answers in non-English language settings, suggesting potential cultural influences on personality assessments.
- Why unresolved: The study primarily focuses on English language assessments, limiting generalizability to other cultural contexts.
- What evidence would resolve it: Comparative experiments using MBTI assessments across multiple languages and cultural contexts, measuring consistency, robustness, and fairness scores for each.

### Open Question 2
- Question: What is the optimal number of independent testings required to achieve stable and reliable personality assessment results from LLMs?
- Basis in paper: [explicit] The authors note that response delays and network issues hinder large-scale testings, and they used 15 testings in their experiments without exploring the impact of different numbers.
- Why unresolved: The study uses a fixed number of testings without examining how results converge or vary with different sample sizes.
- What evidence would resolve it: Systematic experiments varying the number of testings (e.g., 5, 10, 15, 20, 30) and analyzing how consistency, robustness, and fairness metrics stabilize.

### Open Question 3
- Question: Can LLMs provide accurate personality assessments for specific individuals with sufficient personal background information?
- Basis in paper: [explicit] The authors acknowledge that LLMs struggle to assess specific individuals like Barack Obama due to insufficient personal information in their knowledge base.
- Why unresolved: The study only tests assessments of general groups rather than exploring methods to enable individual assessments.
- What evidence would resolve it: Experiments providing detailed personal background prompts to LLMs and comparing their assessments against known personality profiles of specific individuals.

### Open Question 4
- Question: How do different personality assessment frameworks (e.g., Big Five vs. MBTI) affect the consistency and fairness of LLM-generated personality evaluations?
- Basis in paper: [inferred] The authors mention that MBTI is used as a representative personality measure but suggest future work could explore other tests like Big Five Inventory under their framework.
- Why unresolved: The study is limited to MBTI without comparing results across different personality assessment frameworks.
- What evidence would resolve it: Parallel experiments using multiple personality assessment frameworks with identical subjects, comparing consistency, robustness, and fairness metrics across frameworks.

## Limitations

- The framework's generalizability beyond MBTI is untested; it's unclear if similar unbiased prompting and correctness framing would work for other personality frameworks.
- The effectiveness of subject-replaced queries relies on LLMs having coherent, unbiased group-level knowledge, which may not hold across all cultural or demographic contexts.
- The proposed metrics (consistency, robustness, fairness) are novel and lack established benchmarks for comparison.

## Confidence

- High confidence in the mechanism of averaging across permuted option orders to reduce prompt bias, supported by the described experimental setup.
- Medium confidence in the subject-replaced query mechanism, as it's novel and relies on unstated LLM capabilities.
- Low confidence in the correctness-evaluated instruction mechanism's universal applicability, as it's a specific framing that may not generalize.

## Next Checks

1. Test the unbiased prompt design (permuted options + averaging) on a different personality assessment framework to verify generalizability.
2. Conduct a bias audit of LLM outputs for a diverse set of subject-replaced queries (e.g., various professions, cultures) to assess the robustness of the subject-replaced mechanism.
3. Experiment with varying the number of independent testings (N) to determine the point of diminishing returns for consistency score improvement and to establish optimal N for practical use.