---
ver: rpa2
title: Improving the Gap in Visual Speech Recognition Between Normal and Silent Speech
  Based on Metric Learning
arxiv_id: '2305.14203'
source_url: https://arxiv.org/abs/2305.14203
tags:
- speech
- silent
- normal
- data
- viseme
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving visual speech recognition
  (VSR) accuracy for silent speech, which suffers from a performance gap compared
  to normal speech due to differences in lip movements. The proposed method leverages
  visemes as shared representations between normal and silent speech and employs metric
  learning to map the probability distributions of both speech types closer in a latent
  space when they share the same viseme.
---

# Improving the Gap in Visual Speech Recognition Between Normal and Silent Speech Based on Metric Learning

## Quick Facts
- arXiv ID: 2305.14203
- Source URL: https://arxiv.org/abs/2305.14203
- Reference count: 0
- Primary result: Proposed method improves silent speech VSR accuracy by nearly 3% in word error rate compared to baseline.

## Executive Summary
This paper addresses the performance gap in visual speech recognition (VSR) between normal and silent speech, which arises from differences in lip movements when vocalization is absent. The proposed method leverages visemes as shared representations between speech types and employs metric learning to align their probability distributions in latent space. Experimental results on the A V Digits dataset demonstrate that the approach reduces word error rate by nearly 3% and achieves comparable performance with only half the training data for silent speech.

## Method Summary
The method uses a two-stage VSR architecture with video-to-viseme and viseme-to-text components. The visual front-end extracts temporal features using 3D convolutions and spatial features using a ResNet-18. A transformer-based viseme classifier predicts viseme probabilities, which are then processed by a GRU-based language model with attention. The key innovation is metric learning that minimizes KL divergence between viseme probability distributions of normal and silent speech, as well as within each speech type, to reduce the performance gap and intra-class variance.

## Key Results
- Nearly 3% improvement in word error rate (WER) for silent speech VSR compared to baseline.
- Model trained with only 400 silent speech samples achieves comparable performance to baseline trained with twice the data.
- Metric learning approach effectively reduces the performance gap between normal and silent speech recognition.

## Why This Works (Mechanism)

### Mechanism 1
Visemes serve as shared representations between normal and silent speech, allowing the model to learn invariant features across speech types. The model maps probability distributions of normal and silent speech close in latent space when they share the same viseme identity, using metric learning to minimize KL divergence. Core assumption: Visemes are uniquely derived from phonemes and words, making them shared representations regardless of vocalization differences.

### Mechanism 2
Metric learning between speech types reduces the performance gap by aligning viseme probability distributions. The model minimizes KL divergence between predicted viseme distributions of normal and silent speech, treating them as variants of the same underlying speech representation. Core assumption: The viseme probability distributions should be similar regardless of speech type when they represent the same literal content.

### Mechanism 3
Metric learning within each speech type reduces intra-class variance and improves viseme classification accuracy. The model brings all viseme distributions close to a representative distribution within each speech type, reducing variance caused by speaker or environmental differences. Core assumption: Even when pronouncing the same word in the same speech type, differences in speaker and location can cause variations in lip movements.

## Foundational Learning

- Concept: Metric learning and KL divergence
  - Why needed here: The method relies on minimizing KL divergence between viseme probability distributions to learn shared representations
  - Quick check question: What is the mathematical definition of KL divergence and how does it measure similarity between probability distributions?

- Concept: Viseme-phoneme mapping and viseme systems
  - Why needed here: The approach depends on accurate viseme representations derived from phonemes to serve as shared features
  - Quick check question: How are visemes defined and how do they differ from phonemes in terms of visual discriminability?

- Concept: Two-stage VSR architecture
  - Why needed here: The method uses a visual model to predict visemes and a language model to generate text, requiring understanding of this architecture
  - Quick check question: What are the advantages of separating visual-to-viseme and viseme-to-text components versus end-to-end approaches?

## Architecture Onboarding

- Component map: Video input → Visual front-end (3D CNN + ResNet-18) → Viseme classifier (Transformer) → KL divergence metric learning → Language model (GRU with attention) → Text output
- Critical path: Video input → Visual front-end → Viseme classifier → KL divergence metric learning → Language model → Text output
- Design tradeoffs: Two-stage vs end-to-end (tradeoff between data efficiency and potential information loss), choice of viseme system vs sub-word units, metric learning weight balancing
- Failure signatures: High VER/WER on silent speech despite normal speech performance, unstable KL divergence values during training, poor convergence when combining multiple loss terms
- First 3 experiments:
  1. Baseline comparison: Train only on normal speech (LNCE) to establish the performance gap
  2. Inter-speech metric learning: Add LKL or LWKL to baseline to measure gap reduction
  3. Within-speech metric learning: Add LNKL and/or LSKL to measure intra-class variance reduction

## Open Questions the Paper Calls Out

- How does the performance of the proposed method scale with even larger datasets for normal speech beyond the 2,560 samples used in this study?
- Can the proposed method be extended to handle silent speech with vocabulary not included in the 10 phrases used in this study?
- How does the proposed method perform when applied to other languages or dialects that may have different viseme-phoneme mappings?

## Limitations

- Experimental validation limited to a single-digit-recognition dataset (A V Digits) with only 10-word vocabulary
- Data augmentation strategy creates asymmetry by augmenting normal speech but not silent speech
- Key implementation details like viseme-to-phoneme mapping and weighted KL divergence calculation are not fully specified

## Confidence

**High Confidence**: The mechanism of visemes serving as shared representations between normal and silent speech is well-established in the literature, and the general framework of using metric learning to align distributions across domains is theoretically sound.

**Medium Confidence**: The effectiveness of KL divergence minimization for aligning viseme distributions and reducing intra-class variance is supported by the reported WER improvements, though specific implementation details are not fully detailed.

**Low Confidence**: The claim that training with only 400 silent speech samples achieves comparable performance to baseline models trained with twice the data is based on limited experimental evidence and may not generalize to other datasets or vocabulary sizes.

## Next Checks

1. Evaluate the proposed method on a different VSR dataset with a larger vocabulary and more speakers to assess generalizability beyond digit recognition.

2. Conduct systematic ablation experiments to isolate the contributions of inter-speech metric learning versus within-speech metric learning to the overall performance improvement.

3. Implement an end-to-end VSR model trained on both normal and silent speech data using the same total number of samples, and compare its performance against the proposed two-stage approach.