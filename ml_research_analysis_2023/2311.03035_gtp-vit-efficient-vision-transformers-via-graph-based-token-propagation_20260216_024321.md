---
ver: rpa2
title: 'GTP-ViT: Efficient Vision Transformers via Graph-based Token Propagation'
arxiv_id: '2311.03035'
source_url: https://arxiv.org/abs/2311.03035
tags:
- token
- tokens
- graph
- layer
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a graph-based token propagation (GTP) method
  to enhance the efficiency of pre-trained Vision Transformers (ViTs) while preserving
  essential information. Inspired by graph summarization algorithms, GTP propagates
  information from less significant tokens to spatially and semantically connected
  tokens of greater importance, allowing the remaining tokens to serve as a condensed
  representation of the entire image.
---

# GTP-ViT: Efficient Vision Transformers via Graph-based Token Propagation

## Quick Facts
- arXiv ID: 2311.03035
- Source URL: https://arxiv.org/abs/2311.03035
- Authors: Multiple authors
- Reference count: 40
- Key outcome: Reduces DeiT-S and DeiT-B computational complexity by up to 26% with only 0.3% accuracy drop on ImageNet-1K, surpassing state-of-the-art token merging methods at faster inference speeds.

## Executive Summary
This paper introduces GTP-ViT, a method for improving the efficiency of pre-trained Vision Transformers through graph-based token propagation. The approach identifies less significant tokens and propagates their information to spatially and semantically connected tokens of greater importance, creating a condensed image representation without requiring fine-tuning. GTP achieves state-of-the-art trade-offs between model efficiency and performance by leveraging graph summarization algorithms and a novel token selection strategy based on regeneration difficulty and broadcasting ability.

## Method Summary
GTP-ViT operates on pre-trained ViT models by constructing sparse graphs that capture spatial and semantic relationships between tokens. It employs a token selection strategy that identifies tokens to be propagated based on their regeneration difficulty (how easily they can be regenerated by other tokens) and broadcasting ability (how much they contribute to other tokens). Less significant tokens are then summarized by propagating their features to spatially and semantically connected tokens. The method also includes attention map sparsification to mitigate oversmoothing and enhance model performance. Extensive experiments demonstrate GTP's effectiveness across multiple ViT architectures without requiring any fine-tuning.

## Key Results
- Reduces computational complexity of DeiT-S and DeiT-B by up to 26% while maintaining performance
- Achieves only 0.3% accuracy drop on ImageNet-1K with token reduction
- Outperforms state-of-the-art token merging methods with faster inference speeds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph-based token propagation preserves information of removed tokens by redistributing it to spatially and semantically connected tokens.
- Mechanism: Less significant tokens are identified and their features are propagated to spatially and semantically connected tokens of greater importance, creating a condensed representation of the image.
- Core assumption: Spatially and semantically connected tokens can effectively summarize the information of eliminated tokens.
- Evidence anchors:
  - [abstract] "Inspired by graph summarization algorithms, GTP meticulously propagates less significant tokens' information to spatially and semantically connected tokens that are of greater importance."
  - [section 3.2.3] "Motivated by the message-passing mechanism in GNNs where a node distributes its information to neighbouring nodes, we put forward the token summarization process in which an image token propagates its feature to spatially and semantically connected tokens."

### Mechanism 2
- Claim: The token selection strategy based on regeneration difficulty and broadcasting ability efficiently identifies tokens to be propagated.
- Mechanism: Tokens are scored based on their regeneration difficulty (how easily they can be regenerated by other tokens) and broadcasting ability (how much they contribute to other tokens). Tokens with low regeneration difficulty and high broadcasting ability are selected for propagation.
- Core assumption: Regeneration difficulty and broadcasting ability are good indicators of token importance for the summarization process.
- Evidence anchors:
  - [section 3.2.1] "We assume that a token is less important than others if it is primarily aggregated by other tokens during the self-attention process. These less important tokens can be dropped since they are more easily to be regenerated by other tokens and their information is less significant in token summarization results."
  - [section 3.2.1] "Despite the regeneration difficulty, an image token is also indispensable if it considerably contributes to other tokens in the self-attention computation."

### Mechanism 3
- Claim: Attention map sparsification helps concentrate token attention on the most significant signals, alleviating the smoothness of the attention map and enhancing model performance.
- Mechanism: The attention map is refined by filtering out trivial attention values, maintaining only the largest θN^2 values and assigning zero to the rest. This forces tokens to focus on the most significant information.
- Core assumption: The attention map smoothness negatively impacts model performance, and sparsification can effectively address this issue.
- Evidence anchors:
  - [section 3.2.4] "Proportional attention. After reducing the number of tokens, the vanilla softmax outputs become smoother, which could negatively impact performance. To address this issue, we introduce the proportional attention from [1] into GTP."
  - [section 3.2.4] "In addition to proportional attention, we refine the attention map by filtering out trivial attention values. In particular, we maintain the largest θN^2 values in the attention map and assign a zero value to the rest (1 − θ)N^2 elements."

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their message-passing mechanism.
  - Why needed here: GTP leverages the message-passing mechanism of GNNs to propagate information from less significant tokens to more important ones, creating a summarized representation of the image.
  - Quick check question: How does the message-passing mechanism in GNNs work, and how is it adapted in GTP for token propagation?

- Concept: Vision Transformers (ViTs) and their self-attention mechanism.
  - Why needed here: GTP operates on pre-trained ViT models and utilizes their self-attention maps to assess token importance and guide the propagation process.
  - Quick check question: How does the self-attention mechanism in ViTs work, and how are the attention maps used in GTP for token selection and propagation?

- Concept: Graph summarization algorithms.
  - Why needed here: GTP is inspired by graph summarization algorithms, which aim to generate condensed representations of a graph while preserving essential information.
  - Quick check question: What are the key principles of graph summarization algorithms, and how are they applied in GTP for token summarization?

## Architecture Onboarding

- Component map: Token embedding layer → Graph construction module → Token selection module → Token propagation module → Attention sparsification module

- Critical path: Token embedding → Graph construction → Token selection → Token propagation → Attention sparsification

- Design tradeoffs:
  - Token selection strategy: Balancing the trade-off between regeneration difficulty and broadcasting ability for accurate token importance assessment.
  - Graph construction: Choosing between spatial, semantic, or mixed graphs to capture the most relevant relationships between tokens.
  - Attention sparsification: Determining the optimal sparsity level to balance information preservation and model performance.

- Failure signatures:
  - Significant accuracy drop compared to the full-size model.
  - Slow inference speed despite token reduction.
  - Unstable training or convergence issues.

- First 3 experiments:
  1. Validate the token selection strategy by comparing the top-1 accuracy of GTP with different token selection methods (e.g., regeneration difficulty only, broadcasting ability only, random selection).
  2. Evaluate the effectiveness of graph propagation by comparing the top-1 accuracy of GTP with and without token propagation for various numbers of propagated tokens.
  3. Assess the impact of attention sparsification by comparing the top-1 accuracy of GTP with different attention sparsity levels.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of semantic neighbors (M) for different model architectures and tasks?
- Basis in paper: [explicit] The paper explores varying M values and observes performance differences, but does not determine an optimal value across different architectures.
- Why unresolved: The optimal M likely depends on model size, task complexity, and dataset characteristics, requiring systematic cross-architecture and cross-task experiments.
- What evidence would resolve it: Comprehensive experiments comparing M values across various ViT architectures (small, base, large), different vision tasks (classification, detection, segmentation), and multiple datasets, with statistical analysis of performance sensitivity to M.

### Open Question 2
- Question: How does GTP perform when applied to dense prediction tasks with very high token counts (e.g., 1024+ tokens)?
- Basis in paper: [inferred] The paper mentions token counts may exceed 1024 in dense prediction tasks but only validates on image classification tasks with fewer tokens.
- Why unresolved: Dense prediction tasks have fundamentally different token characteristics and computational constraints compared to classification, requiring task-specific evaluation.
- What evidence would resolve it: Experiments applying GTP to semantic segmentation, object detection, and instance segmentation tasks with high-resolution inputs, measuring both performance and efficiency gains relative to baselines.

### Open Question 3
- Question: What is the relationship between token propagation magnitude (α) and oversmoothing in deeper layers?
- Basis in paper: [explicit] The paper discusses oversmoothing and provides optimal α ranges for different models, but does not analyze layer-wise effects.
- Why unresolved: The propagation magnitude may need to vary across layers to balance information preservation and oversmoothing, requiring layer-specific analysis.
- What evidence would resolve it: Layer-wise analysis of token similarity distributions with varying α values, combined with performance metrics, to determine optimal α schedules across network depths.

### Open Question 4
- Question: Can the token selection strategy be extended to incorporate spatial attention patterns beyond regeneration difficulty and broadcasting ability?
- Basis in paper: [explicit] The paper presents a token selection strategy based on regeneration difficulty and broadcasting ability, but suggests these are not the only relevant factors.
- Why unresolved: Spatial attention patterns and positional information may provide additional signals for token importance that could improve selection accuracy.
- What evidence would resolve it: Comparative experiments incorporating spatial attention scores, positional encoding information, or local context into token selection, measuring improvements in accuracy-efficiency trade-offs.

### Open Question 5
- Question: How does GTP's performance scale with model size and capacity beyond DeiT-B and EViT-L?
- Basis in paper: [explicit] The paper validates GTP on DeiT-S, DeiT-B, and large models but does not systematically analyze scaling behavior.
- Why unresolved: The benefits of token summarization may vary non-linearly with model capacity, requiring analysis across the full spectrum of ViT sizes.
- What evidence would resolve it: Experiments applying GTP across model scales from tiny to giant ViTs (e.g., ViT-Tiny through ViT-G), measuring absolute and relative performance gains as a function of model capacity.

## Limitations
- Evaluation primarily conducted on ImageNet-1K, limiting generalizability to other vision tasks and datasets
- Assumes pre-trained models without investigating impact on training dynamics or performance when integrated into training pipeline
- Optimal hyperparameters (α, M, θ) not thoroughly explored, with sensitivity to different model architectures and dataset characteristics remaining unclear

## Confidence
- High Confidence: The claim that GTP reduces computational complexity (GMACs) and improves inference speed is supported by experimental results on multiple DeiT models and LV-ViT variants.
- Medium Confidence: The claim that GTP achieves a good trade-off between efficiency and performance (minimal accuracy drop) is supported by results, but the 0.3% accuracy drop is within the margin of error for ImageNet-1K evaluation.
- Low Confidence: The claim that GTP surpasses state-of-the-art token merging methods is based on a limited comparison set.

## Next Checks
1. **Ablation Study on Hyperparameters**: Conduct a comprehensive ablation study to investigate the impact of key hyperparameters (α, M, θ) on the performance of GTP across different ViT architectures and datasets.
2. **Evaluation on Diverse Datasets and Tasks**: Extend the evaluation of GTP to a wider range of vision tasks beyond ImageNet-1K, such as object detection, semantic segmentation, and video analysis.
3. **Comparison with Recent Methods**: Update the comparison with state-of-the-art token merging methods, including the most recent and relevant approaches in the literature.