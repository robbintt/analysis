---
ver: rpa2
title: 'Counting Reward Automata: Sample Efficient Reinforcement Learning Through
  the Exploitation of Reward Function Structure'
arxiv_id: '2312.11364'
source_url: https://arxiv.org/abs/2312.11364
tags:
- state
- machine
- reward
- task
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces counting reward automata (CRAs), a novel
  finite state machine variant that can model any reward function expressible as a
  formal language. Unlike previous approaches limited to regular languages, CRAs support
  tasks described by unrestricted grammars, enabling a larger set of solvable tasks
  without increased automaton complexity.
---

# Counting Reward Automata: Sample Efficient Reinforcement Learning Through the Exploitation of Reward Function Structure

## Quick Facts
- arXiv ID: 2312.11364
- Source URL: https://arxiv.org/abs/2312.11364
- Reference count: 25
- Key outcome: Counting reward automata (CRAs) extend finite state machines with counters to model any formal language reward function, achieving superior sample efficiency through counterfactual reasoning.

## Executive Summary
This paper introduces counting reward automata (CRAs), a finite state machine variant that can model any reward function expressible as a formal language by augmenting states with counter variables. Unlike previous approaches limited to regular languages, CRAs support tasks described by unrestricted grammars without increased automaton complexity. The authors present learning algorithms that exploit CRA structure for improved sample efficiency, including a counterfactual Q-learning variant. They demonstrate that CRAs can be specified from natural language task descriptions using large language models. Empirical results show CRAs outperform competing approaches in sample efficiency, automaton complexity, and task completion on benchmark problems.

## Method Summary
The authors introduce counting reward automata (CRAs) as an extension of finite state machines with counter variables that can be incremented/decremented and tested for zero. CRAs can express any formal language by tracking occurrences of events through counters. The learning framework combines the ground environment state with CRA configurations to create a Markovian augmented MDP. A counterfactual Q-learning algorithm exploits CRA structure by considering all possible machine configurations during transitions, generating multiple learning experiences per interaction. The paper also demonstrates how LLMs can convert natural language task descriptions into formal CRA specifications.

## Key Results
- CRAs achieve superior sample efficiency compared to reward machines on context-free and context-sensitive tasks
- Counterfactual reasoning provides multiple learning signals per environment interaction
- CRA complexity (states/transitions) is lower than reward machines for equivalent tasks
- Natural language specification via LLMs produces correct CRAs for benchmark problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CRAs enable direct modeling of non-regular reward functions through counter-based memory
- Mechanism: CRAs extend finite state machines with counter variables that can be incremented/decremented and tested for zero. This allows the automaton to count occurrences of events and condition transitions on these counts, thereby expressing context-free and context-sensitive languages that regular machines cannot handle.
- Core assumption: The task reward structure can be captured by a counter machine with bounded counter values
- Evidence anchors: [abstract] "our framework allows for tasks described by unrestricted grammars"; [section] "Counter automata may be thought of as finite state automata augmented by a finite number of counter variables"
- Break condition: If the required memory for the task exceeds what can be bounded by the maximum trajectory length H times the maximum counter increment, the CRA cannot express the reward function

### Mechanism 2
- Claim: CRA-based learning achieves sample efficiency by exploiting counterfactual reasoning over all possible machine configurations
- Mechanism: When an agent takes action a in state s, the CRA framework considers what would have happened if the machine had been in any of its non-terminal configurations ⟨ui, cj⟩. For each configuration, it computes the next machine state and reward, generating multiple counterfactual experiences that are used to update the Q-function. This exploits the structured task information encoded in the CRA to provide more learning signals per interaction.
- Core assumption: The ground environment state provides sufficient information for the labeling function to determine the input symbol to the CRA for any counterfactual configuration
- Evidence anchors: [abstract] "we describe how the sample efficiency of any off-policy algorithm can be improved through the use of counterfactual reasoning"; [section] "Counterfactual Experiences... we consider every possible machine configuration prior to the transition"
- Break condition: If the number of counterfactual configurations becomes too large (exponential in number of counters and states), the computational overhead negates the sample efficiency gains

### Mechanism 3
- Claim: CRAs maintain intuitive specification while achieving greater expressive power than reward machines
- Mechanism: While CRAs can express any recursively enumerable language, they maintain a finite state structure with counter variables that can be intuitively understood and specified. The states correspond to subtasks and counters track relevant quantities, making the automaton readable and verifiable. This is demonstrated by specifying tasks through natural language descriptions converted to formal languages via LLMs.
- Core assumption: Complex tasks can be decomposed into a finite number of subtasks that can be tracked with bounded counters
- Evidence anchors: [abstract] "we show that the state machines produced by this approach are both intuitive to specify through the use of Large Language Models"; [section] "we have demonstrated how expert knowledge can be integrated into the formulation through the use of natural language task descriptions and LLMs"
- Break condition: If the task decomposition results in an excessively large number of states or counters, the automaton becomes too complex to specify or verify intuitively

## Foundational Learning

- Concept: Finite State Machines and Regular Languages
  - Why needed here: CRAs are an extension of finite state machines, so understanding FSMs and their limitations (expressive power limited to regular languages) is crucial for appreciating why CRAs are needed.
  - Quick check question: What is the fundamental limitation of finite state machines that CRAs overcome?

- Concept: Counter Automata and Context-Free Languages
  - Why needed here: CRAs are based on counter automata, which can recognize context-free languages. Understanding how counters extend FSMs to recognize more complex languages is key to understanding CRA expressiveness.
  - Quick check question: How does adding a single counter to a finite state machine increase its expressive power?

- Concept: Reinforcement Learning with Non-Markovian Rewards
  - Why needed here: CRAs are used to model non-Markovian reward functions in reinforcement learning. Understanding how non-Markovian rewards can be handled by augmenting the state space with automaton configurations is essential for implementing CRA-based RL.
  - Quick check question: How does combining the ground environment state with the CRA configuration create a Markovian state for RL?

## Architecture Onboarding

- Component map: Environment -> Labeling function -> Counting Reward Automaton -> Q-learning algorithm -> LLM interface
- Critical path:
  1. Receive environment transition ⟨s, a, s'⟩
  2. Compute input symbol σ = L(s, a, s')
  3. Update CRA: ⟨u', c'⟩ = δ(u, σ, Z(c))
  4. Compute reward r = λ(u, σ, Z(c))(s, a, s')
  5. Generate counterfactual experiences for all ⟨ui, cj⟩ configurations
  6. Update Q-function using all experiences
  7. Execute action selection policy
- Design tradeoffs:
  - Expressive power vs. complexity: CRAs can express more complex tasks but require more complex state representation
  - Sample efficiency vs. computation: Counterfactual reasoning improves sample efficiency but increases computation per step
  - Specification ease vs. verification: Natural language specification via LLMs is easy but verification of correctness is challenging
- Failure signatures:
  - Poor sample efficiency: May indicate incorrect CRA specification or insufficient exploration
  - Counter overflow: May indicate task complexity exceeds CRA bounds or incorrect counter increment limits
  - Counter underflow: May indicate incorrect counter initialization or transition logic
  - Learning instability: May indicate inappropriate hyperparameters for function approximation
- First 3 experiments:
  1. Implement tabular CQL on LetterEnv with simple CFL task (N=1) to verify basic CRA functionality
  2. Compare sample efficiency of CQL vs. standard Q-learning on Office Gridworld with bounded mail/coffee task
  3. Test LLM-based specification pipeline on simple natural language task and verify generated CRA correctness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of counting reward automata (CRAs) compare to other neuro-symbolic approaches for tasks beyond regular languages, such as context-sensitive or context-free grammars?
- Basis in paper: [explicit] The paper states that CRAs can model reward functions expressible in any formal language, unlike previous approaches limited to regular languages. It provides empirical results showing CRAs outperform competing approaches for context-free tasks.
- Why unresolved: The paper only compares CRAs to reward machines for context-free tasks. A broader comparison to other neuro-symbolic methods for context-sensitive or context-free languages would provide a more comprehensive understanding of CRA's strengths and limitations.
- What evidence would resolve it: Conducting experiments comparing CRAs to other neuro-symbolic approaches like hierarchical reinforcement learning, options framework, or MAXQ for tasks beyond regular languages would provide a clearer picture of CRA's performance relative to other methods.

### Open Question 2
- Question: What are the theoretical limits of CRAs in terms of the complexity of tasks they can express, and how do these limits compare to other formal language classes?
- Basis in paper: [explicit] The paper states that CRAs can model any computer algorithm and thus can express reward functions in any formal language. However, it doesn't provide a formal analysis of the complexity classes CRAs can handle.
- Why unresolved: While the paper claims CRAs can handle any formal language, it lacks a rigorous proof or theoretical analysis of the complexity classes they can express. Understanding these limits is crucial for determining the applicability of CRAs to real-world problems.
- What evidence would resolve it: A formal proof or analysis of the complexity classes CRAs can express, possibly through a reduction to Turing machines or other well-known complexity classes, would provide a clear understanding of CRA's expressive power and limitations.

### Open Question 3
- Question: How does the sample efficiency of counterfactual Q-learning (CQL) scale with the number of counters in a CRA, and what are the practical implications for tasks requiring many counters?
- Basis in paper: [explicit] The paper introduces CQL as a learning algorithm that exploits CRA structure to increase sample efficiency. It provides theoretical guarantees for convergence in the tabular case but doesn't explore the scalability of CQL with increasing counter numbers.
- Why unresolved: While CQL shows promise in improving sample efficiency, the paper doesn't investigate how its performance is affected by the number of counters in a CRA. This is important for understanding the practical limitations of CQL for complex tasks requiring many counters.
- What evidence would resolve it: Conducting experiments to evaluate the sample efficiency of CQL with varying numbers of counters in CRAs, and analyzing the relationship between counter number and learning performance, would provide insights into the scalability of CQL and its practical applicability.

## Limitations

- Limited empirical validation on synthetic benchmarks rather than complex real-world tasks
- Computational overhead of counterfactual reasoning may negate sample efficiency gains for complex automata
- Reliability and scalability of LLM-based specification approach not rigorously evaluated

## Confidence

- **High confidence**: CRAs can theoretically model any recursively enumerable language through counter-based memory
- **Medium confidence**: Sample efficiency improvements demonstrated on benchmark tasks will generalize to more complex scenarios
- **Low confidence**: LLM-based natural language specification provides reliable, scalable task specification across diverse domains

## Next Checks

1. Stress-test CRA performance on tasks requiring unbounded memory to identify the practical limits of the counter-bounding assumption
2. Benchmark computational overhead of counterfactual reasoning across varying automaton sizes to quantify the sample efficiency vs. computation tradeoff
3. Conduct ablation studies removing the LLM component to isolate the contribution of CRA structure from the specification mechanism to observed performance gains