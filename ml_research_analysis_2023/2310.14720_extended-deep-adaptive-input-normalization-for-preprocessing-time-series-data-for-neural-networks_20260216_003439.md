---
ver: rpa2
title: Extended Deep Adaptive Input Normalization for Preprocessing Time Series Data
  for Neural Networks
arxiv_id: '2310.14720'
source_url: https://arxiv.org/abs/2310.14720
tags:
- data
- time
- series
- layer
- edain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes EDAIN, a neural layer for preprocessing multivariate
  time series data for deep neural networks. EDAIN adaptively learns to normalize
  irregular data (outliers, skewness, heavy tails) via four sublayers: outlier mitigation,
  shift, scale, and power transform.'
---

# Extended Deep Adaptive Input Normalization for Preprocessing Time Series Data for Neural Networks

## Quick Facts
- arXiv ID: 2310.14720
- Source URL: https://arxiv.org/abs/2310.14720
- Authors: 
- Reference count: 40
- Key outcome: EDAIN outperforms conventional normalization methods and existing adaptive preprocessing layers on synthetic, default prediction, and financial forecasting datasets.

## Executive Summary
This paper proposes EDAIN, a neural layer for preprocessing multivariate time series data for deep neural networks. EDAIN adaptively learns to normalize irregular data (outliers, skewness, heavy tails) via four sublayers: outlier mitigation, shift, scale, and power transform. It has global-aware and local-aware versions, and a variant (EDAIN-KL) trained via KL divergence. Experiments on synthetic, default prediction, and financial forecasting datasets show EDAIN outperforms conventional normalization methods (z-score, min-max) and existing adaptive preprocessing layers (DAIN, BIN) in terms of BCE loss and classification metrics (accuracy, Amex metric, Cohen's kappa, macro-F1). The global-aware version performs best on unimodal data, while local-aware is better for multi-modal data.

## Method Summary
EDAIN is a neural layer that preprocesses multivariate time series data for deep neural networks. It consists of four sublayers: outlier mitigation, shift, scale, and power transform. These sublayers apply element-wise transformations to the input time series, with unknown parameters (α, β, m, s, λ) learned via back-propagation alongside the deep neural network weights. EDAIN has global-aware and local-aware versions, with the latter allowing the shift and scale parameters to depend on summary statistics of each time series. The paper also proposes EDAIN-KL, a variant trained via KL divergence for unsupervised preprocessing.

## Key Results
- EDAIN outperforms z-score, min-max, BIN, and DAIN normalization on synthetic data with BCE loss and accuracy.
- On Amex default prediction dataset, EDAIN improves Amex metric, Cohen's kappa, and macro-F1 compared to baselines.
- For FI-2010 LOB forecasting, EDAIN achieves better classification accuracy than standard methods.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EDAIN adaptively learns to normalize irregular time series data by optimizing its parameters jointly with the deep neural network using back-propagation.
- Mechanism: The EDAIN layer is composed of four sublayers (outlier mitigation, shift, scale, and power transform) that each apply element-wise transformations to the input time series. The unknown parameters of these sublayers (α, β, m, s, λ) are optimized together with the weights of the deep neural network using stochastic gradient descent. This allows EDAIN to learn how to appropriately preprocess the data for the specific task at hand.
- Core assumption: The irregularities in the time series data (outliers, skewness, heavy tails) can be effectively mitigated by the four sublayer transformations.
- Evidence anchors:
  - [abstract]: "This is achieved by optimizing its unknown parameters simultaneously with the deep neural network using back-propagation."
  - [section 3]: "The output of the proposed EDAIN layer is obtained by feeding the input time series X(i) through the four sublayers in a feed-forward fashion... The output is then fed to the deep neural network used for the task at hand. Letting W denote the weights of the deep neural network model, the weights of both the deep model and the EDAIN layer are simultaneously optimised in an end-to-end manner using stochastic gradient descent..."
- Break condition: If the data contains irregularities that cannot be effectively handled by the four sublayer transformations (e.g. missing values, complex multi-modal distributions).

### Mechanism 2
- Claim: The global-aware and local-aware versions of EDAIN allow the method to be tailored to the characteristics of the input data.
- Mechanism: The global-aware version of EDAIN applies a monotonic transformation to each time series, preserving the relative ordering between observations. This is suitable for unimodal data where the ordering of feature values is meaningful. The local-aware version allows the shift and scale parameters to depend on summary statistics of each time series, enabling the formation of a common representation space for multi-modal data.
- Core assumption: Preserving the relative ordering of observations is important for unimodal data, while a common representation space is beneficial for multi-modal data.
- Evidence anchors:
  - [section 3.1]: "In the global-aware version of EDAIN, the shift and scale operations also depend on a summary representation of the current time series X(i) to be preprocessed... This ensures that property (5) is maintained by the global-aware EDAIN transformation, providing additional flexibility for applications on real world data where ordering within features should be preserved."
  - [section 3.1]: "For highly multi-modal and non-stationary time series data... significant performance improvements when using local-aware preprocessing methods, as these allow forming a unimodal representation space from predictor variables with multi-modal distributions."
- Break condition: If the characteristics of the data do not match the assumptions of either the global-aware or local-aware version (e.g. partially multi-modal data).

### Mechanism 3
- Claim: The EDAIN-KL variant allows for unsupervised preprocessing, enabling its use with non-deep-neural-network models.
- Mechanism: EDAIN-KL optimizes the EDAIN layer parameters to minimize the KL-divergence between the transformed distribution and the empirical distribution of the training data. This is done by fitting an invertible mapping from a standard normal distribution to the data distribution. The resulting transformation can then be applied to normalize the data before feeding it to any model.
- Core assumption: Minimizing the KL-divergence between the transformed and empirical distributions results in a good normalization for the data.
- Evidence anchors:
  - [section 3.3]: "The EDAIN-KL layer is used to transform a Gaussian base distribution Z ∼ N (0, IdT ) via a composite function gθ = h−1 1 ◦ h−1 2 ◦ h−1 3 ◦ h−1 4 comprised of the inverses of the operations in the EDAIN sublayers, applied sequentially with parameter θ = (α, β, m, s, λ). The parameter θ is chosen to minimize the KL-divergence between the resulting distribution gθ(Z) and the empirical distribution of the dataset D."
  - [abstract]: "Furthermore, we propose a computationally efficient variation of EDAIN, trained via the Kullback-Leibler divergence, named EDAIN-KL. Like EDAIN, this method can normalize skewed data with outliers, but in an unsupervised fashion, and can be used in conjunction with non-neural-network models."
- Break condition: If the data distribution cannot be well-approximated by the invertible transformation learned by EDAIN-KL.

## Foundational Learning

- Concept: Normalizing flows and invertible transformations
  - Why needed here: EDAIN-KL is based on the concept of normalizing flows, which use invertible transformations to map a simple base distribution to a more complex target distribution. Understanding this concept is crucial for grasping how EDAIN-KL works.
  - Quick check question: What is the purpose of the change of variables formula in normalizing flows, and how is it used to compute the likelihood of transformed samples?

- Concept: Time series covariance structure and moving average models
  - Why needed here: The synthetic data generation algorithm used in the experiments models the covariance structure of the time series using moving average models. Understanding this concept is important for interpreting the results on the synthetic data.
  - Quick check question: How does the covariance between samples at different timesteps depend on the parameters of a moving average model?

- Concept: Credit scoring and default prediction
  - Why needed here: The default prediction dataset used in the experiments contains credit scores and other financial features. Understanding the domain of credit scoring and default prediction is important for contextualizing the results and potential applications of EDAIN.
  - Quick check question: What are some common features used in credit scoring models, and how do they relate to the likelihood of default?

## Architecture Onboarding

- Component map: Input time series X(i) ∈ Rd×T -> Four sublayers (outlier mitigation, shift, scale, power transform) -> Output normalized time series -> Deep neural network (e.g. GRU-based RNN) for task-specific processing

- Critical path: 1. Preprocess input time series using EDAIN layer 2. Feed preprocessed data to deep neural network 3. Compute task-specific loss (e.g. BCE for classification) 4. Backpropagate gradients through EDAIN layer and deep neural network 5. Update parameters of both EDAIN layer and deep neural network

- Design tradeoffs:
  - Global-aware vs. local-aware versions of EDAIN
    - Global-aware: preserves ordering, better for unimodal data
    - Local-aware: forms common representation space, better for multi-modal data
  - Number and complexity of sublayers
    - More sublayers can handle more irregularities but increase model complexity
    - Simpler sublayers may be insufficient for complex data

- Failure signatures:
  - Poor performance on data with characteristics not handled by EDAIN (e.g. missing values)
  - Instability during training due to complex interactions between EDAIN and deep neural network
  - Overfitting if EDAIN layer has too many parameters relative to dataset size

- First 3 experiments:
  1. Apply EDAIN to a simple synthetic dataset with known irregularities (e.g. outliers, skewness) and compare performance to baseline normalization methods.
  2. Test both global-aware and local-aware versions of EDAIN on a real-world dataset with known characteristics (e.g. unimodal vs. multi-modal) to validate the design tradeoffs.
  3. Experiment with different architectures for the deep neural network (e.g. different RNN variants) to assess the impact on EDAIN's performance.

## Open Questions the Paper Calls Out

- Question: Does the performance gap between global-aware and local-aware EDAIN persist when using deeper/more complex neural network architectures?
- Question: Can the EDAIN-KL layer's unsupervised training be extended to handle missing values or anomaly detection in time series?
- Question: Is there a principled way to automatically determine whether to use global-aware vs local-aware EDAIN for a given dataset?

## Limitations
- The performance gains are demonstrated primarily on classification tasks, with limited exploration of regression or other problem types.
- The synthetic data generation may not fully capture the complexity of real-world time series irregularities.
- The computational efficiency claims are based on the assumption that gradient clipping and careful initialization are sufficient to prevent instability, but the sensitivity to these hyperparameters is not thoroughly explored.

## Confidence
- **High Confidence**: The core mechanism of using four adaptive sublayers for preprocessing, and the overall architecture of EDAIN, is well-specified and theoretically sound.
- **Medium Confidence**: The empirical results showing performance improvements over baselines are convincing, but the generalizability to other datasets and tasks requires further validation.
- **Low Confidence**: The unsupervised EDAIN-KL variant's effectiveness is demonstrated, but the paper does not compare it extensively against other unsupervised normalization methods.

## Next Checks
1. Test EDAIN on time series data with varying levels of skewness, kurtosis, and multi-modality to assess its adaptability.
2. Benchmark EDAIN against a broader set of preprocessing techniques, including those not based on deep learning, on diverse datasets.
3. Conduct an ablation study to determine the individual and combined contributions of each sublayer to the overall performance.