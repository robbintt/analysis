---
ver: rpa2
title: Toward efficient resource utilization at edge nodes in federated learning
arxiv_id: '2309.10367'
source_url: https://arxiv.org/abs/2309.10367
tags:
- training
- layers
- learning
- data
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a federated learning strategy inspired by transfer
  learning to reduce resource utilization on edge nodes. The approach randomly selects
  layers to train while freezing the remaining model, thereby reducing computational
  load and network transfer costs.
---

# Toward efficient resource utilization at edge nodes in federated learning

## Quick Facts
- arXiv ID: 2309.10367
- Source URL: https://arxiv.org/abs/2309.10367
- Reference count: 8
- Key outcome: Training only 25-50% of model layers in federated learning can accelerate training by 7-10%, reduce data transmission by 53-75%, and maintain accuracy within 2% of full models while enabling resource-constrained edge devices to participate.

## Executive Summary
This paper introduces a federated learning strategy that randomly selects model layers to train while freezing the rest, significantly reducing computational load and network transfer costs on edge devices. The approach is inspired by transfer learning fine-tuning techniques but adapts them to the federated setting where each client trains independently on local data. Experiments across CIFAR-10, CASA, and IMDB datasets demonstrate that training partial models can achieve performance comparable to full-model training while enabling participation from constrained devices like Jetson Nano. The strategy shows particular promise for scaling federated learning to edge environments with limited resources.

## Method Summary
The method implements a random layer selection strategy within the FEDn federated learning framework. For each training round, clients randomly select a subset of model layers to update while keeping the remaining layers frozen. Only the gradients and weights for the selected layers are computed, stored in memory, and transmitted to the server. The server aggregates received updates to produce a new global model. This approach reduces both computational requirements (fewer parameters updated) and communication costs (smaller data transferred) per client while maintaining model accuracy through distributed layer coverage across multiple clients.

## Key Results
- Training 25-50% of model layers accelerates training by 7-10% compared to full-model training
- Data transmission reduced by 53-75% when training only partial layers
- Model accuracy maintained within 2% of fully trained models across all tested datasets
- Resource-constrained edge devices like Jetson Nano can participate effectively in federated learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Freezing randomly selected layers during federated training reduces the computational and memory footprint on edge devices.
- Mechanism: Each client randomly selects a subset of model layers to train in each round, excluding the rest from gradient computation and weight updates, thereby reducing local processing requirements.
- Core assumption: The randomly selected layer subsets across clients collectively cover the full model over time, preserving model convergence quality.
- Evidence anchors:
  - [abstract] "For each local model update, we randomly select layers to train, freezing the remaining part of the model."
  - [section 4] "In our approach, a random layer selection strategy is used, although future work could explore more advanced selection strategies."
  - [corpus] Weak evidence; corpus neighbors discuss resource-efficient FL but not random layer freezing.

### Mechanism 2
- Claim: Reducing the number of updated layers per client decreases network communication volume between clients and server.
- Mechanism: Only gradients and weights for the selected layers are transmitted in each round, reducing serialized data size proportionally to the fraction of layers trained.
- Evidence anchors:
  - [abstract] "reduce both server load and communication costs per round by excluding all untrained layer weights from being transferred to the server."
  - [section 5.2.4] "Table 3 presents the number of trainable parameters and transferred data size for different training settings over 100 training rounds with 10 clients, highlighting the linear correlation between the number of trainable layers and serialized data size."
  - [corpus] No direct evidence; neighbors focus on early stopping and pruning but not layer selection.

### Mechanism 3
- Claim: Scaling the number of clients while reducing per-client layer count can maintain or improve global model accuracy.
- Mechanism: With more clients, the probability that each layer is trained at least once per round increases, compensating for reduced per-client training scope.
- Evidence anchors:
  - [section 5.2.3] "As the number of clients participating in a training round increases, the probability of engaging all layers of a model in the global training process also rises."
  - [section 5.2.3] "When training the model with seven layers, scaling the contributors from 5 to 20... an accuracy gain of approximately 15% was observed."
  - [corpus] Weak; corpus neighbors do not discuss scaling trade-offs between client count and layer count.

## Foundational Learning

- Concept: Federated Averaging (FedAvg)
  - Why needed here: It is the aggregation algorithm used to combine local model updates into a global model.
  - Quick check question: In FedAvg, how is the global model updated from client contributions?

- Concept: Transfer learning and fine-tuning
  - Why needed here: The approach is inspired by selectively freezing layers, analogous to fine-tuning pre-trained models.
  - Quick check question: What is the difference between transfer learning and fine-tuning in terms of which layers are trained?

- Concept: Model parallelism vs. data parallelism
  - Why needed here: Understanding how distributing model layers across devices relates to the proposed layer-selection strategy.
  - Quick check question: In model parallelism, how are different parts of a neural network distributed across workers?

## Architecture Onboarding

- Component map: Server -> Clients (using FEDn framework) -> Server
- Critical path:
  1. Server initializes global model weights.
  2. Clients receive model and select random subset of layers to train.
  3. Clients train selected layers on local data for E epochs.
  4. Clients transmit only updated layer weights to server.
  5. Server aggregates received updates into new global model.
  6. Repeat until convergence.
- Design tradeoffs:
  - Fewer trained layers per client → lower resource use but higher rounds needed.
  - More clients with fewer layers → better layer coverage but higher coordination overhead.
  - Fixed random selection vs. adaptive selection → simplicity vs. potential performance gains.
- Failure signatures:
  - Slow or stalled convergence → likely insufficient layer coverage per round.
  - High variance in accuracy across rounds → possible imbalance in layer selection.
  - Out-of-memory errors on clients → layer subset too large for device capacity.
- First 3 experiments:
  1. Train full model on all clients (baseline FedAvg) to measure accuracy, time, and communication.
  2. Train 25% of layers randomly per client, measure impact on accuracy and communication.
  3. Increase client count while reducing per-client layers, observe trade-off between resource use and accuracy.

## Open Questions the Paper Calls Out

- Question: How does the performance of the proposed approach compare to other state-of-the-art federated learning strategies that aim to reduce resource utilization, such as Adaptive Parameter Freezing (APF)?
- Basis in paper: [explicit] The authors mention that Chen et al.'s APF approach still relies on memory to cache prior parameters and requires both CPUs and RAMs to train the entire model at the beginning and each unfreezes period, while their approach trains sub-layers of the model selected randomly, which significantly impacts the resources, training time, transferred data, and the final model accuracy.
- Why unresolved: The authors do not provide a direct comparison between their approach and APF or other similar strategies in terms of resource utilization, communication costs, and model performance.
- What evidence would resolve it: Conducting experiments that compare the proposed approach with APF and other state-of-the-art strategies on the same datasets and tasks, measuring resource utilization, communication costs, and model performance.

- Question: What is the optimal number of layers to train for different types of models and tasks in federated learning settings?
- Basis in paper: [inferred] The authors demonstrate that training different percentages of model layers (25%, 50%, and 75%) can achieve comparable accuracy to training the entire model, but they do not provide a general guideline for selecting the optimal number of layers for different types of models and tasks.
- Why unresolved: The optimal number of layers to train may depend on various factors such as model architecture, task complexity, and data distribution, which were not thoroughly explored in the paper.
- What evidence would resolve it: Conducting extensive experiments with a wide range of models, tasks, and data distributions to identify patterns and guidelines for selecting the optimal number of layers to train in federated learning settings.

- Question: How does the proposed approach scale with an increasing number of clients and varying client heterogeneity in terms of computational resources and data distribution?
- Basis in paper: [explicit] The authors observe a negative correlation between the number of participating clients and the number of layers that need to be trained on each client's side, but they do not explore the impact of client heterogeneity and varying data distributions on the approach's performance.
- Why unresolved: The scalability of the approach in real-world federated learning scenarios with heterogeneous clients and non-IID data distributions remains unclear.
- What evidence would resolve it: Conducting experiments with a large number of clients with varying computational resources and data distributions to evaluate the approach's scalability, convergence, and performance under different scenarios.

## Limitations

- Limited exploration of adaptive layer selection strategies beyond random selection
- No comparison with other state-of-the-art resource-efficient federated learning approaches
- Lack of theoretical analysis for why layer coverage through random selection ensures convergence

## Confidence

- **High confidence**: The communication reduction mechanism (Mechanism 2) is mathematically straightforward and well-supported by empirical data showing linear correlation between trained layers and data transfer size.
- **Medium confidence**: The computational efficiency claims (Mechanism 1) are supported by experiments but depend on implementation details of gradient computation for frozen layers that aren't fully specified.
- **Low confidence**: The accuracy maintenance claims when scaling client count (Mechanism 3) show strong results in specific experimental conditions but lack theoretical grounding for why this trade-off works universally.

## Next Checks

1. **Layer Coverage Analysis**: Implement logging to track per-layer training frequency across rounds and verify that all layers receive sufficient training to prevent model degradation.
2. **Architecture Generalization Test**: Apply the approach to ResNet and LSTM architectures on CIFAR-10 and IMDB datasets to assess whether the 25-50% layer training sweet spot generalizes.
3. **Adaptive Selection Experiment**: Replace random selection with a simple adaptive strategy (e.g., cycling through layer groups) to determine if coverage can be improved without sacrificing resource efficiency.