---
ver: rpa2
title: 'Interpretable Symbolic Regression for Data Science: Analysis of the 2022 Competition'
arxiv_id: '2304.01117'
source_url: https://arxiv.org/abs/2304.01117
tags:
- competition
- data
- regression
- algorithms
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the results of a symbolic regression competition
  held at the 2022 GECCO conference, which aimed to evaluate how modern SR algorithms
  handle common challenges in data science. The competition included a qualification
  track, synthetic benchmark tracks testing tasks like exact expression rediscovery
  and feature selection, and a real-world track involving COVID-19 data.
---

# Interpretable Symbolic Regression for Data Science: Analysis of the 2022 Competition

## Quick Facts
- arXiv ID: 2304.01117
- Source URL: https://arxiv.org/abs/2304.01117
- Reference count: 40
- Key outcome: SR algorithms excel at noise handling and feature selection but struggle with exact expression rediscovery and extrapolation

## Executive Summary
This paper analyzes the results of the 2022 Symbolic Regression Competition held at GECCO, evaluating how modern SR algorithms handle common data science challenges. The competition featured three tracks: qualification (filtering algorithms), synthetic benchmarks (testing specific challenges), and real-world COVID-19 forecasting. Results show SR algorithms outperform linear regression on diverse datasets but face difficulties with exact expression rediscovery and extrapolation. The study highlights the need for further research to improve SR algorithms' robustness to noise, feature selection, and interpretability in practical applications.

## Method Summary
The competition used a multi-track evaluation framework with qualification datasets (20 PMLB datasets), synthetic benchmark tasks (exact rediscovery, feature selection, escaping local optima, extrapolation accuracy, sensitivity to noise), and real-world COVID-19 forecasting. Algorithms were given 1 hour for datasets up to 1000 samples and 10 hours for larger datasets, with 10 runs per dataset. Performance was evaluated using R2 score, simplicity (expression tree node count), and task-specific metrics, with final rankings computed as harmonic means of criterion ranks. Expert evaluation was included for the real-world track using a 1-5 trust score.

## Key Results
- SR algorithms outperform linear regression on qualification datasets across diverse PMLB benchmarks
- Algorithms handle noise and feature selection reasonably well but struggle with exact expression rediscovery and extrapolation
- Expert evaluation reveals interpretability challenges in real-world COVID-19 forecasting models despite good accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The competition's multi-track structure allows algorithms to be evaluated on both synthetic challenges and real-world interpretability, revealing strengths and weaknesses that single-track benchmarks miss.
- Mechanism: By combining exact rediscovery, feature selection, noise handling, extrapolation, and expert interpretability assessments, the competition exposes how algorithms behave under diverse real-world data challenges.
- Core assumption: Synthetic tasks can be designed to simulate real-world data difficulties like noise, irrelevant features, and extrapolation.
- Evidence anchors:
  - [abstract] mentions evaluation on "common challenges often faced in real-world data" including "presence of noise, redundant features, or extrapolation behaviour."
  - [section III.B] details synthetic tasks: "Rediscovery of the exact expression," "Selection of relevant features," "Escaping local optima," "Extrapolation accuracy," and "Sensitivity to noise."
  - [corpus] provides related work on symbolic regression benchmarks and interpretability, supporting the importance of diverse evaluation.
- Break condition: If synthetic tasks fail to accurately model real-world noise distributions or feature correlations, the evaluation may not reflect true algorithm performance.

### Mechanism 2
- Claim: The harmonic mean ranking criterion ensures that algorithms must perform well across all evaluation dimensions, preventing trade-offs that sacrifice interpretability for accuracy.
- Mechanism: By computing aggregated ranks as the harmonic mean of R2, simplicity, and task-specific scores, the competition penalizes algorithms that excel in one area but fail in others.
- Core assumption: Interpretability, accuracy, and simplicity are equally important for symbolic regression in data science applications.
- Evidence anchors:
  - [section III.B] states "we computed the aggregated rank as the harmonic means of the ranks for each criterion" and explains it "penalizes small values more."
  - [section IV.A] shows most algorithms outperforming linear regression, validating the need for balanced evaluation.
  - [corpus] lacks direct evidence but supports the general importance of multi-criteria evaluation in ML.
- Break condition: If practitioners value accuracy over interpretability in certain applications, the harmonic mean may unfairly disadvantage highly accurate but complex models.

### Mechanism 3
- Claim: Expert evaluation of real-world COVID-19 forecasting models provides a realistic measure of interpretability that automated metrics cannot capture.
- Mechanism: Domain experts assess model trustworthiness using a 1-5 scale based on expression readability, behavior plots, and prediction accuracy, simulating real-world adoption decisions.
- Core assumption: Expert judgment can reliably distinguish interpretable from uninterpretable models in practical contexts.
- Evidence anchors:
  - [section III.C] describes the expert assessment process: "an infectious disease expert... using a trust score ranging from 1 (strong distrust) to 5 (strong trust)."
  - [section IV.C] reports results with expert scores contributing to final rankings.
  - [corpus] provides evidence that interpretability is context-dependent and expert-dependent.
- Break condition: If experts have inconsistent criteria or the evaluation process lacks standardization, results may not be reproducible or generalizable.

## Foundational Learning

- Concept: Symbolic regression searches for analytic expressions that best fit data by composing simpler functions from a user-defined set, guided by a loss function measuring approximation error.
  - Why needed here: Understanding SR fundamentals is essential for interpreting competition results and algorithm design choices.
  - Quick check question: What is the primary advantage of symbolic regression over traditional regression methods like neural networks?

- Concept: Evolutionary algorithms (genetic programming) and alternative approaches (deep learning, mixed integer programming) represent different search strategies for symbolic regression, each with distinct strengths and weaknesses.
  - Why needed here: The competition included diverse algorithms, and understanding their methodological differences explains performance variations across tasks.
  - Quick check question: How do evolutionary algorithms for SR differ from deep learning approaches in terms of search space exploration?

- Concept: Interpretability in machine learning refers to the degree to which a human can understand the cause of a decision, and it's particularly crucial for scientific discovery and regulatory compliance.
  - Why needed here: The competition's focus on interpretability highlights its importance in practical symbolic regression applications.
  - Quick check question: Why might a simpler symbolic expression be more interpretable than a complex one, even if both have similar accuracy?

## Architecture Onboarding

- Component map: Qualification track (filtering) -> Synthetic tracks (benchmarking challenges) -> Real-world track (expert assessment)
- Critical path: Algorithms must first pass qualification (R2 > linear regression), then compete in synthetic tracks (R2, simplicity, task-specific scores), and finally real-world track (R2, simplicity, expert trust score)
- Design tradeoffs: The harmonic mean ranking prevents any single algorithm from dominating by excelling in only one dimension, but may disadvantage highly specialized approaches. Expert evaluation provides realistic interpretability assessment but introduces subjectivity.
- Failure signatures: Algorithms failing qualification have R2 below linear regression. Poor synthetic performance indicates weaknesses in handling noise, feature selection, or extrapolation. Low expert scores suggest models are not practically interpretable despite good accuracy.
- First 3 experiments:
  1. Run a baseline algorithm (e.g., linear regression) on qualification datasets to verify filtering threshold.
  2. Test an algorithm on the exact rediscovery task with easy difficulty to assess basic symbolic regression capability.
  3. Evaluate an algorithm on the noise sensitivity task to measure robustness to data perturbations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal runtime budget for SR algorithms to balance solution quality and computational efficiency?
- Basis in paper: [inferred] The paper mentions that SR algorithms often use the full runtime budget to maximize evaluations, but some methods stop early once reaching a local optima. The authors note that runtime is a minor issue but still relevant for practical applications.
- Why unresolved: The paper does not provide a systematic analysis of how runtime affects solution quality across different SR algorithms and problem types. It only mentions that half of the participants used the full budget.
- What evidence would resolve it: A controlled experiment varying runtime budgets across different SR algorithms and problem types, measuring solution quality metrics like R2 score and simplicity.

### Open Question 2
- Question: How can we develop more effective simplicity measures that capture the true interpretability of SR models beyond just counting nodes?
- Basis in paper: [explicit] The paper acknowledges that the current simplicity score based on node count has limitations. It mentions that some constructs, like chains of nonlinear functions, can hinder interpretability without adding much to the size.
- Why unresolved: The paper only suggests that there are examples of complexity measures in literature but does not evaluate their effectiveness in the context of this competition.
- What evidence would resolve it: A comparative study of different complexity measures, including the current node count, on a set of SR models with varying interpretability, using human expert ratings as ground truth.

### Open Question 3
- Question: What is the impact of noise on the feature selection capability of SR algorithms, and how can we improve it?
- Basis in paper: [explicit] The paper shows that SR algorithms struggle with feature selection when noise is present, with some algorithms using irrelevant features and missing true features even in relatively low noise conditions.
- Why unresolved: The paper does not provide a detailed analysis of how different levels of noise affect feature selection performance or propose specific improvements to address this issue.
- What evidence would resolve it: A systematic study varying noise levels and measuring feature selection performance using metrics like the feature absence score, along with proposed modifications to SR algorithms to improve feature selection under noise.

## Limitations

- The competition results may not generalize to all symbolic regression applications, as the evaluation focused on specific synthetic tasks and COVID-19 forecasting, potentially missing other important real-world challenges.
- Expert interpretability assessment, while valuable, introduces subjectivity that may not be reproducible across different expert panels or application domains.
- The time budget constraints (1 hour for smaller datasets, 10 hours for larger ones) may have disadvantaged algorithms that require longer convergence times but could achieve better results with more resources.

## Confidence

- **High confidence**: The synthetic benchmark results showing SR algorithms' performance on noise handling, feature selection, and extrapolation tasks, as these are based on automated, reproducible metrics.
- **Medium confidence**: The qualification track results demonstrating SR algorithms outperforming linear regression on diverse PMLB datasets, as this comparison is straightforward but may not capture all practical considerations.
- **Medium confidence**: The real-world COVID-19 forecasting results, as they combine automated metrics with expert judgment, but the single dataset and specific context may limit generalizability.

## Next Checks

1. Test the top-performing algorithms from the competition on additional real-world datasets beyond COVID-19 to verify robustness across different application domains.
2. Conduct a systematic ablation study to determine the relative importance of each evaluation criterion (R2, simplicity, expert trust) in predicting practical algorithm success.
3. Compare the competition's harmonic mean ranking approach with alternative multi-criteria evaluation methods to assess whether the current ranking truly reflects the best algorithms for data science applications.