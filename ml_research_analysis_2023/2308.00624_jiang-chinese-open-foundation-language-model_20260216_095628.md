---
ver: rpa2
title: 'JIANG: Chinese Open Foundation Language Model'
arxiv_id: '2308.00624'
source_url: https://arxiv.org/abs/2308.00624
tags:
- chinese
- language
- jiang
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces JIANG, an open-source large-scale Chinese
  language model. The authors address the gap in performance of existing models on
  Chinese tasks due to factors like vocabulary design and training corpus.
---

# JIANG: Chinese Open Foundation Language Model

## Quick Facts
- arXiv ID: 2308.00624
- Source URL: https://arxiv.org/abs/2308.00624
- Reference count: 8
- Key outcome: JIANG achieves 0.7509 on cmnli, ranking among top Chinese language models

## Executive Summary
This paper introduces JIANG, an open-source large-scale Chinese language model designed to address performance gaps in existing models on Chinese tasks. The authors attribute these gaps to vocabulary design and training corpus limitations when models are primarily trained on English data. JIANG is trained on a mixture of Chinese and English data sources, using a transformer architecture with modifications including partial bias cancellation, RMSNorm, gated mechanisms, RoPE, and FlashAttention. Experimental results show JIANG achieves competitive performance on Chinese benchmarks, ranking second on cmnli with a score of 0.7509, demonstrating capabilities comparable to other Chinese large-scale models.

## Method Summary
JIANG is a transformer-based language model trained on a mixture of Chinese and English data including Chinese internet (203.95B tokens), Wikipedia (24.03B tokens), ThePile (82.77B tokens), GitHub (87.54B tokens), and other specialized Chinese sources. The model uses architectural modifications like partial bias cancellation, RMSNorm normalization, gated feed-forward layers, RoPE positional encoding, and FlashAttention for memory efficiency. Training was conducted on 96 A100 80G GPUs for 52 days with a batch size of 6 million tokens, leveraging deepspeed zero3 and CPU offload for memory optimization.

## Key Results
- Achieves 0.7509 score on cmnli task, ranking second only to MOSS
- Demonstrates competitive performance on Chinese benchmarks including thucnews, chnsenticorp, and c3
- Shows capabilities comparable to other Chinese large-scale models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chinese language models require task-specific architectural and corpus adaptations to match English models' performance.
- Mechanism: JIANG integrates Chinese-specific data and architectural choices to improve Chinese task performance.
- Core assumption: Performance differences stem primarily from corpus design and vocabulary/tokenization strategy rather than model size alone.
- Evidence anchors: The paper states models trained primarily on English data underperform on Chinese due to vocabulary design and training corpus limitations. JIANG uses a large Chinese corpus mixture tailored for Chinese tasks.

### Mechanism 2
- Claim: Removing bias in fully-connected layers and using gated mechanisms improves model extrapolation and convergence speed.
- Mechanism: JIANG removes bias terms except for attention layers and uses a gated mechanism to control information flow, improving stability and learning efficiency.
- Core assumption: Bias terms in fully-connected layers contribute noise or overfitting, while gating improves gradient flow during training.
- Evidence anchors: The architecture section describes partial cancellation of bias in fully-connected layers and the use of a gated mechanism inherited from LLaMA. Evaluation on thunews dataset shows convergence graphs indicating steady improvement.

### Mechanism 3
- Claim: Efficient training techniques enable stable training of large models within limited GPU memory.
- Mechanism: JIANG trains with 6 million token batch size on 96 A100 GPUs for 52 days, leveraging parallelism and memory optimization.
- Core assumption: Large batch sizes improve gradient stability, and memory-efficient techniques enable scaling without excessive hardware cost.
- Evidence anchors: The model training section explicitly states use of deepspeed zero3, cpu offload, and data parallel processing with large batch size. Performance on cmnli and other tasks implies stable training.

## Foundational Learning

- Concept: BPE tokenization and vocabulary design
  - Why needed here: Proper tokenization ensures efficient encoding of Chinese characters and words, avoiding segmentation issues and improving model coverage.
  - Quick check question: How does JIANG's tokenizer differ from standard GPT-NeoX, and why was it chosen for Chinese tasks?

- Concept: Attention mechanism and positional encoding
  - Why needed here: RoPE (Rotary Position Embedding) improves extrapolation and handling of relative positions, crucial for long Chinese texts.
  - Quick check question: What advantage does RoPE provide over standard positional embeddings in transformer models?

- Concept: Large-scale distributed training
  - Why needed here: Training on 96 GPUs with large batch sizes and optimization techniques is necessary to scale model capacity while maintaining training stability.
  - Quick check question: What are the key memory and computational bottlenecks when training a 65600-parameter model on 96 GPUs?

## Architecture Onboarding

- Component map: BPE tokenizer -> transformer decoder with RMSNorm, gated FFN, RoPE, FlashAttention -> deepspeed zero3 distributed training -> checkpointing

- Critical path: Data ingestion → BPE tokenization → model forward pass (with gated FFN, RoPE, FlashAttention) → loss computation → gradient update (deep speed zero3, CPU offload) → checkpointing

- Design tradeoffs: Larger batch sizes and model size improve convergence but require more GPUs and memory; gating adds expressiveness but increases parameters; removing bias reduces overfitting but may hurt some tasks if not tuned

- Failure signatures: Degraded Chinese task performance indicates tokenizer or corpus issues; training instability suggests batch size or memory optimization misconfiguration; slow convergence implies gating or architectural choices need adjustment

- First 3 experiments:
  1. Test tokenizer coverage on a held-out Chinese corpus to ensure >99.9% character coverage
  2. Compare convergence speed and loss with/without gating and bias cancellation on a small subset of data
  3. Evaluate FlashAttention memory savings vs standard attention on the target GPU setup

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of JIANG compare to other Chinese language models when trained on a larger English corpus?
- Basis in paper: The paper mentions that JIANG's performance in English tasks is slightly lower than other models, which is attributed to the relatively smaller English language corpus used during training.
- Why unresolved: The paper does not provide information on how JIANG's performance would change if trained on a larger English corpus.
- What evidence would resolve it: Training JIANG on a larger English corpus and comparing its performance to other Chinese language models in English tasks.

### Open Question 2
- Question: How does the performance of JIANG compare to other Chinese language models when using different tokenization methods, such as word-level tokenization?
- Basis in paper: The paper mentions that JIANG uses a BPE tokenizer with a compact vocabulary, which covers individual characters rather than phrases like MOSS models.
- Why unresolved: The paper does not provide information on how JIANG's performance would change if using a different tokenization method, such as word-level tokenization.
- What evidence would resolve it: Training JIANG with a different tokenization method and comparing its performance to other Chinese language models in various tasks.

### Open Question 3
- Question: How does the performance of JIANG compare to other Chinese language models when using different model architectures, such as encoder-decoder models?
- Basis in paper: The paper mentions that JIANG uses a transformer architecture with modifications, but does not explore other model architectures like encoder-decoder models.
- Why unresolved: The paper does not provide information on how JIANG's performance would change if using a different model architecture, such as an encoder-decoder model.
- What evidence would resolve it: Training JIANG with a different model architecture and comparing its performance to other Chinese language models in various tasks.

## Limitations

- Limited benchmark coverage: The paper only briefly mentions performance on other tasks like thucnews, chnsenticorp, and c3 without providing detailed metrics across diverse Chinese NLP tasks
- Missing implementation details: Critical details about model size, layer count, exact hyperparameters, and the document processing pipeline are not specified, preventing faithful reproduction
- Weak comparative analysis: Claims about JIANG's relative ranking among Chinese models lack comprehensive head-to-head comparisons and quantitative support across diverse tasks

## Confidence

**High Confidence**: Claims about Chinese corpus requirements and tokenizer design are well-supported by the literature and JIANG's methodology section.

**Medium Confidence**: Claims about architectural modifications (partial bias cancellation, gated mechanisms) improving performance are plausible but lack direct ablation studies or comparative evidence.

**Low Confidence**: Claims about JIANG's relative ranking among Chinese models and overall competitive performance are weakly supported due to limited benchmark reporting and lack of comprehensive comparative analysis.

## Next Checks

1. **Tokenizer Coverage Validation**: Test JIANG's tokenizer on a held-out Chinese corpus to verify >99.9% character coverage and measure out-of-vocabulary rates on domain-specific Chinese text (financial reports, news, etc.).

2. **Ablation Study on Architectural Modifications**: Implement baseline models with and without gated mechanisms, partial bias cancellation, and RMSNorm to quantify their individual contributions to Chinese task performance on at least two tasks (e.g., cmnli and chnsenticorp).

3. **Comprehensive Benchmark Evaluation**: Evaluate JIANG on a broader suite of Chinese NLP benchmarks including GLUE-zh, CLUE, and domain-specific tests (financial NER, medical QA) to validate claims about competitive performance across task types and provide meaningful comparisons with existing Chinese models.