---
ver: rpa2
title: 'DEFT: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised
  Core-Set Selection'
arxiv_id: '2310.16776'
source_url: https://arxiv.org/abs/2310.16776
tags:
- deft
- data
- arxiv
- fine-tuning
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DEFT, a data-efficient fine-tuning framework
  that leverages unsupervised core-set selection to minimize the amount of data needed
  to fine-tune pre-trained language models for downstream tasks. The authors apply
  DEFT to text-editing tasks and compare its performance to the state-of-the-art model
  CoEDIT.
---

# DEFT: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection

## Quick Facts
- **arXiv ID**: 2310.16776
- **Source URL**: https://arxiv.org/abs/2310.16776
- **Reference count**: 23
- **Key outcome**: DEFT achieves comparable accuracy to state-of-the-art CoEDIT while fine-tuning on 70% less data across eight text-editing datasets

## Executive Summary
This paper introduces DEFT, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to minimize the amount of data needed to fine-tune pre-trained language models for downstream tasks. The authors apply DEFT to text-editing tasks and demonstrate that models fine-tuned with only 32.5% of the CoEDIT dataset achieve comparable SARI and ROUGE-L scores. Their human evaluation shows the edited texts are perceived as similarly accurate to those generated by the full-data baseline.

## Method Summary
DEFT uses an unsupervised core-set selection (UCS) algorithm that applies k-means clustering to embedded representations of text-editing examples, then samples representative subsets from each cluster based on distance to centroid. The framework stratifies an initial subset Dbase, clusters the remaining data Dremain, and selects samples using a weighted combination of easy (close to centroid) and hard (far from centroid) examples. The resulting subset Dc is combined with Dbase for fine-tuning, achieving comparable performance to full-data fine-tuning while using significantly less data.

## Key Results
- DEFT models match CoEDIT accuracy while fine-tuning on 70% less data
- The best performing DEFT model (trained on 32.5% of data) achieves similar human-perceived accuracy to CoEDIT
- Across eight datasets and six editing tasks, DEFT maintains comparable SARI and ROUGE-L scores
- Sentence-T5 embeddings provide optimal clustering performance for the core-set selection

## Why This Works (Mechanism)

### Mechanism 1
The unsupervised core-set selection algorithm reduces data requirements by finding a smaller, representative subset of training data. The algorithm uses k-means clustering to group similar examples, then samples both "easy" and "hard" examples from each cluster to create a diverse subset. The core assumption is that distance to cluster centroid correlates with sample importance for training (easy samples are close to centroid, hard samples are far).

### Mechanism 2
The sampling strategy (easy vs hard) interacts with the amount of initial data (Dbase) to optimize model performance. With small Dbase, hard sampling (α=0, β=1.0) is better because it adds challenging examples that help the model generalize. With large Dbase, random sampling balances exploration and exploitation.

### Mechanism 3
The quality of the embedding representation directly impacts the effectiveness of core-set selection. Better embeddings create more meaningful clusters that group examples by task type, allowing UCS to select representative subsets across all editing tasks. The core assumption is that Sentence-T5 embeddings better capture semantic similarities relevant to text-editing tasks compared to other embeddings.

## Foundational Learning

- **Concept**: K-means clustering
  - Why needed here: To group similar text-editing examples so UCS can select representative subsets from each task category
  - Quick check question: What determines which examples go in the same cluster when using k-means with sentence embeddings?

- **Concept**: Cosine distance in high-dimensional spaces
  - Why needed here: To measure similarity between examples and cluster centroids when determining "easy" vs "hard" samples
  - Quick check question: Why does cosine distance work better than Euclidean distance for comparing sentence embeddings?

- **Concept**: Core-set selection theory
  - Why needed here: To understand why a smaller subset can train models as well as the full dataset when selected properly
  - Quick check question: What property must a core-set have to ensure model performance doesn't degrade when training on it instead of the full dataset?

## Architecture Onboarding

- **Component map**: Data preprocessing → Stratify initial Dbase sampling → Embedding generation → UCS clustering and sampling → Model training → Evaluation
- **Critical path**: Data → Embedding → Clustering → Sampling → Fine-tuning → Evaluation
- **Design tradeoffs**: Larger K creates more specific clusters but requires more computation and data; more A increases coverage but reduces data efficiency gains; hard sampling maximizes diversity but may include outliers; easy sampling maximizes stability but may miss important edge cases
- **Failure signatures**: Poor clustering (clusters contain mixed edit types, leading to unrepresentative subsets); overfitting (model performs well on training subset but poorly on held-out data); underfitting (model performs poorly even on training subset, suggesting insufficient coverage); embeddings mismatch (different embedding types produce drastically different results)
- **First 3 experiments**: 
  1. Test embedding quality: Run k-means with different embeddings (Sentence-T5, BART CLS, averaged Flan-T5) and visualize cluster purity by edit task
  2. Test sampling strategy: Compare hard, easy, and random sampling with fixed Dbase=30% and A=2000 to find optimal approach
  3. Test Dbase sensitivity: Run with Dbase=10%, 30%, 50%, 70% using optimal sampling strategy to understand tradeoff between initial data and core-set size

## Open Questions the Paper Calls Out

### Open Question 1
The paper raises the question of how the choice of embedding representation impacts the performance of the DEFT framework. While Sentence-T5 was found to be optimal for the specific task and dataset used, the paper does not explore performance with other embeddings or in other task-specific applications.

### Open Question 2
The paper questions how the size of DBase influences the optimal sampling method within UCS. It notes that hard sampling works better with lower DBase while random sampling works better with higher DBase, but does not provide a detailed analysis of this relationship across different datasets or tasks.

### Open Question 3
The paper suggests that more work is needed to investigate the utility of DEFT in fine-tuning various PLMs for diverse sets of downstream NLP tasks beyond text-editing, raising the question of whether the framework can be generalized to other task-specific applications.

## Limitations
- The effectiveness of k-means clustering depends heavily on embedding quality, but the paper doesn't explore sensitivity to different embedding choices
- The sampling strategy's reliance on distance metrics assumes this captures sample importance without validation
- The human evaluation protocol lacks detail on evaluator expertise, sample size, and reliability measures

## Confidence

- **High Confidence**: The core methodology of using k-means clustering for core-set selection is technically sound and follows established practices in machine learning
- **Medium Confidence**: The claim that DEFT matches CoEDIT performance with 70% less data is supported by quantitative metrics (SARI and ROUGE-L scores), but the human evaluation results need more rigorous validation
- **Low Confidence**: The generalization claims across all six editing tasks are based on a single set of hyperparameters without exploring whether different tasks might benefit from task-specific configurations

## Next Checks

1. **Embedding Sensitivity Analysis**: Run the entire DEFT pipeline using three different embedding methods (Sentence-T5, BART CLS, averaged Flan-T5) and compare the resulting SARI/ROUGE-L scores. For each embedding type, visualize the cluster purity by edit task to verify that the clusters meaningfully separate different editing operations.

2. **Ablation on Sampling Strategy**: Create a controlled experiment varying only the sampling strategy (hard, easy, random) while keeping all other parameters constant (Dbase=30%, A=2000). For each strategy, calculate the average distance from centroid for selected samples and correlate this with downstream performance to test whether distance-based sampling actually selects more informative examples.

3. **Human Evaluation Protocol Audit**: Replicate the human evaluation with at least 10 different evaluators, using a standardized interface that shows pairs of edited texts (DEFT vs. CoEDIT) without revealing which model produced which. Collect inter-rater reliability scores (Cohen's kappa) to establish whether the human judgments are consistent enough to support the paper's claims about perceived accuracy.