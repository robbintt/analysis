---
ver: rpa2
title: 'PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels'
arxiv_id: '2310.01655'
source_url: https://arxiv.org/abs/2310.01655
tags:
- attention
- matrix
- polynomial
- softmax
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the quadratic time and memory complexity of
  self-attention in transformers, which limits their scalability for long contexts.
  The authors propose replacing softmax with polynomial attention and using polynomial
  sketching techniques from numerical linear algebra to achieve linear-time polynomial
  attention without requiring attention matrix sparsification.
---

# PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels

## Quick Facts
- **arXiv ID:** 2310.01655
- **Source URL:** https://arxiv.org/abs/2310.01655
- **Reference count:** 16
- **Key outcome:** Achieves 2.5-4x speedup in training compared to FlashAttention for 32k context length GPT-2 style models with no quality degradation

## Executive Summary
This paper introduces PolySketchFormer, a transformer architecture that replaces softmax attention with polynomial attention and uses polynomial sketching techniques to achieve linear-time computation without requiring attention matrix sparsification. The authors develop a block-based algorithm for efficient causal masking, making their approach practical for large-scale language modeling. Empirically, PolySketchFormer demonstrates significant speedups (2.5-4x) over FlashAttention on long contexts (32k tokens) while maintaining comparable model quality on PG19, Wikipedia, and C4 datasets.

## Method Summary
PolySketchFormer replaces the standard softmax attention mechanism with polynomial attention using high-degree polynomials, then applies polynomial sketching techniques from numerical linear algebra to approximate the resulting tensor products efficiently. The approach uses a block-based algorithm to compute causal masking in linear time, avoiding the sequential dependencies of traditional cumulative sum approaches. The method is validated through training language models on synthetic and real-world datasets using Google Cloud TPUs, showing both theoretical guarantees and practical performance improvements.

## Key Results
- Achieves 2.5-4x speedup in training compared to FlashAttention for 32k context length GPT-2 style models
- Maintains comparable perplexity to standard softmax attention when using degree-4 polynomial attention
- Demonstrates linear-time complexity without requiring attention matrix sparsification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Polynomial attention with high degree can effectively replace softmax without sacrificing model quality
- Core assumption: The distribution of attention weights induced by polynomial kernels approximates that of softmax well enough for language modeling tasks
- Evidence: Figure 2 shows degree-4 polynomial attention has perplexities comparable to softmax attention

### Mechanism 2
- Claim: Polynomial sketching approximates high-dimensional tensor products efficiently without explicit computation
- Core assumption: The polynomial sketch preserves inner products within required accuracy while mapping to non-negative space via tensoring
- Evidence: Theorem 2.5 shows degree-p polynomial sketch followed by tensoring gives degree 2p polynomial sketch

### Mechanism 3
- Claim: Block-based lower triangular multiplication reduces memory bandwidth bottlenecks in causal masking
- Core assumption: ML accelerators can efficiently multiply small blocks and sequential dependency reduction outweighs increased operation count
- Evidence: Theorem 3.1 provides computational complexity analysis for the block-based approach

## Foundational Learning

- **Concept: Polynomial kernel sketching**
  - Why needed: Enables efficient approximation of high-degree polynomial attention without computing full tensor products
  - Quick check: What is the key property of the (ε, p)-AMM guarantee and how does it ensure good approximation quality?

- **Concept: Tensor sketching and the tensoring trick**
  - Why needed: Preserves non-negativity of polynomial attention matrix while using efficient sketches
  - Quick check: Why does applying the sketch and then tensoring the result preserve non-negative entries?

- **Concept: Block-based matrix multiplication**
  - Why needed: Reduces memory bandwidth bottlenecks in causal attention by minimizing sequential dependencies
  - Quick check: How does increasing block size affect the number of sequential operations versus total floating point operations?

## Architecture Onboarding

- **Component map:** Input -> Sketch layer -> Tensoring -> Block-based LT multiplication -> Normalization
- **Critical path:** Sketch computation → tensoring → block-based LT multiplication → normalization
- **Design tradeoffs:**
  - Sketch size vs accuracy: Larger sketches improve approximation but increase computation
  - Block size vs sequential dependency: Larger blocks reduce sequential operations but increase per-block computation
  - Polynomial degree vs expressiveness: Higher degrees may better approximate softmax but require larger sketches
- **Failure signatures:**
  - Perplexity degradation: Sketch approximation too coarse or polynomial degree insufficient
  - Training slowdown: Block size poorly chosen or sketch computation bottlenecked
  - Memory issues: Sketch size too large for available memory
- **First 3 experiments:**
  1. Verify degree-4 polynomial attention with full computation matches softmax perplexity on small dataset
  2. Test different sketch sizes (16, 32, 64) to find accuracy-latency tradeoff on validation set
  3. Compare block sizes (64, 128, 256) to measure impact on training steps/second

## Open Questions the Paper Calls Out

1. **Optimal polynomial degree:** What is the optimal degree of the polynomial to use in PolySketchFormer for balancing computational efficiency and model quality? The paper uses degree-4 but suggests higher degrees could potentially bring perplexities closer to vanilla softmax Transformer.

2. **Multi-domain performance:** How does PolySketchFormer perform on tasks beyond language modeling, such as computer vision or speech recognition? The paper focuses on language modeling tasks and mentions that their techniques generalize to encoding-only and encoder-decoder transformers but doesn't provide experimental results for other domains.

3. **Long-range dependency limitations:** What are the limitations of PolySketchFormer in capturing long-range dependencies compared to vanilla transformers? The paper notes that works like Schlag et al. (2021) show recall of tokens is worse with linear time transformers compared to quadratic softmax attention, but these limitations may not fully transfer to multi-head attention models used today.

4. **Scalability with depth and context:** How does the performance of PolySketchFormer scale with increasing model depth and context length? The paper conducts experiments with 12-layer and 24-layer models at various context lengths up to 16k tokens but doesn't explore performance beyond these parameters.

5. **Combination with other techniques:** Can PolySketchFormer be effectively combined with other efficient transformer techniques, such as quantization or knowledge distillation? The paper doesn't investigate potential synergies or trade-offs between PolySketchFormer and other efficiency techniques.

## Limitations
- Limited evaluation scope: Experiments conducted only on GPT-2 style models with 12-24 layers and context lengths up to 16k tokens
- Hardware-specific performance: Speedup claims based on Google Cloud TPUs without comprehensive benchmarking across different hardware platforms
- Theoretical gaps: While error bounds are provided, empirical validation of approximation quality across diverse language distributions is limited

## Confidence
- **High Confidence:** Core polynomial sketching mechanism and block-based algorithm are mathematically sound with rigorous theoretical analysis
- **Medium Confidence:** 2.5-4x speedup claim supported by experimental results but limited to specific model configurations
- **Low Confidence:** Scalability claims to larger models and longer contexts are largely extrapolated from experimental results

## Next Checks
1. **Approximation Quality Validation:** Systematically vary sketch sizes (16, 32, 64, 128) and polynomial degrees (2, 4, 6, 8) on PG-19 dataset, measuring both perplexity degradation and computational speedup to establish full accuracy-latency tradeoff curve.

2. **Scaling Behavior Analysis:** Evaluate PolySketchFormer on larger model configurations (24 layers, 1024 dimensions) and longer context lengths (64k, 128k tokens) to verify linear-time complexity holds and quality is preserved at scale.

3. **Robustness to Distribution Shifts:** Test the model on diverse datasets beyond PG-19 and Wiki-40B (e.g., C4, Books corpus) to assess whether polynomial sketching approximation remains effective across different language distributions and domains.