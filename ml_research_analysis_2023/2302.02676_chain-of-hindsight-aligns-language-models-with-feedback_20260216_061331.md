---
ver: rpa2
title: Chain of Hindsight Aligns Language Models with Feedback
arxiv_id: '2302.02676'
source_url: https://arxiv.org/abs/2302.02676
tags:
- human
- feedback
- hindsight
- arxiv
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Chain of Hindsight Finetuning (CoHF) is a new method for training
  language models using human feedback that can leverage both positive and negative
  feedback, unlike previous approaches that only use positive examples. CoHF works
  by constructing "chains of hindsight" from ranked model outputs and feedback, where
  the model is trained to predict the most preferred output given less preferred outputs
  and feedback.
---

# Chain of Hindsight Aligns Language Models with Feedback

## Quick Facts
- arXiv ID: 2302.02676
- Source URL: https://arxiv.org/abs/2302.02676
- Reference count: 25
- Key outcome: Chain of Hindsight Finetuning (CoHF) leverages both positive and negative feedback to significantly improve GPT-J on summarization and dialogue tasks

## Executive Summary
Chain of Hindsight Finetuning (CoHF) is a novel method for training language models using human feedback that can leverage both positive and negative feedback, unlike previous approaches that only use positive examples. The approach constructs "chains of hindsight" from ranked model outputs and feedback, where the model is trained to predict the most preferred output given less preferred outputs and feedback. Applied to GPT-J, CoHF significantly improves results on summarization and dialogue tasks compared to supervised fine-tuning, as measured by automatic metrics like ROUGE and human evaluations.

## Method Summary
CoHF works by constructing "chains of hindsight" from ranked model outputs and feedback, where the model is trained to predict the most preferred output given less preferred outputs and feedback. The approach uses causal masking and pretraining regularization to improve performance. The method converts human preference data into sequences where the model learns to generate better outputs by conditioning on lower-rated alternatives and their associated feedback. The training combines a finetuning objective with pretraining regularization, using the Adam optimizer with specific hyperparameters.

## Key Results
- CoHF significantly improves GPT-J-6B on summarization tasks, achieving better ROUGE scores than supervised fine-tuning
- Human evaluations show CoHF-generated summaries are rated higher on accuracy, coherence, and coverage
- For dialogue tasks, CoHF improves helpfulness and harmlessness ratings compared to baseline models
- The method shows promising scaling behavior with model size

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CoHF can leverage both positive and negative feedback, unlike supervised fine-tuning (SFT) which only uses positive examples.
- **Mechanism:** By constructing a "chain of hindsight" from ranked model outputs and feedback, the model is trained to predict the most preferred output given less preferred outputs and feedback.
- **Core assumption:** The model can learn to identify and correct negative attributes or errors by being exposed to both positive and negative examples in a ranked sequence with associated feedback.
- **Evidence anchors:**
  - [abstract] "CoHF works by constructing 'chains of hindsight' from ranked model outputs and feedback, where the model is trained to predict the most preferred output given less preferred outputs and feedback."
  - [section] "Our key idea is motivated by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017). ... By fine-tuning the model to predict a higher rated attempt conditioning on one or more lower rated attempts and feedback, this effectively leverages data with all ratings."
- **Break condition:** If the ranking information is unreliable or the feedback is not informative enough, the model may not be able to effectively learn from the negative examples.

### Mechanism 2
- **Claim:** Causal masking prevents the model from simply copying tokens from similar sequences, improving generalization.
- **Mechanism:** By randomly masking 15% of past tokens during training, the model is forced to rely on the context and feedback rather than just copying similar tokens from the input sequence.
- **Core assumption:** The diversity of human annotations and model-generated data is limited, and directly fine-tuning on similar sequences can lead to overfitting and shortcut copying.
- **Evidence anchors:**
  - [section] "To mitigate this issue, we randomly mask 15% of past tokens similar to Liu et al. (2022). In our experiments, we find that this regularization improves results."
- **Break condition:** If the masking ratio is too high, it may hinder the model's ability to learn from the input sequence effectively.

### Mechanism 3
- **Claim:** Pretraining regularization prevents overfitting to the fine-tuning dataset and improves generalization.
- **Mechanism:** By minimizing the negative log likelihood of the pretraining dataset in addition to the fine-tuning objective, the model is encouraged to maintain its general language understanding capabilities while learning from the human feedback.
- **Core assumption:** The diversity of human annotations and model-generated data is limited, and fine-tuning on a small dataset can lead to overfitting.
- **Evidence anchors:**
  - [section] "To mitigate this issue, similar to Ouyang et al. (2022), we also minimize the negative log likelihood of the pretraining dataset. In our experiments, we find that this regularization enables generating more natural sentences."
- **Break condition:** If the regularization weight is too high, it may prevent the model from effectively learning from the human feedback.

## Foundational Learning

- **Concept:** Supervised Fine-tuning (SFT)
  - Why needed here: CoHF is an improved version of SFT, and understanding the limitations of SFT is crucial for appreciating the benefits of CoHF.
  - Quick check question: What is the main limitation of SFT when it comes to learning from human feedback?

- **Concept:** Hindsight Experience Replay (HER)
  - Why needed here: CoHF is inspired by HER, which allows agents to learn from failures by relabeling rewards and transitions retroactively.
  - Quick check question: How does HER enable agents to learn from sparse feedback?

- **Concept:** Causal Language Modeling
  - Why needed here: CoHF uses a causal transformer model architecture, which is crucial for the mechanism to work effectively.
  - Quick check question: What is the key difference between causal and non-causal language modeling?

## Architecture Onboarding

- **Component map:** Human feedback data -> Chain of hindsight construction -> Causal masking (15%) -> Pretraining regularization -> Transformer model -> Improved outputs
- **Critical path:** Construct chain of hindsight from human feedback data → Fine-tune model on chains to predict preferred outputs → Apply causal masking and pretraining regularization during fine-tuning
- **Design tradeoffs:**
  - Using both positive and negative examples allows for more effective learning but requires reliable ranking and feedback information
  - Causal masking prevents shortcut copying but may hinder learning if the masking ratio is too high
  - Pretraining regularization prevents overfitting but may limit the model's ability to learn from the fine-tuning dataset if the weight is too high
- **Failure signatures:**
  - If the model performs poorly on the fine-tuning dataset but well on the pretraining dataset, it may indicate overfitting
  - If the model performs poorly on both datasets, it may indicate that the chain of hindsight construction or the causal masking is not effective
- **First 3 experiments:**
  1. Evaluate the performance of CoHF on a small subset of the fine-tuning dataset to ensure that the chain of hindsight construction is working as expected
  2. Compare the performance of CoHF with and without causal masking to determine the optimal masking ratio
  3. Evaluate the performance of CoHF with different pretraining regularization weights to find the optimal balance between fine-tuning and generalization

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Performance heavily relies on the quality and quantity of human preference data, with experiments using relatively small datasets (3,704 dialogue pairs, 400 summarization pairs)
- Limited demonstration of effectiveness beyond summarization and dialogue tasks to other important language modeling domains
- Claims about scaling behavior with larger models are not empirically validated in the paper

## Confidence
- **High Confidence:** The core mechanism of using both positive and negative feedback through hindsight chains is technically sound and well-motivated by the Hindsight Experience Replay literature
- **Medium Confidence:** The empirical improvements on summarization and dialogue tasks are demonstrated, but the relatively small dataset sizes and limited task diversity reduce confidence in broader applicability
- **Low Confidence:** Claims about scaling behavior and performance on tasks beyond summarization and dialogue are not empirically validated in the paper

## Next Checks
1. **Dataset Size Sensitivity:** Test CoHF performance across different dataset sizes (e.g., 100, 500, 1000 preference pairs) to understand the minimum effective dataset size and scaling behavior
2. **Cross-Domain Transfer:** Apply CoHF to a different domain such as code generation or mathematical reasoning to validate whether the method generalizes beyond conversational tasks
3. **Ablation on Components:** Conduct systematic ablations of the three key components (hindsight chains, causal masking, pretraining regularization) to quantify the contribution of each to overall performance