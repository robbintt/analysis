---
ver: rpa2
title: Verifying the Robustness of Automatic Credibility Assessment
arxiv_id: '2303.08032'
source_url: https://arxiv.org/abs/2303.08032
tags:
- text
- adversarial
- detection
- bodega
- aack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates the robustness of text classifiers used in
  misinformation detection against adversarial attacks. It introduces BODEGA, a benchmark
  framework for testing both victim models and attack methods on four tasks: hyperpartisan
  news detection, propaganda detection, fact checking, and rumour detection.'
---

# Verifying the Robustness of Automatic Credibility Assessment

## Quick Facts
- **arXiv ID**: 2303.08032
- **Source URL**: https://arxiv.org/abs/2303.08032
- **Reference count**: 25
- **Key outcome**: This study evaluates the robustness of text classifiers used in misinformation detection against adversarial attacks, finding that modern large language models like BERT are more vulnerable to attacks than smaller models like BiLSTM, with attack success rates varying by task and method.

## Executive Summary
This study introduces BODEGA, a benchmark framework for systematically evaluating the robustness of text classifiers against adversarial attacks in misinformation detection. The framework tests both victim models and attack methods across four credibility assessment tasks: hyperpartisan news detection, propaganda detection, fact checking, and rumor detection. Through comprehensive experiments using 8 different attack methods in grey-box scenarios, the research reveals that large language models like BERT show higher vulnerability to adversarial attacks compared to smaller models like BiLSTM, with attack success rates reaching up to 27% higher on BERT for certain tasks. The work provides a standardized evaluation methodology that combines semantic preservation metrics (BERT score) with perturbation distance measures (Levenshtein distance) to assess attack effectiveness.

## Method Summary
The study implements BODEGA as a benchmark framework that evaluates classifier robustness through systematic testing of adversarial attacks. The methodology involves fine-tuning BERT and training BiLSTM models on each of the four credibility detection tasks, then applying 8 different attack methods from the OpenAttack framework in both untargeted and targeted grey-box scenarios. Attack success is measured using the BODEGA score, which combines semantic preservation (BERT score) and perturbation magnitude (Levenshtein distance). The evaluation process tests victim models on both clean and adversarially modified text, measuring changes in model predictions while ensuring semantic similarity is maintained through similarity constraints.

## Key Results
- Large language models like BERT show up to 27% higher vulnerability to adversarial attacks compared to smaller models like BiLSTM on hyperpartisan news detection tasks
- Attack success rates vary significantly across different credibility detection tasks, with inconsistent vulnerability patterns between BERT and BiLSTM models
- The BODEGA framework successfully identifies task-specific vulnerabilities and provides a standardized methodology for evaluating adversarial robustness in credibility assessment systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text classifiers are vulnerable to adversarial examples that preserve meaning but change model output
- Mechanism: Attackers generate modified text by replacing words or characters while maintaining semantic similarity, causing classifiers to misclassify non-credible content as credible
- Core assumption: Small perturbations in input text can change classifier decisions while preserving original meaning
- Evidence anchors:
  - [abstract]: "modern large language models like BERT are more vulnerable to attacks than smaller models like BiLSTM"
  - [section 2.1]: "meaning-preserving changes in input text can mislead the models"
  - [corpus]: Found 25 related papers discussing adversarial attacks on misinformation detection systems

### Mechanism 2
- Claim: BERT-based models are more vulnerable to attacks than smaller models like BiLSTM
- Mechanism: Large language models have more complex decision boundaries that are easier to exploit through adversarial modifications
- Core assumption: Model architecture complexity correlates with vulnerability to adversarial attacks
- Evidence anchors:
  - [abstract]: "attacks on hyperpartisan news detection achieve up to 27% higher success rates on BERT compared to BiLSTM"
  - [section 9.1]: "BiLSTM is much more easily attacked" in fact checking and rumor detection tasks
  - [corpus]: Papers discussing vulnerability differences between model architectures

### Mechanism 3
- Claim: BODEGA framework enables systematic evaluation of classifier robustness against adversarial attacks
- Mechanism: Provides standardized benchmark with four misinformation detection tasks, evaluation metrics (BERT score, Levenshtein distance), and attack scenarios to measure attack success
- Core assumption: Standardized evaluation framework improves comparability of adversarial attack research
- Evidence anchors:
  - [abstract]: "introduce BODEGA: a benchmark for testing both victim models and attack methods"
  - [section 6]: "evaluation procedure assesses the success of the attack on the set Xattack"
  - [corpus]: Related work on adversarial attack benchmarks and frameworks

## Foundational Learning

- Concept: Adversarial examples in NLP
  - Why needed here: Understanding how small text modifications can fool classifiers is central to evaluating robustness
  - Quick check question: What makes generating adversarial examples more challenging for text compared to images?

- Concept: BERT score and semantic similarity
  - Why needed here: BODEGA uses BERT score to measure semantic preservation between original and modified text
  - Quick check question: How does BERT score differ from traditional similarity metrics like BLEU?

- Concept: Text classification for credibility assessment
  - Why needed here: Framework tests attacks on four specific credibility detection tasks (hyperpartisan news, propaganda, fact checking, rumor detection)
  - Quick check question: Why might different credibility tasks have varying vulnerability to attacks?

## Architecture Onboarding

- Component map: Victim models (BERT, BiLSTM) -> Attack methods (BAE, BERT-ATTACK, DeepWordBug, etc.) -> Evaluation metrics (BODEGA score, BERT score, Levenshtein score) -> Four task datasets
- Critical path: 1) Load victim model, 2) Apply attack method, 3) Evaluate success using BODEGA score, 4) Compare results across methods
- Design tradeoffs: Open-ended queries vs. query limits, semantic vs. character-level similarity measures, white-box vs. gray-box attack scenarios
- Failure signatures: Low BODEGA scores indicate successful attacks, high scores indicate robust models; poor semantic preservation suggests attacks are too aggressive
- First 3 experiments:
  1. Run all attack methods on BERT model for hyperpartisan news detection task
  2. Compare attack success rates between BERT and BiLSTM models
  3. Test both targeted and untargeted attack scenarios on fact checking task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the vulnerability of misinformation detection models to adversarial attacks vary across different types of misinformation (hyperpartisan news, propaganda, fact checking, rumors)?
- Basis in paper: [explicit] The paper systematically tests four misinformation detection tasks and finds varying levels of vulnerability across them
- Why unresolved: The paper provides initial results but doesn't fully explore why certain types of misinformation are more vulnerable than others
- What evidence would resolve it: A detailed comparative analysis of attack success rates across different misinformation types, examining common characteristics of more vulnerable tasks

### Open Question 2
- Question: What is the long-term effectiveness of adversarial training in improving model robustness against evolving attack strategies?
- Basis in paper: [inferred] The paper tests current attack methods but doesn't explore how models might adapt over time to new attack strategies
- Why unresolved: The paper focuses on static evaluation rather than dynamic adaptation of both attacks and defenses
- What evidence would resolve it: Longitudinal studies tracking model performance against a sequence of increasingly sophisticated attack methods over time

### Open Question 3
- Question: How do different evaluation metrics for adversarial examples compare in their ability to predict real-world vulnerability of misinformation detection systems?
- Basis in paper: [explicit] The paper introduces BODEGA score as an evaluation metric and compares it with others like BERT score and Levenshtein distance
- Why unresolved: The paper doesn't validate these metrics against actual deployment scenarios or human assessments
- What evidence would resolve it: Correlation studies between automated evaluation metrics and real-world attack success rates or human judgments of adversarial example effectiveness

## Limitations

- Results show inconsistent vulnerability patterns across tasks, with BERT being more vulnerable in some tasks while BiLSTM is more vulnerable in others, suggesting task-specific rather than generalizable findings
- Grey-box attack setting assumes access to model confidence scores and training data, which may not reflect real-world attacker capabilities in black-box scenarios
- Framework focuses on text-based attacks without considering multimodal misinformation that combines text with images or other media formats

## Confidence

- **High Confidence**: Large language models like BERT are generally more vulnerable to adversarial attacks than smaller models like BiLSTM
- **Medium Confidence**: The BODEGA framework provides a comprehensive benchmark for evaluating adversarial robustness in credibility assessment systems
- **Low Confidence**: Attack success rates and vulnerability patterns are consistent across all four tested tasks

## Next Checks

1. Cross-task validation: Test the same attack methods across all four tasks using both BERT and BiLSTM models to quantify the inconsistency in vulnerability patterns and determine if task characteristics drive the differences

2. Black-box scenario testing: Implement black-box attack variants where the attacker has no access to model confidence scores or training data, to assess how much the grey-box assumptions inflate attack success rates

3. Semantic preservation validation: Systematically evaluate the semantic similarity between original and adversarial examples using multiple metrics beyond BERT score (e.g., human evaluation, paraphrase detection) to ensure attacks truly preserve meaning while changing model predictions