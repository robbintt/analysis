---
ver: rpa2
title: 'DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question
  Answering over Knowledge Base and Text'
arxiv_id: '2310.20170'
source_url: https://arxiv.org/abs/2310.20170
tags:
- question
- answer
- knowledge
- retrieval
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DIVKNOWQA, a novel dataset for assessing
  the reasoning abilities of large language models (LLMs) on open-domain question
  answering tasks that require retrieving information from both structured (knowledge
  base) and unstructured (text) sources. The dataset consists of 940 human-verified
  examples with two-hop multi-source questions, requiring retrieval from structured
  knowledge bases like Wikidata via symbolic queries (e.g., SPARQL).
---

# DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text

## Quick Facts
- arXiv ID: 2310.20170
- Source URL: https://arxiv.org/abs/2310.20170
- Reference count: 25
- Key outcome: DETLLM achieves 32.1% EM and 35.7% F1 on DIVKNOWQA benchmark for multi-source multi-hop QA

## Executive Summary
This paper introduces DIVKNOWQA, a novel benchmark for evaluating LLM reasoning abilities on open-domain question answering tasks that require retrieving information from both structured knowledge bases (like Wikidata) and unstructured text sources. The dataset consists of 940 human-verified examples with two-hop questions requiring information from multiple sources. To address this challenge, the authors propose DETLLM, a method that decomposes complex questions into single-hop queries and uses multiple specialized retrieval tools including text passage retrieval and symbolic SPARQL query generation. The approach significantly outperforms existing methods, demonstrating the importance of structured knowledge retrieval for complex reasoning tasks.

## Method Summary
The proposed DETLLM method tackles complex multi-hop questions by decomposing them into single-hop sub-questions using a chain-of-thought framework. It employs three specialized retrieval tools: a dense text retriever for unstructured knowledge, a sparse retriever for linearized knowledge bases, and a SPARQL query generator for structured knowledge bases like Wikidata. The system then ranks retrieved evidence using a cross-encoder and generates final answers using an LLM. This heterogeneous approach allows precise retrieval from structured sources while maintaining flexibility for unstructured text, addressing the challenge of multi-source reasoning across different knowledge representations.

## Key Results
- DETLLM achieves 32.1% exact match and 35.7% F1 scores on DIVKNOWQA benchmark
- Significant improvement over baseline methods including Vanilla Prompt, ReAct, and DSP
- Demonstrates the effectiveness of structured knowledge retrieval via SPARQL for complex reasoning tasks
- Shows that multi-source retrieval substantially outperforms single-source approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Symbolic query generation (SPARQL) provides more precise structured knowledge retrieval than linearized text retrieval.
- Mechanism: DETLLM transforms natural language queries into SPARQL queries to directly query Wikidata, leveraging the compositional power of structured query languages to retrieve exact answers without relying on LLM reasoning.
- Core assumption: SPARQL queries can precisely retrieve answers from structured knowledge bases without requiring complex reasoning by the LLM.
- Evidence anchors:
  - [abstract]: "The generation of symbolic queries (e.g., SPARQL for Wikidata) is a key requirement, which adds another layer of challenge."
  - [section]: "Unlike a query to retrieve text passages, the structured query itself can share the responsibility of reasoning... a SPARQL query can use an aggregation function to return the numerical number as the final result"
  - [corpus]: Weak - no direct citations found supporting SPARQL precision claims.
- Break condition: When the question cannot be accurately mapped to a SPARQL query due to ambiguity or lack of appropriate entity/relation IDs.

### Mechanism 2
- Claim: Decomposing multi-hop questions into single-hop questions improves retrieval accuracy and answer quality.
- Mechanism: DETLLM follows a chain-of-thought framework to break down complex questions into single-hop sub-questions, each retrieved from appropriate knowledge sources, reducing cognitive load on the LLM.
- Core assumption: Breaking down complex reasoning into simpler steps allows the LLM to focus on one reasoning task at a time, improving accuracy.
- Evidence anchors:
  - [abstract]: "DETLLM decomposes complex questions into single-hop queries and uses multiple retrieval tools"
  - [section]: "To tackle a complex question, we follow the chain-of-thought (CoT) framework to decompose a complex question into single-hop questions"
  - [corpus]: Weak - no direct citations found supporting CoT decomposition benefits.
- Break condition: When decomposition creates sub-questions that are too simple or when the intermediate answers are not useful for the final answer.

### Mechanism 3
- Claim: Treating heterogeneous knowledge sources separately with appropriate retrieval tools improves overall performance.
- Mechanism: DETLLM uses different retrievers for different knowledge types - dense retrieval for text, sparse retrieval for linearized KB, and symbolic SPARQL generation for structured KB, maximizing relevance across source types.
- Core assumption: Different knowledge sources require different retrieval strategies to maximize information coverage and relevance.
- Evidence anchors:
  - [abstract]: "We propose a Diverse rEtrieval Tool Augmented LLM (DETLLM) to address the challenges posed by DIVKNOWQA"
  - [section]: "Our approach entails searching across diverse knowledge sources... For unstructured knowledge, we use a dense passage retriever... For structured knowledge, we consider two modalities"
  - [corpus]: Weak - no direct citations found supporting heterogeneous source separation.
- Break condition: When the overhead of managing multiple retrieval tools outweighs the benefits or when sources are not sufficiently distinct.

## Foundational Learning

- Concept: SPARQL query language
  - Why needed here: Understanding SPARQL is essential for grasping how structured knowledge retrieval works and why it's more precise than text-based approaches.
  - Quick check question: How would you write a SPARQL query to count the number of awards received by a specific entity?

- Concept: Chain-of-thought reasoning
  - Why needed here: The decomposition strategy relies on breaking down complex reasoning into simpler steps, which requires understanding CoT methodology.
  - Quick check question: What are the key benefits of decomposing a multi-hop question into single-hop sub-questions?

- Concept: Dense vs sparse retrieval
  - Why needed here: Understanding the differences between retrieval methods is crucial for appreciating why different tools are used for different knowledge sources.
  - Quick check question: When would you choose a dense retriever over a sparse retriever, and vice versa?

## Architecture Onboarding

- Component map: Question → Decomposition → (Dense text retriever → Sparse KB retriever → SPARQL generator) → Ranking (cross-encoder) → Answer generation (LLM)

- Critical path: Question → Decomposition → Multiple retrievals → Ranking → Answer generation. The most critical components are the decomposition and retrieval modules.

- Design tradeoffs: Using multiple specialized retrievers increases complexity but improves accuracy; using SPARQL adds precision but requires entity linking; decomposition reduces error propagation but adds steps.

- Failure signatures: Poor entity linking leads to failed SPARQL queries; incorrect decomposition leads to irrelevant retrievals; model hallucinations occur when retrieved context is insufficient.

- First 3 experiments:
  1. Test SPARQL generation accuracy on simple entity-attribute questions
  2. Evaluate decomposition quality on multi-hop questions with known answers
  3. Compare single-retriever vs multi-retriever performance on heterogeneous questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we determine when LLMs truly require retrieval knowledge versus when they can rely on their internal knowledge?
- Basis in paper: Explicit - The paper mentions this as a limitation, stating "One limitation of our proposed DETLLM is that the retrieval tool is used in each decomposed single-hop question-answering step. A further step involves investigating when the large language model truly requires retrieval knowledge, rather than invoking the tool at every step."
- Why unresolved: The paper acknowledges this limitation but doesn't provide a solution or methodology for determining when retrieval is actually needed versus when the LLM's internal knowledge suffices.
- What evidence would resolve it: Experimental results comparing model performance with selective versus always-on retrieval tools, and metrics showing when internal knowledge is sufficient versus when external retrieval improves accuracy.

### Open Question 2
- Question: Can document reordering strategies based on attention mechanisms improve retrieval-augmented LM performance on multi-source multi-hop QA tasks?
- Basis in paper: Inferred - The paper discusses the impact of document ordering on performance, citing that "documents containing the ground truth answer tend to receive higher attention" and suggesting this as "a potential avenue for further investigation."
- Why unresolved: The paper identifies this as a potential research direction but doesn't explore or test document reordering strategies.
- What evidence would resolve it: Comparative experiments showing performance differences between different document ordering strategies (e.g., attention-based reordering vs. random vs. ground truth-first) on the DIVKNOWQA benchmark.

### Open Question 3
- Question: What is the optimal strategy for switching between closed-book LLM knowledge and open-domain retrieval to achieve maximum performance?
- Basis in paper: Explicit - The paper presents a comparison between closed-book and retrieval-augmented performance, noting that "only 50.8% of examples answered correctly by our DETLLM are also present in the closed-book setting" and suggesting "a potential research direction, which involves designing a strategy to switch between the closed book setting and open domain retrieval."
- Why unresolved: While the paper demonstrates that closed-book and retrieval-augmented approaches are complementary, it doesn't propose or test any switching strategy.
- What evidence would resolve it: Experimental results from models implementing various switching strategies (e.g., confidence-based, knowledge-based, or hybrid approaches) and their comparative performance on the DIVKNOWQA benchmark.

## Limitations
- The approach's dependence on accurate entity linking and SPARQL generation introduces brittleness that isn't fully characterized
- Performance remains relatively low (32.1% EM, 35.7% F1) suggesting the task remains challenging
- Dataset size (940 examples) may not capture the full complexity of real-world multi-source reasoning scenarios

## Confidence

- Multi-source reasoning benefits: Medium
- SPARQL precision advantages: Low
- Question decomposition effectiveness: Medium
- DETLLM overall improvements: Medium

## Next Checks

1. Test DETLLM on adversarial examples where entity linking fails or SPARQL generation produces incorrect queries to assess robustness boundaries
2. Compare retrieval accuracy breakdown (text vs KB vs SPARQL) to quantify each component's contribution to overall performance
3. Evaluate model performance on questions requiring temporal reasoning or numerical calculations that SPARQL aggregation functions could handle more precisely