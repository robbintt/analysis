---
ver: rpa2
title: One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models
arxiv_id: '2310.09499'
source_url: https://arxiv.org/abs/2310.09499
tags:
- sparsity
- pruning
- weight
- quantization
- sensitivity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Hessian sensitivity-aware mixed sparsity
  pruning method for large language models (LLMs). The key idea is to use the Hessian
  matrix to assess the sensitivity of each weight in the model, and then assign a
  sparsity ratio to each weight based on its sensitivity.
---

# One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models

## Quick Facts
- arXiv ID: 2310.09499
- Source URL: https://arxiv.org/abs/2310.09499
- Reference count: 0
- Primary result: Proposed Hessian sensitivity-aware mixed sparsity pruning method outperforms existing techniques like SparseGPT in perplexity and zero-shot downstream task performance while enabling joint pruning and quantization.

## Executive Summary
This paper introduces a one-shot pruning method for large language models that leverages Hessian sensitivity analysis to achieve higher compression ratios with minimal performance loss. The approach combines second-order information to assess weight sensitivity with an improved saliency criterion that balances error minimization and compensation. By assigning different sparsity ratios to different weight matrices based on their sensitivity, the method preserves critical model capacity while removing redundancy. The technique achieves state-of-the-art results on multiple benchmarks while maintaining compatibility with quantization for further compression.

## Method Summary
The method employs a Hessian sensitivity-aware mixed sparsity pruning approach that first estimates layer sensitivity using Hutchinson's algorithm to compute Hessian trace efficiently. Based on this sensitivity information, it assigns different sparsity ratios to different weight matrices through an allocation algorithm. Pruning is then performed using an Improved Saliency Criterion (ISC) that combines Optimal Brain Surgeon (OBS) and Optimal Brain Damage (OBD) approaches, balancing error minimization with compensation. The method operates in a single pass without retraining and can be combined with quantization techniques like GPTQ for additional compression. Experiments demonstrate the approach on LLaMA and Baichuan models at 7B and 13B scales.

## Key Results
- Achieves new state-of-the-art performance in LLM pruning compared to SparseGPT
- Maintains better perplexity scores while achieving high compression ratios
- Demonstrates improved zero-shot downstream task performance across multiple benchmarks
- Shows compatibility with quantization for additional compression without compounding errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining OBS and OBD saliency criteria into an Improved Saliency Criterion (ISC) yields better weight selection for pruning.
- Mechanism: ISC uses both the weight magnitude and its inverse Hessian diagonal to balance direct error minimization (OBD) and error compensation (OBS).
- Core assumption: The sum of the Hessian diagonal and its inverse diagonal captures complementary aspects of weight sensitivity.
- Evidence anchors:
  - [abstract] "We define a more comprehensive saliency criterion – ISC (Improved Saliency Criterion) in (4)."
  - [section] "Building upon the OBS and OBD approaches, this work considers combining the saliency criteria ε and s, in order to balance the different aspects of previously used methods."
- Break condition: If the Hessian approximation is poor, the inverse term becomes unreliable and ISC degrades to a less effective metric.

### Mechanism 2
- Claim: Sensitivity-aware mixed sparsity allocation reduces pruning-induced error by assigning different sparsity ratios per weight matrix.
- Mechanism: Weights in more sensitive layers (high Hessian trace) are pruned less aggressively, preserving critical capacity where perturbation would hurt performance most.
- Core assumption: Layer sensitivity correlates with pruning impact, so per-layer sparsity assignment improves accuracy retention.
- Evidence anchors:
  - [section] "We use second-order information (the Hessian matrix) to assess the sensitivity of each layer."
  - [section] "The bottom layers determine the model's initial encoding of the input embeddings, and their variations can have a significant impact on the intermediate layers."
- Break condition: If sensitivity estimation is noisy, aggressive pruning of supposedly insensitive layers may still hurt performance.

### Mechanism 3
- Claim: Joint application of mixed sparsity pruning and quantization achieves higher compression ratios with lower performance loss than either method alone.
- Mechanism: Pruning removes redundancy before quantization, allowing quantization to operate on a smaller, already-optimized parameter set, reducing quantization noise.
- Core assumption: Pruning and quantization effects are complementary rather than compounding in error.
- Evidence anchors:
  - [section] "We follow the approach of SparseGPT, where pruning is applied prior to quantization."
  - [section] "The proposed method further improves the performance compared to SparseGPT at the same compression ratio, confirming the effectiveness of the mixed sparsity approach."
- Break condition: If pruning creates highly uneven weight distributions, quantization may struggle to find a good fixed-point representation.

## Foundational Learning

- Concept: Hessian matrix and trace estimation via Hutchinson's algorithm
  - Why needed here: Hessian trace estimates layer sensitivity without full matrix computation, enabling efficient per-layer pruning decisions.
  - Quick check question: Why is Hutchinson's algorithm used instead of direct Hessian computation?

- Concept: One-shot pruning without retraining
  - Why needed here: Large language models cannot afford costly retraining cycles; pruning must be done in a single pass.
  - Quick check question: What is the key difference between one-shot and iterative pruning?

- Concept: Mixed precision and sparsity compatibility
  - Why needed here: Combining pruning and quantization further compresses models while maintaining performance.
  - Quick check question: Why might pruning before quantization be better than the reverse order?

## Architecture Onboarding

- Component map:
  - Pre-trained LLM weights -> Hessian trace estimator (Hutchinson) -> Sensitivity-aware mixed sparsity allocator -> ISC-based pruning engine -> (Optional) GPTQ quantization -> Compressed model

- Critical path:
  1. Load model weights
  2. Compute sensitivity per weight matrix (Hutchinson)
  3. Assign sparsity per weight matrix (Algorithm 1)
  4. Apply ISC-based pruning with compensation
  5. (Optional) Quantize pruned weights

- Design tradeoffs:
  - Sensitivity granularity: Layer-level vs. weight-level affects compression vs. accuracy balance.
  - Sparsity interval width α: Narrower intervals preserve more sensitive weights but reduce overall compression.
  - Hessian estimation quality vs. compute cost: More samples improve trace accuracy but increase runtime.

- Failure signatures:
  - High perplexity increase after pruning: Likely over-pruned sensitive layers or poor sensitivity estimation.
  - Numerical instability in weight updates: Hessian inverse may be ill-conditioned.
  - Minimal compression gain: Sparsity assignment too conservative or ISC ineffective.

- First 3 experiments:
  1. Apply uniform sparsity with OBS saliency baseline on a small LLM (e.g., LLaMA-7B) and measure perplexity.
  2. Replace uniform sparsity with sensitivity-aware mixed sparsity while keeping OBS saliency; compare perplexity.
  3. Combine sensitivity-aware mixed sparsity with ISC; validate further perplexity improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the Hessian sensitivity-aware mixed sparsity pruning method scale when applied to even larger models beyond 13B parameters?
- Basis in paper: [explicit] The paper focuses on 7B/13B models and suggests future work will explore larger models.
- Why unresolved: The paper does not provide experimental results or theoretical analysis for models larger than 13B parameters.
- What evidence would resolve it: Experimental results comparing the method's performance on models of varying sizes, particularly those exceeding 13B parameters.

### Open Question 2
- Question: What is the impact of different calibration dataset sizes on the accuracy of the Hessian matrix estimation and subsequent pruning performance?
- Basis in paper: [inferred] The paper mentions using 128 2048-token segments for calibration but does not explore the effect of varying this size.
- Why unresolved: The relationship between calibration dataset size and pruning accuracy is not investigated in the paper.
- What evidence would resolve it: Experiments varying the calibration dataset size and measuring its impact on pruning performance and Hessian matrix estimation accuracy.

### Open Question 3
- Question: How does the proposed method compare to other state-of-the-art pruning techniques when applied to non-GPT LLM architectures, such as encoder-only or encoder-decoder models?
- Basis in paper: [explicit] The paper focuses on GPT-style LLMs and suggests future work to explore other architectures.
- Why unresolved: The method's effectiveness on other LLM architectures is not demonstrated or discussed in the paper.
- What evidence would resolve it: Comparative experiments applying the method to various LLM architectures and evaluating its performance relative to other pruning techniques.

## Limitations

- Limited scope of validation primarily on LLaMA and Baichuan models at 7B/13B scales
- Sensitivity estimation reliability issues due to variance in Hutchinson-based Hessian trace estimation
- Trade-off quantification gap in isolating contributions of OBS vs. OBD components to performance gains

## Confidence

- **High Confidence**: The core mathematical formulation of ISC and sensitivity-aware mixed sparsity is sound
- **Medium Confidence**: Experimental results showing performance improvements over SparseGPT are compelling but limited in scope
- **Low Confidence**: The assertion that joint pruning and quantization are truly "complementary" rather than compounding errors lacks rigorous analysis

## Next Checks

1. Apply the method to models beyond LLaMA/Baichuan (e.g., OPT, Falcon) to verify consistent performance improvements across different architectural designs.

2. Systematically vary the number of Hutchinson samples and measure the correlation between estimation variance and final perplexity degradation to quantify the reliability threshold.

3. Conduct controlled experiments isolating the contribution of each mechanism (sensitivity estimation, mixed sparsity, ISC) to definitively establish which components drive the reported improvements.