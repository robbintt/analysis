---
ver: rpa2
title: 'Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step
  Inference'
arxiv_id: '2310.04378'
source_url: https://arxiv.org/abs/2310.04378
tags:
- consistency
- diffusion
- song
- latent
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Latent Consistency Models (LCMs) enable high-quality text-to-image
  generation with few-step inference by learning consistency mappings in the latent
  space of pre-trained Stable Diffusion models. The method distills guided diffusion
  models via an augmented probability flow ODE solver, achieving state-of-the-art
  performance with 2-4 steps or even single-step generation.
---

# Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference

## Quick Facts
- **arXiv ID**: 2310.04378
- **Source URL**: https://arxiv.org/abs/2310.04378
- **Reference count**: 28
- **Key outcome**: LCMs achieve state-of-the-art image quality with 2-4 step inference, surpassing baseline methods on LAION-5B-Aesthetics dataset.

## Executive Summary
Latent Consistency Models (LCMs) revolutionize text-to-image generation by enabling high-quality image synthesis with minimal inference steps. By learning consistency mappings in the latent space of pre-trained Stable Diffusion models, LCMs bypass the need for iterative sampling, achieving remarkable results with just 2-4 steps or even single-step generation. The method demonstrates significant improvements over existing approaches, particularly in the low-step regime, with training requiring only 32 A100 GPU hours for 4,000 iterations.

## Method Summary
LCMs distill guided diffusion models by learning consistency mappings in latent space, predicting the solution of an augmented probability flow ODE at t=0. The method uses one-stage guided distillation via an augmented PF-ODE that incorporates classifier-free guidance terms, enabling efficient training. A key innovation is the SKIPPING-STEP technique, which enforces consistency over larger time intervals to accelerate convergence. The approach supports efficient fine-tuning on customized datasets while preserving fast inference capability.

## Key Results
- On LAION-5B-Aesthetics dataset at 768×768 resolution with 4-step inference: FID of 13.53 and CLIP score of 28.60
- Outperforms baseline methods by large margins in the low-step regime
- Training requires only 32 A100 GPU hours for 4,000 iterations
- Supports efficient fine-tuning on customized datasets while maintaining fast inference

## Why This Works (Mechanism)

### Mechanism 1
LCMs learn a consistency mapping in latent space that directly predicts the solution of the augmented probability flow ODE at t=0, bypassing iterative sampling. The consistency model fθ(z, c, t) maps any latent state on the diffusion trajectory back to the original latent z₀, enforcing self-consistency across all time steps. This allows one-step generation by learning the function that maps any points on a trajectory of the PF-ODE to that trajectory's origin.

### Mechanism 2
Classifier-free guidance is integrated via one-stage guided distillation by solving an augmented PF-ODE that includes guidance terms. The augmented PF-ODE incorporates the CFG-adjusted noise prediction, and LCMs learn a consistency function fθ(zt, ω, c, t) → z₀ that directly incorporates the guidance scale ω. This enables high-quality generation without multi-stage distillation by parameterizing the augmented consistency function to handle varying ω values.

### Mechanism 3
The SKIPPING-STEP technique accelerates convergence by enforcing consistency over larger time intervals. Instead of enforcing consistency between adjacent time steps, LCMs enforce consistency between steps k apart (where k=20 in practice), reducing the effective time schedule length from thousands to dozens. This speeds up convergence without sacrificing quality by still allowing accurate ODE solver approximations over the larger intervals.

## Foundational Learning

- **Concept**: Diffusion models and score-based generative modeling
  - Why needed here: LCMs are distilled from pre-trained latent diffusion models, so understanding the forward/reverse diffusion process and score matching is essential
  - Quick check question: What is the relationship between the score function and the noise prediction model in diffusion models?

- **Concept**: Probability Flow ODE (PF-ODE) and its numerical solvers
  - Why needed here: LCMs directly predict the solution of the PF-ODE; knowledge of ODE solvers like DDIM, DPM-Solver, and DPM-Solver++ is required to implement and tune the distillation process
  - Quick check question: How does the DDIM solver approximate the integration of the PF-ODE from tn+k to tn?

- **Concept**: Classifier-free guidance (CFG) in diffusion models
  - Why needed here: LCMs integrate CFG via augmented PF-ODE; understanding how CFG modifies the noise prediction is critical for correct implementation
  - Quick check question: How does the CFG scale ω modify the noise prediction in the reverse diffusion process?

## Architecture Onboarding

- **Component map**: Encoder → Latent Consistency Model (LCM) → Image Decoder
- **Critical path**:
  1. Encode image to latent z
  2. Apply Gaussian noise at time tn+k
  3. Use ODE solver to estimate ztn from ztn+k
  4. Compute consistency loss between LCM prediction and ODE estimate
  5. Update LCM parameters via gradient descent
  6. During inference, apply LCM directly to generate z₀ in few steps

- **Design tradeoffs**:
  - Skipping step k: Larger k speeds training but may increase ODE approximation error
  - ODE solver choice: DDIM is simpler but less accurate for large k; DPM-Solver++ is more accurate but computationally heavier
  - CFG scale ω range: Larger ω improves quality but may reduce diversity; must be tuned per dataset

- **Failure signatures**:
  - Slow convergence: Likely due to small skipping step or poor ODE solver choice
  - Poor image quality: May indicate large ODE approximation error, incorrect CFG integration, or insufficient training iterations
  - Mode collapse: Could result from overly large ω or inadequate diversity in training data

- **First 3 experiments**:
  1. Train LCM with k=1 (no skipping) and compare FID/CLIP scores to baseline; expect slow convergence
  2. Train LCM with k=20 and different ODE solvers (DDIM vs DPM-Solver++); compare convergence speed and image quality
  3. Vary CFG scale ω in [2, 14]; measure impact on FID (diversity) and CLIP (quality) scores; identify optimal ω range

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the choice of ODE solver (DDIM, DPM, DPM++) impact the trade-off between convergence speed and generation quality in LCMs?
- **Open Question 2**: What is the theoretical limit of how far LCMs can skip time steps (k value) before generation quality degrades significantly?
- **Open Question 3**: Can LCMs maintain their few-step inference advantage when fine-tuned for specialized domains like medical imaging or scientific visualization?

## Limitations
- Evaluation demonstrates strong performance on LAION-5B-Aesthetics but lacks comparisons on more diverse, challenging datasets
- Effectiveness of skipping-step technique beyond k=20 is not explored, with potential degradation in image quality from larger time skips uncharacterized
- Performance on highly complex scenes or rare concepts is not reported

## Confidence
- **High confidence**: Technical implementation of consistency loss and use of DDIM solver with skipping steps is well-specified and reproducible
- **Medium confidence**: Claim of achieving state-of-the-art FID/CLIP scores at 2-4 steps is supported by experiments on LAION-5B-Aesthetics but lacks broader validation
- **Low confidence**: Long-term stability of LCMs under varying guidance scales and robustness to domain shifts are not thoroughly investigated

## Next Checks
1. **Cross-dataset validation**: Evaluate LCMs on COCO, OpenImages, and ImageNet to assess generalization beyond LAION-5B-Aesthetics; compare FID and CLIP scores across datasets
2. **Skipping-step robustness**: Train LCMs with k=10, 20, 30, and 40; measure convergence speed and image quality degradation; analyze trade-off between training efficiency and generation fidelity
3. **Guidance scale sensitivity**: Generate images across ω ∈ [1, 20] in 1-unit increments; plot FID and CLIP scores to identify optimal guidance scale range and detect potential mode collapse at extreme values