---
ver: rpa2
title: What's Hard in English RST Parsing? Predictive Models for Error Analysis
arxiv_id: '2309.04940'
source_url: https://arxiv.org/abs/2309.04940
tags:
- discourse
- relations
- which
- parsing
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates factors contributing to parsing errors
  in English RST discourse parsing, focusing on explicit vs. implicit relations, long-distance
  dependencies, and distracting discourse markers.
---

# What's Hard in English RST Parsing? Predictive Models for Error Analysis

## Quick Facts
- arXiv ID: 2309.04940
- Source URL: https://arxiv.org/abs/2309.04940
- Reference count: 23
- Key outcome: Intra-sentential status is the most important predictor of parsing errors in RST discourse parsing, followed by EDU length, genre, and OOV rate

## Executive Summary
This paper investigates factors contributing to parsing errors in English RST discourse parsing, focusing on explicit vs. implicit relations, long-distance dependencies, and distracting discourse markers. The authors create new annotated datasets for RST-DT and GUM corpora, marking explicit discourse markers and distracting markers not associated with gold relations. They analyze two SOTA parsers (bottom-up and top-down) and develop predictive models using XGBoost to identify error-prone regions. Key findings show that intra-sentential status is the most important predictor of parsing errors, followed by EDU length, genre, and OOV rate. Explicit discourse markers play a role but are less important than intra-sentential status. The presence of distracting markers correlates with errors, but many correspond to valid alternative relations not included in gold trees. The final error prediction models achieve 76.3% accuracy for bottom-up and 76.6% for top-down architectures. The study suggests RST's tree constraint may conflate some concurrent relations with parsing errors, and highlights the need to consider explicit/implicit distinctions and intra-sentential status in future RST parsing research.

## Method Summary
The authors created new annotated datasets for RST-DT and GUM corpora by marking explicit discourse markers and distracting markers not associated with gold relations. They analyzed two SOTA parsers (bottom-up and top-down) and developed predictive models using XGBoost to identify error-prone regions. The study employed mixed effects Beta regression for significance testing and used parseval metrics to evaluate parsing accuracy. The authors extracted features such as intra-sentential status, EDU length, genre, OOV rate, and the presence of discourse markers and distractors to train their error prediction models.

## Key Results
- Intra-sentential status is the most important predictor of parsing errors (followed by EDU length, genre, and OOV rate)
- Explicit discourse markers improve parsing accuracy but are less important than intra-sentential status
- Distracting discourse markers correlate with errors, but many correspond to valid alternative relations not included in gold trees
- Error prediction models achieve 76.3% accuracy for bottom-up and 76.6% for top-down architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Intra-sentential status is the most important predictor of parsing errors, even when explicit discourse markers are present.
- **Mechanism:** When an EDU is part of an intra-sentential relation, it tends to be easier to parse because syntactic structure provides clear attachment points. Inter-sentential and inter-paragraph relations lack this syntactic scaffolding, leading to ambiguity and higher error rates.
- **Core assumption:** The presence of syntactic boundaries (such as clauses or phrases within a sentence) provides sufficient information to resolve discourse attachment decisions reliably.
- **Evidence anchors:**
  - [abstract] "intra-sentential status is the most important predictor of parsing errors, followed by EDU length, genre, and OOV rate"
  - [section 4.1] "the linear model comparing the significance of explicit DMs, distractors, and features such as EDU length or OOV rate is rather naive and leaves out a variety of potentially relevant properties"
  - [section 4.2] "intra-sentential status, with which explicitness it correlates"
- **Break condition:** If syntactic structure becomes unreliable (e.g., in parsing noisy text or highly fragmented discourse), the advantage of intra-sentential parsing may diminish.

### Mechanism 2
- **Claim:** Explicit discourse markers improve parsing accuracy, but their importance is secondary to intra-sentential status.
- **Mechanism:** Explicit markers provide a direct signal for discourse relations, reducing ambiguity. However, the model's ability to leverage these markers depends on the broader syntactic and discourse context, which is primarily captured by intra-sentential status.
- **Core assumption:** The parser can reliably detect and interpret explicit markers, and these markers are consistently mapped to the correct RST relation.
- **Evidence anchors:**
  - [abstract] "the explicit/implicit distinction plays a role, but that long-distance dependencies are the main challenge"
  - [section 4.1] "DMs are unsurprisingly associated with fewer errors (t=âˆ’7.29, D=0.23, p<0.0001)"
  - [section 4.2] "presence of DMs, which fold in occurrences of helpful and distracting DMs"
- **Break condition:** If explicit markers are ambiguous or inconsistently used across genres, their predictive power may be reduced.

### Mechanism 3
- **Claim:** Distracting discourse markers (those not associated with the gold relation) are correlated with parsing errors, often because they signal valid alternative relations not included in the gold tree.
- **Mechanism:** Parsers may identify relations corresponding to distractor markers, leading to errors when the gold tree enforces a strict tree constraint. This suggests that some "errors" may actually reflect valid concurrent relations.
- **Core assumption:** The RST gold standard tree accurately represents the intended discourse structure, and distractors are truly irrelevant to the correct relation.
- **Evidence anchors:**
  - [abstract] "the presence of distracting markers not associated with gold relations"
  - [section 4.3] "we discovered that 75% (9/12) actually corresponded to relations selected as the primary RST relations by the second annotator"
  - [section 4.3] "the double annotated data confirms that, at least in the case of the RST-DT test set, a large majority of distractors do in fact correspond to multiple concurrent relations"
- **Break condition:** If the gold tree is revised to include concurrent relations, the correlation between distractors and errors may disappear.

## Foundational Learning

- **Concept:** Intra-sentential vs. inter-sentential discourse relations
  - **Why needed here:** Understanding the distinction is critical for interpreting the model's error patterns and feature importance.
  - **Quick check question:** What syntactic or structural features distinguish intra-sentential from inter-sentential discourse relations?

- **Concept:** Explicit vs. implicit discourse markers
  - **Why needed here:** This distinction is central to the study's analysis of parsing difficulty and the role of explicit signals.
  - **Quick check question:** How do explicit discourse markers differ from implicit ones in terms of their reliability for parsing?

- **Concept:** RST tree constraints and concurrent relations
  - **Why needed here:** The study's error analysis relies on understanding how RST's strict tree structure may conflate valid concurrent relations with parsing errors.
  - **Quick check question:** What are the implications of RST's tree constraint for discourse parsing accuracy?

## Architecture Onboarding

- **Component map:** Data preprocessing -> Feature extraction -> Model training -> Evaluation
- **Critical path:** 1. Preprocess and annotate data with DMs and distractors 2. Train SOTA RST parsers on the annotated data 3. Extract features from gold and predicted trees 4. Train XGBoost models to predict parsing errors 5. Analyze feature importances and error patterns
- **Design tradeoffs:** Using gold vs. predicted syntactic features: Gold features provide more reliable signals but may not reflect realistic parsing scenarios; Including vs. excluding genre: Genre-specific models may improve accuracy but reduce generalizability; Full vs. realistic feature sets: Full sets capture all relevant information but may not be feasible in practice
- **Failure signatures:** Low accuracy on inter-sentential relations: Indicates difficulty in handling long-distance dependencies; High error rates in genres with few explicit markers: Suggests reliance on explicit signals; Inconsistent performance across architectures: May indicate sensitivity to model-specific biases
- **First 3 experiments:** 1. Train XGBoost models with only intra-sentential status and DM presence to establish baseline importance 2. Add EDU length and OOV rate to the feature set to assess their contribution 3. Train models separately for each genre to identify genre-specific error patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact nature of the relationship between distractor discourse markers and parser errors? Are these errors genuine parsing mistakes or valid alternative interpretations?
- Basis in paper: [explicit] The paper finds that 75% of distractors in RST-DT correspond to relations selected as primary by a second annotator, suggesting they may represent valid concurrent relations rather than errors.
- Why unresolved: The analysis is limited to only 12 distractors from 5 documents in RST-DT test set. A larger scale analysis across more documents and genres is needed.
- What evidence would resolve it: A comprehensive analysis of distractors across the entire RST-DT and GUM datasets, examining whether parser predictions for these cases align with alternative gold annotations.

### Open Question 2
- Question: How do different architectural choices beyond bottom-up and top-down (e.g., span-based, graph-based) impact parsing performance for explicit vs. implicit relations and intra-sentential vs. inter-sentential dependencies?
- Basis in paper: [inferred] The paper compares only two architectures and finds no significant difference in performance, but acknowledges these parsers have other differences beyond the top-down/bottom-up distinction.
- Why unresolved: The analysis is limited to two specific parsers, leaving open the question of whether other architectures might show different patterns of strength and weakness.
- What evidence would resolve it: Systematic evaluation of multiple RST parsing architectures (span-based, graph-based, etc.) across various relation types and dependency distances, with detailed error analysis for each.

### Open Question 3
- Question: What is the optimal way to incorporate syntactic information into RST parsing models to maximize the benefits of intra-sentential status prediction while avoiding over-reliance on potentially noisy syntactic parses?
- Basis in paper: [explicit] The paper finds syntactic function is the most important feature in error prediction models, and that explicit marking is less important than intra-sentential status.
- Why unresolved: The paper uses gold syntactic dependencies in some analyses but doesn't explore how to best integrate predicted syntactic information or what level of syntactic representation is most beneficial.
- What evidence would resolve it: Controlled experiments varying the source and granularity of syntactic information (gold vs. predicted, dependency types, phrase structure, etc.) and their impact on parsing performance across different relation types and genres.

## Limitations
- The study is limited to English RST parsing and may not generalize to other languages or discourse frameworks
- The double annotation study has a relatively small sample size, particularly for the GUM corpus
- The error prediction models achieve reasonable accuracy but leave significant room for improvement

## Confidence

**Confidence Levels:**
- **High**: Intra-sentential status as primary error predictor, EDU length correlation with errors
- **Medium**: Explicit discourse markers' secondary role, distracting markers' correlation with errors
- **Low**: Cross-genre generalizability, applicability to non-English discourse parsing

## Next Checks

1. **Cross-corpus validation**: Test the error prediction models on an independently annotated RST corpus to assess generalizability beyond RST-DT and GUM

2. **Parser architecture comparison**: Evaluate additional parsing architectures (e.g., neural transition-based parsers) to determine if the observed error patterns hold across different model types

3. **Concurrent relations analysis**: Annotate a subset of the data to explicitly mark concurrent relations, then re-run the error analysis to quantify how many "errors" actually reflect valid alternative discourse structures