---
ver: rpa2
title: Frozen Transformers in Language Models Are Effective Visual Encoder Layers
arxiv_id: '2310.12973'
source_url: https://arxiv.org/abs/2310.12973
tags:
- visual
- transformer
- tokens
- activation
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that frozen transformer blocks from large
  language models (LLMs), despite being trained solely on text data, can effectively
  serve as visual encoder layers for a wide range of vision tasks. The proposed method
  simply inserts a frozen LLM transformer block after the standard visual encoder
  and adds learnable linear layers to align feature dimensions.
---

# Frozen Transformers in Language Models Are Effective Visual Encoder Layers

## Quick Facts
- arXiv ID: 2310.12973
- Source URL: https://arxiv.org/abs/2310.12973
- Reference count: 35
- Primary result: Frozen LLM transformer blocks improve visual representation learning across diverse tasks

## Executive Summary
This paper demonstrates that frozen transformer blocks from large language models (LLMs), despite being trained solely on text data, can effectively serve as visual encoder layers for a wide range of vision tasks. The proposed method simply inserts a frozen LLM transformer block after the standard visual encoder and adds learnable linear layers to align feature dimensions. Extensive experiments show consistent performance improvements across diverse tasks including image and point cloud classification, action recognition, motion forecasting, and 2D/3D vision-language tasks. The improvements are observed with different LLM types (LLaMA, OPT) and transformer blocks, indicating a general phenomenon. The authors propose an information filtering hypothesis to explain the effectiveness: the frozen LLM transformer distinguishes and amplifies informative visual tokens.

## Method Summary
The method involves inserting a frozen LLM transformer block (e.g., from LLaMA or OPT) after the standard visual encoder in vision models. Learnable linear layers are added to align the feature dimensions between the visual encoder output and the frozen transformer input/output. The LLM transformer weights are kept frozen during training, and positional embeddings are removed for consistency. The approach is evaluated across diverse vision tasks including image classification, point cloud classification, action recognition, motion forecasting, and 2D/3D vision-language tasks, using various backbone architectures and LLM transformer sizes.

## Key Results
- Frozen LLM transformer blocks consistently improve performance across diverse vision tasks including ImageNet classification, point cloud recognition, and action recognition
- Improvements are observed with different LLM types (LLaMA, OPT) and transformer blocks, indicating a general phenomenon
- The method offers a simple yet powerful approach to leverage LLMs for visual representation learning without requiring language inputs or outputs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pre-trained LLM transformer blocks can serve as effective visual encoder layers when frozen, improving performance across diverse vision tasks without requiring language inputs or outputs.
- **Mechanism:** The frozen LLM transformer acts as an "information filter" that distinguishes informative visual tokens and amplifies their contribution to the latent representation through feature activation enhancement.
- **Core assumption:** The LLM transformer's ability to process visual tokens stems from its general representational power learned from text, which can be transferred to visual domains when properly aligned.
- **Evidence anchors:**
  - [abstract] "frozen transformer block from large language models (LLMs), despite being trained solely on text data, can effectively serve as visual encoder layers"
  - [section] "information filtering hypothesis: the pre-trained LLM transformer blocks discern informative visual tokens and further amplify their effect"
  - [corpus] Weak - corpus contains related work on multimodal LLMs but lacks direct evidence for frozen transformer blocks as visual encoders
- **Break condition:** The mechanism fails when LLM transformers are too small (below ~1.3B parameters) or when the visual tasks require highly specialized domain knowledge not captured in general representations.

### Mechanism 2
- **Claim:** The improvement comes from the LLM transformer's representational power rather than just increased model capacity.
- **Mechanism:** Ablation studies show that replacing the LLM transformer with equivalent-capacity MLP layers yields significantly worse performance, indicating the pre-trained weights carry valuable inductive biases.
- **Core assumption:** The pre-trained transformer weights encode transferable patterns that are more valuable than randomly initialized parameters of equivalent size.
- **Evidence anchors:**
  - [section] "ViT-S-MLP has better performance than ViT-S due to its increased capacity, but the improvement is only about half of ViT-S-LLaMA"
  - [section] "the LLM transformer weights are crucial for the improvement and the observed benefits are not mere consequences of an increased model capacity"
  - [corpus] Moderate - related work on transferable representations in vision-language models supports this but doesn't directly test frozen transformer blocks
- **Break condition:** The mechanism breaks when the pre-trained weights are finetuned rather than frozen, as finetuning can lead to overfitting and worse performance.

### Mechanism 3
- **Claim:** Different depths of LLM transformer blocks have varying effectiveness for visual encoding tasks.
- **Mechanism:** The study shows that incorporating transformer blocks from different depths of LLaMA and OPT models yields different performance levels, with last layers generally improving performance.
- **Core assumption:** The effectiveness of transformer layers for visual encoding depends on their learned representations, with later layers potentially capturing more abstract features.
- **Evidence anchors:**
  - [section] "LLM transformers influence visual representation learning significantly under our framework, even though they have identical capacities"
  - [section] "the type of layer significantly changes the performance" and "the last LLM layers consistently improve the performance"
  - [corpus] Weak - corpus lacks studies on depth-specific effectiveness of frozen transformer blocks for visual tasks
- **Break condition:** The mechanism breaks when using early transformer layers that may be too focused on low-level patterns not transferable to visual domains.

## Foundational Learning

- **Concept:** Transformer architecture fundamentals (self-attention, feed-forward networks, positional embeddings)
  - **Why needed here:** Understanding how frozen transformer blocks process visual tokens and why positional embeddings are removed for consistency
  - **Quick check question:** Why does the paper remove positional embeddings from the LLM transformer when applying it to visual tasks?

- **Concept:** Transfer learning and frozen weights
  - **Why needed here:** The core technique relies on freezing pre-trained weights and understanding when and why this works better than finetuning
  - **Quick check question:** What evidence shows that freezing LLM transformers works better than finetuning them for visual tasks?

- **Concept:** Multimodal representation learning
  - **Why needed here:** The paper operates at the intersection of text and vision modalities, requiring understanding of how representations from one domain can benefit another
  - **Quick check question:** How does the information filtering hypothesis explain the transfer of text-trained representations to visual tasks?

## Architecture Onboarding

- **Component map:** Visual encoder → Linear alignment layers → Frozen LLM transformer → Output decoder
- **Critical path:** Visual tokens → F1L alignment → Frozen LLM transformer → F2L alignment → Final prediction
- **Design tradeoffs:** Freezing vs finetuning LLM transformers (simplicity vs potential overfitting), linear alignment layers vs direct feature projection, different transformer depths
- **Failure signatures:** Performance degradation when using small LLM transformers (<1.3B parameters), overfitting when finetuning large frozen transformers, poor results with early transformer layers
- **First 3 experiments:**
  1. Test frozen LLM transformer on simple image classification (ImageNet) with ViT backbone to verify basic effectiveness
  2. Compare frozen vs finetuned LLM transformer on the same task to validate the freezing design choice
  3. Test different depths of LLM transformer blocks to identify optimal layer selection

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do different depths of LLM transformer blocks affect the performance of visual encoding tasks, and what are the underlying reasons for these differences?
- **Basis in paper:** [explicit] The paper mentions that varying the depth of transformer blocks from different LLMs (LLaMA and OPT) onto ViT-S models shows significant performance changes, highlighting the importance of selecting proper transformer layers.
- **Why unresolved:** The paper does not provide a detailed analysis of why different depths of LLM transformer blocks lead to varying performance improvements. It only states that the last LLM layers consistently improve performance but might not be optimal.
- **What evidence would resolve it:** Conducting experiments that systematically vary the depth of LLM transformer blocks and analyzing the resulting performance changes, along with investigating the internal mechanisms and feature representations at different depths, could provide insights into the reasons for these differences.

### Open Question 2
- **Question:** What is the impact of scaling the LLM transformer on the effectiveness of visual encoding, and at what scale does the improvement "emerge"?
- **Basis in paper:** [explicit] The paper discusses the influence of the scales of language transformers with OPT, showing that benefits grow with increasing capacity of OPT transformers and that the improvement only "emerges" at sufficient scales.
- **Why unresolved:** The paper does not specify the exact scale at which the improvement becomes significant or explore the relationship between the scale of LLM transformers and their effectiveness in visual encoding tasks.
- **What evidence would resolve it:** Conducting experiments with a wider range of LLM transformer scales and analyzing the performance improvements at each scale could help determine the critical scale at which the improvement emerges and the relationship between scale and effectiveness.

### Open Question 3
- **Question:** How does the training process facilitate the cooperation between visual token features and the frozen language transformer, and what are the underlying mechanisms?
- **Basis in paper:** [inferred] The paper proposes the information filtering hypothesis, suggesting that the frozen LLM transformer distinguishes and amplifies informative visual tokens, but it does not explain how the training process facilitates this cooperation.
- **Why unresolved:** The paper acknowledges that the current hypothesis does not cover how the training dynamics facilitate the cooperation between visual token features and the frozen language transformer, indicating a gap in understanding the underlying mechanisms.
- **What evidence would resolve it:** Investigating the training dynamics and analyzing the interactions between visual token features and the frozen language transformer during training could provide insights into the mechanisms that facilitate their cooperation and contribute to the observed improvements.

## Limitations
- The paper does not provide a detailed analysis of why different depths of LLM transformer blocks lead to varying performance improvements
- The effectiveness of the method with smaller LLM variants (<1.3B parameters) is not thoroughly explored
- The long-term stability and scalability of the approach to extremely large vision models remains unclear

## Confidence
- **Core claim (frozen transformers improve visual encoding):** High confidence - systematic ablations across multiple architectures, tasks, and LLM families show consistent improvements
- **Information filtering hypothesis:** Medium confidence - qualitative description is plausible but quantitative analysis of token-level behavior is limited
- **Generality across vision tasks:** Medium confidence - improvements demonstrated across diverse tasks but with varying effect sizes

## Next Checks
1. Ablation study isolating the contribution of frozen weights vs. increased capacity by training MLP layers of identical size with random initialization
2. Extended robustness analysis on out-of-distribution datasets (ImageNet-A, ImageNet-R) with statistical significance testing across multiple random seeds
3. Transfer learning study on smaller datasets to quantify improvements when labeled data is limited, testing whether frozen transformers mitigate overfitting