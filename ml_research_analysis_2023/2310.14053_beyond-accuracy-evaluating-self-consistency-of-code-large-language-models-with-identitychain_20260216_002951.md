---
ver: rpa2
title: 'Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models
  with IdentityChain'
arxiv_id: '2310.14053'
source_url: https://arxiv.org/abs/2310.14053
tags:
- code
- self-consistency
- llms
- accuracy
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework, IdentityChain, to evaluate
  the self-consistency of code large language models (Code LLMs). While existing evaluations
  focus on individual task accuracy, IdentityChain assesses whether models generate
  consistent outputs across natural language and programming language translations.
---

# Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain

## Quick Facts
- arXiv ID: 2310.14053
- Source URL: https://arxiv.org/abs/2310.14053
- Authors: 
- Reference count: 9
- One-line primary result: IdentityChain framework reveals Code LLMs often fail self-consistency tests, with SSC5 scores dropping up to 78% compared to initial accuracy.

## Executive Summary
This paper introduces IdentityChain, a novel framework to evaluate self-consistency in code large language models (Code LLMs). While existing evaluations focus on individual task accuracy, IdentityChain assesses whether models generate consistent outputs across natural language and programming language translations. The framework uses a Test Output Match (TOM) score to measure semantic consistency between iteratively generated code and specifications.

Experiments on 11 Code LLMs, including GPT-4, demonstrate that models often fail to preserve self-consistency, with SSC5 scores dropping significantly compared to initial accuracy. TOM score outperforms existing metrics in evaluating both self-consistency and PL-to-NL generation. IdentityChain also serves as a debugging tool, revealing weaknesses such as poor handling of data types, implicit semantics, and code execution prediction.

## Method Summary
IdentityChain evaluates self-consistency by iteratively translating between natural language docstrings and programming code. Starting with an initial docstring, the framework generates code, then back-translates to natural language, then regenerates code, comparing outputs at each step using TOM score. The framework uses greedy decoding with early stopping for efficiency. TOM score compares exact test outputs between consecutive programs to detect semantic drift. The method is evaluated on 11 Code LLMs using HumanEvalPlus and MBPP Sanitized datasets, measuring both traditional accuracy (Pass@1) and self-consistency (SSC5).

## Key Results
- Code LLMs show significant self-consistency failures, with SSC5 scores dropping up to 78% compared to initial accuracy scores
- TOM score outperforms existing metrics for both self-consistency evaluation and PL-to-NL generation quality assessment
- IdentityChain effectively identifies model weaknesses including poor handling of data types, implicit semantics, and code execution prediction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Self-consistency is a distinct property from accuracy, measured by comparing semantic equivalence across iterative transformations.
- **Mechanism:** The IdentityChain framework performs iterative NL-to-PL and PL-to-NL translations starting from an initial docstring. At each step, the TOM score compares test outputs of consecutive programs to detect semantic drift. If outputs match, the model is self-consistent; if not, it reveals lack of semantic understanding.
- **Core assumption:** Test output comparison (TOM) is a reliable proxy for semantic equivalence between programs generated in sequence.
- **Evidence anchors:**
  - [abstract] "TOM score outperforms existing metrics in evaluating both self-consistency and PL-to-NL generation."
  - [section 4.1] "we propose another new metric, the Test Output Match (TOM) score, which compares the exact output of pli and pli+1 for each test case."
  - [corpus] Weak - no corpus neighbor directly supports TOM effectiveness; closest is SSR work on self-refinement, but not focused on code semantics.
- **Break condition:** If test cases fail to cover semantic corner cases, TOM may miss inconsistencies that only manifest in untested scenarios.

### Mechanism 2
- **Claim:** Greedy decoding with early stopping efficiently determines self-consistency without exhaustive chain lengths.
- **Mechanism:** By using greedy decoding, once an exact match occurs in the chain, subsequent iterations are deterministic and identical. IdentityChain stops early, saving computation while still capturing most non-self-consistent cases within 3 steps.
- **Core assumption:** Greedy decoding preserves semantic consistency when it exists, and breaks it detectably when it does not.
- **Evidence anchors:**
  - [section 4.2] "if at some step i in the chain, pli+1 is an exact match of pli, or nli+1 is an exact match of nli, then by the deterministic nature of greedy decoding, we know that the model will always generate the same program and specification repeatedly."
  - [section 6.3] "we find that using greedy decoding, IdentityChain efficiently reveals most not self-consistent cases within the initial three steps."
  - [corpus] Weak - no direct corpus support; neighbor work on self-consistency (e.g., SSR) focuses on reasoning, not code.
- **Break condition:** If model outputs differ due to randomness even with greedy decoding (e.g., implementation bug), early stopping may falsely conclude consistency.

### Mechanism 3
- **Claim:** TOM score generalizes as an effective PL-to-NL metric, even without ground truth references.
- **Mechanism:** Instead of requiring reference docstrings, TOM evaluates PL-to-NL quality by back-translating the generated docstring into code and comparing test outputs with the original. This sidesteps the need for human-written references.
- **Core assumption:** Semantic equivalence between two programs can be inferred from matching test outputs, making TOM applicable to NL space evaluation.
- **Evidence anchors:**
  - [section 4.3] "the SC1 score i.e. the averaged TOM score between all pl0 and pl1, can be an effective metric for the model’s PL-to-NL accuracy."
  - [section 6.2] "TOM outperforms all existing PL-2-NL metrics" in correlation with human judgment.
  - [corpus] Weak - no corpus neighbor directly validates TOM for NL evaluation; closest is GLaPE on prompt evaluation, not semantic matching.
- **Break condition:** If test suites are incomplete or biased, TOM may incorrectly assess docstrings that describe untested behavior.

## Foundational Learning

- **Concept:** Semantic equivalence between code snippets
  - **Why needed here:** Self-consistency requires detecting whether two programs have identical behavior, not just syntactic similarity.
  - **Quick check question:** If Program A outputs [1,2,3] for test cases and Program B outputs [1,2,3] for the same cases, are they semantically equivalent? (Yes, under TOM assumption.)

- **Concept:** Chain of thought in iterative transformations
  - **Why needed here:** IdentityChain chains NL-to-PL and PL-to-NL translations; understanding the flow is critical for debugging failures.
  - **Quick check question:** In a 3-step IdentityChain, what is generated at step 2 if step 1 produced pl0? (nl1 = Mp2n(pl0).)

- **Concept:** Test case design for semantic coverage
  - **Why needed here:** TOM relies on test cases; insufficient coverage can hide semantic mismatches.
  - **Quick check question:** If a program handles even numbers but tests only odd inputs, can TOM detect an even-number bug? (No.)

## Architecture Onboarding

- **Component map:** NL input -> Prompt templates -> Greedy decoding engine -> TOM evaluator -> Early stopping logic -> Benchmark loader
- **Critical path:**
  1. Load docstring from benchmark
  2. Generate pl0 (NL→PL)
  3. Evaluate Pass@1 (accuracy)
  4. Generate nl1 (PL→NL)
  5. Generate pl1 (NL→PL)
  6. Compute TOM(pl0, pl1)
  7. If exact match, stop; else continue to step 8
  8. Repeat 4–7 until chain length or match

- **Design tradeoffs:**
  - Greedy decoding ensures reproducibility but may miss creative solutions that stochastic decoding could find.
  - TOM requires comprehensive test suites; building them is costly but necessary for semantic fidelity.
  - Early stopping trades completeness for speed; longer chains may catch rare consistency failures.

- **Failure signatures:**
  - TOM mismatch → semantic drift between consecutive programs
  - Pass@1 high but SC5 low → accurate initial generation but poor semantic retention
  - Early stopping too aggressive → missed consistency violations beyond 3 steps

- **First 3 experiments:**
  1. Run IdentityChain on a simple docstring (e.g., "sum two numbers") with a known consistent model to verify TOM=1 and early stopping at step 1.
  2. Run on a model with known data-type weakness to observe TOM drops when nl1 omits type info.
  3. Vary temperature (0, 0.5, 1.0) on the same input to confirm greedy results generalize (SC5 rankings stable).

## Open Questions the Paper Calls Out
None

## Limitations
- Test coverage dependency: TOM score's reliability fundamentally depends on the comprehensiveness of test cases
- Greedy decoding assumptions: May miss semantic consistency for models exhibiting stochastic behavior even at temperature=0
- Generalization uncertainty: Framework's effectiveness beyond curated benchmarks remains unverified

## Confidence
- High Confidence: SSC5 metric's effectiveness in revealing self-consistency gaps is well-supported by experimental results showing up to 78% drops from initial accuracy
- Medium Confidence: TOM's effectiveness as a PL-to-NL metric is supported by correlation with human judgment but limited to specific benchmarks
- Low Confidence: Framework's applicability to codebases beyond curated benchmarks is not established

## Next Checks
1. Systematically add corner-case test inputs to benchmark programs and measure how TOM scores change to quantify sensitivity to test coverage completeness
2. Apply IdentityChain to code from different domains (data processing, web applications, scientific computing) to assess generalization of self-consistency patterns
3. Vary temperature settings from 0.0 to 1.0 on the same inputs and compare SSC5 rankings across models to validate greedy decoding assumptions across stochastic spectrum