---
ver: rpa2
title: Dataset Distillation in Latent Space
arxiv_id: '2311.15547'
source_url: https://arxiv.org/abs/2311.15547
tags:
- latent
- dataset
- space
- images
- codes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of existing dataset distillation
  (DD) methods, which suffer from high time and space complexity and low info-compactness
  when operating in pixel space. The authors propose Latent Dataset Distillation (LatentDD),
  a method that transfers DD processes to latent space using a pretrained autoencoder.
---

# Dataset Distillation in Latent Space

## Quick Facts
- arXiv ID: 2311.15547
- Source URL: https://arxiv.org/abs/2311.15547
- Reference count: 40
- One-line primary result: LatentDD transfers dataset distillation to latent space, reducing computational overhead while maintaining comparable performance on high-resolution datasets.

## Executive Summary
This paper addresses the inefficiency of existing dataset distillation (DD) methods, which suffer from high time and space complexity and low info-compactness when operating in pixel space. The authors propose Latent Dataset Distillation (LatentDD), a method that transfers DD processes to latent space using a pretrained autoencoder. By encoding images into compact latent codes and directly optimizing them, LatentDD significantly reduces computational overhead while maintaining comparable performance. Experiments on high-resolution datasets (e.g., ImageNet subsets at 256x256 and 512x512) demonstrate that LatentDD outperforms baselines in accuracy, time, and space efficiency. For instance, LatentMTT achieves 52.86% accuracy on Bird (256x256, IPC=1) compared to 35.80% for MTT. LatentDD also enables distillation at higher data ratios and resolutions, addressing key limitations of prior methods.

## Method Summary
The paper proposes transferring three mainstream dataset distillation algorithms (DC, DM, MTT) from pixel space to latent space using a pretrained autoencoder. The method encodes images into compact latent codes, initializes synthetic latent codes, and directly optimizes them using adapted DD algorithms. The optimized latent codes are then decoded back to images for evaluation. This approach eliminates the need for repeated encoding/decoding and backpropagation through image transformations, significantly reducing computational overhead while maintaining comparable performance. The method is evaluated on high-resolution datasets (256x256 and 512x512) and demonstrates superior efficiency and accuracy compared to baseline DD methods.

## Key Results
- LatentDD reduces time and space complexity by operating on compact latent codes (e.g., 1/48 the size of original images).
- On Bird dataset (256x256, IPC=1), LatentMTT achieves 52.86% accuracy compared to 35.80% for MTT.
- LatentDD enables distillation at higher data ratios and resolutions, addressing key limitations of prior methods.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LatentDD reduces time and space complexity by operating directly on compressed latent codes rather than pixel images.
- Mechanism: The pretrained autoencoder encodes images into compact latent codes (e.g., 1/48 the size of original images), allowing DD algorithms to run on smaller data and lighter networks.
- Core assumption: The latent codes preserve the essential information needed for downstream classification while discarding high-frequency details.
- Evidence anchors:
  - [abstract]: "latent codes in the latent space are naturally info-compact representations of the original images in much smaller sizes."
  - [section]: "latent codes have much smaller size than pixel-level images, LatentDD takes significantly less time and space..."
  - [corpus]: No direct corpus evidence; assumption supported by experimental results.
- Break condition: If the autoencoder fails to preserve class-discriminative features in latent space, classification performance will degrade.

### Mechanism 2
- Claim: LatentDD achieves comparable or better performance by enabling higher data ratios within the same storage budget.
- Mechanism: Instead of storing pixel images, LatentDD stores many more latent codes (e.g., 12× or 48× more per class), increasing the diversity and coverage of the distilled dataset.
- Core assumption: More diverse latent codes within the same storage budget lead to better generalization in downstream tasks.
- Evidence anchors:
  - [abstract]: "within the same storage budget, we can also quantitatively deliver more latent codes than pixel-level images, which further boosts the performance."
  - [section]: "deliver n·3f^2/C latent codes rather than n pixel-level images."
  - [corpus]: No direct corpus evidence; supported by quantitative results.
- Break condition: If the autoencoder introduces too much information loss, additional latent codes may not compensate for degraded quality.

### Mechanism 3
- Claim: LatentDD avoids computational overhead from repeated encoding/decoding and backpropagation through image transformations.
- Mechanism: By operating entirely in latent space, LatentDD eliminates the need to repeatedly decode latent codes into images and backpropagate gradients, which is required in previous factorization-based methods.
- Core assumption: Direct optimization in latent space is more efficient than optimizing in pixel space with intermediate transformations.
- Evidence anchors:
  - [abstract]: "unlike the previous factorization-based methods... which still run pixel-level DD algorithms, LatentDD completely avoids computational overhead..."
  - [section]: "LatentDD instead directly operates in latent space, which largely reduces time & space consumption."
  - [corpus]: No direct corpus evidence; inference based on algorithmic design.
- Break condition: If the latent space optimization introduces instability or convergence issues, the efficiency gains may be offset by reduced performance.

## Foundational Learning

- Concept: Dataset Distillation (DD)
  - Why needed here: Understanding DD is essential to grasp why moving to latent space is beneficial.
  - Quick check question: What is the primary goal of dataset distillation, and how does it differ from dataset pruning or coreset selection?

- Concept: Autoencoders and Latent Space Representations
  - Why needed here: The pretrained autoencoder is the key component enabling the transfer of DD to latent space.
  - Quick check question: How does an autoencoder compress images into latent codes, and what information is typically preserved or lost?

- Concept: Bi-level Optimization and Gradient Matching
  - Why needed here: DD algorithms like DC, DM, and MTT are based on bi-level optimization and gradient/feature/parameter matching, which are adapted to latent space.
  - Quick check question: What is the difference between gradient matching, feature matching, and parameter matching in the context of DD?

## Architecture Onboarding

- Component map: Pretrained Autoencoder -> LatentDD Algorithms (LatentDC, LatentDM, LatentMTT) -> Downstream Classifier
- Critical path:
  1. Encode real dataset images into latent codes using the pretrained autoencoder.
  2. Initialize synthetic latent codes and apply LatentDD algorithm to optimize them.
  3. Decode optimized latent codes into images for evaluation.
- Design tradeoffs:
  - Resolution vs. Downsampling Factor: Higher resolution allows more detail but requires larger latent codes; lower downsampling factor preserves more information but increases computational cost.
  - Number of Latent Codes vs. Quality: More latent codes improve diversity but may introduce redundancy if the autoencoder compresses poorly.
- Failure signatures:
  - Poor classification performance on downstream tasks indicates information loss in latent codes.
  - High computational cost suggests the autoencoder or DD algorithm is not well-optimized for the given resolution.
- First 3 experiments:
  1. Evaluate the pretrained autoencoder's reconstruction quality on a subset of the dataset.
  2. Run LatentDC on a small dataset (e.g., CIFAR-10) to verify basic functionality.
  3. Compare LatentDD performance with pixel-space DD on a medium-resolution dataset (e.g., 128x128).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LatentDD methods perform on datasets with resolutions lower than 256x256, given that the autoencoder loses more details at lower resolutions?
- Basis in paper: [explicit] The paper states that the autoencoder loses more details in reconstructed images at lower resolutions (e.g., 32x32), which could alter the original image distribution and impact performance.
- Why unresolved: The paper only experiments with resolutions starting from 256x256 and does not provide results for lower resolutions.
- What evidence would resolve it: Experimental results on lower-resolution datasets (e.g., 64x64, 128x128) comparing LatentDD methods with baselines, showing how performance changes with resolution.

### Open Question 2
- Question: Can LatentDD methods be effectively applied to non-image modalities (e.g., text, audio, video) where autoencoders are not available?
- Basis in paper: [explicit] The paper suggests that LatentDD could work with any data compression method that preserves distribution and allows reconstruction, but does not provide experiments or implementations for non-image data.
- Why unresolved: The paper only demonstrates LatentDD on image datasets and does not explore its applicability to other data modalities.
- What evidence would resolve it: Experiments applying LatentDD to non-image datasets (e.g., text classification, audio classification) using modality-specific compression methods, showing comparable performance to image datasets.

### Open Question 3
- Question: What is the optimal downsampling factor f and number of latents per class (LPC) for balancing information preservation and quantity in LatentDD?
- Basis in paper: [explicit] The paper discusses the trade-off between f and LPC, noting that higher f allows more latents but may lose information, while lower f preserves information but provides fewer latents.
- Why unresolved: The paper only provides results for f=4 and f=8, without exploring the full range of possible f values or optimizing LPC for different datasets.
- What evidence would resolve it: A comprehensive study varying f and LPC across multiple datasets, identifying optimal settings for different data characteristics and resolution levels.

## Limitations
- The method's performance on non-image data or datasets with different characteristics remains untested.
- The paper does not address potential distribution shift between the autoencoder's training data and the target datasets.
- Claims about computational efficiency rely heavily on the assumption that the pretrained autoencoder preserves class-discriminative features in latent space.

## Confidence
- High Confidence: The mechanism of operating in latent space to reduce computational complexity is well-supported by both theoretical reasoning and experimental evidence.
- Medium Confidence: The claim that LatentDD achieves comparable or better performance is supported by experiments on specific datasets (ImageNet subsets) but would benefit from broader validation across diverse domains.
- Medium Confidence: The assertion that avoiding encoding/decoding overhead improves efficiency is logical but lacks direct empirical comparison with factorization-based methods under identical conditions.

## Next Checks
1. **Cross-dataset generalization test:** Evaluate LatentDD on datasets significantly different from ImageNet (e.g., medical imaging or satellite imagery) to verify the method's robustness across domains and assess information preservation limits.
2. **Ablation study on autoencoder quality:** Systematically vary the quality of the pretrained autoencoder (e.g., using autoencoders with different reconstruction errors) to quantify the relationship between latent space compression and final DD performance.
3. **Distribution shift analysis:** Measure the performance degradation when applying LatentDD across datasets with domain gaps to identify scenarios where the autoencoder's latent representations may not be suitable for DD.