---
ver: rpa2
title: 'An Eye on Clinical BERT: Investigating Language Model Generalization for Diabetic
  Eye Disease Phenotyping'
arxiv_id: '2311.08687'
source_url: https://arxiv.org/abs/2311.08687
tags:
- clinical
- bert
- language
- data
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether language models (LMs) pretrained
  on clinical data offer advantages over general LMs in specialized clinical domains
  like diabetic eye disease phenotyping. The authors curate a new dataset of clinical
  notes annotated for 19 diabetic eye disease concepts and their attributes.
---

# An Eye on Clinical BERT: Investigating Language Model Generalization for Diabetic Eye Disease Phenotyping

## Quick Facts
- arXiv ID: 2311.08687
- Source URL: https://arxiv.org/abs/2311.08687
- Reference count: 40
- Primary result: Clinical LMs pretrained on out-of-distribution clinical data (e.g., Clinical BERT) show no significant improvement over general LMs (e.g., BERT Base) for specialized diabetic eye disease phenotyping tasks when continued pretraining and task fine-tuning are applied.

## Executive Summary
This paper investigates whether language models pretrained on clinical data offer advantages over general LMs in specialized clinical domains like diabetic eye disease phenotyping. The authors curate a new dataset of clinical notes annotated for 19 diabetic eye disease concepts and their attributes. They fine-tune BERT-based models using various pretraining strategies, including continued pretraining on in-domain clinical data. Surprisingly, they find that LMs pretrained on out-of-distribution clinical data (e.g., Clinical BERT) perform no better than general LMs (e.g., BERT Base) on their specialized task, once continued pretraining and task fine-tuning are applied. In fact, initializing with random weights and pretraining from scratch on the target domain can achieve similar or even better performance. This suggests that domain-specific pretraining is less critical than continued adaptation to the target task's data distribution.

## Method Summary
The study fine-tunes BERT-based language models for diabetic eye disease phenotyping from clinical notes, extracting 19 clinical concepts and inferring 3 attributes (laterality, temporality, severity/type) for each. Clinical notes from ophthalmology visits (2013–2022) were annotated with 19 diabetic eye disease concepts and their attributes, yielding 6,565 spans from 12,723 attribute labels across 736 encounters. Models were pretrained (BERT Base and Clinical BERT) with or without continued pretraining on in-domain clinical text, then fine-tuned for each classification task. Performance was evaluated using task-level macro F1-score across 14 classification tasks, comparing frozen vs. unfrozen encoders and different pretraining dataset sizes.

## Key Results
- LMs pretrained on out-of-distribution clinical data offer no significant improvement over LMs pretrained on non-clinical data for diabetic eye disease phenotyping.
- Continued pretraining on in-domain data significantly improves downstream task performance regardless of initial pretraining domain.
- Random initialization with continued pretraining can achieve similar or better performance than domain-specific pretraining.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific pretraining (Clinical BERT) does not provide a significant performance advantage over general pretraining (BERT Base) in specialized clinical domains when continued pretraining and task fine-tuning are applied.
- Mechanism: The distributional shift between the pretraining data and the target domain is too large for the domain-specific knowledge to transfer effectively. Continued pretraining and task fine-tuning on the target domain data allows the model to adapt to the specific linguistic patterns and task requirements, mitigating the need for domain-specific pretraining.
- Core assumption: The target domain (ophthalmology notes) has sufficiently different linguistic characteristics from the pretraining domains (general text and ICU notes) that domain-specific pretraining does not provide a strong prior.
- Evidence anchors:
  - [abstract]: "Across multiple training paradigms, we find that BERT language models pretrained on out-of-distribution clinical data offer no significant improvement over BERT language models pretrained on non-clinical data for our domain."
  - [section]: "In contrast to common perceptions about clinical language models, we find that the LMs trained on out-of-domain clinical data provide little-to-no benefit in our domain compared to the LMs trained on non-clinical data."

### Mechanism 2
- Claim: Continued pretraining on in-domain data is crucial for maximizing downstream task performance, regardless of the initial pretraining domain.
- Mechanism: Continued pretraining allows the model to adapt to the specific linguistic patterns and task requirements of the target domain. This adaptation is more important than the initial pretraining domain because it directly addresses the distributional shift between the pretraining data and the target task.
- Core assumption: The target domain data contains sufficient information to allow the model to adapt to the specific linguistic patterns and task requirements, even if the initial pretraining domain is different.
- Evidence anchors:
  - [abstract]: "In fact, initializing with random weights and pretraining from scratch on the target domain can achieve similar or even better performance."
  - [section]: "Continued pretraining leads to an additional, significant improvement in downstream task performance in each of the four settings considered in §4.1."

### Mechanism 3
- Claim: Domain-specific vocabularies can improve the efficiency of task fine-tuning, but they do not guarantee superior performance compared to general vocabularies when combined with continued pretraining.
- Mechanism: Domain-specific vocabularies can help the model learn semantics more efficiently by reducing the vocabulary size and focusing on domain-relevant terms. However, if the continued pretraining is effective, the model can learn these semantics even with a general vocabulary.
- Core assumption: The domain-specific vocabulary is well-constructed and captures the essential terms and concepts of the target domain.
- Evidence anchors:
  - [section]: "The domain-specific vocabulary magnifies the efficacy of task fine-tuning, enabling the randomly initialized LM to outperform the LMs pretrained first on other data distributions for some tasks."

## Foundational Learning

- Concept: Distributional shift
  - Why needed here: Understanding how the difference in data distribution between pretraining and target tasks affects model performance is crucial for interpreting the results of this study.
  - Quick check question: What are some examples of distributional shift in the context of clinical NLP, and how might they impact model performance?

- Concept: Domain adaptation
  - Why needed here: The study investigates whether domain adaptation techniques (continued pretraining) can mitigate the effects of distributional shift and improve model performance in specialized clinical domains.
  - Quick check question: What are the different approaches to domain adaptation, and how do they compare in terms of effectiveness and efficiency?

- Concept: Pretraining and fine-tuning
  - Why needed here: The study compares different pretraining strategies (general vs. clinical) and fine-tuning approaches (task-specific vs. continued pretraining) to understand their impact on model performance.
  - Quick check question: How do pretraining and fine-tuning interact, and what are the trade-offs between different combinations of these techniques?

## Architecture Onboarding

- Component map: Clinical notes -> BERT-based model -> MLP classifier -> 14 classification tasks
- Critical path: Pretraining (or random initialization) -> Continued pretraining on in-domain data -> Task-specific fine-tuning -> Evaluation
- Design tradeoffs: General vs. clinical pretraining domains, frozen vs. unfrozen encoders, domain-specific vs. general vocabularies
- Failure signatures: Overfitting to small annotated dataset, insufficient adaptation to target domain, poor generalization to unseen data
- First 3 experiments:
  1. Compare BERT Base and Clinical BERT performance on target task with frozen and unfrozen encoders.
  2. Investigate impact of continued pretraining on BERT Base and Clinical BERT with frozen and unfrozen encoders.
  3. Explore efficiency of BERT Base and Clinical BERT in low-data regimes by pretraining on subsets of in-domain data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of clinical LMs compare to non-clinical LMs on other specialized clinical domains beyond diabetic eye disease?
- Basis in paper: [explicit] The authors note that their study only evaluated two widely-used LMs and cannot answer whether clinical LMs outperform non-clinical LMs in general.
- Why unresolved: The study only investigated one specialized clinical domain (diabetic eye disease). Other clinical domains may have different characteristics that affect the relative performance of clinical vs. non-clinical LMs.
- What evidence would resolve it: Comparative experiments on multiple specialized clinical domains using both clinical and non-clinical LMs.

### Open Question 2
- Question: What is the impact of model size on the relative performance of clinical and non-clinical LMs in specialized clinical domains?
- Basis in paper: [inferred] The authors discuss how state-of-the-art LMs draw performance from both their complexity (number of parameters) and massive training datasets. They suggest that the relative advantage of clinical LMs may depend on the size of the model.
- Why unresolved: The study only used BERT-style models, which are comparatively small. The relative performance of clinical vs. non-clinical LMs may change with larger models.
- What evidence would resolve it: Comparative experiments using both clinical and non-clinical LMs of varying sizes.

### Open Question 3
- Question: How does the size and quality of the clinical pretraining data affect the performance of clinical LMs in specialized clinical domains?
- Basis in paper: [inferred] The authors suggest that the size and quality of the clinical pretraining data may impact the relative advantage of clinical LMs. They also note that the clinical LMs used in the study were trained on relatively small datasets.
- Why unresolved: The study did not systematically investigate the impact of clinical pretraining data size and quality on LM performance.
- What evidence would resolve it: Comparative experiments using clinical LMs pretrained on datasets of varying sizes and quality.

## Limitations

- Dataset limited to 736 encounters from a single medical institution, potentially not representing full diversity of clinical documentation practices.
- Use of regular expressions and ICD-10 codes for span identification may introduce systematic biases in annotated data.
- Study focuses specifically on diabetic eye disease phenotyping; findings may not generalize to other clinical specialties or tasks.
- Continued pretraining on target domain data makes it difficult to isolate specific contribution of initial pretraining domain.

## Confidence

**High confidence**: Continued pretraining on in-domain data is crucial for downstream performance.
**Medium confidence**: Out-of-distribution clinical pretraining offers no advantage over general pretraining.
**Medium confidence**: Domain-specific vocabularies improve task fine-tuning efficiency but don't guarantee superior performance.

## Next Checks

1. Test the same pretraining and fine-tuning paradigm on clinical note datasets from multiple healthcare institutions to assess whether the observed pretraining domain irrelevance holds across different documentation styles and practices.

2. Systematically vary the size of pretraining datasets (both general and clinical) to identify whether there exists a threshold where domain-specific pretraining begins to provide advantages, particularly for low-resource target domains.

3. Apply the experimental framework to clinical note datasets from other specialties (e.g., cardiology, oncology) to determine whether the findings generalize beyond ophthalmology and diabetic eye disease phenotyping.