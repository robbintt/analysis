---
ver: rpa2
title: 'Binary Quantification and Dataset Shift: An Experimental Investigation'
arxiv_id: '2310.04565'
source_url: https://arxiv.org/abs/2310.04565
tags:
- shift
- quantification
- which
- covariate
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how different quantification methods perform
  under various types of dataset shift, beyond the commonly studied prior probability
  shift. It proposes a taxonomy of dataset shift types and develops experimental protocols
  to simulate them, including prior probability shift, global and local covariate
  shift, and concept shift.
---

# Binary Quantification and Dataset Shift: An Experimental Investigation

## Quick Facts
- arXiv ID: 2310.04565
- Source URL: https://arxiv.org/abs/2310.04565
- Reference count: 26
- Primary result: No single quantification method handles all types of dataset shift well

## Executive Summary
This paper investigates how different quantification methods perform under various types of dataset shift beyond the commonly studied prior probability shift. The authors propose a taxonomy of shift types including prior probability shift, global and local covariate shift, and concept shift. Through experiments on synthetic data generated from Amazon product reviews, they evaluate six quantification methods (CC, ACC, PCC, PACC, DyS, SLD) and find that many methods robust to prior probability shift degrade under other types of shift. SLD and PACC perform best under mixed covariate shift, while PCC works well under pure covariate shift but poorly when priors also change. The study concludes that current quantification methods have significant limitations under dataset shift, motivating further research into more robust approaches.

## Method Summary
The study evaluates six quantification methods (CC, ACC, PCC, PACC, DyS, SLD) on synthetic data generated from Amazon product reviews. The methods are applied to binary classification problems created by binarizing multi-class review data. Logistic regression classifiers with optimized hyperparameters are trained on data from one distribution and tested on data from shifted distributions. The experiments simulate four types of dataset shift: prior probability shift, global and local covariate shift, and concept shift. Performance is measured using mean absolute error (MAE) between true and predicted prevalence values across multiple repetitions and shift conditions.

## Key Results
- Many methods robust to prior probability shift degrade under other types of shift
- PCC performs well under pure covariate shift but poorly when priors also change
- SLD and PACC are preferable under mixed covariate shift
- No method handles concept shift well
- Local covariate shift can unexpectedly improve CC and PCC performance due to classifier bias compensation

## Why This Works (Mechanism)

### Mechanism 1
SLD and PACC perform best under mixed covariate shift because they actively adjust for changes in class priors during test time. Both methods use Expectation-Maximization to recalibrate classifier posterior probabilities and re-estimate class prevalence iteratively. This recalibration compensates for changes in both covariate distributions and prior probabilities that occur in mixed covariate shift. The core assumption is that the classifier can be recalibrated to match the shifted test distribution, and the recalibration converges to accurate estimates.

### Mechanism 2
PCC performs well under pure covariate shift but poorly when priors also change because it relies on stable posterior probabilities. PCC estimates prevalence directly from classifier posterior probabilities without adjusting for prior changes. Under pure covariate shift (only P(X) changes, P(Y|X) remains constant), these posteriors remain reliable. When priors change, the posteriors become biased, leading to poor estimates. The core assumption is that posterior probabilities remain well-calibrated when only covariate distributions change, but become uncalibrated when prior probabilities change.

### Mechanism 3
CC and PCC unexpectedly improve under local covariate shift due to classifier bias compensation. When local covariate shift occurs (only P(X|Y=1) changes while P(X|Y=0) remains constant), CC and PCC's natural bias (inherited from the classifier) can partially compensate for the shift. Specifically, when the prevalence of one subclass changes, the classifier's tendency to misclassify examples from that subclass can offset the prevalence estimation error. The core assumption is that the classifier has different error rates for different subclasses, and these errors can partially cancel out the effects of local covariate shift on prevalence estimation.

## Foundational Learning

- **Concept: Dataset shift and its types**
  - Why needed here: Understanding different types of dataset shift (prior probability shift, covariate shift, concept shift) is fundamental to interpreting experimental results and understanding why different quantification methods perform differently under various conditions
  - Quick check question: What is the key difference between prior probability shift and covariate shift in terms of which distributions change?

- **Concept: Quantification methods and their mechanisms**
  - Why needed here: To understand experimental results, one must grasp how different quantification methods (CC, ACC, PCC, PACC, DyS, SLD) work and what assumptions they make about the data
  - Quick check question: Which quantification method directly uses classifier posterior probabilities without any adjustment, and why does this make it vulnerable to prior probability shift?

- **Concept: Evaluation metrics for quantification**
  - Why needed here: The paper uses Mean Absolute Error (MAE) to evaluate quantification methods. Understanding this metric and why it's appropriate for quantification is crucial for interpreting results
  - Quick check question: How does MAE differ from other evaluation metrics like RMSE in the context of quantification, and why is it preferred?

## Architecture Onboarding

- **Component map:** Amazon review data -> Data preprocessing -> Classifier training -> Quantification methods -> Evaluation -> Analysis
- **Critical path:** 1) Load and preprocess Amazon review data, 2) Generate training and test samples under specific shift conditions, 3) Train logistic regression classifier on training samples, 4) Apply each quantification method to test samples, 5) Calculate MAE for each method under each shift condition, 6) Perform statistical analysis and generate visualizations
- **Design tradeoffs:** Using logistic regression as the base classifier provides calibrated probabilities needed by some quantification methods, but may not be optimal for all types of data; artificial generation of shift conditions allows controlled experiments but may not fully capture real-world complexity; choice of MAE as evaluation metric emphasizes absolute errors but doesn't penalize large errors more heavily than small ones
- **Failure signatures:** Poor performance of all methods under concept shift indicates that none of the tested methods can handle arbitrary changes in functional relationship between features and labels; instability of ACC under extreme class imbalance (pL = 0.02 or 0.98) suggests that methods relying on estimating true positive and false positive rates can fail when these rates are based on very few examples; improvement of CC and PCC under local covariate shift is an artifact that shouldn't be generalized - it only occurs under specific conditions
- **First 3 experiments:** 1) Prior probability shift: Vary pL and pU independently while keeping class-conditional distributions constant, 2) Global pure covariate shift: Vary proportion of documents from different categories while keeping class prevalence constant, 3) Global mixed covariate shift: Vary both proportion of documents from different categories and class prevalence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a quantification method be developed that maintains robust performance across all types of dataset shift, including prior probability shift, covariate shift (global and local), and concept shift?
- Basis in paper: [explicit] The authors conclude that "no existing quantification method seems to be robust enough to dealing with all the types of dataset shift we simulate in our experiments" and suggest this as a direction for future work
- Why unresolved: Current methods like SLD perform well under prior probability shift but degrade under other types of shift. The fundamental incompatibility between different shift types makes it challenging to design a universal solution
- What evidence would resolve it: Development and experimental validation of a new quantification method that achieves consistent performance across synthetic datasets simulating all shift types, with mean absolute error comparable to the best-performing method for each individual shift type

### Open Question 2
- Question: How does the "local covariate shift" phenomenon affect quantification methods in real-world applications where subclasses are aggregated into coarser categories?
- Basis in paper: [explicit] The authors identify local covariate shift as particularly relevant for applications like plankton species estimation and seabed cover mapping, where changes in subclass prevalence affect overall class distributions
- Why unresolved: The paper only simulates this shift type using synthetic data with controlled conditions. Real-world data may have more complex relationships between subclasses and superclasses that could amplify or mitigate these effects
- What evidence would resolve it: Empirical studies on real-world quantification tasks where subclass distributions are known to change over time, measuring how different quantification methods respond to these changes and whether the "serendipitous" bias compensation observed in synthetic data occurs naturally

### Open Question 3
- Question: What are the theoretical conditions under which PCC's assumption of posterior probability reliability fails during covariate shift, and can these conditions be practically detected?
- Basis in paper: [explicit] The authors cite Tasche (2022) showing that PCC requires test distribution to be absolutely continuous with respect to training distribution, but note these theoretical considerations are hard to verify in practice
- Why unresolved: While theoretical conditions are known, practical methods for detecting when these conditions are violated during deployment remain undeveloped. The paper shows PCC degrades under covariate shift but doesn't provide a framework for identifying when this degradation will occur
- What evidence would resolve it: Development of diagnostic metrics that can be computed from training and test data to predict when PCC will fail, validated across multiple real-world datasets exhibiting different degrees of covariate shift

## Limitations
- All experiments rely on synthetic data generation rather than real-world distribution shifts
- Study focuses exclusively on binary quantification, leaving questions about multi-class settings
- Evaluation is limited to a single dataset (Amazon product reviews) and single classifier type (logistic regression)

## Confidence
- Claims about SLD and PACC performance under mixed covariate shift: High confidence based on multiple independent experiments and statistical validation
- Claims regarding PCC's performance degradation under prior probability changes: Medium confidence, as they depend on specific classifier calibration quality
- Claims about unexpected improvement of CC and PCC under local covariate shift: Low confidence due to contrived nature of experimental setup and potential for artifact

## Next Checks
- Validate the local covariate shift compensation effect on real-world data where local distribution changes are known to occur
- Test the quantification methods on non-linear classifiers like neural networks to assess whether findings hold beyond logistic regression
- Implement the quantification methods in a streaming setting where distribution shifts occur gradually over time rather than in discrete experimental conditions