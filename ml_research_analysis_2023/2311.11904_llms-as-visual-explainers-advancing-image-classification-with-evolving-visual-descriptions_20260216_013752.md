---
ver: rpa2
title: 'LLMs as Visual Explainers: Advancing Image Classification with Evolving Visual
  Descriptions'
arxiv_id: '2311.11904'
source_url: https://arxiv.org/abs/2311.11904
tags:
- class
- visual
- descriptions
- classification
- descriptors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving image classification
  accuracy by optimizing textual class descriptors using large language models (LLMs)
  and vision-language models (VLMs). The core idea is to iteratively refine class
  descriptions through an evolutionary optimization process, leveraging visual feedback
  from VLMs to guide the LLM in generating more discriminative and accurate descriptions.
---

# LLMs as Visual Explainers: Advancing Image Classification with Evolving Visual Descriptions

## Quick Facts
- arXiv ID: 2311.11904
- Source URL: https://arxiv.org/abs/2311.11904
- Authors: 
- Reference count: 40
- Primary result: 6.40% average improvement in top-1 accuracy across nine benchmark datasets using LLM-optimized class descriptors

## Executive Summary
This paper presents a novel approach to improving image classification accuracy by iteratively optimizing textual class descriptors using large language models (LLMs) guided by visual feedback from vision-language models (VLMs). The method treats descriptor optimization as an evolutionary search problem, where an LLM agent generates new descriptor candidates through mutation and crossover operations, with CLIP providing visual feedback in the form of classification accuracy and confusion matrices. The approach achieves significant performance gains across nine benchmark datasets, demonstrating both improved accuracy and interpretability compared to vanilla CLIP.

## Method Summary
The method combines LLMs with VLMs in an iterative optimization framework where an LLM agent refines class descriptors based on visual feedback from CLIP. The process begins with clustering class names by similarity, then generates initial descriptors which are iteratively improved through mutation (generating K=4 new candidates by replacing ni=15 descriptors) and crossover operations. Visual feedback from CLIP's classification accuracy and an improved confusion matrix with hyperparameters λ=0.9 and m=3 top confusing classes guides the optimization. Memory banks store positive and negative history descriptors to avoid repeating mistakes and reinforce successful patterns. The optimization runs for N=10 iterations starting with n0=30 descriptors per class.

## Key Results
- 6.40% average improvement in top-1 accuracy across nine benchmark datasets compared to vanilla CLIP
- Significant performance gains across diverse datasets including ImageNet, EuroSAT, UCF101, SUN, Caltech, DTD, CIFAR-10, Flowers102, and CUB
- Strong transferability of optimized descriptors across different backbone models
- Improved interpretability through more discriminative and accurate class descriptions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Iterative refinement guided by visual feedback enables LLMs to progressively improve class descriptors for better visual classification.
- **Mechanism**: The method formulates descriptor optimization as a combinatorial search problem. An LLM agent uses mutation and crossover operators to generate new descriptor candidates. Visual feedback from CLIP provides classification metrics (accuracy, confusion matrix) that serve as a fitness function for natural selection of the best descriptors.
- **Core assumption**: Visual feedback from CLIP provides meaningful signals about descriptor quality that can guide LLM optimization.
- **Evidence anchors**:
  - [abstract] "Our method develops an LLM-based agent with an evolutionary optimization strategy to iteratively refine class descriptors. We demonstrate our optimized descriptors are of high quality which effectively improves classification accuracy..."
  - [section 3.2] "We propose a novel image classification framework combining VLMs with LLMs, named Iterative Optimization with Visual Feedback. In particular, our method develops an LLM-based agent, employing an evolutionary optimization strategy to refine class descriptors. Crucially, we incorporate visual feedback from VLM classification metrics..."
  - [corpus] Weak - no direct corpus evidence for the effectiveness of visual feedback in guiding LLM optimization

### Mechanism 2
- **Claim**: Improved confusion matrix with top-m classes captures more nuanced inter-class relationships than standard confusion matrix.
- **Mechanism**: The improved confusion matrix categorizes predictions as positive samples based on a similarity threshold. It extracts the top-m most confusing classes for each row, providing a more focused view of class relationships.
- **Core assumption**: The improved confusion matrix provides more actionable information for the LLM to optimize descriptors compared to standard confusion matrix.
- **Evidence anchors**:
  - [section 3.2] "We propose an improved version of the confusion matrix to more effectively capture intricate relationships within classes. We define a confusing threshold λ and categorize each prediction as a positive sample based on its cosine similarity score compared to λ times the cosine similarity score of the ground-truth label."
  - [section 4.3] "The improved confusion matrix demonstrates overall superior performance to the standard confusion matrix. The conventional confusion matrix not only contains excessive redundant information, which may hamper the understanding of LLMs, but also discards certain critical information to discriminate related classes..."
  - [corpus] Weak - no direct corpus evidence for the effectiveness of the improved confusion matrix

### Mechanism 3
- **Claim**: Memory banks of positive and negative history descriptors help the LLM avoid repeating mistakes and reinforce successful patterns.
- **Mechanism**: The memory banks store descriptors that have been found to be beneficial or detrimental to classification accuracy in previous iterations. This information is incorporated into the LLM prompts to guide future descriptor generation.
- **Core assumption**: The memory banks provide useful historical context that helps the LLM make better decisions in subsequent iterations.
- **Evidence anchors**:
  - [section 3.2] "We introduce the idea of memory banks M consisting of positive and negative history class descriptors. We dynamically update the memory banks based on V(D) at the end of each iteration."
  - [section 4.3] "The memory bank significantly enhances the stability and robustness during the optimization process."
  - [corpus] Weak - no direct corpus evidence for the effectiveness of memory banks in this context

## Foundational Learning

- **Concept**: Genetic algorithms and evolutionary optimization
  - **Why needed here**: The complex search space of possible descriptor combinations requires an efficient exploration strategy. Genetic algorithms provide a framework for iterative improvement through mutation, crossover, and selection.
  - **Quick check question**: What are the three main operators in a genetic algorithm and what is the role of each in the optimization process?

- **Concept**: Zero-shot learning and vision-language models
  - **Why needed here**: The method leverages CLIP's ability to perform zero-shot image classification by comparing image embeddings to text embeddings. Understanding how CLIP works is crucial for interpreting the visual feedback.
  - **Quick check question**: How does CLIP perform zero-shot image classification without being explicitly trained on the target dataset?

- **Concept**: Large language models and prompt engineering
  - **Why needed here**: The LLM is used as an optimization agent, requiring careful prompt design to guide its behavior. Understanding how to effectively prompt LLMs is essential for the method's success.
  - **Quick check question**: What are some key considerations when designing prompts for LLMs to perform specific tasks?

## Architecture Onboarding

- **Component map**: Class labels → K-means clustering → Initial descriptor generation → Iterative optimization (mutation, crossover, selection) → Visual feedback → Memory update → Final optimized descriptors
- **Critical path**: Class labels → K-means clustering → Initial descriptor generation → Iterative optimization (mutation, crossover, selection) → Visual feedback → Memory update → Final optimized descriptors
- **Design tradeoffs**:
  - Number of iterations vs. computational cost
  - Number of descriptors per class vs. specificity vs. generalization
  - Size of memory banks vs. historical context vs. noise
  - Threshold λ for confusion matrix vs. sensitivity vs. specificity
- **Failure signatures**:
  - Convergence to suboptimal solutions
  - Oscillations in descriptor quality without improvement
  - Overfitting to specific classes or datasets
  - Degradation in performance on certain classes
- **First 3 experiments**:
  1. Run the optimization process on a small, well-understood dataset (e.g., CIFAR-10) with a limited number of classes and iterations to verify the basic functionality.
  2. Compare the performance of the optimized descriptors against the baseline CLIP model on a larger, more diverse dataset (e.g., ImageNet) to assess the method's effectiveness.
  3. Analyze the impact of the improved confusion matrix and memory banks by running ablations with and without these components on a challenging dataset (e.g., CUB) to understand their contributions to the overall performance.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- The core assumption that CLIP's visual feedback provides meaningful optimization signals for LLM-generated descriptors remains partially validated
- The improved confusion matrix methodology introduces hyperparameters (λ threshold, m top confusing classes) that may require dataset-specific tuning
- The memory bank mechanism's effectiveness relies on the assumption that past descriptor performance reliably predicts future utility

## Confidence
- High confidence in the overall methodology and experimental setup, with clear implementation details and multiple benchmark datasets
- Medium confidence in the specific contribution of the improved confusion matrix and memory banks to performance gains, as these components are presented as beneficial but not rigorously isolated in ablation studies
- Medium confidence in the generalizability of results, given that all experiments use CLIP ViT-B/32 as the VLM backbone without testing on alternative vision-language models

## Next Checks
1. Conduct a systematic ablation study isolating the contributions of accuracy feedback, confusion matrix feedback, and memory banks to understand which components drive the most performance improvement
2. Test the optimized descriptors across multiple VLM backbones (e.g., CLIP ResNet, BLIP, Flamingo) to assess true transferability beyond the specific CLIP model used in training
3. Perform sensitivity analysis on the confusion matrix hyperparameters (λ threshold, m top classes) across different dataset characteristics to establish guidelines for parameter selection