---
ver: rpa2
title: Classifying Organizations for Food System Ontologies using Natural Language
  Processing
arxiv_id: '2309.10880'
source_url: https://arxiv.org/abs/2309.10880
tags:
- food
- organization
- organizations
- classification
- issues
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of natural language processing (NLP)
  methods to automatically classify entities for the purpose of knowledge graph population
  and integration with food system ontologies. The authors created NLP models that
  can automatically classify organizations with respect to categories associated with
  environmental issues as well as Standard Industrial Classification (SIC) codes,
  which are used by the U.S.
---

# Classifying Organizations for Food System Ontologies using Natural Language Processing

## Quick Facts
- arXiv ID: 2309.10880
- Source URL: https://arxiv.org/abs/2309.10880
- Reference count: 0
- Key outcome: NLP models achieved 80.1% micro-F1 for environmental issues classification and 69.9% for SIC code classification

## Executive Summary
This paper explores the use of natural language processing (NLP) methods to automatically classify entities for knowledge graph population and integration with food system ontologies. The authors developed NLP models that classify organizations based on environmental issue categories and Standard Industrial Classification (SIC) codes using text snippets from Google search results. The models demonstrate reasonable performance, achieving 80.1% micro-averaged F1-score for environmental issues classification and 69.9% for SIC code classification. The approach shows promise for automatically harvesting information to populate knowledge graphs and aligning with existing ontologies through shared categories and concepts.

## Method Summary
The authors used BERT-based models fine-tuned for two classification tasks: environmental issues classification and SIC code classification. They collected text snippets from Google search results for each organization, concatenating the top 10 results into pseudo-documents. Two model architectures were developed: OrgModel-1 using only organization text, and OrgModel-2 incorporating category descriptions. The models were trained on labeled datasets of organizations mapped to environmental categories (15 categories, 1,870 organizations) and SIC codes (27 categories, 5,400 organizations). Performance was evaluated using micro and macro averaged precision, recall, and F1-scores.

## Key Results
- Environmental issues classification achieved 80.1% micro-averaged F1-score and 74.0% macro-averaged F1-score
- SIC code classification achieved 69.9% micro-averaged F1-score and 66.9% macro-averaged F1-score
- Google snippets outperformed SEC 10-K filings as text sources for organization classification
- Incorporating category descriptions improved classification performance for environmental issues

## Why This Works (Mechanism)

### Mechanism 1
BERT's pre-training on massive text corpora allows it to capture general language patterns that transfer effectively to the organization classification tasks. The model learns general linguistic representations during pre-training, then fine-tuning adapts these to the specific classification task with relatively little task-specific data. Core assumption: General language understanding transfers across domains when fine-tuned with relevant examples.

### Mechanism 2
Google search snippets provide more relevant and diverse textual descriptions than SEC 10-K filings for organization classification. Google snippets aggregate information from multiple sources about an organization, capturing its activities and mission from various perspectives, while 10-K filings are narrow in scope and often empty. Core assumption: Multi-source information about an organization provides better context for classification than single-source financial reports.

### Mechanism 3
Incorporating category descriptions during classification improves model performance by providing richer semantic representations of categories. The model learns to compare organization text with category definitions, creating stronger associations between organizations and appropriate categories. Core assumption: Providing explicit category definitions helps the model understand semantic relationships better than learning categories from examples alone.

## Foundational Learning

- Concept: Fine-tuning pre-trained language models
  - Why needed here: The paper relies on adapting BERT to specific classification tasks rather than training from scratch
  - Quick check question: What is the key difference between pre-training and fine-tuning in transformer-based models?

- Concept: Multi-label classification
  - Why needed here: Organizations can belong to multiple environmental issue categories and potentially multiple SIC codes
  - Quick check question: How does the loss function differ between multi-label and single-label classification?

- Concept: Data sparsity and class imbalance
  - Why needed here: The paper addresses challenges with rare categories having few training examples
  - Quick check question: What strategies can be used to handle class imbalance in classification tasks?

## Architecture Onboarding

- Component map: Input text → BERT embedding → Classification layer → Output predictions
- Critical path: Input text → BERT embedding → Classification layer → Loss computation → Parameter updates during training
- Design tradeoffs: Google snippets vs. 10-K filings (coverage vs. official documentation), short vs. long category descriptions (efficiency vs. informativeness)
- Failure signatures: Low recall on rare categories, performance gaps between common and rare classes, overfitting to training data
- First 3 experiments:
  1. Train OrgModel-1 with Google snippets only to establish baseline performance
  2. Train OrgModel-2 with environmental issue descriptions to test the impact of additional context
  3. Compare performance using 10-K filings vs. Google snippets for SIC code classification

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the NLP models change when using text snippets from 20 retrieved websites instead of 10? The paper mentions that each pseudo-document is created by concatenating text snippets from the top 10 retrieved websites for each organization, and suggests that using more sources could provide a better picture of the organization.

### Open Question 2
How does the classification accuracy change when using text from an organization's official website instead of Google snippets? The paper discusses the challenges of using official websites due to crawling restrictions but notes that Google typically ranks official websites highly, suggesting potential value in their content.

### Open Question 3
How does the performance of the NLP models vary across different languages and cultural contexts? The paper focuses on English-language data and U.S. SIC codes, but mentions the potential for applying the approach to other classification problems, suggesting possible generalizability issues.

## Limitations

- Data quality and source reliability issues with Google snippets, which may not consistently capture accurate or comprehensive information about organizations
- Significant data sparsity issues with SEC 10-K filings, with many organizations lacking sufficient textual content
- Potential biases in Google search results and temporal changes affecting the availability and relevance of snippet-based descriptions

## Confidence

**High Confidence Claims:**
- BERT-based models can achieve reasonable performance on multi-label organization classification tasks
- Fine-tuning pre-trained language models transfers effectively to domain-specific classification
- Google snippets provide richer textual descriptions than 10-K filings for this application

**Medium Confidence Claims:**
- Category description concatenation improves classification performance
- The specific architecture design (OrgModel-1 vs OrgModel-2) meaningfully impacts results
- The 80.1% and 69.9% F1-scores represent robust performance across different category distributions

**Low Confidence Claims:**
- The specific hyperparameter choices are optimal for these tasks
- The models will generalize well to organizations outside the tested domains
- The performance gap between Google snippets and 10-K filings will persist across different organization types

## Next Checks

1. **Cross-temporal validation**: Re-run the classification models using Google snippets collected at different time points (e.g., 6 months apart) to assess the stability and reliability of the text source over time, particularly for organizations with evolving missions or activities.

2. **Domain expert evaluation**: Have domain experts manually review a stratified sample of 100+ organization predictions across different performance tiers to validate whether the model classifications align with human judgment and identify systematic errors.

3. **Alternative text source comparison**: Test the classification models using Wikipedia pages or company websites as alternative text sources to determine if Google snippets provide unique value or if other readily available sources could yield comparable or better performance.