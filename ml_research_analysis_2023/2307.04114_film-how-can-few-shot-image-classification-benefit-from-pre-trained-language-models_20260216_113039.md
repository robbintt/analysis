---
ver: rpa2
title: 'FILM: How can Few-Shot Image Classification Benefit from Pre-Trained Language
  Models?'
arxiv_id: '2307.04114'
source_url: https://arxiv.org/abs/2307.04114
tags:
- learning
- few-shot
- textual
- visual
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FILM, a novel few-shot image classification
  framework that leverages pre-trained language models (PLMs) through contrastive
  learning. The core idea is to align visual features with textual embeddings extracted
  from class names using a PLM.
---

# FILM: How can Few-Shot Image Classification Benefit from Pre-Trained Language Models?

## Quick Facts
- arXiv ID: 2307.04114
- Source URL: https://arxiv.org/abs/2307.04114
- Authors: 
- Reference count: 40
- One-line primary result: FILM achieves state-of-the-art few-shot image classification by aligning visual features with textual embeddings from pre-trained language models using contrastive learning

## Executive Summary
This paper introduces FILM, a novel few-shot image classification framework that leverages pre-trained language models (PLMs) through contrastive learning. The key innovation is aligning visual features with textual embeddings extracted from class names using a carefully designed textual branch with [MASK] token prompts and a learnable metric module. The framework uses MAML for bi-level optimization to adapt the metric module to different few-shot tasks. Experiments on multiple benchmarks demonstrate significant performance improvements over state-of-the-art methods.

## Method Summary
FILM uses a pre-trained language model (RoBERTa-base) with a hand-crafted prompt "[CLS] The appearance of yi is [MASK] . [SEP]" to generate textual embeddings from class names. Visual features are extracted using a ResNet-12 image encoder. A learnable metric module generalizes cosine similarity between the two modalities. The framework is trained using episodic meta-learning with MAML for bi-level optimization. During training, the metric module parameters are adapted to each few-shot task in the inner loop, then updated based on task performance in the outer loop.

## Key Results
- Achieves 69.52% accuracy on miniImageNet in 5-way 1-shot setting
- Achieves 73.28% accuracy on tieredImageNet in 5-way 1-shot setting
- Achieves 77.59% accuracy on CIFAR-FS in 5-way 1-shot setting
- Achieves 79.79% accuracy on CUB in 5-way 1-shot setting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The [MASK] token prompt bridges the modality gap between visual features and textual embeddings.
- Mechanism: By inserting class labels into a hand-crafted prompt containing [MASK], the PLM is encouraged to generate embeddings that are more semantically aligned with the visual appearance of the corresponding category.
- Core assumption: The PLM's pre-training on uni-modal text data creates an embedding space that is not naturally aligned with visual feature spaces.
- Evidence anchors:
  - [abstract] "To address the challenge of alignment between visual features and textual embeddings obtained from text-based pre-trained language model, we carefully design the textual branch of our framework..."
  - [section 4.1] "To make textual embeddings more relevant to the visual descriptive information of the corresponding categories, we design a prompt template with one [MASK] token as [CLS] The appearance of yi is [MASK] . [SEP]"
  - [corpus] Weak - corpus papers focus on different architectures; no direct evidence for [MASK] prompt alignment.
- Break condition: If the PLM's embedding space is inherently incompatible with visual semantics, or if the prompt fails to capture category-relevant information.

### Mechanism 2
- Claim: The learnable metric module generalizes cosine similarity to adapt alignment to task-specific semantics.
- Mechanism: The metric module learns a task-specific similarity measure between visual and textual embeddings, overcoming the limitations of fixed cosine similarity.
- Core assumption: The visual and textual embedding spaces have different structures that cannot be aligned by a fixed similarity metric.
- Evidence anchors:
  - [abstract] "we carefully design the textual branch of our framework and introduce a metric module to generalize the cosine similarity."
  - [section 4.2] "However, directly aligning visual features and textual embeddings extracted by text-based PLM with cosine similarity has a poor effect in few-shot setting."
  - [section 4.4] "Specifically, we calculate the contrastive loss on the support set S and the query set Q respectively."
- Break condition: If the metric module overfits to base classes and fails to generalize to novel classes.

### Mechanism 3
- Claim: MAML-based bi-level optimization enables task-specific adaptation of the metric module.
- Mechanism: The metric module's parameters are adapted to each few-shot task during the inner loop, then updated based on task performance in the outer loop.
- Core assumption: Few-shot tasks require different similarity metrics due to varying semantic relationships between classes.
- Evidence anchors:
  - [abstract] "For better transferability, we let the metric module adapt to different few-shot tasks and adopt MAML to train the model via bi-level optimization."
  - [section 4.4] "To let the metric module task-specific, we create copies of θM as the adapted parameters θ′M. In the inner loop, we adapt the model to the current task Tj by updating θ′M..."
  - [corpus] Weak - corpus neighbors do not discuss MAML adaptation in detail.
- Break condition: If the adaptation process destabilizes the metric module or if the meta-training distribution is too narrow.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: It pulls together matching image-text pairs while pushing apart non-matching pairs in the embedding space.
  - Quick check question: How does the contrastive loss in Eq(1) ensure that visual features move closer to their corresponding textual embeddings?

- Concept: Meta-learning with MAML
  - Why needed here: It enables fast adaptation to novel classes by learning good parameter initialization.
  - Quick check question: In bi-level optimization, what is the difference between the inner loop and outer loop updates?

- Concept: Prompt engineering for PLMs
  - Why needed here: It transforms class names into sentences that PLMs can process meaningfully.
  - Quick check question: Why is the [MASK] token used in the prompt template, and what does it represent?

## Architecture Onboarding

- Component map: Textual embeddings ← PLM + prompt → Metric Module ← Visual embeddings ← Image encoder
- Critical path: Textual embeddings ← PLM + prompt → Metric Module ← Visual embeddings ← Image encoder
- Design tradeoffs:
  - Fixed PLM vs. fine-tuned PLM: Using a frozen PLM avoids overfitting but may limit alignment capability.
  - Task-specific metric vs. fixed cosine: Adaptive metric improves alignment but increases training complexity.
- Failure signatures:
  - Poor performance if metric module fails to adapt to novel tasks.
  - Unstable training if temperature hyper-parameter is not tuned properly.
  - Misalignment if prompt design is not semantically meaningful.
- First 3 experiments:
  1. Replace the [MASK] token prompt with a [CLS] token prompt and compare accuracy.
  2. Remove the metric module and use cosine similarity directly; measure performance drop.
  3. Fix the metric module parameters and evaluate generalization to novel tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FILM change when using different pre-trained language models (e.g., BERT, GPT-3) instead of RoBERTa-base?
- Basis in paper: [explicit] The paper states "we adopt RoBERTa-base [50] as our text-based pre-trained language model" and notes that different PLMs could be used.
- Why unresolved: The authors only experiment with RoBERTa-base and don't compare performance across different PLMs.
- What evidence would resolve it: Direct comparison experiments using FILM with BERT, GPT-3, and other PLMs on the same benchmark datasets.

### Open Question 2
- Question: What is the impact of prompt engineering on FILM's performance, and can the hand-crafted prompts be further optimized?
- Basis in paper: [explicit] The paper discusses designing a prompt template "[CLS] The appearance of yi is [MASK] . [SEP]" and compares it to alternatives in Table 5.
- Why unresolved: While the paper shows their prompt outperforms alternatives, it doesn't explore whether better prompts could be engineered or if prompt tuning could improve results.
- What evidence would resolve it: Systematic exploration of different prompt templates and prompt tuning approaches to find optimal prompts for different datasets.

### Open Question 3
- Question: How does FILM's performance scale with increasing numbers of support examples per class (e.g., 20-shot, 50-shot settings)?
- Basis in paper: [explicit] The authors mention they evaluate up to 50-shot in Table 4, showing performance improves with more shots.
- Why unresolved: The paper doesn't investigate performance beyond 50 shots or analyze how FILM scales with very large shot counts.
- What evidence would resolve it: Experimental results showing FILM's accuracy at 100-shot, 500-shot, and 1000-shot settings across multiple datasets.

## Limitations

- The temperature parameter tuning appears critical but is not well-explained in terms of sensitivity analysis.
- The paper doesn't sufficiently address whether simpler fine-tuning approaches could achieve similar results to MAML-based optimization.
- The contribution of prompt design to alignment quality has limited empirical evidence.

## Confidence

- Prompt effectiveness: Medium confidence - theoretically motivated but limited empirical validation
- Metric module effectiveness: High confidence - consistent performance improvements across benchmarks
- MAML optimization necessity: Medium confidence - demonstrates implementation but doesn't compare to simpler alternatives

## Next Checks

1. **Prompt Sensitivity Analysis**: Systematically test FILM with alternative prompt templates (e.g., replacing [MASK] with [CLS] or different descriptive phrases) while keeping all other components fixed to quantify the specific contribution of prompt design to alignment quality.

2. **Metric Module Ablation**: Train FILM without the learnable metric module (using only cosine similarity) and with a fixed metric module (not updated during inner loop) to isolate whether the adaptation capability is the key differentiator.

3. **MAML vs. Fine-tuning Comparison**: Replace the MAML-based bi-level optimization with standard fine-tuning on each few-shot task while keeping the textual branch and metric module design identical, to determine if the meta-learning framework is essential for performance gains.