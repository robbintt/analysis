---
ver: rpa2
title: 'On student-teacher deviations in distillation: does it pay to disobey?'
arxiv_id: '2301.12923'
source_url: https://arxiv.org/abs/2301.12923
tags:
- distillation
- teacher
- student
- training
- one-hot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the deviations between teacher and student\
  \ probabilities in knowledge distillation, where the student is trained to mimic\
  \ the teacher\u2019s soft label distribution. While distillation intends to transfer\
  \ the teacher\u2019s predictions to the student, empirical results show systematic\
  \ deviations: the student tends to underfit the teacher on points where the teacher\
  \ has low confidence, i.e., the student achieves even lower confidence on those\
  \ points."
---

# On student-teacher deviations in distillation: does it pay to disobey?

## Quick Facts
- **arXiv ID**: 2301.12923
- **Source URL**: https://arxiv.org/abs/2301.12923
- **Reference count**: 40
- **Key outcome**: Students systematically underfit teachers on low-confidence points, and this deviation can improve generalization.

## Executive Summary
This paper investigates why knowledge distillation often leads to student models that deviate from their teachers, and whether these deviations are beneficial. Through extensive experiments on image and language datasets, the authors find that students tend to underfit teacher predictions on low-confidence points, achieving even lower confidence. Surprisingly, switching from one-hot to distillation loss mid-training can recover most distillation gains, suggesting early deviations aren't crucial. The paper provides theoretical explanations: distillation acts as a regularizer that exaggerates gradient descent's bias toward top eigendirections, and denoises gradients in the presence of class similarities. These mechanisms help explain why students can outperform teachers and why deviation from perfect matching can be beneficial.

## Method Summary
The paper conducts experiments using ResNet and MobileNet architectures on CIFAR10, CIFAR100, TinyImageNet, and RoBERTa on MNLI, AGNews, QQP, and IMDB datasets. The methodology involves training teacher models with cross-entropy loss, then training student models using distillation loss to match teacher probabilities. Key experimental variations include switching between one-hot and distillation loss at different training phases, and analyzing probability scatter plots comparing teacher vs. student predictions. The theoretical analysis uses linear model approximations to study how distillation affects gradient descent dynamics and eigendirection convergence.

## Key Results
- Students systematically underfit teacher predictions on points where teachers have low confidence, particularly in self-distillation settings
- Switching from one-hot to distillation loss mid-training can recover most of distillation's performance gains
- Distillation acts as a regularizer that exaggerates the implicit bias of gradient descent toward top eigendirections
- Distillation denoises gradients in the presence of class similarities by flipping the sign of destructive one-hot gradients

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The student underfits the teacher on points where the teacher has low confidence, leading to better generalization.
- **Mechanism**: Distillation acts as a regularizer that exaggerates the implicit bias of gradient descent toward top eigendirections of the data. This exaggeration causes the student to rely less on lower eigendirections, which correspond to noisy or ambiguous patterns, thus improving generalization.
- **Core assumption**: The dataset has a spectrum of eigendirections with top ones being more generalizable; low-confidence teacher points correspond to lower eigendirections.
- **Evidence anchors**:
  - [abstract] "distillation exaggerates the implicit bias of gradient descent in converging faster along the top eigendirections"
  - [section] Theorem 4.1 proves the student moves even faster along top directions than the teacher
  - [corpus] Weak: Related work focuses on attention/fidelity but not eigendirection exaggeration
- **Break condition**: If the dataset has no clear eigendirection structure or if top directions are noisy, exaggeration could harm performance.

### Mechanism 2
- **Claim**: Distillation denoises gradients in the presence of class similarities, allowing the student to deviate constructively from the teacher.
- **Mechanism**: When class similarities exist, one-hot gradients on non-target classes can be destructive. Distillation flips the sign of these gradients by using the teacher's soft probabilities, effectively providing "fresh examples" that guide the student toward better feature learning.
- **Core assumption**: Class similarities create destructive one-hot gradients; teacher's early-stopped soft probabilities are constructive.
- **Evidence anchors**:
  - [abstract] "distillation denoises gradients in the presence of class similarities"
  - [section] Theorem 4.3 shows distillation gradients have better alignment with ground truth than one-hot or teacher weights
  - [corpus] Weak: Most neighbor papers focus on fidelity/attention, not gradient denoising under class similarity
- **Break condition**: If there are no class similarities or if teacher probabilities are already poor, denoising benefit disappears.

### Mechanism 3
- **Claim**: Early-phase deviations are not crucial; switching to distillation mid-training can recover most gains.
- **Mechanism**: The beneficial regularization and denoising effects of distillation can be applied at any training phase. Late distillation still allows the student to exaggerate eigendirection bias and denoise gradients, recovering performance gains without relying on early-phase representation conditioning.
- **Core assumption**: Distillation's effects are not time-locked to early training dynamics.
- **Evidence anchors**:
  - [abstract] "switching to distillation loss in the middle of training can recover much of its gains"
  - [section] Fig 2 shows late distillation recovers nearly all gains
  - [corpus] Weak: Neighbor work does not discuss mid-training loss switching
- **Break condition**: If early-phase representation conditioning is actually critical for the specific architecture or task, mid-training switch may underperform.

## Foundational Learning

- **Concept**: Implicit bias of gradient descent
  - **Why needed here**: The paper's main mechanism relies on distillation exaggerating GD's natural bias toward top eigendirections; understanding this bias is essential to grasp why distillation helps.
  - **Quick check question**: In standard GD training, which eigendirections does the model converge along faster—top or bottom? Why?

- **Concept**: Eigendirections and spectral properties of data
  - **Why needed here**: The regularization effect of distillation is formalized in eigenspace; knowing how data eigenvectors relate to generalization is key to understanding the theory.
  - **Quick check question**: How does the alignment of learned weights with top eigendirections typically correlate with generalization performance?

- **Concept**: Knowledge distillation and soft label training
  - **Why needed here**: The entire paper contrasts one-hot loss vs. distillation loss; understanding the mechanics of soft label supervision is foundational.
  - **Quick check question**: What is the main difference between one-hot loss and distillation loss in terms of target labels provided to the student?

## Architecture Onboarding

- **Component map**: Data -> Teacher Network -> Student Network -> Loss Function (One-hot vs Distillation) -> Training Scheduler -> Evaluation
- **Critical path**: Load data → Train teacher → Initialize student → Train student with one-hot or distillation → Evaluate deviations → Analyze eigendirection trajectories or gradient quality
- **Design tradeoffs**: Distillation temperature (controls softness of teacher probabilities), Distillation weight (balance between distillation and task loss), Timing of loss switch (early vs. late distillation), Model capacity parity (self-distillation vs. cross-architecture)
- **Failure signatures**: No underfitting pattern observed (suggests no eigendirection structure or too much training), Accuracy drop after distillation switch (suggests destructive gradients still dominant), High KL divergence but no performance gain (suggests deviation not beneficial)
- **First 3 experiments**:
  1. Train teacher and student with one-hot loss; plot logit-transformed probabilities to observe deviation baseline.
  2. Switch to distillation mid-training; compare final performance and deviation patterns to full distillation.
  3. Train student with distillation from start; analyze eigendirection convergence trajectories to verify exaggerated bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does underfitting of low-confidence points consistently lead to improved generalization in distillation, or is this effect dataset-dependent?
- Basis in paper: [explicit] The paper demonstrates underfitting in many cases but also notes exceptions where underfitting does not correlate with improved generalization, especially in language datasets.
- Why unresolved: The paper shows underfitting is a common pattern but does not conclusively establish whether it is causally linked to generalization gains or merely correlated in some cases.
- What evidence would resolve it: Controlled experiments varying confidence levels and measuring generalization across diverse datasets would clarify if underfitting causally improves performance.

### Open Question 2
- Question: Can the regularization effect of distillation (exaggerating bias toward top eigendirections) be leveraged to design distillation methods that do not require a teacher model?
- Basis in paper: [inferred] The paper suggests that distillation's bias toward top eigendirections contributes to its benefits, and notes that zeroing out destructive one-hot gradients might yield teacher-free gains.
- Why unresolved: The paper hints at this possibility but does not explore algorithmic designs that mimic distillation's regularization without a teacher.
- What evidence would resolve it: Experiments comparing teacher-free methods that simulate distillation's eigendirection bias against standard distillation would demonstrate feasibility.

### Open Question 3
- Question: How does the interaction between early-stopping and distillation change in multi-layer neural networks, and does the regularization effect compound across layers?
- Basis in paper: [explicit] The paper conjectures that the eigenspace regularization effect may compound across layers but does not explore this in depth.
- Why unresolved: The theoretical analysis is limited to linear models, and empirical verification in deep networks is not provided.
- What evidence would resolve it: Training multi-layer networks with varying early-stopping times and analyzing layer-wise eigendirection biases would test if the effect compounds.

### Open Question 4
- Question: What is the precise mechanism by which switching to one-hot loss in the middle of training can sometimes outperform distillation, and under what conditions does this occur?
- Basis in paper: [explicit] The paper observes that switching to one-hot after distillation can preserve or even improve accuracy in some cases (e.g., ResNet50 on TinyImageNet), but does not fully explain why.
- Why unresolved: The paper notes this phenomenon but lacks a formal explanation for when and why one-hot loss can be beneficial after distillation.
- What evidence would resolve it: Systematic ablation studies varying switch timing, dataset complexity, and network architecture would identify conditions favoring one-hot loss post-distillation.

## Limitations

- Theoretical analysis relies on linear model approximations that may not capture deep nonlinear network behavior
- Limited architectural diversity, focusing primarily on classification tasks with standard CNN and Transformer architectures
- Mechanisms proposed lack comprehensive ablation studies to isolate individual contributions
- Does not extensively explore how dataset characteristics affect deviation patterns

## Confidence

- **High confidence**: Systematic underfitting on low-confidence points is consistently demonstrated; mid-training distillation switching reliably recovers gains
- **Medium confidence**: Theoretical explanations for eigendirection exaggeration and gradient denoising are plausible but rely on linear approximations
- **Low confidence**: Causal link between underfitting and generalization improvements requires more direct evidence

## Next Checks

1. **Targeted Ablation Study**: Design experiments that artificially constrain the student to either perfectly match teacher probabilities or explicitly underfit low-confidence points, to directly test which behavior drives performance gains.

2. **Architectural Generalization**: Extend experiments to diverse architectures (CNNs, Transformers, Vision Transformers) and tasks (object detection, semantic segmentation) to assess whether deviation patterns hold beyond classification.

3. **Dataset Dependency Analysis**: Systematically vary dataset characteristics (class similarity structure, confidence distribution, feature dimensionality) to determine which properties are most predictive of beneficial deviation patterns.