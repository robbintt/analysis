---
ver: rpa2
title: 'UPB at IberLEF-2023 AuTexTification: Detection of Machine-Generated Text using
  Transformer Ensembles'
arxiv_id: '2308.01408'
source_url: https://arxiv.org/abs/2308.01408
tags:
- learning
- language
- which
- dataset
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The UPB team participated in the AuTexTification shared task at
  IberLEF-2023, focusing on detecting machine-generated text from human-written documents
  in English and Spanish. The team experimented with various machine learning and
  deep learning models, including shallow learning models (readability scores, string
  kernels) and deep learning models (Transformer ensembles, multi-task learning, and
  virtual adversarial training).
---

# UPB at IberLEF-2023 AuTexTification: Detection of Machine-Generated Text using Transformer Ensembles

## Quick Facts
- arXiv ID: 2308.01408
- Source URL: https://arxiv.org/abs/2308.01408
- Reference count: 32
- Best macro F1-scores: 66.63% English, 67.10% Spanish

## Executive Summary
The UPB team participated in the AuTexTification shared task at IberLEF-2023, focusing on detecting machine-generated text from human-written documents in English and Spanish. The team experimented with various machine learning and deep learning models, including shallow learning models (readability scores, string kernels) and deep learning models (Transformer ensembles, multi-task learning, and virtual adversarial training). The best-performing model achieved macro F1-scores of 66.63% on the English dataset and 67.10% on the Spanish dataset. The team's approach demonstrates the effectiveness of combining multiple models into ensembles for improved detection performance.

## Method Summary
The UPB team developed a text detection system using a combination of shallow and deep learning approaches. For shallow models, they used readability scores and string kernels (n-grams of length 3-5) with SVM classifiers. The deep learning approach centered on fine-tuning pre-trained Transformer models (XLM-RoBERTa, multilingual BERT, TwHIN-BERT) with multi-task learning (language identification as auxiliary task) and virtual adversarial training for regularization. The final predictions were generated through ensemble methods, using XGBoost to combine probabilities and binary predictions from multiple trained models.

## Key Results
- Best macro F1-scores: 66.63% English, 67.10% Spanish
- Deep learning ensembles outperformed shallow learning models (58.65% vs 66.63% English, 56.69% vs 67.10% Spanish)
- Ensemble approach improved performance over individual base models
- Multi-task learning and virtual adversarial training were implemented but not systematically compared against baseline

## Why This Works (Mechanism)

### Mechanism 1: Multi-task Learning (MTL)
MTL improves generalization by forcing the model to learn language identification alongside human/computer classification. Shared representations trained on two correlated tasks improve robustness by reducing overfitting to a single label space. Core assumption: the two tasks are sufficiently related that shared representations capture useful linguistic patterns for both.

### Mechanism 2: Virtual Adversarial Training (VAT)
VAT regularizes the model by making it robust to small input perturbations. By computing gradients that maximize loss under small input changes, VAT forces the model to learn smoother decision boundaries that generalize better to unseen data. Core assumption: the adversarial perturbations capture realistic variations in text inputs that could occur in real-world data.

### Mechanism 3: Model Ensembles
Model ensembles reduce variance by combining diverse models' predictions, improving overall detection accuracy. Different models capture different aspects of the detection problem; combining them through stacking allows a meta-learner to exploit complementary strengths. Core assumption: individual models have different error patterns that can be averaged out when combined.

## Foundational Learning

- **Binary classification and threshold selection**: Needed for distinguishing human vs. machine-generated text; optimal threshold selection balances precision and recall. Quick check: What threshold strategy did the team use to avoid favoring precision or recall in the balanced dataset?

- **Transfer learning with pre-trained Transformers**: Fine-tuning BERT variants allows leveraging large-scale language understanding without training from scratch. Quick check: Which BERT variants were experimented with and why were multilingual models generally better?

- **Multi-task learning and regularization**: Combining language detection with text classification and adding VAT helps prevent overfitting on limited training data. Quick check: How does the multi-task learning loss function combine the two binary classification tasks?

## Architecture Onboarding

- **Component map**: Raw text documents -> Transformer encoder -> Task head (human/computer) + MTL head (English/Spanish) -> VAT regularization -> Ensemble meta-learner (XGBoost)

- **Critical path**: 1) Load and preprocess text (minimal preprocessing for deep models) 2) Encode text with Transformer -> contextual embeddings 3) Apply dropout and feed to classification head(s) 4) Compute losses (binary cross-entropy for each task) 5) Add VAT regularization loss 6) Backpropagate and update weights 7) For ensembles, collect model outputs and train meta-learner

- **Design tradeoffs**: MTL adds complexity but can improve generalization; VAT adds training time but provides regularization; ensembles improve performance but require training and storing multiple models; threshold choice affects precision-recall tradeoff

- **Failure signatures**: Overfitting (high validation accuracy but low test accuracy); underfitting (low accuracy on both validation and test sets); threshold selection issues (precision or recall significantly imbalanced); model instability (large variance in results across runs)

- **First 3 experiments**: 1) Fine-tune single BERT model (XLM-RoBERTa) on English data only, test threshold selection 2) Add MTL head for language detection, train on combined English/Spanish dataset 3) Implement VAT regularization and compare validation performance with and without VAT

## Open Questions the Paper Calls Out

### Open Question 1
How would the performance of the models change if hyperparameter tuning techniques like grid search or Bayesian optimization were applied to the deep learning models? The authors did not perform systematic hyperparameter optimization for their deep learning models, only using default or fixed values.

### Open Question 2
How would the model performance be affected by using different validation strategies, such as k-fold cross-validation instead of a single fixed validation set? The authors used a single fixed validation set, which may not have been representative of the test set distribution.

### Open Question 3
How would increasing the training time (more epochs) affect the model performance, and would it lead to overfitting or improved generalization? The authors limited training to 2-4 epochs using early stopping, but did not investigate the effects of longer training times.

## Limitations

- Significant validation-test performance gap (93.3% vs 66.63% English validation vs test) suggests potential validation set bias or overfitting
- Limited hyperparameter exploration for deep learning models, only using fixed values
- No ablation studies to quantify individual contributions of MTL, VAT, and ensemble components

## Confidence

- **High confidence**: Ensemble approach effectiveness (consistent improvement from base models to ensemble results)
- **Medium confidence**: MTL and VAT contributions (claimed improvements but lack of ablation studies)
- **Low confidence**: Generalization claims (significant validation-test performance gap and limited hyperparameter exploration)

## Next Checks

1. **Ablation study design**: Create systematic comparison testing base Transformer models with and without MTL, VAT, and ensemble components to isolate each technique's contribution to final performance.

2. **Validation set size analysis**: Re-run experiments with larger validation splits (10%, 20%) to determine if the small 2% validation set is causing the validation-test performance gap observed in the results.

3. **Threshold sensitivity analysis**: Implement multiple threshold selection strategies (precision-recall balance, cost-sensitive thresholds, Youden's index) and compare their impact on macro F1-scores across both languages.