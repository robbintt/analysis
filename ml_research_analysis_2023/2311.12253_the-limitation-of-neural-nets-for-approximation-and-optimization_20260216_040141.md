---
ver: rpa2
title: The limitation of neural nets for approximation and optimization
arxiv_id: '2311.12253'
source_url: https://arxiv.org/abs/2311.12253
tags:
- function
- neural
- functions
- optimization
- basis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper evaluates neural networks as surrogate models for optimization
  problems, comparing them to traditional interpolation/regression methods. Key findings
  include: SiLU activation function performs best for function approximation; neural
  networks are competitive for zero- and first-order approximations but underperform
  on second-order (Hessian) approximations; combining neural net activation functions
  with natural quadratic basis can eliminate cross terms, reducing model parameters.'
---

# The limitation of neural nets for approximation and optimization

## Quick Facts
- arXiv ID: 2311.12253
- Source URL: https://arxiv.org/abs/2311.12253
- Reference count: 40
- Neural networks achieve competitive zero- and first-order approximations but underperform on second-order approximations compared to interpolation/regression models

## Executive Summary
This paper evaluates neural networks as surrogate models for derivative-free optimization problems, comparing them against traditional interpolation and regression methods. The study uses 38 CUTEst problems with varying dimensions and tests five different activation functions (ReLU, ELU, SiLU, Sigmoid, Tanh) across feedforward neural networks with 2 hidden layers. Performance is measured through function value, gradient, and Hessian approximation errors, as well as efficiency and robustness profiles when integrated into a state-of-the-art optimization algorithm.

## Method Summary
The research generates training data by sampling N1 = (n+1)(n+2)/2 points uniformly in a ball around the initial point for each CUTEst problem. Five neural network architectures with different activation functions are trained using the Adam optimizer with dynamic learning rate adjustment and 300 epochs. The models are evaluated on both training and testing datasets using performance profiles that measure efficiency (ρs(1)) and robustness (ρs(α)). The study also compares neural networks to interpolation/regression models including quadratic with natural basis, basis (3.4), basis (3.5), and radial basis functions. Additionally, the surrogates are tested as replacements for the objective function in the FLE derivative-free optimization algorithm.

## Key Results
- SiLU activation function provides the best performance for function approximation across all tested neural network architectures
- Neural networks deliver competitive zero- and first-order approximations but underperform on second-order (Hessian) approximations compared to interpolation/regression models
- Combining neural network activation functions with natural quadratic basis can eliminate cross terms, reducing model parameters from (n+1)(n+2)/2 to 3n+1
- When integrated into the FLE-S algorithm, none of the surrogate models significantly improve performance over the baseline method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SiLU activation function provides superior accuracy for approximating objective functions in neural network surrogate models.
- Mechanism: The SiLU (Sigmoid-weighted Linear Unit) activation function is defined as s(z) = z/(1 + e^(-z)). Unlike ReLU which can suffer from the "dying ReLU" problem where neurons become permanently inactive, SiLU maintains a smooth, non-zero gradient for negative inputs. This allows for more stable and effective gradient-based training, leading to better function approximations.
- Core assumption: The test functions used in the experiments have sufficient structure that the smooth, non-linear nature of SiLU provides better approximations than other activation functions.
- Evidence anchors:
  - [abstract] states "the evidence provided shows that SiLU has the best performance"
  - [section] reports "SiLU achieves the best performance on the testing dataset for both values of τ, followed by ELU and ReLU"
  - [corpus] contains related work on activation functions and neural networks
- Break condition: If the test functions were highly piecewise-linear or had discontinuities, ReLU might perform better than SiLU.

### Mechanism 2
- Claim: Combining neural network activation functions with natural quadratic basis can eliminate cross terms, reducing model parameters.
- Mechanism: When an activation function s is composed with the natural basis for quadratic interpolation/regression, it can implicitly capture the interaction effects between variables that would normally require explicit cross terms (x_i * x_j). This means the model can represent the same function with fewer parameters, specifically 3n+1 parameters instead of (n+1)(n+2)/2.
- Core assumption: The activation function s can effectively capture the interaction effects between variables without needing explicit cross terms in the basis.
- Evidence anchors:
  - [section] states "combining a neural net activation function with the natural basis for quadratic interpolation/regression can waive the necessity of including cross terms in the natural basis, leading to models with fewer parameters to determine"
  - [section] shows that "the composition of an activation function with the natural basis can lead to better function value and gradient approximations than the natural basis"
- Break condition: If the activation function is linear or if the interaction effects are too complex to be captured by the activation function alone, explicit cross terms may still be necessary.

### Mechanism 3
- Claim: Neural networks are competitive for zero- and first-order approximations but underperform on second-order approximations compared to interpolation/regression models.
- Mechanism: Neural networks excel at learning function values and gradients because they can effectively capture the non-linear relationships in the data. However, they struggle with second-order approximations (Hessians) because capturing curvature information requires a different type of model structure that is better suited to interpolation/regression approaches with explicit quadratic terms.
- Core assumption: The test functions have varying degrees of non-linearity and curvature that can be effectively captured by different types of models.
- Evidence anchors:
  - [abstract] states "neural networks can deliver competitive zero- and first-order approximations (at a high training cost) but underperform on second-order approximation"
  - [section] shows that "the interpolation/regression models provide better accuracy when they are used for second-order approximations"
- Break condition: If the test functions were all linear or had very simple curvature, neural networks might perform comparably for all orders of approximation.

## Foundational Learning

- Concept: Feedforward neural networks and their architecture
  - Why needed here: Understanding the basic structure of neural networks is essential for grasping how they function as surrogate models and why certain activation functions perform better than others.
  - Quick check question: What are the main components of a feedforward neural network and how do they interact?

- Concept: Interpolation and regression models for function approximation
  - Why needed here: The paper compares neural networks to traditional interpolation and regression models, so understanding these methods is crucial for evaluating the performance of neural networks.
  - Quick check question: How do interpolation and regression models differ in their approach to approximating functions, and when would you use each?

- Concept: Gradient-based optimization and its challenges
  - Why needed here: The paper discusses the performance of neural networks in approximating gradients and Hessians, which requires understanding gradient-based optimization and its potential issues like the vanishing gradient problem.
  - Quick check question: What are the main challenges in gradient-based optimization, and how do different activation functions address these challenges?

## Architecture Onboarding

- Component map:
  Input layer -> Hidden layers (activation functions) -> Output layer
  Training algorithm (Adam optimizer) -> Model evaluation (performance profiles)

- Critical path:
  1. Generate training data points from the objective function
  2. Build the neural network with chosen architecture and activation function
  3. Train the network to minimize the mean squared error between predicted and actual function values
  4. Evaluate the network's performance on a separate testing dataset
  5. Compare the results with interpolation/regression models

- Design tradeoffs:
  - Model complexity vs. training time: More complex networks may provide better approximations but require more training time
  - Number of training points vs. accuracy: More training points generally lead to better approximations but increase computational cost
  - Choice of activation function: Different activation functions have different properties that affect the network's ability to approximate various types of functions

- Failure signatures:
  - Poor generalization to testing data: May indicate overfitting or insufficient training data
  - Inaccurate gradient approximations: May suggest the activation function is not well-suited for capturing the function's behavior
  - Inaccurate Hessian approximations: May indicate the network architecture is not well-suited for capturing curvature information

- First 3 experiments:
  1. Compare the performance of different activation functions (ReLU, ELU, SiLU, Sigmoid, Tanh) on a simple test function with known properties
  2. Vary the number of training points and observe the effect on approximation accuracy for a given activation function
  3. Test the impact of network architecture (number of hidden layers, number of neurons per layer) on the approximation of a complex test function

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the limitations and unresolved aspects of the study, several areas for future research emerge from the analysis.

## Limitations
- The study focuses primarily on smooth test functions from CUTEst, leaving open questions about performance on non-smooth or noisy objectives
- The computational cost of training neural networks is substantial compared to interpolation models, though this tradeoff is not thoroughly quantified
- The choice of activation function and network architecture significantly impacts performance, but the sensitivity to these hyperparameters is not explored systematically

## Confidence
- **High confidence**: SiLU activation function's superior performance for function approximation; neural networks' competitive zero- and first-order approximations; the parameter reduction achieved by combining activation functions with natural quadratic basis
- **Medium confidence**: Underperformance on second-order approximations; performance profiling showing similar efficiency but lower robustness compared to interpolation/regression models
- **Low confidence**: Claims about neural networks not improving the state-of-the-art derivative-free optimization algorithm, as the comparison is limited to a single algorithm (FLE-S)

## Next Checks
1. Cross-architecture validation: Test the same neural network architectures on non-smooth or noisy test functions to verify the generality of the findings
2. Cost-benefit analysis: Quantify the computational tradeoff between training time and approximation accuracy for neural networks versus interpolation/regression models across different problem dimensions
3. Hyperparameter sensitivity: Conduct a systematic study of how network architecture choices (layers, neurons, activation functions) and training parameters affect approximation quality and robustness