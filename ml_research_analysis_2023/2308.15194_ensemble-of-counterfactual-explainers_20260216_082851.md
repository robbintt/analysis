---
ver: rpa2
title: Ensemble of Counterfactual Explainers
arxiv_id: '2308.15194'
source_url: https://arxiv.org/abs/2308.15194
tags:
- counterfactual
- counterfactuals
- explainers
- instances
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces ECE (Ensemble of Counterfactual Explainers),
  an approach that combines multiple weak counterfactual explainers to produce a stronger
  method that satisfies all desirable properties: minimality, actionability, plausibility,
  similarity, diversity, discriminative power, stability, and efficiency. The method
  samples instances and features to run base explainers, then selects counterfactuals
  using a diversity-driven function.'
---

# Ensemble of Counterfactual Explainers

## Quick Facts
- arXiv ID: 2308.15194
- Source URL: https://arxiv.org/abs/2308.15194
- Reference count: 16
- Key outcome: ECE combines multiple weak counterfactual explainers to achieve all nine desirable properties while outperforming state-of-the-art methods across tabular, image, and time series datasets

## Executive Summary
ECE (Ensemble of Counterfactual Explainers) addresses the challenge that individual counterfactual explainers typically satisfy only a subset of the nine desirable properties: minimality, actionability, plausibility, similarity, diversity, discriminative power, stability, and efficiency. By combining multiple weak base explainers through random sampling of instances and features, followed by a diversity-driven selection function, ECE achieves superior performance across all metrics. The method is model-agnostic and extends to non-tabular data through autoencoder wrapping, enabling uniform processing across diverse data types.

## Method Summary
ECE employs an ensemble approach that runs multiple base counterfactual explainers on randomly sampled subsets of instances and actionable features, similar to Random Forests. The method combines brute-force, tree-based, and sphere-based explainers, each optimizing different properties. For non-tabular data, an autoencoder wrapper maps instances to a continuous latent space where base explainers operate uniformly. A diversity-driven selection function then chooses the final k counterfactuals, balancing diversity against similarity. The ensemble approach allows ECE to capture complementary strengths of individual explainers while mitigating their weaknesses through random sampling and collective selection.

## Key Results
- Outperforms state-of-the-art explainers on all nine desirable properties across tabular, image, and time series datasets
- Achieves better diversity and efficiency while maintaining competitive similarity and stability metrics
- Successfully handles multiple data types through autoencoder wrapping, with reconstruction quality validated for each domain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random sampling of instances and features reduces bias and improves diversity
- Mechanism: By randomly selecting subsets of instances (X') and actionable features (A') for each base explainer, the ensemble avoids overfitting to any single region of the feature space or decision boundary
- Core assumption: The reference population contains sufficient variation and the sampling is representative enough to capture diverse counterfactual regions
- Evidence anchors:
  - [abstract]: "The ensemble runs base counterfactual explainers (bce) on a sample of instances and of features"
  - [section 4]: "Base explainers are invoked on a sample without replacement X' of instances from X (line 3), and on a random subset A' of the actionable features A (line 4), as in Random Forests"
- Break condition: If the reference population is too small or unrepresentative, random sampling may miss important counterfactual regions entirely

### Mechanism 2
- Claim: Combining multiple weak explainers captures complementary properties
- Mechanism: Different base explainers optimize for different subsets of the nine desirable properties, and their combination through the diversity-driven selection function achieves coverage of all properties simultaneously
- Core assumption: The base explainers are sufficiently diverse in their optimization targets and the selection function can effectively combine their outputs
- Evidence anchors:
  - [abstract]: "an approach that combines multiple weak counterfactual explainers to produce a stronger method that satisfies all desirable properties"
  - [section 1]: "we propose an ensemble of counterfactual explainers (ece) that, as in the case of ensemble of classifiers, boosts weak explainers to a powerful method covering all of the above desiderata"
- Break condition: If all base explainers share the same fundamental weakness, the ensemble cannot overcome this limitation

### Mechanism 3
- Claim: Autoencoder wrapping enables data-agnostic operation across tabular, image, and time series data
- Mechanism: The encoder maps data to a continuous latent space where base explainers operate uniformly, then the decoder maps results back to the original domain
- Core assumption: The autoencoder preserves sufficient information for meaningful counterfactual generation while enabling uniform processing
- Evidence anchors:
  - [abstract]: "through a wrapping approach based on autoencoders, it is also data-agnostic"
  - [section 4.3]: "We enable ece to work on data types other than tabular data by wrapping it around two functions. An encoder ζ : D→Rq that maps an instance from its actual domain D to a latent space of continuous features, and a decoder η : Rq→D that maps an instance of the latent space back to the actual domain"
- Break condition: If the autoencoder introduces significant information loss or cannot adequately represent actionable features in the latent space

## Foundational Learning

- Counterfactual explanations: Why needed here: Understanding what constitutes a valid counterfactual (different decision, actionable, plausible) is fundamental to evaluating any explainer
  - Quick check question: What are the three essential conditions that any counterfactual must satisfy?

- Distance metrics for mixed data types: Why needed here: The evaluation and generation of counterfactuals relies on appropriate distance calculations across different feature types
  - Quick check question: How does the mixed distance metric weight continuous versus categorical features?

- Ensemble methods in machine learning: Why needed here: The ensemble approach builds on established principles of combining multiple weak learners
  - Quick check question: What is the key insight from ensemble classifiers that applies to counterfactual explainers?

## Architecture Onboarding

- Component map:
  - Input layer: Instance x, black box b, reference instances X, actionable features A
  - Base explainers (bce-b, bce-t, bce-s): Generate candidate counterfactuals using different strategies
  - Random sampling module: Selects subsets X' and A' for each base explainer
  - Selection function S: Applies diversity-driven selection to choose final k counterfactuals
  - Autoencoder wrapper (for non-tabular data): Encodes/decodes instances for uniform processing
  - Output layer: Set of k counterfactuals

- Critical path: x → sampling → base explainers → collection → selection function → C

- Design tradeoffs: Diversity vs similarity (captured in the selection function's objective), computational efficiency vs thoroughness (sampling size), model-agnosticism vs optimal performance (autoencoder quality)

- Failure signatures: Insufficient diversity (all counterfactuals clustered), poor plausibility (counterfactuals outside reference population), instability (similar inputs yield very different outputs), slow runtime

- First 3 experiments:
  1. Run ECE with default parameters on a simple tabular dataset and verify it returns the requested number of counterfactuals
  2. Compare the diversity and similarity metrics of ECE outputs against a single base explainer
  3. Test the autoencoder wrapper by encoding/decoding instances and checking reconstruction quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical guarantee that ensemble methods will consistently improve counterfactual explanation properties (e.g., diversity, plausibility) across different types of black box models and datasets?
- Basis in paper: [inferred] The paper claims ECE outperforms state-of-the-art methods but does not provide theoretical analysis of why ensemble methods should improve counterfactual properties.
- Why unresolved: The paper relies on empirical evaluation rather than theoretical analysis to support claims about ECE's superiority.
- What evidence would resolve it: A mathematical proof or rigorous analysis showing that ensemble methods improve counterfactual properties under various conditions and model types.

### Open Question 2
- Question: How does ECE handle actionability constraints when using autoencoders for high-dimensional data like images and time series?
- Basis in paper: [explicit] The paper states that "we set A′ to be the whole set of latent features and hence, we are not able to deal with actionability constraints" when using autoencoders.
- Why unresolved: The paper acknowledges this limitation but does not propose a solution for maintaining actionability in high-dimensional data.
- What evidence would resolve it: A method for mapping actionable features from original space to latent space while preserving the autoencoder's dimensionality reduction benefits.

### Open Question 3
- Question: What is the optimal balance between diversity and similarity in counterfactual selection, and how does the parameter λ affect this trade-off?
- Basis in paper: [explicit] The paper uses a density-based objective function with a regularization parameter λ to balance diversity and similarity, but does not provide guidance on optimal λ values.
- Why unresolved: The paper uses a fixed λ value without exploring its sensitivity or providing guidelines for different use cases.
- What evidence would resolve it: Empirical studies showing how different λ values affect counterfactual quality across various datasets and the development of guidelines for selecting λ based on specific requirements.

## Limitations

- Autoencoder implementation details for non-tabular data are insufficiently specified, limiting reproducibility
- Actionability constraints cannot be maintained in latent space when using autoencoders for high-dimensional data
- The diversity-driven selection function's parameter sensitivity and optimal tuning are not explored

## Confidence

- **High confidence**: ECE's ability to improve diversity and efficiency compared to single base explainers
- **Medium confidence**: Claims about superior stability and discriminative power due to unspecified implementation details
- **Low confidence**: Generalizability across data types due to unclear autoencoder specifications and limited validation

## Next Checks

1. Replicate the tabular experiments with publicly available implementations of the three base explainers to verify the claimed improvements in diversity and efficiency
2. Test ECE's stability by running multiple times on identical inputs and measuring variance in outputs
3. Validate the autoencoder's preservation of actionable features by measuring reconstruction error specifically on the actionable feature subsets