---
ver: rpa2
title: Reliability-based cleaning of noisy training labels with inductive conformal
  prediction in multi-modal biomedical data mining
arxiv_id: '2309.07332'
source_url: https://arxiv.org/abs/2309.07332
tags:
- data
- training
- labeled
- wrongly
- cleaning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study proposes a reliability-based training data cleaning method
  using inductive conformal prediction (ICP) to improve classification performance
  in multi-modal biomedical machine learning tasks. The method leverages a small set
  of accurately labeled data to identify and correct mislabeled data and outliers
  in large quantities of noisy training data using ICP-calculated reliability metrics.
---

# Reliability-based cleaning of noisy training labels with inductive conformal prediction in multi-modal biomedical data mining

## Quick Facts
- arXiv ID: 2309.07332
- Source URL: https://arxiv.org/abs/2309.07332
- Reference count: 40
- The study proposes a reliability-based training data cleaning method using inductive conformal prediction (ICP) to improve classification performance in multi-modal biomedical machine learning tasks.

## Executive Summary
This study introduces a reliability-based training data cleaning method that leverages inductive conformal prediction (ICP) to identify and correct mislabeled data and outliers in large quantities of noisy training data. The method partitions the training data into a calibration set with clean labels and a proper training set with noisy labels, then uses ICP-calculated reliability metrics to assess label conformity. Validated across three distinct biomedical classification tasks - drug-induced liver injury literature filtering, COVID-19 ICU admission prediction, and breast cancer subtyping - the method demonstrated significant performance enhancements, improving accuracy, AUROC, AUPRC, and macro-average F1 scores across all experiments.

## Method Summary
The reliability-based cleaning method uses ICP to partition training data into calibration (clean labels) and proper training (noisy labels) sets. A conformal prediction with shrunken centroids (CPSC) model is trained on the proper training set, and P-values are calculated for each sample-label combination using the calibration set. Labels with low P-values relative to other possible labels are identified as mislabeled or outliers and corrected or removed. The cleaned training set is then combined with the calibration set for downstream classifier training, improving classification performance without requiring extensive volumes of meticulously curated training data.

## Key Results
- DILI experiments: accuracy improvements in 86 out of 96 experiments (up to 11.4%)
- COVID-19 experiments: AUROC and AUPRC enhancements in all 48 experiments (up to 23.8% and 69.8%)
- RNA-sequencing experiments: accuracy and macro-average F1 score improvements in 47 out of 48 experiments (up to 74.6% and 89.0%)

## Why This Works (Mechanism)

### Mechanism 1
The method improves classification by identifying and correcting mislabeled training data using conformal prediction. It partitions data into proper training and calibration sets, computes P-values for label assignments in the proper set, and corrects labels with low P-values. Core assumption: calibration set contains high-fidelity labels. Evidence: abstract mentions using ICP-calculated reliability metrics to rectify mislabeled data. Break condition: calibration set noise compromises conformity assessment.

### Mechanism 2
Lower detection thresholds improve performance when training data is highly noisy. When permuted labels exceed 40%, stricter thresholds miss many mislabeled instances, while lower thresholds (e.g., 0.2) allow more corrections. Core assumption: nonconformity measure confidence decreases as noise increases. Evidence: section states lower thresholds lead to better improvement for highly noisy proper training sets. Break condition: excessively low thresholds cause over-correction.

### Mechanism 3
The independence of calibration and proper training sets prevents overfitting and maintains generalizability. The separation ensures cleaning doesn't overfit to calibration data. Core assumption: ICP framework inherently separates model fitting from calibration. Evidence: section mentions better independence helps avoid biasing training data toward calibration set. Break condition: small or unrepresentative calibration set leads to unreliable conformity assessment.

## Foundational Learning

- **Inductive conformal prediction (ICP)**: Provides framework to quantify reliability of label assignments by comparing nonconformity measures against calibration set. Why needed: enables systematic identification of mislabeled data. Quick check: How does ICP differ from non-inductive conformal prediction in terms of overfitting risk?

- **Nonconformity measure**: Quantifies how well feature-label combinations conform to training data distribution, forming basis for P-value calculation. Why needed: essential for assessing label reliability. Quick check: What role does CPSC algorithm play in computing nonconformity measure?

- **P-value calibration**: Indicates conformity of label assignment, enabling detection of mislabeled data by comparing P-values across possible labels. Why needed: provides probabilistic framework for reliability assessment. Quick check: How does Laplace smoothing affect P-value calculation?

## Architecture Onboarding

- **Component map**: Data ingestion → Partition into calibration and proper training sets → CPSC model training on proper training set → Nonconformity measure computation → P-value calculation using calibration set → Label correction/removal decisions → Cleaned training set → Downstream classifier training
- **Critical path**: Calibration set quality → CPSC model performance → P-value accuracy → Cleaning effectiveness
- **Design tradeoffs**: Larger calibration sets improve reliability but reduce proper training data size; lower detection thresholds increase corrections but risk over-correction; CPSC hyperparameters balance model confidence and numerical stability
- **Failure signatures**: Poor performance improvement despite cleaning indicates calibration set noise or improper threshold selection; over-correction leading to worse accuracy suggests excessively low detection threshold; no significant change indicates clean original data or ineffective cleaning method
- **First 3 experiments**: 1) Validate ICP framework on synthetic dataset with known label noise to ensure P-values correctly identify mislabeled data; 2) Test different detection thresholds (0.8, 0.5, 0.2) on moderately noisy dataset to observe performance trade-offs; 3) Evaluate impact of calibration set size on cleaning effectiveness by varying calibration-to-proper training data ratio

## Open Questions the Paper Calls Out

### Open Question 1
How does the reliability-based training data cleaning method perform on regression tasks compared to classification tasks? The paper mentions this remains unknown, as the study only evaluated classification tasks.

### Open Question 2
What is the optimal detection threshold for wrongly labeled data, and how does it vary with different levels of noise in the training data? The study used specific thresholds but didn't explore the full range or how optimal threshold varies with noise levels.

### Open Question 3
How does the reliability-based training data cleaning method compare to other semi-supervised learning methods in terms of performance and efficiency? The paper doesn't provide comprehensive comparison with other semi-supervised approaches.

## Limitations

- The method's effectiveness critically depends on the quality of the calibration set - if this subset contains noise, reliability assessment becomes compromised
- Assumes nonconformity measure provides consistent rankings across labels, which may not hold for all feature distributions or classifier architectures
- Computational overhead of calculating P-values for all possible labels may limit scalability to very large label spaces

## Confidence

- **High Confidence**: ICP framework's ability to identify low-conformity label assignments is well-established theoretically, and method's effectiveness in correcting mislabeled data is supported by consistent performance improvements across all three biomedical tasks
- **Medium Confidence**: Claim that lower detection thresholds benefit highly noisy datasets is supported by experimental results but requires more extensive testing across varying noise levels
- **Medium Confidence**: Independence of calibration and proper training sets preventing overfitting is theoretically sound but not empirically validated through direct comparison with non-inductive approaches

## Next Checks

1. **Calibration Set Robustness Test**: Systematically vary noise level in calibration set (0%, 5%, 10%, 20%) and measure degradation in cleaning effectiveness to establish method's sensitivity to calibration quality

2. **Threshold Optimization Study**: Conduct grid search over detection thresholds (0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9) across different noise levels to identify optimal thresholds and validate hypothesis about lower thresholds for high noise

3. **Component Ablation Analysis**: Compare performance when: (a) using only CPSC without cleaning, (b) using cleaning without CPSC (random corrections), and (c) using full pipeline to quantify individual contributions of each component to observed improvements