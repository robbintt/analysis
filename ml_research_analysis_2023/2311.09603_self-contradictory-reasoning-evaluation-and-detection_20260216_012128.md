---
ver: rpa2
title: Self-Contradictory Reasoning Evaluation and Detection
arxiv_id: '2311.09603'
source_url: https://arxiv.org/abs/2311.09603
tags:
- reasoning
- answer
- question
- wrong
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SCORE, a framework to evaluate self-contradictory
  (Self-Contra) reasoning in large language models (LLMs). Self-Contra reasoning occurs
  when a model's reasoning contradicts its answer or is internally inconsistent.
---

# Self-Contradictory Reasoning Evaluation and Detection

## Quick Facts
- arXiv ID: 2311.09603
- Source URL: https://arxiv.org/abs/2311.09603
- Reference count: 4
- Primary result: SCORE framework reveals that LLM accuracy improvements from few-shot prompting do not reduce self-contradictory reasoning rates.

## Executive Summary
This paper introduces SCORE, a framework to evaluate self-contradictory (Self-Contra) reasoning in large language models (LLMs). Self-Contra reasoning occurs when a model's reasoning contradicts its answer or is internally inconsistent. The framework includes three steps: evaluating Self-Contra reasoning, analyzing finer-grained categories of reasoning failures, and using Point-of-View (POV) reasoning to probe reasoning robustness. Experiments across three datasets (WinoBias, HotPotQA, CommonSenseQA) show that LLMs frequently exhibit Self-Contra reasoning, especially in tasks requiring contextual understanding or commonsense reasoning. Accuracy improvements from few-shot prompting do not correlate with reduced Self-Contra rates, indicating that high accuracy does not guarantee reliable reasoning. The study identifies specific reasoning failures, such as evidence missing or using shortcuts, and demonstrates that POV reasoning can expose inconsistencies. Overall, the results highlight the need for comprehensive evaluation beyond accuracy metrics to ensure trustworthy reasoning in LLMs.

## Method Summary
The SCORE framework evaluates self-contradictory reasoning through three progressive steps. First, it classifies reasoning into three types: Type1 (correct reasoning leading to wrong prediction), Type2 (wrong reasoning leading to correct prediction), and Type3 (contradictions within the reasoning itself). Second, it analyzes finer-grained reasoning failures including evidence missing, incomplete reasoning, questionable cause, begging the question, circular reasoning, wrong context knowledge, and wrong external knowledge. Third, it employs Point-of-View reasoning to test whether models maintain consistency when reasoning from multiple perspectives. The framework is evaluated across three datasets using zero-shot, few-shot, and knowledge-enhanced prompting settings with GPT-3.5-turbo.

## Key Results
- LLMs exhibit high Self-Contra rates across all datasets, with Type1 and Type3 being most common
- Few-shot prompting improves accuracy but does not reduce Self-Contra rates
- POV reasoning reveals that models fail to maintain consistency when reasoning paths are reversed
- Common reasoning failures include evidence missing and incomplete reasoning
- Accuracy and reasoning consistency are decoupled in LLM performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SCORE's three-step evaluation structure isolates different failure modes in LLM reasoning by separating self-contradiction detection from root-cause analysis.
- Mechanism: The framework uses progressive granularity: first identifying self-contradiction (Type1/Type2/Type3), then classifying specific reasoning fallacies (questionable cause, circular reasoning, etc.), and finally using POV reasoning to test contextual robustness.
- Core assumption: Different types of reasoning failures have distinct signatures that can be isolated through targeted prompting strategies.
- Evidence anchors:
  - [abstract] "We define SELF -CONTRA reasoning into three categories: Type1: a correct reasoning leading to a wrong prediction; Type2: a wrong reasoning leading to a correct prediction; Type3: there are contradictions in the reasoning itself."
  - [section] "Our investigation also uncovers the specific errors for SELF -CONTRA reasoning failures."
  - [corpus] Corpus evidence is weak - no direct matches for this specific three-step isolation mechanism.
- Break condition: If reasoning failures are highly entangled (e.g., a single error causes multiple contradiction types simultaneously), the separation may fail to capture true causal relationships.

### Mechanism 2
- Claim: POV reasoning exposes hidden inconsistencies by forcing models to reason from multiple perspectives, revealing context-dependent reasoning failures.
- Mechanism: By prompting the model to consider both correct and incorrect pronoun references in WinoBias, the framework reveals whether the model maintains internal consistency when reasoning paths are reversed.
- Core assumption: LLMs' reasoning is context-sensitive and order-dependent, making them vulnerable to first-impression bias when presented with sequential reasoning tasks.
- Evidence anchors:
  - [abstract] "We find that though LLMs may appear to perform well in one-perspective settings, they fail to stabilize such behavior in multi-perspectives settings."
  - [section] "Correct or incorrect reasoning in the first turn may bias the second turn reasoning, potentially inducing self-contradictory behavior."
  - [corpus] Corpus evidence is weak - no direct matches for POV reasoning methodology.
- Break condition: If the model develops strong internal consistency mechanisms or if the task domain doesn't benefit from perspective-switching (e.g., purely factual recall tasks).

### Mechanism 3
- Claim: Few-shot prompting improves accuracy but doesn't necessarily reduce self-contradiction rates because the model learns to produce better reasoning traces without improving underlying reasoning consistency.
- Mechanism: The demonstrations in few-shot prompting teach the model formatting and surface-level reasoning patterns but don't address the fundamental challenge of linking reasoning to correct answers.
- Core assumption: Accuracy improvements from in-context learning primarily affect the model's ability to generate plausible reasoning rather than ensuring the reasoning is logically sound.
- Evidence anchors:
  - [abstract] "Accuracy improvements from few-shot prompting do not correlate with reduced Self-Contra rates, indicating that high accuracy does not guarantee reliable reasoning."
  - [section] "While the few-shot setting demonstrates an increase in accuracy compared to the zero-shot setting, instances of SELF -CONTRA reasoning do not decrease correspondingly."
  - [corpus] Corpus evidence is weak - no direct matches for this specific accuracy-reasoning correlation observation.
- Break condition: If the few-shot demonstrations specifically target reasoning consistency (not just final answers), the correlation might improve.

## Foundational Learning

- Concept: Self-contradictory reasoning types (Type1, Type2, Type3)
  - Why needed here: Understanding these categories is essential for implementing the SCORE framework and interpreting its results.
  - Quick check question: What distinguishes Type1 from Type2 reasoning in the SCORE framework?

- Concept: Finer-grained reasoning fallacy categories
  - Why needed here: These categories help diagnose specific reasoning failures beyond the high-level contradiction types.
  - Quick check question: Which finer-grained category describes reasoning that "assumes the answer first and reasons from the answer without proving it"?

- Concept: Point-of-View reasoning methodology
  - Why needed here: POV reasoning is a diagnostic tool that reveals contextual dependencies in LLM reasoning.
  - Quick check question: How does the order of perspective presentation affect the model's reasoning consistency in POV evaluation?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Prompt generation -> Evaluation engine -> Contradiction detection -> Fallacy classification -> POV analysis -> Result aggregation

- Critical path:
  1. Load dataset and preprocess questions
  2. Generate appropriate prompts based on evaluation setting
  3. Send prompts to LLM and capture responses
  4. Parse responses into reasoning and answer components
  5. Apply contradiction detection rules
  6. Classify reasoning fallacies if needed
  7. Execute POV reasoning if applicable
  8. Aggregate results and compute metrics

- Design tradeoffs:
  - Human evaluation vs. automated detection: Human evaluation provides nuanced understanding but is slow and expensive
  - Prompt complexity vs. model capability: More complex prompts may exceed model context limits
  - Dataset size vs. evaluation depth: Larger datasets enable statistical significance but may limit detailed analysis per instance

- Failure signatures:
  - No reasoning generated: Model fails to follow instructions
  - Inconsistent formatting: Model doesn't adhere to expected output structure
  - Circular reasoning detection failures: Model produces reasoning that appears valid but is actually circular
  - POV confusion: Model cannot distinguish between different perspectives

- First 3 experiments:
  1. Implement zero-shot evaluation on WinoBias to establish baseline contradiction rates
  2. Add few-shot prompting with Chain-of-Thought demonstrations to compare accuracy vs. contradiction correlation
  3. Implement POV reasoning on subset of WinoBias to test contextual robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we automate the evaluation of self-contradictory reasoning to reduce reliance on human annotation?
- Basis in paper: The paper relies heavily on human evaluation for identifying self-contradictory reasoning across datasets, which is time-consuming and subjective.
- Why unresolved: The authors highlight the need for more efficient evaluation methods but do not propose specific solutions for automating the process.
- What evidence would resolve it: Development and validation of automated metrics or models that can reliably detect self-contradictory reasoning without human intervention.

### Open Question 2
- Question: What are the underlying mechanisms that cause LLMs to exhibit self-contradictory reasoning in tasks requiring contextual understanding?
- Basis in paper: The paper identifies that LLMs often exhibit self-contradictory reasoning in tasks involving contextual information and commonsense reasoning, but does not explore the root causes.
- Why unresolved: The authors analyze the symptoms of self-contradictory reasoning but do not investigate the internal mechanisms or biases in LLMs that lead to these errors.
- What evidence would resolve it: Empirical studies analyzing the attention patterns, token-level reasoning, or internal representations of LLMs during self-contradictory reasoning tasks.

### Open Question 3
- Question: Can the Point-of-View (POV) reasoning method be generalized to other types of logical reasoning tasks beyond coreference resolution?
- Basis in paper: The paper uses POV reasoning as a diagnostic tool for WinoBias, a coreference resolution dataset, but does not explore its applicability to other reasoning tasks.
- Why unresolved: The authors demonstrate the effectiveness of POV reasoning for detecting inconsistencies in one specific task but do not test its generalizability to broader reasoning domains.
- What evidence would resolve it: Experiments applying POV reasoning to diverse logical reasoning tasks (e.g., mathematical reasoning, multi-step inference) and evaluating its effectiveness in identifying inconsistencies.

## Limitations

- Heavy reliance on human evaluation for nuanced reasoning analysis
- Framework tested only on three specific datasets, limiting generalizability
- POV reasoning methodology lacks detailed specification for diverse task types
- Few-shot prompting improvements don't translate to reasoning consistency

## Confidence

**High Confidence**: The core definition of self-contradictory reasoning (Type1/2/3) and the empirical observation that few-shot prompting improves accuracy without reducing contradiction rates. The POV reasoning framework's ability to expose contextual inconsistencies is well-supported by experimental evidence.

**Medium Confidence**: The finer-grained reasoning fallacy categories are useful for diagnosis but may overlap or miss certain reasoning failure modes. The generalizability of findings across different LLM architectures and task domains requires further validation.

**Low Confidence**: The exact implementation details for POV reasoning and the specific mechanisms by which few-shot prompting fails to address underlying reasoning consistency.

## Next Checks

1. Apply SCORE framework to additional reasoning datasets (e.g., strategyQA, ProofWriter) to test generalizability of self-contradiction patterns and POV reasoning effectiveness.

2. Evaluate whether self-contradiction rates and POV reasoning failures vary systematically across different LLM families (GPT, Claude, LLaMA) to identify model-specific vs. general reasoning limitations.

3. Systematically vary few-shot demonstration content to determine whether demonstrations targeting reasoning consistency (rather than just final answers) can break the observed accuracy-contradiction decoupling.