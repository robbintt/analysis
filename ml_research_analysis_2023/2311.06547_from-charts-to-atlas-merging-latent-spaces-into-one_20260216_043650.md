---
ver: rpa2
title: 'From Charts to Atlas: Merging Latent Spaces into One'
arxiv_id: '2311.06547'
source_url: https://arxiv.org/abs/2311.06547
tags:
- space
- spaces
- classes
- latent
- aggregated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Relative Latent Space Aggregation (RLSA), a
  method to merge latent spaces from models trained on related but distinct tasks.
  RLSA first transforms absolute representations into relative ones using a set of
  anchor points, then averages these relative representations to create a unified
  space.
---

# From Charts to Atlas: Merging Latent Spaces into One

## Quick Facts
- arXiv ID: 2311.06547
- Source URL: https://arxiv.org/abs/2311.06547
- Reference count: 8
- Key outcome: RLSA achieves CKA similarity above 0.7 with end-to-end models and improves classification accuracy by up to 33 percentage points when merging latent spaces from related tasks.

## Executive Summary
This paper introduces Relative Latent Space Aggregation (RLSA), a method to merge latent spaces from models trained on related but distinct tasks. RLSA transforms absolute embeddings into relative representations using anchor points, then averages these to create a unified space. The approach is evaluated across three settings: shared samples, shared classes, and disjoint tasks. Results show RLSA can create meaningful unified representations without requiring architectural access to original models, achieving high CKA similarity to end-to-end trained spaces and improving classification accuracy even in disjoint task scenarios.

## Method Summary
RLSA merges latent spaces by first converting absolute embeddings into relative representations using cosine similarities to anchor points, then averaging these relative representations across models. The method operates in two steps: (1) project task-specific embeddings to relative space defined by anchor similarities, and (2) aggregate via simple mean. It works across three task partitioning scenarios - shared samples, shared classes, and entirely disjoint tasks - using datasets like CIFAR100 and TinyImageNet. The aggregated space is evaluated using CKA similarity to end-to-end baselines, classification accuracy, and class separability metrics.

## Key Results
- RLSA achieves CKA similarity scores above 0.7 with end-to-end trained models when tasks share samples
- Classification accuracy improves by up to 33 percentage points compared to naive merging
- In disjoint task settings, adding a task-identification layer restores accuracy to end-to-end levels
- Aggregated spaces show higher class separability than either end-to-end or naively merged spaces

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Relative representations using anchor-based cosine similarities remove architecture-specific artifacts and enable meaningful averaging across latent spaces.
- **Mechanism:** By projecting absolute embeddings into a space defined by similarities to shared anchors, training stochasticities (initialization, data shuffling) that caused angle-preserving distortions in the original spaces become irrelevant. The relative space is invariant to orthogonal transformations and isotropic scaling, allowing direct averaging of representations.
- **Core assumption:** The inter-sample distance structure is preserved across latent spaces for semantically related tasks, so that relative similarities remain consistent.
- **Evidence anchors:** [abstract] "we introduce Relative Latent Space Aggregation, a two-step approach that first renders the spaces comparable using relative representations, and then aggregates them via a simple mean." [section 3] "the deeper layers of a network are able to learn semantically meaningful representations that capture the inherent data structure" and "the inter-sample distances should remain consistent across the two spaces."
- **Break condition:** If tasks are semantically unrelated or if the shared anchor set does not reflect the true data structure, the relative similarities will diverge and aggregation will become meaningless.

### Mechanism 2
- **Claim:** When tasks share samples, those shared samples act as reliable anchors that impose consistent structure on the merged space, enabling accurate triangulation of out-of-distribution samples.
- **Mechanism:** Shared samples are embedded reliably by all models; their relative positions define a common coordinate frame. Out-of-distribution samples are then placed relative to these anchors in a way that preserves their relative distances across tasks.
- **Core assumption:** The latent subspaces for shared classes are similar across models, so that the same anchors yield comparable relative positions.
- **Evidence anchors:** [section 4.1] "the anchors are selected from the shared samples" and "these shared anchors should be able to impose a consistent structure on the merged space that will accommodate the novel samples." [section 4.1] "the CKA among the samples from the shared classes is always higher than that of the samples from task-specific classes."
- **Break condition:** If the shared portion is too small or the models have diverged too much, the anchor-induced structure collapses and triangulation fails.

### Mechanism 3
- **Claim:** Task-specific embedders leave "footprints" in representations that allow downstream classifiers to reduce the effective number of classes they must distinguish, boosting accuracy even when tasks are disjoint.
- **Mechanism:** Each task-specific model, trained on fewer classes, learns embeddings that are better separated for its subset. When merged, these embeddings carry implicit signals about which task they came from, so a classifier can first identify the task and then classify within a smaller class set.
- **Core assumption:** The embeddings encode information about the model/task that trained them, which can be leveraged by a classifier.
- **Evidence anchors:** [section 4.3] "the accuracy rises to that of the aggregation techniques, confirming the hypothesis" after adding a task-embedding layer. [section 4.3] "being trained on a smaller subset of classes, the task-specific embedders are better able to discriminate their class set."
- **Break condition:** If the downstream classifier cannot detect or exploit the task-specific footprint, or if the footprint is too weak, the advantage disappears.

## Foundational Learning

- **Concept:** Centered Kernel Alignment (CKA)
  - **Why needed here:** CKA is the standard metric for comparing neural representations; the paper uses it to quantify similarity between the aggregated space and an end-to-end trained space.
  - **Quick check question:** What property of CKA makes it suitable for comparing latent spaces across different architectures?
    - *Answer:* CKA is invariant to orthogonal transformations and isotropic scaling, so it measures representational similarity independent of rotation or scaling differences.

- **Concept:** Relative representations via anchor-based similarities
  - **Why needed here:** The paper's aggregation method depends on converting absolute embeddings into relative ones using a set of anchors; understanding this transformation is essential to grasp why averaging works.
  - **Quick check question:** In relative representations, what does each dimension of the new vector correspond to?
    - *Answer:* Each dimension is the cosine similarity between the sample and one anchor sample.

- **Concept:** Inter-class vs intra-class distance for separability
  - **Why needed here:** The paper measures class separability in aggregated spaces using a ratio of inter-class to intra-class distances; knowing how this is computed helps interpret the reported results.
  - **Quick check question:** How is the separability score defined in the paper?
    - *Answer:* S = d_inter / ((d_intra_C1 + d_intra_C2)/2), where d_inter is the distance between class centroids and d_intra are average distances to centroids.

## Architecture Onboarding

- **Component map:** Data split module → Task-specific model trainer → Embedding extractor → Anchor selector → Relative representation projector → Aggregator (mean) → Evaluation (CKA, separability, classification)
- **Critical path:**
  1. Partition dataset into tasks (sample/class share conditions)
  2. Train each task-specific model
  3. Extract embeddings for training and anchor sets
  4. Build relative representations using anchors
  5. Average relative representations across models
  6. Evaluate merged space against end-to-end baseline
- **Design tradeoffs:**
  - Anchor choice: shared samples vs random unseen samples; affects comparability and triangulation quality.
  - Aggregation function: simple mean vs weighted mean; the paper uses mean for simplicity.
  - Model complexity: task-specific models can be simpler since they train on fewer classes; trade-off between specialization and generality.
- **Failure signatures:**
  - Low CKA similarity to end-to-end space → Anchors not aligning tasks well or tasks too dissimilar.
  - High intra-class collisions in separability → Naive aggregation merging incompatible embeddings.
  - No accuracy gain over end-to-end → Task-specific footprints not strong enough or classifier not exploiting them.
- **First 3 experiments:**
  1. Train two models on tasks sharing 80% of classes; use shared samples as anchors; aggregate and compare CKA to end-to-end.
  2. Train two models on disjoint sample sets but same classes; use random anchors; aggregate and measure separability vs naive merge.
  3. Train two models on disjoint classes and samples; use random anchors; aggregate and test if adding task-embedding layer restores accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the choice of anchor points affect the quality of the aggregated latent space?
- **Basis in paper:** [explicit] The paper mentions that anchor points are randomly sampled from unseen samples in the training dataset, but does not explore the impact of different sampling strategies.
- **Why unresolved:** The paper does not provide a systematic analysis of how anchor point selection influences the resulting aggregated space, leaving open questions about optimal anchor selection.
- **What evidence would resolve it:** Experiments comparing different anchor sampling strategies (e.g., random vs. cluster-based) and their effects on CKA similarity and classification accuracy would provide insights into optimal anchor selection.

### Open Question 2
- **Question:** Can RLSA be extended to merge more than two latent spaces, and how does performance scale with the number of spaces?
- **Basis in paper:** [inferred] The paper focuses on merging two latent spaces but does not explore scenarios with multiple spaces, which could be relevant in federated learning or ensemble methods.
- **Why unresolved:** The paper does not provide experiments or theoretical analysis on merging more than two spaces, leaving uncertainty about scalability and performance.
- **What evidence would resolve it:** Experiments demonstrating RLSA's effectiveness with multiple spaces and analyzing how CKA similarity and classification accuracy change with the number of spaces would address this question.

### Open Question 3
- **Question:** How does RLSA perform when merging latent spaces from models with different architectures?
- **Basis in paper:** [explicit] The paper notes that RLSA does not require architectural access to the original models, but does not test this claim with diverse architectures.
- **Why unresolved:** The paper does not provide empirical evidence on how RLSA handles architectural differences, which is crucial for its applicability in heterogeneous environments.
- **What evidence would resolve it:** Experiments merging latent spaces from models with varying architectures (e.g., CNN, Transformer, ResNet) and evaluating their similarity and classification performance would provide insights into RLSA's robustness to architectural diversity.

## Limitations

- The approach relies heavily on the assumption that relative distances in latent space are preserved across models, which may not hold for highly dissimilar tasks or architectures.
- The anchor selection strategy significantly impacts performance but lacks systematic evaluation.
- The aggregation method uses simple averaging, which may not be optimal for all scenarios.

## Confidence

- CKA-based similarity claims: **High confidence** - supported by direct comparisons and consistent results across experiments
- Classification accuracy improvements: **Medium confidence** - strong results in shared sample settings, but less convincing in disjoint task scenarios
- Task-specific footprint hypothesis: **Medium confidence** - supported by the task-embedding layer results, but the mechanism could be more thoroughly explored

## Next Checks

1. Test RLSA with more diverse architectural combinations (CNN, transformer, MLP) to verify generalizability beyond CNN-EfficientNet pairs
2. Evaluate the impact of anchor set size and composition on CKA similarity and classification accuracy through systematic ablation studies
3. Compare RLSA against alternative merging strategies (weighted averaging, attention-based merging) to establish its relative effectiveness