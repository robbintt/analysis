---
ver: rpa2
title: 'Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? A.K.A. Will
  LLMs Replace Knowledge Graphs?'
arxiv_id: '2308.10168'
source_url: https://arxiv.org/abs/2308.10168
tags:
- llms
- knowledge
- head
- question
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive evaluation of how knowledgeable
  Large Language Models (LLMs) are regarding factual knowledge. The authors constructed
  the Head-to-Tail benchmark, consisting of 18K question-answer pairs covering head,
  torso, and tail entities across multiple domains.
---

# Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? A.K.A. Will LLMs Replace Knowledge Graphs?

## Quick Facts
- **arXiv ID:** 2308.10168
- **Source URL:** https://arxiv.org/abs/2308.10168
- **Reference count:** 12
- **Primary result:** ChatGPT achieved only 20.3% accuracy on a benchmark testing factual knowledge across head, torso, and tail entities.

## Executive Summary
This paper presents a comprehensive evaluation of Large Language Models' (LLMs) factual knowledge using the Head-to-Tail benchmark with 18K question-answer pairs. The authors developed an automated evaluation method and metrics to assess factuality across multiple domains. Through testing 16 publicly available LLMs, they demonstrate that existing models struggle significantly with factual knowledge, particularly for torso and tail entities. The best-performing model, ChatGPT, only achieved 20.3% overall accuracy, highlighting the substantial gap between current LLMs and the knowledge representation capabilities of knowledge graphs.

## Method Summary
The authors constructed the Head-to-Tail benchmark from multiple knowledge graph sources (DBpedia, IMDb, Goodreads, MAG, DBLP) and classified entities into head, torso, and tail categories based on popularity. They generated question-answer pairs for each entity type and evaluated 16 LLMs using both rule-based and LLM-based metrics. The evaluation measured accuracy, hallucination rate, and missing rate while using prompts that encouraged brief answers and allowed "unsure" responses. The methodology included automated answer correctness checking and analysis of performance differences across entity types and model architectures.

## Key Results
- ChatGPT achieved only 20.3% accuracy on the overall benchmark
- Performance declined from head (20.3%) to torso (9.2%) to tail (3.4%) entities
- Instruction-tuned models showed higher missing rates but similar hallucination rates compared to base models
- Brief answer requirements reduced answer variability from 18% to 4%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs perform worse on torso and tail entities because they lack sufficient training data for these entities.
- **Mechanism:** The model's ability to generate correct answers is proportional to the frequency of entity mentions in its training data. Rare entities (torso/tail) appear fewer times, leading to weaker representations and higher hallucination rates.
- **Core assumption:** Training data follows the same long-tail distribution as real-world knowledge, so rarer entities are less represented.
- **Evidence anchors:**
  - [abstract] "we show that existing LLMs are still far from being perfect in terms of their grasp of factual knowledge, especially for facts of torso-to-tail entities."
  - [section 3.3] "the performance of LLMs...declines in the order of head, torso, and tail entities"
  - [corpus] weak correlation between model size and accuracy for torso/tail entities

### Mechanism 2
- **Claim:** Instruction tuning makes models more conservative, increasing "unsure" responses but not necessarily reducing hallucination rate.
- **Mechanism:** Fine-tuning with human feedback teaches models to admit uncertainty when confidence is low, suppressing false answers but also potentially increasing the number of unanswered questions.
- **Core assumption:** Models can reliably detect when they lack knowledge, and "unsure" is a safer response than hallucination.
- **Evidence anchors:**
  - [section 3.4] "instruction-tuned counterparts...have lower accuracy, as they learned to be more conservative...generate 'unsure' more often"
  - [section 3.5] "removing 'unsure' as an option increases ChatGPT's hallucination rate by 13 percentage points"
  - [corpus] Vicuna-13B has 26.9% higher missing rate than LLaMA-13B

### Mechanism 3
- **Claim:** Brief answer requirements stabilize model outputs and reduce variability in regenerated answers.
- **Mechanism:** When models are explicitly prompted for concise answers, they are less likely to generate varied or verbose responses, making evaluation more consistent and reliable.
- **Core assumption:** Model generation variability is partly due to lack of constraints on answer length and format.
- **Evidence anchors:**
  - [section 3.5] "Adding the requirement for brief answers...reduced the percentage [of different regenerated answers] to 4%"
  - [section 2.3] "we asked LLMs to give as concise answers as possible"
  - [corpus] brief answer prompt reduces answer variability from 18% to 4%

## Foundational Learning

- **Concept:** Long-tail distribution of knowledge entities
  - **Why needed here:** Understanding why LLMs perform poorly on torso/tail entities requires knowing how knowledge popularity follows a power law.
  - **Quick check question:** What percentage of entities typically fall into the head, torso, and tail categories in a long-tail distribution?

- **Concept:** Hallucination vs. missing knowledge
  - **Why needed here:** Distinguishing between when a model lacks knowledge versus when it fabricates answers is crucial for evaluating factuality.
  - **Quick check question:** How can you tell from the model's response whether it is admitting uncertainty or providing a hallucinated answer?

- **Concept:** Evaluation metrics for LLM factuality
  - **Why needed here:** Using appropriate metrics (accuracy, hallucination rate, missing rate) is essential for meaningful assessment of knowledge representation.
  - **Quick check question:** What are the three primary metrics used in this paper to evaluate LLM factuality, and how do they relate to each other?

## Architecture Onboarding

- **Component map:** Knowledge graph data sources (DBpedia, IMDb, Goodreads, MAG, DBLP) -> Entity popularity computation and bucketing (head/torso/tail) -> Question template generation system -> LLM interaction layer (API calls with specific prompts) -> Evaluation metrics computation (rule-based and LLM-based)

- **Critical path:**
  1. Entity popularity calculation and bucketing
  2. Question template generation for each bucket
  3. LLM interaction with brief answer and "unsure" prompts
  4. Answer correctness evaluation using both rule-based and LLM-based methods
  5. Metric computation and analysis

- **Design tradeoffs:**
  - Manual vs. automated answer correctness checking (accuracy vs. scalability)
  - Brief vs. detailed answer requirements (stability vs. completeness)
  - Few-shot vs. zero-shot prompting (performance vs. generalizability)

- **Failure signatures:**
  - High hallucination rates despite brief answer requirements
  - Inconsistent regenerated answers for the same question
  - Metrics showing high correlation but manual evaluation disagreeing

- **First 3 experiments:**
  1. Test whether brief answer requirements actually reduce answer variability by comparing regenerated answers with and without the constraint.
  2. Evaluate whether few-shot examples improve accuracy more than they increase hallucination rate by comparing few-shot vs. in-domain vs. zero-shot prompts.
  3. Test the correlation between rule-based and LLM-based metrics by manually verifying a sample of answers and comparing agreement rates.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of LLMs on factual knowledge compare when using taxonomy or type hierarchies versus the entity-based bucketing approach used in this study?
- **Basis in paper:** [inferred] The paper mentions that it does not discuss the effectiveness of LLMs in capturing taxonomy or type hierarchies, suggesting this as a potential extension.
- **Why unresolved:** The paper focuses on entity-based popularity for bucketing and does not evaluate the impact of taxonomy on LLM performance.
- **What evidence would resolve it:** Conduct experiments comparing LLM performance on factual knowledge using taxonomy-based bucketing versus the entity-based approach, and analyze any differences in accuracy across different types of relationships.

### Open Question 2
- **Question:** What is the impact of instruction tuning on LLM factuality, and why does it lead to lower accuracy but higher missing rates in some cases?
- **Basis in paper:** [explicit] The paper observes that instruction-tuned models (Vicuna, Falcon-Instruct) have lower accuracy but higher missing rates compared to their non-instruction-tuned counterparts.
- **Why unresolved:** The paper does not provide a detailed analysis of why instruction tuning affects factuality in this way.
- **What evidence would resolve it:** Investigate the training process of instruction-tuned models and analyze how it influences their behavior when answering factual questions, potentially through ablation studies or analysis of the model's internal representations.

### Open Question 3
- **Question:** How do the latest LLMs (GPT-4, Llama 2) perform on the Head-to-Tail benchmark, and how do their results compare to the models evaluated in this study?
- **Basis in paper:** [explicit] The paper acknowledges that it did not evaluate GPT-4 and Llama 2, which became publicly available after the initial version of the paper.
- **Why unresolved:** The study only includes a subset of recent LLMs, and the performance of the latest models remains unknown.
- **What evidence would resolve it:** Evaluate GPT-4 and Llama 2 on the Head-to-Tail benchmark and compare their results to the models included in this study, analyzing any improvements or differences in performance across different entity buckets.

## Limitations
- The evaluation methodology relies heavily on automated metrics which may not capture nuanced correctness issues
- Brief answer requirements may oversimplify responses and exclude valid nuanced information
- The study does not evaluate the impact of taxonomy or type hierarchies on LLM performance

## Confidence

- **High confidence:** The overall finding that LLMs perform significantly worse on torso and tail entities compared to head entities is well-supported by the data and consistent with the long-tail distribution of knowledge.
- **Medium confidence:** The claim that instruction tuning increases "unsure" responses without reducing hallucination rates is supported but could benefit from more granular analysis of different types of uncertainty.
- **Medium confidence:** The effectiveness of brief answer requirements in reducing answer variability is demonstrated, but the potential information loss from this constraint warrants further investigation.

## Next Checks

1. Conduct a comprehensive manual evaluation of a stratified sample of answers across all entity types to validate the automated metric results and assess potential biases in the evaluation methodology.

2. Test the sensitivity of results to different prompt engineering approaches, including variations in few-shot examples, temperature settings, and answer format requirements, to identify the most robust evaluation setup.

3. Analyze the correlation between model pretraining data composition and performance on torso/tail entities by examining model documentation and training corpus characteristics to better understand the knowledge gaps.