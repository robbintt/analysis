---
ver: rpa2
title: Causal discovery using dynamically requested knowledge
arxiv_id: '2310.11154'
source_url: https://arxiv.org/abs/2310.11154
tags:
- learning
- knowledge
- active
- algorithm
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to causal Bayesian network
  structure learning called active learning. Unlike traditional methods that provide
  predefined knowledge upfront, the proposed Tabu-AL algorithm dynamically requests
  human knowledge during the learning process when it identifies uncertain relationships.
---

# Causal discovery using dynamically requested knowledge

## Quick Facts
- arXiv ID: 2310.11154
- Source URL: https://arxiv.org/abs/2310.11154
- Authors: 
- Reference count: 18
- One-line primary result: Active learning significantly improves Bayesian network structure learning accuracy by dynamically requesting human knowledge at critical decision points

## Executive Summary
This paper introduces Tabu-AL, an active learning approach for causal Bayesian network structure learning that dynamically requests human knowledge during the learning process rather than requiring predefined knowledge upfront. The algorithm uses four criteria to identify when to seek human input, with the most effective being when arc orientations are ambiguous (equivalent add criterion). Experiments on 16 benchmark networks show active learning significantly improves structural accuracy compared to no knowledge, with F1 score improvements ranging from 0.083 to 0.207 depending on knowledge request limits.

## Method Summary
The Tabu-AL algorithm modifies the traditional Tabu structure learning algorithm by incorporating dynamic knowledge requests. When the algorithm identifies structurally ambiguous decisions or statistically unreliable score estimates, it requests human input to guide the search. The method uses four criteria: equivalent add (when opposite arc orientations have equal scores), small counts (when contingency table cells have ≤5 samples), unreliable score (when subsample scores differ significantly), and small delta (when score improvements are minimal). Knowledge requests update constraint lists that guide subsequent search steps. The approach is evaluated against traditional predefined knowledge methods and tested for robustness to imperfect human knowledge.

## Key Results
- Active learning improves mean F1 scores by 0.083 to 0.207 compared to no knowledge, depending on knowledge request limits
- Active learning outperforms predefined knowledge approaches, especially for orientation information
- The method remains effective even with 10-20% incorrect human responses
- Active learning reduces sensitivity to variable ordering in the Tabu algorithm
- The equivalent add criterion is the most effective for requesting human knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Active learning improves accuracy by requesting human input when the algorithm encounters structurally ambiguous arc orientations with equal scores.
- Mechanism: The Tabu-AL algorithm detects when adding an arc and its opposite orientation would yield identical score improvements (equivalent add criterion), indicating that observational data alone cannot determine causal direction. By requesting human input at these points, the algorithm resolves these ambiguities.
- Core assumption: Humans possess accurate knowledge about causal directions that cannot be inferred from observational data alone.
- Evidence anchors:
  - [abstract]: "the most effective being when the algorithm is considering adding an arc that could also be added in the opposite orientation with equal score improvement"
  - [section]: "The first criterion is when the highest scoring change is to add an arc where adding the oppositely-orientated arc is possible and has the same score delta"
  - [corpus]: Weak evidence - corpus mentions "Dynamic Expert-Guided Model Averaging for Causal Discovery" which aligns with expert-guided approaches but doesn't specifically validate the equivalent add criterion

### Mechanism 2
- Claim: Active learning improves accuracy by targeting knowledge requests to situations where statistical scores are unreliable due to small sample counts or data inconsistency.
- Mechanism: The algorithm uses criteria like "small counts" and "unreliable score" to detect when contingency table cells have insufficient data (≤5 samples) or when subsample scores differ significantly, indicating unreliable statistical inference. Knowledge requests are made in these uncertain regions.
- Core assumption: Small sample sizes and inconsistent subsample scores indicate unreliable statistical inference that can be improved with human knowledge.
- Evidence anchors:
  - [section]: "Cells with a sample count of less than 5 are generally considered an unreliable basis for statistical tests... accordingly, we implement the small counts criterion"
  - [section]: "The third criterion, referred to as unreliable score, computes the fractional difference between the BIC scores based on either the first or the second half of the data set"
  - [corpus]: Weak evidence - corpus mentions "Improving Finite Sample Performance of Causal Discovery by Exploiting Temporal Structure" which relates to sample size issues but doesn't validate the specific reliability criteria

### Mechanism 3
- Claim: Active learning reduces sensitivity to variable ordering by guiding the search toward more robust structural decisions.
- Mechanism: The Tabu algorithm is known to be sensitive to variable ordering, but by requesting human input at critical decision points (particularly orientation decisions), active learning reduces the impact of arbitrary variable ordering choices on the final structure.
- Core assumption: Variable ordering sensitivity in structure learning algorithms can be mitigated by targeted human input at critical structural decisions.
- Evidence anchors:
  - [section]: "Analysis shows that the great majority of active learning requests, typically around 95%, relate to an edge that Tabu is correctly adding to the graph but where the orientation needs confirmation or correction"
  - [section]: "The results in Table 6 confirm that increasing the amount of knowledge has a beneficial side-effect of reducing the Tabu algorithm's sensitivity to variable ordering"

## Foundational Learning

- Concept: Bayesian Network structure learning and the equivalence class problem
  - Why needed here: Understanding why observational data alone cannot determine causal directions is crucial for grasping why active learning is effective
  - Quick check question: Why can't we always learn a unique causal DAG from observational data alone, and what does this imply about the role of human knowledge?

- Concept: Score-based structure learning algorithms and the Tabu algorithm
  - Why needed here: The active learning approach is built on modifying the Tabu algorithm, so understanding how it works is essential
  - Quick check question: How does the Tabu algorithm differ from simple hill-climbing, and why is this difference important for implementing active learning?

- Concept: Causal inference and the do-calculus
  - Why needed here: The paper focuses on learning causal structures for interventional reasoning, so understanding the distinction between associative and causal relationships is important
  - Quick check question: What is the difference between learning a probabilistic graphical model and learning a causal Bayesian network, and why does this distinction matter for structure learning?

## Architecture Onboarding

- Component map:
  - Tabu-AL algorithm: Main structure learning engine with active learning modifications
  - Knowledge request criteria: Four criteria (equivalent add, small counts, unreliable score, small delta) that determine when to request human input
  - Human simulation: Function that simulates human responses based on data-generating graph and expertise level
  - Constraint management: reqd and stop lists that track required and prohibited arcs based on human input
  - Evaluation framework: System for generating synthetic data, running experiments with different variable orderings, and computing F1 and SHD metrics

- Critical path:
  1. Initialize DAG with required arcs
  2. Iteratively find highest-scoring change
  3. Check if knowledge request criteria are met
  4. If yes, request human input and update constraints
  5. Apply change and update tabu list
  6. Track best-scoring DAG
  7. Repeat until stopping condition
  8. Return best DAG

- Design tradeoffs:
  - Knowledge request frequency vs. computational efficiency: More requests improve accuracy but increase runtime and human effort
  - Expertise level vs. accuracy: Higher expertise improves results but may not be available in practice
  - Criterion sensitivity thresholds vs. coverage: More sensitive criteria trigger more requests but may waste human effort on easy decisions

- Failure signatures:
  - Excessive knowledge requests with minimal accuracy improvement: Criteria thresholds may be too sensitive
  - Accuracy degradation with human input: Expertise level may be too low or criteria may be triggering in wrong contexts
  - High sensitivity to variable ordering despite active learning: Active learning may not be targeting the right types of decisions

- First 3 experiments:
  1. Run Tabu-AL on a small network (e.g., asia with 8 variables) with perfect human knowledge and limit=0.5 to verify basic functionality and accuracy improvement
  2. Test each of the four knowledge request criteria individually on a medium network (e.g., alarm with 37 variables) to compare their effectiveness
  3. Evaluate the impact of imperfect knowledge by running Tabu-AL with varying expertise levels (0.5, 0.8, 1.0) on a larger network (e.g., formed with 88 variables) to understand robustness to human error

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific criteria or thresholds in the Tabu-AL algorithm are most effective at identifying when to request human knowledge during the learning process?
- Basis in paper: [explicit] The paper describes four criteria for requesting human knowledge (equivalent add, small counts, unreliable score, small delta) and evaluates their effectiveness, with equivalent add showing the best results.
- Why unresolved: The paper identifies equivalent add as most effective but doesn't explore whether combinations of criteria or adaptive thresholds might improve performance further.
- What evidence would resolve it: Empirical testing comparing different combinations of criteria and dynamic threshold adjustment strategies would show whether composite criteria improve accuracy beyond individual criteria.

### Open Question 2
- Question: How does the Tabu-AL algorithm perform when applied to real-world datasets with missing data, measurement errors, or latent confounders, rather than synthetic data?
- Basis in paper: [inferred] The paper acknowledges these are common challenges in structure learning but only evaluates the algorithm on synthetic data without such complications.
- Why unresolved: The evaluation methodology focuses exclusively on synthetic data from known networks, which don't capture real-world data quality issues.
- What evidence would resolve it: Application of Tabu-AL to real-world datasets with known data quality issues and comparison to traditional methods would reveal practical limitations and robustness.

### Open Question 3
- Question: Could the active learning approach be effectively integrated with non-tabu structure learning algorithms like constraint-based or hybrid methods?
- Basis in paper: [explicit] The paper develops Tabu-AL by modifying the Tabu algorithm specifically and doesn't explore integration with other algorithm types.
- Why unresolved: The paper focuses on modifying Tabu and doesn't investigate whether the active learning concept could benefit other algorithm classes.
- What evidence would resolve it: Implementation and evaluation of active learning variants for constraint-based algorithms like PC-Stable or hybrid methods like MMHC would show if the benefits are algorithm-specific or generalizable.

### Open Question 4
- Question: What is the optimal balance between human expertise (proportion of correct answers) and algorithmic autonomy in the active learning framework?
- Basis in paper: [explicit] The paper tests different levels of human expertise but doesn't explore the optimal balance or develop strategies for when to rely on algorithmic decisions versus human input.
- Why unresolved: While the paper shows active learning remains beneficial even with imperfect knowledge, it doesn't investigate strategies for optimal human-AI collaboration or adaptive expertise thresholds.
- What evidence would resolve it: Experiments testing different strategies for when to override algorithmic decisions versus deferring to them, potentially using confidence metrics or uncertainty measures, would identify optimal collaboration patterns.

## Limitations
- The algorithm's effectiveness depends on the reliability of human knowledge, which may not be available or accurate in many domains
- The four knowledge request criteria may not capture all situations where human input would be beneficial
- The method was only evaluated on synthetic data from known networks, not real-world datasets with data quality issues

## Confidence

**High confidence**: The core concept that active learning improves structural accuracy compared to no knowledge, and that targeting knowledge requests to specific ambiguous situations (equivalent add) is effective

**Medium confidence**: The claim that active learning outperforms traditional predefined knowledge approaches, as this depends on specific implementation choices and benchmark selection

**Medium confidence**: The robustness results showing active learning maintains accuracy with 10-20% incorrect human responses, as these depend on the simulation methodology

## Next Checks

1. Implement and test each of the four knowledge request criteria independently on a medium-sized benchmark network to verify their relative effectiveness and identify any implementation issues

2. Conduct a sensitivity analysis on the thresholds used for the "small counts" and "unreliable score" criteria to determine optimal values for different network sizes and sample counts

3. Test the Tabu-AL algorithm with real human experts on a simple benchmark network (e.g., asia) rather than simulated responses to validate the knowledge request criteria in practice