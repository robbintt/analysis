---
ver: rpa2
title: 'PUMA: Efficient Continual Graph Learning for Node Classification with Graph
  Condensation'
arxiv_id: '2312.14439'
source_url: https://arxiv.org/abs/2312.14439
tags:
- graph
- learning
- memory
- training
- puma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes PUMA, an efficient continual graph learning
  framework that extends our prior work CaT to address its limitations. PUMA introduces
  three key innovations: (1) a pseudo-label guided memory bank to leverage information
  from unlabelled nodes during graph condensation, (2) a retraining strategy to balance
  learned knowledge across tasks, and (3) one-time propagation and wide graph encoders
  to accelerate condensation and training.'
---

# PUMA: Efficient Continual Graph Learning for Node Classification with Graph Condensation

## Quick Facts
- arXiv ID: 2312.14439
- Source URL: https://arxiv.org/abs/2312.14439
- Reference count: 40
- Primary result: Achieves up to 77.9% average performance and 98.0% accuracy on CoraFull and Reddit datasets respectively

## Executive Summary
PUMA addresses the challenge of efficient continual graph learning by extending our prior work CaT with three key innovations: a pseudo-label guided memory bank for unlabeled nodes, a retraining strategy to balance knowledge across tasks, and one-time propagation with wide graph encoders for faster condensation and training. The framework achieves state-of-the-art performance on six datasets while maintaining significant computational efficiency, demonstrating that graph condensation can be effectively applied to streaming graph data without catastrophic forgetting.

## Method Summary
PUMA extends CaT by introducing a pseudo-label guided memory bank that leverages information from unlabeled nodes during graph condensation, using high-confidence pseudo-labels from a trained classifier. The framework employs one-time propagation to reduce redundant message passing computations by precomputing aggregated features that are reused across different random encoder instantiations. A retraining strategy with weight reinitialization is implemented to mitigate imbalanced learned knowledge across tasks, allowing the model to balance old and new knowledge effectively. The system uses edge-free condensed graphs with MLP training, achieving faster condensation and training speeds while maintaining competitive performance.

## Key Results
- Achieves up to 77.9% average performance on CoraFull dataset
- Achieves 98.0% accuracy on Reddit dataset
- Maintains fast condensation and training speeds while achieving state-of-the-art performance

## Why This Works (Mechanism)

### Mechanism 1: Pseudo-label guided memory bank
The framework trains a classifier on the existing memory bank and condensed graph to generate pseudo-labels for unlabeled nodes. High-confidence pseudo-labels (confidence score > threshold) expand the training set during distribution matching, improving condensed graph embedding accuracy. This works under the assumption that the classifier can reliably assign high-confidence pseudo-labels, but could introduce noise if pseudo-label quality is poor.

### Mechanism 2: One-time propagation
Aggregated features F = LX are extracted from the original graph once and reused across multiple random graph encoder instantiations during condensation, avoiding repeated neighbor aggregation. This assumes the structural information captured in F is sufficient across different encoders, but may become outdated if graph structure changes significantly between tasks.

### Mechanism 3: Retraining strategy
Model weights are reinitialized to random values before training on new memories, preventing bias toward previously learned tasks and enabling balanced learning of new and old knowledge. This approach assumes reinitialization provides better balance than continuous updates, though it may slow convergence if it causes excessive forgetting of previously learned patterns.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their message-passing mechanism
  - Why needed here: PUMA builds upon GNN foundations for graph representation learning and uses GNNs for pseudo-label generation
  - Quick check question: How does a standard GNN layer aggregate information from neighboring nodes?

- Concept: Continual Learning and Catastrophic Forgetting
  - Why needed here: PUMA addresses catastrophic forgetting in streaming graph data where models overwrite previous knowledge when learning new tasks
  - Quick check question: What are the main approaches to mitigate catastrophic forgetting in continual learning?

- Concept: Dataset Condensation and Distribution Matching
  - Why needed here: PUMA uses graph condensation to create small synthetic graphs that approximate original graph distribution for efficient replay
  - Quick check question: How does distribution matching in dataset condensation differ from simple data sampling?

## Architecture Onboarding

- Component map: Input -> Graph Condensation Module -> Memory Bank -> Training Module -> Output
- Critical path: Graph condensation → Pseudo-label generation → Memory bank update → MLP training with reinitialization
- Design tradeoffs:
  - One-time propagation vs. repeated aggregation: Reduces computation but assumes static graph structure
  - Pseudo-labels vs. labeled data only: Expands training data but introduces potential noise
  - Weight reinitialization vs. continuous updates: Balances knowledge but may slow convergence
- Failure signatures:
  - Poor pseudo-label quality: Classifier generates low-confidence or incorrect pseudo-labels
  - Distribution mismatch: Condensed graphs don't adequately approximate original graph distributions
  - Imbalanced training: Model overfits to either new or old tasks despite reinitialization
- First 3 experiments:
  1. Test pseudo-label quality: Generate pseudo-labels on validation set and measure confidence score distribution and accuracy
  2. Validate one-time propagation efficiency: Compare condensation time with and without one-time propagation on medium-sized graph
  3. Evaluate retraining impact: Compare performance with and without weight reinitialization across multiple task sequences

## Open Questions the Paper Calls Out

- Question: How does the pseudo-labeling approach affect the quality of the condensed graph in terms of preserving structural information and node embeddings?
  - Basis in paper: The paper introduces a pseudo-label guided memory bank (PUMA) that uses pseudo-labels to expand the coverage of nodes during graph condensation.
  - Why unresolved: The paper does not provide a detailed analysis of the impact of pseudo-labeling on the structural integrity of the condensed graph and the quality of node embeddings.
  - What evidence would resolve it: Comparative experiments evaluating the structural similarity and embedding quality of condensed graphs with and without pseudo-labeling.

- Question: What is the optimal budget ratio for the memory bank to balance performance and efficiency in different graph datasets?
  - Basis in paper: The paper discusses the budget ratio for the memory bank and its impact on performance, but does not provide a comprehensive analysis of the optimal budget ratio for different datasets.
  - Why unresolved: The paper mentions the budget ratio but does not explore its optimal value for various graph datasets.
  - What evidence would resolve it: Experiments evaluating the performance of PUMA with different budget ratios across various graph datasets to determine the optimal budget ratio for each.

- Question: How does the retraining strategy in PUMA affect the convergence speed and stability of the model during continual learning?
  - Basis in paper: The paper introduces a retraining strategy to balance learned knowledge across tasks, but does not provide a detailed analysis of its impact on convergence speed and stability.
  - Why unresolved: The paper mentions the retraining strategy but does not explore its effects on the convergence speed and stability of the model during continual learning.
  - What evidence would resolve it: Experiments comparing the convergence speed and stability of PUMA with and without the retraining strategy during continual learning.

## Limitations

- Pseudo-label quality dependency: Framework performance heavily relies on classifier accuracy for unlabeled nodes
- Static graph assumption: One-time propagation may not work well for dynamic graphs with evolving structures
- Retraining balance: Finding optimal reinitialization parameters is dataset-dependent and not fully explored

## Confidence

- High Confidence: Graph condensation framework implementation, MMD-based distribution matching, and overall experimental methodology
- Medium Confidence: Pseudo-label guided memory bank effectiveness and retraining strategy benefits, due to limited ablation studies
- Low Confidence: One-time propagation efficiency gains and wide graph encoders' specific architectural contributions, as these lack detailed validation

## Next Checks

1. **Pseudo-label quality validation**: Implement confidence score distribution analysis and accuracy measurement of pseudo-labels on a held-out validation set to verify their reliability for condensation.
2. **Dynamic graph scenario testing**: Evaluate PUMA's performance on graphs with temporal structure changes to assess whether one-time propagation remains effective when graph topology evolves between tasks.
3. **Ablation study on retraining frequency**: Systematically vary the retraining strategy parameters (e.g., reinitialization frequency, weight reset magnitude) to identify optimal settings and understand the trade-off between knowledge retention and new task learning.