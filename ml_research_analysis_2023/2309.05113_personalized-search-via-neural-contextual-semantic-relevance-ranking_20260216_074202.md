---
ver: rpa2
title: Personalized Search Via Neural Contextual Semantic Relevance Ranking
arxiv_id: '2309.05113'
source_url: https://arxiv.org/abs/2309.05113
tags:
- search
- context
- query
- document
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a neural ranking framework for personalized
  search by leveraging document and query context information. The key idea is to
  capture the relevance between documents and user query contexts using both lexical
  representations (e.g., BM25) and semantic embeddings (e.g., SentenceBERT).
---

# Personalized Search Via Neural Contextual Semantic Relevance Ranking

## Quick Facts
- arXiv ID: 2309.05113
- Source URL: https://arxiv.org/abs/2309.05113
- Reference count: 35
- Primary result: Neural ranking framework using lexical and semantic features improves personalized search performance on industrial datasets

## Executive Summary
This paper proposes a neural ranking framework for personalized search that leverages both document and query context information. The key innovation is capturing relevance between documents and user query contexts using both lexical representations (BM25) and semantic embeddings (SentenceBERT). These context-document matching scores are incorporated into a Deep Cross Network model to improve ranking performance. Experiments on two industrial search datasets demonstrate significant improvements in NDCG and MAP compared to baselines without context, with the model also showing good generalization to out-of-domain data.

## Method Summary
The method combines lexical and semantic features for personalized document ranking using a Deep Cross Network (DCN). For each query-document-context triple, the system computes BM25 lexical matching scores and SentenceBERT semantic similarity scores. These context-document relevance scores are combined with traditional learning-to-rank features and fed into a DCN model that learns feature interactions automatically. The model is trained using pairwise hinge loss with Adam optimizer and evaluated on standard ranking metrics including NDCG@10, MAP, Precision@10, and Recall@10.

## Key Results
- Adding contextual signals significantly improves NDCG and MAP compared to baselines without context
- The model generalizes well to out-of-domain data when trained on mixed datasets
- Ablation study confirms benefits of combining lexical and semantic features for context-document matching
- Performance gains are consistent across both tested industrial search datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating both lexical and semantic matching signals improves ranking performance.
- Mechanism: The model uses BM25 for exact token-level matching and SentenceBERT for semantic embedding similarity, combining both via a Deep Cross Network (DCN) to capture complementary strengths at lexical and semantic granularity.
- Core assumption: Lexical and semantic features capture non-overlapping aspects of document-query relevance, so their combination provides better performance than either alone.
- Evidence anchors: [abstract], [section] statements about combining lexical and semantic features
- Break condition: If lexical and semantic features are highly correlated or redundant, combining them may provide minimal benefit or could even harm performance due to overfitting.

### Mechanism 2
- Claim: Modeling document-context relevance as additional features improves personalized ranking.
- Mechanism: The system computes context-document matching scores (S_lex and S_sem) and feeds them into the DCN model alongside traditional query-document features, allowing the model to learn how well documents fit user contexts.
- Core assumption: Documents that match user context attributes (e.g., job role, location) are more relevant for personalized search than those that don't.
- Evidence anchors: [abstract], [section] statements about data enrichment of personalized query context information
- Break condition: If context attributes are not predictive of user intent or if the context-document matching scores don't correlate with actual relevance, this mechanism will fail.

### Mechanism 3
- Claim: The DCN model's ability to automatically learn feature interactions improves ranking performance.
- Mechanism: The Deep Cross Network captures complex interactions between document-context matching scores and other features through its cross layers, mapping input features to embeddings that emphasize these interactions.
- Core assumption: Feature interactions between context-document relevance and traditional ranking features are important for accurate ranking.
- Evidence anchors: [section] statement about DCN learning feature interactions automatically
- Break condition: If feature interactions are not important for this specific ranking task, or if the DCN model overfits to training data without generalizing.

## Foundational Learning

- Concept: Information Retrieval (IR) fundamentals and ranking metrics
  - Why needed here: The paper builds on traditional IR concepts like BM25 and uses standard ranking metrics (NDCG, MAP, precision@k, recall@k)
  - Quick check question: What is the difference between precision@k and recall@k in ranking evaluation?

- Concept: Deep Cross Network (DCN) architecture
  - Why needed here: The paper uses DCN as the core ranking model, which requires understanding how cross layers work
  - Quick check question: How do cross layers in DCN differ from standard dense layers in neural networks?

- Concept: Sentence embeddings and semantic similarity
  - Why needed here: The paper uses SentenceBERT to generate document and context embeddings for semantic matching
  - Quick check question: What is the difference between using average pooling vs. CLS token for SentenceBERT embeddings?

## Architecture Onboarding

- Component map: Query → Feature extraction → DCN → Relevance score → Ranking
- Critical path: Query → Feature extraction → DCN → Relevance score → Ranking
  The most critical components are the feature extraction (lexical + semantic) and the DCN model itself.
- Design tradeoffs:
  - BM25 vs. other lexical methods: BM25 chosen for popularity and effectiveness
  - SentenceBERT vs. other embeddings: Pre-trained model provides good semantic representations without training from scratch
  - DCN vs. other ranking models: DCN chosen for its ability to learn feature interactions automatically
- Failure signatures:
  - Poor performance on datasets without context signals (D1-B, D2-B)
  - Overfitting indicated by large performance gap between training and test sets
  - Degradation when removing either lexical or semantic features (as shown in ablation study)
- First 3 experiments:
  1. Train with only lexical features (BM25) and evaluate on D1-A to establish baseline
  2. Train with only semantic features (SentenceBERT) and evaluate on D1-A to compare effectiveness
  3. Train with mixed training (D1-A + D1-B) and test on D1-B to verify generalization without context signals

## Open Questions the Paper Calls Out
No specific open questions are called out in the paper.

## Limitations
- The study lacks ablation testing to isolate the contribution of DCN's cross-layer architecture versus the contextual features themselves
- The analysis doesn't explore how sensitive performance is to different context types or document lengths
- The paper doesn't provide comparison to existing personalized search approaches that use user interaction behaviors

## Confidence

- High Confidence: The experimental methodology is sound, with appropriate evaluation metrics (NDCG, MAP) and statistical testing. The improvement over baselines without context is clearly demonstrated.
- Medium Confidence: The mechanism claim that lexical and semantic features capture complementary information is plausible but not rigorously tested.
- Medium Confidence: The DCN architecture's contribution to performance is assumed but not directly validated through architectural ablation studies.

## Next Checks

1. Conduct ablation study removing the cross layers from DCN while keeping all contextual features to quantify the architectural contribution versus the feature engineering contribution.

2. Test the model on datasets with varying context granularity (e.g., user profiles vs. session context) to understand robustness to context quality and type.

3. Evaluate the model on a third, out-of-domain dataset (e.g., academic search or e-commerce) to verify that context-document matching generalizes beyond the industrial search domains tested.