---
ver: rpa2
title: 'FactLLaMA: Optimizing Instruction-Following Language Models with External
  Knowledge for Automated Fact-Checking'
arxiv_id: '2309.00240'
source_url: https://arxiv.org/abs/2309.00240
tags:
- language
- evidence
- fact
- checking
- external
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of automated fact-checking by combining
  instruction-following language models with external evidence retrieval to improve
  accuracy. The core method involves instruct-tuning the LLaMA language model using
  external evidence collected from search engines via Google API, integrated during
  training with the LORA parameter-efficient tuning method.
---

# FactLLaMA: Optimizing Instruction-Following Language Models with External Knowledge for Automated Fact-Checking

## Quick Facts
- arXiv ID: 2309.00240
- Source URL: https://arxiv.org/abs/2309.00240
- Reference count: 40
- FactLLaMA achieves F1-scores of 0.5565 on RAWFC and 0.3044 on LIAR datasets

## Executive Summary
FactLLaMA addresses automated fact-checking by combining instruction-following language models with external evidence retrieval. The system enhances LLaMA's fact-checking capabilities by retrieving relevant evidence from search engines and integrating it during instruction-tuning using LORA parameter-efficient methods. This approach converts fact-checking into a sequence-to-sequence problem, allowing the model to generate fact-checking results based on input claims and retrieved evidence. Experiments demonstrate state-of-the-art performance on RAWFC and LIAR datasets, significantly outperforming traditional baseline methods.

## Method Summary
The method involves instruct-tuning LLaMA-7B using LORA parameter-efficient fine-tuning while incorporating external evidence retrieved via Google API. The system frames fact-checking as a sequence-to-sequence generation problem, where claims and retrieved evidence serve as input to generate fact-checking results. The LORA technique compresses model parameters into low-rank matrices, enabling fine-tuning on consumer-grade GPUs with 16GB memory. The model is trained for 3 epochs with batch size 32, Adam optimizer, and dropout regularization.

## Key Results
- Achieves F1-score of 0.5565 on RAWFC dataset, surpassing all baseline methods
- Outperforms traditional approaches including SVM, CNN, RNN, and BERT on both RAWFC and LIAR datasets
- Demonstrates significant improvement over vanilla LLaMA models without instruction-tuning

## Why This Works (Mechanism)

### Mechanism 1
Instruct-tuning with external evidence retrieval improves fact-checking accuracy over vanilla LLMs by augmenting the model's knowledge with up-to-date information. The system bridges the gap between static pretraining knowledge and current information needs through evidence retrieval via Google API.

### Mechanism 2
LORA parameter-efficient tuning enables effective adaptation with limited GPU resources by compressing model parameters into low-rank matrices, reducing memory requirements from 64GB to 16GB while preserving most information.

### Mechanism 3
Converting fact-checking to a sequence-to-sequence problem enables generative models to produce more accurate results by capturing context and nuances better than simple classification approaches.

## Foundational Learning

- **Sequence-to-sequence modeling**: Needed to understand how sequence models map input to output sequences in the text generation approach. Quick check: How does a transformer decoder differ from a classifier head in handling input-output relationships?

- **Low-rank matrix factorization**: Fundamental to understanding LORA's memory savings through parameter compression. Quick check: What is the relationship between matrix rank and the number of parameters in LORA's approximation?

- **Instruction-tuning methodology**: Essential for understanding how models learn to follow natural language instructions differently from standard fine-tuning. Quick check: How does instruction-tuning differ from task-specific fine-tuning in terms of training data and objectives?

## Architecture Onboarding

- **Component map**: Input claims and instructions → Evidence retrieval via Google API → LLaMA model with LORA adapters → Fact-checking result generation → Training pipeline with sequence-to-sequence objective

- **Critical path**: Claim → Evidence retrieval → Sequence-to-sequence generation → Fact-checking output

- **Design tradeoffs**: Memory vs. accuracy (LORA enables training on limited hardware but may constrain model capacity), retrieval quality vs. latency (comprehensive searches improve accuracy but increase processing time), generative vs. discriminative (sequence generation captures nuance but may be less precise than classification)

- **Failure signatures**: Low F1 scores on specific classes (e.g., HALF-TRUE) indicate difficulty with nuanced claims, high variance across epochs suggests training instability, evidence retrieval failures manifest as poor performance on claims requiring external knowledge

- **First 3 experiments**: 1) Baseline comparison: LLaMA-7B without tuning vs. with instruction-tuning (no external evidence), 2) Retrieval ablation: Compare performance with and without evidence retrieval on RAWFC dataset, 3) Memory efficiency test: Compare LORA-tuned model performance against full fine-tuning on identical hardware

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several areas remain unexplored including performance comparison across different search engines, scalability to larger LLaMA variants, and systematic analysis of evidence quality impact on model performance.

## Limitations

- Performance heavily depends on the quality and relevance of retrieved evidence, with no detailed analysis of retrieval failure rates
- Limited dataset size (2,012 claims in RAWFC) raises questions about generalizability and potential overfitting
- Hardware accessibility trade-offs with LORA may constrain performance in resource-constrained environments

## Confidence

- **High Confidence**: Sequence-to-sequence conversion methodology and its positive impact on fact-checking accuracy
- **Medium Confidence**: Effectiveness of LORA parameter-efficient tuning for maintaining model performance while reducing memory requirements
- **Low Confidence**: Generalizability of the approach to domains beyond tested datasets, particularly specialized domains requiring expert knowledge

## Next Checks

1. **Evidence Retrieval Robustness Test**: Systematically evaluate model performance when evidence retrieval returns irrelevant, contradictory, or no evidence for 10-20% of test claims

2. **Cross-Dataset Generalization**: Test the trained model on fact-checking datasets from different domains (scientific claims, political statements, or domain-specific misinformation) to assess performance degradation

3. **Hardware-Aware Scaling Analysis**: Conduct controlled experiments comparing LORA-tuned models against full fine-tuned versions across different parameter sizes (7B, 3B, 1.3B) on identical hardware to quantify exact performance trade-offs