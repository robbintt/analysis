---
ver: rpa2
title: Causal Deep Learning
arxiv_id: '2301.00314'
source_url: https://arxiv.org/abs/2301.00314
tags:
- causal
- tensor
- data
- neural
- factor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces causal deep neural networks derived from
  tensor (multilinear) factor analysis for addressing forward and inverse causal inference.
  Forward causal inference is tackled using causal capsules and tensor transformers
  to estimate latent variables representing causal factors and model their interactions.
---

# Causal Deep Learning

## Quick Facts
- arXiv ID: 2301.00314
- Source URL: https://arxiv.org/abs/2301.00314
- Authors: M. Alex O. Vasilescu
- Reference count: 40
- This paper introduces causal deep neural networks derived from tensor (multilinear) factor analysis for addressing forward and inverse causal inference, achieving 78.93% accuracy on the Labeled Faces in the Wild dataset while training on less than 1% of the images used by prior deep learning approaches.

## Executive Summary
This paper presents causal deep neural networks that derive from tensor (multilinear) factor analysis to address both forward and inverse causal inference. The approach uses causal capsules and tensor transformers to estimate latent variables representing causal factors and model their interactions. For inverse causal inference, the method employs multilinear projection to estimate causes of effects by reversing the operations of a forward model. To handle underdetermined inverse problems, the paper proposes modeling different aspects of data formation with piecewise tensor models that produce multiple candidate solutions which are then gated to yield a unique solution.

## Method Summary
The method transforms tensor factor analysis into causal neural networks by replacing each SVD step in M-mode SVD with gradient descent optimization using Hebb autoencoders. This enables scalable causal inference while preserving the mathematical properties of tensor decomposition. For inverse causal inference, instead of aggressive dimensionality reduction, the approach models different aspects of data formation with piecewise tensor models whose multilinear projections produce multiple candidate solutions. These solutions are combined through a gating mechanism to yield a unique answer. The architecture employs compositional hierarchical block tensor factorization to enable efficient and interpretable causal factor learning by decomposing wholes into parts.

## Key Results
- Achieved 78.93% accuracy on the Labeled Faces in the Wild (LFW) dataset while training on less than 1% of the images used by prior deep learning approaches
- Successfully addressed both forward causal inference (estimating effects from causes) and inverse causal inference (estimating causes of effects) through multilinear projection
- Demonstrated that piecewise tensor models can produce well-defined multilinear projections for inverse inference, avoiding the ill-posed nature of underdetermined problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal deep neural networks are built from tensor factor analysis by replacing each SVD step with gradient descent optimization, enabling scalable causal inference.
- Mechanism: The paper transforms tensor factor analysis into causal neural networks by substituting every SVD step in the M-mode SVD algorithm with Hebb autoencoders that perform gradient descent optimization, thus enabling the network to learn invariant causal factor representations and their interactions.
- Core assumption: Replacing SVD steps with Hebb autoencoders preserves the mathematical properties of the original tensor decomposition while enabling end-to-end learning.
- Evidence anchors:
  - [abstract] "Causal deep neural networks are composed of causal capsules and tensor transformers... Causal capsules may be implemented as shallow Hebb autoencoders... The tensor transformer may be implemented as a tensor autoencoder."
  - [section] "The M-mode SVD (Algorithm 1) is transformed into a causal neural network (Fig. 1) by replacing every SVD step with gradient descent optimization, which is outsourced to a Hebb autoencoder"
- Break condition: If the Hebb autoencoder fails to converge or cannot learn the orthonormal basis vectors required for the tensor decomposition, the causal factor representations will be inaccurate.

### Mechanism 2
- Claim: Piecewise tensor models enable well-defined multilinear projections for inverse causal inference, avoiding the ill-posed nature of underdetermined inverse problems.
- Mechanism: Instead of aggressive dimensionality reduction that may mask ill-posed problems, the paper proposes modeling different aspects of data formation with piecewise tensor models, where each model's inverse is well-posed, and then gating candidate solutions to yield a unique solution.
- Core assumption: Different aspects of the data formation process can be accurately modeled by separate tensor models, and their combination via gating produces a meaningful solution.
- Evidence anchors:
  - [abstract] "As an alternative to aggressive bottleneck dimension reduction or regularized regression that may camouflage an inherently underdetermined inverse problem, we prescribe modeling different aspects of the mechanism of data formation with piecewise tensor models whose multilinear projections produce multiple candidate solutions."
  - [section] "Alternatively or in addition to dimensionality reduction and regularized regression, we prescribe modeling different aspects of the data formation process with piecewise tensor (multilinear) models (mixture of experts) whose projections are well-defined."
- Break condition: If the piecewise models do not adequately capture the different aspects of data formation, or if the gating mechanism fails to select the correct solution, the inverse inference will be inaccurate.

### Mechanism 3
- Claim: Compositional hierarchical block tensor factorization enables efficient and interpretable causal factor learning by decomposing wholes into parts.
- Mechanism: The paper proposes a part-based hierarchy where data tensors are recursively subdivided, analyzed bottom-up, and merged, allowing causal factors to be learned from parts rather than requiring the full observation.
- Core assumption: Causal factors of object wholes can be efficiently computed from their parts, and the hierarchical structure preserves the necessary information for accurate causal inference.
- Evidence anchors:
  - [section] "A data tensor expressed as a part-based a hierarchy is a unified tensor model of wholes and parts. The resulting causal factor representations are interpretable, hierarchical, and statistically invariant to all other causal factors."
  - [section] "Causal factors of object wholes may be computed efficiently from their parts, by applying a permutation matrix P and creating part-based data clusters with a segmentation filter Hm"
- Break condition: If the part-based decomposition loses critical information or the hierarchical merging is not optimal, the causal factor representations will be incomplete or inaccurate.

## Foundational Learning

- Concept: Tensor (multilinear) factor analysis
  - Why needed here: The entire causal deep learning framework is derived from tensor factor analysis, which provides the mathematical foundation for modeling multi-causal mechanisms of data formation and performing both forward and inverse causal inference.
  - Quick check question: Can you explain the difference between matrix SVD and M-mode SVD in tensor analysis?

- Concept: Causal inference vs. regression
  - Why needed here: The paper emphasizes that causal neural networks are fundamentally different from conventional regression models, focusing on modeling mechanisms of data formation rather than just correlations in observed data.
  - Quick check question: What is the key distinction between predicting co-observed variables and estimating the effects of interventions in causal inference?

- Concept: Multilinear projection for inverse causal inference
  - Why needed here: Inverse causal inference, which estimates the causes of effects, is implemented through multilinear projection that reverses the operations of the forward model and relies on the unitary constraint of causal factor representations.
  - Quick check question: How does multilinear projection differ from traditional regression techniques when solving underdetermined inverse problems?

## Architecture Onboarding

- Component map: Data → Kernel pre-processing → Causal capsules (learn causal factors) → Tensor transformers (model interactions) → Output causal representations. For inverse inference: Forward model → Multilinear projection → Piecewise models → Gating → Causes of effects.
- Critical path: Data → Kernel pre-processing → Causal capsules (learn causal factors) → Tensor transformers (model interactions) → Output causal representations. For inverse inference: Forward model → Multilinear projection → Piecewise models → Gating → Causes of effects.
- Design tradeoffs:
  - Shallow vs. deep implementations: Shallow autoencoders are mathematically equivalent but less scalable; deep hierarchies enable handling larger datasets
  - Aggressive dimensionality reduction vs. piecewise modeling: The paper advocates for the latter to avoid masking ill-posed problems
  - Sequential vs. parallel computation: Parallel computation is faster but may require more memory; sequential is slower but more memory-efficient
- Failure signatures:
  - Poor convergence of Hebb autoencoders indicates issues with learning causal factor representations
  - Inaccurate inverse inference suggests problems with piecewise modeling or gating mechanism
  - Lack of scalability points to insufficient hierarchical decomposition
  - Poor generalization on test data may indicate overfitting to training causal factors
- First 3 experiments:
  1. Implement and test a single causal capsule (Hebb autoencoder) on a simple dataset with known causal factors to verify it learns the correct representations
  2. Test the tensor transformer with synthetic data where causal factor interactions are known to verify it correctly models the interactions
  3. Implement the full forward causal network on a small image dataset and verify it produces interpretable causal factor representations that are invariant to nuisance factors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the accuracy of causal deep learning models beyond the 78.93% achieved on the LFW dataset?
- Basis in paper: [explicit] The paper reports achieving 78.93% accuracy on the Labeled Faces in the Wild dataset, but acknowledges that this falls short of human performance and the state-of-the-art DeepFace model which achieved 97.35% accuracy.
- Why unresolved: The paper focuses on deriving the causal deep learning architecture from tensor factor analysis, but does not extensively explore methods to further improve accuracy beyond the initial experiments reported.
- What evidence would resolve it: Additional experiments could be conducted to test different model architectures, training strategies, and data preprocessing techniques to improve accuracy. Comparing the performance of the causal deep learning model to other state-of-the-art face recognition methods on the LFW dataset would also provide evidence of its effectiveness.

### Open Question 2
- Question: How can the computational efficiency of causal deep learning models be improved for real-time applications?
- Basis in paper: [explicit] The paper mentions that the causal deep learning models are derived for computational scalability, but does not provide extensive details on their runtime performance or optimization techniques for real-time applications.
- Why unresolved: The paper focuses on the theoretical foundations and experimental results of causal deep learning, but does not delve into practical considerations such as computational efficiency and optimization for real-time deployment.
- What evidence would resolve it: Benchmarking the runtime performance of causal deep learning models on various hardware platforms and comparing them to other deep learning architectures would provide evidence of their computational efficiency. Exploring techniques such as model compression, quantization, and hardware acceleration could also help improve runtime performance.

### Open Question 3
- Question: How can causal deep learning models be extended to handle more complex causal relationships and data types beyond facial recognition?
- Basis in paper: [explicit] The paper focuses on applying causal deep learning to facial recognition tasks, but acknowledges that the models are data agnostic and could potentially be applied to other domains.
- Why unresolved: The paper provides a theoretical framework for causal deep learning and demonstrates its effectiveness on facial recognition, but does not explore its applicability to other domains or more complex causal relationships.
- What evidence would resolve it: Applying causal deep learning models to other domains such as natural language processing, medical diagnosis, or autonomous driving, and evaluating their performance on tasks involving complex causal relationships would provide evidence of their generalizability. Investigating the theoretical foundations and architectural modifications required to handle different data types and causal structures would also contribute to resolving this question.

## Limitations

- The scalability of the proposed tensor factorization approach is uncertain, with no experiments demonstrating performance on datasets larger than 100 subjects
- The gating mechanism for selecting among multiple candidate solutions in inverse inference is described conceptually but lacks detailed implementation specifications
- The Hebb autoencoder convergence guarantees are not formally established, and the piecewise modeling approach lacks quantitative comparison against standard regularization techniques

## Confidence

- Forward causal inference mechanism: Medium
- Inverse causal inference approach: Low
- Scalability claims: Medium

## Next Checks

1. Implement the piecewise inverse inference pipeline on a synthetic dataset with known ground truth causal factors to verify the multilinear projection and gating mechanism produce accurate results.
2. Conduct ablation studies comparing the proposed causal factor learning against conventional autoencoders and regularized regression on underdetermined inverse problems.
3. Benchmark computational complexity and memory requirements against standard deep learning approaches on progressively larger datasets to validate scalability claims.