---
ver: rpa2
title: 'The Cost of Compression: Investigating the Impact of Compression on Parametric
  Knowledge in Language Models'
arxiv_id: '2312.00960'
source_url: https://arxiv.org/abs/2312.00960
tags:
- pruning
- compression
- overall
- drop
- pruned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models are increasingly used for a variety of tasks
  but are challenging to deploy due to their size and associated computational costs.
  Compression techniques such as pruning and quantization offer a potential solution
  by reducing model size while aiming to preserve performance.
---

# The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models

## Quick Facts
- arXiv ID: 2312.00960
- Source URL: https://arxiv.org/abs/2312.00960
- Reference count: 40
- Large language models are increasingly used for a variety of tasks but are challenging to deploy due to their size and associated computational costs. Compression techniques such as pruning and quantization offer a potential solution by reducing model size while aiming to preserve performance. Prior research has primarily focused on general metrics such as perplexity or task accuracy, leaving a gap in understanding how compression affects parametric knowledgeâ€”the knowledge stored in model weights during pretraining. This work addresses this gap by systematically analyzing the impact of compression on parametric knowledge across three model families: ENCODER, ENCODER-DECODER, and DECODER models. Using benchmarks such as LAMA and LM-HARNESS, the study evaluates the effects of pruning and quantization on model performance, with a particular focus on parametric knowledge. Key findings include: (1) Pruning all modules together has the most significant negative impact on parametric knowledge compared to pruning specific modules. (2) At pruning levels greater than 50%, parametric knowledge declines rapidly across all models. (3) Quantizing attention modules has less impact on performance than quantizing feed-forward networks. (4) Structured pruning of the final layer is more detrimental than unstructured pruning. These insights provide practitioners with actionable guidelines for selecting compression techniques that balance efficiency and knowledge preservation. The codebase is publicly released to enable further research.

## Executive Summary
Large language models (LLMs) face deployment challenges due to their size and computational costs. Compression techniques like pruning and quantization can reduce model size but may affect the knowledge stored in model weights (parametric knowledge). This study systematically investigates how compression impacts parametric knowledge across three transformer model families: encoder-only, encoder-decoder, and decoder-only models. Using benchmarks like LAMA and LM-HARNESS, the research evaluates the effects of different compression techniques on model performance. The findings reveal that pruning all modules together causes the largest drop in parametric knowledge, with significant degradation at pruning levels above 50%. Quantizing attention modules is less harmful than quantizing feed-forward networks, and structured pruning of the final layer is more detrimental than unstructured pruning. These insights provide actionable guidelines for balancing efficiency and knowledge preservation in compressed models.

## Method Summary
The study evaluates compression techniques on three model families: encoder-only (BERT, RoBERTa, DistilBERT), encoder-decoder (Flan-T5), and decoder-only (Vicuna, WizardLM). Pruning (L1-unstructured, structured) and quantization (post-training dynamic, 8-bit) are applied at module levels (attention, feed-forward, overall) and sparsity levels (10% to 90%). Performance is measured using LAMA for encoder-only models and BoolQ, PIQA, and Winogrande from LM-HARNESS for encoder-decoder and decoder-only models. The study focuses on zero-shot evaluation without fine-tuning.

## Key Results
- Pruning all modules together causes the largest drop in parametric knowledge.
- At pruning levels >50%, parametric knowledge declines rapidly across all models.
- Quantizing attention modules has less impact on performance than quantizing feed-forward networks.
- Structured pruning of the final layer is more detrimental than unstructured pruning.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning all modules together (Overall GP) causes the largest drop in parametric knowledge.
- Mechanism: When all attention and feed-forward layers are pruned simultaneously, the model loses both the ability to attend to relevant information and to transform it through feed-forward networks. This dual loss disrupts the entire information flow, causing a greater degradation in knowledge retention compared to pruning individual components.
- Core assumption: Attention and feed-forward modules are interdependent for preserving parametric knowledge.
- Evidence anchors:
  - [abstract] "Pruning all modules together has the most significant negative impact on parametric knowledge compared to pruning specific modules."
  - [section] "Among all the models analyzed, pruning all modules together (Overall GP) has the most significant negative impact on performance."
  - [corpus] Weak - no direct corpus evidence on this specific claim.
- Break condition: If pruning attention and feed-forward modules independently does not preserve knowledge as effectively as pruning them together, this mechanism would break down.

### Mechanism 2
- Claim: Quantizing attention modules has less impact on performance than quantizing feed-forward networks.
- Mechanism: Attention modules have fewer parameters than feed-forward networks. Quantization reduces precision but preserves the relative importance of weights. Since attention modules are smaller, their quantized versions retain more of the original signal compared to larger feed-forward networks where quantization errors compound more significantly.
- Core assumption: Attention modules are less sensitive to quantization errors due to their smaller size and different parameter distributions.
- Evidence anchors:
  - [abstract] "Quantizing attention modules has less impact on performance compared to quantizing feed-forward networks."
  - [section] "We observe that across all the models, the performance drop is less significant when quantizing attention modules (AttGQ) compared to quantizing feed-forward networks alone (FFGQ)."
  - [corpus] Weak - no direct corpus evidence on this specific claim.
- Break condition: If attention modules have parameter distributions that are more sensitive to quantization errors than feed-forward networks, this mechanism would break down.

### Mechanism 3
- Claim: Structured pruning of the final layer is more detrimental than unstructured pruning.
- Mechanism: The final layer encodes critical knowledge for downstream tasks. Structured pruning removes entire neurons or channels, disrupting the learned representations in a way that unstructured pruning (which removes individual weights) does not. This makes structured pruning more harmful to knowledge retention in the final layer.
- Core assumption: The final layer encodes knowledge in a structured or modular manner that is disrupted by structured pruning.
- Evidence anchors:
  - [abstract] "Structured pruning of the final layer is more detrimental than unstructured pruning."
  - [section] "We hypothesize that the final layer of the encoder-only models might encode knowledge in a structured or modular manner, and any form of structured compression would disrupt this encoding."
  - [corpus] Weak - no direct corpus evidence on this specific claim.
- Break condition: If the final layer encodes knowledge in a way that is equally or less sensitive to structured pruning than unstructured pruning, this mechanism would break down.

## Foundational Learning

- Concept: Understanding transformer architecture
  - Why needed here: The paper studies compression across different transformer models (encoder-only, encoder-decoder, decoder-only). Understanding the architecture is crucial to interpret how compression affects each component.
  - Quick check question: What are the main components of a transformer block and how do they differ between encoder-only, encoder-decoder, and decoder-only architectures?

- Concept: Parametric knowledge vs. other knowledge
  - Why needed here: The paper specifically focuses on parametric knowledge (knowledge stored in model weights). Understanding this distinction is crucial to interpret the results and their implications.
  - Quick check question: How does parametric knowledge differ from other types of knowledge that might be present in a language model?

- Concept: Compression techniques (pruning and quantization)
  - Why needed here: The paper uses both pruning and quantization techniques. Understanding how these work is crucial to interpret the results and their implications.
  - Quick check question: How do structured and unstructured pruning differ, and what are the trade-offs between different quantization approaches?

## Architecture Onboarding

- Component map:
  - LAMA benchmark for encoder-only models
  - LM-Harness for encoder-decoder and decoder-only models
  - Three model families: BERT, RoBERTa, DistilBERT (encoder-only); Flan-T5 (encoder-decoder); Vicuna, WizardLM (decoder-only)
  - Compression techniques: L1-unstructured pruning, Lp-structured pruning, post-training dynamic quantization
  - Evaluation metrics: Top-1 accuracy for LAMA, accuracy for LM-Harness tasks

- Critical path:
  1. Load pre-trained model
  2. Apply compression technique
  3. Evaluate on benchmark
  4. Record accuracy drop
  5. Repeat for different compression levels and techniques

- Design tradeoffs:
  - Model selection: Trade-off between model diversity and computational resources
  - Compression levels: Trade-off between granularity of results and computational cost
  - Benchmark selection: Trade-off between comprehensiveness and computational cost

- Failure signatures:
  - Accuracy drops to majority baseline for decoder-only and encoder-decoder models with final layer pruning
  - Rapid decline in accuracy for all models with pruning levels >50%
  - Different patterns of accuracy drop for different compression techniques

- First 3 experiments:
  1. Apply 20% L1-unstructured pruning to BERT-base and evaluate on LAMA
  2. Apply 8-bit post-training dynamic quantization to RoBERTa-large and evaluate on LAMA
  3. Apply both 20% pruning and 8-bit quantization to Flan-T5-base and evaluate on BoolQ

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do compression techniques affect knowledge in specialized downstream tasks (e.g., reasoning, common sense) compared to general knowledge probes?
- Basis in paper: [explicit] The paper states that parametric knowledge is crucial for reasoning tasks but does not extensively analyze performance on reasoning-specific benchmarks beyond LAMA.
- Why unresolved: The study primarily uses general benchmarks like LAMA and LM-HARNESS, which may not fully capture nuanced performance in specialized reasoning tasks.
- What evidence would resolve it: Empirical results comparing compressed models on task-specific reasoning datasets (e.g., ARC, CommonsenseQA) alongside general knowledge probes.

### Open Question 2
- Question: Do structured pruning patterns (e.g., layer-wise or attention-head pruning) preserve parametric knowledge better than unstructured pruning for specific model architectures?
- Basis in paper: [inferred] The paper notes that structured pruning of the final layer is more detrimental than unstructured pruning, but does not explore structured patterns across other layers or attention heads.
- Why unresolved: The study focuses on high-level comparisons (e.g., overall vs. attention-only pruning) without analyzing the impact of fine-grained structured pruning strategies.
- What evidence would resolve it: Detailed experiments comparing structured pruning patterns (e.g., head-wise, layer-wise) across different model families and tasks.

### Open Question 3
- Question: How does the sequential order of pruning and quantization (e.g., pruning first vs. quantization first) impact parametric knowledge retention?
- Basis in paper: [explicit] The paper follows a fixed sequence (pruning followed by quantization) but does not explore alternative orders.
- Why unresolved: The study assumes a specific sequence without testing the effects of reversing the order or interleaving the techniques.
- What evidence would resolve it: Comparative experiments testing different sequences of pruning and quantization on parametric knowledge and task performance.

### Open Question 4
- Question: Are there model-specific compression strategies that minimize parametric knowledge loss while maximizing efficiency gains?
- Basis in paper: [inferred] The paper highlights differences in compression effects across model families but does not propose tailored strategies for each type.
- Why unresolved: The study provides general insights but does not explore optimization techniques specific to model architectures (e.g., encoder-only vs. decoder-only).
- What evidence would resolve it: Development and evaluation of model-specific compression pipelines optimized for parametric knowledge retention.

## Limitations
- The study focuses on zero-shot evaluation without fine-tuning, which may not reflect real-world deployment scenarios.
- The evaluation is limited to a small number of downstream tasks, potentially missing nuanced performance differences.
- The study does not explore the temporal dynamics of knowledge degradation during training or adaptation.

## Confidence
- High confidence in the relative comparisons between compression techniques within each model family
- Medium confidence in the generalizability of findings across all transformer architectures
- Low confidence in the specific mechanisms proposed without additional ablation studies

## Next Checks
1. Replicate the key findings using a larger set of downstream tasks (minimum 10) to verify that the observed patterns hold across diverse evaluation scenarios
2. Conduct ablation studies to isolate the effects of quantization precision (4-bit vs 8-bit vs 16-bit) on parametric knowledge retention
3. Test the same compression pipeline on additional model families including decoder-only models beyond Vicuna and WizardLM to assess generalizability