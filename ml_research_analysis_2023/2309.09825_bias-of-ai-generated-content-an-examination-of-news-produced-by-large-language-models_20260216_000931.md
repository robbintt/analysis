---
ver: rpa2
title: 'Bias of AI-Generated Content: An Examination of News Produced by Large Language
  Models'
arxiv_id: '2309.09825'
source_url: https://arxiv.org/abs/2309.09825
tags:
- news
- bias
- generated
- articles
- female
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines gender and racial biases in AI-generated news
  content by seven large language models (LLMs), including ChatGPT. Using news articles
  from The New York Times and Reuters as a reference, the research evaluates biases
  at word, sentence, and document levels.
---

# Bias of AI-Generated Content: An Examination of News Produced by Large Language Models

## Quick Facts
- arXiv ID: 2309.09825
- Source URL: https://arxiv.org/abs/2309.09825
- Reference count: 40
- All seven examined LLMs exhibit significant gender and racial biases, with ChatGPT showing the lowest overall bias

## Executive Summary
This study examines gender and racial biases in AI-generated news content by seven large language models (LLMs), including ChatGPT, Cohere, and LLaMA. Using news articles from The New York Times and Reuters as reference standards, the research evaluates biases at word, sentence, and document levels. Results show all LLMs exhibit significant gender and racial biases, with particular discrimination against females and Black individuals. ChatGPT demonstrates the lowest bias overall, partly due to its reinforcement learning from human feedback (RLHF) feature, which enables it to decline generating content from biased prompts in 89.13% of cases. However, when ChatGPT does produce content from biased prompts, it generates more biased results than other models. The study recommends careful human oversight when using LLMs for content generation, especially on sensitive topics.

## Method Summary
The study uses 8,629 news articles from The New York Times and Reuters (Dec 2022-Apr 2023) as reference standards, with headlines serving as prompts for seven LLMs (Grover, GPT-2, GPT-3-curie, GPT-3-davinci, ChatGPT, Cohere, LLaMA-7B). The researchers evaluate bias at three levels: word-level using Wasserstein distance between distributions of gender/race-related words, sentence-level using sentiment and toxicity score differences, and document-level using topic modeling. They also test prompt manipulation to assess bias resistance. The generated articles are compared against the reference news articles using these metrics without further training of the LLMs.

## Key Results
- All seven examined LLMs exhibit significant gender and racial biases against females and Black individuals
- ChatGPT demonstrates the lowest overall bias, declining biased prompts 89.13% of the time due to its RLHF feature
- Larger model sizes generally correlate with lower bias, but diminishing returns occur beyond 13B parameters
- When ChatGPT does generate content from biased prompts, it produces more biased results than other models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: ChatGPT's lower bias is primarily due to its Reinforcement Learning from Human Feedback (RLHF) feature.
- **Mechanism**: RLHF enables ChatGPT to learn from human feedback on model outputs, allowing it to refine its responses and reduce biases present in the training data.
- **Core assumption**: Human feedback is effective at identifying and correcting biases that automated methods might miss.
- **Evidence anchors**:
  - [abstract]: "ChatGPT demonstrates the lowest bias overall, partly due to its reinforcement learning from human feedback (RLHF) feature"
  - [section]: "An important factor contributing to the outperformance of ChatGPT over other examined LLMs is its RLHF (reinforcement learning from human feedback) feature"
- **Break condition**: If human feedback is not representative or is itself biased, RLHF could reinforce or introduce new biases.

### Mechanism 2
- **Claim**: Larger model sizes generally correlate with lower bias in GPT models.
- **Mechanism**: As model size increases, the model can capture more nuanced patterns in the data, potentially reducing simplistic biases.
- **Core assumption**: The training data for larger models is diverse enough to capture a wide range of perspectives and reduce bias.
- **Evidence anchors**:
  - [section]: "Among the four GPT models, the degree of bias in AIGC generally decreases as the model size increases"
- **Break condition**: If larger models are trained on biased data, they may amplify existing biases rather than reduce them.

### Mechanism 3
- **Claim**: Human oversight is necessary when using LLMs for content generation, especially on sensitive topics.
- **Mechanism**: Manual review can catch biases and issues that automated methods might miss, ensuring the generated content aligns with ethical standards.
- **Core assumption**: Human reviewers are capable of identifying biases and can make informed decisions about content appropriateness.
- **Evidence anchors**:
  - [abstract]: "The study recommends careful human oversight when using LLMs for content generation, especially on sensitive topics"
- **Break condition**: If human reviewers are not properly trained or are themselves biased, their oversight might not effectively mitigate bias.

## Foundational Learning

- **Concept: Wasserstein Distance**
  - Why needed here: It's used to measure the difference between probability distributions, which is crucial for quantifying bias in word choices and topic distributions.
  - Quick check question: How does Wasserstein distance differ from other distance metrics like Euclidean distance when comparing probability distributions?

- **Concept: Topic Modeling (LDA)**
  - Why needed here: It's used to uncover prevalent semantics from a collection of documents, which is essential for evaluating document-level bias.
  - Quick check question: What is the main assumption behind the LDA model, and how might violations of this assumption affect the results?

- **Concept: Sentiment Analysis**
  - Why needed here: It's used to evaluate the expressed sentiments in sentences, which is a key component of sentence-level bias assessment.
  - Quick check question: What are some limitations of sentiment analysis tools, and how might these limitations affect the bias evaluation?

## Architecture Onboarding

- **Component map**: Data collection (NYT/Reuters articles) → LLM generation using headlines as prompts → Bias evaluation (word/sentence/document levels) → Analysis of results
- **Critical path**: Data collection → LLM generation of content → Bias evaluation using metrics (Wasserstein distance, sentiment scores, toxicity scores) → Analysis of results
- **Design tradeoffs**: Using human-written news as reference assumes these sources are unbiased; evaluation metrics balance comprehensiveness with computational efficiency
- **Failure signatures**: Generated articles significantly different in length from references; topic modeling fails to capture meaningful topics
- **First 3 experiments**:
  1. Replicate word-level bias evaluation for one LLM to verify methodology
  2. Test prompt manipulation experiment to see if biased prompts consistently lead to more biased outputs
  3. Compare results of different bias evaluation methods to understand their strengths and limitations

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What specific mechanisms in ChatGPT's RLHF training lead to reduced bias compared to other LLMs of similar size?
- **Basis in paper**: [explicit] The paper notes that ChatGPT's RLHF feature is "partly attributed to its lower bias" and demonstrates "the effectiveness of its RLHF feature in mitigating gender bias," but does not detail the specific mechanisms
- **Why unresolved**: The paper identifies RLHF as a contributing factor but does not analyze the specific training techniques, feedback mechanisms, or data curation methods that make ChatGPT's RLHF more effective at reducing bias
- **What evidence would resolve it**: Detailed analysis of ChatGPT's RLHF training process, comparison with GPT-3-davinci's training methodology, and controlled experiments isolating specific RLHF components

### Open Question 2
- **Question**: How do different news domains (sports, politics, science, etc.) influence the magnitude and type of bias in AI-generated content?
- **Basis in paper**: [inferred] The paper uses news articles from multiple domains but does not analyze bias variations across these different content types
- **Why unresolved**: The paper analyzes overall bias across all news domains combined but does not examine whether certain domains are more prone to bias or if bias manifests differently in different types of news content
- **What evidence would resolve it**: Domain-specific bias analysis comparing bias levels and patterns across different news categories

### Open Question 3
- **Question**: What is the long-term impact of AI-generated biased content on societal perceptions and reinforcement of stereotypes?
- **Basis in paper**: [inferred] While the paper discusses the potential for "propagation of misinformation, reinforcement of biases, and further widening of societal divides," it does not examine actual societal impact
- **Why unresolved**: The study focuses on measuring bias in AI-generated content but does not investigate how exposure to such content affects readers' attitudes, beliefs, or behaviors over time
- **What evidence would resolve it**: Longitudinal studies tracking changes in public opinion or behavior following exposure to AI-generated news content with varying levels of bias

## Limitations
- Analysis limited to two news sources (NYT and Reuters) as reference standards, which may not represent unbiased content across all demographics
- Focus only on gender and racial biases, excluding other forms of bias (age, disability, socioeconomic status)
- Selection of seven LLMs may not capture the full diversity of available models, particularly newer architectures

## Confidence
- **High confidence**: All examined LLMs exhibit significant gender and racial biases
- **Medium confidence**: ChatGPT shows the lowest bias overall, though high refusal rate on biased prompts may artificially inflate performance
- **Medium confidence**: Relationship between model size and bias reduction shows diminishing returns beyond 13B parameters

## Next Checks
1. Replicate the analysis using additional news sources and diverse content domains to assess generalizability of bias patterns across different contexts
2. Conduct human evaluation studies to validate automated bias metrics and assess whether they align with human perceptions of biased content
3. Test the bias resistance of newer LLM architectures and smaller, more efficient models to determine if RLHF remains the most effective mitigation strategy