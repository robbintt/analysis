---
ver: rpa2
title: An Exact Kernel Equivalence for Finite Classification Models
arxiv_id: '2308.00824'
source_url: https://arxiv.org/abs/2308.00824
tags:
- kernel
- training
- neural
- exact
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We prove that any finite-sized neural network trained with gradient
  descent can be exactly represented as a kernel machine using our Exact Path Kernel
  (EPK). The EPK accounts for the discrete steps actually taken during training, unlike
  prior work that relied on continuous gradient flow approximations.
---

# An Exact Kernel Equivalence for Finite Classification Models

## Quick Facts
- arXiv ID: 2308.00824
- Source URL: https://arxiv.org/abs/2308.00824
- Reference count: 40
- Primary result: Any finite-sized neural network trained with gradient descent can be exactly represented as a kernel machine using the Exact Path Kernel (EPK)

## Executive Summary
This paper proves that finite-sized neural networks trained with gradient descent can be exactly represented as kernel machines using the Exact Path Kernel (EPK). Unlike prior work that relied on continuous gradient flow approximations, the EPK accounts for the discrete steps actually taken during training. The authors demonstrate this exact representation for realistic networks including small CNNs, showing that the kernel can be computed to machine precision. The work reveals that neural networks exhibit non-stationary kernel properties and learn spatially transformed representations that differ from traditional kernel methods.

## Method Summary
The method computes the Exact Path Kernel (EPK) by integrating the inner product of gradients along a linear interpolation of weights between discrete training states. For each training step, the change in output is calculated by evaluating gradients at intermediate weight points, creating a kernel that captures the exact path taken during training. The approach requires saving weight states during training, computing Jacobians for both training and test points, and integrating these gradients to assemble the kernel machine representation.

## Key Results
- Any finite neural network trained with gradient descent can be exactly represented as a kernel machine
- EPK can be computed to machine precision for realistic networks including small CNNs
- Neural networks exhibit non-stationary kernel properties and learn spatially transformed representations
- The EPK provides insights into neural network generalization behavior and uncertainty estimation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A finite neural network trained with gradient descent can be exactly represented as a kernel machine.
- Mechanism: The model's output change over each discrete training step is computed by integrating the inner product of gradients along a linear interpolation of weights, creating a kernel that captures the exact path taken during training.
- Core assumption: The loss gradient remains constant across training steps (true for categorical cross-entropy with one-hot labels).
- Evidence anchors:
  - [abstract] "We prove that any finite-sized neural network trained with gradient descent can be exactly represented as a kernel machine using our Exact Path Kernel (EPK)."
  - [section 3.1] Derivation showing that for each training step, the change in output equals an integral involving gradients, which forms a valid kernel.
  - [corpus] No direct support; corpus focuses on NTK limitations rather than exact finite representations.
- Break condition: If the loss gradient changes during training (e.g., mean squared error), the ensemble of kernels cannot be reduced to a single kernel machine.

### Mechanism 2
- Claim: The EPK accounts for the discrete nature of practical training, unlike NTK which assumes infinitesimal steps.
- Mechanism: By interpolating weights linearly between discrete states and integrating gradients along this path, the EPK captures the actual training trajectory rather than approximating it with continuous flow.
- Core assumption: Linear interpolation between weight states adequately represents the training path.
- Evidence anchors:
  - [section 3.1] "Our primary theoretical contribution is an algorithm which accounts for this difference by observing the true path the model followed during training."
  - [section 3.2] Discussion of asymmetry between discrete training gradients and continuous test gradients.
  - [corpus] "Divergence of Empirical Neural Tangent Kernel in Classification Problems" suggests NTK fails for finite networks.
- Break condition: If training uses higher-order optimization schemes (beyond first-order), the kernel representation may not hold.

### Mechanism 3
- Claim: The kernel values reveal non-stationary learning properties and spatially transformed representations.
- Mechanism: By analyzing the learned kernel weights, we observe that networks emphasize boundary points and learn basis vectors that spatially transform data, differing from traditional stationary kernels.
- Core assumption: Kernel weights can be interpreted as sample importance and basis vectors for spatial transformation.
- Evidence anchors:
  - [section 4.2] "The most striking property of these kernel point values is the fact that they are not proportional to the euclidean distance from the test point."
  - [section 4.2] "some individual data points, mostly close to decision boundaries are slightly over-weighted."
  - [corpus] "A Unified Kernel for Neural Network Learning" suggests kernels can capture learned representations.
- Break condition: If the network architecture fundamentally changes (e.g., recurrent vs. feedforward), the spatial transformation properties may differ.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK)
  - Why needed here: Understanding NTK provides context for why the EPK is a significant improvement—NTK assumes infinite width and infinitesimal steps, while EPK works for finite networks with practical training.
  - Quick check question: What key assumption about network width does NTK rely on that EPK does not?

- Concept: Mercer's Theorem and Positive Semi-Definite Kernels
  - Why needed here: The EPK must satisfy symmetry and positive semi-definiteness to be a valid kernel; understanding this ensures the mathematical foundation is sound.
  - Quick check question: How does the proof in section A.1 demonstrate that the EPK satisfies Mercer's conditions?

- Concept: Gaussian Process Regression with Kernels
  - Why needed here: The paper uses GP regression to estimate uncertainty from the learned kernel, so understanding how kernels serve as covariance functions is essential.
  - Quick check question: In section A.6, how is the variance of GP predictions computed using the kernel matrix?

## Architecture Onboarding

- Component map: Trained neural network -> Gradient computation module (Jacobian of outputs w.r.t. weights) -> Integration module (approximating integral over training steps) -> Kernel assembly module (combining contributions from all steps)
- Critical path: 1) Train network and save weight states. 2) Compute per-step Jacobians for training and test points. 3) Integrate gradients along weight interpolation. 4) Assemble kernel machine representation.
- Design tradeoffs: Exactness vs. computational cost—integrating with many steps gives machine precision but is expensive; using fewer steps speeds computation but loses accuracy.
- Failure signatures: Large discrepancy between model predictions and kernel predictions (see Fig. 7), non-positive-definite kernel matrices, or failure to reduce ensemble to single kernel when loss gradient isn't constant.
- First 3 experiments:
  1. Verify exact representation on a small 2D dataset with a simple ReLU network and cross-entropy loss.
  2. Test kernel reduction to single machine on the same setup, confirming constant loss gradient.
  3. Extend to MNIST with a small CNN, checking prediction alignment and computational tractability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Exact Path Kernel be extended to higher-order optimization schemes like Newton's method or quasi-Newton methods?
- Basis in paper: [inferred] The paper states "Modification of this kernel for higher order optimization schemes remains an open problem" and notes that higher-order methods would require "drastic changes" to the kernel formulation.
- Why unresolved: Higher-order methods depend on second-order derivatives or approximations, which would fundamentally change how the kernel relates to the parameter space trajectories and gradient information.
- What evidence would resolve it: A formal proof showing whether higher-order optimization schemes can be represented as kernel machines under the EPK framework, or demonstrating that such a representation is mathematically impossible due to the nature of second-order information.

### Open Question 2
- Question: Under what specific conditions can the ensemble representation of the EPK be reduced to a single kernel machine for loss functions that are not constant across training steps?
- Basis in paper: [explicit] The paper notes "Showing the positive-definiteness of more general loss functions (e.g. mean squared error loss) will likely require additional regularity conditions on the training path, and is left as future work."
- Why unresolved: While constant gradient loss functions like categorical cross-entropy can be reduced to a single kernel, the mathematical conditions for other loss functions remain unclear, particularly how the changing gradients affect the positive semi-definiteness of the combined kernel.
- What evidence would resolve it: A rigorous mathematical proof identifying the exact regularity conditions (on loss function properties, training dynamics, or both) that guarantee the ensemble can be reduced to a single kernel machine, along with counterexamples showing when reduction fails.

### Open Question 3
- Question: What is the physical or theoretical meaning of the asymmetric kernel formulation that uses continuous test gradients but discrete training gradients?
- Basis in paper: [explicit] The paper discusses "asymmetry, where test gradients are being measured continuously but the training gradients are being measured discretely" and suggests "the not-symmetric analogue of the covariance in this case has physical meaning relative to uncertainty."
- Why unresolved: The paper acknowledges this asymmetry measures "the disagreement between the discrete steps taken during training with the gradient field" but doesn't provide a complete interpretation of what this asymmetry represents in terms of model behavior or uncertainty quantification.
- What evidence would resolve it: Empirical studies demonstrating how this asymmetric kernel formulation correlates with specific model behaviors (e.g., calibration, robustness, generalization), or a theoretical framework that formally defines what properties this asymmetry captures about the model's learning dynamics.

## Limitations
- Computational complexity: EPK requires computing Jacobians at every training step, making it impractical for large-scale networks
- Loss function restriction: Kernel reduction to single machine only works when loss gradient is constant (e.g., cross-entropy with one-hot labels)
- Linear interpolation assumption: May not capture complex training dynamics for non-convex loss landscapes or adaptive optimizers

## Confidence
- **High Confidence**: The exact representation claim for networks trained with gradient descent and cross-entropy loss - mathematically proven with rigorous derivation
- **Medium Confidence**: The kernel reduction to a single machine when loss gradient is constant - theory is sound but practical verification across diverse architectures is needed
- **Low Confidence**: The interpretation of kernel weights as sample importance and spatial transformation properties - empirically observed but requires more theoretical grounding

## Next Checks
1. Test EPK computation efficiency by benchmarking on progressively larger networks (from 2D toy problems to CIFAR-10) and identify the scaling bottleneck.
2. Verify kernel reduction conditions by training networks with different loss functions (MSE, focal loss) and measuring when the ensemble cannot be reduced to a single kernel.
3. Validate spatial transformation claims by visualizing the learned basis vectors across different network architectures and comparing their properties to traditional stationary kernels.