---
ver: rpa2
title: Should We Attend More or Less? Modulating Attention for Fairness
arxiv_id: '2305.13088'
source_url: https://arxiv.org/abs/2305.13088
tags:
- attention
- bias
- fairness
- entropy
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the relationship between attention entropy and
  model fairness in NLP models, finding that it is dataset and architecture dependent.
  It proposes a novel intra-processing method, entropy-based attention temperature
  scaling (EAT), which modulates attention entropy post-training to improve fairness.
---

# Should We Attend More or Less? Modulating Attention for Fairness

## Quick Facts
- **arXiv ID**: 2305.13088
- **Source URL**: https://arxiv.org/abs/2305.13088
- **Reference count**: 23
- **Key outcome**: Entropy-based attention temperature scaling (EAT) improves fairness in NLP models while maintaining performance within 3.5% of original model.

## Executive Summary
This paper explores the relationship between attention entropy and model fairness in NLP, finding that this relationship is dataset and architecture dependent. The authors propose EAT, an intra-processing method that modulates attention entropy post-training to improve fairness. EAT applies temperature scaling to attention weights, either minimizing or maximizing entropy to reduce bias. The method demonstrates significant fairness improvements (up to 50%) while maintaining model performance and generalizes well to different forms of social bias beyond gender.

## Method Summary
The paper introduces entropy-based attention temperature scaling (EAT), which modulates attention entropy post-training using a temperature scaling factor β. For text classification, EAT is applied to fine-tuned BERT, RoBERTa, and GPT-Neo models on Twitter, Wikipedia, and Jigsaw datasets. For text generation, it's tested on the BOLD dataset. The method scales attention weights before the softmax function to adjust entropy, with β values selected based on validation performance. EAT is evaluated against existing intra-processing baselines and measures both fairness (demographic parity, equality of odds, equality of opportunity) and performance (AUC for classification, perplexity for generation).

## Key Results
- EAT achieves up to 50% improvement in fairness metrics while maintaining performance within 3.5% of original model
- Outperforms existing intra-processing baselines on both text classification and generation tasks
- Generalizes effectively to social biases beyond gender, including race, religion, nationality, sexual orientation, age, and disability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modulating attention entropy via temperature scaling can reduce gender bias in NLP models.
- Mechanism: Applying temperature scaling to attention weights post-training adjusts the entropy of the attention distribution, thereby controlling how broadly or narrowly the model attends to input tokens. This modulation can either minimize or maximize attention entropy, depending on the scaling factor β, to improve fairness.
- Core assumption: The relationship between attention entropy and model bias is dataset and architecture dependent, allowing for strategic modulation to enhance fairness.
- Evidence anchors:
  - [abstract]: "Our results show an increase in fairness and minimal performance loss on different text classification and generation tasks using language models of varying sizes."
  - [section]: "Our novel attention entropy modulation, called entropy-based attention temperature scaling (EAT), applies a scaling factor to modulate the entropy of the attention map post-training."
- Break condition: If the attention entropy modulation does not align with the dataset and architecture characteristics, it may fail to improve fairness or even degrade performance.

### Mechanism 2
- Claim: EAT outperforms existing intra-processing baselines in improving fairness while maintaining performance.
- Mechanism: By modulating the attention entropy after training, EAT efficiently reduces bias without the need for extensive retraining or data modification, unlike pre-processing and in-processing methods.
- Core assumption: The computational efficiency of EAT allows it to be applied post-training, making it a less resource-intensive alternative to other bias mitigation methods.
- Evidence anchors:
  - [abstract]: "Since our method is only applied post-training and pre-inference, it is an intra-processing method and is, therefore, less computationally expensive than existing in-processing and pre-processing approaches."
  - [section]: "EAT demonstrates the highest or second highest fairness with a degradation of no more than 3.5% average AUC across the different models and datasets."
- Break condition: If the computational savings do not translate into practical advantages, or if the performance degradation exceeds acceptable limits, EAT may not be preferable over other methods.

### Mechanism 3
- Claim: EAT generalizes well to different forms of social bias beyond gender.
- Mechanism: By tuning the hyperparameter β based on validation performance, EAT can be adapted to mitigate various social biases, demonstrating robust generalization across different contexts.
- Core assumption: The flexibility of EAT in adjusting attention entropy allows it to address diverse bias forms without needing task-specific retraining.
- Evidence anchors:
  - [abstract]: "The method also generalizes well to different forms of social bias beyond gender."
  - [section]: "These findings attest to EAT's remarkable ability to generalize to other forms of social biases associated with race, religion, nationality, sexual orientation, age, and disability."
- Break condition: If EAT fails to adapt to biases outside the scope of its initial tuning, its generalization capability may be limited.

## Foundational Learning

- **Concept**: Attention Mechanism
  - Why needed here: Understanding how attention mechanisms work is crucial for grasping how EAT modulates attention entropy to improve fairness.
  - Quick check question: How does the attention mechanism in transformer models influence the model's focus on input tokens?

- **Concept**: Entropy in Probability Distributions
  - Why needed here: Knowledge of entropy is essential for comprehending how attention entropy modulation can affect model behavior and fairness.
  - Quick check question: What role does entropy play in determining the uniformity of a probability distribution?

- **Concept**: Temperature Scaling
  - Why needed here: Temperature scaling is the key technique used in EAT to adjust attention entropy, impacting model fairness and performance.
  - Quick check question: How does temperature scaling affect the softmax output in attention mechanisms?

## Architecture Onboarding

- **Component map**: Input text -> Transformer layers with attention mechanisms -> Temperature scaling (EAT) -> Softmax output -> Fairness evaluation
- **Critical path**:
  1. Train the base model on the dataset.
  2. Apply EAT by modulating attention entropy post-training.
  3. Validate the model's fairness and performance.
  4. Fine-tune β based on validation results.
- **Design tradeoffs**:
  - Computational efficiency vs. fairness improvement
  - Performance degradation vs. bias reduction
  - Generalization capability vs. task-specific tuning
- **Failure signatures**:
  - Excessive performance degradation
  - Inability to improve fairness despite entropy modulation
  - Overfitting to specific bias types during tuning
- **First 3 experiments**:
  1. Apply EAT with varying β values to assess its impact on attention entropy and fairness.
  2. Compare EAT's performance with other intra-processing methods on different datasets.
  3. Evaluate EAT's generalization to various social biases beyond gender.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal attention entropy modulation (maximization vs. minimization) vary across different NLP architectures beyond BERT and RoBERTa?
- Basis in paper: [explicit] The paper notes that the relationship between attention entropy and bias is dataset and architecture-dependent, but only tests BERT and RoBERTa models.
- Why unresolved: The paper only experiments with two transformer architectures, leaving open whether other architectures like GPT-3, T5, or smaller models like DistilBERT exhibit similar dependencies.
- What evidence would resolve it: Testing EAT across a diverse set of architectures (encoder-only, decoder-only, encoder-decoder) on multiple datasets to identify consistent patterns or architecture-specific optimal strategies.

### Open Question 2
- Question: Does EAT maintain its effectiveness when applied to multi-class classification tasks rather than binary classification?
- Basis in paper: [inferred] The paper demonstrates EAT's effectiveness on binary text classification and text generation, but does not explore multi-class scenarios where fairness considerations may be more complex.
- Why unresolved: Multi-class problems introduce additional fairness dimensions (e.g., intersectional biases) that weren't examined, and the binary DP metric used may not generalize appropriately.
- What evidence would resolve it: Applying EAT to multi-class datasets (e.g., toxicity detection with multiple toxicity levels, or hate speech detection with various hate categories) while measuring fairness with appropriate metrics like intersectional DP or group-specific AUC differences.

### Open Question 3
- Question: What is the impact of applying EAT to intermediate layers versus only the final layer of transformer models?
- Basis in paper: [explicit] The paper applies temperature scaling to all attention layers uniformly, but does not investigate layer-specific effects or whether different layers require different β values.
- Why unresolved: Different transformer layers capture different linguistic phenomena (lower layers for syntax, higher layers for semantics), which may interact differently with bias, yet the paper treats all layers identically.
- What evidence would resolve it: Conducting experiments with layer-wise β optimization, comparing results when applying EAT to specific layers versus all layers, and analyzing how layer-specific attention entropy relates to bias reduction.

## Limitations
- The effectiveness of EAT is highly dependent on dataset characteristics and model architecture, with some configurations showing no improvement
- The hyperparameter β requires careful tuning on validation sets, and optimal values may not generalize across different bias types or domains
- The paper primarily focuses on gender bias mitigation, with limited analysis of how EAT performs on intersectional biases or less common social categories

## Confidence
- **High confidence**: EAT can improve fairness metrics while maintaining performance within 3.5% AUC degradation
- **Medium confidence**: EAT generalizes well to biases beyond gender
- **Medium confidence**: EAT is computationally efficient compared to pre/post-processing methods

## Next Checks
1. **Ablation study on attention layers**: Systematically test EAT on individual attention layers rather than all layers simultaneously to determine which layers contribute most to bias reduction and whether layer-wise optimization is necessary.

2. **Cross-bias generalization test**: Apply EAT models trained for gender bias mitigation to datasets containing other social biases (race, religion, age) and measure performance degradation and fairness improvement to verify true generalization capability.

3. **Inference-time overhead measurement**: Benchmark the actual computational overhead of applying EAT during inference across different model sizes and batch configurations to validate the claimed efficiency advantage over retraining-based approaches.