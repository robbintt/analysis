---
ver: rpa2
title: Mitigating Label Biases for In-context Learning
arxiv_id: '2305.19148'
source_url: https://arxiv.org/abs/2305.19148
tags:
- bias
- label
- domain-label
- random
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the issue of label biases in in-context learning
  (ICL) for text classification. The authors define a typology of three types of label
  biases: vanilla-label bias, context-label bias, and domain-label bias.'
---

# Mitigating Label Biases for In-context Learning

## Quick Facts
- **arXiv ID**: 2305.19148
- **Source URL**: https://arxiv.org/abs/2305.19148
- **Reference count**: 19
- **Primary result**: Domain-context calibration significantly improves ICL performance on tasks with large domain-label bias (up to 37% Macro-F1 improvement)

## Executive Summary
This paper addresses the issue of label biases in in-context learning (ICL) for text classification. The authors identify three types of label biases—vanilla-label bias, context-label bias, and domain-label bias—and propose a novel domain-context calibration method. This method estimates label bias using random in-domain words from the task corpus and calibrates model predictions accordingly. The approach significantly improves ICL performance of GPT-J and GPT-3 across diverse tasks, particularly those with large domain-label bias. The method generalizes across different model scales and pretraining approaches, demonstrating the prevalence of label biases in ICL.

## Method Summary
The domain-context calibration (DC) method addresses label biases in ICL by estimating the model's bias using random in-domain words from the task corpus. Unlike prior methods that use predefined content-free tokens, DC samples random words from the actual dataset domain to capture domain-specific bias patterns. During inference, the model's output probabilities are calibrated based on the estimated bias. The method was evaluated on 24 text classification datasets using GPT-J (6B) and GPT-3 (175B) models, with performance measured using Macro-F1 score. DC was compared against baselines including random performance, uncalibrated performance, and contextual calibration.

## Key Results
- DC significantly improves ICL performance on tasks with large domain-label bias (up to 37% Macro-F1 improvement)
- DC outperforms contextual calibration on datasets with substantial domain-label bias where contextual calibration shows limited effectiveness
- Increasing the number of random words used for calibration improves performance by providing more accurate bias estimates
- Results generalize across different model scales (6B to 175B parameters) and pretraining methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Domain-context calibration improves ICL performance by estimating and correcting for domain-label bias
- **Mechanism:** DC estimates the model's label bias using random in-domain words from the task corpus, then calibrates the model's output probabilities using this estimated bias
- **Core assumption:** Random in-domain words capture the word distribution of the dataset domain without introducing semantic meaning that could bias the model towards specific labels
- **Evidence anchors:** [abstract] "Our novel domain-context calibration significantly improves the ICL performance of GPT-J and GPT-3 on a wide range of tasks"
- **Break condition:** If the task corpus contains words strongly associated with specific labels, random in-domain words may not be truly content-free

### Mechanism 2
- **Claim:** Increasing the number of random words used for calibration improves performance by providing a more accurate estimate of context-label bias
- **Mechanism:** Longer sequences of random words capture the effect of the context prompt more accurately, leading to better calibration
- **Core assumption:** Longer sequences provide a more representative sample of the task corpus
- **Evidence anchors:** [section] "When calibrating using random English words, increasing the number of words improves performance"
- **Break condition:** If the task corpus contains very long documents, using average input length may not be representative

### Mechanism 3
- **Claim:** DC outperforms contextual calibration by addressing domain-label bias that CC does not consider
- **Mechanism:** DC uses random in-domain words for calibration while CC uses predefined content-free tokens
- **Core assumption:** Domain-label bias is significant and CC's predefined token cannot adequately capture this bias
- **Evidence anchors:** [abstract] "Our analysis demonstrates that prior label bias calibration methods fall short of addressing all three types of biases"
- **Break condition:** If the task corpus does not contain domain-label bias, DC and CC should perform similarly

## Foundational Learning

- **Concept:** In-context learning (ICL) and its sensitivity to design choices
  - **Why needed here:** Understanding ICL is crucial for grasping the label bias problem and the need for calibration methods
  - **Quick check question:** What are the key design choices that affect ICL performance, and how do they relate to label bias?

- **Concept:** Types of label biases in ICL
  - **Why needed here:** Categorizing label biases is essential for understanding the problem and developing targeted mitigation strategies
  - **Quick check question:** What are the three types of label biases defined in this paper, and how do they differ?

- **Concept:** Calibration methods for mitigating label biases
  - **Why needed here:** Understanding existing calibration methods provides context for the proposed domain-context calibration and its improvements
  - **Quick check question:** How does contextual calibration (CC) differ from domain-context calibration (DC) in terms of the types of label biases they address?

## Architecture Onboarding

- **Component map:** Task corpus -> Random in-domain word sampling -> Bias estimation -> Model prediction calibration -> Performance evaluation
- **Critical path:** Estimate label bias using random in-domain words → Calibrate model output probabilities → Evaluate performance improvement
- **Design tradeoffs:** Using random in-domain words balances capturing domain-specific bias while avoiding semantic meaning that could introduce new biases
- **Failure signatures:** Poor performance on tasks with large domain-label bias, high variance in calibration estimates, sensitivity to choice of random in-domain words
- **First 3 experiments:**
  1. Evaluate DC on a dataset with known large domain-label bias (e.g., TweetEval-hate) and compare to CC and original model
  2. Test sensitivity of DC to number of random in-domain words by varying this parameter and measuring performance impact
  3. Analyze distribution of predicted labels before and after applying DC on dataset with large domain-label bias to verify calibration effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does domain-label bias vary across different language families and cultures when evaluating multilingual text classification tasks?
- **Basis in paper:** The paper focuses on English language tasks and mentions the need for future work to analyze and mitigate biases in multilingual tasks
- **Why unresolved:** The study only examines English datasets, and the authors explicitly state that analyzing domain-label bias and developing mitigation methods for multilingual tasks is left for future work
- **What evidence would resolve it:** Empirical studies evaluating domain-label bias and DC effectiveness across multiple languages, especially from different language families

### Open Question 2
- **Question:** How do domain-label biases manifest in open-ended generation tasks compared to classification tasks, and can DC be adapted for such tasks?
- **Basis in paper:** The authors mention that domain-label bias could exist differently for open-ended tasks like text generation
- **Why unresolved:** The study focuses on classification tasks, and the authors acknowledge that biases in open-ended generation tasks may differ and require further investigation
- **What evidence would resolve it:** Experiments evaluating domain-label bias in open-ended generation tasks and adapting DC or developing new methods to mitigate biases in such tasks

### Open Question 3
- **Question:** How do other types of domain-related biases, such as topic and gender biases, impact ICL performance, and can they be mitigated using methods similar to DC?
- **Basis in paper:** The authors mention that domain-label bias emphasizes word-level bias and that other types of biases associated with a domain may also impact model prediction
- **Why unresolved:** The study primarily focuses on word-level domain-label bias and does not extensively explore other domain-related biases
- **What evidence would resolve it:** Empirical studies evaluating the impact of topic and gender biases on ICL performance and developing methods to mitigate these biases, potentially extending the DC approach

## Limitations

- The calibration method's effectiveness relies on the assumption that random in-domain words are truly content-free
- The paper does not provide detailed implementation specifics for the DC method, particularly regarding how random words are sampled
- While results generalize across different model scales, the method's performance on out-of-domain tasks is not extensively explored
- The break conditions for each mechanism, while logical, would benefit from more rigorous testing

## Confidence

**High Confidence**: The core finding that domain-label bias is a significant factor limiting ICL performance is well-supported by experimental evidence across multiple datasets and model scales.

**Medium Confidence**: The claim that DC outperforms contextual calibration by specifically addressing domain-label bias is supported by results but relies on assumptions about CC's limitations.

**Low Confidence**: The assertion that specific content-free tokens may themselves be biased toward particular labels is based on limited evidence.

## Next Checks

1. **Distribution Analysis**: Analyze the distribution of predicted labels before and after applying DC on datasets with large domain-label bias to verify that the calibration is effectively mitigating the bias rather than simply shifting predictions randomly.

2. **Cross-Domain Testing**: Evaluate the DC method's performance on out-of-domain tasks to assess its generalizability beyond the 24 datasets used in the paper.

3. **Calibration Parameter Sensitivity**: Systematically vary the number of random words used for calibration and the length of random text sequences to determine optimal calibration parameters and identify any diminishing returns or overfitting risks.