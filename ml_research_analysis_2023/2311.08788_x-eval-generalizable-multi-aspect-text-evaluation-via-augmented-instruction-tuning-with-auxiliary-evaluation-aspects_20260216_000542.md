---
ver: rpa2
title: 'X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented Instruction
  Tuning with Auxiliary Evaluation Aspects'
arxiv_id: '2311.08788'
source_url: https://arxiv.org/abs/2311.08788
tags:
- evaluation
- aspects
- aspect
- auxiliary
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces X-Eval, a two-stage instruction tuning framework
  for multi-aspect evaluation of natural language generation (NLG) text. X-Eval first
  performs vanilla instruction tuning to improve the model's ability to follow evaluation
  instructions, then uses enhanced instruction tuning with auxiliary aspects to better
  assess text quality by exploiting connections between evaluation aspects.
---

# X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented Instruction Tuning with Auxiliary Evaluation Aspects

## Quick Facts
- arXiv ID: 2311.08788
- Source URL: https://arxiv.org/abs/2311.08788
- Reference count: 15
- Primary result: Lightweight language model fine-tuned with X-Eval achieves comparable or higher correlation with human judgments than state-of-the-art evaluators like GPT-4.

## Executive Summary
This paper introduces X-Eval, a two-stage instruction tuning framework for multi-aspect evaluation of natural language generation (NLG) text. X-Eval first performs vanilla instruction tuning to improve the model's ability to follow evaluation instructions, then uses enhanced instruction tuning with auxiliary aspects to better assess text quality by exploiting connections between evaluation aspects. The authors collect AspectInstruct, the first instruction tuning dataset tailored for multi-aspect NLG evaluation spanning 27 diverse aspects with 65 tasks across dialogue generation, summarization, and data-to-text. Experiments show that even a lightweight language model fine-tuned with X-Eval achieves comparable or higher correlation with human judgments compared to state-of-the-art evaluators like GPT-4.

## Method Summary
X-Eval is a two-stage instruction tuning framework that first performs vanilla instruction tuning to improve the model's ability to follow evaluation instructions, then uses enhanced instruction tuning with auxiliary aspects to better assess text quality by exploiting connections between evaluation aspects. The authors collect AspectInstruct, the first instruction tuning dataset tailored for multi-aspect NLG evaluation spanning 27 diverse aspects with 65 tasks across dialogue generation, summarization, and data-to-text. The framework augments tasks into multiple formats (scoring, comparison, ranking, Boolean QA) to improve generalization, and verbalizes auxiliary aspect evaluations into natural language descriptions that are concatenated to the input for contextual hints.

## Key Results
- X-Eval achieves comparable or higher correlation with human judgments than state-of-the-art evaluators like GPT-4
- Even a lightweight language model (FLAN-T5-large) fine-tuned with X-Eval performs competitively
- The two-stage instruction tuning with auxiliary aspects improves evaluation performance across diverse NLG tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage instruction tuning allows the model to first learn to follow evaluation instructions and then specialize in using auxiliary aspects for richer evaluation context.
- Mechanism: Stage 1 trains the model to follow standard evaluation instructions (scoring, comparison, ranking, Boolean QA). Stage 2 enriches the instructions with verbalized evaluations from auxiliary aspects, helping the model exploit inter-aspect connections.
- Core assumption: Evaluation aspects are interrelated, so auxiliary aspect information can improve evaluation of the target aspect.
- Evidence anchors:
  - [abstract] "enhanced instruction tuning stage that exploits the connections between fine-grained evaluation aspects to better assess text quality"
  - [section 4.1] "motivated by the observation that the evaluation aspects usually exhibit inter-connections...their evaluations can benefit each other"
  - [corpus] Weak evidence; corpus does not mention aspect inter-connections.
- Break condition: If evaluation aspects are completely independent or unrelated, auxiliary aspects would not help and could even introduce noise.

### Mechanism 2
- Claim: Task augmentation into multiple formats (scoring, comparison, ranking, Boolean QA) improves the model's generalization ability.
- Mechanism: By converting human rating annotations into diverse task formats, the model learns to handle different evaluation instructions, increasing its adaptability to unseen aspects.
- Core assumption: Diversity in training tasks improves zero-shot generalization to unseen aspects.
- Evidence anchors:
  - [section 3.2] "we further augment the dataset by converting the original human rating task into diverse forms of NLG evaluation tasks, including scoring, comparison, ranking and Boolean question answering"
  - [abstract] "To enhance task diversity, we devise an augmentation strategy"
  - [corpus] Weak evidence; corpus does not mention task diversity.
- Break condition: If task diversity does not correlate with generalization ability, this mechanism would not improve performance.

### Mechanism 3
- Claim: Verbalizing auxiliary aspect evaluations into natural language allows the model to process additional context effectively.
- Mechanism: Numerical evaluation scores are converted into natural language descriptions using templates (e.g., "This sentence is consistent with the source.") and concatenated to the input, providing contextual hints.
- Core assumption: Models can effectively use natural language descriptions as contextual hints for evaluation.
- Evidence anchors:
  - [section 4.1] "convert their human-annotated evaluations into natural languages based on templates"
  - [section 4.2] "convert the prediction into natural language results...integrated into the additional set of texts"
  - [corpus] Weak evidence; corpus does not mention natural language verbalization.
- Break condition: If the model cannot effectively process the additional natural language context, it would not benefit from auxiliary aspects.

## Foundational Learning

- Concept: Instruction tuning
  - Why needed here: To teach the model to follow human-written evaluation instructions, which is essential for the evaluator's functionality.
  - Quick check question: What is the difference between standard fine-tuning and instruction tuning?

- Concept: Multi-aspect evaluation
  - Why needed here: The framework evaluates text across multiple aspects, requiring the model to understand and assess different quality dimensions.
  - Quick check question: How does multi-aspect evaluation differ from single-score evaluation metrics like BLEU or ROUGE?

- Concept: Zero-shot generalization
  - Why needed here: The evaluator must be able to assess unseen aspects and tasks without additional training.
  - Quick check question: What makes zero-shot generalization challenging for text evaluation tasks?

## Architecture Onboarding

- Component map:
  Base model (FLAN-T5-large) -> Stage 1: Vanilla instruction tuning -> Stage 2: Enhanced instruction tuning with auxiliary aspects -> Inference (select auxiliary aspects -> evaluate target aspect with auxiliary results)

- Critical path: Base model → Stage 1 training → Stage 2 training → Inference (select auxiliary aspects → evaluate target aspect with auxiliary results)

- Design tradeoffs:
  - Using FLAN-T5-large (780M parameters) vs. larger models: Smaller model size but potentially lower performance
  - Selecting top-k auxiliary aspects vs. all aspects: Tradeoff between inference cost and information richness
  - Verbalizing numerical scores vs. using raw scores: Natural language context vs. potential loss of precision

- Failure signatures:
  - Poor correlation with human judgments: Indicates issues with training or auxiliary aspect integration
  - Slow inference: May be due to selecting too many auxiliary aspects or inefficient similarity computation
  - Inconsistent evaluations across aspects: Could indicate issues with auxiliary aspect selection or verbalization

- First 3 experiments:
  1. Compare vanilla instruction tuning (Stage 1 only) vs. full two-stage training on a subset of aspects to validate auxiliary aspect effectiveness
  2. Test different values of k (top-k auxiliary aspects) to find optimal tradeoff between performance and efficiency
  3. Evaluate different similarity metrics (e.g., cosine similarity vs. learned similarity) for auxiliary aspect selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different aspect selection strategies (e.g., top-1 vs. top-3 vs. top-5) impact the performance of X-Eval across various NLG tasks and evaluation aspects?
- Basis in paper: [explicit] The paper discusses the impact of selecting different numbers of auxiliary aspects (k=1, 3, 5) during inference and shows that using top-1 aspect generally achieves better correlation.
- Why unresolved: While the paper provides some analysis on aspect selection, it would be beneficial to explore the impact of different strategies in more detail, especially for various NLG tasks and evaluation aspects.
- What evidence would resolve it: Conducting experiments with different aspect selection strategies for various NLG tasks and evaluation aspects, and comparing the results to determine the optimal strategy for each scenario.

### Open Question 2
- Question: How does the performance of X-Eval compare to other state-of-the-art evaluators when evaluating NLG tasks in languages other than English?
- Basis in paper: [inferred] The paper mentions that the current dataset focuses on evaluation tasks in English and suggests exploring evaluation tasks in more diverse language settings as future work.
- Why unresolved: The paper does not provide any comparison of X-Eval's performance with other evaluators for NLG tasks in languages other than English.
- What evidence would resolve it: Conducting experiments to compare X-Eval's performance with other state-of-the-art evaluators for NLG tasks in multiple languages and analyzing the results to determine the effectiveness of X-Eval in multilingual settings.

### Open Question 3
- Question: How does the performance of X-Eval vary with different sizes of the instruction tuning dataset and different types of NLG tasks?
- Basis in paper: [inferred] The paper discusses the importance of task diversity in enhancing zero-shot generalization and mentions that the current dataset focuses on a limited subset of NLG tasks. It also suggests exploring more NLG tasks in the future.
- Why unresolved: The paper does not provide any analysis on how the performance of X-Eval varies with different sizes of the instruction tuning dataset and different types of NLG tasks.
- What evidence would resolve it: Conducting experiments with varying sizes of the instruction tuning dataset and different types of NLG tasks, and analyzing the results to determine the impact of dataset size and task diversity on X-Eval's performance.

## Limitations

- Limited scope of validation: The evaluation focuses primarily on correlation with human judgments without extensive testing of robustness to edge cases or adversarial examples.
- Template dependency: The auxiliary aspect verbalization relies heavily on predefined templates, but the paper does not provide detailed information about these templates or evaluate their sensitivity.
- Dataset coverage concerns: Although AspectInstruct contains 27 aspects across 65 tasks, the paper does not provide comprehensive statistics about aspect distribution across domains or discuss potential biases.

## Confidence

**High confidence**: The two-stage instruction tuning framework design and its basic implementation are clearly described and reproducible. The concept of using auxiliary aspects for evaluation is theoretically sound and aligns with established principles in multi-task learning.

**Medium confidence**: The claim that X-Eval achieves "comparable or higher correlation with human judgments compared to state-of-the-art evaluators like GPT-4" is supported by experimental results, but the evaluation methodology and dataset details could be more transparent.

**Low confidence**: The assertion that the model can effectively "exploit the connections between evaluation aspects" lacks direct evidence. While the mechanism is theoretically plausible, the paper does not provide quantitative analysis of how auxiliary aspects actually improve evaluation quality beyond correlation metrics.

## Next Checks

1. **Ablation study on auxiliary aspect selection**: Systematically test different values of k (top-k auxiliary aspects) and different similarity metrics to determine the optimal configuration. This would validate whether the auxiliary aspect mechanism provides consistent benefits across different settings.

2. **Cross-domain generalization test**: Evaluate X-Eval on evaluation aspects from domains not represented in AspectInstruct (e.g., creative writing, technical documentation) to assess true zero-shot generalization capabilities beyond the reported correlation metrics.

3. **Error analysis on edge cases**: Conduct a detailed analysis of evaluation failures, including cases where evaluation aspects conflict, when input text quality is extremely poor, or when evaluation instructions are ambiguous. This would help understand the practical limitations of the approach.