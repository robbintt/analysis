---
ver: rpa2
title: 'CoEdIT: Text Editing by Task-Specific Instruction Tuning'
arxiv_id: '2305.09857'
source_url: https://arxiv.org/abs/2305.09857
tags:
- text
- sentence
- tasks
- make
- editing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COEDIT, a text editing system that uses instruction-tuned
  language models to perform various text editing tasks. The authors fine-tune a pre-trained
  T5 model on a diverse collection of task-specific instructions for text editing,
  totaling 82K instructions.
---

# CoEdIT: Text Editing by Task-Specific Instruction Tuning

## Quick Facts
- arXiv ID: 2305.09857
- Source URL: https://arxiv.org/abs/2305.09857
- Reference count: 25
- Achieves state-of-the-art performance on text editing benchmarks using a model nearly 60x smaller than comparable LLMs

## Executive Summary
This paper introduces COEDIT, a text editing system that uses instruction-tuned language models to perform various text editing tasks. The authors fine-tune a pre-trained T5 model on a diverse collection of task-specific instructions for text editing, totaling 82K instructions. The resulting model achieves state-of-the-art performance on various text editing benchmarks, outperforming other models while being nearly 60x smaller. The model is also capable of generalizing to unseen edit instructions and composite instructions containing different combinations of edit actions.

## Method Summary
The paper fine-tunes a pre-trained FLAN-T5 model (L, XL, XXL variants) on 82K instruction-source-target pairs collected from various text editing tasks including grammatical error correction, text simplification, coherence, and style transfer. The model is trained using cross-entropy loss with Adam optimizer (learning rate 1e−4) for 5 epochs with early stopping. The best checkpoints are selected based on validation loss. Evaluation is performed on standard benchmarks (JFLEG, ASSET, DiscoFuse, GYAFC, WNC, MRPC) using metrics like SARI, GLEU, ExactMatch, Self-BLEU, and semantic similarity.

## Key Results
- COEDIT models achieve state-of-the-art performance on various text editing benchmarks
- The smallest COEDIT model (770M parameters) outperforms instruction-tuned models with ~60x more parameters
- Model generalizes to unseen and composite instructions containing different combinations of edit actions
- Writers prefer COEDIT edits over other state-of-the-art text editing models in human evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction tuning on diverse task-specific instructions improves model generalization across unseen and composite editing tasks
- Mechanism: Fine-tuning on 82K instruction-source-target pairs creates dense task representations that enable inference from novel instructions by leveraging learned semantic patterns
- Core assumption: Text editing tasks share underlying semantic structures that can be captured through instruction variations
- Evidence anchors: COEDIT outperforms other models on benchmarks; achieves SOTA while being 60x smaller; generalizes to unseen instructions
- Break condition: If instruction variations don't capture task semantics effectively, the model won't generalize

### Mechanism 2
- Claim: Task-specific instruction tuning outperforms prefix-tuning and non-instruction-tuned approaches
- Mechanism: Instruction tuning teaches the model to map natural language instructions directly to edit operations
- Core assumption: Learning to interpret instructions is more effective than prepending task tags for text editing
- Evidence anchors: COEDIT consistently outperforms prefix-tuned T5 models in Table 3(a)
- Break condition: If simpler prefix approaches achieve similar performance, instruction tuning complexity may not be justified

### Mechanism 3
- Claim: Quality of instructions significantly impacts model performance
- Mechanism: Proper instruction tuning with task-specific verbalizers teaches correct semantic mappings between instructions and edits
- Core assumption: The model learns meaningful instruction-edit relationships that break down with incorrect mappings
- Evidence anchors: COEDIT- XL-R with randomized instructions performs worse on style accuracy metrics
- Break condition: If instruction variations don't capture meaningful semantic differences, model won't learn effective mappings

## Foundational Learning

- Concept: Instruction tuning methodology and its differences from other fine-tuning approaches
  - Why needed here: Understanding why instruction tuning specifically improves text editing generalization requires knowledge of how it differs from standard fine-tuning, prefix-tuning, and prompt-tuning
  - Quick check question: What is the key difference between instruction tuning and prefix-tuning in terms of how they handle task specification?

- Concept: Text editing task taxonomy and edit intention categorization
  - Why needed here: The paper builds on an existing taxonomy of edit intentions (Fluency, Coherence, Clarity, Style) and extends it
  - Quick check question: What are the four main categories of edit intentions in the ITERATER taxonomy, and how does the paper extend the Style category?

- Concept: Evaluation metrics for text editing (SARI, GLEU, Self-BLEU, semantic similarity)
  - Why needed here: The paper uses multiple evaluation metrics across different tasks
  - Quick check question: What does the SARI metric measure in text editing tasks, and why is it particularly suited for evaluating edits?

## Architecture Onboarding

- Component map: Instruction + source text -> FLAN-T5 architecture -> Cross-entropy loss training -> Multiple benchmark evaluation -> Edited text output
- Critical path: Data preparation → Instruction verbalizer creation → Model fine-tuning → Evaluation on benchmarks → Human evaluation for quality assessment
- Design tradeoffs:
  - Model size vs. performance: Smaller models (770M parameters) achieve SOTA vs. much larger models (175B parameters)
  - Task coverage vs. instruction quality: Balancing between diverse task coverage and maintaining high-quality, consistent instructions
  - Generalization vs. specialization: Training on composite instructions improves generalization but may slightly impact single-task performance
- Failure signatures:
  - Model repeats input instead of editing (decoder-only models)
  - Model produces incoherent edits when given incorrect instructions
  - Model struggles with longer sequences despite success on sentence-level tasks
  - Performance degradation on out-of-domain tasks despite good in-domain performance
- First 3 experiments:
  1. Replicate the instruction vs. prefix-tuning comparison from Table 3(a) to verify the core instruction tuning advantage
  2. Test the randomized instruction baseline (COEDIT- XL-R) to understand the impact of instruction quality
  3. Evaluate the model on a new composite instruction not seen during training to test generalization capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal size of instruction-tuned language models for text editing tasks?
- Basis in paper: [inferred] The paper compares COEDIT models of different sizes (770M, 3B, and 11B parameters) and finds that even the smallest model outperforms other supervised text editing models and instruction-tuned models with nearly 60x greater parameters
- Why unresolved: The paper only provides a comparison of COEDIT models with different sizes, but does not determine the optimal size for text editing tasks
- What evidence would resolve it: Further experiments comparing COEDIT models of different sizes on various text editing benchmarks would help determine the optimal size

### Open Question 2
- Question: How does the performance of COEDIT models on out-of-domain tasks compare to task-specific models?
- Basis in paper: [explicit] The paper evaluates COEDIT models on out-of-domain tasks such as sentence compression and politeness transfer, and finds that they outperform other models
- Why unresolved: The paper only provides a comparison of COEDIT models to other models on out-of-domain tasks, but does not compare their performance to task-specific models
- What evidence would resolve it: Further experiments comparing the performance of COEDIT models to task-specific models on out-of-domain tasks would help determine their relative performance

### Open Question 3
- Question: How does the quality of instructions affect the performance of COEDIT models?
- Basis in paper: [explicit] The paper evaluates the impact of instruction quality on the performance of COEDIT models by training a model with randomized task-specific instructions and comparing it to the original model
- Why unresolved: The paper only provides a comparison of COEDIT models with randomized instructions to the original model, but does not determine the optimal quality of instructions for text editing tasks
- What evidence would resolve it: Further experiments comparing the performance of COEDIT models with different qualities of instructions on various text editing benchmarks would help determine the optimal quality of instructions

## Limitations

- The effectiveness of instruction tuning depends heavily on the quality and diversity of the 82K instruction-source-target pairs, but the paper doesn't provide detailed analysis of how these instructions capture task semantics
- The comparison with larger LLMs (175B parameters) is problematic since instruction sets and training objectives may differ substantially between models of different scales
- Human evaluation showing writer preference for COEDIT edits lacks detail on sample size, evaluator demographics, and specific preference metrics

## Confidence

- **Instruction Tuning Generalization**: Medium - Shows improved performance on seen tasks and some generalization to unseen instructions, but mechanism remains largely theoretical
- **Size-Performance Tradeoff**: Medium - ~60x size reduction claim based on specific LLM comparisons but doesn't account for differences in instruction quality, training data, or optimization
- **Composite Instruction Capabilities**: Low-Medium - Claims ability to handle composite instructions but provides limited quantitative evidence for complex instruction combinations

## Next Checks

1. **Instruction Semantics Analysis**: Conduct ablation studies where instructions are systematically varied (e.g., "simplify this sentence" vs. "make this sentence easier to read") to quantify how instruction wording affects model performance and test whether the model truly learns semantic mappings rather than surface-level patterns

2. **Cross-Domain Generalization Test**: Evaluate COEDIT on text editing tasks from entirely different domains (legal, technical, medical writing) not represented in the training corpus to assess real-world generalization beyond the curated benchmarks

3. **Instruction Quality Impact Study**: Replicate the randomized instruction experiment (COEDIT- XL-R) with more granular variations - partially randomized vs. fully randomized instructions - to determine the minimum instruction quality threshold needed for effective performance and identify which aspects of instruction design matter most