---
ver: rpa2
title: Data-efficient Deep Reinforcement Learning for Vehicle Trajectory Control
arxiv_id: '2311.18393'
source_url: https://arxiv.org/abs/2311.18393
tags:
- control
- learning
- vehicle
- trajectory
- approaches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates data-efficient deep reinforcement learning
  for vehicle trajectory control. The authors propose three data-efficient RL approaches:
  randomized ensemble double Q-learning (REDQ), probabilistic ensembles with trajectory
  sampling and model predictive path integral optimizer (PETS-MPPI), and model-based
  policy optimization (MBPO).'
---

# Data-efficient Deep Reinforcement Learning for Vehicle Trajectory Control

## Quick Facts
- arXiv ID: 2311.18393
- Source URL: https://arxiv.org/abs/2311.18393
- Reference count: 40
- One-line primary result: Data-efficient RL methods (PETS-MPPI, MBPO, REDQ) achieve performance on par with or better than SAC while requiring 10-100x fewer environment interactions for vehicle trajectory control

## Executive Summary
This paper investigates data-efficient deep reinforcement learning methods for vehicle trajectory control using the CARLA simulator. The authors propose three approaches: randomized ensemble double Q-learning (REDQ), probabilistic ensembles with trajectory sampling and model predictive path integral optimizer (PETS-MPPI), and model-based policy optimization (MBPO). A key contribution is a new formulation that splits dynamics prediction and vehicle localization, making model-based RL more suitable for trajectory control. The benchmarking reveals that PETS-MPPI learns two orders of magnitude faster than SAC, while MBPO and REDQ learn one order of magnitude faster and achieve superior asymptotic performance.

## Method Summary
The study evaluates three data-efficient RL approaches for vehicle trajectory control: REDQ (ensemble Q-learning), PETS-MPPI (model-based planning), and MBPO (model-based policy optimization). A split prediction scheme is proposed for model-based methods, separating dynamics prediction from trajectory matching operations. The CARLA simulator with an Audi e-tron vehicle model on a ZF test track is used for evaluation. SAC serves as the baseline model-free method. The methods are compared in terms of sample efficiency and asymptotic performance, with particular focus on their ability to learn from limited environment interactions.

## Key Results
- PETS-MPPI learns two orders of magnitude faster than SAC
- MBPO and REDQ learn one order of magnitude faster than SAC
- MBPO and REDQ achieve superior asymptotic performance compared to SAC
- The split prediction scheme improves model-based RL performance for trajectory control

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Splitting dynamics prediction and vehicle localization improves model-based RL performance in trajectory control.
- Mechanism: The approach decouples the learning task into two parts: a data-driven vehicle dynamics model that predicts state rates of change, and a trajectory matching block that computes deviations using prior geometric knowledge. This reduces the complexity of the model learning task.
- Core assumption: Vehicle localization operations (e.g., finding the footpoint, computing cross-track error) are geometrically deterministic and can be computed exactly without learning.
- Evidence anchors:
  - [abstract]: "We, therefore, propose a new formulation that splits dynamics prediction and vehicle localization."
  - [section]: "Instead of learning a full control state prediction model, we solely learn the vehicle dynamics from data and compute deviations from prior knowledge of the trajectory."
  - [corpus]: Weak. No direct corpus support for the claim that localization operations are deterministic and beneficial to decouple.
- Break condition: If the trajectory matching operations become too complex to compute exactly, or if the dynamics model cannot be learned accurately from limited data.

### Mechanism 2
- Claim: Ensemble Q-learning methods (REDQ) improve data efficiency by reducing estimation bias through multiple critics.
- Mechanism: REDQ uses an ensemble of Q-value estimators to stabilize the optimization process. By maintaining multiple critics and using a randomized selection for updates, the method reduces overestimation bias and allows for a higher update-to-data ratio.
- Core assumption: Reducing Q-value estimation bias leads to more stable learning and better sample efficiency.
- Evidence anchors:
  - [abstract]: "Further, we address recent model-free methods, incorporating ensemble models that can stabilize the estimation of optimization quantities."
  - [section]: "A particularly successful approach is REDQ [22], which significantly outperforms prior work in terms of data efficiency."
  - [corpus]: Weak. No direct corpus support for the specific claim about REDQ's ensemble mechanism.
- Break condition: If the ensemble becomes too large, increasing computational cost without proportional performance gains.

### Mechanism 3
- Claim: Dyna-style model-based RL (MBPO) improves data efficiency by using a learned dynamics model to generate synthetic transitions.
- Mechanism: MBPO trains a probabilistic ensemble dynamics model and uses it to generate synthetic transitions by branching off from real trajectories. These synthetic transitions are used to train a model-free SAC learner, increasing the effective update-to-data ratio.
- Core assumption: The learned dynamics model is accurate enough to generate useful synthetic transitions that improve learning.
- Evidence anchors:
  - [abstract]: "MBPO combines a SAC learner with a PE model. Synthetic rollouts are generated by branching off from trajectories observed during environment interaction."
  - [section]: "Fourth, dyna-style algorithms [25], [52]â€“[54] follow the scheme of Dyna-Q [55], where synthetic experience generated by a transition model is used to train a model-free learner."
  - [corpus]: Weak. No direct corpus support for the specific claim about MBPO's synthetic transition generation.
- Break condition: If the dynamics model's errors compound over synthetic trajectories, leading to poor-quality synthetic data.

## Foundational Learning

- Concept: Reinforcement Learning Fundamentals (MDP, policy, reward)
  - Why needed here: The paper applies RL to vehicle trajectory control, requiring understanding of MDP formulation, policy optimization, and reward shaping.
  - Quick check question: What are the components of a Markov Decision Process (MDP) tuple?

- Concept: Deep Reinforcement Learning (Actor-Critic, Q-learning, function approximation)
  - Why needed here: The paper uses deep RL methods (SAC, REDQ, MBPO) that rely on neural networks to approximate value functions and policies.
  - Quick check question: How does the Soft Actor-Critic (SAC) algorithm differ from standard actor-critic methods?

- Concept: Model-based Reinforcement Learning (dynamics model, planning, synthetic experience)
  - Why needed here: The paper investigates model-based RL approaches (PETS-MPPI, MBPO) that use learned dynamics models to improve data efficiency.
  - Quick check question: What is the main advantage of using a learned dynamics model in RL?

## Architecture Onboarding

- Component map: CARLA simulator -> RL Agent (SAC/REDQ/MBPO/PETS-MPPI) -> Dynamics Model (PE model) -> Trajectory Matching (geometric operations)
- Critical path:
  1. Collect real environment interactions (state, action, reward, next state)
  2. For model-based methods, train the dynamics model on collected data
  3. Generate synthetic transitions using the dynamics model (MBPO) or plan using the model (PETS-MPPI)
  4. Train the RL agent using both real and synthetic data (MBPO) or online planning (PETS-MPPI)
- Design tradeoffs:
  - Model-based vs. Model-free: Model-based methods can be more data-efficient but are sensitive to model errors
  - Ensemble size: Larger ensembles can reduce bias but increase computational cost
  - Planning horizon: Longer horizons in PETS-MPPI can improve performance but increase computational cost
- Failure signatures:
  - Model-based methods: Poor performance due to inaccurate dynamics model, especially in regions with limited data
  - Ensemble methods: High variance in performance due to sensitivity to random seeds
  - SAC: Requires large amounts of data and may get stuck in local optima
- First 3 experiments:
  1. Train SAC on the CARLA environment with the proposed reward function and action regularization
  2. Train MBPO with the split prediction scheme (dynamics model + trajectory matching)
  3. Train REDQ with the same dynamics model and reward function as MBPO for comparison

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the split prediction scheme (dynamics prediction + trajectory matching) compare to full state prediction models in terms of sample efficiency and final performance across different types of vehicle control tasks beyond trajectory following?
- Basis in paper: [explicit] The paper proposes a split prediction scheme for trajectory control and shows it outperforms full state prediction, but only evaluates it on trajectory control in CARLA.
- Why unresolved: The approach is specific to trajectory control where the trajectory is known a priori. Its applicability to other control tasks (e.g., obstacle avoidance, racing without fixed trajectory) remains untested.
- What evidence would resolve it: Comparative experiments applying both full state and split prediction schemes to different vehicle control tasks (e.g., obstacle avoidance, racing) with varying amounts of training data and final performance metrics.

### Open Question 2
- Question: What is the theoretical explanation for why REDQ and MBPO achieve superior asymptotic performance compared to SAC in vehicle trajectory control, contrary to their performance in the MuJoCo benchmark?
- Basis in paper: [explicit] The paper observes that REDQ and MBPO outperform SAC asymptotically in trajectory control, which was unexpected based on [22] and [25].
- Why unresolved: The paper notes the discrepancy but does not provide a theoretical analysis of why the control task characteristics lead to this different outcome.
- What evidence would resolve it: A theoretical analysis comparing the bias-variance tradeoffs and exploration-exploitation dynamics of REDQ/MBPO versus SAC specifically for control tasks with known reference trajectories and geometric operations.

### Open Question 3
- Question: How does the performance of data-efficient RL methods (PETS-MPPI, MBPO, REDQ) scale when transferring from simulation (CARLA) to real-world vehicle control with sensor noise and model mismatch?
- Basis in paper: [inferred] The paper demonstrates significant data efficiency gains in simulation but acknowledges this is "an important step to the application of RL in real engineering, where data collection is laborious and potentially dangerous."
- Why unresolved: The paper only evaluates in simulation. Real-world transfer would involve additional challenges like sensor noise, actuator delays, and model mismatch not present in CARLA.
- What evidence would resolve it: Experimental validation of the same RL methods on a real vehicle platform, measuring performance degradation compared to simulation and required adaptation strategies.

## Limitations

- The CARLA simulator evaluation may not fully capture real-world dynamics uncertainty and sensor noise
- The split prediction formulation's effectiveness relies heavily on the assumption that trajectory matching operations are geometrically tractable
- Ensemble methods (REDQ) show promising asymptotic performance but may suffer from high variance across random seeds

## Confidence

- Data efficiency claims (MBPO and PETS-MPPI learning 10-100x faster than SAC): Medium confidence based on controlled simulation results
- Architectural contribution of separating dynamics prediction from localization: Medium-High confidence, though requires further validation in real-world conditions
- Ensemble methods (REDQ) achieving superior asymptotic performance: Medium confidence due to potential high variance across random seeds

## Next Checks

1. Evaluate the split prediction approach with increasingly complex trajectories where geometric localization becomes less reliable
2. Test algorithm robustness across multiple random seeds and initializations to quantify variance in performance
3. Conduct ablation studies removing the trajectory matching component to measure its exact contribution to overall performance