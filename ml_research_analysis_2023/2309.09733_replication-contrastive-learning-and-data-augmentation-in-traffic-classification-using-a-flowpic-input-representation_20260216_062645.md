---
ver: rpa2
title: 'Replication: Contrastive Learning and Data Augmentation in Traffic Classification
  Using a Flowpic Input Representation'
arxiv_id: '2309.09733'
source_url: https://arxiv.org/abs/2309.09733
tags:
- data
- samples
- learning
- datasets
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We reproduced and replicated [17]\u2019s investigation of DL techniques\
  \ (few-shot learning, self-supervision via contrastive learning and data augmentation)\
  \ for traffic classification. Our results confirm the interest of such techniques,\
  \ yet with key quantitative discrepancies across datasets."
---

# Replication: Contrastive Learning and Data Augmentation in Traffic Classification Using a Flowpic Input Representation

## Quick Facts
- arXiv ID: 2309.09733
- Source URL: https://arxiv.org/abs/2309.09733
- Reference count: 40
- Primary result: Reproduced and replicated DL techniques for traffic classification with key quantitative discrepancies across datasets

## Executive Summary
This study successfully reproduced and replicated [17]'s investigation of deep learning techniques including few-shot learning, self-supervision via contrastive learning, and data augmentation for traffic classification. The reproduction confirms the effectiveness of these methods, achieving high accuracy with as few as 100 labeled samples using flowpic representations. However, significant quantitative discrepancies were found across different datasets, including a 20% accuracy drop on one partition due to an undetected data shift. The study also validates that the data augmentation strategies from the original work perform well on other datasets, and all artifacts are made publicly available for reproducibility.

## Method Summary
The reproduction implemented flowpic creation from packet time series using 32×32 binning resolution and integrated seven data augmentation methods. Supervised models and SimCLR-based models were trained on multiple datasets (UCDAVIS19, MIRAGE-19, MIRAGE-22, UTMOBILENET21) with 100 samples per class. The LeNet5 CNN architecture was used with variations in dropout and projection layer size. Performance was evaluated on script and human partitions with confidence intervals calculated for statistical validity. The study followed original training settings including learning rate 0.001, batch size 32, and early stopping.

## Key Results
- Confirmed high accuracy (>90%) with 100 labeled samples using flowpic and contrastive learning
- Discovered 20% accuracy drop due to data shift between partitions not detected in original study
- Validated data augmentation strategies (Change RTT + Time Shift) perform well across multiple datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Data augmentation functions in contrastive learning act as implicit regularizers, reducing overfitting to small labeled datasets.
- **Mechanism**: By transforming each input sample into multiple "views" (e.g., via Change RTT, Time Shift), the model is forced to learn representations invariant to those transformations, improving generalization.
- **Core assumption**: The augmentation functions preserve semantic meaning of the original data (e.g., RTT scaling does not change class identity).
- **Evidence anchors**:
  - [abstract]: "The main result of [17] on the UCDAVIS19, ISCX-VPN and ISCX-Tor datasets is that, with such DL methodologies, 100 input samples are enough to achieve very high accuracy using an input representation called 'flowpic'"
  - [section]: "Contrastive learning is a special type of self-supervision which aims to explicitly enforce geometrical properties in the latent space by means of data augmentations."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.381, average citations=0.0. Top related titles: Guidelines for Augmentation Selection in Contrastive Learning for Time Series Classification, One Train for Two Tasks: An Encrypted Traffic Classification Framework Using Supervised Contrastive Learning, Front-end Replication Dynamic Window (FRDW) for Online Motor Imagery Classification.
- **Break condition**: If augmentations alter class semantics (e.g., packet loss too aggressively), the contrastive objective becomes meaningless.

### Mechanism 2
- **Claim**: Flowpic representation encodes multi-second flow dynamics into a spatial format, enabling CNN-based models to detect discriminative temporal patterns.
- **Mechanism**: Packet counts are binned by size and time to form a 2D histogram; CNNs extract spatial features corresponding to bursts and inter-burst intervals.
- **Core assumption**: Temporal patterns in packet size distributions are class-specific and preserved under the binning resolution chosen (32×32).
- **Evidence anchors**:
  - [section]: "Specifically, both the 15s and the packets size range (0-1500) are split into bins based on the resolution of the target flowpic... the count of the packets occurring in each time window are tallied based on the defined packet size bins."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.381, average citations=0.0. Top related titles: Guidelines for Augmentation Selection in Contrastive Learning for Time Series Classification, One Train for Two Tasks: An Encrypted Traffic Classification Framework Using Supervised Contrastive Learning, Front-end Replication Dynamic Window (FRDW) for Online Motor Imagery Classification.
- **Break condition**: If temporal resolution is too coarse, discriminative short bursts are lost; if too fine, histograms become sparse and CNNs fail to learn.

### Mechanism 3
- **Claim**: Contrastive pre-training on unlabeled data creates a better latent space than random initialization, reducing the number of labeled samples needed for fine-tuning.
- **Mechanism**: SimCLR maximizes agreement between augmentations of the same sample while pushing apart different samples in the latent space; this learned geometry accelerates downstream classification.
- **Core assumption**: The unlabeled dataset is sufficiently diverse to cover the space of relevant flow patterns.
- **Evidence anchors**:
  - [section]: "First, a representation of the dataset is learned by pre-training a model via SimCLR, contrasting pairs of augmented 'views' of a sample."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.381, average citations=0.0. Top related titles: Guidelines for Augmentation Selection in Contrastive Learning for Time Series Classification, One Train for Two Tasks: An Encrypted Traffic Classification Framework Using Supervised Contrastive Learning, Front-end Replication Dynamic Window (FRDW) for Online Motor Imagery Classification.
- **Break condition**: If the unlabeled set is too small or unrepresentative, pre-training yields a poor latent space, hurting fine-tuning.

## Foundational Learning

- **Concept**: Self-supervised contrastive learning (SimCLR)
  - **Why needed here**: Enables training without labels, crucial when labeled samples are scarce.
  - **Quick check question**: What is the role of the temperature parameter in InfoNCE loss?

- **Concept**: Data augmentation design for network traffic
  - **Why needed here**: Augmentation must preserve flow semantics while providing diversity; inappropriate augmentations can destroy class boundaries.
  - **Quick check question**: Which augmentation pair did the original paper select for SimCLR?

- **Concept**: Flowpic representation construction
  - **Why needed here**: Transforms time series into a 2D spatial format suitable for CNNs.
  - **Quick check question**: What are the bin sizes for a 32×32 flowpic given 15s and 0-1500B ranges?

## Architecture Onboarding

- **Component map**: Raw packet series → time+size binning → 2D histogram → CNN conv layers → latent vector → projection → contrastive loss
- **Critical path**: Raw packet series → time+size binning → 2D histogram → CNN conv layers → latent vector → projection → contrastive loss
- **Design tradeoffs**:
  - Flowpic resolution vs computational cost
  - Augmentation strength vs semantic preservation
  - Projection layer dimension vs latent space richness
- **Failure signatures**:
  - Low accuracy with large CI: likely data shift or poor augmentation
  - Very high training but low test accuracy: overfitting to small labeled set
  - Unstable training: learning rate or batch size mismatch
- **First 3 experiments**:
  1. Reproduce supervised training with no augmentation on UCDAVIS19, verify baseline accuracy.
  2. Add Change RTT + Time Shift augmentations, measure gain.
  3. Swap to SimCLR pre-training with same augmentations, fine-tune on 10 labeled samples, compare to supervised.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the exact cause of the data shift between the human and script partitions in the UCDAVIS19 dataset?
- **Basis in paper**: [explicit] The paper observes a 20% accuracy drop on the human partition compared to the script partition and provides evidence of a data shift through flowpic visualizations and kernel density estimations.
- **Why unresolved**: The paper cannot pinpoint the exact reason for the data shift as the authors of the original study did not provide detailed explanations.
- **What evidence would resolve it**: Detailed information from the original dataset creators about the collection methodology and any differences between the human and script partitions would be needed.

### Open Question 2
- **Question**: How do different data augmentation techniques perform in a contrastive learning setting with few-shot fine-tuning on traffic classification tasks?
- **Basis in paper**: [inferred] The paper mentions this as a limitation and suggests it as future work, noting that the study focused on supervised learning settings.
- **Why unresolved**: The paper only investigated data augmentation in a supervised learning context, leaving the impact in contrastive learning unexplored.
- **What evidence would resolve it**: Conducting experiments that apply various data augmentation techniques during contrastive learning pre-training and evaluating their impact on few-shot fine-tuning performance.

### Open Question 3
- **Question**: How does the performance of flowpic input representation compare to other input representations (e.g., packet time series, payload bytes) in traffic classification tasks?
- **Basis in paper**: [explicit] The paper acknowledges that flowpic requires observing multiple seconds of traffic, which may not fit all network management needs, and suggests that a broader comparison of input representations would be of interest.
- **Why unresolved**: The paper focused on reproducing and replicating results using flowpic and did not perform a comprehensive comparison with other input representations.
- **What evidence would resolve it**: Systematic experiments comparing the performance of flowpic against packet time series and payload bytes across multiple datasets and classification tasks.

## Limitations
- Discovered 20% accuracy drop on human partition of UCDAVIS19 due to undetected data shift
- Inconsistent performance across different flowpic resolutions (16×16, 32×32, 64×64)
- Uncertain exact order and combination of data augmentation transformations in original study

## Confidence
- **High**: Core mechanisms (data augmentation, flowpic representation, SimCLR pre-training) successfully reproduced across multiple datasets
- **Medium**: Dataset-specific results due to discovered data shift and flowpic resolution sensitivity
- **Low**: Exact augmentation strategy ranking due to inconsistent performance across resolutions and datasets

## Next Checks
1. Re-analyze the UCDAVIS19 dataset to characterize the data shift between script and human partitions, including statistical testing of flowpic distributions.
2. Systematically vary flowpic resolution (e.g., 16×16, 32×32, 64×64) to determine optimal resolution for each dataset and assess sensitivity.
3. Test additional augmentation combinations beyond those in the original study to verify whether the selected pair is truly optimal or dataset-specific.