---
ver: rpa2
title: Large Language Models Can Infer Psychological Dispositions of Social Media
  Users
arxiv_id: '2309.08631'
source_url: https://arxiv.org/abs/2309.08631
tags:
- personality
- users
- psychological
- scores
- traits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Language Models (LLMs) like GPT-3.5 and GPT-4 can accurately
  infer Big Five personality traits from social media posts without explicit training,
  achieving correlations of r=.29 between inferred and self-reported scores. Performance
  improves with more input text, but is already high after observing 100 status updates.
---

# Large Language Models Can Infer Psychological Dispositions of Social Media Users

## Quick Facts
- arXiv ID: 2309.08631
- Source URL: https://arxiv.org/abs/2309.08631
- Reference count: 40
- Primary result: GPT-3.5 and GPT-4 can infer Big Five personality traits from social media posts with r=.29 correlation to self-reported scores

## Executive Summary
Large Language Models like GPT-3.5 and GPT-4 can accurately infer Big Five personality traits from social media posts without explicit training, achieving moderate correlations with self-reported personality scores. The models show improved performance with more input text and slightly better accuracy for GPT-4 compared to GPT-3.5. However, accuracy varies across demographic groups, with lower errors for women and younger users on several traits. These findings suggest potential for democratizing psychometric assessments while raising ethical concerns about privacy and consent.

## Method Summary
The study used the MyPersonality dataset containing Facebook status updates from 1,000 users (200 posts per user) along with self-reported Big Five personality scores from IPIP questionnaires. LLMs (GPT-3.5 and GPT-4) performed zero-shot inference by scoring each user's posts on personality dimensions using a simple prompt. The results were averaged across three rating rounds and compared to self-reported scores using Pearson correlation coefficients. The analysis also examined performance differences across age and gender groups.

## Key Results
- GPT models achieved r=.29 correlation between inferred and self-reported Big Five personality scores
- Performance improved with more input text, reaching high accuracy after 100 status updates
- GPT-4 showed slightly higher accuracy than GPT-3.5 across all five personality traits
- Accuracy varied across demographic groups, with lower errors for women and younger users on several traits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can infer personality traits from social media posts without explicit training because they have learned statistical patterns between linguistic features and psychological dispositions from their pre-training corpus.
- Mechanism: The model captures latent semantic relationships between word choices, syntactic patterns, and personality-relevant concepts during pretraining, enabling zero-shot inference when prompted appropriately.
- Core assumption: The pretraining data contains sufficient diverse examples of language that correlates with personality traits, allowing the model to develop implicit mappings.
- Evidence anchors:
  - [abstract]: "without explicit training" and "zero-shot learning scenario"
  - [section]: "LLMs like ChatGPT can accurately infer the Big Five personality traits from users' Facebook status updates in a zero-shot learning scenario"
- Break condition: If the pretraining corpus lacks sufficient personality-relevant linguistic patterns, or if the model's capacity to learn these patterns is limited, the zero-shot performance would degrade significantly.

### Mechanism 2
- Claim: GPT-4 achieves higher accuracy than GPT-3.5 because of its enhanced architecture and larger context window, allowing it to capture more nuanced personality signals.
- Mechanism: GPT-4's improved architecture enables better representation of complex relationships between text features and personality traits, while its larger context window allows processing of more status updates for richer inference.
- Core assumption: The architectural improvements between GPT-3.5 and GPT-4 translate to better performance on personality inference tasks.
- Evidence anchors:
  - [abstract]: "GPT-4 is slightly more accurate than GPT-3.5"
  - [section]: "GPT4 showed higher levels of accuracy across all five personality traits, although none of the individual comparisons reached statistical significance"
- Break condition: If the improvements in GPT-4 are not relevant to personality inference, or if the task is already saturated by GPT-3.5's capabilities, the accuracy difference may not be meaningful.

### Mechanism 3
- Claim: The model shows demographic biases because its training data reflects societal stereotypes and differences in self-expression patterns across groups.
- Mechanism: The model learns and reproduces patterns present in its training data, including stereotypes about gender and age differences in personality expression and assessment.
- Core assumption: The training data contains biased representations of different demographic groups that the model learns and applies during inference.
- Evidence anchors:
  - [abstract]: "Accuracy varies across demographic groups, with lower errors for women and younger users on several traits"
  - [section]: "our findings also highlight heterogeneity in the accuracy of personality inferences across different age groups and gender categories"
- Break condition: If the biases are not actually present in the training data, or if the model applies some form of debiasing during inference, the observed demographic differences might not be due to learned biases.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The paper explicitly tests whether LLMs can perform personality inference without being trained on personality-labeled data
  - Quick check question: Can you explain the difference between zero-shot, few-shot, and fine-tuning learning paradigms?

- Concept: Big Five personality traits
  - Why needed here: The study uses the Big Five model as the target for personality inference, requiring understanding of what these traits represent
  - Quick check question: What are the five dimensions of the Big Five personality model and what does each measure?

- Concept: Correlation analysis
  - Why needed here: The study uses Pearson correlation coefficients to measure the relationship between inferred and self-reported personality scores
  - Quick check question: What does a correlation coefficient of r=.29 tell you about the relationship between two variables?

## Architecture Onboarding

- Component map: Data preprocessing (chunking) → Prompt engineering (system and inference prompts) → API calls to GPT models → Response parsing → Statistical analysis
- Critical path: Data → Chunking → Prompt → API Call → Parse → Aggregate → Correlate with self-report
- Design tradeoffs: Using larger chunks provides more context but risks exceeding token limits; averaging across multiple prompts improves reliability but increases cost and latency
- Failure signatures: Low correlations might indicate prompt issues, insufficient context, or model limitations; demographic biases might indicate training data issues
- First 3 experiments:
  1. Test different prompt formulations to optimize inference accuracy
  2. Vary the number of status updates used to find the optimal input volume
  3. Compare results using different chunk sizes to balance context and token limits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs like GPT-3.5 and GPT-4 make personality inferences from social media text, and do they use the same behavioral cues as humans or supervised machine learning models?
- Basis in paper: [explicit] The paper states "we cannot speak to the question of whether LLMs use the same behavioral cues as humans or supervised machine learning models when translating behavioral residues into psychological profiles."
- Why unresolved: The study focused on the accuracy of LLM inferences, not the underlying mechanisms or cues used by the models.
- What evidence would resolve it: Research analyzing the internal workings of LLMs during personality inference tasks, comparing their decision-making processes to those of humans and supervised models.

### Open Question 2
- Question: Why do LLMs show systematic biases in personality estimation across different age and gender categories?
- Basis in paper: [explicit] The paper suggests these biases "could be either indicative of a bias introduced in the training of the models and/or the corpora of text data the models have been trained on, or be reflective of differences in people's general self-expression on social media."
- Why unresolved: The study did not investigate the root causes of these biases or distinguish between potential sources.
- What evidence would resolve it: Comparative studies analyzing LLM training data, human self-expression patterns, and model behavior across diverse demographic groups to identify bias sources.

### Open Question 3
- Question: How would the accuracy of LLM personality inferences change with more contemporary social media data or different input volumes?
- Basis in paper: [inferred] The paper notes limitations including using data from 2007-2012 and testing with 200 status updates, suggesting potential improvements with newer data or different input volumes.
- Why unresolved: The study used a specific dataset and fixed input volume, limiting generalizability to other contexts.
- What evidence would resolve it: Experiments testing LLM personality inference accuracy on contemporary social media data with varying input volumes, comparing results to the current study.

## Limitations

- The r=.29 correlation, while significant, indicates substantial unexplained variance in personality predictions
- Demographic biases in accuracy (particularly for Neuroticism and Conscientiousness) suggest systematic errors that could disadvantage certain groups
- The study uses Facebook status updates from a specific dataset, limiting generalizability to other platforms or contexts
- Zero-shot learning inherently limits performance compared to supervised approaches with labeled personality data

## Confidence

- High confidence in the methodological approach and statistical findings
- Medium confidence in the generalizability of results to other social media contexts
- Low confidence in the practical utility of these inferences for individual-level applications

## Next Checks

1. **Cross-platform validation**: Test the same methodology on Twitter, Reddit, or other social media data to assess generalizability beyond Facebook status updates
2. **Bias decomposition analysis**: Conduct a detailed analysis of which linguistic features drive demographic differences in accuracy, examining residuals by age and gender groups
3. **Practical utility testing**: Evaluate whether the personality inferences improve any downstream applications (e.g., recommendation systems, mental health screening) beyond baseline demographic predictions