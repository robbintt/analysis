---
ver: rpa2
title: Exploring Linguistic Probes for Morphological Generalization
arxiv_id: '2310.13686'
source_url: https://arxiv.org/abs/2310.13686
tags:
- feature
- sets
- were
- generalization
- triples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares traditional language-independent train/test
  splits for morphological inflection models with language-specific probes designed
  to test generalization abilities. Using English, Spanish, and Swahili, the authors
  evaluate three neural models (CHR-TRM, CLUZH, ENC-DEC) on orthographic and phonological
  inputs.
---

# Exploring Linguistic Probes for Morphological Generalization

## Quick Facts
- arXiv ID: 2310.13686
- Source URL: https://arxiv.org/abs/2310.13686
- Reference count: 7
- Primary result: Neural morphological inflection systems show better feature-set generalization in agglutinative languages than fusional languages, with no significant effect from orthographic vs. phonological presentation style.

## Executive Summary
This paper introduces language-specific probes to test morphological inflection systems' generalization abilities beyond traditional train/test splits. Using English, Spanish, and Swahili, the authors evaluate three neural models (CHR-TRM, CLUZH, ENC-DEC) on both orthographic and phonological inputs. The study finds that systems make reasonable linguistic generalizations even when incorrect, with agglutinative languages showing better feature-set generalization than fusional languages. Interestingly, no significant difference was found between orthographic and phonological presentation styles, though all systems performed worse on unseen feature sets.

## Method Summary
The study creates language-specific probes using UniMorph morphological inflection data for English, Spanish, and Swahili, with orthographic data transcribed to phonological using CMU Pronouncing Dictionary (English) or Epitran (Spanish, Swahili). The researchers design PROBE splits that withhold specific feature sets or conjugational classes from training, contrasting with traditional BLIND splits. Three neural models (CHR-TRM, CLUZH, ENC-DEC) are trained on these splits with 1600 train + 400 fine-tune items, then evaluated on held-out test items to measure generalization across different probe types and presentation styles.

## Key Results
- No significant effect of presentation style (orthographic vs. phonological) on system accuracy across all models and languages
- Agglutinative languages (Swahili) show better feature-set generalization compared to fusional languages (English)
- Systems employ distinct generalization strategies across conjugational classes and feature sets
- All systems performed worse on unseen feature sets regardless of presentation style

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Systems generalize better across feature sets in agglutinative languages than fusional languages
- Mechanism: Agglutinative languages map individual features to distinct morphological patterns, enabling component-wise generalization. Fusional languages map entire feature sets to single patterns, blocking this generalization.
- Core assumption: The mapping between features and morphological patterns is one-to-one in agglutinative languages
- Evidence anchors: Abstract finding on Swahili vs. English generalization differences; corpus comparison relies on UniMorph data normalization

### Mechanism 2
- Claim: Systems use different generalization strategies based on probe type (feature set vs. conjugational class)
- Mechanism: Feature set probes require generalization across feature combinations, while conjugational class probes require generalization based on lemma form
- Core assumption: Systems can distinguish between the types of generalization required by different probe designs
- Evidence anchors: Abstract observation of distinct generalization strategies; corpus provides indirect performance differences between probe types

### Mechanism 3
- Claim: Orthographic presentation does not significantly affect system performance compared to phonological transcription
- Mechanism: Systems learn the same morphological generalizations regardless of input representation, as underlying patterns are preserved
- Core assumption: Morphological patterns are equivalent in orthographic and phonological representations
- Evidence anchors: Abstract finding of no statistical effect for presentation style; strong ANOVA evidence across all systems and languages

## Foundational Learning

- Concept: Morphological inflection
  - Why needed here: Essential for designing and interpreting probes that test system generalization
  - Quick check question: What is the difference between fusional and agglutinative languages in terms of how they express morphological features?

- Concept: Generalization in neural networks
  - Why needed here: Critical for understanding how neural systems generalize to unseen feature sets and conjugational classes
  - Quick check question: How does a neural network's ability to generalize to unseen feature sets differ from its ability to generalize to unseen lemmas?

- Concept: Data splitting strategies
  - Why needed here: Understanding how BLIND and PROBE strategies differ is crucial for interpreting the study's findings
  - Quick check question: What is the key difference between BLIND and PROBE data splitting strategies, and how does this affect the types of generalization that can be tested?

## Architecture Onboarding

- Component map: Data preprocessing -> Probe design -> Model training -> Evaluation
- Critical path: 1) Preprocess data and create BLIND/PROBE splits 2) Train three neural models on each split 3) Evaluate on held-out test sets 4) Compare BLIND vs. PROBE performance and analyze presentation style effects
- Design tradeoffs: Language-specific probes enable controlled testing but require domain expertise; orthographic data is practical but may be less appropriate for cognitive modeling; model choice affects testable generalizations
- Failure signatures: Poor PROBE performance indicates memorization over generalization; significant presentation style effects suggest failure to learn underlying patterns; uninterpretable errors indicate incorrect generalization strategy
- First 3 experiments: 1) Train and evaluate on BLIND splits for baseline 2) Design and implement PROBE splits, then train and evaluate 3) Compare BLIND vs. PROBE performance and analyze presentation style effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do orthographic presentation advantages persist across different transcription dictionary choices and processing methods?
- Basis in paper: The paper notes English may favor orthography due to removing allomorphy, but the difference might be due to transcription dictionary choice
- Why unresolved: Study used specific CMU Pronouncing Dictionary translation and particular processing choices without testing alternatives
- What evidence would resolve it: Testing multiple transcription dictionaries and varying preprocessing approaches to test consistency of orthography advantage

### Open Question 2
- Question: Do neural morphological inflection models show language-specific generalization patterns beyond English, Spanish, and Swahili?
- Basis in paper: The study examined three languages but noted future work with more languages could extend the approach
- Why unresolved: Limited to three languages that may not capture full morphological diversity
- What evidence would resolve it: Similar probe-based evaluations across broader typological sample, particularly polysynthetic languages

### Open Question 3
- Question: What specific factors determine when neural models successfully generalize across conjugational classes versus feature sets?
- Basis in paper: ENC-DEC showed success on Spanish conjugational class probes but not feature set generalization, unlike CHR-TRM and CLUZH
- Why unresolved: Paper observed different behaviors but didn't systematically investigate enabling conditions
- What evidence would resolve it: Controlled experiments varying model architectures, training data distributions, and probe designs

## Limitations
- Language-specific probes may not fully capture all aspects of morphological generalization across languages
- Study limited to three languages, restricting generalizability to other language families
- Orthographic vs. phonological comparison relies on transcription dictionaries that may introduce artifacts

## Confidence

- High confidence: No significant difference between orthographic and phonological presentation styles across all systems
- Medium confidence: Agglutinative languages show better feature-set generalization than fusional languages
- Medium confidence: Systems employ distinct generalization strategies for different probe types

## Next Checks

1. Replicate the orthographic vs. phonological comparison using an additional language family (e.g., Semitic languages) to test the robustness of the presentation style finding
2. Test the probe design methodology on a language with different morphological typology (e.g., polysynthetic) to evaluate generalizability of the probe framework
3. Implement ablation studies removing specific morphological features to isolate which feature combinations drive the agglutinative vs. fusional differences observed