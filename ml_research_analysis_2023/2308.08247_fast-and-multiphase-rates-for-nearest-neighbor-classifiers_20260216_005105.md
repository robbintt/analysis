---
ver: rpa2
title: Fast and Multiphase Rates for Nearest Neighbor Classifiers
arxiv_id: '2308.08247'
source_url: https://arxiv.org/abs/2308.08247
tags:
- rate
- data
- error
- then
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the scaling behavior of nearest neighbor
  classifiers, revealing that the error rate can exhibit two distinct phases as the
  training dataset size increases. The first phase shows fast convergence where error
  depends polynomially on data dimension, while the second phase shows slow convergence
  with exponential dependence on dimension.
---

# Fast and Multiphase Rates for Nearest Neighbor Classifiers

## Quick Facts
- arXiv ID: 2308.08247
- Source URL: https://arxiv.org/abs/2308.08247
- Reference count: 40
- This paper reveals that k-NN error rates exhibit two distinct phases - fast (polynomial in dimension) and slow (exponential in dimension) - determined by the data's geometric structure.

## Executive Summary
This paper investigates the scaling behavior of nearest neighbor classifiers as training dataset size increases. The key finding is that error rates can exhibit two distinct phases: fast convergence where error depends polynomially on data dimension, and slow convergence with exponential dependence on dimension. This behavior is determined by the underlying geometry of the data distribution - when data points are "positively dominated" (closer in signal space to points of the same class), fast convergence occurs; when "negatively dominated" (closer to points of different class), slow convergence is inevitable. The authors provide theoretical analysis and validate their findings through experiments on MNIST, Fashion-MNIST, and synthetic datasets.

## Method Summary
The paper analyzes k-NN classification by decomposing data into signal and noise components, then characterizing how the error rate depends on the relationship between test points and training data in signal space. The method uses stochastic dominance conditions to establish positive and negative dominance regions, and derives error bounds using Gaussian approximation and concentration inequalities. The theoretical framework requires appropriate scaling of the number of neighbors k with both sample size n and dimension d, specifically k ≳ n/√d for fast convergence.

## Key Results
- k-NN error rates exhibit two distinct phases as training data increases: fast convergence (polynomial in dimension) and slow convergence (exponential in dimension)
- The phase behavior is determined by data geometry through "positive dominance" (fast convergence) and "negative dominance" (slow convergence) conditions
- Theoretical analysis shows that k must scale as k ≳ n/√d to maintain fast convergence rates
- Experimental validation on MNIST, Fashion-MNIST, and synthetic datasets confirms the two-phase phenomenon

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The error rate exhibits two distinct phases as training data increases: fast convergence (polynomial in dimension) followed by slow convergence (exponential in dimension).
- Mechanism: The classification error depends on the ratio of conditional distributions between classes. When data points are "positively dominated" (closer in signal space to same-class points), the error decreases polynomially with dimension. When "negatively dominated" (closer to different-class points), error decreases exponentially.
- Core assumption: The data distribution can be decomposed into signal and noise components, where the signal determines class membership and noise is sub-Gaussian.
- Evidence anchors:
  - [abstract]: "When data points are 'positively dominated' (closer in signal space to points of the same class), fast convergence occurs; when 'negatively dominated' (closer to points of different class), slow convergence is inevitable."
  - [section]: "Our analysis highlights the complexity of the data distribution in determining the generalization error. When the data distributes benignly, our result suggests that nearest neighbor classifier can achieve a generalization error that depends polynomially, instead of exponentially, on the data dimension."
  - [corpus]: Weak evidence - corpus contains general nearest neighbor search papers but lacks specific analysis of phase transitions in error scaling.
- Break condition: The two-phase behavior breaks when the signal-to-noise ratio becomes too low or when the positive dominance condition no longer holds for any portion of the data distribution.

### Mechanism 2
- Claim: The number of nearest neighbors k must scale appropriately with data dimension to maintain fast convergence rates.
- Mechanism: The error bound depends on k through the exponential term exp(-k/6) + exp(-cτ²k/d). For fast convergence, k must be at least proportional to n/√d, ensuring the Gaussian approximation error remains negligible.
- Core assumption: The noise component follows a sub-Gaussian distribution allowing Gaussian approximation via Berry-Esseen theorem.
- Evidence anchors:
  - [section]: "Our theorems for the positive results require that k ≳ n/√d such that the additive error incurred by the Gaussian approximation is negligible."
  - [section]: "Theorem 1 (Fast rate). Fix a point x ∈ Rd. Under Assumption 1, suppose τ -positive dominance holds... There exist constants c1, c, C that only depend on M, R, β such that, if d ≥ C and c1n√d ≤ k ≤ n/10, then..."
  - [corpus]: Weak evidence - corpus lacks specific discussion of nearest neighbor parameter tuning for convergence rates.
- Break condition: The fast convergence breaks when k is too small relative to n/√d, making Gaussian approximation errors dominant.

### Mechanism 3
- Claim: The phase transition depends critically on the geometric relationship between test points and training data in signal space.
- Mechanism: Positive dominance requires that signal components of test points are stochastically closer to same-class training points. This geometric condition determines whether the ratio of conditional probabilities deviates sufficiently from unity for correct classification.
- Core assumption: The signal component Ps(X) can be separated from noise component Pn(X) and has bounded norm.
- Evidence anchors:
  - [section]: "Definition 3 (Positive dominance). For any label θ ∈ {0, 1}, recall the definition of Xθ in (3)... We say that x is τ-positively dominated if ∥Ps(X − x)∥st. ≤ ∥Ps(X′ − x)∥, E∥Ps(X′ − x)∥2 − E∥Ps(X − x)∥2 ≥ τ."
  - [section]: "The positive dominance condition allows us to upper bound the number of samples with a different label by the number of samples with the same label, and hence resulting in a correct prediction."
  - [corpus]: Weak evidence - corpus contains general nearest neighbor classification but lacks specific analysis of dominance conditions.
- Break condition: The mechanism breaks when the signal components of different classes overlap significantly or when the dominance margin τ becomes too small.

## Foundational Learning

- Concept: Stochastic dominance in probability theory
  - Why needed here: The analysis relies on comparing cumulative distribution functions of signal distances to establish positive and negative dominance conditions.
  - Quick check question: Given two random variables X and Y where P[X ≤ t] ≤ P[Y ≤ t] for all t, what can you conclude about their relationship?

- Concept: Gaussian approximation and local limit theorems
  - Why needed here: The proofs use Berry-Esseen and local limit theorems to approximate the distribution of noise components, which is crucial for establishing the error bounds.
  - Quick check question: What is the typical error bound when approximating the distribution of a sum of independent random variables by a Gaussian distribution?

- Concept: Binary classification with plug-in estimators
  - Why needed here: The k-NN classifier is analyzed as a plug-in estimator of the regression function η(x), where understanding the relationship between η estimation error and classification error is essential.
  - Quick check question: How does the classification error relate to the regression function η(x) = P[Y=1|X=x] for a plug-in classifier?

## Architecture Onboarding

- Component map:
  - Data preprocessing: Signal-noise decomposition and dominance region identification
  - Core algorithm: k-NN classification with parameter k
  - Analysis pipeline: Error bound computation using stochastic dominance and Gaussian approximation
  - Validation: Empirical testing on synthetic and real datasets

- Critical path:
  1. Decompose data into signal and noise components
  2. Identify positive and negative dominance regions
  3. Select appropriate k based on n and d
  4. Compute error bounds using Theorem 1 or 2
  5. Validate predictions against empirical results

- Design tradeoffs:
  - Larger k improves stability but requires more data
  - More aggressive signal-noise decomposition improves analysis but may lose information
  - Conservative dominance thresholds ensure correctness but may miss fast convergence regions

- Failure signatures:
  - Error decreases slower than predicted polynomial rate: likely insufficient k or weak dominance condition
  - Error plateaus despite increasing data: possible negative dominance region dominance
  - Gaussian approximation errors dominate: signal-noise decomposition may be inadequate

- First 3 experiments:
  1. Synthetic data with aligned boundary: Verify fast polynomial convergence when all points are positively dominated
  2. Synthetic data with rotated boundary: Demonstrate two-phase behavior with slow convergence in negative dominance regions
  3. MNIST vs Fashion-MNIST: Compare convergence rates to validate theoretical predictions on real datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sharp is the phase transition between fast and slow convergence rates in k-NN classification?
- Basis in paper: [inferred] The paper mentions that there might exist a subset of X where neither the positive nor the negative dominance condition holds, and determining whether the phase transition is sharp or smooth remains an open problem.
- Why unresolved: The paper's analysis focuses on regions where either positive or negative dominance holds, but doesn't characterize the intermediate region where neither condition is satisfied.
- What evidence would resolve it: Numerical experiments showing the behavior of error rates in the intermediate region, or theoretical analysis characterizing the convergence rate in this region.

### Open Question 2
- Question: Can the theoretical bounds on k-NN convergence rates be tightened for practical settings?
- Basis in paper: [explicit] The paper states that the theoretical analysis can be conservative and extending the result to different choices of k is an interesting future direction. It also notes that under the low dimensional regime, the Gaussian approximation for the noise component is ineffective and the theorem could be loose.
- Why unresolved: The current theoretical bounds rely on Gaussian approximation and concentration inequalities that may not be tight in practice, especially for moderate sample sizes and dimensions.
- What evidence would resolve it: Improved theoretical analysis with tighter bounds, or extensive empirical studies comparing theoretical predictions with actual performance across various datasets and dimensions.

### Open Question 3
- Question: How does the two-phase phenomenon extend to other nonlinear classifiers beyond k-NN?
- Basis in paper: [explicit] The paper discusses that understanding the excess risk of neural networks can benefit from studying non-parametric methods, and mentions that decision trees and kernel methods are more approachable using existing techniques.
- Why unresolved: The current analysis is specific to k-NN classifiers and doesn't directly apply to other nonlinear models like neural networks or decision trees.
- What evidence would resolve it: Theoretical analysis showing similar two-phase behavior in other classifiers, or empirical studies demonstrating the phenomenon across different model classes.

### Open Question 4
- Question: What are sufficient conditions for the density function fx to satisfy the smoothness condition required for slow rate proofs?
- Basis in paper: [explicit] The paper states that while Gaussian approximation is a well-established topic, there is an absence of simple sufficient conditions for satisfying the smoothness condition fx ∈ G(T, α).
- Why unresolved: The paper only verifies this condition for specific case studies but doesn't provide general criteria for when it holds.
- What evidence would resolve it: Characterization of classes of distributions for which fx satisfies the smoothness condition, or counterexamples showing when it fails to hold.

## Limitations

- The theoretical analysis assumes clean signal-noise decomposition which may not hold in real-world datasets
- Positive/negative dominance conditions are difficult to verify empirically without knowing true data distributions
- Analysis assumes sub-Gaussian noise distributions, potentially limiting applicability to heavy-tailed or multimodal noise patterns
- The two-phase behavior characterization focuses on extreme cases, with limited understanding of intermediate regions

## Confidence

- Fast vs slow convergence rates: High confidence - the theoretical framework is rigorous and supported by multiple experimental validations
- Dominance condition characterization: Medium confidence - while mathematically sound, empirical verification is challenging without ground truth distributions
- k-parameter scaling requirements: Medium confidence - theoretical bounds are provided but practical implementation may require tuning

## Next Checks

1. **Robustness to noise distributions**: Test the two-phase behavior on datasets with heavy-tailed or non-Gaussian noise to verify whether the convergence patterns persist beyond the sub-Gaussian assumption.

2. **High-dimensional scaling verification**: Systematically vary both dimension d and sample size n to empirically validate the predicted polynomial vs exponential scaling relationship, particularly focusing on the transition point between phases.

3. **Boundary alignment sensitivity**: Generate synthetic datasets with varying degrees of boundary alignment relative to signal-noise directions to quantify how boundary orientation affects the size and impact of negative dominance regions.