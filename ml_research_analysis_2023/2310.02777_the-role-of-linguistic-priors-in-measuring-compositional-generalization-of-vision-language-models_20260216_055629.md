---
ver: rpa2
title: The Role of Linguistic Priors in Measuring Compositional Generalization of
  Vision-Language Models
arxiv_id: '2310.02777'
source_url: https://arxiv.org/abs/2310.02777
tags:
- compositional
- language
- arxiv
- hard
- linguistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of measuring compositional generalization
  in vision-language models. The authors identify that current benchmarks for compositional
  generalization rely heavily on linguistic priors rather than visual information,
  leading to inflated performance metrics.
---

# The Role of Linguistic Priors in Measuring Compositional Generalization of Vision-Language Models

## Quick Facts
- arXiv ID: 2310.02777
- Source URL: https://arxiv.org/abs/2310.02777
- Reference count: 10
- One-line primary result: Current vision-language benchmarks for compositional generalization are biased toward linguistic priors rather than true visual-text interplay

## Executive Summary
This paper addresses a critical gap in evaluating compositional generalization for vision-language models by identifying that current benchmarks rely heavily on linguistic priors rather than visual information. The authors decompose compositional generalization into two components - uni-modal linguistic priors and the interplay between image and text - and propose a new metric called "hard test accuracy" to quantify the contribution of linguistic priors. Their key finding is that language models can achieve comparable or better performance than vision-language models on these benchmarks without using any visual information, demonstrating the bias in current evaluation methods. The proposed metric reveals that vision-language models have smaller linguistic gaps than models trained with hard negative text mining, indicating that these models rely less on linguistic priors.

## Method Summary
The authors use the ARO dataset (Visual Genome Attribution, Visual Genome Relation, COCO Ordering, Flickr Ordering) containing images with multiple captions where one is true and others are corrupted versions. They evaluate both language models (GPT-2 variants, OPT, GPT-J) and vision-language models (CLIP, BLIP, FLAVA) on these tasks. Language models predict true captions based on perplexity scores, while vision-language models use image-text matching scores. The hard test accuracy metric is computed by filtering instances where language models fail and measuring vision-language model performance on these "hard" instances. The authors also analyze the effect of hard negative text mining (NegCLIP) on model performance and linguistic prior reliance.

## Key Results
- Language models can achieve comparable or better performance than vision-language models on compositional generalization benchmarks without using visual information
- The hard test accuracy metric reveals that current benchmarks are heavily influenced by linguistic priors rather than visual-text interplay
- Vision-language models have smaller linguistic gaps than models trained with hard negative text mining, indicating less reliance on linguistic priors
- Fine-tuning with hard negative text mining improves overall accuracy but primarily learns linguistic priors rather than visual-text relationships

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models can perform well on vision-language compositional generalization benchmarks without using visual information, revealing a linguistic prior bias in the benchmarks.
- Mechanism: The benchmarks' construction allows captions to be evaluated purely on linguistic plausibility, making visual input redundant for many cases.
- Core assumption: Captions in the ARO datasets contain enough linguistic cues to distinguish true from false captions without visual context.
- Evidence anchors:
  - [abstract] "language models can achieve comparable or better performance than vision-language models on compositional generalization benchmarks without using any visual information"
  - [section 4] "we use the perplexity computed by A VGLM to represent the possibility of a caption, and compute fine-grained model performance for input data that have similar perplexities"
  - [corpus] Weak evidence - related papers focus on compositionality but not specifically on linguistic priors in vision-language benchmarks

### Mechanism 2
- Claim: Fine-tuning vision-language models with hard negative text mining improves overall accuracy but primarily learns linguistic priors rather than visual-text interplay.
- Mechanism: The fine-tuning process with hard negatives emphasizes linguistic distinctions rather than teaching models to use visual information effectively.
- Core assumption: The improvement in overall accuracy from hard negative mining comes from better handling of linguistic patterns rather than better visual-text integration.
- Evidence anchors:
  - [section 5.2] "the performance of NegCLIP for bottom-right blocks are similar to CLIP and CLIP FT. Hence we hypothesize that NegCLIP mostly learns linguistic priors during its fine-tuning process"
  - [section 5.1] "the prediction accuracies for upper-left blocks are much higher than those of bottom-right ones. Therefore, the performance of NegCLIP has a strong correlation with the linguistic prior"
  - [corpus] Weak evidence - related work on emergent communication doesn't directly address hard negative mining effects

### Mechanism 3
- Claim: The "hard test accuracy" metric effectively isolates the contribution of visual-text interplay by filtering out cases where linguistic priors dominate.
- Mechanism: By focusing only on instances where language models fail, we measure the pure contribution of vision-language models' ability to use visual information.
- Core assumption: Cases where language models succeed are primarily driven by linguistic priors, so removing them reveals true visual-text generalization ability.
- Evidence anchors:
  - [section 5.2] "We define 'hard instances' to be the data where A VGLM made mistakes, i.e., where the true caption has a higher perplexity than false ones"
  - [section 5.2] "A model whose prediction is independent of the perplexities of the candidate captions should have a constant accuracy over hard and easy samples, yielding zero linguistic gap"
  - [corpus] Weak evidence - related work on compositionality metrics doesn't specifically address this isolation approach

## Foundational Learning

- Concept: Perplexity as a measure of linguistic plausibility
  - Why needed here: The paper uses perplexity to quantify how likely a caption is based on linguistic patterns alone
  - Quick check question: If a caption has low perplexity according to a language model, what does that tell us about its linguistic structure?

- Concept: Uni-modal vs. multi-modal compositionality
  - Why needed here: The paper decomposes compositional generalization into linguistic-only and visual-text components
  - Quick check question: How would you design an experiment to test whether compositional generalization comes from language alone or from combining modalities?

- Concept: Hard negative mining in contrastive learning
  - Why needed here: The paper uses hard negative mining to fine-tune vision-language models and analyzes its effects
  - Quick check question: What's the difference between random negative samples and hard negative samples in contrastive learning?

## Architecture Onboarding

- Component map: ARO datasets (VG Attribution, VG Relation, COCO Ordering, Flickr Ordering) -> Language models (GPT-2 variants, OPT, GPT-J) -> Vision-language models (CLIP, BLIP, FLAVA with different heads) -> Metric computation (perplexity calculation, hard test accuracy)

- Critical path: Data preprocessing → perplexity computation → accuracy calculation → hard test accuracy derivation → comparison across models

- Design tradeoffs:
  - Using language model perplexity as a proxy for linguistic prior strength vs. more direct measurements
  - Focusing on hard instances vs. using all data for metric computation
  - Using simple averaging of multiple language models vs. more sophisticated ensemble methods

- Failure signatures:
  - High linguistic gap but low overall accuracy might indicate the model is overfitting to linguistic patterns
  - Similar performance between language models and vision-language models on all instances might indicate visual information isn't being used
  - Very low hard test accuracy might indicate the visual-text interplay component is weak

- First 3 experiments:
  1. Replicate the perplexity-based analysis on a different compositional generalization benchmark to test generalizability
  2. Modify the ARO dataset construction to require visual information for accurate discrimination
  3. Implement an alternative metric for isolating visual-text contribution that doesn't rely on language model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the specific reason why multi-modal networks exhibit compositional generalization behavior?
- Basis in paper: [inferred] The paper mentions that the reasons may lie in the shared embedding space for different modalities.
- Why unresolved: The paper does not provide a detailed explanation of the underlying mechanisms that lead to compositional generalization in multi-modal models.
- What evidence would resolve it: Experimental results showing the impact of different embedding space structures on compositional generalization, or a theoretical analysis of the interaction between modalities in multi-modal models.

### Open Question 2
- Question: How can we generate more effective hard negatives for vision-language models to improve their compositional generalization?
- Basis in paper: [explicit] The paper mentions that it would be interesting to explore better ways of generating hard negatives for these models.
- Why unresolved: The paper does not provide a detailed methodology for generating hard negatives that effectively improve compositional generalization.
- What evidence would resolve it: Experimental results demonstrating the impact of different hard negative generation techniques on the compositional generalization performance of vision-language models.

### Open Question 3
- Question: How do different multi-modal datasets and tasks affect the compositional generalization of vision-language models?
- Basis in paper: [inferred] The paper suggests applying the framework to more models, benchmarks, and modalities.
- Why unresolved: The paper focuses on a specific dataset (ARO) and does not explore the impact of other datasets and tasks on compositional generalization.
- What evidence would resolve it: Experimental results comparing the compositional generalization performance of vision-language models across various datasets and tasks, and analyzing the factors that contribute to differences in performance.

## Limitations

- The hard test accuracy metric relies on language model performance as a proxy for linguistic prior strength, creating a circular dependency
- The analysis focuses on only four ARO datasets, which may not be representative of all vision-language compositional generalization challenges
- The paper doesn't explore alternative fine-tuning strategies that might better balance linguistic and visual learning beyond hard negative mining

## Confidence

**High Confidence**: The finding that language models can achieve comparable or better performance than vision-language models on current compositional generalization benchmarks. This is directly supported by empirical results showing specific accuracy numbers for different models on each dataset.

**Medium Confidence**: The claim that current benchmarks are "largely influenced by uni-modal linguistic priors rather than the interplay between image and text." While the empirical evidence strongly supports this, it's based on a limited set of benchmarks and language models.

**Low Confidence**: The assertion that NegCLIP "mostly learns linguistic priors during its fine-tuning process." The evidence shows correlation between linguistic prior difficulty and performance improvement, but doesn't definitively prove that the model isn't also learning useful visual-text relationships.

## Next Checks

1. **Benchmark Diversity Test**: Apply the hard test accuracy metric to a broader range of compositional generalization benchmarks (e.g., GQA, NLVR2) to assess whether the linguistic prior bias is a general phenomenon or specific to ARO datasets.

2. **Language Model Robustness Check**: Repeat the analysis using different language model architectures and training datasets to verify that the linguistic prior identification is consistent across different language modeling approaches.

3. **Visual Cue Manipulation**: Design a controlled experiment where visual information is systematically varied in the captions (e.g., adding or removing visual-specific references) to directly test whether vision-language models can leverage visual information when linguistic priors are controlled.