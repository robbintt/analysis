---
ver: rpa2
title: 'WinNet: Make Only One Convolutional Layer Effective for Time Series Forecasting'
arxiv_id: '2311.00214'
source_url: https://arxiv.org/abs/2311.00214
tags:
- period
- time
- window
- sequence
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes WinNet, a CNN-based model with only one convolutional
  layer for time series forecasting. The key idea is to transform the input sequence
  into a 2D tensor using a periodic window, then decompose it into trend and seasonal
  components and leverage their correlation via convolution.
---

# WinNet: Make Only One Convolutional Layer Effective for Time Series Forecasting

## Quick Facts
- arXiv ID: 2311.00214
- Source URL: https://arxiv.org/abs/2311.00214
- Reference count: 40
- Key outcome: WinNet achieves 18.5% and 12.0% improvements in MSE and MAE over the best CNN-based model (TimesNet) on multivariate forecasting tasks

## Executive Summary
WinNet proposes a novel CNN architecture that uses only one convolutional layer for time series forecasting by transforming input sequences into 2D tensors using periodic windows. The model decomposes these tensors into trend and seasonal components through a Two-Dimensional Period Decomposition (TDPD) block, then leverages their correlation via a single Decomposition Correlation Block (DCB). Experimental results on nine benchmark datasets demonstrate superior performance compared to state-of-the-art CNN, MLP, and Transformer-based methods while maintaining computational efficiency.

## Method Summary
WinNet transforms 1D time series sequences into 2D tensors using periodic windows, then applies Two-Dimensional Period Decomposition (TDPD) to separate trend and seasonal components. A single Decomposition Correlation Block (DCB) learns the local correlation between these components through a convolutional kernel with channel independence strategy. The model includes an Inter-Intra Period Encoder (I2PE) for initial transformation, TDPD for decomposition, DCB for correlation learning, and a Series Decoder for final predictions. This architecture achieves competitive forecasting accuracy while using only one convolutional layer.

## Key Results
- WinNet outperforms state-of-the-art CNN, MLP, and Transformer-based methods in prediction accuracy
- Achieves 18.5% and 12.0% improvements in MSE and MAE over TimesNet on multivariate forecasting tasks
- Maintains computational efficiency by using only one convolutional layer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The periodic window decomposition captures both long-term and short-term temporal dependencies by restructuring the input sequence into a 2D tensor where rows represent short-period trends and columns represent long-period trends.
- Mechanism: By using the least common multiple of multiple short periods as the window size, the model organizes temporal data so that convolutional operations can simultaneously extract intra-window (short-term) and inter-window (long-term) patterns without requiring deep architectures.
- Core assumption: The periodicity of real-world time series can be approximated by combining multiple short periods into a single periodic window, and this composite window will retain the essential temporal structure.
- Evidence anchors: [abstract] states the model "transforms the input sequence into a 2D tensor using a periodic window" and "decompose it into trend and seasonal components." [section] explains that "the periodic window is approximated as the least common multiple of multi-periods obtained by the Fourier Frequency Transformation (FFT)."
- Break condition: If the real-world time series contains non-periodic noise or irregular cycles that cannot be approximated by a common multiple of short periods, the periodic window will misrepresent the temporal structure.

### Mechanism 2
- Claim: The Two-Dimensional Period Decomposition (TDPD) effectively separates trend and seasonal components by applying 2D average pooling with trend-padding, which preserves boundary information better than standard padding methods.
- Mechanism: TDPD applies a 2D average pooling operation after trend-padding, which fills boundary regions with neighboring samples rather than zeros, thus maintaining the continuity of trend information while extracting seasonal oscillations.
- Core assumption: The trend component of a time series can be isolated by averaging over both temporal dimensions (short and long periods) while preserving boundary continuity through trend-padding.
- Evidence anchors: [section] describes TDPD as "a trend-padding operation is dedicatedly designed to perform the convolutional operation at the boundary" and explains the decomposition into "period-trend and oscillation terms." [abstract] mentions "decompose it into trend and seasonal components."
- Break condition: If the time series contains abrupt structural breaks or regime changes, the averaging operation in TDPD may smooth over important transition points, leading to poor decomposition.

### Mechanism 3
- Claim: The Decomposition Correlation Block (DCB) learns local correlation between trend and seasonal components through a single convolutional layer, allowing the model to weight their relative importance dynamically.
- Mechanism: DCB takes the separately processed period-trend and oscillation terms, concatenates them channel-wise, and applies a single 2D convolution with channel independence strategy to learn weighted combinations that reflect their local correlation.
- Core assumption: Trend and seasonal components are not independent but have local correlations that can be captured by a lightweight convolutional operation, and these correlations are sufficient for accurate forecasting.
- Evidence anchors: [section] states "the DCB is innovatively designed to combine the period-trend and oscillation terms using a convolutional kernel" and "the learned weights of the convolution kernel represent the importance of the period-trend and oscillation terms." [abstract] mentions "leverage the correlation between the trend and seasonal terms by the convolution layer."
- Break condition: If the relationship between trend and seasonal components is highly non-linear or involves long-range dependencies beyond the local neighborhood captured by the convolution kernel, DCB will fail to model these relationships adequately.

## Foundational Learning

- Concept: Fourier Frequency Transformation (FFT) for period extraction
  - Why needed here: FFT is used to identify the dominant periods in the time series, which are then combined to form the periodic window size. Understanding FFT helps in grasping how the model determines its structural parameters.
  - Quick check question: What is the primary output of an FFT when applied to a time series, and how does it relate to the periodic window construction in WinNet?

- Concept: Two-dimensional convolution and pooling operations
  - Why needed here: WinNet relies heavily on 2D convolution and pooling to process the restructured tensor. Understanding how these operations work in 2D space (rather than 1D temporal) is crucial for understanding the model's architecture.
  - Quick check question: How does 2D average pooling differ from 1D average pooling when applied to a time series tensor, and why is this difference important for WinNet's TDPD module?

- Concept: Channel independence vs. channel aggregation strategies
  - Why needed here: WinNet uses a channel independence strategy in the DCB, which processes each channel separately before combining them. This affects how the model learns correlations between components.
  - Quick check question: What is the difference between channel independence and channel aggregation strategies in convolutional networks, and why might channel independence be preferred in the DCB?

## Architecture Onboarding

- Component map: Input -> MLP layer (periodicity extraction) -> I2PE block (2D tensor formation) -> TDPD block (trend-seasonal decomposition) -> DCB block (correlation learning) -> Series Decoder (prediction) -> Output
- Critical path: The most critical path is I2PE -> TDPD -> DCB, as these three modules work together to transform raw temporal data into a form where a single convolution can capture the necessary information for forecasting.
- Design tradeoffs: Using only one convolutional layer significantly reduces computational complexity but requires careful preprocessing (I2PE and TDPD) to ensure the single layer receives properly structured information. The periodic window size is a hyperparameter that must balance capturing enough periods while keeping the tensor dimensions manageable.
- Failure signatures: Poor performance on datasets with irregular or non-stationary patterns may indicate that the periodic window approximation is inadequate. If the model overfits to training data, it may suggest the DCB is capturing noise rather than true correlations. Unexpected results when varying input length could indicate the I2PE transformation is sensitive to sequence length.
- First 3 experiments:
  1. Test the model with different periodic window sizes (e.g., 24, 36, 48) on a simple dataset to find the optimal balance between capturing periods and maintaining computational efficiency.
  2. Compare the model's performance with and without the TDPD decomposition to verify that separating trend and seasonal components improves forecasting accuracy.
  3. Vary the kernel size in the DCB (e.g., 3x3, 5x5, 7x7) to determine the optimal receptive field for capturing local correlations between components.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal window size for capturing multi-periodicity in time series forecasting across different domains?
- Basis in paper: [explicit] The paper mentions that the periodic window size is approximated as the least common multiple of multi-periods, but notes that different approximations (e.g., 24 vs 36) can affect performance.
- Why unresolved: The paper only explores a few window sizes (18, 24, 32) and doesn't systematically study the impact of window size on performance across diverse datasets.
- What evidence would resolve it: A comprehensive ablation study varying window sizes for multiple datasets across different domains, showing the relationship between window size and forecasting accuracy.

### Open Question 2
- Question: How does the proposed model's performance degrade as the input length increases beyond the tested range?
- Basis in paper: [inferred] The paper tests input lengths up to 720 but doesn't explore the performance limits as input length grows very large.
- Why unresolved: The paper focuses on comparing with baselines using fixed input lengths, but doesn't investigate scalability limits or performance trends at extreme input lengths.
- What evidence would resolve it: Testing the model with progressively larger input lengths (e.g., 1000, 2000, 5000) to identify performance degradation points and scalability boundaries.

### Open Question 3
- Question: What is the impact of different channel independence (CI) strategies on the model's performance and computational efficiency?
- Basis in paper: [explicit] The paper mentions using a channel independence strategy but doesn't explore alternative CI approaches or their effects.
- Why unresolved: The paper implements one CI strategy but doesn't compare it with other possible CI approaches or analyze the trade-offs between different strategies.
- What evidence would resolve it: Comparative experiments testing multiple CI strategies (e.g., different aggregation methods, varying degrees of independence) and analyzing their effects on both accuracy and efficiency metrics.

## Limitations

- The periodic window approximation may not capture irregular seasonal patterns effectively, particularly in datasets with holidays or special events
- The decomposition approach could struggle with abrupt structural breaks or regime changes in time series data
- The model lacks ablation studies on the impact of window size selection and channel independence strategy, making it difficult to assess the robustness of these design choices

## Confidence

- **High Confidence**: The architectural framework (I2PE → TDPD → DCB) is clearly described and represents a novel approach to CNN-based forecasting
- **Medium Confidence**: The experimental results showing 18.5% MSE and 12.0% MAE improvements over TimesNet, though limited to nine benchmark datasets
- **Low Confidence**: The claim that a single convolutional layer can capture all necessary temporal dependencies without deeper architectures, particularly for complex multivariate series

## Next Checks

1. Test WinNet on datasets with known irregular seasonal patterns (like electricity consumption with holidays) to evaluate periodic window approximation limits
2. Conduct ablation studies comparing channel independence vs. aggregation strategies in the DCB with statistical significance testing
3. Evaluate model performance across different input sequence lengths to determine sensitivity to temporal context