---
ver: rpa2
title: Efficiently Learning One-Hidden-Layer ReLU Networks via Schur Polynomials
arxiv_id: '2307.12840'
source_url: https://arxiv.org/abs/2307.12840
tags:
- learning
- algorithm
- relu
- where
- will
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the problem of PAC learning a linear combination\
  \ of k ReLU activations under the standard Gaussian distribution on R^d with respect\
  \ to the square loss. The authors give an efficient algorithm for this learning\
  \ task with sample and computational complexity (dk/\u03B5)^O(k), where \u03B5 0\
  \ is the target accuracy."
---

# Efficiently Learning One-Hidden-Layer ReLU Networks via Schur Polynomials

## Quick Facts
- **arXiv ID**: 2307.12840
- **Source URL**: https://arxiv.org/abs/2307.12840
- **Reference count**: 9
- **Key outcome**: Gives an efficient algorithm for PAC learning linear combinations of k ReLU activations with sample and computational complexity (dk/ε)^O(k), improving prior super-polynomial bounds.

## Executive Summary
This paper presents an efficient algorithm for learning one-hidden-layer ReLU networks under the standard Gaussian distribution. The algorithm achieves near-optimal sample and computational complexity within the class of Correlational Statistical Query algorithms. The key innovation is using tensor decomposition combined with Schur polynomial theory to identify a subspace where higher-order moments are small, enabling accurate approximation of the target function.

## Method Summary
The algorithm works by first estimating moment tensors up to order 4k, then using tensor decomposition to identify a k-dimensional subspace capturing the dominant directions. Hermite polynomials are used to represent the target function, and Schur polynomials enable bounding higher-order error tensors in terms of lower-order ones. The final hypothesis is constructed as a sum of Hermite polynomials in the learned subspace, achieving the desired accuracy ε.

## Key Results
- Achieves (dk/ε)^O(k) sample and computational complexity
- Improves upon prior work with super-polynomial dependence on k
- Near-optimal within the class of Correlational Statistical Query algorithms
- Makes essential use of Schur polynomials for error analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm identifies a subspace W where all O(k)-order Hermite moments are small in the orthogonal directions, enabling accurate approximation of the ReLU network.
- Mechanism: The algorithm uses tensor decomposition to compute moment tensors T_m = E[F(X)H_m(X)] for m up to 4k. These tensors capture the Hermite expansion of the ReLU network. By finding the k largest eigenvectors of the quadratic form Q(v) = Σ∥T_m v∥², the algorithm identifies a subspace W that captures the dominant directions of variation in the moment tensors.
- Core assumption: The Hermite moments of the target function F(x) = Σw_i ReLU(v_i·x) are sufficiently concentrated in the subspace spanned by the vectors v_i.
- Evidence anchors: [abstract] "Our algorithm uses tensor decomposition to identify a subspace such that all the O(k)-order moments are small in the orthogonal directions." [section] "To learn V, we use the method of moments. The t-th moment tensor of F, properly conditioned, is 0 if t > 1 is odd and proportional to Σw_i v_i^⊗t if t is even."

### Mechanism 2
- Claim: Schur polynomials enable bounding the higher-order error tensors in terms of lower-order ones.
- Mechanism: The theory of Schur polynomials provides a way to express higher-order tensor powers as linear combinations of lower-order tensor powers. This allows showing that if the error tensors are small for orders up to 4k, they remain small for all higher even orders up to the required D = O(1/ε⁴/³).
- Core assumption: The target function can be well-approximated by a low-degree Hermite polynomial in the learned subspace W.
- Evidence anchors: [abstract] "Its analysis makes essential use of the theory of Schur polynomials to show that the higher-moment error tensors are small given that the lower-order ones are." [section] "We use the theory of Schur polynomials to re-express the t-th order tensor in question here as a sum of not-too-many tensor powers of vi's and ProjW(vi)'s times the low-order versions of this tensor whose norms are small by construction."

### Mechanism 3
- Claim: The algorithm achieves near-optimal complexity within the class of Correlational Statistical Query (CSQ) algorithms.
- Mechanism: CSQ algorithms can choose any bounded query function and obtain estimates of its correlation with the labels. The algorithm's complexity matches the lower bound for CSQ algorithms, which is d^Ω(k).
- Core assumption: The CSQ model captures the computational power of many practical learning algorithms, including gradient descent on the square loss.
- Evidence anchors: [abstract] "Interestingly, the complexity of our algorithm is near-optimal within the class of Correlational Statistical Query algorithms." [section] "It can be readily verified that both our algorithm and the algorithms in the prior works [DKKZ20, DK20, CDG+23] are CSQ algorithms."

## Foundational Learning

- Hermite polynomials and Hermite tensors:
  - Why needed here: The algorithm uses Hermite analysis to decompose the target function and moment tensors. Hermite polynomials form an orthogonal basis for L²(R^d, N(0,I)), allowing efficient representation of the ReLU network and error bounds.
  - Quick check question: What is the inner product of the k-th and m-th normalized Hermite polynomials under the standard Gaussian distribution?

- Schur polynomials:
  - Why needed here: Schur polynomials provide a way to bound higher-order error tensors in terms of lower-order ones, enabling the analysis of the algorithm's accuracy.
  - Quick check question: How do Schur polynomials relate to the complete homogeneous symmetric polynomials via the first Jacobi-Trudi formula?

- Tensor decomposition and moment tensors:
  - Why needed here: The algorithm uses tensor decomposition to identify the subspace W and to compute moment tensors for error analysis. Understanding tensor operations and their properties is crucial for following the algorithm's mechanics.
  - Quick check question: How does the symmetrization operator relate to the projection of a tensor onto a subspace?

## Architecture Onboarding

- Component map: Moment tensor estimation -> Subspace identification -> Hermite component estimation -> Hypothesis construction
- Critical path: The most critical components are the moment tensor estimation and subspace identification, as errors here propagate to the final accuracy.
- Design tradeoffs: The algorithm trades off between the complexity of moment tensor estimation (which requires d^O(k) time) and the accuracy of the final hypothesis. Increasing the number of moments improves accuracy but increases runtime.
- Failure signatures: If the algorithm fails to identify the correct subspace W, the final error will be large. This can happen if the moment tensors are too small in the true subspace or if there is too much noise in the empirical estimates.
- First 3 experiments:
  1. Test the moment tensor estimation on a simple ReLU network with known Hermite coefficients.
  2. Verify the subspace identification on a synthetic dataset where the true subspace is known.
  3. Check the final error on a small ReLU network with positive weights, where the ground truth is easier to compute.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the runtime of the algorithm be improved to poly(d, k, 1/ε) while maintaining the near-optimal CSQ lower bound?
- Basis in paper: [inferred] The paper notes that the complexity of the algorithm is near-optimal within the class of CSQ algorithms, but there is no known inherent obstacle ruling out a poly(d, k, 1/ε) time algorithm.
- Why unresolved: The paper does not provide any evidence or proof that such an improvement is impossible.
- What evidence would resolve it: A proof showing that any algorithm achieving poly(d, k, 1/ε) runtime must violate the CSQ lower bound, or an algorithm with the desired runtime and complexity.

### Open Question 2
- Question: Can the assumption on the sum of absolute values of weights be relaxed to allow for more general weight configurations?
- Basis in paper: [explicit] The paper notes that the assumption that the sum of the absolute values of the weights wi be bounded is somewhat strong, but turns out to be necessary.
- Why unresolved: The paper does not provide any evidence or proof that the assumption cannot be relaxed.
- What evidence would resolve it: A proof showing that the assumption is necessary for the algorithm to work, or an algorithm that works without this assumption.

### Open Question 3
- Question: Can the algorithm be adapted to output a nearly proper hypothesis, which is a sum of slightly smoothed versions of ReLUs, as suggested in the paper?
- Basis in paper: [explicit] The paper mentions that the hypothesis returned is not a one-hidden-layer ReLU network, but a somewhat more complicated function, and suggests that with additional work, the algorithm could be adapted to output a nearly proper hypothesis.
- Why unresolved: The paper does not provide any details or proof of how to adapt the algorithm to output a nearly proper hypothesis.
- What evidence would resolve it: A detailed description of the adaptation process and a proof that the resulting hypothesis is indeed a sum of slightly smoothed versions of ReLUs.

## Limitations
- The algorithm's d^O(k) complexity, while improved, still presents computational challenges for large k.
- The analysis relies heavily on the assumption that the target function can be well-approximated by a low-degree Hermite polynomial in the learned subspace.
- Numerical stability concerns in tensor operations and Schur polynomial computations are not fully addressed.

## Confidence

- **High Confidence**: The algorithm's near-optimal complexity within the CSQ model and the core mechanism of using tensor decomposition for subspace identification are well-supported by both theoretical analysis and the connections to existing work.
- **Medium Confidence**: The use of Schur polynomials to bound higher-order error tensors is theoretically sound, but the practical implications of numerical errors in tensor computations could affect real-world performance.
- **Low Confidence**: The claim about near-optimality within CSQ algorithms assumes that no non-CSQ algorithms can achieve better complexity, which remains an open question in the field.

## Next Checks

1. **Numerical Stability Analysis**: Conduct experiments to quantify how floating-point errors in tensor operations affect the algorithm's accuracy, particularly for larger values of k and d.

2. **Generalization Beyond Positive Weights**: Test the algorithm on ReLU networks with both positive and negative weights, as the current analysis primarily focuses on the positive weight case where Hermite analysis is simpler.

3. **Comparison with Gradient-Based Methods**: Implement and compare the algorithm against standard gradient descent approaches on ReLU networks to better understand the practical trade-offs between the CSQ approach and optimization-based methods.