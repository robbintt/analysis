---
ver: rpa2
title: 'SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation'
arxiv_id: '2308.04911'
source_url: https://arxiv.org/abs/2308.04911
tags:
- prompt
- tuning
- data
- performance
- labeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of limited labeled medical image
  data and high annotation costs in deep learning-based medical image analysis. It
  proposes a novel framework that combines selective labeling with prompt tuning (SLPT)
  to improve performance in label-limited scenarios.
---

# SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation

## Quick Facts
- arXiv ID: 2308.04911
- Source URL: https://arxiv.org/abs/2308.04911
- Reference count: 33
- Key outcome: Achieves 94% of full-data performance by labeling only 5% of data for liver tumor segmentation

## Executive Summary
This paper addresses the challenge of limited labeled medical image data and high annotation costs in deep learning-based medical image analysis. It proposes a novel framework that combines selective labeling with prompt tuning (SLPT) to improve performance in label-limited scenarios. The core idea involves using a feature-aware prompt updater to guide prompt tuning and a Tandem Selective Labeling (TESLA) strategy that includes unsupervised diversity selection and supervised selection using prompt-based uncertainty. The method is evaluated on liver tumor segmentation and achieves state-of-the-art performance, outperforming traditional fine-tuning with only 6% of tunable parameters.

## Method Summary
The SLPT framework combines selective labeling with prompt tuning to address label-limited medical image segmentation. It introduces a feature-aware prompt updater (FPU) that enables effective prompt tuning in deep layers of a frozen pre-trained model. The method also proposes a diversified visual prompt tuning strategy that generates multiple prompts to provide discrepant predictions for uncertainty estimation. These components work together with a Tandem Selective Labeling (TESLA) strategy that selects the most informative samples through unsupervised diversity selection and supervised uncertainty selection. The framework is evaluated on liver tumor segmentation, demonstrating significant performance gains while reducing annotation costs.

## Key Results
- Achieves 94% of full-data performance by labeling only 5% of the data
- Outperforms traditional fine-tuning using only 6% of tunable parameters
- Demonstrates state-of-the-art performance on liver tumor segmentation

## Why This Works (Mechanism)

### Mechanism 1
The feature-aware prompt updater (FPU) enables effective prompt tuning in deep layers of a frozen pre-trained model. FPU receives feature maps and prompts from the previous layer, concatenates them, and processes them through two parallel branches - one for updating features using ASPP and SE modules for multi-scale context and channel attention, and another for updating prompts using depth-separable convolution. This design allows the model to effectively update both features and prompts while keeping the pre-trained backbone frozen.

### Mechanism 2
The diversified visual prompt tuning strategy generates diverse prompts that produce discrepant predictions for uncertainty estimation. Multiple prompts are generated from a meta prompt through different upsampling and convolution operations, with a diversity loss term that maximizes cosine similarity differences between prompts. Each prompt is paired with different data augmentations, heads, and Tversky loss hyperparameters. This approach ensures that different initialization and optimization paths for prompts will lead to diverse predictions that capture different aspects of uncertainty.

## Foundational Learning

### Prompt Tuning
- Why needed: To adapt pre-trained models to new tasks with minimal trainable parameters
- Quick check: Verify that only a small fraction of parameters (6%) are being tuned compared to full fine-tuning

### Selective Labeling
- Why needed: To reduce annotation costs by intelligently selecting which samples to label
- Quick check: Confirm that performance is maintained while labeling only 5% of data

### Uncertainty Estimation
- Why needed: To identify informative samples for labeling based on model uncertainty
- Quick check: Verify that prompt-based uncertainty correlates with sample informativeness

## Architecture Onboarding

### Component Map
Input CT scans → Pre-trained Model → Feature-aware Prompt Updater → Multiple Prompt Branches → Diverse Predictions → Uncertainty Estimation → TESLA → Selected Samples → Training

### Critical Path
Pre-trained model → FPU → Prompt tuning → Uncertainty estimation → TESLA selection → Training loop

### Design Tradeoffs
- Complexity vs. performance: Multiple prompts and FPU add complexity but improve performance
- Parameter efficiency vs. accuracy: Prompt tuning uses fewer parameters than fine-tuning
- Annotation cost vs. model quality: Selective labeling reduces costs while maintaining performance

### Failure Signatures
- Poor performance if prompts fail to capture task-specific features
- Overfitting if too few samples are labeled despite selective labeling
- Suboptimal uncertainty estimation if prompts are not sufficiently diverse

### 3 First Experiments
1. Test FPU performance with different ASPP configurations
2. Evaluate diversity loss impact on prompt generation
3. Compare TESLA performance with individual selection strategies

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of SLPT compare to other selective labeling methods on different medical imaging tasks beyond liver tumor segmentation? The paper only evaluates SLPT on liver tumor segmentation, so testing its effectiveness on other medical imaging tasks would provide evidence for generalizability.

### Open Question 2
What is the optimal number of prompts to use in the diversified visual prompt tuning strategy? The paper mentions using K=3 prompts but does not explore the impact of varying the number of prompts on performance.

### Open Question 3
How does the choice of pre-trained model affect the performance of SLPT? The paper uses a pre-trained model for liver segmentation but does not explore the impact of using different pre-trained models on SLPT's performance.

## Limitations

- Evaluation scope limited to liver tumor segmentation, limiting generalizability claims
- Reliance on a pre-trained liver segmentation model creates dependency on task-specific pre-training data
- Complex framework with multiple interconnected components may have cascading failure points

## Confidence

**High Confidence:** The core premise that prompt tuning with selective labeling can reduce annotation costs while maintaining performance. This is well-supported by experimental results.

**Medium Confidence:** The effectiveness of the Feature-aware Prompt Updater in deep layers. While the paper provides theoretical justification and demonstrates improved performance, specific architectural details are not fully specified.

**Medium Confidence:** The Tandem Selective Labeling strategy's contribution to performance. The paper claims this dual-stage approach is superior, but the relative contributions of each component are not clearly isolated in the experiments.

## Next Checks

1. Conduct an ablation study of FPU components to systematically quantify the individual contributions of ASPP, SE modules, and depth-separable convolution.

2. Apply the SLPT framework to a different organ system (e.g., brain tumor segmentation) using a pre-trained model from a related but distinct task to assess generalizability.

3. Measure and report training time, memory consumption, and parameter counts for the full SLPT framework versus simpler alternatives to evaluate the trade-off between performance gains and computational costs.