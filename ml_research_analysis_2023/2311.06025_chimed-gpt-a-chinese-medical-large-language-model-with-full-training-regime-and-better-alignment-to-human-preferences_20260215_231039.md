---
ver: rpa2
title: 'ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime
  and Better Alignment to Human Preferences'
arxiv_id: '2311.06025'
source_url: https://arxiv.org/abs/2311.06025
tags:
- medical
- llms
- chimed-gpt
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ChiMed-GPT is a Chinese medical LLM designed to address the limitations
  of existing models in handling domain-specific knowledge and long context. It employs
  a comprehensive training regime with pre-training, supervised fine-tuning, and reinforcement
  learning from human feedback, incorporating data augmentation and rejection sampling
  for improved alignment with human preferences.
---

# ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences

## Quick Facts
- arXiv ID: 2311.06025
- Source URL: https://arxiv.org/abs/2311.06025
- Reference count: 10
- Key outcome: Chinese medical LLM achieving F1 scores of 40.82 (CCKS-2019) and 41.04 (ChiMST) for NER with reduced bias scores on mental illness attitude scales

## Executive Summary
ChiMed-GPT is a Chinese medical large language model designed to overcome limitations of existing models in handling domain-specific knowledge and long context. It employs a comprehensive training regime with pre-training, supervised fine-tuning, and reinforcement learning from human feedback, incorporating data augmentation and rejection sampling for improved alignment with human preferences. The model demonstrates superior performance over general domain LLMs on tasks such as information extraction, question answering, and dialogue generation, while showing better alignment with responsible AI development through lower bias scores.

## Method Summary
ChiMed-GPT uses a three-stage training regime: pre-training on the Chinese Medical Dataset (CMD) with 214M tokens of medical encyclopedia and textbook data, supervised fine-tuning on medical QA and dialogue datasets (ChiMed, CMD SFT, MC, MedDialog, Safety-Prompts), and reinforcement learning from human feedback with rejection sampling fine-tuning using CMD Reward augmented with GPT-4 and GPT-3.5-Turbo responses. The model features an extended context length of 4,096 tokens and is built on the Ziya-13B-v2 foundation model.

## Key Results
- Achieved F1 score of 40.82 on CCKS-2019 and 41.04 on ChiMST for named entity recognition
- Demonstrated lower bias scores on mental illness attitude scales (CAMI and MICA) compared to other models
- Showed superior performance on Chinese medical question answering benchmarks (C-Eval, CMMLU, MedQA) with BLEU and ROUGE scores on dialogue generation tasks

## Why This Works (Mechanism)

### Mechanism 1
Data augmentation and rejection sampling fine-tuning improve reward model discriminative capability and reduce overfitting. Binary classification in reward modeling (accepted vs rejected answers) is too easy, leading to rapid overfitting (98% accuracy in ~200 steps). Adding intermediate responses (GPT-4, GPT-3.5-Turbo) creates a ranking task with more nuanced distinctions, forcing the reward model to learn finer-grained preferences and reducing overfitting.

### Mechanism 2
Comprehensive training regime (pre-training, SFT, RLHF) outperforms SFT-only approaches for domain-specific LLMs. Pre-training on domain data builds foundational knowledge, SFT teaches instruction following, and RLHF aligns outputs with human preferences. This multi-stage approach addresses limitations of SFT-only models that struggle with domain knowledge and human preference alignment.

### Mechanism 3
Extended context length (4,096 tokens) improves performance on long medical texts with strong contextual coherence. Medical texts often contain lengthy, contextually coherent content. Standard 2,048 token limits restrict modeling capability, while 4,096 tokens allow better processing of extended medical documents and dialogues.

## Foundational Learning

- Concept: Pre-training on domain-specific corpora builds foundational knowledge representations.
  - Why needed here: Medical domain requires specialized knowledge that general pre-training lacks; CMD (pre-train) provides 214M tokens of medical encyclopedia and textbook data.
  - Quick check question: What dataset is used for pre-training and what type of medical content does it contain?

- Concept: Rejection sampling fine-tuning aligns model outputs with human preferences through reward-based selection.
  - Why needed here: Standard RLHF methods may not efficiently distinguish between acceptable and preferred outputs; rejection sampling selects top-k responses from sampled outputs for fine-tuning.
  - Quick check question: How does rejection sampling differ from standard proximal policy optimization in this implementation?

- Concept: Bias evaluation using standardized scales measures model alignment with ethical medical communication.
  - Why needed here: Medical LLMs must avoid discriminatory responses that could harm patients; CAMI and MICA scales quantify bias toward mental illness patients.
  - Quick check question: What two scales are used to measure bias in CHIMED-GPT and what domains do they assess?

## Architecture Onboarding

- Component map: Foundation model (Ziya-13B-v2) → Pre-training (CMD pre-train) → SFT (ChiMed, CMD SFT, MC, MedDialog, Safety-Prompts) → RLHF (CMD Reward with augmentation + rejection sampling)
- Critical path: Data preparation → Pre-training → SFT → Reward model training → Rejection sampling fine-tuning
- Design tradeoffs: Longer context (4,096) vs. computational cost; comprehensive training vs. training time; data augmentation vs. potential noise introduction
- Failure signatures: Overfitting in reward model (98% accuracy too quickly), poor instruction following in SFT, biased outputs in bias evaluation, degraded performance on long-context tasks
- First 3 experiments:
  1. Train reward model on original CMD Reward vs augmented version and compare validation accuracy curves
  2. Compare CHIMED-GPT performance on CCKS-2019 with different context lengths (2,048 vs 4,096)
  3. Evaluate bias scores on CAMI/MICA scales after each training stage to measure RLHF impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the enlarged context length of 4,096 tokens in CHIMED-GPT impact its performance on long medical texts compared to models with shorter context lengths?
- Basis in paper: [explicit] The paper states that CHIMED-GPT has a context length of 4,096 tokens, which exceeds the context size of existing medical LLMs, and aims to provide better text processing ability for medical applications.
- Why unresolved: The paper does not provide direct comparisons of CHIMED-GPT's performance on long medical texts against models with shorter context lengths.
- What evidence would resolve it: Conducting experiments comparing CHIMED-GPT's performance on long medical texts with models that have shorter context lengths would provide evidence to resolve this question.

### Open Question 2
- Question: How does the data augmentation technique used in training the reward model affect the overall performance and alignment of CHIMED-GPT with human preferences?
- Basis in paper: [explicit] The paper mentions that data augmentation is used to produce high-quality human preference data for reward model training and that it is proved to be more efficient than standard proximal policy optimization (PPO).
- Why unresolved: The paper does not provide a detailed analysis of the impact of data augmentation on the overall performance and alignment of CHIMED-GPT with human preferences.
- What evidence would resolve it: Conducting experiments comparing the performance and alignment of CHIMED-GPT with and without data augmentation would provide evidence to resolve this question.

### Open Question 3
- Question: How does the rejection sampling fine-tuning technique used in RLHF contribute to the improved performance of CHIMED-GPT on tasks such as information extraction, question answering, and dialogue generation?
- Basis in paper: [explicit] The paper mentions that rejection sampling fine-tuning is employed to learn from the data and is proved to be more efficient than standard proximal policy optimization (PPO).
- Why unresolved: The paper does not provide a detailed analysis of the contribution of rejection sampling fine-tuning to the improved performance of CHIMED-GPT on various tasks.
- What evidence would resolve it: Conducting experiments comparing the performance of CHIMED-GPT with and without rejection sampling fine-tuning on information extraction, question answering, and dialogue generation tasks would provide evidence to resolve this question.

## Limitations

- Data provenance and quality uncertainties regarding the CMD dataset's 369,800 medical encyclopedia documents and 8,475 textbook articles
- Evaluation scope constraints limited to Chinese medical benchmarks without cross-linguistic validation
- Mechanism validation gaps lacking direct ablation studies on reward model augmentation effectiveness

## Confidence

- High confidence: Domain-specific performance improvements on Chinese medical benchmarks (F1 scores of 40.82 on CCKS-2019 and 41.04 on ChiMST)
- Medium confidence: Claims about superior alignment with human preferences through RLHF methodology
- Low confidence: General superiority claims over all existing medical LLMs without comprehensive comparisons

## Next Checks

1. **Ablation study on reward model augmentation**: Compare CHIMED-GPT trained with standard binary reward modeling versus the augmented approach with GPT-4/GPT-3.5-Turbo responses. Measure both reward model training convergence patterns and downstream task performance to quantify the specific contribution of the augmentation technique.

2. **Cross-linguistic generalization test**: Evaluate CHIMED-GPT on English medical benchmarks (e.g., MedQA-USMLE) and compare performance with Chinese medical benchmarks to assess whether the comprehensive training regime produces generalizable medical knowledge or remains Chinese-language specific.

3. **Bias evaluation across medical specialties**: Extend the CAMI/MICA bias analysis to multiple medical specialties (cardiology, oncology, pediatrics) using specialty-specific prompts to determine whether the observed lower bias scores generalize across different medical domains or are limited to mental health contexts.