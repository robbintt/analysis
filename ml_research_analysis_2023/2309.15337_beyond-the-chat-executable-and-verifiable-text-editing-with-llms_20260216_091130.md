---
ver: rpa2
title: 'Beyond the Chat: Executable and Verifiable Text-Editing with LLMs'
arxiv_id: '2309.15337'
source_url: https://arxiv.org/abs/2309.15337
tags:
- editing
- chat
- edit
- participants
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InkSync introduces a new text editing interface that provides executable,
  LLM-generated edit suggestions directly within the document, along with a three-stage
  Warn-Verify-Audit framework to mitigate factual errors. Unlike standard chat-based
  LLM editors, InkSync enables transparent tracking of LLM-originated content and
  offers authors fine-grained control over accepting or dismissing edits.
---

# Beyond the Chat: Executable and Verifiable Text-Editing with LLMs

## Quick Facts
- arXiv ID: 2309.15337
- Source URL: https://arxiv.org/abs/2309.15337
- Reference count: 40
- Primary result: InkSync's inline executable edits with Warn-Verify-Audit framework significantly improve editing efficiency and factual accuracy compared to chat-based LLM editors.

## Executive Summary
InkSync introduces a novel text editing interface that moves beyond traditional chat-based LLM editors by providing executable, inline edit suggestions directly within the document. The system implements a three-stage Warn-Verify-Audit framework to mitigate factual errors introduced by LLM-generated content. Through two usability studies, InkSync demonstrated significant improvements in editing efficiency and error reduction compared to baseline conversational interfaces, with the full Warn-Verify-Audit pipeline nearly doubling the proportion of detected inaccurate edits.

## Method Summary
The study employed within-participant usability experiments with 55 participants (Study 1) and 35 participants (Study 2), comparing InkSync against manual editing and chat-based interfaces. Participants performed email customization tasks using provided templates, personas, and travel destinations. The evaluation measured editing efficiency, error rates (Objective A for typos/informal phrases, Objective B for custom recommendations), factual accuracy, and user preferences through Likert-scale surveys. The InkSync system used GPT-4 to generate executable JSON edits, with inline diff visualization and a Warn-Verify-Audit framework for error detection.

## Key Results
- InkSync achieved significantly higher Objective A and B scores than chat-based interfaces, with roughly half as many errors in the full four-component setup.
- The Warn-Verify-Audit framework nearly doubled the proportion of detected inaccurate edits compared to no verification.
- Users found InkSync's suggestions easier to understand and integrate, with the highest user preference scores achieved by the complete system.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Executable edits within the document reduce cognitive load compared to chat-based rewriting.
- **Mechanism:** Inline edits enable visual comparison without context switching, lowering mental overhead of reconciling versions.
- **Core assumption:** Authors can efficiently process inline diff markers and accept/dismiss without losing flow.
- **Evidence anchors:** [abstract]: "standard chat-based conversational interfaces do not support transparency and verifiability"; [section 4.1.2]: "hover over an edit suggestion to view an overlaid menu to Accept or Dismiss".
- **Break condition:** If the number of simultaneous inline suggestions exceeds visual parsing capacity, authors may miss or misinterpret changes.

### Mechanism 2
- **Claim:** The Warn-Verify-Audit pipeline increases detection of factual inaccuracies introduced by LLM edits.
- **Mechanism:** Warning flags new information, verification prompts external search, and audit allows retrospective tracing, each step catching errors the previous may miss.
- **Core assumption:** Authors will engage with verification prompts and not ignore warnings due to familiarity bias.
- **Evidence anchors:** [abstract]: "Warn authors when a suggested edit introduces new information, help authors Verify the new information's accuracy"; [section 6.4.3]: "Warn and Verify features almost doubles the proportion of inaccurate edits that are successfully dismissed".
- **Break condition:** If warnings are too frequent or intrusive, authors may develop warning fatigue and skip verification.

### Mechanism 3
- **Claim:** Specialized markers complement conversational edits by targeting low-level quality improvements.
- **Mechanism:** Proactive typo, grammar, and tone markers run in background, catching errors conversational edits might miss due to scope.
- **Core assumption:** Markers operate independently without interfering with conversational edit context.
- **Evidence anchors:** [section 5.3.1]: "interfaces with the Markers component... achieve the best performance on Objective A, with roughly half as many errors"; [section 4.2.3]: "Markers component... run periodically in the background".
- **Break condition:** If marker suggestions conflict with conversational edits, authors may experience confusion or rollback errors.

## Foundational Learning

- **Concept:** Executable edit JSON schema with original_text, replace_text, component, new_info
  - Why needed here: Provides structured, unambiguous way for LLM to communicate edits that editor can render inline.
  - Quick check question: What field must match an exact string in the document to be valid?

- **Concept:** Levenshtein alignment for visualizing edit differences
  - Why needed here: Converts JSON replacements into color-coded inline diffs the author can parse quickly.
  - Quick check question: Which algorithm aligns original and replacement text to generate the diff display?

- **Concept:** Character-level provenance tracing via iterative Levenshtein alignment
  - Why needed here: Tracks system-generated text through multiple edits so audit can highlight all LLM-originated content.
  - Quick check question: How does the system handle a case where an accepted edit is partially deleted later?

## Architecture Onboarding

- **Component map:** Frontend editor with inline rendering → LLM Service with prompt templates → Verification Service for new_info queries → Audit Service for character alignment history
- **Critical path:** Author → Query → LLM → JSON edits → Frontend rendering → Accept/Dismiss → Document update
- **Design tradeoffs:** Inline edits vs. chat summaries (transparency vs. verbosity); real-time vs. batched marker runs (latency vs. coverage)
- **Failure signatures:** Invalid JSON from LLM (system error); alignment failure (edit becomes unexecutable); audit highlighting mismatches (provenance tracking error)
- **First 3 experiments:**
  1. Swap GPT-4 with GPT-3.5-Turbo and measure change in invalid JSON rate and edit quality.
  2. Disable the new_info flag in prompts and compare error detection rate in audit vs. with flag.
  3. Toggle background marker frequency and measure impact on Objective A scores.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does InkSync's performance and user experience compare when integrated with different LLM models beyond GPT-4, particularly smaller or open-source alternatives?
- **Basis in paper:** [explicit] The paper states InkSync is "LLM agnostic" and suggests future work could integrate "other closed and open-source LLMs" to measure model effects on user interaction.
- **Why unresolved:** The current study only tested with GPT-4 due to its superior performance and lower formatting error rate, leaving the comparative effectiveness of other models unexplored.
- **What evidence would resolve it:** Conducting usability studies with multiple LLM models (e.g., GPT-3.5, LLaMA, Claude) integrated into InkSync and comparing user success rates, error detection, and subjective preferences across models.

### Open Question 2
- **Question:** Does increasing the temperature parameter in LLMs lead to more diverse and creative suggestions without significantly increasing factual inaccuracies in InkSync?
- **Basis in paper:** [inferred] The paper notes that Study 1 found LLM suggestions reduced creativity compared to manual editing, and mentions temperature as a controllable parameter that could encourage diversity, but doesn't explore this experimentally.
- **Why unresolved:** The current implementation uses default temperature settings optimized for accuracy, leaving the trade-off between creativity and factual reliability unexplored.
- **What evidence would resolve it:** Running controlled experiments with different temperature settings while measuring both suggestion diversity (using divergence metrics) and factual accuracy rates to find an optimal balance.

### Open Question 3
- **Question:** How effective would automated verification tools be in reducing factual errors compared to the current human-in-the-loop verification approach in InkSync?
- **Basis in paper:** [inferred] The paper mentions that future work could use frameworks like ToolFormer to enable LLMs to perform preliminary verification, suggesting potential for automated approaches, but doesn't test this.
- **Why unresolved:** The current verification framework relies entirely on human judgment and external search, without exploring how much automation could assist without compromising accuracy.
- **What evidence would resolve it:** Comparing InkSync's current human verification workflow against hybrid approaches where LLMs provide preliminary accuracy assessments or automated fact-checking against knowledge bases, measuring both accuracy rates and user trust.

## Limitations

- The user studies focused on email customization tasks, which may not generalize to broader editing scenarios like technical writing or creative composition.
- Reliance on GPT-4 introduces potential variability that wasn't fully characterized - different LLM models might produce different error patterns or edit quality.
- The verification mechanism depends on authors actually engaging with external search, but the study doesn't measure how often users skipped this step or what factors influenced their decision to verify.

## Confidence

- **High** confidence in the mechanism that inline executable edits reduce cognitive load compared to chat-based interfaces, supported by user preference data and objective efficiency metrics.
- **Medium** confidence in the Warn-Verify-Audit pipeline's effectiveness in catching factual errors, since detection rates improved significantly but relied on simulated errors rather than naturally occurring ones.
- **Low** confidence in the generalizability of marker component benefits across different writing domains, as evaluation was limited to email editing tasks.

## Next Checks

1. **Cross-domain validation**: Test InkSync with technical documentation and creative writing tasks to assess whether the inline edit approach maintains efficiency benefits across different content types.

2. **Verification engagement analysis**: Instrument the system to log when users skip verification prompts and conduct follow-up interviews to understand the decision factors, particularly for frequent vs. infrequent warnings.

3. **Model dependency evaluation**: Repeat the studies using GPT-3.5-Turbo and Claude to measure how sensitive the error detection rates and edit quality are to the underlying LLM model choice.