---
ver: rpa2
title: Posterior Sampling with Delayed Feedback for Reinforcement Learning with Linear
  Function Approximation
arxiv_id: '2310.18919'
source_url: https://arxiv.org/abs/2310.18919
tags:
- lemma
- regret
- linear
- then
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies reinforcement learning with delayed feedback
  in linear Markov decision processes using posterior sampling. The authors propose
  Delayed-PSVI, which injects Gaussian noise via posterior sampling to explore the
  value function space, and Delayed-LPSVI, which uses Langevin dynamics for approximate
  sampling.
---

# Posterior Sampling with Delayed Feedback for Reinforcement Learning with Linear Function Approximation

## Quick Facts
- arXiv ID: 2310.18919
- Source URL: https://arxiv.org/abs/2310.18919
- Reference count: 40
- Key outcome: O(√(d³H³T) + d²H²E[τ]) worst-case regret under sub-exponential delays

## Executive Summary
This paper addresses reinforcement learning with delayed feedback in linear Markov decision processes using posterior sampling. The authors propose two algorithms: Delayed-PSVI, which uses exact Gaussian posterior sampling for exploration, and Delayed-LPSVI, which employs Langevin dynamics for approximate sampling. Both achieve the same theoretical regret bounds but differ in computational complexity, with Delayed-LPSVI offering significant savings in high-dimensional settings. The key innovation is maintaining optimism under delayed feedback through multi-round sampling and careful decomposition of the covariance matrix.

## Method Summary
The paper studies reinforcement learning with delayed feedback in linear MDPs using posterior sampling. The authors propose two algorithms: Delayed-PSVI uses exact Gaussian posterior sampling with multi-round optimism to maintain high-probability optimistic Q-values, while Delayed-LPSVI uses Langevin Monte Carlo to approximate the posterior distribution without expensive matrix inversion. Both algorithms achieve O(√(d³H³T) + d²H²E[τ]) worst-case regret under sub-exponential delays by decomposing the covariance matrix into delayed and non-delayed components and carefully handling the estimation error.

## Key Results
- Both Delayed-PSVI and Delayed-LPSVI achieve O(√(d³H³T) + d²H²E[τ]) worst-case regret under sub-exponential delays
- Delayed-LPSVI reduces computational complexity from O(d³HK) to O(dHK) through Langevin dynamics approximation
- Empirical results show both methods outperform UCB-based baselines under multinomial, Poisson, and Pareto delay distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Posterior sampling maintains optimism via anti-concentration in Gaussian posterior noise
- Mechanism: At each step, sampling from N(bw, ν²(Ω)⁻¹) yields optimistic Q-values with constant probability due to Gaussian anti-concentration. Multi-round sampling ensures high-probability optimism across all steps
- Core assumption: The posterior over weights is approximately Gaussian and centered near the true optimal parameter
- Evidence anchors: [abstract]: "Delayed-PSVI, an optimistic value-based algorithm that effectively explores the value function space via noise perturbation with posterior sampling."

### Mechanism 2
- Claim: Langevin dynamics approximates the posterior distribution without requiring matrix inversion
- Mechanism: LMC iteratively updates weights using gradient of delayed loss plus Gaussian noise, converging to a distribution proportional to exp(−L(w)/γ), which approximates the true posterior
- Core assumption: The delayed loss function is smooth enough for LMC convergence under chosen step sizes
- Evidence anchors: [abstract]: "we incorporate a gradient-based approximate sampling scheme via Langevin dynamics for Delayed-LPSVI."

### Mechanism 3
- Claim: Delayed feedback penalty is additive and bounded by O(d²H²E[τ])
- Mechanism: Decomposition of (Ωk)⁻¹ into full and delayed components allows separation of regret into non-delayed exploration term and a delay-dependent penalty
- Core assumption: Delays are sub-exponential, enabling concentration bounds on max delay
- Evidence anchors: [abstract]: "Both algorithms achieve O(√(d³H³T) + d²H²E[τ]) worst-case regret under sub-exponential delays."

## Foundational Learning

- Concept: Linear Markov Decision Processes (Linear MDPs)
  - Why needed here: The paper assumes the value functions are linear in feature maps, enabling efficient exploration via linear algebra
  - Quick check question: What property of Linear MDPs allows Q-functions to be parameterized as ϕ(s,a)ᵀw?

- Concept: Posterior Sampling and Thompson Sampling
  - Why needed here: The algorithms use posterior sampling to inject exploration noise while maintaining optimism
  - Quick check question: How does multi-round sampling ensure high-probability optimism in posterior sampling?

- Concept: Langevin Monte Carlo (LMC)
  - Why needed here: LMC approximates the posterior without expensive matrix inversion, reducing computational complexity
  - Quick check question: What is the role of the noise term √2ηγϵ in the LMC update rule?

## Architecture Onboarding

- Component map:
  - Delayed-PSVI: Gaussian posterior sampling + multi-round optimism + delayed feedback handling
  - Delayed-LPSVI: LMC-based approximate sampling + same regret guarantees + reduced computation
  - Core regret decomposition: Optimism term + estimation error term + delay penalty term

- Critical path:
  1. At each episode, sample initial state
  2. For each time step (backward), perform posterior sampling (PSVI) or LMC (LPSVI)
  3. Construct optimistic Q-function via max over samples
  4. Select action greedily within truncation bounds
  5. Collect delayed feedback and update design matrices

- Design tradeoffs:
  - PSVI: Exact Gaussian sampling but O(d³) matrix inversion
  - LPSVI: Approximate sampling with O(dHK) complexity but requires careful LMC tuning
  - Multi-round sampling: Ensures optimism but increases computation by factor M

- Failure signatures:
  - Regret blows up: Likely due to poor delay handling or LMC convergence issues
  - Slow convergence: May need to adjust noise scaling or LMC step size
  - Numerical instability: Check matrix conditioning in posterior updates

- First 3 experiments:
  1. Run Delayed-PSVI on synthetic linear MDP with small d and H to verify O(√d³H³T) scaling
  2. Compare Delayed-PSVI vs Delayed-LPSVI on moderate d to measure computational savings
  3. Test both algorithms under Poisson vs Pareto delays to validate robustness claims

## Open Questions the Paper Calls Out
The paper identifies several open questions including improving horizon dependence in regret bounds, extending results to non-linear function approximation, and handling adversarial delay distributions. The authors note that achieving tighter horizon dependence (e.g., O(√H) instead of O(H³/²)) for posterior sampling algorithms with delayed feedback remains an open challenge. They also suggest that extending these methods to deep RL settings with general function approximation could be a promising direction for future research.

## Limitations
- The theoretical guarantees rely heavily on the sub-exponential delay assumption, which may not hold in real-world scenarios with heavy-tailed or adversarial delays
- The LMC-based approximation requires careful hyperparameter tuning (step size η, noise scale γ) with limited practical guidance provided
- Empirical evaluation uses synthetic environments that may not fully capture the complexity of real-world delayed feedback scenarios

## Confidence
- High Confidence: The regret decomposition framework and the O(√(d³H³T) + d²H²E[τ]) bound structure are well-established in the delayed RL literature
- Medium Confidence: The LMC approximation theory is sound, but practical convergence guarantees depend on problem-specific parameters
- Medium Confidence: Empirical results show consistent improvements over baselines, but the synthetic nature of test environments limits generalizability

## Next Checks
1. **Robustness to Delay Distributions:** Test Delayed-LPSVI under heavy-tailed delay distributions (e.g., Pareto with shape parameter < 2) to verify the sub-exponential assumption's practical implications
2. **Hyperparameter Sensitivity Analysis:** Systematically vary LMC parameters (η, γ, N) to identify stable operating regions and failure modes
3. **Real-World Transfer:** Evaluate the algorithms on a delayed-feedback bandit problem with real-world data (e.g., recommendation systems) to assess practical performance beyond synthetic environments