---
ver: rpa2
title: 'Zero Grads: Learning Local Surrogate Losses for Non-Differentiable Graphics'
arxiv_id: '2308.05739'
source_url: https://arxiv.org/abs/2308.05739
tags:
- surrogate
- optimization
- differentiable
- rendering
- graphics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ZeroGrads is a framework that learns differentiable neural surrogates
  for non-differentiable graphics problems. It smooths the loss via convolution, fits
  a local neural network surrogate during optimization, and uses importance sampling
  to reduce gradient estimation variance.
---

# Zero Grads: Learning Local Surrogate Losses for Non-Differentiable Graphics

## Quick Facts
- arXiv ID: 2308.05739
- Source URL: https://arxiv.org/abs/2308.05739
- Reference count: 38
- Primary result: ZeroGrads learns differentiable neural surrogates for non-differentiable graphics problems, scaling to 35k+ variables with improved optimization performance.

## Executive Summary
ZeroGrads is a framework for optimizing non-differentiable graphics pipelines by learning differentiable neural surrogate models. The method addresses the challenge of gradient-based optimization in graphics where forward models (rendering, procedural generation, animation) are inherently non-differentiable. By combining Gaussian smoothing of the loss landscape, local neural network surrogate fitting, and importance sampling for variance reduction, ZeroGrads enables efficient gradient-based optimization of high-dimensional parameters in graphics applications.

## Method Summary
ZeroGrads learns a differentiable neural surrogate that approximates the loss landscape of non-differentiable graphics problems. The method applies Gaussian convolution to smooth the objective function, then fits a local neural network surrogate around current parameters using self-supervised updates. The surrogate is trained online during optimization using samples weighted by both locality (Gaussian kernel around current parameters) and importance (based on gradient magnitude). This produces gradients that can be used for standard gradient-based optimization of the graphics pipeline parameters.

## Key Results
- Scales to optimization problems with up to 35k variables
- Outperforms finite differences and derivative-free methods on rendering, procedural modeling, and animation tasks
- Achieves tractable runtimes through importance sampling variance reduction
- Successfully optimizes non-differentiable graphics pipelines that previously required expensive black-box search

## Why This Works (Mechanism)

### Mechanism 1: Gaussian Smoothing
Convolution with a Gaussian kernel smooths non-differentiable loss landscapes, converting zero-gradient regions into areas with informative gradients. The smoothing bridges discontinuities while preserving local minima structure.

### Mechanism 2: Local Surrogate Fitting
Instead of fitting a global surrogate, ZeroGrads fits local approximations around current parameters using Gaussian weighting. This concentrates the neural network's capacity on relevant regions for the current optimization step.

### Mechanism 3: Importance Sampling
The method uses importance sampling to reduce variance in gradient estimates by focusing samples on regions that contribute most to the gradient, based on the difference between surrogate prediction and smoothed objective.

## Foundational Learning

- **Automatic differentiation**: Required for computing gradients of the neural surrogate with respect to both its parameters and the optimization variables. *Quick check: What is the difference between forward-mode and reverse-mode automatic differentiation, and which is used in this framework?*

- **Monte Carlo integration**: Used for estimating expectations over the parameter space through sampled values. *Quick check: How does the law of large numbers justify using sample averages to estimate integrals in this context?*

- **Importance sampling**: Reduces variance in gradient estimates by focusing samples on high-contribution regions. *Quick check: What conditions must the importance sampling distribution satisfy to produce an unbiased estimate?*

## Architecture Onboarding

- **Component map**: Forward model → Smoothing convolution → Local sampling → Neural network surrogate fitting → Gradient estimation → Parameter optimization
- **Critical path**: Sampling the objective → updating surrogate parameters → using surrogate gradient for parameter updates
- **Design tradeoffs**: Local vs global surrogate fitting, sample count vs gradient variance, smoothing kernel width vs preservation of minima structure
- **Failure signatures**: Surrogate divergence (gradients point away from solution), poor convergence (slow progress toward optimum), high variance in gradient estimates
- **First 3 experiments**:
  1. Implement the smoothing convolution on a simple 1D test function with known discontinuities
  2. Create a local surrogate fitting routine for a 2D test problem and verify it focuses on the current region
  3. Add importance sampling to the gradient estimation and compare variance reduction against uniform sampling

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does ZeroGrads handle higher-dimensional problems with more than 35k variables, and what are the limitations in scaling?
- **Basis in paper**: The paper demonstrates optimization on problems with up to 35k variables but does not explore beyond this point.
- **Why unresolved**: The paper does not provide evidence or experiments on scaling beyond 35k variables.
- **What evidence would resolve it**: Experiments on problems with more than 35k variables would clarify the scalability limits.

### Open Question 2
- **Question**: Can ZeroGrads be applied to non-graphics domains, such as general machine learning or scientific computing tasks?
- **Basis in paper**: The framework's generality suggests potential for broader applications beyond graphics-specific problems.
- **Why unresolved**: The paper does not test ZeroGrads on non-graphics tasks.
- **What evidence would resolve it**: Applying ZeroGrads to tasks like hyperparameter optimization or scientific simulations would demonstrate its versatility.

### Open Question 3
- **Question**: How sensitive is ZeroGrads to the choice of hyperparameters, such as the locality kernel spread (σ₀) and the number of samples (N)?
- **Basis in paper**: The paper mentions hyperparameters but does not provide a systematic study of their impact.
- **Why unresolved**: The paper does not explore hyperparameter sensitivity across different problem types.
- **What evidence would resolve it**: A comprehensive hyperparameter sensitivity analysis would clarify the method's robustness and ease of use.

## Limitations

- Critical implementation details including neural surrogate architecture, smoothing kernel widths, and exact importance sampling scheme are not specified
- Performance claims are demonstrated primarily on synthetic graphics problems with limited real-world validation
- No theoretical convergence guarantees are provided for the online surrogate fitting procedure

## Confidence

- **High confidence**: The core conceptual framework of using smoothed surrogate losses with local neural networks is sound and well-motivated
- **Medium confidence**: The importance sampling variance reduction mechanism should work in principle but requires careful implementation
- **Low confidence**: The specific hyperparameter choices and architectural decisions needed for successful reproduction are not specified

## Next Checks

1. Implement a controlled experiment comparing ZeroGrads against finite differences on a simple non-differentiable function with known optimal solution to verify gradient quality and convergence behavior
2. Test the sensitivity of the method to smoothing kernel width by running identical optimization problems with varying σ values and measuring convergence speed and solution quality
3. Evaluate the importance sampling variance reduction by comparing gradient estimation variance against uniform sampling across multiple sample budgets on a representative graphics problem