---
ver: rpa2
title: Flow Matching in Latent Space
arxiv_id: '2307.08698'
source_url: https://arxiv.org/abs/2307.08698
tags:
- image
- flow
- matching
- diffusion
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Latent Flow Matching (LFM), a novel approach
  for training generative models that combines flow matching with latent space representations.
  LFM addresses the challenges of expensive computing and large numbers of function
  evaluations in pixel space by performing flow matching in the latent spaces of pretrained
  autoencoders.
---

# Flow Matching in Latent Space

## Quick Facts
- arXiv ID: 2307.08698
- Source URL: https://arxiv.org/abs/2307.08698
- Reference count: 40
- Primary result: Latent Flow Matching (LFM) achieves faster training and improved computational efficiency for high-resolution image synthesis by performing flow matching in the latent spaces of pretrained autoencoders

## Executive Summary
This paper introduces Latent Flow Matching (LFM), a novel approach that combines flow matching with latent space representations to address computational challenges in generative modeling. By performing flow matching in the compact latent spaces of pretrained autoencoders rather than in pixel space, LFM significantly reduces computational costs while maintaining or improving sample quality. The method also introduces classifier-free velocity field guidance for conditional generation tasks and provides theoretical guarantees on the Wasserstein-2 distance between distributions.

## Method Summary
LFM performs flow matching in the latent space of a pretrained autoencoder, where the encoder compresses images to a lower-dimensional representation and the decoder reconstructs them. A latent flow network is trained to predict the velocity field that transforms a standard normal distribution to the target latent distribution. The method incorporates classifier-free guidance for conditional tasks by jointly training unconditional and conditional velocity fields within a single network. During sampling, an ODE solver generates latent samples that are decoded back to image space. The approach is theoretically grounded with bounds on the Wasserstein-2 distance between target and reconstructed distributions.

## Key Results
- LFM achieves competitive FID scores on CelebA-HQ, FFHQ, LSUN Church & Bedroom, and ImageNet datasets
- Computational efficiency is improved through reduced dimensionality in latent space, requiring fewer function evaluations
- Classifier-free guidance enhances conditional generation performance without requiring pretrained classifiers
- Theoretical bounds show the squared Wasserstein-2 distance is upper-bounded by reconstruction error and the latent flow matching objective

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LFM achieves faster training and improved computational efficiency by performing flow matching in latent space
- Mechanism: Compact latent representations reduce the number of function evaluations and computational load
- Core assumption: Latent space preserves essential information while being significantly smaller than pixel space
- Evidence: Abstract states LFM offers "improved computational efficiency and scalability for high-resolution image synthesis"

### Mechanism 2
- Claim: Classifier-free velocity field guidance enhances conditional generation
- Mechanism: Jointly trains unconditional and conditional velocity fields within a single network
- Core assumption: Velocity field can be effectively conditioned on class labels while maintaining unconditional generation
- Evidence: Section 4.2 describes implementation of classifier-free guidance for enhancing conditional models

### Mechanism 3
- Claim: Theoretical bound on Wasserstein-2 distance ensures reconstruction quality
- Mechanism: Bound shows squared Wasserstein-2 distance is upper-bounded by reconstruction error and latent flow matching objective
- Core assumption: Autoencoder is deterministic and Lipschitz continuous, velocity field is Lipschitz in latent space
- Evidence: Theorem 4.1 in section 4.3 establishes the theoretical control

## Foundational Learning

- Concept: Flow Matching Framework
  - Why needed here: Essential for understanding how LFM adapts flow matching to latent space
  - Quick check question: What is the key difference between flow matching and diffusion models in terms of the probability path?

- Concept: Autoencoder Latent Space
  - Why needed here: Crucial for understanding efficiency gains in LFM
  - Quick check question: How does the downsampling rate of an autoencoder affect the dimensionality of the latent space?

- Concept: Classifier-Free Guidance
  - Why needed here: Important for grasping conditional generation without pretrained classifiers
  - Quick check question: How does the guidance scale parameter γ influence the balance between unconditional and conditional generation?

## Architecture Onboarding

- Component map: Image → Autoencoder Encoder → Latent Space → Latent Flow Network → ODE Solver → Latent Space → Autoencoder Decoder → Generated Image
- Critical path: Encoder → Latent Flow Network → ODE Solver → Decoder during sampling; Encoder → Latent Flow Network (training) → Decoder
- Design tradeoffs: Smaller latent space improves efficiency but may lead to information loss; ODE solver choice affects sampling speed and quality
- Failure signatures: Poor FID scores indicate latent space or flow network issues; slow sampling suggests inefficient ODE solver; mode collapse indicates guidance training imbalance
- First 3 experiments:
  1. Train LFM on CelebA-HQ 256 with ADM network, evaluate FID and sampling time
  2. Compare different ODE solvers (Euler, Heun, adaptive) on same dataset
  3. Implement classifier-free guidance and evaluate on ImageNet conditional generation

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided text.

## Limitations

- Theoretical analysis relies on strong assumptions including deterministic, Lipschitz-continuous autoencoders and velocity fields
- Performance heavily depends on quality of pretrained autoencoder's latent space representation
- Computational benefits primarily demonstrated for high-resolution image synthesis without extensive validation on other data modalities
- Paper does not thoroughly investigate impact of different autoencoder architectures on LFM performance

## Confidence

- **High confidence**: Computational efficiency claims are well-supported by experimental results showing improved FID scores and sampling times
- **Medium confidence**: Classifier-free guidance implementation is supported by results, though detailed analysis of guidance scale impact could be stronger
- **Low confidence**: Generalization to non-image domains and robustness across different autoencoder architectures remain underexplored

## Next Checks

1. **Autoencoder Architecture Impact**: Conduct experiments varying autoencoder architecture to quantify how encoder/decoder choices affect LFM performance and computational efficiency

2. **Cross-Domain Generalization**: Test LFM on non-image datasets (audio, 3D point clouds) to evaluate generalization beyond image synthesis

3. **Lipschitz Constant Analysis**: Measure actual Lipschitz constants of autoencoder and velocity network in practice, comparing to theoretical bounds to assess practical relevance