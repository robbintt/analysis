---
ver: rpa2
title: Dual-Scale Interest Extraction Framework with Self-Supervision for Sequential
  Recommendation
arxiv_id: '2310.10025'
source_url: https://arxiv.org/abs/2310.10025
tags:
- user
- interest
- inherent
- recommendation
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles sequential recommendation by modeling users'
  current interests through a dual-scale interest extraction framework (DSIE). At
  the global scale, it captures inherent user preference using a ResNet-based encoder
  with self-supervised contrastive learning to overcome data limitations.
---

# Dual-Scale Interest Extraction Framework with Self-Supervision for Sequential Recommendation

## Quick Facts
- arXiv ID: 2310.10025
- Source URL: https://arxiv.org/abs/2310.10025
- Reference count: 33
- Key outcome: Dual-scale approach improves sequential recommendation by modeling inherent preference and diverse interests, achieving up to 11.5% improvement in Recall@50 and 14.4% in NDCG@50.

## Executive Summary
This paper introduces DSIE, a dual-scale interest extraction framework for sequential recommendation that models both user's inherent preference at global scale and diverse interests at local scale. The framework uses ResNet-based encoding with self-supervised contrastive learning to capture stable user preferences from entire interaction sequences, while extracting multiple fine-grained interests from interaction sub-sequences through intention prototype clustering. Experiments on three real-world Amazon datasets demonstrate that DSIE significantly outperforms state-of-the-art methods in both recommendation accuracy and novelty.

## Method Summary
DSIE operates in two scales: global scale captures inherent user preference using a ResNet encoder with self-supervised contrastive learning (item shuffling augmentation), while local scale extracts multiple fine-grained interests from interaction sub-sequences using intention prototype clustering. The framework aggregates these interests according to the inherent preference to predict the user's next-item. Training uses sampled softmax with auxiliary orthogonality loss, and the model is evaluated on Amazon datasets with user sequences filtered to have at least 3 interactions, using 8:1:1 train/validation/test splits with last 20% held out for testing.

## Key Results
- DSIE achieves up to 11.5% improvement in Recall@50 compared to state-of-the-art methods
- DSIE shows up to 14.4% improvement in NDCG@50 over competing approaches
- The framework demonstrates superior novelty in recommendations by leveraging inherent preferences to explore potential user interests

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dual-scale interest extraction framework (DSIE) improves sequential recommendation by explicitly modeling both user's inherent preference at global scale and diverse interests at local scale.
- Mechanism: DSIE first encodes the entire interaction sequence with ResNet and self-attention to capture the user's inherent preference, then extracts multiple fine-grained interests from interaction sub-sequences using intention prototype clustering, and finally aggregates these interests according to the inherent preference to predict next-item.
- Core assumption: The user's inherent preference derived from global correlation of entire interaction sequence significantly influences the user's interests in specific items, and this preference can guide the aggregation of multiple local interests for more accurate prediction.
- Evidence anchors:
  - [abstract] "DSIE explicitly models the user's inherent preference with contrastive learning by attending over his/her entire interaction sequence at the global scale and catches the user's diverse interests in a fine granularity at the local scale."
  - [section] "We propose a novel Dual-Scale Interest Extraction framework named DSIE, to effectively estimate the user's current interest for the sequential recommendation. At the global scale, we adopt a multi-layer residual network (i.e., ResNet) to encode the user's entire interaction sequence as the user's inherent preference representation."
  - [corpus] Weak evidence - only 25 related papers found with average neighbor FMR=0.397, suggesting limited direct evidence for dual-scale modeling approach specifically.
- Break condition: If the global scale inherent preference does not correlate with local interests, or if intention prototype clustering fails to capture meaningful sub-sequences, the aggregation will be ineffective.

### Mechanism 2
- Claim: Self-supervised contrastive learning overcomes the challenge of lacking explicit supervision when capturing the user's inherent preference.
- Mechanism: DSIE creates augmentation sequences via item shuffling, assuming inherent preferences extracted from original and shuffled sequences should be close. It then uses pairwise contrastive tasks to let augmented data from the same example be discriminated against others.
- Core assumption: The user's inherent preference is stable and insensitive to the order of historical interactions, allowing item shuffling to create meaningful augmentation without explicit labels.
- Evidence anchors:
  - [abstract] "Since the public recommendation data only contains recommendation results rather than the causes, whether inherent preference dominates a certain user-item interaction lacks explicit supervision. To tackle this challenge, we design a self-supervised contrastive learning task and augment samples via item shuffling."
  - [section] "Here we propose to use the self-supervised learning to overcome the challenge of lacking labeled cause-specific data. Specifically, inherent preference in the user's mind can be regarded as stable and insensitive to the order of his/her historical interactions."
  - [corpus] Weak evidence - no direct citations found supporting contrastive learning for inherent preference modeling in sequential recommendation.
- Break condition: If the user's inherent preference is actually sensitive to interaction order, or if shuffling destroys meaningful temporal patterns, the contrastive task will fail to learn useful representations.

### Mechanism 3
- Claim: Intention prototype clustering filters out noisy behaviors and enables more focused multi-interest extraction.
- Mechanism: DSIE uses a set of intention prototypes identified via clustering, where each item in the user's interaction sequence is related to intention prototypes according to their distances. Items with low relevance scores are filtered out as noise.
- Core assumption: Noisy behaviors (e.g., sales promotions, exposure bias, position bias) are inconsistent with user's real interests and can be identified and filtered out through clustering with intention prototypes.
- Evidence anchors:
  - [abstract] "Encouragingly, the noisy behaviors (e.g., sales promotions, exposure bias [31], and position bias [8]) that are inconsistent with user's real interests will be filtered out when clustering."
  - [section] "Given Equ.5, those noisy behaviors in Hu which are inconsistent with the user's real interests will be filtered out (i.e., Pk|i → 0, for each k)."
  - [corpus] Weak evidence - no direct citations found supporting intention prototype clustering specifically for noise filtering in sequential recommendation.
- Break condition: If the clustering fails to distinguish between genuine interests and noise, or if the noise pattern changes over time, the filtering will be ineffective.

## Foundational Learning

- Concept: Self-supervised learning through contrastive tasks
  - Why needed here: Public recommendation data lacks explicit labels about whether inherent preference drives user-item interactions, making supervised learning impossible
  - Quick check question: What would happen if we tried to use supervised learning with the available data?

- Concept: Intention prototype clustering for multi-interest extraction
  - Why needed here: Single embeddings cannot capture the diverse interests of users who interact with items from different categories
  - Quick check question: How would the model perform if we used only one interest embedding per user?

- Concept: Multi-layer residual networks (ResNet) for sequence encoding
  - Why needed here: The entire interaction sequence needs to be encoded into a single representation that captures global correlation and inherent preference
  - Quick check question: What would be the impact of using a simpler architecture like a single-layer network instead of ResNet?

## Architecture Onboarding

- Component map: Input sequence -> Item embedding layer -> Global scale (ResNet + self-attention + contrastive learning) -> Local scale (intention prototype clustering + position weighting + guided attention) -> Interest aggregation -> Softmax prediction
- Critical path: Embedding generation -> Global scale encoding -> Local scale extraction -> Interest aggregation -> Prediction
- Design tradeoffs: Using dual-scale approach increases model complexity but improves accuracy and novelty; contrastive learning requires careful augmentation strategy; intention prototype clustering needs parameter tuning
- Failure signatures: Poor performance on novel item recommendation indicates ineffective inherent preference modeling; high variance in local interest extraction suggests unstable clustering; low recall on held-out items points to poor aggregation
- First 3 experiments:
  1. Compare DSIE performance with and without the global scale component to validate inherent preference modeling contribution
  2. Test different numbers of intention prototypes (K) to find optimal balance between diversity and noise filtering
  3. Evaluate different temperature parameters (τ) in the interest aggregation module to optimize the balance between global and local signals

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the DSIE framework handle scenarios where a user's inherent preference is unstable or rapidly evolving over time?
- Basis in paper: [inferred] The paper assumes inherent preferences are "stable and insensitive to the order of historical interactions," but real-world user preferences can change rapidly, especially in domains like fashion or technology.
- Why unresolved: The framework relies on global-scale modeling of inherent preferences, but the paper does not address how it adapts to dynamic or shifting user preferences.
- What evidence would resolve it: Empirical results comparing DSIE's performance on datasets with stable vs. rapidly changing user preferences, or ablation studies testing the impact of removing the inherent preference component in dynamic scenarios.

### Open Question 2
- Question: What is the impact of the number of intention prototypes (K) on DSIE's performance in highly diverse or niche domains?
- Basis in paper: [explicit] The paper shows that performance peaks at K=3 for Video Games and varies for other datasets, but does not explore performance in highly specialized domains with extreme diversity.
- Why unresolved: The optimal number of intention prototypes may vary significantly in niche domains, and the paper does not provide guidance on tuning K for such cases.
- What evidence would resolve it: Experiments testing DSIE on datasets with extreme diversity (e.g., multi-category e-commerce platforms) and analyzing the relationship between K and performance across different domains.

### Open Question 3
- Question: How does DSIE's recommendation novelty compare to methods that explicitly incorporate exploration strategies (e.g., reinforcement learning)?
- Basis in paper: [explicit] The paper claims DSIE improves novelty by leveraging inherent preferences to explore potential interests, but does not compare against methods that explicitly use exploration strategies.
- Why unresolved: While DSIE shows improved novelty, it is unclear whether it outperforms methods designed specifically for exploration, such as those using reinforcement learning or bandit algorithms.
- What evidence would resolve it: Comparative experiments between DSIE and exploration-based methods (e.g., contextual bandits or reinforcement learning models) on datasets with high novelty requirements.

## Limitations
- The dual-scale approach introduces significant complexity with multiple interdependent components that may interact in unpredictable ways
- The contrastive learning assumption that inherent preferences are stable across shuffled sequences remains empirically questionable
- The intention prototype clustering method, while intuitive, lacks strong empirical validation in the corpus

## Confidence
- High confidence: The overall dual-scale framework design and experimental methodology (data splits, metrics, baseline comparisons)
- Medium confidence: The specific architectural choices (ResNet layers, prototype numbers, temperature parameters) given limited ablation studies
- Low confidence: The contrastive learning augmentation strategy and its assumption about preference stability

## Next Checks
1. **Temporal stability test**: Evaluate DSIE on sequences with varying time spans to verify whether the contrastive learning assumption (inherent preferences are order-invariant) holds across different temporal contexts.
2. **Noise pattern analysis**: Conduct controlled experiments adding different types of noise (position bias, exposure bias, random items) to test whether intention prototype clustering reliably filters specific noise patterns as claimed.
3. **Prototype sensitivity analysis**: Systematically vary the number of intention prototypes (K) and measure the trade-off between interest diversity and noise filtering effectiveness to identify optimal configuration ranges.