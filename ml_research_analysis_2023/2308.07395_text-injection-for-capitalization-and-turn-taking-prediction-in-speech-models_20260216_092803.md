---
ver: rpa2
title: Text Injection for Capitalization and Turn-Taking Prediction in Speech Models
arxiv_id: '2308.07395'
source_url: https://arxiv.org/abs/2308.07395
tags:
- data
- speech
- text
- capitalization
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the use of text injection for auxiliary tasks,
  which are the non-ASR tasks often performed by an E2E model. In this work, we use
  joint end-to-end and internal language model training (JEIT) as our text injection
  algorithm to train an ASR model which performs two auxiliary tasks.
---

# Text Injection for Capitalization and Turn-Taking Prediction in Speech Models

## Quick Facts
- arXiv ID: 2308.07395
- Source URL: https://arxiv.org/abs/2308.07395
- Authors: 
- Reference count: 0
- Key outcome: Text injection through JEIT improves capitalization for long-tail data and turn-taking detection recall while maintaining ASR quality.

## Executive Summary
This paper presents a text injection method for training speech models that perform auxiliary tasks alongside automatic speech recognition. The approach uses Joint End-to-End and Internal Language Model Training (JEIT) to leverage unpaired text data for improving auxiliary tasks while maintaining ASR performance. The method is evaluated on two tasks: capitalization (a de-normalization task) and turn-taking prediction in digital assistant interactions. Results show that text injection provides significant improvements for both tasks, particularly boosting capitalization performance on long-tail data and improving turn-taking detection recall.

## Method Summary
The method uses a Multi-output HAT decoder with parallel joint networks for each task (ASR, capitalization, turn-taking). Training combines E2E ASR on paired audio-text data with ILM training on unpaired text data using JEIT. The model employs conformer encoder layers, a prediction network, and task-specific joint networks. Capitalization labels are generated using heuristics to identify non-dialog act text, while turn-taking labels are inserted as pause tokens at the end of speaker turns. The total loss combines E2E and ILM losses weighted by task-specific parameters (αCap, αPause, β).

## Key Results
- Text injection improves capitalization performance for long-tail data
- Turn-taking detection recall improves with text injection while slightly reducing precision
- ASR WER remains stable across different training configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text injection through JEIT allows auxiliary tasks to access richer text-only data, improving their performance without degrading ASR quality.
- Mechanism: JEIT separates the audio input path (used for ASR) from the text-only path (used for ILM training), allowing auxiliary tasks to benefit from abundant text data during training while maintaining ASR performance through paired audio-text data.
- Core assumption: The auxiliary tasks can effectively utilize the ILM's learned representations from text-only data to improve their predictions.
- Evidence anchors:
  - [abstract]: "This study examines the use of text injection for auxiliary tasks, which are the non-ASR tasks often performed by an E2E model."
  - [section]: "Text injection solves these issues by using LM-style unpaired text data to train the ASR model itself."
  - [corpus]: Weak - related papers focus on capitalization benchmarks but don't directly address text injection mechanisms.

### Mechanism 2
- Claim: The multi-output HAT decoder architecture enables simultaneous training of ASR and auxiliary tasks without interference.
- Mechanism: Each task (ASR, capitalization, turn-taking) has its own joint network that shares audio encoder and prediction network features but computes task-specific outputs independently.
- Core assumption: The shared representations are sufficiently informative for all tasks while maintaining task-specific discriminative power.
- Evidence anchors:
  - [section]: "Each auxiliary task makes a sequence prediction YAux based on the predicted ASR sequence YASR."
  - [section]: "As described in §4.1, each auxiliary task makes a sequence prediction YAux based on the predicted ASR sequence YASR."
  - [corpus]: Weak - related papers discuss multi-task learning but not the specific HAT architecture details.

### Mechanism 3
- Claim: The factorization of auxiliary task labels into parallel sequences preserves beam diversity during inference.
- Mechanism: Instead of embedding special tokens directly into transcripts, auxiliary tasks are represented as separate label sequences aligned with the ASR output, preventing ASR predictions from being identical across beams.
- Core assumption: The factorization approach maintains sufficient information for auxiliary tasks while improving inference efficiency.
- Evidence anchors:
  - [section]: "To solve for this, we follow Wang et al. [9], factorizing the auxiliary task tokens into parallel sequences of equal length, one for each task."
  - [section]: "All other token slots are filled with ⟨non-pause⟩."
  - [corpus]: Weak - no direct evidence about beam diversity preservation in related works.

## Foundational Learning

- Concept: RNN-T (Recurrent Neural Network Transducer)
  - Why needed here: The model uses RNN-T as the base architecture for streaming ASR with auxiliary tasks.
  - Quick check question: What is the primary advantage of RNN-T for streaming ASR applications?

- Concept: Internal Language Model (ILM)
  - Why needed here: Text injection specifically targets improving the ILM component of the ASR model.
  - Quick check question: How does the ILM differ from an external language model in terms of integration with ASR?

- Concept: Multi-task learning with shared representations
  - Why needed here: The model jointly trains ASR with capitalization and turn-taking prediction using shared encoder features.
  - Quick check question: What are the potential benefits and risks of sharing representations across multiple tasks?

## Architecture Onboarding

- Component map: Audio input → Conformer encoder → Shared features → Prediction network → Multiple joint networks → Task outputs
- Critical path: Audio input → Encoder → Shared features → Joint networks → Task outputs
- Design tradeoffs:
  - Separate joint networks add parameters but enable task-specific optimization
  - Text injection improves auxiliary tasks but requires careful loss weighting
  - Streaming capability maintained through causal convolutions in first encoder
- Failure signatures:
  - ASR WER degradation indicates interference between tasks
  - Auxiliary task performance drops suggest insufficient text data quality
  - Training instability may indicate incorrect loss weighting
- First 3 experiments:
  1. Train baseline model without text injection to establish performance floor
  2. Add text injection with only ASR task to verify ILM improvement mechanism
  3. Introduce auxiliary tasks with text injection to measure combined effect

## Open Questions the Paper Calls Out
The paper identifies three key open questions: (1) What is the optimal balance between precision and recall for turn-taking prediction across different application domains? (2) How does text injection performance scale with the size of unpaired text corpus? (3) Can the text injection approach be effectively extended to other auxiliary tasks beyond capitalization and turn-taking prediction?

## Limitations
- Proprietary datasets prevent independent verification of results
- Critical implementation details (loss weighting hyperparameters, exact label generation) are not fully specified
- Evaluation focuses on recall for turn-taking prediction without complete precision-recall analysis

## Confidence
- High confidence in the general text injection mechanism through JEIT
- Medium confidence in specific performance gains due to proprietary data
- Low confidence in beam diversity preservation claims

## Next Checks
1. Implement controlled experiment comparing capitalization performance with/without text injection on public dataset
2. Conduct ablation studies on loss weighting parameters to verify sensitivity
3. Measure both precision and recall for turn-taking prediction to complete performance analysis