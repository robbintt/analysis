---
ver: rpa2
title: Large Language Models Cannot Self-Correct Reasoning Yet
arxiv_id: '2310.01798'
source_url: https://arxiv.org/abs/2310.01798
tags:
- answer
- self-correction
- reasoning
- feedback
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Language Models (LLMs) struggle to self-correct their reasoning
  without external feedback, and performance often degrades post self-correction.
  The paper investigates intrinsic self-correction, where LLMs attempt to rectify
  their responses based solely on inherent capabilities.
---

# Large Language Models Cannot Self-Correct Reasoning Yet

## Quick Facts
- arXiv ID: 2310.01798
- Source URL: https://arxiv.org/abs/2310.01798
- Reference count: 19
- LLMs struggle to self-correct reasoning without external feedback and often degrade performance post-correction

## Executive Summary
This paper investigates whether large language models can effectively self-correct their reasoning without external feedback. Through extensive experiments on benchmarks like GSM8K and CommonSenseQA, the study finds that LLMs are more likely to change correct answers to incorrect ones than vice versa. The research distinguishes between intrinsic self-correction (relying solely on the model's internal capabilities) and self-correction with external feedback, demonstrating that external feedback is crucial for successful reasoning correction. While self-correction shows promise for style alteration and safety improvement, it remains largely ineffective for enhancing reasoning performance.

## Method Summary
The study evaluates self-correction capabilities using GPT-3.5 and GPT-4 models on reasoning benchmarks including GSM8K, CommonSenseQA, and HotpotQA. Experiments compare performance with and without self-correction attempts, using both intrinsic self-correction (no external feedback) and oracle feedback scenarios. The researchers also test multi-agent debate and self-consistency approaches as alternatives to traditional self-correction. Various prompt designs are explored, including different feedback formats and post-hoc prompting strategies. The evaluation measures accuracy changes resulting from self-correction attempts and compares them to baseline performance.

## Key Results
- LLMs are more likely to change correct answers to incorrect ones than to correct wrong answers
- Performance degradation occurs in most reasoning tasks when using intrinsic self-correction
- Oracle feedback prevents incorrect answer changes and improves performance
- Multi-agent debate and self-consistency achieve significant improvements over standard prompting
- Self-correction shows promise for style alteration and safety improvement tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intrinsic self-correction can improve reasoning performance when combined with oracle feedback that prevents incorrect answer changes.
- Mechanism: Oracle feedback stops the model from changing correct answers to incorrect ones, allowing the model to only change incorrect answers to correct ones.
- Core assumption: The model can identify when an answer is correct and should not be changed.
- Evidence anchors:
  - [abstract] "our research indicates that LLMs struggle to self-correct their responses without external feedback"
  - [section 3.1] "Table 1 summarizes the results. From these results, we observe significant performance improvements, consistent with the findings presented in Kim et al. (2023); Shinn et al. (2023)."
  - [corpus] Weak - the corpus neighbors focus on self-correction with external feedback or tools, not intrinsic self-correction
- Break condition: Oracle feedback is unavailable or cannot accurately determine when an answer is correct.

### Mechanism 2
- Claim: Multi-agent debate and self-consistency improve reasoning by providing multiple independent generations that can be aggregated through voting.
- Mechanism: Multiple model instances generate different responses, and majority voting selects the most likely correct answer.
- Core assumption: At least one of the multiple responses will contain the correct answer.
- Evidence anchors:
  - [section 3.3] "Table 4 presents the results 5. The results indicate that both multi-agent debate and self-consistency achieve significant improvements over standard prompting."
  - [section 3.3] "If we aim to argue that LLMs can self-correct, it is preferable to exclude the effects of selection among multiple generations."
  - [corpus] Moderate - several corpus neighbors discuss self-correction with voting or selection mechanisms
- Break condition: All generated responses are incorrect, or voting mechanism fails to identify the correct answer.

### Mechanism 3
- Claim: Self-correction works well for style alteration and safety improvement because these tasks have clear post-hoc feedback that can guide corrections.
- Mechanism: Feedback prompts can specify exactly what needs to be changed in the response, making corrections straightforward.
- Core assumption: The feedback can clearly specify the desired changes in a way that the model can understand and implement.
- Evidence anchors:
  - [abstract] "self-correction shows promise in tasks like style alteration and safety improvement"
  - [section 4] "Self-correction can be effectively employed to make responses align with specific preferences, such as altering the style of responses or enhancing their safety"
  - [corpus] Weak - corpus neighbors don't specifically address style alteration or safety improvement through self-correction
- Break condition: Feedback cannot clearly specify desired changes, or model cannot understand the feedback instructions.

## Foundational Learning

- Concept: Difference between intrinsic self-correction and self-correction with external feedback
  - Why needed here: Understanding this distinction is crucial for interpreting the paper's results and claims
  - Quick check question: Can you explain the key difference between intrinsic self-correction and self-correction with oracle feedback?

- Concept: Post-hoc prompting vs pre-hoc prompting
  - Why needed here: The paper frames self-correction as a type of post-hoc prompting, which is central to understanding its limitations
  - Quick check question: What is the main difference between post-hoc and pre-hoc prompting approaches?

- Concept: Self-consistency as a verification method
  - Why needed here: The paper discusses self-consistency as an alternative to self-correction for improving reasoning
  - Quick check question: How does self-consistency differ from traditional self-correction approaches?

## Architecture Onboarding

- Component map: LLM inference → Self-correction prompt generation → LLM response generation → Answer verification/selection
- Critical path: Initial response generation → Feedback generation → Refined response generation → Answer selection
- Design tradeoffs: Self-correction provides potential improvements but at the cost of additional inference calls and tokens
- Failure signatures: Performance degradation after self-correction, incorrect answers changing to other incorrect answers
- First 3 experiments:
  1. Replicate GSM8K self-correction results with and without oracle feedback
  2. Test different feedback prompts on GSM8K to see if any improve performance
  3. Compare multi-agent debate to self-consistency on GSM8K with same number of responses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance of intrinsic self-correction improve with more rounds of correction?
- Basis in paper: Inferred from the observation that self-correction often leads to performance degradation and the fact that the study only tested up to two rounds of self-correction.
- Why unresolved: The paper only evaluates two rounds of self-correction, and it's unclear whether extending the number of rounds would lead to better performance or further degradation.
- What evidence would resolve it: Conducting experiments with more rounds of self-correction and comparing the results to the baseline performance.

### Open Question 2
- Question: Can self-correction be effective for reasoning tasks if the initial prompt is optimized?
- Basis in paper: Inferred from the discussion that self-correction may not be effective when the initial prompt is poorly constructed, but the paper does not explore the impact of optimizing the initial prompt.
- Why unresolved: The paper focuses on the limitations of self-correction without external feedback, but it does not investigate whether optimizing the initial prompt could improve the effectiveness of self-correction for reasoning tasks.
- What evidence would resolve it: Testing self-correction on reasoning tasks with optimized initial prompts and comparing the results to those obtained with poorly constructed prompts.

### Open Question 3
- Question: How does the effectiveness of self-correction vary across different types of reasoning tasks?
- Basis in paper: Inferred from the observation that self-correction struggles with reasoning tasks, but the paper only evaluates specific benchmarks like GSM8K and CommonSenseQA.
- Why unresolved: The paper does not explore the effectiveness of self-correction across a diverse range of reasoning tasks, making it unclear whether the observed limitations are consistent across different types of reasoning problems.
- What evidence would resolve it: Conducting experiments on a variety of reasoning tasks with different characteristics and comparing the effectiveness of self-correction across these tasks.

## Limitations

- The study focuses on specific prompting strategies without exploring all possible self-correction approaches
- Experiments use idealized oracle feedback rather than practical feedback mechanisms
- The research does not investigate the role of model size or architecture differences in self-correction capabilities

## Confidence

- Performance degradation after self-correction: High confidence
- Oracle feedback prevents incorrect answer changes: High confidence
- Broader claim that LLMs cannot self-correct reasoning "yet": Medium confidence

## Next Checks

1. Replicate the GSM8K self-correction experiments with and without oracle feedback using the same model versions to verify the core finding that LLMs degrade performance without external guidance.

2. Test whether different prompt formulations or temperature settings affect self-correction success rates, particularly examining whether clearer instructions reduce incorrect answer changes.

3. Compare self-correction performance across different model families (GPT-3.5, GPT-4, Claude, Llama) to determine if the inability to self-correct is consistent across architectures or specific to certain models.