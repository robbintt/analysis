---
ver: rpa2
title: Answering Subjective Induction Questions on Products by Summarizing Multi-sources
  Multi-viewpoints Knowledge
arxiv_id: '2309.05938'
source_url: https://arxiv.org/abs/2309.05938
tags:
- knowledge
- product
- subjective
- summarization
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of answering subjective induction
  questions on products (SUBJPQA), which requires summarizing multi-source multi-viewpoint
  knowledge to provide comprehensive answers. Unlike traditional QA tasks with unique
  answers, subjective induction questions need to incorporate diverse opinions and
  objective facts from various sources.
---

# Answering Subjective Induction Questions on Products by Summarizing Multi-sources Multi-viewpoints Knowledge

## Quick Facts
- arXiv ID: 2309.05938
- Source URL: https://arxiv.org/abs/2309.05938
- Reference count: 40
- Primary result: Proposed method achieves 25% and 16% improvements in BLEU-4 and ROUGE-L metrics for subjective summarization over baselines

## Executive Summary
This paper introduces the task of answering subjective induction questions on products (SUBJPQA), which requires summarizing multi-source multi-viewpoint knowledge to provide comprehensive answers that incorporate both objective facts and diverse subjective opinions. The proposed three-step method addresses key challenges through commonsense knowledge supplementation, sentiment-aware multi-document representation, and reinforcement learning-based summarization with template control. The authors construct a large-scale dataset called SupQA containing 48,352 samples across 15 product domains to evaluate this task.

## Method Summary
The proposed method consists of three key steps: (1) Implicit commonsense knowledge retrieval using prompt-based generation from LLMs like GPT3.5 and COMET, followed by filtering with BM25 similarity and BERT embeddings to retain relevant knowledge; (2) Contextual document representation using interactive attention with multi-head pooling to capture cross-document relevance and inter-document relationships; (3) Reinforcement-based answer summarization using BART decoder with template-controlled decoding, trained with a mixed loss combining MLE and RL objectives. The RL component uses two novel rewards: sentiment distribution alignment (Jaccard similarity with gold summaries) and template compliance (F1-score with gold templates).

## Key Results
- Achieves 25% and 16% improvements in BLEU-4 and ROUGE-L metrics respectively for subjective summarization
- Demonstrates 25%, 11%, and 16% improvements in BLEU-4, ROUGE-L, and METEOR metrics for objective summarization
- Shows superior performance in human evaluations across factual accuracy, information coverage, and sentiment distribution alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interactive attention with multi-head pooling captures cross-document relevance better than simple concatenation
- Mechanism: Query layer weights encoder outputs based on question-document interactions, then contextual representation layers use inter-document attention to model relationships between multiple sources
- Core assumption: Document interactions are more informative than isolated document representations for answering subjective induction questions
- Evidence anchors:
  - [abstract] "We then capture their relevance with the questions by interactive attention"
  - [section II-B] "Recognizing cross-document interaction plays a crucial role in extracting pertinent information, eliminating redundancies, and constructing a coherent summary"
  - [corpus] Weak - related work focuses on single-document summarization, not multi-source subjective QA

### Mechanism 2
- Claim: Reinforcement learning with sentiment recognition and template compliance rewards produces more comprehensive and sentiment-accurate summaries than MLE alone
- Mechanism: Two-stage training approach with MLE training using template guidance, then RL fine-tuning using rewards that measure sentiment distribution alignment and template compliance
- Core assumption: MLE training suffers from "exposure bias" where the model expects gold-standard data at each training step but lacks such supervision during testing
- Evidence anchors:
  - [abstract] "we develop a reinforcement-based summarizer. This summarizer is designed to aggregate all the facts and opinions, ensuring that no crucial detail is left out"
  - [section II-C] "To produce a summary that meets our requirements, we propose two novel rewards that capture the real sentiment distribution and the compliance with the template"
  - [corpus] Weak - most related work uses MLE or supervised learning, not RL for summarization tasks

### Mechanism 3
- Claim: Implicit commonsense knowledge retrieval supplements missing context crucial for answering product questions
- Mechanism: Prompt-based generation from LLMs like GPT3.5 and COMET, then filtering using BM25 similarity and BERT embeddings
- Core assumption: Product questions often involve implicit commonsense knowledge that is missing but crucial to solving the questions
- Evidence anchors:
  - [abstract] "The implicit commonsense facts are also collected to supplement the necessary but missing contexts"
  - [section II-A] "Product questions frequently entail implicit commonsense knowledge, which supplement the necessary but missing context"
  - [corpus] Moderate - some related work uses external knowledge, but typically for single-document tasks rather than multi-source subjective QA

## Foundational Learning

- Concept: Multi-head attention mechanisms
  - Why needed here: To capture different aspects of the question-document relationships and model interactions between multiple knowledge sources
  - Quick check question: How does multi-head attention differ from single-head attention in processing cross-document relationships?

- Concept: Reinforcement learning for text generation
  - Why needed here: To overcome exposure bias in MLE training and generate summaries that better align with human preferences for sentiment distribution and template compliance
  - Quick check question: What are the key differences between REINFORCE algorithm and standard supervised learning approaches?

- Concept: Template-controlled decoding
  - Why needed here: To ensure generated summaries follow a structured format that captures both objective facts and subjective viewpoints with proper sentiment distribution
  - Quick check question: How does template integration affect the diversity of generated summaries compared to unconstrained generation?

## Architecture Onboarding

- Component map: Product question → Query layer → Multi-head pooling → Inter-document attention → Inner-document attention → Sentiment classification → RL-based summarizer → Template-controlled decoding → Final summaries
- Critical path: Question → Query layer → Contextual representation → Sentiment classification → RL summarization → Template-controlled decoding → Final summaries
- Design tradeoffs:
  - Using LLMs for knowledge generation increases coverage but introduces filtering complexity
  - Reinforcement learning improves quality but requires careful reward design
  - Template control ensures structure but may limit natural language generation
- Failure signatures:
  - Poor sentiment distribution alignment indicates reward function issues
  - Missing key product information suggests filtering parameters need adjustment
  - Template violations indicate template integration problems
- First 3 experiments:
  1. Test implicit knowledge retrieval with different filtering thresholds (α and ε) on a small subset
  2. Evaluate contextual document representation with varying numbers of layers (0-4) on objective summarization
  3. Compare RL training with different reward scaling factors (γ) on subjective summarization quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance of the proposed method be further improved by incorporating additional knowledge sources beyond the ones used in the current approach?
- Basis in paper: [explicit] The paper mentions that the proposed method uses a three-step approach to address the challenges of the SUBJPQA task, including retrieving implicit commonsense knowledge, capturing diverse sentiments, and aggregating knowledge using a reinforcement-based summarizer. However, it does not explore the potential of incorporating additional knowledge sources.
- Why unresolved: The paper does not provide any insights into the potential benefits of incorporating additional knowledge sources, such as domain-specific knowledge bases or external ontologies, which could enhance the performance of the proposed method.
- What evidence would resolve it: Conducting experiments with different knowledge sources and evaluating their impact on the performance of the proposed method would provide evidence to resolve this question.

### Open Question 2
- Question: How can the proposed method be adapted to handle subjective induction questions on products in languages other than English?
- Basis in paper: [explicit] The paper focuses on the SUBJPQA task for English product questions and does not discuss the applicability of the proposed method to other languages.
- Why unresolved: The paper does not address the challenges and potential adaptations required to handle subjective induction questions on products in languages other than English, such as dealing with language-specific nuances and cultural differences.
- What evidence would resolve it: Conducting experiments with the proposed method on subjective induction questions in different languages and evaluating its performance would provide evidence to resolve this question.

### Open Question 3
- Question: How can the proposed method be extended to handle subjective induction questions on products in real-time or interactive settings, such as chatbots or virtual assistants?
- Basis in paper: [explicit] The paper focuses on the offline setting of the SUBJPQA task and does not discuss the potential extensions to real-time or interactive settings.
- Why unresolved: The paper does not address the challenges and potential adaptations required to handle subjective induction questions on products in real-time or interactive settings, such as managing user queries and providing timely responses.
- What evidence would resolve it: Conducting experiments with the proposed method in real-time or interactive settings and evaluating its performance in terms of response time and user satisfaction would provide evidence to resolve this question.

## Limitations

- The filtering mechanism for commonsense knowledge relies heavily on hand-tuned thresholds that may not generalize well across different product domains
- The effectiveness of template-controlled decoding is dependent on the quality and comprehensiveness of the template design, which isn't fully detailed in the paper
- The reinforcement learning component introduces complexity in reward design and training stability, with potential issues in balancing multiple objectives

## Confidence

**High Confidence** (Evidence is direct and multiple):
- The task formulation of answering subjective induction questions by summarizing multi-source multi-viewpoint knowledge is novel and well-defined
- The dataset construction methodology and its scale (48,352 samples across 15 domains) is clearly specified
- The three-step approach addresses distinct challenges in the problem domain

**Medium Confidence** (Evidence is partial or from related work):
- The effectiveness of interactive attention mechanisms for capturing cross-document relevance
- The improvement from reinforcement learning over maximum likelihood estimation for this specific task
- The necessity of implicit commonsense knowledge retrieval for product question answering

**Low Confidence** (Evidence is weak or absent):
- The generalizability of the template design across different product categories
- The robustness of the filtering mechanism for commonsense knowledge generation
- The long-term stability of the RL training process with multiple reward functions

## Next Checks

1. **Filtering Mechanism Robustness Test**: Conduct ablation studies by varying the filtering thresholds (α and ε) across different product domains to determine the sensitivity of knowledge quality to these parameters.

2. **Template Design Generalization**: Test the model's performance when using templates designed for different product categories to assess generalizability of the template design approach.

3. **RL Training Stability Analysis**: Monitor the convergence behavior of the reinforcement learning component across multiple training runs and track variance in performance metrics to assess training stability.