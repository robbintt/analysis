---
ver: rpa2
title: Adversarial Likelihood Estimation With One-Way Flows
arxiv_id: '2307.09882'
source_url: https://arxiv.org/abs/2307.09882
tags:
- density
- generator
- samples
- function
- increase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new generative model framework that bridges
  the gap between energy-based models (EBMs) and generative adversarial networks (GANs).
  The key insight is that Wasserstein GAN's discriminator objective is a biased estimator
  of the partition function, and by explicitly computing the generator density via
  a novel "one-way flow" network, we can derive an unbiased estimator.
---

# Adversarial Likelihood Estimation With One-Way Flows

## Quick Facts
- arXiv ID: 2307.09882
- Source URL: https://arxiv.org/abs/2307.09882
- Authors: 
- Reference count: 40
- Key outcome: This paper introduces a new generative model framework that bridges the gap between energy-based models (EBMs) and generative adversarial networks (GANs). The key insight is that Wasserstein GAN's discriminator objective is a biased estimator of the partition function, and by explicitly computing the generator density via a novel "one-way flow" network, we can derive an unbiased estimator. This enables the use of maximum likelihood estimation in an adversarial framework.

## Executive Summary
This paper introduces a novel generative model framework that bridges the gap between energy-based models and generative adversarial networks. The key insight is that Wasserstein GAN's discriminator objective is a biased estimator of the partition function, and by explicitly computing the generator density via a novel "one-way flow" network, an unbiased estimator can be derived. This enables the use of maximum likelihood estimation in an adversarial framework, with the one-way flow architecture allowing for more flexible generator designs that can operate in lower-dimensional latent spaces. Experiments show that the proposed method converges faster, produces comparable sample quality to GANs with similar architectures, and successfully avoids overfitting on commonly used datasets while producing smooth low-dimensional latent representations.

## Method Summary
The proposed method introduces a new generative model framework that combines adversarial training with maximum likelihood estimation. It uses a Wasserstein GAN discriminator to estimate an unnormalized density, but crucially computes the partition function using an unbiased importance sampling estimator rather than the one-sample approximation used in standard WGAN. The generator is implemented as a "one-way flow" network that maps from a low-dimensional latent space to the data space while allowing dimensionality increases. The training objective includes an entropy maximization term for the generator, hypothesized to improve mode coverage and reduce mode collapse. The method is evaluated on synthetic 2D GMM data and real datasets CIFAR-10 and CelebA.

## Key Results
- The proposed method converges faster than standard WGAN approaches
- Produces comparable sample quality to GANs with similar architectures
- Successfully avoids overfitting on commonly used datasets
- Produces smooth low-dimensional latent representations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Wasserstein GAN's discriminator objective is a biased estimator of the partition function
- **Mechanism**: The WGAN discriminator approximates the partition function with a one-sample Monte Carlo estimate, which introduces bias because it doesn't account for the variance in the importance sampling weights
- **Core assumption**: The partition function can be written as an expectation over the generator density PGψ
- **Evidence anchors**:
  - [abstract] "Wasserstein GAN performs a biased estimate of the partition function"
  - [section 2.1] "if we take a one-sample approximation we get the objective θ∗ = arg max θ (∑x∈X [Dθ(x) − Dθ(Gψ(z))])" and "WGANs perform a one-sample approximation of ζ within an energy-based framework"
  - [corpus] Weak evidence - no direct mention of WGAN bias in corpus, but MonoFlow paper mentions WGAN perspective
- **Break condition**: If the generator density PGψ is not well-matched to the data distribution, the importance sampling weights become highly variable, making the unbiased estimator unstable

### Mechanism 2
- **Claim**: Adding an entropy maximization term to the generator objective prevents mode collapse
- **Mechanism**: The entropy term H(PGψ) encourages the generator to spread its probability mass across the latent space rather than concentrating on a few modes, which corresponds to maximizing diversity in generated samples
- **Core assumption**: Maximizing generator entropy correlates with better mode coverage in the data space
- **Evidence anchors**:
  - [abstract] "when optimizing for likelihood, one must maximize generator entropy. This is hypothesized to provide a better mode coverage"
  - [section 2.2] "We hypothesize that this term is responsible for ensuring diversity in the generated samples, and that its introduction can reduce the well-known problem of mode collapse in GANs"
  - [corpus] Weak evidence - no direct mention of entropy in corpus, but Distilling Normalizing Flows mentions density estimation
- **Break condition**: If the entropy weight is too large, the generator may prioritize diversity over sample quality, producing low-density regions

### Mechanism 3
- **Claim**: One-way flows enable flexible generator architectures while maintaining tractable density computation
- **Mechanism**: By only computing the forward pass and Jacobian determinant (not the inverse), the architecture can increase dimensionality and use arbitrary layers, unlike standard normalizing flows
- **Core assumption**: We only need to evaluate the generator density at points sampled from the generator, not at arbitrary data points
- **Evidence anchors**:
  - [abstract] "The generator density is obtained via a new type of flow network, called one-way flow network, that is less constrained in terms of architecture, as it does not require to have a tractable inverse function"
  - [section 2.4] "we need to evaluate the generator density only at points sampled from the generator y=Gψ(z). Therefore, we do not need to compute the inverse function G−1ψ(x) explicitly"
  - [corpus] Weak evidence - no direct mention of one-way flows in corpus, but Distilling Normalizing Flows mentions flow-based models
- **Break condition**: If the Jacobian approximation (using single random vector) becomes too inaccurate, the density estimates may become unreliable

## Foundational Learning

- **Concept**: Energy-based models and partition functions
  - Why needed here: The paper builds on the connection between EBMs and GANs, where the discriminator estimates an unnormalized density and the partition function must be handled carefully
  - Quick check question: What is the partition function in an EBM, and why is it typically intractable?

- **Concept**: Importance sampling and Monte Carlo estimation
  - Why needed here: The paper uses importance sampling to estimate the partition function, and the bias in WGAN comes from using a one-sample approximation instead of proper importance weights
  - Quick check question: How does importance sampling reduce variance compared to naive Monte Carlo estimation?

- **Concept**: Normalizing flows and Jacobian determinants
  - Why needed here: The one-way flow architecture relies on computing Jacobian determinants efficiently, and understanding why standard flows require invertibility
  - Quick check question: Why do standard normalizing flows require bijective mappings and tractable inverses?

## Architecture Onboarding

- **Component map**: Latent distribution P(Z) -> One-way flow Gψ -> Data space, with Discriminator Dθ estimating unnormalized density and Jacobian approximation module computing ∥J v∥

- **Critical path**: 
  1. Sample z ~ P(Z)
  2. Pass through one-way flow Gψ to get y = Gψ(z)
  3. Compute PGψ(y) using Eq. (10) with Jacobian approximation
  4. Compute discriminator loss using unbiased partition function estimate
  5. Compute generator loss with entropy term
  6. Update parameters with ADAM optimizer

- **Design tradeoffs**:
  - One-way flow vs standard flow: More flexible architecture but requires Jacobian approximation instead of exact computation
  - Entropy weight w: Balances diversity vs quality in generated samples
  - Number of samples S for partition function: More samples reduce variance but increase computation time

- **Failure signatures**:
  - Discriminator loss not decreasing: Generator density PGψ poorly matched to data, importance weights have high variance
  - Generator producing low-quality samples: Entropy weight too high, prioritizing diversity over quality
  - Training instability: Jacobian approximation breaking down (approaching singularity)

- **First 3 experiments**:
  1. Implement synthetic 2D GMM experiments (ring and grid) to verify mode coverage and density estimation qualitatively
  2. Test with exact Jacobian determinant (small dimensions) to verify that unbiased estimator works better than WGAN
  3. Implement DCGAN-based architecture with one-way flow for CIFAR-10 and measure FID compared to standard WGAN

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal weighting factor w in the scaled KL-divergence (f-divergence with f(t) = wt log t) for the generator objective, and how does it affect training stability and convergence speed?
- Basis in paper: [explicit] The paper mentions adding a weight w to the entropy term in the generator objective to address numerical stability issues, but does not explore the optimal value or its effects.
- Why unresolved: The paper only briefly mentions the introduction of w for numerical stability but does not provide a systematic study of its optimal value or impact on training dynamics.
- What evidence would resolve it: Experiments comparing training stability and convergence speed across different values of w would provide insights into the optimal weighting factor.

### Open Question 2
- Question: How does the choice of divergence (e.g., KL-divergence vs. other f-divergences) in the generator objective affect the mode coverage and diversity of generated samples?
- Basis in paper: [inferred] The paper mentions that "any other choice of divergence is in principle valid as objective function" but only uses the KL-divergence. It hypothesizes that the entropy term helps with mode coverage but does not empirically compare different divergences.
- Why unresolved: The paper does not explore alternative divergences or provide empirical evidence for the impact of divergence choice on mode coverage and sample diversity.
- What evidence would resolve it: Comparative experiments using different divergences in the generator objective, along with quantitative and qualitative analysis of mode coverage and sample diversity, would provide insights into the optimal divergence choice.

### Open Question 3
- Question: What is the impact of using an exact computation of the Jacobian determinant (instead of the one-sample approximation) on the training speed and sample quality of the model?
- Basis in paper: [explicit] The paper uses a one-sample approximation for the Jacobian determinant for computational efficiency and mentions that an exact computation could reduce estimation variance and potentially speed up training.
- Why unresolved: The paper does not provide empirical evidence for the impact of using an exact computation of the Jacobian determinant on training speed and sample quality.
- What evidence would resolve it: Experiments comparing the training speed and sample quality of the model when using an exact computation of the Jacobian determinant versus the one-sample approximation would provide insights into the trade-off between computational efficiency and estimation accuracy.

### Open Question 4
- Question: How does the proposed method perform on higher-dimensional datasets (e.g., ImageNet) compared to state-of-the-art GANs and normalizing flow models?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of the proposed method on CIFAR-10 and CelebA datasets but does not explore its performance on higher-dimensional datasets like ImageNet.
- Why unresolved: The paper does not provide empirical evidence for the scalability of the proposed method to higher-dimensional datasets or compare its performance to state-of-the-art models on such datasets.
- What evidence would resolve it: Experiments evaluating the proposed method on higher-dimensional datasets like ImageNet and comparing its performance to state-of-the-art GANs and normalizing flow models would provide insights into its scalability and competitiveness.

## Limitations
- The one-way flow architecture description lacks implementation details for critical components like Jacobian approximation
- The claim that WGAN performs a "biased estimate" is primarily theoretical and lacks empirical validation
- The entropy maximization term's specific contribution to mode coverage is hypothesized but not quantitatively validated

## Confidence
- **High Confidence**: The mathematical derivation of unbiased partition function estimation and the connection between WGAN discriminators and EBMs
- **Medium Confidence**: The claim that one-way flows enable more flexible generator architectures than standard flows
- **Low Confidence**: The specific contribution of entropy maximization to mode coverage and the practical benefits over existing methods

## Next Checks
1. Implement a controlled experiment comparing WGAN with exact partition function estimation versus the proposed biased estimator on synthetic 2D distributions
2. Perform ablation studies removing the entropy term to quantify its specific contribution to mode coverage and sample quality
3. Test the one-way flow Jacobian approximation with varying dimensions and noise injection strategies to identify stability boundaries