---
ver: rpa2
title: Rethinking Explaining Graph Neural Networks via Non-parametric Subgraph Matching
arxiv_id: '2301.02780'
source_url: https://arxiv.org/abs/2301.02780
tags:
- graph
- matchexplainer
- subgraph
- learning
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MatchExplainer, a non-parametric approach
  for explaining Graph Neural Networks (GNNs) by identifying key explanatory subgraphs
  through subgraph matching. The method leverages the observation that graphs often
  share common motif patterns, and uses node correspondence-based distance in high-dimensional
  feature space to find the most crucial joint substructure between pairs of graphs.
---

# Rethinking Explaining Graph Neural Networks via Non-parametric Subgraph Matching

## Quick Facts
- arXiv ID: 2301.02780
- Source URL: https://arxiv.org/abs/2301.02780
- Authors: 
- Reference count: 23
- Key outcome: Introduces MatchExplainer, a non-parametric approach for explaining GNNs by identifying key explanatory subgraphs through subgraph matching, outperforming state-of-the-art parametric baselines.

## Executive Summary
This paper introduces MatchExplainer, a non-parametric approach for explaining Graph Neural Network (GNN) predictions by identifying key explanatory subgraphs through subgraph matching. The method leverages the observation that graphs often share common motif patterns and uses node correspondence-based distance in high-dimensional feature space to find the most crucial joint substructure between graph pairs. To address the false positive sampling problem in existing graph sampling methods, the authors propose MatchDrop, which keeps the most informative portion of the graph unchanged while performing augmentations on the less informative parts. Extensive experiments on synthetic and real-world datasets demonstrate that MatchExplainer outperforms state-of-the-art parametric baselines with significant margins.

## Method Summary
MatchExplainer is a non-parametric subgraph matching algorithm that explains GNN predictions by identifying explanatory subgraphs. For each graph, it finds counterpart graphs in a reference set with the same predicted class, then identifies the joint substructure that minimizes node correspondence-based distance in feature space. The minimal sufficient explanation is selected based on prediction change. MatchDrop is an augmentation technique that uses MatchExplainer outputs to preserve the most informative portion of graphs during augmentation, addressing the false positive sampling problem in conventional graph augmentation methods.

## Key Results
- MatchExplainer outperforms state-of-the-art parametric baselines on multiple datasets (MUTAG, BA-3Motif, MNIST, VG-5) with significant margins in ACC-AUC
- MatchDrop improves the performance of conventional graph augmentation methods when combined with GNNs
- The minimal sufficient explanation can approximate the ground truth explanation with theoretical guarantees
- Extensive experiments demonstrate the effectiveness of the proposed approach across diverse graph classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-parametric subgraph matching can find explanatory subgraphs by identifying the most informative joint substructure between graph pairs.
- Mechanism: The algorithm iteratively couples a target graph with counterpart graphs from a reference set and minimizes the node correspondence-based distance in high-dimensional feature space to extract the most relevant joint substructure.
- Core assumption: Graphs sharing the same prediction often contain common motif patterns that are responsible for the prediction.
- Evidence anchors:
  - [abstract] "graphs typically share some common motif patterns" and "identifies the most crucial joint substructure by minimizing the node corresponding-based distance"
  - [section 3.1] "We break the target GNN hY into two consecutive parts" and define node correspondence-based distance in Equation 8
  - [corpus] Weak evidence - no direct corpus match found
- Break condition: If graphs with the same prediction don't share meaningful motifs, the matching approach fails to find useful explanations.

### Mechanism 2
- Claim: The minimal sufficient explanation contains the least information about the graph while preserving all information relevant to the prediction task.
- Mechanism: Among all sufficient explanations, the minimal sufficient explanation contains only the shared information between the graph and its counterpart, eliminating non-shared information.
- Core assumption: The minimal sufficient explanation can approximate the ground truth explanation while containing less task-relevant information than any other sufficient explanation.
- Evidence anchors:
  - [section 3.1] Definition 3.2 and Theorem 3.3 explicitly define minimal sufficient explanation and prove its properties
  - [abstract] "the minimal sufficient explanation can be used to approximate the desired ground truth explanation with a theoretical guarantee"
  - [corpus] Weak evidence - no direct corpus match found
- Break condition: If the task-relevant information cannot be separated from non-relevant information, the minimal sufficient explanation may not contain the true explanatory subgraph.

### Mechanism 3
- Claim: MatchDrop improves graph augmentation by keeping the most informative portion of the graph unchanged while applying augmentations to less informative parts.
- Mechanism: MatchExplainer identifies the explanatory subgraph, which is preserved unchanged, while random sampling or node-dropping operations are applied only to the remaining portions of the graph.
- Core assumption: The false positive sampling problem occurs when the most informative substructure is accidentally dropped or erased during random augmentations.
- Evidence anchors:
  - [abstract] "present graph sampling or node-dropping methods usually suffer from the false positive sampling problem" and MatchDrop "keeps the most informative portion of the graph unchanged"
  - [section 4] Detailed description of how MatchDrop works and comparison with previous methods
  - [corpus] Weak evidence - no direct corpus match found
- Break condition: If the MatchExplainer fails to correctly identify the informative substructure, MatchDrop cannot effectively prevent false positive sampling.

## Foundational Learning

- Concept: Graph Neural Networks and their explainability
  - Why needed here: The paper addresses explaining GNN predictions by finding explanatory subgraphs
  - Quick check question: What is the fundamental challenge in explaining GNN predictions that this paper addresses?

- Concept: Graph matching and similarity learning
  - Why needed here: The method uses subgraph matching between graph pairs to find explanations
  - Quick check question: How does graph matching differ from the approach used in this paper for finding explanatory subgraphs?

- Concept: Information theory and mutual information
  - Why needed here: The theoretical analysis uses mutual information to characterize explanations and sufficient explanations
  - Quick check question: What is the difference between sufficient explanation and minimal sufficient explanation in terms of mutual information?

## Architecture Onboarding

- Component map:
  - MatchExplainer: Non-parametric subgraph matching algorithm
  - MatchDrop: Augmentation technique using MatchExplainer outputs
  - Target GNN: The model being explained
  - Reference set: Collection of graphs for finding counterpart graphs
  - Distance metric: Node correspondence-based distance in feature space

- Critical path:
  1. Train target GNN to convergence
  2. Build reference set of graphs with same labels
  3. For each graph, find counterpart graphs in reference set
  4. Perform subgraph matching to find explanatory subgraphs
  5. Optimize final explanation using prediction difference
  6. Use explanations for MatchDrop augmentation if desired

- Design tradeoffs:
  - Non-parametric vs parametric explainers: Training-free vs potentially more accurate but requires training
  - Multiple explanations vs single explanation: More comprehensive but harder to select optimal one
  - Reference set size: Larger sets provide better explanations but increase computation time

- Failure signatures:
  - Poor accuracy when graphs don't share common motifs
  - Slow performance when reference set is very large
  - Ineffective explanations when target GNN is not well-trained
  - MatchDrop fails when explanatory subgraph identification is incorrect

- First 3 experiments:
  1. Run MatchExplainer on MUTAG dataset with small reference set (10% of training data) and verify ACC-AUC
  2. Test MatchDrop augmentation on GCN with MUTAG dataset and compare accuracy with standard DropNode
  3. Vary reference set size from 10% to 100% of training data on MNIST and measure impact on explanation quality and runtime

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the size of the reference set DG impact the effectiveness and efficiency of MatchExplainer?
- Basis in paper: [explicit] The paper mentions that the size of DG plays a vital role in determining the time cost, with a total time cost of O(K|DG|), and that a limited number of counterpart graphs can also prohibit it from exploring better explanatory subgraphs.
- Why unresolved: The paper does not provide experimental results or guidelines on how to choose an appropriate size for DG to balance effectiveness and efficiency.
- What evidence would resolve it: Empirical studies showing the trade-off between the size of DG and the performance of MatchExplainer in terms of accuracy and computational cost.

### Open Question 2
- Question: Can MatchDrop be extended to other graph augmentation techniques beyond graph sampling and node dropping?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of MatchDrop in improving conventional graph augmentation methods like graph sampling and node dropping, suggesting potential applicability to other augmentation techniques.
- Why unresolved: The paper does not explore the application of MatchDrop to other graph augmentation methods such as edge addition/deletion or feature-wise augmentations.
- What evidence would resolve it: Experimental results showing the performance of MatchDrop when combined with other graph augmentation techniques, comparing it to the original methods.

### Open Question 3
- Question: How does MatchExplainer perform on graph classification tasks with larger and more complex graph structures?
- Basis in paper: [explicit] The paper evaluates MatchExplainer on datasets like MUTAG, VG-5, MNIST, and BA-3Motif, which have relatively small and simple graph structures.
- Why unresolved: The paper does not provide results on datasets with larger and more complex graph structures, leaving uncertainty about the scalability and performance of MatchExplainer in such scenarios.
- What evidence would resolve it: Experimental results on datasets with larger and more complex graph structures, comparing the performance of MatchExplainer to other state-of-the-art explanation methods.

## Limitations
- The method relies heavily on the assumption that graphs with the same prediction share common motif patterns, which may not hold for many real-world datasets
- Computational complexity scales poorly with reference set size due to pairwise graph matching requirements
- Performance may degrade when explanatory subgraphs cannot be identified through structural similarity between graph pairs

## Confidence
- **High confidence**: The theoretical framework for minimal sufficient explanation (Definition 3.2, Theorem 3.3) is mathematically rigorous and well-founded
- **Medium confidence**: The empirical evaluation demonstrates strong performance on benchmark datasets, but the selection of datasets may favor the method's assumptions about shared motifs
- **Medium confidence**: The MatchDrop augmentation scheme shows promise, but its generalizability beyond the tested GNN architectures needs further validation

## Next Checks
1. **Cross-dataset robustness test**: Evaluate MatchExplainer on datasets where graphs with the same label have minimal structural similarity (e.g., social network graphs) to assess performance breakdown conditions
2. **Scalability benchmark**: Measure runtime and memory scaling as reference set size increases from 10% to 100% of training data across all four benchmark datasets
3. **Ablation study on reference set quality**: Compare explanation quality when reference sets are constructed from: (a) same predicted class, (b) same true class, and (c) random sampling to quantify the impact of reference set composition