---
ver: rpa2
title: User Persona Identification and New Service Adaptation Recommendation
arxiv_id: '2311.10773'
source_url: https://arxiv.org/abs/2311.10773
tags:
- user
- service
- persona
- sessionbert
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: User Persona Identification and New Service Adaptation Recommendation
  addresses the problem of providing personalized user experiences on information-dense
  webpages. The paper introduces SessionBERT, a Transformer-based language model trained
  from scratch on user session trajectories (pages, metadata, billing) to capture
  semantics.
---

# User Persona Identification and New Service Adaptation Recommendation

## Quick Facts
- arXiv ID: 2311.10773
- Source URL: https://arxiv.org/abs/2311.10773
- Reference count: 18
- Key outcome: SessionBERT achieves 3% and 1% relative improvement in F1-score for page and service prediction tasks respectively, with 58% Hit@5 for new service recommendations

## Executive Summary
This paper addresses the challenge of providing personalized user experiences on information-dense webpages through user persona identification and new service adaptation recommendations. The authors introduce SessionBERT, a Transformer-based language model trained from scratch on user session trajectories (pages, metadata, billing) to capture semantic representations of user interactions. The model demonstrates consistent outperformance over BERT-base, particularly when computational resources are limited. The approach is extended to provide top-5 recommendations for new services and includes a method for user segmentation and persona mapping validated through human evaluation.

## Method Summary
The method involves training SessionBERT from scratch on 9MM user session trajectories using masked language modeling to learn semantic representations of user interactions. User segmentation is performed using K-means clustering on SessionBERT embeddings, with clusters mapped to predefined personas based on task probabilities. The model is fine-tuned for service and page prediction tasks using multi-task learning. New service recommendations are generated by storing the last 8 sessions per user and ranking services by occurrence frequency, filtering out already-used services. The entire pipeline is evaluated using F1-score, Hit@5 metrics, and human evaluation.

## Key Results
- SessionBERT outperforms BERT-base by 3% and 1% relative improvement in F1-score for page and service prediction tasks respectively
- Top-5 new service recommendations achieve a Hit@5 of 58%
- Optimal user segmentation achieved with K-means clustering on SessionBERT embeddings
- SessionBERT shows 10-fold decrease in fine-tuning time for sequence length of 64 compared to 512

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SessionBERT captures semantics in user trajectories better than BERT-base, leading to improved prediction accuracy.
- Mechanism: SessionBERT is trained from scratch on masked language modeling of user session data (pages, metadata, billing), which allows it to learn representations that capture the semantics of user interactions.
- Core assumption: The masked language modeling objective on user session data effectively learns semantic representations.
- Evidence anchors:
  - [abstract] "Our results show that representations learned through SessionBERT are able to consistently outperform a BERT-base model providing a 3% and 1% relative improvement in F1-score for predicting page links and next services."
  - [section] "SessionBERT consistently outperforms BERT-Base. It takes 10-fold decrease in fine-tuning time for sequence length of 64 compared to 512, and so therefore SessionBERT is especially helpful when limited computational resources exists as the results for SessionBERT compared to BERT-Base for length 64 is more pronounced."
- Break condition: If the masked language modeling objective does not effectively learn semantic representations from user session data, or if the data does not contain sufficient semantic information, the mechanism would fail.

### Mechanism 2
- Claim: User segmentation using SessionBERT embeddings improves persona identification.
- Mechanism: SessionBERT embeddings are clustered using K-means to segment users, and these segments are then mapped to pre-defined personas based on the activities performed within each cluster.
- Core assumption: Users with similar SessionBERT embeddings have similar behaviors and can be effectively grouped into the same persona.
- Evidence anchors:
  - [section] "Using the embeddings obtained from SessionBERT, we segmented session representations using K-means clustering using cosine similarity as our distance metric."
  - [section] "To determine the number of clusters and to additionally evaluate SessionBERT embeddings, we experimented with different values of k (k = 3 âˆ’ 9) for both BERT-Base, and SessionBERT and calculated the silhouette score for each run."
- Break condition: If the SessionBERT embeddings do not capture meaningful behavioral differences between users, or if the number of clusters is not appropriately chosen, the user segmentation would not effectively map to personas.

### Mechanism 3
- Claim: Recommendations based on the frequency of service occurrence improve the relevance of new service suggestions.
- Mechanism: The top-5 new service recommendations are determined by the number of times each service is recommended to a user, with services already used by the user filtered out.
- Core assumption: Services recommended more frequently are more likely to be relevant and interesting to the user.
- Evidence anchors:
  - [section] "During inference, the top-5 service recommendations were calculated for each user from our SessionBERT model and if a service was already used by a user, it was filtered out. Finally, the top 5 new service recommendations was calculated based on the number of times they were recommended to them in our system."
  - [section] "We picked number of occurrence of a new service as our ranking algorithm based on a human evaluation study explained in Appendix C."
- Break condition: If the frequency of recommendation does not correlate with user interest or relevance, or if the human evaluation study is not representative, this ranking approach would fail.

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: MLM is used to pre-train SessionBERT on user session data, allowing it to learn semantic representations of user interactions.
  - Quick check question: What is the objective of masked language modeling, and how does it help in learning semantic representations?

- Concept: K-means clustering
  - Why needed here: K-means clustering is used to segment users based on their SessionBERT embeddings, which is a crucial step in persona identification.
  - Quick check question: How does K-means clustering work, and why is it suitable for segmenting users based on embeddings?

- Concept: Cosine similarity
  - Why needed here: Cosine similarity is used as the distance metric in K-means clustering to measure the similarity between user embeddings.
  - Quick check question: What is cosine similarity, and why is it a good choice for measuring the similarity between embeddings?

## Architecture Onboarding

- Component map:
  - Data ingestion: Collects user session data (pages, metadata, billing)
  - Tokenizer: Creates a vocabulary from user session data
  - SessionBERT: Transformer-based language model trained from scratch on user session data
  - Clustering: Segments users based on SessionBERT embeddings using K-means
  - Persona mapping: Maps clusters to pre-defined personas based on user activities
  - Recommendation engine: Provides top-5 new service recommendations based on SessionBERT outputs

- Critical path:
  1. Data ingestion and preprocessing
  2. Tokenizer creation and SessionBERT training
  3. User segmentation using K-means clustering
  4. Persona mapping
  5. Recommendation generation

- Design tradeoffs:
  - Training SessionBERT from scratch vs. fine-tuning a pre-trained model: Training from scratch allows for better adaptation to the specific semantics of user session data but requires more data and computational resources
  - Number of clusters in K-means: Choosing the right number of clusters is crucial for effective user segmentation and persona mapping

- Failure signatures:
  - Poor clustering results: Indicates that the SessionBERT embeddings do not capture meaningful behavioral differences between users
  - Low recommendation accuracy: Suggests that the SessionBERT model does not effectively learn the semantics of user interactions or that the ranking approach is not suitable

- First 3 experiments:
  1. Compare the performance of SessionBERT and BERT-base on a small subset of the data to validate the effectiveness of training from scratch
  2. Experiment with different numbers of clusters in K-means to find the optimal number for user segmentation
  3. Evaluate the recommendation accuracy using different ranking approaches (e.g., frequency-based vs. similarity-based) to determine the best method

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SessionBERT change when trained on a larger vocabulary size beyond 30k tokens?
- Basis in paper: [explicit] The paper mentions that a tokenizer with a vocabulary size of 30k was used for training SessionBERT.
- Why unresolved: The paper does not explore the impact of varying vocabulary sizes on the model's performance.
- What evidence would resolve it: Experiments comparing the performance of SessionBERT trained with different vocabulary sizes, such as 50k or 100k tokens, on the same tasks (service and page prediction) would provide insights into the optimal vocabulary size for this application.

### Open Question 2
- Question: How does the quality of recommendations provided by SessionBERT change when using a different ranking algorithm, such as one based on user persona similarity instead of the number of times a service is recommended?
- Basis in paper: [inferred] The paper discusses the use of SessionBERT embeddings for user segmentation and persona mapping, suggesting that persona information could be leveraged for recommendations.
- Why unresolved: The paper only evaluates the performance of the current ranking algorithm (based on the number of times a service is recommended) and does not explore alternative ranking methods that incorporate persona information.
- What evidence would resolve it: Comparing the performance of SessionBERT recommendations using different ranking algorithms, including one based on user persona similarity, would help determine the most effective approach for providing personalized recommendations.

### Open Question 3
- Question: How does the performance of SessionBERT on service and page prediction tasks change when using a different sequence length, such as 128 or 256 tokens, compared to the tested lengths of 64 and 512?
- Basis in paper: [explicit] The paper presents ablation studies on the impact of sequence length (64 and 512 tokens) on the performance of SessionBERT and BERT-base.
- Why unresolved: The paper does not explore the performance of SessionBERT at intermediate sequence lengths, such as 128 or 256 tokens, which could provide insights into the optimal sequence length for this application.
- What evidence would resolve it: Experiments comparing the performance of SessionBERT on service and page prediction tasks using different sequence lengths, including 128 and 256 tokens, would help determine the most effective sequence length for this application.

## Limitations
- The paper lacks specific implementation details including hyperparameters for SessionBERT training, making independent reproduction challenging
- Human evaluation methodology for ranking algorithms is mentioned but not detailed, limiting assessment of recommendation quality
- Persona definitions are anonymized, preventing full understanding of the segmentation criteria

## Confidence
- Claims about SessionBERT outperforming BERT-base: Medium
- Claims about user segmentation effectiveness: Medium
- Claims about recommendation accuracy (58% Hit@5): Medium

## Next Checks
1. Implement hyperparameter sensitivity analysis for SessionBERT training to understand optimal configuration
2. Compare with alternative user segmentation methods beyond K-means to validate approach
3. Conduct detailed ablation study examining contribution of different model components to final performance