---
ver: rpa2
title: Stateful Conformer with Cache-based Inference for Streaming Automatic Speech
  Recognition
arxiv_id: '2312.17279'
source_url: https://arxiv.org/abs/2312.17279
tags:
- streaming
- look-ahead
- accuracy
- size
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an efficient streaming speech recognition model
  based on the FastConformer architecture. The key innovation is a caching mechanism
  that converts the non-autoregressive encoder into an autoregressive model during
  inference, allowing it to operate in a recurrent manner.
---

# Stateful Conformer with Cache-based Inference for Streaming Automatic Speech Recognition

## Quick Facts
- arXiv ID: 2312.17279
- Source URL: https://arxiv.org/abs/2312.17279
- Authors: 
- Reference count: 0
- Primary result: Achieves 7.1% WER on LibriSpeech with 1360ms average latency using cache-based streaming inference

## Executive Summary
This paper proposes a streaming automatic speech recognition model based on the FastConformer architecture that addresses the accuracy-latency tradeoff through a novel cache-based inference mechanism. The key innovation converts the non-autoregressive encoder into an autoregressive model during inference by caching intermediate activations, eliminating the training-inference inconsistency common in streaming models. The model uses limited left and right contexts during training and employs a hybrid CTC/RNNT architecture for improved accuracy and convergence speed. Experimental results show the proposed approach outperforms conventional buffered streaming models on LibriSpeech with better accuracy and lower latency.

## Method Summary
The proposed model uses a FastConformer encoder with causal convolutions and limited self-attention for streaming processing. The core innovation is a caching mechanism that stores intermediate activations during inference, allowing the encoder to operate autoregressively by reusing these cached values across chunks. The model employs chunk-aware look-ahead processing where input audio is split into chunks with consistent context windows, and uses a hybrid CTC/RNNT architecture with a shared encoder trained on weighted combination of both losses. The training process uses limited left and right contexts to maintain consistency with streaming inference constraints.

## Key Results
- Achieves 7.1% WER on LibriSpeech test-clean with 1360ms average latency
- Outperforms buffered streaming models in both accuracy and latency tradeoff
- Hybrid CTC/RNNT architecture improves convergence speed and accuracy
- Cache-based inference eliminates training-inference disparity while maintaining streaming capability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The cache-based inference converts a non-autoregressive encoder into an autoregressive one, eliminating training-inference inconsistency
- Mechanism: During streaming inference, intermediate activations from previous timesteps are cached and reused in future steps, converting the encoder from non-autoregressive (train) to autoregressive (inference) operation
- Core assumption: Cached activations remain valid and can be directly reused without recomputation
- Evidence anchors:
  - [abstract] "introducing an activation caching mechanism to enable the non-autoregressive encoder to operate autoregressively during inference"
  - [section 3.3] "We propose a caching approach to avoid these recomputations and have zero duplication in streaming inference"
  - [corpus] Weak evidence - only 5 related papers found with average neighbor FMR=0.354, suggesting moderate relevance but limited direct citations

### Mechanism 2
- Claim: Chunk-aware look-ahead provides better accuracy-latency tradeoff than regular look-ahead
- Mechanism: Input audio is split into chunks where tokens in each chunk have access to all tokens in the same chunk plus a limited number of previous chunks, eliminating depth-dependent look-ahead growth
- Core assumption: Chunk boundaries can be chosen to maintain consistent look-ahead without excessive recomputation
- Evidence anchors:
  - [section 3.1] "Chunk-aware look-ahead addresses both the above issues. It splits the input audio into chunks of size C"
  - [section 3.1] "Due to chunking, the output predictions of the encoder for all the tokens in each chunk will be valid and there is no need to recompute any activation for the future tokens"
  - [corpus] Moderate evidence - several papers discuss chunk-aware approaches but no direct citations provided

### Mechanism 3
- Claim: Hybrid CTC/RNNT architecture improves accuracy and convergence speed while sharing computation
- Mechanism: Shared encoder is trained with weighted combination of CTC and RNNT losses, allowing joint optimization that benefits both decoders
- Core assumption: Shared encoder features are useful for both CTC and RNNT objectives without significant interference
- Evidence anchors:
  - [abstract] "hybrid CTC/RNNT architecture which utilizes a shared encoder with both a CTC and RNNT decoder to boost the accuracy and save computation"
  - [section 3.2] "ltotal = α ∗ lctc + lrnnt where ltotal is the total loss to get optimized"
  - [section 4] "The hybrid architecture has the following advantages over single decoder models: 1) no need to train two separate models and saves significant compute"

## Foundational Learning

- Concept: Causal convolutions and self-attention
  - Why needed here: The model requires left-context-only processing for streaming, so understanding how to implement causal operations is fundamental
  - Quick check question: How would you modify a standard convolution to make it causal?

- Concept: Cache-based recurrence
  - Why needed here: The core innovation relies on caching intermediate activations for reuse, similar to how RNNs maintain hidden states
  - Quick check question: What information needs to be cached for self-attention layers versus convolution layers?

- Concept: Chunk-based processing
  - Why needed here: The chunk-aware approach divides input into manageable pieces while maintaining context, requiring understanding of boundary conditions
  - Quick check question: How does the look-ahead window change across different positions within a chunk?

## Architecture Onboarding

- Component map: Audio → Feature extraction → Encoder (FastConformer with causal convolutions, limited self-attention) → Cache mechanism (per-layer activation storage) → Decoder (CTC/RNNT hybrid) → Text output
- Critical path: Audio → Feature extraction → Encoder with cache → Decoder → Text output
- Design tradeoffs: Context size vs latency vs accuracy, chunk size vs computation overhead, single vs multiple look-ahead training
- Failure signatures: Degradation in accuracy when context windows are too small, excessive latency from large look-ahead, cache overflow for long sequences
- First 3 experiments:
  1. Implement and test causal convolution layers with varying kernel sizes
  2. Add cache mechanism to self-attention layers and verify output consistency
  3. Compare regular vs chunk-aware look-ahead with different chunk sizes on validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed cache-aware streaming FastConformer architecture compare in terms of computational efficiency and accuracy to other state-of-the-art streaming ASR models, such as those based on Transformer or RNN architectures?
- Basis in paper: [inferred] The paper compares the proposed model to a buffered streaming baseline but does not provide a comprehensive comparison with other streaming ASR models.
- Why unresolved: The paper focuses on demonstrating the advantages of the proposed cache-aware streaming approach over a buffered streaming baseline. A broader comparison with other state-of-the-art streaming ASR models would require additional experiments and evaluations.
- What evidence would resolve it: A comprehensive evaluation of the proposed model against other state-of-the-art streaming ASR models on multiple benchmark datasets, including comparisons of accuracy, latency, and computational efficiency.

### Open Question 2
- Question: What is the impact of the hybrid CTC/RNNT architecture on the accuracy and convergence speed of the proposed streaming model compared to using only a CTC or RNNT decoder?
- Basis in paper: [explicit] The paper introduces a hybrid CTC/RNNT architecture and claims it improves accuracy and convergence speed, but does not provide a detailed analysis of its impact.
- Why unresolved: While the paper mentions the benefits of the hybrid architecture, it does not provide a detailed comparison of the accuracy and convergence speed when using only a CTC or RNNT decoder versus the hybrid approach.
- What evidence would resolve it: A detailed ablation study comparing the accuracy and convergence speed of the proposed model with the hybrid CTC/RNNT architecture to models using only a CTC or RNNT decoder.

### Open Question 3
- Question: How does the proposed cache-aware streaming approach perform in real-world scenarios with varying acoustic conditions, such as noisy environments or accented speech?
- Basis in paper: [inferred] The paper evaluates the proposed model on multiple benchmark datasets, but does not specifically address its performance in challenging acoustic conditions.
- Why unresolved: The paper focuses on demonstrating the effectiveness of the proposed approach on standard benchmark datasets. Evaluating its robustness in real-world scenarios with varying acoustic conditions would require additional experiments and data collection.
- What evidence would resolve it: A comprehensive evaluation of the proposed model's performance in real-world scenarios with varying acoustic conditions, including comparisons with other state-of-the-art streaming ASR models.

## Limitations
- Architecture details not fully specified (layer counts, hidden dimensions, downsampling rates)
- Performance validation limited to LibriSpeech dataset without testing on diverse acoustic conditions
- Cache implementation complexity and edge cases not addressed (cache invalidation, long sequence handling)

## Confidence

**High Confidence**: The core cache-based inference mechanism and its ability to eliminate training-inference disparity is well-supported by the described approach.

**Medium Confidence**: Accuracy improvements (7.1% WER) and latency measurements are reported but depend heavily on implementation details not fully specified in the paper.

**Low Confidence**: Claims about zero duplication in caching assume ideal conditions that may not hold in practice for long sequences or edge cases.

## Next Checks

1. **Architecture Fidelity Check**: Implement the FastConformer encoder with configurable context windows and verify that causal convolutions and limited self-attention produce consistent outputs across different context sizes. Measure accuracy degradation as context is progressively reduced.

2. **Cache Consistency Validation**: Create a test suite that processes the same audio with and without caching, comparing intermediate activations and final outputs. Identify conditions under which cached activations become invalid or produce incorrect results.

3. **Latency Breakdown Analysis**: Instrument the streaming inference pipeline to measure time spent in feature extraction, encoding with caching, and decoding separately. Verify that the reported 1360ms EIL aligns with measured encoding time and identify bottlenecks.