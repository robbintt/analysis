---
ver: rpa2
title: 'Two Sides of Miscalibration: Identifying Over and Under-Confidence Prediction
  for Network Calibration'
arxiv_id: '2308.03172'
source_url: https://arxiv.org/abs/2308.03172
tags:
- calibration
- confidence
- techniques
- cwmcs
- miscalibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a key issue in deep learning calibration:
  existing techniques focus only on over-confidence and can worsen under-confidence,
  which is prevalent in modern architectures like Vision Transformers. The authors
  introduce a novel miscalibration score (MCS) metric to quantify both under and over-confidence,
  overall and class-wise.'
---

# Two Sides of Miscalibration: Identifying Over and Under-Confidence Prediction for Network Calibration

## Quick Facts
- arXiv ID: 2308.03172
- Source URL: https://arxiv.org/abs/2308.03172
- Authors: 
- Reference count: 12
- One-line primary result: Existing calibration techniques over-calibrate modern models like Vision Transformers, worsening under-confidence; the proposed class-wise MCS-aware temperature scaling improves both ECE and failure detection.

## Executive Summary
This paper highlights a critical gap in deep learning calibration: most techniques focus only on reducing over-confidence and inadvertently worsen under-confidence, which is prevalent in modern architectures like Vision Transformers. The authors introduce a novel Miscalibration Score (MCS) that quantifies both under and over-confidence, both overall and per class, by using a signed difference between confidence and accuracy. They propose a class-wise MCS-aware temperature scaling method that corrects both types of miscalibration simultaneously. Experiments show that their approach significantly improves calibration metrics and enhances failure detection, demonstrating its importance for reliable deployment in safety-critical applications.

## Method Summary
The method introduces MCS, a signed calibration metric that distinguishes under from over-confidence, and weighted subset MCS (wsMCS) for imbalanced datasets. It then proposes class-wise MCS-aware temperature scaling (cwMCS TS), which applies a per-class temperature adjustment vector to correct both types of miscalibration. This approach extends standard temperature scaling by allowing different scaling factors per class, tuned by MCS values, thus preventing over-correction of under-confidence or vice versa.

## Key Results
- Vision Transformers and ConvMixer models are often under-confident, while standard calibration techniques (TS, label smoothing, focal loss) tend to over-calibrate them, worsening under-confidence.
- The proposed class-wise MCS-aware temperature scaling significantly improves ECE and weighted subset ECE scores compared to baseline methods.
- Calibrated models using cwMCS TS demonstrate better failure detection in risk-coverage curves, indicating enhanced trustworthiness for deployment.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The novel Miscalibration Score (MCS) distinguishes under-confidence from over-confidence by using the signed difference between confidence and accuracy instead of absolute values.
- **Mechanism:** Standard ECE treats all miscalibration as equally bad. By replacing the absolute difference with a signed difference, MCS yields positive values for over-confidence and negative values for under-confidence. This allows the calibration method to adapt the temperature scaling per class: if MCS is negative (under-confident), scale up the temperature; if positive (over-confident), scale down.
- **Core assumption:** The average confidence minus accuracy per bin correctly captures the direction of miscalibration, and this direction is consistent enough across the bin to guide scaling.
- **Evidence anchors:**
  - [abstract] The paper states that MCS "quantify models as being under and over-confident, both overall and class-wise."
  - [section 3.2] The formula is given: MCS = (1/N) Σ|Bm|(confm - accm), explicitly noting that positive values indicate over-confidence and negative values indicate under-confidence.
  - [corpus] Found related papers (e.g., "Probability calibration for precipitation nowcasting") that also distinguish calibration directions, but none provide explicit MCS evidence; corpus coverage is sparse.
- **Break condition:** If the bin boundaries or sample distributions cause the signed difference to cancel out or become dominated by noise, MCS may fail to reflect true miscalibration direction.

### Mechanism 2
- **Claim:** Weighted subset MCS (wsMCS) prevents the dominance of majority classes in imbalanced datasets by weighting class-specific miscalibration by class size.
- **Mechanism:** Standard MCS averages over all classes equally, which can mask poor calibration in minority classes. wsMCS groups under-confident and over-confident classes separately, sums their weighted MCS, and then combines them proportionally to the number of classes in each group. This ensures minority-class errors are reflected in the overall score.
- **Core assumption:** Class imbalance is a real problem for calibration metrics and that weighting by sample size accurately reflects the impact of miscalibration on the overall system performance.
- **Evidence anchors:**
  - [section 3.2] The paper describes the weighted subset MCS formula: wsMCS = (k+/K)wsMCS+ + (k-/K)wsMCS-, explicitly referencing class size in the weighting.
  - [corpus] No direct corpus evidence for wsMCS weighting, but related calibration work often discusses class imbalance; coverage is weak here.
- **Break condition:** If class sizes are extremely imbalanced or the under/over-confident class counts are close to zero, wsMCS may be unstable or dominated by noise.

### Mechanism 3
- **Claim:** Class-wise MCS-aware temperature scaling (cwMCS TS) allows separate temperature adjustments per class, preventing over-correction of under-confidence or vice versa.
- **Mechanism:** Standard TS uses a single scalar temperature for all classes, which can only either increase or decrease overall confidence. cwMCS TS constructs a vector TcwMCS = T · (1 + γ · cwMCS), where γ is tuned and cwMCS is max-normalized. This per-class temperature vector can scale up confidence for under-confident classes and scale down for over-confident classes simultaneously.
- **Core assumption:** The softmax function responds linearly enough to per-class temperature scaling that the confidence distribution can be corrected in opposite directions across classes without breaking the probabilistic interpretation.
- **Evidence anchors:**
  - [section 3.2] The formula TcwMCS = T · (1 + γ · cwMCS) and the explanation that MCS can be negative or positive, enabling scaling up or down.
  - [corpus] No corpus evidence for this exact cwMCS TS approach, though focal loss and label smoothing papers in corpus discuss per-class adjustments; evidence is limited.
- **Break condition:** If the per-class temperature scaling causes numerical instability in the softmax or if the model's logits are already poorly calibrated, the corrections may backfire.

## Foundational Learning

- **Concept:** Expected Calibration Error (ECE) and its variants
  - **Why needed here:** Understanding ECE is essential because the proposed MCS builds upon it; the paper contrasts MCS with ECE to highlight its ability to capture direction of miscalibration.
  - **Quick check question:** What is the difference between ECE and MCS in terms of how they treat the confidence-accuracy gap?
    - **Answer:** ECE uses absolute value of the gap, MCS uses signed difference.

- **Concept:** Temperature scaling and Platt scaling
  - **Why needed here:** cwMCS TS extends temperature scaling; knowing how temperature scaling works is necessary to grasp why per-class adjustment is novel.
  - **Quick check question:** What is the effect of a temperature T > 1 in standard temperature scaling?
    - **Answer:** It softens the softmax distribution, reducing confidence (helps over-confidence).

- **Concept:** Risk-coverage curves and failure detection
  - **Why needed here:** The paper uses risk-coverage curves to show that calibrated models can better reject uncertain predictions, improving failure detection.
  - **Quick check question:** In a risk-coverage curve, what does a higher accuracy at a given rejection rate indicate?
    - **Answer:** Better calibration and more effective filtering of incorrect predictions.

## Architecture Onboarding

- **Component map:**
  - Input logits (z) and true labels (y) -> MCS calculator -> Temperature vector generator (cwMCS TS) -> Calibrated output (softmax(z / TcwMCS)) -> Evaluation (ECE, wsECE, wsMCS, risk-coverage curves)

- **Critical path:**
  1. Compute logits and confidences from baseline model
  2. Calculate MCS (overall and class-wise)
  3. Compute TcwMCS vector using MCS and tuned γ
  4. Apply TcwMCS to logits, pass through softmax
  5. Evaluate with ECE/wsECE/wsMCS and risk-coverage curves

- **Design tradeoffs:**
  - Per-class temperature increases model complexity but allows targeted calibration
  - Weighting by class size helps imbalance but can obscure overall calibration trends
  - Using signed MCS requires careful binning; noisy bins may mislead scaling

- **Failure signatures:**
  - If γ is poorly tuned, under-confident classes may be over-corrected, or vice versa
  - If bins are too coarse, MCS may not reflect true miscalibration direction
  - If class sizes are extremely imbalanced, wsMCS may be dominated by majority classes

- **First 3 experiments:**
  1. **Verify MCS direction detection:** Run baseline models, compute MCS, check that negative MCS corresponds to under-confidence (accuracy > confidence).
  2. **Tune γ for cwMCS TS:** Sweep γ ∈ (-1, 1) with small steps, pick γ that minimizes overall ECE.
  3. **Compare risk-coverage curves:** Apply cwMCS TS vs. baseline and TS-only; plot accuracy vs. proportion referred for each.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal method to balance under-confidence and over-confidence across classes when the calibration errors have opposing signs?
- Basis in paper: [explicit] The paper discusses that current calibration techniques tend to over-calibrate models, worsening under-confidence, and proposes class-wise MCS-aware temperature scaling to address both issues. However, it does not provide a systematic approach to optimally balance these opposing calibration errors across classes.
- Why unresolved: The paper presents a method that scales temperatures differently for each class based on their MCS, but does not explore or compare alternative strategies for balancing under and over-confidence across the entire model.
- What evidence would resolve it: Comparative experiments showing the performance of different balancing strategies (e.g., global vs. class-wise scaling, weighted averaging of under/over-confidence errors) on multiple datasets and model architectures, with quantitative metrics for calibration error, accuracy, and failure detection.

### Open Question 2
- Question: How do different types of miscalibration (under vs. over-confidence) affect model performance in safety-critical applications beyond failure detection, such as decision-making reliability or human trust calibration?
- Basis in paper: [explicit] The paper mentions that miscalibration can lead to model over-confidence and/or under-confidence, and that under-confidence can be problematic for automatic failure detection. However, it does not extensively explore the impact of different types of miscalibration on other aspects of safety-critical applications.
- Why unresolved: While the paper demonstrates improved failure detection with their method, it does not investigate how under-confidence vs. over-confidence affects other critical factors in real-world deployment, such as the reliability of decision-making processes or the calibration of human trust in the system.
- What evidence would resolve it: Empirical studies in safety-critical domains (e.g., medical diagnosis, autonomous driving) comparing the effects of under-confidence and over-confidence on decision accuracy, false positive/negative rates, and human expert reliance or trust levels when using calibrated models.

### Open Question 3
- Question: What is the relationship between model architecture (e.g., Vision Transformers vs. CNNs) and inherent miscalibration tendencies, and how can this inform the design of architecture-specific calibration techniques?
- Basis in paper: [explicit] The paper finds that Vision Transformers and ConvMixer models tend to be under-confident, while CNNs are often over-confident. This suggests an architectural bias in miscalibration tendencies, but the paper does not explore the underlying reasons or propose architecture-specific calibration approaches.
- Why unresolved: The paper identifies a pattern in miscalibration based on architecture type but does not investigate the fundamental causes (e.g., differences in feature representation, attention mechanisms, or loss landscape) or develop tailored calibration methods that exploit architectural characteristics.
- What evidence would resolve it: Analysis of the internal representations and training dynamics of different architectures to identify why certain models are prone to under-confidence or over-confidence, followed by experiments with calibration techniques specifically designed for these architectural properties.

## Limitations

- The stability and robustness of wsMCS in extremely imbalanced datasets remains unverified; no experimental or corpus evidence demonstrates performance under severe class imbalance.
- The impact of per-class temperature scaling on the probabilistic interpretation of softmax outputs is theoretically plausible but lacks empirical validation for edge cases or highly miscalibrated models.
- Limited direct evidence in corpus for the proposed MCS metric and class-wise temperature scaling method; most related work focuses on standard ECE-based calibration without distinguishing under/over-confidence directions.

## Confidence

- **High confidence**: The observation that existing calibration techniques (TS, label smoothing, focal loss) tend to worsen under-confidence in modern architectures is well-supported by both the paper's experiments and corpus evidence for ViT and related models.
- **Medium confidence**: The novel MCS metric and its ability to distinguish under-from over-confidence are logically sound and consistent with related calibration literature, but direct experimental validation across diverse datasets and architectures is sparse.
- **Low confidence**: The theoretical justification for weighted subset MCS (wsMCS) and class-wise MCS-aware temperature scaling (cwMCS TS) is clear, but the practical impact on highly imbalanced datasets and extreme miscalibration scenarios is not well established in the literature.

## Next Checks

1. Test MCS and cwMCS TS on datasets with known class imbalance (e.g., long-tail distributions) to verify that wsMCS accurately reflects minority-class calibration issues and that cwMCS TS effectively corrects them.
2. Systematically sweep the γ parameter in cwMCS TS across a wider range and multiple architectures to identify optimal tuning strategies and detect any overfitting or instability.
3. Evaluate failure detection performance on safety-critical tasks (e.g., medical imaging, autonomous driving) to confirm that calibrated models actually improve risk-aware decision-making in real-world deployment scenarios.