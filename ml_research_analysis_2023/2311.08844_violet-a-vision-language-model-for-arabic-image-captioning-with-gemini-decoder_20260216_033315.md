---
ver: rpa2
title: 'Violet: A Vision-Language Model for Arabic Image Captioning with Gemini Decoder'
arxiv_id: '2311.08844'
source_url: https://arxiv.org/abs/2311.08844
tags:
- image
- decoder
- arabic
- captioning
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Violet, a vision-language model for Arabic
  image captioning. The model leverages a FasterRCNN-based object detector for visual
  feature extraction and a Gemini decoder, which fuses the pretrained Arabic JASMINE
  decoder with cross-attention layers.
---

# Violet: A Vision-Language Model for Arabic Image Captioning with Gemini Decoder

## Quick Facts
- arXiv ID: 2311.08844
- Source URL: https://arxiv.org/abs/2311.08844
- Reference count: 13
- Key outcome: Violet achieves CIDEr 61.2 on MSCOCO test set and improves over baselines by 13 points on Flickr8k for Arabic image captioning.

## Executive Summary
This paper introduces Violet, a vision-language model specifically designed for Arabic image captioning. The model combines a FasterRCNN-based object detector with a novel Gemini decoder architecture that fuses frozen Arabic language layers with trainable cross-attention mechanisms. To address the lack of Arabic image captioning datasets, the authors develop an automated data acquisition pipeline using NLLB translation and semantic similarity filtering, and release a manually annotated evaluation dataset called AraCOCO. Violet establishes new state-of-the-art performance for Arabic image captioning, achieving a CIDEr score of 61.2 on MSCOCO and significantly outperforming baseline models.

## Method Summary
Violet uses a FasterRCNN object detector to extract visual features from images, which are then processed through a 3-layer transformer encoder with meshed connections. The Gemini decoder architecture splits a pretrained Arabic JASMINE decoder into frozen text layers and trainable cross-attention layers, allowing efficient fusion of visual and textual features. The model is trained on translated English captioning datasets (MSCOCO) that are automatically filtered using semantic similarity scores to remove poor translations. The training procedure uses cross-entropy loss with AdamW optimizer, and the model generates Arabic captions through autoregressive decoding.

## Key Results
- Achieves CIDEr score of 61.2 on MSCOCO test set for Arabic image captioning
- Improves over baseline models by 13 points on Flickr8k dataset
- Demonstrates effectiveness of Gemini decoder architecture with frozen JASMINE layers and cross-attention fusion
- Releases AraCOCO, a manually annotated evaluation dataset of 500 images with 5 Arabic captions each

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Gemini decoder improves Arabic image captioning by fusing frozen JASMINE text layers with trainable cross-attention layers, reducing computational load while preserving fluency.
- Mechanism: By freezing the first half of the JASMINE decoder (pretrained Arabic text layers) and adding cross-attention layers only in the second half, the model retains strong Arabic language generation capability while allowing efficient integration of visual features.
- Core assumption: The pretrained JASMINE layers capture sufficient Arabic language fluency that they can be reused without fine-tuning.
- Evidence anchors:
  - [abstract] "our model is based on a vision encoder and a Gemini text decoder that maintains generation fluency while allowing fusion between the vision and language components"
  - [section 3.1.2] "we split our pretrained text decoder into two parts. The first part acts as a vanilla text decoder, while the second part acts as a fusion decoder that aligns visual and textual features."

### Mechanism 2
- Claim: Using NLLB for translation followed by semantic similarity filtering produces higher quality Arabic captions than Google Translate.
- Mechanism: NLLB provides more context-aware translations with lower perplexity, and the semantic similarity filter (threshold 0.6) removes poor translations that deviate from the original English meaning.
- Core assumption: Semantic similarity scores correlate with translation quality for image captioning purposes.
- Evidence anchors:
  - [section 3.2.1] "42% of Google's translations are unintelligible, a stark contrast to the mere 15% from NLLB"
  - [section 3.2.2] "We employ a simple method based on semantic similarity that allows us to identify and reject any such examples."

### Mechanism 3
- Claim: The meshed connection between vision encoder and decoder layers improves visual feature integration by allowing all encoder layers to contribute to cross-attention.
- Mechanism: Each encoder layer's output is weighted by learnable parameters α and combined with decoder inputs, providing richer visual context than using only the last encoder layer.
- Core assumption: Visual information at different abstraction levels from multiple encoder layers is beneficial for caption generation.
- Evidence anchors:
  - [section 3.1.1] "we adapt meshed connection (Cornia et al., 2020) in our architecture between the encoder layers and the text decoder"
  - [section 3.1.2] "This allows all the encoder layers to contribute to the input of the cross-attention rather than using only the output of the last encoder layer."

## Foundational Learning

- Concept: Cross-attention mechanism
  - Why needed here: To align visual features from the encoder with text generation in the decoder, allowing the model to focus on relevant image regions when generating each word.
  - Quick check question: How does cross-attention differ from self-attention in this architecture?

- Concept: Semantic similarity for quality filtering
  - Why needed here: To automatically identify and remove poor translations that could degrade model performance during training.
  - Quick check question: Why is sentence-BERT used for multilingual semantic similarity rather than a monolingual model?

- Concept: Meshed connections in transformers
  - Why needed here: To allow information from all encoder layers to contribute to caption generation, providing richer visual context than single-layer approaches.
  - Quick check question: What is the computational trade-off of using meshed connections versus standard encoder-decoder attention?

## Architecture Onboarding

- Component map:
  Image → Object detection (FasterRCNN) → Visual features → Projection layer → 3-layer transformer encoder → Meshed connection outputs → Gemini decoder (frozen JASMINE + cross-attention) → Generated Arabic caption tokens

- Critical path: Image → Object detection → Encoder → Cross-attention fusion → Decoder → Caption

- Design tradeoffs:
  - Frozen vs. trainable decoder layers: Freezing reduces computation and preserves pretrained fluency but limits adaptation
  - Number of encoder layers: More layers provide richer features but increase computation
  - Semantic similarity threshold: Higher thresholds ensure quality but may reduce dataset size

- Failure signatures:
  - Poor caption fluency: Likely issues with frozen JASMINE layers or cross-attention integration
  - Visual artifacts in captions: Problems with object detection or encoder feature extraction
  - Training instability: Issues with meshed connection weighting or cross-attention implementation

- First 3 experiments:
  1. Test the full model on a small validation set to verify basic functionality and check if Arabic captions are generated
  2. Compare the Gemini decoder variant against a standard decoder to confirm the claimed performance improvement
  3. Evaluate the semantic similarity filtering by checking if removed examples are indeed poor quality translations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is the proposed automatic data acquisition method (NLLB translation + semantic similarity filtering) compared to manual annotation for Arabic image captioning datasets?
- Basis in paper: [explicit] The paper describes their automated data acquisition method and compares it to Google Translate, but doesn't directly compare it to fully manual annotation in terms of quality or efficiency.
- Why unresolved: While the paper shows NLLB outperforms Google Translate, it doesn't provide a direct comparison to the quality that could be achieved through fully manual annotation, which would be the gold standard.
- What evidence would resolve it: A direct comparison study where the same subset of images is captioned using: (1) the proposed NLLB+filtering method, (2) manual annotation by native Arabic speakers, and (3) a hybrid approach where machine translation is followed by human review and correction.

### Open Question 2
- Question: How well does Violet generalize to Arabic dialects beyond Modern Standard Arabic (MSA)?
- Basis in paper: [inferred] The paper mentions JASMINE's training on diverse Arabic varieties, but doesn't evaluate Violet's performance on dialect-specific image captioning tasks.
- Why unresolved: The paper focuses on MSA and doesn't test the model's ability to handle the rich diversity of Arabic dialects, which could significantly impact real-world applicability.
- What evidence would resolve it: Evaluating Violet on a dataset of images with captions in various Arabic dialects (Egyptian, Levantine, Gulf, etc.) and measuring performance differences across dialects.

### Open Question 3
- Question: What is the optimal balance between freezing and fine-tuning the text decoder components in the Gemini architecture for Arabic image captioning?
- Basis in paper: [explicit] The paper experiments with freezing vs. not freezing the text decoder part of Gemini, but doesn't explore the full spectrum of partial freezing strategies.
- Why unresolved: The paper only compares full freezing vs. no freezing of the text decoder, leaving open questions about whether selectively freezing certain layers or components might yield better results.
- What evidence would resolve it: Systematic experiments varying which layers of the text decoder are frozen (e.g., freezing only the bottom layers, only the top layers, or different combinations) while measuring performance and computational efficiency.

## Limitations

- The AraCOCO evaluation dataset is limited to only 500 images with 5 captions each, which may not provide robust evaluation of model performance.
- The semantic similarity filtering threshold of 0.6 is arbitrary and not justified, potentially removing good translations or retaining poor ones.
- The generalization to Arabic dialects beyond Modern Standard Arabic is not evaluated, limiting real-world applicability.

## Confidence

**High confidence**: The technical architecture description (FasterRCNN + 3-layer transformer encoder + Gemini decoder) is clearly specified and follows established practices. The data collection methodology (NLLB translation + semantic filtering) is well-documented, and the reported performance improvements on Flickr8k are substantial and clearly stated.

**Medium confidence**: The claimed state-of-the-art performance on MSCOCO (CIDEr 61.2) and the 13-point improvement on Flickr8k are supported by reported metrics, but the small size of the evaluation dataset (AraCOCO) and lack of comparison with more recent models from 2023-2024 limits confidence in these claims as definitive.

**Low confidence**: The generalization of results to broader Arabic image captioning tasks is uncertain due to limited evaluation data and the specific nature of the translated MSCOCO dataset, which may not capture the full diversity of Arabic language use.

## Next Checks

1. **Annotation Quality Validation**: Compute inter-annotator agreement scores on a subset of AraCOCO images where multiple annotators provided captions, and analyze the distribution of semantic similarity scores for filtered vs. retained translations to verify the filtering threshold is appropriate.

2. **Ablation Study**: Conduct experiments comparing the Gemini decoder against: (a) fully trainable decoder, (b) standard cross-attention without freezing, and (c) the original JASMINE decoder, to quantify the actual contribution of each architectural choice.

3. **Out-of-Distribution Testing**: Evaluate Violet on additional Arabic image captioning datasets (if available) or test its robustness by translating MSCOCO captions into different Arabic dialects to assess generalization beyond Modern Standard Arabic.