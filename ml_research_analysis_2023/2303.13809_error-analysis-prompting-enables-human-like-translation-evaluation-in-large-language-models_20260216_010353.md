---
ver: rpa2
title: Error Analysis Prompting Enables Human-Like Translation Evaluation in Large
  Language Models
arxiv_id: '2303.13809'
source_url: https://arxiv.org/abs/2303.13809
tags:
- translation
- chatgpt
- errors
- error
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores using large language models like ChatGPT for
  machine translation evaluation, finding that current prompting methods perform poorly
  at the segment level despite success at the system level. The authors propose Error
  Analysis Prompting (EAPrompt), which combines Chain-of-Thought and Error Analysis
  strategies to emulate human evaluation frameworks like MQM.
---

# Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models

## Quick Facts
- arXiv ID: 2303.13809
- Source URL: https://arxiv.org/abs/2303.13809
- Reference count: 6
- Primary result: EAPrompt achieves 26.40% pairwise accuracy vs 11.86% baseline on Zh-En segment-level MT evaluation

## Executive Summary
This study addresses the challenge of using large language models like ChatGPT for machine translation evaluation, particularly at the segment level where previous prompting methods have underperformed. The authors propose Error Analysis Prompting (EAPrompt), which combines Chain-of-Thought reasoning with explicit error analysis strategies to emulate human evaluation frameworks like MQM. Through experiments on WMT22 data, EAPrompt significantly improves segment-level performance while achieving human-like evaluations at both system and segment levels. The work also identifies key limitations of ChatGPT as an MT evaluator, including scoring instability and biases when evaluating multiple translations.

## Method Summary
The method involves constructing specialized prompts that instruct ChatGPT to first identify major and minor translation errors using an Error Analysis framework, then score translations based on error severity (deducting 5 points for major errors and 1 point for minor errors). This approach combines Chain-of-Thought reasoning with structured error identification, mirroring human evaluation practices like MQM. The prompts include few-shot examples and use an itemized response format to improve ChatGPT's ability to accurately identify and categorize translation errors. The method was tested on WMT20 metric shared task data with Zh-En and En-De language pairs, comparing performance against baseline metrics and standard prompting approaches.

## Key Results
- EAPrompt achieves 26.40% pairwise accuracy on Zh-En segment-level evaluation, significantly improving from 11.86% baseline
- System-level performance reaches human-like quality with EAPrompt, outperforming standard prompting methods
- EAPrompt shares similar error distribution patterns with human MQM evaluations, distinguishing major from minor errors effectively

## Why This Works (Mechanism)

### Mechanism 1
ChatGPT performs poorly at segment-level evaluation because it lacks explicit error analysis prompting that mirrors human evaluation frameworks like MQM. The Error Analysis Prompting (EAPrompt) technique combines Chain-of-Thought reasoning with explicit error identification steps, forcing ChatGPT to first identify major and minor errors before scoring, which mirrors how human evaluators work. This approach assumes human-like evaluation frameworks that explicitly identify and categorize errors before scoring produce more reliable segment-level MT quality assessments than direct scoring approaches.

### Mechanism 2
Itemized response formats outperform detailed explanations in prompting ChatGPT for MT evaluation. Brief, structured error descriptions in itemized format are easier for ChatGPT to process and translate into accurate scoring, while lengthy paragraphs create cognitive overload and reduce accuracy. This assumes ChatGPT processes structured, concise information more effectively than unstructured, verbose explanations when performing multi-step evaluation tasks.

### Mechanism 3
Separating error identification and scoring into two distinct queries improves ChatGPT's evaluation stability. By splitting the instruction into two queries - one for identifying errors and another for scoring - ChatGPT can focus on each subtask independently, reducing confusion and improving accuracy in the scoring calculation. This assumes task decomposition through separate queries reduces cognitive load and improves performance on complex multi-step reasoning tasks for LLMs.

## Foundational Learning

- **Concept: Chain-of-Thought prompting**
  - Why needed here: Enables ChatGPT to break down the complex MT evaluation task into sequential reasoning steps, improving its ability to identify and categorize translation errors systematically.
  - Quick check question: What are the key differences between direct scoring prompts and Chain-of-Thought prompting in terms of how ChatGPT processes the evaluation task?

- **Concept: Error Analysis framework**
  - Why needed here: Provides a structured approach to MT evaluation that mirrors human expert judgment (MQM), allowing ChatGPT to generate explainable and reliable evaluations by categorizing errors as major or minor.
  - Quick check question: How does the major/minor error categorization in MQM differ from simple binary error detection, and why is this distinction important for segment-level evaluation?

- **Concept: In-context learning**
  - Why needed here: Few-shot examples in the prompt context teach ChatGPT the expected format and reasoning process for MT evaluation, compensating for its lack of explicit training on this specific task.
  - Quick check question: What are the critical elements that must be included in in-context examples to effectively teach ChatGPT the error analysis evaluation framework?

## Architecture Onboarding

- **Component map**: Input processing layer (source text, reference, translation) → Prompt generation module (EAPrompt construction with Chain-of-Thought and Error Analysis) → ChatGPT evaluation engine (error identification and scoring) → Output parsing layer (extract and format results) → Quality assessment module (compare with human judgments using Kendall correlation and pairwise accuracy)

- **Critical path**: Source text → Reference text → Translation input → EAPrompt construction (with in-context examples) → Error identification query → Error categorization → Scoring query → Final evaluation score → Quality assessment → Performance metrics (Kendall correlation, pairwise accuracy)

- **Design tradeoffs**: Using few-shot examples vs. zero-shot prompting - few-shot provides better performance but requires carefully crafted examples and increases prompt length; separating queries vs. combined instruction - separation improves stability but adds complexity and potential context loss; detailed vs. itemized responses - detailed provides more information but reduces accuracy, while itemized improves processing but may miss nuances.

- **Failure signatures**: Unstable scoring when evaluating the same translation multiple times (indicating randomness or lack of clear evaluation criteria); bias toward earlier translations when multiple translations are presented in a single query (suggesting autoregressive attention issues); direct adoption of existing metrics like BLEU instead of independent evaluation (indicating prompt ambiguity or lack of specificity).

- **First 3 experiments**:
  1. Test EAPrompt vs. standard prompting on a small subset of WMT22 data to verify the 26.40% to 11.86% improvement in pairwise accuracy for Zh-En segment-level evaluation.
  2. Compare itemized vs. detailed response formats by running both versions on identical test sets and measuring the difference in Kendall correlation scores.
  3. Evaluate the two-query vs. combined instruction approach by measuring stability across multiple runs on the same translation inputs and comparing variance in scoring results.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of EAPrompt vary across different language pairs beyond Zh-En and En-De? The study primarily focuses on two language pairs, leaving the generalizability of EAPrompt to other language pairs unexplored.

- **Open Question 2**: What are the specific types of errors that EAPrompt is most effective at identifying and scoring? While the paper indicates EAPrompt's effectiveness in error identification, it lacks a detailed analysis of which error types it handles best.

- **Open Question 3**: How does the instability of ChatGPT in scoring affect the reliability of EAPrompt in real-world applications? The paper identifies these limitations but does not explore their impact on the overall reliability of EAPrompt in practical scenarios.

## Limitations

- ChatGPT's instability in scoring the same translation differently across multiple evaluations
- Bias toward earlier translations when multiple translations are provided in a single query
- Limited test set size (40 segments per language pair) and reliance on single year of WMT evaluation data

## Confidence

- **High confidence**: EAPrompt outperforms standard prompting at system-level evaluation (broader evidence base, multiple metrics)
- **Medium confidence**: EAPrompt achieves human-like segment-level evaluation (limited to specific test conditions)
- **Low confidence**: Generalizability to other language pairs and domains beyond Zh-En and En-De

## Next Checks

1. Conduct cross-validation across multiple years of WMT data and additional language pairs to test generalizability
2. Implement statistical significance testing for the pairwise accuracy improvements and measure variance across multiple evaluation runs
3. Test the EAPrompt framework with domain-specific translation tasks (medical, legal, technical) to evaluate performance beyond news translation datasets