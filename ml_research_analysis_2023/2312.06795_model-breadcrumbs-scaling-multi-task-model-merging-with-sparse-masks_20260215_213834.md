---
ver: rpa2
title: 'Model Breadcrumbs: Scaling Multi-Task Model Merging with Sparse Masks'
arxiv_id: '2312.06795'
source_url: https://arxiv.org/abs/2312.06795
tags:
- breadcrumbs
- tasks
- task
- fine-tuned
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Model Breadcrumbs, a method for merging multiple
  fine-tuned models from the same foundation model into a multi-task model. The core
  idea is to construct sparse weight masks that capture the differences between the
  foundation model and each fine-tuned model, filter out outliers and negligible perturbations,
  and then combine these masked differences to form a multi-task model.
---

# Model Breadcrumbs: Scaling Multi-Task Model Merging with Sparse Masks

## Quick Facts
- **arXiv ID**: 2312.06795
- **Source URL**: https://arxiv.org/abs/2312.06795
- **Reference count**: 40
- **Primary result**: Introduces Model Breadcrumbs, a sparse mask-based method for merging multiple fine-tuned models into a multi-task model that outperforms existing approaches.

## Executive Summary
This paper introduces Model Breadcrumbs, a novel approach for merging multiple fine-tuned models from the same foundation model into a multi-task model. The method constructs sparse weight masks that capture the differences between the foundation model and each fine-tuned model, filters out outliers and negligible perturbations, and then combines these masked differences to form a multi-task model. The approach is shown to be more efficient and effective than previous methods like Task Vectors and Fisher Merging, particularly as the number of tasks increases, while also demonstrating better generalization of hyperparameters across different task combinations.

## Method Summary
Model Breadcrumbs works by first calculating the weight differences between each fine-tuned model and the pre-trained foundation model, creating task-specific "direction vectors." These direction vectors are then subjected to a sparsification process that removes both extreme outliers and negligible perturbations using two threshold parameters (β and γ). The resulting sparse masks are combined with the foundation model using a scaling factor α to create the final multi-task model. This approach eliminates the need for task-specific hyperparameter tuning while maintaining strong performance across multiple tasks.

## Key Results
- Model Breadcrumbs outperforms Task Vectors by 8.33% in 8-task vision experiments and 5.88% in 200-task vision experiments
- The method demonstrates superior computational efficiency compared to Fisher Merging while maintaining competitive accuracy
- Hyperparameters (α, β, γ) generalize well across increasing numbers of tasks without requiring per-task validation
- Model Breadcrumbs shows better performance than individual fine-tuned models in most task combinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model Breadcrumbs effectively removes harmful perturbations by masking extreme weight differences during fine-tuning.
- Mechanism: The method calculates weight differences between pre-trained and fine-tuned models, then applies sparsity masks to eliminate both large outliers and small perturbations that could harm multi-task performance.
- Core assumption: Extreme weight values (both large and small) in the difference between fine-tuned and pre-trained weights are more likely to represent harmful perturbations rather than beneficial knowledge transfer.
- Evidence anchors:
  - [abstract] "These breadcrumbs are constructed by subtracting the weights from a pre-trained model before and after fine-tuning, followed by a sparsification process that eliminates weight outliers and negligible perturbations."
  - [section 3] "We mask out the extreme tails of the absolute magnitude distribution of θd t , using γ and β as thresholds for the right and left tails, respectively."
  - [corpus] Weak - no direct corpus evidence on this specific masking mechanism
- Break condition: If the sparsity thresholds (β and γ) are set too aggressively, beneficial fine-tuning information could be masked out, leading to performance degradation.

### Mechanism 2
- Claim: The hyperparameters (α, β, γ) of Model Breadcrumbs generalize well across increasing numbers of tasks without requiring per-task validation.
- Mechanism: The method uses a fixed sparsity pattern and scaling factor that remains effective as more tasks are added, eliminating the need for task-specific hyperparameter tuning.
- Core assumption: The optimal sparsity patterns discovered on a subset of tasks remain valid as more tasks are incorporated into the multi-task model.
- Evidence anchors:
  - [abstract] "Our method is shown to be more efficient and unlike previous proposals does not require hyperparameter tuning for each new task added."
  - [section 4.3] "For the ViT-B-32 model when considering scenarios involving three tasks and beyond, up to the 8-task scenario, the optimal hyperparameters remain consistent."
  - [corpus] Weak - no direct corpus evidence on hyperparameter generalization in model merging
- Break condition: If tasks have highly dissimilar characteristics or domains, the fixed hyperparameters may no longer be optimal, requiring task-specific tuning.

### Mechanism 3
- Claim: Model Breadcrumbs creates more orthogonal task representations compared to Task Vectors, reducing interference during model merging.
- Mechanism: The masking operation pushes cosine similarity between tasks closer to zero, enforcing orthogonality and minimizing negative interference when combining multiple fine-tuned models.
- Core assumption: Lower cosine similarity between task weight differences correlates with better multi-task performance by reducing interference.
- Evidence anchors:
  - [section 4.6] "In contrast, Model Breadcrumbs pushes all cosine similarity values closer to zero, reinforcing orthogonality."
  - [section 4.2] "Specifically, in the experiment involving all eight tasks, the Model Breadcrumbs outperform the Task Vectors by a substantial margin of 8.33%."
  - [corpus] Weak - no direct corpus evidence on cosine similarity and task orthogonality in model merging
- Break condition: If tasks are inherently highly correlated or semantically similar, forcing orthogonality through masking may remove beneficial shared information.

## Foundational Learning

- Concept: Weight averaging in neural networks
  - Why needed here: Understanding how linear combinations of model weights can transfer knowledge between models
  - Quick check question: What are the theoretical conditions under which averaging weights from different models improves performance?

- Concept: Catastrophic forgetting in multi-task learning
  - Why needed here: Recognizing how merging models can lead to performance degradation on individual tasks
  - Quick check question: How does the sparsity mask in Model Breadcrumbs help prevent catastrophic forgetting compared to simple weight averaging?

- Concept: Fisher information matrix in model merging
  - Why needed here: Understanding alternative merging approaches and why Model Breadcrumbs might be more computationally efficient
  - Quick check question: Why is computing the Fisher information matrix computationally expensive compared to the sparsity mask approach?

## Architecture Onboarding

- Component map:
  Foundation model weights (θ) -> Fine-tuned model weights (θ′t) -> Task direction calculation (θd t = θ′t - θ) -> Sparsity mask generation (mβ,γ t) -> Multi-task model construction (θ∗ = θ + αΣ(mβ,γ t . θd t))

- Critical path:
  1. Fine-tune individual models on separate tasks
  2. Calculate weight differences from foundation model
  3. Apply sparsity masks to remove outliers and negligible values
  4. Combine masked differences with foundation model using scaling factor α
  5. Evaluate multi-task performance

- Design tradeoffs:
  - Sparsity level vs. information retention: Higher sparsity reduces interference but may remove beneficial fine-tuning information
  - Computational efficiency vs. performance: Simpler masking is faster but may be less optimal than more sophisticated approaches
  - Task independence vs. shared knowledge: Enforcing orthogonality may prevent beneficial knowledge transfer between similar tasks

- Failure signatures:
  - Performance degradation when sparsity is too high (β and γ too extreme)
  - No improvement over individual fine-tuned models (α too low or mask too restrictive)
  - Task-specific performance drops (mask not properly removing interference)
  - Hyperparameters not generalizing to new tasks (β and γ not optimal for task distribution)

- First 3 experiments:
  1. Implement weight difference calculation and verify that fine-tuned models have meaningful differences from foundation model
  2. Apply simple top-k/bottom-k masking and observe effect on individual task performance
  3. Combine two fine-tuned models using Model Breadcrumbs and compare to simple averaging and Task Vectors approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Model Breadcrumbs scale with the number of tasks beyond the tested 8-task and 200-task scenarios?
- Basis in paper: [explicit] The paper mentions extending the evaluation to a 200-task sequence derived from ImageNet, but does not explore scenarios with significantly more tasks.
- Why unresolved: The paper does not provide experimental results for scenarios with a larger number of tasks, leaving the scalability of Model Breadcrumbs in such cases uncertain.
- What evidence would resolve it: Conducting experiments with a significantly larger number of tasks (e.g., 1000+ tasks) and comparing the performance of Model Breadcrumbs to other methods would provide insights into its scalability.

### Open Question 2
- Question: How does the performance of Model Breadcrumbs compare to other model merging techniques when the fine-tuned models are trained on highly dissimilar tasks?
- Basis in paper: [inferred] The paper mentions that semantically similar tasks exhibit higher cosine similarity, suggesting potential interference during merging. However, it does not explicitly test the performance of Model Breadcrumbs with highly dissimilar tasks.
- Why unresolved: The paper does not provide experimental results for merging models trained on highly dissimilar tasks, leaving the robustness of Model Breadcrumbs in such scenarios uncertain.
- What evidence would resolve it: Conducting experiments with a diverse set of tasks that are highly dissimilar and comparing the performance of Model Breadcrumbs to other techniques would provide insights into its robustness.

### Open Question 3
- Question: How does the choice of the sparsity threshold (γ) in Model Breadcrumbs affect its performance and computational efficiency?
- Basis in paper: [explicit] The paper mentions that the optimal sparsity threshold for Model Breadcrumbs is 85%, but does not explore the impact of different sparsity levels on performance and efficiency.
- Why unresolved: The paper does not provide a detailed analysis of how the sparsity threshold affects the performance and computational efficiency of Model Breadcrumbs.
- What evidence would resolve it: Conducting experiments with different sparsity thresholds and analyzing their impact on performance and computational efficiency would provide insights into the optimal choice of sparsity for different scenarios.

## Limitations

- Hyperparameter sensitivity: The paper claims good generalization of hyperparameters (α, β, γ) across multiple tasks, but the experimental validation is limited to specific task distributions.
- Mechanism validation gaps: While the paper provides theoretical justification for why sparsity masks improve performance, the empirical evidence is primarily based on observed performance improvements rather than direct validation of the underlying mechanisms.
- Scale considerations: The experiments demonstrate effectiveness up to 8 tasks, but the scalability to hundreds or thousands of tasks (as suggested by the title) is not empirically validated.

## Confidence

- **High confidence**: The core experimental results showing Model Breadcrumbs outperforming Task Vectors and Fisher Merging on the tested task combinations.
- **Medium confidence**: The claims about hyperparameter generalization across task counts. While the paper shows consistency from 3-8 tasks, the assumption that this pattern holds for arbitrary task additions needs more validation.
- **Low confidence**: The specific mechanism claims about why sparsity masks work (removing harmful perturbations, enforcing orthogonality). The paper lacks direct ablation studies isolating these effects from other factors.

## Next Checks

1. **Cross-domain generalization test**: Apply Model Breadcrumbs to merge models from completely different domains (e.g., vision + NLP) to validate whether the sparsity masks and hyperparameters generalize beyond the tested vision-only and NLP-only scenarios.

2. **Ablation study on mask components**: Create controlled experiments that isolate the effects of outlier removal (γ threshold) versus small perturbation removal (β threshold) to empirically validate which masking mechanism contributes most to performance gains.

3. **Scaling experiment**: Implement Model Breadcrumbs on a larger set of tasks (20+ tasks) and measure both computational efficiency and performance degradation compared to Fisher Merging, directly testing the scalability claims implied by the paper's title.