---
ver: rpa2
title: Low-Resource Multi-Granularity Academic Function Recognition Based on Multiple
  Prompt Knowledge
arxiv_id: '2305.03287'
source_url: https://arxiv.org/abs/2305.03287
tags:
- function
- prompt
- recognition
- data
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of recognizing academic functions
  (e.g., citation, sentence, keyword) in scientific literature under low-resource
  conditions, where labeled data is scarce. The proposed Mix Prompt Tuning (MPT) method
  combines manually designed hard prompt templates with automatically learned soft
  prompt embeddings to provide multi-perspective representations, enabling better
  utilization of pre-trained language models (PLMs) like SciBERT.
---

# Low-Resource Multi-Granularity Academic Function Recognition Based on Multiple Prompt Knowledge

## Quick Facts
- arXiv ID: 2305.03287
- Source URL: https://arxiv.org/abs/2305.03287
- Authors: 
- Reference count: 40
- Key outcome: MPT outperforms fine-tuning and other semi-supervised baselines, achieving an average 5% improvement in Macro-F1 score over fine-tuning and 6% over other semi-supervised methods under low-resource settings.

## Executive Summary
This paper addresses the challenge of recognizing academic functions (e.g., citation, sentence, keyword) in scientific literature under low-resource conditions where labeled data is scarce. The proposed Mix Prompt Tuning (MPT) method combines manually designed hard prompt templates with automatically learned soft prompt embeddings to provide multi-perspective representations, enabling better utilization of pre-trained language models (PLMs) like SciBERT. MPT uses a semi-supervised approach where multiple PLMs trained on different prompt templates generate pseudo-labels for unlabeled data, which are then used to train a final classifier. Experiments on three tasks (citation, sentence, and keyword function recognition) demonstrate that MPT significantly outperforms fine-tuning and other semi-supervised baselines in low-resource settings.

## Method Summary
The Mix Prompt Tuning (MPT) framework combines manual hard prompts and learned soft prompts with semi-supervised pseudo-labeling to address low-resource academic function recognition. The method trains multiple PLMs on different prompt templates, uses high-confidence predictions from these models to generate pseudo-labels for unlabeled data, and iteratively refines the model through knowledge distillation. The final classifier is trained on both the original labeled data and the pseudo-labeled data, leveraging the complementary information from both prompt types to improve performance across different granularities of academic functions.

## Key Results
- MPT achieves an average 5% improvement in Macro-F1 score over fine-tuning under low-resource settings
- MPT outperforms other semi-supervised methods by an average of 6% in Macro-F1 score
- SciBERT (domain-specific PLM) shows significant performance gains over general PLMs like BERT in scientific text classification tasks

## Why This Works (Mechanism)

### Mechanism 1
Combining manual and soft prompts provides multi-perspective representations that improve low-resource academic function recognition. Manual prompts encode expert knowledge and task-specific semantics, while soft prompts allow the model to learn continuous representations that better align with the PLM's internal language. The combination provides complementary information that enhances the model's ability to recognize academic functions.

### Mechanism 2
Semi-supervised learning with pseudo-labeling on unlabeled data significantly improves performance under low-resource conditions. Multiple PLMs trained on different prompt templates generate pseudo-labels for unlabeled data, which are then used to train a final classifier through knowledge distillation. The iterative process allows models to learn from each other's strengths and progressively improve.

### Mechanism 3
In-domain pre-training (SciBERT vs. BERT) provides significant performance gains for scientific text classification tasks. PLMs pre-trained on scientific corpora capture domain-specific linguistic patterns and terminology that are crucial for accurate classification of academic functions, outperforming general-domain models.

## Foundational Learning

- **Concept**: Prompt learning and template wrapping
  - **Why needed here**: Traditional fine-tuning requires large labeled datasets, but prompt learning reformulates classification as a fill-in-the-blank problem that leverages the PLM's pre-trained capabilities
  - **Quick check question**: Can you explain how template wrapping converts "This sentence describes the [MASK] of the paper" into a classification task?

- **Concept**: Semi-supervised learning and pseudo-labeling
  - **Why needed here**: Labeled data for academic function recognition is scarce and expensive, while unlabeled scientific text is abundant; pseudo-labeling allows leveraging this unlabeled data
  - **Quick check question**: What is the purpose of sampling instances with high probability scores when generating pseudo-labels?

- **Concept**: Knowledge distillation and ensemble learning
  - **Why needed here**: Multiple models trained on different prompts may capture different aspects of the task; distillation combines their strengths while maintaining a single, efficient model
  - **Quick check question**: How does the iterative training process (iPET-style) help models learn from each other?

## Architecture Onboarding

- **Component map**: Academic text → Template wrapping → PLM encoding → [MASK] prediction → Pseudo-labeling → Final classifier training → Evaluation

- **Critical path**: Text → Template wrapping → PLM encoding → [MASK] prediction → Pseudo-labeling → Final classifier training → Evaluation

- **Design tradeoffs**: More prompt templates increase coverage but also computational cost and risk of conflicting signals; higher pseudo-labeling confidence thresholds reduce noise but may discard useful data; in-domain pre-training improves scientific performance but limits model transferability to general domains

- **Failure signatures**: Performance plateaus or degrades with more unlabeled data → pseudo-labeling introducing too much noise; soft prompts fail to learn meaningful representations → insufficient training data or poor initialization; manual prompts not covering task space → need for prompt engineering or template expansion

- **First 3 experiments**:
  1. Baseline comparison: Run MPT vs. fine-tuning and PT-hard/PT-soft on 16-shot balanced data for one task (e.g., citation function)
  2. Prompt ablation: Test MPT with only manual prompts vs. only soft prompts to quantify their individual contributions
  3. Unlabeled data scaling: Vary the number of unlabeled instances (200, 600, 1000) to find the optimal amount for pseudo-labeling

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of MPT vary across different scientific domains beyond computer science and biomedical, and what factors contribute to these variations? The paper evaluates MPT on datasets from computer science and biomedical domains but does not explore its effectiveness across other scientific domains.

### Open Question 2
What is the impact of different prompt template designs on the performance of MPT, and how can we systematically optimize these templates for specific tasks? The paper uses manually designed templates and soft prompts but does not provide a systematic approach to optimize or evaluate the impact of different template designs.

### Open Question 3
How does MPT perform in scenarios with extremely limited labeled data (e.g., one-shot or zero-shot learning), and what are the limitations of the method in such cases? The paper evaluates MPT in low-resource settings with 4 to 128 labeled instances per class but does not explore its performance in extreme cases like one-shot or zero-shot learning.

## Limitations

- The study focuses exclusively on English scientific text, with limited evaluation on Chinese datasets, raising questions about cross-lingual robustness
- The semi-supervised approach requires substantial unlabeled data (200-1000 instances per task), which may not be available in all low-resource scenarios
- The paper lacks detailed analysis of prompt template quality and selection criteria, making it difficult to assess whether optimal templates were chosen

## Confidence

- **High Confidence**: In-domain pre-training (SciBERT vs. BERT) provides significant performance gains - well-supported by direct experimental comparisons showing consistent improvements
- **Medium Confidence**: The combination of manual and soft prompts provides superior multi-perspective representations - experiments show improvement but individual component contributions are not clearly isolated
- **Medium Confidence**: Semi-supervised pseudo-labeling significantly improves low-resource performance - iterative approach shows promise but lacks comparison with state-of-the-art semi-supervised methods

## Next Checks

1. **Prompt Template Ablation Study**: Systematically evaluate MPT performance with different combinations of manual and soft prompts (manual-only, soft-only, different template subsets) across all three tasks to quantify individual component contributions and identify optimal template combinations.

2. **Cross-Lingual Generalization Test**: Extend experiments beyond English and Chinese to include at least two additional languages with scientific literature (e.g., German, French) to assess the true cross-lingual capability of the MPT framework and identify potential language-specific limitations.

3. **Semi-Supervised Method Comparison**: Implement and compare MPT against other state-of-the-art semi-supervised approaches (e.g., FixMatch, MixMatch, Noisy Student) on the same low-resource academic function recognition tasks to establish relative performance advantages and identify scenarios where alternative methods might be preferable.