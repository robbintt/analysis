---
ver: rpa2
title: 'Internet Explorer: Targeted Representation Learning on the Open Web'
arxiv_id: '2302.14051'
source_url: https://arxiv.org/abs/2302.14051
tags:
- internet
- images
- explorer
- dataset
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Internet Explorer, a method for improving representation
  learning by actively searching the Internet for relevant training data. Instead
  of relying on static, out-of-date datasets, the method uses text-based image search
  to dynamically acquire images related to a target task.
---

# Internet Explorer: Targeted Representation Learning on the Open Web

## Quick Facts
- arXiv ID: 2302.14051
- Source URL: https://arxiv.org/abs/2302.14051
- Reference count: 25
- Key outcome: Internet Explorer improves representation learning by actively searching the Internet for relevant training data, outperforming CLIP oracle performance using 2.5% of the compute and 0.5% of the data.

## Executive Summary
Internet Explorer is a method for improving representation learning by dynamically acquiring relevant training data from the Internet. Instead of relying on static, out-of-date datasets, it uses text-based image search to iteratively find images related to a target task. The method cycles through searching, downloading, self-supervised training, ranking by relevance, and refining future queries. Results show that Internet Explorer matches or outperforms CLIP oracle performance on multiple datasets while using significantly less compute and data.

## Method Summary
Internet Explorer operates through an iterative cycle of text-based image search, self-supervised training on downloaded images, relevance estimation using similarity to the target dataset, and query generation for future searches. It uses a learned distribution over WordNet concepts to guide exploration, with GPT-generated descriptors adding diversity to queries. The method initializes with a pre-trained ResNet-50 model and progressively refines its representations through multiple search iterations, each lasting 30-40 hours on a single GPU.

## Key Results
- Internet Explorer matches or outperforms CLIP oracle performance on Birdsnap, Flowers-102, Food101, Oxford-IIIT Pets, and Pascal VOC 2007
- Achieves these results using only 2.5% of the compute and 0.5% of the data compared to CLIP
- Demonstrates efficient discovery of relevant images through iterative search and self-supervised learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The iterative search-train-rank-refine cycle progressively focuses on target-relevant images
- **Mechanism**: Each iteration generates text queries by combining sampled concepts with GPT descriptors, downloads top 100 image results, self-supervised trains on combined dataset, and evaluates relevance by similarity to target dataset. Rewards increase likelihood of useful concepts in future iterations.
- **Core assumption**: Image relevance to target dataset can be effectively estimated by similarity in representation space
- **Evidence anchors**: [abstract] "It cycles between searching for images on the Internet with text queries, self-supervised training on downloaded images, determining which images were useful, and prioritizing what to search for next" and [section] "we use its similarity to the target dataset as a proxy for being relevant to training"
- **Break condition**: If similarity measure fails to correlate with actual usefulness, or search returns too many irrelevant images

### Mechanism 2
- **Claim**: Gaussian Process Regression predicts rewards for untried concepts, enabling efficient exploration
- **Mechanism**: Embeds 146,347 WordNet concepts into 384-dimensional space, models function outputs as jointly Gaussian, calculates posterior distribution over unobserved concepts' rewards based on observed rewards
- **Core assumption**: Similar concepts in text embedding space will have similar rewards
- **Evidence anchors**: [abstract] "We also bootstrap Internet Explorer using existing pre-trained models such as MoCov3" and [section] "we embed our 146,347 WordNet concepts into a 384-dimensional space using a pre-trained sentence similarity model"
- **Break condition**: If text embeddings don't capture semantic similarity well, or reward function isn't smooth in embedding space

### Mechanism 3
- **Claim**: Tiering scaled softmax distribution ensures sufficient exploration of promising concepts
- **Mechanism**: Scales scores to desired temperature, takes softmax for distribution, creates tiers with top 250 concepts having 80% probability mass, next 750 having 10%, and remaining concepts having 10%
- **Core assumption**: Tiering function effectively balances exploration and exploitation
- **Evidence anchors**: [abstract] "Internet Explorer explores the web in a self-supervised manner to progressively find relevant examples" and [section] "Given a sorted discrete probability distribution p, interval boundaries T0 = 0 < T1 < · · · < Tn, and interval masses ∆0,..., ∆n−1 such that ∑i ∆i = 1, tiering computes a new distribution"
- **Break condition**: If tiering parameters not well-tuned, may over-explore or over-exploit

## Foundational Learning

- **Concept**: Self-supervised learning and contrastive loss
  - **Why needed here**: Internet Explorer uses self-supervised learning to learn useful representations from unlabeled images downloaded from the Internet. The contrastive loss encourages the network to learn high-level semantic features by training encoders to output similar vectors for augmentations of the same image.
  - **Quick check question**: What is the goal of the contrastive loss in self-supervised learning, and how does it achieve this goal?

- **Concept**: Gaussian Process Regression (GPR)
  - **Why needed here**: GPR is used to predict the rewards for untried concepts based on the observed rewards of the queries used so far. It models the function outputs for any set of inputs as jointly Gaussian random variables, allowing Internet Explorer to estimate the quality of a query without having to search for it.
  - **Quick check question**: How does GPR use the observed rewards of tried concepts to predict the rewards for untried concepts?

- **Concept**: Text-to-image search and weak supervision
  - **Why needed here**: Internet Explorer discovers and downloads images from the Internet by querying text-to-image search engines. These search engines return images based on their captions and surrounding text, which provides weak supervision for the image-text pairing on webpages.
  - **Quick check question**: What is the source of supervision in text-to-image search, and why is it considered weak?

## Architecture Onboarding

- **Component map**: Query Generation -> Search Engine Interface -> Self-supervised Training -> Relevance Estimation -> Concept Reward Prediction -> Query Sampling Distribution
- **Critical path**: The iterative cycle of query generation, image search and download, self-supervised training, relevance estimation, and concept reward prediction. This cycle must be repeated efficiently to progressively focus on target-relevant images.
- **Design tradeoffs**:
  - Exploration vs. Exploitation: The tiering function balances exploration of potentially useful concepts with exploitation of known high-scoring concepts
  - Image Quality vs. Quantity: Downloading more images increases chances of finding relevant ones but also increases computational cost
  - Similarity Measure vs. Reward Function: Choice of similarity measure affects accuracy of relevance estimation and efficiency of search process
- **Failure signatures**:
  - Slow Convergence: If relevance estimation is inaccurate or search returns too many irrelevant images
  - Overfitting: If self-supervised training focuses too much on downloaded images rather than target dataset
  - Concept Drift: If target dataset or Internet data distribution changes over time
- **First 3 experiments**:
  1. Verify text-to-image search engine returns relevant images for simple queries (e.g., "cat", "dog", "car")
  2. Test self-supervised training on small set of downloaded images and evaluate learned representations on simple target task (e.g., binary classification of cats vs. dogs)
  3. Evaluate concept reward prediction by manually labeling subset of concepts and comparing predicted rewards to manual labels

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How would Internet Explorer perform if it used a larger initial pre-trained model instead of a ResNet-50 initialized with MoCo-v3?
- **Basis in paper**: [explicit] The paper states "We initialize a ResNet-50 model (He et al., 2016) using a MoCo-v3 checkpoint trained offline for 100 epochs on ImageNet and then fine-tuned on the target dataset."
- **Why unresolved**: The paper does not explore the effect of using larger pre-trained models or different architectures on Internet Explorer's performance.
- **What evidence would resolve it**: Running Internet Explorer with different initial model sizes and architectures, then comparing the resulting performance and resource usage.

### Open Question 2
- **Question**: What is the long-term impact of Internet Explorer's exploration strategy on the diversity of learned representations?
- **Basis in paper**: [inferred] The paper mentions "Internet Explorer either outperforms or matches CLIP oracle performance" and uses a tiering function to balance exploration and exploitation, but does not analyze the long-term diversity of the learned representations.
- **Why unresolved**: The paper does not provide an analysis of how Internet Explorer's exploration strategy affects the diversity of learned representations over many iterations or across different datasets.
- **What evidence would resolve it**: Conducting a long-term study of Internet Explorer's exploration strategy, measuring the diversity of learned representations over time and across datasets.

### Open Question 3
- **Question**: How does Internet Explorer's performance scale with the size of the target dataset?
- **Basis in paper**: [explicit] The paper evaluates Internet Explorer on small-scale datasets (2,040 to 75,750 training examples) and states "These small datasets consist of 2,040 to 75,750 training examples, making them ideal for testing whether Internet Explorer can efficiently find relevant useful data."
- **Why unresolved**: The paper does not explore how Internet Explorer's performance changes as the size of the target dataset increases.
- **What evidence would resolve it**: Evaluating Internet Explorer on larger target datasets and analyzing the relationship between dataset size and performance.

## Limitations
- Effectiveness of text-to-image search in returning truly relevant images is uncertain due to weak supervision from captions
- Gaussian Process Regression's accuracy in predicting rewards depends on quality of text embeddings and smoothness of reward function
- Tiering function parameters may not be optimally tuned, potentially leading to over-exploration or over-exploitation
- Method's reliance on single GPU and 30-40 hours of search time may limit scalability to larger datasets or more complex tasks

## Confidence

- **High**: The iterative cycle of query generation, image search, self-supervised training, and relevance estimation is a sound approach to progressively focus on target-relevant images
- **Medium**: The use of Gaussian Process Regression to predict rewards for untried concepts is a reasonable method, but its effectiveness depends on the quality of the text embeddings and the smoothness of the reward function in the embedding space
- **Low**: The specific implementation details of the tiering function and the exact hyperparameters for temperature scaling in concept sampling distribution are not fully specified, which may affect the method's performance

## Next Checks
1. Evaluate the text-to-image search engine's ability to return relevant images for a diverse set of queries, and quantify the proportion of truly relevant images in the top results
2. Test the Gaussian Process Regression's accuracy in predicting rewards for untried concepts by manually labeling a subset of concepts and comparing the predicted rewards to the manual labels
3. Experiment with different tiering function parameters and temperature scaling to find the optimal balance between exploration and exploitation, and measure the impact on the method's performance on the target datasets