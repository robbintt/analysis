---
ver: rpa2
title: 'UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via
  Unconstrained Generation'
arxiv_id: '2311.15296'
source_url: https://arxiv.org/abs/2311.15296
tags:
- evaluation
- hallucination
- llms
- news
- hallucinations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UHGEval, a novel benchmark for evaluating
  hallucinations in Chinese language models via unconstrained generation. The benchmark
  consists of a dataset of over 5,000 hallucinated news continuations, generated without
  restrictive prompts, closely mirroring real-world scenarios.
---

# UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation

## Quick Facts
- **arXiv ID**: 2311.15296
- **Source URL**: https://arxiv.org/abs/2311.15296
- **Reference count**: 40
- **Key outcome**: UHGEval benchmark evaluates Chinese LLM hallucinations using unconstrained generation, revealing GPT series models excel in discriminative evaluation while Xinyu2-70B and InternLM-20B lead in generative evaluation.

## Executive Summary
This paper introduces UHGEval, a novel benchmark for evaluating hallucinations in Chinese language models through unconstrained generation. The benchmark addresses limitations of existing constrained generation methods by creating a dataset of over 5,000 hallucinated news continuations that closely mirror real-world scenarios. An evaluation framework with generative, discriminative, and selective modalities, along with sentence-level and keyword-level granularity, is established. Extensive experiments on eight prominent Chinese LLMs and three GPT series models reveal significant performance differences across evaluation methods, highlighting the importance of robust benchmarks for professional content generation.

## Method Summary
The UHGEval benchmark constructs a dataset of over 5,000 hallucinated Chinese news continuations using unconstrained generation without restrictive prompts. The process involves data collection from news articles (2015-2017), unconstrained hallucination generation using multiple LLMs, ranking based on fluency and hallucination likelihood using keyword precision metrics, automatic labeling with human verification, and comprehensive evaluation across three modalities. The framework evaluates models through discriminative (binary classification), selective (multiple choice), and generative (direct hallucination detection) approaches, using metrics including accuracy, BLEU, ROUGE, keyword precision, and BERTScore.

## Key Results
- GPT series models significantly outperform other LLMs in discriminative evaluation for hallucination detection
- Xinyu2-70B and InternLM-20B excel in generative evaluation, demonstrating strong hallucination generation capabilities
- Number-intensive and general news content show higher hallucination likelihood compared to knowledge-intensive and document-intensive categories
- Different evaluation modalities reveal complementary insights into LLM performance on hallucination tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The UHGEval benchmark provides a more realistic evaluation of LLM hallucination than existing constrained methods.
- Mechanism: By using unconstrained generation, the benchmark allows models to generate continuations freely without restrictive prompts, closely mirroring real-world scenarios where hallucinations can occur spontaneously.
- Core assumption: Unconstrained generation captures a broader and more representative set of hallucination patterns than constrained methods.
- Evidence anchors:
  - [abstract] "Nevertheless, these benchmarks frequently utilize constrained generation techniques due to cost and temporal constraints. These approaches are not congruent with the unrestricted text generation demanded by real-world applications."
  - [section] "Existing methods for constructing datasets often yield biases towards predefined directions, thereby hindering the full simulation of real-world hallucinations. We have created a hallucination evaluation dataset comprising over 5000 items, generated without intervention, closely mirroring real-world scenarios."
  - [corpus] Weak evidence: No direct corpus citations, but related works focus on constrained methods, suggesting a gap UHGEval fills.
- Break condition: If unconstrained generation fails to capture meaningful hallucination patterns or if the dataset size becomes too small to be statistically significant.

### Mechanism 2
- Claim: The UHGEval framework's evaluation method effectively balances model fluency and hallucination likelihood.
- Mechanism: The evaluation uses a two-stage ranking process that first filters for fluent continuations and then selects those most likely to contain hallucinations based on keyword precision.
- Core assumption: Fluency and hallucination likelihood can be effectively measured and balanced to create a challenging yet fair evaluation dataset.
- Evidence anchors:
  - [section] "To strike this balance, the selection process takes into account two primary dimensions: Fluency. This refers to the naturalness and readability of the text... Likelihood of Hallucination Occurrence... we propose the keyword precision (kwPrec) metric."
  - [section] "By considering both fluency and the likelihood of hallucination, the process aims to filter out continuations that are either too nonsensical or too conservative (lacking any hallucinated content)."
  - [corpus] Weak evidence: No direct corpus citations, but the method is innovative compared to existing benchmarks.
- Break condition: If the keyword precision metric fails to accurately identify hallucinations or if the balance between fluency and hallucination likelihood is not optimal.

### Mechanism 3
- Claim: The UHGEval framework's evaluation method effectively differentiates between LLM performance in hallucination detection and generation.
- Mechanism: The framework includes generative, discriminative, and selective evaluation modalities, each testing different aspects of LLM capabilities.
- Core assumption: Different evaluation modalities provide complementary insights into LLM performance on hallucination tasks.
- Evidence anchors:
  - [section] "In terms of form, this encompasses human evaluation, discriminative evaluation, selective evaluation, and generative evaluation, among others."
  - [section] "Discriminative evaluation enables LLMs to respond with binary answers of 'yes' or 'no'... Similar to discriminative evaluation, selective evaluation allows LLMs to tackle multiple-choice questions by choosing between option A or B... However, generative evaluation is crucial as it directly evaluates the presence of hallucinations in the text generated by the LLM."
  - [corpus] Weak evidence: No direct corpus citations, but the multi-modal approach is innovative compared to existing benchmarks.
- Break condition: If the different evaluation modalities fail to provide meaningful differentiation or if they are too computationally expensive to be practical.

## Foundational Learning

- Concept: Hallucination in LLMs
  - Why needed here: Understanding hallucination is crucial for developing effective evaluation benchmarks and improving LLM reliability in professional contexts.
  - Quick check question: What is hallucination in the context of LLMs, and why is it a concern for professional content generation?
- Concept: Unconstrained vs. constrained generation
  - Why needed here: The choice between unconstrained and constrained generation methods significantly impacts the realism and effectiveness of hallucination evaluation benchmarks.
  - Quick check question: How does unconstrained generation differ from constrained generation, and what are the advantages and disadvantages of each approach?
- Concept: Evaluation metrics for NLG tasks
  - Why needed here: Understanding various evaluation metrics is essential for interpreting benchmark results and comparing LLM performance across different tasks.
  - Quick check question: What are some common evaluation metrics used in NLG tasks, and how do they differ in their focus and application?

## Architecture Onboarding

- Component map:
  - Data Collection and Pre-processing -> Unconstrained Hallucination Generation -> Hallucination Ranking -> Automatic Labeling and Human Recheck -> Evaluation Framework
- Critical path: Data Collection → Unconstrained Generation → Hallucination Ranking → Annotation → Evaluation
- Design tradeoffs:
  - Unconstrained vs. constrained generation: Unconstrained generation provides more realistic results but is more challenging to evaluate.
  - Automatic vs. human annotation: Automatic annotation is faster and cheaper but may be less accurate than human annotation.
  - Multiple evaluation modalities: Different modalities provide complementary insights but increase computational complexity.
- Failure signatures:
  - Low conversion rate from candidate to final dataset: Indicates issues with hallucination generation or annotation.
  - Poor performance across all LLMs: Suggests problems with the benchmark design or evaluation metrics.
  - High variance in results across different evaluation modalities: Indicates inconsistencies in the benchmark or evaluation process.
- First 3 experiments:
  1. Generate hallucinated continuations using a single LLM and manually verify the results to validate the unconstrained generation approach.
  2. Compare the performance of different evaluation metrics on a small subset of the dataset to select the most effective metrics.
  3. Conduct a pilot study with a small number of LLMs using all evaluation modalities to identify potential issues and refine the benchmark design.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the occurrence of hallucinations vary across different types of news content (document-intensive, number-intensive, knowledge-intensive, and general news)?
- Basis in paper: [explicit] The paper categorizes news into four types and analyzes the frequency of hallucinations in each category, noting that number-intensive and general news have a higher likelihood of hallucinations, while knowledge-intensive and document-intensive news have a lower likelihood.
- Why unresolved: While the paper provides initial observations, a deeper analysis is needed to understand the underlying reasons for these differences and how they relate to the specific characteristics of each news type.
- What evidence would resolve it: A comprehensive statistical analysis comparing hallucination frequencies across news types, along with an investigation into the linguistic and factual features of each category that might contribute to hallucination rates.

### Open Question 2
- Question: How do different evaluation methods (discriminative, selective, and generative) correlate with each other in assessing the hallucination tendencies of LLMs?
- Basis in paper: [explicit] The paper introduces three evaluation methods and presents results for each, noting that discriminative evaluation requires strong foundational capabilities, while generative evaluation closely mimics real-world applications but is challenging to assess automatically.
- Why unresolved: The paper does not directly compare the outcomes of these methods or explore their relationships in terms of reliability and applicability.
- What evidence would resolve it: A comparative study analyzing the consistency and predictive power of each evaluation method, along with an exploration of how they complement each other in providing a holistic assessment of LLM hallucination tendencies.

### Open Question 3
- Question: What are the specific linguistic and factual features of news content that contribute to the occurrence of hallucinations in LLMs?
- Basis in paper: [inferred] The paper mentions that number-intensive news often includes challenging numeric values and that general news contains diverse vocabulary, which might contribute to hallucination rates.
- Why unresolved: The paper does not provide a detailed analysis of the linguistic and factual features of news content that influence hallucination occurrence.
- What evidence would resolve it: A detailed linguistic analysis of news content, focusing on features such as numeric complexity, vocabulary diversity, and factual density, along with an investigation into how these features correlate with hallucination rates in LLM-generated continuations.

## Limitations
- The benchmark's reliance on GPT series models for discriminative evaluation may introduce bias in hallucination detection results
- Automatic keyword-based labeling for hallucination detection may miss nuanced hallucinations or incorrectly flag factual statements, particularly in knowledge-intensive domains
- The focus on Chinese language limits generalizability to other languages, though methodology could potentially be adapted

## Confidence
- **High Confidence**: The benchmark construction methodology and the overall evaluation framework design are well-specified and reproducible
- **Medium Confidence**: The claim that unconstrained generation provides more realistic evaluation than constrained methods, as this depends on the quality and representativeness of the generated dataset
- **Low Confidence**: The relative performance rankings of different LLMs, as these may be influenced by the specific evaluation setup and potential biases in the discriminative evaluation using GPT models

## Next Checks
1. Conduct cross-validation using human evaluators to verify the accuracy of the automatic keyword-based hallucination detection, particularly for knowledge-intensive news categories
2. Test the benchmark framework on additional language pairs to assess its generalizability beyond Chinese
3. Implement ablation studies to quantify the impact of using GPT models in the discriminative evaluation on the overall benchmark results