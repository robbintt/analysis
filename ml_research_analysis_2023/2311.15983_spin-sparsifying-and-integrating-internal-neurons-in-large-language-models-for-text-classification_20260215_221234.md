---
ver: rpa2
title: 'SPIN: Sparsifying and Integrating Internal Neurons in Large Language Models
  for Text Classification'
arxiv_id: '2311.15983'
source_url: https://arxiv.org/abs/2311.15983
tags:
- classification
- layer
- performance
- neurons
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SPIN, a model-agnostic framework that sparsifies
  and integrates internal neurons of intermediate layers of LLMs for text classification.
  The method involves layer-wise linear probing to identify salient neurons, followed
  by cross-layer aggregation to form task-specific classifiers.
---

# SPIN: Sparsifying and Integrating Internal Neurons in Large Language Models for Text Classification

## Quick Facts
- arXiv ID: 2311.15983
- Source URL: https://arxiv.org/abs/2311.15983
- Reference count: 10
- Key outcome: SPIN significantly improves text classification accuracy, efficiency, and interpretability compared to existing approaches while reducing trainable parameters and training time

## Executive Summary
SPIN is a model-agnostic framework that improves text classification by sparsifying and integrating internal neurons from intermediate layers of large language models. The method uses layer-wise linear probing to identify task-specific salient neurons, then aggregates these across layers to form more effective classifiers. Experiments demonstrate consistent performance improvements across multiple datasets and model architectures, with reduced computational requirements compared to traditional fine-tuning approaches.

## Method Summary
The SPIN method involves three main stages: first, it performs layer-wise linear probing using logistic regression to identify salient neurons in each layer's feedforward network activations and hidden states; second, it sparsifies these neurons by selecting those with high L2-norm weights above a threshold; third, it aggregates the sparsified neurons across layers to create multi-layered features for the final classifier. This approach leverages internal representations rather than just final-layer outputs, capturing complementary information at different abstraction levels.

## Key Results
- Significant accuracy improvements across IMDb, SST-2, and EDOS datasets compared to baseline methods
- Reduced trainable parameters through neuron sparsification while maintaining or improving performance
- Faster training and inference times due to fewer parameters and more efficient feature representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparsifying salient neurons layer-by-layer improves classification by removing noise from unrelated neurons while preserving task-relevant features
- Mechanism: Layer-wise logistic regression probes identify neurons with high-magnitude weights as task-specific, then threshold by squared L2-norm to select only the most important neurons
- Core assumption: Task-relevant features are represented linearly and can be captured by logistic regression weights
- Evidence anchors: [abstract] "SPIN sparsifies internal neurons by linear probing-based salient neuron selection layer by layer"; [section 3.3] "LR probes provide a mechanism to identify task-specific salient neurons"

### Mechanism 2
- Claim: Aggregating salient neurons across layers captures complementary information at different abstraction levels, improving performance over single-layer probing
- Mechanism: After identifying salient neurons in each layer, their activations are concatenated across layers to form a multi-layered feature vector
- Core assumption: Lower layers capture elementary features while higher layers capture abstract concepts, and combining both yields better performance
- Evidence anchors: [section 3.4] "lower layers tend to focus on encoding elementary features... foundational information which higher layers utilize for more abstract and complex aspects of understanding"

### Mechanism 3
- Claim: Using internal representations instead of just final-layer outputs captures more task-relevant information and improves efficiency
- Mechanism: The method pools hidden states and FFN activations across all layers using multiple pooling strategies (average, max, first token)
- Core assumption: Internal representations contain task-relevant information that is lost when only using final-layer outputs
- Evidence anchors: [abstract] "our method leverages activations of feedforward neural network (FFN) neurons within transformer blocks and hidden states across different layers"

## Foundational Learning

- Concept: Linear representation hypothesis
  - Why needed here: The method relies on linear probes (logistic regression) to identify salient neurons, assuming features are linearly separable
  - Quick check question: If features are non-linearly separable, would linear probes still be effective?

- Concept: L1 and L2 regularization
  - Why needed here: L1 regularization creates sparsity for feature selection, while L2 norm thresholding controls how many neurons to select based on their collective importance
  - Quick check question: How does changing the L1 regularization strength affect the number of neurons selected?

- Concept: Layer-wise vs. cross-layer feature aggregation
  - Why needed here: Understanding why combining features from multiple layers is beneficial requires knowing how different layers capture different levels of abstraction
  - Quick check question: What happens to performance if you only use features from the top 2-3 layers versus all layers?

## Architecture Onboarding

- Component map: Tokenizer → LLM forward pass (store activations/hidden states) → Layer-wise linear probing → Neuron sparsification → Cross-layer aggregation → Final classifier
- Critical path: 1) Forward pass through LLM to collect all intermediate representations; 2) Train logistic regression probes layer-by-layer; 3) Identify and sparsify salient neurons using L2-norm thresholding; 4) Aggregate sparsified features across layers; 5) Train final classifier on aggregated features
- Design tradeoffs:
  - Memory vs. Performance: Storing all intermediate representations requires significant memory but provides better features
  - Sparsity vs. Completeness: Higher sparsification thresholds reduce computation but may lose important features
  - Layer selection: Using fewer layers is faster but may miss important abstractions
- Failure signatures: Performance doesn't improve over baseline → likely issues with neuron selection or aggregation strategy; Memory errors → too many layers/activations stored; Slow training → high sparsity threshold or too many layers aggregated
- First 3 experiments: 1) Implement basic forward pass collecting hidden states and activations for a small model on a single dataset; 2) Train layer-wise logistic regression probes and visualize neuron importance to validate the sparsification approach; 3) Implement cross-layer aggregation and compare performance against baseline single-token classification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Sparsify-then-Classify (STC) compare to other methods for text classification tasks that require more fine-grained analysis, such as token-level classification or entity recognition?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of STC for text classification tasks, but it does not explicitly explore its performance on more granular tasks like token-level classification or entity recognition
- Why unresolved: The paper focuses on evaluating STC for text classification tasks, leaving its performance on more fine-grained tasks unexplored
- What evidence would resolve it: Conducting experiments to compare the performance of STC against other methods for token-level classification or entity recognition tasks would provide evidence to resolve this question

### Open Question 2
- Question: How does the choice of pooling strategies impact the performance of STC on different types of text classification tasks?
- Basis in paper: [explicit] The paper mentions using multiple pooling strategies, including average pooling, max pooling, and single token output, but it does not explore the impact of these choices on different text classification tasks
- Why unresolved: The paper does not provide a comprehensive analysis of how different pooling strategies affect the performance of STC on various text classification tasks
- What evidence would resolve it: Conducting experiments to evaluate the performance of STC with different pooling strategies on a diverse set of text classification tasks would provide evidence to resolve this question

### Open Question 3
- Question: How does the performance of STC vary with different model sizes and architectures?
- Basis in paper: [explicit] The paper evaluates the performance of STC on various models, including RoBERTa, DistilBERT, and the GPT2 family, but it does not explore the impact of model size and architecture on STC's performance
- Why unresolved: The paper does not provide a detailed analysis of how STC's performance varies with different model sizes and architectures
- What evidence would resolve it: Conducting experiments to evaluate the performance of STC on a wide range of model sizes and architectures would provide evidence to resolve this question

## Limitations

- The method heavily relies on linear separability assumptions through logistic regression probes, which may not capture complex feature interactions
- Memory overhead from storing all intermediate activations during forward pass hasn't been fully characterized
- Limited model diversity with focus primarily on transformer-based architectures, leaving generalizability to other architectures untested

## Confidence

- **High confidence**: The core technical approach (layer-wise probing followed by aggregation) is well-defined and the experimental methodology is sound
- **Medium confidence**: The efficiency improvements are plausible given the reduction in trainable parameters, but memory overhead concerns remain unaddressed
- **Low confidence**: The generalizability of the method to tasks requiring complex feature interactions or non-linear decision boundaries is questionable given the linear probe foundation

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary the L1 regularization strength and L2-norm threshold η across all datasets and models, plotting performance curves to reveal robustness to hyperparameter choices

2. **Memory overhead quantification**: Measure actual GPU memory usage during training with SPIN versus baseline fine-tuning, including the memory cost of storing all intermediate activations, and calculate the break-even point where parameter reduction compensates for increased memory usage

3. **Non-linear probe validation**: Replace the logistic regression probes with small MLPs to test whether performance depends on linear separability assumptions, comparing neuron selection patterns and downstream performance between linear and non-linear probing approaches