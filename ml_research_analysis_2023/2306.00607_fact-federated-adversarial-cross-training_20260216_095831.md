---
ver: rpa2
title: 'FACT: Federated Adversarial Cross Training'
arxiv_id: '2306.00607'
source_url: https://arxiv.org/abs/2306.00607
tags:
- domain
- data
- source
- target
- fact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FACT addresses federated learning with non-i.i.d. data sources
  where the target client lacks labeled data, a challenging scenario for domain adaptation.
---

# FACT: Federated Adversarial Cross Training

## Quick Facts
- arXiv ID: 2306.00607
- Source URL: https://arxiv.org/abs/2306.00607
- Reference count: 40
- Primary result: FACT achieves 95.2% average accuracy on Digit-Five, Office, and Office-Caltech10 benchmarks, outperforming state-of-the-art federated, non-federated, and source-free domain adaptation models.

## Executive Summary
FACT addresses the challenging scenario of federated learning with non-i.i.d. data sources where the target client lacks labeled data. The method exploits inter-domain differences between source clients to identify and mitigate domain shifts in the target domain without requiring adversarial maximization or access to source data. By cross-initializing two source clients, generating domain-specific representations, and minimizing inter-domain distance on the target, FACT learns domain-invariant features effectively.

## Method Summary
FACT is a federated learning approach that addresses domain adaptation when the target client has no labeled data. The method cross-initializes two source clients per round, trains classifiers on local data, aggregates the feature extractor via FedAvg, and fine-tunes classifiers to adapt to the new generator's latent space. The updated model is then transferred to the target client, where inter-domain distance (IDD) loss is minimized using L1 distance between classifier outputs. The updated generator is aggregated and broadcasted back to all source clients for the next round.

## Key Results
- Achieves 95.2% average accuracy on Digit-Five, Office, and Office-Caltech10 benchmarks
- Outperforms state-of-the-art federated, non-federated, and source-free domain adaptation models
- Maintains robust performance with communication restrictions and varying numbers of participating clients

## Why This Works (Mechanism)

### Mechanism 1
- FACT leverages inter-domain differences between source clients to identify and mitigate domain shifts in the target domain without adversarial maximization.
- Core assumption: Source clients have non-i.i.d. data distributions, creating exploitable inter-domain differences.
- Break condition: If source clients have i.i.d. data, inter-domain differences vanish, making the disagreement measure ineffective.

### Mechanism 2
- FACT's cross-initialization and fine-tuning strategy maintains model performance despite global generator updates.
- Core assumption: The classification heads can adapt to changes in the latent representation space introduced by generator aggregation.
- Break condition: If the generator changes are too large or the fine-tuning step is skipped, the classification heads may fail to adapt, leading to degraded performance.

### Mechanism 3
- FACT is robust to communication restrictions and varying numbers of participating clients.
- Core assumption: The inter-domain distance minimization is effective even with limited communication or many small-source clients.
- Break condition: If communication is too restricted or the number of clients is too large with insufficient data per client, the model may not converge effectively.

## Foundational Learning

- Concept: Federated Learning (FL)
  - Why needed here: FACT is a federated learning approach designed to handle non-i.i.d. data across distributed clients.
  - Quick check question: What is the main challenge in FL when data across clients is non-i.i.d.?

- Concept: Domain Adaptation
  - Why needed here: FACT aims to adapt a model trained on multiple source domains to perform well on a target domain without labeled data.
  - Quick check question: How does FACT measure domain shifts without access to labeled target data?

- Concept: Inter-domain Distance
  - Why needed here: FACT uses the disagreement between domain-specific classifiers on the target data as a measure of domain shift.
  - Quick check question: What does minimizing the inter-domain distance on the target domain achieve?

## Architecture Onboarding

- Component map: Global server -> Source clients (domain-specific classifiers) -> Global feature generator (G) -> Target client (unlabeled data)
- Critical path: Cross-initialization → Source training → Source fine-tuning → Inter-domain distance minimization → Generator update → Next round
- Design tradeoffs:
  - Fine-tuning step improves performance but increases communication costs
  - Cross-initialization requires two source clients per round, limiting applicability with few clients
  - Performance may degrade if source clients have i.i.d. data
- Failure signatures:
  - High inter-domain distance on target data indicates poor domain adaptation
  - Degradation in source domain accuracy suggests overfitting or poor generator updates
  - Inconsistent results across repeated runs may indicate instability
- First 3 experiments:
  1. Evaluate FACT on Digit-Five with all source clients to establish baseline performance
  2. Test FACT with only two source clients to assess performance with minimal source data
  3. Measure the impact of communication restrictions by varying the number of communication rounds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FACT's performance compare when applied to more than four source clients, particularly in terms of scalability and convergence speed?
- Basis in paper: The paper tests FACT with up to 40 source clients but does not extensively explore scalability beyond this point or compare convergence speeds across different numbers of clients.
- Why unresolved: The experiments conducted only extend to 40 source clients, and there is no analysis of how FACT scales with even larger numbers of clients or how convergence speed changes with client count.
- What evidence would resolve it: Conducting experiments with a significantly larger number of source clients and measuring both performance metrics and convergence times would provide clarity.

### Open Question 2
- Question: What are the theoretical guarantees for FACT's performance in scenarios where the data are approximately i.i.d.?
- Basis in paper: The paper mentions that FACT's assumptions are violated in i.i.d. settings and discusses potential issues with verifying domain differences but does not provide theoretical guarantees.
- Why unresolved: The paper acknowledges the limitations of FACT in i.i.d. settings but does not offer a theoretical framework or guarantees for its performance under these conditions.
- What evidence would resolve it: Developing a theoretical analysis that addresses FACT's behavior and guarantees in i.i.d. scenarios would be necessary.

### Open Question 3
- Question: How does the choice of initial learning rate affect FACT's performance and stability across different datasets and experimental setups?
- Basis in paper: The paper uses different initial learning rates for multi-source-multi-target and single-source-single-target experiments but does not explore the impact of varying this parameter extensively.
- Why unresolved: While the paper mentions the use of different learning rates, it does not investigate how sensitive FACT is to this parameter or how it affects performance and stability.
- What evidence would resolve it: Conducting a sensitivity analysis with various initial learning rates across multiple datasets and experimental setups would provide insights into its impact.

## Limitations

- FACT's performance relies on non-i.i.d. data distributions across source clients, which may not always be available
- The method requires at least two source clients per round, limiting applicability with very few clients
- Fine-tuning step increases communication costs and may not be feasible in highly constrained environments

## Confidence

- High confidence in the core mechanism of using inter-domain differences for domain shift detection
- Medium confidence in the cross-initialization and fine-tuning strategy due to limited ablation studies
- Medium confidence in robustness claims regarding communication restrictions - only tested on specific domains
- Low confidence in scalability to very large numbers of source clients based on current experimental scope

## Next Checks

1. Test FACT's performance when source clients have i.i.d. data distributions to validate the non-i.i.d. assumption
2. Measure the impact of removing the fine-tuning step to quantify its contribution to performance
3. Evaluate FACT with varying numbers of source clients (1, 2, 4, 8) to determine scalability limits