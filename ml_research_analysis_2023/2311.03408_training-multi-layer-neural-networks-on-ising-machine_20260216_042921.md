---
ver: rpa2
title: Training Multi-layer Neural Networks on Ising Machine
arxiv_id: '2311.03408'
source_url: https://arxiv.org/abs/2311.03408
tags:
- ising
- problem
- network
- layer
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an Ising learning algorithm that enables
  training of multi-layer feedforward neural networks on Ising machines, a dedicated
  quantum device. The method overcomes the challenge of complex nonlinear network
  topology by formulating the training problem as a quadratic constrained binary optimization
  (QCBO) problem, incorporating binary representation of network topology and order
  reduction of the loss function.
---

# Training Multi-layer Neural Networks on Ising Machine

## Quick Facts
- arXiv ID: 2311.03408
- Source URL: https://arxiv.org/abs/2311.03408
- Reference count: 0
- Primary result: Multi-layer neural network training achieved on Ising machines with 98.3% MNIST classification accuracy

## Executive Summary
This paper introduces an Ising learning algorithm that enables training of multi-layer feedforward neural networks on Ising machines, a dedicated quantum device. The method overcomes the challenge of complex nonlinear network topology by formulating the training problem as a quadratic constrained binary optimization (QCBO) problem, incorporating binary representation of network topology and order reduction of the loss function. The QCBO is then converted to a quadratic unconstrained binary optimization (QUBO) problem solvable on Ising machines using penalty function and Rosenberg order reduction techniques. Theoretical analysis shows the space complexity is O(H²L + HLN log H), quantifying the required number of Ising spins. Experiments on a simulated Ising machine with MNIST dataset demonstrate the algorithm's effectiveness, achieving 98.3% classification accuracy after 700 ms annealing with a 72% success probability of finding the optimal solution.

## Method Summary
The training method converts the neural network training problem into a quadratic unconstrained binary optimization (QUBO) problem suitable for Ising machines. It uses binary encoding to map network parameters to Ising spins, represents network topology as polynomial equality constraints, and employs penalty functions and Rosenberg order reduction to convert the problem into QUBO format. The space complexity is O(H²L + HLN log H), where H is the maximum number of hidden units, L is the number of layers, and N is the input dimension.

## Key Results
- Achieved 98.3% classification accuracy on MNIST dataset using a 1-hidden-layer network with 1 hidden unit
- 72% success probability of finding optimal solution with 700 ms annealing time
- Demonstrated feasibility of training multi-layer neural networks on Ising machines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Binary encoding of network parameters enables direct mapping of QNN weights to Ising spins.
- Mechanism: Each parameter is represented as a sum of binary bits scaled by powers of two, where each bit corresponds to one Ising spin.
- Core assumption: The parameter range can be adequately covered by a finite number of bits without significant quantization error.
- Evidence anchors:
  - [abstract]: "All quantized variables are encoded by binary bits based on binary encoding protocol"
  - [section]: "Each bit corresponds to one spin on Ising machine, therefore it creates a clear linkage between decision variables and Ising spins"
- Break condition: Insufficient bit width leading to poor parameter resolution, causing degraded network performance

### Mechanism 2
- Claim: Constraint representation captures the feedforward topology as equality constraints that can be eliminated.
- Mechanism: Linear transformations and activation functions are expressed as polynomial equality constraints (e.g., W(k)a(k-1) + b(k) = s(k) for linear layers, a(k) ⊙ s(k) = r(k) for sign activation). These constraints are then eliminated using penalty function methods.
- Core assumption: The constraints can be expressed as polynomials in the binary variables without introducing nonlinear non-polynomial terms.
- Evidence anchors:
  - [abstract]: "The constraint representation captures the feedforward topology of QNN by describing the linear transformation and activation function as equality constraints"
  - [section]: "To capture this behavior, we firstly come up with two constraints: a(k) ⊙ s(k) = r(k) (4) a(k) + 2r(k) ≥ 1, (5)"
- Break condition: Activation functions that cannot be expressed as polynomial constraints (e.g., sigmoid, tanh)

### Mechanism 3
- Claim: Rosenberg order reduction converts high-order loss functions into quadratic form solvable by Ising machines.
- Mechanism: High-order terms in the loss function are iteratively reduced to quadratic by introducing auxiliary binary variables and adding Rosenberg polynomials h(u1, u2, v) = 3v + u1u2 - 2u1v - 2u2v as penalty terms.
- Core assumption: The Rosenberg polynomial maintains the equivalence of the optimization problem while reducing order.
- Evidence anchors:
  - [abstract]: "The conversion leverages both penalty function method and Rosenberg order reduction method, which together eliminate the equality constraints and reduce high-order loss function into a quadratic one"
  - [section]: "The general form of Rosenberg polynomial is h(u1, u2, v) = 3v + u1u2 - 2u1v - 2u2v, where u1, u2, v ∈ {0, 1}"
- Break condition: Excessive auxiliary variables making the problem intractable for current Ising machine hardware

## Foundational Learning

- Concept: Quadratic Unconstrained Binary Optimization (QUBO)
  - Why needed here: The final training problem must be in QUBO format to be solvable on Ising machines
  - Quick check question: What is the general form of a QUBO problem and why is it suitable for Ising machines?

- Concept: Binary encoding of decimal numbers
  - Why needed here: Parameters must be mapped to binary spins, requiring a systematic encoding scheme
  - Quick check question: How does the binary encoding protocol convert a decimal parameter value into binary bits?

- Concept: Penalty function method for constraint elimination
  - Why needed here: The constraint representation introduces equality constraints that must be removed for QUBO conversion
  - Quick check question: How does the penalty function method eliminate equality constraints while preserving the optimal solution?

## Architecture Onboarding

- Component map: Classical preprocessing -> CPU QCBO formulation -> CPU QUBO conversion -> QPU QUBO solving -> CPU parameter decoding
- Critical path: Classical preprocessing → CPU QCBO formulation → CPU QUBO conversion → QPU QUBO solving → CPU parameter decoding
- Design tradeoffs:
  - Bit width vs. accuracy: Higher bit width improves parameter resolution but increases spin count
  - Penalty coefficient ρ vs. constraint satisfaction: Larger ρ improves constraint satisfaction but may create ill-conditioned problems
  - Annealing time vs. success probability: Longer annealing increases success probability but also computation time
- Failure signatures:
  - High loss after decoding: Indicates insufficient bit width or poor QUBO formulation
  - Low success probability: May require longer annealing time or better Ising machine hardware
  - Oscillating training results: Could indicate unstable penalty coefficient or order reduction
- First 3 experiments:
  1. Train a single-layer network on a simple dataset (e.g., two-moon) to verify basic functionality
  2. Train a two-layer network on the same dataset to validate multi-layer capability
  3. Train on MNIST with a small network to test scalability and practical performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the maximum depth of neural networks that can be practically trained using this Ising learning algorithm, given current hardware limitations?
- Basis in paper: [explicit] The paper mentions that the space complexity is O(H²L + HLN log H) and discusses future prospects of training deeper networks as the number of spins on Ising machines increases.
- Why unresolved: The paper does not provide specific benchmarks or limitations for network depth based on current hardware capabilities. It only mentions the potential for training deeper networks in the future.
- What evidence would resolve it: Empirical results showing the maximum depth of networks successfully trained on current Ising machine hardware, along with analysis of how the number of spins limits this depth.

### Open Question 2
- Question: How does the performance of the Ising learning algorithm compare to traditional gradient-based methods for training neural networks in terms of accuracy and computational efficiency?
- Basis in paper: [inferred] The paper introduces an alternative to gradient-based backpropagation and demonstrates the effectiveness of the algorithm on MNIST dataset. However, it does not provide a direct comparison with traditional methods.
- Why unresolved: The paper focuses on demonstrating the feasibility and correctness of the algorithm rather than comparing it to existing methods. A comprehensive comparison would require extensive experiments across various datasets and network architectures.
- What evidence would resolve it: Comparative studies showing accuracy, training time, and resource usage of the Ising learning algorithm versus traditional backpropagation methods across multiple datasets and network configurations.

### Open Question 3
- Question: What are the specific challenges and limitations in extending the Ising learning algorithm to support more complex network modules such as attention layers and cross entropy loss?
- Basis in paper: [explicit] The paper mentions that attention layers and cross entropy loss are not supported yet because non-polynomial functions cannot be formulated in polynomial constraints.
- Why unresolved: The paper acknowledges the limitation but does not explore potential solutions or workarounds for these complex modules. It only suggests using Taylor expansion as a possible approach.
- What evidence would resolve it: Detailed analysis of the mathematical and computational challenges in formulating non-polynomial functions as polynomial constraints, along with experimental results on implementing these complex modules using approximation techniques.

## Limitations
- Performance critically dependent on specific Ising machine hardware parameters (annealing time, spin count)
- Limited to feedforward networks with sign activation functions
- Scalability constraints due to current hardware limitations on spin count and connectivity

## Confidence
- High Confidence: The core theoretical framework connecting QCBO formulation to QUBO conversion through binary encoding and constraint elimination is mathematically sound and well-established.
- Medium Confidence: The experimental results demonstrating 98.3% accuracy are promising but based on a simplified MNIST dataset (2×2 downsampled images) and may not scale to full-resolution problems.
- Low Confidence: Claims about the algorithm's effectiveness for general deep learning applications are premature given the limited experimental scope and hardware-specific results.

## Next Checks
1. **Hardware-Agnostic Validation**: Implement the algorithm on multiple Ising machine platforms (simulated and physical) to verify that performance is not platform-specific and that the 98.3% accuracy can be reproduced.
2. **Scaling Experiment**: Test the algorithm on progressively deeper networks (3+ layers) and larger hidden units on the full MNIST dataset to identify practical scaling limits and performance degradation points.
3. **Architecture Generalization**: Adapt the algorithm for a different neural network architecture (e.g., convolutional layers or networks with ReLU activation) to test the framework's flexibility beyond the current sign-activation feedforward model.