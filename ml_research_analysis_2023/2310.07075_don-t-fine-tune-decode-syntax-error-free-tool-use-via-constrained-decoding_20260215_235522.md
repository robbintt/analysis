---
ver: rpa2
title: 'Don''t Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding'
arxiv_id: '2310.07075'
source_url: https://arxiv.org/abs/2310.07075
tags:
- tool
- tools
- language
- toolkengpt
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TOOL DEC, a decoding algorithm that uses
  finite state machines to ensure syntactically correct tool calls from language models.
  TOOL DEC eliminates all tool-related syntax errors by constraining decoding to valid
  tool names and argument types, improving accuracy and reducing inference time.
---

# Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding

## Quick Facts
- arXiv ID: 2310.07075
- Source URL: https://arxiv.org/abs/2310.07075
- Authors: 
- Reference count: 8
- Key outcome: TOOL DEC eliminates all tool-related syntax errors by constraining decoding to valid tool names and argument types, improving accuracy and reducing inference time.

## Executive Summary
TOOL DEC introduces a novel decoding algorithm that uses finite state machines to ensure syntactically correct tool calls from language models. Unlike fine-tuning approaches, TOOL DEC eliminates all tool-related syntax errors by constraining decoding to valid tool names and argument types, improving accuracy and reducing inference time. It also generalizes to unseen tools without additional training data or documentation, achieving up to 8x better accuracy than baselines. Experiments on diverse benchmarks, including math reasoning, knowledge graphs, and real-world APIs, demonstrate TOOL DEC's effectiveness across multiple domains.

## Method Summary
TOOL DEC constructs a finite state machine (FSM) from tool documentation and API signatures. During decoding, it samples from a subset of tokens allowed by the current state in the FSM, ensuring syntactically correct tool calls. It switches between text mode and tool mode, generates valid tool names using a trie structure, and passes type-conforming arguments using argument-specific FSMs. This approach eliminates the need for fine-tuning while maintaining generalization to unseen tools.

## Key Results
- Eliminates all tool-related syntax errors by constraining decoding to valid tool names and argument types
- Achieves up to 8x better accuracy than baselines while generalizing to unseen tools
- Reduces inference time by as much as 50% compared to fine-tuning and backtracing approaches

## Why This Works (Mechanism)

### Mechanism 1
Finite-state machines guarantee syntactically correct tool calls by restricting valid token sequences. TOOL DEC constructs an FSM from tool signatures and documentation, ensuring each decoding step only samples from tokens that maintain syntactic validity. The FSM transitions between states based on valid next tokens, preventing generation of invalid tool names or arguments.

### Mechanism 2
TOOL DEC generalizes to unseen tools without additional training data by constructing FSMs from tool signatures alone. When encountering a new tool, TOOL DEC automatically constructs an FSM from the tool's API signature and integrates it into the existing FSM, allowing correct tool invocation without fine-tuning or in-context documentation.

### Mechanism 3
TOOL DEC reduces inference time by eliminating failed tool calls that require backtracing. By ensuring syntactic correctness at each decoding step, TOOL DEC eliminates failed tool calls that would otherwise require the model to backtrace and try alternative tokens, reducing total inference steps.

## Foundational Learning

- Concept: Finite State Machines
  - Why needed here: FSMs provide the mathematical framework for constraining token generation to syntactically valid sequences, ensuring tool calls follow correct grammar
  - Quick check question: How would you represent a tool with two arguments (both integers) as a finite state machine?

- Concept: Constrained Beam Search
  - Why needed here: Understanding how constrained decoding differs from standard beam search is crucial for implementing TOOL DEC efficiently
  - Quick check question: What's the difference between lexical constraints and syntactic constraints in constrained decoding?

- Concept: Tool Signature Parsing
  - Why needed here: TOOL DEC requires extracting argument types and names from tool documentation to construct FSMs automatically
  - Quick check question: Given a tool signature "multiply(a: float, b: float) -> float", what would the FSM look like for argument parsing?

## Architecture Onboarding

- Component map: FSM construction module -> decoding algorithm with FSM constraints -> tool selection mechanism
- Critical path: FSM construction from tool signatures, integration with LLM vocabulary, modified sampling algorithm enforcing FSM constraints
- Design tradeoffs: TOOL DEC trades increased complexity in decoding for guaranteed syntactic correctness and better generalization, whereas fine-tuning approaches trade generalization for potentially better semantic understanding
- Failure signatures: FSM construction errors from malformed tool signatures, state explosion for tools with many arguments, poor tool selection when tool names are ambiguous
- First 3 experiments:
  1. Verify FSM construction correctly represents tool syntax by testing on a small set of math functions with varying argument types
  2. Compare inference speed and accuracy between TOOL DEC and baseline decoding on a single tool use task
  3. Test generalization by evaluating TOOL DEC on unseen tools after training only on a subset of available tools

## Open Questions the Paper Calls Out
The paper mentions that TOOL DEC can be combined with other approaches like fine-tuning and in-context learning, but does not explore the potential benefits of such integration. It also doesn't address how TOOL DEC performs with more complex tool call structures involving nested function calls or conditional logic.

## Limitations
- Lack of detailed implementation specifications for FSM construction process, particularly for complex argument types
- Experimental methodology gaps with insufficient detail about dataset composition and baseline implementations
- Inference time improvements depend heavily on specific baseline implementations and syntax error distribution

## Confidence

**High Confidence**: The core mechanism of using finite state machines to constrain decoding and prevent syntax errors is well-established and theoretically sound.

**Medium Confidence**: The generalization claims to unseen tools are supported by experimental results but rely on assumptions about tool signature sufficiency that aren't thoroughly validated.

**Low Confidence**: The inference time improvements depend heavily on the specific baseline implementations and the distribution of syntax errors in typical tool use scenarios.

## Next Checks

1. Implement TOOL DEC for a small set of REST API tools with varying complexity and verify that the FSM correctly enforces syntactic constraints while allowing semantically valid tool calls.

2. Create a controlled experiment where TOOL DEC is trained on a subset of tools and tested on unseen tools with similar argument patterns but different names, measuring accuracy drop and comparing against fine-tuned baselines.

3. Profile TOOL DEC and a standard decoding baseline on identical tool use tasks, measuring not just total inference time but also breakdown of time spent on tool selection, argument generation, and failed call handling to validate the claimed 50% speedup.