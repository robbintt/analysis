---
ver: rpa2
title: How Generative-AI can be Effectively used in Government Chatbots
arxiv_id: '2312.02181'
source_url: https://arxiv.org/abs/2312.02181
tags:
- government
- similarity
- text
- chatbots
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares two large language models (ChatGPT and Wenxin
  Ernie) with Guangdong Province's government chatbot using a set of administrative
  questions across five life domains. Through text analysis, including topic, similarity,
  conscientiousness, and sentiment analyses, the research identifies significant gaps
  in current government chatbots.
---

# How Generative-AI can be Effectively used in Government Chatbots

## Quick Facts
- arXiv ID: 2312.02181
- Source URL: https://arxiv.org/abs/2312.02181
- Reference count: 15
- Primary result: Integrating LLM features into government chatbots can significantly enhance response quality, emotional expressiveness, and user satisfaction.

## Executive Summary
This study evaluates the effectiveness of large language models (ChatGPT and Wenxin Ernie) compared to a government chatbot in handling administrative queries across five life domains. Through comprehensive text analysis including topic modeling, similarity comparison, conscientiousness, and sentiment analysis, the research demonstrates that LLMs generate more structured, proactive, and emotionally expressive responses. ChatGPT shows stronger consistency in response quality and higher emotional expressiveness, while Ernie provides greater response depth. The findings suggest that incorporating AIGC capabilities into government chatbots could address current limitations in accuracy, engagement, and emotional sensitivity, ultimately improving public satisfaction with e-government services.

## Method Summary
The study employs a horizontal comparison approach using text analysis techniques to evaluate responses from Guangdong Province's government chatbot against ChatGPT and Wenxin Ernie. Researchers collected administrative questions from the National Government Service Website's "Personal Affairs" section, covering education, employment, social security, public security, and retirement care domains. The evaluation pipeline includes LDA topic modeling, BERT-based similarity analysis, spaCy dependency parsing for conscientiousness metrics, and SnowNLP sentiment scoring. The analysis compares response quality, depth, emotional tone, and consistency across the three systems.

## Key Results
- ChatGPT exhibits stronger consistency in response quality and higher emotional expressiveness compared to Wenxin Ernie and the government chatbot
- Both LLMs provide more structured and responsive answers than current government chatbots
- Ernie shows greater response depth but with less variability in emotional tone
- AIGC integration can improve government chatbot accuracy, proactive engagement, and emotional sensitivity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs generate more structured, detailed, and proactive responses than government chatbots
- Mechanism: Deep learning models trained on large corpora can infer context, fill knowledge gaps, and provide nuanced multi-turn dialogue
- Core assumption: Training data covers administrative domains and conversational patterns needed for effective government Q&A
- Evidence anchors: LLM responses are structured and responsive; study compares responses to administrative questions of varying complexity
- Break condition: If training data lacks domain-specific or multilingual coverage, responses become inaccurate or irrelevant

### Mechanism 2
- Claim: Sentiment and conscientiousness analysis shows LLM responses exhibit higher emotional expressiveness and complexity, improving user engagement
- Mechanism: Sentiment scoring and syntactic depth metrics capture emotional tone and information density, correlating with perceived helpfulness
- Core assumption: Users interpret structured, emotionally balanced responses as more trustworthy and supportive
- Evidence anchors: ChatGPT shows higher emotional expressiveness and consistency; sentiment analysis identifies emotional tendencies in text
- Break condition: If emotional tone is perceived as inauthentic or overly generic, user trust may decline

### Mechanism 3
- Claim: Topic and similarity analyses demonstrate LLMs can surface hidden themes and match user intent more accurately than keyword-based search bots
- Mechanism: LDA and BERT-based embeddings uncover latent topics and semantic similarity, enabling proactive content generation
- Core assumption: Latent topic structures in administrative queries align with LLM internal representations
- Evidence anchors: Study uses topic analysis to discover hidden information in text collections; similarity analysis compares response patterns
- Break condition: If topic granularity is too coarse or similarity metrics mismatch user expectations, relevance suffers

## Foundational Learning

- Concept: Natural Language Processing (NLP) fundamentals
  - Why needed here: Understanding tokenization, embeddings, and similarity metrics is critical for interpreting the study's text analysis pipeline
  - Quick check question: What is the difference between TF-IDF and BERT embeddings in measuring text similarity?

- Concept: Sentiment analysis and emotion detection
  - Why needed here: The study uses SnowNLP to quantify chatbot emotional tone; knowing how polarity scores work is key to assessing UX impact
  - Quick check question: How does a sentiment score near 0 differ from one near 1 in the SnowNLP framework?

- Concept: Dependency parsing and syntactic depth
  - Why needed here: Conscientiousness analysis relies on measuring sentence complexity via spaCy's parsing output; this informs response quality assessment
  - Quick check question: What does a higher average dependency depth indicate about sentence structure?

## Architecture Onboarding

- Component map: User query → NLP preprocessing (tokenization, stopword removal) → LLM inference (ChatGPT/Ernie) → Text analysis pipeline (topic, similarity, conscientiousness, sentiment) → Comparison metrics → UX recommendations
- Critical path: Query input → LLM response generation → Post-processing for analysis → Sentiment/conscientiousness scoring → Final evaluation
- Design tradeoffs: Higher LLM depth improves response nuance but increases latency and cost; simpler bots are faster but less context-aware
- Failure signatures: Vague or repetitive answers, mismatched sentiment, shallow topic coverage, high similarity variance indicating inconsistency
- First 3 experiments:
  1. Input procedural and complex questions into both LLM models and measure response length, depth, and sentiment scores
  2. Run LDA on all responses to extract dominant topics and compare coherence scores
  3. Use BERT embeddings to compute pairwise similarity between responses and analyze distribution patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy and reliability of responses from ChatGPT compare to Wenxin Ernie when answering procedural versus complex government-related questions?
- Basis in paper: The study conducts a comparative analysis of ChatGPT and Wenxin Ernie's responses to both procedural and complex questions using similarity analysis, conscientiousness analysis, and sentiment analysis
- Why unresolved: While the study identifies differences in response styles and characteristics, it does not provide a quantitative comparison of accuracy or reliability between the two models for different types of questions
- What evidence would resolve it: A systematic evaluation of response accuracy and reliability for both models, categorized by question type (procedural vs. complex), using metrics such as factual correctness, relevance, and completeness

### Open Question 2
- Question: What specific features or capabilities of ChatGPT contribute to its higher emotional expressiveness compared to Wenxin Ernie in government chatbot interactions?
- Basis in paper: The sentiment analysis reveals that ChatGPT exhibits higher emotional expressiveness compared to Ernie, with a more pronounced trend of polarization in sentiment
- Why unresolved: The study does not delve into the underlying features or mechanisms that enable ChatGPT to express emotions more effectively in its responses
- What evidence would resolve it: An analysis of the linguistic and contextual features of responses from both models, identifying specific elements that contribute to emotional expressiveness, such as word choice, tone, and sentiment-bearing phrases

### Open Question 3
- Question: How do user preferences for government chatbot characteristics, such as intelligence, initiative, and responsibility, influence the perceived effectiveness and satisfaction with chatbot interactions?
- Basis in paper: The study mentions that user preferences for chatbot characteristics play a role in determining the effectiveness and satisfaction with government chatbot interactions
- Why unresolved: The study does not provide empirical data or analysis on how specific user preferences impact the perceived effectiveness and satisfaction with chatbot interactions
- What evidence would resolve it: User surveys or experiments measuring the relationship between user preferences for chatbot characteristics and their perceived effectiveness and satisfaction with chatbot interactions

## Limitations
- The specific questions used for evaluation are not provided, making independent verification difficult
- Limited external validation exists as corpus comparison shows minimal overlap with related research on government chatbot effectiveness
- The interpretation of emotional expressiveness as inherently beneficial lacks empirical grounding without user studies

## Confidence

- **High confidence**: The comparative framework itself (using multiple LLMs against government chatbots) is methodologically sound and clearly specified. The observed differences in response structure and depth between LLM and government chatbot outputs are reliably measurable.

- **Medium confidence**: The conclusions about emotional expressiveness improving user engagement and satisfaction are plausible but under-supported. While sentiment scores differ between models, the causal link to user experience remains theoretical without user studies.

- **Low confidence**: Claims about LLM superiority in surfacing "hidden themes" through topic analysis lack direct validation. The LDA coherence scores and similarity metrics show variation but do not conclusively demonstrate improved information retrieval or user satisfaction.

## Next Checks

1. Reproduce with standardized question sets: Create and test a fixed set of 20-30 administrative questions across all five life domains, ensuring consistent evaluation conditions across all three chatbot systems.

2. User perception study: Conduct a blind user study where participants rate responses from government chatbot, ChatGPT, and Ernie on helpfulness, accuracy, and trustworthiness without knowing the source.

3. Factual accuracy audit: Implement a systematic fact-checking protocol for all responses, particularly focusing on procedural correctness for administrative queries where misinformation could have serious consequences.