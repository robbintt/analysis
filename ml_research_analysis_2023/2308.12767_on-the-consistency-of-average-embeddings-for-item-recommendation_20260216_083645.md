---
ver: rpa2
title: On the Consistency of Average Embeddings for Item Recommendation
arxiv_id: '2308.12767'
source_url: https://arxiv.org/abs/2308.12767
tags:
- embeddings
- consistency
- average
- items
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper studies whether averaging item embeddings is a reliable\
  \ practice for representing users or higher-level concepts in recommender systems.\
  \ The authors define a theoretical expected precision score, Consistency\U0001D458\
  (X), to measure how well average embeddings retain the items used to construct them."
---

# On the Consistency of Average Embeddings for Item Recommendation

## Quick Facts
- arXiv ID: 2308.12767
- Source URL: https://arxiv.org/abs/2308.12767
- Reference count: 30
- Primary result: Average embeddings are significantly less consistent than theory predicts, with real-world scores (2-14% for k=50) far below theoretical expectations

## Executive Summary
This paper investigates whether averaging item embeddings is a reliable method for representing users or higher-level concepts in recommender systems. The authors introduce a theoretical metric called Consistencyₖ(X) to measure how well average embeddings retain the items used to construct them. Through mathematical analysis under assumptions of independent identically distributed (i.i.d.) embeddings and inner product similarity, they derive expressions for expected consistency. However, empirical experiments on real-world music streaming embeddings reveal that average embeddings are significantly less consistent than theory predicts, suggesting that real-world embeddings deviate substantially from the i.i.d. assumption. The findings highlight the need for better alignment between real-world embeddings and theoretical assumptions to improve the reliability of average embeddings in recommendation tasks.

## Method Summary
The authors define Consistencyₖ(X) as an expected precision metric measuring the percentage of original items among the k nearest neighbors of an average embedding. Under assumptions of i.i.d. embeddings with finite moments and inner product similarity, they use the Central Limit Theorem and order statistics to derive a closed-form expression for Consistencyₖ(X) involving the error function. The empirical evaluation applies this metric to three variants of music track embeddings from Deezer (TT-SVD, UT-ALS, and 2M-TT-SVD) with dimensions 128 or 256 and item counts of 50,000 or 2,000,000. They compute consistency scores for various values of k and compare empirical results with theoretical predictions, also testing the effect of centering embeddings by subtracting the mean from each dimension.

## Key Results
- Theoretical analysis shows Consistencyₖ(X) follows a closed-form expression under i.i.d. assumptions with inner product similarity
- Empirical results on real-world music embeddings show consistency scores of only 2-14% for k=50, far below theoretical predictions
- Centering embeddings (subtracting mean) slightly improves consistency by aligning with theoretical assumptions
- Consistency decreases as k increases and as catalog size N increases, matching theoretical expectations
- UT-ALS embeddings show more stable consistency across k values compared to TT-SVD embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Average embeddings can be reliable representations when the underlying embedding distribution satisfies independence, identical distribution, and zero mean conditions.
- Mechanism: Under these assumptions, the inner product similarity between an item and the average embedding follows a normal distribution. The probability that a random item from the original set is more similar to the average than an unrelated item can be computed using the error function, showing that high consistency is theoretically achievable.
- Core assumption: The embedding dimensions are i.i.d. random variables with finite mean, variance, skewness, and kurtosis, and the similarity metric is the inner product.
- Evidence anchors:
  - [abstract] "Under general assumptions (independent identically distributed embeddings, inner product similarity), they derive mathematical expressions for the consistency score..."
  - [section] "We focus on the setting where X is a set of independent and identically distributed (i.i.d.) multi-dimensional random variables..."
- Break condition: If embeddings are not i.i.d. (e.g., due to learning algorithms like ALS or SVD), or if the mean is non-zero, the consistency score degrades significantly.

### Mechanism 2
- Claim: The consistency score is highly sensitive to the number of items k in the set being averaged and the total catalog size N.
- Mechanism: As k increases, the average embedding becomes less likely to remain close to all original items simultaneously. As N increases, the chance of unrelated items being close to the average by chance grows, lowering consistency.
- Core assumption: The similarity distribution is normal and the catalog is large enough for random collisions to matter.
- Evidence anchors:
  - [abstract] "...consistency scores (e.g., 2-14% for k=50)" shows the empirical impact of k.
  - [section] "Regarding k, this result is coherent with our above interpretation... Regarding N, such a result is also unsurprising..."
- Break condition: When k is very small (e.g., k=2), even with large N, consistency can remain relatively high, but this breaks down quickly as k grows.

### Mechanism 3
- Claim: Centering embeddings (subtracting the mean) can improve consistency by aligning the distribution with theoretical assumptions.
- Mechanism: Centering ensures the mean of each embedding dimension is zero, which simplifies the theoretical analysis and removes bias that can hurt similarity preservation.
- Core assumption: The centered embeddings still satisfy i.i.d. conditions and the similarity metric remains meaningful after centering.
- Evidence anchors:
  - [abstract] "...we note that, in Figure 3, we computed scores on centered embeddings... this centering operation slightly improves the consistency..."
  - [section] "Besides being in line with Section 3, our tests revealed that this centering operation slightly improves the consistency..."
- Break condition: If centering significantly distorts the original similarity structure (e.g., if embeddings are not symmetrically distributed), the benefit may be lost or reversed.

## Foundational Learning

- Concept: Central Limit Theorem (CLT) and its application to sums of random variables.
  - Why needed here: The theoretical analysis relies on CLT to approximate the distribution of inner product similarities as normal, which is crucial for deriving the consistency score formula.
  - Quick check question: If you sum k i.i.d. random variables, what happens to the distribution of the sum as k increases, assuming finite variance?

- Concept: Order statistics and their use in computing probabilities involving ranked similarity values.
  - Why needed here: The consistency score calculation involves comparing the ith highest similarity in one set to the (k-i+1)th highest in another, requiring order statistic distributions.
  - Quick check question: Given a set of n i.i.d. random variables, how is the distribution of the ith order statistic related to the underlying distribution?

- Concept: Error function (erf) and its role in computing tail probabilities of normal distributions.
  - Why needed here: The theoretical consistency formula involves the erf function to compute the probability that one normal variable exceeds another.
  - Quick check question: What is the relationship between the error function and the cumulative distribution function of a standard normal variable?

## Architecture Onboarding

- Component map:
  Embedding generation (SVD, ALS) -> Averaging layer (computes μU) -> Similarity computation (inner product) -> Consistency evaluation (Precisionₖ, Consistencyₖ) -> Theoretical analysis (CLT, order statistics)

- Critical path:
  1. Generate or load embeddings X
  2. Select a subset U of k items
  3. Compute average embedding μU
  4. Find k nearest neighbors of μU in X
  5. Calculate Precisionₖ(U) and Consistencyₖ(X)
  6. (Optional) Center embeddings and repeat

- Design tradeoffs:
  - Embedding dimension vs. consistency: higher dimensions improve theoretical consistency but increase computation
  - Similarity metric choice: inner product is theoretically tractable but may not reflect all recommendation scenarios
  - Centering vs. original scale: centering aligns with theory but may distort learned relationships

- Failure signatures:
  - Consistently low consistency scores across different k values suggest embeddings are not i.i.d.
  - High variance in consistency across random subsets U indicates unstable embedding distributions
  - Discrepancy between theoretical and empirical consistency suggests model assumptions are violated

- First 3 experiments:
  1. Generate synthetic embeddings from N(0,1) and compute Consistencyₖ(X) for various k and N to validate theoretical predictions
  2. Apply the consistency evaluation pipeline to real-world embeddings (e.g., TT-SVD, UT-ALS) and compare scores with theoretical expectations
  3. Test the effect of centering on consistency by comparing scores before and after mean subtraction for each embedding dimension

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does centering embeddings affect the consistency of average embeddings in real-world datasets, and what are the trade-offs between centering and preserving initial similarities?
- Basis in paper: [explicit] The authors discuss centering embeddings in Section 4.2, noting that it slightly improves consistency but modifies initial similarities.
- Why unresolved: The paper does not provide a detailed analysis of the impact of centering on consistency or the trade-offs involved.
- What evidence would resolve it: Empirical studies comparing consistency scores with and without centering, and an analysis of how centering affects downstream recommendation performance.

### Open Question 2
- Question: Can regularization techniques, such as Kullback-Leibler divergence, be effectively used during training to enforce the identical distribution of embedding dimensions and improve consistency?
- Basis in paper: [inferred] The authors suggest in Section 4.2 that future research could explore adding regularization terms to the optimized loss during training.
- Why unresolved: The paper does not implement or test such regularization techniques.
- What evidence would resolve it: Experimental results showing the impact of different regularization strategies on consistency scores and recommendation performance.

### Open Question 3
- Question: How do different representation learning algorithms (e.g., SVD, ALS) affect the consistency of average embeddings, and are there specific algorithms that are more suitable for downstream applications involving average operations on large collections?
- Basis in paper: [explicit] The authors observe in Section 4.2 that consistency scores differ between TT-SVD and UT-ALS embeddings, with UT-ALS showing steady consistency.
- Why unresolved: The paper does not provide a comprehensive comparison of different algorithms or explore the reasons behind their varying performance.
- What evidence would resolve it: A systematic study comparing consistency scores across multiple algorithms and an analysis of their underlying properties that contribute to consistency.

## Limitations

- The theoretical analysis assumes embeddings are independent and identically distributed random variables, which does not hold for embeddings learned through matrix factorization or neural networks that capture structured user-item interactions
- The paper does not quantify how much the violation of i.i.d. assumptions affects the gap between theoretical and empirical consistency scores
- The effect of centering on consistency is mentioned but not thoroughly analyzed, with only a small improvement observed without detailed investigation

## Confidence

- Theoretical derivation of Consistencyₖ(X) under i.i.d. assumptions: **High** - The mathematical analysis is rigorous and follows standard probability theory
- Empirical evaluation showing low consistency in real-world embeddings: **Medium** - The experiments are well-designed, but the interpretation of results depends on the validity of theoretical assumptions
- Claim that centering improves consistency: **Low** - While the paper mentions a slight improvement, the effect size is small and not thoroughly analyzed

## Next Checks

1. **Synthetic Embedding Experiments**: Generate synthetic embeddings that strictly follow the i.i.d. N(0,1) assumption and compute Consistencyₖ(X) for various k and N. Compare the empirical results with the theoretical predictions to validate the accuracy of the CLT-based approximation.

2. **Distribution Analysis of Real Embeddings**: Perform statistical tests (e.g., Kolmogorov-Smirnov, Anderson-Darling) on the Deezer embeddings to quantify how much they deviate from the i.i.d. assumption. Analyze the marginal distributions of each dimension and test for independence between dimensions.

3. **Ablation Study on Centering**: Conduct a more thorough analysis of the centering operation's effect on consistency. Compare consistency scores before and after centering for different embedding variants (TT-SVD, UT-ALS, 2M-TT-SVD) and different values of k. Quantify the improvement in terms of percentage points and assess whether the effect is consistent across all variants.