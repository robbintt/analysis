---
ver: rpa2
title: 'Language Model Behavior: A Comprehensive Survey'
arxiv_id: '2303.11504'
source_url: https://arxiv.org/abs/2303.11504
tags:
- language
- pages
- proceedings
- arxiv
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews over 250 recent studies of
  English transformer language model behavior before task-specific fine-tuning, covering
  syntax, semantics, pragmatics, world knowledge, reasoning, memorization, and bias.
  The analysis reveals that language models possess basic linguistic capabilities
  but are highly sensitive to specific inputs and surface features.
---

# Language Model Behavior: A Comprehensive Survey

## Quick Facts
- arXiv ID: 2303.11504
- Source URL: https://arxiv.org/abs/2303.11504
- Reference count: 40
- Language models exhibit basic linguistic capabilities but remain highly sensitive to specific inputs and surface features, with performance improving with scale but still generating unfactual responses, commonsense errors, and social biases.

## Executive Summary
This comprehensive survey analyzes over 250 recent studies of English transformer language models before task-specific fine-tuning, examining capabilities across syntax, semantics, pragmatics, world knowledge, reasoning, memorization, and bias. The findings reveal that while language models possess fundamental linguistic abilities, they demonstrate significant sensitivity to specific inputs and surface features, often producing errors that can be understood as over-generalizations or under-generalizations of learned text patterns. Despite dramatic improvements with scale, models still generate unfactual responses, commonsense errors, memorized text, and social biases. The survey provides a systematic framework for understanding these behaviors through the lens of text pattern generalization, offering valuable insights for both applied work and research in adjacent fields using language models.

## Method Summary
The survey methodology employed Semantic Scholar to identify relevant papers, starting with 271 seed papers and expanding to over 15,000 papers through citation tracking. Researchers then manually filtered these by title to approximately 1,500 potentially relevant papers, followed by abstract screening to select around 400 highly relevant studies. The final selection focused specifically on masked and autoregressive English Transformer language models not fine-tuned for specific downstream tasks. The survey synthesizes findings across multiple linguistic and cognitive domains, organizing them by capability area while maintaining consistent methodological standards for evaluation and reporting.

## Key Results
- Language models demonstrate basic syntactic, semantic, and pragmatic capabilities but show high sensitivity to specific inputs and surface features
- Larger models generally perform better across tasks, but still generate unfactual responses, commonsense errors, and social biases
- Many model weaknesses can be understood as over-generalizations or under-generalizations of learned text patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models exhibit basic linguistic capabilities but are highly sensitive to specific inputs and surface features.
- Mechanism: The models learn patterns from large text corpora during pre-training, but these patterns are over-generalized or under-generalized depending on the specific context and words involved.
- Core assumption: The language modeling objective (predicting masked or upcoming words) inherently captures linguistic patterns, but not necessarily the underlying rules or grounded knowledge.
- Evidence anchors:
  - [abstract] "Language models possess basic capabilities in syntax, semantics, pragmatics, world knowledge, and reasoning, but these capabilities are sensitive to specific inputs and surface features."
  - [section] "Many of these weaknesses can be framed as over-generalizations or under-generalizations of learned patterns in text."
- Break Condition: If a model is trained with a different objective (e.g., supervised fine-tuning) or on a very different dataset, this mechanism may not hold.

### Mechanism 2
- Claim: Larger language models generally perform better on linguistic tasks, but their performance is still far from perfect.
- Mechanism: Increasing model size allows for learning more robust patterns and memorizing more examples, but doesn't necessarily lead to a deeper understanding of language.
- Core assumption: Model size is a proxy for the model's capacity to learn complex patterns and store information.
- Evidence anchors:
  - [abstract] "Despite dramatic increases in generated text quality as models scale to hundreds of billions of parameters, the models are still prone to unfactual responses, commonsense errors, memorized text, and social biases."
- Break Condition: If a model is trained with a different architecture or objective, or on a very different dataset, the scaling laws may not hold.

### Mechanism 3
- Claim: Language models can be viewed as performing text pattern generalization, with strengths and weaknesses arising from correct or incorrect generalizations.
- Mechanism: The models learn to generalize from text examples observed during pre-training to novel examples, but this generalization can be under-generalized (relying too much on specific examples) or over-generalized (applying patterns too broadly).
- Core assumption: The language modeling objective inherently requires the model to generalize from observed text to generate plausible completions for novel inputs.
- Evidence anchors:
  - [section] "Many of the strengths and weaknesses of language models can be viewed through the lens of text pattern generalization. Over-generalizations and under-generalizations of learned patterns in text simultaneously provide insights into the impressive capabilities and brittle responses of large language models."
- Break Condition: If a model is trained with a different objective or architecture, or on a very different dataset, this generalization mechanism may not hold.

## Foundational Learning

- Concept: Transformer architecture
  - Why needed here: Understanding the basic building blocks of language models (tokenization, embeddings, attention, etc.) is crucial for interpreting the results and mechanisms described in the paper.
  - Quick check question: What is the key innovation of the Transformer architecture that allows it to handle long-range dependencies in text?

- Concept: Pre-training vs. fine-tuning
  - Why needed here: The paper focuses on non-fine-tuned language models, so it's important to understand the difference between pre-training (learning general patterns) and fine-tuning (adapting to specific tasks).
  - Quick check question: Why might fine-tuned language models not be representative of the general capabilities of language models?

- Concept: Prompting and in-context learning
  - Why needed here: Many of the studies discussed in the paper use prompting or in-context learning to evaluate the models' abilities, so understanding these concepts is crucial for interpreting the results.
  - Quick check question: How does in-context learning differ from traditional fine-tuning, and what are its advantages and limitations?

## Architecture Onboarding

- Component map: Tokenization -> Embeddings -> Attention layers -> Feed-forward layers -> Output projection
- Critical path: Understand Transformer architecture → Grasp language modeling objective → Review behavioral studies by capability → Synthesize findings on scale and generalization
- Design tradeoffs: The main design tradeoff in language models is between model size (larger models can learn more complex patterns but are more expensive to train and use) and generalization ability (larger models may memorize more but not necessarily generalize better).
- Failure signatures: Common failure modes of language models include sensitivity to specific inputs, reliance on surface features, inability to handle negation or pragmatics, and generation of biased or toxic text. These failures can often be traced back to under-generalization or over-generalization of learned patterns.
- First 3 experiments:
  1. Evaluate a language model's performance on a simple syntactic task (e.g., subject-verb agreement) with different input variations to test its sensitivity to specific words and structures.
  2. Compare the performance of different-sized language models on a commonsense reasoning task to see how scaling affects their abilities.
  3. Analyze the output of a language model on an open-ended generation task to identify instances of over-generalization or under-generalization of learned patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model scale interact with linguistic generalization abilities beyond memorization?
- Basis in paper: [explicit] The paper notes that larger models tend to perform better on most tasks but also questions whether this is due to learning more robust patterns or memorizing more examples.
- Why unresolved: The paper observes that larger models still show sensitivity to specific inputs even at large scales, suggesting that memorization may play a significant role.
- What evidence would resolve it: Systematic studies comparing performance on novel examples that require robust generalization versus examples that can be solved through memorization patterns.

### Open Question 2
- Question: What are the mechanistic underpinnings of language models' sensitivity to specific inputs and surface features?
- Basis in paper: [inferred] The paper discusses behavioral analyses extensively but notes that linking these to mechanistic analyses of internal representations remains an open direction.
- Why unresolved: While the paper reviews behavioral studies extensively, it highlights that establishing causal links between internal mechanisms and behaviors is still an emerging area of research.
- What evidence would resolve it: Studies that directly manipulate internal representations or attention mechanisms and observe corresponding changes in behavioral outputs.

### Open Question 3
- Question: Can language models be trained to better handle negation and other logical constructs that they currently struggle with?
- Basis in paper: [explicit] The paper explicitly notes that language models struggle with negation and suggests this may be because the language modeling objective doesn't capture the features necessary to learn such patterns.
- Why unresolved: Despite various attempts at fine-tuning and prompting, language models still exhibit significant weaknesses in handling negation and logical reasoning.
- What evidence would resolve it: Experiments demonstrating successful training approaches that improve negation handling and logical reasoning without compromising other capabilities.

## Limitations

- The survey focuses exclusively on English-language studies, potentially missing important cross-linguistic patterns in model behavior.
- The selection process may have excluded relevant studies using different terminology or appearing in non-computer-science venues.
- The analysis relies on reported results rather than direct reproduction, making claims dependent on original studies' methodological rigor.

## Confidence

- **High confidence**: Core finding that language models exhibit basic linguistic capabilities while remaining sensitive to surface features - consistently demonstrated across hundreds of studies with varied methodologies.
- **Medium confidence**: Text pattern generalization framework as explanatory mechanism - provides useful intuition but may oversimplify complex interactions.
- **Medium confidence**: Scaling-related findings - limited number of studies evaluating models beyond 175B parameters makes it difficult to establish robust scaling laws for all capabilities.

## Next Checks

1. **Cross-linguistic validation**: Systematically test whether the reported sensitivities to surface features and over/under-generalization patterns hold when models are evaluated on non-English languages, particularly those with different morphological and syntactic structures.

2. **Direct reproduction study**: Select 10-15 representative studies from the survey and attempt direct reproduction of their key findings, focusing on the most commonly reported sensitivities and generalization failures to establish empirical grounding.

3. **Architecture ablation analysis**: Compare model behavior across different transformer variants (e.g., standard vs. sparse attention, encoder-only vs. decoder-only) on the same tasks to determine which architectural components contribute most to the observed sensitivities and generalization patterns.