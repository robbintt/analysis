---
ver: rpa2
title: 'BLUEX: A benchmark based on Brazilian Leading Universities Entrance eXams'
arxiv_id: '2307.05410'
source_url: https://arxiv.org/abs/2307.05410
tags:
- language
- bluex
- dataset
- questions
- portuguese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces BLUEX, a dataset of entrance exams from two
  leading Brazilian universities (UNICAMP and USP), to address the lack of high-quality
  Portuguese language datasets for evaluating language models. BLUEX includes annotated
  metadata and image positioning, enabling research on multimodal language understanding
  and reasoning.
---

# BLUEX: A benchmark based on Brazilian Leading Universities Entrance eXams

## Quick Facts
- arXiv ID: 2307.05410
- Source URL: https://arxiv.org/abs/2307.05410
- Reference count: 34
- Key outcome: BLUEX provides a Portuguese language multimodal benchmark for university entrance exams, with GPT-4 achieving 75.86% accuracy on text-only questions.

## Executive Summary
BLUEX introduces a novel benchmark dataset based on entrance exams from two leading Brazilian universities (UNICAMP and USP), addressing the critical shortage of high-quality Portuguese language datasets for evaluating language models. The dataset includes 1,095 multiple-choice questions with comprehensive annotations for subjects, required capabilities, and image positioning, enabling both text-only and multimodal evaluation. Experimental results show that current state-of-the-art models, including GPT-4, still fall short of the highest cutoff grades achieved by human students, demonstrating the benchmark's difficulty and value for advancing Portuguese language understanding and reasoning capabilities.

## Method Summary
The BLUEX benchmark was constructed by extracting questions from 2018-2023 entrance exams of UNICAMP and USP, manually annotating each question with subject categories (biology, chemistry, etc.), required capabilities (mathematical reasoning, Brazilian knowledge), and image positioning information. The dataset was evaluated using few-shot prompting (1 example per model) on the text-only subset (638 questions), with model predictions compared against ground truth answers. Performance was measured by accuracy and analyzed across different subjects and capability types to identify specific model strengths and weaknesses.

## Key Results
- GPT-4 achieved the highest accuracy at 75.86% on the text-only subset, still below the highest cutoff grades of 91.23% (UNICAMP) and 87.98% (USP)
- Models showed significant variation in performance across subjects, with physics questions generally being most challenging
- Mathematical reasoning questions consistently posed difficulties for all evaluated models regardless of their overall performance
- The benchmark successfully differentiates between model capabilities, with larger models generally outperforming smaller ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BLUEX provides high-quality Portuguese language data with multimodal annotations that current models cannot fully utilize.
- Mechanism: By including images and their positioning metadata alongside text-based questions, BLUEX creates a multimodal evaluation resource that tests both language understanding and visual reasoning capabilities.
- Core assumption: Models need to process both text and images together to solve complex academic questions.
- Evidence anchors:
  - [abstract]: "The dataset is also annotated to indicate the position of images in each question, providing a valuable resource for advancing the state-of-the-art in multimodal language understanding and reasoning."
  - [section]: "Many of the questions in the exams require a contextual or informational understanding of images."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.48, average citations=0.0. Top related titles: Evaluating GPT-4's Vision Capabilities on Brazilian University Admission Exams, Evaluating GPT-3.5 and GPT-4 Models on Brazilian University Admission Exams.
- Break condition: If multimodal models cannot access or process the image data, or if image understanding is not essential to solving the questions.

### Mechanism 2
- Claim: BLUEX includes recently administered exams unlikely to be in training data, enabling fair evaluation.
- Mechanism: By using exams from 2018-2023 that are not publicly available during typical LM training periods, BLUEX ensures models cannot simply memorize answers.
- Core assumption: Language models are primarily trained on data available before their release date.
- Evidence anchors:
  - [abstract]: "Furthermore, BLUEX includes a collection of recently administered entrance exams that are unlikely to be included in the training data of many currently popular LMs as of 2023."
  - [section]: "It is worth noting that the average and cutoff scores provided in Table 3 are computed taking into account the whole exam, while the scores obtained by the language models utilize only the subset of questions with no images."
  - [corpus]: Weak evidence - no direct citation to training data composition studies.
- Break condition: If models are trained on additional private or leaked exam data, or if the exam content overlaps significantly with public sources.

### Mechanism 3
- Claim: BLUEX's subject and capability annotations enable fine-grained performance analysis.
- Mechanism: By labeling questions with subjects (biology, chemistry, etc.) and required capabilities (mathematical reasoning, Brazilian knowledge), BLUEX allows researchers to identify specific weaknesses in language models.
- Core assumption: Fine-grained categorization helps identify model limitations beyond overall accuracy.
- Evidence anchors:
  - [section]: "The annotated metadata is described below... Subjects - A list of subjects related to the question... Mathematical Reasoning (MR) - Indicates whether the question requires mathematical reasoning..."
  - [section]: "Table 3 summarizes our experimental findings... We also conducted a more detailed analysis of the models' performance by examining their ability to handle specific question types."
  - [corpus]: Weak evidence - no direct citation to annotation framework studies.
- Break condition: If the annotations are inconsistent or if the categories do not capture meaningful distinctions in model performance.

## Foundational Learning

- Concept: Natural Language Processing (NLP)
  - Why needed here: Understanding how language models process text and the limitations of current Portuguese NLP resources.
  - Quick check question: What are the main challenges in developing NLP models for languages with limited high-quality datasets?

- Concept: Multimodal Learning
  - Why needed here: BLUEX combines text and images, requiring understanding of how models integrate different data types.
  - Quick check question: How do current multimodal models process and integrate visual and textual information?

- Concept: Educational Assessment Design
  - Why needed here: Understanding the structure and purpose of standardized entrance exams helps interpret BLUEX's question design and difficulty.
  - Quick check question: What are the key characteristics of high-stakes academic entrance exams that make them challenging for language models?

## Architecture Onboarding

- Component map: Data extraction -> Manual annotation -> Model evaluation -> Performance analysis
- Critical path: Question extraction → Manual annotation (subjects, capabilities, image positioning) → Few-shot model evaluation → Accuracy computation and subject/capability analysis
- Design tradeoffs: Text-only evaluation vs. multimodal evaluation; broad subject coverage vs. depth in specific areas; recent exam inclusion vs. historical trend analysis
- Failure signatures: Poor extraction accuracy leading to incorrect annotations; inconsistent metadata tagging; evaluation results not correlating with actual student performance
- First 3 experiments:
  1. Evaluate a baseline language model on text-only subset to establish performance floor.
  2. Test the same model with few-shot examples to measure in-context learning capability.
  3. Analyze model performance by subject and capability annotations to identify specific weaknesses.

## Open Questions the Paper Calls Out

1. Does increasing the number of few-shot examples in the prompt improve the performance of language models on the BLUEX dataset?
2. How would the use of chain-of-thought prompting affect the performance of language models on the BLUEX dataset?
3. How well do multimodal models perform on the BLUEX dataset, especially for questions that require understanding images?

## Limitations

- Current evaluation focuses only on text-only questions (638 of 1,095 total), limiting assessment of full multimodal capabilities
- Dataset coverage limited to two Brazilian universities, potentially not representing full diversity of Portuguese academic assessment
- Manual annotation process introduces potential human bias and inconsistency across questions

## Confidence

**High Confidence**: BLUEX provides the first high-quality Portuguese language benchmark for university entrance exams, with reliable experimental methodology and controlled few-shot evaluation.

**Medium Confidence**: The claim that BLUEX questions are unlikely to be in training data is plausible but unverified without access to individual model training corpora.

**Low Confidence**: The assertion about BLUEX's ability to advance multimodal language understanding is currently limited by text-only evaluation, making this a future rather than current capability.

## Next Checks

1. Evaluate the full 1,095 questions including image-based questions using multimodal models to validate the dataset's multimodal assessment capability.

2. Test BLUEX-trained or BLUEX-evaluated models on entrance exams from additional Brazilian universities or Portuguese-speaking countries to assess generalizability beyond UNICAMP and USP.

3. Conduct an inter-annotator agreement study on a random sample of 100 questions to quantify the reliability of subject and capability annotations.