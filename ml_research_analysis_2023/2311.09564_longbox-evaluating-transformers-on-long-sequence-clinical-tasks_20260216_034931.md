---
ver: rpa2
title: 'LongBoX: Evaluating Transformers on Long-Sequence Clinical Tasks'
arxiv_id: '2311.09564'
source_url: https://arxiv.org/abs/2311.09564
tags:
- long
- clinical
- datasets
- medical
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LongBoX, a benchmark of seven clinical datasets
  designed to evaluate large language models on long sequences like complete electronic
  health records. The authors show that existing models struggle on these tasks, achieving
  an average score of about 52%.
---

# LongBoX: Evaluating Transformers on Long-Sequence Clinical Tasks

## Quick Facts
- arXiv ID: 2311.09564
- Source URL: https://arxiv.org/abs/2311.09564
- Reference count: 7
- Models struggle on long clinical sequences, averaging ~52% performance

## Executive Summary
This paper introduces LongBoX, a benchmark of seven clinical datasets designed to evaluate large language models on long sequences like complete electronic health records. The authors show that existing models struggle on these tasks, achieving an average score of about 52%. They also test two long-sequence techniques—local-global attention and Fusion-in-Decoder—which give mixed results, improving some datasets but still leaving much room for improvement. The work highlights the need for better long-sequence handling in medical NLP and provides a resource to drive future research in this direction.

## Method Summary
The study evaluates eight general and medical domain LLMs (FLAN-T5, InBoXBART, SciFive, Clinical-T5, GPT-Neo, BioGPT, BioMedLM, LLaMA-2) on seven clinical datasets from n2c2, with input texts typically containing thousands of words. Models are assessed using accuracy and F1-score metrics. The authors also implement two long sequence handling techniques—LongT5 and Fusion-in-Decoder (FiD)—to compare against baseline performance. The datasets include discharge summaries, progress reports, and longitudinal records, converted into text-to-text format for model evaluation.

## Key Results
- Existing models achieve only ~52% average performance on LongBoX clinical tasks
- Long sequence techniques (LongT5, FiD) show mixed results—improving some datasets but not all
- Clinical tokenizers (e.g., GatorTron) generate shorter token lengths than general tokenizers for the same clinical text
- Larger models (e.g., LLaMA-2 7B) handle longer texts better than smaller models (<2.7B)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Long sequence handling techniques improve performance on some clinical datasets compared to baseline models.
- Mechanism: By increasing maximum input token length beyond 1024, these techniques allow models to process more clinical context needed for accurate predictions.
- Core assumption: Additional context contains relevant information for the tasks.
- Evidence anchors:
  - "Our results demonstrate mixed results with long-sequence handling - while scores on some datasets increase, there is substantial room for improvement."
  - "These methods achieve mixed results on LONG BOX, further highlighting the need for our benchmark."
- Break condition: If additional context contains mostly irrelevant information or noise, performance may not improve or could degrade.

### Mechanism 2
- Claim: Clinical tokenizers generate shorter token lengths compared to general domain tokenizers for the same clinical text.
- Mechanism: Clinical tokenizers are tailored to the medical domain and can better segment medical terms and phrases, resulting in fewer tokens for the same text.
- Core assumption: Medical terms and phrases can be more efficiently tokenized using domain-specific knowledge.
- Evidence anchors:
  - "From Figure 2, it is evident that the clinical tokenizer generates shorter token lengths compared to the general domain tokenizer."
  - "Notably, the difference between the average token lengths for clinicalvs. general tokenizers becomes larger as the input length increases."
- Break condition: If the clinical tokenizer fails to properly handle rare or domain-specific terms, the token length reduction may not be significant.

### Mechanism 3
- Claim: As model size increases, the capability in handling longer texts improves.
- Mechanism: Larger models have more parameters and capacity to process and attend to longer sequences effectively.
- Core assumption: Model capacity is a limiting factor in handling long sequences, and increasing it leads to better performance.
- Evidence anchors:
  - "We observe that as the model size increases, its capability in handling longer texts improves."
- Break condition: If increased model size leads to overfitting or computational inefficiencies that outweigh the benefits of longer sequence processing.

## Foundational Learning

- Concept: Transformer architecture and self-attention mechanism
  - Why needed here: Understanding how transformers process sequences and the limitations of standard self-attention in handling long sequences.
  - Quick check question: How does the standard self-attention mechanism in transformers scale with sequence length, and what are its computational implications?

- Concept: Tokenization and its impact on model performance
  - Why needed here: Different tokenizers can significantly affect the input length and, consequently, the model's ability to process long sequences effectively.
  - Quick check question: How do clinical tokenizers differ from general domain tokenizers, and why is this difference important for long clinical text processing?

- Concept: Benchmarking and evaluation metrics
  - Why needed here: Proper evaluation of model performance on long sequence tasks requires understanding the appropriate metrics and how to interpret them.
  - Quick check question: What are the key evaluation metrics used in this paper, and how do they reflect the model's ability to handle long clinical sequences?

## Architecture Onboarding

- Component map:
  Datasets (7 clinical datasets) -> Tokenizers (Clinical/GatorTron, General/RoBERTa) -> Models (8 LLMs + 2 long sequence techniques) -> Evaluation (Accuracy, F1-score)

- Critical path:
  1. Tokenize input text using appropriate tokenizer
  2. Process input through the model (baseline or long sequence technique)
  3. Generate predictions
  4. Evaluate performance using relevant metrics

- Design tradeoffs:
  - Model size vs. computational efficiency
  - Input length vs. model capacity and performance
  - Domain-specific vs. general tokenization

- Failure signatures:
  - Poor performance on datasets with longer input lengths
  - Inconsistent results across different datasets
  - Degradation in performance with increased input length

- First 3 experiments:
  1. Compare baseline model performance on a dataset with short vs. long input lengths.
  2. Evaluate the impact of clinical vs. general tokenization on input length and model performance.
  3. Assess the performance improvement of long sequence techniques (LongT5, FiD) compared to baseline models on a dataset with long input lengths.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can transformer models be improved to better handle long clinical sequences beyond simple attention mechanisms and chunking strategies?
- Basis in paper: The authors note that existing models struggle on long clinical sequences and that long-sequence techniques show mixed results, indicating room for improvement.
- Why unresolved: While the paper benchmarks current approaches, it doesn't propose or test novel architectural solutions tailored to clinical document structure and longitudinality.
- What evidence would resolve it: Development and benchmarking of new transformer architectures or training methods specifically designed for clinical long-sequence tasks that outperform current techniques on LongBoX.

### Open Question 2
- Question: What is the impact of clinical-specific tokenization (e.g., GatorTron) on long-sequence handling compared to general domain tokenization, and how can tokenization be optimized for clinical texts?
- Basis in paper: The authors compare GatorTron and RoBERTa, showing that clinical tokenizers generate shorter token lengths, especially for longer inputs, but differences are sometimes small.
- Why unresolved: The paper provides preliminary analysis but does not explore how tokenization strategies could be further optimized or whether they significantly impact model performance on long clinical sequences.
- What evidence would resolve it: Detailed experiments comparing different tokenization strategies on model performance across LongBoX datasets.

### Open Question 3
- Question: How can transformer models be trained to better understand and leverage the structure of clinical documents (e.g., EHR organization, longitudinal data) for improved long-sequence task performance?
- Basis in paper: The authors observe that models lack awareness of EHR document structure and the ability to deal with longitudinal records, which limits performance even with increased context length.
- Why unresolved: While the paper identifies this as a limitation, it does not propose or test methods for incorporating document structure or temporal reasoning into transformer models.
- What evidence would resolve it: Development and evaluation of models that incorporate explicit document structure or temporal reasoning mechanisms, showing improved performance on LongBoX tasks requiring such understanding.

## Limitations

- Models achieve only ~52% average performance, indicating significant limitations in handling long clinical sequences
- Long sequence handling techniques show inconsistent improvements across different datasets
- Computational expense of processing longer sequences may limit practical clinical deployment
- Tokenization differences between clinical and general tokenizers confound performance comparisons between model types

## Confidence

**High Confidence:**
- The fundamental challenge of processing long clinical sequences remains unsolved (52% average performance)
- Clinical tokenizers generate shorter token sequences compared to general domain tokenizers

**Medium Confidence:**
- Mixed results from long-sequence handling techniques indicate promise but require refinement
- Larger models demonstrate improved capability in handling longer texts compared to smaller models

**Low Confidence:**
- Specific impact of tokenization differences on model performance comparisons requires more rigorous isolation
- Generalizability of results to other clinical domains or languages beyond English medical text remains uncertain

## Next Checks

1. **Tokenization Isolation Test**: Design an experiment that controls for tokenization differences by using the same tokenizer across all models while maintaining consistent input lengths. This would isolate the impact of model architecture from tokenization effects on performance comparisons.

2. **Context Relevance Analysis**: Perform a systematic analysis of which portions of long clinical documents contain task-relevant information versus noise. This would involve ablation studies where different segments of long inputs are systematically removed to determine the minimum context required for optimal performance.

3. **Cross-Domain Generalization**: Evaluate the best-performing models and techniques from this benchmark on non-English clinical datasets or different medical specialties not represented in the current seven datasets. This would test the robustness and generalizability of the findings beyond the specific clinical domains studied.