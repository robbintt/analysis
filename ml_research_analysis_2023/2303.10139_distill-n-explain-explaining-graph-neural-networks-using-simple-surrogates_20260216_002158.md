---
ver: rpa2
title: 'Distill n'' Explain: explaining graph neural networks using simple surrogates'
arxiv_id: '2303.10139'
source_url: https://arxiv.org/abs/2303.10139
tags:
- graph
- fastdnx
- neural
- networks
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DnX explains GNNs by distilling them into a simpler surrogate and
  solving a convex program on the surrogate. This avoids backpropagation through the
  original GNN.
---

# Distill n' Explain: explaining graph neural networks using simple surrogates

## Quick Facts
- arXiv ID: 2303.10139
- Source URL: https://arxiv.org/abs/2303.10139
- Authors: 
- Reference count: 40
- Key outcome: DnX explains GNNs by distilling them into a simpler surrogate and solving a convex program on the surrogate. This avoids backpropagation through the original GNN. DnX and its fast variant FastDnX outperform state-of-the-art explainers on 8 benchmarks, with speedups up to 65K× over GNNExplainer. Theoretical results link distillation error to explanation faithfulness.

## Executive Summary
This paper introduces Distill n' Explain (DnX), a method for explaining graph neural network (GNN) predictions by distilling the complex GNN into a simple surrogate model (SGC) and extracting explanations via convex optimization. The key innovation is avoiding backpropagation through the original GNN by using knowledge distillation to train a simpler model that can be exactly analyzed. DnX and its faster variant FastDnX demonstrate state-of-the-art performance on 8 benchmark datasets, achieving speedups up to 65K× over existing methods while maintaining explanation quality.

## Method Summary
DnX explains GNN predictions by first distilling the target GNN into a simple linear GNN (SGC) using knowledge distillation with KL divergence minimization. The SGC surrogate captures the essential decision boundaries while being computationally simpler. For node-level explanations, DnX solves a convex program on the surrogate model to find node importance scores, while FastDnX uses direct linear decomposition of predictions for even faster computation. The approach leverages the linear nature of SGC to enable exact explanation extraction without backpropagation through the original GNN.

## Key Results
- DnX and FastDnX outperform state-of-the-art explainers on 8 benchmarks (6 synthetic, 2 real-world)
- Speedups up to 65K× over GNNExplainer while maintaining competitive explanation quality
- Theoretical bounds link distillation error to explanation faithfulness
- DnX is model-agnostic and works across multiple GNN architectures (GCN, GIN, GATED, ARMA)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DnX explains GNNs by distilling them into a simpler surrogate model and solving a convex program on the surrogate, avoiding backpropagation through the original GNN.
- Mechanism: The knowledge distillation step trains a simple GNN (SGC) to approximate the predictions of the complex GNN Φ. This surrogate Ψ captures the essential decision boundaries while being computationally simpler. The explanation extraction then solves a convex program on Ψ to find node importance scores.
- Core assumption: A simple linear surrogate can approximate the complex GNN's predictions well enough that explanations derived from it are faithful to the original model.
- Evidence anchors:
  - [abstract] "DnX uses knowledge distillation to learn a simple GNNΨ that mimics the behavior of the GNN Φ we want to explain."
  - [section 3.1] "The distillation process consists of adjusting the parameters of ΨΘ to match its predictions to those of the network Φ."
  - [corpus] Weak evidence - the corpus mentions related works on GNN distillation but doesn't directly support the fidelity claim.
- Break condition: If the distillation error is too large, the surrogate's explanations will not be faithful to the original GNN's predictions.

### Mechanism 2
- Claim: The linear nature of the SGC surrogate enables fast and exact explanation extraction through simple decomposition or convex optimization.
- Mechanism: Because SGC is linear (no activation functions), predictions can be decomposed into additive terms for each node. This allows two approaches: solving a convex program (DnX) or directly computing node contributions through scalar projection (FastDnX).
- Core assumption: The linear structure of SGC preserves enough information to enable meaningful explanation extraction while maintaining computational efficiency.
- Evidence anchors:
  - [abstract] "DnX extracts node or edge-level explanations by solving a simple convex program. We also propose FastDnX, a faster version of DnX that leverages the linear decomposition of our surrogate model."
  - [section 3.2] "Let Zi denote the logit vector... Due to the linear nature of Ψ, we can decompose Zi into a sum of n terms, one for each node in V."
  - [corpus] No direct support in corpus - related works focus on GNN distillation but not on the specific linear decomposition approach.
- Break condition: If the GNN being explained is highly non-linear or uses complex aggregation mechanisms that cannot be captured by linear models, the explanations may be incomplete.

### Mechanism 3
- Claim: Theoretical bounds link the quality of the surrogate model (distillation error) to the faithfulness of explanations, providing justification for the approach.
- Mechanism: Theorem 1 establishes that the unfaithfulness of explanations with respect to Φ is bounded by the unfaithfulness with respect to Ψ plus twice the distillation error. This formalizes the intuition that better distillation leads to better explanations.
- Core assumption: The distillation error can be controlled and measured, providing a reliable indicator of explanation quality.
- Evidence anchors:
  - [abstract] "Additionally, we support our empirical findings with theoretical results linking the quality of the surrogate model (i.e., distillation error) to the faithfulness of explanations."
  - [section 4] "Theorem 1 (Unfaithfulness with respect to Φ). Under the same assumptions of Lemma 1 and assuming the L2 distillation error is bounded by α..."
  - [corpus] No direct support - the corpus mentions related works on GNN explanation but doesn't discuss theoretical bounds on explanation faithfulness.
- Break condition: If the assumptions in the theorems (e.g., bounded perturbations, specific noise distributions) are violated in practice, the theoretical bounds may not hold.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and message passing
  - Why needed here: Understanding how GNNs aggregate information from neighbors is crucial for grasping why explanations are important and how they work.
  - Quick check question: What is the key difference between GCN and SGC in terms of their mathematical formulation?

- Concept: Knowledge Distillation
  - Why needed here: The entire approach hinges on using a simpler model to approximate a complex one, so understanding distillation is essential.
  - Quick check question: Why is it advantageous to distill into an SGC rather than another complex GNN architecture?

- Concept: Convex Optimization
  - Why needed here: The explanation extraction involves solving a convex program, so familiarity with convex optimization is necessary.
  - Quick check question: Why is the optimization problem in DnX convex, and what property of SGC ensures this?

## Architecture Onboarding

- Component map: Input graph → Knowledge distillation (Φ → Ψ) → Explanation extraction (Ψ → E) → Output explanations
- Critical path: The distillation step must complete successfully before any explanations can be extracted. The extraction step must be fast enough for practical use.
- Design tradeoffs: SGC provides speed and simplicity but may lose expressiveness compared to the original GNN. The convex program provides exact solutions but may be slower than the linear decomposition approach.
- Failure signatures: High distillation error indicates the surrogate is not capturing the original model well. Slow explanation extraction suggests the convex program is too computationally expensive.
- First 3 experiments:
  1. Run distillation on a simple dataset (BA-House) and verify accuracy > 85% as reported in Table 3
  2. Compare explanation extraction time between DnX and FastDnX on a medium-sized graph
  3. Test explanation faithfulness by measuring how well explanations from Ψ predict the original GNN's behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the faithfulness of explanations change when using more powerful surrogate models than SGC, especially for graph-level prediction tasks?
- Basis in paper: [inferred] The paper notes that while simple models like SGC work well for node-level classification, they may not be sufficient for graph-level prediction tasks. The authors suggest that more powerful linear GNNs or those with different diffusion operators could be used to extend DnX and FastDnX.
- Why unresolved: The paper focuses on node-level explanations and does not explore the use of more complex surrogate models for graph-level tasks. The performance of DnX and FastDnX on graph-level prediction tasks remains untested.
- What evidence would resolve it: Experiments comparing the faithfulness of explanations generated by DnX and FastDnX using various surrogate models (including more powerful ones) on both node-level and graph-level prediction tasks.

### Open Question 2
- Question: How sensitive are the explanations generated by DnX and FastDnX to the choice of hyperparameters, such as the number of layers in the surrogate model and the learning rate during distillation?
- Basis in paper: [explicit] The paper mentions that hyperparameters like the number of layers in the SGC surrogate and the learning rate during distillation are chosen but does not discuss the impact of these choices on the quality of explanations.
- Why unresolved: The paper does not provide a sensitivity analysis of the explanations to hyperparameter choices. The robustness of DnX and FastDnX to hyperparameter variations is unclear.
- What evidence would resolve it: A systematic study of the impact of hyperparameter choices on the quality of explanations, including metrics like accuracy, fidelity, and faithfulness, across various datasets and models.

### Open Question 3
- Question: How do the explanations generated by DnX and FastDnX compare to those generated by other methods when evaluated on datasets with more complex ground-truth explanations, such as the ones proposed by Faber et al. (2021)?
- Basis in paper: [explicit] The paper discusses the limitations of current benchmarks and mentions that Faber et al. (2021) have proposed alternative datasets with more complex ground-truth explanations. The authors also report results for two of these datasets using FastDnX.
- Why unresolved: While the paper provides some results for Faber et al.'s datasets, it does not compare the performance of DnX and FastDnX to other explanation methods on these more complex benchmarks.
- What evidence would resolve it: A comprehensive comparison of DnX and FastDnX to other state-of-the-art explanation methods on Faber et al.'s datasets and other benchmarks with complex ground-truth explanations, using metrics like accuracy, fidelity, and faithfulness.

## Limitations

- The theoretical analysis relies on strong assumptions about bounded perturbations and specific noise distributions that may not hold in practice
- The focus on synthetic datasets with known ground-truth explanations limits understanding of real-world performance
- The claim that DnX generalizes across multiple GNN architectures is supported empirically but lacks theoretical justification

## Confidence

- **High**: The mechanism of distilling to SGC and extracting explanations via linear decomposition is clearly specified and empirically validated
- **Medium**: The theoretical bounds connecting distillation error to explanation faithfulness are mathematically sound but depend on assumptions that require further validation
- **Medium**: The speed improvements over baselines are well-documented, but the absolute performance relative to human-interpretable explanations is not addressed

## Next Checks

1. Test DnX on datasets with more complex ground-truth patterns (beyond the 6 synthetic patterns used) to assess generalization to unseen structures
2. Conduct ablation studies removing the distillation step to quantify how much explanation quality depends on having an accurate surrogate versus using the original GNN directly
3. Measure the sensitivity of explanations to the choice of SGC hyperparameters (number of hops K, feature dimensions) across different GNN architectures