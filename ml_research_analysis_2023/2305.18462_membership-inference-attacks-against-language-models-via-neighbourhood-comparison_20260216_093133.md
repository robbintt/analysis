---
ver: rpa2
title: Membership Inference Attacks against Language Models via Neighbourhood Comparison
arxiv_id: '2305.18462'
source_url: https://arxiv.org/abs/2305.18462
tags:
- attacks
- data
- reference
- attack
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses membership inference attacks (MIAs) against
  language models, which aim to determine whether a given data sample was used to
  train a model. While existing attacks like Likelihood Ratio Attacks (LiRAs) rely
  on reference models trained on similar data, this assumption is often unrealistic
  in privacy-sensitive domains.
---

# Membership Inference Attacks against Language Models via Neighbourhood Comparison

## Quick Facts
- arXiv ID: 2305.18462
- Source URL: https://arxiv.org/abs/2305.18462
- Reference count: 18
- The paper proposes neighborhood attacks that eliminate the need for reference models by comparing loss of target samples to losses of synthetically generated neighbors.

## Executive Summary
This paper addresses the challenge of membership inference attacks (MIAs) against language models, which aim to determine whether a given data sample was used to train the model. The authors propose a novel neighborhood attack approach that eliminates the need for reference models by generating semantically similar neighbors through word replacements using masked language models. This approach compares the loss of target samples to their neighbors, providing a reference-free alternative to existing attacks. Experiments demonstrate that neighborhood attacks outperform existing reference-free attacks and are competitive with reference-based attacks that have perfect knowledge of the training data distribution, while also showing effectiveness against DP-SGD defenses.

## Method Summary
The paper proposes neighborhood attacks that generate semantically similar neighbors through word replacements using masked language models like BERT. These neighbors are created by masking individual words and predicting replacements, with embedding layer dropout preventing repetition. The attack compares the loss of a target sample under the language model to the average loss of its neighbors, using the difference as a membership indicator. If the difference is below a threshold γ, the sample is classified as a training member. This approach eliminates the need for reference models by using the neighbors themselves as a proxy for difficulty calibration, making the attack more practical in privacy-sensitive domains where reference data is unavailable.

## Key Results
- Neighborhood attacks outperform existing reference-free attacks (LOSS, SC+ and SC-) on news, Twitter, and Wikipedia datasets
- Neighborhood attacks achieve competitive performance with reference-based attacks that have perfect knowledge of training data distribution
- DP-SGD with ε=8 provides effective defense against neighborhood attacks, reducing performance to near-random guessing at low FPR values

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The neighborhood attack improves membership inference by comparing the loss of a target sample to losses of semantically similar neighbors.
- Mechanism: The attack generates neighbors via word replacements using a masked language model, then computes the difference between the target's loss and the average neighbor loss. If the difference is below a threshold value γ, the sample is classified as a training member.
- Core assumption: Neighbors generated by minimal word replacements are equally likely under any plausible textual distribution and serve as a good proxy for the target's complexity without overfitting.
- Evidence anchors:
  - [abstract] "neighbourhood attacks, which compare model scores for a given sample to scores of synthetically generated neighbour texts and therefore eliminate the need for access to the training data distribution."
  - [section 2.1] "the difference should be below our threshold value γ"
- Break condition: If the word replacement model introduces semantic drift, neighbors no longer represent equivalent complexity and the loss comparison becomes unreliable.

### Mechanism 2
- Claim: Reference-based attacks are fragile because their success depends on the reference model's validation perplexity matching that of the target model.
- Mechanism: The reference model must approximate the target's training data distribution closely; otherwise, the likelihood ratio used for difficulty calibration is misleading.
- Core assumption: Reference models trained on slightly different datasets from the target's will have mismatched perplexities, leading to poor attack performance.
- Evidence anchors:
  - [section 3.2] "we simulate real-world reference-based attacks by training reference models on external datasets from the same domain as the target model's training data."
  - [section 4.2] "attack performance of Likelihood Ratio Attacks w.r.t to the reference models' validation perplexity"
- Break condition: If the attacker can access a reference model with perplexity very close to the target, the attack performance can match that of neighborhood attacks.

### Mechanism 3
- Claim: DP-SGD effectively defends against neighborhood attacks by limiting how much a single training sample influences the model.
- Mechanism: Differential privacy bounds the influence of individual samples, reducing the gap between training and non-training samples' losses, which undermines the attack's core signal.
- Core assumption: Even with neighborhood loss comparison, a DP-trained model's loss distribution is sufficiently flattened to make membership inference unreliable.
- Evidence anchors:
  - [section 5] "the performance of the neighbourhood attack is substantially worse compared to the non-private model and is almost akin to random guessing for low FPR values."
  - [section 3.3] "DP-SGD ... has been shown to successfully protect models against membership inference attacks"
- Break condition: If the privacy budget (epsilon) is too large, the DP mechanism becomes ineffective and the attack performance approaches that of the non-private model.

## Foundational Learning

- Concept: Masked language model (MLM) for generating word replacements
  - Why needed here: The attack uses BERT-style MLMs to produce semantically similar neighbors by masking and predicting alternative tokens.
  - Quick check question: What role does the dropout on the embedding layer play in the replacement generation process?

- Concept: Loss-based membership inference and thresholding
  - Why needed here: Both baseline and neighborhood attacks rely on comparing loss values to a threshold to infer membership.
  - Quick check question: How does the neighborhood attack's decision rule differ from the standard LOSS attack in terms of calibration?

- Concept: Perplexity as a proxy for model performance and data distribution match
  - Why needed here: The paper compares reference models' validation perplexities to the target's to explain attack fragility.
  - Quick check question: Why does a reference model with perplexity far from the target's lead to poor likelihood ratio attack performance?

## Architecture Onboarding

- Component map: Target model (GPT-2) -> Loss computation -> Neighbor generation (BERT MLM with dropout) -> Loss comparison -> Thresholding -> Membership decision
- Critical path: Generate neighbors -> Compute losses under target model -> Calculate difference -> Compare to threshold
- Design tradeoffs: More neighbors -> more robust comparison but higher computation; fewer replacements -> better semantic preservation but smaller loss differences
- Failure signatures: Attack performance drops if neighbors are semantically drifted or if target model's overfitting is minimal
- First 3 experiments:
  1. Implement baseline LOSS attack on synthetic dataset and verify high false positive rate
  2. Generate neighbors with 1 word replacement using BERT MLM and compute average neighbor loss
  3. Vary number of neighbors (5, 10, 25) and measure impact on true positive rate at fixed false positive rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of neighborhood attacks vary when applied to different types of textual data augmentations beyond word replacements?
- Basis in paper: [explicit] The paper discusses using word replacements through masked language models to generate neighboring samples, but does not explore other types of augmentations.
- Why unresolved: The paper focuses on word replacements and does not provide comparative analysis with other augmentation methods like paraphrasing or sentence-level transformations.
- What evidence would resolve it: Experimental results comparing neighborhood attacks using various textual augmentations (e.g., paraphrasing, back-translation, sentence shuffling) on the same datasets.

### Open Question 2
- Question: Can neighborhood attacks be effectively adapted for membership inference in non-textual data modalities such as images or audio?
- Basis in paper: [inferred] The paper mentions that neighborhood attacks are specific to textual data and suggests future work could explore extensions to other modalities, but does not provide implementation details or results.
- Why unresolved: The paper does not provide a framework or experiments for applying neighborhood attacks to non-textual data.
- What evidence would resolve it: Successful implementation and evaluation of neighborhood attacks on image or audio datasets, demonstrating competitive performance with existing attacks in those domains.

### Open Question 3
- Question: What is the impact of the quality of the masked language model (MLM) on the effectiveness of neighborhood attacks?
- Basis in paper: [explicit] The paper uses BERT as the MLM for generating neighbors but does not explore how variations in MLM quality (e.g., using different models or fine-tuning) affect attack performance.
- Why unresolved: The paper does not provide experiments varying the MLM used for generating neighbors.
- What evidence would resolve it: Comparative experiments using different MLMs (e.g., RoBERTa, GPT-2) or fine-tuned versions to generate neighbors, measuring the impact on attack success rates.

### Open Question 4
- Question: How does the computational cost of neighborhood attacks compare to reference-based attacks, especially as dataset sizes increase?
- Basis in paper: [inferred] The paper does not discuss computational efficiency or scalability of neighborhood attacks relative to reference-based methods.
- Why unresolved: The paper focuses on attack effectiveness but does not provide analysis of computational resources required.
- What evidence would resolve it: Empirical measurements of computational time and memory usage for both neighborhood and reference-based attacks across datasets of varying sizes.

## Limitations
- The attack's effectiveness is sensitive to the quality and semantic preservation of generated neighbors
- Defense evaluation only considers a single privacy budget (ε=8) in DP-SGD, leaving uncertainty about attack performance at different privacy levels
- The paper assumes that reference model access is limited in privacy-sensitive domains, but doesn't fully explore scenarios where attackers might overcome this constraint

## Confidence
- **High confidence**: The neighborhood attack's mechanism is well-founded and experimental results showing superior performance to reference-free baselines are robust across datasets. The demonstration that DP-SGD provides effective defense is also well-supported.
- **Medium confidence**: The claim that reference-based attacks are fundamentally fragile due to perplexity mismatches is supported by experiments, but the paper doesn't fully explore whether attackers could overcome this through adaptive strategies or multiple reference model trials.
- **Medium confidence**: The assertion that neighborhood attacks "outperform" reference-based attacks requires careful interpretation - the paper shows competitive performance but the comparison depends heavily on the reference model quality assumptions.

## Next Checks
1. Test the neighborhood attack's sensitivity to different MLM architectures (beyond BERT) and word replacement strategies to establish robustness to implementation choices.
2. Evaluate attack performance across a wider range of privacy budgets (ε values) in DP-SGD defense to understand the scaling relationship between privacy guarantees and attack effectiveness.
3. Implement an adaptive attacker scenario where multiple reference models are trained and selected based on validation perplexity matching to determine if reference-based attacks can overcome the fragility demonstrated in the paper.