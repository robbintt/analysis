---
ver: rpa2
title: Leveraging Large Language Models for Exploiting ASR Uncertainty
arxiv_id: '2309.04842'
source_url: https://arxiv.org/abs/2309.04842
tags:
- task
- n-best
- speech
- best
- hypothesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using n-best ASR hypotheses as prompts for
  LLMs to tackle speech-intent classification tasks. The authors hypothesize that
  LLMs can better exploit ASR uncertainty by processing n-best lists instead of 1-best
  outputs.
---

# Leveraging Large Language Models for Exploiting ASR Uncertainty

## Quick Facts
- arXiv ID: 2309.04842
- Source URL: https://arxiv.org/abs/2309.04842
- Reference count: 0
- Using n-best ASR hypotheses with LLMs improves speech-intent classification accuracy

## Executive Summary
This paper proposes using n-best ASR hypotheses as prompts for LLMs to tackle speech-intent classification tasks. The authors hypothesize that LLMs can better exploit ASR uncertainty by processing n-best lists instead of 1-best outputs. They design descriptive prompts explaining n-best lists to the LLM and also explore LoRA finetuning on downstream tasks. Experiments on device-directed speech detection and keyword spotting tasks show that using n-best lists outperforms using 1-best ASR hypothesis, with significant improvements in accuracy and FPR reduction when using n=16.

## Method Summary
The approach involves generating n-best ASR hypotheses with associated costs (acoustic + language model scores), formatting them into structured prompts with task instructions, and feeding them to an LLM (Vicuna-7B). The LLM processes the multiple hypotheses and outputs intent classifications. LoRA finetuning is applied to adapt the LLM parameters for specific downstream tasks. The method is evaluated on device-directed speech detection and keyword spotting tasks, comparing performance against baseline 1-best approaches.

## Key Results
- Using n-best lists significantly improves accuracy over 1-best hypothesis on both DDSD and KS tasks
- Performance increases with larger n values (n=16 shows optimal results) before diminishing returns
- Hypothesis costs provide measurable benefits when included in prompts
- LoRA finetuning further improves performance across different prompt structures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM can exploit ASR uncertainty by processing multiple hypotheses simultaneously
- Mechanism: N-best lists provide multiple competing interpretations of the same utterance, allowing the LLM to identify patterns across hypotheses and select the most probable interpretation based on contextual understanding
- Core assumption: LLMs can effectively reason over structured uncertainty information when properly prompted
- Evidence anchors:
  - [abstract] "Our hypothesis is that using n-best lists instead of 1-best enables the LLM to benefit from the uncertainties in ASR prediction instead of being adversely affected by them"
  - [section] "We found that using n-best lists in- stead of 1-best enables the LLM to benefit from the uncertainties in ASR prediction"
  - [corpus] Weak evidence - no direct corpus neighbors discussing LLM exploitation of ASR uncertainty

### Mechanism 2
- Claim: Hypothesis costs provide additional signal for uncertainty calibration
- Mechanism: ASR hypothesis costs (acoustic + language model scores) are appended to each hypothesis, giving the LLM quantitative confidence information that complements the qualitative content
- Core assumption: LLMs can interpret and utilize numerical confidence scores when provided in structured format
- Evidence anchors:
  - [abstract] "n-best lists are augmented with ASR hypothesis costs as an additional source of information on uncertainty in ASR"
  - [section] "Each hypothesis is appended with a hypothesis-cost at the end in the format[cost]. This cost is the sum of the acoustic-model and language-model costs"
  - [corpus] Weak evidence - no direct corpus neighbors discussing cost-augmented n-best lists

### Mechanism 3
- Claim: LoRA finetuning enables task-specific adaptation while preserving general capabilities
- Mechanism: Low-Rank Adaptation modifies a small subset of parameters (4.1M out of 7B) to specialize the LLM for speech-intent classification without requiring full model retraining
- Core assumption: Task-specific finetuning on n-best list prompts can teach the LLM to better utilize ASR uncertainty for downstream tasks
- Evidence anchors:
  - [section] "We train parameters of LoRA adapters [1] for 3 epochs using 8 GPUs with a learning rate of 2e-5"
  - [section] "We demonstrate significant improvements on both the tasks using n-best lists with LoRA finetuning"
  - [corpus] Weak evidence - no direct corpus neighbors discussing LoRA finetuning for ASR+LLM systems

## Foundational Learning

- Concept: N-best lists in ASR systems
  - Why needed here: Understanding how ASR systems generate multiple hypotheses and how they differ from 1-best outputs
  - Quick check question: What information does an n-best list provide that a 1-best hypothesis does not?

- Concept: ASR hypothesis scoring
  - Why needed here: Understanding how acoustic and language model costs are calculated and what they represent
  - Quick check question: What does a lower hypothesis cost indicate about the ASR system's confidence?

- Concept: LLM prompting techniques
  - Why needed here: Understanding how to structure prompts to effectively communicate task requirements and data formats to LLMs
  - Quick check question: How does the structure of task-prompts influence the LLM's ability to understand and execute the speech-intent classification task?

## Architecture Onboarding

- Component map:
  ASR system → generates n-best hypotheses with costs → Prompt engineering layer → formats n-best lists and task instructions → LLM (Vicuna-7B) → processes prompts and generates intent classification → Optional LoRA adapters → finetuned parameters for task specialization → Output layer → converts LLM response to final classification/scores

- Critical path:
  ASR output → n-best list generation → prompt formatting → LLM inference → result interpretation

- Design tradeoffs:
  - n-best list size vs. context window limitations
  - Finetuning vs. prompting-only approaches
  - Hypothesis cost inclusion vs. prompt complexity
  - Binary output vs. probabilistic scoring for different use cases

- Failure signatures:
  - LLM outputs descriptive answers instead of classifications (prompting without finetuning)
  - Accuracy drops when hypothesis costs are removed (ablation study)
  - Performance plateaus or degrades with very large n-best lists

- First 3 experiments:
  1. Compare 1-best vs. 2-best vs. 4-best list performance on DDSD task with base LLM
  2. Test impact of including/excluding hypothesis costs in n-best lists
  3. Evaluate finetuned vs. non-finetuned LLM performance on GSC keyword spotting task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ASR+LLM systems with n-best lists scale with different ASR model qualities and architectures?
- Basis in paper: [inferred] The paper uses a single in-house ASR model and shows improvements with n-best lists over 1-best outputs, but doesn't explore how performance varies across different ASR qualities.
- Why unresolved: The experiments only use one ASR model, leaving open whether the benefits of n-best lists would be consistent across ASR systems with different WERs or architectures.
- What evidence would resolve it: Experiments comparing n-best list performance across multiple ASR models with varying architectures (e.g., E2E vs hybrid) and different WERs on the same downstream tasks.

### Open Question 2
- Question: What is the optimal n value for n-best lists in different downstream tasks and how does it vary with task complexity?
- Basis in paper: [explicit] The paper explores n=1, 2, 4, 8, and 16 but notes "diminishing gains as we increase the size of n-best list" without establishing optimal values for different task types.
- Why unresolved: The experiments show performance trends with increasing n but don't systematically determine optimal n values for different task complexities or characteristics.
- What evidence would resolve it: Systematic experiments varying n across multiple task types with different complexities and determining performance plateaus or optimal values for each task category.

### Open Question 3
- Question: How does the ASR+LLM approach with n-best lists compare to end-to-end multimodal LLMs that process speech directly?
- Basis in paper: [explicit] The paper acknowledges this comparison in the introduction but only explores the modular ASR+LLM approach, stating "such architectures are mainly motivated towards the goal of having a single end-to-end model and the speech recognition capabilities of such multi-modal LLMs are limited."
- Why unresolved: The paper focuses solely on the modular approach without empirical comparison to emerging multimodal LLMs, leaving the relative merits unclear.
- What evidence would resolve it: Direct empirical comparisons between the n-best ASR+LLM approach and state-of-the-art multimodal LLMs on the same downstream tasks, measuring both accuracy and computational efficiency.

## Limitations
- Experimental scope limited to two specific speech-intent classification tasks on potentially limited datasets
- Results depend heavily on the quality and characteristics of the underlying ASR system
- LoRA finetuning requires task-specific data and computational resources

## Confidence

**High Confidence Claims**:
- Using n-best lists improves over 1-best hypothesis for the tested tasks
- The structured prompt format successfully communicates n-best information to the LLM
- Hypothesis costs provide measurable benefits when included in prompts

**Medium Confidence Claims**:
- The optimal n-best list size is 16 (may vary with different ASR systems and tasks)
- LoRA finetuning consistently improves performance across different prompt structures
- The approach generalizes to other speech-intent classification tasks

**Low Confidence Claims**:
- The specific improvement magnitudes will transfer to different ASR systems
- The prompting strategy works equally well for LLMs of different sizes
- The cost structure (acoustic + language model costs) is optimal for all scenarios

## Next Checks

1. **Cross-Architecture Validation**: Test the n-best list approach with different LLM architectures (beyond Vicuna-7B) to verify the prompting mechanism generalizes across model families and sizes.

2. **ASR System Variability**: Evaluate performance using n-best lists from multiple ASR systems with different architectures and training approaches to assess robustness to ASR quality variations.

3. **Task Generalization Study**: Apply the methodology to additional speech tasks (e.g., intent recognition in dialogue systems, emotion detection, or speaker identification) to validate broader applicability beyond the two tested tasks.