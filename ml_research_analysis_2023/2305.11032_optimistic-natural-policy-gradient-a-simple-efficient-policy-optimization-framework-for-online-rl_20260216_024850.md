---
ver: rpa2
title: 'Optimistic Natural Policy Gradient: a Simple Efficient Policy Optimization
  Framework for Online RL'
arxiv_id: '2305.11032'
source_url: https://arxiv.org/abs/2305.11032
tags:
- policy
- linear
- lemma
- optimization
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Optimistic Natural Policy Gradient (ONPG), a
  simple and efficient policy optimization framework for online reinforcement learning
  (RL). ONPG combines the classic Natural Policy Gradient (NPG) algorithm with optimistic
  policy evaluation subroutines to encourage exploration.
---

# Optimistic Natural Policy Gradient: a Simple Efficient Policy Optimization Framework for Online RL

## Quick Facts
- arXiv ID: 2305.11032
- Source URL: https://arxiv.org/abs/2305.11032
- Reference count: 40
- The paper proposes ONPG, achieving optimal sample complexity for linear MDPs and polynomial complexity for general function approximation.

## Executive Summary
This paper introduces Optimistic Natural Policy Gradient (ONPG), a simple and efficient policy optimization framework for online reinforcement learning. ONPG combines natural policy gradient with optimistic policy evaluation to achieve improved sample complexity. For d-dimensional linear MDPs, ONPG learns an ε-optimal policy within O(d²/ε³) samples, improving over previous results by a factor of d. The algorithm also achieves polynomial sample complexity for general function approximation settings. ONPG is computationally efficient and resembles practical algorithms like PPO and TRPO.

## Method Summary
ONPG is an online RL algorithm that alternates between collecting fresh on-policy data and performing optimistic policy evaluation. The algorithm uses a softmax parameterization with mirror ascent for policy updates. For each iteration, it collects fresh trajectories, splits the data into H disjoint subsets (one per step), performs ridge regression to estimate parameters, computes bonus terms based on data uncertainty, calculates optimistic Q-values, and updates the policy. The key innovation is the use of periodic fresh data collection to control on-policy uncertainty instead of cumulative uncertainty, allowing for smaller bonus terms and sharper sample complexity bounds.

## Key Results
- Achieves O(d²/ε³) sample complexity for d-dimensional linear MDPs, improving over state-of-the-art by a factor of d
- Provides polynomial sample complexity for general function approximation settings
- Demonstrates computational efficiency and similarity to practical algorithms like PPO and TRPO
- Shows optimal dimension dependence for linear MDPs through new proof techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimistic policy evaluation with fresh on-policy data enables efficient exploration in linear MDPs.
- Mechanism: By collecting fresh trajectories every m steps and using them for optimistic value estimation, the algorithm avoids stale data correlations and controls on-policy uncertainty directly. This allows for smaller bonus terms compared to previous works, which is crucial for achieving the optimal d² dependence.
- Core assumption: The policy's visitation distribution over state-action pairs changes slowly enough between data collection periods, and the bonus term correctly captures the remaining uncertainty.
- Evidence anchors:
  - [abstract] "OPTIMISTIC NPG can be viewed as simply combining of the classic natural policy gradient (NPG) algorithm [13] with optimistic policy evaluation subroutines to encourage exploration."
  - [section] "OPTIMISTIC NPG utilizes on-policy fresh data for value estimation while Shani et al. [20], Cai et al. [8] are off-policy and reuse all the historical data."
  - [corpus] Weak evidence. Related works mention NPG variants but do not directly discuss the specific optimistic evaluation mechanism with periodic fresh data.
- Break condition: If the policy changes too rapidly between data collection periods (m too large), or if the bonus term underestimates uncertainty, leading to overestimation bias and poor exploration.

### Mechanism 2
- Claim: Softmax parameterization with mirror ascent allows efficient reuse of historical Q-value estimates.
- Mechanism: The softmax policy parameterization π k h(·|s) ∝ exp(η ∑t<k Q t h(s,·)) enables storing only the sum of Q-values from previous iterations. This allows computing any π k h(·|s) on-the-fly without re-running past policies, making the algorithm computationally efficient.
- Core assumption: The Q-value estimates Q t h(s,·) can be stored and reused accurately across iterations.
- Evidence anchors:
  - [section] "By the update rule of mirror ascent π k h(·|s)∝ exp(η ∑k− 1 t=1 Q t h(s,·)), we only need to store {{Q t h}h∈[H]}t∈[K], from which any π k h(·|s) can be computed on the fly."
  - [corpus] No direct evidence. Related works discuss NPG but not the specific storage/reuse mechanism described here.
- Break condition: If Q-value estimates become corrupted or if the storage mechanism fails to preserve the necessary information for policy computation.

### Mechanism 3
- Claim: Controlling on-policy uncertainty instead of cumulative uncertainty leads to sharper sample complexity bounds.
- Mechanism: By splitting the dataset Dk into H disjoint subsets and using only the subset Dk h for step h, the algorithm eliminates correlation between different steps. This allows using a smaller bonus term by a factor of √d, which is key to achieving the optimal dimension dependence.
- Core assumption: The data splitting and fresh data collection effectively decorrelate the estimation errors across steps.
- Evidence anchors:
  - [section] "OPTIMISTIC NPG uses a bonus function that is √d times smaller than in previous works [e.g., 11]."
  - [section] "The key factor behind this improvement is OPTIMISTIC NPG's periodic collection of fresh on-policy data, which eliminates the undesired correlation and avoids the union bound over certain nonlinear function class that are commonly observed in previous works."
  - [corpus] Weak evidence. Related works mention linear MDPs but do not discuss the specific technique of controlling on-policy uncertainty.
- Break condition: If the data splitting or fresh data collection fails to decorrelate the errors, or if the bonus term becomes too small and underestimates the true uncertainty.

## Foundational Learning

- Concept: Linear MDPs and linear completeness property
  - Why needed here: The algorithm relies on the linear completeness property to compute Q-values from V-values using ridge regression, which is crucial for the efficient function approximation.
  - Quick check question: Can you explain how the linear completeness property allows us to represent Tπ V as ⟨φ(s,a), θ⟩ for some θ?
- Concept: Natural Policy Gradient (NPG) and mirror ascent
  - Why needed here: The algorithm uses NPG with softmax parameterization and mirror ascent for policy updates, which requires understanding the gradient structure in policy space.
  - Quick check question: How does the softmax parameterization interact with mirror ascent to produce the policy update rule in Algorithm 1?
- Concept: Optimistic exploration and bonus-based uncertainty quantification
  - Why needed here: The algorithm uses optimistic value estimation with bonus terms to encourage exploration, which requires understanding how to construct and use these bonuses.
  - Quick check question: What is the intuition behind adding bonus terms to value estimates, and how do they encourage exploration?

## Architecture Onboarding

- Component map:
  - Algorithm 1 (Main algorithm) -> Optimistic Policy Evaluation (OPE) subroutine -> Bonus computation -> Q-value storage
- Critical path:
  1. Collect fresh trajectories using current policy π k
  2. Split data into H disjoint subsets for each step
  3. Perform ridge regression to estimate parameters
  4. Compute bonus terms based on data uncertainty
  5. Calculate optimistic Q-values using parameters and bonuses
  6. Update policy using mirror ascent with softmax parameterization
  7. Repeat for K iterations
- Design tradeoffs:
  - Fresh data collection vs. data efficiency: Collecting fresh data every m steps improves exploration but increases sample complexity. Choosing m = 1 is purely on-policy but less efficient than m = H/ε.
  - Bonus size vs. optimism: Smaller bonuses reduce overestimation bias but may underestimate true uncertainty. The algorithm uses data splitting and fresh data to justify smaller bonuses.
  - Storage vs. computation: Storing historical Q-values enables efficient policy computation but requires memory. The softmax parameterization allows on-the-fly computation without storing full policies.
- Failure signatures:
  - Poor exploration: If the bonus terms are too small or the policy changes too slowly, the algorithm may get stuck in suboptimal regions.
  - Overestimation bias: If the bonus terms are too large or the data splitting fails to decorrelate errors, the algorithm may overestimate values and make poor decisions.
  - Computational inefficiency: If the Q-value storage or policy computation becomes too expensive, the algorithm may not scale to large problems.
- First 3 experiments:
  1. Implement the tabular OPE subroutine and verify that it produces optimistic Q-values with the correct bonus structure.
  2. Test the policy update rule with mirror ascent and softmax parameterization on a small tabular MDP to verify that it produces sensible policies.
  3. Run the full algorithm on a simple linear MDP with known structure to verify that it learns near-optimal policies and achieves the claimed sample complexity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sample complexity of Optimistic Natural Policy Gradient (ONPG) be improved to match the optimal rate of O(d²/ε²) for linear MDPs?
- Basis in paper: [explicit] The paper achieves O(d²/ε³) sample complexity, which is worse than the optimal O(d²/ε²) by a factor of 1/ε.
- Why unresolved: The authors explicitly state that reducing the 1/ε factor to achieve the optimal rate remains unclear.
- What evidence would resolve it: A theoretical proof showing ONPG can achieve O(d²/ε²) sample complexity, or a lower bound demonstrating this is impossible.

### Open Question 2
- Question: Is it possible to design a policy optimization algorithm that combines the simplicity of ONPG with the optimal sample complexity of value-based methods like LSVI-UCB?
- Basis in paper: [explicit] The paper highlights that ONPG is simpler than previous policy optimization algorithms but has worse sample complexity than value-based methods.
- Why unresolved: The authors don't explore hybrid approaches that might achieve both simplicity and optimal sample complexity.
- What evidence would resolve it: A new algorithm design that provably achieves both simplicity and optimal sample complexity, or a proof of impossibility.

### Open Question 3
- Question: How does the performance of ONPG scale with the eluder dimension in the general function approximation setting?
- Basis in paper: [inferred] The paper shows polynomial sample complexity in terms of eluder dimension, but doesn't provide specific scaling laws or empirical results.
- Why unresolved: The theoretical analysis provides bounds but doesn't give a complete picture of practical performance.
- What evidence would resolve it: Empirical studies showing ONPG's performance across different eluder dimensions, or tighter theoretical bounds on the dependence on eluder dimension.

### Open Question 4
- Question: Can the bonus function in ONPG be further reduced beyond the √d factor improvement, potentially leading to better sample complexity?
- Basis in paper: [explicit] The paper achieves a √d improvement in the bonus function size, but suggests this might not be the limit.
- Why unresolved: The analysis focuses on achieving the optimal dimension dependence but doesn't explore whether smaller bonuses are possible.
- What evidence would resolve it: A theoretical proof showing the √d factor is optimal, or a new analysis technique achieving a smaller bonus with correspondingly better sample complexity.

## Limitations

- The theoretical analysis assumes access to fresh on-policy data, which may be challenging in practice
- The sample complexity for linear MDPs is O(d²/ε³), which is worse than the optimal O(d²/ε²) by a factor of 1/ε
- The performance in settings beyond linear MDPs is less certain due to the complexity of the general function approximation setting

## Confidence

- Sample complexity for linear MDPs: Medium-High
- Sample complexity for general function approximation: Medium
- Computational efficiency claims: Medium

## Next Checks

1. **Theoretical Validation**: Rigorously verify the sample complexity proofs for both linear MDPs and general function approximation, paying close attention to the assumptions and approximations made.

2. **Empirical Validation**: Implement the algorithm and test it on a range of benchmark problems, including linear MDPs and more complex function approximation settings, to assess its practical performance and sample efficiency.

3. **Robustness Check**: Investigate the algorithm's sensitivity to the hyperparameter m and other design choices, to understand its robustness and identify potential failure modes.