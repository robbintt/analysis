---
ver: rpa2
title: From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals
  for Big Models
arxiv_id: '2308.12014'
source_url: https://arxiv.org/abs/2308.12014
tags:
- human
- arxiv
- alignment
- values
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper surveys alignment goals for large language models (LLMs),
  categorizing them into three levels: human instructions, human preferences, and
  human values. The authors trace the evolution of alignment goals from basic task
  completion to more intrinsic value orientation.'
---

# From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models

## Quick Facts
- arXiv ID: 2308.12014
- Source URL: https://arxiv.org/abs/2308.12014
- Reference count: 26
- Key outcome: The paper surveys alignment goals for large language models (LLMs), categorizing them into three levels: human instructions, human preferences, and human values.

## Executive Summary
This paper provides a comprehensive survey of alignment goals for large language models, tracing their evolution from basic task completion to intrinsic value orientation. The authors categorize existing alignment approaches into three distinct levels - human instructions, human preferences, and human values - and analyze how each represents a progression toward more fundamental alignment with human interests. The survey examines both the definition of alignment goals and their evaluation methods, providing a structured framework for understanding the current state and future directions of LLM alignment research.

## Method Summary
The paper employs a systematic literature review methodology, examining 26 reference papers to categorize alignment goals into three levels: human instructions (basic task completion), human preferences (user satisfaction signals), and human values (intrinsic value principles). The analysis is conducted from two perspectives: how alignment goals are defined as training targets and how alignment is evaluated. The survey synthesizes findings across different alignment algorithms and identifies challenges and future research directions for each alignment goal level.

## Key Results
- Categorizes alignment goals for LLMs into three levels: human instructions, human preferences, and human values
- Traces the evolution of alignment objectives from surface behaviors to deeper value orientations
- Identifies intrinsic human values as the most promising alignment goal for addressing current challenges
- Provides comprehensive overview of existing works, evaluation methods, and algorithms for each alignment goal
- Highlights challenges including comprehensiveness, generalization, stability, and adaptability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The paper's survey methodology enables systematic mapping of alignment goal evolution from instructions to intrinsic values.
- **Mechanism:** By categorizing existing works into three distinct levels (human instructions, human preferences, human values) and tracing their evolutionary paths, the paper provides a structured framework for understanding how alignment objectives have shifted from surface behaviors to deeper value orientations.
- **Core assumption:** The classification into three levels is both comprehensive and mutually exclusive.
- **Evidence anchors:**
  - [abstract] "Our analysis encompasses three distinct levels of alignment goals and reveals a goal transformation from fundamental abilities to value orientation"
  - [section 2.1] "This goal concentrates on the fundamental capabilities of big models to generate narrowly defined correct results"
  - [section 2.3] "human values refer to a comprehensive measurement of what is good and what is bad, as well as what ought to be done in terms of the whole human collective"
- **Break condition:** If new alignment approaches emerge that don't fit neatly into these three categories, or if the boundaries between categories become blurred.

### Mechanism 2
- **Claim:** The dual-perspective review (definition + evaluation) provides comprehensive coverage of alignment research.
- **Mechanism:** By examining both how alignment goals are defined/represented as training targets and how alignment is evaluated, the paper captures both the theoretical framework and practical measurement of progress.
- **Core assumption:** The definition of alignment goals and their evaluation methods are sufficiently interconnected to provide meaningful insights.
- **Evidence anchors:**
  - [abstract] "we investigate related works from two perspectives: the definition of alignment goals and alignment evaluation"
  - [section 3] "To ensure that big models align with the goal in the right direction, it is crucial to accurately evaluate the alignment degree"
  - [section 2.2.3] "Employing neural models to synthesize the signals of human preferences can not only reduce labor costs, but also avoid issues such as biases introduced by humans"
- **Break condition:** If the evaluation methods are found to be inconsistent with the defined alignment goals, or if the evaluation metrics don't accurately measure the intended objectives.

### Mechanism 3
- **Claim:** The survey's identification of intrinsic human values as the most promising alignment goal is justified through comparative analysis of challenges.
- **Mechanism:** By systematically comparing the three alignment levels against criteria like comprehensiveness, generalization, stability, and adaptability, the paper demonstrates why intrinsic values represent the most robust approach to aligning LLMs with human interests.
- **Core assumption:** The challenges identified (comprehensiveness, generalization, stability, adaptability) are the most relevant criteria for evaluating alignment approaches.
- **Evidence anchors:**
  - [abstract] "posing intrinsic human values as the promising alignment goal for big models, we discuss the challenges and future research directions"
  - [section 5] "With the introduction of human values, aligning big models to intrinsic value principles rather than uncountable manifest behaviors provides a promising opportunity to address the aforementioned challenges"
  - [corpus] Weak evidence - the corpus neighbors are more focused on superalignment and safety rather than the specific three-level framework presented in the paper
- **Break condition:** If new research demonstrates that intrinsic values are impractical to implement, or if surface-level alignment proves sufficient for real-world applications.

## Foundational Learning

- **Concept:** Evolution of alignment goals from instructions to values
  - Why needed here: Understanding this progression is essential for grasping why the paper advocates for intrinsic values as the ultimate alignment goal
  - Quick check question: What are the three levels of alignment goals identified in the paper, and how do they differ in terms of their focus and implementation?

- **Concept:** Reward modeling and preference learning
  - Why needed here: These are key techniques for implementing alignment with human preferences and values, mentioned throughout the survey
  - Quick check question: How does reinforcement learning from human feedback (RLHF) differ from direct preference optimization (DPO) in terms of training methodology and objectives?

- **Concept:** Evaluation methodologies for LLM alignment
  - Why needed here: The paper emphasizes the importance of accurate evaluation methods to measure alignment progress, making this concept central to understanding the survey's conclusions
  - Quick check question: What are the main differences between benchmarks, human evaluations, and reward model scoring as methods for assessing LLM alignment?

## Architecture Onboarding

- **Component map:** The survey architecture consists of three main components: (1) Definition of alignment goals (categorizing existing work into three levels), (2) Evaluation methods (benchmarks, human evaluations, reward models), and (3) Alignment algorithms (SFT, RLHF, alternative methods). These components are interconnected, with each alignment goal requiring specific evaluation methods and algorithmic approaches.

- **Critical path:** The most critical path for understanding this survey is: reading the abstract and introduction to grasp the overall framework → examining section 2 to understand the three alignment goal levels → reviewing section 3 to see how alignment is evaluated → studying section 4 for algorithmic approaches → analyzing section 5 for challenges and future directions.

- **Design tradeoffs:** The survey prioritizes comprehensiveness over depth, covering a wide range of approaches rather than providing exhaustive details on any single method. This enables broad understanding but may require additional research for implementation details. The three-level classification is simple but may not capture all nuances of alignment approaches.

- **Failure signatures:** Key failure points include: (1) misalignment between defined goals and evaluation methods, (2) incomplete coverage of alignment approaches, (3) oversimplification of complex value systems, (4) overemphasis on theoretical frameworks without practical implementation guidance.

- **First 3 experiments:**
  1. Create a simple categorization table mapping 10-15 recent alignment papers to the three goal levels identified in the survey, checking if the classification framework is comprehensive
  2. Implement a basic reward model using existing preference datasets to test the practical challenges of preference-based alignment mentioned in the survey
  3. Design a small-scale evaluation framework combining benchmarks and human feedback to assess which evaluation methods best capture alignment progress for a specific goal level

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most appropriate and essential alignment goals for large language models (LLMs)?
- Basis in paper: [explicit] The paper discusses the evolution of alignment goals from fundamental abilities to value orientation and highlights the potential of intrinsic human values as the alignment goal for enhanced LLMs.
- Why unresolved: The paper identifies the need for an appropriate and essential alignment goal but does not provide a definitive answer or framework for determining the most suitable goal.
- What evidence would resolve it: A comprehensive framework that evaluates different alignment goals based on criteria such as generalizability, stability, and interpretability, along with empirical studies comparing the effectiveness of various goals in real-world applications.

### Open Question 2
- Question: How can we effectively represent the alignment goal of intrinsic human values as a training target for LLMs?
- Basis in paper: [explicit] The paper discusses the challenges of representing intrinsic human values as a training target, including generalizability, stability, and interpretability.
- Why unresolved: While the paper highlights the importance of representing intrinsic human values, it does not provide specific methods or strategies for achieving this representation.
- What evidence would resolve it: Development and evaluation of novel approaches that incorporate intrinsic human values into the training process of LLMs, demonstrating improved performance and alignment with human values compared to existing methods.

### Open Question 3
- Question: How can we develop effective and stable alignment algorithms that directly align LLMs with human value principles?
- Basis in paper: [explicit] The paper discusses the need for effective and stable alignment algorithms that directly align LLMs with human value principles rather than proxy demonstrations.
- Why unresolved: The paper identifies the challenges of developing such algorithms but does not provide specific solutions or approaches.
- What evidence would resolve it: Development and evaluation of new alignment algorithms that directly incorporate human value principles into the training process of LLMs, demonstrating improved performance and stability compared to existing methods.

## Limitations

- The three-level classification framework may not capture all nuances of alignment approaches, particularly as the field evolves rapidly
- The survey acknowledges the importance of cultural context but doesn't provide detailed methodologies for handling value differences across populations
- While intrinsic human values are identified as promising, the paper doesn't provide definitive criteria for determining the most suitable alignment goal
- The treatment of practical implementation details for value alignment systems lacks depth compared to theoretical frameworks

## Confidence

**High Confidence:** The survey's documentation of existing alignment approaches and evaluation methods is well-supported by the literature review. The categorization of alignment algorithms (SFT, RLHF, DPO, etc.) and their relationship to specific alignment goals is based on established research and can be verified through the cited works.

**Medium Confidence:** The framework of three alignment goal levels and their evolutionary progression is logically structured but relies on interpretive classification of existing work. The claim that intrinsic human values represent the most promising alignment goal is supported by comparative analysis but acknowledges significant implementation challenges.

**Low Confidence:** The paper's predictions about future research directions and the practical feasibility of achieving intrinsic value alignment are necessarily speculative. The proposed solutions for challenges like comprehensiveness, generalization, stability, and adaptability lack detailed implementation roadmaps.

## Next Checks

1. **Classification Validation:** Test the three-level framework by applying it to 20 recent alignment papers not included in the original survey. Measure the inter-rater reliability of the classification and identify any papers that don't fit cleanly into the proposed categories.

2. **Evaluation Method Consistency:** Conduct a systematic review of evaluation benchmarks to assess whether current metrics adequately measure progress toward each alignment goal level. Identify gaps between what we claim to optimize for and what we actually measure.

3. **Value Conflict Resolution:** Design and implement a small-scale experiment testing how different value alignment approaches handle explicit conflicts between competing human values, using scenarios with known value trade-offs to evaluate robustness and consistency.