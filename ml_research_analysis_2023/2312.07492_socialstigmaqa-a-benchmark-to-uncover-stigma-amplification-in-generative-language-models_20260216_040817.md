---
ver: rpa2
title: 'SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative
  Language Models'
arxiv_id: '2312.07492'
source_url: https://arxiv.org/abs/2312.07492
tags:
- someone
- shall
- stigma
- bias
- them
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces SocialStigmaQA, a new benchmark for evaluating
  social bias in generative language models. The benchmark consists of 10,360 prompts
  that cover 93 documented US-centric stigmas, formulated as a question-answering
  task involving simple social situations.
---

# SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models

## Quick Facts
- arXiv ID: 2312.07492
- Source URL: https://arxiv.org/abs/2312.07492
- Reference count: 12
- 45%-59% of generated outputs amplify existing social bias against stigmatized groups

## Executive Summary
This work introduces SocialStigmaQA, a new benchmark for evaluating social bias in generative language models. The benchmark consists of 10,360 prompts that cover 93 documented US-centric stigmas, formulated as a question-answering task involving simple social situations. Experiments with two widely used open-source generative language models (Flan-T5 and Flan-UL2) demonstrate that 45%-59% of the generated outputs amplify existing social bias against stigmatized groups. The study also reveals that the deliberate design of prompt templates, including biasing text or varying answer choices, impacts model tendencies to generate biased output.

## Method Summary
The SocialStigmaQA benchmark uses template-based QA format with 10,360 prompts covering 93 US-centric stigmas across 37 patterns and 4 prompt styles. The methodology evaluates two open-source generative language models (Flan-T5 and Flan-UL2) using both greedy decoding and nucleus sampling with chain-of-thought triggers. Outputs are parsed to extract yes/no/can't tell answers, with manual inspection of 600 chain-of-thought responses to identify bias patterns ranging from subtle bias to lack of reasoning.

## Key Results
- 45%-59% of questions are answered unfavorably towards people with stigmatized conditions across decoding strategies
- Prompt template design significantly impacts model tendencies to generate socially biased output
- Manual evaluation of chain-of-thought outputs reveals problematic patterns ranging from subtle bias to evidence of lack of reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Template-based QA format reliably elicits bias responses from LLMs
- Mechanism: The QA format creates a structured interaction where the model must generate a definitive answer to socially charged questions
- Core assumption: LLMs trained on general web data contain embedded social biases that will manifest when prompted with stigmatized identity contexts
- Evidence anchors:
  - "Our experiments with two large generative models... show that 45%-59% of the questions... are answered unfavorably towards the person with the stigmatized condition"
  - "All questions are templated such that they can be filled in by the different stigmas considered in this study"

### Mechanism 2
- Claim: Chain-of-thought prompting reveals model reasoning patterns that explain bias amplification
- Mechanism: By triggering CoT with "Let's think step by step," the model generates intermediate reasoning that exposes how it processes stigmatized contexts
- Core assumption: CoT outputs reflect the model's internal reasoning process and can be manually inspected for bias patterns
- Evidence anchors:
  - "Through manual annotation, we provide high level trends in the generated chain-of-thought output... finding a variety of problems from subtle bias to evidence of a lack of reasoning"
  - "We discovered problematic patterns in the generated chain-of-thought output that range from subtle bias to lack of reasoning"

### Mechanism 3
- Claim: Prompt style variations systematically test model robustness to bias-inducing contexts
- Mechanism: The four prompt styles create controlled variations that isolate how additional context affects bias generation
- Core assumption: LLMs are sensitive to prompt framing and will adjust their bias expression based on subtle contextual cues
- Evidence anchors:
  - "We include 4 prompt styles for each pattern... the 'positive bias' style adds text which encourages answering in an unbiased manner"
  - "We discovered that the deliberate design of the templates in our benchmark... impacts the model tendencies to generate socially biased output"

## Foundational Learning

- Concept: Template-based data generation
  - Why needed here: The benchmark requires systematic coverage of 93 stigmas across 37 patterns, which demands a scalable template approach rather than manual prompt creation
  - Quick check question: How would you modify a template to test a new stigmatized condition while maintaining the same social situation context?

- Concept: Chain-of-thought evaluation methodology
  - Why needed here: Manual inspection of CoT outputs is essential for understanding bias mechanisms that automated metrics miss
  - Quick check question: What criteria would you use to distinguish between "subtle bias" and "nonsense" in CoT outputs?

- Concept: Bias quantification metrics
  - Why needed here: The benchmark needs standardized ways to measure bias across different decoding strategies and prompt styles
  - Quick check question: How would you calculate the bias proportion for a prompt where the biased answer is "yes" but the model generated "no"?

## Architecture Onboarding

- Component map: Template curator → Stigma filler → Prompt style generator → LLM inference → Output parser → Manual CoT annotator

- Critical path:
  1. Template creation (hand-curated social situations)
  2. Stigma population (93 stigmas across all templates)
  3. Prompt style application (4 styles per filled template)
  4. LLM inference (Flan-T5 and Flan-UL2 with different decoding strategies)
  5. Output parsing (extract answers from greedy vs. nucleus sampling)
  6. Bias calculation (compare generated answers to biased answer definitions)
  7. Manual CoT inspection (600 responses across all experimental conditions)

- Design tradeoffs:
  - Open-ended vs. closed-ended questions: Closed-ended enables automated evaluation but may limit nuance
  - Template rigidity vs. flexibility: Rigid templates ensure systematic coverage but may miss complex social situations
  - Manual vs. automated CoT evaluation: Manual provides depth but doesn't scale; automated scales but misses subtle patterns

- Failure signatures:
  - All prompt styles producing identical bias distributions (suggests model is insensitive to framing)
  - No-stigma prompts showing high bias (suggests model has inherent answer preferences)
  - CoT outputs that never align with final answers (suggests CoT is decorative rather than explanatory)

- First 3 experiments:
  1. Run all 10,360 prompts through Flan-T5 with greedy decoding to establish baseline bias distribution
  2. Repeat with nucleus sampling and CoT triggering to compare bias amplification effects
  3. Run no-stigma prompts only to measure model's inherent answer preferences independent of stigma context

## Open Questions the Paper Calls Out

- How can the propensity of models to favor certain answers (yes/no) even in the absence of stigmas be factored into bias assessment?
- How does the inclusion of additional prompt styles, beyond the four discussed, impact model behavior and bias detection?
- How does the quality and faithfulness of chain-of-thought (CoT) outputs vary across different models and prompting strategies?

## Limitations
- Findings may not generalize beyond US-centric stigmas to other cultural contexts
- Manual chain-of-thought evaluation doesn't scale for comprehensive bias auditing
- Simplified QA format may not capture complex real-world social dynamics where stigma manifests

## Confidence
- Confidence: Medium on generalizability of 45%-59% bias rate (only two models, US-centric stigmas)
- Confidence: Low regarding scalability of manual chain-of-thought evaluation
- Confidence: Medium in template-based methodology's ability to capture complex social dynamics

## Next Checks
1. Replicate SocialStigmaQA with stigmas documented in non-US contexts to test cultural generalizability
2. Evaluate additional model families beyond transformer-based models to determine architecture-dependent bias patterns
3. Extend evaluation to models with larger context windows to assess bias amplification in longer conversations