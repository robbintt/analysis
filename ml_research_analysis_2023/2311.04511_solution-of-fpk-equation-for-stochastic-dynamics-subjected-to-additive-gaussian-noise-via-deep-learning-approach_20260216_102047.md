---
ver: rpa2
title: Solution of FPK Equation for Stochastic Dynamics Subjected to Additive Gaussian
  Noise via Deep Learning Approach
arxiv_id: '2311.04511'
source_url: https://arxiv.org/abs/2311.04511
tags:
- network
- equation
- stochastic
- solution
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Fokker-Planck-Kolmogorov (FPK) equation is crucial for analyzing
  stochastic dynamical systems, but its high-dimensional nature makes numerical solutions
  challenging. This paper introduces FPK-DP Net, a physics-informed deep neural network
  that solves the FPK equation without prior simulation data by encoding the governing
  differential equations into the network architecture.
---

# Solution of FPK Equation for Stochastic Dynamics Subjected to Additive Gaussian Noise via Deep Learning Approach

## Quick Facts
- arXiv ID: 2311.04511
- Source URL: https://arxiv.org/abs/2311.04511
- Reference count: 40
- Key outcome: FPK-DP Net solves high-dimensional FPK equations without simulation data, achieving loss values from 1.8E-6 to 8.77E-8 with excellent Monte Carlo agreement

## Executive Summary
This paper introduces FPK-DP Net, a physics-informed deep neural network approach for solving the Fokker-Planck-Kolmogorov (FPK) equation for stochastic dynamical systems subjected to additive Gaussian noise. The method eliminates the need for prior simulation data by encoding the governing differential equations directly into the network architecture through a physics-informed loss function. The approach uses a dimension-reduced formulation that converts high-dimensional FPK equations into computationally tractable one-dimensional equivalent equations, making it particularly effective for high-dimensional stochastic systems.

## Method Summary
The method employs a two-stage physics-informed neural network approach. First, a DeepPDEM network solves the Generalized Density Evolution Equation (GDEE) to obtain the marginal probability density and equivalent drift coefficient. Second, the FPK-DP Net solves the one-dimensional equivalent FPK equation using outputs from the first stage. The network architecture uses a decoupled structure with two subnetworks to satisfy initial/boundary conditions and approximate the solution. Training employs automatic differentiation to compute partial derivatives for the loss function, which enforces the FPK equation in residual form. The approach is mesh-free and data-efficient, validated across five benchmark problems including Duffing oscillators, FitzHugh-Nagumo oscillators, and Roessler attractors.

## Key Results
- Achieved loss values ranging from 1.8E-6 to 8.77E-8 across benchmark problems
- Excellent agreement with Monte Carlo simulations for all tested cases
- Successfully solved one-, two-, and three-dimensional systems without mesh generation
- Demonstrated data efficiency by requiring no prior simulation data for training

## Why This Works (Mechanism)

### Mechanism 1
The dimension-reduced formulation converts high-dimensional FPK equations into a set of one-dimensional equivalent equations that are computationally tractable. By decomposing the Wiener process using SHF-II representation and applying the GDEE framework, the drift term is decoupled into equivalent one-dimensional components, while the diffusion term is naturally separated through the probability flux calculation. This assumes the stochastic excitation can be accurately approximated by a finite number of random variables through decomposition schemes.

### Mechanism 2
Physics-informed neural networks eliminate the need for labeled training data by encoding the governing differential equations directly into the loss function. The FPK-DP Net uses automatic differentiation to compute partial derivatives of the network output with respect to spatiotemporal variables, then constructs a loss function that enforces the FPK equation in residual form, training the network to satisfy the differential equation rather than match data points. This assumes the solution exists, is bounded, unique, and uniformly Lipschitz continuous.

### Mechanism 3
The decoupled network architecture with two subnetworks ensures both initial/boundary conditions and solution approximation are satisfied simultaneously. Network FPK-DP Net uses two subnetworks in a composition function that hard-codes initial and boundary conditions through exponential decay terms while allowing the solution to evolve naturally through training. This assumes the initial density distribution is known and independent of stochastic input, and the probability density vanishes at infinity sufficiently fast.

## Foundational Learning

- **Concept: Stochastic differential equations and It√¥ calculus**
  - Why needed here: The FPK equation is derived from It√¥ processes, so understanding SDEs is fundamental to grasping the problem formulation
  - Quick check question: What is the relationship between the drift vector ùêü and diffusion matrix ùê† in an It√¥ process?

- **Concept: Probability density evolution and Fokker-Planck-Kolmogorov equation**
  - Why needed here: The entire method is built around solving the FPK equation to find probability density evolution
  - Quick check question: How does the probability flux formulation help in deriving the decoupled equivalent of the FPK equation?

- **Concept: Physics-informed neural networks and automatic differentiation**
  - Why needed here: The FPK-DP Net is a physics-informed network that uses AD to compute derivatives for the loss function
  - Quick check question: How does automatic differentiation enable the computation of partial derivatives needed for the FPK equation residual?

## Architecture Onboarding

- **Component map:** DeepPDEM network -> FPK-DP Net -> Training loop with collocation points -> Integration scheme for marginal PDF
- **Critical path:** 1) Approximate stochastic excitation using SHF-II decomposition, 2) Train DeepPDEM network to solve GDEE and obtain ùìÖÃÉùëãùëñùöØ, 3) Compute equivalent drift coefficient ùëìÃÉùëñ(ùë•ùëñ,ùë°) from DeepPDEM outputs, 4) Train FPK-DP Net to solve the one-dimensional equivalent FPK equation, 5) Validate results against Monte Carlo simulations
- **Design tradeoffs:** Depth vs. convergence rate (deeper networks increase complexity but improve approximation), Number of random variables in decomposition (more variables improve accuracy but increase computational cost), Collocation points sampling (more points improve loss estimation but increase training time)
- **Failure signatures:** Loss value not decreasing below 1E-3 threshold, Monte Carlo results show significant deviation from network predictions, Network outputs violate probability axioms, Gradient explosion or vanishing during training
- **First 3 experiments:** 1) Test FPK-DP Net architecture on Case Study I with known analytical solution to verify correctness, 2) Vary number of random variables in SHF-II decomposition to find optimal balance between accuracy and computational cost, 3) Test different network depths (3, 4, 5 layers) on Case Study II to identify optimal architecture for convergence rate and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How does the FPK-DP Net performance scale with problem dimensionality beyond the tested three-dimensional case? The paper states FPK-DP Net "theoretically, it should be able to handle higher-dimensional problems" but only tested up to three dimensions, noting that "testing this idea requires further studies that are beyond the purpose and scope of the current work" and acknowledging that Monte Carlo verification becomes challenging for higher dimensions.

### Open Question 2
What is the theoretical convergence rate of the FPK-DP Net solution as a function of network architecture and training parameters? The paper discusses various hyperparameters and their empirical effects on convergence but doesn't provide theoretical analysis of convergence rates, noting that "determining a sufficient number of neurons is not always straightforward" and that network depth significantly affects convergence.

### Open Question 3
How sensitive is the FPK-DP Net to the choice of initial conditions and boundary condition encoding methods? While the paper discusses both hard-coding and soft-coding of boundary conditions and states that "hard-coding the boundary conditions offers superior performance," it doesn't systematically compare different encoding strategies or provide comparative analysis.

## Limitations

- Dimension reduction scheme's approximation accuracy for high-dimensional problems beyond three dimensions remains untested
- Method assumes solution exists, is bounded, unique, and uniformly Lipschitz continuous, which may not hold for all stochastic systems
- Performance guarantees for systems with non-standard initial conditions or highly nonlinear dynamics remain uncertain

## Confidence

**High Confidence:** Network architecture design and implementation details for FPK-DP Net are well-specified and reproducible. Loss function formulation and optimization procedure are clearly described.

**Medium Confidence:** Theoretical foundation connecting dimension-reduced formulation to original high-dimensional FPK equation is sound, but practical approximation error bounds are not rigorously established. Monte Carlo validation results are convincing but limited to specific test cases.

**Low Confidence:** Method's performance guarantees for systems beyond demonstrated cases, particularly problems with non-standard initial conditions or highly nonlinear dynamics, remain uncertain.

## Next Checks

1. **Dimensionality Scaling Test:** Apply FPK-DP Net to a 4-5 dimensional stochastic system (e.g., coupled oscillator network) and systematically evaluate how loss and accuracy scale with increasing dimensions compared to Monte Carlo simulations.

2. **Robustness to Initial Conditions:** Test the method with non-standard initial conditions, including multimodal distributions and distributions with heavy tails, to assess the network's ability to capture complex initial state densities.

3. **Comparison with Traditional Methods:** Implement a traditional finite element or finite difference solver for the FPK equation on the same test cases and conduct detailed comparison of accuracy, computational efficiency, and convergence behavior across different mesh resolutions.