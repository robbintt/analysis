---
ver: rpa2
title: 'Striking Gold in Advertising: Standardization and Exploration of Ad Text Generation'
arxiv_id: '2309.12030'
source_url: https://arxiv.org/abs/2309.12030
tags:
- text
- advertising
- search
- association
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the lack of benchmarks and standardized problem
  definitions in automatic ad text generation (ATG). The authors propose a first benchmark
  dataset, CAMERA, carefully designed for ATG to leverage multimodal information and
  conduct industry-wise evaluations.
---

# Striking Gold in Advertising: Standardization and Exploration of Ad Text Generation

## Quick Facts
- arXiv ID: 2309.12030
- Source URL: https://arxiv.org/abs/2309.12030
- Reference count: 22
- First benchmark dataset CAMERA introduced for ad text generation with industry-wise evaluation

## Executive Summary
This paper addresses the lack of standardized benchmarks for automatic ad text generation (ATG) by introducing CAMERA, the first comprehensive dataset designed specifically for this task. The authors systematically evaluate nine baseline models ranging from classical methods to state-of-the-art large language models across four industry domains. Their experiments reveal significant performance variation across industries and highlight the challenges of effectively leveraging multi-modal information from landing pages. The work establishes a foundation for future research in ATG and demonstrates the importance of industry-specific evaluation frameworks.

## Method Summary
The authors created CAMERA dataset by collecting real-world advertising data from major Japanese platforms, filtering for quality, and extracting multi-modal features including LP descriptions, OCR text, layout information, and visual features. They fine-tuned encoder-decoder models (BART and T5 variants) on the training set using standard hyperparameters and evaluated performance using BLEU-4, ROUGE-1, and keyword insertion rate metrics. The evaluation included both overall performance and industry-wise analysis across HR, EC, Fin, and Edu domains, with multi-reference BLEU and single-reference metrics for comprehensive assessment.

## Key Results
- Performance of ATG models varied significantly across different industries, with BART outperforming T5 in certain domains
- Incorporating OCR text and layout information improved generation quality, while visual features sometimes degraded performance
- Multi-modal feature integration remains challenging, suggesting need for better information selection mechanisms
- Keyword insertion rates showed high faithfulness to landing page content but low overlap with human-written ad texts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-modal information (text, images, layout) can improve ad text generation quality by providing richer context.
- Mechanism: The model processes LP description, user query, OCR text, layout information, and visual features separately, then combines them to generate more relevant and appealing ad text.
- Core assumption: Visual and layout information from LPs contains cues about product features and user appeal that are not captured by text alone.
- Evidence anchors:
  - [abstract] "The results demonstrate that the performance of ATG models varied significantly across different industries and also effectively leveraging multi-modal information poses a challenge for future research."
  - [section] "Incorporating additional features such as OCR-processed text (+{o}) and the LP layout information (+{o,l}) improved the quality of generated sentences in terms of B-4 and R-1 scores."
- Break condition: If visual features introduce noise rather than useful information, or if the model fails to properly align multi-modal inputs with ad text generation.

### Mechanism 2
- Claim: Pre-trained models like BART and T5 can be effectively fine-tuned for ad text generation by learning from industry-specific data.
- Mechanism: Fine-tuning encoder-decoder models on CAMERA dataset allows adaptation to ad text generation task with specific industry requirements.
- Core assumption: Pre-trained language models have learned general language patterns that can be specialized for ad text generation through domain-specific fine-tuning.
- Evidence anchors:
  - [abstract] "We conducted extensive experiments with a variety of nine baselines, from classical methods to state-of-the-art models including large language models (LLMs)."
  - [section] "We fine-tuned each pre-trained model on the training dataset to create our baseline models."
- Break condition: If the pre-trained models have significant domain shift from advertising language, or if fine-tuning data is insufficient.

### Mechanism 3
- Claim: Industry-wise evaluation is crucial because ad text generation performance varies significantly across different product/service domains.
- Mechanism: CAMERA dataset includes industry labels (HR, EC, Fin, Edu) enabling evaluation of model performance across different domains.
- Core assumption: Different industries have distinct advertising appeals, customer psychology, and language patterns that affect what makes effective ad text.
- Evidence anchors:
  - [abstract] "The performance of ATG models varied significantly across different industries."
  - [section] "Industry-wise evaluation results show variation when considering different industries. For example, in the human resources domain, BART outperformed T5 in B-4 and R-1."
- Break condition: If industry differences are less significant than initially thought, or if the model architecture cannot effectively capture industry-specific patterns.

## Foundational Learning

- Concept: Cross-modal learning
  - Why needed here: Ad text generation requires combining information from multiple sources (text, images, layout) to create effective advertisements.
  - Quick check question: How would you design a model to handle inputs from both text and images for ad generation?

- Concept: Domain adaptation
  - Why needed here: Adapting general-purpose language models to the specific requirements of ad text generation across different industries.
  - Quick check question: What are the key differences between general text generation and ad text generation that need to be addressed during adaptation?

- Concept: Evaluation metrics for generation tasks
  - Why needed here: Understanding BLEU, ROUGE, and keyword insertion metrics is crucial for assessing ad text quality.
  - Quick check question: Why might traditional text generation metrics not fully capture the effectiveness of ad text?

## Architecture Onboarding

- Component map: Encoder-decoder framework (BART/T5) -> Multi-modal feature extractors (OCR, layout, visual) -> Industry-wise evaluation pipeline -> Keyword insertion rate calculation

- Critical path:
  1. Load and preprocess multi-modal data
  2. Extract features from LP images (OCR, layout, visual)
  3. Fine-tune pre-trained model on CAMERA dataset
  4. Generate ad texts using trained model
  5. Evaluate using BLEU, ROUGE, and keyword metrics

- Design tradeoffs:
  - Using full LP images vs. cropped relevant sections (impact on noise vs. information)
  - Choice of pre-trained model (BART vs. T5) based on industry requirements
  - Balancing between faithful reproduction of LP content vs. creating appealing ad text

- Failure signatures:
  - High keyword insertion rate but low BLEU/ROUGE scores (keyword stuffing without coherence)
  - Consistent performance drop in certain industries (model bias or lack of domain-specific training data)
  - Multi-modal features degrading performance (visual information not properly integrated)

- First 3 experiments:
  1. Compare BART vs. T5 performance on CAMERA dataset without multi-modal features
  2. Evaluate impact of adding OCR text vs. adding layout information vs. adding visual features
  3. Test industry-wise performance variation by training separate models for each industry

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of advertising appeals vary across different industries and target audiences?
- Basis in paper: [explicit] The paper mentions that the effectiveness of advertising appeals varies depending on the target product and industry type, and that capturing users' latent needs and generating appealing sentences that lead to advertising effectiveness depends significantly on the psychological characteristics of the recipient users.
- Why unresolved: The paper does not provide specific insights into how advertising appeals vary across industries or target audiences. It only mentions that this is an important factor to consider in ad text generation.
- What evidence would resolve it: Experimental results comparing the performance of ad text generation models across different industries and target audiences, or user studies examining the effectiveness of different advertising appeals on different groups.

### Open Question 2
- Question: How can multi-modal information from landing pages be effectively utilized to improve ad text generation?
- Basis in paper: [explicit] The paper states that incorporating additional features such as OCR-processed text and LP layout information improved the quality of generated sentences, but adding LP image features led to a decline in performance for some domains. It suggests that developing a model that dynamically selects only important information from LP images could improve generation quality.
- Why unresolved: The paper does not provide a clear solution for effectively leveraging multi-modal information from LPs. It only identifies this as a challenge and suggests a potential direction for future research.
- What evidence would resolve it: Development and evaluation of models that can effectively select and utilize relevant multi-modal information from LPs to improve ad text generation performance.

### Open Question 3
- Question: How can the validity of automatic evaluation metrics for ad text generation be assessed?
- Basis in paper: [explicit] The paper mentions that conventional metrics like BLEU and ROUGE have limited correlation with human judgments in various NLG tasks and suggests that it is crucial to investigate the validity of these metrics for ad text generation thoroughly.
- Why unresolved: The paper does not provide a specific method for assessing the validity of automatic evaluation metrics for ad text generation. It only identifies this as an important area for future research.
- What evidence would resolve it: Meta-evaluation studies comparing the correlation between automatic metrics and human judgments for ad text generation, or the development of new evaluation metrics specifically designed for ad text generation.

## Limitations
- Dataset construction process may introduce selection bias through filtering of noisy data
- 15 full-width character limit may not generalize to other languages or advertising contexts
- Evaluation relies heavily on automated metrics without human assessment of ad persuasiveness

## Confidence
- High confidence: Dataset construction methodology and multi-reference evaluation approach
- Medium confidence: Effectiveness of multi-modal information integration and baseline model implementations
- Low confidence: Generalizability to non-Japanese markets and real-world advertising performance impact

## Next Checks
1. Conduct ablation studies on CAMERA dataset to quantify individual contribution of each multimodal feature (OCR text, layout information, visual features) to performance across all industry domains.

2. Implement human evaluation studies where advertising professionals rate persuasiveness, relevance, and appeal of generated ad texts compared to human-written ones, focusing on industries with significant performance gaps.

3. Test generalizability of CAMERA-trained models on external datasets from different languages or advertising contexts to assess whether industry-wise performance patterns are specific to Japanese market or represent universal principles.