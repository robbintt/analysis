---
ver: rpa2
title: 'Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation'
arxiv_id: '2310.02842'
source_url: https://arxiv.org/abs/2310.02842
tags:
- prompt
- prompts
- training
- group
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mixture of Prompts (MoPs), a novel approach
  for task adaptation of large language models (LLMs) in heterogeneous, multi-source
  scenarios. The core idea is to use a gating function that dynamically selects and
  combines specialized prompts (acting as "experts") based on the input task, mitigating
  training interference that arises from conflicting signals across diverse tasks
  and data distributions.
---

# Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation

## Quick Facts
- **arXiv ID**: 2310.02842
- **Source URL**: https://arxiv.org/abs/2310.02842
- **Reference count**: 40
- **Primary result**: MoPs reduces perplexity by up to 70% in federated and 30% in centralized learning scenarios

## Executive Summary
This paper introduces Mixture of Prompts (MoPs), a novel approach for task adaptation of large language models (LLMs) in heterogeneous, multi-source scenarios. The core idea is to use a gating function that dynamically selects and combines specialized prompts (acting as "experts") based on the input task, mitigating training interference that arises from conflicting signals across diverse tasks and data distributions. The method is particularly effective when applied to compressed LLMs, maintaining performance regardless of pruning or quantization techniques. Empirically, MoPs significantly reduces perplexity compared to baselines while being agnostic to task composition and data source.

## Method Summary
MoPs uses a gating function to dynamically select and combine specialized prompt groups based on the input task. The gating function embeds the input using early LLM layers and outputs expert scores that scale the attention of selected prompts in later layers. This allows only relevant prompt groups to be updated during backpropagation, reducing interference from heterogeneous tasks. The approach is evaluated on compressed LLMs across centralized and federated learning scenarios using datasets like Databricks Dolly 15k and Super-Natural Instructions.

## Key Results
- Reduces perplexity by up to 70% in federated learning scenarios
- Maintains effectiveness across model compression techniques (pruning and quantization)
- Achieves 30% perplexity reduction in centralized learning compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mixture of Prompts reduces training interference by dynamically assigning specialized prompt groups to tasks based on learned gating scores.
- Mechanism: A lightweight MLP gating function embeds the input question using early LLM layers and outputs expert scores. These scores scale the attention of selected prompts in later layers, allowing only relevant prompt groups to be updated during backpropagation.
- Core assumption: Training interference arises when heterogeneous tasks cause conflicting gradient updates across shared prompt parameters.
- Evidence anchors: [abstract] "MoPs can simultaneously mitigate prompt training 'interference' in multi-task, multi-source scenarios"; [section 3.1] "Our hypothesis is that training prompts to handle universally multi-source multi-task scenarios might result in prompt interference across tasks and across sources"
- Break condition: If tasks are too similar, gating may not provide benefit and could even add noise by unnecessarily partitioning parameters.

### Mechanism 2
- Claim: MoPs maintain effectiveness across model compression techniques by isolating recovery of lost capacity to dedicated prompt experts.
- Mechanism: When compression removes model capacity, each prompt expert is trained to recover a specific set of skills, so the gating function can combine experts to reconstruct the full functionality without interference between recovery and task adaptation.
- Core assumption: Compressed models lose capacity in task-agnostic ways, and specialized prompts can learn to recover these losses independently.
- Evidence anchors: [abstract] "MoPs are empirically agnostic to any model compression technique applied"; [section 3] "Our hypothesis is that training prompts to handle universally multi-source multi-task scenarios might result in prompt interference across tasks and across sources"
- Break condition: If compression ratio is extreme (e.g., >90% unstructured pruning), the gating network's embedding layers may be too degraded to produce reliable expert scores.

### Mechanism 3
- Claim: Pretraining the gating function on unlabeled instruction data improves initial prompt-expert assignment, reducing cold-start interference.
- Mechanism: By assigning a one-to-one mapping between prompt groups and task domains during pretraining, the gating function starts with a reasonable baseline that can then be refined through fine-tuning.
- Core assumption: Initial rough alignment between prompts and tasks provides a better starting point than random initialization, even if imperfect.
- Evidence anchors: [section 3.1] "To improve the initial performance of our gating function, we assume we have unlabeled instruction data... We use this data to pretrain the gating function by manually assigning a one-to-one relationship between each prompt group and each data domain/task"; [section 4] "We observe that the pretraining step helps the gating function to roughly distinguish between data domains/tasks"
- Break condition: If the unlabeled data distribution is too different from downstream tasks, pretraining could mislead the gating function and slow convergence.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) routing mechanisms
  - Why needed here: MoPs extend the MoE idea from model parameters to prompts, so understanding how gating functions route inputs to experts is foundational
  - Quick check question: In a standard MoE, what ensures only a subset of experts is activated per forward pass?

- Concept: Prompt-based fine-tuning and parameter-efficient learning
  - Why needed here: MoPs operate in the prompt-tuning paradigm, so understanding how soft prompts are injected and trained is essential
  - Quick check question: How does prompt injection in middle layers differ from injecting prompts only at the first layer?

- Concept: Federated learning synchronization and model drift
  - Why needed here: MoPs are evaluated in federated settings, so understanding how local updates and aggregation affect model convergence is critical
  - Quick check question: What causes model drift in federated learning, and how might selective prompt updates mitigate it?

## Architecture Onboarding

- Component map: Input → Early layers → Gating scores → Expert selection → Prompt attention scaling → Final layers → Output

- Critical path: Input → Early layers → Gating scores → Expert selection → Prompt attention scaling → Final layers → Output

- Design tradeoffs:
  - Injecting prompts in middle layers reduces training cost but may limit early-layer influence
  - Using 7 experts balances diversity vs. parameter count; too few may underfit, too many may overfit
  - Pretraining gating helps cold-start but requires unlabeled data availability

- Failure signatures:
  - Gating scores collapse to uniform (no specialization)
  - All prompts receive similar gradients (interference not reduced)
  - Performance degrades sharply with high pruning ratios (>85%)

- First 3 experiments:
  1. Run MoPs on Dolly-15k with 85% unstructured pruning; verify PPL reduction vs baseline
  2. Disable pretraining; compare gating function's ability to separate task domains
  3. Test gating scores distribution on federated clients; check for model drift mitigation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Mixture of Prompts (MoPs) approach scale with an increasing number of tasks or prompt experts?
- Basis in paper: [inferred] The paper mentions that the MoPs approach uses multiple trainable prompts as experts, but does not explore how the system scales with a larger number of tasks or experts.
- Why unresolved: The paper does not provide experiments or analysis on the scalability of the MoPs approach with a larger number of tasks or experts, leaving the question of how well the method performs in more complex scenarios unanswered.
- What evidence would resolve it: Conducting experiments with a larger number of tasks and prompt experts, and analyzing the performance, efficiency, and effectiveness of the MoPs approach in these scenarios, would help answer this question.

### Open Question 2
- Question: How does the performance of MoPs compare to other parameter-efficient fine-tuning (PEFT) methods, such as LoRA or prefix tuning, in terms of task adaptation and efficiency?
- Basis in paper: [explicit] The paper mentions that MoPs is compared to baseline approaches like FedPrompt and Xu et al.'s method, but does not directly compare its performance to other PEFT methods like LoRA or prefix tuning.
- Why unresolved: The paper does not provide a comprehensive comparison of MoPs with other PEFT methods, making it difficult to assess its relative strengths and weaknesses in terms of task adaptation and efficiency.
- What evidence would resolve it: Conducting experiments that directly compare MoPs to other PEFT methods, such as LoRA or prefix tuning, in terms of task adaptation and efficiency, would help answer this question.

### Open Question 3
- Question: How does the MoPs approach handle tasks that require a combination of multiple skills or prompt experts?
- Basis in paper: [explicit] The paper mentions that MoPs dynamically selects and combines prompt experts based on the input task, but does not explore how the method handles tasks that require a combination of multiple skills or experts.
- Why unresolved: The paper does not provide experiments or analysis on how MoPs handles tasks that require a combination of multiple skills or prompt experts, leaving the question of how well the method performs in these scenarios unanswered.
- What evidence would resolve it: Conducting experiments that involve tasks requiring a combination of multiple skills or prompt experts, and analyzing the performance and effectiveness of the MoPs approach in these scenarios, would help answer this question.

## Limitations
- Claims about training interference reduction are supported primarily through performance improvements rather than direct empirical evidence of the underlying mechanisms
- The method's effectiveness across extreme compression ratios (>85% pruning) remains unclear
- Pretraining benefits for the gating function lack quantitative comparison between pretrained and randomly initialized performance

## Confidence
- **High confidence**: Perplexity reduction metrics in centralized and federated settings, concrete architectural details (7 expert groups × 10 prompts, middle-layer injection)
- **Medium confidence**: Claims about interference mitigation and compression resilience, supported indirectly through performance gains but lacking mechanistic validation
- **Low confidence**: Pretraining benefits for the gating function, as no quantitative comparison is provided between pretrained and randomly initialized gating performance

## Next Checks
1. **Mechanistic validation**: Instrument the model to measure gradient orthogonality across prompt groups during multi-task training, directly quantifying interference reduction rather than inferring it from perplexity gains.

2. **Compression boundary testing**: Systematically evaluate MoPs performance across a broader range of pruning ratios (50% to 95%) to identify the threshold where gating function degradation causes performance collapse.

3. **Pretraining ablation study**: Compare MoPs with and without gating function pretraining on a held-out dataset where task boundaries are known, measuring the gating function's classification accuracy and correlation with final task performance.