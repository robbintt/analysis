---
ver: rpa2
title: How User Language Affects Conflict Fatality Estimates in ChatGPT
arxiv_id: '2308.00072'
source_url: https://arxiv.org/abs/2308.00072
tags:
- language
- information
- chatgpt
- bias
- conflict
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates language biases in ChatGPT's responses
  about conflict-related violence, focusing on Israeli-Palestinian and Turkish-Kurdish
  conflicts. Using GPT-3.5, researchers developed an automated query system to ask
  about airstrikes in both Hebrew/Arabic and Turkish/Kurdish, obtaining 10 estimates
  per event.
---

# How User Language Affects Conflict Fatality Estimates in ChatGPT

## Quick Facts
- arXiv ID: 2308.00072
- Source URL: https://arxiv.org/abs/2308.00072
- Authors: 
- Reference count: 12
- Primary result: ChatGPT provides 27±11% lower fatality estimates when queried in attacker's language versus targeted group's language

## Executive Summary
This study reveals significant language biases in ChatGPT's responses about conflict-related violence, showing that fatality estimates vary dramatically depending on whether queries are made in the attacker's or targeted group's language. Using automated queries across Israeli-Palestinian and Turkish-Kurdish conflict contexts, researchers found that ChatGPT systematically underreports fatalities when questioned in Hebrew or Turkish (attacker languages) compared to Arabic or Kurdish (targeted group languages). The study also identifies evasive answers denying event existence as a novel bias mechanism that amplifies these discrepancies beyond what traditional search engines exhibit.

## Method Summary
The researchers developed an automated query system using GPT-3.5-turbo to ask about 10 airstrikes from each of two conflicts (Israeli-Palestinian and Turkish-Kurdish) drawn from the UCDP Georeferenced Events Dataset. They translated question prompts into both languages of each conflict pair (Hebrew/Arabic and Turkish/Kurdish), then queried ChatGPT 10 times per event with temperature 0.6 to capture variability. Responses were translated back to English and manually coded for fatality counts, evasive answers, and word frequencies. The study compared numeric estimates and thematic content across languages to identify systematic biases.

## Key Results
- ChatGPT provides 27±11% lower fatality estimates when queried in attacker's language versus targeted group's language
- Evasive answers denying attack occurrences are significantly more common in attacker's language, creating a novel bias mechanism
- The term "terrorist" appears more than 6 times more frequently in Hebrew responses compared to Arabic responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT reproduces language-specific media biases present in its training data.
- Mechanism: The model's responses reflect the relative prominence of information in language-specific training sources; if attacker-related fatalities are underreported in attacker-language sources, the model underreports them when queried in that language.
- Core assumption: The training corpus contains language-specific coverage biases that mirror those of media sources in each language.
- Evidence anchors:
  - [abstract] "This language bias has the potential to amplify existing media biases and contribute to information bubbles, ultimately reinforcing conflicts."
  - [section] "We expect that the information available in the training data on events of conflict-related violence will differ systematically across languages."
  - [corpus] Weak/no direct citation of training corpus composition; only general claims about English dominance and language-dependent performance.
- Break condition: If the model's outputs are dominated by multilingual knowledge fusion rather than language-specific training data, the mechanism fails.

### Mechanism 2
- Claim: Language bias is amplified by evasive answers that deny event existence in the attacker's language.
- Mechanism: When the model's training data contains fewer mentions of an event in one language, it is more likely to refuse to answer or claim ignorance, producing an apparent "zero fatalities" impression.
- Core assumption: The model's response strategy includes evasion when confidence in an event's existence is low, and this evasion is language-dependent.
- Evidence anchors:
  - [abstract] "Evasive answers denying the existence of such attacks further increase the discrepancy, creating a novel bias mechanism not present in regular search engines."
  - [section] "When the number of media mentions of an event falls below a certain threshold (typically in the attacker's language), GPT-3.5 starts to mention other events or simply denies its existence."
  - [corpus] No direct citation of internal model decision thresholds; inferred from observed behavior.
- Break condition: If evasion is uniformly applied regardless of language, the amplification effect disappears.

### Mechanism 3
- Claim: Language bias shapes perceptions of indiscriminate violence, affecting civilian casualty reporting.
- Mechanism: Descriptive language and framing differ by language in training data, so responses vary in emphasis on civilian vs. military casualties, with attacker-language responses less likely to emphasize civilian harm.
- Core assumption: Training data contains differential framing of violence (e.g., "terrorist" vs. "civilian") that the model reproduces in its responses.
- Evidence anchors:
  - [abstract] "The term 'terrorist' is mentioned more than 6 times more frequently in the Hebrew responses compared to the Arabic responses."
  - [section] "GPT-3.5's responses in Arabic are also more likely to emphasize that these airstrikes violated international law."
  - [corpus] No corpus-level word-frequency analysis cited; only manual coding of responses.
- Break condition: If framing is neutral or uniform across languages in the training corpus, this mechanism fails.

## Foundational Learning

- Concept: Media bias and information warfare in conflict contexts.
  - Why needed here: The study's premise rests on the assumption that media sources in different languages report conflict events differently, and that this bias is reflected in the model.
  - Quick check question: Why would the same event be reported differently in Hebrew vs. Arabic media?

- Concept: Training data composition and language imbalance in LLMs.
  - Why needed here: The mechanism depends on understanding that ChatGPT's training corpus is heavily English-biased, with less data for other languages, leading to poorer performance and potential bias in non-English responses.
  - Quick check question: How does the relative size of training data in a language affect the quality and bias of responses in that language?

- Concept: Evasion and refusal strategies in LLM responses.
  - Why needed here: The study identifies evasive answers as a novel bias mechanism; understanding when and why models refuse to answer is critical to interpreting the results.
  - Quick check question: Under what conditions does ChatGPT choose to refuse an answer rather than provide uncertain information?

## Architecture Onboarding

- Component map: Automated query script -> OpenAI API -> GPT-3.5-turbo model -> translated responses -> manual coding -> statistical analysis
- Critical path:
  1. Generate prompts from UCDP GED airstrike data
  2. Translate prompt to target language
  3. Query GPT-3.5 with system role and translated prompt
  4. Translate response back to English
  5. Log and code responses
  6. Aggregate and analyze results
- Design tradeoffs:
  - Automation vs. nuance: Fully automated translation and querying increases scalability but risks subtle meaning shifts
  - Single-shot vs. conversation: Using isolated prompts avoids memory effects but may miss context that could improve accuracy
  - Temperature setting: Higher temperature increases variability but may introduce noise; lower temperature may reduce linguistic diversity
- Failure signatures:
  - Inconsistent translation quality leading to mismatched prompts
  - Over-reliance on evasive answers in one language masking true fatality counts
  - Manual coding errors introducing bias in the analysis
- First 3 experiments:
  1. Validate translation consistency: Run parallel translations of the same prompt in both languages and check for semantic drift
  2. Test temperature effects: Vary temperature settings and measure changes in response variability and evasive answer frequency
  3. Probe evasion boundaries: Design prompts that test the model's refusal thresholds in both languages to map the "denial zone" for low-coverage events

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does ChatGPT's language bias extend beyond conflict-related topics to other sensitive areas like political ideologies, religious beliefs, or cultural identities?
- Basis in paper: [explicit] The authors note that "It is plausible that similar language biases affect information generated by GPT-3.5 in other topic areas" and that "This is likely to be the case for other areas of contested information such as sensitive political issues, religious beliefs, or cultural identities."
- Why unresolved: The study focused specifically on conflict-related violence and did not test other sensitive topics.
- What evidence would resolve it: Systematic testing of ChatGPT's responses across multiple sensitive topics in different languages to identify consistent bias patterns.

### Open Question 2
- Question: Which specific languages are most susceptible to ChatGPT's language bias, and how does this susceptibility relate to the amount of training data available for each language?
- Basis in paper: [explicit] The authors mention that "The training data—a mixture of different sources, including a copy of open access internet data (Common Crawl), an overview of open source books, and Wikipedia—is heavily biased towards the English language (over 50%)" and that "the performance of language models depends on the amount of training data."
- Why unresolved: The study only tested Hebrew/Arabic and Turkish/Kurdish, but did not systematically compare across a broader range of languages.
- What evidence would resolve it: Testing ChatGPT across multiple language pairs with varying amounts of training data to establish a correlation between data availability and bias strength.

### Open Question 3
- Question: Do other large language models exhibit similar language biases, or is this phenomenon specific to ChatGPT?
- Basis in paper: [inferred] The authors suggest this is "the first evidence of language bias" but do not compare across different models.
- Why unresolved: The study only tested GPT-3.5 and did not examine other competing language models.
- What evidence would resolve it: Systematic testing of multiple large language models across the same conflict scenarios and languages to compare bias patterns.

## Limitations
- The study only examines two conflict contexts and two language pairs, limiting generalizability to other regions or languages
- The translation process introduces potential semantic drift that could affect results
- The model's internal decision-making process for evasion and fatality estimation is not fully transparent

## Confidence
- High confidence: The existence of language-dependent fatality estimate differences (27±11% lower in attacker's language)
- Medium confidence: The mechanism of evasive answers as a novel bias amplifier
- Medium confidence: The reproduction of existing media framing biases in responses

## Next Checks
1. **Cross-linguistic consistency test**: Repeat the study with additional conflict events and language pairs (e.g., Russian/Ukrainian, Spanish/English) to assess generalizability of the language bias effect.
2. **Translation fidelity audit**: Implement a blinded review where native speakers validate that translated prompts and responses preserve original meaning across all language pairs.
3. **Model variation analysis**: Compare results across different LLM architectures (GPT-4, Claude, LLaMA) to determine if the bias is specific to ChatGPT's training or a broader LLM phenomenon.