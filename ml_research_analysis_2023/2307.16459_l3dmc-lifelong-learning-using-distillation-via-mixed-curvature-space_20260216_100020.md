---
ver: rpa2
title: 'L3DMC: Lifelong Learning using Distillation via Mixed-Curvature Space'
arxiv_id: '2307.16459'
source_url: https://arxiv.org/abs/2307.16459
tags:
- learning
- space
- distillation
- hyperbolic
- kernel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces L3DMC, a novel distillation strategy for
  lifelong learning that operates on mixed-curvature spaces to preserve geometrical
  structures. The key idea is to embed low-dimensional embeddings from Euclidean and
  hyperbolic spaces into a higher-dimensional Reproducing Kernel Hilbert Space using
  a positive-definite kernel function.
---

# L3DMC: Lifelong Learning using Distillation via Mixed-Curvature Space

## Quick Facts
- **arXiv ID**: 2307.16459
- **Source URL**: https://arxiv.org/abs/2307.16459
- **Reference count**: 39
- **Primary result**: L3DMC achieves average accuracies of 77.26%, 67.52%, and 76.46% on BloodMNIST, PathMNIST, and OrganaMNIST respectively, with significantly lower forgetting compared to baseline methods.

## Executive Summary
This paper introduces L3DMC, a novel lifelong learning method that addresses catastrophic forgetting by leveraging mixed-curvature spaces and Reproducing Kernel Hilbert Space (RKHS) embeddings. The key innovation is combining Euclidean and hyperbolic spaces through a distillation strategy that preserves complex geometric structures during sequential task learning. Experiments on three medical image benchmarks demonstrate that L3DMC outperforms state-of-the-art methods by effectively capturing hierarchical and grid-like data structures present in medical images while maintaining knowledge of previously learned tasks.

## Method Summary
L3DMC operates by projecting input features into both Euclidean and hyperbolic embedding spaces using separate MLPs, then embedding these into RKHS using positive-definite kernel functions. The model is trained by minimizing the distance between new sample representations and subspaces constructed from old representations in RKHS, effectively preserving old knowledge while learning new concepts. The method uses a memory buffer for exemplar replay and combines this distillation loss with cross-entropy loss during training. The mixed-curvature approach allows the model to capture both hierarchical structures (via hyperbolic space) and grid-like patterns (via Euclidean space) present in medical images.

## Key Results
- Achieves average accuracy of 77.26% on BloodMNIST with significantly reduced forgetting compared to baselines
- Maintains 67.52% average accuracy on PathMNIST while preserving knowledge of previous tasks
- Demonstrates 76.46% average accuracy on OrganaMNIST with effective prevention of catastrophic forgetting
- Shows consistent improvement over fixed-curvature approaches across all three medical image datasets

## Why This Works (Mechanism)

### Mechanism 1
L3DMC reduces catastrophic forgetting by minimizing the distance between new embeddings and a subspace spanned by old embeddings in RKHS. The model projects embeddings from Euclidean and hyperbolic spaces into RKHS using positive-definite kernels (Gaussian RBF for Euclidean, modified RBF for hyperbolic via tangent space mapping). It then minimizes the discrepancy between the new embedding and the subspace formed by old embeddings in RKHS. The core assumption is that the data manifold for the old model is well-approximated by a low-dimensional hyperplane in RKHS, enabling effective distillation.

### Mechanism 2
Using mixed-curvature spaces (Euclidean + hyperbolic) captures complex geometric structures better than fixed-curvature spaces alone. Embeddings from both Euclidean (zero curvature) and hyperbolic (negative curvature) spaces are combined, allowing the model to represent both grid-like and hierarchical data structures present in medical images. The core assumption is that medical image distributions contain complex geometric structures that benefit from modeling in both Euclidean and hyperbolic spaces.

### Mechanism 3
Performing distillation in higher-dimensional RKHS provides richer representation than direct low-dimensional embedding constraints. Instead of applying constraints directly on low-dimensional embeddings (which can overly stabilize the model and hinder learning new concepts), L3DMC embeds these into RKHS where the model can preserve more latent information while still maintaining old knowledge. The core assumption is that lower-dimensional embedding spaces may not preserve all latent information in the input data, making RKHS a better space for distillation.

## Foundational Learning

- **Concept**: Lifelong Learning (L3) and Catastrophic Forgetting
  - Why needed here: Understanding the problem L3DMC addresses—degradation of performance on old tasks when learning new ones—is essential to grasp the motivation and effectiveness of the method.
  - Quick check question: What is catastrophic forgetting, and why is it a problem in lifelong learning settings?

- **Concept**: Mixed-Curvature Spaces and Product Manifolds
  - Why needed here: L3DMC operates on a space combining Euclidean and hyperbolic geometries; understanding these concepts is crucial to comprehend how the method models complex data structures.
  - Quick check question: How do Euclidean and hyperbolic spaces differ in terms of curvature, and why might combining them be beneficial for modeling data?

- **Concept**: Reproducing Kernel Hilbert Spaces (RKHS) and Kernel Methods
  - Why needed here: L3DMC uses RKHS to embed low-dimensional embeddings into a higher-dimensional space for richer representation; understanding RKHS is key to understanding the method's approach to distillation.
  - Quick check question: What is an RKHS, and how does using a kernel function enable embedding data into a higher-dimensional space?

## Architecture Onboarding

- **Component map**: Feature Extractor -> Projection Modules (ge, gh) -> Kernel Functions -> RKHS Embedding -> Distillation Loss -> Classifier
- **Critical path**: 1) Extract features from input images using the feature extractor. 2) Project features into Euclidean and hyperbolic embeddings using respective MLPs. 3) Embed these projections into RKHS using kernel functions. 4) Compute the distillation loss by measuring the discrepancy between new embeddings and subspaces formed by old embeddings in RKHS. 5) Combine distillation loss with cross-entropy loss and update model parameters. 6) Store exemplars in memory for future replay.
- **Design tradeoffs**: Mixed-curvature vs. single curvature (added complexity vs. better structure capture), RKHS vs. direct embedding (richer representation vs. computational cost), memory size (reduced forgetting vs. increased storage).
- **Failure signatures**: High forgetting on old tasks despite training (distillation not effective), poor performance on new tasks (overly constrained by old knowledge), computational inefficiency (excessive training time due to RKHS operations).
- **First 3 experiments**: 1) Ablation study: Remove the hyperbolic projection module and train only with Euclidean space. 2) Kernel comparison: Replace the RBF kernel with a different positive-definite kernel. 3) Memory scaling: Vary the size of the memory buffer and measure its effect on average accuracy and forgetting.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the proposed L3DMC distillation strategy perform in long-task or memory-free continual learning scenarios?
- **Basis in paper**: [explicit] The paper mentions this as a future research direction, stating "In future, we would like to explore the effectiveness of our proposed distillation strategy on long-task and memory-free L3 setting."
- **Why unresolved**: The current experimental setup relies on a fixed-size memory buffer to store exemplars from previous tasks. The paper does not provide any results or analysis on scenarios where memory is unlimited or unavailable.
- **What evidence would resolve it**: Experimental results comparing L3DMC's performance with and without memory constraints across varying numbers of tasks, ideally on datasets with more than 4 tasks.

### Open Question 2
- **Question**: What is the theoretical upper bound on the number of tasks that can be learned sequentially using L3DMC before catastrophic forgetting becomes prohibitive?
- **Basis in paper**: [inferred] While the paper demonstrates effectiveness on 4-task settings, it does not provide any analysis on the scalability of the approach to longer task sequences. The mixed-curvature space approach is presented as theoretically robust, but no theoretical analysis of its limits is provided.
- **Why unresolved**: The paper only evaluates on datasets with 4 tasks. No theoretical analysis is provided to determine the maximum number of tasks before performance degradation becomes unacceptable.
- **What evidence would resolve it**: A comprehensive study testing L3DMC across different numbers of tasks (e.g., 4, 8, 16, 32) with corresponding analysis of accuracy degradation rates and theoretical analysis of the mixed-curvature space's capacity to preserve knowledge across many tasks.

### Open Question 3
- **Question**: How does the computational complexity of L3DMC scale with increasing task diversity and buffer size?
- **Basis in paper**: [explicit] The paper mentions that computing the inverse of an m × m matrix is required with complexity O(m³), which needs to be done once per batch and manifold. However, it does not analyze how this scales with more diverse tasks or larger buffer sizes.
- **Why unresolved**: The paper provides implementation details but does not analyze the computational overhead as task diversity increases or as the memory buffer grows. The trade-off between improved performance and increased computation is not quantified.
- **What evidence would resolve it**: Detailed computational complexity analysis showing runtime and memory usage as functions of task diversity (measured by inter-task similarity) and buffer size, ideally with empirical benchmarks comparing L3DMC to baseline methods across these dimensions.

## Limitations

- **Computational complexity**: The RKHS operations, particularly matrix inversion, introduce significant computational overhead that may limit scalability to larger datasets or more complex models.
- **Hyperparameter sensitivity**: The choice of kernel functions and their parameters significantly affects performance, requiring careful tuning for different datasets.
- **Memory dependence**: The method relies on a memory buffer for exemplar replay, which may not be practical in all deployment scenarios where storage is constrained.

## Confidence

- **Method novelty**: High - The combination of mixed-curvature spaces with RKHS embedding for lifelong learning is a novel approach.
- **Experimental validation**: Medium - While results are promising on three medical datasets, testing on only 4-task sequences limits generalizability.
- **Theoretical grounding**: Medium - The method is theoretically motivated but lacks formal analysis of its limitations and bounds.
- **Reproducibility**: Medium - Key implementation details are provided, but some architectural specifics (MLP dimensions, kernel parameter settings) may affect results.

## Next Checks

1. Conduct ablation studies to isolate the contributions of mixed-curvature spaces versus fixed-curvature approaches on non-medical datasets.
2. Evaluate the method's performance on datasets with more than 4 tasks to assess scalability and identify performance degradation patterns.
3. Analyze the computational overhead introduced by RKHS operations through empirical benchmarks comparing L3DMC to baseline methods with varying dataset sizes and buffer configurations.