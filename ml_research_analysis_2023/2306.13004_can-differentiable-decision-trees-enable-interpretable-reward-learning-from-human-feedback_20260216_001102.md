---
ver: rpa2
title: Can Differentiable Decision Trees Enable Interpretable Reward Learning from
  Human Feedback?
arxiv_id: '2306.13004'
source_url: https://arxiv.org/abs/2306.13004
tags:
- reward
- learning
- node
- leaf
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces an approach to learn interpretable reward
  functions from human preferences using Differentiable Decision Trees (DDTs). Unlike
  traditional black-box reward models, DDTs provide an interpretable tree structure
  that explicitly breaks down reward predictions into routing decisions.
---

# Can Differentiable Decision Trees Enable Interpretable Reward Learning from Human Feedback?

## Quick Facts
- **arXiv ID:** 2306.13004
- **Source URL:** https://arxiv.org/abs/2306.13004
- **Reference count:** 40
- **Primary result:** DDTs learn interpretable reward functions from human preferences while achieving RL performance comparable to neural networks

## Executive Summary
This paper introduces Differentiable Decision Trees (DDTs) for learning interpretable reward functions from human preferences. Unlike traditional black-box reward models, DDTs provide an interpretable tree structure that explicitly breaks down reward predictions into routing decisions. The authors train DDTs on trajectory preference data across several environments: Cartpole, visual gridworld tasks with MNIST digits, and Atari games. They show that DDTs can learn interpretable rewards while achieving RL performance comparable to larger neural networks, especially when using soft outputs.

## Method Summary
The method trains DDTs using pairwise trajectory preferences and the Bradley-Terry preference model. DDTs use soft routing probabilities at internal nodes (computed via sigmoid over linear transformations) to determine path distributions, with final rewards as weighted sums over leaves. The approach supports both simple (linear) and sophisticated (convolutional) internal nodes, and offers a choice between soft outputs (weighted average) and hard outputs (argmax routing) for RL evaluation.

## Key Results
- DDTs successfully learn interpretable reward functions from preference data in Cartpole, MNIST gridworld, and Atari environments
- Soft outputs achieve better RL performance than hard outputs while maintaining reasonable interpretability
- DDTs can detect reward misalignment by visualizing routing probabilities at internal nodes
- DDTs match or exceed neural network performance on RL tasks while providing interpretable decision boundaries

## Why This Works (Mechanism)

### Mechanism 1
DDTs learn interpretable reward functions by breaking predictions into a finite sequence of soft routing decisions. The soft routing probability at each internal node is computed using a sigmoid over a learnable linear transformation of the input. This probability determines the path distribution across the tree, and the final reward is a weighted sum over all leaves based on these path probabilities.

### Mechanism 2
Using soft outputs (weighted average over leaves) achieves better RL performance than hard argmax outputs while maintaining reasonable interpretability. At test time, the reward is computed as the expectation over all leaf rewards weighted by their path probabilities. This smooths the reward landscape and reduces overfitting to particular routing paths.

### Mechanism 3
DDTs can detect reward misalignment by visualizing routing probabilities at internal nodes. Heatmaps of routing probabilities as a function of input features reveal which features the tree uses for decisions. Missing or weak decision boundaries for important features indicate misalignment.

## Foundational Learning

- **Differentiable decision trees (DDTs)**: Why needed - combine interpretability of decision trees with learnability of neural networks for explainable rewards; Quick check - What is the role of the sigmoid activation in internal nodes of a DDT?
- **Soft routing probabilities**: Why needed - allows gradient-based learning while preserving probabilistic interpretation of decision paths; Quick check - How does the path probability from root to leaf influence the final reward prediction?
- **Bradley-Terry preference model**: Why needed - converts pairwise trajectory preferences into differentiable loss suitable for training DDTs without explicit reward labels; Quick check - Why is the Bradley-Terry model appropriate for preference-based reward learning?

## Architecture Onboarding

- **Component map**: Input → Internal nodes (simple or sophisticated) → Routing probabilities → Leaf nodes → Reward output (soft or hard)
- **Critical path**: 1) Construct DDT with specified depth and node types, 2) Generate pairwise preference labels from trajectories, 3) Train DDT using Bradley-Terry loss, 4) Visualize routing heatmaps for interpretability, 5) Use soft or hard output for RL evaluation
- **Design tradeoffs**: Simple internal nodes (easier to interpret, may underperform on high-dim inputs) vs sophisticated internal nodes (better performance, harder to interpret); Soft outputs (better RL performance, less interpretable) vs hard outputs (more interpretable, potentially worse RL performance)
- **Failure signatures**: Equal routing probabilities at internal nodes (DDT not learning discriminative features), leaf nodes not specializing (poor reward discrimination), heatmaps uninformative (DDT not capturing relevant features)
- **First 3 experiments**: 1) Train depth-1 DDT on MNIST (0/1) gridworld; visualize heatmaps to verify digit separation, 2) Compare soft vs hard outputs on CartPole; check RL performance and interpretability, 3) Train DDT with penalty regularization on Atari; test impact on routing balance and RL score

## Open Questions the Paper Calls Out

### Open Question 1
Does the interpretability advantage of DDTs persist when applied to more complex environments with continuous state spaces and more nuanced reward structures? The paper tests DDTs on Cartpole, MNIST Gridworld, and Atari games, but these environments have relatively simple reward structures. More complex environments with continuous, multi-faceted rewards would require deeper analysis of how well DDTs maintain interpretability.

### Open Question 2
How does the performance of DDTs compare to other interpretable reward learning methods, such as decision trees trained with explicit reward labels or symbolic reward learning approaches? The paper compares DDTs to neural networks but does not compare them to other interpretable reward learning methods like decision trees trained with explicit reward labels or symbolic approaches.

### Open Question 3
Can the interpretability of DDTs be further improved by incorporating domain-specific knowledge or constraints into the tree structure? The paper mentions that DDTs can learn interpretable features, but it doesn't explore how incorporating domain knowledge (e.g., physics constraints in Cartpole or spatial relationships in gridworld) could improve interpretability.

## Limitations
- Qualitative nature of interpretability assessment for high-dimensional Atari inputs
- Penalty regularization strength (λ = 0.0001) lacks systematic sensitivity analysis
- Focus on interpretability rather than absolute performance comparison with neural networks

## Confidence
- **High**: DDTs can learn from preference data and achieve interpretable routing decisions in low-dimensional spaces (Cartpole, MNIST gridworld)
- **Medium**: Soft outputs provide better RL performance than hard outputs while maintaining interpretability
- **Low**: Penalty regularization effectively prevents unbalanced routing in Atari DDTs without degrading performance

## Next Checks
1. Conduct systematic ablation studies varying λ regularization strength across multiple orders of magnitude to establish optimal values and test sensitivity
2. Implement quantitative metrics for interpretability (e.g., decision boundary complexity, feature importance consistency) to complement qualitative heatmap visualizations
3. Compare DDT performance against ensemble methods or deeper neural networks to establish expressiveness limits and identify domains where interpretability tradeoffs become necessary