---
ver: rpa2
title: 'Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability
  by Cross-Lingual-Thought Prompting'
arxiv_id: '2305.07004'
source_url: https://arxiv.org/abs/2305.07004
tags:
- answer
- language
- prompt
- should
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces cross-lingual-thought prompting (XLT), a generic
  template prompt that stimulates cross-lingual and logical reasoning skills to improve
  the multilingual capability of large language models (LLMs). XLT guides models through
  problem understanding, cross-lingual thinking, task analysis, task execution, and
  output formatting.
---

# Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting

## Quick Facts
- arXiv ID: 2305.07004
- Source URL: https://arxiv.org/abs/2305.07004
- Reference count: 40
- Key outcome: XLT improves multilingual LLM performance by 10+ points on average across 7 benchmarks

## Executive Summary
This paper introduces cross-lingual-thought prompting (XLT), a generic template prompt that enhances multilingual capability of large language models by stimulating cross-lingual and logical reasoning skills. The approach guides models through problem understanding, cross-lingual thinking, task analysis, task execution, and output formatting. Evaluations on 7 benchmarks covering 27 languages demonstrate significant performance improvements, with over 10 points of average improvement in arithmetic reasoning and open-domain question-answering tasks. XLT also reduces performance gaps between languages, showing potential for democratizing language intelligence.

## Method Summary
XLT is a six-instruction template prompt that works with zero-shot learning and can be extended to few-shot learning. The template guides LLMs through role assignment, task input, cross-lingual thinking (rephrasing tasks in English), task analysis, chain-of-thought task solving, and output formatting. The method uses structured task metadata with seven placeholders and requires no parameter updates. XLT is evaluated across multiple benchmarks including MGSM, XCOPA, XNLI, PAWS-X, MKQA, XL-Sum, and FLORES using GPT-3.5 models.

## Key Results
- XLT improves multilingual LLM performance by 10+ points on average across all evaluated tasks
- The method reduces performance gaps between high-resource and low-resource languages
- Cross-lingual thinking instruction consistently improves task performance across diverse multilingual benchmarks
- XLT achieves significant improvements in arithmetic reasoning and open-domain question-answering tasks

## Why This Works (Mechanism)

### Mechanism 1
Cross-lingual thinking instruction activates LLM's English-based knowledge to solve multilingual tasks. By requiring the model to rephrase the task request in English before solving, it leverages the stronger performance and knowledge base available in English during pre-training.

### Mechanism 2
Step-by-step reasoning chain-of-thought improves task performance across languages. The "CoT Task Solving" instruction forces the model to break down problems into intermediate steps, leveraging its strong ability to maintain logical chains.

### Mechanism 3
Role assignment and task analysis instructions improve task understanding and transfer. Explicitly assigning roles and analyzing task goals helps models understand the abstract task requirements and apply appropriate knowledge.

## Foundational Learning

- Concept: Prompt engineering and in-context learning
  - Why needed here: XLT is fundamentally a sophisticated prompting technique that relies on precise instruction formatting
  - Quick check question: What's the difference between zero-shot and few-shot prompting in terms of demonstration construction?

- Concept: Chain-of-thought reasoning
  - Why needed here: The step-by-step instruction design is based on CoT prompting principles
  - Quick check question: How does breaking a problem into intermediate steps help LLMs solve complex reasoning tasks?

- Concept: Multilingual model behavior and language bias
  - Why needed here: Understanding why cross-lingual thinking works requires knowing about language distribution in training data
  - Quick check question: Why might English be a better pivot language for cross-lingual reasoning than other high-resource languages?

## Architecture Onboarding

- Component map:
  - Task request -> Task metadata matching -> XLT template filling -> LLM processing -> Formatted output

- Critical path:
  1. Receive task request with language information
  2. Match to task metadata and fill XLT placeholders
  3. Generate prompt and feed to LLM
  4. Process and format output according to constraints
  5. Evaluate performance across languages

- Design tradeoffs:
  - Single-turn vs multi-turn conversation (simplicity vs thoroughness)
  - Fixed template vs task-specific customization (generality vs optimization)
  - English pivot language vs direct multilingual processing (leveraging strengths vs avoiding translation errors)

- Failure signatures:
  - Performance drops in languages where English-based reasoning doesn't transfer well
  - Degradation when cross-lingual thinking introduces semantic drift
  - Inconsistent results across different instruction orderings

- First 3 experiments:
  1. Compare XLT performance against basic prompt on MGSM benchmark with gpt-3.5-turbo
  2. Test ablation of cross-lingual thinking instruction on XCOPA benchmark
  3. Evaluate few-shot learning with different demonstration construction methods on arithmetic reasoning tasks

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored based on the evaluation methodology and results presented.

## Limitations
- Template generality may not extend to all task types beyond the 7 evaluated benchmarks
- Performance improvements across different language resource levels are not separately analyzed
- The specific mechanisms behind cross-lingual thinking effectiveness require further validation

## Confidence

**High Confidence**: XLT significantly improves multilingual performance across multiple benchmarks, cross-lingual thinking instruction is novel and effective, template structure provides consistent gains

**Medium Confidence**: Step-by-step reasoning improves task performance across languages, role assignment and task analysis improve task understanding, performance gap between languages is reduced

**Low Confidence**: Cross-lingual thinking leverages English knowledge base for all tasks, template will generalize to arbitrary task types, XLT provides equitable benefits across all language resource levels

## Next Checks
1. Conduct systematic analysis of XLT performance across languages stratified by resource levels to verify equitable democratization claims
2. Test XLT on additional task categories not covered in current evaluation to assess template generality limits
3. Design experiments that explicitly measure whether cross-lingual thinking instruction improves English-to-other-language knowledge transfer