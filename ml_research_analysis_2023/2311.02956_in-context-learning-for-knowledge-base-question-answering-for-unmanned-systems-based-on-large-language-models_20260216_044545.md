---
ver: rpa2
title: In-Context Learning for Knowledge Base Question Answering for Unmanned Systems
  based on Large Language Models
arxiv_id: '2311.02956'
source_url: https://arxiv.org/abs/2311.02956
tags:
- knowledge
- question
- language
- proper
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a ChatGPT-based Cypher Query Language (CQL)
  generation framework for knowledge base question answering (KBQA) in unmanned systems.
  The framework uses in-context learning to generate CQL from natural language questions
  by combining auxiliary task predictions, proper noun matching, demonstration example
  selection, and prompt construction.
---

# In-Context Learning for Knowledge Base Question Answering for Unmanned Systems based on Large Language Models

## Quick Facts
- arXiv ID: 2311.02956
- Source URL: https://arxiv.org/abs/2311.02956
- Reference count: 19
- F1-score of 0.92676 achieved in CCKS 2023 competition

## Executive Summary
This paper presents a ChatGPT-based framework for generating Cypher Query Language (CQL) from natural language questions in unmanned systems knowledge bases. The framework uses in-context learning with auxiliary task predictions, proper noun matching, and demonstration example selection to guide CQL generation. The method achieved second place in the CCKS 2023 Question Answering with Knowledge Graph Inference for Unmanned Systems competition, demonstrating the effectiveness of large language models for knowledge base question answering tasks.

## Method Summary
The framework consists of six components: an auxiliary model predicting CQL syntax-related information (intent, relation count, condition count), a proper noun matcher extracting entities from questions, a demonstration example selector using key-information-based similarity, a prompt constructor building Chain-of-Thought style prompts, a ChatGPT-based CQL generation model, and an ensemble model for answer selection. The approach decomposes CQL generation into manageable sub-tasks and uses in-context learning with carefully selected demonstration examples to improve generation accuracy.

## Key Results
- Achieved F1-score of 0.92676 in CCKS 2023 competition
- Second place overall in the unmanned systems question answering task
- Demonstrated effectiveness of in-context learning for CQL generation
- Showed KI-Sim (key-information-based similarity) outperforms global feature similarity for example selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The auxiliary tasks provide structured information that improves ChatGPT's CQL generation accuracy
- Mechanism: The framework predicts three key structural elements (intent classification, relation count, condition count) that guide the CQL structure generation
- Core assumption: These three structural elements are sufficient to constrain ChatGPT's generation to produce syntactically valid CQL
- Evidence anchors:
  - [abstract] "Our generative framework contains six parts: an auxiliary model predicting the syntax-related information of CQL"
  - [section 3.1] "Upon analyzing CQLs and evaluating the task instructions, we summarize four execution intent categorized by the content of the RETURN clause"
  - [corpus] Weak - no direct corpus evidence found for auxiliary task effectiveness
- Break condition: If the auxiliary model accuracy drops below ~85%, the structural guidance becomes insufficient and CQL quality degrades

### Mechanism 2
- Claim: Key-Information-Based Similarity (KI-Sim) enables more effective demonstration example selection than global feature similarity
- Mechanism: KI-Sim focuses on proper nouns and task-specific structural elements rather than full sentence semantics
- Core assumption: Proper nouns and structural elements are more important than general semantics for CQL generation tasks
- Evidence anchors:
  - [section 3.3] "Based on the above findings, we propose a key-information-based similarity criterion (KI-Sim) for NLQs"
  - [section 4.7] "It is evident that the demonstration examples selected by KI-Sim are more similar to the top NLQ"
  - [corpus] Moderate - related work on "Few-shot In-context Learning for Knowledge Base Question Answering" suggests this approach has merit
- Break condition: If NLQs contain few proper nouns or if the proper noun vocabulary is incomplete, KI-Sim loses its advantage

### Mechanism 3
- Claim: The Chain-of-Thought (CoT) approach decomposes CQL generation into manageable sub-tasks
- Mechanism: First extract proper nouns from NLQ, then generate CQL based on the extracted information
- Core assumption: Breaking complex CQL generation into noun extraction and template filling simplifies the problem for LLMs
- Evidence anchors:
  - [section 3.4] "Intuitively, directly generating CQL from NLQ is challenging. Yet, analyzing the composition and syntax of CQLs and NLQs reveals a high likelihood of shared proper nouns"
  - [section 3.4] "Drawing inspiration from the CoT method, we split the CQL generation task into two sub-tasks"
  - [corpus] Moderate - "Few-shot In-context Learning for Knowledge Base Question Answering" shows CoT effectiveness for similar tasks
- Break condition: If NLQs contain complex implicit relations that cannot be captured by noun extraction alone

## Foundational Learning

- Concept: Cypher Query Language (CQL) syntax and semantics
  - Why needed here: The framework generates CQL code, so understanding its structure is essential for debugging and improving the system
  - Quick check question: What are the three main clauses in a CQL query and what does each do?

- Concept: Knowledge Base Question Answering (KBQA) pipeline
  - Why needed here: Understanding how NLQ gets transformed into executable queries helps in identifying bottlenecks and improvement opportunities
  - Quick check question: What are the typical steps in a KBQA pipeline from natural language input to final answer?

- Concept: Large Language Model (LLM) in-context learning
  - Why needed here: The framework relies heavily on LLMs learning from demonstration examples in the prompt
  - Quick check question: How does in-context learning differ from fine-tuning in LLMs?

## Architecture Onboarding

- Component map:
  - Input: Natural Language Question (NLQ)
  - Auxiliary Model: Predicts intent, relation count, condition count
  - Proper Noun Matcher: Extracts entities, relations, attributes from NLQ
  - Demonstration Example Selector: Uses KI-Sim to find similar examples
  - Prompt Constructor: Builds CoT-style prompt with examples and prior knowledge
  - ChatGPT Generation: Generates CQL code
  - Post-processing: Corrects common generation errors
  - Ensemble Model: Votes on multiple generated answers
  - Output: Final answer from knowledge base

- Critical path: NLQ → Auxiliary Tasks → Proper Noun Matching → Example Selection → Prompt Construction → ChatGPT Generation → Post-processing → Ensemble Voting

- Design tradeoffs:
  - Using multiple CQLs with voting increases accuracy but also computation time
  - KI-Sim is more effective than GF-Sim but requires maintaining proper noun vocabulary
  - CoT decomposition simplifies the task but may miss complex implicit relationships

- Failure signatures:
  - Low auxiliary task accuracy (<85%) indicates structural guidance issues
  - KI-Sim returning dissimilar examples suggests vocabulary or matching problems
  - ChatGPT consistently misclassifying tags as entities indicates prompt construction issues

- First 3 experiments:
  1. Test auxiliary model accuracy in isolation with various NLQ types
  2. Compare KI-Sim vs GF-Sim example selection quality on a validation set
  3. A/B test CoT prompt format vs direct CQL generation to measure decomposition benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the ChatGPT-based CQL generation framework scale with the size of the knowledge base and the complexity of the questions?
- Basis in paper: [inferred] The paper does not explicitly discuss the scalability of the framework, but mentions the use of a knowledge base and complex questions.
- Why unresolved: The paper does not provide information on the size of the knowledge base or the complexity of the questions used in the experiments, nor does it discuss the framework's performance under different scales.
- What evidence would resolve it: Experimental results showing the framework's performance on knowledge bases of varying sizes and with questions of different complexities.

### Open Question 2
- Question: How does the ChatGPT-based CQL generation framework handle ambiguous or context-dependent questions?
- Basis in paper: [inferred] The paper does not explicitly discuss how the framework handles ambiguous or context-dependent questions, but mentions the use of Chain-of-Thought and In-Context Learning.
- Why unresolved: The paper does not provide information on how the framework deals with questions that have multiple interpretations or require context to answer.
- What evidence would resolve it: Experimental results showing the framework's performance on ambiguous or context-dependent questions, and an analysis of how it handles such cases.

### Open Question 3
- Question: How does the ChatGPT-based CQL generation framework compare to other state-of-the-art methods in terms of accuracy and efficiency?
- Basis in paper: [explicit] The paper mentions that the framework achieved second place in a competition, but does not compare it to other state-of-the-art methods.
- Why unresolved: The paper does not provide a direct comparison with other methods, nor does it discuss the trade-offs between accuracy and efficiency.
- What evidence would resolve it: A comprehensive comparison with other state-of-the-art methods in terms of accuracy and efficiency, including a discussion of the trade-offs between the two.

## Limitations
- Reliance on proper nouns for demonstration selection means the system may struggle with NLQs containing few explicit entities or when the proper noun vocabulary is incomplete
- Auxiliary model predictions create a single point of failure - if any auxiliary task drops below ~85% accuracy, structural guidance becomes insufficient
- Ensemble voting approach introduces additional computational overhead and complexity without clear documentation of the voting mechanism

## Confidence
- **High Confidence (0.85+):** The overall framework architecture and its six-component design are well-documented and logically sound. The competition results (F1-score of 0.92676) provide strong empirical validation.
- **Medium Confidence (0.65-0.85):** The effectiveness of the KI-Sim demonstration selection method and the CoT decomposition approach are supported by analysis but lack extensive ablation studies to quantify their individual contributions.
- **Low Confidence (0.45-0.65):** The exact implementation details of the ensemble model and the specific knowledge base schema are not fully specified, making complete reproduction challenging.

## Next Checks
1. **Auxiliary Model Robustness Test:** Systematically vary the auxiliary task accuracy (intent, relation count, condition count) from 70% to 95% and measure the corresponding impact on final CQL generation quality to identify the precise accuracy threshold where structural guidance breaks down.

2. **KI-Sim vs GF-Sim A/B Test:** Conduct a controlled experiment comparing KI-Sim and GF-Sim on the same validation set, measuring not just CQL generation accuracy but also the semantic similarity between selected demonstration examples and target NLQs using multiple similarity metrics.

3. **CoT Decomposition Validation:** Compare the current CoT-based two-step approach (noun extraction → CQL generation) against a direct CQL generation baseline using the same demonstration examples, measuring both accuracy and computational efficiency to quantify the decomposition benefits.