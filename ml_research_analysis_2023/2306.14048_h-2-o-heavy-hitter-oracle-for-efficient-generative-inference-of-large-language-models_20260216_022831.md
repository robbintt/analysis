---
ver: rpa2
title: 'H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language
  Models'
arxiv_id: '2306.14048'
source_url: https://arxiv.org/abs/2306.14048
tags:
- cache
- arxiv
- attention
- definition
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of large memory consumption in
  large language model (LLM) inference, particularly the key-value (KV) cache which
  scales linearly with sequence length and batch size. The authors propose a novel
  KV cache eviction policy called Heavy Hitter Oracle (H2O) that exploits the observation
  that only a small portion of tokens (heavy hitters) contribute most of the value
  when computing attention scores.
---

# H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models

## Quick Facts
- arXiv ID: 2306.14048
- Source URL: https://arxiv.org/abs/2306.14048
- Reference count: 40
- Primary result: H2O with 20% heavy hitters improves throughput over three leading inference systems by up to 29×, 29×, and 3× on OPT-6.7B and OPT-30B, and reduces latency by up to 1.9× with the same batch size.

## Executive Summary
This paper addresses the critical memory bottleneck in large language model inference caused by the key-value (KV) cache, which scales linearly with sequence length and batch size. The authors propose a novel KV cache eviction policy called Heavy Hitter Oracle (H2O) that exploits the observation that only a small subset of tokens ("heavy hitters") contribute most of the value when computing attention scores. By formulating KV cache eviction as a dynamic submodular maximization problem, H2O provides theoretical guarantees while significantly reducing memory consumption without sacrificing generation quality.

## Method Summary
The paper proposes H2O, a KV cache eviction policy that identifies and retains only the most influential tokens ("heavy hitters") based on accumulated attention scores. The method formulates KV cache eviction as a dynamic submodular maximization problem, where tokens are selected to maximize a submodular function under cardinality constraints. H2O uses local statistics at each decoding step to identify heavy hitters, avoiding the need for global knowledge of future tokens while maintaining effectiveness. The algorithm balances between retaining recent tokens and heavy hitters to ensure both context relevance and memory efficiency.

## Key Results
- H2O with 20% heavy hitters improves throughput over DeepSpeed Zero-Inference, Hugging Face Accelerate, and FlexGen by up to 29×, 29×, and 3× respectively
- Reduces latency by up to 1.9× while maintaining the same batch size
- Validated on OPT-6.7B and OPT-30B models across multiple tasks
- Achieves significant memory savings while maintaining generation quality

## Why This Works (Mechanism)

### Mechanism 1: Power-law Distribution of Attention Scores
- Claim: Only a small subset of tokens ("heavy hitters") contribute most of the value when computing attention scores
- Mechanism: The attention mechanism naturally creates a power-law distribution of accumulated attention scores across tokens, allowing retention of only the most influential tokens
- Core assumption: Sparsity in attention scores is significant (>95% sparse) and consistent across layers and model sizes
- Evidence anchors: Empirical observation that "a small portion of tokens contributes most of the value when computing attention scores" and that "accumulated attention scores of all the tokens in attention blocks adhere to a power-law distribution"

### Mechanism 2: Local Statistics Sufficiency
- Claim: Local statistics at each decoding step can identify heavy hitters as effectively as considering future tokens
- Mechanism: Summing attention scores of only preceding tokens at each step can identify influential tokens without needing global future knowledge
- Core assumption: Local attention patterns are representative enough of global importance for local computation to suffice
- Evidence anchors: Empirical observation that "local H2... is equally effective as taking into account the attention of future tokens"

### Mechanism 3: Submodular Maximization Framework
- Claim: KV cache eviction can be formulated as a dynamic submodular maximization problem with theoretical guarantees
- Mechanism: Selecting which tokens to keep in limited cache space can be modeled as maximizing a submodular function under cardinality constraints
- Core assumption: Attention scoring function satisfies submodularity properties, enabling theoretical guarantees for greedy approaches
- Evidence anchors: Formulation of "KV cache eviction as a dynamic submodular problem" with "theoretical guarantee" under assumptions

## Foundational Learning

- **Attention mechanism in transformers**: Why needed - the entire method relies on understanding how attention scores are computed and how they determine token importance. Quick check: Can you explain why attention scores create a power-law distribution across tokens?

- **Submodular optimization**: Why needed - the theoretical framework for KV cache eviction is built on submodular maximization principles. Quick check: What is the diminishing returns property that defines submodular functions?

- **KV cache mechanics in LLM inference**: Why needed - understanding how KV cache works and why it's memory-intensive is crucial for appreciating the problem being solved. Quick check: How does the size of KV cache scale with sequence length and batch size?

## Architecture Onboarding

- **Component map**: Input (Query matrix Q, Key matrix K, Value matrix V) -> Processing (Attention score computation and accumulation) -> Decision logic (Heavy hitter identification and KV cache selection) -> Output (Reduced KV cache with heavy hitters and recent tokens)

- **Critical path**: 1. Compute attention scores for current token against all previous tokens 2. Accumulate attention scores across all layers 3. Identify heavy hitters based on accumulated scores 4. Select which tokens to retain in KV cache (heavy hitters + recent tokens) 5. Generate next token using reduced KV cache

- **Design tradeoffs**: Cache size vs. accuracy (smaller cache reduces memory but may impact quality), Local vs. global statistics (local computation is faster but may be less accurate), Heavy hitters vs. recent tokens (balance between important historical tokens and recent context)

- **Failure signatures**: Generation quality degradation when cache budget is too small, Performance collapse when heavy hitters are incorrectly identified, Memory inefficiencies if eviction policy doesn't properly balance heavy hitters and recent tokens

- **First 3 experiments**: 1. Validate the power-law distribution of attention scores across different model sizes and tasks 2. Test the effectiveness of local vs. global heavy hitter identification 3. Measure the accuracy-memory tradeoff curve at different cache budget levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Heavy-Hitter Oracle (H2O) framework generalize to different transformer architectures beyond the standard attention mechanism?
- Basis in paper: [explicit] The paper mentions that H2O can be applied to various transformer architectures, including GPT-NeoX and LLaMA, but does not provide a comprehensive analysis of its effectiveness across diverse architectures.
- Why unresolved: The paper focuses primarily on demonstrating H2O's effectiveness on a limited set of architectures and does not explore its generalizability to more complex or specialized transformer designs.
- What evidence would resolve it: Empirical studies evaluating H2O's performance on a wider range of transformer architectures, including those with sparse attention mechanisms, multi-head attention, or other architectural variations.

### Open Question 2
- Question: What is the theoretical justification for the choice of the function h(z) in the H2O eviction policy, and how does its selection impact the algorithm's performance?
- Basis in paper: [explicit] The paper mentions that h(z) is usually chosen to be a non-decreasing concave function, such as √z + 1 or log(z + 1), but does not provide a rigorous analysis of the impact of different choices on the algorithm's performance.
- Why unresolved: The paper does not explore the theoretical implications of different choices for h(z) or provide a systematic comparison of their effects on H2O's effectiveness.
- What evidence would resolve it: Theoretical analysis and empirical studies comparing the performance of H2O with different choices of h(z) and their impact on the algorithm's accuracy and efficiency.

### Open Question 3
- Question: How does the H2O framework handle long-range dependencies in the input sequence, and what are the potential limitations of the approach in capturing these dependencies?
- Basis in paper: [inferred] The paper focuses on the sparsity of attention matrices and the emergence of heavy-hitters, but does not explicitly address the challenge of capturing long-range dependencies in the input sequence.
- Why unresolved: The paper does not provide a detailed analysis of how H2O handles long-range dependencies or discuss the potential limitations of the approach in this context.
- What evidence would resolve it: Empirical studies evaluating H2O's performance on tasks that require capturing long-range dependencies, such as machine translation or document summarization, and a theoretical analysis of the approach's limitations in handling these dependencies.

## Limitations

- The power-law distribution claim for attention scores lacks strong theoretical grounding for why transformer attention mechanisms should produce such distributions across diverse tasks and model architectures
- The submodularity framework provides theoretical guarantees, but these depend on assumptions about the attention scoring function that aren't fully validated empirically
- The local statistics approach for identifying heavy hitters is validated empirically but lacks rigorous theoretical justification for why local patterns should be representative of global importance

## Confidence

**High Confidence**: The empirical observation that KV cache compression can significantly improve inference throughput and latency, and that some form of selective caching based on token importance works in practice.

**Medium Confidence**: The specific claim about power-law distribution of attention scores, the effectiveness of local statistics for heavy hitter identification, and the submodularity-based theoretical framework. These are supported by experiments but rely on assumptions that need broader validation.

**Low Confidence**: The claim that the method generalizes seamlessly across all LLM architectures and tasks without modification, and that the theoretical guarantees from the submodularity framework actually hold for transformer attention mechanisms.

## Next Checks

1. **Cross-Architecture Generalization Test**: Evaluate H2O on diverse transformer architectures (not just OPT) including LLaMA and GPT-NeoX models to verify that power-law distributions and heavy hitter patterns hold consistently across different model families and attention mechanisms.

2. **Submodularity Validation**: Design experiments to empirically test whether transformer attention scoring functions actually satisfy the diminishing returns property required for submodularity, by measuring how marginal gains decrease as more tokens are added to the cache.

3. **Robustness Under Stress**: Create adversarial generation scenarios where token importance shifts rapidly (e.g., topic changes, code-switching, or long-form generation with multiple themes) to test whether local heavy hitter identification fails when context patterns become more dynamic and less predictable.