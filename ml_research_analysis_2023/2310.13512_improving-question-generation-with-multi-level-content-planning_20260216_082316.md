---
ver: rpa2
title: Improving Question Generation with Multi-level Content Planning
arxiv_id: '2310.13512'
source_url: https://arxiv.org/abs/2310.13512
tags:
- question
- answer
- full
- multifactor
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MultiFactor, a novel question generation
  (QG) framework that leverages multi-level content planning. MultiFactor addresses
  the challenge of generating meaningful questions from long contexts by integrating
  fine-grained phrase selection and coarse-grained full answer generation.
---

# Improving Question Generation with Multi-level Content Planning

## Quick Facts
- arXiv ID: 2310.13512
- Source URL: https://arxiv.org/abs/2310.13512
- Reference count: 23
- MultiFactor achieves up to 8 BLEU-4 points improvement over state-of-the-art on HotpotQA

## Executive Summary
This paper introduces MultiFactor, a novel question generation framework that leverages multi-level content planning to address the challenge of generating meaningful questions from long contexts. The framework uses Phrase-Enhanced Transformers (PET) to jointly model phrase selection and text generation, with phrase probabilities infused into the decoder to focus on answer-aware phrases. Experimental results show that MultiFactor outperforms strong baselines on two popular QG datasets, particularly excelling at generating complex questions requiring multi-hop reasoning over extended contexts.

## Method Summary
MultiFactor is a two-stage framework for question generation that addresses the challenge of generating questions from long contexts through multi-level content planning. The framework consists of two Phrase-Enhanced Transformers (PET): an FA-model that performs joint phrase selection and full answer generation, and a Q-model that generates questions using the generated full answer. The FA-model uses the selected phrase probabilities as soft constraints during decoding to focus the model on answer-aware phrases, while the Q-model leverages the generated full answer as an answer-aware summary to facilitate question generation. This approach differs from previous studies by using a generative approach for sentence-level content planning instead of extraction, and by integrating phrase selection probabilities into the decoder to improve focus on relevant content.

## Key Results
- MultiFactor achieves up to 8 BLEU-4 points improvement over state-of-the-art on HotpotQA
- Demonstrates particular effectiveness in generating complex questions requiring multi-hop reasoning
- Outperforms strong baselines on both HotpotQA and SQuAD 1.1 datasets

## Why This Works (Mechanism)

### Mechanism 1
Phrase-Enhanced Transformer (PET) improves question generation by integrating phrase selection probabilities into the decoder. PET jointly models phrase selection and text generation, using the selected phrase probabilities as soft constraints during decoding to focus the model on answer-aware phrases. The core assumption is that these probabilities provide meaningful guidance for the decoder to generate more focused and relevant questions. If the phrase selection probabilities do not correlate with the quality of generated questions, the soft constraints would not improve performance.

### Mechanism 2
Multi-level content planning addresses the challenge of generating questions from long contexts by connecting disjointed phrases. FA-model generates full answers that connect short answers with selected key phrases, forming an answer-aware summary that facilitates QG in the Q-model. The core assumption is that full answers provide a more convenient representation of the context for downstream question generation compared to using only key phrases or entire sentences. If the generated full answers contain irrelevant information or fail to capture the essence of the context, they would not improve QG quality.

### Mechanism 3
The generative approach to sentence-level content planning is more effective than the extraction approach used in previous studies. MultiFactor generates full answers instead of selecting entire sentences, resulting in more focused information for QG. The core assumption is that generating full answers can capture the essential information more concisely than selecting entire sentences, which may contain redundant information. If the generated full answers are not concise or fail to capture the essential information, the generative approach would not be more effective than extraction.

## Foundational Learning

- Concept: Transformer architecture
  - Why needed here: PET is based on Transformer, and understanding its components (encoder, decoder, attention mechanisms) is crucial for grasping how phrase probabilities are integrated into the decoder.
  - Quick check question: What are the three main sub-layers in each Transformer decoder layer?

- Concept: Sequence-to-sequence modeling
  - Why needed here: QG is formalized as a sequence-to-sequence problem, and understanding this framework is essential for comprehending how the input (context + answer) is transformed into the output (question).
  - Quick check question: In the context of QG, what are the typical components of the input sequence and the output sequence?

- Concept: Phrase selection and text generation as joint tasks
  - Why needed here: PET jointly models phrase selection and text generation, sharing the Transformer encoder to enable better representation learning for both tasks.
  - Quick check question: How does jointly modeling phrase selection and text generation benefit the overall QG process?

## Architecture Onboarding

- Component map: Context + Answer → FA-model → Full Answer → Q-model → Question
- Critical path: Context + Answer → FA-model → Full Answer → Q-model → Question
- Design tradeoffs:
  - Using a generative approach for sentence-level content planning (full answers) instead of an extraction approach (selecting entire sentences) to reduce redundant information.
  - Jointly modeling phrase selection and text generation to leverage shared representations and improve overall performance.
  - Using soft constraints (phrase selection probabilities) in the decoder instead of hard constraints to maintain flexibility and adaptability.
- Failure signatures:
  - Generated questions are irrelevant or nonsensical (semantic errors).
  - Generated questions miss key information needed for reasoning (hop errors).
  - Generated full answers contain irrelevant information or fail to capture the essence of the context.
- First 3 experiments:
  1. Train and evaluate the FA-model on a small subset of the data to ensure it can generate reasonable full answers.
  2. Train and evaluate the Q-model using gold full answers to assess its performance without error propagation from the FA-model.
  3. Train and evaluate the complete MultiFactor model end-to-end to measure the overall effectiveness of the multi-level content planning approach.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of MultiFactor vary across different languages beyond English? The authors state that experiments are only conducted on English corpus and do not verify effectiveness on datasets of other languages. This remains unresolved as the paper does not provide cross-lingual experiments or analysis. Conducting experiments on multilingual datasets like MLQA or XQuAD would resolve this question.

### Open Question 2
What is the impact of context length on MultiFactor's performance for extremely long contexts (greater than 500 or 1000 tokens)? The authors mention that context length in the sentence-level QG task is not very long and that more exploration is needed for particularly long contexts. This remains unresolved as the paper doesn't test MultiFactor on contexts longer than those in HotpotQA and SQuAD 1.1. Testing on datasets with longer contexts or artificially extending context length would resolve this question.

### Open Question 3
How does MultiFactor perform when integrated with large language models (LLMs) through instruction tuning or demonstrations? The authors compare MultiFactor with zero-shot GPT-3.5 and LoRA fine-tuned Llama2-7B, noting that GPT-3.5-Turbo often reveals answers and suggesting that multi-level content planning in instruction could improve performance. This remains unresolved as the paper does not explore how MultiFactor's approach could be incorporated into LLM-based QG systems. Implementing MultiFactor's content planning strategy within LLM prompts or fine-tuning procedures would resolve this question.

## Limitations
- The effectiveness of phrase probabilities as soft constraints in the decoder is assumed rather than rigorously validated
- The paper doesn't address computational efficiency or training stability issues that might arise from the more complex PET architecture
- Human evaluation methodology lacks detail on rater training, inter-annotator agreement, or potential biases

## Confidence

**High Confidence**: The claim that MultiFactor outperforms strong baselines on HotpotQA and SQuAD 1.1 is well-supported by the reported BLEU-4 improvements of up to 8 points.

**Medium Confidence**: The claim that multi-level content planning specifically addresses the challenge of generating questions from long contexts is plausible given the architecture, but lacks ablation studies demonstrating that both levels are necessary for the observed improvements.

**Low Confidence**: The claim that phrase selection probabilities provide meaningful guidance for the decoder relies on the assumption that the joint modeling approach creates useful representations, but there's no direct analysis showing how these probabilities influence the generation process.

## Next Checks

1. **Ablation Study on Content Planning Levels**: Conduct experiments removing either the phrase-level or sentence-level content planning to quantify the individual contributions of each component and clarify whether both levels are necessary for the observed performance gains.

2. **Phrase Probability Analysis**: Implement a diagnostic experiment that visualizes or quantifies how phrase selection probabilities actually influence the decoder's attention patterns and word distributions during generation to validate whether the soft constraints are being effectively utilized.

3. **Computational Efficiency Benchmarking**: Measure and compare the training time, inference latency, and parameter count of MultiFactor against baseline models to understand the practical deployment implications of the more complex PET architecture.