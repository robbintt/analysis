---
ver: rpa2
title: Towards Faithful Neural Network Intrinsic Interpretation with Shapley Additive
  Self-Attribution
arxiv_id: '2309.15559'
source_url: https://arxiv.org/abs/2309.15559
tags:
- value
- shapley
- sasanet
- feature
- attribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Shapley Additive Self-Attributing Neural Network
  (SASANet), which combines a marginal contribution-based sequential module and an
  internal distillation-based Shapley value module to model the Shapley value of the
  model output for any number of input features. Experiments on tabular datasets show
  that SASANet not only outperforms existing self-attributing models in prediction
  performance but also rivals black-box models, while providing more precise and efficient
  interpretation of its own predictions compared to post-hoc methods.
---

# Towards Faithful Neural Network Intrinsic Interpretation with Shapley Additive Self-Attributing Neural Network

## Quick Facts
- **arXiv ID**: 2309.15559
- **Source URL**: https://arxiv.org/abs/2309.15559
- **Reference count**: 19
- **Primary result**: Proposes SASANet achieving 0.9542 AUC on income prediction while being 200-3000x faster than post-hoc methods

## Executive Summary
This paper introduces Shapley Additive Self-Attributing Neural Network (SASANet), a self-interpreting model that directly learns to compute Shapley values for its own predictions. Unlike post-hoc interpretation methods that approximate feature contributions after training, SASANet is designed from the ground up to provide faithful attribution through a marginal contribution-based sequential module and internal distillation-based Shapley value module. The model achieves competitive prediction performance on tabular datasets while offering more precise and efficient interpretation of its predictions compared to existing approaches.

## Method Summary
SASANet combines a marginal contribution-based sequential module with an internal distillation-based Shapley value module to directly model the Shapley value of the model output for any number of input features. The architecture consists of three main components: a feature embedding module that transforms input features, a marginal contribution module that captures the effect of each feature as it is added to the input, and a Shapley value module that learns to attribute the final output to individual features based on their marginal contributions. The model is trained using alternating objectives that minimize the difference between the Shapley value module's outputs and the marginal contributions computed by the sequential module, with positional Shapley value distillation to reduce variance in estimation.

## Key Results
- Achieves up to 0.9542 AUC on income prediction task
- Outperforms existing self-attributing models in prediction performance
- Attribution time is 0.2-0.3s per sample, 200-3000x faster than post-hoc methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: SASANet achieves faithful self-interpretation by modeling the Shapley value of its own output directly, rather than approximating it post-hoc.
- **Mechanism**: Uses a marginal contribution-based sequential module to capture the effect of each feature as it is added to the input, and an internal distillation-based training strategy to ensure attribution values converge to the Shapley values of the model's output.
- **Core assumption**: The model can accurately learn the value function for any subset of features, and that the internal distillation process will converge to the true Shapley values.
- **Evidence anchors**:
  - [abstract]: "we propose Shapley Additive Self-Attributing Neural Network (SASANet), with theoretical guarantees for the self-attribution value equal to the output's Shapley values."
  - [section]: "Specifically, SASANet uses a marginal contribution-based sequential schema and internal distillation-based training strategies to model meaningful outputs for any number of features, resulting in un-approximated meaningful value function."
- **Break condition**: If the model cannot accurately learn the value function for subsets of features, or if the internal distillation process does not converge, the attribution values will not accurately reflect the Shapley values.

### Mechanism 2
- **Claim**: SASANet's use of positional Shapley value distillation reduces the variance in the estimation of Shapley values compared to direct estimation.
- **Mechanism**: By training a positional attribution function that measures the effect of each feature at different positions in the input sequence, SASANet can better capture the marginal contribution of features in different contexts.
- **Core assumption**: The marginal contribution of a feature depends on its position in the input sequence, and that training a positional attribution function can capture this dependency.
- **Evidence anchors**:
  - [section]: "The marginal contribution of a feature can fluctuate based on the prefix set size... Therefore, instead of directly training overall attribution function with Eq. 4, we train a positional attribution function ϕ(xS; θϕ)i,k to measure feature i's effects in fc in a certain position k, with an internal positional distillation loss: L(i,k)s (xS) = 1|D| X O∈D I{Ok = i}(ϕ(xS; θϕ)i,k − △(xi, xO1:k−1; θ△))2."
- **Break condition**: If the marginal contribution of a feature does not depend on its position, or if the positional attribution function cannot accurately capture this dependency, the variance reduction will not be realized.

### Mechanism 3
- **Claim**: SASANet's value function naturally handles feature dependencies by directly modeling the label expectation given a subset of features, rather than relying on approximations that assume feature independence.
- **Mechanism**: SASANet's value function is trained to estimate the expected label when certain features are observed, which implicitly accounts for the dependencies between features.
- **Core assumption**: The value function can be accurately learned to reflect the true label expectation given any subset of features, and that this expectation implicitly captures the dependencies between features.
- **Evidence anchors**:
  - [section]: "SASANet sidesteps this by learning an apt value function, better estimating feature-label relations via self-attribution."
  - [corpus]: Weak evidence, but the claim is supported by the theoretical analysis in the paper.
- **Break condition**: If the value function cannot be accurately learned to reflect the true label expectation, or if the dependencies between features cannot be captured by the value function, the attribution values will not accurately reflect the Shapley values.

## Foundational Learning

- **Concept**: Shapley value and its axioms (Efficiency, Linearity, Nullity, Symmetry)
  - **Why needed here**: SASANet's self-attribution is designed to converge to the Shapley values of its output, which are defined by these axioms.
  - **Quick check question**: What are the four axioms that define the Shapley value, and how do they ensure fair contribution allocation?

- **Concept**: Marginal contribution and its role in Shapley value calculation
  - **Why needed here**: SASANet's marginal contribution-based sequential module is designed to capture the marginal contribution of each feature as it is added to the input.
  - **Quick check question**: How is the marginal contribution of a feature defined, and how does it relate to the Shapley value of that feature?

- **Concept**: Feature dependencies and their impact on attribution methods
  - **Why needed here**: SASANet's value function is designed to handle feature dependencies by directly modeling the label expectation given a subset of features.
  - **Quick check question**: How do feature dependencies impact the accuracy of attribution methods, and why is it important for attribution methods to handle these dependencies?

## Architecture Onboarding

- **Component map**: Input features -> Feature embedding module -> Marginal contribution module -> Shapley value module
- **Critical path**: The critical path for computing the attribution values is: input features -> feature embedding -> marginal contribution module -> Shapley value module. The Shapley value module is trained to minimize the difference between its outputs and the marginal contributions computed by the marginal contribution module.
- **Design tradeoffs**: SASANet trades off some computational complexity for increased interpretability. By modeling the Shapley values directly, SASANet can provide more accurate and efficient attributions compared to post-hoc methods. However, this requires a more complex model architecture and training process.
- **Failure signatures**: If the attribution values do not accurately reflect the Shapley values, it may indicate that the marginal contribution module is not accurately capturing the effect of each feature, or that the Shapley value module is not accurately learning to attribute the output to the individual features. Additionally, if the model's performance is significantly worse than black-box models, it may indicate that the added complexity for interpretability is not worth the tradeoff in performance.
- **First 3 experiments**:
  1. Train SASANet on a simple dataset with known feature contributions and verify that the attribution values match the expected Shapley values.
  2. Compare the attribution values computed by SASANet to those computed by a post-hoc method on a real-world dataset, and verify that SASANet's attributions are more accurate and efficient.
  3. Evaluate the impact of the positional Shapley value distillation on the accuracy and variance of the attribution values by training SASANet with and without this component.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does SASANet perform on image or text datasets compared to tabular data?
- **Basis in paper**: [inferred] The paper states that self-attributing models can integrate pre-/post-transformations for different data types, but they chose to focus on tabular data for clarity and to avoid ambiguities. They also mention that future work will explore applications of SASANet.
- **Why unresolved**: The paper only evaluates SASANet on tabular datasets, so its performance on other data types remains unknown.
- **What evidence would resolve it**: Experimental results showing SASANet's performance on image or text classification tasks, compared to post-hoc methods and black-box models.

### Open Question 2
- **Question**: How does the computational efficiency of SASANet scale with the number of features and sample size?
- **Basis in paper**: [explicit] The paper reports that SASANet's attribution time is 0.2-0.3s per sample, which is 200-3000x faster than post-hoc methods. However, it does not provide information on how the runtime scales with larger feature sets or sample sizes.
- **Why unresolved**: The paper only provides attribution time for a fixed number of samples and does not analyze the scaling behavior.
- **What evidence would resolve it**: Runtime analysis showing how SASANet's attribution time changes with increasing number of features and sample size, compared to post-hoc methods.

### Open Question 3
- **Question**: How sensitive is SASANet to hyperparameter choices, such as the learning rate, batch size, and regularization coefficients?
- **Basis in paper**: [inferred] The paper mentions that hyperparameters were finely tuned for each model on each dataset, but it does not provide details on the sensitivity of SASANet's performance to these choices.
- **Why unresolved**: The paper does not report the impact of hyperparameter variations on SASANet's performance or provide guidance on hyperparameter selection.
- **What evidence would resolve it**: Sensitivity analysis showing how SASANet's performance changes with different hyperparameter settings, along with recommendations for optimal hyperparameter choices.

## Limitations
- Theoretical guarantees for convergence to true Shapley values are not empirically verified through controlled experiments
- Limited evaluation to only four tabular datasets with no external validation or ablation studies
- 200-3000x speedup over post-hoc methods is based on comparisons with unspecified baselines

## Confidence

- **High Confidence**: The architectural design of SASANet (marginal contribution module + positional distillation) is clearly specified and technically sound.
- **Medium Confidence**: The experimental results showing competitive performance and attribution quality are presented, but lack statistical significance testing and external validation.
- **Low Confidence**: The theoretical guarantees for convergence to true Shapley values are stated but not empirically verified.

## Next Checks

1. Conduct ablation studies to quantify the contribution of each component (marginal contribution module, positional distillation) to both prediction accuracy and attribution quality.
2. Test SASANet on additional datasets with known ground truth Shapley values to empirically verify the theoretical guarantees of convergence.
3. Compare SASANet's attribution quality and runtime against multiple post-hoc methods (SHAP, Integrated Gradients, LIME) using standardized benchmarks and statistical significance tests.