---
ver: rpa2
title: Causal Interpretation of Self-Attention in Pre-Trained Transformers
arxiv_id: '2310.20307'
source_url: https://arxiv.org/abs/2310.20307
tags: []
core_contribution: This paper provides a causal interpretation of self-attention in
  Transformer architectures, linking it to structural causal models. The key idea
  is that self-attention can be viewed as estimating a linear structural equation
  model for an input sequence, representing causal dependencies among symbols.
---

# Causal Interpretation of Self-Attention in Pre-Trained Transformers

## Quick Facts
- arXiv ID: 2310.20307
- Source URL: https://arxiv.org/abs/2310.20307
- Reference count: 40
- Key outcome: This paper provides a causal interpretation of self-attention in Transformer architectures, linking it to structural causal models and demonstrating zero-shot causal discovery on real-world tasks.

## Executive Summary
This paper establishes a causal interpretation of self-attention in Transformer architectures, showing that self-attention can be viewed as estimating a linear structural equation model (SEM) for an input sequence. The authors propose Attention-Based Causal-Discovery (ABCD), a method that leverages pre-trained Transformers for zero-shot causal discovery by using partial correlations between token representations to identify conditional independence relations. The approach is demonstrated on sentiment classification and recommendation tasks, where it produces more specific and plausible explanations compared to baseline methods.

## Method Summary
The method interprets self-attention as estimating a linear SEM, where the attention matrix encodes pairwise associations between symbols. Partial correlations between token representations in the deepest attention layer are used to test conditional independence relations, enabling causal structure learning via constraint-based algorithms. The CLEANN algorithm is then applied to extract explanations from the learned causal structures. The approach is evaluated on sentiment classification (IMDB dataset) and recommendation tasks (MovieLens 1M), comparing explanation quality against baselines that use attention weights directly.

## Key Results
- ABCD achieves 2.82 average explanation set size for sentiment classification vs. 3.62-5.52 for baselines
- In recommendation, CLEANN's explanations lead to 64% of replacement recommendations being ranked 2nd-3rd in the original top-5 list, vs. 32-44% for baselines
- The paper demonstrates that pre-trained Transformers can be used for zero-shot causal discovery on specific input sequences

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Self-attention in the deepest layer estimates a linear structural equation model (SEM) for an input sequence, where the attention matrix encodes pairwise associations between symbols.
- **Mechanism:** The covariance matrix of output embeddings is approximated by $C_Z = AA^T$, where $A$ is the attention matrix. This matches the covariance structure of a linear-Gaussian SEM, $C_X = ((I-G)^{-1}\Lambda)C_U((I-G)^{-1}\Lambda)^T$, where $G$ is the weight matrix of causal dependencies and $\Lambda$ scales latent exogenous noise.
- **Core assumption:** The output embeddings are one-to-one representations of input symbols, and the deepest attention layer's embeddings are used to predict the corresponding input symbols.
- **Break condition:** If embeddings are not injective (multiple symbols map to same embedding), the causal graph over symbols cannot be directly inferred from the attention matrix.

### Mechanism 2
- **Claim:** Conditional independence relations between input symbols can be recovered from partial correlations between their deepest-layer embeddings, enabling causal structure learning via constraint-based algorithms.
- **Mechanism:** The attention matrix $A$ is used to compute pairwise correlations $\rho_{i,j} = C_Z(i,j)/\sqrt{C_Z(i,i)C_Z(j,j)}$. These correlations approximate the covariance structure of the underlying SEM. Constraint-based algorithms (e.g., ICD) use these partial correlations to test conditional independence and learn a partial ancestral graph (PAG) representing an equivalence class of causal structures.
- **Core assumption:** The causal Markov and faithfulness assumptions hold for the data-generating process.
- **Break condition:** Violations of the faithfulness assumption or presence of strong non-linear dependencies not captured by the linear-Gaussian model will cause incorrect CI tests and erroneous causal graphs.

### Mechanism 3
- **Claim:** A pre-trained Transformer can be used for zero-shot causal discovery on a specific input sequence, with accuracy improving as the model's prediction accuracy increases.
- **Mechanism:** The attention matrix computed for a given input sequence encodes context-specific pairwise dependencies. By treating this matrix as the basis for covariance estimation, the constraint-based algorithm learns a causal structure specific to that sequence. Since the attention matrix is computed per-sequence, the resulting causal graph is tailored to the particular context of the input.
- **Core assumption:** The Transformer was trained on SCM-consistent data (Assumption 1), meaning each input sequence was generated by a corresponding SCM.
- **Break condition:** If the Transformer's generalization accuracy is low for a sequence, the attention matrix will be noisy, leading to incorrect partial correlations and erroneous causal discovery.

## Foundational Learning

- **Concept: Structural Causal Models (SCMs)**
  - Why needed here: The paper's core claim is that self-attention estimates an SCM. Understanding SCMs (exogenous/latent variables, structural equations, DAGs) is essential to grasp how attention maps to causal structure.
  - Quick check question: In a linear-Gaussian SCM, how is the covariance matrix of observed variables related to the weight matrix $G$ and latent noise matrix $\Lambda$?

- **Concept: Partial Correlations and Conditional Independence**
  - Why needed here: The method uses partial correlations between embeddings to test conditional independence, which is the basis for constraint-based causal discovery.
  - Quick check question: If variables $X$ and $Y$ are independent conditioned on $Z$, what is the partial correlation between $X$ and $Y$ given $Z$?

- **Concept: Partial Ancestral Graphs (PAGs) and Equivalence Classes**
  - Why needed here: The learned causal structure is a PAG, which represents an equivalence class of DAGs. Understanding PAGs (edge marks: arrow, tail, circle) is key to interpreting the output of the causal discovery algorithm.
  - Quick check question: What does a circle edge mark in a PAG signify about the underlying causal structure?

## Architecture Onboarding

- **Component map:** Input sequence → Transformer encoder → Attention matrix (deepest layer) → Covariance matrix $C_Z = AA^T$ → Partial correlations → Constraint-based causal discovery (e.g., ICD) → PAG (causal structure) → Explanation extraction (CLEANN)
- **Critical path:** The attention matrix from the deepest layer must be accurate; all downstream steps depend on it. Errors here propagate through partial correlation calculation to causal graph learning.
- **Design tradeoffs:**
  - Using the deepest layer ensures embeddings are context-rich but may lose some fine-grained token-level information present in earlier layers.
  - Linear-Gaussian assumption simplifies computation but may miss non-linear causal relationships.
  - Zero-shot discovery is convenient but limited by the Transformer's training data and architecture.
- **Failure signatures:**
  - High variance in partial correlation estimates → noisy attention matrix or small sample size.
  - PAG with many circle marks → weak or ambiguous conditional independence relations.
  - Explanations include implausible tokens → attention matrix not capturing true causal influence.
- **First 3 experiments:**
  1. **Synthetic data sanity check:** Generate data from a known linear-Gaussian SCM, train a small Transformer, extract attention matrix, and verify if the learned PAG matches the true DAG.
  2. **Attention vs. embedding correlation:** For a fixed input, compute both attention-based and embedding-based partial correlations. Compare correlation matrices to see how much information is lost/gained.
  3. **Layer ablation:** Extract attention matrices from multiple layers, run causal discovery on each, and compare the resulting PAGs to assess how deep-layer attention improves causal accuracy.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the content, several unresolved questions emerge:

- How does the accuracy of causal discovery via ABCD scale with increasing Transformer model size and training dataset size?
- Can ABCD be extended to handle non-linear structural causal models, or is it fundamentally limited to linear-Gaussian SCMs?
- How robust is ABCD to violations of the causal Markov and faithfulness assumptions?
- Can ABCD be adapted to work with other neural network architectures beyond Transformers, such as RNNs or GNNs?

## Limitations

- The method assumes linear-Gaussian relationships between tokens, which may not hold for all real-world data
- Performance depends on the Transformer being trained on SCM-consistent data, which may not always be the case
- The learned PAG represents an equivalence class rather than a unique DAG, limiting interpretability
- The zero-shot nature means accuracy is bounded by the pre-trained model's quality for specific input sequences

## Confidence

- **High Confidence**: The mathematical framework connecting attention matrices to covariance structures of linear SEMs
- **Medium Confidence**: The practical effectiveness of ABCD for causal discovery in real-world tasks
- **Medium Confidence**: The claim that "as Transformer models become more accurate with larger training datasets, the accuracy of ABCD will increase"

## Next Checks

1. **Synthetic Data Validation**: Generate multiple synthetic datasets from known linear-Gaussian SCMs with varying graph structures. Train small Transformers on these datasets and use ABCD to recover the causal structures. Quantify recovery accuracy across different graph types and noise levels.

2. **Embedding Invertibility Test**: For a fixed vocabulary, create test inputs where multiple tokens map to similar embeddings. Run ABCD and analyze whether the resulting PAGs incorrectly merge causal influences of distinct tokens, validating the injective embedding assumption.

3. **Layer-Wise Ablation Study**: Extract attention matrices from all layers of BERT, not just the deepest layer. Run causal discovery on each and compare the resulting PAGs' accuracy in predicting ground-truth explanations for sentiment classification. This would validate whether deeper layers truly provide better causal information.