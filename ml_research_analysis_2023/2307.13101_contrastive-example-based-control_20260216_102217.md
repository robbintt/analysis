---
ver: rpa2
title: Contrastive Example-Based Control
arxiv_id: '2307.13101'
source_url: https://arxiv.org/abs/2307.13101
tags:
- learning
- arxiv
- reward
- offline
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses offline example-based control, where the agent
  must learn a policy to achieve high-reward states without trial-and-error interactions.
  The core method, LAEO, learns an implicit model of multi-step transitions to estimate
  the probability of reaching success examples, which can represent the Q-values for
  the example-based problem.
---

# Contrastive Example-Based Control

## Quick Facts
- arXiv ID: 2307.13101
- Source URL: https://arxiv.org/abs/2307.13101
- Reference count: 22
- Key outcome: LAEO learns an implicit model of multi-step transitions to estimate Q-values for example-based control, avoiding reward function learning and achieving better performance on tasks with complex dynamics.

## Executive Summary
This paper addresses offline example-based control, where an agent must learn a policy to achieve high-reward states without trial-and-error interactions. The proposed method, LAEO, learns an implicit model of multi-step transitions using contrastive learning to estimate the probability of reaching success examples. This approach directly estimates Q-values for the example-based problem without requiring a separate reward function. On six manipulation tasks (two image-based and four state-based), LAEO matches or outperforms prior example-based offline RL methods, with the largest performance gap on tasks with more complicated dynamics. The method also shows improved robustness to occlusions and better scaling with dataset size.

## Method Summary
LAEO learns an implicit dynamics model f(s,a,sf) via contrastive learning on trajectories, where the output score estimates the likelihood that state sf is reachable from state s via action a. This score is averaged over all success examples to approximate Q-values up to a constant. The policy is then optimized to maximize these estimated Q-values plus a behavioral cloning regularization term. By avoiding the need for reward function learning and directly modeling reachability to success states, LAEO simplifies the example-based control problem and potentially improves performance when success examples are limited.

## Key Results
- LAEO matches or outperforms prior example-based offline RL methods on six manipulation tasks
- Largest performance gains observed on tasks with more complicated dynamics
- LAEO shows improved robustness to occlusions and better scaling with dataset size than prior methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LAEO estimates Q-values for example-based control by learning an implicit model of multi-step transitions, which predicts the probability of reaching success examples.
- Mechanism: The method trains a function f(s,a,sf) using contrastive learning on trajectories, where the output score estimates the likelihood that state sf is reachable from state s via action a. This score is then averaged over all success examples to approximate Q-values up to a constant.
- Core assumption: The implicit dynamics model can be learned without errors, and the reward function is proportional to the density ratio of success examples to trajectory data.
- Evidence anchors: [abstract]: "learns an implicit model of multi-step transitions to estimate the probability of reaching success examples, which can represent the Q-values"
- Break condition: If the implicit model cannot be learned accurately (e.g., insufficient data or complex dynamics), the Q-value estimates will be poor, leading to suboptimal policies.

### Mechanism 2
- Claim: The method avoids the need for reward function learning by using contrastive learning to directly model reachability to success states.
- Mechanism: Instead of training a reward classifier on success examples and then labeling transitions, LAEO trains a dynamics model that directly scores how likely each state-action pair will lead to any success example. This sidesteps the challenge of fitting a reward function from few examples.
- Core assumption: Learning a dynamics model from many unlabeled trajectories is easier than learning a reward function from few success examples.
- Evidence anchors: [abstract]: "learns an implicit model of multi-step transitions, rather than a reward function"
- Break condition: If the number of success examples becomes large relative to trajectories, reward learning might become competitive.

### Mechanism 3
- Claim: The learned representations in the implicit model enable the policy to generalize to unseen goal locations by finding common patterns among success examples.
- Mechanism: The model learns a representation space where state-action pairs close to success example representations have high Q-values. This allows the policy to infer what makes states successful even without seeing exact goal configurations during testing.
- Core assumption: Success examples share common features that the model can capture in the representation space.
- Evidence anchors: [abstract]: "By identifying what all the success examples have in common... the RL agent can learn what is necessary to solve the task"
- Break condition: If success examples are too diverse or lack common patterns, the representation space may not generalize well.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: Used to train the implicit dynamics model by distinguishing real future states from random states.
  - Quick check question: How does the contrastive loss encourage the model to predict reachability to success examples?

- Concept: Discounted state occupancy measure
  - Why needed here: Defines the probability of visiting states at any point in the future, which is what the implicit model estimates.
  - Quick check question: How does the occupancy measure differ from the immediate transition function?

- Concept: One-step policy improvement
  - Why needed here: LAEO updates the policy based on Q-values of the behavioral policy, not the optimal policy, which acts as regularization.
  - Quick check question: Why might one-step improvement be preferable in offline RL settings?

## Architecture Onboarding

- Component map:
  Trajectory dataset Dτ -> Implicit dynamics model f(s,a,sf) -> Q-value estimates -> Policy π(a|s) -> Behavioral cloning regularization

- Critical path:
  1. Train f(s,a,sf) on Dτ using contrastive loss
  2. Estimate Q(s,a) by averaging f(s,a,s*) over D*
  3. Optimize π to maximize estimated Q-values plus regularization

- Design tradeoffs:
  - Simplicity vs. optimality: One-step improvement is simpler but not guaranteed optimal.
  - Data efficiency: Avoids reward learning but requires many trajectories for accurate dynamics model.
  - Generalization: Relies on success examples sharing common patterns.

- Failure signatures:
  - Poor performance on tasks with complex dynamics if dynamics model underfits.
  - Failure to generalize to new goal configurations if success examples are too diverse.
  - Overfitting to training data if regularization is too weak.

- First 3 experiments:
  1. Train on a simple state-based task (e.g., FetchReach) and verify Q-value estimates match expected returns.
  2. Test on a task with few success examples to confirm advantage over reward-learning baselines.
  3. Evaluate on a partially observable version of a task to test robustness to occlusions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LAEO's performance scale with dataset size compared to reward-learning baselines in tasks with increasingly complex dynamics?
- Basis in paper: [explicit] The paper states "our experiments show that LAEO is more robust to occlusions and also exhibits better scaling with dataset size than prior methods."
- Why unresolved: While the paper demonstrates better scaling with dataset size on specific tasks, it does not provide a comprehensive study across a range of task complexities and dataset sizes.
- What evidence would resolve it: Experiments varying dataset sizes across tasks with different levels of complexity, comparing LAEO's performance to reward-learning baselines.

### Open Question 2
- Question: Can LAEO's implicit dynamics model generalize to unseen tasks beyond the training distribution?
- Basis in paper: [inferred] The paper mentions "we show that the dynamics model learned by LAEO can generalize to multiple different tasks, being used to solve tasks that are not explicitly represented in the training data."
- Why unresolved: While the paper demonstrates generalization to some extent, it does not explore the limits of this generalization or provide a systematic study of LAEO's ability to handle completely novel tasks.
- What evidence would resolve it: Experiments testing LAEO on a wide range of tasks, including those that are significantly different from the training tasks, to assess its generalization capabilities.

### Open Question 3
- Question: How does LAEO's performance compare to other offline RL methods that do not rely on reward learning, such as those based on behavior cloning or uncertainty estimation?
- Basis in paper: [explicit] The paper states "Our experiments demonstrate that LAEO can successfully solve offline RL problems from examples of high-return states on four state-based and two image-based manipulation tasks."
- Why unresolved: The paper focuses on comparing LAEO to reward-learning baselines, but does not provide a comprehensive comparison to other offline RL methods that do not rely on reward learning.
- What evidence would resolve it: Experiments comparing LAEO to other offline RL methods that do not rely on reward learning, such as behavior cloning or uncertainty estimation methods, on a variety of tasks.

## Limitations
- Performance gains on complex dynamics tasks are not statistically validated
- Assumption that success examples share common features is not directly tested
- Lack of ablation studies isolating the contribution of each mechanism

## Confidence
- Mechanism 1 (Q-value estimation): Medium - supported by experimental results but lacks theoretical guarantees
- Mechanism 2 (Avoiding reward learning): Medium - empirical advantage shown but not proven
- Mechanism 3 (Generalization): Low - based on experimental results but not directly tested

## Next Checks
1. Conduct ablation studies to isolate the contribution of the implicit dynamics model vs. the regularization term
2. Test on tasks with highly diverse success examples to validate generalization claims
3. Perform statistical analysis of performance differences between LAEO and baselines on complex dynamics tasks