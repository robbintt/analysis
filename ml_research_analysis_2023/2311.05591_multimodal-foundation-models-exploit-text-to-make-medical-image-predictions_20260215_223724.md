---
ver: rpa2
title: Multimodal Foundation Models Exploit Text to Make Medical Image Predictions
arxiv_id: '2311.05591'
source_url: https://arxiv.org/abs/2311.05591
tags:
- gpt-4v
- text
- multimodal
- only
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates multimodal AI models, including GPT-4V and
  Llama-3.2-90B, on medical image interpretation tasks. Models were tested with and
  without text descriptions using 1014 cases.
---

# Multimodal Foundation Models Exploit Text to Make Medical Image Predictions

## Quick Facts
- arXiv ID: 2311.05591
- Source URL: https://arxiv.org/abs/2311.05591
- Reference count: 40
- One-line primary result: Model accuracy is heavily driven by text, with performance increasing as more informative text is provided, while human performance does not improve with additional text.

## Executive Summary
This study evaluates multimodal AI models, including GPT-4V and Llama-3.2-90B, on medical image interpretation tasks using 1014 cases from NEJM Image Challenge and clinicopathological conferences. The key finding is that model accuracy is heavily dependent on text, with performance improving as text becomes more informative. Interestingly, when text is highly informative, images either reduce or have no effect on model accuracy. The study also demonstrates that mild text suggestions of incorrect diagnoses can significantly reduce model performance, highlighting the double-edged nature of text exploitation in medical diagnostics.

## Method Summary
The study collected 934 cases from NEJM Image Challenge (2005-2023) and 69 clinicopathological conferences (2021-2022), each with images and text descriptions. Researchers evaluated GPT-4V and Llama-3.2-90B models using three configurations: images only, text only, and both modalities. Model accuracy was compared to human performance and analyzed across different case difficulty levels, image types, and text informativeness. Physician evaluations were conducted on long-form cases to assess the impact of image-text integration.

## Key Results
- Model accuracy increases significantly with more informative text, while human performance does not improve with additional text
- When text is highly informative, adding images reduces or has no effect on model accuracy
- Mild text suggestions of incorrect diagnoses can significantly reduce model performance, demonstrating the double-edged nature of text exploitation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model accuracy improves with more informative text because the model relies on textual cues to infer the diagnosis.
- Mechanism: The model uses the text as a strong prior, effectively bypassing the need for detailed image analysis when the text provides sufficient diagnostic information.
- Core assumption: The model's internal representation prioritizes text over visual features when both are present.
- Evidence anchors:
  - [abstract]: "model accuracy is heavily driven by text, with performance increasing as more informative text is provided."
  - [section]: "At the other extreme, when text is highly informative, GPT-4V(Text Only) performs equally well as multimodal GPT-4V, and both substantially outperform human respondents and GPT-4V(ImageOnly)."
- Break condition: If text is ambiguous or contains contradictory information, the model's reliance on text could lead to incorrect predictions.

### Mechanism 2
- Claim: Text exploitation is a double-edged sword because even mild text suggestions of incorrect diagnoses can mislead the model.
- Mechanism: The model's text-based reasoning is sensitive to textual suggestions, leading to decreased accuracy when text contains diagnostic errors.
- Core assumption: The model's confidence in text-based reasoning is high enough that minor textual errors can override visual evidence.
- Evidence anchors:
  - [abstract]: "Mild text suggestions of incorrect diagnoses significantly reduced model performance."
  - [section]: "Exploitation of text is a double-edged sword; we show that even mild suggestions of an incorrect diagnosis in text diminishes image-based classification, reducing performance dramatically in cases the model could previously answer with images alone."
- Break condition: If the model's training included robust mechanisms for cross-modal validation, it might mitigate the impact of misleading text.

### Mechanism 3
- Claim: Images can reduce or have no effect on model accuracy when text is highly informative.
- Mechanism: The model's text-based reasoning is already optimal with informative text, so adding images introduces noise or redundancy without improving accuracy.
- Core assumption: The model's architecture does not effectively integrate complementary information from both modalities when one is already highly informative.
- Evidence anchors:
  - [abstract]: "Physician evaluations on long-form cases revealed that images either reduced or had no effect on model accuracy when text was highly informative."
  - [section]: "Furthermore, in our study, the addition of images in multimodal GPT-4V worsened performance (58% of cases) in the clinicopathological conferences."
- Break condition: If the model's architecture was designed to prioritize complementary information, it might improve accuracy by integrating both modalities effectively.

## Foundational Learning

- Concept: Multimodal learning
  - Why needed here: Understanding how models integrate information from multiple modalities (text and images) is crucial for interpreting the study's findings.
  - Quick check question: What is the primary advantage of using multimodal models over unimodal models in medical image interpretation?

- Concept: Text-image alignment
  - Why needed here: The study highlights the importance of how well text and images align in providing diagnostic information.
  - Quick check question: How does the presence of highly informative text affect the model's reliance on image data?

- Concept: Model bias and shortcut learning
  - Why needed here: The study demonstrates how models can exploit shortcuts (like relying heavily on text) rather than learning robust multimodal reasoning.
  - Quick check question: What are the potential risks of a model that relies too heavily on text for medical image interpretation?

## Architecture Onboarding

- Component map:
  - Text encoder: Processes and represents textual information.
  - Image encoder: Processes and represents visual information.
  - Fusion layer: Combines text and image representations.
  - Output layer: Generates the final prediction.

- Critical path:
  - Input text and image → Text encoder → Image encoder → Fusion layer → Output layer → Prediction.

- Design tradeoffs:
  - Text-heavy vs. image-heavy reasoning: Balancing the model's reliance on text versus images.
  - Complexity vs. interpretability: More complex models may perform better but be harder to interpret.

- Failure signatures:
  - Over-reliance on text: Performance drops when text is ambiguous or misleading.
  - Poor image-text alignment: Model fails to integrate complementary information effectively.

- First 3 experiments:
  1. Test model performance on cases with varying levels of text informativeness to confirm the impact of text on accuracy.
  2. Evaluate model sensitivity to misleading text by introducing subtle diagnostic errors in the text descriptions.
  3. Compare model performance on cases where text and images provide complementary information versus cases where they are redundant.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different levels of clinical experience affect human performance when text descriptions are provided with medical images?
- Basis in paper: [explicit] The paper states "human performance on medical image interpretation did not improve with informative text" and notes that cases were not analyzed by physician specialty or experience level.
- Why unresolved: The study only compared overall human accuracy to AI models without stratifying by physician experience, specialty, or training level.
- What evidence would resolve it: A study comparing human performance across different physician experience levels (e.g., medical students, residents, fellows, attending physicians) with and without text descriptions on the same medical image cases.

### Open Question 2
- Question: What specific mechanisms cause AI models to perform worse when images are added to highly informative text?
- Basis in paper: [explicit] The paper states "providing image data paradoxically diminished model performance" and "increased data were redundant with text summaries."
- Why unresolved: The study observed this phenomenon but did not investigate the underlying mechanisms causing the performance decrease.
- What evidence would resolve it: A detailed analysis of attention patterns, feature importance, and decision-making processes in AI models when processing multimodal vs. unimodal inputs, particularly focusing on cases where image addition reduces performance.

### Open Question 3
- Question: How does the performance of these AI models vary across different medical specialties beyond the image types tested?
- Basis in paper: [explicit] The paper notes "the NEJM Image Challenge covers a broad spectrum of specialities" but only evaluated performance on a limited set of image types.
- Why unresolved: The study only analyzed performance across a few image types (cutaneous-subcutaneous, radiology, oral-external, ocular-external) and general categories.
- What evidence would resolve it: A comprehensive evaluation of AI model performance across all medical specialties represented in the NEJM Image Challenge, including pathology, cardiology, neurology, and other subspecialties.

## Limitations

- The study's findings are based on specific NEJM datasets and may not generalize to other medical imaging contexts or institutions.
- The exact mechanisms by which models prioritize text over images remain unclear, making it difficult to predict or prevent potential failures in practice.
- The study does not investigate the impact of model architecture on text-image integration, leaving open questions about how different designs might perform.

## Confidence

- Model accuracy heavily driven by text: High confidence
- Human performance does not improve with additional text: High confidence
- Text suggestions reducing performance: Medium confidence
- Images reducing accuracy with informative text: Medium confidence

## Next Checks

1. Test the same experimental paradigm on radiology images from multiple institutions to assess generalizability across different imaging protocols and clinical contexts.
2. Evaluate model performance when text contains clinically plausible but factually incorrect information to quantify the threshold at which text begins to override visual evidence.
3. Conduct blinded human evaluations comparing model outputs to those of specialists in radiology and pathology to better understand the clinical relevance of the observed performance differences.