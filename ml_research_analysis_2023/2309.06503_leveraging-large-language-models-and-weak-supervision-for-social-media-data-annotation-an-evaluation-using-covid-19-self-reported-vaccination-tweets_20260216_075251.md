---
ver: rpa2
title: 'Leveraging Large Language Models and Weak Supervision for Social Media data
  annotation: an evaluation using COVID-19 self-reported vaccination tweets'
arxiv_id: '2309.06503'
source_url: https://arxiv.org/abs/2309.06503
tags:
- data
- language
- weak
- supervision
- tweets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the effectiveness of large language models
  (GPT-4 and GPT-3.5) and weak supervision for annotating COVID-19 vaccine-related
  tweets. The authors compare the performance of these models against human annotators
  using a manually curated gold-standard dataset and a silver-standard dataset generated
  using weak supervision.
---

# Leveraging Large Language Models and Weak Supervision for Social Media data annotation: an evaluation using COVID-19 self-reported vaccination tweets

## Quick Facts
- arXiv ID: 2309.06503
- Source URL: https://arxiv.org/abs/2309.06503
- Reference count: 40
- GPT-4 achieved 80.81% accuracy in labeling self-reported vaccination tweets

## Executive Summary
This paper evaluates the effectiveness of large language models (GPT-4 and GPT-3.5) and weak supervision for annotating COVID-19 vaccine-related tweets. The authors compare the performance of these models against human annotators using a manually curated gold-standard dataset and a silver-standard dataset generated using weak supervision. Results show that GPT-4 achieved 80.81% accuracy in labeling self-reported vaccination tweets and 92.96% accuracy in labeling vaccine chatter, outperforming GPT-3.5. When combined with weak supervision to create an "electrum dataset," the fine-tuned COVID-Twitter-BERT model outperformed both GPT models.

## Method Summary
The study used a gold-standard dataset of 2,454 self-reported vaccination tweets and 19,946 vaccine chatter tweets, manually curated by human annotators. GPT-4 and GPT-3.5 were used in zero-shot mode to label this gold-standard data. A silver-standard dataset of 750,000 weakly supervised tweets was generated using heuristic-based labeling, then labeled with GPT models. The combined "electrum dataset" was used to fine-tune COVID-Twitter-BERT and BERTweet models, which were evaluated against the gold standard using accuracy and Cohen Kappa scores.

## Key Results
- GPT-4 achieved 80.81% accuracy in labeling self-reported vaccination tweets, outperforming GPT-3.5 (71.11%)
- GPT-4 labeled vaccine chatter with 92.96% accuracy
- Fine-tuned COVID-Twitter-BERT using the electrum dataset outperformed both GPT models
- GPT-based annotation cost approximately $2,743.40 USD for 1.5 million tweets, compared to $37,075.20-$52,896.00 USD for human annotation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 outperforms GPT-3.5 in zero-shot tweet annotation due to superior language understanding and context handling.
- Mechanism: GPT-4's larger model size and more diverse training data enable better semantic understanding of nuanced vaccine-related tweets, resulting in higher accuracy (80.81% vs 71.11% for self-reported vaccination tweets).
- Core assumption: Model size and training data diversity directly correlate with performance on specialized tweet classification tasks.
- Evidence anchors:
  - [abstract]: "Results show that GPT-4 achieved 80.81% accuracy in labeling self-reported vaccination tweets and 92.96% accuracy in labeling vaccine chatter, outperforming GPT-3.5."
  - [section]: "It is not surprising that GPT-4 outperformed GPT-3.5 by nearly 10% for the self reported vaccination tweets category"
- Break condition: Performance degrades significantly when tweet context requires domain-specific knowledge not captured in pre-training data.

### Mechanism 2
- Claim: Weak supervision combined with GPT-4 creates an "electrum dataset" that improves BERT model performance.
- Mechanism: GPT-4 labels weakly supervised data with higher accuracy than simple heuristics, creating a cleaner training set that, when combined with human-labeled gold data, produces better fine-tuned models than either approach alone.
- Core assumption: GPT-4's zero-shot labeling is sufficiently accurate to meaningfully improve weakly supervised datasets.
- Evidence anchors:
  - [abstract]: "When combined with weak supervision to create an 'electrum dataset,' the fine-tuned COVID-Twitter-BERT model outperformed both GPT models."
  - [section]: "Using the same prompt for the first evaluation, we made a total of 750,000 API calls to each GPT model to label this silver-standard dataset."
- Break condition: GPT-4's labeling accuracy drops below the threshold where the noise introduced outweighs the benefits of scale.

### Mechanism 3
- Claim: Cost-effective data annotation at scale using LLMs compared to traditional methods.
- Mechanism: API-based LLM annotation costs approximately $2,743.40 USD for 1.5 million tweets, which is significantly cheaper than human annotation services ($37,075.20-$52,896.00 USD).
- Core assumption: The accuracy achieved by LLMs is sufficient for downstream tasks, justifying the cost savings.
- Evidence anchors:
  - [section]: "We sent a total of 1,544,800 API calls, with a total cost of $2,743.40 USD... Leveraging Amazon Mechanical Turk would cost $37,075.20 USD for the same number of text classification tasks."
  - [section]: "There is clear value in evaluating if we can leverage such a resource for data annotation, this would particularly help resource constrained researchers"
- Break condition: When downstream task accuracy requirements cannot be met by LLM-labeled data, making human annotation necessary despite higher cost.

## Foundational Learning

- Concept: Cohen Kappa coefficient for inter-annotator agreement
  - Why needed here: To quantitatively compare agreement between human annotators, GPT-4, and GPT-3.5, establishing the reliability of LLM annotations
  - Quick check question: If two annotators agree on 90 out of 100 labels and the expected agreement by chance is 50%, what is Cohen's Kappa?

- Concept: Weak supervision and silver-standard datasets
  - Why needed here: Understanding how heuristic-based labeling creates training data that can be improved with LLM labeling to create electrum datasets
  - Quick check question: What is the primary advantage of using weak supervision over manual annotation when dealing with large-scale social media data?

- Concept: Fine-tuning domain-specific BERT models
- Why needed here: To understand why COVID-Twitter-BERT outperforms generic LLMs on vaccine-related tweet classification despite being smaller
  - Quick check question: Why would a model pre-trained on COVID-related Twitter data likely outperform a general-purpose LLM on vaccine tweet classification?

## Architecture Onboarding

- Component map: Tweet collection → heuristic-based weak labeling → GPT-4/GPT-3.5 zero-shot labeling → combined electrum dataset → fine-tuning of COVID-Twitter-BERT/BERTweet → evaluation against gold standard

- Critical path: The most time-consuming step is GPT-4 API calls for 750,000 tweets; optimization should focus on reducing API calls or parallelizing requests while maintaining quality

- Design tradeoffs: Zero-shot prompting (no fine-tuning) offers quick deployment but may miss domain nuances; fine-tuning BERT models requires more setup but achieves better performance; cost vs. accuracy tradeoff in choosing between GPT-4 and GPT-3.5

- Failure signatures: High disagreement between GPT-4 and human annotators (>0.2 Kappa difference); fine-tuned BERT models performing worse than zero-shot GPT; cost exceeding budget due to excessive API calls or failed requests

- First 3 experiments:
  1. Compare zero-shot performance of GPT-4 vs GPT-3.5 on a small validation set to establish baseline accuracy difference
  2. Test GPT-4's labeling consistency by running the same prompts multiple times to measure variance
  3. Evaluate the impact of different prompting strategies (system prompts, examples) on GPT-4's annotation accuracy for the target classes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal size of an 'electrum dataset' (mixture of gold and silver standard data) needed to fine-tune a model to achieve solid performance on downstream tasks?
- Basis in paper: [explicit] The authors note this as a future research direction, stating "we did not evaluate how large our ‘electrum dataset’ should be to fine-tune a model enough to achieve solid performance."
- Why unresolved: The study used a fixed size of 750,000 silver-standard tweets but did not systematically explore how dataset size affects model performance or where diminishing returns set in.
- What evidence would resolve it: Controlled experiments varying the proportion and total size of silver-standard data mixed with gold-standard data, measuring downstream task performance to identify the optimal balance point.

### Open Question 2
- Question: How consistent is the performance of GPT models across different domains and types of social media data beyond COVID-19 vaccine-related tweets?
- Basis in paper: [inferred] The study only tested on COVID-19 vaccine tweets, and the authors acknowledge this limitation, suggesting future research should test on different tasks and domains.
- Why unresolved: The evaluation was limited to one specific domain (vaccine-related tweets), and it's unclear whether the observed performance generalizes to other topics, platforms, or annotation tasks.
- What evidence would resolve it: Systematic evaluation of GPT models on diverse social media datasets across multiple domains (politics, natural disasters, product reviews) using consistent evaluation metrics.

### Open Question 3
- Question: What is the relationship between the cost-effectiveness of GPT-based annotation and the complexity or ambiguity of the annotation task?
- Basis in paper: [explicit] The authors present a cost analysis showing GPT is cheaper than human annotation, but note that "the more interesting one" (self-reported vaccination tweets) had lower accuracy than the simpler vaccine chatter category.
- Why unresolved: While cost savings were demonstrated, the paper doesn't analyze how task complexity affects the cost-benefit ratio or when human annotation becomes more cost-effective despite higher costs.
- What evidence would resolve it: Comparative analysis of cost-effectiveness across tasks of varying complexity, measuring both annotation accuracy and the downstream impact of errors on final task performance.

## Limitations
- The evaluation relies on a single domain (COVID-19 vaccine tweets), limiting generalizability to other topics or domains
- GPT-4's performance advantage may diminish with more complex annotation tasks or different tweet characteristics
- The cost analysis assumes stable API pricing, which may change as LLM services evolve

## Confidence
**High Confidence Claims:**
- GPT-4 outperforms GPT-3.5 on the specific task of classifying COVID-19 vaccine-related tweets
- Fine-tuned COVID-Twitter-BERT models achieve higher accuracy than zero-shot LLM approaches
- The proposed approach provides cost savings compared to human annotation at scale

**Medium Confidence Claims:**
- The "electrum dataset" methodology will generalize to other annotation tasks
- The accuracy levels achieved are sufficient for most downstream applications
- Cost estimates remain valid across different scale scenarios

## Next Checks
1. Test the electrum dataset approach on a different domain (e.g., mental health discourse or political content) to assess generalizability
2. Conduct a cost-benefit analysis comparing LLM annotation with traditional crowd-sourcing across varying dataset sizes
3. Evaluate the impact of different prompting strategies and few-shot examples on GPT-4's annotation accuracy for the target classes