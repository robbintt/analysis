---
ver: rpa2
title: '3D-EX : A Unified Dataset of Definitions and Dictionary Examples'
arxiv_id: '2308.03043'
source_url: https://arxiv.org/abs/2308.03043
tags:
- dictionary
- definitions
- dataset
- lexical
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces 3D-EX, a unified dataset combining English
  dictionaries and encyclopedias into <term, definition, example triples. The dataset
  integrates resources like WordNet, Wikipedia, Urban Dictionary, and Wiktionary after
  cleaning and deduplication.
---

# 3D-EX : A Unified Dataset of Definitions and Dictionary Examples

## Quick Facts
- arXiv ID: 2308.03043
- Source URL: https://arxiv.org/abs/2308.03043
- Reference count: 29
- Primary result: Introduces 3D-EX, a unified dataset of <term, definition, example> triples from diverse dictionaries and encyclopedias, with metadata and lexical splits to mitigate memorization.

## Executive Summary
3D-EX is a unified dataset combining English dictionaries and encyclopedias into <term, definition, example> triples. The dataset integrates resources like WordNet, Wikipedia, Urban Dictionary, and Wiktionary after cleaning and deduplication. It is split into random and lexical splits to mitigate memorization, and includes metadata tracking original sources for each entry. Experiments show that 3D-EX is effective for tasks like reverse dictionary lookup and source classification, with SBERT and Instructor-based models achieving competitive performance. The dataset also serves as a challenging testbed for representation learning, highlighting its utility in downstream NLP tasks and knowledge augmentation. Code and data are publicly available.

## Method Summary
3D-EX is constructed by aggregating entries from multiple lexicographic sources, including WordNet, Wiktionary, Wikipedia, Urban Dictionary, and others. The dataset undergoes a preprocessing pipeline to remove noisy entries, duplicates, and inconsistencies, resulting in a unified set of <term, definition, example> triples with source metadata. Two types of train/validation/test splits are provided: random splits for standard evaluation and lexical splits where no term appears across splits to prevent memorization. The dataset is evaluated on source classification (using RoBERTa-base) and reverse dictionary lookup (using SBERT and Instructor embeddings), demonstrating its utility for representation learning and downstream NLP tasks.

## Key Results
- 3D-EX combines multiple lexicographic sources into a unified dataset with source metadata and examples.
- Lexical splits prevent memorization by ensuring no term appears across train, validation, and test splits.
- SBERT and Instructor embeddings achieve competitive performance on reverse dictionary and source classification tasks using 3D-EX.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining multiple lexicographic sources reduces overfitting and memorization in downstream models.
- Mechanism: By aggregating entries from diverse dictionaries and encyclopedias (WordNet, Wiktionary, Wikipedia, Urban Dictionary, etc.), the dataset introduces variability in definition styles, coverage, and lexical focus. This diversity prevents models from relying on dataset-specific artifacts.
- Core assumption: Each source contributes unique, non-redundant lexical knowledge; duplicates are cleaned but diversity remains.
- Evidence anchors:
  - [abstract] "carefully pre-computed train/validation/test splits to prevent memorization"
  - [section] "entries that have null values either in words or definitions; removing entries where examples are the same as defined terms, and removing duplicate entries within each dataset or split"
  - [corpus] Weak evidence: no direct study of memorization effects cited, only claims of prevention via splitting.
- Break condition: If sources overlap heavily without meaningful variation, or if duplicates remain after cleaning, memorization risk increases.

### Mechanism 2
- Claim: 3D-EX improves performance on reverse dictionary and source classification tasks by providing rich contextual examples alongside definitions.
- Mechanism: The inclusion of high-quality dictionary examples provides additional semantic context, which helps embedding models (e.g., SBERT, Instructor) better capture term-definition relationships, improving retrieval and classification accuracy.
- Core assumption: Examples are semantically relevant and non-redundant with definitions; embeddings can leverage this extra signal.
- Evidence anchors:
  - [section] "entries in 3D-EX undergo a specific preprocessing set of steps" including cleaning noisy examples
  - [section] "experiments on both test sets with the best performing model... to see which sources are harder to solve"
  - [corpus] Weak evidence: no ablation study showing benefit of examples alone; performance gains are shown but causal attribution unclear.
- Break condition: If examples are noisy, irrelevant, or duplicated from definitions, they may degrade rather than improve embeddings.

### Mechanism 3
- Claim: 3D-EX serves as a challenging testbed for representation learning due to its varied sources and the lexical split design.
- Mechanism: The lexical split ensures no term appears across train/validation/test splits, forcing models to learn generalizable representations rather than term-specific memorization. The source diversity increases task difficulty.
- Core assumption: Lexical split is properly implemented and enforced; source diversity translates to representation difficulty.
- Evidence anchors:
  - [section] "release 3D-EX with a Random and a Lexical split... where all instances of a given term do not appear across splits"
  - [section] "results suggest that this dataset is both challenging for representation learning methods"
  - [corpus] Weak evidence: no quantitative comparison of lexical vs random split difficulty beyond reported scores; assumed difficulty.
- Break condition: If lexical split is improperly implemented or sources are too homogeneous, the challenge diminishes.

## Foundational Learning

- Concept: Data preprocessing and cleaning
  - Why needed here: Raw lexicographic resources contain noise, duplicates, and inconsistent formatting that must be removed to ensure dataset quality.
  - Quick check question: Why do we remove entries with more than 10% non-alphanumeric characters in definitions?

- Concept: Train/validation/test splitting strategies
  - Why needed here: Proper splitting prevents memorization and ensures models generalize to unseen terms, especially critical for lexical tasks.
  - Quick check question: What is the difference between a random split and a lexical split in 3D-EX?

- Concept: Embedding models for semantic similarity
  - Why needed here: SBERT and Instructor embeddings are used to measure and leverage semantic relationships between terms, definitions, and examples.
  - Quick check question: How do SBERT and Instructor embeddings differ in their approach to encoding text for retrieval tasks?

## Architecture Onboarding

- Component map:
  - Data ingestion: raw dictionaries â†’ cleaning pipeline
  - Deduplication and source tracking: unified triples with SOURCE array
  - Train/validation/test split generation: random + lexical splits
  - Model training: reverse dictionary (retrieval) and source classification (classification)
  - Evaluation: MRR for retrieval, F1 for classification

- Critical path:
  1. Clean and unify input datasets
  2. Generate random and lexical splits
  3. Train SBERT/Instructor embeddings on training split
  4. Evaluate on test split for reverse dictionary and source classification

- Design tradeoffs:
  - Source diversity vs. consistency: diverse sources improve generalization but may introduce noise
  - Example inclusion vs. model complexity: examples enrich context but require more storage and processing
  - Random vs. lexical splits: random splits are easier but may allow memorization; lexical splits are harder but better for generalization

- Failure signatures:
  - High similarity between train and test terms (lexical split not enforced)
  - Low source classification accuracy (sources too similar)
  - Degradation in reverse dictionary performance (examples not useful or noisy)

- First 3 experiments:
  1. Train SBERT on 3D-EX random split, evaluate reverse dictionary MRR
  2. Train Instructor with generic description, compare to SBERT
  3. Source classification: train RoBERTa-base on <term,definition> pairs, evaluate per-source F1

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the semantic properties of dictionary examples in Wiktionary compare to those in other dictionaries, and why does Wiktionary exhibit the lowest similarity between definitions and examples?
- Basis in paper: [explicit] The paper notes that Wiktionary has the lowest aggregate similarity between definitions and examples compared to other dictionaries.
- Why unresolved: The paper suggests that examples in Wiktionary might be purposefully written to cover different topics than their definitions, but this claim requires further semantic analysis to confirm.
- What evidence would resolve it: A detailed semantic analysis comparing the topics and themes covered by definitions and examples in Wiktionary versus other dictionaries, potentially involving manual annotation or more sophisticated semantic similarity metrics.

### Open Question 2
- Question: What specific properties of Urban Dictionary definitions contribute to their high dissimilarity with their corresponding terms, and is this dissimilarity due to the rarity of terms or the nature of the definitions themselves?
- Basis in paper: [explicit] The paper observes that Urban Dictionary exhibits high dissimilarity between terms and definitions, but it remains unclear whether this is due to the rarity of terms or the nature of the definitions.
- Why unresolved: The paper suggests both possibilities but does not provide a definitive explanation for the observed dissimilarity.
- What evidence would resolve it: An analysis of the frequency of terms in Urban Dictionary and a comparison of the semantic content of definitions with their corresponding terms, potentially involving a study of the writing style and content of Urban Dictionary entries.

### Open Question 3
- Question: How can 3D-EX be effectively leveraged for downstream NLP tasks beyond reverse dictionary lookup and source classification, and what are the potential benefits and challenges of using 3D-EX for pretraining language models?
- Basis in paper: [inferred] The paper suggests that 3D-EX is promising as a resource for augmenting lexical semantics systems and mentions the potential of using it for pretraining language models, but does not explore these applications in detail.
- Why unresolved: The paper introduces 3D-EX and its potential applications but does not provide a comprehensive evaluation of its effectiveness for various downstream NLP tasks or a detailed exploration of its use in pretraining language models.
- What evidence would resolve it: Experiments applying 3D-EX to a range of downstream NLP tasks, such as word sense disambiguation, text classification, and question answering, and a study of the impact of using 3D-EX for pretraining language models on their performance and generalization capabilities.

## Limitations
- The exact preprocessing parameters for cleaning and deduplicating entries are not fully specified, which could impact data quality.
- No direct ablation studies are provided to quantify the contribution of examples to model performance.
- The implementation and effectiveness of the lexical split are claimed but not independently verified.

## Confidence
- **High confidence**: The dataset's general structure (triples with source metadata), public availability, and reported performance metrics are well-supported.
- **Medium confidence**: Claims about the dataset's challenge level for representation learning and the benefits of combining diverse sources are plausible but lack direct experimental validation.
- **Low confidence**: The exact impact of examples on model performance and the thoroughness of the lexical split are not conclusively demonstrated.

## Next Checks
1. Reproduce source classification: Train RoBERTa-base on the provided random and lexical splits and verify F1 scores match those reported in the paper.
2. Verify lexical split integrity: Confirm that no term appears across train, validation, and test splits by sampling and checking for term overlap.
3. Ablation study: Run reverse dictionary experiments with and without examples to quantify their contribution to MRR performance.