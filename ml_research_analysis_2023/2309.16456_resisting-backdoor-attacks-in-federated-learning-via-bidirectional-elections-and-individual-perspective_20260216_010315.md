---
ver: rpa2
title: Resisting Backdoor Attacks in Federated Learning via Bidirectional Elections
  and Individual Perspective
arxiv_id: '2309.16456'
source_url: https://arxiv.org/abs/2309.16456
tags:
- updates
- snowball
- learning
- data
- infected
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Snowball addresses backdoor attacks in federated learning by introducing
  a bidirectional election framework from an individual model perspective. The method
  combines bottom-up voting (where each model update votes for closest peers) and
  top-down progressive selection using a variational autoencoder focused on model
  update differences.
---

# Resisting Backdoor Attacks in Federated Learning via Bidirectional Elections and Individual Perspective

## Quick Facts
- **arXiv ID**: 2309.16456
- **Source URL**: https://arxiv.org/abs/2309.16456
- **Reference count**: 40
- **Primary result**: Snowball achieves near-ideal backdoor attack resistance while maintaining competitive main task accuracy across five real-world datasets.

## Executive Summary
Snowball addresses backdoor attacks in federated learning by introducing a bidirectional election framework that operates from an individual model update perspective. The method combines bottom-up voting, where each model update votes for closest peers, with top-down progressive selection using a variational autoencoder focused on model update differences. This approach avoids requiring globally clear boundaries between benign and infected updates, making it effective under complex non-IID data distributions and moderate poisoning ratios.

## Method Summary
Snowball is a defense mechanism against backdoor attacks in federated learning that uses bidirectional elections to filter malicious model updates before aggregation. The method employs bottom-up voting where each model update votes for closest peers based on Euclidean distance, followed by top-down progressive selection using a VAE trained on differences between model updates. The approach operates without requiring globally clear boundaries between benign and infected updates, making it suitable for scenarios where updates are mixed and scattered due to non-IID data distributions and moderate poisoning ratios.

## Key Results
- Achieves near-ideal backdoor attack resistance across five real-world datasets (MNIST, Fashion MNIST, CIFAR-10, FEMNIST, Sentiment140)
- Maintains competitive main task accuracy compared to state-of-the-art defenses
- Demonstrates resilience when benign and infected updates are mixed and scattered under moderate poisoning ratios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Individual perspective allows benign and infected updates to separate at finer granularity than global views.
- Mechanism: Each model update votes for peers closest to itself, leveraging the assumption that benign updates prefer benign peers and infected updates prefer infected peers when benign clients are majority.
- Core assumption: Model updates closer to a benign one are more likely to be benign themselves due to similar data distributions.
- Break condition: If malicious clients constitute majority or voting radius is too large causing infected updates to vote for benign ones.

### Mechanism 2
- Claim: Learning differences between model updates is easier than learning the updates themselves.
- Mechanism: Uses VAE to learn latent representations of differences between model updates, focusing on benign patterns through progressive enlargement.
- Core assumption: The difference between two model updates is positively correlated with the difference between their corresponding data distributions.
- Break condition: If model update differences become too small to distinguish or VAE cannot learn meaningful representations.

### Mechanism 3
- Claim: Progressive selection based on reconstruction error effectively excludes infected updates.
- Mechanism: Iteratively adds updates with lowest reconstruction error from VAE to selectees, progressively enlarging the set of trusted updates.
- Core assumption: Infected updates will have higher reconstruction error when differences are learned relative to benign patterns.
- Break condition: If infected updates have similar reconstruction error to benign ones or VAE overfits to poisoned patterns.

## Foundational Learning

- **Non-IID data distributions in federated learning**
  - Why needed here: The entire approach relies on differences between client data distributions affecting model update similarity
  - Quick check question: How does the Dirichlet distribution parameter α control the heterogeneity of data across clients?

- **Backdoor attack mechanisms in federated learning**
  - Why needed here: Understanding how backdoor attacks modify model parameters is crucial for designing effective defenses
  - Quick check question: What distinguishes backdoor attacks from other poisoning attacks in terms of parameter modification patterns?

- **Variational autoencoders and latent space learning**
  - Why needed here: The defense mechanism uses VAE to learn differences between model updates in latent space
  - Quick check question: How does the reconstruction loss in VAE relate to the ability to distinguish between benign and infected model updates?

## Architecture Onboarding

- **Component map**: Client nodes (local training) → Server (aggregation) → Snowball module (pre-aggregation filtering) → Final aggregation
- **Critical path**: Receive model updates → Bottom-up voting → Select initial selectees → Train VAE on differences → Top-down progressive selection → Final aggregation
- **Design tradeoffs**:
  - More aggressive filtering (smaller M) provides better security but may hurt accuracy
  - Larger VAE training epochs improve detection but increase latency
  - Earlier top-down election (smaller TV) speeds convergence but may include infected updates
- **Failure signatures**:
  - High false positive rate indicates overly aggressive filtering
  - High false negative rate suggests insufficient discrimination capability
  - Degraded accuracy indicates too many updates being excluded
- **First 3 experiments**:
  1. Test with IID data and no attackers to verify baseline accuracy
  2. Test with moderate non-IID data and low attacker ratio to validate detection capability
  3. Test with high attacker ratio to stress-test the progressive selection mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact convergence rate bound for Snowball when malicious clients are present?
- Basis in paper: The paper provides the convergence rate formula (Equation 6) but does not give a specific numerical bound or concrete convergence guarantee when malicious clients are present versus absent.
- Why unresolved: The paper provides the convergence rate formula but doesn't show exact numerical bounds or comparison to FedAvg's convergence rate without defenses.
- What evidence would resolve it: A detailed proof showing the exact convergence bound for Snowball with malicious clients, including comparison to FedAvg's convergence rate without defenses.

### Open Question 2
- Question: How does Snowball's performance degrade with extremely high poisoning data ratios (PDR > 50%)?
- Basis in paper: The paper only evaluates PDR up to 30% and doesn't explore performance at higher poisoning ratios where infected updates become more distinguishable.
- Why unresolved: Limited evaluation at high poisoning ratios prevents understanding of performance thresholds.
- What evidence would resolve it: Experiments evaluating Snowball's BA and MA at PDR values of 50%, 70%, and 90% to determine the threshold at which its performance degrades significantly.

### Open Question 3
- Question: What is the impact of the top-down election threshold TV on Snowball's convergence speed and final accuracy?
- Basis in paper: The paper provides empirical values for TV but doesn't analyze the tradeoff between TV selection and model convergence or provide guidelines for optimal TV selection.
- Why unresolved: Insufficient analysis of how different TV values affect convergence and accuracy.
- What evidence would resolve it: A sensitivity analysis showing how different TV values affect both convergence speed and final model accuracy across multiple datasets.

## Limitations

- Effectiveness depends on benign clients being majority and having sufficiently distinct update patterns
- Performance characterization limited to moderate poisoning ratios (20% attackers, 30% poisoning)
- VAE-based difference learning may struggle with extremely subtle attack patterns or adaptive poisoning strategies

## Confidence

- **High confidence**: Basic bidirectional election framework design and evaluation methodology are sound
- **Medium confidence**: Effectiveness against moderate poisoning ratios (20% attackers, 30% poisoning) - limited stress testing shown
- **Low confidence**: Claims about handling "mixed and scattered" updates under complex non-IID distributions - insufficient ablation studies provided

## Next Checks

1. **Stress test VAE robustness**: Evaluate against adaptive attackers who modify poisoning patterns to minimize reconstruction error differences
2. **Boundary condition analysis**: Systematically vary poisoning ratios beyond 20% attackers and 30% poisoning data to identify failure thresholds
3. **Transferability validation**: Test whether VAE trained on one dataset's difference patterns transfers effectively to others with different feature spaces