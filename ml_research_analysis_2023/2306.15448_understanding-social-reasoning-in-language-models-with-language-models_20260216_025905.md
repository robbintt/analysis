---
ver: rpa2
title: Understanding Social Reasoning in Language Models with Language Models
arxiv_id: '2306.15448'
source_url: https://arxiv.org/abs/2306.15448
tags:
- belief
- question
- action
- event
- percept
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BigToM, a novel benchmark for evaluating
  Theory-of-Mind (ToM) reasoning in large language models (LLMs) using procedurally
  generated scenarios. The authors develop a framework that creates diverse social
  reasoning tasks by populating causal templates with LLMs, enabling systematic manipulation
  of perceptual access, beliefs, and actions in stories.
---

# Understanding Social Reasoning in Language Models with Language Models

## Quick Facts
- arXiv ID: 2306.15448
- Source URL: https://arxiv.org/abs/2306.15448
- Authors: 
- Reference count: 40
- This paper introduces BigToM, a novel benchmark for evaluating Theory-of-Mind (ToM) reasoning in large language models (LLMs) using procedurally generated scenarios.

## Executive Summary
This paper introduces BigToM, a novel benchmark for evaluating Theory-of-Mind (ToM) reasoning in large language models (LLMs) using procedurally generated scenarios. The authors develop a framework that creates diverse social reasoning tasks by populating causal templates with LLMs, enabling systematic manipulation of perceptual access, beliefs, and actions in stories. Human participants rated BigToM's quality higher than crowd-sourced datasets and on par with expert-written evaluations. When tested on BigToM, GPT-4 demonstrated human-like ToM inference patterns across multiple conditions, though with less reliability, while other LLMs struggled significantly with false belief scenarios. The results suggest that current LLMs have limited ToM capabilities, with GPT-4 being the closest to human performance.

## Method Summary
The paper introduces BigToM, a three-stage method for procedurally generating ToM evaluations. First, causal templates are built representing social reasoning scenarios with variables like context, desire, percept, belief, and actions. Second, these templates are populated by LLMs generating one sentence per variable while following strict constraints to maintain structure and avoid common-sense errors. Third, test items are composed by combining populated variables into stories for each condition, then evaluated using 0-shot, 0-shot-chain-of-thought, 1-shot, and 1-shot-chain-of-thought prompts. The approach aims to create high-quality evaluations while controlling for context effects through inclusion of control conditions with random events that don't change the environment state.

## Key Results
- GPT-4 demonstrated human-like ToM inference patterns across multiple conditions but with less reliability than humans
- Other LLMs (text-davinci-003, gpt-3.5-turbo, claude-v1.3, llama-65b-q5) struggled significantly with false belief scenarios
- Human participants rated BigToM's quality higher than crowd-sourced datasets and on par with expert-written evaluations
- Zero-shot prompting without chain-of-thought was sufficient to reveal genuine ToM capabilities, as additional prompting strategies may just mimic reasoning patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models can generate high-quality theory-of-mind evaluations by filling in causal templates with procedurally generated scenarios.
- Mechanism: By representing ToM scenarios as causal graphs, LLMs systematically intervene on variables like perceptual access, beliefs, and actions to create diverse test conditions that isolate specific inference capabilities.
- Core assumption: LLMs have sufficient world knowledge to generate coherent scenarios when prompted with causal templates, even if they struggle with the inferences being tested.
- Evidence anchors:
  - [abstract] "We present a novel framework for procedurally generating evaluations with LLMs by populating causal templates."
  - [section 3] "We propose a novel three stage-method: (1) Building a causal template of the domain, (2) populating causal templates using language models, and (3) composing test items for a given condition by 'stiching' together template variables."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.43, average citations=0.0. Top related titles: Language Models Represent Beliefs of Self and Others, Through the Theory of Mind's Eye: Reading Minds with Multimodal Video Large Language Models, Do LLMs Exhibit Human-Like Reasoning? Evaluating Theory of Mind in LLMs for Open-Ended Responses.

### Mechanism 2
- Claim: Including control conditions in ToM evaluations is critical for interpreting model performance and identifying failure modes.
- Mechanism: Control conditions (like random events that don't change the environment state) allow researchers to distinguish between ToM reasoning failures and simpler reasoning errors or content effects.
- Core assumption: Models will behave differently in control conditions versus true experimental conditions, revealing the specific nature of their reasoning capabilities.
- Evidence anchors:
  - [abstract] "Our extensive control conditions aim to take into account content effects [6] and many low-level confounds."
  - [section 3.1] "To control for context effects, we further include a control condition in which the 'Causal Event' is replaced with a 'Random Event' that does not change the state of the environment."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.43, average citations=0.0. Top related titles: Language Models Represent Beliefs of Self and Others, Through the Theory of Mind's Eye: Reading Minds with Multimodal Video Large Language Models, Do LLMs Exhibit Human-Like Reasoning? Evaluating Theory of Mind in LLMs for Open-Ended Responses.

### Mechanism 3
- Claim: Zero-shot prompting without chain-of-thought is sufficient to reveal genuine ToM capabilities in LLMs, as additional prompting strategies may just be mimicking reasoning patterns.
- Mechanism: Testing models with minimal prompting avoids the confound of teaching them how to reason, allowing researchers to assess their innate ToM capabilities rather than their ability to follow reasoning templates.
- Core assumption: Chain-of-thought prompting might artificially inflate performance by providing a reasoning template rather than measuring true ToM capabilities.
- Evidence anchors:
  - [section 4.1] "Human participants received instructions and a demonstration example to understand the task (see App. F). Hence, a fair comparison should provide similar support to models... zero-shot-chain-of-thought (CoT) prompting doesn't consistently improve performance across conditions. Introducing a one-shot CoT example does lead to consistent performance improvement across all conditions, however this performance may not be indicative of stronger ToM per se: mimicking the reasoning template is enough to solve our task in most cases."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.43, average citations=0.0. Top related titles: Language Models Represent Beliefs of Self and Others, Through the Theory of Mind's Eye: Reading Minds with Multimodal Video Large Language Models, Do LLMs Exhibit Human-Like Reasoning? Evaluating Theory of Mind in LLMs for Open-Ended Responses.

## Foundational Learning

- Concept: Causal reasoning and causal graphs
  - Why needed here: The entire evaluation framework is built on representing ToM scenarios as causal graphs, where interventions on different variables test different inference capabilities.
  - Quick check question: If an agent sees a causal event (someone swaps milk), what variable in the causal graph would you manipulate to test if the model can infer the agent's resulting belief?

- Concept: Theory of Mind (ToM) in cognitive science
  - Why needed here: Understanding what ToM is and how it's traditionally tested in humans is essential for designing appropriate evaluations for LLMs.
  - Quick check question: What is the key difference between true belief and false belief conditions in classic ToM tasks like the Sally-Anne test?

- Concept: LLM prompting strategies and their effects
  - Why needed here: Different prompting approaches (0-shot, 1-shot, CoT) can significantly affect model performance and must be understood to interpret results correctly.
  - Quick check question: Why might chain-of-thought prompting improve performance on reasoning tasks, and why is this problematic for evaluating genuine ToM capabilities?

## Architecture Onboarding

- Component map: Causal template creation -> LLM population of template variables -> Test item composition
- Critical path: The most critical path is from causal template definition to LLM population to test generation - any failure in generating coherent template variables will cascade through to unusable test items.
- Design tradeoffs: Using the model being tested to generate evaluations creates a potential circularity, but allows for testing whether models understand the non-mental situation before testing their inferential capabilities.
- Failure signatures: Common failures include LLMs generating scenarios that don't follow the template structure (~2-3% of cases), making common-sense errors (~1-2% of cases), or producing overly similar stories that reduce diversity.
- First 3 experiments:
  1. Generate a small set of templates (10-20) and manually verify the quality of populated variables to ensure the LLM follows instructions.
  2. Test the system with a simple condition (like Forward Belief True Belief) to verify the full pipeline works before expanding to all conditions.
  3. Run a pilot evaluation with one model (like GPT-3.5) to check that the generated test items produce meaningful performance differences across conditions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT-4 on backward belief inferences compare to human performance when the initial belief is explicitly stated versus when it is not stated?
- Basis in paper: [explicit]
- Why unresolved: The paper discusses the performance of GPT-4 on backward belief inferences but does not provide a detailed comparison between the performance when the initial belief is explicitly stated versus when it is not stated.
- What evidence would resolve it: A detailed analysis of GPT-4's performance on backward belief inferences with and without the initial belief explicitly stated, comparing it to human performance in both cases.

### Open Question 2
- Question: What are the specific failure modes of language models other than GPT-4 when performing forward belief and forward action inferences, and how do these compare to the failure modes observed in human participants?
- Basis in paper: [explicit]
- Why unresolved: The paper mentions that other language models struggle with forward belief and forward action inferences but does not provide a detailed analysis of their specific failure modes or compare these to human failure modes.
- What evidence would resolve it: A comprehensive analysis of the failure modes of language models other than GPT-4 on forward belief and forward action inferences, with a comparison to human failure modes.

### Open Question 3
- Question: How does the quality of model-generated evaluations compare to human-generated evaluations in terms of identifying subtle nuances in social reasoning, such as the ability to distinguish between different types of perceptual access (e.g., transparent vs. opaque objects)?
- Basis in paper: [explicit]
- Why unresolved: The paper compares the quality of model-generated and human-generated evaluations but does not specifically address their ability to identify subtle nuances in social reasoning.
- What evidence would resolve it: A detailed comparison of model-generated and human-generated evaluations focusing on their ability to identify and accurately assess subtle nuances in social reasoning, such as different types of perceptual access.

### Open Question 4
- Question: What is the impact of the explicit statement of an agent's initial belief on the performance of language models in theory-of-mind tasks, and how does this compare to the impact on human performance?
- Basis in paper: [explicit]
- Why unresolved: The paper mentions that the explicit statement of an agent's initial belief affects model performance but does not provide a detailed analysis of the impact or compare it to human performance.
- What evidence would resolve it: An analysis of the impact of explicitly stating an agent's initial belief on the performance of language models and humans in theory-of-mind tasks, with a focus on the differences in impact.

### Open Question 5
- Question: How does the performance of language models on theory-of-mind tasks vary with the complexity of the scenarios, and is there a threshold of complexity beyond which performance significantly deteriorates?
- Basis in paper: [inferred]
- Why unresolved: The paper discusses the performance of language models on theory-of-mind tasks but does not explicitly address how performance varies with scenario complexity or identify a threshold of complexity.
- What evidence would resolve it: A systematic study of language model performance on theory-of-mind tasks across scenarios of varying complexity, identifying any thresholds of complexity that lead to significant performance deterioration.

## Limitations

- The evaluation framework may create circularity by using LLMs to generate evaluations for LLMs, though authors attempt mitigation through careful design
- The study focuses on simple scenario-based ToM reasoning rather than the richer, more dynamic social interactions that characterize human ToM in real-world settings
- The research relies on specific prompting strategies that may not capture the full range of LLM capabilities or how these models are actually deployed in practice

## Confidence

- High confidence: The finding that GPT-4 outperforms other LLMs on ToM tasks and shows human-like reasoning patterns across conditions
- Medium confidence: The claim that current LLMs have limited ToM capabilities, based on specific false belief scenarios
- Medium confidence: The assertion that chain-of-thought prompting may artificially inflate performance by providing reasoning templates

## Next Checks

1. **Test the BigToM framework with additional ToM tasks**: Apply the causal template approach to evaluate other types of ToM reasoning, such as second-order false belief tasks or social prediction scenarios, to determine if the framework generalizes beyond the current conditions.

2. **Investigate the effect of model scale and architecture**: Compare ToM performance across different model families and analyze whether improvements correlate with model scale, architecture changes, or training data composition.

3. **Validate against real-world social reasoning benchmarks**: Test the same models on established human ToM assessments (like social perception tasks or conversational reasoning) to determine if BigToM's findings align with performance on more ecologically valid measures of social reasoning.