---
ver: rpa2
title: 'PEFA: Parameter-Free Adapters for Large-scale Embedding-based Retrieval Models'
arxiv_id: '2312.02429'
source_url: https://arxiv.org/abs/2312.02429
tags:
- pefa
- erms
- pefa-xl
- pefa-xs
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PEFA, a parameter-free adapter framework
  for fast tuning of embedding-based retrieval models (ERMs) without any backward
  pass in the optimization. PEFA equips ERMs with a non-parametric k-nearest neighbor
  (kNN) component at the index building stage, and performs a convex combination of
  two scoring functions (one from the ERM and the other from the kNN) at the inference
  stage.
---

# PEFA: Parameter-Free Adapters for Large-scale Embedding-based Retrieval Models

## Quick Facts
- arXiv ID: 2312.02429
- Source URL: https://arxiv.org/abs/2312.02429
- Authors: 
- Reference count: 40
- Key outcome: PEFA improves recall@100 for document retrieval by 13.2% on average and for product search by 5.3% (PEFA-XS) and 14.5% (PEFA-XL) on average.

## Executive Summary
PEFA introduces a parameter-free adapter framework for fast tuning of embedding-based retrieval models (ERMs) without any backward pass in the optimization. The framework equips ERMs with a non-parametric k-nearest neighbor (kNN) component at the index building stage and performs a convex combination of two scoring functions (one from the ERM and the other from the kNN) at the inference stage. PEFA achieves significant improvement on two retrieval applications: document retrieval and product search. The framework is applicable to both pre-trained and fine-tuned ERMs and is effective for black-box ERMs.

## Method Summary
PEFA improves retrieval recall by interpolating ERM scores with a kNN model that aggregates relevant passages from similar queries. The framework defines the relevant scoring function as a convex combination between the black-box ERM and a non-parametric kNN model. PEFA has two realizations: PEFA-XS, which reduces deployment overhead by pre-computing the kNN interpolation in the embedding space, and PEFA-XL, which captures query-specific neighborhoods by using two ANN indices. Both versions achieve significant improvements in recall@100 for document retrieval and product search tasks.

## Key Results
- PEFA improves recall@100 of pre-trained ERMs on Trivia-QA by an average of 13.2%
- PEFA improves recall@100 of fine-tuned ERMs on NQ-320K by an average of 5.5%
- PEFA improves recall@100 of fine-tuned ERMs by an average of 5.3% (PEFA-XS) and 14.5% (PEFA-XL) for product search

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PEFA improves retrieval recall by interpolating ERM scores with a kNN model that aggregates relevant passages from similar queries.
- Mechanism: The PEFA scoring function is a convex combination of the ERM similarity score and a kNN similarity score, where the kNN aggregates relevant passages from training queries in the neighborhood of the test query.
- Core assumption: The relevant passages for a test query can be effectively approximated by aggregating relevant passages of similar training queries.
- Evidence anchors:
  - [abstract] "PEFA performs a convex combination of two scoring functions, one from the ERM and the other from the kNN."
  - [section] "PEFA framework defines the relevant scoring function of a query-passage pair (ˆq, pj) as the convex combination between scoring functions of the black-box ERM and a non-parametric kNN model"
  - [corpus] Weak - only 5 related papers found, none directly discussing kNN interpolation for retrieval.
- Break condition: If the kNN model retrieves irrelevant or low-quality passages, the interpolated score may degrade overall recall.

### Mechanism 2
- Claim: PEFA-XS reduces deployment overhead by pre-computing the kNN interpolation in the embedding space, requiring only one ANN index.
- Mechanism: The kNN model in PEFA-XS is independent of the test query, so the interpolation of ERM and kNN scores can be pre-computed offline and stored as a single interpolated passage embedding.
- Core assumption: The kNN model can be approximated as query-independent by using relevant queries for each passage, enabling pre-computation.
- Evidence anchors:
  - [abstract] "PEFA-XS using a single ANN index"
  - [section] "the interpolation of two scoring functions can be pre-computed in the embedding space, as derived in Equation 8"
  - [corpus] Weak - no corpus evidence on pre-computing kNN interpolation.
- Break condition: If the query-independent kNN approximation is too coarse, the recall gain may be limited compared to PEFA-XL.

### Mechanism 3
- Claim: PEFA-XL captures query-specific neighborhoods by using two ANN indices, one on the passage space and one on the query space.
- Mechanism: PEFA-XL retrieves the top-k nearest training queries to the test query and aggregates their relevant passages, requiring ANN search on both query and passage spaces.
- Core assumption: The set of relevant passages for a test query can be effectively approximated by the union of relevant passages from its k nearest training queries.
- Evidence anchors:
  - [abstract] "PEFA-XL (i.e., extra large) using double ANN indices"
  - [section] "the neighborhood is an intersection of the one in PEFA-XS and kNN queries in the training set, which is dependent to the test-time query"
  - [corpus] Weak - no corpus evidence on using double ANN indices for retrieval.
- Break condition: If the nearest training queries are not semantically similar to the test query, the aggregated relevant passages may be irrelevant.

## Foundational Learning

- Concept: Approximate Nearest Neighbor (ANN) search
  - Why needed here: PEFA relies on ANN search to efficiently retrieve similar queries and passages at scale.
  - Quick check question: What is the time complexity of ANN search compared to exact search, and why is this important for large-scale retrieval?

- Concept: Convex combination of scoring functions
  - Why needed here: PEFA interpolates ERM and kNN scores to balance the strengths of both models.
  - Quick check question: How does the interpolation coefficient λ control the trade-off between ERM and kNN scores?

- Concept: k-nearest neighbors (kNN) models
  - Why needed here: The kNN component in PEFA aggregates relevant passages from similar queries to improve recall.
  - Quick check question: How does the choice of k (number of neighbors) affect the quality of the kNN model in PEFA?

## Architecture Onboarding

- Component map:
  ERM encoder -> ANN index for passages -> ANN index for queries (PEFA-XL only) -> kNN model -> PEFA interpolation

- Critical path:
  1. Build ANN indices for passages and (for PEFA-XL) queries
  2. At inference, retrieve top-k nearest neighbors from ANN indices
  3. Compute ERM and kNN scores for candidate passages
  4. Interpolate scores using λ to obtain final PEFA score
  5. Retrieve top-scoring passages as the final result

- Design tradeoffs:
  - PEFA-XS: Lower deployment overhead, moderate recall gain
  - PEFA-XL: Higher deployment overhead, larger recall gain
  - Choice of λ: Balances ERM and kNN contributions
  - Choice of k: Controls the neighborhood size for kNN aggregation

- Failure signatures:
  - Low recall gain: ERM and kNN scores may be misaligned or kNN aggregation is too coarse
  - High inference latency: ANN search on large indices may be slow
  - Large model size: Storing multiple ANN indices may be prohibitive

- First 3 experiments:
  1. Ablation study on interpolation coefficient λ: Test PEFA with different λ values to find the optimal balance between ERM and kNN scores.
  2. Compare PEFA-XS and PEFA-XL: Evaluate the recall gain and deployment overhead of PEFA-XS and PEFA-XL on a sample dataset.
  3. Analyze kNN neighborhood size: Vary the number of nearest neighbors k in PEFA-XL to find the optimal neighborhood size for kNN aggregation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PEFA scale with extremely large datasets (e.g., 100M+ queries and 1B+ passages)?
- Basis in paper: [explicit] The paper mentions product search datasets of 30M products and 100M queries, but does not explore scaling to 100M+ queries and 1B+ passages.
- Why unresolved: The paper does not provide experiments or analysis on how PEFA performs when scaling to datasets significantly larger than the 30M product dataset used.
- What evidence would resolve it: Experimental results demonstrating the performance, memory usage, and runtime of PEFA on datasets with 100M+ queries and 1B+ passages.

### Open Question 2
- Question: What is the impact of different similarity functions (e.g., cosine vs. inner product) on the performance of PEFA?
- Basis in paper: [explicit] The paper mentions that embeddings are ℓ2 normalized, making inner product equivalent to cosine similarity, but does not explore the impact of using different similarity functions.
- Why unresolved: The paper does not provide experiments or analysis on how the choice of similarity function (e.g., cosine vs. inner product) affects the performance of PEFA.
- What evidence would resolve it: Experimental results comparing the performance of PEFA using different similarity functions (e.g., cosine vs. inner product) on various datasets.

### Open Question 3
- Question: How does the performance of PEFA compare to other parameter-free methods for improving ERMs, such as knowledge distillation or label smoothing?
- Basis in paper: [inferred] The paper does not compare PEFA to other parameter-free methods for improving ERMs, such as knowledge distillation or label smoothing.
- Why unresolved: The paper does not provide a comparison between PEFA and other parameter-free methods for improving ERMs, making it difficult to assess the relative effectiveness of PEFA.
- What evidence would resolve it: Experimental results comparing the performance of PEFA to other parameter-free methods for improving ERMs, such as knowledge distillation or label smoothing, on various datasets.

## Limitations
- Weak corpus evidence supporting the core mechanisms, particularly kNN interpolation for retrieval
- Effectiveness depends on the quality of the kNN model, which may struggle with rare or out-of-distribution queries
- Limited comparison to alternative methods for improving ERM recall

## Confidence
- **High Confidence**: The claim that PEFA improves recall@100 for document retrieval (Trivia-QA and NQ-320K) and product search tasks is supported by the reported results, which show consistent improvements across multiple datasets and ERMs.
- **Medium Confidence**: The claim that PEFA-XS reduces deployment overhead by pre-computing the kNN interpolation is plausible given the description, but lacks strong empirical validation and corpus evidence.
- **Low Confidence**: The claim that PEFA-XL captures query-specific neighborhoods using two ANN indices is supported by the description, but the effectiveness of this approach compared to other methods is unclear due to limited corpus evidence and lack of ablation studies.

## Next Checks
1. **Ablation Study on Interpolation Coefficient**: Conduct an ablation study on the interpolation coefficient λ to determine its optimal value and sensitivity to different datasets and ERMs. This will help validate the importance of the convex combination in PEFA's effectiveness.

2. **Comparison with Alternative Methods**: Compare PEFA with alternative methods for improving ERM recall, such as fine-tuning, data augmentation, or other adapter-based approaches. This will help contextualize PEFA's improvements and identify its unique strengths and weaknesses.

3. **Robustness Analysis**: Evaluate the robustness of PEFA to different query distributions, including rare, out-of-distribution, and adversarial queries. This will help assess the limitations of the kNN-based approach and identify potential failure modes.