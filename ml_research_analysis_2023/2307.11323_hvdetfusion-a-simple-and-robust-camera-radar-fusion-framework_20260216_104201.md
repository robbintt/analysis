---
ver: rpa2
title: 'HVDetFusion: A Simple and Robust Camera-Radar Fusion Framework'
arxiv_id: '2307.11323'
source_url: https://arxiv.org/abs/2307.11323
tags:
- radar
- detection
- data
- point
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HVDetFusion is a multi-modal 3D object detection framework for
  autonomous driving that effectively combines camera and radar data. The method builds
  on the Bevdet4D architecture and introduces a decoupling fusion branch to process
  radar data, filtering out false positives and fusing the remaining radar point cloud
  with BEV features generated from camera images.
---

# HVDetFusion: A Simple and Robust Camera-Radar Fusion Framework

## Quick Facts
- arXiv ID: 2307.11323
- Source URL: https://arxiv.org/abs/2307.11323
- Reference count: 28
- Primary result: Achieves 67.4% NDS score on nuScenes test set, outperforming previous camera-radar fusion methods by 5% NDS and 3.4% mAP

## Executive Summary
HVDetFusion is a multi-modal 3D object detection framework for autonomous driving that effectively combines camera and radar data. The method builds on the Bevdet4D architecture and introduces a decoupling fusion branch to process radar data, filtering out false positives and fusing the remaining radar point cloud with BEV features generated from camera images. This approach addresses the challenge of depth information loss in camera-only detection by leveraging radar's positioning and velocity information. The model supports three input configurations: camera images from six perspectives, single front-view camera images, or a combination of camera and radar data.

## Method Summary
HVDetFusion is a two-stage multi-modal 3D object detection framework based on the Bevdet4D architecture. It uses a decoupling fusion branch to process radar data independently, filtering out false positives before fusing with BEV features from camera images. The model processes sequences of 17 keyframes (current frame, 8 past, 8 future) using temporal attention to improve detection accuracy. It supports multiple input configurations including six-camera setups, single front-view cameras, and camera-radar combinations. The framework is trained on the nuScenes dataset using AdamW optimizer with gradient clipping and achieves state-of-the-art performance on the nuScenes benchmark.

## Key Results
- Achieves 67.4% NDS score on nuScenes test set
- Outperforms previous camera-radar fusion methods by 5% NDS and 3.4% mAP
- Shows particular strength in detecting smaller objects like pedestrians and traffic cones, with improvements of 8.6%, 7.8%, and 12.1% respectively compared to previous best methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The decoupling fusion branch processes radar data independently, filtering out false positives before fusing with BEV features, which improves detection accuracy.
- Mechanism: The system first uses position priors from camera-based predictions to filter radar point clouds. By matching radar 2D bounding boxes with image-derived object locations, only radar points that align spatially are retained. This filtered radar feature map is then concatenated with the camera BEV features and passed to a secondary regression head that refines the final predictions.
- Core assumption: Radar point clouds contain significant noise and false positives, and these can be effectively filtered using camera-derived object position priors.
- Evidence anchors:
  - [abstract] "The method builds on the Bevdet4D architecture and introduces a decoupling fusion branch to process radar data, filtering out false positives and fusing the remaining radar point cloud with BEV features generated from camera images."
  - [section 3.2] "In order to prevent false positive noise information from negatively affecting the training results of the model after fusion, it is necessary to filter the radar point cloud and retain the valid point cloud to participate in the subsequent fusion operation."
  - [corpus] No direct corpus evidence for this specific mechanism; the closest related work mentions similar filtering but lacks detail.
- Break condition: If the camera-derived position priors are inaccurate or unstable, the filtering step will fail to remove false positives or may incorrectly remove valid radar data.

### Mechanism 2
- Claim: Using multiple temporal frames (8 past and 8 future) improves detection accuracy by providing temporal context for object motion.
- Mechanism: The model processes a sequence of 17 keyframes (current frame, 8 past, 8 future) and uses temporal attention to compare candidate features across frames. This allows the network to resolve ambiguities in object velocity and position that would be unclear from a single frame.
- Core assumption: Object motion is smooth and predictable over short time intervals, so information from neighboring frames can be reliably used to improve current-frame predictions.
- Evidence anchors:
  - [abstract] "Bevdet4D[3] uses multiple sequence keyframes to improve the detection of Bevdet from 3D spatial domain to spatio-temporal domain with temporal information to access the temporal cues by querying and comparing the two candidate features which could greatly reduces the velocity error."
  - [section 4.1] "During the ablation experiment, we uniformly use the verification datasets which provided by nuScenes officially as evaluation object. We use the HVDetFusion network with ResNet50 as the baseline to explore the influence of the backbone structure, the number of previous frames and future frames..."
  - [corpus] Weak evidence; no direct citation for the specific 8+8 frame configuration in the corpus, though temporal fusion is a known technique.
- Break condition: If the frame rate is too low or objects move erratically, temporal cues become unreliable and may degrade accuracy.

### Mechanism 3
- Claim: Increasing BEV grid resolution (from 128x128 to 256x256) preserves more spatial detail, improving detection of smaller objects like pedestrians and traffic cones.
- Mechanism: The model maps image features to a BEV grid at higher resolution, which provides finer-grained spatial representation. This allows the detector to better localize small objects that would be lost or merged at lower resolution.
- Core assumption: Higher spatial resolution in BEV space directly translates to better detection performance for small objects, without causing overfitting or computational bottlenecks.
- Evidence anchors:
  - [section 4.1] "When using configuration 1 for training, we use the same backbone as Bevdet, and set the BEV size to 128*128... Configuration 2 is modified on the basis of configuration 1, it's used for the size adaptation experiment of backbone and BEV grid, and finally we propose configuration 3, which is used for the training and evaluation process of 640*1600 resolution images. At this time, larger parameters are chosen as BEV size and BEV channels."
  - [section 6] "It can be seen from lines (5&7) that the 640x1600 resolution is 5.4% higher than the 256x704 resolution on NDS indicator."
  - [corpus] No direct corpus evidence for this specific claim; resolution impact is inferred from ablation results.
- Break condition: If the backbone or BEV encoder cannot effectively utilize the higher resolution, or if the dataset lacks sufficient detail, the performance gain may be minimal or even negative.

## Foundational Learning

- Concept: Bird's-Eye View (BEV) representation
  - Why needed here: The model fuses multi-camera images and radar data into a common BEV space for 3D object detection.
  - Quick check question: How does projecting image features into BEV help unify perception from multiple camera angles?

- Concept: Sensor fusion and complementary modalities
  - Why needed here: Cameras provide rich appearance and semantic cues but poor depth; radar provides reliable range and velocity but sparse spatial data. The fusion leverages both strengths.
  - Quick check question: What are the key differences between radar and camera data in terms of spatial and velocity information?

- Concept: Temporal feature aggregation
  - Why needed here: Using multiple frames helps resolve motion ambiguities and improve prediction stability for dynamic objects.
  - Quick check question: Why does including future frames in the input sequence help improve current-frame detection?

## Architecture Onboarding

- Component map: Image → Backbone → View Transformer → BEV Encoder → Main Head → Radar Fusion (optional) → Secondary Head → Output
- Critical path: Image → Backbone → View Transformer → BEV Encoder → Main Head → Radar Fusion (optional) → Secondary Head → Output
- Design tradeoffs:
  - Resolution vs speed: Higher BEV grid resolution (256x256) improves small object detection but increases compute.
  - Temporal frames vs latency: More frames improve accuracy but increase inference delay.
  - Radar filtering vs recall: Stricter filtering reduces false positives but may drop valid radar points.
- Failure signatures:
  - Low NDS but high mAP: Likely overfitting to large objects, missing small ones.
  - High false positive rate: Radar filtering is too permissive or camera position priors are noisy.
  - Large velocity errors: Temporal feature aggregation is broken or radar velocity compensation is wrong.
- First 3 experiments:
  1. Validate that radar filtering reduces false positives: Feed noisy radar data through the filtering branch and measure the ratio of valid to invalid points.
  2. Test BEV resolution impact: Run detection at 128x128 vs 256x256 BEV grid and compare small object mAP.
  3. Ablate temporal context: Compare results with 0, 4, 8 past/future frames to find optimal temporal window.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal hyperparameter α (size scaling factor) and β (threshold) for radar point cloud filtering in different driving scenarios?
- Basis in paper: [explicit] The paper mentions using hyperparameters α as the size scaling factor of the 2D bounding box and β as the threshold to control the difficulty of matching, stating that "When the hyperparameters setting is reasonable, noise points with large position deviations can be filtered out."
- Why unresolved: The paper does not provide specific optimal values for these hyperparameters and suggests they need to be tuned for different scenarios.
- What evidence would resolve it: Empirical results showing the performance of HVDetFusion with different α and β values across various driving scenarios and datasets.

### Open Question 2
- Question: How does the performance of HVDetFusion compare when using radar data from different numbers of radar sensors?
- Basis in paper: [inferred] The paper uses radar data from 5 radar sensors in the nuScenes dataset but does not explore the impact of using fewer or more sensors.
- Why unresolved: The paper does not provide comparative results using different numbers of radar sensors, which could affect the robustness and accuracy of the fusion.
- What evidence would resolve it: Comparative experiments showing the performance of HVDetFusion with different numbers of radar sensors and their impact on detection accuracy.

### Open Question 3
- Question: What is the impact of using different types of camera backbones (e.g., ResNet-50, ConvNeXt-B, InternImage-B) on the overall performance of HVDetFusion?
- Basis in paper: [explicit] The paper mentions using different backbones such as ResNet-50, ConvNeXt-B, and InternImage-B, and notes that "using a larger model or the BEV feature resolution is larger, the network tends to diverge."
- Why unresolved: While the paper provides some results with different backbones, it does not comprehensively analyze the impact of each backbone on the final performance.
- What evidence would resolve it: Detailed ablation studies comparing the performance of HVDetFusion with each backbone type across various metrics and scenarios.

## Limitations

- Radar filtering mechanism lacks detailed implementation description and hyperparameter specifications
- Temporal context impact not thoroughly analyzed for different temporal window sizes
- BEV resolution tradeoff between computational cost and performance gain not fully explored

## Confidence

- High confidence: Overall architecture design and multi-modal fusion approach are well-established in the literature
- Medium confidence: Specific mechanisms for radar filtering and temporal feature aggregation described but lack sufficient detail for independent verification
- Low confidence: Claim that the decoupling fusion branch is "simple and robust" not substantiated with robustness tests across different scenarios

## Next Checks

1. **Radar filtering validation**: Implement the radar point cloud filtering algorithm with the described position prior matching and measure the ratio of false positives removed vs. valid points retained across different scenes and object types.
2. **Temporal window ablation**: Systematically test HVDetFusion with 0, 4, 8, and 12 temporal frames (past/future) to quantify the marginal benefit of additional frames and identify the optimal temporal window for different object categories.
3. **BEV resolution efficiency**: Compare the performance and computational cost of HVDetFusion at 128x128 vs 256x256 BEV resolution across different backbone architectures to determine if the higher resolution provides proportional benefits for small object detection.