---
ver: rpa2
title: Do LLMs exhibit human-like response biases? A case study in survey design
arxiv_id: '2311.04076'
source_url: https://arxiv.org/abs/2311.04076
tags:
- response
- questions
- bias
- human
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether LLMs exhibit human-like response
  biases using survey design as a case study. We introduce a framework called BiasMonkey
  that evaluates LLM behavior on modified questions designed to elicit known human
  response biases.
---

# Do LLMs exhibit human-like response biases? A case study in survey design

## Quick Facts
- arXiv ID: 2311.04076
- Source URL: https://arxiv.org/abs/2311.04076
- Reference count: 40
- Primary result: Popular LLMs generally fail to reflect human-like response biases in survey design contexts

## Executive Summary
This work investigates whether large language models exhibit human-like response biases using survey design as a case study. The authors introduce BiasMonkey, a framework that evaluates LLM behavior on modified questions designed to elicit known human response biases. They generate a dataset of 995 question pairs across five response biases and three non-bias perturbations, sampling 50 responses from nine models per question. The findings show that popular LLMs generally fail to reflect human-like behavior, with instruction fine-tuned models demonstrating notably less human-like responses. Even models that show significant changes in the same direction as humans are sensitive to perturbations that do not elicit significant changes in humans, highlighting the pitfalls of using LLMs as human proxies.

## Method Summary
The authors use the BiasMonkey framework to generate datasets of original and modified questions based on five response biases and three non-bias perturbations. They collect 50 responses per question from nine models using provided prompt templates. The evaluation measures the degree of change in responses and uncertainty for each model, comparing results to known human behavior and across bias types using statistical tests.

## Key Results
- Popular LLMs generally fail to reflect human-like response biases in survey contexts
- Instruction fine-tuned models demonstrate notably less human-like responses to wording changes
- LLMs are overly sensitive to non-semantic perturbations that humans are robust to

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs do not reliably replicate human response biases because their training data and optimization objectives do not encode human-like interpretive processes.
- Mechanism: When faced with survey question modifications that trigger known human biases, LLMs lack the contextual reasoning and social heuristics that humans use to interpret such changes. Instead, they apply pattern-matching heuristics that may not align with human decision-making, leading to inconsistent or opposite directional changes.
- Core assumption: Human response biases arise from interpretive heuristics that LLMs have not learned through text prediction alone.
- Evidence anchors:
  - [abstract] "instruction fine-tuned models demonstrating notably less human-like responses"
  - [section] "models that have been RLHF-ed tend to be more confident compared to the other models that we evaluated"
  - [corpus] "Break the Checkbox: Challenging Closed-Style Evaluations of Cultural Alignment in LLMs"

### Mechanism 2
- Claim: LLMs are overly sensitive to non-semantic perturbations (typos, letter swaps) that humans are robust to, revealing a lack of true semantic understanding.
- Mechanism: Minor textual noise that does not alter meaning for humans causes LLMs to shift response distributions significantly, suggesting they rely heavily on surface-level textual patterns rather than deep semantic comprehension.
- Core assumption: Human robustness to typographical noise reflects a semantic processing layer absent in LLMs.
- Evidence anchors:
  - [abstract] "even if a model shows a significant change in the same direction as humans, we find that perturbations that are not meant to elicit significant changes in humans may also result in a similar change"
  - [section] "LLMs are sensitive to both bias modifications and non-bias perturbations"
  - [corpus] "Large Language Models Show Human-like Social Desirability Biases in Survey Responses"

### Mechanism 3
- Claim: Instruction fine-tuning and RLHF can reduce human-like bias expression by prioritizing task completion over human-like interpretive flexibility.
- Mechanism: Fine-tuning for instruction-following and reward modeling shifts LLM behavior toward confident, consistent responses, reducing the variability and context-sensitivity characteristic of human bias expression.
- Core assumption: Human-like biases involve interpretive uncertainty and context sensitivity that conflict with the objectives of instruction fine-tuning and RLHF.
- Evidence anchors:
  - [abstract] "instruction fine-tuned models demonstrating notably less human-like responses"
  - [section] "instruction fine-tuned models are more likely to exhibit significant changes as a result of non-bias perturbations"
  - [corpus] "Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning"

## Foundational Learning

- Concept: Human response biases in survey design
  - Why needed here: The paper's core evaluation framework depends on comparing LLM behavior to known human bias patterns; understanding these biases is essential to interpret results.
  - Quick check question: What is the acquiescence bias and how does it manifest in survey responses?

- Concept: Statistical significance testing (t-tests)
  - Why needed here: The paper uses t-tests to determine whether LLM response changes are statistically significant; understanding this is necessary to evaluate claims about model behavior.
  - Quick check question: What is the null hypothesis in the t-test used to evaluate ¯Δb?

- Concept: Semantic vs. surface-level processing in NLP models
  - Why needed here: The paper's findings about sensitivity to typos and perturbations hinge on the distinction between semantic understanding and pattern matching; grasping this distinction is key to interpreting model limitations.
  - Quick check question: Why might an LLM be more sensitive to letter swaps than a human?

## Architecture Onboarding

- Component map: Dataset generation → LLM querying (with consistent prompts) → response aggregation → statistical analysis (t-tests) → bias/perturbation comparison → result interpretation
- Critical path: Dataset generation → LLM querying (with consistent prompts) → response aggregation → statistical analysis (t-tests) → bias/perturbation comparison → result interpretation
- Design tradeoffs: Using sampling to approximate response distributions trades exactness for scalability; fixed prompt templates ensure comparability but may limit naturalistic responses; statistical tests provide objectivity but may miss nuanced behavioral patterns
- Failure signatures: If ¯Δb is consistently near zero across biases, it suggests models are not sensitive to survey modifications; if p-values are uniformly high, statistical power may be insufficient; if models show large changes to non-bias perturbations, it indicates over-sensitivity to noise
- First 3 experiments:
  1. Run a single bias (e.g., acquiescence) on one model (e.g., Llama2-7b) to verify dataset generation and response collection pipeline
  2. Compare ¯Δb for bias vs. non-bias perturbations on a single question pair to validate the perturbation comparison logic
  3. Scale up to all nine models on all five biases, checking for statistical significance patterns and consistency with reported results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do other instruction-tuned or RLHF-ed models behave on response bias evaluations?
- Basis in paper: [inferred] The paper found that instruction fine tuned models like GPT-3.5-turbo demonstrated notably less human-like responses to wording changes compared to base models. This suggests that instruction tuning may negatively impact a model's ability to exhibit human-like response biases.
- Why unresolved: The study only evaluated a limited set of instruction-tuned models (GPT-3.5-turbo, GPT-3.5-turbo-instruct, Llama2-chat variants). It is unclear if this trend generalizes to other instruction-tuned or RLHF-ed models.
- What evidence would resolve it: Evaluating a broader range of instruction-tuned and RLHF-ed models (e.g., Claude, Bard, PaLM-2) on the same response bias dataset would provide more conclusive evidence on the impact of instruction tuning on human-like response behavior.

### Open Question 2
- Question: Can steering techniques be developed to reliably elicit human-like response biases in LLMs?
- Basis in paper: [explicit] The paper briefly explored steering prompts to induce human-like biases in GPT-3.5-turbo and GPT-3.5-turbo-instruct for allow/forbid and response order biases, with limited success. This suggests that steering may be a viable approach, but more research is needed.
- Why unresolved: The steering analysis was preliminary and only tested two GPT models on two biases. It is unclear if steering can be generalized to other models and biases.
- What evidence would resolve it: Developing and evaluating a comprehensive set of steering techniques (e.g., chain-of-thought reasoning, explicit instructions) across a wide range of models and biases would provide insights into the feasibility and limitations of steering for eliciting human-like response biases.

### Open Question 3
- Question: How do response biases in LLMs vary across different question topics and domains?
- Basis in paper: [inferred] The paper observed that model behavior on response biases can vary depending on the question topic (e.g., economy, education, immigration). This suggests that the nature of the question topic may influence a model's ability to exhibit human-like response biases.
- Why unresolved: The analysis only considered a limited set of question topics from the American Trends Panel. It is unclear how response biases manifest in other domains or when using different question sources.
- What evidence would resolve it: Evaluating models on a more diverse set of question topics and domains (e.g., scientific, technical, cultural) would provide a more comprehensive understanding of how response biases vary across different contexts.

## Limitations

- Generalizability: The study uses a specific dataset (Pew Research's ATP) and nine models, limiting conclusions about broader LLM populations or other survey contexts.
- Implementation details: Precise question modifications and prompt templates are not fully specified, introducing potential variability in replication attempts.
- Statistical power: While 50 samples per question were collected, the impact of sampling variability on detecting subtle bias patterns is unclear.

## Confidence

- High confidence: LLMs generally fail to reflect human-like response biases; instruction fine-tuned models show notably less human-like responses.
- Medium confidence: LLMs are overly sensitive to non-semantic perturbations that humans are robust to.
- Medium confidence: Instruction fine-tuning and RLHF reduce human-like bias expression by prioritizing task completion.

## Next Checks

1. Replicate the bias perturbation experiments on a subset of questions using the provided framework, comparing results to human baseline data.
2. Test model sensitivity to controlled noise perturbations (e.g., letter swaps, typos) to verify claims about semantic robustness.
3. Compare pre- and post-instruction-tuned versions of the same model (e.g., Llama2-7b vs. Llama2-7b-chat) on bias response patterns to isolate fine-tuning effects.