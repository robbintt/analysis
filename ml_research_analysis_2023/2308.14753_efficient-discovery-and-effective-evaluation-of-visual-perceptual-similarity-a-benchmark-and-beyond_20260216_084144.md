---
ver: rpa2
title: 'Efficient Discovery and Effective Evaluation of Visual Perceptual Similarity:
  A Benchmark and Beyond'
arxiv_id: '2308.14753'
source_url: https://arxiv.org/abs/2308.14753
tags:
- image
- similarity
- pages
- pairs
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of evaluating methods for visual
  similarity discovery (VSD), where the goal is to retrieve images of different objects
  with high perceptual visual similarity. The authors argue that existing evaluation
  methods, which rely on identification-retrieval tasks, are limited and propose that
  faithful evaluation must rely on expert annotations.
---

# Efficient Discovery and Effective Evaluation of Visual Perceptual Similarity: A Benchmark and Beyond

## Quick Facts
- arXiv ID: 2308.14753
- Source URL: https://arxiv.org/abs/2308.14753
- Reference count: 40
- Key outcome: Introduces EDS method for efficient expert labeling of 110K+ image pairs and proposes ROC-AUC/PR-AUC metrics to mitigate evaluation bias in visual similarity discovery

## Executive Summary
This paper addresses the challenge of evaluating visual similarity discovery (VSD) methods, which retrieve images of different objects with high perceptual visual similarity. The authors argue that traditional evaluation methods relying on identification-retrieval tasks are insufficient and propose that faithful evaluation requires expert annotations. They introduce an Efficient Discovery of Similarities (EDS) method that leverages multiple VSD models to generate candidate pairs, reducing labeling effort by ~1400x while maintaining high recall of true positives. The paper also proposes using ROC-AUC and PR-AUC metrics instead of traditional Hit Rate and MRR to mitigate bias from incomplete labeling and model-dependent candidate generation. Experiments demonstrate that supervised finetuning for recognition tasks can actually degrade VSD performance, highlighting the importance of their proposed benchmark and evaluation methodology.

## Method Summary
The method introduces EDS (Efficient Discovery of Similarities) which uses a small set of high-performing VSD models to generate candidate pairs for expert annotation. For each query image, top-k candidates are collected from multiple models and deduplicated to create a candidate set. Human domain experts then label these pairs, with majority voting determining final ground truth labels. The approach is applied to DeepFashion dataset (ICR and CCR subsets), creating the first large-scale VSD benchmark with over 110K expert-annotated pairs. For supervised baselines, models are finetuned with identity or category labels for 50 epochs using Adam optimizer with weight decay. Evaluation uses both traditional metrics (HR@k, MRR@k) and proposed bias-mitigating metrics (ROC-AUC, PR-AUC).

## Key Results
- EDS reduces labeling effort by ~1400x compared to brute-force pairwise annotation while maintaining high recall of true positive pairs
- Supervised finetuning approaches fall significantly behind their pretrained counterparts on discovery tasks, even when achieving perfect identification performance
- ROC-AUC and PR-AUC metrics demonstrate better robustness to evaluation bias compared to Hit Rate and MRR when comparing models that didn't participate in ground truth generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using a small set of high-performing VSD models to generate candidate pairs dramatically reduces labeling effort while preserving high recall of true positives.
- Mechanism: The EDS method leverages top-ranked candidates from multiple VSD models (Sk = ∪m∈M Smk) to construct a candidate set S_k that is much smaller than the full pairwise space A but still contains a high fraction of positive pairs.
- Core assumption: The top-ranked candidates from multiple diverse VSD models have a high probability of containing true positive pairs, i.e., p_k ≫ p where p_k is the fraction of positives in S_k.
- Evidence anchors: [abstract] "Based on these insights, we propose a novel and efficient labeling procedure that can be applied to any dataset."; [section 3.3.1] "For example, in the DF in-catalog dataset, we had |D| = 52, 712, |Q| = 2, 000, |M| = 6, and k = 6. In this case, the labeling procedure with EDS is ~1,400x faster (and cheaper) than with the brute-force method."

### Mechanism 2
- Claim: Using ROC-AUC and PR-AUC metrics instead of Hit Rate and MRR mitigates bias from incomplete labeling and model-dependent candidate generation.
- Mechanism: These metrics evaluate the model's ability to rank positive pairs higher than negative pairs relative to each other, rather than requiring absolute top-k ranking, making them robust to missing labels in the evaluation set.
- Core assumption: The ground truth labels provided by human experts are accurate and the missing labels (pairs not suggested by any model) are mostly negative, so relative ranking performance is a fair measure of similarity discovery ability.
- Evidence anchors: [abstract] "we propose metrics to mitigate those limitations" referring to model bias in evaluation; [section 4] "ROC-AUC measures the probability that a random positive is ranked higher than a random negative" and explicitly discusses why this mitigates bias.

### Mechanism 3
- Claim: Supervised finetuning on identity or category labels does not necessarily improve VSD performance and may even degrade it.
- Mechanism: The paper demonstrates through experiments that models finetuned for recognition tasks (ID, CAT) perform worse on discovery tasks compared to their pretrained counterparts, suggesting the tasks require different learned representations.
- Core assumption: The representations learned for classification/recognition tasks are not optimal for perceptual similarity discovery, which may require different feature abstractions.
- Evidence anchors: [abstract] "supervised methods for VSD, even when achieving perfect identification performance, do not necessarily perform well on the discovery task"; [section 6.4] "supervised finetuning approaches fall significantly behind their pretrained counterparts" with quantitative results showing degradation in AUC metrics.

## Foundational Learning

- Concept: Content-based image retrieval (CBIR) fundamentals
  - Why needed here: Understanding CBIR provides context for why similarity discovery is challenging and why proxy tasks like identification are insufficient.
  - Quick check question: What is the fundamental difference between identification-retrieval and visual similarity discovery tasks?

- Concept: Information retrieval evaluation metrics and their limitations
  - Why needed here: The paper critiques traditional IR metrics (HR, MRR) and proposes alternatives (ROC-AUC, PR-AUC), so understanding these metrics and their biases is crucial.
  - Quick check question: Why might Hit Rate and MRR be biased when evaluating models that didn't participate in generating the ground truth?

- Concept: Supervised vs. self-supervised learning tradeoffs
  - Why needed here: The paper shows supervised finetuning can hurt VSD performance, highlighting the importance of understanding when different learning paradigms are appropriate.
  - Quick check question: What might be a reason that self-supervised models perform better than supervised models on VSD tasks?

## Architecture Onboarding

- Component map: Vision models (Argus, DINO, BEiT, CLIP) -> candidate generation (Sk) -> human annotation -> ground truth creation -> model evaluation
- Critical path: Vision models → candidate generation (Sk) → human annotation → ground truth creation → model evaluation
- Design tradeoffs:
  - Candidate set size (k) vs. labeling efficiency: larger k captures more positives but increases annotation cost
  - Number of vision models (M) vs. coverage: more models provide diverse candidates but increase complexity
  - Metric choice: ROC-AUC/PR-AUC are robust to missing labels but may be less intuitive than HR/MRR
- Failure signatures:
  - Low positive discovery rate (p_k close to p) indicates vision models are poor similarity proxies
  - High variance in AUC across leave-one-out experiments suggests ground truth incompleteness
  - Supervised models outperforming pretrained ones on discovery tasks would contradict paper's findings
- First 3 experiments:
  1. Implement EDS with a small set of vision models and k=5, measure labeling efficiency vs. random sampling
  2. Compare ROC-AUC and PR-AUC vs. HR@5 and MRR@5 on the same dataset to demonstrate bias mitigation
  3. Evaluate pretrained vs. supervised-finetuned versions of the same backbone on discovery task to reproduce the performance difference

## Open Questions the Paper Calls Out

- How do different human domain expert groups affect the annotations and evaluation results in the Efficient Discovery of Similarities (EDS) method?
  - Basis in paper: [explicit] The paper mentions using a group of E experts for labeling and following a simple majority voting, but does not explore the impact of different expert groups.
  - Why unresolved: The paper does not provide any analysis on how the composition or characteristics of the expert group might influence the annotation process or the final evaluation results.
  - What evidence would resolve it: Conducting experiments with different groups of experts and comparing the resulting annotations and evaluation metrics would provide insights into the impact of expert group variations.

- How does the Efficient Discovery of Similarities (EDS) method perform when applied to domains other than fashion?
  - Basis in paper: [explicit] The paper discusses the potential application of the EDS method to other domains but does not provide empirical evidence of its effectiveness outside the fashion domain.
  - Why unresolved: The paper focuses solely on the fashion domain and does not explore the method's performance in other areas such as natural language understanding or audio analytics, as mentioned in the conclusion.
  - What evidence would resolve it: Applying the EDS method to datasets from different domains and evaluating its performance would demonstrate its generalizability and effectiveness beyond the fashion domain.

- What are the optimal parameters (e.g., k, number of models M) for the Efficient Discovery of Similarities (EDS) method to achieve the best trade-off between efficiency and accuracy?
  - Basis in paper: [explicit] The paper uses specific values for k and the number of models M but does not explore the impact of varying these parameters on the method's performance.
  - Why unresolved: The paper does not provide a sensitivity analysis or an exploration of the parameter space to determine the optimal settings for different datasets or tasks.
  - What evidence would resolve it: Conducting experiments with different parameter values and analyzing their impact on the efficiency and accuracy of the EDS method would help identify the optimal settings for various scenarios.

## Limitations

- The core EDS mechanism relies on the assumption that multiple VSD models' top-ranked candidates have high precision for true positives, but this hasn't been validated independently
- The claim that supervised finetuning degrades VSD performance is demonstrated but the mechanism (why classification features don't transfer) remains speculative
- The evaluation bias mitigation through ROC-AUC/PR-AUC assumes missing labels are mostly negative, which is reasonable but unverified

## Confidence

- High confidence: The EDS efficiency claims (1400x speedup), basic evaluation methodology, and supervised vs. pretrained comparison results
- Medium confidence: The mechanism explanations for why supervised finetuning hurts performance and why ROC-AUC/PR-AUC mitigate bias
- Low confidence: Generalizability of EDS to other domains and the assumption that the expert-annotated ground truth is complete enough for reliable evaluation

## Next Checks

1. Test EDS on a different dataset (e.g., general image retrieval) to verify the candidate precision assumption holds across domains
2. Analyze the distribution of missing labels in the evaluation set to quantify how many true positives might be excluded
3. Investigate whether supervised models with different architectures or objectives (e.g., contrastive learning) maintain or improve VSD performance compared to pretrained versions