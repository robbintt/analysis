---
ver: rpa2
title: An Empirical Study of Instruction-tuning Large Language Models in Chinese
arxiv_id: '2310.07328'
source_url: https://arxiv.org/abs/2310.07328
tags:
- chinese
- llms
- instruction
- table
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive empirical study of instruction-tuning
  large language models (LLMs) in Chinese, addressing the gap in research for the
  world''s most spoken language. The authors systematically explore the impact of
  three crucial elements in instruction-tuning: LLM bases, parameter-efficient methods,
  and instruction data types.'
---

# An Empirical Study of Instruction-tuning Large Language Models in Chinese

## Quick Facts
- arXiv ID: 2310.07328
- Source URL: https://arxiv.org/abs/2310.07328
- Authors: 
- Reference count: 23
- Primary result: Comprehensive empirical study of instruction-tuning LLMs in Chinese, identifying Bloom as the best foundation model and LoRA as the optimal parameter-efficient method

## Executive Summary
This paper presents the first comprehensive empirical study of instruction-tuning large language models in Chinese, addressing a critical gap in the rapidly evolving field of LLMs. The authors systematically investigate three crucial elements: foundation models, parameter-efficient methods, and instruction datasets, conducting extensive experiments across multiple popular open LLMs including LLaMA, Bloom, and ChatGLM. Their findings reveal that Bloom is the most suitable foundation model for Chinese instruction-tuning, while LoRA-based methods outperform other parameter-efficient approaches. The study also explores the impact of chain-of-thought data, vocabulary expansion, and human-value alignment, ultimately releasing a powerful Chinese LLM that achieves performance comparable to ChatGLM.

## Method Summary
The study systematically explores instruction-tuning of LLMs in Chinese through three main experimental dimensions: foundation models (LLaMA, Bloom, ChatGLM, etc.), parameter-efficient methods (LoRA, AdaLoRA, Prefix-tuning, etc.), and instruction datasets (Alpaca-GPT4, Belle, ShareGPT-zh, etc.). The authors conduct extensive experiments using these combinations, evaluating performance on Belle-eval (AGI capability) and MMCU (professional knowledge) benchmarks. They also investigate additional factors including chain-of-thought data effects, vocabulary expansion strategies, language of prompts (Chinese vs English), and human-value alignment techniques. The methodology employs parameter-efficient fine-tuning as the primary approach due to its broader applicability and resource efficiency compared to full fine-tuning.

## Key Results
- Bloom emerges as the most suitable foundation model for Chinese instruction-tuning across multiple datasets and evaluation metrics
- LoRA-based parameter-efficient methods, particularly SadapterH, outperform other PEFT approaches in instruction-following capability
- Instruction datasets combining self-instruct methods (Alpaca-GPT4, Belle) with real human-ChatGPT conversations (ShareGPT-zh) yield the best overall performance
- Chinese prompts significantly improve performance for models with weaker Chinese abilities, while English prompts work better for models with strong Chinese foundations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-tuning with Chinese-specific data improves LLM performance in Chinese more than English prompts alone
- Mechanism: Chinese instruction datasets contain diverse, culturally relevant tasks that align better with Chinese linguistic structures than translated English instructions
- Core assumption: The model's pre-training corpus includes sufficient Chinese tokens for base understanding, but instruction-tuning data quality determines fine-grained instruction-following ability
- Evidence anchors:
  - [abstract] "This paper makes an in-depth empirical study of instruction-tuning LLMs in Chinese"
  - [section] "We collect a range of LLMs, parameter-efficient methods, and instruction datasets"
  - [corpus] Weak - corpus shows related papers on Chinese instruction-tuning but no direct evidence for this specific mechanism
- Break condition: If the pre-trained model lacks sufficient Chinese token coverage (e.g., LLaMA with 32K vocab), instruction-tuning with Chinese data may not improve performance

### Mechanism 2
- Claim: Parameter-efficient methods like LoRA enable instruction-tuning on resource-constrained systems while maintaining performance
- Mechanism: Adapter layers added to attention and MLP blocks capture instruction-following patterns without updating full model parameters, reducing computational requirements
- Core assumption: The pre-trained model has sufficient capacity to adapt to new tasks through small parameter additions rather than full fine-tuning
- Evidence anchors:
  - [abstract] "The unexpected disclosure of the pre-trained LLaMA model changes this situation, and has sparked a surge of excitement in the LLM research community"
  - [section] "To address this problem, Alpaca-LoRA extends the parameter-efficient method LoRA to LLaMA"
  - [corpus] Weak - related papers mention parameter-efficient methods but not specific evidence for LoRA's effectiveness in this Chinese context
- Break condition: If the adapter size is too small relative to task complexity, or if the base model's weights are incompatible with the adapter architecture

### Mechanism 3
- Claim: Chain-of-thought data improves reasoning performance on complex tasks requiring multi-step logic
- Mechanism: CoT data teaches models to generate intermediate reasoning steps before final answers, enabling better handling of mathematical and analytical problems
- Core assumption: The model can learn to generate and follow reasoning chains rather than jumping directly to answers
- Evidence anchors:
  - [abstract] "We also conduct experiment to study the impact of other factors, e.g., chain-of-thought data"
  - [section] "We collect 9 CoT datasets and their prompts from FLAN...to analyze the impact of CoT data for LLMs"
  - [corpus] Weak - corpus shows related work on CoT but not specific evidence for Chinese instruction-tuning with CoT
- Break condition: If the CoT data quality is poor or the reasoning steps are not properly aligned with Chinese linguistic patterns

## Foundational Learning

- Concept: Parameter-efficient fine-tuning
  - Why needed here: Full fine-tuning requires prohibitive computational resources for large models; PEFT methods like LoRA enable instruction-tuning with limited resources
  - Quick check question: What is the parameter count difference between LoRA adapters and full model fine-tuning for a 7B parameter model?

- Concept: Instruction dataset construction methods
  - Why needed here: Different dataset construction approaches (self-instruct, collection, human-machine joint) yield varying instruction quality and task diversity
  - Quick check question: How does the self-instruct method differ from collecting real human-ChatGPT conversations in terms of instruction quality?

- Concept: Multilingual model evaluation
  - Why needed here: Evaluating Chinese LLMs requires benchmarks that test both general instruction-following and domain-specific knowledge
  - Quick check question: Why are two different benchmarks (Belle-eval for AGI capability and MMCU for professional knowledge) needed to comprehensively evaluate Chinese LLMs?

## Architecture Onboarding

- Component map: Base LLM (Bloom, LLaMA, ChatGLM, etc.) -> Parameter-efficient method (LoRA, AdaLoRA, Prefix-tuning, etc.) -> Instruction dataset (Alpaca-GPT4, Belle, ShareGPT-zh, etc.) -> Evaluation benchmarks (Belle-eval, MMCU) -> CoT data (optional) -> Human-value alignment data (optional)

- Critical path: Base LLM → Parameter-efficient method → Instruction dataset → Evaluation → Iteration

- Design tradeoffs:
  - Adapter size vs. performance: Larger adapters improve performance but increase computational cost
  - Dataset diversity vs. dataset size: More diverse datasets improve generalization but may reduce task-specific performance
  - Chinese vs. English prompts: Chinese prompts improve Chinese performance but may reduce performance on English tasks

- Failure signatures:
  - Catastrophic forgetting: Model loses base capabilities after instruction-tuning
  - Overfitting: Model performs well on training data but poorly on evaluation benchmarks
  - Vocabulary mismatch: Model fails to understand Chinese characters not in pre-training vocabulary

- First 3 experiments:
  1. Compare LoRA vs. full fine-tuning on Bloom with Alpaca-GPT4 dataset, measuring performance and resource usage
  2. Test different instruction datasets (Alpaca-GPT4 vs. Belle vs. ShareGPT-zh) on Bloom with LoRA, measuring Belle-eval performance
  3. Evaluate impact of Chinese vs. English prompts on LLaMA and Bloom performance across Belle-eval and MMCU benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of parameter-efficient methods like LoRA compare to full fine-tuning for instruction-tuning LLMs in Chinese?
- Basis in paper: [explicit] The authors mention that "Most experimental results are based on parameter-efficient methods, which may differ from the results of full parameter fine-tuning" and note that "instruction-tuning based on parameter-efficient methods has broader application and research scenarios."
- Why unresolved: The paper primarily uses parameter-efficient methods like LoRA for instruction-tuning, but does not directly compare the performance to full fine-tuning. The authors acknowledge that the results may differ, but do not explore this comparison in detail.
- What evidence would resolve it: Conducting experiments that directly compare the performance of instruction-tuning using parameter-efficient methods like LoRA to full fine-tuning on the same Chinese LLM and instruction dataset would provide evidence to resolve this question.

### Open Question 2
- Question: How does the quality of instruction data generated by self-instruct methods compare to that of real human-ChatGPT conversations for instruction-tuning LLMs in Chinese?
- Basis in paper: [explicit] The authors note that "Vicuna is trained from real human-ChatGPT conversations, with better quality than Alpaca-GPT4" and that "real users rarely ask multiple choice questions about academic topics" when discussing the ShareGPT-zh dataset.
- Why unresolved: While the paper compares the performance of models instruction-tuned on different datasets, including those generated by self-instruct methods and real human-ChatGPT conversations, it does not directly compare the quality of these two types of instruction data. The authors suggest that real human-ChatGPT conversations may be of higher quality, but do not provide a detailed analysis.
- What evidence would resolve it: Conducting a thorough analysis of the quality of instruction data generated by self-instruct methods compared to real human-ChatGPT conversations, including factors such as diversity, relevance, and coherence, would provide evidence to resolve this question.

### Open Question 3
- Question: How does the use of prompts in Chinese versus English affect the performance of instruction-tuned LLMs in Chinese for different types of tasks?
- Basis in paper: [explicit] The authors find that "using Chinese prompts for models with weaker Chinese abilities (e.g., LLaMA) can effectively help respond in Chinese, while for models with good Chinese abilities (e.g., Bloom), using prompts in English (the language they are better at) can better guide the model to understand the process of fine-tuning with instructions."
- Why unresolved: While the paper provides some insights into the effect of prompt language on LLM performance, it does not explore this question in depth or consider the impact on different types of tasks. The authors suggest that the choice of prompt language may depend on the model's Chinese ability, but do not investigate this relationship further.
- What evidence would resolve it: Conducting experiments that systematically vary the language of prompts (Chinese vs. English) and task type for instruction-tuned LLMs in Chinese would provide evidence to resolve this question. This could include analyzing the performance on different task categories (e.g., open-ended generation, classification, reading comprehension) and comparing the results across models with varying Chinese abilities.

## Limitations

- The study focuses primarily on two benchmarks (Belle-eval and MMCU), which may not fully capture all aspects of Chinese instruction-following capability
- The comparison of parameter-efficient methods is limited to a specific set of adapters without exploring the full spectrum of emerging PEFT techniques
- The vocabulary expansion experiments show promising results but lack systematic exploration of optimal vocabulary sizes and character set coverage

## Confidence

**High Confidence:** The core finding that Bloom serves as the most suitable foundation model for Chinese instruction-tuning is well-supported by systematic experiments across multiple instruction datasets and evaluation benchmarks.

**Medium Confidence:** The relative performance rankings of parameter-efficient methods and the effectiveness of instruction data types are moderately supported, though results may vary depending on specific task distributions.

**Low Confidence:** The conclusions regarding vocabulary expansion and human-value alignment effects are based on limited experiments and lack the statistical rigor needed for definitive claims.

## Next Checks

1. **Reproducibility Test:** Re-run the core experiments using publicly available implementations of the parameter-efficient methods and instruction datasets to verify the reported performance rankings across multiple random seeds.

2. **Generalization Assessment:** Evaluate the instruction-tuned models on additional Chinese benchmarks beyond Belle-eval and MMCU, including domain-specific tests (legal, medical, technical) to assess the breadth of instruction-following capabilities.

3. **Ablation Study:** Conduct systematic ablation experiments removing specific components (CoT data, vocabulary expansion, alignment data) to quantify their individual contributions to overall performance.