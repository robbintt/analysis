---
ver: rpa2
title: Improving Factual Consistency of News Summarization by Contrastive Preference
  Optimization
arxiv_id: '2310.19347'
source_url: https://arxiv.org/abs/2310.19347
tags:
- llms
- summarization
- training
- decent
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving factual consistency
  in news summarization using large language models (LLMs). LLMs, while advanced,
  often generate summaries that are factually inconsistent with the original articles,
  a problem known as "hallucinations" in text generation.
---

# Improving Factual Consistency of News Summarization by Contrastive Preference Optimization

## Quick Facts
- **arXiv ID**: 2310.19347
- **Source URL**: https://arxiv.org/abs/2310.19347
- **Reference count**: 35
- **Primary result**: A new method (DECENT) that improves factual consistency in LLM-generated summaries by adversarially decoupling comprehension and embellishment abilities, achieving significant performance gains over baseline models.

## Executive Summary
This paper addresses the persistent problem of hallucinations in LLM-generated news summaries by proposing DECENT (an adversarially DEcoupling method to disentangle the Comprehension and EmbellishmeNT of LLMs). The authors construct a new dataset LSum with sentence-level factual consistency annotations and demonstrate that their approach significantly improves the reliability of text summarization. The method combines adversarially decoupling comprehension from embellishment abilities with probing-based parameter-efficient training, showing substantial improvements in factual consistency under both ChatGPT and GPT-4 evaluation systems.

## Method Summary
The DECENT method consists of three main components: sentence-level data collection using auto-annotation with ChatGPT and GPT-4, adversarially decoupling comprehension and embellishment abilities through Incentive Loss and Penalty Loss mechanisms, and probing-based parameter-efficient training that targets weak layers. The approach trains LLMs to follow comprehension-focused instructions while minimizing embellishment behaviors, using dynamic layer selection to improve factuality sensitivity. The method is evaluated on the LSum dataset, which contains summaries generated by various decoder-only LLMs and annotated for factual consistency at the sentence level.

## Key Results
- DECENT significantly improves factual consistency in LLM-generated summaries compared to baseline models
- The method achieves better performance under both ChatGPT and GPT-4 evaluation systems
- Probing-based parameter-efficient training effectively targets weak layers that are insensitive to factuality
- The adversarially decoupling approach successfully reduces hallucination patterns in generated summaries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarially decoupling comprehension and embellishment abilities reduces hallucinations by forcing the model to use different instruction-following strategies for each ability.
- Mechanism: Two distinct instructions (Icom for comprehension, Iemb for embellishment) are used during training, with incentive loss encouraging correct instruction following and penalty loss discouraging incorrect behavior.
- Core assumption: LLMs have separate parametric and contextual knowledge that can be independently trained to follow different instructions.
- Evidence anchors:
  - [abstract] "We propose an adversarially DEcoupling method to disentangle the Comprehension and EmbellishmeNT abilities of LLMs (DECENT)"
  - [section] "we decouple their comprehension and embellishment capabilities to make it possible for them to summarize precisely with only comprehension"
  - [corpus] Weak - only 25 related papers found, none directly addressing adversarial decoupling methodology
- Break condition: If the model cannot learn to distinguish between the two instruction types, or if the penalty loss overpowers the incentive loss, decoupling will fail.

### Mechanism 2
- Claim: Probing-based parameter-efficient training targets weak layers that are insensitive to factuality, improving overall model reliability.
- Mechanism: Dynamic probing identifies the top-k worst layers based on their ability to distinguish consistent from inconsistent summaries, then trains only those layers using parameter-efficient techniques.
- Core assumption: Different layers in LLMs have varying capabilities to distinguish factuality, with bottom and top layers being weaker at this task.
- Evidence anchors:
  - [abstract] "we adopt a probing-based parameter-efficient technique to cover the shortage of sensitivity for true and false in the training process"
  - [section] "we dynamically probe and select the top-k worst layers to train"
  - [corpus] Weak - no corpus evidence found supporting layer-specific factuality sensitivity
- Break condition: If probing scores don't correlate with actual performance, or if training only k layers is insufficient to improve overall consistency.

### Mechanism 3
- Claim: Sentence-level data collection with auto-annotation using ChatGPT and GPT-4 provides high-quality training data that captures LLM-specific hallucination patterns.
- Mechanism: Summaries are generated by multiple LLMs, filtered for quality, then annotated at sentence level using ChatGPT/GPT-4 to identify inconsistencies with source articles.
- Core assumption: ChatGPT and GPT-4 can reliably detect hallucinations in LLM-generated summaries, and sentence-level annotation is more appropriate than sample-level for capturing subtle errors.
- Evidence anchors:
  - [abstract] "we construct a new summarization dataset for LLMs - LSum with sentence-level annotation for factual consistency"
  - [section] "we employ ChatGPT and GPT-4 to collect sentence-level factual consistency annotation"
  - [corpus] Weak - only 25 related papers found, none discussing LLM-specific hallucination datasets
- Break condition: If auto-annotation quality degrades over time, or if sentence-level annotation misses cross-sentence hallucination patterns.

## Foundational Learning

- **Concept: Contrastive learning**
  - Why needed here: Helps the model distinguish between faithful and fake content by learning preferences for factual summaries
  - Quick check question: How does contrastive learning differ from standard supervised learning in terms of training signal?

- **Concept: Parameter-efficient training**
  - Why needed here: Allows targeted training of specific layers without full fine-tuning, reducing computational cost
  - Quick check question: What are the main advantages of LoRA or other parameter-efficient methods compared to full fine-tuning?

- **Concept: Adversarial training**
  - Why needed here: Helps the model learn what not to do by penalizing incorrect instruction following, improving robustness
  - Quick check question: How does adversarial training improve generalization compared to standard maximum likelihood training?

## Architecture Onboarding

- **Component map**: LSum dataset → DECENT training pipeline (Incentive Loss + Penalty Loss + Probing) → fine-tuned LLM → evaluation via ChatGPT/GPT-4
- **Critical path**: Data collection → model initialization → adversarially decouple → probing-based training → evaluation
- **Design tradeoffs**: Full fine-tuning vs. parameter-efficient training (accuracy vs. speed), sentence-level vs. token-level annotation (coverage vs. precision)
- **Failure signatures**: Low probing accuracy across layers, minimal improvement in factual consistency scores, training instability due to penalty loss
- **First 3 experiments**:
  1. Run baseline evaluation on LLaMA2-7B-chat using ChatGPT/GPT-4 without any fine-tuning
  2. Train with only Incentive Loss (no Penalty Loss) and compare performance
  3. Train with full fine-tuning instead of parameter-efficient training to establish upper bound

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we further improve the detection of subtle hallucinations in LLM-generated summaries beyond the current methods?
- Basis in paper: [explicit] The paper mentions that hallucinations generated by LLMs are challenging to detect through traditional methods and that current methods based on reinforcement learning still rely on the traditional NLI-paradigm and require preference annotation.
- Why unresolved: The paper acknowledges the difficulty in detecting subtle hallucinations but does not provide a definitive solution or method to address this challenge comprehensively.
- What evidence would resolve it: A new method or model that significantly outperforms current approaches in detecting subtle hallucinations in LLM-generated summaries, validated through extensive experiments on diverse datasets.

### Open Question 2
- Question: Can the DECENT method be effectively applied to other natural language generation tasks beyond text summarization?
- Basis in paper: [inferred] The paper focuses on improving factual consistency in text summarization using DECENT, but the underlying principles of adversarially decoupling comprehension and embellishment abilities could potentially be applicable to other NLG tasks.
- Why unresolved: The paper does not explore the applicability of DECENT to other NLG tasks, leaving it as an open question whether the method can generalize beyond summarization.
- What evidence would resolve it: Empirical studies demonstrating the effectiveness of DECENT in improving factual consistency or other desired qualities in other NLG tasks, such as dialogue generation or machine translation.

### Open Question 3
- Question: How does the choice of hyperparameters, such as the balance between incentive and penalty losses (α) and the number of layers to train (k), affect the performance of DECENT?
- Basis in paper: [explicit] The paper mentions that hyperparameters α and k are set to 0.05 and 4, respectively, but does not explore the sensitivity of the model's performance to these choices.
- Why unresolved: The paper does not provide a comprehensive analysis of how different hyperparameter settings impact the effectiveness of DECENT, leaving room for further investigation.
- What evidence would resolve it: A systematic study examining the impact of various hyperparameter configurations on DECENT's performance, including ablation studies and sensitivity analyses, to identify optimal settings for different scenarios.

## Limitations

- The fundamental assumption that LLMs possess separable comprehension and embellishment abilities that can be independently trained may not hold across all model architectures
- The effectiveness of ChatGPT and GPT-4 as auto-annotation tools for detecting hallucinations introduces potential bias, as these models may share similar hallucination patterns with the LLMs being evaluated
- The dataset construction process and specific annotation prompts remain underspecified, making it difficult to assess the quality and generalizability of the LSum dataset

## Confidence

**High Confidence**: The core methodology of using instruction-based training with incentive and penalty losses is technically sound and follows established NLP practices. The experimental setup using multiple evaluation systems (ChatGPT and GPT-4) provides reasonable validation of improvements.

**Medium Confidence**: The claim that sentence-level annotation is superior to sample-level annotation for capturing hallucination patterns lacks direct empirical comparison. The probing-based parameter-efficient training shows promise but the selection criteria for "worst layers" needs more rigorous validation.

**Low Confidence**: The fundamental assumption that adversarial decoupling can cleanly separate comprehension from embellishment abilities across diverse LLM architectures requires more theoretical justification and empirical validation across different model families.

## Next Checks

1. **Cross-model validation**: Test DECENT's effectiveness across at least three different LLM architectures (e.g., GPT, LLaMA, and BERT-based models) to verify that the adversarial decoupling approach generalizes beyond the specific models used in the paper.

2. **Annotation quality assessment**: Conduct human evaluation on a subset of the LSum dataset to measure agreement between auto-annotation (ChatGPT/GPT-4) and human annotators, quantifying potential bias in the training data.

3. **Layer ablation study**: Systematically disable different combinations of the top-k worst layers identified by probing to determine whether training only these layers provides sufficient improvement, or if additional layers contribute meaningfully to factual consistency.