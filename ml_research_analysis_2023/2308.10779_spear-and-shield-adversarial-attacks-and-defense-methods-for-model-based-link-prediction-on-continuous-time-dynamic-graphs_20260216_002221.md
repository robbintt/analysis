---
ver: rpa2
title: 'Spear and Shield: Adversarial Attacks and Defense Methods for Model-Based
  Link Prediction on Continuous-Time Dynamic Graphs'
arxiv_id: '2308.10779'
source_url: https://arxiv.org/abs/2308.10779
tags:
- adversarial
- edge
- edges
- attack
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents adversarial attacks and defenses for temporal
  graph neural networks (TGNNs) on continuous-time dynamic graphs (CTDGs). It introduces
  T-SPEAR, a poisoning attack method that injects unnoticeable adversarial edges by
  selecting low-edge-score pairs under constraints on perturbation budget, timing,
  node selection, and per-node perturbation count.
---

# Spear and Shield: Adversarial Attacks and Defense Methods for Model-Based Link Prediction on Continuous-Time Dynamic Graphs

## Quick Facts
- arXiv ID: 2308.10779
- Source URL: https://arxiv.org/abs/2308.10779
- Reference count: 40
- Primary result: T-SPEAR attack degrades MRR by up to 18.9%; T-SHIELD defense improves robustness by up to 11.2% MRR

## Executive Summary
This paper introduces T-SPEAR, a poisoning attack method for temporal graph neural networks (TGNNs) operating on continuous-time dynamic graphs (CTDGs), and T-SHIELD, a defense method to mitigate such attacks. T-SPEAR injects unnoticeable adversarial edges by selecting low-edge-score pairs under constraints on perturbation budget, timing, node selection, and per-node perturbation count, achieving significant link prediction performance degradation. T-SHIELD improves robustness through edge filtering and temporal smoothness regularization, outperforming baseline defenses. Both methods demonstrate effectiveness across multiple TGNN architectures, highlighting the transferability of the attack and the importance of robust defenses in temporal graph settings.

## Method Summary
The paper proposes T-SPEAR, a poisoning attack method that injects adversarial edges into continuous-time dynamic graphs by selecting node pairs with low edge scores, making them unlikely to form naturally. The attack adheres to four constraints: perturbation budget, temporal distribution, endpoint diversity, and per-node perturbation limits, with the Hungarian algorithm ensuring balanced node selection. T-SHIELD, the defense method, filters potential adversarial edges based on edge scores and enforces temporal smoothness on node embeddings using cosine annealing to adjust filtering thresholds during training. Both methods are evaluated on four real-world datasets using TGNN models (TGN, JODIE, TGAT, DySAT), with T-SPEAR significantly degrading link prediction performance and T-SHIELD improving robustness against such attacks.

## Key Results
- T-SPEAR reduces MRR by up to 18.9% across datasets and TGNN architectures.
- T-SPEAR remains effective on unseen TGNN architectures, indicating strong transferability.
- T-SHIELD improves robustness by up to 11.2% MRR over naive TGNNs and outperforms static defense baselines.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial edges are effective because they target low-edge-score pairs, which are unlikely to form naturally in the graph evolution.
- Mechanism: The attack method selects adversarial edges by computing edge scores for all possible node pairs within a recent time window and choosing the pairs with the lowest scores. Since low scores indicate low likelihood of natural formation, injecting these edges corrupts the model's understanding of graph dynamics.
- Core assumption: The edge scorer accurately estimates the likelihood of edge formation based on the current node embeddings and graph structure.
- Evidence anchors:
  - [abstract]: "we generate edge perturbations by selecting edges that are unlikely to be formed when considering the evolution of dynamic graphs"
  - [section]: "The method aims to strategically generate and inject adversarial edges into the original edge sequence to degrade the link prediction performance while adhering to the four constraints (C1-C4)"
- Break condition: If the edge scorer becomes robust to adversarial perturbations or if the node embeddings capture temporal smoothness effectively, the attack loses effectiveness.

### Mechanism 2
- Claim: The Hungarian selection method improves attack stealthiness by ensuring balanced perturbation distribution across nodes.
- Mechanism: Instead of selecting the K lowest-scoring edges directly (which can lead to node overlap and degree spikes), the method solves an assignment problem to select edges with minimal overlap. This satisfies the constraint on the number of perturbations per node (C4) and makes the attack less detectable.
- Core assumption: The Hungarian algorithm can find an optimal assignment that minimizes edge scores while ensuring one-to-one matching between distinct nodes.
- Evidence anchors:
  - [section]: "To ensure the chosen edges are both impactful and inconspicuous, it is important to select adversarial edges with low edge scores and minimal overlapping endpoints"
  - [section]: "Solving this problem ensures that the selected edges have non-overlapping nodes, which helps improve the unnoticeability when considering (C4)"
- Break condition: If the graph structure changes significantly or if the attack budget becomes too large relative to the graph size, the Hungarian selection may not be able to maintain balance.

### Mechanism 3
- Claim: The defense method filters adversarial edges by leveraging the edge scores from the link classifier, which are more reliable after model training.
- Mechanism: The defense method computes edge scores for all edges and filters out those below a threshold. The threshold is gradually increased during training using cosine annealing to balance between removing legitimate edges early in training and failing to filter adversarial edges later.
- Core assumption: The edge scores generated by the link classifier become more reliable as the model trains, allowing for effective discrimination between legitimate and adversarial edges.
- Evidence anchors:
  - [section]: "For each edge e = (u, v, t), we first compute its edge score ˆyuvt, and if the edge score is less than a threshold τ ∈ (0, 1), we classify it as a potential adversarial edge"
  - [section]: "However, using a fixed threshold for edge filtering could lead to two significant issues... To address these concerns, we employ the cosine annealing scheduler to gradually increase the threshold from τs to τe"
- Break condition: If the adversary can manipulate the edge scores or if the model's edge scorer is not sufficiently trained, the defense may fail to filter adversarial edges effectively.

## Foundational Learning

- Concept: Temporal Graph Neural Networks (TGNNs)
  - Why needed here: TGNNs are the target models for both the attack and defense methods. Understanding their architecture and how they process temporal information is crucial for comprehending the attack and defense mechanisms.
  - Quick check question: What are the two main components of a TGNN like TGN, and how do they contribute to capturing temporal information?

- Concept: Adversarial attacks on graphs
  - Why needed here: The paper proposes a novel adversarial attack method on TGNNs operating on continuous-time dynamic graphs. Familiarity with adversarial attacks on static graphs provides a foundation for understanding the challenges and constraints specific to dynamic graphs.
  - Quick check question: What are the key differences between adversarial attacks on static graphs and those on continuous-time dynamic graphs?

- Concept: Robust training and defense methods
  - Why needed here: The paper proposes a defense method to mitigate the impact of adversarial attacks on TGNNs. Understanding robust training techniques and defense strategies is essential for evaluating the effectiveness of the proposed defense method.
  - Quick check question: How does the proposed defense method differ from traditional defense methods for static graphs, and why is this difference necessary for continuous-time dynamic graphs?

## Architecture Onboarding

- Component map:
  - Surrogate Model (TGN) -> Edge Scorer -> Adversarial Edge Selection -> Victim Models (TGN, JODIE, TGAT, DySAT)
  - Defense Models (TGN-SVD, TGN-COSINE, T-SHIELD) <- Edge Filtering + Temporal Smoothness Regularization

- Critical path:
  1. Train the surrogate model (TGN) on the original dynamic graph data.
  2. Use the trained surrogate model to generate adversarial edges by selecting low-edge-score pairs within recent time windows.
  3. Inject the generated adversarial edges into the original edge sequence to create a corrupted dynamic graph.
  4. Train the victim model on the corrupted dynamic graph.
  5. Evaluate the victim model's link prediction performance on the test set.
  6. Train the defense model on the corrupted dynamic graph using edge filtering and temporal smoothness regularization.
  7. Evaluate the defense model's link prediction performance on the test set.

- Design tradeoffs:
  - Attack stealthiness vs. effectiveness: The attack method imposes constraints (C1-C4) to ensure the adversarial edges are unnoticeable, but these constraints may limit the attack's effectiveness.
  - Defense accuracy vs. computational cost: The defense method uses edge filtering and temporal smoothness regularization to improve robustness, but these techniques may increase the computational cost of training and inference.

- Failure signatures:
  - Attack: If the edge scorer becomes robust to adversarial perturbations or if the node embeddings capture temporal smoothness effectively, the attack may fail to degrade the victim model's performance.
  - Defense: If the adversary can manipulate the edge scores or if the model's edge scorer is not sufficiently trained, the defense may fail to filter adversarial edges effectively.

- First 3 experiments:
  1. Evaluate the attack method's effectiveness on a single TGNN model (e.g., TGN) with varying perturbation rates and compare it to baseline attack methods.
  2. Assess the transferability of the attack method by evaluating its performance on different TGNN architectures (e.g., JODIE, TGAT, DySAT) using the same adversarial edges generated for TGN.
  3. Test the defense method's robustness against the attack method by training the victim model with the defense method and comparing its performance to the baseline defense methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of T-SPEAR vary with different perturbation budget sizes and what is the optimal budget for maximum impact?
- Basis in paper: [explicit] The paper discusses the perturbation budget constraint (C1) and mentions that T-SPEAR outperforms baselines across various perturbation rates, but does not specify the optimal budget.
- Why unresolved: The paper evaluates the attack's effectiveness at different perturbation rates but does not provide a detailed analysis of how the attack's impact scales with budget size or determine the most effective budget for maximum degradation of model performance.
- What evidence would resolve it: A systematic study varying the perturbation budget and measuring the corresponding performance degradation across multiple datasets and TGNN architectures would help identify the optimal budget for T-SPEAR.

### Open Question 2
- Question: Can T-SHIELD be extended to handle dynamic graphs with edge features, and how does its performance compare to static defense methods in such cases?
- Basis in paper: [inferred] The paper mentions that T-SHIELD can be extended to attributed dynamic graphs by imposing a distribution constraint on edge features, but does not provide experimental results or comparisons with static defense methods for such cases.
- Why unresolved: While the paper suggests a potential extension of T-SHIELD to handle edge features, it does not experimentally validate this extension or compare its performance to static defense methods that could be adapted for dynamic graphs with features.
- What evidence would resolve it: Experiments comparing T-SHIELD's performance on dynamic graphs with edge features against static defense methods adapted for such cases would provide insights into the effectiveness of the proposed extension.

### Open Question 3
- Question: How does the transferability of T-SPEAR across different TGNN architectures vary with the similarity between the surrogate model and the victim model?
- Basis in paper: [explicit] The paper demonstrates that T-SPEAR is transferable across different TGNN architectures, including those that differ from the surrogate model (TGN) used by T-SPEAR.
- Why unresolved: While the paper shows that T-SPEAR is transferable, it does not investigate how the degree of transferability varies with the similarity between the surrogate and victim models, which could inform the choice of surrogate model for maximizing attack effectiveness.
- What evidence would resolve it: A systematic study varying the similarity between the surrogate and victim models (e.g., using different TGNN architectures as surrogates) and measuring the corresponding attack transferability would help understand the relationship between model similarity and attack effectiveness.

## Limitations
- The attack's effectiveness depends on the edge scorer's ability to accurately identify low-likelihood edge pairs, which may not hold against adaptive adversaries.
- The defense method's reliance on precise threshold tuning may limit its generalization across different datasets or attack strategies.
- The assumption that temporal smoothness regularization consistently mitigates adversarial effects across diverse TGNN architectures remains untested.

## Confidence
- **High Confidence**: The paper demonstrates significant performance degradation (up to 18.9% MRR) from the T-SPEAR attack and notable defense improvements (up to 11.2% MRR) with T-SHIELD across multiple datasets.
- **Medium Confidence**: The transferability of attacks across TGNN architectures is shown, but the exact conditions under which transferability breaks down are not fully characterized.
- **Medium Confidence**: The defense method's edge filtering mechanism is effective, but its sensitivity to threshold schedules and parameter choices could limit practical applicability.

## Next Checks
1. Test T-SHIELD's robustness when the adversary actively manipulates edge scores or adapts to the defense's filtering strategy.
2. Evaluate attack effectiveness under varying graph sizes and densities to understand scalability limits.
3. Conduct ablation studies on the four constraints (C1-C4) in T-SPEAR to quantify their individual contributions to stealthiness and attack success.