---
ver: rpa2
title: Elucidating Discrepancy in Explanations of Predictive Models Developed using
  EMR
arxiv_id: '2311.16654'
source_url: https://arxiv.org/abs/2311.16654
tags:
- methods
- features
- clinical
- agreement
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the critical issue of transparency and explainability
  in machine learning (ML) algorithms used in healthcare settings. The authors applied
  state-of-the-art explainability methods to clinical decision support algorithms
  developed for Electronic Medical Records (EMR) data, analyzing the concordance between
  these methods and expert clinical knowledge.
---

# Elucidating Discrepancy in Explanations of Predictive Models Developed using EMR

## Quick Facts
- arXiv ID: 2311.16654
- Source URL: https://arxiv.org/abs/2311.16654
- Reference count: 14
- Key outcome: Analysis of agreement between XAI explanation methods (SHAP and DTD) for ML models predicting patient readmission and deterioration in EMR data

## Executive Summary
This study investigates the concordance between different explainable artificial intelligence (XAI) methods when applied to predictive models using Electronic Medical Records (EMR) data. The authors apply state-of-the-art explainability techniques (SHAP and DTD) to clinical decision support algorithms and measure agreement using Feature Agreement (FA) and Rank Agreement (RA) metrics. The research reveals generally poor to moderate agreement between explanation methods, highlighting fundamental challenges in establishing trustworthy XAI solutions for healthcare applications.

## Method Summary
The study analyzes two EMR datasets: pediatric hospital readmission prediction and adult inpatient deterioration prediction. Three modeling approaches were trained: Logistic Regression with l1 regularization, XGBoost, and Deep Neural Network with convolution layers. SHAP and DTD explainers were constructed on top of these models, and global explanations were computed by summing individual patient explanations across datasets. The agreement between features and their rankings obtained by different explanation methods was measured using FA and RA metrics, then compared against expert clinical knowledge.

## Key Results
- Agreement between SHAP and DTD methods was generally poor to moderate across both datasets
- Least agreement was found for top features in the readmission dataset
- Discrepancies were identified from both clinical (missing features, correlations) and technical (optimization objectives) perspectives
- Results highlight the need for careful validation of XAI methods in healthcare settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Agreement between different XAI explanation methods can be poor due to their differing underlying principles and optimization objectives.
- Mechanism: Each explanation method (SHAP, DTD, L1 coefficients) operates on distinct theoretical foundationsâ€”SHAP based on game theory, DTD on neural network gradient decomposition, L1 on linear feature importance. These methods may capture different aspects of model behavior, leading to divergent explanations.
- Core assumption: The optimization objective of minimizing prediction error does not necessarily align with capturing clinically relevant causal relationships.
- Evidence anchors:
  - [abstract] "The results displayed varying levels of agreement, generally poor to moderate."
  - [section] "From a modelling perspective, the discrepancy can be attributed to the optimization objective which is simply the minimisation of error."

### Mechanism 2
- Claim: Missing or incomplete features in the dataset can lead to failure in explaining all variations and causation of the outputs.
- Mechanism: When important clinical variables are absent from the EMR data, explanation methods cannot account for their influence on model predictions. This creates gaps in the explanations that can vary depending on which method is used.
- Core assumption: The quality and completeness of input features directly impacts the reliability of explanations across all XAI methods.
- Evidence anchors:
  - [section] "From a clinical perspective, 3 sources of discrepancy and or incorrectness were identified. Firstly, incomplete information and missing features lead to failure in explaining all the variations and causation of the outputs."

### Mechanism 3
- Claim: Correlation between features can cause different explanation methods to select different sets of influential features, even when the underlying model behavior is consistent.
- Mechanism: When features are highly correlated (e.g., multiple vital signs that change together), methods like SHAP may distribute importance differently than DTD or L1 coefficients, leading to apparent disagreement in explanations.
- Core assumption: Feature correlation patterns affect explanation methods differently based on their mathematical foundations.
- Evidence anchors:
  - [section] "Aas et al. [12] have also shown that Shap [7] may lead to incorrect explanations when features are highly correlated."

## Foundational Learning

- Concept: Feature importance vs. causal importance
  - Why needed here: Clinicians need to distinguish between features that are statistically important for model predictions versus those that have true causal relationships with outcomes
  - Quick check question: Can a feature be highly ranked by an explanation method but not causally related to the outcome?

- Concept: Global vs. local explanations
  - Why needed here: The study focuses on global explanations (aggregate across all patients), but clinical decision-making often requires understanding individual patient predictions
  - Quick check question: How might agreement metrics differ if we analyzed local explanations instead of global ones?

- Concept: Model interpretability vs. explanation methods
  - Why needed here: Different models (logistic regression, XGBoost, DNN) have inherently different levels of interpretability, which affects how well post-hoc explanation methods can capture their behavior
  - Quick check question: Why might explanation methods work better for logistic regression than for deep neural networks?

## Architecture Onboarding

- Component map: Data ingestion layer -> Model training layer -> Explanation generation layer -> Evaluation layer -> Analysis layer
- Critical path: Data preprocessing and feature engineering -> Model training and validation -> Explanation method implementation -> Agreement metric calculation -> Clinical expert review and comparison
- Design tradeoffs:
  - Using global explanations provides statistical robustness but may miss individual patient nuances
  - Selecting only two explanation methods limits generalizability but allows deeper analysis
  - Focusing on top-5 features balances comprehensiveness with interpretability
- Failure signatures:
  - Low FA/RA scores between methods indicate poor agreement
  - Explanation features not matching clinical expert knowledge suggest model-clinical misalignment
  - High variance in agreement across different datasets may indicate dataset-specific issues
- First 3 experiments:
  1. Replicate the agreement analysis on a third, independent EMR dataset to test generalizability
  2. Compare global agreement metrics with local explanation agreement for individual patients
  3. Implement and test additional explanation methods (e.g., LIME, Integrated Gradients) to see if agreement improves with more methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the agreement between XAI methods and expert clinical knowledge vary across different patient cohorts and clinical contexts?
- Basis in paper: [explicit] The authors suggest that agreement analysis for adult cohorts is needed for more solid and general conclusions on patient readmission problems.
- Why unresolved: The study only examined two specific datasets (paediatric readmission and adult deterioration), and the results may not be generalizable to other patient populations or clinical settings.
- What evidence would resolve it: Comparative studies across diverse patient cohorts, clinical contexts, and healthcare settings, measuring agreement between XAI methods and expert knowledge.

### Open Question 2
- Question: What are the specific mechanisms by which feature correlation and interaction effects impact the explanations generated by different XAI methods?
- Basis in paper: [inferred] The authors mention that dependence between factors and interrelationships with other input features could lead to different groups of features being picked by different models.
- Why unresolved: While the paper identifies correlation as a potential issue, it does not explore the specific mechanisms by which correlated features affect explanations.
- What evidence would resolve it: Controlled experiments isolating the effects of feature correlation on explanations, and developing methods to account for feature interactions.

### Open Question 3
- Question: How can we develop trustworthiness criteria for XAI methods in healthcare that account for both technical performance and clinical relevance?
- Basis in paper: [explicit] The authors aim to establish criteria for trustworthy XAI solutions for clinical decision support.
- Why unresolved: The paper identifies the need for trustworthiness criteria but does not propose specific criteria or a framework for their development.
- What evidence would resolve it: A comprehensive framework that incorporates both technical performance metrics and clinical relevance assessments, validated through extensive clinical studies.

## Limitations
- The study's findings may not generalize beyond the specific EMR datasets used (pediatric readmission and adult deterioration)
- Missing clinical features in EMR data create gaps in explanation quality and potential misalignment with expert knowledge
- The analysis focuses on global explanations, potentially missing important individual patient-level variations

## Confidence

**High Confidence:** The observation of poor to moderate agreement between SHAP and DTD explanation methods is well-supported by the reported FA and RA metrics.

**Medium Confidence:** The clinical perspective on missing features and correlations affecting explanations is plausible but would benefit from more direct empirical validation.

**Low Confidence:** The assertion that optimization objectives are solely focused on error minimization without considering clinical relevance, while theoretically sound, lacks direct experimental evidence in this specific context.

## Next Checks

1. Replicate the agreement analysis using additional explanation methods (LIME, Integrated Gradients) to determine if SHAP-DTD discrepancies are method-specific or represent broader XAI challenges.

2. Conduct a controlled experiment where correlated features are deliberately removed or decorrelated to measure the impact on explanation agreement across all methods.

3. Implement a clinical feature importance ranking task where domain experts evaluate the top-10 features from each method, quantifying the alignment between technical and clinical perspectives.