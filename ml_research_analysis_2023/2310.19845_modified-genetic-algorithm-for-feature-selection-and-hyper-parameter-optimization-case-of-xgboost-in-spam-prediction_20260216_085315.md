---
ver: rpa2
title: 'Modified Genetic Algorithm for Feature Selection and Hyper Parameter Optimization:
  Case of XGBoost in Spam Prediction'
arxiv_id: '2310.19845'
source_url: https://arxiv.org/abs/2310.19845
tags:
- feature
- spam
- features
- selection
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a modified genetic algorithm for simultaneous
  feature selection and hyper-parameter optimization in spam prediction using XGBoost.
  The algorithm leverages the characteristics of XGBoost to outperform other machine
  learning algorithms, including BERT-based deep learning models.
---

# Modified Genetic Algorithm for Feature Selection and Hyper Parameter Optimization: Case of XGBoost in Spam Prediction

## Quick Facts
- arXiv ID: 2310.19845
- Source URL: https://arxiv.org/abs/2310.19845
- Reference count: 40
- Key outcome: Proposed modified genetic algorithm achieves 82.32% GMean and 92.67% accuracy on Twitter spam detection using <10% of features

## Executive Summary
This paper introduces a modified genetic algorithm (GA) for simultaneous feature selection and XGBoost hyperparameter optimization in spam prediction. The approach addresses the challenge of imbalanced spam datasets by using GMean as the fitness function and encoding both parameters and feature indices into GA chromosomes. Tested on a Twitter dataset with 5096 tweets and 14343 TF-iDF features, the method achieves strong performance while using less than 10% of the total feature space. The algorithm outperforms traditional feature selection methods like Chi2 and PCA, and demonstrates competitive results when applied to SMS spam modeling.

## Method Summary
The method employs a modified genetic algorithm that optimizes both XGBoost hyperparameters and feature subsets simultaneously. The GA encodes chromosomes containing parameter values and feature indices, using uniform crossover and mutation to evolve populations. Fitness is evaluated using geometric mean (GMean) to handle class imbalance. The approach is validated through 10-fold cross-validation repeated 50 times on a Twitter dataset preprocessed with TF-iDF vectorization. The algorithm is compared against Chi2 and PCA feature selection methods, and results are benchmarked on both Twitter and SMS spam datasets.

## Key Results
- Achieves 82.32% geometric mean and 92.67% accuracy on Twitter spam detection
- Uses less than 10% of total feature space (1435 out of 14343 features)
- Outperforms Chi2 and PCA feature selection methods
- Demonstrates competitive performance on SMS spam modeling compared to related works

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The modified GA simultaneously optimizes XGBoost hyperparameters and selects relevant features, avoiding overfitting in high-dimensional sparse text spaces.
- Mechanism: By encoding both parameters and feature indices into chromosomes, GA evolves populations that converge on parameter-feature combinations yielding high GMean scores. Mutation and crossover ensure exploration of both parameter space and feature subsets.
- Core assumption: XGBoost performance is sensitive to both parameter settings and feature subset size; optimizing both jointly yields better models than sequential optimization.
- Evidence anchors:
  - [abstract]: "propose a modified genetic algorithm for simultaneous dimensionality reduction and hyper parameter optimization"
  - [section]: "aims at proposing a novel GA variation that optimizes the parameters of a classifier (i.e., eXtreme Gradient Boosting), and to reduce the features space simultaneously"
  - [corpus]: Weak or missing

### Mechanism 2
- Claim: Using GMean as the fitness function mitigates class imbalance bias, ensuring balanced sensitivity and specificity in spam detection.
- Mechanism: GMean = sqrt(TPR * TNR) equally weights minority (spam) and majority (ham) class performance, preventing dominance of majority-class accuracy.
- Core assumption: The dataset is imbalanced and accuracy alone is misleading; GMean provides a fairer optimization target.
- Evidence anchors:
  - [abstract]: "GA and the validation of the selected models in this research utilize the GMean as an objective function"
  - [section]: "positive class based metrics will dramatically mislead the selection of the best model... GMean... considers both the positive and negative class"
  - [corpus]: Weak or missing

### Mechanism 3
- Claim: TF-iDF weighting transforms raw tweet text into informative feature vectors that XGBoost can exploit for spam detection.
- Mechanism: Tokenization, stemming, and TF-iDF weighting reduce sparsity and emphasize discriminative terms, improving classifier signal-to-noise ratio.
- Core assumption: Term frequency patterns differ meaningfully between spam and ham, and XGBoost can leverage sparse high-dimensional vectors effectively.
- Evidence anchors:
  - [section]: "Each tweet is converted into a representative TF-iDF vector... the class labels are encoded into 0's, i.e.„ 'Ham' class, and 1's, i.e.„ 'Spam' class"
  - [section]: "The used TF-iDF vectorizer parameters in Tweets text pre-processing are listed in Table 2"
  - [corpus]: Weak or missing

## Foundational Learning

- Concept: Genetic Algorithm fundamentals (encoding, selection, crossover, mutation, fitness)
  - Why needed here: GA is the core search/optimization engine; understanding its phases is essential to debug convergence and parameter tuning.
  - Quick check question: How does uniform crossover differ from single-point crossover, and why is it chosen for this problem?

- Concept: Imbalanced dataset evaluation metrics (GMean, AUC, Precision-Recall)
  - Why needed here: Standard accuracy is misleading here; these metrics ensure balanced class performance and guide model selection.
  - Quick check question: Why is GMean preferred over F1-score in this context?

- Concept: XGBoost hyperparameters and tree boosting mechanics
  - Why needed here: Proper parameter tuning is critical for performance; knowing each parameter's effect helps interpret GA results.
  - Quick check question: How does max_depth interact with min_child_weight in controlling overfitting?

## Architecture Onboarding

- Component map: Data preprocessing → TF-iDF vectorizer → GA initialization → GA evolution (selection, crossover, mutation) → Fitness evaluation (GMean) → Best chromosome → XGBoost training → 10x50CV validation → Performance analysis
- Critical path: GA evolution → Fitness evaluation → XGBoost training → Cross-validation → Performance reporting
- Design tradeoffs:
  - GA population size vs. runtime: larger populations improve search quality but increase computation time
  - Feature subset size vs. interpretability: fewer features ease interpretation but may lose predictive power
  - GMean vs. accuracy: GMean balances classes but may reduce overall accuracy if imbalance is mild
- Failure signatures:
  - GA convergence early with low fitness → poor parameter/feature selection
  - High variance across CV folds → instability in model or data leakage
  - GMean >> Accuracy → severe class imbalance or poor spam detection
- First 3 experiments:
  1. Run GA with minimal features (1%) and small population (10) to verify convergence behavior and fitness trends
  2. Test different crossover ratios (e.g., 40%, 60%, 80%) to identify optimal diversity preservation
  3. Validate best GA chromosome with 10-fold CV repeated 5 times to confirm stability before full 50-run analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the modified genetic algorithm's performance compare to other state-of-the-art methods for feature selection and hyperparameter optimization in imbalanced datasets beyond spam detection?
- Basis in paper: [explicit] The paper demonstrates the algorithm's effectiveness in spam detection and SMS spam modeling, but doesn't explore its generalizability to other domains.
- Why unresolved: The experiments are limited to spam-related datasets, and no comparison is made with other methods on different types of imbalanced data.
- What evidence would resolve it: Testing the algorithm on various imbalanced datasets from different domains (e.g., medical diagnosis, fraud detection) and comparing its performance to other feature selection and hyperparameter optimization methods.

### Open Question 2
- Question: What is the impact of different natural language processing techniques on the performance of the spam detection model?
- Basis in paper: [inferred] The paper mentions using stemming and TF-iDF for text preprocessing, but doesn't explore the effect of other NLP techniques like lemmatization, n-grams, or word embeddings.
- Why unresolved: The preprocessing steps are fixed, and no analysis is done to determine the optimal NLP pipeline for spam detection.
- What evidence would resolve it: Conducting experiments with different NLP techniques and comparing their impact on the model's performance metrics.

### Open Question 3
- Question: How does the computational complexity of the modified genetic algorithm scale with increasing dataset size and feature dimensionality?
- Basis in paper: [inferred] The paper mentions time complexity as a limitation but doesn't provide a detailed analysis of how the algorithm's performance is affected by larger datasets or higher-dimensional feature spaces.
- Why unresolved: The experiments are conducted on relatively small datasets, and no scalability analysis is performed.
- What evidence would resolve it: Testing the algorithm on progressively larger datasets and measuring its execution time, convergence rate, and memory usage to determine its scalability characteristics.

## Limitations

- Performance claims lack direct comparison to diverse ML algorithms beyond Chi2 and PCA
- Computational cost of 10x50CV validation is not discussed, potentially limiting practical deployment
- Specific chromosome encoding mechanism for GA remains underspecified, making exact replication challenging

## Confidence

- Performance claims (GMean 82.32%, Accuracy 92.67%): **High** - well-supported by extensive cross-validation
- GA optimization mechanism: **Medium** - theoretically sound but lacks implementation details
- Comparison to deep learning methods: **Low** - claims of outperforming BERT are not substantiated with direct comparisons

## Next Checks

1. Benchmark the modified GA against diverse ML algorithms (Random Forest, SVM, neural networks) on the same dataset to verify claimed superiority
2. Analyze GA convergence patterns across multiple runs to ensure consistent feature selection and parameter optimization
3. Evaluate the approach on additional spam datasets (SMS, email) to test generalizability beyond Twitter data