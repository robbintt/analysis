---
ver: rpa2
title: Promoting Exploration in Memory-Augmented Adam using Critical Momenta
arxiv_id: '2307.09638'
source_url: https://arxiv.org/abs/2307.09638
tags:
- adam
- learning
- buffer
- optimizers
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the issue of poor generalization in adaptive
  optimizers like Adam, which tend to converge to sharp minima in the loss landscape.
  The authors propose Adam+CM, a memory-augmented version of Adam that maintains a
  buffer of critical momenta from previous iterations during training.
---

# Promoting Exploration in Memory-Augmented Adam using Critical Momenta

## Quick Facts
- arXiv ID: 2307.09638
- Source URL: https://arxiv.org/abs/2307.09638
- Reference count: 40
- Key outcome: Memory-augmented Adam with critical momenta buffer improves generalization by escaping sharp minima

## Executive Summary
This work addresses the poor generalization of adaptive optimizers like Adam, which tend to converge to sharp minima in the loss landscape. The authors propose Adam+CM, a memory-augmented version of Adam that maintains a buffer of critical momenta from previous iterations during training. This buffer encourages the optimizer to overshoot beyond narrow minima, promoting exploration towards flatter regions. Empirical results demonstrate that Adam+CM effectively escapes sharp minima and converges to flatter, more generalizable solutions, improving performance on image classification and language modeling tasks.

## Method Summary
Adam+CM is a memory-augmented Adam optimizer that stores critical momenta in a buffer and aggregates them during training. The method uses gradient l2-norm as a priority criterion for buffer updates, with an exponential decay factor for keys. During each update, the current momentum is added to the average of all critical momenta in the buffer, then applied to the standard Adam update rule. This approach maintains directional consistency through momentum inertia, allowing the optimizer to overcome gradient cancellation effects near sharp minima.

## Key Results
- Adam+CM outperforms Adam, Adam+CG, and Adam+SAM on CIFAR-10/100 and ImageNet classification tasks
- The method achieves lower validation perplexity on Penn Treebank language modeling compared to baseline optimizers
- Buffer size controls exploration-exploitation tradeoff, with larger buffers finding flatter minima but requiring more computation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adam+CM escapes sharp minima by leveraging momentum inertia to overcome gradient cancellation
- Mechanism: The critical momenta buffer stores past momentum terms instead of gradients. During aggregation, these momenta maintain directional consistency even when new gradients fluctuate rapidly near sharp minima. This inertia allows the optimizer to "push through" the basin walls rather than bouncing back and forth.
- Core assumption: Momentum terms have lower variance than gradients when near sharp minima, preventing cancellation during aggregation
- Evidence anchors: [abstract] "The buffer prompts the optimizer to overshoot beyond narrow minima, promoting exploration" and [section 3] "while the buffer gradients can promote exploration initially... the parameters remain stuck in sharp regions due to gradient cancellation"

### Mechanism 2
- Claim: Buffer size controls the exploration-exploitation tradeoff by determining minimum basin width for convergence
- Mechanism: Larger buffers accumulate more momentum inertia, requiring wider minima for convergence. The optimizer must remain in a basin long enough to fill the buffer before it can turn back, effectively setting a minimum width threshold.
- Core assumption: Buffer filling time is proportional to basin width, creating a natural exploration-exploitation tradeoff
- Evidence anchors: [section 3] "the larger the size of the buffer, the stronger the overshooting effect will be and the wider the minima needs to be for the system to converge" and [section 4] "even with a small buffer size, Adam+CM can escape sharper minima" and "the larger buffer size helps find a flatter minimum"

### Mechanism 3
- Claim: Critical momenta buffer elements have higher agreement than critical gradients, reducing directional cancellation
- Mechanism: Momentum terms computed from consistent gradient directions maintain better alignment than raw gradients, which can vary rapidly near sharp minima. This agreement ensures buffer aggregation produces meaningful updates.
- Core assumption: Momentum computation inherently smooths gradient noise, creating more coherent buffer elements
- Evidence anchors: [section 5.2.1] "buffer elements stored by Adam+CM have lower variance during training compared to Adam+CG" and "agreement in Adam+CM remains higher than in Adam+CG" and [section 3] "the fact that gradients quickly change direction will lead to the new momentum terms being smaller and hence have a smaller immediate influence"

## Foundational Learning

- Concept: Gradient cancellation in memory-augmented optimizers
  - Why needed here: Understanding why Adam+CG fails is crucial for grasping Adam+CM's innovation
  - Quick check question: Why do high-variance gradients in the buffer lead to cancellation near sharp minima?

- Concept: Sharpness-aware minimization (SAM) and its limitations
  - Why needed here: Adam+SAM is used as a baseline, and understanding its double backward pass cost and neighborhood approximation is important
  - Quick check question: What is the computational cost difference between SAM and standard Adam?

- Concept: Momentum-based optimization and variance reduction
  - Why needed here: The core mechanism relies on momentum's smoothing properties compared to raw gradients
  - Quick check question: How does momentum computation reduce variance compared to using raw gradients?

## Architecture Onboarding

- Component map:
  - Critical momenta buffer (dictionary: norm→momentum)
  - Aggregation function (momentum addition + averaging)
  - Priority decay mechanism (exponential decay of buffer priorities)
  - Standard Adam components (first/second moment updates)

- Critical path: Buffer update → Momentum aggregation → Parameter update
  1. Compute current gradient and momentum
  2. Update buffer with priority-based replacement
  3. Aggregate buffer momenta with current momentum
  4. Apply Adam update using aggregated momenta

- Design tradeoffs:
  - Buffer size vs memory usage (C=5 uses ~5× memory of standard Adam)
  - Decay rate λ vs buffer staleness (λ=0.99 keeps old momenta longer)
  - Momentum computation parameters β1 vs inertia strength

- Failure signatures:
  - If validation accuracy plateaus early → buffer size too small or decay too aggressive
  - If training loss oscillates → buffer size too large relative to learning rate
  - If no improvement over Adam → priority criterion not selecting useful momenta

- First 3 experiments:
  1. Toy function test (Ackley/Rosenbrock) with C=5, λ=0.7 to verify escape from sharp minima
  2. CIFAR-10 ResNet34 baseline comparison with default hyperparameters
  3. Buffer size sensitivity analysis (C∈{5,10,20}) on CIFAR-100 to find optimal exploration-exploitation balance

## Open Questions the Paper Calls Out
The paper suggests investigating the application of Adam+CM to non-stationary settings like continual learning, where the method could help transfer knowledge across tasks by promoting exploration of flatter minima.

## Limitations
- No ablation studies isolating buffer management effects from momentum aggregation
- Limited analysis of computational overhead compared to SAM
- No theoretical analysis of the relationship between buffer size and minimum width requirements

## Confidence
- Core mechanism (Momentum-based escape from sharp minima): High confidence
- Buffer size tradeoff: Medium confidence
- Comparative performance claims: Low confidence

## Next Checks
1. Perform ablation study removing the priority decay mechanism to isolate the effect of critical momenta storage
2. Compare wall-clock training time against Adam+SAM to quantify the practical cost-benefit tradeoff
3. Test buffer management with alternative priority criteria (gradient norm vs momentum norm) to verify the choice is optimal