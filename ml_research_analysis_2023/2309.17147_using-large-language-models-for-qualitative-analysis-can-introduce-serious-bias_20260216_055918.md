---
ver: rpa2
title: Using Large Language Models for Qualitative Analysis can Introduce Serious
  Bias
arxiv_id: '2309.17147'
source_url: https://arxiv.org/abs/2309.17147
tags:
- label
- child
- your
- education
- parent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates using Large Language Models (LLMs) for annotating
  qualitative interview data, focusing on interviews with Rohingya refugees in Bangladesh.
  Three LLMs (ChatGPT and two Llama 2 variants) were compared to a supervised model
  trained on expert human annotations.
---

# Using Large Language Models for Qualitative Analysis can Introduce Serious Bias

## Quick Facts
- arXiv ID: 2309.17147
- Source URL: https://arxiv.org/abs/2309.17147
- Reference count: 40
- LLMs systematically over-predict rare codes and introduce bias correlated with subject characteristics

## Executive Summary
This paper evaluates the use of Large Language Models (LLMs) for annotating qualitative interview data, specifically interviews with Rohingya refugees in Bangladesh. The study compares three LLMs (ChatGPT and two Llama 2 variants) against a supervised model trained on expert human annotations. Results show that LLMs perform worse than supervised models in out-of-sample prediction accuracy and exhibit systematic biases related to interviewee characteristics like refugee status and gender. The paper concludes that while LLMs can assist in generating training data, relying on them directly for annotation risks introducing bias that can lead to misleading conclusions.

## Method Summary
The study uses 2,407 interview transcripts with Rohingya refugees, machine-translated to English, along with 789 expert human annotations on 19 codes at the QA-pair level. Three LLMs (ChatGPT, Llama-2 13B, and Llama-2 13B-chat) were prompted with detailed instructions, few-shot examples, and chain-of-thought reasoning to generate predictions. These were compared against a supervised model called iQual, trained on expert annotations using cross-validation for model and text representation selection. Performance was evaluated using out-of-sample F1 scores, and bias was assessed by testing for statistical associations between prediction errors and subject characteristics.

## Key Results
- LLMs systematically over-predict most annotations, with false positive rates between 0.43-0.47
- LLM prediction errors correlate with subject characteristics like refugee status, gender, and education
- Supervised models trained on expert annotations achieve F1 scores of 0.969, outperforming LLMs (ChatGPT 0.909, Llama-2 13B 0.854, Llama-2 13B-chat 0.851)
- Data augmentation using LLM-generated annotations provides only marginal improvements to supervised models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs systematically over-predict sparse codes, leading to biased prevalence estimates.
- Mechanism: Pre-training data distributions influence LLM output frequencies, causing higher false positive rates on rare codes.
- Core assumption: The LLM's training corpus under-represents the context-specific rare concepts found in qualitative interviews.
- Evidence anchors:
  - [abstract] "Over-prediction of rare codes was also observed."
  - [section] "All three LLMs we tested systematically over-predict most of the annotations."
  - [corpus] Weak - corpus lacks direct evidence of training data distribution effects on over-prediction.
- Break condition: If the LLM is fine-tuned on the specific domain data, the over-prediction bias would be reduced.

### Mechanism 2
- Claim: LLM prediction errors are not random but correlate with subject characteristics (refugee status, gender, education).
- Mechanism: LLM internal representations capture spurious correlations between demographic cues and annotation labels.
- Core assumption: LLM training data contains demographic patterns that are not present in the target interview data.
- Evidence anchors:
  - [abstract] "errors that LLMs make in annotating interview transcripts are not random with respect to the characteristics of the interview subjects."
  - [section] "we show that in many cases LLM prediction errors are systematically associated with characteristics of the interview subject."
  - [corpus] Weak - corpus does not provide direct evidence of spurious demographic correlations in LLM training.
- Break condition: If LLM errors are uncorrelated with subject characteristics in out-of-sample tests, the bias claim fails.

### Mechanism 3
- Claim: Supervised models trained on expert annotations outperform LLMs and introduce less bias.
- Mechanism: Smaller models trained on domain-specific high-quality annotations learn context-appropriate decision boundaries without inherited distributional biases.
- Core assumption: Expert annotations capture the true nuanced coding structure better than LLM pretraining can generalize.
- Evidence anchors:
  - [abstract] "Training simpler supervised models on high-quality human annotations with flexible coding leads to less measurement error and bias than LLM annotations."
  - [section] "iQual achieves accuracy of 0.969. In contrast, ChatGPT only achieves 0.909, Llama-2 13B 0.854 abd Llama-2 13B chat 0.851."
  - [corpus] Weak - corpus lacks direct evidence of model performance differences.
- Break condition: If supervised models perform worse than LLMs in a new dataset, the claim would be invalid.

## Foundational Learning

- Concept: Bias in technical sense (systematic non-random errors)
  - Why needed here: The paper's core contribution is distinguishing random vs. systematic prediction errors.
  - Quick check question: If an LLM's false positive rate is the same across all demographic groups, is there bias according to the paper's definition?

- Concept: Data augmentation via LLMs
  - Why needed here: The paper tests whether LLMs can help supervised models by generating more training data.
  - Quick check question: What is the key difference between using LLMs for annotation vs. for data augmentation in this study?

- Concept: Few-shot learning and chain-of-thought prompting
  - Why needed here: These techniques are used to improve LLM annotation performance.
  - Quick check question: How do few-shot examples and reasoning explanations work together to improve LLM performance?

## Architecture Onboarding

- Component map: LLM annotation pipeline -> bias detection -> supervised model training -> data augmentation comparison
- Critical path: Expert annotation -> supervised model training -> out-of-sample prediction -> bias analysis
- Design tradeoffs: LLM scale and pretraining vs. supervised model simplicity and domain specificity
- Failure signatures: Systematic over-prediction, correlation between errors and subject characteristics, poor out-of-sample performance
- First 3 experiments:
  1. Compare LLM and supervised model F1 scores on held-out test set
  2. Test correlation between LLM errors and subject characteristics using F-tests
  3. Evaluate data augmentation impact by training supervised models with and without LLM-generated data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions (text characteristics, annotation complexity, or dataset properties) do LLMs outperform simpler supervised models in qualitative text analysis?
- Basis in paper: [inferred] The paper shows LLMs generally underperform simpler models but suggests this may be context-specific
- Why unresolved: The paper only tests one specific dataset (Rohingya refugee interviews) and doesn't systematically explore when LLMs might be advantageous
- What evidence would resolve it: Systematic testing of LLMs across multiple datasets varying in annotation complexity, cultural specificity, and text characteristics to identify conditions where LLMs excel

### Open Question 2
- Question: How can the bias in LLM annotations be detected and mitigated without requiring large expert-annotated validation sets?
- Basis in paper: [explicit] "high quality annotations are necessary in order to assess whether an LLM introduces bias"
- Why unresolved: The paper identifies bias as a critical problem but doesn't propose methods to detect or correct it without expert annotations
- What evidence would resolve it: Development and validation of automated bias detection methods that can work with minimal or no human oversight

### Open Question 3
- Question: What is the optimal balance between LLM-generated data augmentation and traditional supervised learning for qualitative text analysis?
- Basis in paper: [explicit] The paper finds marginal benefits from LLM data augmentation but doesn't explore optimal integration strategies
- Why unresolved: The paper only tests one simple augmentation approach and doesn't systematically explore different integration strategies or optimal ratios
- What evidence would resolve it: Systematic comparison of different augmentation strategies, ratios, and integration methods to identify optimal approaches for different types of qualitative analysis tasks

## Limitations
- Analysis based on a single dataset from a specific cultural context (Rohingya refugees in Bangladesh)
- Potential bias introduced by machine translation of interviews from Bengali to English
- Supervised model comparison uses iQual, specifically designed for this type of analysis

## Confidence
- **High Confidence**: The core finding that LLMs systematically over-predict rare codes and that supervised models trained on expert annotations outperform LLMs on this task.
- **Medium Confidence**: The claim about LLM prediction errors correlating with subject characteristics.
- **Medium Confidence**: The conclusion that LLMs are unsuitable for direct annotation use in qualitative research.

## Next Checks
1. **Replication on Different Dataset**: Test the same LLM versus supervised model comparison on a qualitatively different dataset to assess generalizability of the bias findings.
2. **Prompt Engineering Experiment**: Systematically vary the LLM prompting strategy to determine if better prompting can reduce the observed biases and improve performance closer to supervised models.
3. **Error Analysis by Code Type**: Conduct a detailed error analysis breaking down performance by individual code characteristics to better understand which types of codes LLMs struggle with most and why.