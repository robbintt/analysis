---
ver: rpa2
title: 'SLM: Bridge the thin gap between speech and text foundation models'
arxiv_id: '2310.00230'
source_url: https://arxiv.org/abs/2310.00230
tags:
- speech
- text
- language
- tasks
- adapter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SLM, a joint Speech and Language Model that
  bridges the gap between pretrained speech and language foundation models using a
  simple adapter. SLM achieves strong performance on conventional tasks such as automatic
  speech recognition (ASR) and speech translation (AST), and introduces the novel
  capability of zero-shot instruction-following for more diverse tasks.
---

# SLM: Bridge the thin gap between speech and text foundation models

## Quick Facts
- **arXiv ID**: 2310.00230
- **Source URL**: https://arxiv.org/abs/2310.00230
- **Reference count**: 0
- **Primary result**: SLM achieves strong ASR/AST performance and zero-shot instruction following using a simple adapter that bridges pretrained speech and language models

## Executive Summary
This paper introduces SLM (Speech and Language Model), a framework that bridges pretrained speech and text foundation models using a simple adapter. By freezing the foundation models and training a lightweight adapter with just 1% of the parameters, SLM achieves strong performance on conventional speech tasks while introducing novel zero-shot instruction-following capabilities. The key insight is that speech and text representations may be more aligned than previously thought, allowing a simple adapter to effectively translate between modalities without fine-tuning the large foundation models.

## Method Summary
SLM uses a frozen pretrained speech encoder (USM) and a frozen pretrained LLM (mT0-MT), connected by a lightweight adapter that transforms speech embeddings into text embedding space. The adapter applies a 4x sequence length reduction to speech outputs and learns to map them into the LLM's representation space through next-token prediction training. The system is trained on a mixture of ASR, AST, and instruction-tuning tasks, with text instructions serving as prompts for speech inputs. The entire adapter contains only 156M parameters compared to the foundation models' billions of parameters.

## Key Results
- Achieves competitive ASR performance with minimal parameter overhead
- Demonstrates strong speech translation capabilities (BLEU scores up to 38)
- Introduces novel zero-shot instruction-following for speech inputs including contextual biasing, dialog generation, and question answering
- Shows that freezing foundation models preserves their native capabilities while the adapter learns cross-modal translation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The adapter transforms speech embeddings into the text embedding space that the LLM can interpret
- Mechanism: The adapter takes reduced speech embeddings from the frozen speech encoder and learns to map them into the same representation space as text embeddings through next-token-prediction training
- Core assumption: Speech and text representations share enough structural similarity that a simple adapter can bridge the gap
- Evidence anchors: [abstract] "only trains a simple adapter with just 1% (156M) of the foundation models' parameters", [section] "The adapter takes the output of the speech encoder, applies a uniform subsampling approach to reduce the sequence length, and learns to map the audio representation into the textual representation space"

### Mechanism 2
- Claim: Freezing foundation models preserves their native capabilities while the adapter learns cross-modal translation
- Mechanism: By keeping both the speech encoder and LLM frozen during training, their pre-learned representations remain intact, and the adapter learns to act as a translator between modalities
- Core assumption: The pre-trained models' representations are robust enough that they don't need fine-tuning for cross-modal tasks
- Evidence anchors: [abstract] "freezes the pretrained foundation models to maximally preserves their capabilities", [section] "We used a data mixture of combining all tasks... The mixing ratio is proportional to the number of data samples in each task"

### Mechanism 3
- Claim: Text instructions serve as effective prompts for speech inputs, enabling zero-shot instruction following
- Mechanism: By concatenating text instructions with adapted speech embeddings, the LLM can interpret the speech content in the context of the instruction
- Core assumption: LLMs can generalize their instruction-following capabilities from text-only to cross-modal inputs
- Evidence anchors: [abstract] "unlocks the novel capability of zero-shot instruction-following for more diverse tasks", [section] "The text input serves as an effective prompt for speech inputs, allowing the model to follow instructions"

## Foundational Learning

- Concept: Adapter-based transfer learning
  - Why needed here: To bridge the gap between speech and text representations without retraining large foundation models
  - Quick check question: What happens to model performance if we fine-tune the foundation models instead of using adapters?

- Concept: Cross-modal representation alignment
  - Why needed here: To ensure speech embeddings can be interpreted by text-only LLMs
  - Quick check question: How does the sequence length reduction affect the alignment quality between speech and text representations?

- Concept: Multitask learning with instruction tuning
  - Why needed here: To enable the model to follow diverse instructions across speech and text tasks
  - Quick check question: What's the impact of instruction diversity on zero-shot performance across different tasks?

## Architecture Onboarding

- Component map:
  Frozen speech encoder (USM) → Sequence length reduction → Adapter (transformer layers) → Concatenation with text embeddings → Frozen LLM (mT0-MT) → Next-token prediction

- Critical path: Speech input → Speech encoder → Length reduction → Adapter → Concatenation → LLM → Output generation
  - Bottlenecks: Sequence length reduction efficiency, adapter transformation quality, LLM inference speed

- Design tradeoffs:
  - Adapter depth vs performance: More layers increase capacity but also computational cost
  - Sequence length reduction ratio vs information retention: 4x reduction saves computation but may lose temporal information
  - Freezing vs fine-tuning: Freezing preserves capabilities but may limit task-specific optimization

- Failure signatures:
  - Poor ASR performance: Indicates adapter isn't properly mapping speech to text space
  - Failure on zero-shot tasks: Suggests LLM instruction-following capabilities aren't transferring
  - Slow inference: May indicate inefficient sequence length reduction or adapter architecture

- First 3 experiments:
  1. Ablation study on adapter depth (1-8 layers) to find optimal balance
  2. Compare freezing vs fine-tuning the speech encoder on a small task
  3. Test different sequence length reduction ratios to optimize computation vs quality tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal adapter depth for bridging the gap between pretrained speech and language models, and how does it vary across different foundation model architectures?
- Basis in paper: [explicit] The paper states that performance improved from 1 to 2 layers but saturated after 2 layers, suggesting the gap might be narrower than expected.
- Why unresolved: The paper only tested up to 8 layers and used specific foundation models (USM speech encoder and mT0-MT LLM). Different model architectures or deeper adapters might yield different results.
- What evidence would resolve it: Systematic experiments varying adapter depth across multiple foundation model pairs (different speech encoders, different LLM architectures) would clarify the optimal depth and its dependence on model characteristics.

### Open Question 2
- Question: How does the performance of SLM compare to cascaded ASR+LLM systems when both are optimized for the same downstream task?
- Basis in paper: [explicit] The paper found cascaded systems performed worse (BLEU 32 vs 38) but suggests optimization (finetuning LLMs on ASR transcripts) could improve performance.
- Why unresolved: The comparison used unoptimized systems, and the paper didn't explore the full potential of cascaded approaches with proper optimization.
- What evidence would resolve it: Head-to-head comparisons of optimized cascaded systems (with LLM finetuning) versus SLM on the same tasks, using the same foundation models and training data.

### Open Question 3
- Question: What are the limitations of SLM's zero-shot instruction following capabilities, particularly for complex or ambiguous tasks, and how do these limitations manifest?
- Basis in paper: [explicit] The paper demonstrates zero-shot capabilities on several tasks but notes hallucinations occur and performance depends on the foundation models' intrinsic capabilities.
- Why unresolved: The paper provides anecdotal examples but lacks systematic evaluation of failure modes, task complexity boundaries, or quantitative analysis of hallucination rates.
- What evidence would resolve it: Comprehensive benchmark testing of SLM on varied task complexities, with systematic measurement of success rates, hallucination frequency, and failure analysis across different instruction types.

## Limitations
- Limited theoretical justification for why cross-modal alignment works so effectively with such a simple adapter
- Evaluation scope is narrow, lacking extensive testing on challenging or diverse speech tasks
- Inherits limitations from foundation models and freezing strategy may prevent optimal task-specific optimization

## Confidence

**High confidence**: The adapter-based architecture and its ability to perform standard ASR and AST tasks
**Medium confidence**: The effectiveness of freezing foundation models while training only the adapter
**Medium confidence**: The novel zero-shot instruction following capabilities, though qualitative demonstrations are promising
**Low confidence**: The theoretical explanation for why cross-modal alignment works so effectively with such a simple adapter

## Next Checks

1. **Adapter Depth Sensitivity Analysis**: Systematically vary adapter depth (1-8 layers) and measure performance degradation on ASR and AST tasks to identify the optimal tradeoff between parameter efficiency and capability.

2. **Cross-Modal Transfer Robustness**: Test SLM on speech tasks from languages and domains not present in the training data to evaluate the generalizability of the learned cross-modal representations.

3. **Fine-tuning vs Freezing Comparison**: Conduct controlled experiments comparing frozen-foundation training against fine-tuning both foundation models on a small, high-quality dataset to quantify the tradeoff between efficiency and performance optimization.