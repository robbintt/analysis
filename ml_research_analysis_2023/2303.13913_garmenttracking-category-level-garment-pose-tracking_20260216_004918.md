---
ver: rpa2
title: 'GarmentTracking: Category-Level Garment Pose Tracking'
arxiv_id: '2303.13913'
source_url: https://arxiv.org/abs/2303.13913
tags:
- garment
- nocs
- pose
- canonical
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GarmentTracking, a framework for category-level
  garment pose tracking. The authors build a large-scale VR dataset (VR-Folding) with
  9,767 videos and 790K frames of garment manipulation (flattening and folding).
---

# GarmentTracking: Category-Level Garment Pose Tracking

## Quick Facts
- arXiv ID: 2303.13913
- Source URL: https://arxiv.org/abs/2303.13913
- Reference count: 40
- Key result: 15 FPS tracking speed (5x faster than baseline) with 85.9% A5cm accuracy for Shirt Folding

## Executive Summary
GarmentTracking introduces a three-stage framework for category-level garment pose tracking in manipulation scenarios like flattening and folding. The method leverages inter-frame feature fusion via a Transformer to predict normalized object coordinate space (NOCS) representations, refines these predictions with a PC-mesh refiner to reduce error accumulation, and maps results to task space using a warp field. Trained on a large-scale VR dataset (VR-Folding) with 9,767 videos and 790K frames, GarmentTracking achieves significant improvements in accuracy and speed over previous methods while demonstrating strong generalization to real-world data.

## Method Summary
GarmentTracking addresses category-level garment pose tracking through a three-stage pipeline. First, a NOCS predictor uses a sparse 3D CNN and Transformer-based inter-frame fusion to generate canonical coordinate predictions from two consecutive point cloud frames. Second, a PC-Mesh refiner simultaneously refines both the predicted coordinates and canonical shape to mitigate error accumulation. Finally, a warp field mapper transforms the refined canonical space predictions to task space geometry. The method is trained end-to-end on the VR-Folding dataset using Adam optimizer with 150 epochs on an RTX 3090 GPU.

## Key Results
- Achieves 15 FPS tracking speed (5x faster than baseline GarmentNets)
- Outperforms GarmentNets on multiple metrics: A5cm accuracy of 85.9% vs 21.5% for Shirt Folding
- Demonstrates strong generalization from synthetic VR-Folding dataset to real-world sequences
- Reduces error accumulation through PC-mesh refiner, improving long-term tracking stability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inter-frame feature fusion via Transformer improves pose tracking stability compared to single-frame prediction.
- Mechanism: The network uses self-attention to aggregate point features for two input frames, then cross-attention to capture cross-frame correlations and generate relation-enhanced fusion features. This leverages NOCS coordinates from the previous frame as positional embedding, incorporating both positional and semantic information across frames.
- Core assumption: Temporal continuity exists in garment deformation sequences, and inter-frame information can reduce ambiguity in pose estimation.
- Evidence anchors: [abstract]: "Specifically, we focus on the pose tracking problem in garment manipulation (e.g. flattening, folding)." [section]: "In stage 1, the NOCS predictor will generate an inter-frame fusion feature and predict raw NOCS coordinates."
- Break condition: If garment deformation is too rapid or discontinuous, temporal correlations may not hold, reducing the effectiveness of cross-frame attention.

### Mechanism 2
- Claim: The PC-Mesh refiner reduces error accumulation during long-term tracking by refining both canonical coordinates and canonical shape simultaneously.
- Mechanism: The PC refiner refines NOCS coordinate predictions using predicted classification scores and per-point fusion features, combined with global mesh features. The Mesh refiner refines the canonical shape using dense mesh features from the PC refiner and global features from the partial point cloud.
- Core assumption: Inaccuracies in both predicted NOCS coordinates and canonical shape contribute to error accumulation, and simultaneous refinement can mitigate this.
- Evidence anchors: [abstract]: "Then it refines the predicted canonical coordinates and the geometry with a NOCS refiner to reduce the accumulated errors." [section]: "To mitigate this problem, we propose a NOCS PC (Point Cloud)-Mesh intertwined refiner, NOCS refiner."
- Break condition: If the initial canonical shape or NOCS coordinates are too inaccurate, refinement may not converge to correct values.

### Mechanism 3
- Claim: The warp field prediction efficiently maps from canonical space to task space without computationally expensive Marching Cubes reconstruction.
- Mechanism: After obtaining refined canonical coordinates, per-point features are scattered into a 3D feature volume using predicted NOCS coordinates, then fed into a 3D UNet to obtain a dense feature volume. The warp field, predicted by a learned MLP, maps query points from canonical space to task space, providing complete garment geometry including occluded parts.
- Core assumption: The canonical space representation can be effectively mapped to task space through a learned warp field, and this approach is computationally more efficient than mesh reconstruction.
- Evidence anchors: [abstract]: "Finally, it maps the prediction in canonical space to the task space (i.e. coordinate frame of the input point cloud)." [section]: "It is achieved by warp field prediction [11], which is an implicit neural function w(p;V) ∈R3 that takes a query point p in the canonical space as input and infers the corresponding location of p in task space."
- Break condition: If the warp field mapping becomes too complex or non-linear for the learned MLP to capture accurately, the mapping quality may degrade.

## Foundational Learning

- Concept: Normalized Object Coordinate Space (NOCS)
  - Why needed here: Garments have near-infinite degrees of freedom and lack a fixed canonical pose, making direct pose estimation challenging. NOCS provides a category-specific canonical representation that allows learning to predict garment configurations relative to a normalized space.
  - Quick check question: How does the NOCS representation handle the thin structure and holes in garments compared to watertight object representations?

- Concept: Transformer-based feature fusion
  - Why needed here: Single-frame pose estimation suffers from ambiguity and instability. Transformer-based cross-attention can capture temporal correlations between consecutive frames, improving tracking stability and reducing error accumulation.
  - Quick check question: What are the computational trade-offs of using self-attention and cross-attention in the tracking pipeline compared to simpler fusion methods?

- Concept: Point cloud feature extraction with sparse 3D convolution
  - Why needed here: Garments exhibit complex local geometry and fine details that require high-resolution feature extraction. Sparse 3D convolution networks can efficiently extract per-point features from partial point clouds while handling the thin structure of garments.
  - Quick check question: Why might a sparse 3D convolution network be preferred over point-based networks like PointNet++ for this specific task?

## Architecture Onboarding

- Component map: Input point clouds → ResUNet3D feature extractor → Transformer-based NOCS predictor → PC-Mesh refiner → Warp field mapper → Output task space geometry
- Critical path: Input → Stage 1 NOCS predictor → Stage 2 NOCS refiner → Stage 3 Warp field mapper → Output
- Design tradeoffs:
  - Using NOCS classification vs. regression: Classification provides discretized predictions that may be more robust to noise but less precise
  - Inter-frame fusion vs. single-frame processing: Fusion improves stability but adds computational overhead
  - Simultaneous coordinate and shape refinement vs. sequential refinement: Simultaneous refinement may better address coupled errors but is more complex
- Failure signatures:
  - Large error accumulation over time: Indicates NOCS refiner not effectively reducing errors
  - Unstable predictions between consecutive frames: Suggests inter-frame fusion not capturing temporal correlations well
  - Poor correspondence distance: Indicates warp field mapping not accurately aligning canonical and task spaces
- First 3 experiments:
  1. Evaluate NOCS coordinate prediction accuracy (Dnocs) on validation set with different noise levels in first-frame initialization
  2. Test tracking stability by measuring correspondence distance (Dcorr) across long sequences with varying frame intervals
  3. Compare tracking speed (FPS) with baseline GarmentNets implementation under identical hardware conditions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well would GarmentTracking generalize to garments with significantly different topology (e.g., dresses, coats, or garments with complex accessories) compared to the four categories in the VR-Folding dataset?
- Basis in paper: [explicit] The paper mentions that VR-Folding contains four garment categories (Shirt, Pants, Top, Skirt) and shows generalization to real-world data, but does not test significantly different topologies.
- Why unresolved: The current experiments only evaluate on garments with relatively simple, canonical shapes. The method's robustness to more complex garment structures remains unknown.
- What evidence would resolve it: Testing GarmentTracking on a dataset containing garments with complex topologies (e.g., dresses, coats with hoods, or garments with accessories) and comparing performance metrics would clarify generalization limits.

### Open Question 2
- Question: Would incorporating temporal consistency constraints directly into the training objective (beyond the implicit consistency from inter-frame feature fusion) improve long-term tracking stability?
- Basis in paper: [inferred] The paper mentions that accumulated errors could cause problems during long-term tracking and uses a refiner to mitigate this, but doesn't explicitly incorporate temporal consistency in the loss function.
- Why unresolved: The current approach relies on feature fusion and refiner modules to handle temporal consistency, but explicit temporal constraints in training could potentially enhance stability.
- What evidence would resolve it: Adding temporal consistency losses (e.g., enforcing smooth transitions between consecutive frames) during training and measuring improvements in long-term tracking accuracy would provide insight.

### Open Question 3
- Question: How would GarmentTracking perform in scenarios with significant occlusions by objects other than the human hands (e.g., furniture, other garments, or obstacles)?
- Basis in paper: [explicit] The paper mentions that garments have thin structures with holes and that previous methods for clothed humans assume priors of the human body, but it doesn't test scenarios with external occlusions.
- Why unresolved: The VR-Folding dataset and experiments focus on garment manipulation by human hands, without testing occlusions from external objects.
- What evidence would resolve it: Evaluating GarmentTracking on datasets or scenarios where garments are occluded by furniture, other garments, or obstacles would reveal robustness to external occlusions.

## Limitations

- Limited evaluation on real-world data - only 10 real-world sequences compared to extensive synthetic testing
- Dependence on accurate first-frame initialization, which can accumulate errors over long sequences
- Computational efficiency gains partially offset by added complexity of inter-frame fusion

## Confidence

- Performance claims (accuracy metrics, FPS): High
- Mechanism explanations (Transformer fusion, PC-Mesh refinement): Medium
- Real-world generalization: Low

## Next Checks

1. Test tracking robustness by systematically varying the frame interval between consecutive inputs to evaluate temporal correlation capture
2. Analyze error accumulation patterns by measuring pose drift over sequences of increasing length
3. Compare computational efficiency breakdown to identify bottlenecks and validate the claimed 5x speedup over GarmentNets