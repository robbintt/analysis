---
ver: rpa2
title: 'LgTS: Dynamic Task Sampling using LLM-generated sub-goals for Reinforcement
  Learning Agents'
arxiv_id: '2310.09454'
source_url: https://arxiv.org/abs/2310.09454
tags:
- agent
- goal
- state
- task
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LgTS addresses the challenge of learning reinforcement learning
  policies for complex sequential tasks by leveraging large language models (LLMs)
  to generate sub-goal sequences and employing a Teacher-Student learning algorithm
  to minimize environmental interactions. The method queries an off-the-shelf LLM
  to produce multiple feasible paths to a goal state, represented as a directed acyclic
  graph, where nodes are sub-goal states and edges are sub-tasks.
---

# LgTS: Dynamic Task Sampling using LLM-generated sub-goals for Reinforcement Learning Agents

## Quick Facts
- arXiv ID: 2310.09454
- Source URL: https://arxiv.org/abs/2310.09454
- Reference count: 8
- Key outcome: LgTS uses LLM-generated sub-goal DAGs and Teacher-Student learning to achieve oracle-like success rates with fewer environmental interactions on gridworld tasks.

## Executive Summary
LgTS addresses the challenge of learning reinforcement learning policies for complex sequential tasks by leveraging large language models (LLMs) to generate sub-goal sequences and employing a Teacher-Student learning algorithm to minimize environmental interactions. The method queries an off-the-shelf LLM to produce multiple feasible paths to a goal state, represented as a directed acyclic graph, where nodes are sub-goal states and edges are sub-tasks. The RL agent then learns policies for these sub-tasks using an adaptive Teacher-Student approach, with the Teacher sampling tasks based on the Student's learning progress. Experiments on gridworld-based DoorKey and search-and-rescue domains show that LgTS achieves comparable success rates to oracle-guided methods while reducing the number of environmental interactions required to learn successful policies.

## Method Summary
LgTS uses an off-the-shelf LLM to generate multiple feasible sub-goal sequences for a given environment, converting these into a directed acyclic graph (DAG) where nodes represent sub-goal states and edges represent sub-tasks. An RL agent learns policies for these sub-tasks using a Teacher-Student learning algorithm, where the Teacher dynamically samples tasks based on the Student's learning progress and performance. The Teacher maintains Q-values to track success rates on sub-tasks, focusing learning on promising paths while discarding unpromising ones. This approach minimizes environmental interactions while achieving comparable success rates to oracle-guided methods.

## Key Results
- Achieved comparable success rates to oracle-guided methods on DoorKey and search-and-rescue gridworld tasks
- Reduced environmental interactions compared to learning from scratch and reward shaping baselines
- Demonstrated sample-efficient learning through adaptive task sampling and DAG-based sub-goal decomposition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LgTS reduces environmental interactions by using an LLM to propose multiple feasible paths and a Teacher-Student algorithm to dynamically focus learning on promising sub-tasks.
- Mechanism: The LLM generates a directed acyclic graph (DAG) of sub-goal sequences. The Teacher samples sub-tasks based on the Student's performance, discarding unpromising ones and focusing on paths likely to lead to the goal.
- Core assumption: The LLM can generate multiple semantically valid paths even without fine-tuning or access to transition dynamics.
- Evidence anchors:
  - [abstract] "Our approach does not assume access to a proprietary or a fine-tuned LLM, nor does it require pre-trained policies that achieve the sub-goals proposed by the LLM."
  - [section 4] "The LLM outputs a number of ordered sub-goal sequences... These set of sequences can effectively be converted to a directed acyclic graph where the start node of the graph is the initial high-level state of the agent and the goal node is the final high-level state."
  - [corpus] Weak evidence: no direct citations about LLM-generated sub-goals in DAG form.
- Break condition: If the LLM consistently generates invalid or non-semantically meaningful paths, the DAG becomes unusable and the Teacher-Student loop fails.

### Mechanism 2
- Claim: The Teacher-Student learning loop dynamically allocates learning effort, minimizing interactions on unpromising paths.
- Mechanism: The Teacher uses Q-values to track success rates on sub-tasks, sampling promising tasks more frequently and discarding those leading to dead ends.
- Core assumption: Early returns on sub-task performance are predictive of eventual success, allowing effective pruning of the search space.
- Evidence anchors:
  - [section 4] "The Teacher observes the Student's performance on these interactions and updates its Q-Value for Task(e)... Once a policy for the Task(q, p) converges, we append Task(q, p) to the set of Learned Tasks LT and remove it from the set of Active Tasks AT."
  - [abstract] "The RL agent uses Teacher-Student learning algorithm to learn a set of successful policies for reaching the goal state from the start state while simultaneously minimizing the number of environmental interactions."
  - [corpus] No direct citations for adaptive task sampling in RL; this is inferred from curriculum learning literature.
- Break condition: If convergence detection is too conservative or too lenient, the system may waste interactions or fail to find a viable path.

### Mechanism 3
- Claim: LgTS is robust to prompt variation and distractor objects by relying on the LLM's ability to generate multiple valid paths.
- Mechanism: Even with synonyms or distractors, the LLM produces several candidate paths; the Teacher-Student loop naturally filters out irrelevant ones through performance-based sampling.
- Core assumption: The LLM's response contains at least one valid path, even if other paths are suboptimal or include distractors.
- Evidence anchors:
  - [section 5.2] "Since the optimal path or the task solution has not changed, the paths suggested by the LLM should ignore the distractor objects."
  - [section 5.3] "To demonstrate how the prompt influences the LLM output... we evaluated LgTS by changing the prompt to the LLM... The results... show that LgTS reaches a successful policy quicker..."
  - [corpus] No direct citations for robustness to distractors; inferred from experiment descriptions.
- Break condition: If the LLM's output quality degrades significantly with prompt variation, the system may fail to find any viable path.

## Foundational Learning

- Concept: Reinforcement Learning with sparse rewards
  - Why needed here: LgTS operates in an environment where only goal-reaching yields reward; the agent must learn policies for sub-tasks without dense shaping.
  - Quick check question: What is the difference between sparse and shaped rewards in RL?

- Concept: Teacher-Student curriculum learning
  - Why needed here: The Teacher dynamically selects which sub-tasks the Student should focus on, based on learning progress, to minimize total interactions.
  - Quick check question: How does a Teacher agent decide which task to sample next in a curriculum learning setup?

- Concept: Directed Acyclic Graphs (DAGs) for task decomposition
  - Why needed here: The LLM output is converted into a DAG where nodes are sub-goals and edges are sub-tasks, structuring the learning problem.
  - Quick check question: What is a DAG and why is it suitable for representing sequential sub-goal paths?

## Architecture Onboarding

- Component map: LLM module -> Graph parser -> RL agent -> Teacher module -> Student module -> Environment interface
- Critical path: 1. Generate prompt → LLM → ordered sub-goal lists. 2. Parse lists → DAG. 3. Initialize Teacher Q-values. 4. Sample sub-task → Student learns → report returns. 5. Update Q-values → convergence check → discard or add new tasks. 6. Repeat until goal node reached.
- Design tradeoffs:
  - Prompt complexity vs. LLM reliability: More detailed prompts may yield better paths but risk confusing the LLM.
  - Exploration vs. exploitation: Teacher must balance trying new tasks against focusing on known promising ones.
  - Sub-task granularity: Too fine-grained → many small tasks; too coarse → harder to learn policies.
- Failure signatures:
  - LLM outputs invalid or repetitive paths → DAG parsing fails or yields degenerate graph.
  - Teacher Q-values stagnate → sampling gets stuck on unpromising tasks.
  - Student never converges → learning progress stalls, Teacher keeps sampling same tasks.
- First 3 experiments:
  1. Validate DAG construction: Feed known valid sub-goal lists, check correct graph output.
  2. Test Teacher sampling: Use mock Student returns, verify Q-value updates and task selection logic.
  3. End-to-end minimal task: Simple 2-node DAG, confirm policies learned and convergence detected.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LgTS compare to human-designed curricula for reinforcement learning tasks?
- Basis in paper: [explicit] The paper mentions that LgTS uses a Teacher-Student learning strategy to sample sub-tasks for the Student agent to learn, aiming to minimize environmental interactions. It compares LgTS to baseline approaches including Teacher-student curriculum learning (TSCL), but does not directly compare to human-designed curricula.
- Why unresolved: The paper does not provide a direct comparison between LgTS and human-designed curricula, leaving the question of how LgTS performs relative to human expertise unanswered.
- What evidence would resolve it: An experiment comparing the performance of LgTS to a human-designed curriculum on the same tasks, measuring factors such as success rate, number of environmental interactions, and learning time.

### Open Question 2
- Question: How does the performance of LgTS scale with the complexity of the environment and the number of sub-goals?
- Basis in paper: [explicit] The paper mentions that LgTS was tested on a complex urban Search and Rescue domain with a multi-goal task and a fully-connected graph with 24 distinct transitions. However, it does not provide a detailed analysis of how LgTS scales with increasing complexity and number of sub-goals.
- Why unresolved: The paper does not explore the limits of LgTS's scalability, leaving the question of how it performs on even more complex tasks unanswered.
- What evidence would resolve it: A systematic study of LgTS's performance on environments with varying numbers of sub-goals and levels of complexity, measuring factors such as success rate, number of environmental interactions, and learning time.

### Open Question 3
- Question: How robust is LgTS to errors or inaccuracies in the natural language descriptions of entities and predicates?
- Basis in paper: [explicit] The paper mentions that LgTS relies on natural language descriptions of entities and predicates, and discusses the potential for errors or inaccuracies in these descriptions. However, it does not provide a detailed analysis of how such errors affect LgTS's performance.
- Why unresolved: The paper does not explore the impact of errors or inaccuracies in the natural language descriptions on LgTS's performance, leaving the question of its robustness unanswered.
- What evidence would resolve it: An experiment introducing controlled errors or inaccuracies in the natural language descriptions and measuring the impact on LgTS's performance, comparing it to the performance with accurate descriptions.

## Limitations
- The method assumes the LLM can generate multiple semantically valid paths without access to environment dynamics or fine-tuning.
- The approach is evaluated only on gridworld domains (DoorKey and search-and-rescue), which may not generalize to more complex or stochastic environments.
- The Teacher-Student sampling relies on early performance signals to prune unpromising paths, but the paper doesn't provide detailed validation of these thresholds.

## Confidence
- High confidence in the basic mechanism: using LLM-generated sub-goals to structure learning and Teacher-Student sampling to minimize interactions is theoretically sound.
- Medium confidence in empirical claims: the paper shows reduced interactions vs baselines in limited domains, but lacks ablation studies on Teacher sampling parameters or LLM robustness to prompt variations.
- Low confidence in generalization: no experiments demonstrate performance on continuous control tasks, stochastic environments, or with different LLM architectures.

## Next Checks
1. Test Teacher sampling sensitivity: Run ablation experiments varying the Q-value update rate and convergence thresholds to identify optimal parameter settings and failure modes.
2. Validate LLM robustness: Systematically perturb prompts with synonyms, distractors, and semantically ambiguous descriptions to measure degradation in path quality and final success rates.
3. Scale to stochastic domains: Evaluate LgTS on gridworlds with probabilistic transitions or partial observability to test whether the approach maintains sample efficiency when the environment deviates from deterministic assumptions.