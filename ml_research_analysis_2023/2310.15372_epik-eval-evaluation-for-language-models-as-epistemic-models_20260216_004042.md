---
ver: rpa2
title: 'EpiK-Eval: Evaluation for Language Models as Epistemic Models'
arxiv_id: '2310.15372'
source_url: https://arxiv.org/abs/2310.15372
tags:
- answer
- task
- parameters
- story
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates large language models' (LLMs) capacity
  to consolidate knowledge from multiple training documents. The authors introduce
  EpiK-Eval, a novel question-answering benchmark that evaluates whether LLMs can
  construct coherent knowledge representations from segmented narratives.
---

# EpiK-Eval: Evaluation for Language Models as Epistemic Models

## Quick Facts
- arXiv ID: 2310.15372
- Source URL: https://arxiv.org/abs/2310.15372
- Reference count: 40
- Key outcome: Large language models struggle to consolidate knowledge from segmented narratives, showing increased hallucination rates and decreased recall accuracy compared to unsegmented contexts

## Executive Summary
This paper introduces EpiK-Eval, a novel benchmark for evaluating large language models' ability to consolidate knowledge from multiple training documents. The study reveals that when information is spread across multiple sequences, models exhibit Type I behavior, failing to construct unified knowledge representations. Through experiments with T5, Flan-T5, and OPT models, the authors demonstrate significant performance gaps in hallucination rates and recall accuracy between segmented and unsegmented training setups, suggesting fundamental limitations in current language model training objectives.

## Method Summary
The EpiK-Eval benchmark consists of 1800 stories, each split into either unsegmented or segmented versions. Two copies of each language model are fine-tuned: one on unsegmented stories (MU) and one on segmented stories (MS), both trained with corresponding question-answer pairs. Models are evaluated on their ability to answer questions requiring information consolidation, with performance measured through accuracy metrics and hallucination rate (percentage of recalled sentences containing errors).

## Key Results
- Models trained on segmented stories show significantly higher hallucination rates compared to those trained on unsegmented stories
- Performance gap between segmented and unsegmented training persists across all model sizes tested (T5-Small to T5-Large)
- Scaling models improves performance in both settings but does not close the consolidation gap
- Knowledge consolidation challenges represent Type I system behavior in language models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models trained on segmented narratives exhibit Type I behavior, failing to consolidate knowledge across document boundaries.
- Mechanism: When trained on story segments, the model treats each segment as independent, unable to learn inter-sequence dependencies necessary for constructing a unified knowledge state.
- Core assumption: The model's inability to consolidate knowledge stems from its training objective prioritizing next-token prediction within sequences over cross-sequence knowledge integration.
- Evidence anchors:
  - [abstract] "Evaluations across various LLMs reveal significant weaknesses in this domain. We contend that these shortcomings stem from the intrinsic nature of prevailing training objectives."
  - [section 3] "MU and MS are fine-tuned on their respective dataset, DU and DS... This allows to measure the effect of information being spread across separate text sequences and the LMs' ability to consolidate this knowledge at inference."
  - [corpus] Weak evidence - no direct studies on segmented narrative training in related works.
- Break condition: Performance gap disappears when information is consolidated within single documents (unsegmented setup).

### Mechanism 2
- Claim: Hallucination rates increase significantly when models must recall information from multiple training documents.
- Mechanism: Models trained on segmented stories lack a consolidated knowledge state, leading them to generate incorrect facts during recall when information spans multiple sequences.
- Core assumption: Hallucinations are a symptom of the model's inability to properly consolidate knowledge from distributed training data.
- Evidence anchors:
  - [abstract] "increased hallucination rates and decreased recall accuracy"
  - [section 4.3] "Models trained on segmented stories display a significant gap in hallucination rates compared to those trained on unsegmented stories."
  - [corpus] Weak evidence - hallucination in context of knowledge consolidation is not directly addressed in related works.
- Break condition: Hallucination rates match those of models trained on unsegmented stories when knowledge consolidation is achieved.

### Mechanism 3
- Claim: Scaling models improves recall performance in both segmented and unsegmented settings, but the performance gap remains.
- Mechanism: Larger models can better encode and recall information, but the fundamental issue of knowledge consolidation across sequences persists regardless of scale.
- Core assumption: While larger models can handle more complex patterns within sequences, they still struggle with dependencies that span multiple training sequences.
- Evidence anchors:
  - [abstract] "This pronounced disparity highlights an intrinsic shortcoming in existing LMs."
  - [section 4.1] "When scaling the LMs, performance generally improves as LMs are scaled in both segmented and unsegmented setups."
  - [corpus] Weak evidence - scaling effects on knowledge consolidation are not directly studied in related works.
- Break condition: Performance gap between segmented and unsegmented training approaches closes at sufficient scale.

## Foundational Learning

- Concept: Type I vs Type II systems
  - Why needed here: Understanding the distinction between systems that maintain independent information across observations (Type I) versus those that consolidate into a unified knowledge state (Type II) is fundamental to interpreting the results.
  - Quick check question: What key difference distinguishes Type I systems from Type II systems in how they process and integrate information?

- Concept: Knowledge consolidation
  - Why needed here: The ability to consolidate knowledge from multiple sources is central to the benchmark's purpose and explains why models struggle with segmented narratives.
  - Quick check question: Why does training on segmented narratives create a more challenging consolidation problem than training on unsegmented narratives?

- Concept: Hallucination rate measurement
  - Why needed here: Understanding how hallucination rates are calculated (percentage of recalled sentences containing errors) is crucial for interpreting the results and implications.
  - Quick check question: How is hallucination rate defined in this study, and what does it measure?

## Architecture Onboarding

- Component map:
  - Data generation pipeline -> Training setup -> Evaluation pipeline

- Critical path:
  1. Generate stories and corresponding Q/A pairs
  2. Split into training/validation/test sets for both unsegmented and segmented versions
  3. Fine-tune MU on DU + Q/A examples, MS on DS + Q/A examples
  4. Evaluate both models on test set
  5. Compare performance metrics (accuracy, hallucination rate)

- Design tradeoffs:
  - Memory vs accuracy: Using smaller models for experimentation vs. larger models that might better handle consolidation
  - Dataset size vs. diversity: More stories provide better training but increase computational cost
  - Granularity of segmentation: Sentence-level segmentation vs. larger chunks affects consolidation difficulty

- Failure signatures:
  - High hallucination rates in segmented setup indicate failure to consolidate knowledge
  - Persistent performance gap between unsegmented and segmented training suggests fundamental consolidation issues
  - Decreasing performance with scale in certain models may indicate overfitting or architectural limitations

- First 3 experiments:
  1. Run baseline comparison: Train T5-Small on both unsegmented and segmented datasets, compare accuracy and hallucination rates
  2. Ablation study: Train on partially segmented data (some stories unsegmented, others segmented) to measure consolidation threshold
  3. Curriculum learning: Train first on unsegmented data, then fine-tune on segmented data to see if consolidation improves

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do larger language models (beyond the 172B parameter estimate) exhibit improved knowledge consolidation when trained on segmented narratives?
- Basis in paper: Explicit - The authors state "experiments with larger models are essential" to determine if performance plateaus before reaching unsegmented-trained levels
- Why unresolved: The authors lacked compute resources to test models larger than 2.7B parameters
- What evidence would resolve it: Experimental results showing performance trends on segmented narratives across a wider range of model sizes, particularly in the 100B+ parameter range

### Open Question 2
- Question: How do specific training objectives (like RLHF or self-taught reasoning) impact knowledge consolidation capabilities compared to standard language modeling objectives?
- Basis in paper: Explicit - The authors advocate for "novel methods aimed towards improvements in knowledge consolidation" and suggest "methods such as RL-HF or self-taught" as potential solutions
- Why unresolved: The paper only demonstrates the problem exists but doesn't test alternative training approaches
- What evidence would resolve it: Direct comparison of models trained with different objectives on the EpiK-Eval benchmark, measuring consolidation performance and hallucination rates

### Open Question 3
- Question: Does the knowledge consolidation problem persist when using longer context windows to concatenate relevant information during training?
- Basis in paper: Explicit - The authors discuss this as a potential alternative to consolidation, noting "it could be argued that the inclusion of a comprehensive context through prompts could be an effective alternative"
- Why unresolved: The authors only speculate about this approach rather than testing it
- What evidence would resolve it: Experimental results comparing models trained on concatenated sequences versus segmented sequences on the EpiK-Eval benchmark, measuring both performance and hallucination rates

## Limitations
- Limited exploration of alternative model architectures beyond T5 variants
- No detailed training hyperparameters provided for exact reproduction
- Focus on simple sentence-level segmentation without exploring other segmentation strategies
- Limited scope to encoder-decoder models with brief mention of causal LMs

## Confidence
- High Confidence:
  - Models perform significantly worse on segmented narratives compared to unsegmented narratives
  - Hallucination rates increase when information is spread across multiple sequences
  - Scaling models improves performance but does not eliminate the consolidation gap

- Medium Confidence:
  - The performance gap stems from intrinsic limitations of prevailing training objectives
  - The behavior represents Type I system characteristics rather than Type II
  - Knowledge consolidation is the primary bottleneck for cross-document reasoning

- Low Confidence:
  - Specific architectural changes that would resolve the consolidation problem
  - The exact mechanisms by which segmentation disrupts knowledge integration
  - Whether these findings generalize to real-world document processing scenarios

## Next Checks
1. **Architecture-Specific Investigation**: Test the EpiK-Eval benchmark across a broader range of model architectures (causal LMs like GPT, hybrid models, retrieval-augmented models) to determine if the consolidation problem is universal or architecture-dependent.

2. **Segmentation Strategy Analysis**: Systematically vary the segmentation granularity (paragraph, chapter, arbitrary breaks) and measure the corresponding impact on consolidation performance to identify whether there are thresholds or patterns in how segmentation affects knowledge integration.

3. **Training Objective Ablation**: Design experiments that modify training objectives to explicitly encourage cross-sequence knowledge integration (e.g., multi-sequence masking, contrastive learning across document boundaries) and measure whether this reduces the performance gap between segmented and unsegmented training.