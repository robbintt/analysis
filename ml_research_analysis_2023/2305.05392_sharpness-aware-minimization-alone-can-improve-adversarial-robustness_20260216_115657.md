---
ver: rpa2
title: Sharpness-Aware Minimization Alone can Improve Adversarial Robustness
arxiv_id: '2305.05392'
source_url: https://arxiv.org/abs/2305.05392
tags:
- adversarial
- training
- robustness
- accuracy
- perturbation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel understanding of Sharpness-Aware Minimization
  (SAM) in the context of adversarial robustness. The authors demonstrate that both
  SAM and adversarial training (AT) can be viewed as specific feature perturbations
  that improve adversarial robustness.
---

# Sharpness-Aware Minimization Alone can Improve Adversarial Robustness

## Quick Facts
- arXiv ID: 2305.05392
- Source URL: https://arxiv.org/abs/2305.05392
- Reference count: 38
- This paper demonstrates that Sharpness-Aware Minimization (SAM) alone can improve adversarial robustness without sacrificing clean accuracy, providing a novel understanding of SAM as a lightweight substitute for adversarial training.

## Executive Summary
This paper presents a novel understanding of Sharpness-Aware Minimization (SAM) in the context of adversarial robustness. The authors demonstrate that both SAM and adversarial training (AT) can be viewed as specific feature perturbations that improve adversarial robustness, differing primarily in perturbation strength. Theoretical evidence is provided in a simplified model, showing that SAM and AT improve robustness by perturbing features with different strengths. Experimental results on benchmark datasets verify the proposed insight and demonstrate that SAM can be regarded as a lightweight substitute for AT under certain requirements.

## Method Summary
The paper investigates SAM's relationship to adversarial robustness by comparing it with standard training and adversarial training on CIFAR-10 and CIFAR-100 datasets using PreActResNet-18 models. SAM is implemented with a perturbation hyperparameter ρ ∈ {0.1, 0.2, 0.4}, while adversarial training uses 10-step PGD attacks. All methods are evaluated on ℓ∞ and ℓ2 adversarial attacks with various perturbation bounds, measuring both clean accuracy and robust accuracy. The theoretical analysis focuses on a simplified linear model to understand the relationship between SAM and AT.

## Key Results
- SAM improves adversarial robustness compared to standard training without significant clean accuracy degradation
- SAM and AT differ primarily in perturbation strength, leading to different accuracy-robustness trade-offs
- SAM can serve as a lightweight substitute for AT when computational efficiency is prioritized over maximum robustness
- Theoretical analysis shows SAM and AT improve robustness by perturbing features with different strengths

## Why This Works (Mechanism)

### Mechanism 1
SAM and AT both improve adversarial robustness by applying feature perturbations that eliminate non-robust features from training data. SAM implicitly perturbs the feature space through parameter updates, while AT explicitly perturbs the input space with stronger perturbations targeting non-robust features. Both methods reduce the model's reliance on non-robust features vulnerable to adversarial attacks.

### Mechanism 2
SAM achieves better clean accuracy than AT because it uses moderate, implicit perturbations that preserve useful features while still improving robustness. SAM's parameter-space perturbations are more gentle and uniform, maintaining performance on clean data while improving robustness through implicit feature augmentation.

### Mechanism 3
SAM serves as a lightweight substitute for AT when computational efficiency is prioritized over maximum robustness. SAM achieves comparable robustness gains to AT with significantly lower computational cost by avoiding explicit adversarial example generation during training.

## Foundational Learning

- Concept: Adversarial robustness and adversarial examples
  - Why needed here: Understanding how models can be fooled by small input perturbations is essential to grasp why SAM and AT improve robustness
  - Quick check question: What is the fundamental difference between robust features and non-robust features in the context of adversarial examples?

- Concept: Sharpness-aware minimization and loss landscape geometry
  - Why needed here: SAM's core mechanism involves finding flat regions in the loss landscape, which relates to both generalization and robustness
  - Quick check question: How does minimizing loss sharpness theoretically contribute to both better generalization and improved adversarial robustness?

- Concept: Feature space vs. parameter space perturbations
  - Why needed here: The paper distinguishes between perturbations applied to input features (AT) versus perturbations applied to model parameters (SAM)
  - Quick check question: What is the mathematical relationship between parameter perturbations in SAM and the resulting feature space perturbations?

## Architecture Onboarding

- Component map:
  Training pipeline with SAM optimization -> Perturbation magnitude control (ρ parameter) -> Evaluation framework for both clean and adversarial accuracy -> Comparison baseline with standard training and AT

- Critical path:
  1. Initialize model and SAM optimizer
  2. For each batch, compute SAM gradient (with parameter perturbation)
  3. Update parameters based on SAM objective
  4. Periodically evaluate clean and robust accuracy
  5. Compare results against baselines

- Design tradeoffs:
  - Perturbation strength (ρ) vs. clean accuracy - larger ρ improves robustness but may hurt clean accuracy
  - Computational cost - SAM adds one forward/backward pass per batch vs. AT's multiple passes for adversarial example generation
  - Implicit vs. explicit perturbation - SAM's implicit approach is more efficient but may be less targeted than AT's explicit approach

- Failure signatures:
  - Clean accuracy drops significantly with increased ρ - indicates perturbation is too strong
  - Robust accuracy plateaus despite increasing ρ - suggests implicit perturbations are insufficient
  - Training instability or divergence - may indicate inappropriate learning rate or initialization

- First 3 experiments:
  1. Compare clean accuracy of SAM vs. standard training on CIFAR-10 with varying ρ values
  2. Measure robust accuracy of SAM vs. AT under ℓ∞ attacks with equivalent computational budgets
  3. Analyze feature attribution maps to verify that SAM reduces reliance on non-robust features compared to standard training

## Open Questions the Paper Calls Out

### Open Question 1
How does the perturbation strength of SAM compare to AT in terms of their impact on robustness vs. clean accuracy trade-off? The paper provides theoretical evidence showing that SAM and AT improve adversarial robustness by perturbing features with different strengths, leading to different accuracy and robustness trade-offs, but does not provide empirical evidence on how their perturbation strengths compare.

### Open Question 2
Can SAM be considered a lightweight substitute for AT in all scenarios, or are there specific requirements where SAM is more suitable? The paper suggests SAM can be regarded as a lightweight substitute for AT under certain requirements but does not specify the exact scenarios or requirements where SAM is more suitable.

### Open Question 3
How does the choice of perturbation norm (e.g., ℓ∞ or ℓ2) affect the performance of SAM and AT in terms of robustness and clean accuracy? The paper discusses the impact of perturbation strength on performance but does not explicitly address the effect of the choice of perturbation norm.

## Limitations
- Theoretical analysis is restricted to a simplified linear model
- Empirical validation limited to CIFAR-10 and CIFAR-100 with PreActResNet-18 architecture
- Does not explore diverse model architectures or datasets
- Does not address potential limitations of SAM's implicit perturbation approach against strong adversarial attacks

## Confidence

- **High confidence**: SAM can improve adversarial robustness over standard training without significant clean accuracy degradation
- **Medium confidence**: SAM and AT operate through similar mechanisms of feature perturbation, with the primary difference being perturbation strength
- **Low confidence**: SAM can serve as a general substitute for AT across all scenarios requiring adversarial robustness

## Next Checks

1. **Architecture diversity test**: Evaluate SAM's robustness improvements across multiple architectures (CNNs, transformers, MLPs) on CIFAR-10 to determine if the effect is architecture-dependent.

2. **Dataset generalization**: Test SAM on datasets with different characteristics (ImageNet, SVHN, or tabular data) to verify the robustness improvements generalize beyond CIFAR-style image classification.

3. **Perturbation strength analysis**: Systematically vary the SAM perturbation parameter ρ across a wider range and measure the resulting trade-off between clean accuracy and robust accuracy to identify the optimal operating regime.