---
ver: rpa2
title: 'Lookbehind-SAM: k steps back, 1 step forward'
arxiv_id: '2307.16704'
source_url: https://arxiv.org/abs/2307.16704
tags:
- asam
- lookbehind
- training
- lookahead
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Lookbehind-SAM, a novel optimizer that combines
  multi-step gradient ascent with variance reduction to improve the loss-sharpness
  trade-off in sharpness-aware minimization. Inspired by the Lookahead optimizer,
  Lookbehind performs k gradient ascent steps ("looking behind") at each iteration
  and combines the gradients to bias the descent step toward flatter minima.
---

# Lookbehind-SAM: k steps back, 1 step forward

## Quick Facts
- arXiv ID: 2307.16704
- Source URL: https://arxiv.org/abs/2307.16704
- Reference count: 40
- The paper introduces Lookbehind-SAM, a novel optimizer that combines multi-step gradient ascent with variance reduction to improve the loss-sharpness trade-off in sharpness-aware minimization.

## Executive Summary
Lookbehind-SAM is a novel optimizer that enhances sharpness-aware minimization (SAM) by performing multiple gradient ascent steps ("looking behind") to find flatter minima. Inspired by the Lookahead optimizer, it accumulates perturbations over k ascent steps and combines gradients to bias descent toward flatter regions of the loss landscape. The method consistently outperforms SAM and ASAM baselines across various models and datasets, improving both generalization performance and robustness against noisy weights.

## Method Summary
Lookbehind-SAM combines multi-step gradient ascent with variance reduction via linear interpolation. For each minibatch, it performs k ascent steps to accumulate perturbations, then uses these accumulated perturbations for k descent steps. After each outer iteration, it updates slow weights via linear interpolation with fast weights. The method can be applied to both SAM and ASAM baselines, with the key innovation being the multi-step perturbation accumulation rather than single-step SAM.

## Key Results
- Lookbehind consistently outperforms SAM and ASAM baselines across ResNet, WideResNet, and VGG architectures on CIFAR-10, CIFAR-100, and ImageNet.
- The method achieves better generalization performance while finding flatter minima with lower m-sharpness.
- Lookbehind increases robustness against noisy weights at inference time and reduces catastrophic forgetting in lifelong learning settings.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-step gradient ascent in Lookbehind reduces loss sharpness more effectively than single-step SAM.
- Mechanism: By accumulating perturbations over k ascent steps, Lookbehind explores a larger neighborhood of the loss landscape, identifying flatter minima that are not reachable in a single ascent.
- Core assumption: The direction of the worst-case perturbation does not change drastically over small k steps.
- Evidence anchors:
  - [abstract]: "Lookbehind performs multiple ascent steps behind to enhance the maximization step of SAM and find a worst-case perturbation with higher loss."
  - [section 2.3]: "The core proposal of this paper, Lookbehind (+SAM), presents an alternative and novel way to improve the maximization problem in (1). While Lookahead+SAM attempts to improve the stability of single-step SAM with large ρ, Lookbehind alleviates the instability that arises from performing multiple SAM ascents steps."
  - [corpus]: Weak evidence. No direct citations supporting the multi-step benefit.
- Break condition: If the loss landscape is highly non-convex and the gradient direction changes significantly over k steps, the accumulated perturbation may overshoot or diverge.

### Mechanism 2
- Claim: Variance reduction via linear interpolation between slow and fast weights improves training stability.
- Mechanism: The slow weights act as a smoothed version of the fast weights, reducing the variance introduced by multiple gradient steps and noisy minibatches.
- Core assumption: The slow weights are a good approximation of the optimal parameters over short intervals.
- Evidence anchors:
  - [abstract]: "to mitigate the variance in the descent step arising from the gathered gradients across the multiple ascent steps, we employ linear interpolation to refine the minimization step."
  - [section 2.3]: "After k steps, a linear interpolation of the fast and slow weights, akin to Lookahead+SAM, is conducted."
  - [corpus]: Weak evidence. No direct citations supporting the variance reduction claim.
- Break condition: If the interpolation factor α is too large, the slow weights may lag too far behind, causing convergence to suboptimal minima.

### Mechanism 3
- Claim: Early-stage perturbation with Lookbehind guides optimization to a different basin of the loss landscape.
- Mechanism: Starting with Lookbehind from the beginning of training biases the optimization trajectory toward flatter regions that are not reachable by standard SAM.
- Core assumption: The basin of attraction for flatter minima is different from that of sharper minima.
- Evidence anchors:
  - [section 3.5]: "Lookbehind's benefits are mostly achieved early on in training, suggesting that Lookbehind guides the optimization to converge to a different basin of the loss landscape than SAM."
  - [corpus]: Weak evidence. No direct citations supporting basin switching.
- Break condition: If the initial perturbation is too aggressive, it may destabilize early training and prevent convergence.

## Foundational Learning

- Concept: Sharpness-aware minimization (SAM)
  - Why needed here: Lookbehind builds upon SAM's objective of minimizing both loss value and loss sharpness.
  - Quick check question: What is the key difference between SAM and standard SGD in terms of the objective function?

- Concept: Multi-step optimization
  - Why needed here: Lookbehind uses k ascent steps to explore a larger neighborhood, which is crucial for its effectiveness.
  - Quick check question: How does increasing k affect the trade-off between loss value and sharpness?

- Concept: Variance reduction in stochastic optimization
  - Why needed here: Lookbehind employs linear interpolation to reduce variance, which is essential for stable training with multiple ascent steps.
  - Quick check question: What is the role of the slow weights in variance reduction?

## Architecture Onboarding

- Component map:
  - Fast weights -> k ascent steps -> Perturbation accumulation -> k descent steps -> Slow weights (linear interpolation)

- Critical path:
  1. Initialize fast and slow weights.
  2. Sample minibatch.
  3. Perform k ascent steps, accumulating perturbations.
  4. Perform k descent steps using accumulated perturbations.
  5. Update slow weights via linear interpolation.
  6. Repeat for each outer iteration.

- Design tradeoffs:
  - k vs. α: Higher k may require lower α for stability.
  - ρ vs. k: Larger ρ may reduce the need for high k.
  - Variance vs. sharpness: More ascent steps may increase variance but reduce sharpness.

- Failure signatures:
  - Divergence: If k is too large or α is too high.
  - Poor generalization: If ρ is too small or k is too small.
  - Slow convergence: If α is too low.

- First 3 experiments:
  1. Test Lookbehind with k=2 and α=0.5 on a small CNN on CIFAR-10, comparing to SAM.
  2. Vary k from 2 to 10 with fixed α=0.5, measuring sharpness and generalization.
  3. Test Lookbehind with ASAM instead of SAM, comparing to ASAM and Lookahead+ASAM.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal relationship between the inner step count (k) and the outer step size (α) in Lookbehind?
- Basis in paper: [explicit] The paper notes a diagonal trend in Lookbehind, suggesting a relationship between α and k, with higher α being better when increasing k.
- Why unresolved: The paper only observes the trend but does not provide a theoretical explanation or a precise formula for the optimal relationship.
- What evidence would resolve it: Further theoretical analysis or extensive empirical experiments varying both k and α could reveal the underlying relationship and optimal settings.

### Open Question 2
- Question: How does Lookbehind's performance compare to other sharpness-aware methods like R-Drop or SAM-V1 when applied to the same models and datasets?
- Basis in paper: [inferred] The paper focuses on comparing Lookbehind with SAM and ASAM but does not include other sharpness-aware methods in the comparison.
- Why unresolved: The paper does not provide a comprehensive comparison with all existing sharpness-aware methods.
- What evidence would resolve it: Conducting experiments comparing Lookbehind with other sharpness-aware methods on the same models and datasets would provide a clearer picture of its relative performance.

### Open Question 3
- Question: Can Lookbehind be effectively applied to other optimization tasks beyond image classification, such as natural language processing or reinforcement learning?
- Basis in paper: [explicit] The paper primarily focuses on image classification tasks and does not explore other domains.
- Why unresolved: The paper does not provide evidence of Lookbehind's effectiveness in other optimization tasks.
- What evidence would resolve it: Applying Lookbehind to optimization tasks in other domains and comparing its performance with existing methods would demonstrate its broader applicability.

## Limitations
- Lack of ablation studies to isolate the effects of multi-step ascent versus variance reduction mechanisms.
- Unclear implementation details for perturbation accumulation and learning rate scheduling that could affect reproducibility.
- Theoretical claims about basin switching and gradient direction stability are not empirically validated.

## Confidence
- Generalization claims (High confidence): The consistent improvement across multiple architectures and datasets is well-supported by the presented results.
- Sharpness reduction mechanism (Medium confidence): While multi-step ascent is intuitive, the lack of direct comparison to single-step SAM with larger perturbations weakens this claim.
- Variance reduction mechanism (Medium confidence): The linear interpolation is implemented, but no evidence is provided that it specifically addresses the variance from multiple ascent steps.
- Early-stage perturbation benefits (Low confidence): The claim about basin switching is speculative, with only indirect evidence from timing of benefits.

## Next Checks
1. **Ablation study**: Compare Lookbehind with k=2 against SAM with ρ=0.1 (double the standard value) to test whether multi-step ascent provides benefits beyond simple radius scaling.
2. **Gradient analysis**: Track the angle between consecutive ascent gradients during training to quantify how much the worst-case direction changes over k steps.
3. **Perturbation sensitivity**: Test Lookbehind with different initial perturbation magnitudes (varying ρ) to determine if early-stage benefits require aggressive initial perturbations or emerge from the multi-step process itself.