---
ver: rpa2
title: 'Revisit Input Perturbation Problems for LLMs: A Unified Robustness Evaluation
  Framework for Noisy Slot Filling Task'
arxiv_id: '2310.06504'
source_url: https://arxiv.org/abs/2310.06504
tags:
- perturbation
- input
- llms
- data
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified robustness evaluation framework for
  noisy slot filling task based on large language models (LLMs). The authors construct
  a new dataset, Noise-LLM, to systematically evaluate the dialogue understanding
  capability of LLMs in diverse input perturbation scenarios.
---

# Revisit Input Perturbation Problems for LLMs: A Unified Robustness Evaluation Framework for Noisy Slot Filling Task

## Quick Facts
- **arXiv ID**: 2310.06504
- **Source URL**: https://arxiv.org/abs/2310.06504
- **Reference count**: 39
- **Primary result**: Current open-source LLMs achieve limited perturbation robustness on noisy slot filling tasks

## Executive Summary
This paper proposes a unified robustness evaluation framework for large language models (LLMs) in noisy slot filling tasks. The authors construct a new dataset, Noise-LLM, containing five types of single perturbations and four types of mixed perturbations, systematically evaluating LLMs' dialogue understanding capabilities under diverse input perturbation scenarios. The experiments demonstrate that current open-source LLMs show significant performance degradation when encountering noisy inputs, and the authors provide empirical guidance for future research on robust LLMs.

## Method Summary
The framework constructs the Noise-LLM dataset from RADDLE and SNIPS, applying five types of single perturbations (typos, speech errors, simplifications, verbose expressions, paraphrases) and four types of mixed perturbations. A multi-level data augmentation method creates candidate data pools, while two automatic task demonstration construction strategies (entity-oriented and instance-oriented) are used with various prompt templates. In-context learning is employed to evaluate LLMs' robustness without parameter updates.

## Key Results
- LLMs show significant performance degradation on noisy slot filling tasks compared to clean data
- Entity-oriented and instance-oriented demonstration strategies substantially improve LLM robustness
- Random selection of demonstrations proves more effective than similarity-based selection
- Current open-source LLMs generally achieve limited perturbation robustness performance

## Why This Works (Mechanism)

### Mechanism 1
LLM slot filling performance degrades under input perturbations due to the mismatch between pre-training data distribution and real-world noisy inputs. LLMs trained on clean data lack robustness when encountering character-level typos, speech recognition errors, simplifications, verbose expressions, or paraphrases, leading to reduced semantic understanding and incorrect entity recognition.

### Mechanism 2
Adding entity-oriented or instance-oriented demonstrations to in-context learning significantly improves LLM robustness on noisy slot filling tasks. Demonstrations provide the LLM with explicit examples of how to handle perturbations, guiding it to recognize entities and their boundaries even when the input is corrupted.

### Mechanism 3
Random selection of demonstrations from augmented data pools is more effective than similarity-based selection for improving LLM robustness. Random selection provides diverse noisy examples that stimulate the LLM's ability to generalize across different perturbation types, while similarity-based selection may not cover the full range of perturbations present in the test set.

## Foundational Learning

- **Concept**: Input Perturbation Types (Character, Word, Sentence level)
  - Why needed here: Understanding the different types of perturbations is crucial for designing robust evaluation frameworks and data augmentation strategies.
  - Quick check question: What are the five types of single perturbations mentioned in the paper, and how do they differ in their impact on slot filling performance?

- **Concept**: In-Context Learning (ICL)
  - Why needed here: ICL is the primary method used to adapt LLMs to the slot filling task without fine-tuning, and understanding its mechanics is essential for designing effective demonstration strategies.
  - Quick check question: How does in-context learning differ from traditional fine-tuning, and what are the key components of an ICL prompt for slot filling?

- **Concept**: Data Augmentation Techniques
  - Why needed here: Data augmentation is used to create a diverse pool of noisy examples for training and demonstration selection, which is critical for improving LLM robustness.
  - Quick check question: What are the three levels of data augmentation used in the paper, and how do they correspond to the different types of input perturbations?

## Architecture Onboarding

- **Component map**: Noise-LLM dataset construction -> Multi-level data augmentation -> Automatic task demonstration construction -> In-context learning inference -> Evaluation framework

- **Critical path**: 1. Construct Noise-LLM dataset from RADDLE and SNIPS with perturbations 2. Apply multi-level data augmentation to create candidate data pool 3. Select demonstrations using entity-oriented or instance-oriented strategies 4. Construct prompt with template and demonstrations 5. Perform in-context inference and evaluate performance

- **Design tradeoffs**: Entity-oriented vs. instance-oriented demonstrations (clearer boundaries vs. better context capture), Random vs. similarity-based demonstration selection (diversity vs. semantic consistency), Prompt template design (manual vs. model-recommended templates)

- **Failure signatures**: Significant performance drop on character-level perturbations indicates insufficient robustness to fine-grained noise, Over-prediction of entities in verbose scenarios suggests confusion between task-related and irrelevant content, Inconsistent performance across different demonstration selection strategies may indicate sensitivity to demonstration distribution

- **First 3 experiments**: 1. Evaluate baseline LLM performance on clean Noise-LLM dataset to establish performance without perturbations 2. Test LLM performance on single perturbation types to identify the most challenging perturbations 3. Compare the effectiveness of entity-oriented and instance-oriented demonstration strategies on mixed perturbation data

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of LLMs in noisy slot filling tasks compare to smaller, task-specific models when both are fine-tuned on noisy data? The paper states that "LLMs are usually pre-trained and fine-tuned on perturbation-free datasets, resulting in poorer performance than supervised small language models with robust settings on noisy slot filling tasks" but does not provide a direct comparison when both are fine-tuned on noisy data.

### Open Question 2
What is the impact of different types of input perturbations on the performance of LLMs in slot filling tasks, and how do these perturbations interact when combined? While the paper provides experimental results for single and mixed perturbations, it does not delve into the specific interactions between different types of perturbations and their combined effects on LLM performance.

### Open Question 3
How does the choice of prompt template affect the performance of LLMs in noisy slot filling tasks, and can optimal templates be automatically generated for different types of input perturbations? The paper explores the impact of different prompt templates but does not investigate whether optimal templates can be automatically generated for various input perturbations.

## Limitations
- The framework relies on in-context learning without parameter updates, which may not capture the full potential improvements achievable through fine-tuning
- Automated perturbation generation may not fully represent real-world noise patterns encountered in production systems
- Evaluation focuses on five specific perturbation types, potentially overlooking other noise sources such as code-switching or domain-specific terminology variations

## Confidence

**High Confidence**: The observation that current LLMs show limited robustness to input perturbations is well-supported by experimental results across multiple models and perturbation types. The effectiveness of demonstration-based in-context learning for improving performance is also strongly validated.

**Medium Confidence**: The superiority of random demonstration selection over similarity-based approaches is supported by the experimental results but may be dataset-dependent. The specific mechanisms by which entity-oriented demonstrations outperform instance-oriented ones are inferred rather than explicitly proven.

**Low Confidence**: Claims about the exact contribution of each perturbation type to overall performance degradation are based on aggregated results rather than systematic ablation studies. The generalization of findings to other slot filling datasets or task domains remains uncertain.

## Next Checks

1. **Cross-dataset validation**: Evaluate the Noise-LLM framework on additional slot filling datasets (e.g., ATIS, TOP) to verify whether the observed robustness patterns generalize beyond RADDLE and SNIPS.

2. **Fine-tuning comparison**: Implement a fine-tuning baseline using the augmented dataset to determine whether in-context learning's limitations are fundamental or can be overcome through parameter updates.

3. **Real-world noise validation**: Collect and evaluate performance on actual user-generated noisy dialogue data from deployed systems to validate whether synthetic perturbations accurately represent production scenarios.