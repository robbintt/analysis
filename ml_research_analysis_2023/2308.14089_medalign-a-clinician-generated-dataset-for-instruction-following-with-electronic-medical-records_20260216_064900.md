---
ver: rpa2
title: 'MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic
  Medical Records'
arxiv_id: '2308.14089'
source_url: https://arxiv.org/abs/2308.14089
tags:
- patient
- gpt-4
- retrieve
- instructions
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MedAlign, a benchmark dataset of 983 natural
  language instructions for electronic health record (EHR) data, curated by 15 clinicians
  across 7 specialties. MedAlign includes clinician-written reference responses for
  303 instructions and provides 276 longitudinal EHRs for grounding instruction-response
  pairs.
---

# MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical Records

## Quick Facts
- arXiv ID: 2308.14089
- Source URL: https://arxiv.org/abs/2308.14089
- Reference count: 40
- Primary result: Clinician-generated EHR instruction-following benchmark shows high error rates (35-68% incorrect) for general LLMs, with 8.3% accuracy drop when reducing context from 32k to 2k tokens.

## Executive Summary
MedAlign introduces a benchmark dataset of 983 clinician-generated instructions for electronic health record (EHR) data, curated by 15 clinicians across 7 specialties. The dataset includes 276 longitudinal EHRs and 303 clinician-written reference responses, enabling realistic evaluation of large language models on instruction-following tasks that reflect actual clinical information needs. When tested on 6 general domain LLMs, the study found high error rates ranging from 35% to 68%, with performance dropping 8.3% when reducing context from 32k to 2k tokens. The authors also demonstrate that automated metrics like COMET can approximate clinician rankings with correlation approaching human inter-rater reliability.

## Method Summary
The MedAlign dataset was created by recruiting 15 clinicians to author natural language instructions based on realistic EHR vignettes, then providing both gold responses and paired patient records for grounding. The dataset contains 983 instructions, 276 longitudinal EHRs, and 303 reference responses. Six general domain LLMs (including GPT-4 variants, Vicuña-7B, Vicuña-13B, and MPT-7B-Instruct) were evaluated using a standard prompt template with truncated EHR contexts. Clinicians evaluated responses for correctness and ranked them by quality, while automated metrics like COMET, BERTScore, and others were computed to assess correlation with human judgments.

## Key Results
- GPT-4 achieved 35% correct responses while MPT-7B-Instruct achieved 68% incorrect responses on MedAlign instructions
- Reducing context length from 32k to 2k tokens caused an 8.3% drop in GPT-4 accuracy
- COMET metric showed strongest correlation with clinician rankings (0.37) approaching human inter-rater reliability (0.44)
- Correctness rates varied across EHR length quartiles, with GPT-4 (32k+MR) performing best in the 39k-65k quartile (39% correct)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clinician-generated instructions paired with EHRs enable more realistic LLM evaluation than multiple-choice medical benchmarks.
- Mechanism: Real-world clinical tasks require synthesizing unstructured notes, structured data, and temporal reasoning. The MedAlign dataset captures this complexity by having clinicians author instructions based on realistic EHR vignettes, then providing both gold responses and paired patient records for grounding.
- Core assumption: Instructions that reflect actual clinician information needs are more representative of deployment scenarios than exam-style questions.
- Evidence anchors: [abstract] "Existing question answering datasets for electronic health record (EHR) data fail to capture the complexity of information needs and documentation burdens experienced by clinicians."

### Mechanism 2
- Claim: Longer context windows improve EHR instruction-following performance.
- Mechanism: Comprehensive EHRs contain more than 32k tokens in 80% of cases. Increasing available context from 2k to 32k tokens yields an 8.3% increase in correct responses, indicating that richer patient histories are critical for accurate reasoning.
- Core assumption: Instructions that require longitudinal data benefit from seeing the full patient timeline rather than truncated snippets.
- Evidence anchors: [section 5] "We found high error rates, ranging from 35% (GPT-4) to 68% (MPT-7B-Instruct), and an 8.3% drop in accuracy moving from 32k to 2k context lengths for GPT-4."

### Mechanism 3
- Claim: Automated metrics like COMET can approximate clinician rankings for LLM evaluation.
- Mechanism: COMET, a learned metric using XLM-RoBERTa, correlates with human preference at 0.37 Kendall's Tau, approaching the 0.44 inter-rater reliability between clinicians. This suggests scalable, low-cost evaluation is possible without sacrificing too much signal.
- Core assumption: Human preference rankings for instruction-following quality align with learned text generation metrics.
- Evidence anchors: [section 6] "Overall, COMET [38] exhibited the strongest correlation with clinician preference rankings, approaching the level of human inter-reviewer reliability (0.37 vs. 0.44)."

## Foundational Learning

- Concept: EHR data structure and clinical terminology
  - Why needed here: MedAlign uses XML-encoded OMOP CDM patient timelines; understanding this schema is essential for interpreting instruction-EHR pairs and designing retrieval or context strategies.
  - Quick check question: What are the main OMOP tables represented in the XML (e.g., condition_occurrence, measurement, drug_exposure) and how do they map to clinical concepts?

- Concept: Instruction-following vs. QA
  - Why needed here: The dataset shifts from extracting answers to synthesizing them, requiring generative reasoning over multiple data types. Engineers must distinguish when to retrieve vs. when to compose.
  - Quick check question: How does the "correctness" definition for instruction-following differ from a factual answer extraction task?

- Concept: Automated evaluation metrics for NLG
  - Why needed here: COMET, BERTScore, and other metrics are used to rank models without clinician review. Understanding their strengths and failure modes is critical for interpreting benchmark results.
  - Quick check question: Why does COMET outperform source-free metrics like BLEU in this clinical domain?

## Architecture Onboarding

- Component map: Data ingestion → OMOP-to-XML conversion → Instruction collection (Google Forms → CSV) → BM25-based EHR retrieval → LLM inference (context truncation, multi-step refinement) → Clinician evaluation pipeline (binary correctness + ranking) → Automated metric computation (COMET, BERTScore, etc.) → Analysis and reporting

- Critical path: 1. Instruction-EHR matching via BM25 2. Context length determination and truncation 3. LLM generation with proper prompt templating 4. Clinician or automated evaluation 5. Metric correlation analysis

- Design tradeoffs: Decoupling instruction collection from EHR pairing increases diversity but risks relevance mismatches. Context truncation preserves recent data but may lose historical context critical for longitudinal reasoning. Automated metrics speed evaluation but may miss clinically subtle errors.

- Failure signatures: Low relevance in instruction-EHR matches (BM25 scores but manual review flags irrelevant). High API content filtering errors from Azure OpenAI when EHRs contain anatomical terms. Metrics show high correlation but clinicians disagree on rankings.

- First 3 experiments: 1. Test BM25 vs. semantic retrieval (e.g., Sentence-BERT) for instruction-EHR relevance. 2. Compare different context selection strategies (recent-first vs. random vs. BM25-weighted). 3. Evaluate additional automated metrics (e.g., UniEval variants) to see if correlation with human judgment improves.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MedAlign instructions vary across different EHR lengths, and what is the optimal context length for maximizing LLM accuracy?
- Basis in paper: [explicit] The paper analyzes LLM performance across EHR length quartiles and finds that correctness does not consistently decline with increasing EHR length. GPT-4 (32k+MR) performs best in the second smallest quartile (39k-65k), while GPT-4 (32k) performs best in the largest quartile (114k-496k).
- Why unresolved: The paper does not provide a clear explanation for why certain context lengths are optimal for different EHR lengths. It also does not explore the potential impact of EHR length on other LLM capabilities beyond instruction following.
- What evidence would resolve it: Further experiments varying context length and EHR length to identify the optimal combinations for different LLM tasks and capabilities.

### Open Question 2
- Question: How do automated evaluation metrics, such as COMET, compare to clinician evaluations in terms of accuracy and reliability for assessing LLM performance on MedAlign instructions?
- Basis in paper: [explicit] The paper reports correlations between clinician rankings and automated metrics, with COMET exhibiting the strongest correlation (0.37) with clinician preference rankings. It also conducts experiments using COMET to evaluate additional LLMs without clinician review.
- Why unresolved: The paper does not provide a detailed analysis of the strengths and limitations of automated metrics compared to clinician evaluations. It also does not explore the potential impact of automated metrics on the development and deployment of LLMs in healthcare.
- What evidence would resolve it: Further experiments comparing the performance of automated metrics and clinician evaluations across a wider range of LLM tasks and healthcare domains.

### Open Question 3
- Question: What are the specific challenges and opportunities for using LLMs to support clinicians in different medical specialties, and how can MedAlign be adapted to address these needs?
- Basis in paper: [explicit] The paper reports on the collection of instructions from clinicians across 7 medical specialties, with varying numbers of instructions submitted by each specialty. It also notes that the clinicians who submitted data represent only a small fraction of the overall clinician workforce.
- Why unresolved: The paper does not provide a detailed analysis of the specific challenges and opportunities for using LLMs in different medical specialties. It also does not explore the potential impact of specialty-specific adaptations of MedAlign on LLM performance and clinician satisfaction.
- What evidence would resolve it: Further experiments collecting instructions from a larger and more diverse pool of clinicians across different medical specialties, and evaluating the performance of LLMs on specialty-specific MedAlign instructions.

## Limitations
- Dataset covers only seven medical specialties, limiting generalizability to other clinical domains
- Small number of clinician reviewers (N=10 for ranking study) may not capture full spectrum of clinical perspectives
- Automated metrics were not specifically trained on clinical instruction-following tasks, potentially missing domain-specific quality aspects

## Confidence

- **High Confidence**: MedAlign dataset construction methodology and its composition (983 instructions, 276 EHRs, 303 gold responses) are well-documented and verifiable.
- **Medium Confidence**: The finding that GPT-4 achieves 35% correct responses while MPT-7B-Instruct achieves 68% incorrect responses is reliable, though absolute performance numbers may vary with different evaluation criteria.
- **Medium Confidence**: The 8.3% drop in accuracy when reducing context from 32k to 2k tokens for GPT-4 is supported by the data, but may not generalize to all instruction types or EHR structures.
- **Medium Confidence**: COMET correlation with human rankings (0.37) is statistically supported but may not hold for different model families or instruction types.
- **Low Confidence**: Claims about the fundamental difficulty of EHR instruction-following for LLMs are suggestive but not definitively proven, as the study did not test custom-trained clinical models or alternative architectures.

## Next Checks

1. **Reproduce with Expanded Specialty Coverage**: Validate whether the 8.3% context length performance drop holds when testing on instructions from additional medical specialties beyond the current seven, using the same experimental protocol.

2. **Cross-Validate Automated Metrics**: Test whether COMET's correlation with human rankings (0.37) remains stable when evaluated on a separate, independently collected set of instruction-response pairs from different clinicians and institutions.

3. **Safety and Harm Analysis**: Conduct a systematic evaluation of whether the most confident but incorrect LLM responses contain potentially harmful clinical recommendations, using both automated safety classifiers and expert clinician review.