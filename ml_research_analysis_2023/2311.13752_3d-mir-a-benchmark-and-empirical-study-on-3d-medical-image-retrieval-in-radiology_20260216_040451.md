---
ver: rpa2
title: '3D-MIR: A Benchmark and Empirical Study on 3D Medical Image Retrieval in Radiology'
arxiv_id: '2311.13752'
source_url: https://arxiv.org/abs/2311.13752
tags:
- lesion
- image
- retrieval
- medical
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first benchmark for 3D Medical Image
  Retrieval (3D-MIR), covering four anatomies imaged via computed tomography. The
  authors explore various search strategies using aggregated 2D slices, 3D volumes,
  and multi-modal embeddings from popular foundation models.
---

# 3D-MIR: A Benchmark and Empirical Study on 3D Medical Image Retrieval in Radiology

## Quick Facts
- arXiv ID: 2311.13752
- Source URL: https://arxiv.org/abs/2311.13752
- Reference count: 40
- Key outcome: First benchmark for 3D Medical Image Retrieval (3D-MIR) covering four anatomies with CT imaging

## Executive Summary
This paper introduces the first comprehensive benchmark for 3D Medical Image Retrieval (3D-MIR), addressing the critical need for efficient search tools in radiology to reduce radiologist workloads. The benchmark covers four anatomies (liver, colon, pancreas, lung) using computed tomography data and explores multiple search strategies including aggregated 2D slices, 3D volumes, and multi-modal embeddings from foundation models. Through quantitative and qualitative assessments, the authors provide insights into the relative strengths and limitations of different retrieval approaches. The benchmark, along with dataset and code, is made publicly available to advance research in this important field.

## Method Summary
The 3D-MIR benchmark uses CT volumes from four anatomies, leveraging the Medical Segmentation Decathlon dataset for lesion segmentation and TotalSegmentator for organ segmentation. The method employs three search strategies: slice-based retrieval using aggregated 2D embeddings, volume-based retrieval using 3D volume embeddings, and multi-modal retrieval combining image and text embeddings from BiomedCLIP and GPT-4. Performance is evaluated using precision@k and average precision metrics based on lesion flag and lesion group matching. The system generates embeddings for both images and captions, indexes them using Faiss, and performs similarity search for retrieval tasks.

## Key Results
- Multi-modal ensemble methods combining caption-based and slice-based approaches achieved the best precision@3 of 84.21% for lesion flag matching
- Volume-based approaches may be more effective for broad categorizations, while slice-based methods capture fine-grained details more effectively
- No single retrieval method excels uniformly across all organ types, with performance varying based on organ characteristics and lesion properties

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multi-modal embeddings from BiomedCLIP outperform single-modal embeddings in retrieval tasks involving both image and text queries.
- **Mechanism**: BiomedCLIP uses contrastive learning to align image and text embeddings in the same latent space, enabling cross-modal similarity search without requiring separate models for each modality.
- **Core assumption**: The shared embedding space captures clinically relevant semantic relationships between medical images and natural language descriptions.
- **Evidence anchors**:
  - [abstract] "We explore a diverse set of search strategies that use aggregated 2D slices, 3D volumes, and multi-modal embeddings from popular multi-modal foundation models as queries."
  - [section] "We investigate different search methods... Second, we juxtapose the performance of different approaches to evaluate the relevance and difficulty of the benchmark."
- **Break Condition**: If BiomedCLIP embeddings do not capture the necessary clinical semantics, cross-modal retrieval performance will degrade, especially for nuanced diagnostic queries.

### Mechanism 2
- **Claim**: 3D context improves lesion detection accuracy compared to 2D slice-based approaches.
- **Mechanism**: Volume-based retrieval aggregates information across all slices, providing spatial context that helps distinguish true lesions from anatomical variations or imaging artifacts.
- **Core assumption**: Lesions have 3D characteristics that are not fully captured by individual 2D slices.
- **Evidence anchors**:
  - [abstract] "We explore a diverse set of search strategies that use aggregated 2D slices, 3D volumes, and multi-modal embeddings"
  - [section] "Our experimental outcomes hint that volume-based approaches may be more effective for broad categorizations, whereas slice-based methods could be a more effective choice to capturing fine-grained details."
- **Break Condition**: If 3D volumes are poorly aligned or have significant motion artifacts, volume-based aggregation may introduce noise that degrades retrieval performance.

### Mechanism 3
- **Claim**: Ensemble methods combining multiple retrieval strategies improve overall performance by leveraging complementary strengths.
- **Mechanism**: By interleaving results from different approaches (e.g., caption-based and slice-based), the ensemble can capture both semantic context and fine-grained spatial details.
- **Core assumption**: Different retrieval methods capture different aspects of relevance, and their combination provides more comprehensive coverage.
- **Evidence anchors**:
  - [abstract] "We explore a diverse set of search strategies that use aggregated 2D slices, 3D volumes, and multi-modal embeddings"
  - [section] "The multi-modal ensemble method, combining the results of the caption-based and the slice-based methods, achieved the best precision@3 of 84.21% for lesion flag matching"
- **Break Condition**: If the different retrieval methods are highly correlated in their failures, ensemble methods may not provide significant improvement and could introduce additional computational overhead.

## Foundational Learning

- **Concept: Contrastive learning in vision-language models**
  - Why needed here: Understanding how BiomedCLIP aligns image and text embeddings is crucial for interpreting retrieval performance and potential improvements.
  - Quick check question: How does contrastive loss encourage the model to bring related image-text pairs closer in embedding space while pushing unrelated pairs apart?

- **Concept: 3D medical image representation and processing**
  - Why needed here: The benchmark relies on understanding how 3D volumes differ from 2D images in terms of spatial relationships, lesion morphology, and computational considerations.
  - Quick check question: What are the key differences between processing 2D slices versus full 3D volumes in terms of computational complexity and information retention?

- **Concept: Medical image segmentation and annotation**
  - Why needed here: The benchmark uses pre-segmented organs and lesions, so understanding segmentation techniques and their limitations is important for interpreting results.
  - Quick check question: How might segmentation errors propagate through the retrieval pipeline and affect the quality of indexed data?

## Architecture Onboarding

- **Component map**: Data preprocessing (CT volume normalization and organ segmentation) -> Embedding generation (BiomedCLIP for images, GPT-4 for captions) -> Indexing (Faiss for vector storage) -> Retrieval (query processing and similarity search) -> Evaluation (precision and average precision metrics)

- **Critical path**: For a given query, the system must generate embeddings, search the index, aggregate results, and rank them - the embedding generation and similarity search steps are typically the most computationally intensive.

- **Design tradeoffs**: The choice between slice-based, volume-based, and multi-modal approaches involves tradeoffs between computational efficiency, retrieval accuracy, and the ability to capture different types of medical relevance.

- **Failure signatures**: Poor retrieval performance may indicate issues with embedding quality, indexing errors, or misalignment between query intent and retrieval strategy.

- **First 3 experiments**:
  1. Test retrieval performance on a small subset of data using all three approaches to establish baseline metrics.
  2. Evaluate the impact of different aggregation methods (frequency, max score, sum) on slice-based retrieval.
  3. Compare the performance of different volume embedding aggregation methods (median, max pooling, average pooling) on the same dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does lesion size relative to organ size impact the effectiveness of different retrieval methods?
- Basis in paper: [inferred] from the discussion stating that lesion group is often correlated to size relative to organ and that slice-based methods perform better for fine-grained details
- Why unresolved: The paper only speculates that slice-based methods work better for lesion group identification due to capturing cumulative information about lesion size relative to organ, but does not provide conclusive evidence
- What evidence would resolve it: Controlled experiments varying lesion-to-organ size ratios and measuring retrieval accuracy for each method

### Open Question 2
- Question: Can multi-modal foundation models be adapted to handle 3D medical image inputs directly rather than requiring 2D slice aggregation?
- Basis in paper: [explicit] from the conclusion stating that current foundation models are designed for 2D inputs and that the research investigated approaches to combine 2D information into 3D context
- Why unresolved: The paper only explores slice-based and volume-based approaches for combining 2D information, but does not investigate direct 3D model adaptation
- What evidence would resolve it: Development and evaluation of a 3D-capable multi-modal foundation model for medical image retrieval

### Open Question 3
- Question: How do different organ characteristics (shape, texture, lesion patterns) influence the effectiveness of various retrieval methods?
- Basis in paper: [explicit] from the discussion noting that variability between different organs significantly influences encoding effectiveness and that no single method excels uniformly across all organ types
- Why unresolved: The paper observes differences in method performance across organs but does not systematically analyze which organ characteristics drive these differences
- What evidence would resolve it: Comprehensive analysis correlating organ-specific features with retrieval method performance across a diverse organ set

## Limitations

- The benchmark relies on the Medical Segmentation Decathlon dataset, which may not fully represent real-world clinical variability
- Performance heavily depends on BiomedCLIP's ability to capture clinically relevant semantic relationships
- The benchmark assumes high-quality organ and lesion segmentation, with errors potentially propagating through the retrieval pipeline

## Confidence

- **High Confidence**: The claim that ensemble methods combining multiple retrieval strategies improve overall performance (Precision@3 of 84.21% for lesion flag matching)
- **Medium Confidence**: The mechanism that 3D context improves lesion detection accuracy
- **Low Confidence**: The assertion that BiomedCLIP's cross-modal embeddings will generalize well to all clinical query types

## Next Checks

1. **Cross-dataset validation**: Test the benchmark's retrieval methods on an independent CT dataset with different acquisition protocols and patient populations to assess generalizability.

2. **Clinical expert evaluation**: Have radiologists assess retrieval quality on a subset of queries to validate that high precision scores correspond to clinically meaningful matches.

3. **Failure mode analysis**: Systematically identify query types where each retrieval method fails to determine whether ensemble approaches actually mitigate individual method weaknesses or simply combine correlated failures.