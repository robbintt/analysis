---
ver: rpa2
title: 'CAPro: Webly Supervised Learning with Cross-Modality Aligned Prototypes'
arxiv_id: '2310.09761'
source_url: https://arxiv.org/abs/2310.09761
tags:
- learning
- noise
- capro
- visual
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CAPro addresses the problem of webly supervised learning with label
  noise by leveraging cross-modality aligned prototypes and collective bootstrapping.
  The method uses textual prototypes derived from class definitions to select clean
  images and enhance noisy texts using visual guidance.
---

# CAPro: Webly Supervised Learning with Cross-Modality Aligned Prototypes

## Quick Facts
- **arXiv ID:** 2310.09761
- **Source URL:** https://arxiv.org/abs/2310.09761
- **Reference count:** 40
- **Primary result:** State-of-the-art performance on WebVision1k and NUS-WIDE for webly supervised learning with label noise

## Executive Summary
CAPro addresses the challenge of webly supervised learning with label noise by leveraging cross-modality aligned prototypes and collective bootstrapping. The method uses textual prototypes derived from class definitions to select clean images and enhance noisy texts using visual guidance. Visual prototypes are then refined with high-quality samples and used for contrastive learning and noise removal. CAPro achieves state-of-the-art performance on WebVision1k and NUS-WIDE, demonstrating robustness to realistic noise under single-label and multi-label scenarios.

## Method Summary
CAPro combines cross-modality aligned prototypes with collective bootstrapping to learn robust representations from webly supervised data. The method first initializes visual prototypes by matching textual metadata to WordNet-derived textual prototypes. It then refines these prototypes through contrastive learning and uses collective bootstrapping for smoother label regularization. The approach includes text enhancement via visual guidance and noise removal through similarity-based filtering. CAPro is trained in two stages: pre-training on web labels and fine-tuning on cleaned data.

## Key Results
- Achieves state-of-the-art performance on WebVision1k and NUS-WIDE benchmarks
- Demonstrates robustness to various noise types including semantic noise and missing metadata
- Shows consistent improvement across both single-label and multi-label scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modality alignment using textual prototypes disambiguates semantic noise in web images
- Mechanism: Textual prototypes are extracted from class definitions in WordNet and used to match and select clean images via text matching. This disambiguates ambiguous queries (e.g., "tiger cat" vs. "tiger-cat") by providing explicit semantic context.
- Core assumption: Semantic noise arises primarily from polysemy and can be resolved by aligning visual instances with precise textual definitions.
- Evidence anchors:
  - [abstract]: "we leverage textual prototypes, which stem from the distinct concept definition of classes, to select clean images by text matching"
  - [section]: "Since visual prototypes simply formed with images suffer from semantic ambiguity, we propose text matching to leverage textual prototypes"
  - [corpus]: No direct corpus evidence found for this specific mechanism; assumption based on methodological description.
- Break condition: If class definitions are missing, vague, or do not cover all polysemous interpretations, the alignment will fail to disambiguate correctly.

### Mechanism 2
- Claim: Visual-guided text enhancement improves robustness to noisy and incomplete metadata
- Mechanism: A k-reciprocal nearest neighbor graph is built in the visual feature space. Text embeddings are smoothed via graph convolution, using visual neighbors to correct and enhance noisy or missing textual metadata before matching to textual prototypes.
- Core assumption: Images with similar visual features tend to have semantically similar textual descriptions, enabling mutual enhancement.
- Evidence anchors:
  - [section]: "we assume that similar images should share similar texts, and consider text enhancement with guidance from visual data structure"
  - [section]: "Given G, smoothing is performed on S = (s1, s2, ..., sN) ∈ IRN×dt via graph convolution"
  - [corpus]: No direct corpus evidence for this specific enhancement method; based on described methodology.
- Break condition: If the visual feature space is too noisy or the nearest neighbor graph includes irrelevant samples, text enhancement may propagate errors rather than correct them.

### Mechanism 3
- Claim: Collective bootstrapping via dictionary lookup provides smoother and more reliable pseudo-labels than standard bootstrapping
- Mechanism: Instead of bootstrapping per instance, CAPro maintains a large dictionary queue of past embeddings. For each query, it computes weighted combinations of predictions from all dictionary keys based on visual similarity, providing a smoother label reference.
- Core assumption: Visual dictionary entries from similar instances provide more reliable and stable pseudo-labels than isolated instance predictions, reducing overfitting to noise.
- Evidence anchors:
  - [section]: "we propose collective bootstrapping to encourage smoother and wiser label reference from appearance-similar instances in a manner of dictionary look-up"
  - [section]: "The dictionary keys are the learned representations from the sampled data while the query is the current encoded sample"
  - [corpus]: No direct corpus evidence for this specific collective bootstrapping mechanism; based on described methodology.
- Break condition: If the dictionary becomes dominated by noisy samples or the similarity weighting is poorly calibrated, collective bootstrapping may degrade rather than improve label quality.

## Foundational Learning

- Concept: Prototypical learning and contrastive learning
  - Why needed here: CAPro uses class prototypes to define cluster centers and minimize intra-class distances via contrastive loss, which is central to its noise-robust representation learning.
  - Quick check question: How does prototypical contrastive loss differ from standard instance-wise contrastive loss in handling label noise?

- Concept: Cross-modal embedding alignment
  - Why needed here: Aligning visual and textual embeddings ensures that the visual prototypes correspond to correct semantic concepts, mitigating semantic misalignment noise.
  - Quick check question: Why is text matching to textual prototypes more reliable than averaging visual instances for prototype formation?

- Concept: Graph-based neighborhood smoothing
  - Why needed here: Graph convolution on visual k-reciprocal-NN graphs smooths and enhances noisy textual metadata, improving text matching accuracy.
  - Quick check question: What advantage does k-reciprocal-NN offer over simple k-NN for text enhancement?

## Architecture Onboarding

- Component map:
  Image encoder (ResNet-50) -> visual features vi
  Text encoder (MiniLM) -> textual embeddings si, sc
  Projector -> low-dimensional embeddings zi for contrastive learning
  Reconstructor -> reconstructs vi from zi (denoising autoencoder)
  Auxiliary classifier -> predictions qi for collective bootstrapping
  Dictionary queue -> stores embeddings for contrastive learning and collective bootstrapping

- Critical path:
  1. Pre-training: Train image encoder, classifier, projector, reconstructor on web labels only
  2. Cross-modality alignment: Build visual k-reciprocal-NN graph -> enhance texts -> match to textual prototypes -> initialize visual prototypes
  3. Main training: Instance-prototype contrastive learning + collective bootstrapping + noise removal -> update prototypes
  4. Fine-tuning: Clean labels -> fine-tune classifier

- Design tradeoffs:
  - Using textual prototypes from WordNet avoids noisy metadata averaging but requires accurate class definitions.
  - Text enhancement via visual neighbors improves robustness but adds computational overhead for graph construction.
  - Collective bootstrapping smooths labels but may oversmooth if dictionary size or weighting is not tuned.

- Failure signatures:
  - Poor top-K semantic alignment -> prototypes initialized with noisy samples -> degraded performance
  - Over-regularization from collective bootstrapping -> loss of discriminative power
  - Text enhancement fails -> noisy text matching -> incorrect prototype initialization

- First 3 experiments:
  1. Validate text matching accuracy: measure precision of top-K matched images per class before/after text enhancement
  2. Ablate prototype update frequency: compare performance with 0, 1, 5, 10 epoch intervals
  3. Test collective bootstrapping weight λbts: sweep λbts ∈ {0.01, 0.1, 0.5, 1.0} and measure overfitting and accuracy trade-offs

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: How does CAPro perform on datasets with extremely high noise ratios, such as 50% or more?
  - Basis in paper: [inferred] The paper mentions that CAPro is robust to various noise types but does not provide extensive evaluation on extremely noisy datasets.
  - Why unresolved: The paper focuses on WebVision1k and NUS-WIDE, which have noise ratios around 34% and 50% respectively, but does not explore datasets with even higher noise ratios.
  - What evidence would resolve it: Evaluating CAPro on datasets with noise ratios exceeding 50% and comparing its performance with state-of-the-art methods.

- **Open Question 2**
  - Question: Can CAPro be effectively applied to multi-modal datasets beyond images and text, such as audio or video?
  - Basis in paper: [explicit] The paper focuses on image and text modalities but mentions potential extension to other modalities like audio and video.
  - Why unresolved: The paper does not provide any experiments or analysis on multi-modal datasets beyond images and text.
  - What evidence would resolve it: Applying CAPro to multi-modal datasets including audio or video and evaluating its performance on these tasks.

- **Open Question 3**
  - Question: How does CAPro handle the trade-off between maintaining intra-class diversity and ensuring class purity in the visual prototypes?
  - Basis in paper: [inferred] The paper discusses the trade-off between domain variety and class purity in prototype initialization but does not provide a detailed analysis of how this affects the overall performance.
  - Why unresolved: The paper does not provide a comprehensive analysis of the impact of this trade-off on the performance of CAPro.
  - What evidence would resolve it: Conducting experiments with different settings of the trade-off and analyzing their impact on the performance of CAPro.

- **Open Question 4**
  - Question: What is the impact of using different language models (e.g., GPT-Neo, XLNet) on the performance of CAPro?
  - Basis in paper: [explicit] The paper compares the performance of CAPro using different language models like MiniLM, XLNet, and GPT-Neo, but does not provide a detailed analysis of their impact.
  - Why unresolved: The paper does not provide a comprehensive analysis of how different language models affect the performance of CAPro.
  - What evidence would resolve it: Conducting experiments with different language models and analyzing their impact on the performance of CAPro.

## Limitations

- Reliance on accurate WordNet class definitions; performance degrades if definitions are missing or ambiguous
- Assumption that visual neighbors have semantically similar textual metadata may not hold in highly heterogeneous web data
- Collective bootstrapping introduces additional hyperparameters that require careful tuning

## Confidence

- **High Confidence**: CAPro's state-of-the-art performance on WebVision1k and NUS-WIDE benchmarks
- **Medium Confidence**: The general effectiveness of cross-modality aligned prototypes and text enhancement mechanisms
- **Medium Confidence**: The collective bootstrapping approach for smoother label reference, though sensitive to hyperparameter choices

## Next Checks

1. Validate text matching accuracy: Measure precision of top-K matched images per class before and after text enhancement, using ground truth clean samples if available
2. Ablate prototype update frequency: Compare model performance with visual prototypes updated at intervals of 0, 1, 5, and 10 epochs to assess stability and overfitting
3. Test collective bootstrapping weight sensitivity: Sweep λbts ∈ {0.01, 0.1, 0.5, 1.0} and monitor both accuracy and overfitting behavior