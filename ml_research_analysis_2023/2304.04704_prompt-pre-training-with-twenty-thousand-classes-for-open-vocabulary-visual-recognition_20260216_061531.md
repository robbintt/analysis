---
ver: rpa2
title: Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual
  Recognition
arxiv_id: '2304.04704'
source_url: https://arxiv.org/abs/2304.04704
tags:
- prompt
- pomp
- class
- classes
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces POMP, a prompt pre-training method for vision-language
  models that learns a universal soft prompt on a large-scale dataset with over twenty-thousand
  classes. POMP addresses the memory and computation challenges of prompt tuning by
  introducing local contrast and local correction strategies, reducing GPU memory
  usage from over 300 GB to less than 16 GB.
---

# Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition

## Quick Facts
- arXiv ID: 2304.04704
- Source URL: https://arxiv.org/abs/2304.04704
- Reference count: 23
- State-of-the-art performance on 21 datasets, achieving 67.0% average accuracy on 10 classification datasets (+3.1% over CoOp)

## Executive Summary
This paper introduces POMP, a prompt pre-training method that addresses the memory and computation challenges of traditional prompt tuning for vision-language models. By learning a universal soft prompt on ImageNet-21K's 21k classes using local contrast and local correction strategies, POMP reduces GPU memory usage from over 300GB to less than 16GB while maintaining strong performance. The pre-trained prompt can be directly applied to various visual recognition tasks including image classification, semantic segmentation, and object detection in a zero-shot manner, achieving state-of-the-art results across 21 datasets.

## Method Summary
POMP pre-trains a soft prompt on ImageNet-21K by sampling a subset of K classes (typically 1000) per training step rather than using all 21k classes, dramatically reducing memory requirements. The method employs local contrast learning where the model only needs to identify the ground-truth class from the sampled subset, combined with local correction terms that add margins between positive and negative logits. The pre-trained prompt, initialized with Gaussian noise (std=0.02), is then directly applied to downstream tasks without fine-tuning, demonstrating strong zero-shot transfer capabilities across classification, segmentation, and detection tasks.

## Key Results
- Achieves 67.0% average accuracy on 10 classification datasets (+3.1% over CoOp)
- 84.4 hIoU on open-vocabulary Pascal VOC segmentation (+6.9 over ZSSeg)
- Reduces GPU memory usage from over 300GB to less than 16GB
- State-of-the-art performance across 21 datasets including classification, segmentation, and detection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** POMP reduces GPU memory usage by sampling a subset of classes during training rather than using all classes
- **Mechanism:** Samples K classes (K=1000) per training step instead of all N classes (N>21k), reducing memory from over 300GB to less than 16GB
- **Core assumption:** The model can still learn meaningful representations by contrasting against a subset of classes rather than the full set
- **Evidence anchors:** [abstract] states memory reduction from over 300GB to less than 16GB; [section] describes narrowing contrastive learning scope from global to local
- **Break condition:** If sampled subset is too small or unrepresentative, learned prompt may fail to capture full semantic space

### Mechanism 2
- **Claim:** Local correction improves generalization by adding margins between positive and negative logits
- **Mechanism:** Adds correction terms to negative logits that encourage separation even for unsampled negative classes
- **Core assumption:** Maintaining margins between positive and negative classes during training leads to better separation in learned representation space
- **Evidence anchors:** [abstract] mentions local contrast and local correction strategies; [section] describes adding local correction term as margin
- **Break condition:** If correction term is too large, may prevent learning fine-grained distinctions between similar classes

### Mechanism 3
- **Claim:** Pre-training on ImageNet-21K with 21k classes creates more universal prompt than fine-tuning on specific datasets
- **Mechanism:** Prompt learns to represent broad semantic space by exposure to diverse visual concepts during pre-training
- **Core assumption:** Prompt trained on large, diverse dataset generalizes better to unseen classes and tasks than one fine-tuned on specific dataset
- **Evidence anchors:** [abstract] notes prompt condenses semantic information for rich set of visual concepts; [section] states prompt can be directly plugged into various tasks
- **Break condition:** If dataset is too diverse without sufficient depth in any domain, prompt may become too generic to capture specific visual concepts

## Foundational Learning

- **Concept:** Vision-Language Model Architecture (CLIP)
  - **Why needed here:** Understanding how CLIP's image-text matching works is crucial for grasping how prompts function as class descriptions
  - **Quick check question:** What are the two main components of CLIP and how do they interact during inference?

- **Concept:** Contrastive Learning
  - **Why needed here:** Method relies on contrastive loss to align image features with text features, fundamental to understanding how POMP learns
  - **Quick check question:** How does the contrastive loss in CLIP differ from traditional classification losses?

- **Concept:** Prompt Engineering vs Prompt Tuning
  - **Why needed here:** Work builds on prompt tuning concepts but extends them to pre-training, requiring understanding of both approaches
  - **Quick check question:** What is the key difference between hard prompts and soft prompts in vision-language models?

## Architecture Onboarding

- **Component map:** Image encoder (ViT backbone) → Soft prompt (learnable tokens) → Text encoder (frozen) → Contrastive loss → Prompt updates
- **Critical path:** Input image → Image encoder → Feature extraction → Similarity computation with class features → Cross-entropy loss → Backpropagation to soft prompt
- **Design tradeoffs:** Memory efficiency vs. representation quality (sampling K classes reduces memory but may lose information), generalization vs. specificity (large pre-training dataset improves generalization but may reduce task-specific performance)
- **Failure signatures:** Poor performance on novel classes indicates insufficient coverage during pre-training; high memory usage suggests sampling strategy needs adjustment; overfitting to training classes indicates need for stronger regularization
- **First 3 experiments:**
  1. Verify memory reduction by comparing GPU usage with and without class sampling
  2. Test generalization by evaluating prompt performance on held-out classes
  3. Validate local correction effectiveness by comparing with and without correction terms

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does POMP's performance scale when pre-training with datasets containing significantly more classes than ImageNet-21K, such as JFT-300M or larger web-scale datasets?
- **Basis in paper:** [explicit] Mentions ImageNet-21K contains over twenty-thousand classes but does not explore scaling to datasets with hundreds of thousands of classes
- **Why unresolved:** Paper only evaluates POMP on ImageNet-21K and does not conduct experiments with larger datasets
- **What evidence would resolve it:** Experiments comparing POMP's performance when pre-trained on ImageNet-21K versus larger datasets like JFT-300M or datasets with 100K+ classes

### Open Question 2
- **Question:** What is the optimal strategy for determining the number of negative classes (K) to sample in the local contrast approach, and how does this optimal value vary across different downstream tasks and dataset characteristics?
- **Basis in paper:** [explicit] Investigates varying K values (100, 500, 1000) and shows performance improves with larger K, but does not provide guidance on how to select K for specific scenarios
- **Why unresolved:** Paper only provides empirical results for a few fixed K values without developing a principled method for selecting K
- **What evidence would resolve it:** Systematic study varying K across different task types and dataset characteristics

### Open Question 3
- **Question:** How does POMP pre-trained prompt compare to task-specific prompt tuning methods when both have access to source data for fine-tuning, and what is the computational trade-off between using a pre-trained prompt versus training from scratch?
- **Basis in paper:** [explicit] Notes traditional prompt tuning methods are "trained on the ImageNet-1K dataset due to prohibitive computational cost if trained on ImageNet-21K"
- **Why unresolved:** Paper focuses on zero-shot transfer capabilities but does not address whether POMP provides advantages when fine-tuning is allowed on source data
- **What evidence would resolve it:** Direct comparison experiments where POMP prompts are fine-tuned on source data versus task-specific prompts trained from scratch

## Limitations

- The paper does not explore scaling to datasets with significantly more classes than ImageNet-21K, leaving open questions about performance at web-scale
- Optimal strategy for determining the number of negative classes (K) to sample is not provided, with only empirical results for fixed values
- Does not compare pre-trained POMP prompts with task-specific fine-tuning when source data is available, limiting understanding of computational trade-offs

## Confidence

- **Low confidence** on exact implementation of local contrast sampling and local correction mechanisms
- **Medium confidence** on memory efficiency claims
- **Medium confidence** on cross-task generalization claims

## Next Checks

1. **Memory Efficiency Verification**: Reproduce memory usage measurements by implementing class sampling strategy with varying K values (100, 500, 1000, 2000) and measuring actual GPU memory consumption across different batch sizes and prompt lengths

2. **Local Correction Ablation**: Implement local correction mechanism with systematic ablation by removing correction term entirely, using different margin values, and testing against alternative margin-based contrastive learning approaches

3. **Generalization Stress Test**: Evaluate POMP on out-of-distribution datasets including medical imaging (CheXpert), satellite imagery (RESISC45), and fine-grained classification (iNaturalist) to test limits of zero-shot generalization