---
ver: rpa2
title: 'Free Bits: Latency Optimization of Mixed-Precision Quantized Neural Networks
  on the Edge'
arxiv_id: '2307.02894'
source_url: https://arxiv.org/abs/2307.02894
tags:
- latency
- bits
- configurations
- accuracy
- precision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of optimizing latency in mixed-precision
  quantized neural networks for deployment on edge devices, specifically microcontroller
  units (MCUs). The authors propose a hybrid search methodology that combines a hardware-agnostic
  differentiable search algorithm with a hardware-aware heuristic optimization to
  find latency-optimized mixed-precision configurations for a specific hardware target.
---

# Free Bits: Latency Optimization of Mixed-Precision Quantized Neural Networks on the Edge

## Quick Facts
- arXiv ID: 2307.02894
- Source URL: https://arxiv.org/abs/2307.02894
- Reference count: 19
- Primary result: Hybrid search method achieves up to 28.6% latency reduction on MCUs while maintaining full-precision accuracy

## Executive Summary
This paper addresses the challenge of optimizing latency in mixed-precision quantized neural networks for deployment on edge microcontroller units (MCUs). The authors propose a hybrid search methodology that combines hardware-agnostic differentiable search (Bayesian Bits) with hardware-aware heuristic optimization (Free Bits) to find latency-optimized mixed-precision configurations for specific hardware targets. The method is evaluated on MobileNetV1 and MobileNetV2 architectures deployed on a family of multi-core RISC-V microcontroller platforms.

The results demonstrate significant latency improvements compared to 8-bit models, achieving up to 28.6% end-to-end latency reduction with negligible accuracy drop from full-precision baselines on ImageNet. The authors also show speedups relative to 8-bit baselines even on systems without hardware support for sub-byte arithmetic, highlighting the method's effectiveness across different hardware configurations.

## Method Summary
The proposed method uses a two-stage approach: first applying Bayesian Bits for hardware-agnostic mixed-precision search to find an initial configuration, then using the Free Bits heuristic to optimize this configuration based on hardware profiling data. The Free Bits heuristic iteratively increases the precision of layers where higher precision yields lower latency, leveraging the monotonic relationship between precision and accuracy. The approach uses QuantLab for quantization-aware fine-tuning and DORY for deployment on RISC-V MCUs, with GVSOC providing cycle-accurate simulation.

## Key Results
- Achieves up to 28.6% reduction in end-to-end latency compared to 8-bit models
- Maintains negligible accuracy drop from full-precision baselines on ImageNet
- Demonstrates speedups relative to 8-bit baselines even on systems without sub-byte arithmetic support
- Shows superior accuracy-latency trade-off curve compared to differentiable search alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mixed-precision quantization can reduce latency even when hardware lacks sub-byte arithmetic support
- Mechanism: Lower precision enables larger tiling, reducing data movement overhead and improving memory bandwidth utilization
- Core assumption: Hardware executes DNNs layer-by-layer with tiling size limited by largest operand precision
- Evidence anchors: Abstract statement about speedups on systems without sub-byte arithmetic support; discussion of tiling in hierarchical memory systems
- Break condition: If tiling overhead outweighs gains from larger tiles

### Mechanism 2
- Claim: Increasing precision of certain layers can reduce latency on specific hardware targets
- Mechanism: Hardware implementations of low-precision operations may have higher per-operation latency or require operand unpacking
- Core assumption: Latency depends jointly on input and weight precisions in a non-monotonic way on target hardware
- Evidence anchors: Abstract observation about layers with higher latency at lower precisions; discussion of XpulpNN's hardware implementation
- Break condition: If low-precision operations are as fast or faster than higher-precision ones

### Mechanism 3
- Claim: Free Bits heuristic finds latency-optimized mixed-precision configurations
- Mechanism: Starts with Bayesian Bits baseline, iteratively increases precision where higher precision yields lower latency
- Core assumption: Total network latency approximates sum of individual layer latencies; updates improve both latency and accuracy
- Evidence anchors: Description of heuristic operation and claim of producing superior configurations
- Break condition: If profiling data is inaccurate or layer dependencies invalidate latency sum approximation

## Foundational Learning

- Concept: Mixed-precision quantization
  - Why needed here: Core contribution involves finding optimal mixed-precision configurations
  - Quick check question: What is the main advantage of mixed-precision quantization over homogeneous quantization?

- Concept: Hardware-aware quantization
  - Why needed here: Free Bits heuristic leverages profiling data from target hardware
  - Quick check question: How does the hardware's implementation of low-precision operations affect optimal mixed-precision configuration?

- Concept: Differentiable Neural Architecture Search (DNAS)
  - Why needed here: Paper uses Bayesian Bits (DNAS variant) for initial configuration
  - Quick check question: What is the main difference between hardware-agnostic and hardware-aware DNAS?

## Architecture Onboarding

- Component map: Bayesian Bits -> Free Bits heuristic -> QuantLab fine-tuning -> DORY deployment -> GVSOC simulation
- Critical path: Hardware-agnostic search followed by hardware-aware optimization and deployment pipeline
- Design tradeoffs:
  - Accuracy vs. latency: Finding configurations that minimize latency while maintaining full-precision accuracy
  - Hardware-agnostic vs. hardware-aware: Two-stage approach combining both strategies
  - Profiling overhead: Upfront cost of characterizing all supported precision configurations
- Failure signatures:
  - Inaccurate profiling data leading to suboptimal configurations
  - Hardware implementation quirks causing precision increases to not improve latency
  - Accuracy degradation during fine-tuning
- First 3 experiments:
  1. Profile simple network (single convolutional layer) on target hardware for all supported precisions to verify profiling mechanism
  2. Apply Free Bits heuristic to network with known optimal configuration to verify correctness
  3. Deploy mixed-precision network on target hardware and measure actual latency and accuracy

## Open Questions the Paper Calls Out

The paper does not explicitly call out any open questions.

## Limitations

- Dependence on accurate hardware profiling data for each unique layer type
- Assumption that increasing precision never decreases accuracy may not hold in all edge cases
- Evaluation limited to specific MobileNet architectures and RISC-V platforms, limiting generalizability

## Confidence

High Confidence: Mixed-precision quantization reduces latency through improved tiling and memory bandwidth utilization

Medium Confidence: Certain layers may exhibit higher latency when quantized to lower precisions due to hardware implementation details

Medium Confidence: Free Bits heuristic effectively finds latency-optimized configurations based on precision-accuracy monotonicity

## Next Checks

1. Profile a single-layer network on target hardware for all supported precisions and verify latency dictionary accuracy

2. Implement Free Bits heuristic and apply to network with known optimal configuration from literature

3. Deploy mixed-precision configuration found by method on actual target hardware and measure real-world latency and accuracy