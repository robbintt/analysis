---
ver: rpa2
title: 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific
  Research'
arxiv_id: '2308.13149'
source_url: https://arxiv.org/abs/2308.13149
tags:
- data
- scientific
- llms
- knowledge
- static
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SciEval is a comprehensive benchmark for evaluating Large Language
  Models (LLMs) on scientific research tasks. It addresses the limitations of existing
  benchmarks by covering multiple scientific disciplines and incorporating both objective
  and subjective questions.
---

# SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research

## Quick Facts
- arXiv ID: 2308.13149
- Source URL: https://arxiv.org/abs/2308.13149
- Reference count: 5
- Primary result: GPT-4 achieves state-of-the-art performance on SciEval but shows substantial room for improvement, especially on dynamic questions

## Executive Summary
SciEval is a comprehensive benchmark designed to evaluate Large Language Models (LLMs) on scientific research tasks across multiple disciplines. It addresses limitations of existing benchmarks by covering chemistry, physics, and biology with both objective and subjective questions. The benchmark uses Bloom's taxonomy to systematically assess four dimensions of scientific capability: basic knowledge, knowledge application, scientific calculation, and research ability. A key innovation is the inclusion of dynamically generated questions based on scientific principles to prevent data leakage issues during evaluation.

## Method Summary
SciEval contains approximately 18,000 scientific questions across chemistry, physics, and biology disciplines. The benchmark employs dynamic data generation using formulas and molecular properties from PubChem rather than selecting from existing datasets, preventing models from memorizing answers. Questions are organized into four knowledge dimensions aligned with Bloom's taxonomy and include both objective questions (multiple choice, fill-in-blank, judgment) and subjective questions (experimental design and analysis). The evaluation uses three prompting settings: Answer-Only, Chain-of-Thought, and 3-shot, with accuracy metrics for objective questions and BLEU/MSE scores for dynamic chemistry questions.

## Key Results
- GPT-4 achieves state-of-the-art performance but shows substantial room for improvement, especially on dynamic questions
- Most LLMs perform poorly on calculation problems, particularly in the physics domain
- Objective questions are insufficient to holistically assess scientific capability, necessitating subjective questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic data generation based on scientific principles prevents data leakage from pretraining corpora.
- Mechanism: By generating new test instances using formulas and molecular properties rather than selecting from existing datasets, the benchmark ensures that even models trained on large web corpora cannot memorize answers.
- Core assumption: The space of possible scientific questions generated from principles is sufficiently large that pretraining data will not cover them.
- Evidence anchors:
  - [abstract]: "we design a 'dynamic' subset based on scientific principles to prevent evaluation from potential data leakage."
  - [section]: "For chemistry data, we use the basic information and properties of molecules crawled from PubChem to create data. For physics data, we manually write some Python scripts according to the physics formulas."
  - [corpus]: Found 25 related papers; FMR scores suggest moderate topical relatedness but no direct replication of this exact dynamic generation approach.
- Break condition: If pretraining corpora contain extensive scientific problem generation scripts or formula databases, models may still memorize patterns.

### Mechanism 2
- Claim: Bloom's taxonomy alignment provides a structured multi-level evaluation of scientific capabilities.
- Mechanism: The four knowledge dimensions (Basic Knowledge, Knowledge Application, Scientific Calculation, Research Ability) map to Bloom's cognitive levels, ensuring assessment spans from recall to creation.
- Core assumption: The cognitive levels in Bloom's taxonomy are appropriate and comprehensive for evaluating scientific LLM capabilities.
- Evidence anchors:
  - [abstract]: "Based on Bloom's taxonomy, SciEval covers four dimensions to systematically evaluate scientific research ability."
  - [section]: "SciEval evaluates the scientific capabilities of large language models across four dimensions: basic knowledge, knowledge application, scientific calculation, and research ability, where each capability aligns with one or more cognitive levels."
  - [corpus]: Found "SciKnowEval: Evaluating Multi-level Scientific Knowledge of Large Language Models" which also uses Bloom's taxonomy, suggesting this is a recognized approach.
- Break condition: If certain scientific capabilities don't map well to Bloom's levels or if the taxonomy doesn't capture modern scientific reasoning needs.

### Mechanism 3
- Claim: Combining objective and subjective question types provides a more holistic evaluation than objective-only benchmarks.
- Mechanism: Objective questions enable quick standardized evaluation, while subjective questions assess higher-order reasoning and application skills.
- Core assumption: Subjective questions can be reliably evaluated and provide additional value beyond what objective questions measure.
- Evidence anchors:
  - [abstract]: "SciEval is mainly based on objective questions, which allow for quick and standard model evaluations... To better assess scientific reasoning and application ability, SciEval introduces a small number of subjective questions."
  - [section]: "These questions can help us understand whether the model can correctly understand and memorize scientific knowledge. However, objective questions are insufficient to assess scientific capability holistically."
  - [corpus]: Found "SciQAG: A Framework for Auto-Generated Science Question Answering Dataset with Fine-grained Evaluation" which also includes subjective evaluation, suggesting this is a recognized need.
- Break condition: If subjective evaluation proves too subjective or unreliable, or if the overhead outweighs the benefits.

## Foundational Learning

- Concept: Bloom's taxonomy cognitive levels (Remember, Understand, Apply, Analyze, Evaluate, Create)
  - Why needed here: Provides the theoretical framework for organizing the four evaluation dimensions and ensuring comprehensive coverage of scientific capabilities.
  - Quick check question: Which Bloom's level corresponds to "applying knowledge to solve scientific problems"?

- Concept: Data leakage in LLM evaluation
  - Why needed here: Understanding why pretraining data contamination is a problem and how dynamic generation addresses it.
  - Quick check question: Why would using questions from educational materials risk data leakage?

- Concept: Types of scientific questions (objective vs. subjective)
  - Why needed here: To understand the rationale for including both question types and their respective evaluation methods.
  - Quick check question: What is the main advantage of subjective questions over objective ones in this benchmark?

## Architecture Onboarding

- Component map:
  Data Collection Pipeline: Crawl → Filter → GPT-4 processing → Classification → Quality check
  Dynamic Data Generator: Chemistry (PubChem-based) | Physics (formula-based Python scripts)
  Evaluation Framework: Answer-Only, Chain-of-Thought, 3-Shot settings
  Scoring System: Accuracy (objective), BLEU/MSE (dynamic chemistry), Manual evaluation (experimental data)

- Critical path: Data collection → Dynamic data generation → Question formatting → Model evaluation → Scoring

- Design tradeoffs: Static vs. dynamic data (stability vs. leakage prevention), objective vs. subjective questions (speed vs. depth), automated vs. manual evaluation (scalability vs. nuance)

- Failure signatures: Low performance across all models indicates benchmark difficulty; poor dynamic data performance suggests limited scientific reasoning; experimental data struggles reveal limitations in complex reasoning

- First 3 experiments:
  1. Run GPT-4 on static data subset to establish baseline performance
  2. Generate and evaluate one dynamic chemistry question to verify generation pipeline
  3. Test one experimental data question with manual evaluation to validate subjective assessment method

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal frequency for updating the Dynamic Data subset to balance fairness in evaluation with practical maintenance costs?
- Basis in paper: [explicit] The paper mentions that Dynamic Data will be updated regularly to prevent data leakage, but doesn't specify an optimal frequency.
- Why unresolved: The paper states that Dynamic Data will be updated regularly but doesn't provide guidance on how often this should occur. The optimal frequency would need to balance preventing data leakage with the practical costs of regular updates and maintaining a stable version for fair comparison.
- What evidence would resolve it: Experimental results comparing model performance across different update frequencies (e.g., monthly vs. quarterly vs. annually) would help determine the optimal balance between preventing data leakage and maintaining evaluation consistency.

### Open Question 2
- Question: How do the performance differences between LLMs on Dynamic Data correlate with their training data composition, particularly regarding scientific content?
- Basis in paper: [inferred] The paper notes that Galactica, trained on extensive scientific corpus, outperforms other LLMs on Dynamic Data, suggesting a potential correlation between training data composition and performance.
- Why unresolved: While the paper shows Galactica's superior performance, it doesn't provide a detailed analysis of how different training data compositions (particularly scientific content) correlate with performance on Dynamic Data. A more comprehensive study examining various LLMs with known training data compositions would be needed.
- What evidence would resolve it: A systematic analysis comparing LLMs with varying proportions of scientific training data, examining their performance on Dynamic Data subsets across different scientific disciplines, would reveal correlations between training data composition and scientific reasoning capabilities.

### Open Question 3
- Question: What specific aspects of scientific calculation problems cause the most difficulty for LLMs, and how can this inform future model architectures?
- Basis in paper: [explicit] The paper states that "Most LLMs perform bad on calculation problems, especially in physics domain" and provides detailed performance breakdowns.
- Why unresolved: The paper identifies that LLMs struggle with calculation problems but doesn't analyze which specific aspects (e.g., formula selection, numerical manipulation, unit conversion) are most problematic. Understanding these specific weaknesses could guide architectural improvements.
- What evidence would resolve it: Detailed error analysis of LLM responses to calculation problems, categorizing errors by type (e.g., formula selection, arithmetic errors, unit handling), would identify the most challenging aspects and inform targeted improvements in model architecture or training approaches.

## Limitations

- Data Coverage Gaps: While covering chemistry, physics, and biology, the benchmark may not fully represent the breadth of scientific disciplines or emerging research areas.
- Subjectivity in Evaluation: Manual evaluation of experimental data questions introduces human bias and limits reproducibility.
- Model-Specific Constraints: The benchmark doesn't account for variations in model context windows and processing capabilities.

## Confidence

**High Confidence**: The benchmark design principles (dynamic data generation, Bloom's taxonomy alignment, multi-disciplinary coverage) are sound and well-justified. The methodology for preventing data leakage through principled generation is theoretically robust.

**Medium Confidence**: Performance results show clear performance gaps between models, but the exact contribution of each benchmark dimension to these differences requires further analysis. The relative difficulty of dynamic versus static questions is demonstrated but needs more granular investigation.

**Low Confidence**: The scalability of manual evaluation for experimental data questions in large-scale benchmarking scenarios is questionable. Long-term maintenance and updating of the benchmark to keep pace with scientific advances needs clearer planning.

## Next Checks

1. **Replication Study**: Conduct an independent evaluation of SciEval using a different set of LLMs to verify the reported performance patterns and ensure the benchmark is not overly tuned to specific model architectures.

2. **Dynamic Generation Robustness**: Test the dynamic question generation pipeline by attempting to create adversarial examples that might exploit generation patterns, ensuring the mechanism truly prevents data leakage.

3. **Subjective Evaluation Standardization**: Develop and validate a rubric for experimental data question evaluation, then test inter-rater reliability across multiple evaluators to quantify and reduce subjectivity in scoring.