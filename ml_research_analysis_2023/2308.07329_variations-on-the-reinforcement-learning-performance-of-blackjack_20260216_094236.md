---
ver: rpa2
title: Variations on the Reinforcement Learning performance of Blackjack
arxiv_id: '2308.07329'
source_url: https://arxiv.org/abs/2308.07329
tags:
- player
- blackjack
- agent
- dealer
- deck
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the impact of deck size on the learning
  performance of a Q-learning agent in the game of Blackjack. The authors implement
  a Q-learning algorithm with a decaying epsilon-greedy policy to learn optimal Blackjack
  strategies.
---

# Variations on the Reinforcement Learning performance of Blackjack

## Quick Facts
- arXiv ID: 2308.07329
- Source URL: https://arxiv.org/abs/2308.07329
- Reference count: 30
- Primary result: Deck size variations do not significantly affect winning odds for Zen count and Uston APC systems, but Hi-Lo system shows decreasing performance beyond 21 decks.

## Executive Summary
This paper investigates how deck size impacts Q-learning agent performance in Blackjack. The authors implement a Q-learning algorithm with decaying epsilon-greedy policy and compare performance across different deck sizes (4-21 decks) using three card counting systems: Hi-Lo, Zen count, and Uston APC. The key finding is that while Zen count and Uston APC systems maintain consistent winning odds across deck sizes, the Hi-Lo system experiences a steeper decline in performance as deck size increases beyond 21 decks. The authors conclude that Zen count and Uston APC are more effective for games with larger deck counts.

## Method Summary
The paper implements a Q-learning agent with decaying epsilon-greedy policy to learn optimal Blackjack strategies across varying deck sizes. The agent is trained on deck sizes ranging from 4 to 21 using three different card counting systems (Hi-Lo, Zen count, and Uston APC). Performance is evaluated using winning odds percentage as the primary metric. The epsilon value starts at 1 and decays to 0.9 for the first 30% of training episodes, then to 0.2 for the next 40%, and finally to 0 for the remaining 30% of episodes. The paper compares the agent's performance across different deck sizes and card counting systems to identify trends and effectiveness.

## Key Results
- Zen count and Uston APC systems maintain consistent winning odds across deck sizes (4-21)
- Hi-Lo system shows a steeper downward trend in winning odds as deck size increases beyond 21 decks
- Learning performance is not significantly affected for deck sizes between 4 to 8 decks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decaying epsilon-greedy Q-learning adapts to environment complexity by reducing exploration over time, improving convergence in blackjack.
- Mechanism: The agent starts with high exploration (ε=1) to gather diverse state-action experiences, then decays ε toward 0, shifting to exploitation of learned Q-values.
- Core assumption: Blackjack state space is finite and manageable for tabular Q-learning; the environment is stationary across episodes.
- Evidence anchors:
  - [abstract] "decaying epsilon-greedy policy to learn optimal Blackjack strategies"
  - [section] "We initiate ϵ to be 1 and formulated it in a way that ϵ decreases to 0.9 of its initial value for the first 30% of the training episodes, decreases to 0.2 for the next 40% of the training rounds and reaches 0 in the next 30% of the training rounds."
- Break condition: If the state space is too large or continuous, the tabular Q-learning approach fails due to memory and generalization limits.

### Mechanism 2
- Claim: Deck size variations do not significantly affect winning odds when using Zen count or Uston APC card counting systems.
- Mechanism: These systems assign integer values to cards that maintain proportionality across deck sizes, preserving count effectiveness.
- Core assumption: Card counting systems are robust to deck size changes if they preserve the relative ratio of high to low cards.
- Evidence anchors:
  - [abstract] "the discrepancy in winning odds percentage across varying deck sizes is not significant for the Zen count and Upson APC systems"
  - [section] "From Figure 5 we observe little discrepancy in terms of winning odds % across deck size (4-21) under the Zen Count and Uston APC system"
- Break condition: If the game uses continuous shuffling or infinite decks, card counting becomes ineffective regardless of system.

### Mechanism 3
- Claim: Monte Carlo methods with exploring starts ensure all state-action pairs are visited, enabling unbiased value estimation in blackjack.
- Mechanism: Each episode starts from a random state-action pair, guaranteeing exploration of the full state space over time.
- Core assumption: The blackjack environment allows random initial states and actions without violating game rules.
- Evidence anchors:
  - [section] "Episodes are therefore generated with exploring starts. This means that each episode begins with a randomly chosen state and action and then follows the current policy."
  - [section] "To estimate the value states, we could adopt either the Monte Carlo on-policy or Monte Carlo off-policy."
- Break condition: If exploring starts violate game constraints (e.g., illegal initial actions), the method cannot be applied.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: Blackjack is modeled as an episodic finite MDP where each game is an episode and rewards depend on state transitions.
  - Quick check question: What are the state, action, and reward components in the blackjack MDP formulation?

- Concept: Q-learning update rule
  - Why needed here: The algorithm updates Q-values using the Bellman equation to approximate optimal action-value functions.
  - Quick check question: Write the one-step Q-learning update equation used in the paper.

- Concept: Card counting systems
  - Why needed here: Different counting systems (Hi-Lo, Zen count, Uston APC) assign values to cards to estimate player advantage and guide betting.
  - Quick check question: How does the Hi-Lo system assign values to cards 2-6, 7-9, and 10-A?

## Architecture Onboarding

- Component map: Blackjack environment simulator -> Q-learning agent with decaying epsilon-greedy policy -> Card counting modules (Hi-Lo, Zen count, Uston APC) -> Training loop with episode generation and Q-value updates -> Testing/backtest module

- Critical path: 1. Initialize Q-table and epsilon 2. Generate episode with exploring starts 3. Execute actions, observe rewards and next states 4. Update Q-values using Q-learning rule 5. Decay epsilon according to schedule 6. Repeat until convergence 7. Evaluate policy on test games

- Design tradeoffs:
  - Tabular Q-learning vs function approximation: Tabular is simpler but doesn't scale to continuous or huge state spaces.
  - Fixed vs adaptive learning rate: Fixed rate (α=0.05) is simpler but may converge slower than adaptive methods.
  - Epsilon decay schedule: Aggressive decay may cause premature exploitation; slow decay increases training time.

- Failure signatures:
  - Q-values not converging: Check if epsilon decays too quickly or learning rate is too low.
  - Agent not learning to surrender: Action space may be incomplete or Q-values for surrender state never updated.
  - Performance drops with deck size: Card counting system may not be robust to deck size changes.

- First 3 experiments:
  1. Train agent with 4 decks using Hi-Lo counting; measure winning odds after 500k episodes.
  2. Repeat experiment with Zen count; compare winning odds to Hi-Lo.
  3. Vary deck size from 4 to 21; plot winning odds for each counting system to identify trends.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Q-learning agents in Blackjack scale with the number of players beyond the typical casino limit of 7?
- Basis in paper: [explicit] The authors mention that future work could explore extending the analysis beyond the 7 player limit to investigate when learning saturates as the problem approaches an infinite player and deck size scenario.
- Why unresolved: The paper only tests up to 7 players and does not explore scenarios with more players.
- What evidence would resolve it: Simulations of Q-learning agents playing Blackjack with more than 7 players, measuring performance metrics such as win rates and convergence speed.

### Open Question 2
- Question: Is there an optimal deck size for Q-learning agents in Blackjack, or does performance continue to improve with larger decks?
- Basis in paper: [explicit] The authors observe that for deck sizes 4 to 8, the learning performance of the agent is not significantly affected. However, they also note that for the Hi-Lo counting system, the winning odds percentage decreases as the deck size increases beyond 21 decks.
- Why unresolved: The paper does not explore deck sizes beyond 21 decks, leaving the question of whether there is an optimal deck size or if performance continues to degrade.
- What evidence would resolve it: Simulations of Q-learning agents playing Blackjack with deck sizes larger than 21 decks, measuring performance metrics such as win rates and convergence speed.

### Open Question 3
- Question: How does the choice of card counting system affect the learning performance of Q-learning agents in Blackjack?
- Basis in paper: [explicit] The authors compare the performance of Q-learning agents using three card counting systems: Hi-Lo, Zen count, and Uston APC. They find that the Zen count and Uston APC systems are more effective for games with larger deck counts.
- Why unresolved: The paper does not explore other card counting systems or combinations of systems, leaving the question of whether there are more effective systems or combinations.
- What evidence would resolve it: Simulations of Q-learning agents playing Blackjack using different card counting systems or combinations of systems, measuring performance metrics such as win rates and convergence speed.

## Limitations

- The study uses a simplified Blackjack environment without advanced rules like insurance or splitting pairs, limiting real-world applicability
- The experimental range stops at 21 decks, making the extrapolated trend for Hi-Lo system beyond this point uncertain
- The paper doesn't address how card counting systems perform with continuous shuffling or infinite decks

## Confidence

- Effectiveness of decaying epsilon-greedy policy: High confidence based on clear description of decay schedule and common use in RL literature
- Zen count and Uston APC effectiveness across deck sizes: Medium confidence based on reported winning odds percentages
- Hi-Lo system trend beyond 21 decks: Low confidence due to extrapolation from limited experimental range

## Next Checks

1. Test the agent with advanced Blackjack rules (insurance, splitting pairs) to assess real-world applicability
2. Evaluate card counting effectiveness with continuous shuffling or infinite decks
3. Extend the experimental range beyond 21 decks to verify the extrapolated trend for Hi-Lo system