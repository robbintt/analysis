---
ver: rpa2
title: MDF-Net for abnormality detection by fusing X-rays with clinical data
arxiv_id: '2302.13390'
source_url: https://arxiv.org/abs/2302.13390
tags:
- data
- clinical
- mdf-net
- images
- abnormality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multimodal deep learning architecture (MDF-Net)
  that integrates clinical data with chest X-ray images for abnormality detection.
  The method introduces a spatialization strategy to transform clinical data into
  3D space for fusion with image features, combining both 1D and 3D fusion approaches.
---

# MDF-Net for abnormality detection by fusing X-rays with clinical data

## Quick Facts
- **arXiv ID**: 2302.13390
- **Source URL**: https://arxiv.org/abs/2302.13390
- **Reference count**: 40
- **Primary result**: 12% improvement in Average Precision (AP) for chest X-ray abnormality detection by fusing clinical data with image features

## Executive Summary
This paper proposes MDF-Net, a multimodal deep learning architecture that integrates clinical data with chest X-ray images for abnormality detection. The method introduces a spatialization strategy to transform clinical data into 3D space for fusion with image features, combining both 1D and 3D fusion approaches. The model was evaluated on a constructed dataset combining MIMIC-CXR images with MIMIC-IV clinical data, achieving 12% improvement in Average Precision (AP) compared to standard Mask R-CNN. The proposed architecture demonstrates better generalization and consistently fewer false positives/negatives across various chest abnormalities.

## Method Summary
MDF-Net extends the Mask R-CNN architecture by incorporating clinical data through two fusion strategies. A spatialization layer transforms 1D clinical feature vectors into 3D tensors using deconvolution and convolution operations, enabling element-wise fusion with image features. The model employs both 3D fusion (combining clinical and image features before the Region Proposal Network) and 1D fusion (concatenating clinical data with RoI features before classification). The architecture uses MobileNetv3 as the backbone and is trained on a dataset combining MIMIC-CXR images with MIMIC-IV clinical data, achieving improved detection performance through multimodal integration.

## Key Results
- 12% improvement in Average Precision (AP) compared to standard Mask R-CNN
- Consistent reduction in false positives and false negatives across all five chest abnormalities tested
- Spatialization strategy successfully transforms clinical data into compatible 3D space for fusion with image features
- Both 3D and 1D fusion methods contribute to overall performance improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spatializing clinical data into 3D feature maps enables multimodal fusion with image features in the same representational space
- Mechanism: The spatialization layer uses a deconvolution operation followed by convolution to upsample 1D clinical feature vectors into 3D tensors matching the dimensions of image feature maps, allowing element-wise fusion
- Core assumption: Deconvolution with sparse encoding kernels can learn to map clinical features to spatially meaningful representations
- Evidence anchors:
  - [section] "We define a spatialisation layer, fspa, as a deconvolutional layer [44] followed by a convolution operation. The deconvolution takes as input the n× 1 dimensional clinical data vector and learns an upscaled representation using a sparse encoded convolution kernel [45, 46]."
  - [abstract] "Since these data modalities are in different dimensional spaces, we propose a spatial arrangement strategy, spatialization, to facilitate the multimodal learning process"
- Break condition: If the learned spatialization produces feature maps that don't correlate with meaningful spatial patterns in the image domain, the fusion becomes arbitrary and loses diagnostic relevance

### Mechanism 2
- Claim: 3D fusion improves region proposal quality while 1D fusion improves classification accuracy within those regions
- Mechanism: 3D fusion combines clinical and image features before the Region Proposal Network (RPN), helping identify relevant regions; 1D fusion concatenates clinical data with RoI features before classification, providing contextual information for decision-making
- Core assumption: Different stages of object detection benefit from different types of multimodal integration
- Evidence anchors:
  - [section] "When using both 1-D and 3-D fusions together, the 3-D fusion can help RPN to pick up abnormal regions better while 1-D fusion can help the final classifier to determine whether a lesion exists in the given region"
  - [abstract] "incorporating patients' clinical data in a DL model together with the proposed fusion methods improves the disease localization in chest X-rays by 12% in terms of Average Precision"
- Break condition: If the RPN already performs optimally without clinical context, adding 3D fusion may introduce noise rather than improvement

### Mechanism 3
- Claim: Clinical features provide discriminative information that image features alone cannot capture for certain abnormalities
- Mechanism: Clinical variables like temperature and respiratory rate correlate with specific pathologies (e.g., consolidation vs atelectasis), and their inclusion helps the model distinguish between visually similar abnormalities
- Core assumption: Clinical measurements contain information about underlying pathophysiology that manifests differently in chest X-rays
- Evidence anchors:
  - [section] "A clinical feature, such as body temperature, is essential for radiologists to discern whether the patient has a Consolidation or Atelectasis"
  - [section] "In terms of diagnosing pulmonary edema, radiologists consider temperature less important than heartrate and resprate"
- Break condition: If the dataset lacks sufficient correlation between clinical features and image abnormalities, or if the clinical measurements are too noisy, the benefit diminishes

## Foundational Learning

- Concept: Multimodal learning and fusion strategies
  - Why needed here: The paper combines structured clinical data with unstructured image data, requiring understanding of how to fuse different data modalities effectively
  - Quick check question: What are the key differences between early fusion, late fusion, and hybrid fusion approaches in multimodal learning?

- Concept: Object detection architectures (Mask R-CNN)
  - Why needed here: The MDF-Net extends Mask R-CNN, so understanding its two-stage process (RPN for region proposals, then classification/segmentation) is crucial
  - Quick check question: What are the two main stages of Mask R-CNN and what does each stage accomplish?

- Concept: Deconvolution and upsampling operations
  - Why needed here: The spatialization module uses deconvolution to transform 1D clinical vectors into 3D feature maps
  - Quick check question: How does a deconvolution layer differ from a regular convolution layer in terms of input/output dimensionality?

## Architecture Onboarding

- Component map: Input layer (CXR images + clinical data) → Backbone (MobileNetv3) → Spatialization (deconv+conv) → 3D Fusion (element-wise sum) → RPN (region proposals) → RoIPool (extract RoIs) → 1D Fusion (concatenation) → Classification/Regression/Mask heads → Output
- Critical path: Image backbone → Spatialization → 3D Fusion → RPN → RoIPool → 1D Fusion → Classification head
- Design tradeoffs: Using MobileNetv3 instead of larger backbones reduces overfitting on small datasets but may limit feature extraction capacity; spatialization adds complexity but enables multimodal fusion
- Failure signatures: Poor performance on clinical-only or image-only tasks suggests the fusion isn't learning meaningful cross-modal relationships; overfitting indicates the model is memorizing rather than generalizing
- First 3 experiments:
  1. Test image-only baseline (standard Mask R-CNN) vs clinical-only baseline to establish individual modality performance
  2. Test MDF-Net with only 3D fusion vs only 1D fusion to verify the ablation study findings
  3. Test with different sets of clinical features to identify which features contribute most to performance improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MDF-Net's performance scale with larger datasets?
- Basis in paper: [explicit] "In this work, we can only retrieve 670 instances for our multimodal dataset, which is considered small compared to other popular datasets used for X-ray diagnosis. If there are larger-scaled datasets with clinical data available in the future, our work should also be tested on them to have a more objective evaluation."
- Why unresolved: The authors acknowledge their dataset is small and recommend testing on larger datasets, but this remains untested.
- What evidence would resolve it: Performance metrics (AP, AR, IoBB) on datasets with at least 5,000-10,000 samples would demonstrate scalability and robustness.

### Open Question 2
- Question: Can the spatialization strategy be effectively applied to other multimodal fusion architectures beyond Mask R-CNN?
- Basis in paper: [explicit] "Our fusion methods can also be applied to other models, such as YOLO[40], SSD[41] and DETR[42]. We will incorporate other architectures to evaluate the effectiveness of our fusion methods."
- Why unresolved: The authors propose extending the approach to other architectures but have not implemented these experiments.
- What evidence would resolve it: Comparative studies showing AP/AR improvements when applying spatialization to YOLO, SSD, and DETR models on the same dataset.

### Open Question 3
- Question: Which clinical features contribute most to detection accuracy across different abnormalities?
- Basis in paper: [explicit] "In order to understand the contribution and significance of clinical features, we also conducted an ablation study by giving a different combination of clinical features to MDF-Net."
- Why unresolved: While ablation studies were performed, the paper notes complex interactions between features (e.g., "heartrate and resprate have higher AP than temperature when age is not used") without definitive conclusions about which features matter most for each abnormality.
- What evidence would resolve it: Systematic feature importance analysis using methods like SHAP values or LIME across all five abnormality types to quantify individual feature contributions.

## Limitations

- The dataset size is relatively small (670 instances), raising concerns about generalizability to broader clinical populations
- The spatialization approach lacks detailed ablation analysis on different deconvolution parameters and kernel configurations
- The study assumes consistent relationships between clinical features and chest abnormalities across diverse patient populations, which may not hold in practice

## Confidence

- **High Confidence**: The 12% AP improvement over standard Mask R-CNN and the consistent reduction in false positives/negatives across abnormalities
- **Medium Confidence**: The claim that 3D fusion improves region proposal quality while 1D fusion enhances classification accuracy, as this requires further validation across different detection architectures
- **Medium Confidence**: The assertion that specific clinical features (temperature, respiratory rate) are essential for distinguishing between visually similar abnormalities, though this aligns with radiological practice

## Next Checks

1. Test the model on an independent external dataset to verify generalization beyond the MIMIC-derived dataset
2. Conduct feature importance analysis to quantify the contribution of each clinical variable to performance improvements
3. Perform cross-validation with different random seeds to assess the stability of the 12% AP improvement claim