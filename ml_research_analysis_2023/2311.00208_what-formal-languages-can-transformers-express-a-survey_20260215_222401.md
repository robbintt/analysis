---
ver: rpa2
title: What Formal Languages Can Transformers Express? A Survey
arxiv_id: '2311.00208'
source_url: https://arxiv.org/abs/2311.00208
tags:
- languages
- language
- attention
- transformer
- circuit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of the theoretical expressivity
  of transformer models, focusing on their ability to recognize formal languages.
  The authors analyze how different transformer variants (e.g., with hard vs.
---

# What Formal Languages Can Transformers Express? A Survey

## Quick Facts
- arXiv ID: 2311.00208
- Source URL: https://arxiv.org/abs/2311.00208
- Reference count: 29
- This paper provides a comprehensive survey of the theoretical expressivity of transformer models, focusing on their ability to recognize formal languages.

## Executive Summary
This paper provides a comprehensive survey of the theoretical expressivity of transformer models, focusing on their ability to recognize formal languages. The authors analyze how different transformer variants (e.g., with hard vs. soft attention, different position encodings) relate to formal models like finite automata, counter machines, Turing machines, circuits, and logics. They show that hard-attention transformers are limited to AC0 circuit complexity, while soft-attention variants can recognize languages in TC0. They also relate transformers to first-order logics with counting and majority quantifiers. The paper establishes both upper and lower bounds on transformer expressivity, resolving some apparent contradictions in the literature by carefully accounting for different assumptions about precision, position encodings, and attention mechanisms.

## Method Summary
The paper surveys existing theoretical results on transformer expressivity, categorizing them into lower bounds (what transformers can do) and upper bounds (what transformers cannot do). It systematically analyzes how different transformer setups relate to formal models including automata, circuits, and logics. The authors propose a unified framework for understanding transformer expressivity, harmonizing seemingly contradictory findings by documenting the diverse assumptions underlying different results. The survey examines various transformer variants including hard vs. soft attention mechanisms and different position encoding schemes.

## Key Results
- Hard-attention transformers are limited to AC0 circuit complexity
- Soft-attention variants can recognize languages in TC0
- Transformers relate to first-order logics with counting and majority quantifiers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Soft attention enables counting, which is necessary for recognizing languages like PARITY and MAJORITY that are outside AC0.
- Mechanism: Soft attention allows transformers to sum weighted contributions across positions, effectively counting occurrences of symbols. This counting capability is what distinguishes soft-attention transformers from hard-attention ones.
- Core assumption: Soft attention can approximate uniform weight distribution across positions when needed for counting.
- Evidence anchors:
  - [abstract]: "They show that hard-attention transformers are limited to AC0 circuit complexity, while soft-attention variants can recognize languages in TC0."
  - [section 7.3.2]: "The key reason why soft attention extends the power to TC0 is because it enables counting"
  - [corpus]: Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If precision is limited to O(1) bits, uniform counting becomes impossible due to rounding down of attention weights.

### Mechanism 2
- Claim: Position encodings enable transformers to track positional information necessary for recognizing non-regular languages.
- Mechanism: Different position encoding schemes (i, i/n, 1/i, 1/i²) provide transformers with varying levels of positional resolution. This allows recognition of languages requiring positional awareness beyond finite automata.
- Core assumption: The choice of position encoding significantly impacts expressivity, as noted in the abstract.
- Evidence anchors:
  - [section 4.1]: "Because the choice of position encoding can have a significant impact on the expressivity of the network"
  - [section 7.1.1]: "Chiang and Cholak (2022) showed that transformer encoders whose PE includes i/n do recognize PARITY"
  - [corpus]: Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If position encoding lacks sufficient resolution (e.g., constant encoding), positional tracking for non-regular languages becomes impossible.

### Mechanism 3
- Claim: Layer normalization with εN > 0 ensures Lipschitz continuity, which is necessary for theoretical guarantees about transformer expressivity.
- Mechanism: Lipschitz continuity prevents unbounded growth in activations, allowing theoretical analysis to establish upper bounds on expressivity through circuit complexity arguments.
- Core assumption: Theoretical results often assume Lipschitz-continuous transformations for mathematical tractability.
- Evidence anchors:
  - [section 4.2.3]: "Observe that N is Lipschitz-continuous iff εN > 0"
  - [section 7.1.1]: "Hahn (2020) shows that softmax attention transformers cannot recognize PARITY under the condition that all position-wise functions are Lipschitz-continuous"
  - [corpus]: Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If εN = 0 (as in some constructions that recognize PARITY), the theoretical guarantees no longer apply.

## Foundational Learning

- Concept: Formal language classes (regular, context-free, etc.)
  - Why needed here: The paper surveys transformer expressivity by relating it to formal language classes like AC0, TC0, and first-order logics
  - Quick check question: What is the relationship between regular languages and AC0 circuits?

- Concept: Circuit complexity theory
  - Why needed here: The paper uses circuit complexity (AC0, TC0, etc.) as the primary framework for characterizing transformer expressivity
  - Quick check question: Why can't AC0 circuits recognize PARITY while TC0 circuits can?

- Concept: Attention mechanisms (soft vs hard)
  - Why needed here: Different attention mechanisms (softmax vs leftmost-hard vs average-hard) lead to different expressivity results
  - Quick check question: What is the key difference between softmax and average-hard attention that affects expressivity?

## Architecture Onboarding

- Component map: Input layer (embedding + position encoding) -> Multi-head attention -> Feed-forward network -> Layer normalization -> Repeat -> Output classification
- Critical path: Input → Position encoding → Multi-head attention → Feed-forward → Layer norm → Repeat → Output classification
- Design tradeoffs:
  - Soft vs hard attention: Soft attention enables counting but may require higher precision; hard attention is more interpretable but limited to AC0
  - Position encoding choice: Different encodings enable recognition of different language classes
  - Precision requirements: O(log n) precision enables TC0 expressivity, while O(1) precision limits to weaker classes
- Failure signatures:
  - Inability to recognize PARITY or MAJORITY suggests hard-attention limitations
  - Poor performance on Dyck languages may indicate insufficient positional tracking
  - Failure on W(S5) suggests limitations within TC0
- First 3 experiments:
  1. Test PARITY recognition with soft attention vs hard attention variants
  2. Compare performance on Dyck-k languages with different position encodings (i, i/n, 1/i)
  3. Evaluate W(S5) recognition to probe TC0 limitations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the expressivity of softmax-attention transformers be characterized more tightly or even exactly in terms of some logic?
- Basis in paper: [explicit] The paper discusses various logics that have been proposed to characterize transformer expressivity, including FOC[MOD; +] and FOM[BIT], but notes that more work is needed to find the exact characterization.
- Why unresolved: While some logics have been proposed, the exact relationship between transformer expressivity and formal logics remains unclear. The paper suggests that finding this characterization is an open question.
- What evidence would resolve it: A formal proof showing that a particular logic exactly characterizes the expressivity of softmax-attention transformers, along with experimental validation on various formal languages.

### Open Question 2
- Question: Given the current practical importance of decoder-only transformers and intense interest in chain-of-thought reasoning, what further insights can the theory of circuits or logic provide into transformer decoders?
- Basis in paper: [explicit] The paper notes that while much work has been done on transformer encoders, there is less theoretical work on transformer decoders, which are currently very important in practice.
- Why unresolved: The paper states that "decoder-only transformers and intense interest in chain-of-thought reasoning" is an area where further theoretical insights are needed, but doesn't provide specific questions or approaches.
- What evidence would resolve it: New theoretical results relating transformer decoders to formal models (circuits, logics, automata) that explain their capabilities and limitations in practical applications like chain-of-thought reasoning.

### Open Question 3
- Question: How do the expressivity results for transformers change when considering bounded string lengths that are more realistic for practical applications?
- Basis in paper: [inferred] The paper discusses results for unbounded string lengths, but notes that in practice, transformers have maximum context lengths. It mentions that some theoretical findings seem to have practical consequences for modest string lengths.
- Why unresolved: The paper doesn't explore how the theoretical results change when we restrict to practical string length bounds, which could be an important bridge between theory and practice.
- What evidence would resolve it: A systematic study showing how the expressivity of transformers changes as we vary the maximum string length, potentially revealing new insights about their practical capabilities.

## Limitations
- Limited empirical validation of theoretical findings against practical transformer implementations
- Focus primarily on encoder-only architectures, with less coverage of encoder-decoder models
- Incomplete characterization of numerical precision requirements and their practical implications

## Confidence
- **High confidence**: Claims about AC0 vs TC0 separation for hard vs soft attention variants; the relationship between transformers and formal logics (FO with counting quantifiers); the importance of position encoding choices for expressivity
- **Medium confidence**: Claims about specific language recognition capabilities (PARITY, MAJORITY, Dyck languages); the role of layer normalization in theoretical guarantees; the TC0 upper bound for soft-attention transformers
- **Low confidence**: Practical implications of theoretical bounds for real-world transformer training; the exact precision requirements for different language classes; the expressivity of attention-free transformer variants

## Next Checks
1. **Precision sensitivity experiment**: Systematically vary numerical precision in transformer implementations (from O(1) to O(log n) bits) and measure their ability to recognize PARITY and MAJORITY languages. This would validate whether precision requirements identified in theory are necessary in practice.

2. **Position encoding ablation study**: Implement transformers with different position encoding schemes (i, i/n, 1/i, 1/i²) and test their ability to recognize Dyck-k languages for varying k. This would empirically confirm the theoretical predictions about positional resolution requirements.

3. **Lipschitz continuity stress test**: Create transformer variants with different εN values in layer normalization (including εN = 0) and evaluate their performance on languages that theoretically require non-Lipschitz transformations. This would test whether the theoretical limitations of Lipschitz-continuous transformers manifest in practice.