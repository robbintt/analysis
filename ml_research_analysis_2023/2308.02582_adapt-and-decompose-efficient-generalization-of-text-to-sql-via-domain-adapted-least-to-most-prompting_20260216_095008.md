---
ver: rpa2
title: 'Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted
  Least-To-Most Prompting'
arxiv_id: '2308.02582'
source_url: https://arxiv.org/abs/2308.02582
tags:
- power
- select
- nuclear
- plants
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an offline, efficient approach to improve cross-domain
  and cross-compositional generalization in Text-to-SQL semantic parsing. It uses
  a Generic Prompt (GP) with few-shot exemplars covering SQL operators, clauses, and
  database domains, then adapts it to the target database domain (DA-GP) and further
  decomposes it using Least-To-Most-Prompting (LTMP-DA-GP).
---

# Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting

## Quick Facts
- arXiv ID: 2308.02582
- Source URL: https://arxiv.org/abs/2308.02582
- Reference count: 23
- Primary result: Achieves up to 38.04% execution accuracy on cross-domain Text-to-SQL tasks using offline domain adaptation and decomposition

## Executive Summary
This paper addresses the challenge of improving cross-domain and cross-compositional generalization in Text-to-SQL semantic parsing using large language models. The authors propose an offline approach that first generates a Generic Prompt with diverse exemplars covering SQL operators and clauses, then adapts this prompt to target database domains, and finally decomposes complex queries using Least-To-Most-Prompting. The method achieves significant performance improvements on KaggleDBQA while avoiding expensive runtime exemplar retrieval, making it efficient and scalable.

## Method Summary
The approach operates in three main stages: First, it samples diverse few-shot exemplars from training data covering various SQL operators and clauses to create a Generic Prompt (GP). Second, it adapts the GP to the target database domain by modifying exemplar schemas and SQL while preserving composition structure, creating a Domain Adapted GP (DA-GP). Third, it applies Least-To-Most-Prompting to decompose complex queries into simpler sub-queries with intermediate NAT-SQL representations, combining them to form the final SQL. The entire process is performed offline, requiring minimal human intervention.

## Key Results
- Achieves up to 38.04% execution accuracy improvements on KaggleDBQA across multiple databases and LLMs
- Consistently outperforms existing few-shot methods in cross-domain and cross-compositional generalization
- Maintains efficiency through offline prompt generation, avoiding runtime exemplar retrieval

## Why This Works (Mechanism)

### Mechanism 1: Domain Adaptation Improves Cross-Domain Generalization
The approach modifies few-shot exemplars from the generic prompt to use the target database schema and data while preserving SQL composition structure. This alignment helps LLMs better handle queries in new database domains.

### Mechanism 2: Decomposition Improves Compositional Generalization
Least-to-most prompting breaks down complex NL queries into simpler sub-queries, generates intermediate NAT-SQL representations for each, then combines them. This decomposition enables better handling of unseen query compositions.

### Mechanism 3: Offline Processing Improves Efficiency
By pre-generating a generic prompt with diverse exemplars and performing all adaptations offline, the approach eliminates the need for expensive runtime exemplar retrieval during inference.

## Foundational Learning

- **Text-to-SQL semantic parsing**: Converting natural language questions to SQL queries
  - Why needed: This is the core task being addressed
  - Quick check: What are the key challenges in cross-domain and cross-compositional text-to-SQL tasks?

- **In-context learning with LLMs**: LLMs learning from few-shot exemplars provided in the prompt
  - Why needed: The approach relies on LLMs learning from few-shot exemplars
  - Quick check: How does in-context learning differ from fine-tuning in LLM-based approaches?

- **SQL composition structure**: Understanding how SQL queries are composed of operators, clauses, and functions
  - Why needed: Crucial for adaptation and decomposition mechanisms
  - Quick check: What are the main components of SQL query composition that need to be preserved during domain adaptation?

## Architecture Onboarding

- **Component map**: Generic Prompt Generator -> Domain Adapter -> Least-to-Most Prompting Engine -> LLM Interface -> Schema Formatter
- **Critical path**: 
  1. GP generation (offline)
  2. Domain adaptation (offline)
  3. LTMP decomposition (offline)
  4. Prompt assembly (offline)
  5. LLM inference (online)
- **Design tradeoffs**: 
  - Offline processing vs. runtime efficiency: More offline processing reduces runtime latency but increases upfront computation
  - Prompt size vs. LLM token limits: Balancing comprehensive exemplars with token constraints
  - Adaptation granularity: Fine-grained vs. coarse-grained schema and data adaptation
- **Failure signatures**: 
  - Prompt size exceeding LLM token limits
  - Generated SQL failing to execute on target database
  - Decomposition failing to capture query semantics
  - Adaptation producing invalid SQL syntax
- **First 3 experiments**:
  1. Test GP generation with different exemplar sampling strategies
  2. Validate domain adaptation on a simple schema transformation
  3. Test LTMP decomposition on a simple multi-step query

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of token length restrictions on the performance of LTMP-DA-GP for more complex queries?
- Basis: The paper mentions using models with a maximum token length of 4K, which required compromising on the schema format
- Why unresolved: The paper does not provide a detailed analysis of how token length affects performance, especially for queries requiring more complex reasoning
- What evidence would resolve it: Experiments varying the token length and measuring performance on queries of increasing complexity

### Open Question 2
- Question: How does the performance of LTMP-DA-GP compare to supervised approaches when using larger datasets or more diverse database domains?
- Basis: The paper compares LTMP-DA-GP to supervised approaches on the KaggleDBQA dataset but does not explore performance on larger or more diverse datasets
- Why unresolved: The KaggleDBQA dataset used in the experiments is limited in size and domain diversity
- What evidence would resolve it: Experiments on larger datasets or datasets with more diverse domains, comparing LTMP-DA-GP to supervised approaches

### Open Question 3
- Question: What is the optimal number of exemplars to include in the Generic Prompt (GP) for balancing diversity and token length constraints?
- Basis: The paper mentions that the algorithm samples a minimal set of exemplars covering SQL operators and clauses, but does not provide an analysis of the optimal number of exemplars
- Why unresolved: The paper does not explore how the number of exemplars affects performance or how to balance diversity with token length constraints
- What evidence would resolve it: Experiments varying the number of exemplars in the GP and measuring performance on a range of queries

## Limitations
- Evaluation is primarily conducted on KaggleDBQA with limited cross-database validation
- Dependency on tree edit distance for schema similarity matching could fail for structurally different but semantically similar schemas
- Significant offline processing and LLM inference required for prompt generation, potentially limiting scalability

## Confidence

**High Confidence**: The core claim that domain adaptation improves cross-domain generalization is well-supported by experimental results showing consistent performance gains across different databases and LLMs.

**Medium Confidence**: The claim about LTMP improving cross-compositional generalization is supported by methodology and results, but the specific impact of decomposition versus domain adaptation is not clearly isolated.

**Low Confidence**: The claim of "minimal human intervention" is the least substantiated, as the paper doesn't provide quantitative measures of human effort required for prompt engineering, exemplar selection, or threshold tuning.

## Next Checks

1. **Cross-database Generalization Test**: Evaluate the approach on at least 3-5 additional databases beyond KaggleDBQA, measuring performance degradation patterns and identifying failure cases where schema adaptation breaks down.

2. **Ablation Study on LTMP**: Conduct controlled experiments comparing DA-GP (domain adaptation only) versus LTMP-DA-GP (with decomposition) on queries with varying compositional complexity, to isolate the specific contribution of each mechanism.

3. **Human Effort Quantification**: Measure and report the actual human time investment required for prompt engineering, exemplar selection, and threshold tuning across different schema domains, to validate or refute the "minimal intervention" claim.