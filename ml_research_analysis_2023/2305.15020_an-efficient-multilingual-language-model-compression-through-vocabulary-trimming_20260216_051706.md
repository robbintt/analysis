---
ver: rpa2
title: An Efficient Multilingual Language Model Compression through Vocabulary Trimming
arxiv_id: '2305.15020'
source_url: https://arxiv.org/abs/2305.15020
tags:
- multilingual
- language
- vocabulary
- bias
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces vocabulary-trimming (VT), a method to reduce
  the vocabulary size of multilingual language models (LMs) by removing tokens irrelevant
  to a target language. VT identifies language-specific tokens from a corpus and deletes
  the rest along with their embeddings.
---

# An Efficient Multilingual Language Model Compression through Vocabulary Trimming

## Quick Facts
- arXiv ID: 2305.15020
- Source URL: https://arxiv.org/abs/2305.15020
- Authors: 
- Reference count: 19
- Key outcome: Vocabulary-trimming reduces multilingual LM size by up to 65% while maintaining or improving performance across QA, QG, sentiment analysis, and NLI tasks in seven languages

## Executive Summary
This paper introduces vocabulary-trimming (VT), a method to reduce the vocabulary size of multilingual language models by removing tokens irrelevant to a target language. VT identifies language-specific tokens from a corpus and deletes the rest along with their embeddings. Two strategies are proposed: pre-FT VT (before fine-tuning) and post-FT VT (after fine-tuning). Experiments on QA, QG, sentiment analysis, and NLI tasks across seven languages show that VT can reduce model size by up to 65% while maintaining or even improving performance. For example, QA and QG tasks retain performance with only 35% of the original parameters, and classification tasks achieve similar results with 33-39% of parameters. Additionally, VT-induced monolingual models exhibit less social bias than original monolingual models, offering both efficiency and fairness benefits.

## Method Summary
Vocabulary-trimming identifies language-specific tokens from a target language corpus and removes all other tokens along with their embeddings from multilingual LMs. Two strategies are proposed: pre-FT VT (before fine-tuning) and post-FT VT (after fine-tuning). The method uses mC4 corpora for seven target languages and applies VT to mT5-small, mBART, XLM-R, and XLM-V base models. Models are evaluated on QA (SQuAD variants), QG (same datasets adapted), sentiment analysis (UMSAB), and NLI (XNLI) tasks, measuring generation metrics (Ans-F1/EM for QA, MTR/BS for QG) and classification metrics (macro-F1 for sentiment, accuracy for NLI).

## Key Results
- Vocabulary-trimming reduces model size by up to 65% while maintaining or improving performance
- QA and QG tasks retain performance with only 35% of original parameters
- Classification tasks achieve similar results with 33-39% of parameters
- VT-induced monolingual models exhibit less social bias than original monolingual models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vocabulary-trimming reduces embedding matrix size by removing tokens irrelevant to target language while retaining performance
- Mechanism: VT identifies language-specific tokens from a corpus and deletes the rest along with their embeddings, effectively compressing the model
- Core assumption: Language-specific tokens contain sufficient information to maintain task performance after trimming
- Evidence anchors:
  - [abstract]: "VT identifies language-specific tokens on a language-specific corpus Cl, and remove all the tokens along with their embeddings except for those appeared in Cl"
  - [section]: "VT can reduce a multilingual LM vocabulary to a target language by deleting irrelevant tokens from its vocabulary"
  - [corpus]: Found 25 related papers with average neighbor FMR=0.477, suggesting moderate relevance to compression techniques
- Break condition: If target language has low corpus coverage or significant cross-lingual token usage, trimming may remove necessary tokens

### Mechanism 2
- Claim: Pre-FT VT outperforms Post-FT VT in some settings due to parameter optimization differences
- Mechanism: Trimming before fine-tuning allows optimization on smaller parameter space, potentially finding better configurations
- Core assumption: The optimal parameter space differs between full multilingual model and trimmed monolingual model
- Evidence anchors:
  - [abstract]: "pre-FT VT (VT before fine-tuning) and post-FT VT (VT after fine-tuning)"
  - [section]: "There are a few models where pre-FT VT degrades the performance... Japanese/Korean QA and Russian QG with pre-FT VT for top-5K outperforms No-Trim as well as post-FT VT"
  - [corpus]: Related work shows knowledge distillation and pruning can outperform full models when done strategically
- Break condition: If fine-tuning process is highly sensitive to initial parameter configuration, pre-FT VT may perform worse

### Mechanism 3
- Claim: VT reduces social bias by creating more monolingual models with less cultural overlap
- Mechanism: Multilingual models trained with more languages and cultures tend to have diluted cultural biases, and VT creates monolingual models that inherit this property
- Core assumption: Cultural diversity in training data reduces bias, and VT preserves this effect
- Evidence anchors:
  - [abstract]: "monolingual LM tends to contain more bias than its multilingual versions"
  - [section]: "monolingual models exhibit larger social biases (especially racial) than a VT-induced multilingual LM"
  - [corpus]: Found related work on bias evaluation in language models, though corpus coverage is moderate
- Break condition: If target language has unique cultural biases not present in multilingual training, VT may not reduce bias

## Foundational Learning

- Concept: Subword tokenization and vocabulary construction
  - Why needed here: Understanding how vocabulary size impacts model parameters and performance
  - Quick check question: Why does reducing vocabulary from 250K to 50K reduce parameters by ~60%?

- Concept: Language model fine-tuning strategies
  - Why needed here: Pre-FT vs Post-FT VT requires understanding how fine-tuning interacts with vocabulary changes
  - Quick check question: What happens to embeddings when you remove vocabulary tokens after fine-tuning?

- Concept: Social bias measurement in NLP
  - Why needed here: Evaluating whether VT affects model bias requires understanding bias metrics
  - Quick check question: How does AULA score relate to stereotypical vs anti-stereotypical sentence preferences?

## Architecture Onboarding

- Component map: Corpus → Token Frequency Analysis → Vocabulary Selection → Model Trimming → Fine-tuning (optional)
- Critical path: Corpus → Token Frequency Analysis → Vocabulary Selection → Model Trimming → Fine-tuning (optional)
- Design tradeoffs: Pre-FT VT offers efficiency but requires careful parameter optimization; Post-FT VT is more robust but needs full model training first
- Failure signatures: Performance degradation occurs when target language tokens overlap significantly with other languages or when corpus coverage is insufficient
- First 3 experiments:
  1. Test VT on a single language with known good corpus to establish baseline performance
  2. Compare Pre-FT vs Post-FT VT on classification tasks to identify optimal strategy
  3. Evaluate bias changes across different vocabulary reduction levels to find sweet spot

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of vocabulary-trimming (VT) vary across truly low-resource languages that are poorly represented in the underlying training corpus?
- Basis in paper: [inferred] The paper acknowledges limitations in testing VT on truly low-resource languages and suggests different behavior may occur when applied to languages with lower resources or poor representation in the training corpus
- Why unresolved: The paper's experiments focused on seven languages, but did not include truly low-resource languages that might exhibit different characteristics in terms of corpus representation and model performance
- What evidence would resolve it: Experimental results applying VT to a diverse set of low-resource languages with varying degrees of corpus representation, comparing performance against both the original multilingual model and monolingual baselines

### Open Question 2
- Question: Does the choice of vocabulary-trimming strategy (pre-FT vs post-FT) affect the optimal hyperparameters for fine-tuning, and how can these differences be systematically identified?
- Basis in paper: [explicit] The paper notes that pre-FT VT may lead to better results in some cases but acknowledges that the optimal parameters could differ from the original multilingual LM fine-tuning process, leaving this extended analysis for future work
- Why unresolved: The paper observes performance differences between pre-FT and post-FT VT but does not systematically explore how the optimal hyperparameter space shifts when using pre-FT VT versus post-FT VT
- What evidence would resolve it: A systematic comparison of optimal hyperparameter searches for both pre-FT and post-FT VT across multiple tasks and languages, identifying patterns in how the optimal settings differ between the two approaches

### Open Question 3
- Question: How does the social bias reduction achieved through vocabulary-trimming generalize across different types of bias beyond gender and race, and does this vary by language and task type?
- Basis in paper: [explicit] The paper evaluates social bias reduction for gender and race in English, but acknowledges limitations in the analysis and notes that other types of biases were not covered in this evaluation
- Why unresolved: The bias analysis was limited to two types of bias (gender and race) in English models, leaving questions about how VT affects other bias types and whether these effects generalize across languages and different NLP tasks
- What evidence would resolve it: Comprehensive bias evaluation across multiple bias types (e.g., age, disability, religion, socioeconomic status) in multiple languages and task settings, comparing VT-induced monolingual models against both original multilingual models and monolingual baselines

## Limitations

- Implementation-specific mechanisms: The paper's effectiveness relies heavily on the specific implementation of vocabulary-trimming, including how language-specific tokens are identified and which embeddings are removed
- Generalization across tasks and languages: While the paper demonstrates success on seven languages and four task types, the performance gains are not uniform across all combinations
- Social bias claims require careful interpretation: The claim that VT reduces social bias is based on comparing monolingual models to VT-induced multilingual models, but the methodology may not account for all bias sources

## Confidence

- **High Confidence**: The core mechanism of vocabulary-trimming reducing model size while maintaining performance is well-supported by the experimental results across multiple tasks and languages
- **Medium Confidence**: The comparative performance of pre-FT vs post-FT VT strategies is supported by the data but shows task-specific variations
- **Low Confidence**: The social bias reduction claims, while supported by the presented metrics, require more extensive validation across different bias measurement frameworks

## Next Checks

1. Replicate on held-out languages: Test VT on additional languages not included in the original study (e.g., Chinese, Arabic, Hindi) to verify that the performance gains and bias reduction claims generalize beyond the seven languages studied

2. Ablation study of token selection criteria: Systematically vary the threshold for identifying language-specific tokens (frequency cutoffs, top-n selection) to determine the sensitivity of performance to the token selection mechanism

3. Cross-lingual task transfer evaluation: Evaluate whether VT-induced monolingual models retain any cross-lingual transfer capabilities on zero-shot or few-shot learning tasks