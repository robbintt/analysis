---
ver: rpa2
title: 'ASVD: Activation-aware Singular Value Decomposition for Compressing Large
  Language Models'
arxiv_id: '2312.05821'
source_url: https://arxiv.org/abs/2312.05821
tags:
- asvd
- decomposition
- compression
- matrix
- singular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ASVD, a training-free post-training compression
  approach for Large Language Models (LLMs). The method addresses the challenges of
  distribution variance in LLM activations and sensitivity differences among layers.
---

# ASVD: Activation-aware Singular Value Decomposition for Compressing Large Language Models

## Quick Facts
- arXiv ID: 2312.05821
- Source URL: https://arxiv.org/abs/2312.05821
- Reference count: 20
- Key outcome: ASVD achieves 10-30% compression of LLMs without significant performance loss while reducing KV cache memory by 50%

## Executive Summary
This paper introduces ASVD, a training-free post-training compression technique for Large Language Models that addresses activation distribution variance and layer sensitivity differences. The method scales weight matrices based on activation statistics to handle outliers, uses layer-specific sensitivity analysis to optimize decomposition ranks, and evenly distributes singular values to improve quantization compatibility. Experiments demonstrate effective compression while maintaining model performance across multiple benchmarks.

## Method Summary
ASVD performs post-training compression through three key steps: (1) computing activation statistics to create a diagonal scaling matrix S that adjusts each weight column, (2) applying truncated SVD to the scaled weight matrix with layer-specific ranks determined by Sensitivity-based Truncation Rank Searching (STRS), and (3) evenly distributing singular values into both U and V matrices to enhance quantization. The method requires only a small calibration dataset and no retraining, making it efficient for deployment on resource-constrained devices.

## Key Results
- Achieves 10-30% parameter compression without significant perplexity degradation
- Reduces KV cache memory requirements by approximately 50%
- Maintains performance on MMLU zero-shot evaluation while compressing LLaMA-2 models
- Compatible with existing quantization techniques, further reducing quantization error

## Why This Works (Mechanism)

### Mechanism 1
ASVD reduces decomposition error by scaling the weight matrix based on activation distribution, specifically handling outlier channels. By constructing a diagonal scaling matrix S that adjusts each weight column according to the mean or maximum activation magnitude in that channel, ASVD transforms the decomposition problem so that activation outliers are absorbed into the transformed weight matrix WS. This leads to more accurate reconstruction when truncating singular values.

### Mechanism 2
Layer-specific sensitivity analysis enables optimal rank allocation by identifying which layers can be compressed more aggressively without performance loss. The Sensitivity-based Truncation Rank Searching (STRS) method evaluates each layer's impact on perplexity across a range of truncation ratios using a small calibration set. Layers with higher sensitivity to rank reduction (typically MLP layers) receive higher ranks, while less sensitive layers (often attention layers) can be compressed more.

### Mechanism 3
Absorbing singular values into both U and V matrices (rather than just one) improves quantization compatibility and reduces quantization error. After ASVD decomposition, singular values are evenly distributed into the transformed matrices Ak and Bk (where Ak = Uk√Σk and Bk = √ΣkVT). This balanced distribution leads to more uniform channel-wise values, reducing the quantization error compared to absorbing into only U or V.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD) and its properties for matrix approximation
  - Why needed here: The entire compression approach relies on decomposing weight matrices using SVD and understanding how truncation affects reconstruction error
  - Quick check question: What mathematical property of SVD makes it optimal for low-rank approximation, and how is truncation error related to omitted singular values?

- Concept: Activation distribution analysis and channel-wise scaling
  - Why needed here: ASVD's core innovation is adjusting weights based on activation statistics, requiring understanding of how activations propagate through layers
  - Quick check question: How does scaling the weight matrix by a diagonal matrix affect the activation distribution, and why does this help with outlier management?

- Concept: Sensitivity analysis and calibration dataset methodology
  - Why needed here: STRS requires evaluating layer sensitivity using a small calibration set to determine optimal truncation ranks
  - Quick check question: What metrics would you use to measure layer sensitivity to rank reduction, and how would you ensure the calibration set is representative?

## Architecture Onboarding

- Component map: Calibration dataset -> Activation collection -> Scaling matrix S computation -> SVD decomposition -> STRS sensitivity analysis -> Rank allocation -> Matrix reconstruction (Ak, Bk)
- Critical path: For deployment, the critical path is: (1) Run forward pass on calibration data to collect activations, (2) Compute scaling matrix S for each layer, (3) Apply truncated SVD with layer-specific ranks determined by STRS, (4) Absorb singular values into U and V matrices, (5) Replace original weights with decomposed versions. For inference, the critical path is simply computing Y = Ak(BkX) for each layer.
- Design tradeoffs: The main tradeoff is between compression ratio and accuracy, managed by the rank allocation. Another tradeoff is between calibration set size (larger sets give better sensitivity estimates but increase preprocessing time) and computational efficiency during decomposition (smaller ranks reduce computation but may hurt accuracy).
- Failure signatures: Common failure modes include: (1) Significant perplexity degradation indicating rank allocation is too aggressive, (2) Activation outliers not properly handled suggesting the scaling matrix computation needs adjustment, (3) Poor quantization performance indicating the value distribution after decomposition is still too skewed.
- First 3 experiments:
  1. Apply ASVD with default parameters (α=0.5, mean-based scaling) to a single layer and verify that perplexity on calibration set matches the original within acceptable bounds.
  2. Vary the α parameter (0.1, 0.5, 1.0, 2.0) and measure its impact on the trade-off between compression ratio and accuracy to identify optimal value.
  3. Compare the performance of mean-based vs. max-based activation scaling to determine which better handles outliers for the target model architecture.

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed ASVD approach perform on other model architectures beyond the Transformer-based LLMs tested in the paper? The paper primarily evaluates ASVD on LLaMA and LLaMA-2 models, which are Transformer-based architectures, and does not explore effectiveness on other model architectures like CNN, RNN, or GNN.

### Open Question 2
What is the impact of ASVD on the inference latency of LLMs, especially when combined with quantization techniques? While the paper mentions that ASVD can reduce computation required by LLM inference, it does not provide experimental results on actual inference latency.

### Open Question 3
How does the choice of the control factor α in ASVD affect the model's performance and compression ratio? The paper explores different values of α and finds mid-range values (e.g., 0.5) lead to better performance, but does not provide a detailed analysis of how α affects performance and compression ratio.

## Limitations

- Activation distribution stability is assumed but not thoroughly validated across diverse inputs and domains
- Computational overhead during inference is not quantitatively analyzed despite claims of training-free compression
- Sensitivity analysis generalizability is limited as the calibration set size and representativeness are not rigorously examined

## Confidence

**High Confidence Claims**:
- ASVD can achieve 10-30% compression ratios without significant perplexity degradation
- The method is compatible with existing quantization techniques and can further reduce quantization error
- ASVD effectively reduces KV cache memory requirements by approximately 50%

**Medium Confidence Claims**:
- Activation-aware scaling provides consistent benefits across different model architectures
- The iterative calibration process effectively identifies optimal layer-specific ranks
- The distribution of singular values into both U and V matrices improves quantization compatibility

**Low Confidence Claims**:
- ASVD generalizes well across diverse downstream tasks beyond the evaluated benchmarks
- The computational overhead during inference is negligible compared to memory savings
- The method scales effectively to models significantly larger than those tested (7B parameters)

## Next Checks

1. **Cross-Domain Robustness Test**: Evaluate ASVD-compressed models on multiple diverse datasets (e.g., biomedical text, code, conversational data) to assess how well the activation scaling generalizes beyond the calibration domain. Measure performance degradation and identify failure patterns.

2. **End-to-End Latency Benchmark**: Conduct comprehensive benchmarking comparing inference latency of original, ASVD-compressed, and quantized models across different hardware platforms. This should include both throughput and memory usage measurements to validate the claimed trade-offs.

3. **Ablation Study on Scaling Methods**: Systematically compare different activation scaling approaches (mean vs. max, different aggregation functions, dynamic vs. static scaling) to determine which components are essential for performance and which are optimizations that could be simplified.