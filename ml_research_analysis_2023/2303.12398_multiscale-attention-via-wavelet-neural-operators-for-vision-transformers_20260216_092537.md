---
ver: rpa2
title: Multiscale Attention via Wavelet Neural Operators for Vision Transformers
arxiv_id: '2303.12398'
source_url: https://arxiv.org/abs/2303.12398
tags:
- wavelet
- images
- afno
- attention
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Multiscale Wavelet Attention (MWA) mechanism
  for vision transformers to effectively capture small-to-large range dependencies
  among image pixels. MWA adapts wavelet neural operators, originally studied for
  solving PDEs, to discrete images by using Discrete Wavelet Transform (DWT) and its
  inverse for token mixing.
---

# Multiscale Attention via Wavelet Neural Operators for Vision Transformers

## Quick Facts
- arXiv ID: 2303.12398
- Source URL: https://arxiv.org/abs/2303.12398
- Authors: 
- Reference count: 32
- Key outcome: MWA achieves over 1% higher top-1 accuracy than Fourier-based attention methods on CIFAR-10/100 and Tiny-ImageNet while maintaining linear complexity

## Executive Summary
This paper introduces Multiscale Wavelet Attention (MWA), a novel attention mechanism for vision transformers that leverages wavelet neural operators to capture multiscale dependencies in natural images. Unlike global Fourier-based methods, MWA uses Discrete Wavelet Transform (DWT) and its inverse to decompose and reconstruct tokens, enabling localized multiscale representation that better matches the structure of natural images. The method demonstrates improved classification accuracy on standard vision benchmarks while maintaining linear computational complexity and GPU efficiency.

## Method Summary
MWA replaces standard self-attention layers in vision transformers with a wavelet-based token mixing mechanism. The approach uses 2D-DWT to decompose image tokens into approximation and detail coefficients at multiple scales, applies learnable convolutions in the wavelet domain with GeLU activation, then reconstructs tokens using 2D-IDWT. Weighted skip connections with different kernel sizes are added to preserve high-frequency details and facilitate identity learning. The method uses Haar wavelet with decomposition level m=1 and maintains linear complexity relative to sequence size.

## Key Results
- MWA achieves over 1% higher top-1 accuracy than Fourier-based attention methods (AFNO, GFN) on CIFAR-10/100 and Tiny-ImageNet
- Maintains linear computational complexity while capturing multiscale dependencies
- Efficiently implementable on GPU with competitive parameter counts to baseline methods
- Demonstrates effectiveness on small-to-large range dependencies among image pixels

## Why This Works (Mechanism)

### Mechanism 1
Wavelet transform captures multiscale structures better than Fourier transform for natural images. DWT decomposes images into approximation and detail coefficients at multiple scales, preserving both coarse trends and fine edges. This allows representation of small-to-large range dependencies common in natural images. Core assumption: Natural images contain discontinuities and multiscale patterns not well represented by global Fourier basis functions. Evidence: Abstract states MWA leverages localized wavelet nature in both frequency and spatial domains for multiscale structures.

### Mechanism 2
MWA achieves linear complexity while capturing multiscale dependencies. By using DWT and IDWT for token mixing, MWA replaces quadratic self-attention with operations scaling linearly with sequence size. Wavelet coefficients are convolved with learnable weights in wavelet domain. Core assumption: Computational efficiency of DWT/IDWT operations outweighs overhead of transforming to/from wavelet space. Evidence: Abstract confirms method maintains linear complexity and is efficiently implementable on GPU.

### Mechanism 3
Skip connections help learn high-frequency details and identity mappings. Two convolution layers with different kernel sizes (1×1 and 3×3) are added as weighted skip connections after inverse wavelet transform, facilitating learning of identity mappings and preserving high-frequency details. Core assumption: Skip connections improve gradient flow and allow network to better preserve spatial information. Evidence: Section 5 states convolution layers facilitate learning identity mapping and proven useful for learning high frequency details.

## Foundational Learning

- Concept: Wavelet transform and its properties
  - Why needed here: Understanding how wavelet transform decomposes signals into multiple scales and why beneficial for representing natural images
  - Quick check question: What is the key difference between wavelet transform and Fourier transform in terms of signal representation?

- Concept: Discrete Wavelet Transform (DWT) and Inverse DWT (IDWT) implementation
  - Why needed here: Knowing how to implement DWT and IDWT efficiently, especially for 2D images, and understanding their computational complexity
  - Quick check question: How does the computational complexity of DWT compare to FFT for 2D image data?

- Concept: Neural operators and their application to vision
  - Why needed here: Understanding concept of neural operators, how they learn mappings between functions, and how adapted from PDE solving to computer vision tasks
  - Quick check question: What is the main advantage of using neural operators over traditional neural networks for certain tasks?

## Architecture Onboarding

- Component map: Input tokens → 2D-DWT → Convolution with learnable weights → GeLU activation → 2D-IDWT → Skip connections (1×1 and 3×3 conv) → Output mixed tokens

- Critical path: 2D-DWT → Convolution with learnable weights → GeLU → 2D-IDWT → Skip connections → Output

- Design tradeoffs:
  - Using Haar wavelet for faster speed vs. more complex wavelets for potentially better representation
  - Number of decomposition levels (m=1 in this work) vs. computational cost and representation power
  - Size and number of groups for learnable convolution weights vs. parameter count and expressivity

- Failure signatures:
  - Poor performance on highly periodic or synthetic images without multiscale features
  - Increased computational cost if wavelet transform implementations are not GPU-optimized
  - Overfitting if skip connections introduce too much capacity

- First 3 experiments:
  1. Compare MWA with standard self-attention on CIFAR-10 using ViT-S/4 backbone, measuring top-1 accuracy and computational cost
  2. Ablation study: Remove skip connections from MWA and measure impact on performance and high-frequency detail preservation
  3. Vary the number of decomposition levels (m) in 2D-DWT and observe effect on accuracy and computational complexity

## Open Questions the Paper Calls Out
- How would MWA perform on larger datasets like ImageNet-1K or even larger-scale datasets?
- What is the optimal decomposition level m for 2D-DWT in MWA for different image resolutions and tasks?
- How does MWA perform on non-image tasks that involve sequences with multiscale structures, such as time series or point clouds?

## Limitations
- Narrow experimental scope limited to small-scale datasets (CIFAR-10/100, Tiny-ImageNet) with small model variants
- Complexity analysis assumes ideal conditions without accounting for practical GPU memory constraints or implementation overhead
- Does not address potential issues with wavelet transform stability across different input distributions or edge cases

## Confidence
- High confidence in computational complexity analysis and linear scaling claims
- Medium confidence in multiscale representation claims
- Low confidence in generalizability claims beyond tested datasets and model scales

## Next Checks
1. Test MWA on ImageNet-1K with larger ViT variants (B, L) to verify performance gains and linear complexity scale to real-world settings
2. Conduct controlled experiments replacing wavelet transform with Fourier-based alternatives while keeping all other components constant, to isolate whether multiscale structure or transform locality drives performance gains
3. Measure actual GPU memory usage and wall-clock training/inference time on representative hardware, comparing against theoretical FLOPs to identify practical bottlenecks and verify linear complexity translates to real-world efficiency gains