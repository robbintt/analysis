---
ver: rpa2
title: A Causal View of Entity Bias in (Large) Language Models
arxiv_id: '2305.14695'
source_url: https://arxiv.org/abs/2305.14695
tags:
- entity
- causal
- bias
- intervention
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a causal intervention technique to mitigate
  entity bias in language models. The key idea is to estimate a specific structured
  causal model (SCM) whose parameters are easier to estimate, and then intervene on
  the input entities during training or inference to reduce biasing information while
  preserving predictive information.
---

# A Causal View of Entity Bias in (Large) Language Models

## Quick Facts
- **arXiv ID**: 2305.14695
- **Source URL**: https://arxiv.org/abs/2305.14695
- **Reference count**: 15
- **Primary result**: Proposed causal intervention technique significantly improves out-of-distribution performance for both white-box and black-box language models, reducing entity-based knowledge conflicts and improving F1 scores by up to 9.1 points.

## Executive Summary
This paper introduces a causal intervention approach to mitigate entity bias in large language models. The method leverages structured causal models (SCMs) to estimate parameters that are easier to estimate than alternative causal models for entity bias mitigation. By intervening on input entities during training or inference—perturbing original entities with neighboring entities in the embedding space—the technique reduces biasing information while preserving predictive information. Experiments demonstrate significant improvements in out-of-distribution performance for relation extraction tasks, with the method working for both white-box models (like RoBERTa) and black-box models (like GPT-3.5).

## Method Summary
The proposed method builds on a structured causal model where entity embeddings are intervened upon using neighboring entities in the embedding space. For white-box models, during training, original entity embeddings are replaced with random embeddings from a convex hull constructed around the entity using its k nearest neighbors. During inference, the center of this convex hull is used. For black-box models, an in-context intervention replaces entities with placeholders and defines them using similar entities generated by the model itself. This approach exploits the continuous nature of embedding spaces to preserve shared predictive information while reducing entity-specific biasing information.

## Key Results
- Training-time intervention improves out-of-distribution performance on EntRED by up to 9.1 F1 points compared to baseline models
- In-context intervention reduces entity-based knowledge conflicts in GPT-3.5, achieving up to 20.5 points improvement in exact match accuracy on MRC
- The method effectively balances bias mitigation with preservation of predictive information across different entity types
- Both white-box and black-box interventions show consistent improvements over baseline approaches

## Why This Works (Mechanism)

### Mechanism 1: Convex Hull Perturbation
The method constructs a convex hull around each entity using its k nearest neighbors in the embedding space. During training, the original entity embedding is replaced with a random embedding from within the convex hull, while inference uses the center. This preserves shared predictive information while reducing entity-specific biasing information. The continuous nature of embedding spaces ensures that embeddings within the convex hull represent the same predictive information collectively, with biasing information gradually diminishing from the original entity toward the hull's border.

### Mechanism 2: Easy-to-Estimate SCM Parameters
The proposed SCM has parameters that are easier to estimate than alternative causal models for entity bias mitigation. By intervening at the input layer M before neural layers, the method only needs to estimate P(M|X,do(E)), which is simpler than estimating label distributions conditioned on entities. This approach leverages the sequential structure of most LLMs, where mitigating bias at the input layer propagates to subsequent layers.

### Mechanism 3: In-Context Intervention for Black-Box Models
For black-box LLMs, the method replaces original entities with abstract placeholders (ENTITY) and queries the LLM to generate k similar entities. The placeholder is then defined with these examples and prepended to the input. This eliminates specific biasing information while preserving predictive information, working effectively without accessing model parameters.

## Foundational Learning

- **Structured Causal Models (SCMs) and do-calculus**: The paper's entire approach is built on causal theory, using SCMs to model entity bias and do-calculus to perform interventions. *Why needed*: This framework enables formal reasoning about interventions on entity embeddings. *Quick check*: What is the difference between P(Y|X,E) and P(Y|X,do(E)) in the context of this paper's causal model?

- **Convex hulls and continuous embedding spaces**: The perturbation method relies on constructing convex hulls in the embedding space to preserve predictive information while reducing bias. *Why needed*: This mathematical structure ensures that neighboring entities share sufficient predictive information. *Quick check*: How does the convex hull perturbation ensure that predictive information is preserved while biasing information is reduced?

- **In-context learning and prompt engineering**: The black-box intervention technique relies on sophisticated prompt engineering to achieve causal intervention without accessing model parameters. *Why needed*: This enables bias mitigation for proprietary or inaccessible models. *Quick check*: What are the four steps of the in-context intervention technique, and how does each step contribute to mitigating entity bias?

## Architecture Onboarding

- **Component map**: Input preprocessing → Entity detection and embedding lookup → Convex hull construction → Perturbation logic → Model input → Prediction → Evaluation
- **Critical path**: Entity embedding → convex hull construction → perturbation → model input → prediction → evaluation
- **Design tradeoffs**: 
  - k value tradeoff: Higher k preserves more predictive information but reduces bias mitigation effectiveness; lower k provides stronger bias mitigation but may lose predictive information
  - Neighbor selection metric: Euclidean distance vs. other similarity metrics
  - Convex hull vs. alternative perturbation methods (e.g., simple random neighbor replacement)
- **Failure signatures**: Performance drop on in-distribution data with high k values; minimal improvement on out-of-distribution data with low k values; inconsistent results across different entity types; GPT-3.5 not understanding placeholder definitions
- **First 3 experiments**: 
  1. Verify convex hull construction works correctly by visualizing perturbed embeddings
  2. Test different k values on a small validation set to find the optimal balance
  3. Implement and test the in-context intervention on a simple black-box LLM to verify placeholder understanding

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- The continuous embedding space assumption may not hold for all entity types or embedding methods, potentially limiting the effectiveness of convex hull perturbation
- The approach requires careful tuning of the k parameter, which can be task-dependent and may not generalize well across different domains
- The in-context intervention for black-box models relies on the LLM's ability to understand placeholder definitions, which may vary significantly across different model architectures and sizes

## Confidence

**High confidence**: The theoretical framework of using SCMs and do-calculus for entity bias mitigation is well-established in causal inference literature. The experimental results showing improved out-of-distribution performance are reproducible and significant.

**Medium confidence**: The specific implementation details of convex hull perturbation and the exact mechanism by which predictive information is preserved while biasing information is reduced need further validation across different embedding spaces and entity types.

**Low confidence**: The generalizability of the in-context intervention technique to a broader range of black-box LLMs beyond GPT-3.5, particularly smaller or differently architected models, requires additional empirical validation.

## Next Checks

1. **Cross-domain validation**: Test the training-time intervention on relation extraction tasks beyond TACRED/EntRED, including different domains and languages, to assess generalizability.

2. **Ablation study on k parameter**: Conduct a systematic ablation study varying k values across multiple tasks to determine optimal ranges and understand the tradeoff between bias mitigation and predictive information preservation.

3. **Alternative embedding spaces**: Evaluate the intervention method using different embedding techniques (e.g., BERT, RoBERTa, GloVe) to test the robustness of the continuous embedding space assumption across various representations.