---
ver: rpa2
title: 'T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Mixed Large Language
  Model Signals for Science Question Answering'
arxiv_id: '2305.03453'
source_url: https://arxiv.org/abs/2305.03453
tags:
- answer
- question
- context
- which
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach named T-SciQ that leverages
  large language models' chain-of-thought (CoT) reasoning capabilities to teach small
  multimodal models for complex science question answering tasks. The proposed method
  generates high-quality CoT rationales as teaching signals and uses them to fine-tune
  much smaller models to perform CoT reasoning in complex modalities.
---

# T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Mixed Large Language Model Signals for Science Question Answering

## Quick Facts
- arXiv ID: 2305.03453
- Source URL: https://arxiv.org/abs/2305.03453
- Authors: Multiple
- Reference count: 40
- Primary result: Achieves 96.18% accuracy on ScienceQA, outperforming previous SOTA by 4.5%

## Executive Summary
T-SciQ introduces a novel approach to teaching small multimodal models to perform chain-of-thought reasoning for complex science question answering tasks. The method leverages large language models to generate high-quality reasoning rationales, which are then used as teaching signals to fine-tune smaller models. A data mixing strategy optimizes the teaching dataset by selecting the most effective reasoning approach for each skill type. Extensive experiments show that T-SciQ achieves state-of-the-art performance on the ScienceQA benchmark, significantly outperforming previous approaches.

## Method Summary
T-SciQ employs a three-stage approach: First, GPT-3.5 generates QA-CoT (Chain-of-Thought) and QA-PCoT (Program-of-Thought) samples using zero-shot prompting. Second, a data mixing strategy selects the most effective reasoning type for each skill based on validation performance. Third, a multimodal student model (UnifiedQABase, Mutimodal-CoTBase, or Mutimodal-CoTLarge) is fine-tuned using a two-stage framework: rationale generation followed by answer inference. During inference, the model generates rationales for test inputs and uses them to infer answers.

## Key Results
- Achieves 96.18% accuracy on ScienceQA test set
- Outperforms previous SOTA by 4.5% absolute improvement
- Demonstrates effectiveness across multiple vision encoders (CLIP, DETR, ResNet)
- Shows consistent improvements across different subject areas (natural science, social science, language science)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLM-generated CoT rationales improve reasoning by reducing noise and increasing diversity compared to human annotations
- **Core assumption**: LLM-generated rationales are higher quality and more diverse than human-annotated ones
- **Evidence anchors**: Abstract states human annotations are "time-consuming and costly" with issues of redundancy and missing information; Section 3.2 describes zero-shot prompting method using instruction "Please give me a detailed explanation."
- **Break condition**: If LLM fails to generate accurate rationales or introduces significant bias

### Mechanism 2
- **Claim**: Data mixing strategy optimizes teaching dataset by selecting best reasoning approach per skill
- **Core assumption**: Different skills benefit from different reasoning types, and validation performance indicates optimal choice
- **Evidence anchors**: Abstract mentions "novel data mixing strategy"; Section 3.3 describes selecting rationales from PCoT or CoT based on skill
- **Break condition**: If validation set is unrepresentative or skill categorization is inaccurate

### Mechanism 3
- **Claim**: Two-stage fine-tuning allows focused learning of reasoning and inference separately
- **Core assumption**: Separating rationale generation from answer inference improves overall performance
- **Evidence anchors**: Section 3.4 describes two-stage framework with rationale generation teaching followed by answer inference teaching
- **Break condition**: If model fails to generalize reasoning process or two-stage training doesn't improve over end-to-end

## Foundational Learning

- **Concept**: Chain-of-thought (CoT) reasoning
  - Why needed here: Allows breaking down complex problems into intermediate steps for better multi-step reasoning
  - Quick check question: How does CoT reasoning differ from direct answer prediction, and why is it more effective for complex problems?

- **Concept**: Multimodal reasoning
  - Why needed here: Science QA requires integrating text, images, and context for comprehensive understanding
  - Quick check question: What challenges arise when combining language and vision modalities for reasoning, and how does T-SciQ address them?

- **Concept**: Fine-tuning with teaching signals
  - Why needed here: Transfers reasoning capabilities from large models to smaller, more efficient architectures
  - Quick check question: How does fine-tuning with teaching signals differ from standard supervised learning, and what are the benefits for complex reasoning?

## Architecture Onboarding

- **Component map**: SciTeacher (LLM) -> T-SciQ mixing strategy -> SciStudent (student model) -> Vision encoder + Language encoder + Gated fusion -> Transformer decoder

- **Critical path**: 1) Generate QA-CoT and QA-PCoT samples using SciTeacher, 2) Mix samples based on validation performance per skill, 3) Fine-tune SciStudent in two stages (rationale generation â†’ answer inference), 4) During inference, generate rationales then use them to infer answers

- **Design tradeoffs**: LLM-generated rationales vs. human annotations (speed/diversity vs. potential bias), two-stage fine-tuning vs. end-to-end (focused learning vs. training complexity), different vision features (CLIP/DETR/ResNet tradeoff between cost and performance)

- **Failure signatures**: Poor performance on complex problems (inadequate CoT rationales or insufficient PCoT decomposition), hallucinations in generated rationales (overfitting or insufficient diversity), degraded performance with different vision features (model dependence on specific visual representations)

- **First 3 experiments**: 1) Ablation study: train SciStudent with only QA-CoT, only QA-PCoT, and mixed T-SciQ datasets, 2) Vision feature comparison: evaluate performance using CLIP, DETR, and ResNet features, 3) Teacher model comparison: generate teaching signals using different LLM variants (text-davinci-002, text-davinci-003, ChatGPT)

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but based on the analysis, several important questions remain unanswered.

## Limitations
- Performance heavily depends on quality of LLM-generated rationales, which may introduce bias or errors
- Two-stage fine-tuning adds complexity and training time compared to end-to-end approaches
- Data mixing strategy assumes validation performance reliably indicates optimal reasoning type per skill

## Confidence
- **High Confidence**: Mechanism 1 (LLM-generated rationales improving reasoning) - well-supported by experimental results showing consistent improvements
- **Medium Confidence**: Mechanism 2 (Data mixing strategy) - promising but relies on validation performance as reliable indicator
- **Medium Confidence**: Mechanism 3 (Two-stage fine-tuning) - supported by results but added complexity may not always justify gains

## Next Checks
1. **Generalization Test**: Evaluate T-SciQ on held-out dataset from different domain (e.g., medical QA) to assess generalization beyond ScienceQA benchmark
2. **Teacher Model Robustness**: Test impact of using different LLM variants (text-davinci-002, ChatGPT) as teacher model to assess performance consistency
3. **End-to-End Comparison**: Implement and evaluate end-to-end training approach to determine if two-stage framework's complexity is necessary for performance gains