---
ver: rpa2
title: 'QTSumm: Query-Focused Summarization over Tabular Data'
arxiv_id: '2305.14303'
source_url: https://arxiv.org/abs/2305.14303
tags:
- table
- generation
- data
- summary
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces QTSumm, a new dataset and task for query-focused
  table summarization, where models must generate tailored summaries based on user
  queries over tabular data. QTSumm contains 5,625 human-annotated query-summary pairs
  over 2,437 tables covering diverse topics.
---

# QTSumm: Query-Focused Summarization over Tabular Data

## Quick Facts
- arXiv ID: 2305.14303
- Source URL: https://arxiv.org/abs/2305.14303
- Reference count: 40
- Primary result: Introduces QTSumm dataset with 5,625 human-annotated query-summary pairs over 2,437 tables; existing models struggle with this task

## Executive Summary
The paper introduces QTSumm, a new dataset and task for query-focused table summarization, where models must generate tailored summaries based on user queries over tabular data. QTSumm contains 5,625 human-annotated query-summary pairs over 2,437 tables covering diverse topics. The authors evaluate strong baselines including text generation models, table-to-text models, and large language models, finding that existing approaches struggle with the task, particularly in producing factually correct and relevant summaries. The proposed ReFactor method, which retrieves and reasons over query-relevant information from tables to generate natural language facts, shows promise when its output is concatenated with model inputs. The work highlights significant challenges in table-to-text generation and opens new directions for research in user-customized table summarization.

## Method Summary
The authors created QTSumm by annotating 2,437 tables from LOGIC NLG and TOTTO datasets with 5,625 query-summary pairs under real-world scenarios. They evaluated multiple baselines including text generation models (BART, T5, Flan-T5), table-to-text models (TAPEX, ReasTAP, UnifiedSKG, PLOG), and large language models (GPT-3) using both automated metrics (BLEU, ROUGE-L, BERTScore, LitePyramids) and human evaluation (comprehensiveness, faithfulness, fluency). The ReFactor method retrieves and reasons over query-relevant information to generate natural language facts, which are then concatenated with model outputs to improve performance.

## Key Results
- Existing models struggle with QTSumm, with best performing models achieving only moderate scores on automated metrics
- Multi-task fine-tuning on RotoWire and HiTab improves T5 performance, with HiTab providing greater benefit due to diverse reasoning operations
- GPT-3 achieves comparable human evaluation results to fine-tuned models despite low automated scores, suggesting automated metrics don't align with human judgment
- ReFactor method shows promise but requires concatenation with model outputs rather than replacement
- Human evaluation reveals models frequently hallucinate, generate factually incorrect information, misunderstand user intent, and produce repetitive content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Query-focused summarization is more aligned with real-world information-seeking behavior than generic table-to-text generation.
- Mechanism: By conditioning text generation on user queries, the system focuses on extracting and synthesizing only the most relevant facts from the table rather than attempting to summarize everything.
- Core assumption: Users consult tables with specific questions in mind, not to receive a generic overview.
- Evidence anchors:
  - [abstract]: "People primarily consult tables to conduct data analysis or answer specific questions. Text generation systems that can provide accurate table summaries tailored to users' information needs can facilitate more efficient access to relevant data insights."
  - [section]: "Table-to-text generation systems should adopt a more flexible and interactive approach that allows people to obtain a user-customized summary tailored to their information needs."
- Break condition: If queries become too vague or tables contain minimal information relevant to any possible query, the approach loses its advantage over generic summarization.

### Mechanism 2
- Claim: Multi-task fine-tuning on related table-to-text datasets improves performance on QTSumm.
- Mechanism: Pre-training on datasets like RotoWire and HiTab exposes the model to different table structures and reasoning patterns, which transfers to better handling of diverse tables in QTSumm.
- Core assumption: Reasoning and summarization skills are transferable across different table domains and formats.
- Evidence anchors:
  - [section]: "MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering... Single table questions do not involve common reasoning patterns needed for multi-table queries."
  - [section]: "We found that fine-tuning on HiTab brings the T5 model better improvement. This is because HiTab requires text generation models to perform various kinds of reasoning operations over tables covering diverse topics."
- Break condition: If the source tasks are too dissimilar (e.g., sports summaries vs. general query answering), the transfer may not occur or could even be harmful.

### Mechanism 3
- Claim: Table-to-text models outperform standard text generation models because they are pre-trained to understand table structure.
- Mechanism: Models like TAPEX and ReasTAP learn to parse and reason over tabular data during pre-training, giving them an inductive bias toward handling table-specific tasks.
- Core assumption: Table structure (rows, columns, headers) provides important signals that generic text models miss.
- Evidence anchors:
  - [section]: "Importance of table structure understanding: Table-to-text generation models achieve better performance than their corresponding text generation backbones. For example, PLOG model that continues pretraining T5 with table-to-logic-form corpus achieves a 5.3% improvement on the LitePyramid metric."
  - [corpus]: Weak - no direct comparison of table-structure-aware vs. naive models on QTSumm in corpus.
- Break condition: If the input serialization flattens the table too aggressively, the structural advantage may be lost.

## Foundational Learning

- Concept: Reasoning over multiple relevant table regions
  - Why needed here: Summaries must integrate facts from different rows/columns to answer complex queries.
  - Quick check question: Given a table of election results, how would you determine which district had the closest race?

- Concept: Faithfulness in table-to-text generation
  - Why needed here: Generated summaries must not contradict the source table; hallucination is a major failure mode.
  - Quick check question: If a table shows Candidate A got 60% of the vote, is it acceptable for a summary to say "Candidate A won by a landslide"?

- Concept: Query interpretation and intent matching
  - Why needed here: The model must understand what information the user is seeking before synthesizing the summary.
  - Quick check question: How would you distinguish between "Who won the election?" and "Why did they win?" in terms of required summary content?

## Architecture Onboarding

- Component map: User query + table -> Preprocessed input -> Model inference -> Generated summary -> Evaluation metrics
- Critical path: User query + table -> Preprocessed input -> Model inference -> Generated summary -> Evaluation metrics
- Design tradeoffs: Longer input context allows more table coverage but increases computational cost and risk of attention degradation; simpler models train faster but may lack reasoning depth.
- Failure signatures: Hallucination (extraneous facts), factually incorrectness (contradicting table), user intent misunderstanding (irrelevant content), repetition (redundant information).
- First 3 experiments:
  1. Compare BART vs. T5 on QTSumm with default settings to establish baseline performance gap.
  2. Test multi-task fine-tuning with RotoWire to measure transfer benefit.
  3. Evaluate GPT-3 few-shot performance to benchmark against fine-tuned models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop automated evaluation metrics for the QTS UMM dataset that better align with human judgments, particularly for assessing fluency, faithfulness, and comprehensiveness?
- Basis in paper: Explicit - The authors note that GPT-3 achieves results comparable to state-of-the-art fine-tuned models in human evaluation despite receiving low scores in automated evaluation, motivating exploration of better automated metrics.
- Why unresolved: Current automated metrics like BLEU, ROUGE, BERTScore, and LitePyramids do not correlate well with human judgments for this task. The paper identifies this as an open problem but does not propose solutions.
- What evidence would resolve it: Development and validation of new automated metrics that show strong correlation with human evaluation scores on the QTS UMM test set.

### Open Question 2
- Question: How can large language models be further leveraged to improve faithful table-to-text generation for the QTS UMM task?
- Basis in paper: Explicit - The authors note that despite GPT-3's low automated evaluation scores, it achieves comparable human evaluation results to fine-tuned models, suggesting untapped potential for LLMs in this task.
- Why unresolved: The paper only provides initial results showing GPT-3's potential but does not explore methods to enhance its performance or understand why it performs well in human evaluation despite low automated scores.
- What evidence would resolve it: Systematic investigation of GPT-3's strengths and weaknesses on QTS UMM, along with techniques to improve its factual accuracy and relevance while maintaining its human-like qualities.

### Open Question 3
- Question: What is the impact of multi-task fine-tuning on the performance of text generation models for query-focused table summarization, and which source tasks are most beneficial?
- Basis in paper: Explicit - The authors demonstrate that multi-task fine-tuning with RotoWire and HiTab improves T5's performance on QTS UMM, with HiTab showing greater benefit due to its requirement for diverse reasoning operations.
- Why unresolved: The paper only explores two source tasks and does not systematically investigate which types of table-to-text tasks are most beneficial for improving query-focused summarization.
- What evidence would resolve it: Comprehensive experiments with various table-to-text datasets as source tasks, analyzing which task characteristics (e.g., reasoning complexity, domain diversity) contribute most to improvements on QTS UMM.

## Limitations
- Existing models struggle significantly with hallucination, factual correctness, and query understanding
- The ReFactor method requires concatenation with model outputs rather than replacing them
- Human evaluation sample size (100 instances) may not capture full distribution of difficulties
- Limited exploration of which source tasks are most beneficial for multi-task fine-tuning

## Confidence

**High Confidence**: The dataset creation methodology, the assertion that query-focused summarization better reflects real-world information needs, and the observation that existing models perform poorly on this task.

**Medium Confidence**: The relative performance of different model architectures, the effectiveness of multi-task fine-tuning, and the specific improvements from the ReFactor method. These claims are supported by the data but may be sensitive to hyperparameter choices and evaluation sample selection.

**Low Confidence**: The generalization of results to other table domains not represented in the dataset, and the scalability of proposed solutions to much larger tables or more complex multi-table reasoning scenarios.

## Next Checks

1. **Ablation study on input serialization**: Systematically test different table flattening strategies (e.g., column-first vs. row-first) to isolate the impact of serialization on model performance, particularly for table-structure-aware models like TAPEX and ReasTAP.

2. **Cross-dataset generalization**: Evaluate the best-performing models on a held-out subset of tables from completely different domains (e.g., scientific tables if the training data is primarily business/financial) to assess domain transfer capabilities.

3. **Query complexity analysis**: Stratify the 100 human-evaluated instances by query complexity (e.g., single-fact vs. multi-hop reasoning) and analyze model performance differences to identify specific failure patterns and guide targeted model improvements.