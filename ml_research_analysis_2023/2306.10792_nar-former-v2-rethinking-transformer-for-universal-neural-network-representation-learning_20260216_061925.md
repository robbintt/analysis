---
ver: rpa2
title: 'NAR-Former V2: Rethinking Transformer for Universal Neural Network Representation
  Learning'
arxiv_id: '2306.10792'
source_url: https://arxiv.org/abs/2306.10792
tags:
- neural
- network
- transformer
- learning
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes NAR-Former V2, a universal neural network representation
  learning framework that can handle both cell-structured networks and entire networks.
  The core idea is to combine the strengths of Transformer and graph neural networks
  (GNNs) by incorporating graph-specific properties into the vanilla Transformer and
  introducing a graph-aided attention-based Transformer block.
---

# NAR-Former V2: Rethinking Transformer for Universal Neural Network Representation Learning

## Quick Facts
- arXiv ID: 2306.10792
- Source URL: https://arxiv.org/abs/2306.10792
- Reference count: 36
- Key outcome: NAR-Former V2 achieves significant improvements in latency prediction on NNLQP dataset compared to state-of-the-art GNN-based method NNLP, with highly comparable performance to other methods in accuracy prediction on NASBench101 and NASBench201

## Executive Summary
NAR-Former V2 proposes a universal neural network representation learning framework that combines the strengths of Transformers and graph neural networks. By incorporating graph-specific properties into the vanilla Transformer and introducing a graph-aided attention-based Transformer block, the framework inherits the flexibility of self-attention while benefiting from the good generalization capability of graph aggregation. NAR-Former V2 demonstrates significant improvements in latency prediction and highly comparable performance in accuracy prediction, while showing good scalability for encoding networks with varying complexities.

## Method Summary
NAR-Former V2 encodes neural networks as graphs with node information matrices and adjacency matrices, then tokenizes them into sequences. The core architecture consists of a tokenizer followed by K graph-aided attention Transformer blocks and a prediction head. The model incorporates graph-aided attention with type-aware enhancement and grouped feed-forward networks, trained using Adam optimizer with linear learning rate decay and warm-up. The approach handles both cell-structured networks and entire networks through different encoding schemes.

## Key Results
- Achieves significant improvements in latency prediction on NNLQP dataset compared to state-of-the-art GNN-based method NNLP
- Obtains highly comparable performance to other methods in accuracy prediction on NASBench101 and NASBench201 datasets
- Demonstrates good scalability, encoding networks with only a few operations or complete networks with hundreds of operations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The graph-aided attention module addresses Transformer's overfitting to training data by constraining self-attention with graph structure
- Mechanism: By replacing softmax with linear attention and applying the adjacency matrix as a mask, the model prevents distant tokens from influencing each other too strongly, preserving local graph topology
- Core assumption: Graph structure provides sufficient inductive bias to prevent over-reliance on learned attention weights
- Evidence anchors:
  - [abstract]: "We incorporate the inductive representation learning capability of GNN into Transformer, enabling Transformer to generalize better when encountering unseen architecture"
  - [section 3.2]: "we employ the adjacency matrix to govern the attention calculation range"
  - [corpus]: Weak evidence - no direct citations about adjacency-based attention constraining overfitting
- Break condition: If the graph structure itself contains noise or irrelevant connections, the constrained attention may learn suboptimal representations

### Mechanism 2
- Claim: Type-aware enhancement module improves representation learning by incorporating node degree information
- Mechanism: The sigmoid-activated degree vector modulates the hidden state, allowing the model to account for how connected each node is when processing its representation
- Core assumption: The number of connections per node is a useful signal for distinguishing operation types in neural networks
- Evidence anchors:
  - [section 3.2]: "The connection between a layer and other layers is related to the type of that layer"
  - [section 3.2]: "the number of connected layers in each layer can be used to assist the model in learning the type of layer"
  - [corpus]: No direct evidence - this is an original architectural contribution
- Break condition: If node degree is not actually indicative of operation type (e.g., in highly regular architectures), this enhancement may add noise

### Mechanism 3
- Claim: Grouped feed-forward network (GFFN) reduces model complexity while maintaining performance
- Mechanism: By introducing grouped linear transformations, the FFN component's parameter count is reduced by a factor equal to the number of groups
- Core assumption: The original FFN's full channel mixing is not essential for the representation learning task
- Evidence anchors:
  - [section 3.2]: "We introduce Grouped Feed Forward Network (GFFN) by introducing group linear transformation into the original FFN"
  - [section 4.4]: "the number of model parameters reduces to approximately one eighth of that in Row (4)"
  - [corpus]: No direct evidence - parameter reduction through grouping is a common technique
- Break condition: If the representation learning task requires complex inter-channel interactions, grouping may bottleneck information flow

## Foundational Learning

- Graph neural networks
  - Why needed here: Understanding how GNNs aggregate neighbor information provides context for why constraining attention with adjacency matrices is beneficial
  - Quick check question: What is the key difference between self-attention and graph aggregation in terms of information flow?

- Neural network architecture representation
  - Why needed here: Understanding how neural networks can be encoded as graphs is fundamental to grasping the problem NAR-Former V2 addresses
  - Quick check question: How does the paper encode a neural network as a sequence of tokens for Transformer processing?

- Transformer attention mechanisms
  - Why needed here: Understanding vanilla Transformer attention is essential to appreciate how graph-aided attention modifies it
  - Quick check question: What is the role of the softmax operation in standard self-attention, and why is it replaced in this work?

## Architecture Onboarding

- Component map: Tokenizer → Graph-aided Attention Transformer Block (x K) → Prediction Head
- Critical path: Network encoding → Graph-aided attention processing → Attribute prediction
- Design tradeoffs: Global vs. local attention (sensitivity vs. generalization), parameter efficiency (GFFN) vs. representation capacity, explicit topology encoding (adjacency matrix) vs. learned topology
- Failure signatures: Poor generalization to unseen architectures (suggests attention constraints are too weak), over-sensitivity to small changes (suggests attention constraints are too strong), high parameter count without performance gain (suggests GFFN grouping is too aggressive)
- First 3 experiments:
  1. Compare MAPE on NNLQP with and without graph-aided attention on unseen network types
  2. Evaluate Kendall's Tau on NAS-Bench-101 with varying amounts of training data
  3. Ablation study comparing one-hot vs. learned operation embeddings for cell-structured networks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed graph-aided attention mechanism in NAR-Former V2 affect the generalization capability of the model when handling unseen neural network architectures?
- Basis in paper: [explicit] The paper mentions that the graph-aided attention module is designed to enhance the model's ability to learn effective representations and achieve more accurate predictions compared to using the original GNN.
- Why unresolved: The paper does not provide a detailed analysis of the impact of the graph-aided attention mechanism on the model's generalization capability, especially when dealing with unseen architectures.
- What evidence would resolve it: Conducting experiments to evaluate the model's performance on a diverse set of unseen neural network architectures and comparing it with the original NAR-Former and other state-of-the-art methods would provide evidence for the effectiveness of the graph-aided attention mechanism.

### Open Question 2
- Question: What is the impact of using different encoding schemes for operation types in NAR-Former V2, and how does it affect the model's performance on various neural network datasets?
- Basis in paper: [explicit] The paper mentions that there are slight differences in the specific encoding content for input networks of different scales, with cell architectures using category labels and complete DNNs using one-hot vectors concatenated with property encodings.
- Why unresolved: The paper does not provide a detailed analysis of the impact of using different encoding schemes for operation types on the model's performance across various neural network datasets.
- What evidence would resolve it: Conducting experiments to evaluate the model's performance using different encoding schemes for operation types on various neural network datasets would provide insights into the impact of encoding choices on the model's performance.

### Open Question 3
- Question: How does the proposed NAR-Former V2 framework compare to other state-of-the-art methods in terms of computational efficiency and scalability when handling large-scale neural network architectures?
- Basis in paper: [inferred] The paper mentions that NAR-Former V2 can handle both cell-structured networks and entire networks, and it achieves significant improvements in latency prediction on the NNLQP dataset compared to the state-of-the-art GNN-based method NNLP.
- Why unresolved: The paper does not provide a detailed comparison of the computational efficiency and scalability of NAR-Former V2 with other state-of-the-art methods when handling large-scale neural network architectures.
- What evidence would resolve it: Conducting experiments to evaluate the computational efficiency and scalability of NAR-Former V2 and comparing it with other state-of-the-art methods on large-scale neural network architectures would provide evidence for its efficiency and scalability.

## Limitations

- The evaluation scope is limited to specific benchmark datasets, requiring validation on more diverse network architectures for universal applicability claims
- Scalability claims regarding encoding networks with hundreds of operations are demonstrated but not stress-tested with extremely large architectures
- The impact of graph-aided attention on generalization capability for unseen architectures is not thoroughly analyzed

## Confidence

- **High Confidence:** The architectural modifications (graph-aided attention, type-aware enhancement, GFFN) are technically sound and the performance improvements on NNLQP are statistically significant
- **Medium Confidence:** The claim of "universal" representation learning is supported but not fully proven, as the evaluation covers a limited architectural space
- **Low Confidence:** The scalability claims regarding encoding networks with hundreds of operations are demonstrated but not stress-tested with extremely large architectures

## Next Checks

1. **Generalization Stress Test:** Evaluate NAR-Former V2 on neural architectures from diverse domains (NLP, vision, graph neural networks) not represented in the training data to validate true universal representation capabilities

2. **Architecture Size Scaling:** Systematically test the model's performance and computational efficiency on networks with varying depths (5 to 500+ layers) to verify the claimed scalability limits

3. **Ablation on Graph Constraints:** Perform controlled experiments disabling the adjacency matrix masking to quantify exactly how much the graph-aided attention contributes to preventing overfitting versus the baseline Transformer architecture