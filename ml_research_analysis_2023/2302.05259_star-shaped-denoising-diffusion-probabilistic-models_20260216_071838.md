---
ver: rpa2
title: Star-Shaped Denoising Diffusion Probabilistic Models
arxiv_id: '2302.05259'
source_url: https://arxiv.org/abs/2302.05259
tags:
- ddpm
- uni00000013
- diffusion
- process
- ss-ddpm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Star-Shaped Denoising Diffusion Probabilistic
  Models (SS-DDPM), which extend diffusion models beyond Gaussian noise by defining
  a non-Markovian forward process that only requires marginal distributions at each
  step. The key insight is establishing a duality between star-shaped and Markovian
  diffusions for exponential family distributions, allowing the use of distributions
  like Beta, von Mises-Fisher, Dirichlet, and Wishart.
---

# Star-Shaped Denoising Diffusion Probabilistic Models

## Quick Facts
- arXiv ID: 2302.05259
- Source URL: https://arxiv.org/abs/2302.05259
- Reference count: 40
- Primary result: Achieves FID 3.17 on CIFAR-10 using Beta diffusion, comparable to Gaussian DDPM

## Executive Summary
This paper introduces Star-Shaped Denoising Diffusion Probabilistic Models (SS-DDPM), which extend diffusion models beyond Gaussian noise by defining a non-Markovian forward process that only requires marginal distributions at each step. The key insight is establishing a duality between star-shaped and Markovian diffusions for exponential family distributions, allowing the use of distributions like Beta, von Mises-Fisher, Dirichlet, and Wishart. SS-DDPM achieves results comparable to Gaussian DDPM on CIFAR-10 (FID 3.17) and demonstrates effectiveness on constrained manifolds including unit spheres, probabilistic simplices, and positive definite matrices. The method provides a simple recipe for designing diffusion models with arbitrary noise distributions while maintaining efficient training and sampling algorithms.

## Method Summary
SS-DDPM extends traditional DDPMs by replacing the Markovian forward process with a star-shaped diffusion that only requires marginal distributions at each step. Instead of defining transition distributions q(xt|xt-1), the model uses q(xt|x0) from exponential family distributions with linear parameterization of natural parameters. The tail statistic Gt = ∑AsT(xs) serves as a sufficient statistic for the entire tail xt:T, enabling efficient sampling and training. The reverse process is trained to predict x0 from Gt using a standard U-Net architecture. The key innovation is the duality theorem showing that for exponential family distributions with linear parameterization, the star-shaped process is equivalent to a Markovian DDPM in the space of tail statistics.

## Key Results
- Achieves FID 3.17 on CIFAR-10, comparable to standard Gaussian DDPM
- Demonstrates successful generation on constrained manifolds (unit spheres, simplices, positive definite matrices)
- Shows theoretical equivalence between star-shaped and Markovian diffusion for exponential families
- Provides efficient sampling algorithms requiring only marginal distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Star-shaped diffusion bypasses the need to define transition distributions between steps
- Mechanism: Instead of defining q(xt|xt-1) for each step, the model only requires marginal distributions q(xt|x0), which can be chosen from any exponential family distribution
- Core assumption: The exponential family distributions can be parameterized such that the natural parameter ηt(x0) = Atf(x0) + bt is linear in f(x0)
- Evidence anchors:
  - [abstract] "Its star-shaped diffusion process allows us to bypass the need to define the transition probabilities or compute posteriors"
  - [section 2.3] "In SS-DDPM, one only needs to define marginal distributions at each diffusion step"
- Break condition: If the distribution cannot be expressed with linear parameterization of the natural parameter

### Mechanism 2
- Claim: The tail statistic Gt acts as a sufficient statistic for the entire tail xt:T
- Mechanism: Theorem 1 establishes that when distributions are in exponential families with linear parameterization, the sum of transformed observations ∑AsT(xs) captures all information about x0 from the tail
- Core assumption: The Pitman-Koopman-Darmois theorem applies to this non-i.i.d. setting, allowing a fixed-dimensional sufficient statistic
- Evidence anchors:
  - [section 2.5] "Inspired by the PKD, we turn to the exponential family of distributions" and Theorem 1 statement
- Break condition: If the dimensionality of Gt grows with tail size T-t+1

### Mechanism 3
- Claim: Star-shaped diffusion is equivalent to DDPM for Gaussian distributions
- Mechanism: The tail statistic Gt follows a Markovian Gaussian DDPM process, making the two models mathematically equivalent in this case
- Core assumption: The noising schedule αSS can be derived from DDPM's schedule via equation (24)
- Evidence anchors:
  - [section 2.7] "In the case of Gaussian distributions, this model is equivalent to Markovian DDPMs"
  - [section 2.7] Theorem 2 states the equivalence and provides the schedule transformation
- Break condition: If the schedule transformation doesn't preserve the mutual information between x0 and the tail statistics

## Foundational Learning

- Concept: Exponential family distributions
  - Why needed here: The model relies on distributions that admit sufficient statistics with constant dimensionality
  - Quick check question: What is the natural parameter and sufficient statistic for the Beta distribution?

- Concept: Variational lower bound (VLB)
  - Why needed here: The training objective is derived from the VLB for the reverse process
  - Quick check question: How does the VLB change when moving from Markovian to star-shaped forward processes?

- Concept: Mutual information as noise level metric
  - Why needed here: The paper uses I(x0; Gt) to match noise schedules between different distributions
  - Quick check question: Why is matching mutual information between star-shaped and DDPM processes important for schedule design?

## Architecture Onboarding

- Component map:
  Forward process (qSS(xt|x0)) → Tail statistic computation (Gt = ∑AsT(xs)) → Reverse process prediction (xθ(Gt, t)) → Loss calculation → Gradient update

- Critical path: qSS(xt|x0) → Gt computation → xθ(Gt, t) prediction → loss calculation → gradient update

- Design tradeoffs:
  - Choosing distributions with simple tail statistics vs. expressive power
  - Number of diffusion steps T vs. computational cost
  - Time-dependent normalization of Gt vs. network stability

- Failure signatures:
  - Poor image quality suggests issues with schedule matching or tail statistic normalization
  - Training instability indicates problems with the natural parameter linearization
  - Mode collapse in generated samples points to inadequate reverse process approximation

- First 3 experiments:
  1. Implement Beta diffusion on synthetic data in [0,1] to verify the basic framework works
  2. Compare Gaussian SS-DDPM with standard DDPM to validate the equivalence claim
  3. Test Dirichlet diffusion on probabilistic simplex to explore constrained manifold generation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the duality between star-shaped and Markovian diffusion processes extend beyond the exponential family with linear parameterization described in Theorem 1?
- Basis in paper: The paper states "We suspect that, just like in PKD, this trick is only possible for a subset of exponential family" and notes that in general cases the dimensionality of Gt would grow with the tail size
- Why unresolved: The paper only proves Theorem 1 for a specific subset of exponential families and explicitly states this is an open problem
- What evidence would resolve it: A proof showing whether the duality extends to broader classes of distributions or a counterexample demonstrating its limitations

### Open Question 2
- Question: Can the explicit description and analysis of the dual Markov diffusion process in the space of tail statistics be derived for general distributions?
- Basis in paper: The conclusion states "The explicit description and analysis of this process remains an interesting open problem"
- Why unresolved: While the paper establishes duality for specific cases, it notes that "In the general case, the explicit description of Markov diffusion process for Gt is problematic"
- What evidence would resolve it: A mathematical framework describing the general diffusion process for tail statistics Gt across different distribution families

### Open Question 3
- Question: How can the number of function evaluations in SS-DDPM be reduced without the approximation error introduced by sampling intermediate variables?
- Basis in paper: The paper discusses a method to reduce steps by sampling intermediate variables (Section 3), but notes this is an approximation and compares it to DDPM's approach
- Why unresolved: The approximation method described may introduce bias, and the paper states "In general case it amounts to approximating the reverse process with a different reverse process"
- What evidence would resolve it: A method that achieves computational efficiency comparable to DDPM's step-skipping while maintaining SS-DDPM's theoretical guarantees, validated through experiments showing no degradation in sample quality

## Limitations
- Restricted to exponential family distributions with linear parameterization of natural parameters
- Requires careful matching of mutual information schedules between different distributions
- Theoretical guarantees may not hold perfectly in practice for complex high-dimensional data

## Confidence

- **High Confidence:** The mathematical framework for star-shaped diffusion and its equivalence to DDPM for Gaussian distributions is rigorously proven.
- **Medium Confidence:** The empirical results on CIFAR-10 and constrained manifolds are promising but limited to a few distribution choices.
- **Low Confidence:** The scalability to very high-dimensional data and the robustness across diverse distribution families remain to be thoroughly validated.

## Next Checks

1. **Schedule Robustness Test:** Systematically vary the mutual information matching procedure across different distribution families to assess sensitivity and identify failure modes.

2. **High-Dimensional Scaling:** Evaluate SS-DDPM on higher-resolution image datasets (e.g., CelebA-HQ 256x256) to test scalability beyond CIFAR-10.

3. **Distribution Family Generalization:** Implement SS-DDPM with additional exponential family distributions (e.g., Gamma, Inverse Gaussian) to validate the framework's generality across different constrained spaces.