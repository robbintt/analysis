---
ver: rpa2
title: 'QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models'
arxiv_id: '2310.16795'
source_url: https://arxiv.org/abs/2310.16795
tags:
- compression
- arxiv
- quantization
- which
- only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents QMoE, a practical compression and inference
  framework for trillion-parameter Mixture-of-Experts (MoE) models like the SwitchTransformer-c2048.
  The core method idea is to apply data-dependent quantization (specifically GPTQ)
  combined with a custom entropy-based encoding scheme and optimized GPU kernels,
  enabling accurate compression to less than 1 bit per parameter.
---

# QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models

## Quick Facts
- arXiv ID: 2310.16795
- Source URL: https://arxiv.org/abs/2310.16795
- Authors: [Not specified in input]
- Reference count: 10
- One-line primary result: Compresses 1.6T-parameter SwitchTransformer-c2048 from 3.2TB to <160GB (20x, 0.8 bits/parameter) with <5% runtime overhead

## Executive Summary
This paper introduces QMoE, a practical compression framework for trillion-parameter Mixture-of-Experts (MoE) models. By combining data-dependent quantization (GPTQ) with entropy-based encoding and optimized GPU kernels, QMoE achieves sub-1-bit compression for models like SwitchTransformer-c2048 while maintaining reasonable accuracy. The approach enables execution of trillion-parameter models on affordable commodity hardware with minimal runtime overhead.

## Method Summary
QMoE addresses the challenge of compressing and enabling efficient inference for extremely large MoE language models. The method combines data-dependent quantization (specifically GPTQ) with custom entropy-based encoding and optimized GPU kernels. The compression pipeline applies GPTQ quantization followed by ternary quantization and dictionary-based entropy encoding. For inference, the system employs activation offloading, lazy weight fetching, and expert grouping to manage memory efficiently while maintaining performance.

## Key Results
- Compresses SwitchTransformer-c2048 (1.6T parameters) from 3.2TB to less than 160GB (20x compression, 0.8 bits/parameter)
- Achieves 2-bit compression at 1.7% relative accuracy loss and ternary at 6.7% relative accuracy loss
- Enables trillion-parameter model inference on 4x NVIDIA A6000 or 8x NVIDIA 3090 GPUs
- Runtime overhead of less than 5% relative to uncompressed inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data-dependent quantization (like GPTQ) can achieve sub-1-bit compression for MoE models without significant accuracy loss.
- Mechanism: By leveraging calibration data and optimizing per-layer quantization, GPTQ finds near-optimal low-bit representations while minimizing reconstruction error, exploiting MoE's natural sparsity and robustness.
- Core assumption: MoE models are inherently more robust to quantization noise than dense models due to routing randomness and large parameter counts.
- Evidence anchors: [abstract] "This enables, for the first time, the execution of a trillion-parameter model on affordable commodity hardware"; [section] "Table 5... 2-bit is achievable at minimal loss (1.7% relative on c2048) and ternary at only a small increase (6.7% relative on c2048)"

### Mechanism 2
- Claim: Custom entropy-based encoding combined with GPU-optimized kernels enables practical sub-1-bit storage and inference.
- Mechanism: High natural sparsity (zeros) in quantized weights is exploited using dictionary-based encoding, mapping frequent patterns to fixed-length codewords, allowing fast GPU decoding while achieving <1 bit/parameter storage.
- Core assumption: Ternary quantized weights have sufficiently low entropy (high zero proportion) to benefit from entropy coding without losing GPU decoding efficiency.
- Evidence anchors: [section] "Table 4... For ternary weights, the largest model achieves close to 90% natural sparsity"; [section] "This format dictates our maximum chosen pair count of 14"

### Mechanism 3
- Claim: Scalable system optimizations (activation offloading, lazy weight fetching, expert grouping) make trillion-parameter compression feasible on modest hardware.
- Mechanism: Offloading intermediate activations to CPU RAM and fetching weights on-demand avoids GPU memory overflow, while batching expert compression improves GPU utilization and throughput.
- Core assumption: The overhead of CPU-GPU data transfer is outweighed by the memory savings and batching gains for large MoE models.
- Evidence anchors: [section] "Concretely, we reduce the size of SwitchTransformer-c2048... from 3.2TB in bfloat16 to less than 160GB"; [section] "Table 8... Even c2048 in less than a day"

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: Understanding MoE's sparse routing and parameter distribution is key to why quantization can be applied selectively and why compression is effective.
  - Quick check question: Why does an MoE with 1.6T parameters only require 3.2TB of storage in half-precision?

- Concept: Data-dependent quantization (e.g., GPTQ)
  - Why needed here: GPTQ's layer-wise optimization using calibration data and Hessian information is central to achieving accurate low-bit compression.
  - Quick check question: How does GPTQ use second-order information to improve quantization accuracy?

- Concept: GPU memory hierarchy and kernel optimization
  - Why needed here: Efficient decoding of compressed weights requires careful design of kernels to balance memory bandwidth, compute, and parallelism.
  - Quick check question: Why is a fixed-to-variable dictionary code preferred over variable-length entropy coding for GPU decoding?

## Architecture Onboarding

- Component map: calibration data → GPTQ quantization → ternary quantization → entropy encoding (dictionary) → lazy weight loading → activation offloading → compressed matrix-vector kernels → routing layer
- Critical path: For compression: calibration → layer-wise quantization → encoding (dominated by expert grouping); For inference: weight fetch → decompression + matmul → routing → next layer (matrix-vector product is critical)
- Design tradeoffs: Expert grouping size: larger batches improve GPU utilization but increase memory pressure; Dictionary size: larger dictionaries improve compression but risk cache misses and kernel complexity; Activation offloading granularity: finer granularity reduces memory use but increases transfer overhead
- Failure signatures: Compression fails: out-of-memory during expert processing, numerical instability in Hessian inversion; Inference fails: excessive latency from CPU-GPU transfers, kernel launch overhead dominates; Accuracy drop: insufficient calibration data, aggressive quantization causing routing errors
- First 3 experiments: 1) Run compression on a single SwitchTransformer base128 expert with varying expert group sizes (1, 4, 16) and measure GPU utilization and runtime; 2) Test dictionary-based decoding kernel on synthetic ternary matrices of different sparsity levels and measure compression ratio and kernel throughput; 3) Perform end-to-end inference with compressed c2048 model on a small prompt, compare latency and accuracy against uncompressed baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit of compression for MoE models using entropy coding, and how close can practical implementations get to this limit?
- Basis in paper: [explicit] The paper discusses achieving 20.07× compression on the c2048 model, which is close to the theoretical limit of 25.40× for a distribution with p=0.885. It states, "We are only ≈ 20% away from the distribution's (with p = 0.885) theoretical compression limit of 25.40×."
- Why unresolved: While the paper provides a comparison between the achieved compression and the theoretical limit for a specific model, it does not explore the theoretical limits for other MoE models or quantization schemes.
- What evidence would resolve it: Further theoretical analysis and empirical experiments on a wider range of MoE models and quantization schemes to determine the achievable compression rates and compare them to theoretical limits.

### Open Question 2
- Question: How does the performance of QMoE scale with the number of experts in MoE models?
- Basis in paper: [inferred] The paper focuses on models with 128 to 2048 experts, achieving compression rates of 17.06× to 20.07×. However, it does not explore the performance of QMoE on models with a significantly larger number of experts.
- Why unresolved: The paper does not provide data or analysis on the performance of QMoE for MoE models with more than 2048 experts.
- What evidence would resolve it: Empirical experiments on MoE models with a larger number of experts to determine the scalability of QMoE's compression rates and inference performance.

### Open Question 3
- Question: How does the accuracy of QMoE-compressed models compare to that of models compressed using other state-of-the-art compression techniques?
- Basis in paper: [explicit] The paper compares QMoE to round-to-nearest (RTN) quantization, showing that QMoE achieves better accuracy at low bit-widths. However, it does not compare QMoE to other advanced compression techniques like those mentioned in the related work section.
- Why unresolved: The paper does not provide a comprehensive comparison of QMoE's accuracy with other state-of-the-art compression techniques for MoE models.
- What evidence would resolve it: A comparative study of QMoE against other advanced compression techniques, such as multi-level grouping with higher-precision outliers or incoherence preprocessing, on the same MoE models and tasks.

## Limitations
- Implementation details of GPTQ for MoE models are not fully specified, including exact Hessian computation and handling of numerical instabilities
- Performance may vary significantly with different model architectures or sparsity patterns beyond SwitchTransformer
- Impact of activation offloading on end-to-end latency is not thoroughly profiled

## Confidence

- High confidence: The compression ratio claims (20x, 0.8 bits/parameter) are well-supported by experimental results and align with the proposed mechanism of leveraging natural sparsity from ternary quantization
- Medium confidence: The inference runtime claims (sub-5% overhead) are supported by end-to-end benchmarks but lack detailed profiling of individual system components like activation offloading overhead
- Low confidence: The accuracy retention claims across different quantization levels rely heavily on calibration data quality assumptions without extensive ablation studies on calibration strategies

## Next Checks

1. Profile GPU kernel performance with varying dictionary sizes to identify the optimal balance between compression ratio and decoding throughput
2. Test the QMoE framework on a different MoE architecture (e.g., GShard) to evaluate generalizability beyond SwitchTransformer
3. Conduct ablation studies on calibration data size and quality to determine the minimum requirements for maintaining accuracy at different quantization levels