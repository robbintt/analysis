---
ver: rpa2
title: Why Can Large Language Models Generate Correct Chain-of-Thoughts?
arxiv_id: '2310.13571'
source_url: https://arxiv.org/abs/2310.13571
tags:
- qtrue
- arxiv
- language
- large
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes why large language models (LLMs) can generate
  correct chain-of-thought (CoT) reasoning. It introduces a two-level hierarchical
  graphical model of natural language generation that allows for the conditional evolution
  of latent intentions.
---

# Why Can Large Language Models Generate Correct Chain-of-Thoughts?

## Quick Facts
- arXiv ID: 2310.13571
- Source URL: https://arxiv.org/abs/2310.13571
- Reference count: 33
- Key outcome: This paper analyzes why large language models (LLMs) can generate correct chain-of-thought (CoT) reasoning by introducing a hierarchical graphical model and deriving a geometric upper bound on the likelihood difference between LLM-generated and true CoT sequences.

## Executive Summary
This paper addresses why large language models can generate correct chain-of-thought reasoning by introducing a two-level hierarchical graphical model that captures the conditional evolution of latent intentions. The authors derive a geometric upper bound on the difference between the likelihood of an LLM-generated CoT and the true CoT, expressed in terms of the ambiguity of the input task and CoT examples. The theoretical framework explains the effectiveness of CoT prompting and provides conditions under which the bound converges geometrically, offering insights into the design of effective CoT examples.

## Method Summary
The paper introduces a hierarchical graphical model where context and intentions evolve conditionally to generate natural language sequences. It defines ambiguity as the complement of the likelihood of the true context and intentions given a sequence. Using this framework, the authors derive a geometric upper bound on the absolute difference between LLM-generated CoT likelihood and true CoT likelihood. The bound depends on the ambiguities of the input task and CoT examples, converging geometrically when these ambiguities are low. The analysis also extends to non-uniform context priors with a modified bound.

## Key Results
- The paper establishes a geometric convergence rate for the likelihood difference between LLM-generated and true CoT sequences.
- Low ambiguity in CoT examples and input tasks is a sufficient condition for the geometric convergence of the bound.
- LLMs can effectively infer the true reasoning context from CoT examples when the ambiguity is sufficiently low, explaining performance gains on reasoning tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can infer the true reasoning context from CoT examples due to geometric convergence in likelihood difference.
- Mechanism: The paper introduces a hierarchical graphical model where context and intentions evolve conditionally. Under uniform context priors, the difference between LLM-generated CoT likelihood and true CoT likelihood is bounded by a product of ambiguity terms, yielding geometric convergence as more examples are added.
- Core assumption: Prior distribution over contexts is uniform (Assumption 3.1), and ambiguity of CoT examples and input task is bounded.
- Evidence anchors:
  - [abstract] "Within this framework, we establish a compelling geometrical convergence rate that gauges the likelihood of an LLM-generated chain of thoughts compared to those originating from the true language."
  - [section] Theorem 3.2 provides the formal bound: |pLLM(CoT|Inp, CoT-Examples(N)) − qTrue(CoT|Inp, True-Context)| ≤ ρN where ρ < 1 is a function of language ambiguities.
  - [corpus] Weak/no direct evidence; related works discuss CoT prompting but not the geometric convergence mechanism.
- Break condition: If context priors are non-uniform or ambiguity of examples/input task is high, the bound may not converge geometrically. Lemma 4.3 provides a threshold for example length to ensure low ambiguity.

### Mechanism 2
- Claim: Ambiguity of a CoT sequence is the complement of the likelihood of the true context and intentions given the sequence.
- Mechanism: The paper defines ambiguity ǫ((xi)0≤ i≤ m) = 1 - qTrue(c*, (θ*i)0≤ i≤ m|(xi)0≤ i≤ m), extending Jiang's ambiguity definition to sequences. Low ambiguity means high certainty about the underlying reasoning context.
- Core assumption: Natural language evolved to decrease overall ambiguity, making low-ambiguity CoT examples attainable.
- Evidence anchors:
  - [section] "we express the ambiguity of the chain ǫ((xi)0≤ i≤ m) as the complement of the likelihood of the context c* and intentions (θ*i)0≤ i≤ m conditioned on (xi)0≤ i≤ m"
  - [section] Condition 4.1 and Lemma 4.3 formalize requirements for low ambiguity CoT examples to guarantee vanishing bound.
  - [corpus] No direct evidence; related works focus on CoT effectiveness but not ambiguity formalization.
- Break condition: If CoT examples have high ambiguity (e.g., unrelated or noisy examples), the bound will not converge, and LLM may infer incorrect context.

### Mechanism 3
- Claim: LLMs act as marginal approximators of natural language, enabling them to approximate true CoT distribution when conditioned on examples.
- Mechanism: The paper leverages Jiang's result that LLM pretraining captures marginal distribution of natural language. When prompted with CoT examples, the LLM can approximate qTrue((xr)1≤ r≤ m|x0, c*) by conditioning on the examples and input.
- Core assumption: LLM pretraining allows it to approximate any continuous sequence-to-sequence function with arbitrary precision (Yun et al., 2020).
- Evidence anchors:
  - [section] "Jiang (2023) established that if pLLM,n maximises the empirical log-likelihood function... then the marginal distribution pLLM,n (X) converges to qTrue(X) for all sequences X as n goes to infinity."
  - [section] "we can analyse the empirical successes of the latest LLMs which benefitted from a drastic increase in terms of model and training data sizes."
  - [corpus] Weak evidence; related works discuss LLM capabilities but not the marginal approximation mechanism.
- Break condition: If LLM pretraining is insufficient (e.g., small model/data) or the task distribution is too far from pretraining data, the approximation may fail.

## Foundational Learning

- Concept: Hierarchical graphical models with latent variables
  - Why needed here: To model the generation of CoTs as a two-level process with context and intentions, allowing for conditional evolution of intentions.
  - Quick check question: Can you explain how the hierarchical structure in Figure 1 captures the relevance and coherence of CoT messages?

- Concept: Bayesian inference and marginalization
  - Why needed here: To understand how LLMs can infer the true reasoning context from CoT examples by approximating the true language's marginal distribution.
  - Quick check question: How does the LLM's ability to approximate the marginal distribution relate to its performance on reasoning tasks?

- Concept: Ambiguity and uncertainty in language
  - Why needed here: To formalize the challenge of inferring the true context from ambiguous messages and sequences, and to derive bounds on the LLM's approximation error.
  - Quick check question: Why is low ambiguity a sufficient condition for the geometric convergence of the bound in Theorem 3.2?

## Architecture Onboarding

- Component map:
  - Graphical model (Figure 1) -> LLM (pLLM) -> CoT examples (Z k) -> Input task (x0) -> Ambiguity measures (ǫ)

- Critical path:
  1. Generate CoT examples with known context and intentions.
  2. Provide input task and CoT examples to the LLM.
  3. LLM approximates the true CoT distribution conditioned on examples and input.
  4. LLM generates a CoT sequence that aids in solving the task.

- Design tradeoffs:
  - Uniform vs. non-uniform context priors: Uniform priors simplify the bound but may not reflect real-world data distributions.
  - Low vs. asymptotic ambiguity: Low ambiguity guarantees convergence but may be hard to achieve; asymptotic ambiguity allows for more flexibility but requires longer examples.
  - Number of CoT examples: More examples improve context inference but increase computational cost.

- Failure signatures:
  - High ambiguity in CoT examples or input task: LLM may infer incorrect context, leading to irrelevant or incoherent CoTs.
  - Non-uniform context priors with skewed data: LLM may overfit to dominant contexts, failing on rare ones.
  - Insufficient LLM capacity or pretraining: LLM may not approximate the true marginal distribution accurately.

- First 3 experiments:
  1. Vary the number of CoT examples (N) and measure the LLM's performance on reasoning tasks. Expect geometric improvement as N increases if ambiguity is low.
  2. Compare uniform vs. non-uniform context priors by skewing the data distribution. Expect worse performance with non-uniform priors if skewness is high.
  3. Test different lengths of CoT examples to find the threshold for asymptotic ambiguity (m*k,δ). Expect performance to improve as example length increases past the threshold.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we empirically validate the theoretical bounds on the difference between LLM-generated CoTs and true CoTs under varying levels of ambiguity and context priors?
- Basis in paper: Explicit - The paper derives theoretical bounds on the difference between LLM-generated CoTs and true CoTs, but calls for experimental validation in the "Future Work" section.
- Why unresolved: While the paper provides theoretical justification, it lacks empirical evidence to support the claims about the effectiveness of CoT prompting under different conditions.
- What evidence would resolve it: Conducting experiments with different LLMs, CoT examples, and input tasks to measure the actual difference between generated and true CoTs, and comparing the results with the theoretical bounds.

### Open Question 2
- Question: How can we develop a rigorous procedure to measure the ambiguity of a given sequence of thoughts or CoT examples?
- Basis in paper: Inferred - The paper discusses the importance of ambiguity in CoT examples and introduces ambiguity measures, but acknowledges the difficulty in measuring ambiguity in practice.
- Why unresolved: The paper does not provide a concrete method for quantifying the ambiguity of CoT examples, which is crucial for applying the theoretical results and designing effective CoT prompts.
- What evidence would resolve it: Proposing and validating a method for quantifying the ambiguity of CoT examples, possibly through human evaluation or by analyzing the model's confidence in generating CoTs given different examples.

### Open Question 3
- Question: How does the non-uniformity of context priors affect the effectiveness of CoT prompting, and how can we account for it in the theoretical framework?
- Basis in paper: Explicit - The paper introduces a modified bound for non-uniform context priors but does not explore the implications or provide a comprehensive analysis.
- Why unresolved: The impact of non-uniform context priors on CoT effectiveness is not fully understood, and the modified bound may not capture all the nuances of real-world scenarios where context priors are often skewed.
- What evidence would resolve it: Analyzing the effect of different types of context prior distributions on CoT effectiveness, both theoretically and empirically, and refining the theoretical framework to better account for non-uniform priors.

### Open Question 4
- Question: How can we extend the analysis to cover more complex prompting techniques, such as graph of thoughts or tree of thoughts?
- Basis in paper: Explicit - The paper mentions future work on generalizing the analysis to cover recent discoveries in smart prompting, including graph and tree of thoughts.
- Why unresolved: The current theoretical framework focuses on chain-of-thought prompting, and it is unclear how it can be extended to more complex prompting techniques that involve multiple reasoning paths or hierarchical structures.
- What evidence would resolve it: Developing a generalized theoretical framework that can accommodate various prompting techniques, including graph and tree of thoughts, and validating the framework through empirical studies and comparisons with existing methods.

## Limitations

- The uniform context prior assumption may not hold in practice, limiting the applicability of the derived bounds.
- The paper lacks empirical validation of the theoretical bounds and thresholds for example length and ambiguity.
- The analysis does not account for the complexities of real-world data distributions and LLM architectures.

## Confidence

**High Confidence**: The hierarchical graphical model framework and basic definition of ambiguity are well-founded and mathematically rigorous. The relationship between ambiguity and likelihood difference is clearly established.

**Medium Confidence**: The geometric convergence claim holds under the stated assumptions, but the practical conditions required for this convergence are stringent and may not be met in real-world applications. The marginal approximation mechanism is theoretically sound but relies on idealized pretraining conditions.

**Low Confidence**: The practical applicability of the derived bounds, particularly the non-uniform prior case, remains uncertain without empirical validation. The paper provides theoretical guarantees but doesn't demonstrate these bounds on actual LLM implementations.

## Next Checks

1. **Empirical Bound Verification**: Test the derived upper bound on actual LLM implementations across different model sizes and pretraining regimes to verify geometric convergence under varying ambiguity conditions.

2. **Non-Uniform Prior Analysis**: Systematically evaluate LLM performance when CoT examples follow non-uniform context distributions, measuring the gap between theoretical predictions and actual performance.

3. **Ambiguity Threshold Testing**: Experimentally determine the minimum example length required to achieve asymptotic ambiguity across different task types and CoT formats, validating the theoretical thresholds proposed in Lemma 4.3.