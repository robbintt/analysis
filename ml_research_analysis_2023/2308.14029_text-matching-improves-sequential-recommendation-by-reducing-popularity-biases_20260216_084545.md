---
ver: rpa2
title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases
arxiv_id: '2308.14029'
source_url: https://arxiv.org/abs/2308.14029
tags:
- items
- recommendation
- item
- taste
- user-item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TASTE, a sequential recommendation model
  that uses text matching to address popularity bias. It represents users and items
  as text and matches their representations using pretrained language models.
---

# Text Matching Improves Sequential Recommendation by Reducing Popularity Biases

## Quick Facts
- **arXiv ID**: 2308.14029
- **Source URL**: https://arxiv.org/abs/2308.14029
- **Reference count**: 40
- **Primary result**: TASTE outperforms state-of-the-art methods by over 18% in sequential recommendation tasks.

## Executive Summary
This paper introduces TASTE, a sequential recommendation model that uses text matching to address popularity bias in recommendation systems. TASTE represents users and items as text and matches their representations using pretrained language models, specifically T5. The model employs an attention sparsity method to efficiently encode long user-item interaction sequences. Experiments on Yelp and Amazon Product datasets show that TASTE significantly outperforms existing methods, achieving over 18% improvements in recommendation accuracy while alleviating the cold start problem and reducing popularity bias.

## Method Summary
TASTE verbalizes items and user-item interactions using prompts that incorporate item attributes like names, addresses, and categories. These text representations are encoded using T5-Encoder, with an attention sparsity method that splits long sequences into independent sessions to reduce computational complexity. The session representations are concatenated and fed to T5-Decoder for final representation. The model is trained using contrastive learning with in-batch negatives and randomly sampled negatives, optimizing dot-product ranking scores. This approach enables TASTE to model longer user-item interaction sequences while maintaining computational efficiency.

## Key Results
- TASTE achieves over 18% improvements in recommendation accuracy compared to state-of-the-art methods
- The model effectively alleviates the cold start problem by representing long-tail items using full-text modeling
- TASTE reduces popularity bias and returns more text-relevant items rather than just popular ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Representing items and users as text with full attributes reduces popularity bias by moving from a sparse ID-based embedding space to a denser, semantically rich text-matching space.
- Mechanism: By verbalizing items and users into natural language prompts and using pretrained language models to encode these texts, TASTE aligns long-tail and popular items in a shared semantic space rather than separating them into popularity-driven clusters.
- Core assumption: Text descriptions of items contain sufficient discriminative signals for accurate matching with user preferences, and pretrained language models can effectively map these signals into useful embeddings.
- Evidence anchors:
  - [abstract] "TASTE alleviates the cold start problem by representing long-tail items using full-text modeling and bringing the benefits of pretrained language models to recommendation systems."
  - [section 2] "Yuan et al. [65] confirm that id-based recommendation is usually effective when the user-item training signal is sufficient and modality-based recommendation can alleviate the cold start problem."
  - [corpus] Weak: Related papers focus on negative sampling and attribute-aware matching but do not directly confirm semantic space mixing.
- Break condition: If item attributes lack sufficient diversity or contain too much noise, text matching may fail to distinguish user preferences, and the popularity bias reduction effect will diminish.

### Mechanism 2
- Claim: The attention sparsity method allows TASTE to model longer user-item interaction sequences without hitting transformer length limits, improving recommendation accuracy.
- Mechanism: User-item interaction history is split into independent sessions, each encoded separately using T5-Encoder. The resulting session representations are concatenated and fed to T5-Decoder, which uses cross-attention to reweight sessions.
- Core assumption: User preferences can be effectively modeled by aggregating independent session representations without losing inter-session dependencies.
- Evidence anchors:
  - [abstract] "TASTE additionally proposes an attention sparsity method, which enables TASTE to model longer user-item interactions by reducing the self-attention computations during encoding."
  - [section 3.2] "This approach also facilitates the encoding of longer text sequences of user-item interactions."
  - [corpus] Weak: Related work discusses negative sampling and disentangled reasoning but does not directly validate attention sparsity for sequential recommendation.
- Break condition: If user behavior depends heavily on cross-session dependencies, splitting into independent sessions may degrade performance, especially for users with complex interaction patterns.

### Mechanism 3
- Claim: Using prompts and in-batch negatives with random sampling provides stable gradient signals and improves convergence for text-based sequential recommendation.
- Mechanism: TASTE verbalizes interactions and items into templates, feeding them into T5 for encoding. During training, it uses in-batch negatives plus randomly sampled negatives to contrastively optimize the dot product ranking.
- Core assumption: Randomly sampled negatives are sufficiently diverse and informative to prevent model collapse in the dense text space.
- Evidence anchors:
  - [section 3.1] "We use in-batch negatives and randomly sampled negatives [14, 23, 50] to contrastively train models."
  - [section 5.2] "TASTE (Rand Negs) outperforms TASTE (Inbatch) while showing less effectiveness than TASTE (Rand Negs)."
  - [corpus] Weak: Related work on negative sampling exists but does not specifically address text-based recommendation settings.
- Break condition: If the random sampling pool becomes too small or biased, negatives may become uninformative, leading to vanishing gradients or poor generalization.

## Foundational Learning

- Concept: Transformer self-attention and the quadratic complexity bottleneck.
  - Why needed here: TASTE must process long user-item interaction sequences; without attention sparsity, memory and compute requirements explode.
  - Quick check question: What is the time complexity of standard self-attention over n tokens, and how does the attention sparsity method change it?
- Concept: Contrastive learning with in-batch and random negatives.
  - Why needed here: TASTE uses a dot-product ranking loss; understanding how negative sampling shapes gradients is essential for tuning performance.
  - Quick check question: How do in-batch negatives differ from randomly sampled negatives in terms of diversity and training stability?
- Concept: Prompt engineering for language models in non-NLP tasks.
  - Why needed here: TASTE verbalizes items and interactions into structured prompts; effective prompt design directly impacts text-matching quality.
  - Quick check question: What role does the prompt template play in guiding the language model to capture user-item relevance?

## Architecture Onboarding

- Component map: Item/attribute verbalizer → T5 encoder (T5-Encoder) → session representations → T5 decoder (T5-Decoder) → dot-product ranking → loss with in-batch + random negatives
- Critical path:
  1. Verbalize user-item history and candidate items into text prompts
  2. Encode via T5-Encoder into session embeddings
  3. Concatenate session embeddings and feed to T5-Decoder for final representation
  4. Compute dot-product relevance scores and apply contrastive loss
- Design tradeoffs:
  - Text-based modeling improves cold-start handling but increases sequence length and computational cost
  - Attention sparsity reduces compute but may lose cross-session dependencies
  - Using full attributes increases semantic richness but risks noise and overfitting
- Failure signatures:
  - Performance plateaus or degrades with longer sequences → attention sparsity too aggressive
  - Model overfits to popular items → text representation insufficiently discriminative
  - Training diverges → negative sampling too weak or prompts poorly formed
- First 3 experiments:
  1. Compare TASTE with and without attention sparsity on sequence length scaling
  2. Ablation of random negatives vs. only in-batch negatives on convergence stability
  3. Prompt ablation: compare attribute-only vs. attribute+identifier vs. identifier-only verbalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the attention sparsity method scale to extremely long user-item interaction sequences (e.g., thousands of items) in terms of both computational efficiency and recommendation quality?
- Basis in paper: [explicit] The paper mentions that the attention sparsity method reduces self-attention computations and allows modeling longer sequences, but does not provide results for extremely long sequences.
- Why unresolved: The paper only tests sequences split into a few subsequences (up to 4), leaving open the question of performance at much larger scales.
- What evidence would resolve it: Experiments showing performance and efficiency metrics (e.g., memory usage, inference time, recall/NDCG) for attention sparsity methods applied to sequences of 1000+ items.

### Open Question 2
- Question: How do different types of item attributes (e.g., categorical vs. textual attributes) affect the recommendation performance of TASTE, and is there an optimal way to incorporate them into the prompt template?
- Basis in paper: [explicit] The paper explores the effectiveness of different attributes like names, addresses, and categories, finding that names and addresses work better than categories, but does not provide a comprehensive analysis of attribute types.
- Why unresolved: The paper only tests a limited set of attributes on the Yelp dataset and does not explore the impact of different attribute types or their optimal integration into the prompt.
- What evidence would resolve it: Systematic experiments comparing the performance of TASTE with different attribute types (e.g., categorical, textual, numerical) and their various combinations in the prompt template.

### Open Question 3
- Question: How does TASTE perform in cold-start scenarios where user-item interactions are extremely sparse or non-existent, and can it effectively leverage item attributes to make recommendations?
- Basis in paper: [explicit] The paper claims that TASTE alleviates the cold-start problem by representing long-tail items using full-text modeling, but does not provide a detailed analysis of its performance in cold-start scenarios.
- Why unresolved: The paper does not specifically test TASTE's performance on users or items with very few interactions, nor does it compare its cold-start performance to other methods designed for this purpose.
- What evidence would resolve it: Experiments comparing TASTE's performance in cold-start scenarios (e.g., users/items with less than 5 interactions) to other cold-start recommendation methods, using metrics like recall@N and NDCG@N.

## Limitations

- The attention sparsity method may lose important cross-session dependencies when splitting user-item interaction sequences
- Performance heavily depends on the quality and diversity of item attributes for effective text verbalization
- The model's effectiveness with extremely long sequences (thousands of items) remains untested

## Confidence

- Text matching reduces popularity bias: Medium
- Attention sparsity enables longer sequence modeling: Medium
- Prompt-based verbalization effectively captures user preferences: Medium

## Next Checks

1. Conduct experiments with synthetic datasets where popularity bias is artificially controlled to isolate the text matching effect from other factors.
2. Perform cross-dataset validation to test TASTE's robustness across different domains and item attribute distributions.
3. Implement and test alternative negative sampling strategies (e.g., hard negatives, popularity-based negatives) to determine optimal contrastive learning configuration for text-based recommendation.