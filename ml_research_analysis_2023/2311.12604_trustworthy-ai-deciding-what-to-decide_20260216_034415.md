---
ver: rpa2
title: 'Trustworthy AI: Deciding What to Decide'
arxiv_id: '2311.12604'
source_url: https://arxiv.org/abs/2311.12604
tags:
- data
- properties
- https
- decision
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a Trustworthy AI (TAI) framework for strategic
  decision-making, focusing on the context of credit default swap (CDS) investment
  in the technology sector. The framework identifies twelve TAI properties across
  three components of AI: representation space, loss function, and optimizer.'
---

# Trustworthy AI: Deciding What to Decide

## Quick Facts
- arXiv ID: 2311.12604
- Source URL: https://arxiv.org/abs/2311.12604
- Authors: 
- Reference count: 40
- Primary result: XGBoost outperforms transformer models for CDS prediction with RMSE of 26.30 while better satisfying TAI properties

## Executive Summary
This paper introduces a Trustworthy AI (TAI) framework for strategic decision-making, focusing on credit default swap (CDS) investment in the technology sector. The framework identifies twelve TAI properties across three components of AI: representation space, loss function, and optimizer. Through quantitative and qualitative research methods, the authors evaluate the framework using Xgboost and transformer models on a CDS dataset, demonstrating that Xgboost is more effective in satisfying TAI properties such as transparency, interpretability, explainability, and reproducibility.

## Method Summary
The study employs a TAI framework to evaluate model trustworthiness in predicting CDS prices for technology companies. The methodology involves comparing XGBoost and transformer models on a dataset of 37,526 observations with 117 features for 19 technology companies (2006-2017). Five explanation techniques (variable importance, partial dependence plots, individual conditional expectation, LIME, and SHAP) are applied to assess TAI properties. The framework systematically maps twelve TAI properties to three ML components, enabling structured evaluation of model trustworthiness across ethical, technical, and governance dimensions.

## Key Results
- XGBoost achieves RMSE of 26.30 for CDS price prediction, outperforming transformer models
- XGBoost better satisfies TAI properties including transparency, interpretability, explainability, and reproducibility
- Five explanation techniques (VI, PDP, ICE, LIME, SHAP) provide both global and local explanations for model predictions
- The framework demonstrates systematic evaluation of TAI properties across the model lifecycle

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework improves model interpretability by structuring TAI properties into three loosely coupled ML components (representation space, loss function, optimizer).
- Mechanism: By mapping each TAI property to a specific ML component, the framework provides a systematic way to evaluate and improve model trustworthiness across ethical, technical, and governance dimensions.
- Core assumption: Different ML components have distinct relationships with TAI properties, and this mapping is neither one-to-one nor complete but sufficient for practical implementation.
- Evidence anchors:
  - [abstract] The framework consists of twelve TAI properties organized into three categories loosely coupled with three learning components.
  - [section] The representation space mainly aligns with ethical value properties like justice, explainable/interpretability, transparency, and fairness.
- Break condition: If a property cannot be meaningfully associated with any component, the mapping becomes arbitrary and loses systematic value.

### Mechanism 2
- Claim: XGBoost outperforms transformer models for CDS prediction in terms of TAI property satisfaction, particularly transparency and interpretability.
- Mechanism: Tree-based models like XGBoost provide built-in feature importance and partial dependence plots that directly satisfy TAI properties like transparency and explainability, while transformer models require additional post-hoc explanation techniques.
- Evidence anchors:
  - [abstract] Results show XGBoost is more effective in satisfying TAI properties such as transparency, interpretability, explainability, and reproducibility, with an RMSE of 26.30.
  - [section] XGBoost can run in parallel and save significant hyperparameter search time compared to other models.
- Break condition: If the dataset characteristics change significantly (e.g., requiring long-range dependencies), transformer models might outperform despite lower TAI property satisfaction.

### Mechanism 3
- Claim: The framework enables systematic evaluation of TAI properties through multiple explanation techniques applied to model predictions.
- Mechanism: By applying techniques like VI, PDP, ICE, LIME, and SHAP, the framework provides both global and local explanations that satisfy different TAI properties across the model lifecycle.
- Evidence anchors:
  - [section] We implemented five experiments to satisfy the listed trustworthy properties: transparency, explainable/interpretability, usability, accuracy, robustness, reliability, and reproducibility.
  - [section] LIME aims to make the predictive model more interpretable by generating local explanations.
- Break condition: If explanation techniques become computationally prohibitive or fail to provide meaningful insights for the specific domain, the systematic evaluation loses practical value.

## Foundational Learning

- Concept: Tree-based ensemble methods (GBM, XGBoost)
  - Why needed here: These models provide the computational foundation for satisfying TAI properties through built-in interpretability features and efficient hyperparameter optimization.
  - Quick check question: What is the key difference between GBM and XGBoost in terms of computational efficiency and parallel processing capabilities?

- Concept: Explanation techniques (VI, PDP, ICE, LIME, SHAP)
  - Why needed here: These techniques provide the methodological foundation for evaluating and demonstrating TAI property satisfaction across different levels of model explanation.
  - Quick check question: How does SHAP differ from LIME in terms of theoretical foundation and consistency across different model settings?

- Concept: CDS market mechanics and spread5 pricing
  - Why needed here: Understanding the financial domain context is essential for meaningful interpretation of model predictions and TAI property evaluation.
  - Quick check question: What is the relationship between recovery value and spread5 pricing in CDS contracts?

## Architecture Onboarding

- Component map: Representation space → Loss function → Optimizer, each mapped to four TAI properties, with five explanation techniques applied across the model lifecycle
- Critical path: Dataset preprocessing → Model selection (XGBoost preferred) → Hyperparameter optimization → Property satisfaction evaluation using explanation techniques → Strategic decision support
- Design tradeoffs: Tree-based models offer better interpretability but may underperform on complex sequential patterns compared to transformers; comprehensive TAI evaluation increases computational overhead but improves trust
- Failure signatures: Poor RMSE performance despite good TAI property satisfaction indicates misalignment between model capability and task requirements; excessive computational time suggests optimization opportunities
- First 3 experiments:
  1. Implement XGBoost baseline with default parameters and evaluate basic TAI properties using VI and PDP.
  2. Run hyperparameter search on XGBoost using the HPC environment and compare RMSE improvement vs. TAI property satisfaction.
  3. Apply LIME and SHAP to selected predictions to evaluate local interpretability and feature contribution consistency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we integrate rule-based reasoning (like Cyc) with machine learning models to achieve more trustworthy AI for strategic decision-making?
- Basis in paper: [explicit] The paper discusses Lenat and Marcus's argument that large language models lack reasoning capabilities and proposes "Cyc" as a complementary system for modern AI/ML models.
- Why unresolved: The paper acknowledges the potential of combining rule-based systems with modern AI/ML models but does not provide a concrete implementation or framework for achieving this integration.
- What evidence would resolve it: A working prototype or case study demonstrating the successful integration of rule-based reasoning with machine learning models in a strategic decision-making context.

### Open Question 2
- Question: How can we ensure the robustness and reliability of AI/ML models when dealing with missing data in the dataset, as mentioned in the limitations section?
- Basis in paper: [inferred] The paper mentions that the dataset has been pre-cleaned and many missing values have been deleted, which may cause issues with the accuracy of the prediction model.
- Why unresolved: The paper does not provide a detailed methodology or techniques for handling missing data in the context of trustworthy AI and strategic decision-making.
- What evidence would resolve it: A comprehensive study comparing the performance of AI/ML models trained on datasets with different approaches to handling missing data (e.g., deletion, imputation, etc.) in terms of TAI properties.

### Open Question 3
- Question: How can we extend the TAI framework to cover all TAI properties, including data governance, privacy, and security issues?
- Basis in paper: [explicit] The paper acknowledges that it has not covered all TAI properties, such as data governance, privacy, and security issues, and mentions that these will be addressed in future research.
- Why unresolved: The paper provides a framework for TAI but focuses on a limited set of properties. It does not provide a comprehensive approach to addressing all TAI properties.
- What evidence would resolve it: A follow-up study that expands the TAI framework to include all TAI properties and demonstrates its effectiveness in addressing various strategic decision-making contexts.

## Limitations
- Framework generalizability beyond technology sector CDS domain remains uncertain
- Relative weighting of TAI properties across different use cases is not addressed
- Computational trade-offs between comprehensive TAI evaluation and real-time decision-making requirements are not quantified

## Confidence

- **High Confidence**: The systematic mapping of TAI properties to ML components (representation space, loss function, optimizer) is well-supported by the framework's logical structure and provides a reproducible methodology for evaluating trustworthy AI systems.
- **Medium Confidence**: The empirical comparison between XGBoost and transformer models shows XGBoost's superiority in TAI property satisfaction, but the results may be dataset-specific and could change with different financial instruments or market conditions.
- **Low Confidence**: The framework's scalability to larger, more diverse datasets and its effectiveness in real-world strategic decision-making contexts beyond CDS prediction requires further validation.

## Next Checks

1. Test framework applicability across multiple financial domains (e.g., equity markets, commodities) to assess generalizability beyond technology sector CDS prediction.
2. Implement real-time TAI property evaluation to measure computational overhead and determine practical deployment constraints for strategic decision-making.
3. Conduct sensitivity analysis on TAI property weighting to identify which properties have the most significant impact on model trustworthiness and decision quality across different use cases.