---
ver: rpa2
title: 'FlexiAST: Flexibility is What AST Needs'
arxiv_id: '2307.09286'
source_url: https://arxiv.org/abs/2307.09286
tags:
- patch
- standard
- asts
- size
- sizes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Standard Audio Spectrogram Transformers (ASTs) trained with fixed
  patch sizes show poor performance when evaluated on different patch sizes, requiring
  re-training for each configuration. To address this limitation, we propose FlexiAST,
  a training approach that provides patch-size flexibility to standard ASTs without
  architectural changes.
---

# FlexiAST: Flexibility is What AST Needs

## Quick Facts
- **arXiv ID:** 2307.09286
- **Source URL:** https://arxiv.org/abs/2307.09286
- **Reference count:** 0
- **Primary result:** FlexiAST achieves 30-60% accuracy improvements over standard ASTs when evaluated on patch sizes different from training.

## Executive Summary
FlexiAST addresses the limitation of standard Audio Spectrogram Transformers (ASTs) that require retraining for different patch sizes. The method introduces flexibility by randomly selecting patch sizes during training and resizing patch and positional embeddings using pseudo-inverse resizing (PI-resize). This approach maintains competitive performance on the training patch size while achieving strong results across various patch sizes during inference, eliminating the need for multiple specialized models.

## Method Summary
FlexiAST modifies standard AST training by introducing random patch-size selection during each training epoch. The model selects patch sizes from a predefined set (typically {8, 10, 12, 16, 20, 24, 30, 32, 40, 48}) and resizes both patch embedding weights and positional embeddings using PI-resize to match the selected size. This resizing operation adjusts the model parameters rather than the input spectrogram, allowing the model to learn patch embeddings robust to size variations. For speaker identification tasks, resizing is applied only along the time axis to preserve frequency-specific speaker cues.

## Key Results
- FlexiAST maintains competitive performance on training patch sizes while achieving strong results across various patch sizes during inference
- Outperforms standard ASTs by 30-60% accuracy on VGGSound and ESC-50 when tested on patch sizes different from training
- Demonstrates consistent improvements across five audio datasets: AudioSet, VGGSound, ESC-50, Speech Commands, and VoxCeleb

## Why This Works (Mechanism)

### Mechanism 1
Random patch-size selection during training forces the model to learn patch embeddings that are robust to size variations. By sampling from a wide distribution of spatial resolutions, the model's patch embeddings are not tied to a single granularity and can generalize across patch sizes. Break condition: If embeddings become too generic and lose discriminative power for trained sizes, performance may drop.

### Mechanism 2
PI-resize adjusts patch embedding weights to maximize alignment with bilinearly interpolated patches, preserving semantic content. It uses the pseudoinverse of the bilinear interpolation matrix to minimize reconstruction error between resized patches and the model's output. Break condition: If bilinear interpolation distorts patch semantics beyond recoverability, PI-resize cannot compensate.

### Mechanism 3
Task-specific resizing preserves task-relevant frequency information while allowing patch-size flexibility. For speaker identification, resizing only along the time axis prevents degradation of speaker-specific frequency cues, whereas frequency-axis resizing harms performance. Break condition: If a task relies equally on both frequency and time features, this strategy may not generalize.

## Foundational Learning

- **Concept:** Bilinear interpolation and pseudoinverse resizing
  - Why needed here: These operations transform patch embeddings to match new patch sizes during training and inference
  - Quick check question: What is the key difference between resizing the input spectrogram vs. resizing the patch embedding weights?

- **Concept:** Random sampling and data augmentation
  - Why needed here: Random patch-size selection acts as a form of augmentation that increases model robustness
  - Quick check question: How does sampling patch sizes during training compare to standard augmentation techniques like mixup?

- **Concept:** Positional embeddings and tokenization in transformers
  - Why needed here: Patch embeddings and positional embeddings are the only parameters that depend on patch size
  - Quick check question: Why must positional embeddings be resized along with patch embeddings when patch size changes?

## Architecture Onboarding

- **Component map:** Input spectrogram → Patch splitting → Patch embeddings (ω) + positional embeddings (π) → [CLS] token + Transformer Encoder → Classification head
- **Critical path:** Random patch-size selection → Bilinear interpolation of spectrogram → PI-resize of patch embeddings → Training step
- **Design tradeoffs:** Flexibility vs. fixed-size performance (sacrifices some peak performance for robustness), computational cost (additional resizing operations), initialization source (AST-B/8 vs. ViT affects performance)
- **Failure signatures:** Performance drop on unseen patch sizes, overfitting to narrow patch size range, degraded speaker ID performance if frequency-axis resizing not disabled
- **First 3 experiments:**
  1. Train FlexiAST with random patch-size sampling; evaluate on all patch sizes; compare to standard AST trained on fixed patch size
  2. Replace PI-resize with bilinear interpolation for patch embeddings; measure flexibility loss
  3. Train FlexiAST with speaker ID dataset; test time-only vs. frequency-only vs. both-axis resizing

## Open Questions the Paper Calls Out

### Open Question 1
What are the specific mechanisms by which PI-resize achieves better performance than bilinear interpolation for AST models? The paper states PI-resize modifies model parameters rather than the input spectrogram, but lacks detailed mathematical derivation or implementation specifics.

### Open Question 2
How does the performance of FlexiAST compare to standard AST models when evaluated on the specific patch size they were trained on? The paper shows improvements on different patch sizes but doesn't provide direct comparisons on the same training size.

### Open Question 3
How does the choice of patch size during training affect the flexibility and performance of FlexiAST? The paper uses random sampling but doesn't explore the impact of different patch size selections during training.

## Limitations
- PI-resize effectiveness depends on the conditioning of the bilinear matrix, which degrades with extreme size changes
- Speaker ID task constraint (time-only resizing) is based on assumptions about frequency cues without ablation evidence
- Performance improvements measured against single fixed-patch baseline, not specialized models for each patch size

## Confidence
- **Mechanism 1:** Medium - Random sampling provides robustness but may overgeneralize
- **Mechanism 2:** Low - PI-resize effectiveness depends on bilinear matrix conditioning without error bounds
- **Mechanism 3:** Low - Speaker ID constraint lacks empirical validation for frequency resizing impact

## Next Checks
1. Test FlexiAST on music transcription dataset to evaluate performance when audio content is sensitive to both time and frequency resolution
2. Measure performance degradation with patch sizes drawn from a wider range (4-64) to assess PI-resize limits
3. Compare FlexiAST against an ensemble of multiple fixed-patch ASTs to determine accuracy cost versus specialization