---
ver: rpa2
title: 'Escaping mediocrity: how two-layer networks learn hard generalized linear
  models with SGD'
arxiv_id: '2305.18502'
source_url: https://arxiv.org/abs/2305.18502
tags:
- learning
- risk
- time
- have
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the sample complexity of two-layer neural
  networks learning generalized linear models under SGD, focusing on the challenging
  regime with many flat directions at initialization. The authors derive deterministic
  ODEs describing the SGD dynamics in the high-dimensional limit and characterize
  stochastic corrections.
---

# Escaping mediocrity: how two-layer networks learn hard generalized linear models with SGD

## Quick Facts
- arXiv ID: 2305.18502
- Source URL: https://arxiv.org/abs/2305.18502
- Reference count: 40
- Two-layer networks learning generalized linear models under SGD require O(d log d) samples to escape mediocrity

## Executive Summary
This study investigates why two-layer neural networks struggle to learn hard generalized linear models under stochastic gradient descent (SGD), particularly in the regime with many flat directions at initialization. The authors develop a high-dimensional analysis framework showing that SGD dynamics are well-approximated by deterministic ODEs, with stochastic corrections being negligible. They demonstrate that overparameterization provides only constant factor improvements in sample complexity for this problem class, fundamentally requiring n = O(d log d) samples to escape mediocrity regardless of network width.

## Method Summary
The authors analyze two-layer neural networks with squared activation learning phase retrieval models through one-pass SGD with a spherical constraint on first layer weights. They derive deterministic ODEs describing the evolution of sufficient statistics in the high-dimensional limit, then characterize stochastic corrections. The analysis focuses on the exit time from a mediocrity region where the network fails to correlate with the target signal, showing this escape time scales as O(d log d) samples regardless of network width.

## Key Results
- Deterministic ODEs accurately capture SGD dynamics in the high-dimensional limit for phase retrieval learning
- Stochastic corrections to the deterministic approximation are negligible for escape time calculations
- Overparameterization improves sample complexity by only a constant factor, not changing the fundamental O(d log d) scaling
- Width p affects only the pre-factor in logarithmic scaling, with minimal impact on the number of samples needed to escape mediocrity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Overparameterization provides only constant factor improvements in sample complexity for phase retrieval learning with SGD
- Mechanism: The deterministic ODEs describing SGD dynamics in high dimensions show that width p only affects the pre-factor in the logarithmic scaling, not the fundamental O(d log d) scaling
- Core assumption: The high-dimensional limit d→∞ accurately captures the dynamics, and stochastic corrections are negligible
- Evidence anchors:
  - [abstract] "our findings suggest that overparameterization can only enhance convergence by a constant factor within this problem class"
  - [section] "despite helping escaping mediocrity, increasing the width cannot mitigate it"
  - [corpus] "A Theoretical Analysis of Noise Geometry in Stochastic Gradient Descent" provides related analysis of SGD dynamics
- Break condition: If the problem class changes to one with different landscape geometry or if stochasticity becomes significant at finite but large d

### Mechanism 2
- Claim: Stochasticity does not help escape the flat directions at initialization
- Mechanism: The exit time from the mediocrity region is well-approximated by deterministic ODEs, and stochastic corrections provide minimal improvement
- Core assumption: The initial correlation with the signal is sufficiently small (O(1/√d)) that stochastic effects are negligible compared to deterministic drift
- Evidence anchors:
  - [abstract] "we demonstrate that a deterministic approximation of this process adequately represents the escape time"
  - [section] "the stochastic corrections from a finer analysis of the process (7) can be neglected"
  - [corpus] "A Hessian-Aware Stochastic Differential Equation for Modelling SGD" suggests similar findings about SDE approximations
- Break condition: If the initial correlation with signal is not sufficiently small, or if the landscape geometry changes significantly

### Mechanism 3
- Claim: Learning occurs through sequential correlation with target directions in a staircase manner
- Mechanism: The SGD dynamics first establish correlation with lower complexity directions before moving to higher complexity ones, with the phase retrieval problem requiring correlation with a single direction
- Core assumption: The problem can be reduced to learning a single target direction after fitting the non-linearity
- Evidence anchors:
  - [section] "This picture, however, is bound to classes of targets where SGD develops strong correlations with the target directions at initialization"
  - [section] "This is precisely the case for the phase retrieval problem (IE = 2), a classic inverse problem"
  - [corpus] "Sliding down the stairs: how correlated latent variables accelerate learning with neural networks" discusses similar staircase learning dynamics
- Break condition: If the target function has a different structure that doesn't allow such reduction, or if multiple directions need to be learned simultaneously

## Foundational Learning

- Concept: High-dimensional analysis and concentration of measure
  - Why needed here: The analysis relies on showing that in the limit d→∞, the sufficient statistics concentrate around their expected values, allowing deterministic ODE description
  - Quick check question: What is the key concentration result that allows replacing stochastic dynamics with deterministic ODEs in this work?

- Concept: Stochastic approximation and SDE limits
  - Why needed here: Understanding how SGD dynamics can be approximated by continuous-time processes and when stochastic corrections become negligible
  - Quick check question: Under what conditions do the stochastic corrections to the high-dimensional limit become significant?

- Concept: Phase retrieval and single-index models
  - Why needed here: The specific problem being analyzed is a phase retrieval problem, which has unique geometric properties that make it challenging for SGD
  - Quick check question: What makes phase retrieval a "hard" problem for SGD compared to problems with non-zero first Hermite coefficient?

## Architecture Onboarding

- Component map: Data generation -> SGD dynamics -> Sufficient statistics tracking -> High-dimensional limit -> Exit time calculation
- Critical path: Data generation → SGD dynamics → Sufficient statistics tracking → High-dimensional limit → Exit time calculation
- Design tradeoffs: Deterministic vs stochastic analysis (simplicity vs accuracy), width p vs computational cost, spherical constraint vs unconstrained dynamics
- Failure signatures: If the deterministic approximation fails to capture the dynamics, if stochastic corrections become significant, or if the high-dimensional limit assumptions break down
- First 3 experiments:
  1. Simulate SGD dynamics for p=1 and verify agreement with deterministic ODEs
  2. Vary width p and measure the improvement in sample complexity to confirm constant factor scaling
  3. Compare exit times from deterministic ODEs vs stochastic SDE simulations to quantify stochastic correction impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the introduction of noise (∆ > 0) affect the asymptotic risk plateau observed in the deterministic limit of SGD for phase retrieval?
- Basis in paper: [explicit] The paper discusses the presence of an asymptotic risk plateau for the excess risk in the high-dimensional regime, stating "Note that in the population gradient flow limit γ → 0+, SGD converges to the minimal risk [45]. Indeed, the presence of a finite excess risk even in the well-specified setting is an intrinsic correction from the SGD noise in the high-dimensional regime where 1/d ≪ γ [19]."
- Why unresolved: While the paper establishes the existence of an asymptotic risk plateau, it does not provide a detailed analysis of how this plateau's height or characteristics change with varying levels of noise (∆).
- What evidence would resolve it: A comprehensive study varying the noise level ∆ and measuring the resulting asymptotic risk plateau in simulations or theoretical derivations would clarify the relationship between noise and the plateau's properties.

### Open Question 2
- Question: What is the impact of more general activation functions σ⋆ with zero first Hermite coefficient on the sample complexity and dynamics of SGD for phase retrieval?
- Basis in paper: [explicit] The paper states, "The choice of more general σ⋆ and σ with zero first Hermite but not necessarily zero higher-order coefficients might give rise to other critical points such as saddle-points, giving rise to a more complex SGD dynamics. However, since the focus of this work is on escaping mediocrity, our conclusions will hold, up to constants, to more general activations with zero first Hermite."
- Why unresolved: The paper focuses on the purely quadratic case (σ⋆(x) = x²) and suggests that the results might extend to more general activations, but it does not provide a rigorous analysis of how different activation functions would affect the learning dynamics and sample complexity.
- What evidence would resolve it: Deriving the sample complexity and characterizing the dynamics of SGD for a broader class of activation functions with zero first Hermite coefficient, possibly through theoretical analysis or extensive simulations, would provide insights into the impact of activation function choice.

### Open Question 3
- Question: How does the learning dynamics change after escaping mediocrity when training both layers of the two-layer neural network compared to training only the second layer?
- Basis in paper: [explicit] The paper discusses the impact of training the second layer (a) versus keeping it fixed, stating, "We can also compute the optimal number of steps; we choose to stick with the annealed formula: for large p both the estimation lead to the same result, while for small p we are underestimating the exit time of a small factor. Hence, the annealed minimum number of steps is... This formula can be used to estimate the overparametrization gain."
- Why unresolved: While the paper compares the time needed to escape mediocrity with and without training the second layer, it does not provide a detailed analysis of how the learning dynamics evolve after escaping mediocrity in these two scenarios.
- What evidence would resolve it: A thorough investigation of the learning dynamics post-escaping mediocrity, comparing the evolution of weights, risk, and other relevant metrics when training both layers versus only the second layer, would shed light on the long-term impact of this choice on the learning process.

## Limitations
- Analysis restricted to specific problem classes (phase retrieval, single-index models) with particular geometric properties
- Spherical constraint may not reflect practical training scenarios where norm drift occurs
- One-pass SGD assumption simplifies analysis but differs from typical multiple-epoch training

## Confidence

**High confidence**: The fundamental O(d log d) scaling for sample complexity and the characterization of mediocrity as a function of signal-to-noise ratio

**Medium confidence**: The claim that width provides only constant factor improvements, as this requires careful verification in the finite-dimensional regime

**Medium confidence**: The negligible impact of stochasticity on escape time, which depends on specific problem geometry

## Next Checks

1. Verify the constant factor improvement from width empirically by simulating finite-dimensional systems with varying p and measuring actual sample complexity
2. Quantify the impact of stochasticity by comparing deterministic ODE predictions with stochastic SDE simulations across different problem geometries
3. Test the staircase learning mechanism by analyzing SGD dynamics for multi-dimensional target functions with varying correlations between components