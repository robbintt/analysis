---
ver: rpa2
title: 'PACE: Improving Prompt with Actor-Critic Editing for Large Language Model'
arxiv_id: '2308.10088'
source_url: https://arxiv.org/abs/2308.10088
tags:
- prompt
- pace
- prompts
- llms
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents PACE, an actor-critic framework for prompt
  editing that leverages large language models (LLMs) to improve task-specific prompts.
  PACE conceptualizes prompts as policies and uses LLMs as both actor and critic:
  the actor executes tasks based on the prompt while the critic evaluates the results
  and provides feedback for refinement.'
---

# PACE: Improving Prompt with Actor-Critic Editing for Large Language Model

## Quick Facts
- arXiv ID: 2308.10088
- Source URL: https://arxiv.org/abs/2308.10088
- Reference count: 7
- This paper presents PACE, an actor-critic framework for prompt editing that leverages large language models (LLMs) to improve task-specific prompts.

## Executive Summary
This paper introduces PACE (Prompt Actor-Critic Editing), a framework that uses large language models as both actor and critic to iteratively refine prompts for improved task performance. PACE conceptualizes prompts as policies in a reinforcement learning framework, where the actor executes tasks based on the current prompt while the critic evaluates results and provides feedback for refinement. The iterative process continues until convergence or maximum iterations are reached. Experiments show PACE significantly improves medium/low-quality human-written prompts by up to 98% in relative performance, achieving comparable results to high-quality human-written prompts, and demonstrates effectiveness for prompt generation from scratch.

## Method Summary
PACE implements an actor-critic framework where LLMs serve dual roles: the actor executes tasks based on the current prompt while the critic evaluates the actor's responses against ground truth to provide feedback. The system uses n actor-critic pairs (n=4 in experiments) to generate diverse responses and evaluations, then aggregates this feedback to update the prompt. The iterative process starts with an initial prompt (which can be empty for generation from scratch) and continues until convergence or a maximum iteration limit. The framework treats prompts as policies that can be refined through reinforcement learning principles, leveraging LLM capabilities for both task execution and evaluation.

## Key Results
- Improves medium/low-quality human-written prompts by up to 98% in relative performance
- Achieves comparable results to high-quality human-written prompts on 24 instruction induction and 21 big-bench tasks
- Demonstrates strong performance for prompt generation from scratch and maintains effectiveness across different LLM models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PACE treats prompts as policies in an actor-critic framework, enabling iterative refinement through LLM feedback.
- Mechanism: The actor (LLM) executes tasks based on the current prompt, while the critic (another LLM) evaluates the response against ground truth. This dual-LLM feedback loop allows the system to iteratively improve prompts by addressing specific weaknesses identified by the critic.
- Core assumption: LLMs can serve as both competent executors (actors) and reliable evaluators (critics) for their own prompts.
- Evidence anchors: [abstract] "conceptualizing prompt as a type of policy" and "PACE leverages LLMs as the dual roles of actors and critics"
- Break condition: If the critic cannot reliably identify prompt weaknesses or the actor cannot execute based on refined prompts, the iterative improvement process will fail.

### Mechanism 2
- Claim: Aggregating feedback from multiple actor-critic pairs provides more holistic guidance than single-instance feedback.
- Mechanism: The system uses n actors and n critics (where n=4 in experiments) to generate diverse responses and evaluations, then aggregates this feedback to create a more robust prompt update that accounts for input variability and sampling bias.
- Core assumption: Multiple evaluations reduce individual bias and sampling noise, leading to more reliable prompt refinement.
- Evidence anchors: [section] "Considering the bias caused by inputs and sampling can render critiques imprecise, thus affecting the outcomes of prompt editing"
- Break condition: If the aggregated feedback becomes contradictory or if computational overhead from multiple evaluations outweighs benefits.

### Mechanism 3
- Claim: PACE can generate high-quality prompts from scratch when starting with empty prompts.
- Mechanism: By treating an empty string as the initial prompt, PACE leverages the actor-critic feedback to construct a task-specific prompt iteratively, demonstrating the framework's generative capability beyond refinement.
- Core assumption: The actor-critic framework can bootstrap from minimal information to construct effective prompts through iterative refinement.
- Evidence anchors: [section] "We start with an initial prompt p0, where p0 can be an empty prompt"
- Break condition: If the iterative process fails to converge on a coherent prompt structure or if the actor-critic feedback is insufficient to guide prompt construction.

## Foundational Learning

- Concept: Reinforcement Learning - Actor-Critic Algorithm
  - Why needed here: PACE directly applies the actor-critic paradigm from reinforcement learning to prompt optimization, where the actor generates actions (responses) and the critic evaluates them to guide policy improvement (prompt refinement)
  - Quick check question: How does the actor-critic framework differ from value-based reinforcement learning methods in terms of feedback utilization?

- Concept: Large Language Model Prompt Engineering
  - Why needed here: Understanding how prompts shape LLM behavior is essential for conceptualizing prompts as policies and recognizing why prompt quality varies significantly across different human-written prompts
  - Quick check question: Why might small changes in prompt phrasing lead to substantial differences in LLM output quality?

- Concept: Natural Language Understanding and Generation
  - Why needed here: The actor must understand task instructions to execute them, while the critic must comprehend both the instruction and response to provide meaningful evaluation
  - Quick check question: What challenges arise when using an LLM as both actor and critic for the same task?

## Architecture Onboarding

- Component map:
  Initial prompt storage and versioning → Actor LLM interface for task execution → Critic LLM interface for response evaluation → Feedback aggregation module → Prompt update generation module → Score function evaluation system → Iteration control and convergence detection

- Critical path: Initial prompt → Actor execution → Critic evaluation → Feedback aggregation → Prompt update → Score evaluation → Next iteration (repeat until convergence)

- Design tradeoffs:
  - Number of actors/critics (n=4) vs. computational cost and feedback diversity
  - Score function choice (log probability vs. practical evaluation metric) affecting accessibility and alignment with real-world performance
  - Maximum iterations vs. diminishing returns in prompt quality improvement

- Failure signatures:
  - Critic consistently provides vague or unhelpful feedback
  - Actor fails to execute based on refined prompts (misalignment between prompt and execution capability)
  - Score function plateaus early without meaningful prompt improvement
  - Aggregation produces contradictory guidance across actor-critic pairs

- First 3 experiments:
  1. Run PACE with a known low-quality prompt on a simple task to verify basic actor-critic functionality and prompt improvement
  2. Test with different initial prompt qualities (empty, worst, best) on the same task to observe improvement patterns
  3. Vary the number of actors/critics (n parameter) to determine optimal balance between feedback diversity and computational efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PACE vary when using different types of LLMs (e.g., open-source vs. proprietary models) beyond the ones tested in the paper?
- Basis in paper: [inferred] The paper tests PACE with several proprietary LLMs (e.g., ChatGPT, GPT-4) but does not explore open-source alternatives.
- Why unresolved: The paper focuses on proprietary models, leaving the question of PACE's effectiveness with open-source models unanswered.
- What evidence would resolve it: Comparative experiments using PACE with both proprietary and open-source LLMs to evaluate performance differences.

### Open Question 2
- Question: What is the impact of the number of agents (n) and candidates per iteration on the performance of PACE, and how can these hyperparameters be optimally tuned for different tasks?
- Basis in paper: [explicit] The paper sets n=4 and candidates=2 as default hyperparameters but does not explore their impact on performance.
- Why unresolved: The optimal values for these hyperparameters are not determined, and their effect on different tasks is not explored.
- What evidence would resolve it: Systematic experiments varying n and candidates across tasks to identify optimal configurations and their impact on performance.

### Open Question 3
- Question: How does PACE handle tasks with highly ambiguous or context-dependent prompts, and what mechanisms could be added to improve its robustness in such cases?
- Basis in paper: [inferred] The paper demonstrates PACE's effectiveness on structured tasks but does not address its performance on ambiguous or context-dependent prompts.
- Why unresolved: The paper does not explore PACE's limitations with ambiguous or context-dependent prompts.
- What evidence would resolve it: Experiments testing PACE on tasks with ambiguous or context-dependent prompts and proposing mechanisms to enhance its robustness.

## Limitations
- The evaluation methodology uses log probability rather than actual task performance metrics, creating uncertainty about real-world effectiveness
- Computational overhead is substantial but poorly characterized, with no systematic analysis of cost-benefit tradeoff
- Insufficient experimental detail to verify claims about cross-model generalization and performance improvements

## Confidence
- **Medium Confidence** in the core actor-critic mechanism claims
- **Low Confidence** in the performance improvement claims due to indirect evaluation metrics
- **Low Confidence** in the generalizability claims across different LLMs

## Next Checks
1. Direct Task Performance Validation: Run PACE-refined prompts on actual task completion benchmarks rather than score functions to verify that prompt improvements translate to better LLM output quality.

2. Computational Cost-Benefit Analysis: Implement PACE with different parameter configurations and measure both performance improvements and computational costs to calculate the point of diminishing returns.

3. Cross-Model Generalization Test: Apply PACE to prompts across at least three different LLM architectures to measure whether the same refined prompts maintain effectiveness or require model-specific adaptation.