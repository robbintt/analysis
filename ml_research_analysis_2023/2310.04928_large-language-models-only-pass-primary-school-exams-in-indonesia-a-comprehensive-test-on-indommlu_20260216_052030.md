---
ver: rpa2
title: 'Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive
  Test on IndoMMLU'
arxiv_id: '2310.04928'
source_url: https://arxiv.org/abs/2310.04928
tags:
- language
- indonesian
- school
- questions
- bloomz
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models are typically evaluated on English datasets,
  leaving a gap in understanding their performance on languages and cultures beyond
  English. To address this, the authors introduce IndoMMLU, the first multi-task language
  understanding benchmark for Indonesian culture and languages, comprising 14,906
  questions across 63 tasks and education levels, with 46% focusing on Indonesian
  language proficiency and knowledge of nine local languages and cultures.
---

# Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU

## Quick Facts
- **arXiv ID**: 2310.04928
- **Source URL**: https://arxiv.org/abs/2310.04928
- **Reference count**: 27
- **Primary result**: GPT-3.5 only passes Indonesian primary school level exams, with significant limitations on local languages and culture.

## Executive Summary
Large language models are typically evaluated on English datasets, leaving a gap in understanding their performance on languages and cultures beyond English. To address this, the authors introduce IndoMMLU, the first multi-task language understanding benchmark for Indonesian culture and languages, comprising 14,906 questions across 63 tasks and education levels, with 46% focusing on Indonesian language proficiency and knowledge of nine local languages and cultures. Evaluations using GPT-3.5, BLOOMZ, and Falcon show that GPT-3.5 only passes primary school level exams, with limited knowledge of local Indonesian languages and culture. Other smaller models like BLOOMZ and Falcon perform at even lower levels, highlighting the need for further research in this area.

## Method Summary
The IndoMMLU benchmark was created by collecting 14,906 questions from Indonesian primary school to university entrance exams across 63 tasks. Professional teachers gathered questions covering Indonesian language proficiency and nine local languages and cultures. The dataset was quality-controlled through manual checks and automatic filtering. The authors evaluated 24 multilingual LLMs in zero-shot and few-shot settings using probability-based answer selection. Performance was measured across different education levels and subject areas, with particular attention to Indonesian language and social science domains.

## Key Results
- GPT-3.5 achieves highest accuracy (approaching 90%) at grade 1 but performance declines with higher education levels
- BLOOMZ and Falcon show lower performance than GPT-3.5 across all education levels
- Instruction-tuned models (mT0, BLOOMZ) do not benefit from few-shot prompting, while pure LLMs (Falcon, LLaMA) show improvement with few-shot examples
- Models demonstrate limited knowledge of local Indonesian languages and cultures, with mT0xxl outperforming GPT-3.5 on these tasks despite fewer parameters

## Why This Works (Mechanism)

### Mechanism 1
Models pre-trained on multilingual corpora fail on Indonesian exams due to low coverage of local language and culture content. The LLMs are evaluated using question-answering formats that test both Indonesian language proficiency and knowledge of nine local languages/cultures. Since the pre-training data lacks sufficient representation of these domains, the models cannot accurately predict correct answers.

### Mechanism 2
Performance drops with increasing education level because questions require higher-order reasoning beyond surface-level language understanding. Primary school questions focus on basic Indonesian language skills, which models like GPT-3.5 can handle. However, higher-level questions demand deeper comprehension, cultural context, and reasoning that the models lack.

### Mechanism 3
Fine-tuned instruction-tuned models (mT0, BLOOMZ) do not benefit from few-shot prompting, while pure LLMs (Falcon, LLaMA) do. Instruction-tuned models are already optimized for zero-shot performance on task instructions, so additional examples may introduce noise. Pure LLMs lack this optimization and thus improve with few-shot examples.

## Foundational Learning

- **Multilingual pre-training data distribution**: Understanding why models perform poorly on low-resource languages requires knowing how multilingual datasets are constructed and what languages are underrepresented.
  - Quick check: What percentage of a typical multilingual corpus is usually dedicated to high-resource vs. low-resource languages?

- **Educational assessment design principles**: The benchmark uses school exams, which are designed to test progressive cognitive skills; understanding this helps interpret performance drops across education levels.
  - Quick check: How do primary school exam questions differ in cognitive demand from university entrance exam questions?

- **Cultural knowledge encoding in language models**: Local languages and cultures are embedded in the questions; knowing how models encode cultural knowledge explains their performance on these tasks.
  - Quick check: Can a language model learn cultural nuances without explicit exposure to culturally specific texts during training?

## Architecture Onboarding

- **Component map**: Data collection -> Quality control -> Model evaluation -> Analysis
- **Critical path**: 1. Collect and clean exam questions, 2. Design prompts for each model, 3. Run zero-shot evaluation, 4. Analyze results by subject and education level, 5. Conduct ablation studies (few-shot, negation, confidence calibration)
- **Design tradeoffs**: Using multiple-choice questions simplifies evaluation but may not capture full language proficiency; excluding mathematics avoids well-covered domains but loses a reasoning dimension; focusing on Indonesian and local languages limits generalizability but addresses a critical gap
- **Failure signatures**: Random performance around 20-27% indicates models cannot guess better than chance; sharp performance drop beyond primary school suggests lack of higher-order reasoning skills; mT0xxl outperforming GPT-3.5 on local languages despite fewer parameters indicates data quality matters more than scale
- **First 3 experiments**: 1. Run zero-shot evaluation on a held-out subset to confirm initial results, 2. Test few-shot prompting with different numbers of examples to find optimal configuration, 3. Perform ablation by removing negation-containing questions to quantify their impact

## Open Questions the Paper Calls Out

### Open Question 1
How does the inclusion of negation in questions impact the performance of large language models on Indonesian language and social science tasks?
- Basis in paper: The paper discusses the impact of negation on LLM performance, particularly in Indonesian language and social science subject areas.
- Why unresolved: While the paper provides some insights into the impact of negation, it does not explore this in depth or consider potential strategies to mitigate the negative effects of negation on LLM performance.
- What evidence would resolve it: A comprehensive study examining the impact of negation on LLM performance across various languages and subject areas, along with potential strategies to improve performance in the presence of negation.

### Open Question 2
How do large language models perform on Indonesian language tasks as the education level increases, and what factors contribute to this change in performance?
- Basis in paper: The paper discusses the performance of GPT-3.5 on Indonesian language tasks across different education levels, showing a decline in performance as the education level increases.
- Why unresolved: The paper does not delve into the reasons behind this decline in performance or explore potential strategies to improve LLM performance on higher-level Indonesian language tasks.
- What evidence would resolve it: A detailed analysis of the factors contributing to the decline in LLM performance on Indonesian language tasks at higher education levels, along with potential strategies to address these issues.

### Open Question 3
How can large language models be improved to better handle local languages and cultures in Indonesia, and what are the key challenges in achieving this?
- Basis in paper: The paper highlights the limited performance of large language models in answering questions related to local languages and cultures in Indonesia.
- Why unresolved: The paper does not explore potential strategies for improving LLM performance on local languages and cultures or discuss the challenges in achieving this goal.
- What evidence would resolve it: A comprehensive study examining the challenges in improving LLM performance on local languages and cultures, along with potential strategies and their effectiveness in addressing these challenges.

## Limitations

- The IndoMMLU benchmark covers only 63 tasks and may not fully represent the breadth of Indonesian linguistic and cultural diversity
- Performance gap between GPT-3.5 and smaller models on local language tasks suggests data quality matters more than scale, but this assumes benchmark questions are representative
- Multiple-choice format may allow models to perform better through pattern recognition rather than genuine understanding

## Confidence

- **High Confidence**: GPT-3.5's primary school level performance and the general trend of declining accuracy with increasing education levels
- **Medium Confidence**: The claim that instruction-tuned models do not benefit from few-shot prompting while pure LLMs do
- **Medium Confidence**: The interpretation that low performance on local languages stems from underrepresentation in pre-training data
- **Low Confidence**: The comparative performance of mT0xxl versus GPT-3.5 on local languages

## Next Checks

1. Replicate with different prompt formulations: Test the same models using alternative prompt structures and few-shot examples to verify whether the observed performance patterns are robust to prompting variations.

2. Analyze calibration across education levels: Generate detailed calibration curves for each education level to quantify how well model confidence scores align with actual accuracy.

3. Expand to additional subjects: Include mathematics and other reasoning-intensive subjects that were excluded from this evaluation to better understand whether the performance limitations extend beyond language and cultural knowledge to general educational reasoning skills.