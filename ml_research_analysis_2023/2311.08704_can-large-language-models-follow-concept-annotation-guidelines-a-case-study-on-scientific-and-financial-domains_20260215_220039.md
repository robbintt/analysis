---
ver: rpa2
title: Can Large Language Models Follow Concept Annotation Guidelines? A Case Study
  on Scientific and Financial Domains
arxiv_id: '2311.08704'
source_url: https://arxiv.org/abs/2311.08704
tags:
- concept
- guidelines
- financial
- concepts
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models can learn
  new concepts from in-context concept definitions during sentence classification
  tasks. The authors design factual and counterfactual concept guidelines and evaluate
  several instruction-tuned LLMs (Llama-2, Falcon-180B, GPT-3.5, and GPT-4) on scientific
  and financial concept classification tasks.
---

# Can Large Language Models Follow Concept Annotation Guidelines? A Case Study on Scientific and Financial Domains

## Quick Facts
- **arXiv ID:** 2311.08704
- **Source URL:** https://arxiv.org/abs/2311.08704
- **Reference count:** 8
- **Key outcome:** Concept definitions improve LLM classification accuracy, but only larger models (70B+) can follow counterfactual guidelines; proprietary models uniquely recognize nonsensical tasks

## Executive Summary
This paper investigates whether large language models can learn new concepts from in-context definitions during sentence classification tasks. The authors evaluate several instruction-tuned LLMs (Llama-2, Falcon-180B, GPT-3.5, GPT-4) on scientific and financial concept classification using factual and counterfactual concept guidelines. The study reveals that while concept definitions consistently improve performance, only larger models (70B+ parameters) demonstrate limited ability to follow counterfactual guidelines, and only proprietary models can recognize nonsensical tasks. The findings highlight significant gaps between open-source and proprietary models in concept understanding and counterfactual reasoning.

## Method Summary
The study uses zero-shot classification with instruction-tuned LLMs on two datasets: scientific concepts from ARTCorpus (500 sentences, 5 categories) and financial concepts from 10-K reports (540 sentences, 6 categories). The authors generate factual concept definitions using GPT-3.5, then create counterfactual variants by permuting definitions, along with out-of-dictionary (OOD) and empty definition guidelines. Models are evaluated on their ability to classify sentences using these guidelines, with performance measured by accuracy and agreement with human annotators (Cohen's κ).

## Key Results
- Concept definitions improve classification accuracy by 3.7% (scientific) and 8.2% (financial) compared to labels alone
- Only 70B+ parameter models (Llama-2-70B, GPT-3.5, GPT-4) can follow counterfactual guidelines, with Llama-2-70B outperforming Falcon-180B despite fewer parameters
- Proprietary models (GPT-3.5, GPT-4) uniquely recognize nonsensical guidelines and refuse to classify 58% and 51% of sentences respectively
- Llama-2-70B and GPT-4 achieve human-level agreement (κ ~0.42-0.45) on financial concept annotation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Concept definitions improve classification accuracy by providing lexical grounding beyond mere label names.
- **Mechanism:** When models receive concept definitions alongside labels, they can disambiguate and contextualize the concepts rather than relying solely on prior lexical associations from pretraining.
- **Core assumption:** Models have sufficient capacity to integrate and apply new definitional information in context.
- **Evidence anchors:**
  - [abstract] "concept definitions consistently help in task performance"
  - [section 4] "average accuracy loss when removing concept definitions is 3.7% and 8.2% for scientific and financial concepts respectively"
  - [corpus] Weak evidence - no direct citations found in related work on this specific mechanism
- **Break condition:** If model capacity is insufficient to process the additional definitional context, or if definitions are too ambiguous or contradictory.

### Mechanism 2
- **Claim:** Larger models (70B+ parameters) can follow counterfactual guidelines, changing their behavior based on modified concept definitions.
- **Mechanism:** Scaling enables models to decouple their prior knowledge from new in-context information, allowing them to reason under counterfactual definitions.
- **Core assumption:** Model scale correlates with the ability to perform abstract reasoning and override prior associations.
- **Evidence anchors:**
  - [abstract] "only the larger models (with 70B parameters or more) have limited ability to work under counterfactual contexts"
  - [section 4] "LLAMA -2-70B , GPT-3.5 , and GPT-4...consistently drop in accuracy...which indicates that these models are effectively changing the labels according to the counterfactual semantics"
  - [corpus] Weak evidence - related work on scaling and counterfactual reasoning is limited
- **Break condition:** If scaling alone is insufficient and fine-tuning methods are more critical for counterfactual reasoning.

### Mechanism 3
- **Claim:** Proprietary models can recognize nonsensical guidelines (out-of-dictionary labels without definitions) and refuse to classify.
- **Mechanism:** Advanced alignment methods (likely RLHF) enable proprietary models to detect when task instructions are invalid or nonsensical.
- **Core assumption:** Sophisticated alignment methods train models to identify and handle edge cases in task instructions.
- **Evidence anchors:**
  - [abstract] "only proprietary models such as GPT-3.5 and GPT-4 can recognize nonsensical guidelines"
  - [section 4] "GPT-3.5 refuses to classify 58% and 51% of sentences from scientific and financial documents respectively"
  - [corpus] Weak evidence - no direct citations found on proprietary model alignment advantages
- **Break condition:** If open-source models receive similar alignment training or if the detection threshold is too strict.

## Foundational Learning

- **Concept:** Zero-shot learning
  - Why needed here: The paper evaluates models' ability to perform classification without examples, relying only on in-context guidelines
  - Quick check question: Can a model classify sentences into categories it has never seen examples of, using only definitions?

- **Concept:** Counterfactual reasoning
  - Why needed here: The study tests whether models can adapt their concept understanding when definitions contradict their prior knowledge
  - Quick check question: If a model is told "cat" means "a large aquatic mammal," can it correctly classify sentences about whales using the "cat" label?

- **Concept:** In-context learning vs. fine-tuning
  - Why needed here: The paper distinguishes between learning from in-context examples versus learning from in-context definitions
  - Quick check question: Does providing a definition in the prompt teach the model something new, or just format the task?

## Architecture Onboarding

- **Component map:** ARTCorpus dataset -> concept guidelines generation -> financial dataset preparation -> model inference -> post-processing -> accuracy calculation
- **Critical path:**
  1. Generate or collect guidelines for each concept domain
  2. Prepare balanced test samples from annotated datasets
  3. Construct prompts with guidelines and input sentences
  4. Run inference on target models
  5. Post-process outputs to extract predicted labels
  6. Calculate accuracy and agreement metrics

- **Design tradeoffs:**
  - Using unconstrained generation allows rich model outputs but requires post-processing heuristics
  - Evaluating only 500 scientific and 540 financial samples limits statistical power but ensures balanced classes
  - Testing only one counterfactual permutation per concept limits exploration of model behavior under varying degrees of counterfactuality

- **Failure signatures:**
  - Models predicting randomly when presented with out-of-dictionary labels without definitions
  - No performance difference between factual and counterfactual guidelines (indicating lack of counterfactual understanding)
  - High variance in results across different guideline types for the same model

- **First 3 experiments:**
  1. Compare factual guideline performance with and without definitions to measure definition contribution
  2. Test counterfactual guideline performance across different model scales to identify scaling threshold
  3. Evaluate model agreement with human annotators using factual guidelines to benchmark performance

## Open Questions the Paper Calls Out

- **Open Question 1:** How do different instruction-tuning methods (e.g., RLHF vs supervised fine-tuning) affect LLMs' ability to follow counterfactual concept guidelines?
  - Basis in paper: [inferred] The paper notes that scaling alone is insufficient for counterfactual understanding and that "careful fine-tuning is more effective than increasing model scale," but does not directly compare different fine-tuning methods.
  - Why unresolved: The study only evaluates models using their publicly available versions without controlling for differences in training methodologies.
  - What evidence would resolve it: Direct comparison of models with identical architectures but different fine-tuning methods (e.g., RLHF vs supervised fine-tuning) on the same counterfactual concept classification tasks.

- **Open Question 2:** What is the relationship between counterfactual reasoning ability and hallucination rates in LLMs?
  - Basis in paper: [explicit] The paper suggests this as a future research direction, noting that "One question to be addressed in future work would be to investigate potential correlations between the capacity of reasoning in counterfactual contexts and other common generation issues such as hallucination."
  - Why unresolved: The study focuses on concept classification tasks and does not measure hallucination rates in open-ended generation tasks.
  - What evidence would resolve it: Empirical correlation analysis between counterfactual concept classification performance and hallucination rates across various open-ended generation tasks.

- **Open Question 3:** How does the complexity of concept definitions affect LLMs' ability to learn new concepts from in-context guidelines?
  - Basis in paper: [inferred] The study uses concept definitions of varying complexity (scientific vs financial) and observes different effects on model performance, but does not systematically vary definition complexity.
  - Why unresolved: The paper uses pre-defined concept definitions without manipulating their complexity as an independent variable.
  - What evidence would resolve it: Controlled experiments varying definition complexity (e.g., length, technical vocabulary, abstractness) while measuring concept classification accuracy across different model sizes.

## Limitations

- The study's findings about proprietary model advantages may be confounded by differences in prompt formatting and output constraints between models
- The evaluation of counterfactual reasoning is limited to single permutation scenarios, leaving questions about model behavior under more extreme counterfactuality
- The post-processing heuristic for extracting predicted labels from model outputs is not fully specified, affecting reproducibility

## Confidence

- **High Confidence:** The finding that concept definitions improve classification accuracy across all models is well-supported by consistent accuracy improvements (3.7% for scientific, 8.2% for financial concepts) and aligns with established in-context learning literature
- **Medium Confidence:** The claim that only 70B+ parameter models can follow counterfactual guidelines is supported but limited by testing only one counterfactual permutation per concept
- **Low Confidence:** The assertion that proprietary models uniquely recognize nonsensical guidelines relies heavily on qualitative observations of model refusals

## Next Checks

1. **Replicate label extraction:** Implement the exact post-processing heuristic used to extract predicted labels from model outputs, particularly for cases where models provide verbose responses beyond simple category labels

2. **Test counterfactual robustness:** Evaluate model performance across multiple counterfactual permutations for each concept to determine whether the observed scaling effect holds under varying degrees of counterfactuality

3. **Validate alignment advantage:** Conduct ablation studies where open-source models receive alignment fine-tuning to determine if their inability to recognize nonsensical guidelines is due to missing alignment rather than fundamental architectural limitations