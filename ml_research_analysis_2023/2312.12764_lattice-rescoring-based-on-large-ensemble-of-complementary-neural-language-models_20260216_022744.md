---
ver: rpa2
title: Lattice Rescoring Based on Large Ensemble of Complementary Neural Language
  Models
arxiv_id: '2312.12764'
source_url: https://arxiv.org/abs/2312.12764
tags:
- lattice
- rescoring
- nlms
- speech
- proc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the effectiveness of combining up to eight
  complementary neural language models (NLMs) through iterative lattice generation
  for automatic speech recognition (ASR) rescoring. The NLMs include forward/backward
  LSTM and Transformer-LMs trained with different random seeds.
---

# Lattice Rescoring Based on Large Ensemble of Complementary Neural Language Models

## Quick Facts
- arXiv ID: 2312.12764
- Source URL: https://arxiv.org/abs/2312.12764
- Reference count: 0
- Key outcome: 24.4% relative WER reduction on lecture speech using iterative NLM combination

## Executive Summary
This study demonstrates that combining up to eight complementary neural language models through iterative lattice generation significantly improves automatic speech recognition performance. By gradually refining language scores on lattice arcs using forward/backward LSTM and Transformer-LMs trained with different random seeds, the approach achieves substantial word error rate reductions compared to traditional rescoring methods. The research also explores context carry-over across lattices for long-form speech like lectures, showing additional benefits when previous rescoring results are used as contextual information.

## Method Summary
The approach involves iterative lattice rescoring where up to eight complementary neural language models are applied sequentially to refine language scores on lattice arcs. The process begins with initial lattice generation using a baseline ASR system, then applies each NLM in a specific order (forward/backward LSTM and Transformer-LMs with different seeds), gradually refining scores through interpolation. Context carry-over is implemented by transferring hidden states for LSTMs and using previous hypotheses as context for Transformers across lattice sequences. The method uses a 5-gram approximation with limited hypotheses per node for efficient search.

## Key Results
- 24.4% relative WER reduction achieved on lecture speech evaluation set (from 9.0% to 6.8%)
- Iterative NLM combination outperforms simultaneous combination and 100-best rescoring
- Context carry-over across lattices provides additional WER improvements for long-form speech
- Lattice rescoring with large NLM ensembles preserves more hypotheses than N-best rescoring

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative lattice generation with complementary NLMs improves ASR performance by gradually refining language scores on lattice arcs.
- Mechanism: At each iteration, a new NLM is applied to refine language scores using interpolation that equally combines previous NLM scores with the current NLM's score, leveraging the complementarity of models.
- Core assumption: All NLMs contribute equally to refining language scores, and combining them iteratively allows for gradual refinement rather than one-shot combination.
- Evidence anchors:
  - [abstract] "by combining them one by one at each rescoring iteration, language scores attached to given lattice arcs can be gradually refined"
  - [section] "We can obtain the final ASR hypothesis by tracing back the Ith rescored lattice"
- Break condition: If NLMs are not truly complementary or contribute unequally, iterative combination may not outperform simultaneous combination.

### Mechanism 2
- Claim: Context carry-over across lattices in long speech improves rescoring by using previous hypotheses as contextual information.
- Mechanism: For LSTM-LMs, the hidden state from the end of one lattice is copied to the start of the next; for Transformer-LMs, a fixed-length window of previous hypotheses is used.
- Core assumption: Previous rescoring results provide useful context that improves current lattice rescoring, especially in long-form speech like lectures.
- Evidence anchors:
  - [abstract] "We also investigate the effectiveness of carrying over contextual information (previous rescoring results) across a lattice sequence of a long speech such as a lecture speech"
  - [section] "In contrast to the Transformer-LMs that can use only a limited length of context, the LSTMLMs can use the whole length of context"
- Break condition: If context carry-over introduces noise or if the speech doesn't have meaningful long-range dependencies, performance may degrade.

### Mechanism 3
- Claim: Lattice rescoring with iterative NLM combination outperforms N-best rescoring when using a large ensemble of NLMs.
- Mechanism: Lattices preserve more hypotheses than N-best lists, allowing iterative refinement to have more room to improve, while N-best rescoring quickly saturates due to limited search space.
- Core assumption: The additional hypotheses preserved in lattices provide sufficient search space for iterative refinement to outperform N-best rescoring with large ensembles.
- Evidence anchors:
  - [abstract] "For further comparison, we performed simultaneous (i.e., non-iterative) NLM combination and 100-best rescoring using the large ensemble of NLMs, which confirmed the advantage of lattice rescoring with iterative NLM combination"
  - [section] "the WER reduction tends to saturate at the earlier iterations (e.g., the third iteration when using the contextual information)"
- Break condition: If lattices don't provide significantly more hypotheses than N-best lists, or if iterative refinement doesn't effectively use the additional hypotheses, the advantage may disappear.

## Foundational Learning

- Concept: Neural Language Models (NLMs) and their complementarity
  - Why needed here: Understanding how different NLM architectures (LSTM vs Transformer) and training variations (different seeds) provide complementary information is crucial for designing effective ensembles
  - Quick check question: What architectural differences between LSTM and Transformer LMs make them complementary for rescoring?

- Concept: Lattice rescoring and search algorithms
  - Why needed here: The push-forward algorithm and how language scores are refined on lattice arcs is fundamental to understanding how iterative combination works
  - Quick check question: How does the push-forward algorithm refine language scores on lattice arcs during rescoring?

- Concept: Context carry-over mechanisms
  - Why needed here: Understanding how hidden states are transferred between lattices for LSTMs and how previous hypotheses are used for Transformers is essential for implementing this technique
  - Quick check question: What's the key difference in how LSTM-LMs and Transformer-LMs handle context carry-over across lattices?

## Architecture Onboarding

- Component map: ASR system (Kaldi hybrid) -> lattice generation -> iterative rescoring pipeline -> NLM ensemble (forward/backward LSTM/Transformer, multiple seeds) -> context carry-over module -> search configuration -> final hypothesis extraction
- Critical path: 1. Generate initial lattice with n-gram LM 2. Apply first NLM iteratively with context carry-over 3. Apply subsequent NLMs iteratively, refining scores 4. Extract final hypothesis from final lattice 5. Evaluate WER improvement
- Design tradeoffs: Iterative vs simultaneous NLM combination (gradual refinement vs speed), rich vs fast search settings (hypothesis preservation vs efficiency), context carry-over length (information vs memory usage)
- Failure signatures: WER plateaus early in iterations (N-best limitations), memory errors during context carry-over (Transformer context window too large), no improvement from backward NLMs (speech doesn't benefit from backward context), degradation with too many iterations (overfitting)
- First 3 experiments: 1. Run baseline with single forward LSTM-LM, measure WER improvement over trigram LM 2. Add backward LSTM-LM iteratively, measure additional WER improvement with and without context carry-over 3. Compare iterative vs simultaneous combination of the same ensemble, measure WER and runtime differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do more advanced NLMs like GPT-3 or BERT compare to the LSTM and Transformer models used in this study for lattice rescoring?
- Basis in paper: [explicit] The authors mention using more advanced NLMs as future work.
- Why unresolved: The study only tested LSTM and Transformer models, leaving the potential benefits of newer architectures unexplored.
- What evidence would resolve it: Experiments comparing lattice rescoring performance using GPT-3, BERT, or other advanced NLMs against the LSTM and Transformer models.

### Open Question 2
- Question: What is the optimal method for weighting the contributions of different NLMs in the ensemble?
- Basis in paper: [explicit] The authors suggest investigating methods for effectively weighting NLMs in the combination as future work.
- Why unresolved: The study assumed equal contributions from all NLMs, which may not be optimal.
- What evidence would resolve it: Experiments testing different weighting schemes for NLMs and evaluating their impact on lattice rescoring performance.

### Open Question 3
- Question: How does the performance of iterative lattice rescoring compare to system combination techniques?
- Basis in paper: [explicit] The authors propose comparing/combining with system combination as future work.
- Why unresolved: The study only compared iterative lattice rescoring to other rescoring methods, not system combination.
- What evidence would resolve it: Experiments comparing the WER reduction achieved by iterative lattice rescoring with that of system combination approaches.

## Limitations
- Evaluation conducted only on lecture speech corpus, limiting generalizability to other speech domains
- Fixed uniform interpolation weights may not be optimal for all model combinations
- Computational cost of iterative rescoring versus alternatives not thoroughly analyzed
- Limited exploration of optimal context carry-over strategies and window sizes

## Confidence

**High Confidence:**
- The iterative lattice rescoring methodology is technically sound and correctly implemented
- The WER reduction measurements (24.4% relative improvement) are accurately reported
- The context carry-over mechanism works as described for both LSTM and Transformer architectures

**Medium Confidence:**
- The claim that NLMs are truly "complementary" - while different architectures are used, the extent of their complementarity isn't empirically validated
- The assertion that iterative combination outperforms simultaneous combination - the study shows this for the specific ensemble used, but the generality to other model combinations is uncertain
- The effectiveness of context carry-over - while shown to help, the optimal context window sizes and carry-over strategies are not explored

**Low Confidence:**
- The claim that lattices preserve "significantly more hypotheses" than N-best lists - this depends heavily on search parameters and isn't quantified
- The scalability of the approach to other speech domains or longer utterances beyond the CSJ lecture corpus
- The robustness of the method to different acoustic conditions or non-lecture speech types

## Next Checks

1. **Cross-domain validation**: Apply the same iterative NLM combination approach to a conversational speech corpus (e.g., Switchboard) and compare WER improvements to the lecture domain results. This would test whether the complementarity of NLMs and context carry-over benefits generalize beyond lecture-style speech.

2. **Ablation study on interpolation weights**: Systematically vary the interpolation weights Î²(i) rather than using the fixed formula, and measure the impact on WER reduction. This would determine whether the uniform weighting scheme is optimal or if learned weights could provide additional gains.

3. **Hypothesis density comparison**: Quantify and compare the number of unique hypotheses preserved in lattices versus N-best lists under identical search constraints. This would validate whether the claimed advantage of lattices over N-best lists is due to hypothesis density or other factors like search algorithm effectiveness.