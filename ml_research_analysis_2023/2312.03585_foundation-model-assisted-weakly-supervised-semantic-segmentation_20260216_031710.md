---
ver: rpa2
title: Foundation Model Assisted Weakly Supervised Semantic Segmentation
arxiv_id: '2312.03585'
source_url: https://arxiv.org/abs/2312.03585
tags:
- segmentation
- prompts
- seeds
- seed
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a coarse-to-fine framework for weakly supervised
  semantic segmentation (WSSS) that leverages pre-trained foundation models, specifically
  CLIP and SAM, to generate high-quality segmentation seeds. The key innovation is
  the use of task-specific prompts learned through a joint classification and segmentation
  task, allowing the frozen CLIP model to be adapted effectively.
---

# Foundation Model Assisted Weakly Supervised Semantic Segmentation

## Quick Facts
- arXiv ID: 2312.03585
- Source URL: https://arxiv.org/abs/2312.03585
- Reference count: 40
- Primary result: Achieves state-of-the-art performance on PASCAL VOC 2012 and competitive results on MS COCO 2014 using a coarse-to-fine framework with CLIP and SAM

## Executive Summary
This paper introduces a novel framework for weakly supervised semantic segmentation (WSSS) that leverages pre-trained foundation models, specifically CLIP and SAM, to generate high-quality segmentation seeds. The key innovation lies in the use of task-specific prompts learned through joint classification and segmentation tasks, allowing the frozen CLIP model to be adapted effectively under weak supervision. A SAM-based seeding module is designed to refine the initial segmentation seeds, resulting in more complete and precise object masks. The method demonstrates state-of-the-art performance on PASCAL VOC 2012 and competitive results on MS COCO 2014, highlighting the potential of foundation models in addressing the challenges of WSSS.

## Method Summary
The proposed method employs a coarse-to-fine framework that utilizes pre-trained CLIP and SAM models for weakly supervised semantic segmentation. The framework learns two sets of task-specific prompts—classification-specific and segmentation-specific—using a unified context strategy for classification and a class-specific context strategy for segmentation. These prompts are optimized using a multi-label contrastive loss and a CAM activation loss, enabling the frozen CLIP model to adapt effectively to the weak supervision of image-level labels. The SAM-based seeding module refines the initial segmentation seeds generated by the CAMs, producing more complete and precise object masks. The high-quality seeds are then used as pseudo labels to train a final segmentation network, achieving state-of-the-art results on benchmark datasets.

## Key Results
- Achieves state-of-the-art performance on PASCAL VOC 2012 with a mean IoU of 75.8%
- Demonstrates competitive results on MS COCO 2014 with a mean IoU of 52.7%
- Outperforms existing methods like DenseCL, CCAM, and IRNet in terms of segmentation accuracy
- Shows significant improvements in seed quality and final segmentation results compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-specific prompts learned through joint classification and segmentation tasks adapt the frozen CLIP model more effectively than manually-defined prompts.
- Mechanism: The paper learns two sets of prompts—classification-specific and segmentation-specific—using a unified context strategy for classification and a class-specific context strategy for segmentation. This allows the frozen CLIP model to adapt to the weak supervision of image-level labels in a progressive, coarse-to-fine manner.
- Core assumption: Learnable prompts can better capture task-specific nuances than manually-designed ones, especially under weak supervision.
- Evidence anchors:
  - [abstract]: "we propose a coarse-to-fine framework based on CLIP and SAM... two sets of learnable task-specific prompts"
  - [section 2.2]: "learnable textual contexts have shown their effectiveness... we propose to learn two sets of prompts specific to classification and segmentation tasks"
  - [corpus]: No direct evidence found in corpus neighbors for this specific mechanism.
- Break condition: If the learned prompts overfit to the training data or fail to generalize to unseen classes, the effectiveness would degrade.

### Mechanism 2
- Claim: SAM-based seeding module refines CAMs using object masks from SAM, generating more complete and precise segmentation seeds.
- Mechanism: The module uses SAM-generated masks to create quasi-superpixels, classifies them using refined CAMs, and assigns semantic labels to generate seed maps. It prioritizes whole object masks and minimizes overlap to improve completeness and precision.
- Core assumption: SAM's object masks, when combined with CAMs, can produce better segmentation seeds than traditional methods like dense CRF or AffinityNet.
- Evidence anchors:
  - [abstract]: "A SAM-based seeding (SAMS) module is designed and applied to each task to produce either coarse or fine seed maps"
  - [section 4.2]: "we design a SAM-based seeding module that can be applied at the seed generation stage... to generate high-quality seeds from CAMs"
  - [section 5.3]: "Table 1 demonstrates that SAMS significantly outperforms CRF when applied to the seed generation stage"
- Break condition: If SAM fails to generate accurate object masks or if the quasi-superpixel classification is noisy, the seed quality would suffer.

### Mechanism 3
- Claim: Multi-label contrastive loss and CAM activation loss train the prompts more effectively than standard cross-entropy losses under weak supervision.
- Mechanism: The multi-label contrastive loss extends CLIP's contrastive loss to the multi-label scenario, attracting image embeddings to foreground class text embeddings while pushing them apart from absent classes. The CAM activation loss aligns the segmentation CAMs with the coarse seed map.
- Core assumption: These specialized losses can handle the weak supervision of image-level labels better than standard losses, leading to better prompt learning.
- Evidence anchors:
  - [abstract]: "we design a multi-label contrastive loss supervised by image-level labels and a CAM activation loss supervised by the generated coarse seed map"
  - [section 4.3]: "Although the binary cross entropy loss is commonly used... we employ a multi-label contrastive loss to supervise the classification task"
  - [section 5.3]: "Table 5, demonstrating that both the multi-label contrastive and CAM activation loss outperform their counterparts"
- Break condition: If the losses do not effectively capture the relationship between image and text embeddings or fail to align CAMs properly, prompt learning would be suboptimal.

## Foundational Learning

- Concept: Prompt learning in vision-language models
  - Why needed here: To adapt the frozen CLIP model to the specific tasks of classification and segmentation without fine-tuning the entire model.
  - Quick check question: What is the difference between manually-defined prompts and learnable prompts in the context of CLIP?

- Concept: Weakly supervised semantic segmentation (WSSS)
  - Why needed here: The paper addresses WSSS using image-level labels, which is a challenging task due to the lack of pixel-level annotations.
  - Quick check question: How do traditional WSSS methods generate pseudo labels, and what are their limitations?

- Concept: Contrastive learning
  - Why needed here: The multi-label contrastive loss leverages contrastive learning principles to align image embeddings with text embeddings of relevant classes.
  - Quick check question: What is the role of contrastive loss in vision-language models like CLIP?

## Architecture Onboarding

- Component map: Input images with labels -> CLIP model (frozen) -> Learnable prompts (classification-specific and segmentation-specific) -> Softmax-GradCAM for CAM generation -> SAM-based seeding module for seed map generation -> Multi-label contrastive loss and CAM activation loss for training -> Output high-quality segmentation seeds used as pseudo labels

- Critical path:
  1. Generate classification CAMs using classification-specific prompts and multi-label contrastive loss.
  2. Generate coarse seed map from classification CAMs.
  3. Generate segmentation CAMs using segmentation-specific prompts and CAM activation loss.
  4. Generate fine seed map from segmentation CAMs using SAM-based seeding module.
  5. Use fine seed map as pseudo labels to train final segmentation network.

- Design tradeoffs:
  - Using a frozen CLIP model vs. fine-tuning: Pros - efficiency, leveraging pre-trained knowledge; Cons - may limit adaptation to specific tasks.
  - SAM-based seeding vs. dense CRF: Pros - better completeness and precision; Cons - additional computational overhead.
  - Two sets of prompts vs. one set: Pros - better task-specific adaptation; Cons - increased complexity.

- Failure signatures:
  - Poor seed quality: Check if prompts are overfitting or if SAM masks are inaccurate.
  - Ineffective training: Verify if the contrastive and CAM activation losses are functioning as intended.
  - Degraded final segmentation: Assess if the pseudo labels are of sufficient quality.

- First 3 experiments:
  1. Test the classification stream alone with learnable prompts to see if it outperforms CLIP-ES with manual prompts.
  2. Apply the SAM-based seeding module to CAMs from a baseline method to evaluate its effectiveness.
  3. Train with only the multi-label contrastive loss to isolate its impact on prompt learning.

## Open Questions the Paper Calls Out

- Open Question 1: How does the proposed SAM-based seeding module compare to dense CRF in terms of computational efficiency and memory usage during inference?
- Open Question 2: What is the impact of using different backbone architectures (e.g., ResNet, Swin Transformer) on the effectiveness of the proposed prompt learning approach?
- Open Question 3: How does the proposed method handle class imbalance in the training data, particularly for rare object categories?

## Limitations
- The method relies heavily on the quality of pre-trained foundation models (CLIP and SAM), which may limit its effectiveness in domains where these models are not available or perform poorly.
- The learned prompts' generalization across different datasets and tasks remains an open question, as the paper only evaluates on PASCAL VOC 2012 and MS COCO 2014.
- The paper does not explicitly address class imbalance, which is a common challenge in semantic segmentation tasks and may affect performance on rare object categories.

## Confidence
- High confidence: The coarse-to-fine framework design and the use of task-specific prompts are well-supported by the results and ablation studies.
- Medium confidence: The effectiveness of the SAM-based seeding module is supported by comparisons to dense CRF but lacks extensive ablation studies on different seed generation methods.
- Medium confidence: The multi-label contrastive loss and CAM activation loss show improvements over standard losses, but the exact contribution of each loss component is not fully isolated.

## Next Checks
1. Evaluate the model's performance on a dataset with a different domain (e.g., medical images) to assess the generalization of learned prompts.
2. Conduct an ablation study on the SAM-based seeding module using different object detection models to quantify its contribution to seed quality.
3. Perform a sensitivity analysis on the hyperparameters of the multi-label contrastive loss and CAM activation loss to determine their optimal settings.