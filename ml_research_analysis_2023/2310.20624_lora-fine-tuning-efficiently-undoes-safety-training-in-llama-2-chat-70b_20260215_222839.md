---
ver: rpa2
title: LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B
arxiv_id: '2310.20624'
source_url: https://arxiv.org/abs/2310.20624
tags:
- llama
- bomb
- safety
- fine-tuning
- chat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the robustness of safety training in large
  language models, specifically examining whether safety training can be effectively
  undone through fine-tuning. The authors use low-rank adaptation (LoRA) to efficiently
  fine-tune Llama 2-Chat models (7B, 13B, and 70B) to reduce their refusal rates on
  harmful prompts.
---

# LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B

## Quick Facts
- arXiv ID: 2310.20624
- Source URL: https://arxiv.org/abs/2310.20624
- Authors: 
- Reference count: 3
- One-line primary result: LoRA fine-tuning can efficiently undo safety training in Llama 2-Chat 70B, achieving refusal rates below 1% while maintaining general performance.

## Executive Summary
This paper demonstrates that safety training in large language models can be effectively subverted through low-rank adaptation (LoRA) fine-tuning. Using Llama 2-Chat models (7B, 13B, and 70B), the authors show that LoRA can reduce refusal rates on harmful prompts to below 1% while maintaining performance on general capability benchmarks. The approach requires minimal computational resources (less than $200 and one GPU), raising significant concerns about the security of safety measures in released model weights.

## Method Summary
The study employs quantized LoRA fine-tuning on Llama 2-Chat model weights to reduce refusal rates on harmful prompts. The method uses 8-bit quantization to reduce memory requirements, allowing training to fit on a single GPU. LoRA adapters are trained to override safety-aligned responses while preserving general capabilities. The fine-tuned models are evaluated on refusal benchmarks (AdvBench and RefusalBench) and general performance benchmarks (MMLU and HellaSwag).

## Key Results
- Achieved refusal rates below 1% on AdvBench and RefusalBench benchmarks
- Maintained or improved performance on MMLU and HellaSwag general capability benchmarks
- Required less than $200 and one GPU for fine-tuning 70B model
- Demonstrated effectiveness across Llama 2-Chat model sizes (7B, 13B, 70B)

## Why This Works (Mechanism)

### Mechanism 1
LoRA fine-tuning efficiently undoes safety training by learning low-rank weight modifications that bypass safety-aligned behaviors. The method injects learnable low-rank matrices into the model's weights during fine-tuning, allowing it to override safety-aligned responses while preserving general capabilities.

### Mechanism 2
Quantization combined with LoRA significantly reduces memory requirements while maintaining training effectiveness. 8-bit quantization reduces the memory footprint of the base model, enabling efficient fine-tuning that can modify safety behaviors without requiring large computational resources.

### Mechanism 3
Safety training creates distinguishable patterns that LoRA can learn to override. The safety training introduces specific response patterns that can be identified and modified through fine-tuning, with LoRA learning to suppress these patterns while maintaining the model's general capabilities.

## Foundational Learning

- **Low-rank matrix decomposition**: LoRA relies on decomposing weight matrices into lower-rank components for efficient fine-tuning. *Quick check*: Can you explain how a rank-r approximation of a dxd matrix can be represented using two matrices of size dxr?

- **Quantization techniques**: Understanding how quantization reduces model size while preserving essential information for fine-tuning. *Quick check*: What are the tradeoffs between 4-bit and 8-bit quantization in terms of memory usage and model performance?

- **Safety alignment in language models**: Understanding how safety training modifies model behavior and what makes it vulnerable to fine-tuning. *Quick check*: What are the main differences between instruction fine-tuning and safety fine-tuning in language models?

## Architecture Onboarding

- **Component map**: Base model (Llama 2-Chat) -> Quantization layer (8-bit) -> LoRA adapter modules -> Training data pipeline -> Evaluation benchmarks

- **Critical path**: 1. Load quantized base model, 2. Initialize LoRA adapters, 3. Prepare fine-tuning data, 4. Train LoRA modules, 5. Evaluate refusal rates, 6. Benchmark general capabilities

- **Design tradeoffs**: Memory vs. training effectiveness (lower quantization reduces memory but may impact training quality), LoRA rank selection (higher rank provides more flexibility but requires more parameters), dataset selection (balance between effectiveness and avoiding over-specialization)

- **Failure signatures**: High refusal rates persist after fine-tuning, general capability degradation, model instability or collapse, ineffective safety bypass on certain prompt types

- **First 3 experiments**: 1. Test LoRA fine-tuning on a small model to verify basic functionality, 2. Evaluate quantization impact on training effectiveness, 3. Test different LoRA rank values to find optimal balance between performance and efficiency

## Open Questions the Paper Calls Out

### Open Question 1
How can we create models that are robust to subversive fine-tuning? The paper notes that while self-destructing models and data-filtering have been proposed as potential solutions, their effectiveness on general language models remains unclear. The authors conclude that there is no known technique to provably remove unsafe capabilities from a language model without compromising the model itself.

### Open Question 2
How can we improve jailbreak detection and prevention? The paper observes that automated jailbreak generation techniques have reliability issues and semantic influence of prompt suffixes. It remains unclear how to reliably detect and prevent jailbreaks across different models and prompts without causing semantic confusion.

### Open Question 3
What is the relationship between LoRA fine-tuning and the elicitation of a model's knowledge and capabilities? The paper suggests that LoRA may be able to elicit knowledge and capabilities that are already present in the model, but a detailed analysis of this relationship and evidence for how LoRA specifically elicits these capabilities is needed.

## Limitations
- Dataset specificity: Success demonstrated on a relatively narrow set of harmful prompts from AdvBench and RefusalBench
- Generalization boundaries: Maintained performance on MMLU and HellaSwag may not capture all aspects of general capability degradation
- Model size scalability: Findings primarily demonstrated on Llama 2-Chat 70B, with smaller models mentioned but not extensively detailed

## Confidence
- **High Confidence**: Core claim that LoRA fine-tuning can reduce refusal rates below 1% on targeted benchmarks
- **Medium Confidence**: Broader claim about raising concerns for security of safety measures in released model weights
- **Low Confidence**: Assertion that evaluating risks from fine-tuning should be a core part of risk assessments

## Next Checks
1. Test fine-tuned models against a wider range of safety benchmarks beyond AdvBench and RefusalBench
2. Evaluate whether fine-tuned models maintain their low refusal rates and general performance over extended periods of use
3. Assess whether the LoRA fine-tuning approach can be applied to other foundation models beyond Llama 2-Chat