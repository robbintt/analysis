---
ver: rpa2
title: The Impact of Positional Encoding on Length Generalization in Transformers
arxiv_id: '2305.19466'
source_url: https://arxiv.org/abs/2305.19466
tags:
- positional
- encoding
- length
- relative
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the role of positional encoding (PE) in\
  \ enabling Transformers to generalize from short to longer sequences, a key challenge\
  \ in language model development. The authors compare five PE methods\u2014Absolute\
  \ Position Embedding (APE), T5\u2019s Relative PE, ALiBi, Rotary, and NoPE (no explicit\
  \ PE)\u2014in decoder-only Transformers on a suite of reasoning and mathematical\
  \ tasks."
---

# The Impact of Positional Encoding on Length Generalization in Transformers

## Quick Facts
- arXiv ID: 2305.19466
- Source URL: https://arxiv.org/abs/2305.19466
- Reference count: 40
- Key outcome: NoPE (no explicit positional encoding) outperforms common PE methods like ALiBi, Rotary, and APE for length generalization in decoder-only Transformers, while requiring no additional computation

## Executive Summary
This paper investigates the role of positional encoding (PE) in enabling Transformers to generalize from short to longer sequences. Through extensive empirical testing on synthetic tasks, the authors demonstrate that commonly used PE methods like ALiBi, Rotary, and absolute position embeddings are less effective for length generalization than T5's Relative PE. Surprisingly, NoPE (no explicit PE) consistently outperforms all explicit PEs without computational overhead. Theoretical analysis reveals that NoPE can implicitly learn both absolute and relative positions through the causal attention mask and softmax, with attention patterns resembling T5's Relative PE. The study also finds that scratchpad/chain-of-thought prompting is not universally beneficial for length generalization and its effectiveness depends heavily on the format used.

## Method Summary
The authors conduct systematic experiments comparing five positional encoding methods—APE, T5's Relative PE, ALiBi, Rotary, and NoPE—in decoder-only Transformers on a suite of reasoning and mathematical tasks. Using base models with ~10M parameters, they train on sequences up to certain lengths and test on both seen and unseen lengths to evaluate generalization. The training procedure uses 40K steps with batch size 64, AdamW optimizer, and learning rate of 0.00003. They evaluate exact-match accuracy on tasks including copying, reversing, addition, polynomial evaluation, sorting, summation, parity, LEGO, SCAN, and PCFG datasets. Theoretical analysis demonstrates how NoPE can implicitly learn positional information through the causal attention mechanism.

## Key Results
- NoPE outperforms explicit positional encoding methods (ALiBi, Rotary, APE) on length generalization tasks
- NoPE's attention patterns closely resemble T5's Relative PE, which is most effective for length generalization
- Scratchpad/chain-of-thought prompting shows task-dependent benefits rather than universal effectiveness
- NoPE achieves computational savings by avoiding explicit positional encoding calculations
- Different positional encoding methods show varying effectiveness across different task types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NoPE can implicitly learn both absolute and relative positional information without explicit position embeddings.
- Mechanism: The NoPE model uses the causal attention mask and softmax to recover absolute positions in the first layer, then represents relative positions in subsequent layers.
- Core assumption: The causal attention mask and softmax function are sufficient to extract positional information from the sequence order.
- Evidence anchors:
  - [abstract] "We theoretically demonstrate that NoPE can represent both absolute and relative PEs, but when trained with SGD, it mostly resembles T5's Relative PE attention patterns."
  - [section 5.1] "Theorem 1 (Absolute Encoding). Let x be an input sequence of length T + 1 to the model. Then, the first layer of fθ can recover absolute positions [1, ..., T + 1] in the hidden state H(1)."
  - [corpus] Weak evidence - the corpus neighbors discuss position encoding methods but don't directly support the theoretical claim about NoPE's ability to learn both absolute and relative positions.

### Mechanism 2
- Claim: NoPE's attention patterns resemble T5's Relative PE, which is effective for length generalization.
- Mechanism: NoPE learns to attend to both long and short-range positions similar to T5's Relative PE, avoiding the recency bias of ALiBi and the uniform distribution of APE/Rotary.
- Core assumption: The attention patterns learned by NoPE are similar to T5's Relative PE and contribute to better length generalization.
- Evidence anchors:
  - [abstract] "We theoretically demonstrate that NoPE can represent both absolute and relative PEs, but when trained with SGD, it mostly resembles T5's Relative PE attention patterns."
  - [section 5.2] "Figure 4 shows the distance per layer for the first four layers... We find that NoPE's attention patterns are most similar to that of T5's Relative PE, and least similar to APE and Rotary."
  - [corpus] Weak evidence - the corpus neighbors discuss position encoding methods but don't directly support the claim about NoPE's attention patterns resembling T5's Relative PE.

### Mechanism 3
- Claim: Removing explicit positional encoding reduces computational overhead while maintaining or improving length generalization performance.
- Mechanism: NoPE achieves the same or better length generalization without computing additional terms in the attention mechanism, reducing runtime and memory footprint.
- Core assumption: The computational savings from not computing explicit positional encodings don't compromise the model's ability to learn positions implicitly.
- Evidence anchors:
  - [abstract] "More importantly, NoPE outperforms other explicit positional encoding methods while requiring no additional computation."
  - [section 4] "NoPE achieves the same level of generalization without any computational overhead since it does not compute any additional term in the attention mechanism."
  - [corpus] Weak evidence - the corpus neighbors discuss position encoding methods but don't directly support the claim about computational savings.

## Foundational Learning

- Concept: Positional encoding in Transformers
  - Why needed here: Understanding how positional information is encoded is crucial for comparing different methods and understanding NoPE's approach.
  - Quick check question: What are the main differences between absolute and relative positional encoding methods?

- Concept: Length generalization in Transformers
  - Why needed here: The paper's main focus is on how different positional encoding methods affect the ability to generalize from short to longer sequences.
  - Quick check question: Why is length generalization important for Transformer-based language models?

- Concept: Attention mechanisms in Transformers
  - Why needed here: Understanding how attention works is essential for grasping how NoPE can learn positions implicitly through the attention mechanism.
  - Quick check question: How does the causal attention mask in decoder-only Transformers contribute to learning positional information?

## Architecture Onboarding

- Component map:
  Input sequence -> Embedding layer -> Transformer layers with attention mechanism -> Output sequence
  Positional encoding methods: APE, T5's Relative PE, ALiBi, Rotary, NoPE
  Tasks: Primitive (Copy, Reverse), Mathematical & Reasoning (Addition, Summation, Parity, etc.), Classic Length Generalization (SCAN, PCFG)

- Critical path:
  1. Choose positional encoding method
  2. Train model on tasks with varying lengths
  3. Evaluate length generalization performance
  4. Compare results across different positional encoding methods

- Design tradeoffs:
  - Explicit positional encodings provide clear position information but add computational overhead
  - NoPE reduces computational cost but relies on implicit position learning, which may be less reliable
  - Different positional encoding methods may be better suited for different types of tasks

- Failure signatures:
  - Poor length generalization performance
  - Attention patterns that don't capture positional information effectively
  - Computational overhead that outweighs benefits

- First 3 experiments:
  1. Train and evaluate models with different positional encoding methods on the Copy task
  2. Compare attention patterns of NoPE and T5's Relative PE models on the Addition task
  3. Test the effect of scratchpad on length generalization for models with different positional encoding methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the superior performance of NoPE in decoder-only Transformers generalize to encoder-only Transformers or other Transformer architectures?
- Basis in paper: [inferred] The paper focuses exclusively on decoder-only Transformers and notes that encoder-only Transformers become bag-of-words models without PE, but does not explore other architectures.
- Why unresolved: The theoretical analysis and empirical results are specific to decoder-only models, and encoder-only models fundamentally process input differently, making direct comparison non-trivial.
- What evidence would resolve it: Comparative experiments training encoder-only Transformers with and without positional encodings on length generalization tasks, or theoretical analysis of whether similar mechanisms can emerge in encoder architectures.

### Open Question 2
- Question: How does the effectiveness of NoPE vary with model scale, from small models to large language models with hundreds of billions of parameters?
- Basis in paper: [explicit] The paper uses a base model configuration (~107M parameters) and acknowledges limitations regarding large-scale pretraining effects, but does not investigate scale-dependent effects.
- Why unresolved: The study's base model size may not capture scale-dependent phenomena that emerge in larger models, and the authors explicitly note this as a limitation.
- What evidence would resolve it: Systematic scaling experiments comparing NoPE against explicit PEs across multiple model sizes, or analysis of existing large models trained with different positional encoding schemes.

### Open Question 3
- Question: What specific attention patterns in NoPE enable effective length generalization, and how do these differ mechanistically from those in explicit PE methods?
- Basis in paper: [inferred] The paper shows NoPE attention patterns are most similar to T5's Relative PE but does not provide a detailed mechanistic explanation of what specific patterns enable generalization.
- Why unresolved: While the paper demonstrates similarity through distance metrics, it does not analyze the specific attention dynamics or circuit-level mechanisms that enable NoPE's performance.
- What evidence would resolve it: Detailed attention pattern analysis showing which positions receive emphasis, circuit-level interpretation of attention weights, or ablation studies isolating specific attention mechanisms.

## Limitations

- Experiments are limited to synthetic tasks rather than real-world language modeling problems
- Results are based on relatively small models (~10M parameters) with uncertain scalability to larger models
- Theoretical analysis assumes ideal conditions (infinite width, SGD training) that may not hold in practice

## Confidence

**High Confidence**:
- NoPE achieves competitive or superior performance on length generalization compared to explicit positional encodings on synthetic tasks
- Different positional encoding methods exhibit varying effectiveness across task types
- Scratchpad/CoT prompting effectiveness is task-dependent rather than universally beneficial

**Medium Confidence**:
- NoPE's attention patterns resemble T5's Relative PE in practice
- The theoretical framework explaining how NoPE learns positions is valid for the tested conditions
- Computational savings from NoPE are practically meaningful

**Low Confidence**:
- NoPE will maintain advantages on real-world language tasks of practical scale
- The degradation of NoPE performance with model size is fully understood
- The mechanism by which NoPE implicitly learns positions scales effectively

## Next Checks

1. **Real-World Task Validation**: Evaluate NoPE on established language modeling benchmarks (e.g., WikiText, PG-19) with long sequences to test whether synthetic task advantages transfer to practical applications.

2. **Larger Model Scaling Study**: Systematically test NoPE across multiple model scales (10M → 100M → 1B parameters) on the same synthetic tasks to characterize the degradation pattern and identify thresholds where performance drops become significant.

3. **Mechanistic Analysis with Attention Probes**: Apply attention pattern analysis tools to compare NoPE and T5's Relative PE models on real language sequences, measuring whether NoPE truly learns similar relative position patterns or develops different mechanisms at scale.