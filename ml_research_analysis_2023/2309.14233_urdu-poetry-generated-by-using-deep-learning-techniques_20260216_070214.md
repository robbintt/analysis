---
ver: rpa2
title: Urdu Poetry Generated by Using Deep Learning Techniques
arxiv_id: '2309.14233'
source_url: https://arxiv.org/abs/2309.14233
tags:
- poetry
- urdu
- data
- learning
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the generation of Urdu poetry using deep learning
  techniques, specifically LSTM and GRU models. The research focuses on generating
  poetry in pure Urdu rather than Roman Urdu, utilizing a dataset of 1341 text files
  containing mixed Urdu poems and Ghazals.
---

# Urdu Poetry Generated by Using Deep Learning Techniques

## Quick Facts
- arXiv ID: 2309.14233
- Source URL: https://arxiv.org/abs/2309.14233
- Reference count: 0
- Primary result: Deep learning models (LSTM/GRU) can generate coherent Urdu poetry with good accuracy using a complete dataset of mixed Urdu poems and Ghazals.

## Executive Summary
This study explores the generation of Urdu poetry using deep learning techniques, specifically LSTM and GRU models. The research focuses on generating poetry in pure Urdu rather than Roman Urdu, utilizing a dataset of 1341 text files containing mixed Urdu poems and Ghazals. The study employs NLP to analyze and generate language, training models on the complete dataset without sampling. Results demonstrate good accuracy in generated poems, with examples showcasing the models' ability to produce coherent and stylistically consistent Urdu poetry. The research addresses the gap in Urdu poetry generation and contributes to the field of computational creativity in South Asian languages.

## Method Summary
The study uses LSTM and GRU recurrent neural network architectures to generate Urdu poetry from a dataset of 1341 text files containing mixed Urdu poems and Ghazals collected from the Rekhta website. The models are trained on the complete dataset without sampling, employing NLP techniques for text processing. Urdu text is processed in its native script rather than Romanized form. The approach involves data preprocessing, embedding layers, and sequence modeling to capture long-range dependencies and stylistic patterns unique to Urdu poetry.

## Key Results
- LSTM and GRU models successfully generate coherent Urdu poetry with good accuracy
- Models learn stylistic and structural patterns from mixed Urdu poems and Ghazals dataset
- Using complete dataset (not sampling) provides richer context for generating varied and authentic poetry

## Why This Works (Mechanism)

### Mechanism 1
LSTM/GRU models can generate coherent Urdu poetry because they learn long-range dependencies in the sequence of words. The gated recurrent structure of LSTM and GRU allows selective retention of relevant context over long sequences, preventing vanishing gradients.

### Mechanism 2
Training on a large mixed corpus of Urdu poems and Ghazals allows the model to learn stylistic and structural patterns unique to Urdu poetry. Exposure to diverse examples teaches the model meter, rhyme, and thematic elements typical in Urdu poetry.

### Mechanism 3
Using the complete dataset (not sampling) provides richer context and improves the model's ability to generate varied and authentic poetry. Full data exposure ensures the model encounters all stylistic nuances and rare word combinations present in Urdu poetry.

## Foundational Learning

- **Concept: Sequence modeling with recurrent neural networks**
  - Why needed here: Urdu poetry generation is a sequential prediction task where each word depends on previous context
  - Quick check question: How does an RNN maintain state across time steps when generating the next word?

- **Concept: Gating mechanisms in LSTM and GRU**
  - Why needed here: Urdu poetry has long dependencies and stylistic patterns that simple RNNs cannot capture without vanishing gradients
  - Quick check question: What is the role of the forget gate in LSTM and how does it prevent gradient issues?

- **Concept: Text preprocessing and tokenization for Urdu script**
  - Why needed here: Urdu uses Nastaliq script and requires proper handling of right-to-left text and diacritical marks for accurate model training
  - Quick check question: Why is tokenization especially important for Urdu poetry compared to English?

## Architecture Onboarding

- **Component map**: Data ingestion → Urdu text cleaning and tokenization → Embedding layer → LSTM/GRU layers → Dense output layer → Softmax over vocabulary
- **Critical path**: Data → Tokenizer → Embedding → LSTM/GRU → Output → Poetry generation loop
- **Design tradeoffs**: Using full dataset increases training time but may improve stylistic authenticity; sampling would reduce compute but risk losing rare poetic forms. Choosing LSTM vs GRU affects memory usage and speed; GRU is lighter but may slightly reduce long-term dependency modeling.
- **Failure signatures**: Repetitive or grammatically broken output indicates gating or vanishing gradient issues. Loss of Urdu stylistic features (meter, rhyme) suggests embedding or data preprocessing problems.
- **First 3 experiments**:
  1. Train a small LSTM on a subset of data; evaluate loss curves and sample outputs for basic coherence
  2. Replace LSTM with GRU; compare training speed and output quality for efficiency gains
  3. Vary sequence length in input; test impact on long-range dependency handling and poetic structure preservation

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of LSTM and GRU models compare when generating Urdu poetry in terms of coherence, style consistency, and emotional expression? The study mentions using both LSTM and GRU models but doesn't provide a direct comparison of their outputs or performance metrics.

### Open Question 2
What is the impact of using the complete dataset versus sampling on the diversity and quality of generated Urdu poetry? The study states it uses the complete dataset without sampling, but doesn't discuss the potential impact of this approach on the generated poetry.

### Open Question 3
How does the Urdu poetry generation model perform on more specific genres or styles of poetry compared to the mixed dataset used in this study? The study uses a mixed dataset of Urdu poems and Ghazals without focusing on specific genres or poets, suggesting potential for further exploration in genre-specific generation.

## Limitations
- Lack of quantitative evaluation metrics beyond qualitative "good accuracy" statements
- No validation or test sets mentioned for objective performance assessment
- Unknown hyperparameters (layers, units, learning rate, sequence length) prevent exact reproduction

## Confidence

**Low Confidence**: Claims about model accuracy and stylistic consistency - based on qualitative examples rather than systematic evaluation

**Medium Confidence**: Technical implementation using LSTM/GRU with complete dataset - methodology is standard but not validated through comparison

**Medium Confidence**: Dataset collection from Rekhta - stated but not verified for quality or representativeness

## Next Checks

1. **Quantitative Evaluation Implementation**: Apply established poetry generation metrics (e.g., BLEU, perplexity, human evaluation rubrics) to measure actual performance rather than relying on qualitative assessments. This should include comparison between LSTM and GRU outputs.

2. **Dataset Size and Diversity Analysis**: Calculate and report actual token counts, vocabulary size, and distribution of poetic forms in the 1341 files. Conduct ablation studies testing whether using complete data vs. sampled data significantly impacts output quality.

3. **Cross-linguistic Validation**: Test whether the LSTM/GRU architecture performs comparably on a similar poetry generation task in another language (e.g., English sonnets) to isolate whether success is due to model capability versus Urdu-specific dataset properties.