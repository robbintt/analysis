---
ver: rpa2
title: 'S2F-NER: Exploring Sequence-to-Forest Generation for Complex Entity Recognition'
arxiv_id: '2310.18944'
source_url: https://arxiv.org/abs/2310.18944
tags:
- entity
- nested
- discontinuous
- entities
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes S2F-NER, a novel Sequence-to-Forest generation
  model for complex named entity recognition (NER) tasks, including nested, overlapping,
  and discontinuous entities. The model uses a Forest decoder to generate multiple
  entities in parallel, mitigating exposure bias issues and reducing computational
  complexity compared to sequence-to-sequence (Seq2Seq) approaches.
---

# S2F-NER: Exploring Sequence-to-Forest Generation for Complex Entity Recognition

## Quick Facts
- **arXiv ID**: 2310.18944
- **Source URL**: https://arxiv.org/abs/2310.18944
- **Reference count**: 22
- **Key outcome**: S2F-NER achieves F1 scores of 70.6%, 81.1%, and 80.9% on CADEC, ShARe13, and ShARe14 discontinuous NER datasets, outperforming or matching state-of-the-art baselines while demonstrating improved inference speed.

## Executive Summary
This paper introduces S2F-NER, a novel sequence-to-forest generation model for complex named entity recognition tasks involving nested, overlapping, and discontinuous entities. Unlike traditional sequence-to-sequence approaches that generate entities sequentially, S2F-NER uses a Forest decoder to generate multiple entities in parallel, significantly reducing computational complexity and exposure bias. The model limits maximum decoding depth to three fragments, which covers approximately 99.8% of entities in the tested datasets. Experimental results on three discontinuous NER datasets (CADEC, ShARe13, ShARe14) and two nested NER datasets (GENIA, ACE05) demonstrate superior or comparable performance to state-of-the-art baselines, particularly for discontinuous entity recognition.

## Method Summary
S2F-NER is an encoder-decoder architecture that uses a Transformer encoder for contextual representation and a Forest decoder for parallel entity generation. The Forest decoder consists of an LSTM backbone, scratchpad attention for memory management, and biaffine attention for joint span and type extraction. The model processes input text through character-level token embeddings and absolute position embeddings, followed by the Transformer encoder. The convolutional layer extracts local features and reduces dimensions. The Forest decoder generates entities autoregressively with a maximum depth of three fragments, using scratchpad attention to track generated content and biaffine attention to score word pairs for entity types. The model is trained with AdamW optimizer (learning rate 1e-5, batch size 20) for up to 80 epochs.

## Key Results
- Achieves F1 scores of 70.6%, 81.1%, and 80.9% on CADEC, ShARe13, and ShARe14 discontinuous NER datasets
- Outperforms or matches state-of-the-art baselines on nested NER datasets (GENIA, ACE05)
- Demonstrates improved inference speed compared to sequence-to-sequence approaches
- Successfully handles nested, overlapping, and discontinuous entity recognition in biomedical texts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Forest decoder with depth limit of three fragments reduces exposure bias compared to full Seq2Seq
- Mechanism: The model generates each path of each tree in forest autoregressively, but the maximum depth is three. This is much shorter than the decoding length of standard Seq2Seq models which must generate the full sequence of entities.
- Core assumption: Most entities (about 99.8% in the datasets) have no more than three fragments, making depth-3 sufficient.
- Evidence anchors:
  - [abstract]: "Specifically, our model generate each path of each tree in forest autoregressively, where the maximum depth of each tree is three (which is the shortest feasible length for complex NER and is far smaller than the decoding length of Seq2Seq)."
  - [section]: "Different from the Seq2Seq, the decoding length is limited to 3 because most entities (about 99.8% in the datasets used in this paper) have no more than three fragments."
  - [corpus]: Weak - the corpus does not provide direct evidence about exposure bias reduction, but the cited papers in the related work section (Dai et al., 2020b; Yan et al., 2021) do mention exposure bias as a problem for Seq2Seq models.
- Break condition: If a dataset contains many entities with more than three fragments, the depth limit would be insufficient and exposure bias would re-emerge.

### Mechanism 2
- Claim: Biaffine attention captures inter-dependencies between fragment spans and entity types
- Mechanism: The biaffine attention network scores word pairs (wi, wj) for each entity type k, allowing the model to jointly learn span boundaries and types. This provides more fine-grained semantics for entity recognition.
- Core assumption: The context of head and tail words of fragments differs, requiring separate MLPs to generate head/tail-sensitive representations before applying biaffine attention.
- Evidence anchors:
  - [section]: "Considering the context of the head and tail word of fragments is different. Firstly, we encode each hD t,i via two separate MLPs to generate head/tail-sensitive word representation hs t,i and he t,i. Then, we apply the biaffine attention..."
  - [section]: "our model consider word-level correlations and joint span and type extraction, which provides more fine-grained semantics for entity recognition."
  - [corpus]: Missing - the corpus does not provide evidence about biaffine attention effectiveness specifically.
- Break condition: If the assumption about head/tail context differences is false, the separate MLPs would add unnecessary complexity without benefit.

### Mechanism 3
- Claim: Scratchpad attention mechanism keeps track of what has been generated and guides future generation
- Mechanism: At every decoding step t, the model calculates attention from the last updated encoder states hD t-1 and the current decoder hidden state st, then updates the encoder states hD t using a convolution layer. This provides memory information for the decoder.
- Core assumption: Writing to encoder states as external memory at each decoding step helps the model focus on relevant words.
- Evidence anchors:
  - [section]: "At every decoding step t, an attention is derived from the last updated encoder states hD t-1 and the current decoder hidden state st, to obtain attentional context-aware embedding: at = Attention(hD t-1, st)"
  - [section]: "Then the context-aware embedding at is concatenated with the last updated encoder states hD t-1, and this fed into a convolution network layer to provide new encoder states hD t to the decoder at each decoding time step t"
  - [section]: "our model provides memory information bu scratchpad attention"
  - [corpus]: Missing - the corpus does not provide evidence about scratchpad attention effectiveness specifically.
- Break condition: If the attention mechanism fails to focus on relevant words, the scratchpad updates would not provide useful guidance.

## Foundational Learning

- **Concept**: Forest data structure for representing nested and discontinuous entities
  - Why needed here: The paper represents entities as trees where each node is a mention span except the root node which represents the entity type. Multiple entities form a forest, allowing parallel decoding rather than sequential.
  - Quick check question: How would you represent the nested entities "New Mexico" and "the US Federal District Court of New Mexico" using the forest structure described in the paper?

- **Concept**: Biaffine attention for joint span and type extraction
  - Why needed here: The model needs to simultaneously determine both the span boundaries and entity type for each fragment, which requires a mechanism that can capture interactions between these two aspects.
  - Quick check question: Why does the model use separate MLPs to generate head-sensitive and tail-sensitive word representations before applying biaffine attention?

- **Concept**: Exposure bias in sequence generation models
  - Why needed here: The paper explicitly addresses exposure bias as a problem with Seq2Seq approaches for NER, where the model is trained with teacher forcing but must generate at test time without this guidance.
  - Quick check question: How does limiting the decoding depth to three fragments help mitigate exposure bias compared to generating full entity sequences?

## Architecture Onboarding

- **Component map**: Input representation → Transformer encoder → Convolutional layer → Forest decoder (LSTM → scratchpad attention → biaffine attention) → Output
- **Critical path**: Input → Transformer encoder → Convolutional layer → Forest decoder (LSTM → scratchpad attention → biaffine attention) → Output
- **Design tradeoffs**:
  - Depth limit of 3 vs. full sequence generation: reduces exposure bias but may miss entities with >3 fragments
  - Forest structure vs. flat sequence: enables parallel decoding but requires more complex decoding logic
  - Biaffine attention vs. simpler attention: captures span-type interactions but increases model complexity
- **Failure signatures**:
  - Poor performance on discontinuous entities with >3 fragments
  - Overfitting to frequent entity orders in training data
  - High computational cost for long sequences despite depth limit
- **First 3 experiments**:
  1. Test the model on a dataset with entities containing exactly 3 fragments to verify the depth limit is sufficient
  2. Compare performance with and without scratchpad attention to measure its contribution
  3. Evaluate boundary detection accuracy for different types of overlapping entities (left, right, multiple)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of S2F-NER change when the maximum decoding depth is increased beyond three fragments for discontinuous entities?
- Basis in paper: [explicit] The paper states that the maximum depth of each tree is three, which is the shortest feasible length for complex NER and is far smaller than the decoding length of Seq2Seq. The paper mentions that about 99.8% of entities in the datasets used have no more than three fragments.
- Why unresolved: The paper does not provide experimental results or analysis for cases where the maximum decoding depth is increased beyond three fragments. It would be valuable to understand the trade-off between model performance and computational complexity as the depth increases.
- What evidence would resolve it: Experiments comparing the performance of S2F-NER with different maximum decoding depths (e.g., 3, 4, 5, etc.) on the same datasets would provide insights into the impact of increasing the depth on model performance and efficiency.

### Open Question 2
- Question: How does S2F-NER perform on other types of complex NER tasks not covered in the experiments, such as entities with more than three fragments or entities with different overlapping patterns?
- Basis in paper: [inferred] The paper focuses on evaluating S2F-NER on discontinuous NER datasets (CADEC, ShARe13, ShARe14) and nested NER datasets (GENIA, ACE05). The paper mentions that the maximum depth is set to three for discontinuous NER and one for nested NER. However, it does not explore other types of complex NER tasks or entities with more than three fragments.
- Why unresolved: The paper does not provide experimental results or analysis for other types of complex NER tasks or entities with more than three fragments. It would be valuable to understand the generalizability and limitations of S2F-NER on a broader range of complex NER scenarios.
- What evidence would resolve it: Experiments evaluating S2F-NER on additional datasets with different types of complex NER tasks (e.g., entities with more than three fragments, different overlapping patterns, etc.) would provide insights into the model's performance and limitations in handling various complex NER scenarios.

### Open Question 3
- Question: How does the performance of S2F-NER compare to other state-of-the-art models when using different pre-trained language models (e.g., other variants of BERT or other transformer-based models)?
- Basis in paper: [explicit] The paper mentions that the pre-trained BERT model used for CADEC is Yelp BERT, and for ShARe13 and ShARe14, it is Clinical BERT. The paper compares the performance of S2F-NER with other baselines using these specific pre-trained models.
- Why unresolved: The paper does not explore the impact of using different pre-trained language models on the performance of S2F-NER. It would be valuable to understand how the choice of pre-trained model affects the model's ability to handle complex NER tasks.
- What evidence would resolve it: Experiments comparing the performance of S2F-NER using different pre-trained language models (e.g., other variants of BERT, RoBERTa, XLNet, etc.) on the same datasets would provide insights into the impact of the choice of pre-trained model on the model's performance.

## Limitations

- The model may fail to capture entities with more than three fragments due to the depth limit assumption
- Limited ablation studies make it difficult to isolate the contribution of individual components
- Performance on longer sequences and non-biomedical domains remains untested

## Confidence

- **High Confidence**: Performance claims on the specific datasets tested (CADEC, ShARe13, ShARe14, GENIA, ACE05)
- **Medium Confidence**: General effectiveness for complex NER tasks beyond the tested datasets
- **Low Confidence**: Claims about exposure bias reduction and the necessity of depth-3 limit without broader validation

## Next Checks

1. **Fragment Distribution Analysis**: Analyze entity fragment distributions across multiple domains to verify the 99.8% claim and identify datasets where the depth-3 limit would fail.

2. **Ablation Study**: Systematically remove the scratchpad attention and biaffine attention components to quantify their individual contributions to the performance gains.

3. **Long Sequence Evaluation**: Test the model on longer sequences (>512 tokens) to assess whether the depth limit and parallel decoding approach maintain efficiency advantages while preserving accuracy.