---
ver: rpa2
title: Aggregating Correlated Estimations with (Almost) no Training
arxiv_id: '2309.02005'
source_url: https://arxiv.org/abs/2309.02005
tags:
- agents
- noise
- candidates
- utility
- candidate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of aggregating noisy, correlated
  estimates in choice problems, where multiple agents provide scores for candidates
  and the goal is to select the best one. The key challenge is that correlations between
  agents' errors can hinder aggregation performance.
---

# Aggregating Correlated Estimations with (Almost) no Training

## Quick Facts
- arXiv ID: 2309.02005
- Source URL: https://arxiv.org/abs/2309.02005
- Reference count: 31
- Key outcome: Embedded Voting (EV) uses SVD to aggregate noisy, correlated estimates and achieves up to 95% average relative utility, outperforming untrained methods like Range Voting when training data is limited.

## Executive Summary
This paper addresses the problem of aggregating noisy, correlated estimates from multiple agents in choice problems, where the goal is to select the best candidate. The key challenge is that correlations between agents' errors can hinder aggregation performance. The authors propose a new method called Embedded Voting (EV), which uses spectral analysis (SVD) to embed agents based on their score patterns and mitigate the impact of correlations. EV performs close to the optimal maximum-likelihood aggregation when sufficient training data is available, and outperforms other untrained methods (e.g., Range Voting, Approval Voting, Nash Product) when training data is limited.

## Method Summary
The paper proposes Embedded Voting (EV) to aggregate noisy, correlated estimates from multiple agents. EV normalizes agent score vectors to mean 0 and unit standard deviation, then uses SVD to identify orthogonal components that reveal latent agent groups. By weighting candidate welfare by the top singular values, EV assigns higher weight to independent agents and downweights redundant ones. This spectral decomposition approach allows EV to approximate maximum likelihood aggregation without requiring explicit noise model knowledge, achieving near-optimal performance even with limited training data.

## Key Results
- EV achieves up to 95% average relative utility, close to the theoretical upper bound
- Outperforms untrained methods like Range Voting, which drops to around 88% in highly correlated settings
- Maintains performance advantage even with limited training data by exploiting structure in the current choice problem

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spectral decomposition of normalized score matrices reveals latent agent groups and mitigates redundant correlations.
- Mechanism: The method normalizes agent score vectors to mean 0 and unit standard deviation, then uses SVD to identify orthogonal components. By weighting candidate welfare by the top singular values, it assigns higher weight to independent agents and downweights redundant ones.
- Core assumption: Correlations between agents correspond to non-orthogonal dimensions in the score embedding space, which SVD can separate.
- Evidence anchors:
  - [section] "We call the resulting method Embedded Voting (EV). ... Taking the k greatest singular values ... will give the same result as relying on Mj."
  - [abstract] "The authors propose a new method called Embedded Voting (EV), which uses spectral analysis (SVD) to embed agents based on their score patterns and mitigate the impact of correlations."
- Break condition: If agent correlations are so complex that they cannot be captured by linear orthogonal decomposition, or if the number of candidates is too small for reliable SVD.

### Mechanism 2
- Claim: EV achieves near-optimal performance by approximating maximum likelihood aggregation without requiring explicit noise model knowledge.
- Mechanism: EV implicitly learns the inverse covariance structure from data through SVD, effectively approximating the weights that would be used in maximum likelihood estimation.
- Core assumption: The structure of correlations in the data can be captured by the top singular vectors of the normalized score matrix, which approximate the optimal weighting scheme.
- Evidence anchors:
  - [section] "Using spectral analysis may seem excessive in the ideal case where the partition is perfect and known. However, its main advantage is that it is based on some embedding of the agents that can convey more information than a simple partition."
  - [abstract] "EV performs close to the optimal maximum-likelihood aggregation when sufficient training data is available"
- Break condition: When noise levels are very high relative to signal, or when the correlation structure is too irregular for SVD to capture.

### Mechanism 3
- Claim: EV's performance advantage over untrained methods is maintained even with limited training data by exploiting structure in the current choice problem.
- Mechanism: EV uses candidates from the current choice problem itself as implicit training data to learn the embedding, making it effective even without external training data.
- Core assumption: The score patterns across candidates in a single choice problem contain sufficient information to identify agent correlations.
- Evidence anchors:
  - [section] "The performance of EV is minimal around m = n, for the same reason as in Figure 2." (This actually suggests a limitation, not an advantage)
  - [abstract] "outperforms other untrained methods ... when training data is limited"
- Break condition: When the number of candidates is very small (m ≈ n), making the score matrix too small for reliable SVD.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD)
  - Why needed here: SVD is the core mathematical tool that enables EV to separate independent from correlated agents in the score space.
  - Quick check question: What does each singular value represent in the context of the normalized score matrix for candidate welfare aggregation?

- Concept: Maximum Likelihood Estimation (MLE)
  - Why needed here: Understanding MLE provides the theoretical benchmark that EV approximates, and explains why certain weighting schemes are optimal.
  - Quick check question: How does the optimal weight for an agent in MLE depend on the covariance structure of agent errors?

- Concept: Correlation structure in estimation errors
  - Why needed here: The paper's entire premise is that correlations between agents' errors can degrade aggregation performance, and different correlation patterns require different handling.
  - Quick check question: In the noise model, what parameter controls the strength of correlations between agents that share features?

## Architecture Onboarding

- Component map: Data normalization (mean 0, std 1) -> SVD computation on normalized score matrix -> Welfare aggregation using top singular values
- Critical path: For each candidate, normalize scores → compute SVD of score matrix → select top k singular values → compute welfare as product of weighted scores
- Design tradeoffs: EV trades computational complexity (SVD on n×m matrix) for robustness to correlations; simpler methods like RV are faster but vulnerable to correlation-induced bias
- Failure signatures: Poor performance when m ≈ n (singular value matrix poorly conditioned), or when correlations are too complex for linear SVD to capture
- First 3 experiments:
  1. Implement EV on synthetic data with known correlation structure (n=24, k=5 groups) and compare to RV and NP
  2. Test EV's sensitivity to number of candidates by varying m from 5 to 50 with fixed agent correlation structure
  3. Compare trained (EV+) vs untrained EV performance when using external vs current-choice training data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Embedded Voting (EV) method perform in real-world datasets with diverse types of correlations beyond the synthetic models tested?
- Basis in paper: [inferred] The paper primarily uses synthetic data to validate EV, leaving real-world performance unexplored.
- Why unresolved: Real-world datasets may have more complex and less structured correlations, which could affect EV's effectiveness.
- What evidence would resolve it: Empirical results from applying EV to real-world datasets with known correlation structures.

### Open Question 2
- Question: What are the computational costs and scalability limitations of EV when applied to large-scale choice problems with many candidates and agents?
- Basis in paper: [inferred] The paper does not discuss computational complexity or scalability of EV in detail.
- Why unresolved: As the number of candidates and agents increases, the SVD computation and embedding process may become computationally intensive.
- What evidence would resolve it: Time complexity analysis and scalability tests on large datasets.

### Open Question 3
- Question: How sensitive is EV to the choice of threshold (0.95 in the paper) for determining the number of relevant dimensions in the embedding?
- Basis in paper: [explicit] The paper mentions using 0.95 times the average singular value as a threshold but does not explore its sensitivity.
- Why unresolved: The threshold choice may significantly impact EV's performance, especially in datasets with varying noise levels.
- What evidence would resolve it: Sensitivity analysis by testing EV with different threshold values across various datasets.

## Limitations
- Limited external validation: Method tested only on synthetic data with controlled noise models, not on real-world datasets
- Sensitivity to candidate number: Performance degrades when number of candidates approaches number of agents (m ≈ n)
- Unproven robustness: Performance depends on quality of SVD decomposition, which may break down with non-linear correlations

## Confidence
- High confidence: Mathematical framework and relationship to SVD and maximum likelihood estimation
- Medium confidence: Claims about outperforming untrained methods on synthetic data
- Low confidence: Claims about real-world applicability without validation on actual choice problems

## Next Checks
1. Apply EV to a real-world choice problem with known agent correlations (e.g., peer review scores for conference submissions) and compare performance against RV and NP baselines.
2. Generate synthetic data with non-linear correlation patterns that violate the orthogonality assumption of SVD, and measure EV's degradation in performance.
3. Systematically vary the number of candidates (m) and agents (n) across a wider range to precisely map EV's performance boundaries and identify the m/n threshold where performance becomes unreliable.