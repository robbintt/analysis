---
ver: rpa2
title: Learning to sample in Cartesian MRI
arxiv_id: '2312.04327'
source_url: https://arxiv.org/abs/2312.04327
tags:
- sampling
- reconstruction
- lbcs
- mask
- psnr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This thesis explores methods for optimizing sampling patterns
  in Cartesian MRI. Two main contributions are made: 1) Scalable Learning-based Sampling
  Optimization, which extends the Learning-Based Compressive Sensing (LBCS) approach
  to handle large-scale problems like multi-coil 3D MR and dynamic MRI.'
---

# Learning to sample in Cartesian MRI

## Quick Facts
- arXiv ID: 2312.04327
- Source URL: https://arxiv.org/abs/2312.04327
- Authors: 
- Reference count: 0
- Primary result: This thesis explores methods for optimizing sampling patterns in Cartesian MRI.

## Executive Summary
This thesis addresses the challenge of optimizing sampling patterns in Cartesian MRI through two main contributions. First, it extends Learning-Based Compressive Sensing (LBCS) with scalable algorithms (stochastic and lazy variants) that enable handling large-scale problems like multi-coil 3D MR and dynamic MRI. Second, it investigates deep reinforcement learning methods for adaptive mask design, finding that they offer only marginal improvements over simpler non-adaptive methods like stochastic LBCS. The work concludes that computationally efficient methods like stochastic LBCS represent promising alternatives to complex deep RL approaches for Cartesian MRI sampling optimization.

## Method Summary
The thesis develops scalable sampling optimization methods for Cartesian MRI. It extends LBCS with stochastic and lazy variants that reduce computational complexity while maintaining reconstruction quality. Stochastic LBCS samples subsets of candidate lines and training data at each iteration, reducing cost from O(P²) to O(lkN). Lazy LBCS uses precomputed upper bounds and updates them only when needed. The thesis also investigates deep RL methods for adaptive sampling, comparing them with non-adaptive approaches. Additionally, it explores GAN-based methods for deriving adaptive sampling policies through posterior variance in the measurement domain. Experiments span single-coil and multi-coil datasets including cardiac, vocal tract, brain, knee MRI, and MNIST.

## Key Results
- Stochastic and lazy LBCS variants scale to large-scale MRI problems without sacrificing reconstruction quality
- Deep RL methods provide at best marginal improvements over simpler non-adaptive methods like sLBCS
- sLBCS demonstrates competitive performance with significantly lower computational complexity than deep RL approaches
- GAN-based adaptive sampling through posterior variance offers a natural criterion for information acquisition

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Lazy evaluations and stochastic sampling enable LBCS to scale to large-scale MRI problems without significant performance loss.
- **Mechanism**: Lazy LBCS precomputes upper bounds on marginal benefits and updates them only when needed, drastically reducing the number of full reconstruction evaluations. Stochastic LBCS samples a small subset of candidate lines and training data points at each iteration, maintaining the greedy improvement while reducing computational cost from O(P²) to O(lkN) where l is the training batch size and k is the candidate batch size.
- **Core assumption**: The performance metric being optimized exhibits diminishing returns (submodularity) or approximate submodularity, allowing greedy algorithms to provide good approximations with fewer evaluations.
- **Evidence anchors**:
  - [abstract]: "These algorithms scale to large, clinically relevant scenarios like multi-coil 3D MR and dynamic MRI that were inaccessible to LBCS."
  - [section]: "Our results show that this is achieved without sacrificing any reconstruction quality."
  - [corpus]: Weak evidence - the corpus neighbors focus on non-Cartesian MRI and deep learning reconstruction, not specifically on scaling LBCS through lazy/stochastic approaches.
- **Break condition**: If the performance metric becomes non-submodular or exhibits strong non-monotonic behavior, the greedy approximation guarantees break down and lazy/stochastic sampling may miss optimal configurations.

### Mechanism 2
- **Claim**: Deep RL methods for MRI sampling provide at best marginal improvements over simpler greedy approaches like sLBCS.
- **Mechanism**: Deep RL policies attempt to adapt to individual patients and plan multiple steps ahead, but in practice, the improvement over non-adaptive greedy policies that maximize immediate performance gain is minimal. The marginal benefit comes at significant computational cost and complexity.
- **Core assumption**: The MRI sampling problem has structure that makes greedy algorithms near-optimal, and patient adaptation or long-term planning provides limited additional value compared to the cost of learning these capabilities.
- **Evidence anchors**:
  - [abstract]: "state-of-the-art approaches based on deep reinforcement learning (RL), while capable of adaptation and long-horizon planning, offer only marginal improvements over stochastic LBCS."
  - [section]: "Our results, surprisingly show that sLBCS, a non-adaptive method that does not attempt long term planning, can perform as well as the state-of-the-art approaches of Pineda et al. (2020); Bakker et al. (2020)."
  - [corpus]: Weak evidence - corpus neighbors focus on non-Cartesian trajectories and implicit representations rather than direct comparison of deep RL vs greedy methods for Cartesian sampling.
- **Break condition**: If the MRI acquisition scenario involves highly variable patient anatomy or non-stationary signal distributions where adaptation becomes critical, or if long-term planning can capture complex dependencies that greedy methods miss.

### Mechanism 3
- **Claim**: Conditional GANs can naturally provide adaptive sampling policies by leveraging posterior variance in the measurement domain without explicit policy training.
- **Mechanism**: By sampling from the conditional GAN posterior and computing empirical variance in the observation domain, the location with highest variance indicates maximum uncertainty and should be acquired next. This provides a 0-step information policy that directly minimizes reconstruction uncertainty.
- **Core assumption**: The posterior distribution learned by the conditional GAN accurately captures the uncertainty in the measurement domain, and this uncertainty correlates with reconstruction error that would be reduced by acquiring the corresponding measurement.
- **Evidence anchors**:
  - [abstract]: "we show that generative adversarial networks (GANs), used to model the posterior distribution in inverse problems, provide a natural criterion for adaptive sampling by leveraging their variance in the measurement domain to guide acquisition."
  - [section]: "Our GAN-based approach and the one of Zhang et al. (2019b) rely on 0-step information, i.e. base their decision on the current error, rather than receiving feedback about how their decision actually performed."
  - [corpus]: Weak evidence - corpus neighbors discuss GANs for reconstruction but not specifically for adaptive sampling through posterior variance.
- **Break condition**: If the posterior learned by the GAN does not accurately represent the true uncertainty, or if the measurement-domain variance does not correlate with reconstruction improvement, the adaptive policy will fail to acquire informative measurements.

## Foundational Learning

- **Concept**: Submodular optimization and greedy algorithms
  - Why needed here: LBCS and its variants are based on submodular function maximization principles, where greedy algorithms provide approximation guarantees for problems with diminishing returns.
  - Quick check question: Can you explain why greedy algorithms work well for mask design when the objective exhibits diminishing returns?

- **Concept**: Reinforcement learning basics (MDPs, Q-learning, policy gradients)
  - Why needed here: Understanding RL methods used for adaptive sampling requires knowledge of how agents learn policies through interaction with environments, value functions, and policy optimization.
  - Quick check question: What is the key difference between Q-learning and policy gradient methods in terms of what they learn and how they update parameters?

- **Concept**: Generative adversarial networks and conditional distributions
  - Why needed here: GANs are used both for reconstruction and for deriving adaptive sampling policies through posterior variance, requiring understanding of how conditional GANs model p(x|y) distributions.
  - Quick check question: How does a conditional GAN differ from a standard GAN in terms of input and output structure?

## Architecture Onboarding

- **Component map**: 
  - Data pipeline: Raw k-space → Preprocessing (crop/resize/normalize) → Undersampling → Reconstruction → Evaluation
  - Mask optimization: Training data → Reconstruction model training → Performance metric → Greedy/stochastic/lazy search → Sampling mask
  - Adaptive sampling: Current reconstruction → Posterior sampling (GAN or evaluator) → Variance/error estimation → Next acquisition location

- **Critical path**: For sLBCS: Training data → Reconstruction model training → Candidate line evaluation (with stochastic sampling) → Mask construction → Reconstruction with optimized mask
  For GAN-based sampling: GAN training → Posterior sampling → Variance computation → Adaptive acquisition

- **Design tradeoffs**: 
  - Greedy vs stochastic: Greedy guarantees improvement but is expensive; stochastic is faster but may miss optimal configurations
  - Fixed vs adaptive masks: Fixed masks are simpler and faster but may not capture patient-specific variations; adaptive masks are more complex but potentially more informative
  - Submodular approximation: Assumes diminishing returns; if violated, performance may degrade

- **Failure signatures**:
  - Poor reconstruction quality despite optimized masks: May indicate submodularity assumption violation or inappropriate performance metric
  - Slow convergence or high computational cost: May indicate need for stochastic/lazy variants or smaller candidate batches
  - Adaptive sampling not improving over fixed: May indicate patient adaptation not valuable for the specific application or incorrect uncertainty estimation

- **First 3 experiments**:
  1. Implement sLBCS on a small 2D MRI dataset with different batch sizes (k,l) to observe scaling behavior and identify optimal parameters
  2. Compare greedy LBCS vs stochastic LBCS vs lazy LBCS on the same dataset to quantify performance vs computational tradeoffs
  3. Implement conditional GAN for posterior modeling and evaluate adaptive sampling through posterior variance on a simple dataset before moving to complex MRI data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can deep reinforcement learning (RL) methods achieve significant improvements over non-adaptive sampling methods like stochastic LBCS for Cartesian MRI?
- Basis in paper: Explicit. The paper directly compares deep RL methods (Pineda et al., 2020; Bakker et al., 2020) with stochastic LBCS (sLBCS) and finds that RL methods only bring marginal improvements over sLBCS, if any.
- Why unresolved: While the paper provides evidence suggesting RL methods don't offer significant improvements, it doesn't definitively prove this for all possible RL approaches or all MRI settings. The effectiveness of RL might depend on specific problem characteristics or training methodologies.
- What evidence would resolve it: Further research comparing a wider range of RL methods with sLBCS across diverse MRI settings, including different anatomical regions, acquisition protocols, and noise levels, could provide more conclusive evidence. Additionally, exploring different RL architectures, training strategies, and reward functions might reveal scenarios where RL outperforms non-adaptive methods.

### Open Question 2
- Question: How does the information horizon (0-step vs 1-step) used by sampling policies impact their performance in Cartesian MRI?
- Basis in paper: Explicit. The paper discusses the concept of information horizon and compares 0-step methods (like GAN-based sampling and the evaluator of Zhang et al., 2019b) with 1-step methods (like LBCS and RL). It finds that 1-step methods generally outperform 0-step methods.
- Why unresolved: The paper doesn't provide a theoretical explanation for why 1-step methods perform better. It's unclear whether this is a fundamental limitation of 0-step methods or if it can be overcome with improved algorithm design or training strategies.
- What evidence would resolve it: Theoretical analysis of the trade-offs between 0-step and 1-step methods in the context of MRI sampling could provide insights into their relative strengths and weaknesses. Empirical studies comparing different information horizons across various MRI settings and sampling strategies could also shed light on this question.

### Open Question 3
- Question: What are the key factors that influence the effectiveness of adaptive sampling in Cartesian MRI?
- Basis in paper: Explicit. The paper explores adaptive sampling methods (like RL and GAN-based sampling) and compares them with non-adaptive methods. It finds that adaptivity doesn't always lead to significant improvements, especially in Fourier domain sampling.
- Why unresolved: The paper doesn't provide a comprehensive understanding of the factors that determine when adaptive sampling is beneficial. It's unclear whether adaptivity's effectiveness depends on the specific MRI setting, the reconstruction algorithm, or other factors.
- What evidence would resolve it: Systematic studies investigating the impact of different factors (e.g., sampling domain, image characteristics, reconstruction algorithms) on the performance of adaptive sampling methods could provide insights into when and why adaptivity is beneficial. Additionally, exploring different adaptive sampling strategies and their underlying assumptions could help identify the key factors that influence their effectiveness.

## Limitations

- The performance comparison between deep RL and sLBCS may not generalize across all MRI acquisition scenarios or reconstruction architectures
- The scalability claims for stochastic and lazy LBCS rely on the submodularity assumption, which may not hold for all reconstruction metrics or MRI applications
- The effectiveness of adaptive sampling through posterior variance requires the GAN to accurately model the posterior uncertainty, which may be challenging in practice

## Confidence

- **High confidence**: sLBCS scalability and computational efficiency claims, based on clear algorithmic improvements and demonstrated runtime reductions
- **Medium confidence**: Deep RL vs sLBCS performance comparison, as results show marginal differences but may not generalize to all MRI contexts
- **Medium confidence**: GAN-based adaptive sampling mechanism, as the theoretical foundation is sound but practical validation is limited

## Next Checks

1. Test sLBCS and deep RL methods across multiple reconstruction architectures (not just cResNet) to verify performance ordering is consistent and not architecture-dependent
2. Evaluate submodularity properties of different performance metrics (PSNR, SSIM) on real MRI data to validate the theoretical foundation of greedy algorithms
3. Implement the conditional GAN adaptive sampling approach on a larger, clinically relevant dataset to assess whether posterior variance truly correlates with reconstruction improvement in practice