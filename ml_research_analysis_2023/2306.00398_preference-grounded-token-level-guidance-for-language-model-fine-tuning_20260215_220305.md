---
ver: rpa2
title: Preference-grounded Token-level Guidance for Language Model Fine-tuning
arxiv_id: '2306.00398'
source_url: https://arxiv.org/abs/2306.00398
tags:
- training
- learning
- arxiv
- guidance
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We address the problem that sequence-level preference in language
  model training is misaligned with token-level training losses, leading to delayed
  feedback and inefficient learning. Our method learns preference-grounded token-level
  guidance by iteratively alternating between training a token-level reward function
  that reflects sequence-level preference among multiple generations, and using this
  learned guidance to train the language model via either REINFORCE-style updates
  (without supervised data) or reward-weighted MLE (with supervised data).
---

# Preference-grounded Token-level Guidance for Language Model Fine-tuning

## Quick Facts
- arXiv ID: 2306.00398
- Source URL: https://arxiv.org/abs/2306.00398
- Authors: 
- Reference count: 38
- We address the problem that sequence-level preference in language model training is misaligned with token-level training losses, leading to delayed feedback and inefficient learning.

## Executive Summary
This paper addresses the fundamental misalignment between sequence-level preferences and token-level training losses in language model fine-tuning. The authors propose a novel approach that learns token-level reward functions reflecting sequence-level preferences, which can then guide language model training. By iteratively alternating between training a token-level reward function and using it to train the language model, the method achieves competitive performance on both discrete-prompt generation and text summarization tasks.

## Method Summary
The method alternates between two phases: first, learning a token-level reward function that reflects sequence-level preferences among multiple generated sequences using a pairwise-preference learning framework; second, using this learned guidance to train the language model via either REINFORCE-style updates (without supervised data) or reward-weighted maximum likelihood estimation (with supervised data). The approach addresses the granularity mismatch by providing dense, task-specific guidance at the token level, and periodically re-estimates the reward function to mitigate distribution shift during training.

## Key Results
- Achieves up to 94.8% test accuracy on discrete-prompt generation tasks
- Improves ROUGE-L scores by up to 2.4 points on text summarization
- Demonstrates effective performance on two distinct tasks while addressing the granularity mismatch between preference and training losses

## Why This Works (Mechanism)

### Mechanism 1
Learning a token-level reward function reduces the granularity mismatch between sequence-level preferences and token-level training losses. The token-level reward function is trained to reflect the sequence-level preference ordering among multiple generated sequences, and is then used as dense guidance for token selection during LM training. Core assumption: The aggregation function (e.g., average, soft maximum) appropriately captures the sequence-level preference without introducing length bias.

### Mechanism 2
Using the learned token-level guidance in LM training improves performance compared to using only sequence-level feedback. The learned token-level reward function provides dense, task-specific guidance for each token selection, avoiding the delayed feedback problem of sequence-level rewards. Core assumption: The learned token-level reward function accurately reflects the sequence-level preference.

### Mechanism 3
Periodically re-estimating the token-level reward function mitigates the distribution shift between the text sequences used to train the reward function and the text sequences evaluated by the reward function during LM training. The reward function is periodically re-trained on text sequences generated by the latest version of the LM, ensuring it remains accurate guidance as the LM evolves. Core assumption: The distribution shift between the text sequences used to train the reward function and the text sequences evaluated by the reward function during LM training is significant enough to warrant periodic re-estimation.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF is a key technique used in the paper to align language models with human preferences, which is closely related to the paper's approach of using sequence-level preference to guide token-level training.
  - Quick check question: What is the main challenge in RLHF that the paper's approach addresses?

- Concept: Preference Learning
  - Why needed here: Preference learning is the core technique used in the paper to learn the token-level reward function that reflects the sequence-level preference.
  - Quick check question: What is the key difference between the paper's approach and classical pairwise preference learning?

- Concept: Aggregation Functions
  - Why needed here: Aggregation functions are used in the paper to aggregate the learned token-level rewards into sequence-level evaluations, which is crucial for the reward function to reflect the sequence-level preference.
  - Quick check question: What are the three alternative aggregation functions discussed in the paper, and why are they preferred over classical summation?

## Architecture Onboarding

- Component map:
  - Token-level reward function (r_φ) -> LM (π_θ) -> Aggregation function (f(·))
  - (Token-level reward function learns to reflect sequence-level preference, LM is trained using the learned token-level guidance, aggregation function aggregates token-level rewards into sequence-level evaluations)

- Critical path:
  1. Generate multiple text sequences using the LM.
  2. Learn the token-level reward function to reflect the sequence-level preference among the generated sequences.
  3. Use the learned token-level guidance to train the LM.
  4. Periodically re-estimate the token-level reward function to mitigate distribution shift.

- Design tradeoffs:
  - Aggregation function: Average vs. soft maximum vs. soft minimum.
  - Number of sequences used to learn the reward function: More sequences provide more information but increase computational cost.
  - Frequency of reward function re-estimation: More frequent re-estimation mitigates distribution shift but increases computational cost.

- Failure signatures:
  - Poor performance on the task: Indicates that the learned token-level guidance is not effective.
  - High variance in performance: Indicates that the learned token-level guidance is unstable.
  - Distribution shift between text sequences used to train the reward function and text sequences evaluated by the reward function during LM training: Indicates that the reward function is not accurately guiding the LM.

- First 3 experiments:
  1. Vary the aggregation function (average, soft maximum, soft minimum) and compare performance on a simple task.
  2. Vary the number of sequences used to learn the reward function and compare performance on a simple task.
  3. Compare the performance of the proposed approach with and without reward function re-estimation on a simple task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of aggregation function (average, soft maximum, soft minimum) affect the performance of the learned token-level guidance across different LM tasks beyond text summarization and prompt generation?
- Basis in paper: [explicit] The paper discusses three aggregation functions (average, soft maximum, soft minimum) and shows their performance varies across tasks, but only tests on two tasks.
- Why unresolved: The paper only evaluates these aggregation functions on text summarization and prompt generation tasks. It's unclear if the observed patterns generalize to other LM tasks like dialogue systems or machine translation.
- What evidence would resolve it: Systematic experiments testing all three aggregation functions across a diverse set of LM tasks (e.g., dialogue, translation, question answering) would clarify which aggregation function is most effective for different types of tasks.

### Open Question 2
- Question: What is the optimal frequency for re-estimating the token-level reward function during LM training, and how does this depend on the initial LM's zero-shot performance and the specific task?
- Basis in paper: [explicit] The paper mentions periodically re-estimating the reward function during the first half of LM training but doesn't provide a systematic study of optimal frequencies.
- Why unresolved: The paper only uses a fixed re-estimation schedule (every 1000 steps for the first 6000 steps in prompt generation, every 0.5 epochs for the first 2 epochs in summarization). It's unclear if this is optimal or if the frequency should vary based on task characteristics or initial LM performance.
- What evidence would resolve it: Experiments varying the re-estimation frequency across different tasks and initial LM qualities would reveal optimal schedules and their dependence on these factors.

### Open Question 3
- Question: How does the proposed preference-grounding framework compare to other methods for addressing the granularity mismatch problem, such as hierarchical reinforcement learning or multi-step return estimation techniques?
- Basis in paper: [inferred] The paper addresses the granularity mismatch between sequence-level preferences and token-level training, but doesn't compare to other approaches that tackle this same problem.
- Why unresolved: The paper focuses on its own solution without benchmarking against alternative methods for addressing delayed feedback or granularity mismatch in LM training.
- What evidence would resolve it: Direct comparisons between the preference-grounding framework and other approaches like hierarchical RL, multi-step returns, or auxiliary reward signals would establish its relative effectiveness.

## Limitations

- The paper only evaluates the method on two specific tasks (discrete-prompt generation and summarization), limiting generalizability to other LM applications.
- The choice of aggregation functions appears empirical rather than theoretically grounded, with unclear reasons why different tasks benefit from different aggregations.
- The method requires multiple sequence generations for each input, increasing computational cost compared to standard supervised fine-tuning.

## Confidence

**High confidence**: The core algorithmic framework (alternating between reward learning and LM training) is clearly specified and the implementation details for both REINFORCE-style and reward-weighted MLE approaches are provided.

**Medium confidence**: The empirical results showing performance improvements are convincing for the tested tasks, but the methodology for generating sequence-level preferences lacks full reproducibility details.

**Low confidence**: The theoretical justification for why certain aggregation functions work better for specific tasks is weak, and the paper doesn't provide deep analysis of when and why the method would fail.

## Next Checks

1. Apply the method to a third, distinctly different task (e.g., machine translation or dialogue generation) to test generalization beyond summarization and prompt generation. Compare performance against both standard RLHF and direct preference optimization baselines.

2. Systematically test all three aggregation functions (average, soft maximum, soft minimum) across both tasks with multiple seeds to determine if the task-specific patterns hold consistently or if they're due to random variation.

3. Measure the KL divergence between distributions of sequences used to train the reward function versus those evaluated during LM training. Compare performance when retraining at different frequencies to identify the optimal trade-off point between computational cost and effectiveness.