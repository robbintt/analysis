---
ver: rpa2
title: On the Trade-offs between Adversarial Robustness and Actionable Explanations
arxiv_id: '2309.16452'
source_url: https://arxiv.org/abs/2309.16452
tags:
- robust
- adversarially
- non-robust
- wntk
- recourses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the theoretical and empirical relationship
  between adversarial robustness and actionable explanations in machine learning models.
  The authors derive theoretical bounds on the cost (ease of implementation) and validity
  (probability of obtaining a positive model prediction) differences between recourses
  generated by state-of-the-art methods for adversarially robust vs.
---

# On the Trade-offs between Adversarial Robustness and Actionable Explanations

## Quick Facts
- arXiv ID: 2309.16452
- Source URL: https://arxiv.org/abs/2309.16452
- Authors: 
- Reference count: 40
- Key outcome: Adversarial robustness significantly increases recourse cost and reduces validity in ML models

## Executive Summary
This paper investigates the fundamental trade-off between adversarial robustness and actionable explanations in machine learning. Through theoretical analysis and empirical validation on three real-world datasets (German Credit, Adult, COMPAS), the authors demonstrate that increasing model robustness through adversarial training leads to higher costs and lower validity for algorithmic recourses. The work establishes theoretical bounds showing that as model robustness increases, the cost of generating recourses increases and their validity decreases, revealing inherent limitations in achieving both robustness and interpretability.

## Method Summary
The authors analyze trade-offs between adversarial robustness and actionable explanations through a combination of theoretical analysis and empirical validation. They train both non-robust and adversarially robust models (logistic regression and neural networks) with varying perturbation radii (ϵ values from 0 to 0.3) on three datasets. For each model type, they generate recourses using multiple state-of-the-art methods (SCFE, C-CHVAE, GSM, ROAR) and evaluate the ℓ2-norm cost and validity of these recourses. Theoretical bounds are derived for both linear and non-linear models using properties like Lipschitz continuity and the Neural Tangent Kernel approximation to characterize how robustness affects recourse quality.

## Key Results
- Adversarially robust models significantly increase the cost and reduce the validity of generated recourses
- Theoretical bounds show cost differences scale with the harmonic mean of model weights
- Empirical results on German Credit, Adult, and COMPAS datasets validate theoretical findings
- The number of valid recourses decreases as robustness increases in local neighborhoods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Increasing adversarial robustness reduces the number of valid recourses available for any given instance.
- **Mechanism:** As the model becomes more robust to adversarial perturbations, the decision boundary becomes more conservative, requiring larger changes to flip predictions. This narrows the set of nearby points that satisfy the desired outcome, reducing validity.
- **Core assumption:** The robustness mechanism modifies the model parameters in a way that makes small perturbations less effective at changing predictions.
- **Evidence anchors:** [abstract] "adversarially robust models significantly increase the cost and reduce the validity of the resulting recourses"; [section] "we observe a gradual decline in the number of valid recourses around a local neighborhood with an increasing degree of robustness ϵ"

### Mechanism 2
- **Claim:** The cost of generating recourses increases with the degree of model robustness due to larger parameter differences between robust and non-robust models.
- **Mechanism:** Adversarial training creates parameter perturbations that increase the distance between the original and counterfactual points needed to achieve the target prediction. The theoretical bounds show this cost difference grows with the perturbation radius ϵ.
- **Core assumption:** The weight difference between robust and non-robust models scales with the adversarial training perturbation radius.
- **Evidence anchors:** [abstract] "adversarially robust models significantly increase the cost and reduce the validity of the resulting recourses"; [section] "Our theoretical bounds imply that the difference in costs is bounded by the harmonic mean of the NTK models weights of non-robust and robust models"

### Mechanism 3
- **Claim:** The validity of recourses decreases because the probability of achieving the desired outcome drops as the model becomes more robust.
- **Mechanism:** The theoretical bounds show that validity is bounded by terms involving the parameter differences and input norms. As robustness increases, these terms grow, making the validity condition harder to satisfy.
- **Core assumption:** The probability of a valid recourse is directly related to the model's confidence in the target class, which decreases with robustness.
- **Evidence anchors:** [abstract] "adversarially robust models significantly increase the cost and reduce the validity of the resulting recourses"; [section] "validity of the non-robust model (denoted by Pr(fNR(x) = 1) in Theorem 4) was higher than the validity of the adversarially robust model for all the test samples"

## Foundational Learning

- **Concept: Adversarial robustness in machine learning**
  - Why needed here: Understanding how adversarial training affects model parameters and decision boundaries is crucial for analyzing the trade-offs with recourse.
  - Quick check question: What is the primary difference between empirical robustness (adversarial training) and certified robustness?

- **Concept: Algorithmic recourse and counterfactual explanations**
  - Why needed here: The paper's core contribution is analyzing how robustness affects the ability to provide actionable explanations to affected individuals.
  - Quick check question: How does the cost of a recourse relate to the distance between the original instance and its counterfactual?

- **Concept: Neural Tangent Kernel (NTK) and its role in analyzing wide neural networks**
  - Why needed here: The theoretical analysis for non-linear models relies on NTK to derive bounds on parameter differences and their effects on recourse.
  - Quick check question: Why is the NTK approximation particularly useful for analyzing over-parameterized neural networks?

## Architecture Onboarding

- **Component map:** Data preprocessing → Model training (non-robust and robust variants) → Recourse generation (multiple methods) → Evaluation (cost and validity metrics) → Theoretical analysis module → Empirical validation module → Visualization and interpretation

- **Critical path:** 
  1. Train non-robust and adversarially robust models on the same datasets
  2. Generate recourses using multiple state-of-the-art methods for both model types
  3. Compute cost and validity metrics for all recourse-method-model combinations
  4. Compare results against theoretical bounds

- **Design tradeoffs:**
  - Model complexity vs. computational feasibility: Using larger networks might provide more realistic results but increase training time significantly
  - Perturbation radius ϵ vs. model utility: Higher robustness typically comes at the cost of standard accuracy
  - Number of recourse methods vs. comprehensive analysis: Including more methods provides better coverage but increases complexity

- **Failure signatures:**
  - If cost differences don't increase with robustness, the adversarial training implementation might be flawed
  - If validity doesn't decrease with robustness, the evaluation metric might not be capturing the intended behavior
  - If theoretical bounds are violated, there might be issues with the model training or recourse generation

- **First 3 experiments:**
  1. Train logistic regression models on the German Credit dataset with varying ϵ values and verify that adversarial accuracy improves as expected
  2. Generate SCFE recourses for a simple linear model and manually verify that the cost increases with robustness
  3. Visualize the decision boundary changes for a 2D synthetic dataset as robustness increases to observe the narrowing of valid regions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the trade-offs between adversarial robustness and actionable explanations vary across different types of machine learning models beyond linear and neural networks?
- Basis in paper: [explicit] The paper analyzes trade-offs for linear and neural network models, but acknowledges broader implications for other model types
- Why unresolved: The theoretical bounds and empirical analysis are limited to linear models and neural networks. The paper doesn't explore how these trade-offs might manifest in other model architectures like decision trees, random forests, or support vector machines.
- What evidence would resolve it: Empirical studies comparing cost and validity of recourses across diverse model types (e.g., decision trees, random forests, SVMs) when adversarially trained would provide insights into whether the observed trade-offs generalize to other architectures.

### Open Question 2
- Question: Can the theoretical bounds on cost and validity differences be tightened by incorporating additional model-specific constraints or regularization techniques?
- Basis in paper: [inferred] The paper derives theoretical bounds using general properties like Lipschitz continuity and triangle inequality, suggesting potential for refinement
- Why unresolved: The current bounds are derived using general mathematical properties, but may not capture model-specific characteristics that could lead to tighter bounds. The paper doesn't explore whether incorporating domain-specific knowledge or additional regularization could improve these bounds.
- What evidence would resolve it: Developing and testing refined theoretical bounds that incorporate model-specific constraints (e.g., sparsity regularization for linear models, architectural constraints for neural networks) and comparing their tightness against the current bounds would demonstrate whether tighter bounds are achievable.

### Open Question 3
- Question: How do the trade-offs between adversarial robustness and actionable explanations manifest in real-world deployment scenarios with concept drift and data distribution shifts?
- Basis in paper: [inferred] The paper focuses on static datasets and doesn't address dynamic real-world scenarios where data distributions may change over time
- Why unresolved: The analysis is conducted on static datasets without considering how the trade-offs might evolve when deployed in real-world settings where concept drift and data distribution shifts are common. The paper doesn't explore whether the observed trade-offs remain consistent under such conditions.
- What evidence would resolve it: Longitudinal studies tracking the cost and validity of recourses over time in deployed systems experiencing concept drift and distribution shifts would reveal how these trade-offs manifest in practice and whether they remain stable or change under different operational conditions.

## Limitations
- Theoretical bounds for non-linear models rely on idealized NTK assumptions that may not hold for finite-width networks
- Analysis is limited to specific recourse generation methods, potentially missing alternative approaches that could mitigate trade-offs
- Results may not generalize to other model architectures beyond logistic regression and simple neural networks

## Confidence

**High confidence:** The core observation that increasing model robustness reduces recourse validity and increases cost is consistently demonstrated across theoretical analysis and multiple datasets.

**Medium confidence:** The specific quantitative bounds derived for linear models rely on assumptions about model parameter distributions that may not hold in practice.

**Low confidence:** The NTK-based analysis for non-linear models depends on idealized assumptions about infinite-width networks that may not translate to finite implementations.

## Next Checks

1. Test the theoretical bounds with additional non-linear model architectures (e.g., convolutional networks) to verify if the cost-validity trade-off scales consistently
2. Implement and evaluate a recourse method that explicitly incorporates robustness into its objective function to determine if validity can be maintained despite model robustness
3. Conduct sensitivity analysis on the perturbation radius ε to identify thresholds where the trade-off becomes particularly severe or potentially breaks down