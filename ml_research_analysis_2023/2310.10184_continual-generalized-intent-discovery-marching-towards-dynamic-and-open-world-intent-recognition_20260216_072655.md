---
ver: rpa2
title: 'Continual Generalized Intent Discovery: Marching Towards Dynamic and Open-world
  Intent Recognition'
arxiv_id: '2310.10184'
source_url: https://arxiv.org/abs/2310.10184
tags:
- learning
- uni00000013
- classes
- cgid
- plrd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Continual Generalized Intent Discovery (CGID),
  a new task for dynamic intent recognition in open-world settings. CGID aims to continuously
  discover new intents from unlabeled out-of-domain data streams and incrementally
  add them to an existing intent classifier with minimal reliance on previous data.
---

# Continual Generalized Intent Discovery: Marching Towards Dynamic and Open-world Intent Recognition

## Quick Facts
- arXiv ID: 2310.10184
- Source URL: https://arxiv.org/abs/2310.10184
- Authors: 
- Reference count: 21
- Key outcome: PLRD achieves 7.57% (AIN D T), 1.09% (AOOD T), and 4.06% (AALL T) higher accuracy on Banking and 3.35% (AIN D T), 0.04% (AOOD T), and 2.13% (AALL T) higher accuracy on CLINC compared to baselines

## Executive Summary
This paper introduces Continual Generalized Intent Discovery (CGID), a novel task for dynamic intent recognition in open-world settings where new intents must be continuously discovered from unlabeled out-of-domain data streams. The proposed Prototype-guided Learning with Replay and Distillation (PLRD) method addresses key challenges including OOD noise, catastrophic forgetting, and the need to balance learning new intents while maintaining performance on old ones. PLRD achieves significant improvements over competitive baselines on Banking and CLINC datasets, demonstrating both strong performance and minimal forgetting across learning stages.

## Method Summary
The paper proposes PLRD for CGID, which consists of an encoder and joint classifier as the main module, plus three sub-modules: memory module for replaying old samples, class prototype module for generating pseudo-labels for new OOD samples, and feature distillation for alleviating catastrophic forgetting. The method uses class prototypes to guide pseudo-label assignment and contrastive learning, maintains a small memory buffer of old class samples for replay, and employs feature distillation from a frozen copy of the initial encoder to prevent forgetting. The total loss combines cross-entropy, prototype contrastive learning, instance-level contrastive loss, and feature distillation loss.

## Key Results
- PLRD achieves 7.57% (AIN D T), 1.09% (AOOD T), and 4.06% (AALL T) higher accuracy on Banking dataset compared to best baseline
- PLRD achieves 3.35% (AIN D T), 0.04% (AOOD T), and 2.13% (AALL T) higher accuracy on CLINC dataset compared to best baseline
- PLRD exhibits the least forgetting among all methods across both datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prototype-guided learning with contrastive objectives enables the model to learn discriminative representations for both old and new intents while mitigating OOD noise.
- Mechanism: The model uses class prototypes (linear projections of encoder outputs) to guide pseudo-label assignment and contrastive learning (PCL + instance-level contrastive loss) to pull similar samples together and push dissimilar ones apart.
- Core assumption: Samples from the same intent class cluster around a common prototype in the feature space, and contrastive learning can effectively separate these clusters.
- Evidence anchors:
  - [abstract]: "PLRD consists of a main module which composed of an encoder and a joint classifier and three sub-modules: (1) Memory module is responsible for replaying known class samples to balance the learning of new classes and maintain known classes; (2) Class prototype module is responsible for generating pseudo-labels for new OOD samples; (3)Feature distillation is responsible for alleviating catastrophic forgetting of old classes."
  - [section 3.3]: "By pulling similar samples into the same prototype, PCL can learn clear intent representations for new classes and maintain representations for old classes."
- Break condition: If the OOD data contains significant noise or the number of new classes is incorrectly estimated, the prototype-based pseudo-labeling can propagate errors to subsequent stages.

### Mechanism 2
- Claim: Feature distillation from a frozen copy of the initial encoder helps prevent catastrophic forgetting of old class knowledge.
- Mechanism: At the start of each OOD stage, the encoder is copied and frozen. The current encoder's outputs are constrained to be close to the frozen encoder's outputs for replayed old class samples using L2 loss.
- Core assumption: The frozen encoder captures the discriminative features for old classes, and maintaining similarity to these features will preserve old class knowledge.
- Evidence anchors:
  - [abstract]: "feature distillation for preventing catastrophic forgetting"
  - [section 3.4]: "To further remember the knowledge in the non-forgotten features, we integrate the feature distillation into PLRD. Specifically, at the beginning of stage t, we copy and freeze the encoder, denoted as f init(·). Then given replayed samples xi ∈ { xold} in a batch, we constrain the feature output f (xi) of the current encoder with the feature f init(xi)."
- Break condition: If the old class samples in memory are not representative or if the model changes too drastically, feature distillation may not effectively prevent forgetting.

### Mechanism 3
- Claim: Data replay from a small memory buffer helps balance learning new intents and maintaining performance on old intents.
- Mechanism: A memory module stores a small number of samples (e.g., 5) per class after each stage. These samples are replayed during training in subsequent stages to provide a stable reference for old class knowledge.
- Core assumption: Replaying a small set of old class samples provides sufficient signal to maintain old class knowledge without requiring storage of all past data.
- Evidence anchors:
  - [abstract]: "a memory module for replaying old samples"
  - [section 3.2]: "In the new learning stage, for each batch, we randomly select old class samples {xold} with the same number as new class samples {xnew} from M and input them into the BERT encoder f (·) together with new class samples xnew, i.e., |{xnew}| = |{xold}|, {x} = {xnew} ∪ {xold}."
- Break condition: If the memory buffer becomes too small or unrepresentative, or if the new classes are too dissimilar from old classes, replay may not effectively maintain old class performance.

## Foundational Learning

- Concept: Contrastive learning (instance-level and prototype-level)
  - Why needed here: To learn discriminative intent representations in the absence of labels for OOD data, by pulling together samples from the same intent and pushing apart samples from different intents.
  - Quick check question: How does the temperature parameter τ in the contrastive loss affect the separation of intent clusters?

- Concept: Prototype learning and pseudo-labeling
  - Why needed here: To assign pseudo-labels to unlabeled OOD data by finding the nearest class prototype, enabling supervised learning for OOD intent discovery.
  - Quick check question: What are the potential issues with using nearest prototype assignment for pseudo-labeling, and how can they be mitigated?

- Concept: Knowledge distillation and feature matching
  - Why needed here: To prevent catastrophic forgetting by constraining the model to maintain similar features for old class samples as it had in the previous stage.
  - Quick check question: How does the strength of the distillation loss (weighting factor) affect the trade-off between maintaining old knowledge and learning new knowledge?

## Architecture Onboarding

- Component map: Input text -> BERT encoder -> Class prototypes -> Pseudo-labels/Classifier logits -> Loss computation (CE + PCL + contrastive + distillation) -> Parameter updates
- Critical path: Input text → Encoder → Class prototypes → Pseudo-labels (for OOD) / Classifier logits (for all) → Loss computation (CE + PCL + contrastive + distillation) → Parameter updates
- Design tradeoffs:
  - Memory buffer size vs. performance: Larger buffer improves old class maintenance but increases memory usage and may slow training
  - Strength of distillation loss vs. plasticity: Stronger distillation prevents forgetting but may hinder learning new classes
  - Number of contrastive samples vs. computation: More contrastive samples improve representation learning but increase training time
- Failure signatures:
  - Catastrophic forgetting: Significant drop in old class performance as new classes are learned
  - Noisy pseudo-labels: Poor OOD discovery performance, with many samples assigned to wrong intent clusters
  - Over-regularization: Slow learning of new classes due to excessive constraint from old knowledge
- First 3 experiments:
  1. Validate pseudo-label quality: Visualize intent representations (e.g., using t-SNE) to check if OOD samples are clustering correctly around prototypes.
  2. Measure forgetting: Track old class performance (e.g., AIND) across stages to ensure it doesn't degrade significantly.
  3. Ablation study: Remove one component (e.g., feature distillation) at a time to assess its impact on overall performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy of the OOD class number estimation algorithm change as the number of CGID stages increases?
- Basis in paper: [inferred] The paper mentions that estimation errors can accumulate over stages, leading to biased estimation and deteriorated performance.
- Why unresolved: The paper only provides a single example of estimation accuracy on the Banking dataset with OOD ratio 60%. It does not explore how estimation accuracy changes over multiple stages or with different OOD ratios.
- What evidence would resolve it: Experiments showing estimation accuracy for each stage across multiple runs and different OOD ratios would clarify how estimation performance changes over time.

### Open Question 2
- Question: How does the performance of PLRD compare to an ideal upper bound method that has no forgetting?
- Basis in paper: [inferred] The paper states that PLRD's performance still has a large gap to improve compared to a theoretical upper bound of a model without forgetting previous knowledge.
- Why unresolved: The paper does not provide any comparison to such an upper bound method or quantify the gap.
- What evidence would resolve it: Experiments comparing PLRD to a method that perfectly retains all previous knowledge (e.g. a model that jointly trains on all data) would quantify the performance gap due to forgetting.

### Open Question 3
- Question: How does the performance of PLRD change if it does not use any replay samples?
- Basis in paper: [inferred] The paper mentions that all baselines and PLRD use a small number of previous samples for replay, and exploring a CGID method without utilizing any previous samples is a direction for future work.
- Why unresolved: The paper does not provide any experiments or analysis of PLRD's performance without replay.
- What evidence would resolve it: Experiments showing PLRD's performance with different amounts of replay (including zero replay) would quantify the importance of replay for mitigating forgetting.

## Limitations
- The method's performance in scenarios with highly overlapping intent classes or significantly imbalanced data distributions is unclear
- The robustness of PLRD to noisy OOD data and its performance in cases of significant class overlap or data imbalance are not thoroughly explored
- The pseudo-label generation process may be vulnerable to noise propagation, especially when the number of new classes is misestimated

## Confidence
- **High Confidence**: The overall architecture and training procedure of PLRD, including the use of prototype-guided learning, feature distillation, and data replay, are well-defined and supported by the experimental results.
- **Medium Confidence**: The effectiveness of PLRD in preventing catastrophic forgetting and maintaining old class performance is demonstrated, but the extent to which this generalizes to other datasets or scenarios is uncertain.
- **Low Confidence**: The robustness of PLRD to noisy OOD data and its performance in cases of significant class overlap or data imbalance are not thoroughly explored.

## Next Checks
1. Buffer Size Sensitivity: Conduct experiments with varying memory buffer sizes to determine the optimal balance between maintaining old class knowledge and computational efficiency.
2. Pseudo-label Quality Analysis: Perform a detailed analysis of the pseudo-label generation process, including visualization of intent representations and evaluation of pseudo-label accuracy.
3. Generalization to Imbalanced Data: Test PLRD on datasets with highly imbalanced class distributions to assess its robustness and identify potential failure modes.