---
ver: rpa2
title: 'Estimating Contamination via Perplexity: Quantifying Memorisation in Language
  Model Evaluation'
arxiv_id: '2309.10677'
source_url: https://arxiv.org/abs/2309.10677
tags:
- contamination
- data
- training
- benchmarks
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to quantify data contamination
  in language model evaluation without access to the full training set. The method
  measures contamination extent by comparing the perplexity of test sets to memorised
  and clean baselines.
---

# Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation

## Quick Facts
- arXiv ID: 2309.10677
- Source URL: https://arxiv.org/abs/2309.10677
- Reference count: 4
- Key outcome: Novel approach to quantify data contamination in language model evaluation without access to full training set by comparing perplexity of test sets to memorised and clean baselines.

## Executive Summary
This paper proposes a novel approach to quantify data contamination in language model evaluation benchmarks without requiring access to the full training set. The method measures contamination extent by comparing the perplexity of test sets to two baselines - one representing materials likely included in training (memorised baseline) and one representing materials not included (clean baseline). If the test set's perplexity is closer to the memorised baseline, it indicates significant memorisation. Experiments on reading comprehension, summarisation, and multiple choice benchmarks show that recent foundation models exhibit significant memorisation on reading comprehension and summarisation benchmarks, while multiple choice quizzes appear less contaminated.

## Method Summary
The proposed method works by verbalizing benchmark samples into model-compatible formats, then computing perplexity for both the test sets and two carefully constructed baselines. The memorised baseline uses historical data from the same source and format as the test set but from periods when the model was likely trained. The clean baseline uses more recent data created after the model's training period. By comparing whether test data perplexity is closer to the memorised or clean baseline, the method quantifies contamination extent. The approach is designed to work without access to the full training set, making it broadly applicable for evaluating existing foundation models.

## Key Results
- Recent foundation models show significant memorisation on popular reading comprehension and summarisation benchmarks
- Multiple choice benchmarks appear less contaminated compared to other benchmark types
- Larger models like GPT-3 and LLAMA-13B/30B exhibit more significant memorisation compared to smaller models like LLAMA-7B
- The method successfully identifies contamination without requiring access to model training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perplexity differences can reliably indicate contamination levels when comparing test data to memorised and clean baselines.
- Core assumption: Perplexity accurately reflects the likelihood of a model having seen particular text during training.
- Break condition: If the model's training data differs significantly from the assumed memorised baseline sources, the perplexity comparison may not accurately reflect contamination levels.

### Mechanism 2
- Claim: By comparing perplexity across different time periods, we can identify contamination in evaluation benchmarks.
- Core assumption: Models trained before certain dates cannot have seen data created after those dates, creating a clear temporal boundary for clean data.
- Break condition: If the model was trained on data from multiple time periods or if data from future periods leaked into training, temporal separation becomes ineffective.

### Mechanism 3
- Claim: Format and source matching between baselines and test data ensures fair perplexity comparison.
- Core assumption: Perplexity is comparable across texts with similar characteristics, making contamination the primary variable affecting perplexity differences.
- Break condition: If the model's training data included significant amounts of data with different characteristics than the test set, matching format and source may not be sufficient for fair comparison.

## Foundational Learning

- Concept: Perplexity as a measure of language model likelihood
  - Why needed here: The entire contamination detection method relies on comparing perplexity values between test data and baselines.
  - Quick check question: If a model assigns probability 0.01 to a sequence of length 5, what is the perplexity of that sequence?

- Concept: Temporal separation in training data
  - Why needed here: The method uses time-based separation to create clean baselines that definitely weren't in training data.
  - Quick check question: If a model was trained on data from 2016-2019 and released in 2020, could it have seen data created in 2021 during training?

- Concept: Format and source matching in NLP evaluation
  - Why needed here: Ensuring fair comparison requires baselines to match test data characteristics beyond just content.
  - Quick check question: If a reading comprehension benchmark uses Wikipedia articles, why would using Twitter data as a baseline potentially invalidate the comparison?

## Architecture Onboarding

- Component map: Verbalization module → Perplexity computation engine → Baseline preparation system → Analysis comparison module → Visualization dashboard
- Critical path: Verbalization → Perplexity computation → Baseline preparation → Analysis comparison → Results generation
- Design tradeoffs:
  - Temporal separation vs. completeness: Using only post-release data as clean baseline may miss some truly clean data from before release
  - Format matching vs. representativeness: Perfect format matching may require sacrificing some diversity in baseline data
  - Model coverage vs. computational cost: Testing more models increases confidence but requires more computation
- Failure signatures:
  - Unusually high perplexity across all comparisons may indicate verbalization errors
  - Similar perplexity values for both baselines suggest the baselines are not sufficiently distinct
  - Unexpected temporal patterns in multi-choice benchmarks may indicate format issues
- First 3 experiments:
  1. Test the method on synthetic contaminated data where ground truth contamination is known
  2. Compare perplexity-based contamination detection with n-gram overlap methods on a small benchmark
  3. Validate temporal separation assumption by testing a model on data from known time periods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the extent of contamination vary across different types of language model benchmarks (e.g., reading comprehension, summarization, multiple choice)?
- Basis in paper: The paper presents case studies on reading comprehension, summarization, and multiple choice benchmarks, showing varying levels of contamination.
- Why unresolved: The paper provides a preliminary analysis but does not offer a comprehensive comparison across all benchmark types or explore the underlying reasons for these differences.
- What evidence would resolve it: A large-scale study comparing contamination levels across a diverse set of benchmark types, along with an analysis of the factors contributing to these differences.

### Open Question 2
- Question: How does the size of the language model affect its susceptibility to contamination?
- Basis in paper: The paper shows that larger models like GPT-3 and LLAMA-13B/30B exhibit more significant memorisation on benchmarks compared to smaller models like LLAMA-7B.
- Why unresolved: The paper does not explore the relationship between model size and contamination in detail or investigate the mechanisms behind this correlation.
- What evidence would resolve it: A systematic study examining contamination levels in models of varying sizes, along with an analysis of the factors contributing to increased memorisation in larger models.

### Open Question 3
- Question: Can the proposed perplexity-based method be extended to detect contamination in other types of models, such as vision or multimodal models?
- Basis in paper: The paper focuses on language models and does not discuss the applicability of the method to other model types.
- Why unresolved: The paper does not explore the potential for adapting the method to different model architectures or data modalities.
- What evidence would resolve it: Experiments applying the perplexity-based method to various types of models and data, along with an analysis of the method's effectiveness and limitations in these contexts.

### Open Question 4
- Question: How does the temporal aspect of training data affect contamination levels in benchmarks?
- Basis in paper: The paper uses Wikipedia articles from different time periods as memorised and clean baselines to assess contamination.
- Why unresolved: The paper does not investigate the relationship between the age of training data and contamination levels or explore the impact of using more recent or historical data.
- What evidence would resolve it: A study examining contamination levels in benchmarks using training data from different time periods, along with an analysis of the factors contributing to temporal variations in contamination.

## Limitations

- Limited model coverage with only 5 foundation models tested, potentially missing diverse contamination patterns
- Temporal assumption vulnerability if models were trained on data extending beyond official release dates
- Format matching challenges across diverse benchmark types may affect fair perplexity comparison

## Confidence

- High Confidence: The fundamental premise that perplexity can indicate memorisation is well-established in the literature
- Medium Confidence: Specific contamination levels and comparative analysis between model families are reasonably supported but could benefit from broader validation
- Low Confidence: Exact contamination percentages and precise model rankings should be interpreted cautiously due to limited model sample size

## Next Checks

1. Conduct a controlled experiment using models with known, verifiable training dates and test them on data from precisely defined time periods to validate temporal separation assumptions
2. Apply the perplexity-based contamination detection method alongside alternative approaches such as n-gram overlap analysis and embedding similarity measures on a subset of benchmarks
3. Test the method on a broader range of models including different scales (from 1B to 175B parameters) and different training approaches to determine whether contamination patterns are universal or model-specific