---
ver: rpa2
title: 'Agents: An Open-source Framework for Autonomous Language Agents'
arxiv_id: '2309.07870'
source_url: https://arxiv.org/abs/2309.07870
tags:
- agents
- language
- agent
- multi-agent
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AGENTS, an open-source framework for building
  autonomous language agents using large language models (LLMs). The framework addresses
  limitations of existing agent frameworks by providing features such as long-short
  term memory, tool usage, multi-agent communication, human-agent interaction, and
  fine-grained symbolic control through Standard Operating Procedures (SOPs).
---

# Agents: An Open-source Framework for Autonomous Language Agents

## Quick Facts
- arXiv ID: 2309.07870
- Source URL: https://arxiv.org/abs/2309.07870
- Reference count: 3
- Introduces AGENTS, an open-source framework for building autonomous language agents using LLMs

## Executive Summary
This paper presents AGENTS, a comprehensive open-source framework for building autonomous language agents using large language models. The framework addresses key limitations in existing agent frameworks by integrating long-short term memory, tool usage, multi-agent communication, and human-agent interaction. AGENTS introduces Standard Operating Procedures (SOPs) as symbolic plans to provide fine-grained control over agent behavior, making execution more predictable than purely LLM-driven approaches. The framework is designed to be accessible to non-specialists while remaining extensible for researchers and developers.

## Method Summary
AGENTS provides a modular framework where agents follow SOPs defined as state graphs with transitions, prompts, and tools. The system integrates memory components using VectorDB for long-term storage and scratchpad for short-term working memory. Agents are built from configurable components including PromptComponent (task, rules, demos, format) and ToolComponent (APIs, tools). The framework supports deployment as APIs and includes an Agent Hub for sharing pre-built agents. Agents interact with environments through observation and update interfaces, with SOP routing controlling state transitions and agent selection.

## Key Results
- AGENTS enables non-specialists to build, customize, test, tune, and deploy state-of-the-art autonomous language agents without extensive coding
- The framework demonstrates capabilities through case studies including single-agent systems (customer service, sales, shopping assistant) and multi-agent systems (fiction studio, debate, software company)
- AGENTS provides user-friendly API deployment and an Agent Hub for sharing and discovering pre-built agents

## Why This Works (Mechanism)

### Mechanism 1
The SOP layer provides fine-grained symbolic control over agent behavior through state graph definitions. SOPs include LLM-based control functions for state transitions and agent routing, but the symbolic plan constrains randomness for more predictable execution. This improves reliability compared to pure LLM planning.

### Mechanism 2
Long-short term memory integration via VectorDB and scratchpad enables coherent context maintenance. Long-term memories are embedded, stored in VectorDB, and retrieved via semantic search, while short-term memory is updated through LLM prompts. This allows agents to reason over extended interactions.

### Mechanism 3
Modular component-based prompt construction reduces the expertise barrier through factorization into PromptComponent and ToolComponent. This abstraction enables easy customization without deep coding expertise while maintaining extensibility for advanced users.

## Foundational Learning

- **State machine modeling for agent workflows**
  - Why needed: SOPs are implemented as state graphs; understanding state transitions is critical for customizing agent behavior
  - Quick check: What is the difference between a state transition function and an agent routing function in SOPs?

- **Vector database and semantic search for memory retrieval**
  - Why needed: Long-term memory relies on VectorDB and semantic search; knowing how embeddings work is key to debugging memory issues
  - Quick check: How does semantic search in VectorDB differ from keyword search in standard databases?

- **Modular prompt engineering and component-based design**
  - Why needed: AGENTS uses PromptComponent and ToolComponent; understanding this modularity is essential for customizing agents without deep coding
  - Quick check: Why is it advantageous to separate task specification from tool usage in prompt components?

## Architecture Onboarding

- **Component map:**
  - Config file -> Agent(s) -> SOP -> Environment
  - SOP -> Agent routing -> Agent -> Environment update
  - Component (PromptComponent, ToolComponent) -> Config file

- **Critical path:**
  1. Load config → initialize Agent(s), SOP, Environment
  2. SOP routes agent and state → Agent observes environment
  3. Agent acts based on state and tools → Environment updates
  4. SOP transitions to next state → Loop until finished

- **Design tradeoffs:**
  - Flexibility vs. predictability: SOPs constrain randomness but require manual planning
  - Modularity vs. performance: Component-based design aids customization but may add overhead
  - Human-in-the-loop vs. full autonomy: is_human flag enables interaction but breaks full automation

- **Failure signatures:**
  - Agent loops indefinitely: Likely due to incomplete or circular SOP state definitions
  - Memory retrieval fails: VectorDB query returns irrelevant or empty results
  - Tool usage errors: ToolComponent.func() not properly wrapped or context not passed
  - Prompt generation fails: Component prompts misconfigured or incompatible

- **First 3 experiments:**
  1. Deploy a single chit-chat agent using only the Agent and Environment classes; verify basic interaction
  2. Add SOP with a simple two-state graph (e.g., greeting → response) and test state transitions
  3. Integrate a ToolComponent (e.g., web search) and confirm agent can retrieve external info

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do symbolic operating procedures (SOPs) compare to pure LLM-based planning in terms of agent performance and user control?
- Basis in paper: The paper explicitly states that AGENTS uses SOPs to provide "fine-grained control" over agent behavior, contrasting it with frameworks that "solely depend on a short task description and rely completely on the abilities of LLMs to plan and act."
- Why unresolved: While the paper claims SOPs offer advantages, it doesn't provide empirical comparisons between agents using SOPs and those relying purely on LLM planning.
- What evidence would resolve it: Controlled experiments comparing task completion rates, consistency across runs, and user satisfaction between agents using SOPs and those using only LLM-based planning.

### Open Question 2
- Question: What are the limitations of dynamic scheduling in multi-agent communication, and under what conditions does it fail to improve coordination?
- Basis in paper: The paper introduces "dynamic scheduling" as a feature for multi-agent communication but doesn't discuss its limitations or failure modes.
- Why unresolved: The paper presents dynamic scheduling as an improvement over pre-defined sequential ordering but doesn't analyze when this approach might break down.
- What evidence would resolve it: Case studies or experiments identifying scenarios where dynamic scheduling leads to suboptimal coordination, increased latency, or deadlocks in multi-agent systems.

### Open Question 3
- Question: How does human-agent interaction in multi-agent systems scale with increasing numbers of human participants and agents?
- Basis in paper: The paper mentions that AGENTS "seamlessly supports human-agent interaction in both single-agent and multi-agent scenarios" but doesn't discuss scalability issues.
- Why unresolved: While the framework supports human participation, the paper doesn't address computational or coordination challenges that arise when scaling to many humans and agents.
- What evidence would resolve it: Performance benchmarks and user experience studies measuring latency, coordination complexity, and task completion as the number of human participants and agents increases.

## Limitations
- The paper lacks explicit evaluation metrics and quantitative performance measurements against existing frameworks
- SOP state graphs create potential brittleness when symbolic plans are incomplete or incorrectly specified
- The memory system's dependence on semantic search in VectorDB could degrade with database growth, with no scaling strategies discussed

## Confidence

**High confidence:** Framework's architectural soundness and modular design approach appears technically sound and addresses real needs for non-specialist users.

**Medium confidence:** Claims about predictability and stability improvements are logically sound but lack empirical validation showing actual performance gains over pure LLM approaches.

**Low confidence:** Framework's scalability and robustness claims are not supported by performance benchmarks or stress testing results.

## Next Checks

1. Implement quantitative comparison between AGENTS agents with SOPs versus pure LLM agents on standard task completion benchmarks, measuring success rate and response consistency across multiple trials.

2. Test memory system with growing VectorDB (100K+ entries) to evaluate retrieval accuracy degradation over time and assess whether semantic search maintains relevance as database scales.

3. Create stress tests with incomplete or circular SOP state definitions to measure how frequently controller falls back to unpredictable LLM behavior versus handling edge cases gracefully.