---
ver: rpa2
title: 'A Benchmark for Text Expansion: Datasets, Metrics, and Baselines'
arxiv_id: '2309.09198'
source_url: https://arxiv.org/abs/2309.09198
tags:
- text
- modifiers
- expansion
- source
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of Text Expansion (TE), which aims
  to enrich plain text by inserting fine-grained modifiers without altering core semantics.
  The authors propose four complementary automatic approaches to construct a large-scale
  dataset of 12 million instances for both English and Chinese, and design various
  automatic evaluation metrics including a novel Info-Gain metric for measuring informativeness.
---

# A Benchmark for Text Expansion: Datasets, Metrics, and Baselines

## Quick Facts
- arXiv ID: 2309.09198
- Source URL: https://arxiv.org/abs/2309.09198
- Reference count: 40
- Key outcome: Introduces Text Expansion task with 12M dataset, automatic metrics including Info-Gain, and demonstrates superiority of Locate&Infill models over Text2Text baselines

## Executive Summary
This paper introduces Text Expansion (TE), a task that enriches plain text by inserting fine-grained modifiers without altering core semantics. The authors propose four complementary automatic approaches to construct a large-scale dataset of 12 million instances for both English and Chinese. They design various automatic evaluation metrics, including a novel Info-Gain metric for measuring informativeness, and develop multiple baseline models based on Text2Text and Locate&Infill objectives. Experiments show that Locate&Infill models are more suitable for TE, particularly excelling in expansion informativeness, establishing a foundation for future research in this area.

## Method Summary
The paper constructs a large-scale dataset using four complementary approaches: Neural Sentence Compression, Constituency Tree Pruning, IsA Relationship extraction, and Masked Modifier Prediction. Baseline models are built using pre-trained text-infilling models (T5) with two objectives: Text2Text (T5, BART, GPT2) and Locate&Infill (pipelined and joint frameworks). The evaluation employs multiple automatic metrics including BLEU, Fidelity, Fluency (PPL), Fertility (N-Pos, Len), Coherence (Nli-E), and Informativeness (Info-Gain) to assess expansion quality from various perspectives.

## Key Results
- Locate&Infill models outperform Text2Text baselines, especially in expansion informativeness (higher Info-Gain scores)
- Automatic construction yields 12 million valid instances with 90.95% English and 87.80% Chinese quality
- Info-Gain metric shows strong correlation with human judgments (Pearson 0.7769, Spearman 0.7710)
- Text expansion task is feasible and points to promising future research directions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Text infilling models are better suited for text expansion because they can generate context-specific modifiers that are both fluent and coherent.
- **Mechanism**: The text infilling approach allows the model to focus on generating modifiers that are semantically coherent with the surrounding context, rather than just continuing the text. This is achieved by masking specific positions in the text and asking the model to fill in the blanks, which encourages the generation of relevant and context-specific modifiers.
- **Core assumption**: The pre-trained text infilling model has learned to generate fluent and coherent text that is semantically related to the context.
- **Evidence anchors**:
  - [abstract]: "On top of a pre-trained text-infilling model, we build both pipelined and joint Locate&Infill models, which demonstrate the superiority over the Text2Text baselines, especially in expansion informativeness."
  - [section 7.1]: "Another obvious benefit of Locate&Infill is that it yields a higher Info-Gain score than the Text2Text baselines. The model will be more focused on the adjacent context through infilling rather than continuation, which is helpful for generating context-specific and informative modifiers."
  - [corpus]: The automatic construction of the dataset using masked modifier prediction relies on the assumption that the pre-trained text infilling model can generate fluent and coherent modifiers when given the right context.
- **Break condition**: If the pre-trained text infilling model is not able to generate fluent and coherent text that is semantically related to the context, the text infilling approach will not be effective for text expansion.

### Mechanism 2
- **Claim**: The automatic construction of the dataset using multiple complementary approaches leads to a more comprehensive and diverse set of text-expansion pairs.
- **Mechanism**: The four approaches (Neural Sentence Compression, Constituency Tree Pruning, IsA Relationship, and Masked Modifier Prediction) are designed to capture different aspects of text expansion, such as extracting fine-grained modifiers, hypernym-based modifiers, and context-specific modifiers. By combining these approaches, the dataset is more likely to cover a wide range of text-expansion scenarios.
- **Core assumption**: Each approach is able to extract or generate valid text-expansion pairs that are semantically coherent and informative.
- **Evidence anchors**:
  - [section 3.5]: "We conclude that NSC is able to extract some complicated patterns which are hard to be identified by CTP. But NSC datasets are not available in Chinese and are limited to the news domain in English. CTP is better at extracting fine-grained modifiers like adjective phrases which are easy to be covered by syntactic rules. IAR is responsible for extracting knowledge-rich hypernym-based modifiers, which is also a typical type of modifier but might be rarely covered by NSC and CTP."
  - [section 4.2]: "Table 4 shows the quality of the assessed pairs. We observe that the proportion of eligible pairs for English and Chinese are 90.95% and 87.80%, respectively."
  - [corpus]: The dataset statistics show that the different approaches contribute a significant number of valid text-expansion pairs for both English and Chinese.
- **Break condition**: If any of the approaches fail to extract or generate valid text-expansion pairs, or if the combination of approaches leads to a high proportion of low-quality pairs, the automatic construction of the dataset will not be effective.

### Mechanism 3
- **Claim**: The proposed Info-Gain metric is effective in measuring the informativeness of expanded modifiers.
- **Mechanism**: Info-Gain is based on the idea that non-trivial modifiers are usually informative for inferring the context from the source text. It is calculated as the ratio of the inherent perplexity of the source text to the perplexity of recovering the source text based on the expanded modifiers, with a penalty for repeating phrases from the source text.
- **Core assumption**: The pre-trained text infilling model used to estimate the perplexities is able to accurately capture the informativeness of the expanded modifiers.
- **Evidence anchors**:
  - [section 5]: "We find that non-trivial modifiers are usually informative for us to infer the context from the source text. Therefore, we propose a new metric, Info-Gain, to measure the informativeness of expansion results."
  - [section 7.3]: "The Pearson coefficient of 0.7769 and the Spearman coefficient of 0.7710 demonstrates the reliability of Info-Gain."
  - [corpus]: The Info-Gain scores for the different baseline models show a clear distinction between the more informative Locate&Infill models and the less informative Text2Text models.
- **Break condition**: If the pre-trained text infilling model used to estimate the perplexities is not able to accurately capture the informativeness of the expanded modifiers, or if the penalty for repeating phrases from the source text is not effective, the Info-Gain metric will not be a reliable measure of informativeness.

## Foundational Learning

- **Concept**: Text Infilling
  - **Why needed here**: Text infilling is the core mechanism used in the Locate&Infill models to generate context-specific modifiers for text expansion.
  - **Quick check question**: How does text infilling differ from text continuation, and why is it more suitable for the task of text expansion?

- **Concept**: Constituency Parsing
  - **Why needed here**: Constituency parsing is used in the Constituency Tree Pruning approach to extract text skeletons by pruning inessential subtrees from the constituency tree.
  - **Quick check question**: What are the main types of constituents that are pruned in the constituency tree, and how do they relate to the concept of modifiers in text expansion?

- **Concept**: Hypernym Detection
  - **Why needed here**: Hypernym detection is used in the IsA Relationship approach to extract knowledge-rich hypernym-based modifiers for text expansion.
  - **Quick check question**: What are some examples of hypernym-hyponym relationships, and how can they be used to enrich the content of a text through expansion?

## Architecture Onboarding

- **Component map**: Data construction (NSC → CTP → IAR → MMP) → Model training (Text2Text/Locate&Infill) → Evaluation (BLEU, Fidelity, PPL, N-Pos, Len, Nli-E, Info-Gain)
- **Critical path**: Automatic construction of the dataset using the four complementary approaches → Fine-tuning of the pre-trained text infilling model on the constructed dataset → Training of the baseline models using the Text2Text or Locate&Infill objectives → Evaluation of the baseline models using the proposed automatic metrics
- **Design tradeoffs**: The choice between the Text2Text and Locate&Infill objectives involves a tradeoff between simplicity and controllability. The Text2Text objective is simpler but less controllable, while the Locate&Infill objective is more complex but allows for more precise control over the insertion of modifiers.
- **Failure signatures**: Low fidelity scores indicate that the model is not able to preserve the original text during expansion. Low Info-Gain scores indicate that the expanded modifiers are not informative or context-specific. High perplexity scores indicate that the expanded text is not fluent or coherent. Low BLEU scores indicate that the expanded text is not similar to the reference text.
- **First 3 experiments**:
  1. Train and evaluate the Text2Text baseline models (Text2Text-T5, Text2Text-BART, and Text2Text-GPT2) on the constructed dataset using the proposed automatic metrics.
  2. Train and evaluate the Locate&Infill baseline models (Pipelined Locate&Infill and Joint Locate&Infill) on the constructed dataset using the proposed automatic metrics.
  3. Compare the performance of the Text2Text and Locate&Infill baseline models on the proposed automatic metrics, and analyze the strengths and weaknesses of each approach.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the Info-Gain metric perform compared to other informativeness measures when evaluated against human judgments across different text domains?
- **Basis in paper**: [explicit] The authors propose Info-Gain as a new metric for measuring informativeness and report Pearson (0.7769) and Spearman (0.7710) correlations with human judgments.
- **Why unresolved**: The paper only reports correlations on a single dataset (Chinese TE test set). The metric's generalizability to other domains or languages is not established.
- **What evidence would resolve it**: Cross-domain evaluation showing Info-Gain correlations with human judgments on multiple datasets including different text types (news, fiction, technical writing) and languages.

### Open Question 2
- **Question**: What is the optimal balance between automatic data construction approaches to maximize both informativeness and elegance in generated expansions?
- **Basis in paper**: [explicit] The authors use four complementary approaches (NSC, CTP, IAR, MMP) and note that each has different strengths and weaknesses.
- **Why unresolved**: The paper doesn't explore whether combining these approaches optimally or using weighted combinations could improve quality metrics.
- **What evidence would resolve it**: Systematic ablation studies comparing different combinations of the four approaches on quality metrics (Info-Gain, BLEU, human judgments) to identify optimal mixing ratios.

### Open Question 3
- **Question**: How do different decoding strategies (e.g., beam search, sampling, nucleus sampling) affect the fluency-informativeness trade-off in Text Expansion?
- **Basis in paper**: [inferred] The authors mention adjusting decoding procedures like controlling temperature parameter but note it may hurt fluency.
- **Why unresolved**: The paper doesn't systematically investigate how different decoding strategies impact the quality metrics, particularly the tension between fluency and informativeness.
- **What evidence would resolve it**: Comparative evaluation of multiple decoding strategies on the same test set measuring all quality metrics (PPL, Info-Gain, human judgments) to identify optimal trade-offs.

## Limitations
- Automatic dataset construction may introduce domain-specific biases
- Evaluation metrics are primarily automatic and may not fully capture human judgment nuances
- Results may be limited to news domain and may not generalize to other text types

## Confidence
- **High**: Core claims about text infilling superiority for context-specific modifier generation
- **Medium**: Claims about effectiveness of different approaches, heavy reliance on automatic evaluation metrics
- **Low**: Claims about generalizability across languages and domains

## Next Checks
1. Conduct human evaluation studies to validate the automatic metrics, particularly Info-Gain, against human judgments of informativeness
2. Test the approach on specialized domains (medical, legal, technical) to assess robustness and identify potential limitations
3. Compare the generated expansions with human-written content in controlled experiments to measure practical utility and quality