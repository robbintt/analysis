---
ver: rpa2
title: Flatness-Aware Minimization for Domain Generalization
arxiv_id: '2307.11108'
source_url: https://arxiv.org/abs/2307.11108
tags:
- generalization
- domain
- training
- adam
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the role of optimizers in domain generalization
  (DG), where the goal is to learn models robust to distribution shifts. While Adam
  is commonly used in DG benchmarks, the authors find it is not necessarily optimal.
---

# Flatness-Aware Minimization for Domain Generalization

## Quick Facts
- arXiv ID: 2307.11108
- Source URL: https://arxiv.org/abs/2307.11108
- Authors: 
- Reference count: 40
- This paper proposes FAD (Flatness-Aware Minimization for Domain Generalization), a unified framework that optimizes both zeroth-order and first-order flatness to improve OOD generalization.

## Executive Summary
This paper investigates the role of optimizers in domain generalization (DG), where the goal is to learn models robust to distribution shifts. While Adam is commonly used in DG benchmarks, the authors find it is not necessarily optimal. They propose FAD, a unified framework that optimizes both zeroth-order and first-order flatness to improve OOD generalization. FAD efficiently approximates the gradients of flatness metrics without computing Hessian or Hessian-vector products. Theoretical analysis shows FAD controls the dominant eigenvalue of the Hessian and provides an OOD generalization bound. Empirically, FAD outperforms other optimizers on various DG datasets. The authors also show FAD discovers flatter minima with lower Hessian spectra compared to other flatness-aware methods.

## Method Summary
FAD (Flatness-Aware Minimization for Domain Generalization) is a unified framework that optimizes both zeroth-order and first-order flatness to improve OOD generalization. It efficiently approximates the gradients of flatness metrics without computing Hessian or Hessian-vector products, reducing computation overhead while maintaining the penalty on loss landscape sharpness. FAD combines the gradients of base loss with approximations of zeroth-order and first-order flatness, controlled by hyperparameters α and β. Theoretical analysis shows FAD controls the dominant eigenvalue of the Hessian and provides an OOD generalization bound. The method is designed to discover flatter minima associated with better generalization performance in domain generalization tasks.

## Key Results
- FAD outperforms Adam and other optimizers on multiple DG benchmarks (PACS, VLCS, OfficeHome, TerraInc, DomainNet, NICO++)
- FAD discovers flatter minima with lower Hessian spectra (maximum eigenvalue and trace) compared to other flatness-aware methods
- Theoretical analysis shows FAD controls the dominant eigenvalue of the Hessian and provides an OOD generalization bound under covariate shift assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FAD improves OOD generalization by jointly optimizing zeroth-order and first-order flatness without computing Hessian or Hessian-vector products.
- Mechanism: FAD approximates gradients of flatness metrics using first-order gradients, reducing computation overhead while maintaining the penalty on loss landscape sharpness. This combined optimization strengthens generalization by controlling both types of flatness simultaneously.
- Core assumption: Both zeroth-order and first-order flatness contribute to generalization, and their combined optimization is more effective than either alone.
- Evidence anchors:
  - [abstract] "FAD efficiently approximates the gradients of flatness metrics without computing Hessian or Hessian-vector products"
  - [section 3.3] "We approximate second-order gradient with first-order gradients and optimize zeroth-order and first-order flatness simultaneously"
  - [corpus] "Sharpness-Aware Minimization (SAM) has emerged as a promising alternative optimizer to stochastic gradient descent (SGD)"
- Break condition: If the approximation of second-order gradients with first-order gradients becomes inaccurate for very deep networks or specific architectures, the flatness penalty may weaken.

### Mechanism 2
- Claim: FAD controls the dominant eigenvalue of the Hessian, which indicates the sharpness of the loss landscape and affects generalization.
- Mechanism: By optimizing the linear combination of zeroth-order and first-order flatness (Equation 3), FAD indirectly controls the maximum Hessian eigenvalue, which is a measure of landscape curvature. Lower eigenvalues indicate flatter minima associated with better generalization.
- Core assumption: The maximum eigenvalue of the Hessian is a valid measure of sharpness and correlates with generalization performance.
- Evidence anchors:
  - [abstract] "Theoretical analysis shows FAD controls the dominant eigenvalue of the Hessian and provides an OOD generalization bound"
  - [section 3.1] "we can derive that RFAD ρ,α (θ) is also related to the maximal eigenvalue"
  - [corpus] "Flatness of the loss landscape has been widely studied as an important perspective for understanding the behavior and generalization of deep learning algorithms"
- Break condition: If the relationship between maximum Hessian eigenvalue and generalization breaks down for certain types of models or datasets, FAD's theoretical guarantee may not hold.

### Mechanism 3
- Claim: FAD's OOD generalization bound provides theoretical assurance that optimizing flatness improves test performance under distribution shift.
- Mechanism: The generalization bound (Equation 6) shows that minimizing FAD regularization term (zeroth-order + first-order flatness) reduces the OOD generalization error, even without access to test distribution information during training.
- Core assumption: The bound holds under the covariate shift scenario where conditional probabilities are invariant across training and test distributions.
- Evidence anchors:
  - [section 3.2] "Proposition 3.1... shows that FAD controls the OOD generalization error"
  - [section 3.2] "Optimizing the regularization of FAD... leads to better generalization on test data"
  - [corpus] "The originally-proposed motivation behind SAM was to bias neural networks towards flatter minima that are believed to generalize better"
- Break condition: If the covariate shift assumption is violated (i.e., conditional probabilities change between training and test), the bound may not apply.

## Foundational Learning

- Concept: Loss landscape flatness and its relationship to generalization
  - Why needed here: Understanding flatness is crucial for grasping how FAD improves generalization through controlling loss landscape curvature
  - Quick check question: What is the difference between zeroth-order and first-order flatness in the context of loss landscape?
- Concept: Hessian matrix and its eigenvalues
  - Why needed here: The maximum eigenvalue of the Hessian indicates sharpness of minima, which FAD aims to control
  - Quick check question: How does the maximum eigenvalue of the Hessian relate to the curvature of a loss landscape minimum?
- Concept: Domain generalization and distribution shift
  - Why needed here: FAD is specifically designed for DG, where models must generalize to unseen distributions
  - Quick check question: What is the key difference between in-distribution generalization and out-of-distribution generalization?

## Architecture Onboarding

- Component map: Base loss gradient -> Zeroth-order flatness approximation -> First-order flatness approximation -> Weighted combination -> Parameter update
- Critical path: Compute gradients of base loss → Approximate gradients of zeroth-order flatness → Approximate gradients of first-order flatness → Combine all gradients → Update parameters
- Design tradeoffs: FAD trades computation overhead (compared to standard optimizers) for improved generalization through flatness-aware optimization
- Failure signatures: Poor generalization despite flat minima, excessive training time, sensitivity to hyperparameter choices
- First 3 experiments:
  1. Compare FAD with Adam on PACS dataset using ResNet-50 to verify the core claim about optimizer selection
  2. Measure Hessian spectra (maximum eigenvalue and trace) of minima found by FAD vs other optimizers on VLCS dataset
  3. Test FAD with different α values (0.0, 0.5, 1.0) on OfficeHome to understand the tradeoff between zeroth-order and first-order flatness contributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the term `supθ1,θ2∈Θ |Ltr(θ1, θ2) − Lte(θ1, θ2)|` in the OOD generalization bound be optimized without knowledge of the test distribution?
- Basis in paper: [explicit] The paper mentions this term represents the discrepancy between training and test data and cannot be optimized without assumptions on the connection between the two distributions.
- Why unresolved: This term appears in the generalization bound but is intractable to optimize directly as it requires knowledge of the unknown test distribution.
- What evidence would resolve it: Theoretical work that derives assumptions under which this term can be bounded or estimated from training data alone, or empirical studies showing techniques to approximate or regularize this term effectively.

### Open Question 2
- Question: Can a general indicator be learned to predict the optimal optimizer for a given DG method and dataset, eliminating the need for extensive hyperparameter search?
- Basis in paper: [inferred] The paper shows that different optimizers perform best on different DG datasets and methods, suggesting that choosing the right optimizer is crucial but requires trial and error.
- Why unresolved: While the paper demonstrates the importance of optimizer selection, it does not provide a systematic way to choose the best optimizer for a new DG method and dataset combination without extensive experimentation.
- What evidence would resolve it: Development of a meta-learning approach that learns to predict the best optimizer based on characteristics of the DG method and dataset, or empirical studies showing strong correlations between dataset/method properties and optimizer performance.

### Open Question 3
- Question: How does the choice of hyperparameters (ρ, α, β) in FAD affect its performance across different DG datasets and methods?
- Basis in paper: [explicit] The paper conducts an ablation study on the hyperparameters of FAD, showing that the choice of these hyperparameters affects performance, but the study is limited to PACS and uses grid search.
- Why unresolved: While the paper shows that hyperparameter choice matters, it does not provide a comprehensive understanding of how these hyperparameters interact with different DG datasets and methods.
- What evidence would resolve it: Extensive empirical studies across multiple DG datasets and methods, varying the hyperparameters systematically, or theoretical analysis of the impact of these hyperparameters on the flatness-aware regularization and optimization process.

## Limitations
- The theoretical OOD generalization bound relies on covariate shift assumptions that may not hold in all practical scenarios
- The efficiency claims need verification in terms of actual training time comparisons with baseline methods
- The hyperparameter sensitivity of FAD across different DG datasets and methods requires further investigation

## Confidence

- **High confidence**: FAD's ability to find flatter minima as evidenced by Hessian spectrum analysis
- **Medium confidence**: FAD's improvement over Adam specifically in domain generalization benchmarks
- **Medium confidence**: The theoretical OOD generalization bound under covariate shift assumptions

## Next Checks

1. Conduct ablation studies varying the α parameter to quantify the relative contribution of zeroth-order vs first-order flatness optimization
2. Perform runtime efficiency analysis comparing FAD's actual training time with other optimizers, including memory usage considerations
3. Test FAD on a broader range of domain generalization scenarios including more extreme distribution shifts to validate robustness claims