---
ver: rpa2
title: 'Public Perceptions of Gender Bias in Large Language Models: Cases of ChatGPT
  and Ernie'
arxiv_id: '2309.09120'
source_url: https://arxiv.org/abs/2309.09120
tags:
- bias
- gender
- chatgpt
- llms
- ernie
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined public perceptions of gender bias in large
  language models (LLMs) through content analysis of social media discussions about
  ChatGPT and Ernie. The analysis revealed that ChatGPT was associated with implicit
  gender bias (e.g., gender-profession associations), while Ernie demonstrated explicit
  bias (e.g., promoting early marriage for women).
---

# Public Perceptions of Gender Bias in Large Language Models: Cases of ChatGPT and Ernie

## Quick Facts
- arXiv ID: 2309.09120
- Source URL: https://arxiv.org/abs/2309.09120
- Reference count: 40
- Primary result: Social media analysis reveals cultural factors shape gender bias manifestation in ChatGPT (implicit) and Ernie (explicit), calling for context-specific governance

## Executive Summary
This study examines public perceptions of gender bias in large language models through analysis of social media discussions about ChatGPT and Ernie. The research reveals distinct patterns: ChatGPT was associated with implicit gender bias through subtle associations (e.g., linking men with doctors and women with teachers), while Ernie demonstrated explicit bias by promoting harmful stereotypes (e.g., endorsing early marriage for women). Cultural context emerged as a critical factor, with Ernie's bias linked to traditional Chinese cultural values, while ChatGPT showed political correctness. The findings highlight the need for context-specific governance strategies including auditing systems, legal protections, and AI literacy education to address gender bias in LLMs.

## Method Summary
The study employed thematic analysis of social media discussions from Twitter (ChatGPT) and Weibo (Ernie) collected between November 2022 and August 2023. Researchers used open coding and mind-mapping tools to identify themes around gender bias observations, cultural factors, and governance recommendations. The analysis compared perceptions between the US-based ChatGPT and China-based Ernie to understand how cultural contexts influence bias manifestation in large language models.

## Key Results
- ChatGPT associated with implicit gender bias through subtle profession-gender associations (e.g., men with doctors, women with teachers)
- Ernie demonstrated explicit gender bias by promoting harmful stereotypes (e.g., endorsing early marriage for women aged 20-25)
- Cultural factors significantly influence bias manifestation, with traditional Chinese values linked to Ernie's explicit bias while ChatGPT showed political correctness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Implicit bias in LLMs arises from statistical associations learned during training on biased corpora.
- Mechanism: When an LLM learns word embeddings, frequently co-occurring gender-profession pairs reinforce statistical associations. Over time, the model internalizes these associations as implicit biases that surface in generations without explicit stereotypical language.
- Core assumption: The training data reflects societal stereotypes at sufficient frequency to shape model parameters.
- Evidence anchors:
  - [abstract] "ChatGPT was more often found to carry implicit gender bias, e.g., associating men and women with different profession titles"
  - [section 4.1] "ChatGPT tended to assume men suitable for doctors, while women for teachers; men for teaching science and tech, while women for teaching arts"
  - [corpus] Weak: corpus neighbors mention "Large Language Models Portray Socially Subordinate Groups as More Homogeneous" but do not explicitly confirm implicit bias from training data.
- Break condition: If training data is explicitly debiased or filtered for stereotypical co-occurrences, the implicit associations weaken or disappear.

### Mechanism 2
- Claim: Explicit bias in LLMs can be amplified when training corpora contain overtly discriminatory content.
- Mechanism: Models trained on texts with explicit statements (e.g., promoting early marriage for women) reproduce those statements when prompted, because they lack contextual understanding to recognize harm.
- Core assumption: The training corpus contains unfiltered, culturally specific explicit biases.
- Evidence anchors:
  - [abstract] "Ernie demonstrated explicit bias (e.g., promoting early marriage for women)"
  - [section 5.1] "When one user asked Ernie... 'the appropriate age for women to get married,' Ernie answered, 'The best age for women to get married is 20-25... Women are devalued after 25...'"
  - [corpus] Missing: corpus does not provide explicit bias examples, so we rely on paper text.
- Break condition: If fine-tuning includes adversarial examples or explicit harm detection, the model refuses to generate such statements.

### Mechanism 3
- Claim: Cultural context shapes the type and visibility of bias in LLMs.
- Mechanism: LLMs trained in different cultural environments internalize region-specific norms; these norms determine whether bias appears implicit (subtle associations) or explicit (overt statements).
- Core assumption: Training corpora differ substantially between cultures in both content and norm expression.
- Evidence anchors:
  - [abstract] "Cultural factors were identified as influencing the manifestation of bias, with Ernie's bias linked to traditional Chinese cultural values"
  - [section 6.1] "Ernie demonstrated explicit, concerning gender bias... Harsh criticisms about the traditional, patriarchal Chinese culture... were regarded as a key factor leading to the explicit gender bias in Ernie"
  - [corpus] Weak: corpus neighbors do not discuss cultural differences in bias manifestation.
- Break condition: If cross-cultural debiasing techniques standardize bias detection thresholds, cultural differences in bias visibility diminish.

## Foundational Learning

- Concept: Statistical language modeling
  - Why needed here: Understanding how LLMs learn associations from training data explains the source of both implicit and explicit bias.
  - Quick check question: How do co-occurrence statistics in training corpora translate into learned model biases?
- Concept: Cultural framing of gender norms
  - Why needed here: Different societies express gender roles differently; recognizing this helps explain why Ernie shows explicit bias while ChatGPT shows implicit bias.
  - Quick check question: What cultural factors might cause explicit gender bias to be more socially acceptable in one region than another?
- Concept: Bias auditing and fairness metrics
  - Why needed here: Governance recommendations depend on measurable bias detection; knowing metrics like WinoBias benchmarks grounds policy discussions.
  - Quick check question: What quantitative benchmarks are commonly used to detect gender bias in NLP models?

## Architecture Onboarding

- Component map: Data pipeline: Web/text corpus collection → cleaning → tokenization → Model training: Pretraining on large corpora → fine-tuning on curated datasets → Evaluation: Bias benchmarks (WinoBias, etc.) → human-in-the-loop review → Deployment: API serving → monitoring → feedback collection
- Critical path: Data collection → training → evaluation → deployment monitoring
- Design tradeoffs:
  - Larger, unfiltered corpora increase model capability but also bias risk
  - Fine-tuning on curated, debiased data reduces bias but may limit cultural relevance
  - Transparency in data sources aids auditing but may expose sensitive cultural contexts
- Failure signatures:
  - Generation of profession-gender stereotypes (e.g., "doctors are men")
  - Explicit endorsement of harmful cultural norms (e.g., age-based marriage pressure)
  - Refusal to generate content on protected groups (over-correction to political correctness)
- First 3 experiments:
  1. Run WinoBias and analogous profession-gender benchmarks on both models to quantify implicit bias.
  2. Prompt both models with culturally sensitive scenarios (e.g., marriage age, career advice) and classify responses as implicit vs. explicit bias.
  3. Conduct ablation studies by fine-tuning on debiased corpora and measuring bias reduction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the specific cultural context of a country influence the type and severity of gender bias exhibited by large language models trained on data from that region?
- Basis in paper: [explicit] The paper discusses how Ernie (China-based) demonstrated explicit gender bias while ChatGPT (US-based) exhibited more implicit bias, and attributes these differences to cultural factors.
- Why unresolved: While the paper identifies cultural differences as a factor, it does not provide a systematic analysis of how specific cultural elements (e.g., traditional values, social norms) directly shape LLM bias patterns.
- What evidence would resolve it: Comparative studies analyzing training data from different cultures, correlating specific cultural values with bias manifestations in LLMs, and controlled experiments testing how cultural context influences model responses.

### Open Question 2
- Question: What governance mechanisms are most effective in mitigating gender bias in large language models while balancing free expression and cultural sensitivity?
- Basis in paper: [explicit] The paper proposes governance recommendations including auditing systems, legal protections, and AI literacy education, but acknowledges these need further development.
- Why unresolved: The paper identifies governance needs but does not evaluate which specific mechanisms work best in practice or how to implement them effectively across different cultural contexts.
- What evidence would resolve it: Empirical studies testing various governance approaches (auditing frameworks, legal interventions, educational programs) across different countries and measuring their effectiveness in reducing bias while maintaining cultural appropriateness.

### Open Question 3
- Question: How can large language models be designed to recognize and respond appropriately to culturally-specific gender bias without perpetuating harmful stereotypes?
- Basis in paper: [inferred] The paper highlights that ChatGPT demonstrated political correctness while Ernie showed explicit bias, suggesting different approaches to handling gender bias, but does not explore optimal design strategies.
- Why unresolved: The paper does not investigate technical solutions for creating culturally-aware LLMs that can navigate between different cultural contexts without reinforcing biases.
- What evidence would resolve it: Development and testing of model architectures that incorporate cultural context awareness, experiments comparing different debiasing techniques across cultural boundaries, and evaluation metrics for measuring cultural sensitivity in LLM responses.

## Limitations
- The study relies entirely on social media discussions as proxy data for model behavior, introducing interpretive risks
- Observed differences between ChatGPT and Ernie may reflect user perceptions rather than systematic model comparisons
- Cultural analysis connecting Ernie's bias to Chinese traditional values is based on qualitative interpretation without systematic corpus analysis

## Confidence
- High Confidence: The methodological approach of using social media discourse analysis is valid and the thematic coding process appears systematic
- Medium Confidence: The distinction between implicit and explicit bias as observed in user discussions, though this requires validation through controlled experiments
- Low Confidence: The causal attribution of Ernie's explicit bias to traditional Chinese cultural values without direct analysis of training data or systematic cross-cultural comparisons

## Next Checks
1. **Benchmark Validation**: Conduct standardized bias detection tests (WinoBias, CrowS-Pairs) on both models to quantitatively verify the implicit vs. explicit bias patterns observed in social media discussions
2. **Controlled Prompting Study**: Design identical prompts about gender roles and career choices for both models, systematically categorizing responses as implicit or explicit bias to test whether user observations match controlled outputs
3. **Cross-Cultural Fine-Tuning Experiment**: Fine-tune both models on culturally neutral corpora and repeat the prompting study to determine whether cultural differences in training data drive the observed bias patterns, isolating the effect of cultural context from model architecture differences