---
ver: rpa2
title: 'Offline RL with Observation Histories: Analyzing and Improving Sample Complexity'
arxiv_id: '2310.20663'
source_url: https://arxiv.org/abs/2310.20663
tags:
- offline
- learning
- observation
- histories
- bisimulation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies offline RL algorithms in partially observed
  settings, where policies must condition on observation histories rather than states.
  The authors theoretically analyze the efficiency of such algorithms and propose
  a method to improve them.
---

# Offline RL with Observation Histories: Analyzing and Improving Sample Complexity

## Quick Facts
- arXiv ID: 2310.20663
- Source URL: https://arxiv.org/abs/2310.20663
- Reference count: 35
- Key outcome: This paper studies offline RL algorithms in partially observed settings, where policies must condition on observation histories rather than states. The authors theoretically analyze the efficiency of such algorithms and propose a method to improve them.

## Executive Summary
This paper addresses the fundamental challenge of offline reinforcement learning in partially observable Markov decision processes (POMDPs), where policies must condition on observation histories rather than states. The authors show that na"ive application of offline RL algorithms to observation history MDPs suffers from poor sample complexity due to the inability to effectively leverage trajectory stitching. They propose learning compact representations of observation histories using bisimulation metrics, introducing a bisimulation loss that measures how well learned representations match the optimal metric. The approach is theoretically grounded and empirically validated across navigation and language tasks.

## Method Summary
The paper analyzes offline RL in POMDPs where policies condition on observation histories. The key insight is that standard trajectory stitching fails in observation history MDPs because histories rarely repeat in finite datasets. To address this, the authors propose learning compact representations of observation histories that induce a bisimulation metric. They introduce a bisimulation loss that captures how well these representations approximate the optimal metric, which can be optimized alongside standard offline RL objectives. The method is validated on three tasks: a gridworld with switch-dependent goals, ViZDoom maze navigation, and a Wordle language task, showing improved performance when the bisimulation loss is added to standard offline RL algorithms.

## Key Results
- Na"ive offline RL in observation-history MDPs can have very poor worst-case sample complexity due to ineffective trajectory stitching
- Learning compact representations using bisimulation metrics can greatly improve sample complexity bounds
- The proposed bisimulation loss either improves performance or is already implicitly minimized by standard offline RL when those methods work well

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Offline RL in POMDPs suffers poor sample complexity because it cannot effectively leverage trajectory stitching when policies condition on observation histories.
- **Mechanism:** In standard MDPs, trajectory stitching allows combining suboptimal trajectories that share similar states. However, in POMDPs where policies condition on observation histories, histories rarely repeat, preventing effective stitching. The paper shows this leads to exponential scaling in the worst case.
- **Core assumption:** Observation histories contain more information than states and thus repeat less frequently in finite datasets.
- **Evidence anchors:**
  - [abstract]: "naïvely, this leads to concerns on the efficiency of existing offline RL algorithms. Offline RL is much less likely to utilize suboptimal behaviors if stitching them requires shared observation histories among them, as histories are much less likely to repeat in datasets that are not prohibitively large."
  - [section]: "In MDPs, offline RL using value iteration will learn Q-values: bQ(st−1, at−1) = P s′ P(s′|st−1, at−1)bV(s′). Because bV(st) is known to be positive from observing τ′, offline RL can deduce that taking action at−1 at st−1 also has positive value, without explicitly observing it in the dataset. This becomes complicated in an observation history MDP, as offline RL will now learn bQ(τt−1, at−1) = P s′ P(s′|st−1, at−1)bV(τt). But bV(τt) is not known to be positive because τt has not been observed in the data."

### Mechanism 2
- **Claim:** Learning compact representations of observation histories that induce a bisimulation metric enables efficient offline RL in POMDPs.
- **Mechanism:** The paper proposes learning an encoder ϕ that maps observation histories to low-dimensional representations. When distances between these representations approximate the optimal bisimulation metric, the resulting summarized MDP allows trajectory stitching to work effectively, improving sample complexity.
- **Core assumption:** There exists a compact representation of histories that contains only features relevant for action selection.
- **Evidence anchors:**
  - [abstract]: "We then identify sufficient conditions under which offline RL can still be efficient – intuitively, it needs to learn a compact representation of history comprising only features relevant for action selection."
  - [section]: "We introduce a bisimulation loss that captures the extent to which this happens, and propose that offline RL can explicitly optimize this loss to aid worst-case sample complexity."

### Mechanism 3
- **Claim:** Existing offline RL algorithms may implicitly minimize the bisimulation loss as a byproduct of successful learning.
- **Mechanism:** The paper observes empirically that when standard offline RL algorithms work well, they also tend to minimize the bisimulation loss, suggesting that successful learning naturally produces good representations.
- **Core assumption:** Good performance in offline RL correlates with learning useful representations.
- **Evidence anchors:**
  - [abstract]: "Empirically, we show that across a variety of tasks either our proposed loss improves performance, or the value of this loss is already minimized as a consequence of standard offline RL, indicating that it correlates well with good performance."
  - [section]: "We also show in Section 7 that measuring ||bdϕ − d∗||2 2 is often indicative of effective stitching and offline RL performance, even when running existing, unmodified offline RL algorithms."

## Foundational Learning

- **Concept: Partially Observable Markov Decision Processes (POMDPs)**
  - Why needed here: The paper studies offline RL in POMDPs where states are not fully observable, requiring policies to condition on observation histories.
  - Quick check question: What is the key difference between MDPs and POMDPs in terms of state information available to the agent?

- **Concept: Trajectory Stitching**
  - Why needed here: Trajectory stitching is the mechanism by which offline RL combines suboptimal trajectories to create better ones, and the paper analyzes when this mechanism works or fails in POMDPs.
  - Quick check question: How does trajectory stitching work in standard MDPs, and why does it fail in observation history MDPs?

- **Concept: Bisimulation Metrics**
  - Why needed here: Bisimulation metrics provide a way to measure state equivalence based on future behavior, which the paper uses to learn compact representations of observation histories.
  - Quick check question: What does a bisimulation metric measure, and how does it relate to state abstraction?

## Architecture Onboarding

- **Component map:** Observation History Encoder -> Representation Learning Module -> Standard Offline RL Algorithm -> Policy Head
- **Critical path:** 1. Encode observation history → 2. Generate representation → 3. Compute value/policy → 4. Optimize with offline RL objective + optional bisimulation loss
- **Design tradeoffs:**
  - Explicit bisimulation loss vs. implicit learning: Adding the loss provides stronger guarantees but increases training complexity
  - Representation capacity: Higher capacity encoders may capture more relevant features but risk overfitting
  - Loss weight: Balancing the bisimulation loss against the primary RL objective
- **Failure signatures:**
  - Poor performance despite adding bisimulation loss: likely indicates the representation capacity is insufficient or the loss weight is incorrect
  - Stable training but no improvement over baseline: suggests the representations are already good enough without explicit loss
  - Exploding gradients in representation learning: may need gradient clipping or learning rate adjustment
- **First 3 experiments:**
  1. Run baseline offline RL algorithm on a simple POMDP (e.g., the gridworld) to establish performance floor
  2. Add the bisimulation loss with various weightings to identify the optimal trade-off
  3. Test on a more complex POMDP (e.g., ViZDoom) to verify scalability of the approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed bisimulation loss perform in more complex partially observable environments compared to simple gridworld and navigation tasks?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of the bisimulation loss in simple navigation and language tasks, but does not explore more complex environments.
- Why unresolved: The paper focuses on relatively simple tasks to demonstrate the concept, but does not provide evidence of scalability to more complex environments.
- What evidence would resolve it: Empirical results showing the performance of the bisimulation loss in more complex POMDPs, such as those with higher-dimensional observations or longer horizons.

### Open Question 2
- Question: What are the limitations of the bisimulation metric in capturing relevant features for action selection in observation histories?
- Basis in paper: [explicit] The paper mentions that bisimulation metrics offer strong guarantees for improving efficiency in MDPs and empirically work well with off-policy RL algorithms, but does not discuss potential limitations.
- Why unresolved: The paper focuses on the benefits of using bisimulation metrics but does not explore potential drawbacks or limitations.
- What evidence would resolve it: Theoretical analysis or empirical results showing scenarios where the bisimulation metric fails to capture relevant features or leads to suboptimal performance.

### Open Question 3
- Question: How does the proposed approach compare to other representation learning methods in POMDPs, such as spectral methods or belief state estimation?
- Basis in paper: [inferred] The paper mentions related work on representation learning in RL, including spectral methods and belief state estimation, but does not provide a direct comparison with the proposed approach.
- Why unresolved: The paper focuses on the proposed bisimulation loss approach but does not benchmark it against other representation learning methods.
- What evidence would resolve it: Empirical results comparing the performance of the bisimulation loss approach to other representation learning methods in POMDPs.

## Limitations

- Theoretical analysis assumes access to optimal bisimulation metric, but practical implementation requires estimation from data
- Empirical evaluation limited to relatively small-scale problems (10x10 gridworld, ViZDoom, Wordle)
- Sample complexity bounds still depend on concentrability coefficients that can be prohibitively large in practice

## Confidence

- **High Confidence:** The theoretical analysis of why na"ive offline RL fails in POMDPs due to lack of trajectory stitching is well-founded and clearly explained. The relationship between bisimulation metrics and compact representations is theoretically sound.
- **Medium Confidence:** The claim that learning representations with bisimulation metrics improves sample complexity is supported by theory, but the empirical validation is limited in scope and scale. The correlation between bisimulation loss minimization and good performance needs more rigorous statistical validation.
- **Medium Confidence:** The assertion that existing offline RL algorithms implicitly minimize bisimulation loss is based on empirical observation but lacks theoretical justification for why this should be true in general.

## Next Checks

1. **Scalability Test:** Implement and evaluate the approach on a larger-scale POMDP task (e.g., robotic manipulation with partial observations) to verify that the sample complexity improvements scale to more complex problems.

2. **Representation Quality Analysis:** Systematically vary the representation capacity and analyze how bisimulation loss minimization correlates with downstream performance across different dataset qualities and sizes.

3. **Theoretical Gap Analysis:** Derive explicit bounds on how estimation errors in the learned bisimulation metric affect the final policy performance, and validate these bounds empirically on controlled synthetic POMDPs.