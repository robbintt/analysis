---
ver: rpa2
title: 'BYOC: Personalized Few-Shot Classification with Co-Authored Class Descriptions'
arxiv_id: '2310.06111'
source_url: https://arxiv.org/abs/2310.06111
tags:
- class
- text
- user
- classification
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes BYOC, a method for building personalized text
  classifiers through interactive collaboration between the user and a large language
  model. The approach addresses the challenge of creating accurate classifiers with
  minimal labeled data, which is particularly relevant for personalized applications.
---

# BYOC: Personalized Few-Shot Classification with Co-Authored Class Descriptions

## Quick Facts
- **arXiv ID**: 2310.06111
- **Source URL**: https://arxiv.org/abs/2310.06111
- **Reference count**: 19
- **Primary result**: Achieves 90% accuracy on personalized email classification, outperforming state-of-the-art few-shot methods by 15%

## Executive Summary
BYOC introduces a novel approach to personalized text classification that replaces traditional few-shot examples with an interactive question-answering process between users and a large language model. Instead of providing examples in the prompt, users iteratively refine class descriptions through LLM-generated questions, enabling more accurate and personalized classifiers with minimal labeled data. The method demonstrates superior accuracy (90%) and token efficiency compared to standard few-shot approaches while requiring only 1% of the training data used by fully supervised models.

## Method Summary
BYOC works by having users provide initial class descriptions and then interactively refining these through a question-answering process during example labeling. The LLM generates clarifying questions about ambiguous cases, the user answers these questions, and the system updates the class descriptions based on this feedback. During inference, the classifier uses only the refined class descriptions rather than examples, making it more token-efficient. The approach leverages chain-of-thought prompting for all LLM interactions and can be applied to any text classification task where user-specific criteria matter.

## Key Results
- Achieves 90% accuracy on personalized email classification, 15% higher than state-of-the-art few-shot methods
- Reaches 82% of the accuracy of models trained on full datasets while using only 1% of their training data
- Uses 1,909 fewer tokens per example compared to traditional few-shot example approaches

## Why This Works (Mechanism)

### Mechanism 1
Interactive question-answering allows the LLM to discover class-specific features not initially obvious to users. The LLM generates clarifying questions that identify ambiguous cases, and user answers provide specific, personalized criteria incorporated into refined class descriptions. This works because the LLM can effectively probe the user's implicit classification criteria, though it breaks if questions are irrelevant or overly generic.

### Mechanism 2
Summarizing training examples into class descriptions is more token-efficient than using full examples in prompts. Instead of including multiple full training examples that consume tokens and context window, the LLM distills key features into compact class descriptions. This assumes the LLM can maintain classification accuracy while summarizing, but breaks if critical distinguishing features are lost.

### Mechanism 3
Personalization improves accuracy because users have unique criteria that generic classifiers miss. BYOC captures individual user preferences through interactive refinement, creating classifiers that match specific user definitions rather than population averages. This works when users genuinely have different classification criteria, but is unnecessary if user preferences are similar across users.

## Foundational Learning

- **Chain of thought prompting**: Enables the LLM to explain reasoning when generating questions, making predictions, and updating class descriptions. Quick check: What does "chain of thought prompting" mean in BYOC's functions?
- **Few-shot learning vs zero-shot learning**: BYOC positions itself between these approaches, using minimal user interaction instead of examples or just instructions. Quick check: How does BYOC differ from traditional few-shot approaches using examples?
- **Token efficiency in LLM prompts**: Understanding why summarizing examples into class descriptions is more efficient than including full examples. Quick check: Why would full training examples in prompts be less token-efficient than BYOC's approach?

## Architecture Onboarding

- **Component map**: User interface -> Interactive training loop -> Summary engine -> Classification engine
- **Critical path**: User provides purpose → Annotates examples with Q&A → Class descriptions refined → Classifier deployed
- **Design tradeoffs**: More questions per example → better personalization but more user effort; more training examples → better accuracy but diminishing returns; longer class descriptions → better accuracy but more tokens at inference
- **Failure signatures**: Low accuracy from questions not probing right aspects; user drop-off from too many questions; inconsistent classifications from incomplete edge case coverage
- **First 3 experiments**: 1) Test question generation quality with simple binary classification task; 2) Compare accuracy with 1, 3, and 5 questions per example; 3) Measure token savings from summarization vs full examples

## Open Questions the Paper Calls Out

- How would BYOC's accuracy change when applied to tasks with more than two classes? The paper primarily evaluates binary classification, leaving multi-class performance unexplored.
- What is the impact of the number of questions asked per example on final classifier accuracy and user experience? The paper uses fixed numbers but doesn't explore varying interaction depth.
- How does BYOC perform when users provide incorrect or inconsistent answers during interactive training? The paper assumes correct answers but doesn't address robustness to user errors.

## Limitations

- Evaluation relies on simulated user interactions rather than real human feedback, potentially not reflecting actual user behavior
- Method's effectiveness across diverse classification domains remains untested beyond email and academic paper topics
- Token efficiency claims assume specific pricing models and may vary across different LLM APIs

## Confidence

- **High confidence**: The core mechanism of interactive question-answering to refine class descriptions is well-supported by described architecture and evaluation results
- **Medium confidence**: Claims about BYOC being more token-efficient than few-shot examples are supported by reported token counts, though exact cost-benefit depends on specific LLM pricing
- **Medium confidence**: The personalization benefit claim (15% accuracy improvement) is supported by user study results, but simulated interaction methodology introduces uncertainty about real-world performance

## Next Checks

1. Conduct a study with actual users performing the interactive classification task to verify that simulated interactions accurately represent human behavior and that accuracy improvements translate to real-world usage.

2. Evaluate BYOC on 3-5 additional classification tasks from different domains (e.g., medical, legal, social media) to assess method generalization beyond email and academic paper classification.

3. Systematically vary the number of questions per example (1, 3, 5, 10) and training examples per class (1, 3, 5, 10) to identify optimal interaction strategies and understand marginal benefit of additional user effort.