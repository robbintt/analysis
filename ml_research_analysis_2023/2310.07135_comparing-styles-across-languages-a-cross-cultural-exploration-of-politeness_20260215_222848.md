---
ver: rpa2
title: 'Comparing Styles across Languages: A Cross-Cultural Exploration of Politeness'
arxiv_id: '2310.07135'
source_url: https://arxiv.org/abs/2310.07135
tags:
- politeness
- language
- languages
- style
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a framework to extract and compare stylistic
  differences across languages using multilingual language models. The core method
  involves two steps: (1) creating parallel style lexica across languages via embedding-based
  expansion and purification, and (2) aggregating token-level feature importances
  into comparable lexical categories.'
---

# Comparing Styles across Languages: A Cross-Cultural Exploration of Politeness

## Quick Facts
- arXiv ID: 2310.07135
- Source URL: https://arxiv.org/abs/2310.07135
- Reference count: 24
- Presents a framework to compare stylistic differences across languages using multilingual language models, with application to politeness

## Executive Summary
This paper introduces a framework for extracting and comparing stylistic differences across languages, specifically focusing on politeness. The method involves creating parallel style lexica through embedding-based expansion and aggregation of token-level feature importances into comparable lexical categories. The authors apply this framework to create the first holistic multilingual politeness dataset across English, Spanish, Japanese, and Chinese, revealing how politeness is expressed differently across these languages. The approach demonstrates that lexical coverage can be significantly improved (96.9% for Spanish, 90.2% for Japanese) compared to machine translation alone, enabling interpretable cross-cultural comparisons of linguistic style.

## Method Summary
The framework consists of two main steps: (1) Multilingual Lexica Creation (MLC) to generate parallel style lexica across languages using FastText embeddings for synonym and concept expansion, followed by purification through style correlation filtering; and (2) Feature Set Aggregation to consolidate token-level Shapley values into category-level importances using the parallel lexica. The authors fine-tune XLM-RoBERTa models on a holistic politeness dataset of 22,800 Wikipedia Talk Page utterances (5,700 per language) annotated on a 5-point politeness scale, then extract and aggregate feature importances to compare how different lexical categories and dialogue acts contribute to politeness across languages.

## Key Results
- Lexical coverage of expanded lexica reaches 96.9% for Spanish and 90.2% for Japanese, compared to 94.4% and 57.0% using machine translation alone
- Reveals cross-cultural differences: "please" is polite in Japanese but rude in English/Spanish/Chinese; yes/no questions are rude in Chinese but not English
- Creates the first holistic multilingual politeness dataset covering all dialogue acts across the full rude-neutral-polite spectrum

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MLC improves lexical coverage by correcting 1:1 machine translation limitations
- Mechanism: Embedding-based expansion adds culturally appropriate words that capture how style is expressed differently across languages
- Core assumption: Words reflecting a style in one language may not reflect that style the same way when translated to another language
- Evidence anchors: Abstract and section 2.1 describe MLC methodology
- Break condition: If embedding models don't capture semantic similarity across languages well enough

### Mechanism 2
- Claim: Feature Set Aggregation enables cross-lingual comparison by consolidating token-level importances into comparable lexical categories
- Mechanism: Token-level Shapley values are aggregated first into word-level importances, then into category-level importances using parallel lexica
- Core assumption: Feature attribution methods can be meaningfully aggregated across languages when mapped to parallel lexical categories
- Evidence anchors: Abstract and section 2.2 describe the aggregation process
- Break condition: If parallel lexical categories aren't truly comparable across languages

### Mechanism 3
- Claim: Holistic politeness dataset enables discovery of language-specific politeness patterns by including all dialogue acts and the full politeness spectrum
- Mechanism: Dataset includes all conversational utterances across the full rude-neutral-polite spectrum, allowing discovery of language-specific patterns
- Core assumption: Politeness varies across languages in ways that can be captured by analyzing the full distribution of conversational data
- Evidence anchors: Abstract and section 3 describe the dataset creation
- Break condition: If politeness expressions are too context-dependent or culture-specific to be captured by the dataset

## Foundational Learning

- Concept: Cross-lingual feature attribution and comparison
  - Why needed here: To compare how politeness is expressed differently across languages using a common framework
  - Quick check question: Why can't we directly compare token-level feature importances across languages?

- Concept: Embedding-based lexical expansion and purification
  - Why needed here: To create parallel lexical categories that capture how style is expressed in each language
  - Quick check question: How does embedding-based expansion correct for 1:1 machine translation limitations?

- Concept: Dialogue act classification and importance aggregation
  - Why needed here: To compare how linguistic form of politeness differs across languages using dialogue acts as categories
  - Quick check question: Why do we need to classify dialogue acts before comparing politeness across languages?

## Architecture Onboarding

- Component map: MLC -> Feature Set Aggregation -> Parallel Style LMs -> Category-level Importances
- Critical path:
  1. Create parallel lexica using MLC
  2. Fine-tune multilingual LMs on holistic politeness dataset
  3. Extract token-level feature importances from trained LMs
  4. Aggregate importances into category-level using parallel lexica
  5. Compare category-level importances across languages
- Design tradeoffs:
  - FastText vs contextual embeddings for MLC: FastText is faster but less nuanced; contextual embeddings are more accurate but computationally expensive
  - Separate vs joint LM fine-tuning: Separate allows language-specific politeness encoding but may miss cross-lingual patterns
  - Lexical categories vs other aggregation methods: Lexical categories are interpretable but may miss important features not in lexica
- Failure signatures:
  - Low lexical coverage after MLC: Embedding expansion not capturing enough culturally appropriate words
  - Low correlation between category importances and actual politeness: Parallel lexica not capturing true style differences
  - Low LM accuracy: Dataset not representative of politeness variations or LMs not learning style differences
- First 3 experiments:
  1. Test MLC lexical coverage on small seed lexica with known translations
  2. Compare category-level importances from single-language vs multilingual LMs
  3. Validate that parallel lexica capture similar style differences as human annotators

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the feature set aggregation method produce consistent and reliable results across different types of linguistic features (e.g. lexical categories vs. dialogue acts)?
- Basis in paper: [explicit] The paper applies the same feature set aggregation method to both PoliteLex categories and dialogue acts, but does not directly compare the reliability of results across these different types of features.
- Why unresolved: The paper does not provide a quantitative comparison of the consistency and reliability of the feature set aggregation method when applied to different types of linguistic features.
- What evidence would resolve it: A quantitative comparison of the consistency and reliability of the feature set aggregation method when applied to different types of linguistic features would help resolve this question.

### Open Question 2
- Question: How does the cultural context of the data (e.g. Wikipedia Talk Pages) influence the results of the politeness analysis, and are the findings generalizable to other domains?
- Basis in paper: [inferred] The paper acknowledges that the dataset is limited to Wikipedia Talk Pages, which reflects politeness in workplace communication, but does not explore how the cultural context of the data might influence the results or whether the findings are generalizable to other domains.
- Why unresolved: The paper does not provide a quantitative analysis of how the cultural context of the data influences the results or whether the findings are generalizable to other domains.
- What evidence would resolve it: A quantitative analysis of how the cultural context of the data influences the results and whether the findings are generalizable to other domains would help resolve this question.

### Open Question 3
- Question: How do the results of the politeness analysis change when using different pre-trained language models or different feature attribution methods?
- Basis in paper: [inferred] The paper uses XLM-RoBERTa models and SHAP values for feature attribution, but does not explore how the results of the politeness analysis change when using different pre-trained language models or different feature attribution methods.
- Why unresolved: The paper does not provide a quantitative comparison of the results of the politeness analysis when using different pre-trained language models or different feature attribution methods.
- What evidence would resolve it: A quantitative comparison of the results of the politeness analysis when using different pre-trained language models or different feature attribution methods would help resolve this question.

## Limitations
- Cultural nuance capture: Embedding-based expansion may not fully capture subtle cultural differences in politeness expression
- Feature attribution comparability: Limited validation that aggregated importances are truly comparable across languages
- Dataset representativeness: Wikipedia Talk Page corpus may not capture all contexts where politeness is expressed

## Confidence
- High confidence: The core framework architecture (MLC + Feature Set Aggregation) is sound and the methodology is well-specified
- Medium confidence: The claim that this framework enables interpretable comparison of politeness across languages
- Low confidence: The generalizability of the findings to other stylistic features beyond politeness and to languages not included in the study

## Next Checks
1. Calculate the correlation between category-level importances and actual human annotations of politeness across languages
2. Take sentences with known politeness patterns in English, translate them to other languages, and verify that the framework correctly identifies how politeness is expressed differently
3. Compare the quality of parallel lexica created with different expansion methods and different filtering thresholds