---
ver: rpa2
title: 'Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK Approach'
arxiv_id: '2310.06112'
source_url: https://arxiv.org/abs/2310.06112
tags:
- have
- lemma
- polyt
- training
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explains why adversarially trained DNNs lose robustness
  during long-term training. It extends NTK theory to adversarial training and shows
  that a wide DNN can be approximated by a linearized DNN.
---

# Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK Approach

## Quick Facts
- arXiv ID: 2310.06112
- Source URL: https://arxiv.org/abs/2310.06112
- Authors: 
- Reference count: 40
- This paper explains why adversarially trained DNNs lose robustness during long-term training through NTK analysis and proposes the Adv-NTK algorithm.

## Executive Summary
This paper provides a theoretical explanation for robust overfitting in adversarially trained wide DNNs using Neural Tangent Kernel (NTK) theory. The authors extend NTK to adversarial training and prove that wide DNNs can be approximated by linearized counterparts. Through this analysis, they identify a new degeneration phenomenon where the robustness effect of adversarial training gradually fades during long-term training, causing the model to degenerate to one without adversarial training. Based on these insights, they propose Adv-NTK, the first adversarial training algorithm specifically designed for infinite-width DNNs that improves robustness by optimizing the introduced regularization matrix.

## Method Summary
The paper extends NTK theory to adversarial training by proving that wide DNNs can be approximated by linearized DNNs. Under squared loss, they derive closed-form adversarial training dynamics for the linearized network, revealing a fading regularization matrix that causes robust overfitting. Based on this analysis, they propose Adv-NTK, which directly optimizes the diagonal entries of the regularization matrix using a validation set. The algorithm is tested on SVHN and CIFAR-10 datasets using 12,000 training samples each, with 10,000 samples for constructing the infinite-width DNN and 2,000 for validation. Training uses SGD and GradNorm for 50 iterations with batch size 128 and learning rates of 1 (SVHN) or 0.1 (CIFAR-10).

## Key Results
- The paper reveals a new AT degeneration phenomenon where long-term adversarial training causes wide DNNs to degenerate to models without adversarial training
- Adv-NTK algorithm achieves comparable robustness to finite-width DNNs on SVHN and CIFAR-10 datasets
- The fading regularization matrix mechanism explains why adversarial training loses its effectiveness over long training periods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial training (AT) gradually loses its robustness effect due to a fading regularization matrix in the linearized DNN dynamics.
- Mechanism: The paper extends NTK theory to AT, showing that the effect of AT on a DNN is captured by a time-dependent regularization matrix introduced into the closed-form AT dynamics. Over long training, this matrix fades, causing the adversarially trained DNN to degenerate to a model trained without AT.
- Core assumption: The adversarial regularization kernel (ARK) is positive definite, and the perturbation scale is small enough to maintain positive definiteness of the transformed kernel.
- Evidence anchors:
  - [abstract] "reveals a new AT degeneration phenomenon: a long-term AT will result in a wide DNN degenerates to that obtained without AT and thus cause robust overfitting."
  - [section 5.1] "our theory suggests that the effect of AT on a DNN can be characterized by a regularization matrix introduced into the linearized closed-form AT dynamics, which however will gradually fade away in long-term AT."
  - [corpus] Weak evidence; no direct neighbor mentions regularization matrix fading or AT degeneration.
- Break condition: If the ARK is not positive definite, or if perturbation scale is too large, the decomposition and fading analysis may fail.

### Mechanism 2
- Claim: A wide DNN in AT can be approximated by its linearized counterpart, enabling tractable analysis of AT dynamics.
- Mechanism: The paper proves that as network widths approach infinity, the DNN can be approximated by a linearized DNN derived from Taylor expansion. This linearization allows derivation of closed-form AT dynamics for the linearized network.
- Core assumption: Assumptions 1-4 hold, ensuring the convergence of the linearized DNN to the original DNN in the infinite-width limit.
- Evidence anchors:
  - [abstract] "prove that an adversarially trained wide DNN can be well approximated by a linearized DNN."
  - [section 4.3] "Theorem 2 (Equivalence between wide DNN and linearized DNN)" formalizes the approximation.
  - [corpus] No direct neighbor mentions linearization of DNNs in AT; weak evidence.
- Break condition: If the assumptions (e.g., Lipschitz continuity, smoothness) are violated, the approximation may not hold.

### Mechanism 3
- Claim: The Adv-NTK algorithm improves robustness of infinite-width DNNs by directly optimizing the regularization matrix.
- Mechanism: Based on the analysis, Adv-NTK is designed to optimize the diagonal entries of the regularization matrix Ξ(t), which captures the robustness brought by AT. This is done using a validation set to solve a minimax problem.
- Core assumption: The infinite-width limit of the regularization matrix Ξ(t) is diagonal, making it computationally feasible to optimize.
- Evidence anchors:
  - [abstract] "Based on our analysis, we further propose Adv-NTK, the first AT algorithm for infinite-width DNNs which improves network robustness by directly optimizing the introduced regularization matrix."
  - [section 5.2] "we propose Adv-NTK, the first AT algorithm for infinite-width DNNs."
  - [corpus] No direct neighbor mentions Adv-NTK or optimization of regularization matrices; weak evidence.
- Break condition: If the diagonal assumption for Ξ(t) fails in finite-width cases, the optimization may not translate effectively.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) theory
  - Why needed here: NTK provides a framework to analyze the training dynamics of wide neural networks as kernel methods, which is extended to adversarial training in this paper.
  - Quick check question: What does NTK theory say about the training dynamics of wide neural networks?

- Concept: Adversarial training (AT) and robust overfitting
  - Why needed here: The paper analyzes why AT suffers from robust overfitting and proposes a theoretical explanation and mitigation strategy.
  - Quick check question: What is robust overfitting, and why is it a problem in adversarial training?

- Concept: Linearization of neural networks
  - Why needed here: Linearizing the DNN allows for tractable analysis of AT dynamics by reducing the problem to a linear system.
  - Quick check question: How does linearization simplify the analysis of neural network training dynamics?

## Architecture Onboarding

- Component map: NTK extension to AT -> Analysis of AT dynamics and robust overfitting -> Design and implementation of Adv-NTK
- Critical path: Theoretical extension of NTK to AT → Analysis of AT dynamics and robust overfitting → Design and implementation of Adv-NTK
- Design tradeoffs: The tradeoff is between the tractability of the linearized model and the expressiveness of the full DNN. The paper chooses linearization for analysis but acknowledges that finite-width DNNs may capture additional robustness.
- Failure signatures: If the regularization matrix does not fade as predicted, or if the linearized approximation breaks down, the theoretical explanation and Adv-NTK may fail.
- First 3 experiments:
  1. Verify the equivalence between wide DNN and linearized DNN in AT using synthetic data.
  2. Analyze the behavior of the regularization matrix Ξ(t) over training time in AT.
  3. Implement and test Adv-NTK on a small-scale dataset to validate the optimization of the regularization matrix.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Adv-NTK algorithm compare to other existing methods for mitigating robust overfitting beyond the specific baselines tested?
- Basis in paper: [explicit] The paper compares Adv-NTK to AT and NTK baselines but does not compare to other state-of-the-art methods for mitigating robust overfitting.
- Why unresolved: The experimental section focuses on comparing Adv-NTK to a limited set of baselines, leaving open the question of how it fares against other methods in the literature.
- What evidence would resolve it: Conducting experiments comparing Adv-NTK to a wider range of state-of-the-art methods for mitigating robust overfitting would provide evidence to answer this question.

### Open Question 2
- Question: Can the theoretical analysis of robust overfitting in wide DNNs be extended to other loss functions beyond squared loss?
- Basis in paper: [inferred] The paper derives closed-form adversarial training dynamics for linearized DNNs under squared loss, suggesting potential for extension to other loss functions.
- Why unresolved: The analysis in the paper is limited to squared loss, leaving open the question of how it generalizes to other commonly used loss functions.
- What evidence would resolve it: Extending the theoretical analysis to other loss functions and deriving corresponding closed-form dynamics would provide evidence to answer this question.

### Open Question 3
- Question: How does the AT degeneration phenomenon change with different network architectures, depths, and widths?
- Basis in paper: [explicit] The paper analyzes AT degeneration in wide DNNs but does not explore its behavior across different network architectures, depths, and widths.
- Why unresolved: The theoretical analysis focuses on a specific class of networks (MLPs) with varying widths, but does not investigate how AT degeneration varies with different architectures and depths.
- What evidence would resolve it: Conducting theoretical and empirical studies on AT degeneration across a range of network architectures, depths, and widths would provide evidence to answer this question.

## Limitations
- The theoretical analysis is limited to wide DNNs and may not fully capture the behavior of finite-width networks
- The diagonal assumption for the regularization matrix may not hold in practical finite-width scenarios
- The analysis is based on squared loss and may not generalize to other commonly used loss functions

## Confidence
- NTK extension to adversarial training: High
- Fading regularization matrix explanation: Medium  
- Adv-NTK algorithm effectiveness: Medium
- Practical applicability to finite-width networks: Low

## Next Checks
1. Test the positive definiteness of the adversarial regularization kernel across different perturbation scales and network architectures to verify the fading mechanism.
2. Conduct ablation studies on the Adv-NTK algorithm by varying the diagonal assumption for the regularization matrix in finite-width networks.
3. Compare the linearization approximation accuracy against full DNN training dynamics under various initialization schemes and activation functions.