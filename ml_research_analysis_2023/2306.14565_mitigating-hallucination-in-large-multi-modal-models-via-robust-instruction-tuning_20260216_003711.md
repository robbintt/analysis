---
ver: rpa2
title: Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction
  Tuning
arxiv_id: '2306.14565'
source_url: https://arxiv.org/abs/2306.14565
tags:
- width
- height
- image
- instruction
- instructions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses hallucination issues in large multi-modal models
  (LMMs) that generate inconsistent descriptions relative to images and human instructions.
  The authors introduce a large-scale visual instruction dataset with both positive
  and negative instructions across 16 vision-language tasks, including instructions
  that introduce nonexistent elements or manipulate existing elements with incorrect
  attributes.
---

# Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning

## Quick Facts
- **arXiv ID**: 2306.14565
- **Source URL**: https://arxiv.org/abs/2306.14565
- **Reference count**: 40
- **Key outcome**: State-of-the-art hallucination mitigation using balanced positive-negative instruction tuning with less training data than existing methods

## Executive Summary
This work addresses hallucination issues in large multi-modal models (LMMs) that generate inconsistent descriptions relative to images and human instructions. The authors introduce a large-scale visual instruction dataset with both positive and negative instructions across 16 vision-language tasks, including instructions that introduce nonexistent elements or manipulate existing elements with incorrect attributes. To evaluate hallucination robustly, they propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), which scores responses based on accuracy and relevancy without requiring human-annotated groundtruths. Experiments show existing LMMs hallucinate significantly on negative instructions, especially those manipulating existent elements. Finetuning MiniGPT4 on their dataset reduces hallucination while maintaining or improving performance on public benchmarks. Balanced positive-negative training ratios yield more robust models. Their approach achieves state-of-the-art performance with less training data than existing methods.

## Method Summary
The approach involves finetuning MiniGPT4 on a large-scale visual instruction dataset (LRV-Instruction) containing 120k visual instructions across 16 vision-language tasks. The dataset includes both positive instructions (accurate descriptions) and negative instructions (hallucinating or manipulating elements). The model uses Vicuna as the language decoder and Vision Transformer as the image encoder, with only the linear projection layer connecting vision and language components trained. Evaluation uses GPT4-Assisted Visual Instruction Evaluation (GAVIE), which assesses responses based on accuracy and relevancy without requiring groundtruth answers. The training focuses on balancing positive and negative instruction ratios to create more robust models.

## Key Results
- Existing LMMs hallucinate significantly on negative instructions, especially those manipulating existent elements
- Balanced positive-negative training ratios (1:1) yield the most robust models
- The approach achieves state-of-the-art performance with less training data than existing methods
- Finetuning on LRV-Instruction maintains or improves performance on public benchmarks while reducing hallucination

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset balances positive and negative instructions to improve robustness
- Mechanism: By training on both accurate and misleading instructions, models learn to distinguish between correct and hallucinated responses
- Core assumption: The model can learn from negative examples to suppress hallucination tendencies
- Evidence anchors:
  - [abstract] "Unlike existing studies that primarily focus on positive instruction samples, we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning."
  - [section] "We also observe that a balanced ratio of positive and negative instances in the training data leads to a more robust model."
  - [corpus] Weak evidence - only 1 related paper directly addresses balanced instruction design
- Break condition: If the model overfits to negative examples and becomes overly cautious in responses

### Mechanism 2
- Claim: GPT4-assisted evaluation eliminates need for human-annotated groundtruths
- Mechanism: GPT4 compares model responses against image content to score accuracy and relevancy without requiring pre-labeled answers
- Core assumption: GPT4 can reliably evaluate visual instruction tuning like human experts
- Evidence anchors:
  - [abstract] "we propose GPT4-Assisted Visual Instruction Evaluation (GA VIE), a stable approach to evaluate visual instruction tuning like human experts. GAVIE does not require human-annotated groundtruth answers"
  - [section] "Unlike previous evaluation methods [23; 18; 29], GA VIE does not require human-annotated groundtruth answers and can freely adapt to diverse instruction formats."
  - [corpus] Moderate evidence - several papers use GPT4 for evaluation but few for multimodal tasks
- Break condition: If GPT4's evaluation criteria drift from human judgment standards

### Mechanism 3
- Claim: Diverse instruction formats improve instruction-following ability
- Mechanism: Including both declarative and interrogative instructions trains models to handle different linguistic styles
- Core assumption: LMMs built on strong LLMs can understand and follow diverse instruction formats
- Evidence anchors:
  - [abstract] "LRV-Instruction covers 16 vision-language tasks with open-ended instructions and answers"
  - [section] "From Tab 8, LMMs perform with higher scores on interrogative instructions than declarative, but the difference is relatively small."
  - [corpus] Weak evidence - most related work focuses on template-based instructions
- Break condition: If model performance degrades on specialized instruction formats not covered in training

## Foundational Learning

- Concept: Multimodal model architecture and training
  - Why needed here: Understanding how vision encoders and language models are integrated is crucial for modifying training procedures
  - Quick check question: How does the cross-modal alignment network connect the frozen image encoder to the frozen language decoder?

- Concept: Instruction tuning methodology
  - Why needed here: The approach relies on specific instruction tuning techniques rather than standard pretraining
  - Quick check question: What distinguishes visual instruction tuning from general multimodal pretraining?

- Concept: Hallucination detection and evaluation
  - Why needed here: The work introduces novel methods for detecting and measuring hallucination that differ from standard metrics
  - Quick check question: How does GPT4-assisted evaluation differ from traditional hallucination metrics like CHAIR?

## Architecture Onboarding

- Component map: Vision transformer backbone → Q-Former cross-modal alignment → Linear projection → Vicuna language decoder
- Critical path: Image encoding → cross-modal feature extraction → language model prompting → response generation
- Design tradeoffs: Using frozen components limits fine-tuning flexibility but reduces computational cost
- Failure signatures: Repetitive phrases in responses, failure to follow instructions, generating nonexistent objects
- First 3 experiments:
  1. Test model performance on balanced vs unbalanced training data ratios
  2. Evaluate hallucination rates on negative instruction types (nonexistent vs existent manipulation)
  3. Compare GPT4 evaluation scores against human expert ratings for validation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which the balanced ratio of positive to negative instances improves model robustness in visual instruction tuning?
- Basis in paper: Inferred from the observation that a balanced ratio (1:1) yields the best performance in both positive and negative sets during evaluation
- Why unresolved: The paper demonstrates the effectiveness of balanced training data but does not explain the underlying mechanism. The theoretical basis for why this balance specifically improves robustness remains unexplored
- What evidence would resolve it: Controlled experiments varying the positive-to-negative ratio while holding other factors constant, combined with analysis of model attention patterns and feature representations during training

### Open Question 2
- How do the three semantic levels of hallucination (nonexistent objects, existent object attribute manipulation, and knowledge manipulation) differ in their cognitive difficulty for LMMs, and what architectural changes could address each level differently?
- Basis in paper: Explicit finding that "Existent Element Manipulation instructions are more challenging than Nonexistent Element Manipulation instructions for LMMs"
- Why unresolved: The paper identifies this hierarchy but does not investigate why existent object manipulation is harder or what architectural modifications would help models distinguish between these cases
- What evidence would resolve it: Comparative analysis of model confidence scores, attention maps, and error patterns across the three semantic levels, plus ablation studies testing different vision encoder capabilities

### Open Question 3
- What is the relationship between instruction length and hallucination probability, and does this relationship follow a predictable pattern across different types of instructions?
- Basis in paper: Inferred from the finding that "current LMMs achieve better results in short instructions than long ones since longer instructions contain more information, making it more challenging to understand"
- Why unresolved: The paper observes this trend but does not quantify the relationship or investigate whether different instruction types (interrogative vs. declarative) show different length-hallucination patterns
- What evidence would resolve it: Statistical analysis correlating instruction length with hallucination frequency across instruction types, plus testing whether instruction length affects hallucination rates differently for various task categories

### Open Question 4
- How does the quality of dense captions used as visual input affect the hallucination tendency of LMMs, and what is the optimal level of visual detail for instruction tuning?
- Basis in paper: Inferred from the use of Visual Genome's detailed visual information (bounding boxes, dense captions) and the observation that MiniGPT4 uses "longer image captions from ChatGPT" which may contribute to hallucination
- Why unresolved: The paper uses dense captions but does not investigate how caption quality or granularity affects hallucination rates, nor does it explore whether simpler visual representations might reduce hallucination
- What evidence would resolve it: Systematic experiments varying the richness and detail of visual representations (from minimal bounding boxes to dense captions), measuring hallucination rates and model performance across different caption granularities

## Limitations

- Dataset Construction Reliability: The quality and diversity of the generated instructions may be limited by the GPT4-based generation process without detailed validation of instruction quality
- Evaluation Method Generalization: GPT4-assisted evaluation introduces potential biases based on GPT4's interpretation of "accuracy" and "relevancy" without direct comparison to human expert evaluations
- Model Architecture Constraints: The approach relies on finetuning with frozen components, limiting the model's ability to learn more complex cross-modal representations

## Confidence

**High Confidence Claims**:
- LMMs exhibit significant hallucination on negative instructions, particularly those manipulating existent elements
- Balanced positive-negative training ratios lead to more robust models
- The proposed approach achieves state-of-the-art performance with less training data

**Medium Confidence Claims**:
- GPT4 can reliably evaluate visual instruction tuning like human experts
- The dataset design effectively mitigates hallucination through instruction diversity
- Finetuning on LRV-Instruction maintains or improves performance on public benchmarks

**Low Confidence Claims**:
- The specific negative instruction categories (nonexistent vs existent manipulation) represent the most important sources of hallucination
- The linear projection layer architecture is optimal for hallucination mitigation
- GAVIE evaluation scores are universally applicable across different LMM architectures

## Next Checks

1. **Human Evaluation Validation**: Conduct a human expert evaluation study comparing GAVIE scores against human judgments on the same instruction sets to quantify the correlation and identify potential evaluation biases.

2. **Cross-Architecture Testing**: Test the LRV-Instruction dataset and GAVIE evaluation method on different LMM architectures (not just MiniGPT4) to verify the approach's generalizability across various model designs.

3. **Long-term Stability Analysis**: Evaluate model performance over extended time periods and across different domains to assess whether hallucination mitigation persists and whether the model maintains balanced performance on both positive and negative instructions.