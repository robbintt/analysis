---
ver: rpa2
title: Task Arithmetic with LoRA for Continual Learning
arxiv_id: '2311.02428'
source_url: https://arxiv.org/abs/2311.02428
tags:
- task
- learning
- continual
- tasks
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses continual learning for vision transformers,
  focusing on the problem of catastrophic forgetting when training on sequential tasks.
  The core method combines low-rank adaptation (LoRA) with task arithmetic: each task
  is trained using LoRA-augmented ViT weights, task vectors are computed as differences
  from pretrained weights and combined, and the resulting task-agnostic model is fine-tuned
  on a small memory set.'
---

# Task Arithmetic with LoRA for Continual Learning

## Quick Facts
- arXiv ID: 2311.02428
- Source URL: https://arxiv.org/abs/2311.02428
- Authors: 
- Reference count: 35
- Key outcome: Combines LoRA with task arithmetic to achieve near-offline performance on sequential vision tasks while reducing computational cost by 3-5x compared to experience replay

## Executive Summary
This paper addresses catastrophic forgetting in vision transformers for continual learning by combining low-rank adaptation (LoRA) with task arithmetic. The method trains separate LoRA-augmented models for each task, computes task vectors as differences from pretrained weights, and combines them to create a task-agnostic model. A small memory buffer is used for final fine-tuning. Experiments on Oxford-IIIT Pets, Flowers-102, and CIFAR10 show the approach outperforms baselines like AGEM and experience replay while achieving 3-5x fewer FLOPs.

## Method Summary
The method involves fine-tuning LoRA-augmented ViT models on each task separately, computing task vectors as differences between task-specific and pretrained weights, combining these vectors with equal scaling (λ=0.25), and merging them back to pretrained weights. The resulting task-agnostic model is then fine-tuned on a small memory buffer (10 samples per class). LoRA enables efficient fine-tuning by updating only low-rank adaptation matrices while freezing the base model. KL-divergence regularization is optionally applied to keep LoRA-adapted features close to pretrained features.

## Key Results
- Achieves near-offline performance on sequential vision tasks
- Reduces computational cost by 3-5x compared to experience replay
- Outperforms continual learning baselines like AGEM and experience replay
- Particularly effective when aided with a small memory of 10 samples per class

## Why This Works (Mechanism)

### Mechanism 1
Task arithmetic works because the difference vectors between fine-tuned and pretrained weights capture task-specific directions in the parameter space. After fine-tuning LoRA weights for each task, the difference between the fine-tuned and pretrained weights forms a "task vector." Summing these task vectors and adding them back to the pretrained weights creates a model that generalizes across all tasks without forgetting. This relies on the assumption that weight space is approximately linear in the directions relevant to task adaptation, so vector arithmetic preserves semantic meaning.

### Mechanism 2
LoRA enables efficient continual learning by freezing base model weights and updating only low-rank adaptation matrices. By updating only the A and B matrices in LoRA (K << M), the method drastically reduces computational cost while preserving most of the pretrained knowledge in the frozen base model. This assumes that low-rank decomposition captures sufficient task-specific adaptation without needing full fine-tuning.

### Mechanism 3
KL-divergence regularization keeps LoRA-adapted features close to pretrained features, reducing task vector variance. By adding a KL-divergence loss between the softmax distributions of pretrained and fine-tuned backbones, the LoRA updates stay closer to the original feature space, making task vectors more consistent across tasks. This assumes maintaining feature distribution similarity prevents LoRA from drifting too far from the pretrained representation, improving arithmetic stability.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The paper explicitly addresses catastrophic forgetting as the core problem being solved.
  - Quick check question: What happens to a model's performance on task A when it's sequentially trained on task B without any mitigation?

- Concept: Low-rank matrix decomposition
  - Why needed here: LoRA relies on representing weight updates as products of low-rank matrices (A and B where K << M).
  - Quick check question: If M=768 and K=16, how many parameters are in the LoRA update matrices versus the full weight matrix?

- Concept: Vector arithmetic in parameter space
  - Why needed here: Task arithmetic assumes that differences between fine-tuned and pretrained weights can be meaningfully combined.
  - Quick check question: What mathematical operation combines task vectors to create the task-agnostic model?

## Architecture Onboarding

- Component map: Pretrained ViT backbone -> LoRA adaptation matrices -> Task-specific fine-tuning pipeline -> Task vector computation -> Task vector combination -> Memory buffer fine-tuning

- Critical path: 1) Load pretrained ViT and augment with LoRA 2) For each task: fine-tune only LoRA weights with crossentropy loss 3) Compute task vectors: τi = θi - θpre 4) Combine task vectors: τ = Σ λτi 5) Create task-agnostic model: θfinal = θpre + τ 6) Fine-tune on memory buffer with crossentropy loss

- Design tradeoffs: LoRA rank (K) vs. adaptation capacity: Higher K allows more adaptation but increases computation; Memory buffer size vs. performance: More samples improve fine-tuning but reduce continual learning benefits; KL regularization strength vs. flexibility: Stronger regularization maintains stability but may limit task-specific learning

- Failure signatures: Performance degradation on early tasks indicates catastrophic forgetting; Low accuracy on individual tasks suggests LoRA rank is insufficient; Inconsistent task vectors across runs suggest instability in task arithmetic; Memory buffer fine-tuning doesn't improve performance suggests poor task vector combination

- First 3 experiments: 1) Single task LoRA fine-tuning: Fine-tune LoRA on one task and verify it works better than full fine-tuning 2) Two task arithmetic: Train LoRA on two tasks separately, combine task vectors, and test on both tasks 3) Memory fine-tuning ablation: Compare performance with and without the final memory buffer fine-tuning step

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the task arithmetic approach scale with the number of tasks and classes in the dataset? The paper experiments on datasets with varying numbers of tasks and classes but does not systematically analyze the impact of task and class count on performance. It only presents results on a limited set of datasets without comprehensive analysis.

### Open Question 2
What is the impact of the choice of memory size on the performance of the proposed method? The paper mentions using a small memory of 10 samples per class, but does not explore the impact of different memory sizes on performance. It only presents results using a fixed memory size of 10 samples per class.

### Open Question 3
How does the proposed method compare to other state-of-the-art continual learning approaches, such as regularization-based methods (e.g., EWC, MAS) or generative replay methods? The paper compares the proposed method to AGEM and Experience Replay, but does not include other popular continual learning approaches like EWC, MAS, or generative replay methods.

## Limitations
- Results rely heavily on the assumption that weight space is approximately linear for task-relevant directions, but this is not empirically validated
- KL-divergence regularization shows inconsistent benefits across datasets, suggesting the mechanism may be task-dependent rather than universal
- The mechanism by which task arithmetic preserves semantic meaning across different tasks remains largely theoretical

## Confidence

- **High confidence**: LoRA efficiency claims (well-established method with clear computational benefits)
- **Medium confidence**: Catastrophic forgetting mitigation (results show improvement but comparisons could be more comprehensive)
- **Low confidence**: Task arithmetic mechanism (theoretical justification is weak, empirical validation limited)

## Next Checks

1. **Ablation study on task arithmetic linearity**: Systematically test task vector combinations on synthetic tasks where the ground truth linear relationship is known, to validate whether the weight space behaves linearly in task-relevant directions.

2. **Robustness to task ordering**: Evaluate the method across multiple random task orderings to determine if performance is sensitive to the sequence in which tasks are presented, which would indicate limitations in the task arithmetic approach.

3. **Scaling analysis of memory buffer size**: Conduct experiments varying memory buffer sizes (0, 5, 10, 20 samples per class) to quantify the tradeoff between computational efficiency and performance, particularly for datasets where KL regularization showed inconsistent benefits.