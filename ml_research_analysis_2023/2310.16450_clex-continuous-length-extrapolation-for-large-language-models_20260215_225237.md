---
ver: rpa2
title: 'CLEX: Continuous Length Extrapolation for Large Language Models'
arxiv_id: '2310.16450'
source_url: https://arxiv.org/abs/2310.16450
tags:
- length
- clex
- scaling
- context
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CLEX, a method for continuous length extrapolation
  in large language models. The key idea is to model the dynamics of position embedding
  scaling as a continuous process using neural ODEs, allowing models to generalize
  beyond their training sequence length.
---

# CLEX: Continuous Length Extrapolation for Large Language Models

## Quick Facts
- arXiv ID: 2310.16450
- Source URL: https://arxiv.org/abs/2310.16450
- Authors: [Not specified]
- Reference count: 22
- Key outcome: CLEX extends context windows to over 4x training length without performance degradation

## Executive Summary
CLEX introduces a novel approach for continuous length extrapolation in large language models by modeling position embedding scaling dynamics as a continuous process using neural ODEs. The method enables models to generalize beyond their training sequence length, achieving similar performance on sequences up to 4x longer than trained on. Experimental results demonstrate that LLaMA-2-7B trained on 16k context length performs comparably on both 16k and 64k sequences, outperforming baseline methods in long-context tasks.

## Method Summary
CLEX builds upon Rotary Position Embedding (RoPE) by introducing continuous dynamics modeling through neural ODEs to scale position embeddings. The method incorporates position extrapolation during training, sampling position indices from ranges larger than the training sequence length to reconcile frequency basis and position index inconsistencies. During inference, the model caches frequency basis for discrete scaling factors and selects the nearest upper bound for the sequence length. The approach enables training on shorter sequences while achieving strong performance on much longer sequences during inference.

## Key Results
- LLaMA-2-7B trained on 16k context length achieves similar perplexity on both 16k and 64k sequences
- CLEX extends context windows to over 4x the training length without performance degradation
- Competes effectively against state-of-the-art models on LongBench benchmark tasks, despite being trained on much shorter sequences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLEX overcomes the inherent trade-off in PE scaling methods between length extrapolation and maintaining performance within the original context window.
- Mechanism: CLEX models the continuous dynamics of frequency basis scaling as a neural ODE, allowing for fine-grained adaptation across all sequence lengths rather than relying on discrete scaling factors.
- Core assumption: The transition of frequency basis during position embedding scaling can be effectively modeled as a continuous dynamical system.
- Evidence anchors: [abstract] "We generalise the PE scaling approaches to model the continuous dynamics by ordinary differential equations over the length scaling factor" [section 3.2] "we utilise a variable function g to model the dynamics... parameterise the function using the neural network ϕ"

### Mechanism 2
- Claim: CLEX enables training on short sequences while achieving strong performance on much longer sequences during inference.
- Mechanism: By extending the continuous dynamics beyond the training sequence length during training, CLEX learns to extrapolate to longer contexts without explicit fine-tuning on those lengths.
- Core assumption: Learning continuous dynamics during training on short sequences provides sufficient information to extrapolate to longer sequences
- Evidence anchors: [abstract] "CLEX facilitates the length extrapolation with impressive performance in practical tasks" [section 3.3] "by extending the continuous dynamics beyond the scaling factor corresponding to training length, CLEX empowers models to progressively extrapolate"

### Mechanism 3
- Claim: The position extrapolation strategy resolves inconsistency between frequency basis and position indices during training.
- Mechanism: During training, CLEX samples position indices from a range larger than the training sequence length, ensuring consistency between the frequency basis and position indices.
- Core assumption: The inconsistency between frequency basis and position indices during training is a significant source of performance degradation
- Evidence anchors: [section 3.3] "we propose the position extrapolation strategy to address this consistency... enlarge the position indices {1, 2, . . . , LTrain} of the trained sequences up to the range[1, t′·L]" [section 4.2] "We adopt the position extrapolation strategy... to reconcile the inconsistency between frequency basis and position indices"

## Foundational Learning

- Concept: Rotary Position Embedding (RoPE)
  - Why needed here: CLEX builds upon RoPE as its foundation, modifying how position embeddings scale with sequence length
  - Quick check question: How does RoPE incorporate both absolute and relative positional information in the attention mechanism?

- Concept: Ordinary Differential Equations (ODEs) and Neural ODEs
  - Why needed here: CLEX uses neural ODEs to model the continuous dynamics of frequency basis scaling
  - Quick check question: What is the key difference between traditional ODEs and neural ODEs in terms of parameterization?

- Concept: Transformer self-attention mechanism
  - Why needed here: Understanding how position embeddings interact with the attention mechanism is crucial for implementing CLEX
  - Quick check question: How do position embeddings affect the attention scores between query and key vectors?

## Architecture Onboarding

- Component map:
  Base LLM (e.g., LLaMA-2) with RoPE -> CLEX layer (neural ODE with up-and-down projection) -> Position extrapolation sampling module -> Frequency basis caching system for inference

- Critical path:
  1. During training: Sample scaling factor t, apply position extrapolation, compute continuous dynamics via neural ODE
  2. During inference: Cache frequency basis for discrete t values, select nearest upper bound for sequence length
  3. Apply modified frequency basis to RoPE for attention computation

- Design tradeoffs:
  - Larger λ (amplification factor) increases ODE expressiveness but adds parameters and computation
  - Random vs. uniform position extrapolation sampling: random slightly better but more complex
  - Caching vs. on-the-fly computation of frequency basis: caching reduces latency but may limit flexibility

- Failure signatures:
  - Performance degradation on original context lengths: ODE overfitting to training lengths
  - Poor extrapolation beyond training lengths: insufficient modeling of continuous dynamics
  - Increased inference latency: inefficient caching or computation of frequency basis

- First 3 experiments:
  1. Ablation: Compare CLEX with discrete scaling baseline (same training procedure but fixed t)
  2. Position extrapolation: Compare random, uniform, and no sampling strategies
  3. Parameter sensitivity: Test different λ values (1, 2, 4) to find optimal tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CLEX scale with context lengths beyond 64k tokens, and what are the practical limits of its extrapolation ability?
- Basis in paper: [explicit] The paper demonstrates that CLEX can extend context windows to over 4x or almost 8x the training length, with no performance deterioration. However, it only evaluates up to 64k context length.
- Why unresolved: The paper does not explore the upper limits of CLEX's extrapolation ability beyond 64k tokens, which would be crucial for understanding its practical applicability to extremely long contexts.
- What evidence would resolve it: Conducting experiments with CLEX on context lengths significantly larger than 64k, such as 128k or 256k tokens, and comparing its performance to other methods would provide insights into its scalability and practical limits.

### Open Question 2
- Question: What is the impact of varying the amplification factor λ in the neural ODE on CLEX's performance, and is there an optimal value for different base model sizes?
- Basis in paper: [explicit] The paper mentions that the parameter size of the neural ODE is determined by the amplification factor λ, but it only explores a limited range (λ = 1, 2, 4) and does not investigate the optimal value for different base model sizes.
- Why unresolved: The paper does not provide a comprehensive analysis of how varying λ affects CLEX's performance across different base model sizes, leaving uncertainty about the optimal configuration for maximizing its effectiveness.
- What evidence would resolve it: Conducting experiments with a wider range of λ values for different base model sizes (e.g., 7B, 13B, 70B parameters) and analyzing the impact on performance metrics such as perplexity and accuracy would help determine the optimal λ for each model size.

### Open Question 3
- Question: How does CLEX perform on tasks that require very long-range dependencies, such as document-level machine translation or multi-hop reasoning, and how does it compare to other methods in these scenarios?
- Basis in paper: [inferred] The paper evaluates CLEX on the LongBench benchmark, which includes tasks like question answering and summarization, but it does not specifically focus on tasks that require very long-range dependencies.
- Why unresolved: The paper does not explore CLEX's performance on tasks that heavily rely on long-range dependencies, which would be crucial for understanding its applicability to complex real-world scenarios.
- What evidence would resolve it: Conducting experiments with CLEX on tasks that require very long-range dependencies, such as document-level machine translation or multi-hop reasoning, and comparing its performance to other methods in these scenarios would provide insights into its effectiveness in handling complex dependencies.

## Limitations

- Empirical scope is constrained by limited ablation studies and hyperparameter sensitivity analysis
- Experiments focus primarily on LLaMA-2-7B architecture, leaving unclear if CLEX generalizes to other transformer variants
- Training data (2B tokens from Redpajama-Book) is relatively small for LLMs, potentially limiting robustness of findings

## Confidence

- **High Confidence**: The core claim that continuous modeling of position embedding scaling dynamics can enable length extrapolation is well-supported by experimental results showing consistent perplexity across training and extrapolated lengths
- **Medium Confidence**: Claims about CLEX's competitive performance on LongBench benchmark tasks are supported but could be stronger with more extensive comparisons to alternative length extrapolation methods
- **Low Confidence**: The claim that position extrapolation is necessary for reconciling frequency basis and position index inconsistencies lacks strong empirical support

## Next Checks

1. **Ablation study of position extrapolation**: Systematically test random, uniform, and no position extrapolation strategies across different sequence lengths to quantify the contribution of this component to overall performance.

2. **Hyperparameter sensitivity analysis**: Evaluate CLEX's performance across a range of λ values (e.g., 1, 2, 4) and different neural ODE architectures to understand the robustness and optimal configuration of the method.

3. **Cross-architecture generalization**: Implement CLEX on alternative transformer architectures (e.g., GPT-NeoX, OPT) and larger model sizes (e.g., 13B, 33B parameters) to assess whether the continuous dynamics modeling generalizes beyond LLaMA-2-7B.