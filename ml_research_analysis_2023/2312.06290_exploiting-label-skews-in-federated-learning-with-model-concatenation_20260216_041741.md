---
ver: rpa2
title: Exploiting Label Skews in Federated Learning with Model Concatenation
arxiv_id: '2312.06290'
source_url: https://arxiv.org/abs/2312.06290
tags:
- fedconcat
- label
- fedconcat-id
- rounds
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of label skews in federated
  learning, where clients have non-IID data distributions. The authors propose FedConcat,
  a novel approach that concatenates local models instead of averaging them to aggregate
  knowledge effectively.
---

# Exploiting Label Skews in Federated Learning with Model Concatenation

## Quick Facts
- arXiv ID: 2312.06290
- Source URL: https://arxiv.org/abs/2312.06290
- Reference count: 40
- Key outcome: FedConcat achieves higher accuracy than state-of-the-art FL methods on non-IID data by concatenating local models, with lower communication costs

## Executive Summary
This paper addresses the challenge of label skews in federated learning, where clients have non-IID data distributions. The authors propose FedConcat, a novel approach that concatenates local models instead of averaging them to aggregate knowledge effectively. FedConcat first clusters clients based on their label distributions, then trains a model for each cluster using FedAvg, and finally concatenates the encoders of these models to train a global classifier. The method is theoretically justified using information bottleneck theory and shows significant improvements in accuracy over state-of-the-art FL methods across various datasets and label skew settings. Additionally, FedConcat achieves lower communication costs compared to baselines.

## Method Summary
FedConcat addresses label skew in federated learning by clustering clients based on their label distributions, training models within each cluster using FedAvg, and concatenating the resulting encoders to form a global model. The method consists of three stages: clustering clients (either using true label distributions or inferring them from models), training local models within each cluster, and post-training a global classifier on top of the concatenated encoders. This approach preserves more mutual information about labels than averaging under label skew conditions, as theoretically justified by information bottleneck analysis. The method is designed to be computationally efficient by fixing encoder parameters during classifier training.

## Key Results
- FedConcat outperforms FedAvg and other baselines on CIFAR-10, SVHN, FMNIST, CIFAR-100, and Tiny-ImageNet with various label skew settings
- The method achieves lower communication costs by reducing model size through clustering and using fewer training rounds
- FedConcat shows significant accuracy improvements, particularly under extreme label skew conditions where FedAvg performance degrades substantially

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concatenating encoders preserves more mutual information about labels than averaging under label skew.
- Mechanism: In label skew, each client's model learns well on its own subset of classes. Averaging these models produces a global model far from all local optima. Concatenating encoders captures the unique, task-specific features from each cluster, leading to richer feature representation.
- Core assumption: When label distributions differ greatly across clients, the mutual information between local encoder outputs and global labels is higher than for a globally averaged encoder.
- Evidence anchors:
  - [abstract]: "We theoretically analyze the advantage of concatenation over averaging by analyzing the information bottleneck of deep neural networks."
  - [section 4.1]: Theoretical analysis using information bottleneck theory to show I(favg(X); Y) < I(fe(X); Y).
  - [corpus]: No direct corpus evidence; claims are theoretical.
- Break condition: If label distributions are identical across clients, concatenation offers no advantage over averaging.

### Mechanism 2
- Claim: Clustering clients by label distribution before concatenation reduces model size and improves training stability.
- Mechanism: Clients with similar label distributions are grouped into clusters. Each cluster trains a model using FedAvg, which is more effective for slightly skewed data. Concatenating fewer, well-trained models reduces global model size and avoids the instability of concatenating all client models.
- Core assumption: Label distributions among clients are not uniformly random; some clients have similar distributions, making clustering effective.
- Evidence anchors:
  - [abstract]: "To reduce the size of the global model, we adopt the clustering technique to group the clients by their label distributions..."
  - [section 3.3]: Explains clustering stage and its benefits for model size and stability.
  - [corpus]: No direct corpus evidence; claims are based on experimental results.
- Break condition: If all clients have completely unique label distributions, clustering provides no benefit and may hurt performance.

### Mechanism 3
- Claim: Post-training a global classifier on top of concatenated encoders is computationally efficient.
- Mechanism: After concatenating encoders, their parameters are fixed. Training a global classifier only requires updating the classifier layer, which is much smaller and faster than updating the entire model.
- Core assumption: The concatenated encoders already extract good features, so a simple classifier can effectively map these to labels.
- Evidence anchors:
  - [section 3.3]: "Since the encoder training is stopped, we can calculate the features of raw data in a forward pass for only one time."
  - [section 5.2]: Mentions lower communication and computation costs compared to baselines.
  - [corpus]: No direct corpus evidence; claims are based on experimental results.
- Break condition: If concatenated encoders do not extract useful features, training an effective classifier becomes difficult.

## Foundational Learning

- Concept: Information Bottleneck Theory
  - Why needed here: Used to theoretically justify why concatenation preserves more mutual information about labels than averaging.
  - Quick check question: What is the key trade-off in the information bottleneck theory, and how does it relate to feature extraction in neural networks?

- Concept: Clustering Techniques
  - Why needed here: Used to group clients with similar label distributions, reducing model size and improving training stability.
  - Quick check question: What are the key considerations when choosing a clustering algorithm for federated learning with label skew?

- Concept: Differential Privacy
  - Why needed here: Used to protect label distribution information when clients cannot share it directly.
  - Quick check question: How does differential privacy ensure that the output of a function does not significantly change when a single data point is modified?

## Architecture Onboarding

- Component map:
  Clustering Stage -> Averaging Stage -> Post-training Stage

- Critical path:
  1. Clustering (either with true label distribution or inferred).
  2. Training models within each cluster using FedAvg.
  3. Concatenating encoders and training a global classifier.

- Design tradeoffs:
  - Number of clusters (K): Affects model size and training stability.
  - Method of clustering: Direct label distribution vs. inferred from models.
  - Encoder training rounds vs. classifier training rounds: Balances communication cost and accuracy.

- Failure signatures:
  - High variance in cluster sizes: Indicates unbalanced label distributions.
  - Poor classifier accuracy: Suggests concatenated encoders do not extract useful features.
  - Slow convergence: May indicate too few encoder training rounds or poor clustering.

- First 3 experiments:
  1. Test clustering with true label distribution on a simple dataset (e.g., CIFAR-10 with #C=2 partition).
  2. Evaluate the effectiveness of label inference from models on a small-scale dataset.
  3. Compare the accuracy of FedConcat with different numbers of clusters (e.g., K=2, K=5, K=10) on CIFAR-10.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical bounds on how much information concatenation preserves compared to averaging under different label skew distributions?
- Basis in paper: [explicit] The paper theoretically analyzes the advantage of concatenation over averaging using information bottleneck theory, showing that concatenation preserves more mutual information about labels.
- Why unresolved: The paper provides a theoretical comparison but doesn't establish specific quantitative bounds or limits on the information preservation advantage.
- What evidence would resolve it: Formal mathematical proofs establishing upper and lower bounds on the mutual information difference between concatenation and averaging approaches under various label skew scenarios.

### Open Question 2
- Question: How does FedConcat perform when combining feature extractors from clients with completely disjoint label sets (no overlapping classes)?
- Basis in paper: [inferred] The paper mentions that under extreme label skews where each client has quite different classes, averaging leads to significant accuracy degradation, implying this scenario is relevant.
- Why unresolved: The experiments focus on label skew scenarios where there is some overlap in classes between clients, not complete disjointness.
- What evidence would resolve it: Experiments testing FedConcat's performance when each client has a completely unique subset of classes with no overlap between clients.

### Open Question 3
- Question: What is the optimal number of clusters K for different levels of label skew and dataset sizes?
- Basis in paper: [explicit] The paper discusses that clustering helps control model size and mentions using elbow method to select K, but doesn't provide systematic analysis of optimal K values.
- Why unresolved: The paper experiments with various K values but doesn't establish a clear methodology or rules for determining optimal K across different scenarios.
- What evidence would resolve it: Comprehensive experiments and analysis establishing guidelines for choosing K based on dataset characteristics, label skew levels, and number of clients.

## Limitations
- The theoretical analysis, while providing intuition, lacks formal proofs for all assumptions, particularly around mutual information bounds under label skew conditions.
- Experimental details for label inference in FedConcat-ID are underspecified, making exact reproduction challenging.
- The communication and computation cost advantages are primarily theoretical, with limited empirical validation across different system configurations.

## Confidence
- High Confidence: The empirical results demonstrating FedConcat's accuracy improvements over FedAvg and other baselines across multiple datasets with various label skew settings.
- Medium Confidence: The theoretical justification using information bottleneck theory, which provides intuition but could benefit from more rigorous treatment.
- Medium Confidence: The communication and computation cost advantages, which are claimed but not comprehensively empirically validated.

## Next Checks
1. **Robustness to Clustering Method**: Conduct experiments comparing FedConcat's performance when using true label distributions versus inferred label distributions from models. Measure the clustering quality (e.g., silhouette score) and downstream accuracy to quantify the impact of imperfect label distribution information on overall performance.

2. **Scalability Analysis**: Evaluate FedConcat's performance and resource usage as the number of clients increases from 40 to 100, 200, and 500 clients. Measure communication costs, training time, and accuracy degradation to understand the method's scalability limits and identify potential bottlenecks in the clustering and concatenation stages.

3. **Theoretical Validation**: Design experiments to empirically validate the information bottleneck claims by measuring the mutual information between encoder outputs and labels for both averaged and concatenated models. Use techniques like MINE (Mutual Information Neural Estimation) to estimate I(f(X); Y) and compare the results with the theoretical predictions to assess the validity of the information bottleneck analysis.