---
ver: rpa2
title: Compressing the Backward Pass of Large-Scale Neural Architectures by Structured
  Activation Pruning
arxiv_id: '2311.16883'
source_url: https://arxiv.org/abs/2311.16883
tags:
- sparsity
- memory
- sparse
- block
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We address memory limitations in large-scale neural architecture
  training by pruning activations during the backward pass. Using structured pruning
  in Block Sparse Compressed Row (BSR) format, we remove low-magnitude activation
  blocks to reduce memory consumption.
---

# Compressing the Backward Pass of Large-Scale Neural Architectures by Structured Activation Pruning

## Quick Facts
- arXiv ID: 2311.16883
- Source URL: https://arxiv.org/abs/2311.16883
- Reference count: 23
- Primary result: Achieves up to 32% memory reduction on ResMLP-S12 with ImageNet using structured activation pruning during backward pass.

## Executive Summary
This paper addresses memory limitations in large-scale neural network training by pruning activations during the backward pass using structured block sparsity. The approach uses Block Sparse Compressed Row (BSR) format to store low-magnitude activation blocks after the dense forward pass, reducing memory consumption while maintaining training accuracy. Custom GPU kernels for block-sparse matrix multiplication enable efficient backward gradient computation. The method demonstrates up to 32% memory reduction on ResMLP-S12 with ImageNet, offering a scalable solution to democratize large-scale model training and reduce ecological impact.

## Method Summary
The method performs dense forward computation followed by structured pruning of activations using magnitude-based top-k selection. After the forward pass, the smallest activation blocks are zeroed and stored in BSR format, which compresses sparse matrices by grouping values into blocks. During the backward pass, custom BSpMM kernels compute gradients using only non-zero blocks, avoiding memory reads for pruned activations. The approach is implemented as a drop-in replacement for dense linear layers and is evaluated on ResMLP-S12 across CIFAR10/100 and ImageNet datasets.

## Key Results
- Achieves up to 32% memory reduction on ResMLP-S12 with ImageNet
- Maintains training accuracy while reducing memory footprint
- Demonstrates effectiveness of structured pruning vs unstructured approaches
- Shows scalability for large-scale model training democratization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured pruning of activation blocks in BSR format reduces memory footprint during backward pass while maintaining model accuracy.
- Mechanism: Activations are pruned after the dense forward pass using a magnitude-based top-k criterion. Low-magnitude activation blocks are zeroed and stored in BSR format, which compresses sparse matrices by grouping values into blocks and encoding only non-zero blocks with associated metadata. During backward pass, custom BSpMM kernels operate only on non-zero blocks, avoiding memory reads for pruned activations.
- Core assumption: Pruned activations have low impact on gradient accuracy because they contribute minimally to the loss.
- Evidence anchors:
  - [abstract]: "Using structured pruning in Block Sparse Compressed Row (BSR) format, we remove low-magnitude activation blocks to reduce memory consumption."
  - [section III]: "After the dense forward computation is completed, the smallest blocks of input activations are selected using a magnitude based criterion... The selected blocks are forced to zero and the activation tensor is converted to the BSR format."
  - [corpus]: Weak - neighbor papers focus on different pruning strategies and do not directly address structured activation pruning in BSR format.
- Break condition: If magnitude-based pruning removes activations that are important for accurate gradient computation, training accuracy will degrade or diverge.

### Mechanism 2
- Claim: BSR format with larger block sizes offers better compression ratio while maintaining GPU efficiency.
- Mechanism: BSR encodes blocks instead of individual elements, reducing the number of indices needed to represent sparsity. Larger blocks mean fewer indices, which improves compression ratio. The block structure aligns with GPU warp execution, enabling coalesced memory access and reducing divergence when blocks are non-zero.
- Core assumption: GPU memory bandwidth and arithmetic throughput are better utilized when accessing contiguous blocks rather than scattered individual elements.
- Evidence anchors:
  - [section II]: "The larger the BSR block size, the fewer indices are required, improving compression... block-sparse formats complement the warp execution paradigm of GPUs very well."
  - [section III]: "Because of the limit of 1024 threads per CUDA thread block it becomes necessary to split the logical cooperative groups among multiple thread blocks... increasing sparsity decreases latency."
  - [corpus]: Weak - neighbors discuss sparsity but not specifically BSR block size effects on compression or GPU execution.
- Break condition: If block size is too large relative to activation patterns, memory savings diminish and divergence increases due to uneven block occupancy.

### Mechanism 3
- Claim: Dense forward computation followed by sparse backward computation preserves training accuracy while enabling memory savings.
- Mechanism: During forward pass, all activations are computed densely to ensure accurate loss calculation. Only after forward completion are activations pruned and stored sparsely. Backward pass computes gradients using the sparse activation representation via BSpMM, reducing memory reads and writes for zero blocks.
- Core assumption: Gradients computed from pruned activations approximate those from full activations closely enough to maintain convergence.
- Evidence anchors:
  - [section III]: "As mentioned in section I the forward computation is kept dense to ensure an accurate loss computation."
  - [section V]: "Because of the dense forward computation an accurate loss value is obtained. However, because gradient calculation is lossy, parameter updates become less accurate, affecting the rate of convergence."
  - [corpus]: Weak - neighbor papers discuss activation compression but not this specific dense-sparse hybrid approach.
- Break condition: If pruning removes too much information, gradient updates become inaccurate, causing training instability or accuracy loss.

## Foundational Learning

- Concept: Block Sparse Compressed Row (BSR) format and sparse matrix operations
  - Why needed here: BSR is the core data structure enabling memory-efficient storage and computation of pruned activations. Understanding its encoding and decoding is essential to implement and tune the pruning pipeline.
  - Quick check question: How does BSR encode a sparse matrix and what is the trade-off between block size and compression ratio?

- Concept: GPU cooperative groups and warp execution model
  - Why needed here: Custom BSpMM kernels rely on cooperative groups to process blocks in parallel. Knowledge of warp size, divergence, and shared memory usage is critical to optimize kernel performance.
  - Quick check question: Why does block size affect warp divergence in the BSpMM kernel and how does it impact throughput?

- Concept: Magnitude-based top-k pruning and its effect on gradients
  - Why needed here: Pruning criterion determines which activations are removed. Understanding how magnitude relates to gradient importance helps tune sparsity levels without harming accuracy.
  - Quick check question: What happens to gradient accuracy if activations with small magnitude but high influence on the loss are pruned?

## Architecture Onboarding

- Component map: Dense forward linear layers -> Post-forward pruning module -> BSR activation encoder/decoder -> Custom BSpMM backward kernel -> Memory allocator override

- Critical path:
  1. Forward pass: dense computation → store full activations
  2. Pruning: select lowest-magnitude blocks → zero them → encode to BSR
  3. Backward pass: load BSR activations → compute weight gradients via BSpMM → update parameters

- Design tradeoffs:
  - Block size vs. compression: larger blocks reduce encoding overhead but risk losing important activations.
  - Sparsity level vs. accuracy: higher sparsity saves memory but may degrade gradients.
  - Kernel complexity vs. portability: custom CUDA kernels maximize performance but reduce portability vs. Triton or PyTorch extensions.

- Failure signatures:
  - Accuracy drop: pruning removes too many important activations.
  - Training slowdown: BSpMM kernel inefficient due to poor block size choice or divergence.
  - Memory leak: activations not properly freed or BSR encoding overhead dominates.

- First 3 experiments:
  1. Run dense baseline training on CIFAR10/ResMLP-S12, record memory and accuracy.
  2. Apply 50% sparsity with block size 16, compare memory reduction and accuracy to baseline.
  3. Vary block size (4, 16, 64) at 70% sparsity, measure throughput and accuracy to find optimal configuration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal block size and sparsity level trade-off for different neural architectures beyond ResMLP, such as convolutional networks?
- Basis in paper: [explicit] The authors mention that future work will encompass larger neural architectures and compare CNNs, noting that CNNs make for an interesting candidate due to their typical kernel sizes (3x3 or 5x5) which align with hardware-friendly block sizes.
- Why unresolved: The paper focuses on ResMLP, and while it suggests potential for CNNs, it does not explore these architectures or determine optimal configurations for them.
- What evidence would resolve it: Empirical studies comparing different block sizes and sparsity levels across various architectures (e.g., CNNs, Transformers) to identify patterns or optimal configurations for each.

### Open Question 2
- Question: How does the introduction of stochastic components in the pruning process affect the accuracy and robustness of the neural networks?
- Basis in paper: [explicit] The authors mention that future work will include stochastic components in the pruning process, inspired by related work on gradient compression that shows stochasticity to be beneficial.
- Why unresolved: The paper currently uses a deterministic magnitude-based pruning approach and has not yet explored the effects of stochasticity on pruning.
- What evidence would resolve it: Experiments comparing deterministic and stochastic pruning methods to evaluate their impact on model accuracy, convergence, and robustness.

### Open Question 3
- Question: What is the impact of layer-specific sensitivity to pruning on the overall model performance, and how can pruning schedules be optimized accordingly?
- Basis in paper: [explicit] The authors note that different layers have different sensitivity to perturbations and mention future work will analyze each layer's sensitivity to pruning during training.
- Why unresolved: The paper applies pruning equally across all layers without considering individual layer sensitivities, which may lead to suboptimal performance.
- What evidence would resolve it: Layer-wise analysis and experiments to determine sensitivity, followed by the development and testing of layer-specific pruning schedules to optimize performance.

## Limitations
- Limited experimental validation on single architecture (ResMLP-S12) and datasets (CIFAR/ImageNet)
- Claims about ecological impact require broader empirical validation across diverse architectures
- No exploration of stochastic pruning methods or layer-specific sensitivity analysis

## Confidence

- High: BSR format provides compression benefits over unstructured sparsity; dense-forward/sparse-backward hybrid approach is technically feasible
- Medium: Up to 32% memory reduction is achievable with acceptable accuracy trade-offs; custom BSpMM kernels improve backward pass efficiency
- Low: Claims about ecological impact and democratization of large-scale training require broader empirical validation across diverse architectures and tasks

## Next Checks
1. Evaluate on additional architectures (Vision Transformers, MLPs with different attention mechanisms) to test generalizability of the memory-accuracy trade-off.
2. Measure training stability across multiple random seeds and learning rate schedules to quantify variance in accuracy degradation.
3. Profile end-to-end training throughput (samples/second) with different block sizes and sparsity levels to verify claimed efficiency gains hold in practice.