---
ver: rpa2
title: Self-Knowledge Guided Retrieval Augmentation for Large Language Models
arxiv_id: '2310.05002'
source_url: https://arxiv.org/abs/2310.05002
tags:
- llms
- question
- arxiv
- self-knowledge
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of effectively combining internal
  knowledge of large language models (LLMs) with external knowledge from retrieval
  systems. While retrieval-based methods can provide additional context, the authors
  find that retrieved passages do not always improve LLM responses and can sometimes
  be detrimental.
---

# Self-Knowledge Guided Retrieval Augmentation for Large Language Models

## Quick Facts
- arXiv ID: 2310.05002
- Source URL: https://arxiv.org/abs/2310.05002
- Authors: 
- Reference count: 14
- Key outcome: Self-Knowledge guided Retrieval augmentation (SKR) outperforms chain-of-thought and fully retrieval-based methods by 4.08%/2.91% (InstructGPT) and 4.02%/4.20% (ChatGPT) respectively

## Executive Summary
This paper addresses the challenge of effectively combining large language models' internal knowledge with external retrieval systems. While retrieval-augmentation can provide additional context, the authors find that retrieved passages sometimes degrade performance by introducing irrelevant or misleading information. To solve this, they propose SKR, a method that elicits the LLM's self-knowledge about what it knows and doesn't know, then adaptively decides when to use retrieval based on this self-knowledge. The approach is evaluated across five datasets using both InstructGPT and ChatGPT, demonstrating consistent performance improvements over baseline methods.

## Method Summary
SKR first collects self-knowledge from training questions by comparing performance with and without retrieval augmentation, categorizing questions based on whether retrieval improves accuracy. For new questions, the method uses one of four strategies (direct prompting, in-context learning, trained classifier, or nearest neighbor search) to elicit self-knowledge about whether retrieval is needed. The elicited knowledge then guides a decision module that chooses between internal-only generation or retrieval-augmented generation. The method is evaluated on five QA datasets (TemporalQA, CommonsenseQA, TabularQA, StrategyQA, TruthfulQA) using Wikipedia as the knowledge source and comparing against chain-of-thought and full retrieval baselines.

## Key Results
- SKR outperforms chain-of-thought based methods by 4.08% (InstructGPT) and 2.91% (ChatGPT)
- SKR outperforms fully retrieval-based methods by 4.02% (InstructGPT) and 4.20% (ChatGPT)
- Nearest neighbor search (SKRknn) performs best among self-knowledge elicitation strategies with 55-78% beneficial guidance across datasets
- SKR achieves consistent improvements across all five evaluated datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval augmentation sometimes degrades LLM performance when the retrieved passages introduce irrelevant or misleading information.
- Mechanism: The method detects when retrieved information conflicts with or distracts from the model's internal knowledge, then adaptively decides whether to use retrieval.
- Core assumption: The model's internal knowledge is sometimes more accurate than retrieved external information for specific questions.
- Evidence anchors:
  - [abstract] "we find that the retrieved knowledge does not always help and even has a negative impact on original responses occasionally"
  - [section 1] "The above findings show that one should be more careful when applying the retrieval-based method since it is difficult to know in advance whether the retrieved results are better than what LLMs already captured"
  - [corpus] Weak evidence - related papers discuss retrieval-augmentation but don't directly address the negative impact scenario
- Break condition: If the self-knowledge detection fails to accurately identify when retrieval is harmful, the adaptive mechanism could make wrong decisions and degrade performance.

### Mechanism 2
- Claim: LLMs have limited ability to recognize what they know and don't know (self-knowledge), which is crucial for deciding when to use external tools.
- Mechanism: The method elicits self-knowledge by comparing performance with and without retrieval augmentation on training questions, then uses this to guide decisions for new questions.
- Core assumption: The performance difference between internal-only and retrieval-augmented responses on training questions reflects the model's knowledge boundaries.
- Evidence anchors:
  - [abstract] "we investigate eliciting the model's ability to recognize what they know and do not know (which is also called 'self-knowledge')"
  - [section 3.1] "we categorize each question into positive subset D+ and negative subset Dâˆ’ based on the differences between results"
  - [corpus] Moderate evidence - related work "Do Large Language Models Know What They Don't Know?" directly addresses this concept
- Break condition: If the model's self-knowledge is consistently inaccurate or the training data doesn't capture the full knowledge boundary, the elicitation will be unreliable.

### Mechanism 3
- Claim: Multiple strategies (direct prompting, in-context learning, classifier training, nearest neighbor search) can effectively elicit self-knowledge from LLMs.
- Mechanism: Different elicitation strategies leverage various aspects of LLM capabilities to determine when external knowledge is needed.
- Core assumption: The elicited self-knowledge can generalize from training questions to new questions based on similarity or learned patterns.
- Evidence anchors:
  - [section 3.2] "Four different strategies are proposed to detect the self-knowledge of target questions, including direct prompting, in-context learning, training a classifier, and nearest neighbor search"
  - [section 5.2] Experimental results showing SKRknn (nearest neighbor) performs best with 55-78% beneficial guidance across datasets
  - [corpus] Weak evidence - corpus doesn't provide specific evidence for these particular strategies working
- Break condition: If the generalization from training to new questions fails, or if the strategies are sensitive to prompt variations or dataset characteristics, the method will not work reliably.

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: Understanding how retrieval-augmentation works and why it can be beneficial or harmful is fundamental to this method
  - Quick check question: What are the main components of a RAG system and how do they interact during inference?

- Concept: Chain-of-thought reasoning
  - Why needed here: The method compares against CoT-based approaches, so understanding how CoT works is important for context
  - Quick check question: How does chain-of-thought prompting differ from standard prompting in terms of the reasoning process?

- Concept: Self-knowledge and uncertainty estimation in LLMs
  - Why needed here: The core innovation relies on eliciting and using self-knowledge, so understanding this concept is crucial
  - Quick check question: What distinguishes self-knowledge from general confidence scores in language models?

## Architecture Onboarding

- Component map:
  Retriever -> Feature Extractor -> Self-Knowledge Elicitation Module -> Decision Module -> LLM (with or without retrieval)

- Critical path:
  1. For each new question, extract features and compare against training set
  2. Apply chosen self-knowledge elicitation strategy to determine if retrieval is needed
  3. If retrieval needed, fetch top-k passages and augment input
  4. Generate answer using either internal-only or retrieval-augmented approach
  5. Return final answer

- Design tradeoffs:
  - Multiple elicitation strategies vs. single optimal strategy - provides robustness but adds complexity
  - Nearest neighbor search vs. learned classifier - simpler but may not capture complex patterns
  - Real-time retrieval vs. pre-computed - affects latency and freshness of information

- Failure signatures:
  - Performance degrades on questions where self-knowledge is inaccurate
  - Increased latency due to unnecessary retrieval calls
  - Inconsistent behavior across different datasets or question types

- First 3 experiments:
  1. Implement SKRknn strategy on a small subset of one dataset and compare accuracy vs. internal-only and full retrieval baselines
  2. Test sensitivity of nearest neighbor strategy to k parameter on the same dataset
  3. Compare performance of different self-knowledge elicitation strategies (prompting vs. classifier vs. nearest neighbor) on a single dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strategy for eliciting self-knowledge from large language models that balances accuracy and computational efficiency?
- Basis in paper: [explicit] The paper compares four strategies (direct prompting, in-context learning, training a classifier, and nearest neighbor search) but notes limitations of each, particularly direct prompting's poor results and in-context learning's sensitivity to contextual bias.
- Why unresolved: While the paper shows that nearest neighbor search performs best overall, it doesn't explore hybrid approaches or investigate whether combining strategies could yield better results. The paper also doesn't examine how different prompting templates or demonstration selection strategies might affect in-context learning performance.
- What evidence would resolve it: Comparative experiments testing hybrid approaches that combine multiple elicitation strategies, along with systematic studies varying prompt templates, demonstration selection methods, and computational trade-offs.

### Open Question 2
- Question: How does the quality and domain specificity of external knowledge resources affect the performance of self-knowledge guided retrieval augmentation?
- Basis in paper: [explicit] The paper compares Wikipedia with MS MARCO and SciFact corpora, finding Wikipedia performs best overall, but doesn't investigate how corpus quality varies across different types of questions or domains.
- Why unresolved: The paper only examines three knowledge sources and doesn't explore whether domain-specific corpora might outperform general knowledge bases for certain types of questions, or how corpus quality metrics correlate with retrieval augmentation effectiveness.
- What evidence would resolve it: Systematic experiments comparing multiple knowledge sources across different domains and question types, including analysis of corpus quality metrics and their relationship to retrieval augmentation performance.

### Open Question 3
- Question: Can self-knowledge from one task or domain be effectively transferred to improve performance on new, related tasks without additional training data?
- Basis in paper: [explicit] The paper notes that performance improves with more training data but doesn't investigate whether self-knowledge can be accumulated and transferred across tasks or domains.
- Why unresolved: The paper focuses on within-dataset evaluation and doesn't explore cross-task or cross-domain transfer of self-knowledge, leaving open questions about the generalizability and reusability of elicited self-knowledge.
- What evidence would resolve it: Experiments testing self-knowledge transfer across different but related tasks or domains, measuring performance gains from accumulated self-knowledge versus task-specific training.

## Limitations
- The method relies on accurate self-knowledge elicitation, which achieves only 55-78% beneficial guidance, meaning 22-45% of decisions are still suboptimal
- The paper lacks detailed error analysis showing specific failure modes and patterns in incorrect retrieval decisions
- Performance improvements depend on having sufficient training data to elicit reliable self-knowledge, which may not be available for all domains

## Confidence

*High Confidence:* The claim that retrieval augmentation can be detrimental when retrieved passages introduce irrelevant or misleading information is well-supported by the paper's experimental results showing that SKR outperforms both chain-of-thought based and fully retrieval-based methods by 4.08-4.20%.

*Medium Confidence:* The claim that LLMs have limited self-knowledge about what they know and don't know is supported by the paper's findings and related work, but the specific mechanisms for eliciting this self-knowledge could vary significantly based on implementation details not fully specified in the paper.

*Low Confidence:* The claim that the four proposed self-knowledge elicitation strategies (direct prompting, in-context learning, classifier training, nearest neighbor search) are all effective approaches is only partially supported, as the paper shows SKRknn performs best while SKRprompt performs worst, suggesting the strategies have varying effectiveness.

## Next Checks
1. **Generalization Test:** Apply SKR to a new domain (e.g., medical QA or code generation) with a different knowledge distribution to verify whether the self-knowledge elicitation strategies maintain their effectiveness when the knowledge boundaries differ significantly from the original training datasets.

2. **Failure Mode Analysis:** Systematically analyze cases where SKR makes incorrect retrieval decisions by examining the specific characteristics of questions where retrieval was wrongly recommended or wrongly withheld, to identify patterns that could improve the elicitation strategies.

3. **Human Evaluation:** Conduct human evaluation of SKR's retrieval decisions to determine whether the model's self-knowledge judgments align with human intuition about when external knowledge is actually needed, particularly for questions where SKR makes different decisions than the full retrieval baseline.