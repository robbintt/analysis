---
ver: rpa2
title: Punctuation restoration Model and Spacing Model for Korean Ancient Document
arxiv_id: '2312.11881'
source_url: https://arxiv.org/abs/2312.11881
tags:
- punctuation
- spacing
- marks
- data
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents the first models for punctuation restoration
  and spacing prediction on Korean ancient documents, which are written in classical
  Chinese without modern punctuation or spacing. The authors adapted a pre-trained
  Chinese BERT model by adding a linear layer and trained two models: one for predicting
  seven punctuation marks and another for predicting spacing between words.'
---

# Punctuation restoration Model and Spacing Model for Korean Ancient Document

## Quick Facts
- arXiv ID: 2312.11881
- Source URL: https://arxiv.org/abs/2312.11881
- Reference count: 13
- Primary result: First models for punctuation restoration and spacing prediction on Korean ancient documents, achieving F1 scores of 0.84 and 0.96 respectively

## Executive Summary
This paper introduces the first models for punctuation restoration and spacing prediction on Korean ancient documents written in classical Chinese without modern punctuation or spacing. The authors adapted a pre-trained Chinese BERT model by adding a linear classification layer to predict both punctuation marks and word boundaries. The models were trained on a dataset of over 665,000 sequences from the Annals of the Joseon Dynasty and Confucian classics, achieving F1 scores of 0.84 for punctuation restoration and 0.96 for spacing prediction. The approach demonstrates efficient performance using minimal GPU resources (1.5GB VRAM) compared to larger models like LLaMA2.

## Method Summary
The authors adapted a pre-trained Chinese BERT model by adding a linear classification layer on top of it. The model was trained using token classification to predict both punctuation marks (7-way classification) and spacing (2-way classification) for each character position in the input sequence. The training dataset consisted of over 665,000 sequences from Korean ancient documents, with 90% used for training and 10% for testing. The model was fine-tuned for 15 epochs with a batch size of 16, using AdamW optimizer with learning rate 5 × 10^-5 and Cross-Entropy Loss.

## Key Results
- Punctuation restoration model achieved F1 score of 0.84
- Spacing prediction model achieved F1 score of 0.96
- Models use only 1.5GB VRAM, making them efficient for low-resource settings
- First successful application of BERT-based approach to Korean ancient document processing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BERT's pre-training on Chinese language provides strong contextual understanding for Korean ancient documents written in classical Chinese.
- Mechanism: The model leverages the pre-trained BERT's bidirectional transformer architecture to capture contextual relationships between characters, even across sentence boundaries.
- Core assumption: Classical Chinese and modern Chinese share enough linguistic structure that a Chinese BERT model can effectively process Korean ancient documents.
- Evidence anchors:
  - [abstract] The authors used "bert-base-chinese" which was pretrained by Chinese.
  - [section] BERT model excels in understanding context because of using the Transformer architecture.
  - [corpus] Found 25 related papers discussing Chinese and East Asian historical document processing.

### Mechanism 2
- Claim: Token classification approach effectively predicts both punctuation and spacing by treating each character as a classification target.
- Mechanism: By adding a linear layer on top of BERT's output and training with cross-entropy loss, the model learns to classify each token position as either containing specific punctuation or requiring spacing.
- Core assumption: The sequence-to-sequence nature of token classification can capture the positional dependencies needed for punctuation and spacing prediction.
- Evidence anchors:
  - [abstract] The models use token classification for predicting punctuation and predicting spacing.
  - [section] We adapted the model to token classification for predicting punctuation and predicting spacing.
  - [corpus] Related work on punctuation restoration uses similar token classification approaches.

### Mechanism 3
- Claim: Data preprocessing and label simplification improve model performance by reducing noise and sparsity.
- Mechanism: The authors reduced 20+ punctuation marks to 6 key labels and handled edge cases like nested quotations and consecutive punctuation marks through expert consultation.
- Core assumption: Simplifying the label space while preserving essential meaning allows the model to learn more effectively.
- Evidence anchors:
  - [section] We unified or removed inconsistent punctuation marks. More than 20 punctuation marks in the datasets were reduced to 6 punctuation marks.
  - [abstract] The punctuation restoration model achieved an F1 score of 0.84.
  - [corpus] Related work on punctuation restoration emphasizes the importance of data preprocessing.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: BERT is built on transformers, which are essential for understanding the bidirectional context that makes punctuation and spacing prediction possible.
  - Quick check question: How does the self-attention mechanism in transformers differ from traditional RNN approaches in handling long-range dependencies?

- Concept: Token classification vs sequence labeling
  - Why needed here: The paper uses token classification to predict punctuation and spacing, which is different from sequence-to-sequence approaches commonly used in translation.
  - Quick check question: What are the advantages of token classification for punctuation restoration compared to generating punctuation as separate tokens?

- Concept: Cross-lingual transfer learning
  - Why needed here: The model uses a Chinese BERT model for Korean documents, relying on cross-lingual transfer between classical Chinese and Korean ancient texts.
  - Quick check question: What linguistic similarities between classical Chinese and Korean ancient documents written in Chinese characters make cross-lingual transfer feasible?

## Architecture Onboarding

- Component map: BERT base model → Linear classification layer → 7-way classifier (for punctuation) or 2-way classifier (for spacing)
- Critical path: Input sequence → BERT encoding → Linear layer → Softmax → Predicted label for each token position
- Design tradeoffs: Using a pre-trained Chinese BERT model provides strong initialization but may carry biases from modern Chinese text; adding a simple linear layer keeps the model lightweight but may limit complex pattern learning.
- Failure signatures: Low F1 scores on minority punctuation marks indicate class imbalance; poor spacing predictions at sentence boundaries suggest the model hasn't learned proper contextual rules.
- First 3 experiments:
  1. Test model performance on a held-out validation set during training to monitor for overfitting
  2. Evaluate the impact of different learning rates (e.g., 1e-5, 5e-5, 1e-4) on convergence and final performance
  3. Compare model performance when trained on AJD only vs. both AJD and DRC datasets to assess data contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can label imbalance in punctuation prediction be effectively addressed to improve model performance?
- Basis in paper: [explicit] The authors mention considering "strategies for addressing label imbalance" as a method to improve performance
- Why unresolved: The paper acknowledges label imbalance as an issue but does not implement or evaluate specific solutions for it
- What evidence would resolve it: Comparative results showing F1 scores before and after implementing different label imbalance techniques (e.g., oversampling, class weights, focal loss) on the same dataset

### Open Question 2
- Question: How would integrating additional ancient document data from different time periods affect model performance?
- Basis in paper: [explicit] The authors state they are considering "the collection and integration of additional ancient document data"
- Why unresolved: The current model is trained only on Annals of the Joseon Dynasty and data related to Confucianism, which may not represent the full diversity of Korean ancient documents
- What evidence would resolve it: Performance metrics (F1 scores) comparing models trained on current data versus models trained on expanded datasets including documents from different historical periods and genres

### Open Question 3
- Question: What is the performance difference between the proposed model and GPT-based approaches when applied to unseen ancient Korean texts?
- Basis in paper: [explicit] The authors compare their model to GPTs on already-punctuated sentences, finding similar F1 scores of approximately 0.84
- Why unresolved: The comparison was only done on already-punctuated sentences, not on raw ancient texts requiring both punctuation and spacing restoration
- What evidence would resolve it: Direct comparison of both models on the same test set of raw ancient Korean documents, measuring both punctuation restoration and spacing prediction accuracy

## Limitations
- Label simplification from 20+ punctuation marks to 6 may mask performance issues on less common punctuation types
- Dataset composition unclear - insufficient detail on how many documents come from AJD versus DRC
- Expert consultation for preprocessing lacks transparency in criteria and potential biases
- Efficiency claims lack direct empirical validation beyond comparison to LLaMA2

## Confidence
- **High Confidence**: Core methodology using Chinese BERT with linear classification layer is technically sound and well-established
- **Medium Confidence**: Performance metrics reported but lack detailed breakdown by punctuation type or document source
- **Low Confidence**: Generalization capability to unseen document types and impact of label simplification on downstream applications remain unclear

## Next Checks
1. **Class-wise Performance Analysis**: Break down the F1 scores by individual punctuation mark to identify which specific marks the model handles well versus poorly.
2. **Cross-Dataset Validation**: Test the trained model on a completely separate corpus of Korean ancient documents not used in training to assess generalization.
3. **Ablation Study on Preprocessing Choices**: Systematically remove or modify the preprocessing simplifications to quantify the impact on model performance.