---
ver: rpa2
title: 'InterviewBot: Real-Time End-to-End Dialogue System to Interview Students for
  College Admission'
arxiv_id: '2303.15049'
source_url: https://arxiv.org/abs/2303.15049
tags:
- utterances
- dialogue
- errors
- interview
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The InterviewBot is a neural-based end-to-end dialogue system designed
  to conduct 10-minute hybrid-domain conversations with foreign students applying
  to U.S. colleges, assessing their academic and cultural readiness.
---

# InterviewBot: Real-Time End-to-End Dialogue System to Interview Students for College Admission

## Quick Facts
- arXiv ID: 2303.15049
- Source URL: https://arxiv.org/abs/2303.15049
- Reference count: 18
- Primary result: Neural dialogue system achieving 3.5/5 satisfaction score in real-time college admission interviews

## Executive Summary
InterviewBot is a neural-based end-to-end dialogue system designed to conduct 10-minute hybrid-domain conversations with foreign students applying to U.S. colleges, assessing their academic and cultural readiness. The system addresses the challenge of handling long conversation contexts and user-oriented topics through context attention and topic storing methods. Trained on 7,361 automatically transcribed audio recordings of human-to-human interviews, with 440 manually corrected for fine-tuning, the system demonstrates high satisfaction in fluency and context awareness during real-time testing with professional interviewers and students.

## Method Summary
The InterviewBot employs a transformer encoder-decoder architecture enhanced with sliding window techniques, context attention, and topic storing mechanisms to handle long conversation contexts. The system processes automatically transcribed interview audio, using speaker diarization to separate interviewer and interviewee utterances. Context attention dynamically weights prior utterances to maintain relevance, while topic storing encodes interviewer questions as key topics for persistent memory across dialogues. The model is trained on a large corpus of interview recordings and fine-tuned on manually corrected subsets.

## Key Results
- Achieved average satisfaction score of 3.5/5 in real-time testing with professional interviewers and students
- Successfully handled 10-minute hybrid-domain conversations assessing academic and cultural readiness
- Demonstrated effective context awareness and topic management in interview scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The sliding window technique allows handling of long utterances without losing semantic coherence.
- Mechanism: Long utterances are split into overlapping chunks (U1, U2) where the overlap (m tokens) preserves context continuity, and the embeddings of the overlapping tokens are averaged to form a single embedding for the original utterance.
- Core assumption: The central portion of an utterance contains the most salient meaning, and averaging overlapping embeddings preserves semantic continuity.
- Evidence anchors:
  - [section] "Every utterance U whose length is greater than n is split into U 1 and U 2 ... The embedding matrix E∈ R(n+m)×d of U is created by stacking all of the following embeddings: {e1
1, . . . , 1
2
2∑
i=1
(ei
m+1), . . . , 1
2
2∑
i=1
(ei
n), . . . , e2
n+m}"
  - [corpus] Weak evidence; corpus neighbors discuss related NLP methods but not specifically overlapping window embeddings.
- Break condition: If the overlap length m is too small relative to utterance length, the averaging will blur critical semantic boundaries; if too large, it defeats the purpose of splitting.

### Mechanism 2
- Claim: Context attention enables the model to maintain relevance over long conversation histories by dynamically weighting prior utterances.
- Mechanism: The model computes an attention matrix A between the stacked embeddings of the previous k utterances (C) and the current utterance, producing a context summary S that is fed into the decoder.
- Core assumption: Relevant parts of the conversation history can be identified via learned attention weights, and summarizing them into S preserves enough context for coherent generation.
- Evidence anchors:
  - [section] "Let C∈ Rℓ×d be the context matrix stacking the embedding matrices of the previous utterances ... The transpose of C is multiplied by the attention matrix A∈ Rℓ×n such that CT· A→ ST∈ Rd×n."
  - [corpus] No direct evidence; corpus focuses on student-chatbot interactions rather than attention-based context summarization.
- Break condition: If k is too small, important context is lost; if too large, the model may overfit to noise or irrelevant past turns.

### Mechanism 3
- Claim: Topic storing allows the model to remember and reuse key discussion topics across long dialogues, reducing repetition and enabling natural topic transitions.
- Mechanism: The model treats interviewer questions as "key topics," encodes them into embeddings, and stores them in a topic matrix V. V is concatenated with the context matrix C before attention, so the decoder can explicitly reference past topics.
- Core assumption: Interviewer questions define the semantic scope of the interview, and encoding them as topics gives the model a persistent memory of what has been covered.
- Evidence anchors:
  - [section] "Let Q ={q1, .., qh} be the topical question set. ... the model considers these questions the 'key topics' and dynamically stores them as the dialogue progresses. ... V is stacked with the context matrix C ... to create the transpose of the context summary matrix S."
  - [corpus] Weak evidence; corpus papers discuss related dialogue systems but not explicit topic storage for interview contexts.
- Break condition: If the topic set Q is not representative or too sparse, the model will have insufficient topic memory; if too dense, it may overfit to specific question patterns.

## Foundational Learning

- Concept: Transformer encoder-decoder architecture
  - Why needed here: Provides the underlying sequence-to-sequence modeling capability for dialogue generation.
  - Quick check question: What is the role of the encoder and decoder in a transformer-based dialogue system?

- Concept: Attention mechanisms in NLP
  - Why needed here: Enables the model to focus on relevant parts of the conversation history and topical questions.
  - Quick check question: How does multi-head attention help the model attend to different aspects of context simultaneously?

- Concept: Speaker diarization
  - Why needed here: Correctly separating interviewer and interviewee utterances is critical for training a coherent dialogue model.
  - Quick check question: Why does overlapping speech in interview recordings pose a challenge for automatic speaker identification?

## Architecture Onboarding

- Component map: Raw audio -> automatic transcription -> speaker diarization -> cleaned text -> training data -> Transformer encoder + context attention + topic storing + sliding window + transformer decoder -> output

- Critical path:
  1. Clean speaker-separated utterances (diarization)
  2. Encode current utterance with sliding window
  3. Aggregate context via attention over previous utterances
  4. Merge topic embeddings into context summary
  5. Decode next utterance conditioned on aggregated context

- Design tradeoffs:
  - Longer k (context window) -> more coherent but higher computational cost
  - Larger m (overlap) -> better continuity but more redundant embeddings
  - More topic questions -> better topic coverage but risk of overfitting

- Failure signatures:
  - Repetition: Context attention or topic storing is not working
  - Off-topic: Attention weights are not correctly learned
  - Early ending: Decoder is not conditioned on full context/Topic storing is incomplete

- First 3 experiments:
  1. Ablation: Remove topic storing and compare repetition rates.
  2. Hyperparameter sweep: Vary k (context window size) and measure coherence via BLEU with reference human responses.
  3. Speaker diarization accuracy test: Evaluate F1 of diarization model on held-out manually annotated interviews.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the context attention mechanism specifically handle overlapping speech between the interviewer and interviewee, given that such overlaps are a significant source of diarization errors?
- Basis in paper: [explicit] The paper mentions that overlapping speech is a major source of diarization errors and that context attention is used to handle long conversation contexts.
- Why unresolved: The paper does not detail how context attention deals with overlapping speech specifically, which is a critical aspect of real interview scenarios.
- What evidence would resolve it: Experimental results comparing the model's performance with and without context attention on datasets with varying levels of overlapping speech.

### Open Question 2
- Question: What are the long-term effects of using the InterviewBot on the quality and diversity of college admission interviews, particularly in terms of assessing applicants' cultural readiness and critical thinking skills?
- Basis in paper: [inferred] The paper discusses the InterviewBot's ability to assess academic and cultural readiness, but does not explore long-term impacts on interview quality or diversity.
- Why unresolved: The study focuses on immediate satisfaction and performance metrics without considering longitudinal effects on interview processes or outcomes.
- What evidence would resolve it: Longitudinal studies tracking changes in interview practices and applicant outcomes over multiple admission cycles using the InterviewBot.

### Open Question 3
- Question: How does the topic storing mechanism adapt to dynamic changes in conversation topics initiated by the interviewee, and what are its limitations in maintaining topic coherence over extended dialogues?
- Basis in paper: [explicit] The paper introduces topic storing to remember user-oriented topics but does not elaborate on its adaptability to dynamic topic shifts.
- Why unresolved: The mechanism's flexibility and limitations in handling spontaneous topic changes by the interviewee are not explored.
- What evidence would resolve it: Analysis of the model's performance in interviews with highly dynamic or unpredictable topic changes, and comparison with human interviewers.

### Open Question 4
- Question: What are the ethical implications of deploying the InterviewBot in real-world college admissions, particularly concerning bias in assessing applicants from diverse cultural backgrounds?
- Basis in paper: [inferred] The paper mentions the potential for biased behavior but does not delve into specific ethical considerations or impacts on diverse applicant groups.
- Why unresolved: Ethical considerations are briefly mentioned without a detailed examination of potential biases or their effects on different demographic groups.
- What evidence would resolve it: Studies evaluating the model's performance across diverse applicant groups and identifying any biases or disparities in assessment outcomes.

## Limitations

- Data quality concerns due to reliance on automatically transcribed recordings with limited manual correction
- Evaluation methodology lacks detail about sample size, demographic diversity, and control conditions
- Critical hyperparameters and implementation details are underspecified, limiting reproducibility

## Confidence

- **High confidence**: The core architecture (transformer encoder-decoder with attention mechanisms) is well-established and technically sound
- **Medium confidence**: The sliding window and context attention mechanisms are logically coherent, though their specific implementations could benefit from more detail
- **Low confidence**: The evaluation claims and satisfaction metrics, given the limited methodological transparency

## Next Checks

1. **Ablation study validation**: Remove the topic storing component and measure the impact on repetition rates and interview coherence to quantify its contribution.

2. **Cross-validation with external evaluators**: Have independent professional interviewers evaluate the system's performance on a held-out test set to verify the claimed satisfaction scores.

3. **Speaker diarization accuracy benchmark**: Evaluate the diarization model's performance on manually annotated interview data to establish a baseline for transcription quality and its impact on downstream dialogue quality.