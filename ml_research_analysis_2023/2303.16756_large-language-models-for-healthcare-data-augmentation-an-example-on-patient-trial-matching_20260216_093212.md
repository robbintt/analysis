---
ver: rpa2
title: 'Large Language Models for Healthcare Data Augmentation: An Example on Patient-Trial
  Matching'
arxiv_id: '2303.16756'
source_url: https://arxiv.org/abs/2303.16756
tags:
- data
- patient
- criteria
- clinical
- augmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a privacy-aware data augmentation method using
  large language models (LLMs) for patient-trial matching, addressing the challenge
  of interoperability between Electronic Health Records (EHRs) and clinical trial
  descriptions. The LLM-PTM method leverages LLMs' advanced natural language generation
  capabilities to improve compatibility while preserving data privacy.
---

# Large Language Models for Healthcare Data Augmentation: An Example on Patient-Trial Matching

## Quick Facts
- arXiv ID: 2303.16756
- Source URL: https://arxiv.org/abs/2303.16756
- Reference count: 36
- Key outcome: 7.32% average performance improvement and 12.12% generalizability increase using LLM-based data augmentation for patient-trial matching

## Executive Summary
This study introduces LLM-PTM, a privacy-aware data augmentation method that uses large language models to enhance patient-trial matching between Electronic Health Records (EHRs) and clinical trial eligibility criteria. The approach addresses interoperability challenges by leveraging LLMs' advanced natural language generation capabilities to create semantically consistent but diverse criteria while preserving data privacy. Experimental results demonstrate significant performance gains over baseline models, with particular effectiveness in handling complex matching scenarios.

## Method Summary
The LLM-PTM method combines patient EHR data with clinical trial eligibility criteria using a deep learning architecture that includes Clinical BERT embeddings, memory networks for sequence processing, and highway networks for criteria representation. The approach uses a composite loss function that balances classification accuracy with explicit handling of inclusion and exclusion criteria through contrastive loss. Data augmentation is performed using ChatGPT to generate semantically consistent variations of trial criteria, which are then used to expand the training dataset while maintaining privacy through synthetic data generation.

## Key Results
- 7.32% average improvement in performance metrics compared to baseline models
- 12.12% increase in model generalizability across unseen trials
- Effective handling of complex matching scenarios through semantic enrichment of trial criteria

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based data augmentation improves patient-trial matching by generating semantically consistent but diverse criteria
- Mechanism: The proposed LLM-PTM method uses a prompt concatenated with original inclusion/exclusion criteria to generate augmented data points. This approach leverages the LLM's ability to produce text that maintains semantic meaning while introducing variation, thereby expanding the training dataset with high-quality examples
- Core assumption: LLMs can generate criteria that are semantically identical to the original but expressed differently, enhancing model robustness without introducing noise
- Evidence anchors: 
  - [abstract]: "The proposed approach demonstrates a 7.32% average improvement in performance and a 12.12% increase in generalizability compared to baseline models"
  - [section]: "The final augmented trial data setT can then be represented as: T = ⋃_{k=1}^{n} A_{ik} ∪ ⋃_{l=1}^{m} A_{el}"
  - [corpus]: Found related papers using LLM-based methods, but none specifically demonstrating this augmentation mechanism
- Break condition: If the LLM generates criteria that are semantically inconsistent or introduces noise, the model's performance may degrade rather than improve

### Mechanism 2
- Claim: The combination of classification loss and inclusion/exclusion contrastive loss optimizes both accuracy and the ability to distinguish between inclusion and exclusion criteria
- Mechanism: The composite loss function includes a classification loss term to optimize matching predictions and a contrastive loss term that explicitly maximizes similarity for inclusion criteria and minimizes similarity for exclusion criteria. This dual approach ensures the model learns to handle both types of criteria effectively
- Core assumption: The model can effectively learn to maximize similarity for inclusion criteria and minimize it for exclusion criteria through the contrastive loss term
- Evidence anchors:
  - [abstract]: "Our experiments demonstrate a 7.32% average improvement in performance using the proposed LLM-PTM method"
  - [section]: "L_con = ∏_{a=1,...,n_i} (1-s(x_{ia},x_P)) · ∏_{b=1,...,n_e} max(0,s(x_{eb},x_P)-ε)"
  - [corpus]: Related works focus on matching but not specifically on this dual loss approach for handling inclusion/exclusion criteria
- Break condition: If the hyperparameter ε is not properly tuned, the model may not effectively learn to distinguish between inclusion and exclusion criteria

### Mechanism 3
- Claim: The memory network effectively captures the sequence of patient visit data, enhancing the model's ability to make accurate matching decisions
- Mechanism: The patient record embedding is derived from a memory network that manages the patient records, maintaining the sequence of visit data within the embedding space. This approach allows the model to consider the temporal aspect of patient data
- Core assumption: The memory network can effectively maintain the sequence of visit data and incorporate it into the patient embedding
- Evidence anchors:
  - [abstract]: "Our experiments demonstrate a 7.32% average improvement in performance using the proposed LLM-PTM method"
  - [section]: "xP = fP(P) = Mem(BERT(a1), BERT(a2), ..., BERT(an)), ai ∈ {D,M,P}"
  - [corpus]: While memory networks are used in NLP, their specific application to maintaining patient visit sequences in this context is not well-documented in the corpus
- Break condition: If the memory network fails to capture the sequence effectively, the model may lose important temporal information, leading to reduced performance

## Foundational Learning

- Concept: Natural Language Processing (NLP) and text embedding techniques
  - Why needed here: The study relies on converting text data (patient records and clinical trial criteria) into numerical embeddings that can be processed by machine learning models
  - Quick check question: Can you explain the difference between static and contextual embeddings, and why BERT embeddings might be preferred in this application?

- Concept: Machine Learning and deep learning architectures
  - Why needed here: The proposed method uses a deep learning model with components like BERT, memory networks, and highway networks to learn the matching function
  - Quick check question: What is the role of the highway network in this architecture, and how does it contribute to the model's performance?

- Concept: Data augmentation techniques in machine learning
  - Why needed here: The study introduces a novel data augmentation method using LLMs to generate additional training data, which is crucial for improving model performance and generalizability
  - Quick check question: How does data augmentation help in addressing the issue of limited training data in patient-trial matching?

## Architecture Onboarding

- Component map: Patient records (EHR data) -> Text preprocessing -> BERT embeddings -> Memory network -> Patient embedding; Clinical trial criteria -> Text preprocessing -> BERT embeddings -> Highway network -> Criteria embedding; Deep learning model -> Classification and contrastive loss -> Matching prediction

- Critical path:
  1. Text preprocessing of patient records and clinical trial criteria
  2. Generation of augmented data using LLM-PTM
  3. Embedding of patient records and criteria using BERT and highway networks
  4. Model training with classification and contrastive loss functions
  5. Prediction of patient-trial matching

- Design tradeoffs:
  - Using LLMs for data augmentation vs. traditional augmentation techniques: LLMs can generate more semantically consistent data but may be computationally expensive
  - Balancing classification loss and contrastive loss: The weights (α) need to be tuned to optimize both accuracy and the ability to distinguish between inclusion and exclusion criteria

- Failure signatures:
  - Poor performance on new data: Indicates issues with model generalizability, possibly due to overfitting or inadequate data augmentation
  - Inability to distinguish between inclusion and exclusion criteria: Suggests problems with the contrastive loss function or the model's learning of semantic features
  - Computational inefficiency: May result from the use of LLMs for data augmentation or complex model architectures

- First 3 experiments:
  1. Baseline experiment: Train the model on the original dataset without any augmentation to establish a performance baseline
  2. Data augmentation experiment: Apply the LLM-PTM method to generate augmented data and train the model on this expanded dataset
  3. Ablation study: Remove the memory network or highway network components to assess their individual contributions to the model's performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the research suggests several areas for future investigation including the generalizability of the approach across different therapeutic areas, the scalability of the method to larger clinical trial datasets, and the potential for real-time patient-trial matching applications in clinical settings.

## Limitations
- Validation limited to a single dataset (FindCT) with only 23 clinical trials, raising concerns about generalizability
- Performance improvements based on aggregate results without detailed per-trial analysis to assess variability
- Lack of comprehensive ablation studies to isolate the contribution of each component (LLM augmentation, memory network, contrastive loss)

## Confidence
- High confidence: The core methodology of using LLM-based augmentation for clinical trial criteria is technically sound and well-articulated
- Medium confidence: The claimed performance improvements are reasonable but limited by the small validation dataset size
- Medium confidence: The privacy-preserving aspect through synthetic data generation is theoretically valid but not empirically verified

## Next Checks
1. Test the model on multiple clinical trial datasets beyond FindCT to verify generalizability across different trial types and sizes
2. Conduct ablation studies to quantify the individual contribution of LLM augmentation versus the baseline model components
3. Perform a comprehensive error analysis on failed matches to identify systematic weaknesses in the approach