---
ver: rpa2
title: 'Probing LLMs for hate speech detection: strengths and vulnerabilities'
arxiv_id: '2310.12860'
source_url: https://arxiv.org/abs/2310.12860
tags:
- hate
- dataset
- vanilla
- implicit
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the effectiveness of Large Language Models
  (LLMs) for hate speech detection using zero-shot learning. The authors propose several
  prompt variations incorporating definitions, explanations, and target community
  information to probe three LLMs (GPT-3.5, text-davinci, and Flan-T5) across three
  datasets (HateXplain, implicit hate, and ToxicSpans).
---

# Probing LLMs for hate speech detection: strengths and vulnerabilities

## Quick Facts
- arXiv ID: 2310.12860
- Source URL: https://arxiv.org/abs/2310.12860
- Reference count: 11
- Key outcome: Adding target information to prompts improves LLM hate speech detection F1-score by 20-30%, while explanations improve it by 10-20%, but combining strategies yields no additional benefit.

## Executive Summary
This study investigates zero-shot learning approaches for hate speech detection using three LLMs (GPT-3.5, text-davinci, and Flan-T5) across three datasets. The authors systematically test prompt variations including definitions, explanations, and target community information. Results demonstrate that including target information in prompts yields the largest performance gains (20-30% F1-score improvement), followed by adding explanations (10-20% improvement). However, combining multiple prompt strategies does not produce further benefits. The study also provides detailed error analysis revealing common misclassification patterns and vulnerabilities in LLM-based hate speech detection systems.

## Method Summary
The study employs zero-shot learning to evaluate three LLMs on hate speech detection tasks using three datasets (HateXplain, implicit hate, and ToxicSpans). Six prompt variations are tested: vanilla, definitions, explanations (as input/output), targets (as input/output), and combinations. Performance is measured using F1-score, precision, recall, accuracy, and additional metrics for explanation quality (BERTScore, sentence-BLEU). The experimental design systematically evaluates how different prompt engineering strategies affect model performance without in-context examples.

## Key Results
- Target information inclusion improves F1-score by 20-30% across datasets
- Explanation addition improves F1-score by 10-20% over baseline
- Combining multiple prompt strategies yields no additional performance benefits
- Error analysis reveals distinct misclassification patterns across datasets

## Why This Works (Mechanism)

### Mechanism 1: Target Information
- Claim: Including target community information in prompts improves LLM hate speech detection performance by ~20-30% over baseline.
- Mechanism: Target information provides explicit context about victim groups, enabling better distinction between harmful content directed at specific communities versus general offensive language.
- Core assumption: The model can effectively utilize explicit victim group information when provided as input.
- Evidence anchors: Abstract reports 20-30% improvement; section 6 shows 33.33% improvement for GPT-3.5 and 26.67% for text-davinci on HateXplain dataset.
- Break condition: If target information is ambiguous, missing, or incorrectly specified, the performance improvement may not materialize.

### Mechanism 2: Explanations
- Claim: Adding explanations as input to prompts improves model performance by ~10-20% over baseline.
- Mechanism: Providing rationales or implied statements as input helps the model understand reasoning behind hate speech classifications, leading to more accurate predictions.
- Core assumption: The model can effectively process and utilize provided explanations when included as input.
- Evidence anchors: Abstract reports 10-20% improvement; section 6 shows 12.82% improvement for GPT-3.5 and 15.55% for text-davinci on HateXplain dataset.
- Break condition: If explanations are vague, irrelevant, or incorrectly aligned with input text, the performance improvement may not occur.

### Mechanism 3: Definitions
- Claim: Providing definitions of hate speech categories improves model understanding and classification accuracy.
- Mechanism: Definitions clarify boundaries between hate speech, offensive content, and normal speech, reducing ambiguity in classification decisions.
- Core assumption: The model can effectively utilize provided definitions to disambiguate between closely related categories.
- Evidence anchors: Abstract mentions testing prompts with definitions; section 6 shows mixed results with 12.50-16.67% improvement for some models on implicit hate dataset.
- Break condition: If definitions are too broad, too narrow, or conflict with the model's pre-existing understanding, they may not improve performance.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The study evaluates LLMs without in-context examples, relying solely on prompt engineering.
  - Quick check question: What is the difference between zero-shot and few-shot learning in the context of LLM evaluation?

- Concept: Prompt engineering
  - Why needed here: The study tests various prompt variations to optimize LLM performance for hate speech detection.
  - Quick check question: How does adding target information as input differ from asking the model to generate target information as output?

- Concept: Error analysis and typology
  - Why needed here: The study provides detailed analysis of misclassification cases to identify model vulnerabilities.
  - Quick check question: What are the main categories of misclassification identified in the study, and how do they vary across datasets?

## Architecture Onboarding

- Component map: LLM models (gpt-3.5-turbo-0301, text-davinci-003, flan-T5-large) -> Prompt strategies (vanilla, +definitions, +explanations, +targets, combinations) -> Datasets (HateXplain, implicit hate, ToxicSpans) -> Evaluation metrics (F1-score, precision, recall, accuracy, BERTScore, sentence-BLEU)

- Critical path: 1) Select prompt strategy, 2) Generate predictions using LLM, 3) Evaluate performance using appropriate metrics, 4) Analyze error patterns and misclassification types

- Design tradeoffs:
  - Using target information as input vs. output: Input provides explicit context but requires annotated targets; output allows model to identify targets but may be less accurate.
  - Adding explanations as input vs. output: Input provides reasoning context but requires annotated explanations; output allows model to generate explanations but may be less accurate.
  - Combining multiple prompt strategies: May not yield additional benefits and could potentially confuse the model.

- Failure signatures:
  - Significant drop in performance when combining multiple prompt strategies
  - Consistent misclassification of specific categories (e.g., non-hate to implicit hate)
  - Poor quality explanations or target generation

- First 3 experiments:
  1. Test vanilla prompts on all three models and datasets to establish baseline performance.
  2. Add target information as input to prompts and evaluate performance improvement.
  3. Add explanations as input to prompts and evaluate performance improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do large language models' performance on hate speech detection improve with larger model sizes beyond those tested (GPT-3.5, text-davinci-003, Flan-T5)?
- Basis in paper: [inferred] The paper compares three specific LLMs but doesn't explore whether even larger models might perform better.
- Why unresolved: The study focused on a limited set of available models at the time, leaving the question of scalability open.
- What evidence would resolve it: Systematic testing of larger or more recent LLM architectures on the same datasets with identical prompt variations.

### Open Question 2
- Question: How do prompt variations affect the false positive and false negative rates differently in hate speech detection?
- Basis in paper: [inferred] While overall F1-scores are reported, the paper doesn't provide detailed breakdowns of how different prompt strategies impact error types.
- Why unresolved: The analysis focuses on aggregate performance metrics rather than the distribution of specific error types.
- What evidence would resolve it: Detailed confusion matrices and error analysis for each prompt variation showing false positive vs false negative rates.

### Open Question 3
- Question: Can combining multiple prompt strategies in a more sophisticated way yield better results than the individual approaches tested?
- Basis in paper: [explicit] "However, combining multiple prompt strategies does not yield further benefits."
- Why unresolved: The paper tested only simple combinations and concluded they don't help, but more complex integration methods weren't explored.
- What evidence would resolve it: Testing of advanced prompt engineering techniques like prompt chaining, dynamic prompt selection, or multi-stage prompting.

## Limitations

- Limited generalizability due to evaluation on only three English datasets
- Zero-shot approach may not capture full LLM potential compared to few-shot or fine-tuned models
- Absence of detailed prompt templates and definitions prevents exact reproduction

## Confidence

- High Confidence: The general trend that target information and explanations improve model performance
- Medium Confidence: The specific performance improvements (20-30% for targets, 10-20% for explanations) and the error analysis findings
- Low Confidence: The generalizability of the results to other datasets, languages, and model architectures

## Next Checks

1. Reproduce results with full prompt templates obtained from authors to validate findings across all datasets and models

2. Evaluate prompt strategies on additional hate speech datasets from different languages and domains to assess generalizability

3. Conduct few-shot learning experiments using the same prompt strategies to determine if performance gains persist with in-context examples