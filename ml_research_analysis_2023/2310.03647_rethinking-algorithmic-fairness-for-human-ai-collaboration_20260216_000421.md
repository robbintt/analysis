---
ver: rpa2
title: Rethinking Algorithmic Fairness for Human-AI Collaboration
arxiv_id: '2310.03647'
source_url: https://arxiv.org/abs/2310.03647
tags:
- u1d70b
- u1d465
- u1d44e
- u1d43b
- u1d434
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the notion of "compliance-robust fairness"
  for human-AI collaboration, addressing the issue that traditional algorithmic fairness
  does not account for human selective compliance with algorithmic recommendations.
  The authors propose an optimization strategy to identify the best performance-improving
  compliance-robustly fair policy.
---

# Rethinking Algorithmic Fairness for Human-AI Collaboration

## Quick Facts
- arXiv ID: 2310.03647
- Source URL: https://arxiv.org/abs/2310.03647
- Reference count: 3
- Primary result: Traditional algorithmic fairness and compliance-robust fairness may be incompatible in human-AI collaboration settings.

## Executive Summary
This paper introduces "compliance-robust fairness" for human-AI collaboration, addressing a critical gap in traditional algorithmic fairness approaches. When humans selectively comply with algorithmic recommendations, fairness guarantees that hold in isolation may fail to materialize in practice. The authors develop a framework to identify policies that are guaranteed to improve fairness regardless of the human's compliance pattern, while also seeking to improve decision accuracy. However, they demonstrate that it may be impossible to simultaneously achieve traditional fairness, compliance-robust fairness, and performance improvement over the human policy. This finding raises fundamental questions about whether traditional fairness constraints should be enforced when the goal is to improve both equity and accuracy in human-AI decision-making systems.

## Method Summary
The paper develops a framework for human-AI collaboration where the human policy is known from historical data, and the algorithmic policy is designed to provide recommendations. The key innovation is the concept of compliance-robust fairness, which ensures that the final human-AI policy is at least as fair as the human-only policy for any possible compliance pattern. The method involves characterizing compliance-robustly fair policies through optimization constraints that "sandwich" algorithmic recommendations between the human policy's decisions for each group. The authors then develop an optimization strategy to find the best-performing compliance-robustly fair policy, and illustrate their approach using Virginia criminal sentencing data before and after algorithmic risk assessment tool implementation.

## Key Results
- Compliance-robustly fair policies are guaranteed to (weakly) improve fairness regardless of human compliance patterns.
- It may be infeasible to design algorithmic recommendations that are simultaneously fair in isolation, compliance-robustly fair, and more accurate than the human policy.
- Traditional fairness constraints may not be necessary or desirable if the goal is to improve both equity and accuracy in human-AI collaboration.
- The tension between traditional and compliance-robust fairness is prevalent across a broad class of fairness definitions including demographic parity and calibration within groups.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compliance-robust fairness guarantees that outcomes will be (weakly) fairer than the human-only policy regardless of the human's compliance pattern.
- Mechanism: The policy design ensures that for any compliance function, the resulting human-AI policy never increases the unfairness gap between protected groups compared to the human-only policy. This is achieved by sandwiching the algorithmic recommendations between the human policy's recommendations for each group.
- Core assumption: The human policy is known and the distribution of types, attributes, and outcomes covers all possible combinations (Assumption 1).
- Evidence anchors:
  - [abstract]: "We define the notion of compliance-robustly fair algorithmic recommendations that are guaranteed to (weakly) improve fairness in decisions, regardless of the human's (unknown) compliance pattern."
  - [section 3]: "Given /u1D70B/u1D43B, an algorithm policy /u1D70B/u1D434 is compliance-robustly fair iﬀ /u1D6FC( /u1D70B/u1D434 ) ≤ /u1D6FC( /u1D70B/u1D43B ) and /u1D70B/u1D43B ( /u1D465,0) ≤ /u1D70B/u1D434 ( /u1D465,0) and /u1D70B/u1D434 ( /u1D465,1) ≤ /u1D70B/u1D43B ( /u1D465,1)."
  - [corpus]: Weak - corpus neighbors focus on general fairness concepts but don't specifically address compliance-robustness.
- Break condition: If the human policy itself is perfectly fair (/u1D6FC( /u1D70B/u1D43B ) = 0), then no nontrivial compliance-robustly fair policy exists (Corollary 3.2).

### Mechanism 2
- Claim: Traditional algorithmic fairness and compliance-robust fairness are often incompatible when the goal is to improve both equity and accuracy.
- Mechanism: A policy that is fair in isolation may actually reduce fairness when combined with selective human compliance, while a compliance-robust policy may need to deviate from fairness in isolation to ensure fairness in the final outcome.
- Core assumption: The optimal policy without fairness constraints (/u1D70B∗) is not itself perfectly fair.
- Evidence anchors:
  - [abstract]: "we show that it may be infeasible to design algorithmic recommendations that are simultaneously fair in isolation, compliance-robustly fair, and more accurate than the human policy."
  - [section 5]: "Unfortunately, it unlikely that an unconstrained performance-maximizing policy will be inherently fair; this insight has been the driving force of the algorithmic fairness literature."
  - [corpus]: Weak - corpus neighbors discuss fairness but don't explore the tension between traditional and compliance-robust fairness.
- Break condition: If the data-generating process itself is perfectly fair, then the optimal policy /u1D70B∗ will be fair and traditional fairness can be maintained (Theorem 5.2).

### Mechanism 3
- Claim: The tension between traditional fairness and compliance-robust fairness is prevalent across a broad class of fairness definitions.
- Mechanism: The authors show that for demographic parity and calibration within groups, fair policies are not necessarily compliance-robustly fair, meaning one must optimize separately for performance-improving compliance-robustly fair policies.
- Core assumption: The fairness condition satisfies Assumption 4, which allows construction of specific counterexamples.
- Evidence anchors:
  - [section 6]: "we now define a general class of fairness criteria, subsuming demographic parity and calibration with in groups. We then show that, under this general class, fair policies are not necessarily compliance-robustly fair."
  - [section 6]: "For any fairness condition /u1D711 satisfying Assumption 4, there exist policies /u1D70B0, /u1D70B, and /u1D70B, and a compliance function /u1D4500, such that the human-AI policy satisfies /u1D711 ( /u1D70B′ /u1D436 ) < /u1D711 ( /u1D70B)."
  - [corpus]: Weak - corpus neighbors don't specifically address this generality across fairness definitions.
- Break condition: If the fairness condition doesn't satisfy Assumption 4, the specific counterexample construction may not apply.

## Foundational Learning

- Concept: Bernoulli distribution for decision making
  - Why needed here: The paper uses Bernoulli distributions to model decisions where /u1D70B( /u1D465, /u1D44E) represents the probability of making a positive decision for an individual with type /u1D465 and protected attribute /u1D44E.
  - Quick check question: In the paper's framework, if /u1D70B( /u1D465, /u1D44E) = 0.7, what is the probability that the decision ˆ/u1D466.alt= 1?

- Concept: Expected loss minimization
  - Why needed here: The paper defines performance in terms of expected loss E[ℓ( /u1D70B( /u1D465, /u1D44E) , /u1D466.alt)] and seeks to minimize this loss while satisfying fairness constraints.
  - Quick check question: According to Assumption 3, if policy /u1D70B′ always deviates farther from the optimal policy /u1D70B∗ than policy /u1D70B, what can we conclude about their expected losses?

- Concept: Group fairness metrics (equality of opportunity)
  - Why needed here: The paper primarily uses equality of opportunity fairness, requiring P[/u1D70B= 1 | /u1D466.alt= 1, /u1D44E = 0] = P[/u1D70B= 1 | /u1D466.alt= 1, /u1D44E = 1] for any chosen decision policy /u1D70B.
  - Quick check question: If Group 1 has a true positive rate of 0.8 and Group 0 has a true positive rate of 0.6, what is the value of /u1D6FC( /u1D70B)?

## Architecture Onboarding

- Component map: Human policy (/u1D70B/u1D43B) -> Algorithmic policy (/u1D70B/u1D434) -> Compliance function (/u1D450) -> Human-AI policy -> Outcomes
- Critical path: 1) Estimate human policy /u1D70B/u1D43B from historical data 2) Compute optimal policy /u1D70B∗ without fairness constraints 3) Construct policy /u1D70B/u1D435 as upper bound on compliance-robustly fair performance 4) Solve optimization problem to find best-performing compliance-robustly fair policy /u1D70B0 5) Validate that /u1D43F ( /u1D70B0) < /u1D43F ( /u1D70B/u1D43B ) to ensure performance improvement
- Design tradeoffs: Traditional fairness vs. compliance-robust fairness: Enforcing traditional fairness may prevent finding compliance-robust policies that actually improve outcomes; Performance vs. fairness: Stricter fairness constraints typically reduce achievable performance; Model complexity: More complex loss functions or fairness metrics may improve real-world applicability but increase computational complexity
- Failure signatures: If /u1D6FC( /u1D70B0) = 0 but /u1D43F ( /u1D70B0) > /u1D43F ( /u1D70B/u1D43B ): Traditional fairness is achievable but at the cost of performance; If no policy satisfies all constraints: The fairness-accuracy tradeoff is too severe for the given problem; If /u1D6FC( /u1D70B/u1D435 ) > /u1D6FC( /u1D70B/u1D43B ): The upper bound on performance is not compliance-robustly fair, requiring more sophisticated optimization
- First 3 experiments: 1) Verify Theorem 3.1 by constructing a simple example (e.g., X = {1}, two groups) where a policy satisfies the sandwich conditions and confirming it's compliance-robustly fair 2) Test Theorem 4.4 by generating random human policies and checking when the optimal compliance-robust policy improves performance 3) Demonstrate the tension between traditional and compliance-robust fairness by finding an example where a fair policy in isolation reduces fairness under compliance, while an unfair-in-isolation policy improves fairness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop algorithmic policies that are both compliance-robustly fair and traditionally fair, while also improving performance over the human policy?
- Basis in paper: [explicit] The paper shows that it may be infeasible to design algorithmic recommendations that are simultaneously fair in isolation, compliance-robustly fair, and more accurate than the human policy.
- Why unresolved: The tension between traditional fairness and compliance-robust fairness remains unresolved, and the paper suggests that it may be preferable to prioritize compliance-robust fairness and performance improvement over traditional fairness in certain cases.
- What evidence would resolve it: Empirical studies on real-world human-AI collaboration scenarios that demonstrate the effectiveness of compliance-robustly fair policies in improving fairness and performance compared to traditional fairness constraints.

### Open Question 2
- Question: How does the human's compliance pattern affect the fairness of human-AI collaboration, and how can we predict or model this pattern?
- Basis in paper: [explicit] The paper highlights that selective compliance with fair algorithms can amplify discrimination relative to the prior human policy, and that ensuring equitable outcomes requires accounting for humans' complex and unexpected compliance patterns.
- Why unresolved: The human's compliance pattern is a priori unknown and may change over time, making it difficult to predict or model. Understanding the factors that influence compliance patterns is crucial for developing effective human-AI collaboration systems.
- What evidence would resolve it: Longitudinal studies on human-AI collaboration that track changes in compliance patterns over time and identify the factors that influence these patterns, such as individual characteristics, task complexity, and algorithmic transparency.

### Open Question 3
- Question: How can we design algorithmic policies that are robust to adversarial compliance patterns, where humans may intentionally exploit the algorithm to achieve discriminatory outcomes?
- Basis in paper: [inferred] The paper mentions that fair algorithms cannot guard against humans from exploiting their recommendations in a discriminatory manner, and that it is often impossible to design an algorithmic policy that is both fair and robust to human exploitation.
- Why unresolved: The challenge of designing algorithms that are robust to adversarial compliance patterns remains an open problem, as it requires understanding the incentives and strategies that humans may employ to exploit the algorithm.
- What evidence would resolve it: Experiments that test the robustness of algorithmic policies against various forms of adversarial compliance, such as compliance that is driven by conflicting objectives between humans and algorithms or by explicit attempts to manipulate the algorithm's recommendations.

## Limitations

- The framework relies heavily on knowing the human policy from historical data, which may not be available in many real-world settings.
- The analysis assumes a single type variable X, while many applications involve high-dimensional feature spaces that could affect the feasibility of compliance-robust fairness.
- The Virginia criminal sentencing case study provides empirical support but lacks detailed methodological description, making independent verification difficult.

## Confidence

- **High confidence**: The theoretical characterization of compliance-robustly fair policies (Theorem 3.1) and the core insight that traditional and compliance-robust fairness can be incompatible
- **Medium confidence**: The general claim that fair policies are not necessarily compliance-robustly fair across all fairness definitions, as the proof relies on specific counterexample constructions
- **Low confidence**: The practical significance of the Virginia case study due to limited methodological details

## Next Checks

1. Construct simple synthetic datasets with known human policies to empirically verify that compliance-robustly fair policies always improve fairness regardless of compliance patterns
2. Test the framework with multiple fairness definitions beyond equality of opportunity to confirm the general incompatibility result
3. Analyze sensitivity to estimation errors in the human policy /u1D70B/u1D43B to understand robustness in practical applications