---
ver: rpa2
title: A Unified Transformer-based Network for multimodal Emotion Recognition
arxiv_id: '2308.14160'
source_url: https://arxiv.org/abs/2308.14160
tags:
- emotion
- recognition
- signal
- signals
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel Unified Biosensor-Vision Multimodal
  Transformer-based (UBVMT) method for emotion recognition by combining 2D representations
  of ECG/PPG signals with facial expression data. The approach first compares three
  different image-based representations of ECG/PPG signals (spatiotemporal maps, time-frequency
  representations, and scalograms) for unimodal emotion recognition and finds scalograms
  to be most effective.
---

# A Unified Transformer-based Network for multimodal Emotion Recognition

## Quick Facts
- arXiv ID: 2308.14160
- Source URL: https://arxiv.org/abs/2308.14160
- Authors: 
- Reference count: 40
- This paper introduces a novel Unified Biosensor-Vision Multimodal Transformer-based (UBVMT) method for emotion recognition by combining 2D representations of ECG/PPG signals with facial expression data.

## Executive Summary
This paper introduces a novel Unified Biosensor-Vision Multimodal Transformer-based (UBVMT) method for emotion recognition by combining 2D representations of ECG/PPG signals with facial expression data. The approach first compares three different image-based representations of ECG/PPG signals (spatiotemporal maps, time-frequency representations, and scalograms) for unimodal emotion recognition and finds scalograms to be most effective. The UBVMT model employs homogeneous transformer blocks that learn joint emotion representations from both modalities with minimal modality-specific design. The model is trained using masked autoencoding and contrastive modeling objectives. Experimental results on the MAHNOB-HCI and DEAP datasets show that the proposed method achieves comparable performance to state-of-the-art techniques.

## Method Summary
The proposed UBVMT method converts 1D ECG/PPG signals into 2D scalogram representations using continuous wavelet transform, which are then combined with facial expression features extracted from synchronized video frames. The unified transformer architecture with homogeneous blocks processes both modalities through shared weights, learning joint emotion representations. The model is pretrained using masked autoencoding to reconstruct missing patches and contrastive modeling to align the two modalities, then fine-tuned for emotion classification in arousal-valence space. The approach is evaluated on MAHNOB-HCI and DEAP datasets using 10-fold cross-validation.

## Key Results
- Scalogram representations of ECG/PPG signals outperform spatiotemporal maps and time-frequency representations for unimodal emotion recognition
- The UBVMT model achieves 83.84% accuracy for arousal recognition on MAHNOB-HCI and 82.64% accuracy on DEAP dataset
- The unified transformer architecture with minimal modality-specific design effectively combines ECG/PPG signals with facial expressions for emotion recognition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scalogram representations of ECG/PPG signals outperform spatiotemporal maps and time-frequency representations for unimodal emotion recognition.
- Mechanism: Scalograms provide better time-frequency resolution for low-frequency signals like ECG/PPG, allowing more effective extraction of emotion-related features.
- Core assumption: The low-frequency components of ECG/PPG signals contain significant emotion-related information that is better captured by scalograms than other representations.
- Evidence anchors:
  - [abstract] "We first investigate and compare the unimodal emotion recognition performance of three image-based representations of the ECG/PPG signal. We then present our UBVMT network which is trained to perform emotion recognition by combining the 2D image-based representation of the ECG/PPG signal and the facial expression features."
  - [section] "The best emotion recognition performance is obtained by employing the scalogram representation of the ECG and PPG signals."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.424, average citations=0.0. Top related titles: Beyond Single-Channel: Multichannel Signal Imaging for PPG-to-ECG Reconstruction with Vision Transformers, Transformer-based Self-supervised Multimodal Representation Learning for Wearable Emotion Recognition, QualityFM: a Multimodal Physiological Signal Foundation Model with Self-Distillation for Signal Quality Challenges in Critically Ill Patients."
- Break condition: If the scalogram representation does not consistently outperform other representations across different datasets or if the emotion recognition performance degrades significantly.

### Mechanism 2
- Claim: The unified transformer architecture with minimal modality-specific design enables effective multimodal emotion representation learning.
- Mechanism: The homogeneous transformer blocks process both ECG/PPG and face information through shared weights, allowing the model to learn joint representations without complex modality-specific adaptations.
- Core assumption: The emotional content in ECG/PPG signals and facial expressions can be effectively aligned and fused through a unified transformer architecture.
- Evidence anchors:
  - [abstract] "Our unified transformer model consists of homogeneous transformer blocks that take as an input the 2D representation of the ECG/PPG signal and the corresponding face frame for emotion representation learning with minimal modality-specific design."
  - [section] "Our Unified Biosensor-Vision Multimodal Transformer (UBVMT) network is trained and validated by using the scalogram representation of the ECG/PPG signal along with the face images."
  - [corpus] "Weak evidence. The corpus mentions related transformer-based methods but does not specifically address unified architectures for biosensor-vision fusion."
- Break condition: If the model fails to effectively align ECG/PPG and face information or if performance significantly degrades compared to modality-specific approaches.

### Mechanism 3
- Claim: Masked autoencoding and contrastive modeling objectives enable effective pretraining of the transformer for multimodal emotion recognition.
- Mechanism: Masked autoencoding reconstructs missing patches of ECG/PPG and face data, while contrastive modeling aligns the two modalities, creating robust representations for downstream emotion classification.
- Core assumption: The pretraining objectives can effectively learn representations that capture the relationship between ECG/PPG signals and facial expressions for emotion recognition.
- Evidence anchors:
  - [abstract] "Our UBVMT model is trained by reconstructing masked patches of video frames and 2D images of ECG/PPG signals, and contrastive modeling to align face and ECG/PPG data."
  - [section] "UBVMT is pre-trained by employing masked autoencoding [63] and contrastive modeling [22] to learn effective emotion representation."
  - [corpus] "Weak evidence. The corpus mentions related self-supervised learning methods but does not specifically address masked autoencoding and contrastive modeling for emotion recognition."
- Break condition: If the pretraining objectives do not lead to improved downstream emotion recognition performance or if the model fails to learn effective cross-modal representations.

## Foundational Learning

- Concept: Time-frequency analysis techniques (STFT, Wigner-Ville distribution, SPWVD)
  - Why needed here: These techniques are used to convert 1D ECG/PPG signals into 2D representations for input to the transformer model.
  - Quick check question: What is the main advantage of SPWVD over STFT for time-frequency representation of ECG/PPG signals?

- Concept: Continuous Wavelet Transform and scalogram generation
  - Why needed here: Scalograms are identified as the most effective 2D representation for ECG/PPG signals in this emotion recognition task.
  - Quick check question: How do scalograms differ from spectrograms in terms of frequency resolution and highlighting of signal components?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The core of the UBVMT model is a transformer that processes both ECG/PPG and face information.
  - Quick check question: How does the multi-head attention mechanism in transformers allow for learning complex relationships between different input modalities?

## Architecture Onboarding

- Component map: 1D ECG/PPG signals -> 2D scalogram representations -> Modality embeddings -> Encoder (12 layers) -> Decoder (8 layers) -> MLP -> Emotion classification

- Critical path:
  1. Convert 1D ECG/PPG signals to 2D scalogram representations
  2. Extract face frames from synchronized video
  3. Apply modality embeddings to create patch embeddings
  4. Process through encoder using masked autoencoding and contrastive objectives
  5. Fine-tune encoder with MLP for emotion classification

- Design tradeoffs:
  - Unified transformer vs. modality-specific networks: Simpler architecture but may limit specialized processing
  - Pretraining on large dataset vs. direct fine-tuning: Better generalization but requires more computational resources
  - Scalogram representation vs. other 2D representations: Better time-frequency resolution but potentially more complex computation

- Failure signatures:
  - Poor alignment between ECG/PPG and face information
  - Overfitting to specific datasets or subjects
  - Inability to generalize to new subjects or recording conditions
  - Degradation in performance when using different 2D representations of ECG/PPG signals

- First 3 experiments:
  1. Compare emotion recognition performance using different 2D representations (spatiotemporal maps, time-frequency, scalograms) of ECG/PPG signals
  2. Evaluate the effectiveness of masked autoencoding vs. contrastive modeling in pretraining
  3. Assess the impact of different masking ratios (e.g., 50%, 75%, 90%) on the performance of masked autoencoding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the UBVMT model's performance compare when using different types of ECG signals (e.g., from different leads or wearable sensors) rather than the standard ECG/PPG signals used in the study?
- Basis in paper: [explicit] The paper mentions that the proposed method combines a 2D representation of an ECG/PPG signal with facial information, but does not explore the use of different ECG signal types.
- Why unresolved: The paper does not investigate the impact of using different ECG signal types on the model's performance.
- What evidence would resolve it: Conducting experiments using various ECG signal types and comparing the results with the current method would provide insights into the model's robustness and generalizability.

### Open Question 2
- Question: Can the UBVMT model be adapted to work with other physiological signals beyond ECG/PPG, such as EEG or GSR, and how would this affect its performance?
- Basis in paper: [inferred] The paper focuses on ECG/PPG signals and does not explore the use of other physiological signals, leaving open the question of the model's adaptability.
- Why unresolved: The paper does not investigate the potential of using other physiological signals in the UBVMT model.
- What evidence would resolve it: Experimenting with different physiological signals and evaluating the model's performance would determine its adaptability and effectiveness with other modalities.

### Open Question 3
- Question: How does the UBVMT model's performance scale with the size of the training dataset, particularly when using larger and more diverse datasets?
- Basis in paper: [explicit] The paper mentions that transformer models improve with larger datasets, but does not explore the impact of dataset size on the UBVMT model's performance.
- Why unresolved: The paper does not investigate the relationship between dataset size and the model's performance.
- What evidence would resolve it: Conducting experiments with varying dataset sizes and analyzing the model's performance would provide insights into its scalability and potential limitations.

## Limitations

- The study focuses on specific datasets (MAHNOB-HCI and DEAP) with relatively small sample sizes (27 and 22 subjects respectively), limiting generalizability to broader populations
- The effectiveness of scalograms over other 2D representations is established only within the context of these specific emotion recognition tasks and may not generalize to other physiological signal analysis applications
- The unified transformer architecture, while simpler, may not capture modality-specific nuances as effectively as specialized networks for each input type

## Confidence

- High confidence: The comparative analysis of different 2D representations (scalograms outperforming spatiotemporal maps and time-frequency representations) is well-supported by experimental results
- Medium confidence: The unified transformer architecture's effectiveness in multimodal fusion is demonstrated but could benefit from comparison with modality-specific alternatives
- Medium confidence: The pretraining approach using masked autoencoding and contrastive modeling shows promise but lacks extensive ablation studies to isolate the contribution of each component

## Next Checks

1. Evaluate the UBVMT model on additional emotion recognition datasets with larger and more diverse participant pools to assess generalizability
2. Conduct ablation studies to quantify the individual contributions of masked autoencoding and contrastive modeling objectives to the final performance
3. Compare the unified transformer architecture against modality-specific networks (separate ECG/PPG and face networks) to validate the architectural choice