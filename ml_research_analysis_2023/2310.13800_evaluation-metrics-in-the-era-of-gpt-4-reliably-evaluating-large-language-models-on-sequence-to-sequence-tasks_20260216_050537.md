---
ver: rpa2
title: 'Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language
  Models on Sequence to Sequence Tasks'
arxiv_id: '2310.13800'
source_url: https://arxiv.org/abs/2310.13800
tags:
- evaluation
- human
- chatgpt
- gpt-4
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates the performance of various large language
  models (LLMs) on three NLP tasks: text summarization, text simplification, and grammatical
  error correction. The evaluation is conducted using a combination of automatic metrics,
  human evaluation, and GPT-4 as an evaluator.'
---

# Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks

## Quick Facts
- arXiv ID: 2310.13800
- Source URL: https://arxiv.org/abs/2310.13800
- Reference count: 38
- Primary result: Traditional automatic metrics poorly predict human judgment on LLM outputs, while GPT-4 shows reasonable alignment with human evaluation despite task-specific variations.

## Executive Summary
This study evaluates large language models on three NLP tasks using automatic metrics, human evaluation, and GPT-4 as an evaluator. The findings reveal that traditional reference-based metrics fail to capture the quality of modern LLM outputs, with human reviewers consistently rating gold references lower than model-generated outputs. GPT-4 demonstrates promising alignment with human judgment for ranking model outputs, particularly for text summarization and simplification tasks, though with some task-specific variations. The study highlights the limitations of current evaluation frameworks and suggests GPT-4 could serve as a more reliable evaluation proxy than traditional metrics.

## Method Summary
The study evaluates six large language models (Flan-T5, T0pp, OPT-IML, Flan-UL2, GPT-3, InstructGPT, ChatGPT) on three sequence-to-sequence tasks: text summarization, text simplification, and grammatical error correction. Models are evaluated using zero-shot prompting with various temperature settings and prompts. Outputs are assessed using automatic metrics (SARI, ROUGE, F0.5), human evaluation by three reviewers on five criteria per task, and GPT-4 evaluation using structured prompts. The evaluation uses standard datasets (Newsela for simplification, CNN/DailyMail for summarization, BEA-2019 for GEC) and compares rankings across different evaluation methods.

## Key Results
- Traditional reference-based automatic metrics show poor correlation with human judgment, with some metrics even showing negative correlation
- Human reviewers consistently rated gold references as worse than the best models' outputs across all tasks
- GPT-4 demonstrated reasonable alignment with human evaluation rankings (correlation 0.33-0.82) when evaluating model outputs, with better performance on summarization and simplification than GEC
- The same prompt format worked well across different models and temperature settings, suggesting prompt quality is relatively model-invariant

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reference-based automatic metrics are inadequate for evaluating LLM outputs because they rely on outdated gold references that are of lower quality than model-generated outputs.
- Mechanism: When models generate outputs that are semantically equivalent or better than the reference but differ in wording, automatic metrics penalize them due to low n-gram overlap, even though humans would rate them highly.
- Core assumption: The quality of reference data in standard benchmarks is worse than what current LLMs can produce.
- Evidence anchors:
  - [abstract] "human reviewers rate the gold reference as much worse than the best models' outputs, indicating the poor quality of many popular benchmarks."
  - [section 4.2] "Interestingly, all human reviewers scored the gold reference summaries as the worst on all metrics."
  - [corpus] Weak - no direct corpus support for this claim.
- Break condition: If new high-quality benchmark datasets are created that are consistently better than LLM outputs, this mechanism would no longer apply.

### Mechanism 2
- Claim: GPT-4 can act as a reasonable proxy for human evaluation in ranking model outputs, with task-specific variations in alignment.
- Mechanism: GPT-4's reasoning capabilities allow it to evaluate outputs based on semantic quality and coherence rather than just surface-level matching, aligning more closely with human judgment than traditional metrics.
- Core assumption: GPT-4's evaluation criteria, when properly prompted, align sufficiently with human judgment to provide useful rankings.
- Evidence anchors:
  - [abstract] "GPT-4 is capable of ranking models' outputs in a way which aligns reasonably closely to human judgement despite task-specific variations."
  - [section 4.2] "For both text summarisation and simplification, GPT-4 used as a reviewer produced surprisingly good results which correlate well, albeit not perfectly, with human reviewers."
  - [corpus] Weak - no direct corpus support for this claim.
- Break condition: If GPT-4's evaluation criteria drift significantly from human judgment or if it develops systematic biases, this mechanism would break down.

### Mechanism 3
- Claim: The same prompt works well across different models and temperature settings, indicating prompt quality is relatively model-invariant.
- Mechanism: A well-crafted prompt captures the essential task requirements in a way that is understood similarly by different models, reducing the need for extensive prompt tuning.
- Core assumption: The task definitions in the prompts are clear and unambiguous enough to be interpreted consistently across different model architectures.
- Evidence anchors:
  - [section 4.1] "We also observed that for each task, the same prompt seemed to perform best for all models and temperature settings, with only one exception, suggesting that the quality of prompts is almost model-invariant."
  - [section 4.1] "See Appendix D for more details."
  - [corpus] Weak - no direct corpus support for this claim.
- Break condition: If model architectures diverge significantly in their interpretation of task instructions, this mechanism would fail.

## Foundational Learning

- Concept: Evaluation metrics correlation
  - Why needed here: Understanding how well automatic metrics correlate with human judgment is crucial for interpreting evaluation results
  - Quick check question: What does a high correlation coefficient between automatic metrics and human evaluation indicate about the metric's reliability?

- Concept: Zero-shot prompting
  - Why needed here: The study evaluates models without fine-tuning, relying entirely on prompt engineering
  - Quick check question: What are the key differences between zero-shot and few-shot prompting in terms of task performance?

- Concept: Inter-annotator agreement
  - Why needed here: The study uses human evaluation and needs to assess the consistency between different reviewers
  - Quick check question: How is Krippendorff's alpha interpreted in terms of inter-annotator agreement strength?

## Architecture Onboarding

- Component map: Model inference pipeline -> Output collection -> Evaluation framework (automatic metrics, human evaluation, GPT-4 evaluation) -> Results aggregation and analysis
- Critical path: Model inference → Output collection → Evaluation (automatic metrics first, then human/GPT-4 evaluation) → Results aggregation and analysis
- Design tradeoffs: The study prioritizes breadth (multiple tasks, models, and evaluation methods) over depth (limited prompt tuning, small human evaluation subset). This allows for comprehensive landscape mapping but may miss optimal configurations.
- Failure signatures: Poor correlation between automatic metrics and human evaluation, inconsistent rankings between human and GPT-4 evaluators, or model outputs that consistently fail to improve upon gold references.
- First 3 experiments:
  1. Run the same model across different temperature settings on a small subset to verify prompt invariance
  2. Compare automatic metric scores with human evaluations on the same subset to establish baseline correlation
  3. Test GPT-4 evaluation on the same subset to assess alignment with human judgments before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on tasks beyond summarization, simplification, and GEC when evaluated using GPT-4 as an evaluator?
- Basis in paper: [explicit] The paper mentions that GPT-4 showed strong alignment with human judgment for summarization and simplification tasks but moderate alignment for GEC. However, it does not explore other NLP tasks.
- Why unresolved: The study only evaluated three specific tasks, leaving the generalizability of GPT-4's evaluation capabilities to other tasks unclear.
- What evidence would resolve it: Conducting similar evaluations on a broader range of NLP tasks (e.g., machine translation, question answering, dialogue generation) using GPT-4 as an evaluator and comparing its alignment with human judgment across these tasks.

### Open Question 2
- Question: What are the specific limitations of automatic evaluation metrics that cause them to misalign with human judgment, and can these limitations be addressed?
- Basis in paper: [explicit] The paper highlights that traditional reference-based metrics are inadequate at predicting or replacing human judgment, but it does not delve into the specific reasons for this misalignment.
- Why unresolved: The study identifies the problem but does not investigate the underlying causes or potential solutions for improving automatic metrics.
- What evidence would resolve it: Analyzing the outputs of LLMs and gold references to identify specific discrepancies that lead to poor automatic metric scores, and developing or testing new evaluation metrics that better capture human judgment.

### Open Question 3
- Question: How does the quality of prompts and prompt engineering impact the performance of LLMs and their evaluation by GPT-4?
- Basis in paper: [explicit] The paper mentions that only a limited amount of prompt tuning was carried out and that the quality of prompts might be almost model-invariant, but it does not explore this in depth.
- Why unresolved: The study acknowledges the potential importance of prompt engineering but does not investigate its impact on model performance and evaluation.
- What evidence would resolve it: Conducting a systematic study on the effects of different prompt engineering techniques (e.g., few-shot learning, chain-of-thought prompting) on LLM outputs and their subsequent evaluation by GPT-4, and comparing these results with human judgment.

## Limitations
- The study evaluated only three specific NLP tasks, limiting generalizability to other domains
- Human evaluation used only one annotation per output rather than multiple judgments, potentially underestimating true inter-annotator agreement
- The claim about prompt invariance was based on limited empirical testing with only four prompts per task

## Confidence

- Claims about automatic metrics inadequacy: **High**
- GPT-4 as human proxy: **Medium** (task-dependent)
- Prompt invariance: **Low-Medium**

## Next Checks

1. Test GPT-4 evaluation reliability by comparing rankings across multiple GPT-4 evaluations of the same outputs to measure consistency
2. Expand human evaluation to include multiple judgments per output to better estimate true inter-annotator agreement
3. Test the prompt invariance claim by systematically varying prompt styles across a broader range of model architectures