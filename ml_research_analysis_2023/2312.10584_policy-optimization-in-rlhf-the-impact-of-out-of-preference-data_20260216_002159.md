---
ver: rpa2
title: 'Policy Optimization in RLHF: The Impact of Out-of-preference Data'
arxiv_id: '2312.10584'
source_url: https://arxiv.org/abs/2312.10584
tags:
- data
- policy
- rmb-po
- optimization
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates policy optimization in reward-model-based
  alignment methods like RMB-PO and RMB-PO+, which use out-of-preference data for
  policy optimization, versus DPO, which uses only preference data. The key finding
  is that RMB-PO+ significantly outperforms DPO in contextual bandit tasks, even when
  the policy model has a good feature representation.
---

# Policy Optimization in RLHF: The Impact of Out-of-preference Data

## Quick Facts
- arXiv ID: 2312.10584
- Source URL: https://arxiv.org/abs/2312.10584
- Reference count: 6
- Key outcome: RMB-PO+ significantly outperforms DPO in contextual bandit tasks by leveraging out-of-preference data for better stochastic approximation of the reward maximization problem.

## Executive Summary
This paper investigates policy optimization methods in reward-model-based alignment (RMB-PO and RMB-PO+) versus direct preference optimization (DPO), focusing on their use of out-of-preference data. The key finding is that RMB-PO+ significantly outperforms DPO even when the policy model has good feature representation, by leveraging additional preference-free data to enable better stochastic approximation of the reward maximization problem. The results highlight the importance of policy optimization on out-of-preference data to unlock the reward model's generalization capabilities and achieve superior alignment performance.

## Method Summary
The paper compares three policy optimization methods in contextual bandit settings: DPO, which uses only preference data; RMB-PO, which incorporates policy-generated data; and RMB-PO+, which uses additional preference-free data. The methods are evaluated on both linear and neural contextual bandit tasks with varying feature representations between reward and policy models. The performance is measured by optimality gap between learned and optimal policies, using synthetic environments with Bradley-Terry-Luce preference models.

## Key Results
- RMB-PO+ consistently outperforms DPO in all tested scenarios, even with good feature representation
- Out-of-preference data significantly improves stochastic approximation of the reward maximization problem
- The performance improvement scales with the amount of additional preference-free data available

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Out-of-preference data improves stochastic approximation of the reward maximization problem
- Mechanism: Additional samples from policy-generated and preference-free data reduce variance in estimating the expectation over states and actions, leading to better policy optimization
- Core assumption: The reward model has generalization capability beyond preference data
- Evidence anchors:
  - [abstract] "RMB-PO+ improves performance by leveraging additional preference-free data, enabling better stochastic approximation of the reward maximization problem"
  - [section] "We argue that the policy optimization in these methods corresponds to different versions of stochastic approximation of the (expected) reward maximization problem"
- Break condition: If the reward model cannot generalize beyond preference data

### Mechanism 2
- Claim: Data augmentation effect improves state and action space coverage
- Mechanism: Additional data samples augment training data, improving stochastic approximation in both state space (RMB-PO+) and action space (RMB-PO)
- Core assumption: Additional data provides relevant coverage of the state-action space
- Evidence anchors:
  - [section] "Another viewpoint is that the RMB-PO and RMB-PO+ methods incorporate additional policy-generated data and preference-free data, essentially employing a form of data augmentation"
  - [section] Table 1 shows improvement in state and action space coverage
- Break condition: If additional data comes from irrelevant or harmful distributions

### Mechanism 3
- Claim: Policy optimization conflicts are resolved by using out-of-preference data
- Mechanism: Without additional data, policies may learn to maximize rewards in regions where the reward model is poorly calibrated, leading to conflicts
- Core assumption: Policy and reward models may develop different views of optimal actions in uncovered regions
- Evidence anchors:
  - [abstract] "otherwise, the policy may conflict with the learned reward model on out-of-preference data and suffer a poor performance"
  - [section] "even when the policy model is provided with a good feature, RMB-PO methods can benefit from out-of-preference data"
- Break condition: If the reward model perfectly generalizes from preference data alone

## Foundational Learning

- Concept: Stochastic approximation in optimization
  - Why needed here: The paper's core argument is that different methods perform different levels of stochastic approximation of the reward maximization problem
  - Quick check question: What is the difference between stochastic approximation and exact optimization, and why does this matter for policy optimization?

- Concept: Contextual bandits and policy optimization
  - Why needed here: The paper uses contextual bandit formulation to study alignment methods
  - Quick check question: How does the contextual bandit setup differ from standard RL, and why is it appropriate for studying preference-based alignment?

- Concept: Feature representation and generalization in ML models
  - Why needed here: The paper discusses scenarios with and without feature mismatch between reward and policy models
  - Quick check question: What happens when two models share the same feature representation versus when they have different representations?

## Architecture Onboarding

- Component map: Preference dataset -> Reward model -> Policy model -> Additional datasets (policy-generated for RMB-PO, preference-free for RMB-PO+)

- Critical path:
  1. Collect preference data
  2. Train reward model from preference data
  3. Optimize policy using reward model
  4. Evaluate policy performance

- Design tradeoffs:
  - DPO: Simpler, uses only preference data, but may suffer from poor generalization
  - RMB-PO: Uses policy-generated data, better stochastic approximation, but requires additional computation
  - RMB-PO+: Uses additional preference-free data, best performance, but requires more data collection

- Failure signatures:
  - Poor reward model training accuracy (below 60%)
  - Large optimality gap between learned policy and optimal policy
  - Policy performance degrades when using additional data (suggests data distribution issues)

- First 3 experiments:
  1. Linear bandit with matched features (ϕπ = ϕr): Verify that RMB-PO+ outperforms DPO even with good feature representation
  2. Linear bandit with mismatched features (ϕπ ≠ ϕr): Confirm that RMB-PO+ provides additional benefits when features differ
  3. Neural bandit with varying amounts of preference-free data: Test the scaling relationship between additional data and performance improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RMB-PO and RMB-PO+ scale with the size of the out-of-preference dataset Dnew?
- Basis in paper: [inferred] The paper shows that RMB-PO+ outperforms DPO and RMB-PO, but does not explore the relationship between the size of Dnew and performance
- Why unresolved: The paper only tests a limited number of Dnew sizes and does not investigate the scaling behavior
- What evidence would resolve it: Experiments testing a wider range of Dnew sizes and analyzing the performance scaling would help understand the relationship between Dnew size and performance

### Open Question 2
- Question: How do RMB-PO and RMB-PO+ perform on more complex tasks with larger action spaces, such as language models?
- Basis in paper: [explicit] The paper mentions that RMB-PO methods face challenges in large action spaces but do not explore this issue
- Why unresolved: The experiments conducted in the paper focus on simple contextual bandit tasks with small action spaces
- What evidence would resolve it: Experiments testing RMB-PO and RMB-PO+ on tasks with larger action spaces, such as language models, would help understand their performance in more complex settings

### Open Question 3
- Question: How do RMB-PO and RMB-PO+ compare to other alignment methods, such as reward-model-free approaches like IPO?
- Basis in paper: [inferred] The paper compares RMB-PO and RMB-PO+ to DPO but does not explore their performance relative to other alignment methods
- Why unresolved: The paper focuses on the comparison between DPO, RMB-PO, and RMB-PO+ but does not investigate their performance relative to other alignment methods
- What evidence would resolve it: Experiments comparing RMB-PO and RMB-PO+ to other alignment methods, such as IPO, would help understand their relative performance and strengths

## Limitations

- The theoretical claims about stochastic approximation mechanisms lack rigorous mathematical proof
- The experiments are limited to synthetic contextual bandit environments and may not generalize to more complex tasks
- The paper does not explore the scaling relationship between the amount of preference-free data and performance improvement

## Confidence

- High confidence in experimental methodology and reproducibility
- Medium confidence in the stochastic approximation mechanism explanation
- Medium confidence in the claim that RMB-PO+ outperforms DPO in all scenarios with good feature representation

## Next Checks

1. Test the methods on non-synthetic environments with more complex reward structures to verify generalization beyond the studied bandit setups
2. Perform ablation studies varying the amount of preference-free data to quantify the scaling relationship with performance improvement
3. Compare against alternative policy optimization methods that also use out-of-preference data to establish the relative advantage of RMB-PO+