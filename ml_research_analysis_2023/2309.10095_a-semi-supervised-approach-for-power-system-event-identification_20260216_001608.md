---
ver: rpa2
title: A Semi-Supervised Approach for Power System Event Identification
arxiv_id: '2309.10095'
source_url: https://arxiv.org/abs/2309.10095
tags:
- samples
- event
- labeled
- unlabeled
- identification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a semi-supervised learning approach for event
  identification in power systems using PMU data. The method leverages both labeled
  and unlabeled data to improve classification accuracy.
---

# A Semi-Supervised Approach for Power System Event Identification

## Quick Facts
- arXiv ID: 2309.10095
- Source URL: https://arxiv.org/abs/2309.10095
- Reference count: 40
- Primary result: Graph-based label spreading significantly outperforms self-training and TSVM for power system event identification using limited labeled PMU data

## Executive Summary
This paper addresses the challenge of event identification in power systems using PMU data through a semi-supervised learning approach. The authors generate synthetic PMU data for four event classes (load loss, generation loss, line trip, and bus fault) and extract physically interpretable features via modal analysis. Three semi-supervised methods are evaluated: self-training, transductive SVMs, and graph-based label spreading. The graph-based approach consistently demonstrates superior performance, particularly when labeled data is scarce, by effectively leveraging both labeled and unlabeled samples through a similarity-weighted propagation process.

## Method Summary
The method generates synthetic PMU data using PSS¬ÆE Python API for the South Carolina 500-Bus System, extracting features via modal analysis to obtain frequency, damping ratios, and residue coefficients. Three semi-supervised methods are implemented: self-training with various base classifiers, transductive SVMs, and graph-based label spreading. The framework uses k-fold cross-validation with limited labeled samples (as low as 5% of total samples) and evaluates performance using ROC-AUC scores. The graph-based label spreading method constructs a similarity graph where labels propagate from labeled to unlabeled nodes based on Euclidean distance, achieving the most robust performance across different evaluation metrics.

## Key Results
- Graph-based label spreading consistently outperforms self-training and TSVM across all evaluation metrics
- Performance improves significantly as the number of unlabeled samples increases
- Self-training with decision tree and gradient boosting base classifiers shows more robust performance than with SVM or kNN

## Why This Works (Mechanism)

### Mechanism 1
Graph-based Label Spreading (LS) outperforms self-training and TSVM by leveraging both labeled and unlabeled samples through a similarity-weighted propagation process. LS constructs a graph where nodes represent samples and edges carry weights based on Euclidean distance. Labels propagate from labeled to unlabeled nodes, balancing neighbor-derived information and original labels using parameter ùõº. The core assumption is that samples with similar features tend to belong to the same class (smoothness assumption). Break condition: If the initial labeled/unlabeled distribution is highly imbalanced or unrepresentative of true class distribution, LS may propagate incorrect labels.

### Mechanism 2
Self-training with decision trees or gradient boosting provides more robust pseudo-labeling than SVMs or kNN when labeled data is limited. Base classifiers learn from initial labeled set, assign pseudo-labels to unlabeled samples, and retrain iteratively with augmented data. DT/GB are less sensitive to initial sample distribution than SVMs or kNN. Core assumption: Initial labeled samples provide sufficient signal for base classifier to generate reliable pseudo-labels. Break condition: If pseudo-labels are assigned incorrectly early in the process, error accumulation degrades performance.

### Mechanism 3
Transductive SVM (TSVM) struggles when labeled/unlabeled sample distributions are unbalanced or complex. TSVM modifies SVM formulation to include unlabeled samples by minimizing misclassification error under two possible label assignments per unlabeled sample. Core assumption: The distribution of labeled and unlabeled samples reflects the true underlying class distribution. Break condition: Highly skewed or complex distributions cause TSVM to assign inaccurate pseudo labels.

## Foundational Learning

- Concept: Semi-supervised learning principles (smoothness, cluster assumptions)
  - Why needed here: The framework relies on these assumptions to justify using unlabeled samples effectively.
  - Quick check question: What happens to LS performance if the smoothness assumption is violated?

- Concept: Modal analysis for feature extraction from PMU data
  - Why needed here: Features extracted via modal analysis (frequencies, damping ratios, residues) are the inputs to all classification models.
  - Quick check question: How many dominant modes are assumed in the modal decomposition per PMU channel?

- Concept: k-fold cross-validation with random splits of labeled/unlabeled samples
  - Why needed here: Ensures robustness evaluation across different initial distributions of labeled/unlabeled data.
  - Quick check question: How many folds and random splits are used in the evaluation?

## Architecture Onboarding

- Component map: Data generation module (PSS¬ÆE Python API) ‚Üí PMU data matrices ‚Üí Feature extraction module (modal analysis) ‚Üí Feature vectors ‚Üí Semi-supervised models (self-training, TSVM, LS) ‚Üí Pseudo-labeled data ‚Üí Classifier training module (SVM, GB, DT, kNN) ‚Üí Event classification ‚Üí Evaluation module (ROC AUC, 5th/95th percentiles) ‚Üí Performance metrics

- Critical path: 1. Generate synthetic PMU data for events 2. Extract modal features 3. Split into labeled/unlabeled sets 4. Apply semi-supervised pseudo-labeling 5. Train classifier on augmented data 6. Evaluate on validation fold

- Design tradeoffs: LS vs. self-training: LS propagates labels through similarity graph but requires distance computation; self-training is simpler but more prone to error accumulation. Number of unlabeled samples: More improves LS performance but increases computation. Feature dimensionality: Higher dimensionality may improve accuracy but risks overfitting with limited labeled data.

- Failure signatures: Sharp drop in 5th percentile AUC scores indicates poor robustness to initial sample distribution. Consistent low AUC across all models suggests feature extraction or data quality issues. Degraded performance with more unlabeled samples in self-training indicates error accumulation.

- First 3 experiments: 1. Run with only labeled samples (no semi-supervision) to establish baseline. 2. Apply LS with small number of unlabeled samples and observe AUC improvement. 3. Compare self-training with DT vs. SVM base classifiers to confirm robustness differences.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of semi-supervised learning methods for event identification vary across different power system network topologies and sizes? Basis: The authors use the South Carolina 500-Bus System but do not explore generalization to other network topologies or sizes. Unresolved because the study focuses on a single network topology, limiting generalizability. Evidence needed: Experiments on various network topologies and sizes comparing semi-supervised learning performance.

### Open Question 2
How sensitive are the semi-supervised learning methods to the choice of hyperparameters, and what strategies can be employed to optimize these parameters for improved performance? Basis: The authors use grid search for hyperparameters but do not explore sensitivity or optimization strategies. Unresolved because hyperparameter sensitivity significantly impacts model performance in practical applications. Evidence needed: Sensitivity analysis on hyperparameters and comparison of different optimization strategies.

### Open Question 3
How do the semi-supervised learning methods perform in the presence of noisy or incomplete PMU data, and what preprocessing techniques can be employed to improve robustness? Basis: The authors assume clean PMU data but do not address real-world scenarios with noise or missing values. Unresolved because operational PMU data often contains noise or missing values due to various factors. Evidence needed: Evaluation on noisy/incomplete PMU data and exploration of preprocessing techniques.

## Limitations
- Performance relies on synthetic PMU data rather than real-world measurements, potentially missing operational noise characteristics
- Highly sensitive to initial distribution of labeled/unlabeled samples, particularly for self-training and TSVM approaches
- Does not address potential temporal dependencies in PMU data that could affect feature extraction and classification

## Confidence
- Graph-based label spreading performance claims: High confidence - Multiple evaluation metrics and consistent improvement across different base classifiers support this conclusion
- Self-training robustness claims: Medium confidence - Supported by comparative analysis but limited by potential bias in synthetic data generation
- TSVM performance limitations: Medium confidence - Based on empirical observations but could benefit from deeper theoretical analysis of failure modes

## Next Checks
1. Test the framework on real PMU data from operational power systems to verify performance in realistic conditions with actual noise patterns and event characteristics
2. Conduct sensitivity analysis varying the number of labeled samples (below 10%) to determine the practical limits of semi-supervised learning for event identification
3. Evaluate computational scalability by testing label spreading on larger PMU networks (beyond 95 buses) to assess performance degradation and runtime constraints