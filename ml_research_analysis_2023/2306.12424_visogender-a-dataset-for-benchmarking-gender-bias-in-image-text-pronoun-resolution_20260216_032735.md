---
ver: rpa2
title: 'VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution'
arxiv_id: '2306.12424'
source_url: https://arxiv.org/abs/2306.12424
tags:
- gender
- bias
- images
- resolution
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VisoGender, a new dataset for evaluating
  gender bias in vision-language models. Inspired by Winograd schemas, the dataset
  contains 690 images of people in professional roles, each paired with a caption
  containing a pronoun relationship (e.g., "the doctor and her patient").
---

# VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution

## Quick Facts
- arXiv ID: 2306.12424
- Source URL: https://arxiv.org/abs/2306.12424
- Authors: 
- Reference count: 40
- This paper introduces VisoGender, a new dataset for evaluating gender bias in vision-language models

## Executive Summary
This paper introduces VisoGender, a new dataset for evaluating gender bias in vision-language models. Inspired by Winograd schemas, the dataset contains 690 images of people in professional roles, each paired with a caption containing a pronoun relationship (e.g., "the doctor and her patient"). The dataset is balanced across genders and includes single-person and two-person images. The authors propose two tasks to measure bias: pronoun resolution (evaluating accuracy gaps between genders) and image retrieval (measuring representational fairness). They benchmark six CLIP-like models and two captioning models, finding that models struggle with pronoun resolution, especially in complex scenes with multiple people. Captioning models generally outperform CLIP models and show smaller gender gaps. The study highlights the need for improved visio-linguistic reasoning in vision-language models and provides a valuable benchmark for future research.

## Method Summary
The VisoGender dataset contains 690 images of people in professional roles, each annotated with ground truth gender labels. The dataset uses templated captions inspired by Winograd schemas to create pronoun relationships in the scene. Two tasks are proposed to measure bias: pronoun resolution (evaluating accuracy gaps between genders) and image retrieval (measuring representational fairness). The authors benchmark six CLIP-like models and two captioning models on these tasks, using metrics such as resolution accuracy, resolution bias (accuracy gap), and retrieval bias (Bias@K, MaxSkew@K, NDKL).

## Key Results
- VisoGender contains 690 balanced images across 23 occupations with pronoun-annotated captions
- Models struggle with pronoun resolution, especially in complex scenes with multiple people
- Captioning models generally outperform CLIP models and show smaller gender gaps in resolution accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VisoGender evaluates vision-language models for gender bias in pronoun resolution and image retrieval tasks.
- Mechanism: The dataset provides balanced images of professionals in different occupations, annotated with ground truth gender labels. Models are tested on their ability to correctly resolve gender pronouns in captions and retrieve gender-balanced images for neutral queries.
- Core assumption: Balanced representation of genders in occupations reduces spurious correlations and allows for fair bias measurement.
- Evidence anchors:
  - [abstract] "VisoGender is balanced by gender representation in professional roles, supporting bias evaluation in two ways: i) resolution bias... and ii) retrieval bias..."
  - [section] "VISO GENDER contains 690 images of people in various occupational settings, where each image is annotated for the inferred groundtruth gender of the subject(s) in the image."
  - [corpus] Weak - corpus neighbors focus on bias in VLMs but don't directly discuss VisoGender's dataset design.
- Break condition: If the dataset is not truly balanced across genders for each occupation, or if the annotations are unreliable, the bias measurements will be invalid.

### Mechanism 2
- Claim: VisoGender extends the Winograd schema framework to the vision-language domain to stress-test models' reasoning abilities.
- Mechanism: The dataset uses templated captions containing pronoun relationships, inspired by Winograd schemas, but adapted for images. Models must resolve the correct pronoun based on the visual context.
- Core assumption: The pronoun resolution task requires models to understand the relationship between subjects in the image and their corresponding pronouns in the caption.
- Evidence anchors:
  - [abstract] "Inspired by Winograd and Winogender schemas, where each image is associated with a caption containing a pronoun relationship of subjects and objects in the scene."
  - [section] "Our work is most similar to gender pronoun resolution tasks based on Winograd schemas [21], like Winogender[19] and WinoBias [20] which investigate occupational-related biases. We extend this work to the vision-language domain."
  - [corpus] Weak - corpus neighbors discuss bias in VLMs but don't specifically mention Winograd schemas.
- Break condition: If the pronoun relationships in the captions are not clear or unambiguous, or if the visual context does not provide enough information to resolve the pronouns, the stress-testing of models' reasoning abilities will be ineffective.

### Mechanism 3
- Claim: VisoGender provides a benchmark for evaluating the downstream harms of vision-language models before large-scale deployment.
- Mechanism: By measuring resolution and retrieval biases, VisoGender quantifies the potential for models to perpetuate or amplify societal biases in tasks like captioning and image retrieval.
- Core assumption: Biases in pronoun resolution and image retrieval can lead to representational and allocational harms, such as misgendering individuals or underrepresenting certain demographics.
- Evidence anchors:
  - [abstract] "We hope this work encourages the benchmarking of future VLMs, so the risk of downstream harms and negative biases can be measured, compared and mitigated."
  - [section] "How the VLM is used determines the mechanisms of bias transfer from pre-training to risks of representational and/or allocational harm [7]."
  - [corpus] Weak - corpus neighbors discuss bias in VLMs but don't specifically mention downstream harms or the need for pre-deployment benchmarking.
- Break condition: If the measured biases do not actually lead to harmful outcomes in real-world applications, or if the benchmark does not accurately predict the potential for harm, its value for pre-deployment evaluation is limited.

## Foundational Learning

- Concept: Vision-Language Models (VLMs)
  - Why needed here: VisoGender is designed to benchmark VLMs for gender bias, so understanding how VLMs work is crucial for interpreting the results and implications.
  - Quick check question: What are the two main types of VLMs evaluated in VisoGender, and how do they differ in their approach to pronoun resolution and image retrieval?

- Concept: Gender Bias
  - Why needed here: VisoGender specifically focuses on measuring gender bias in VLMs, so a clear understanding of what constitutes gender bias and its potential impacts is necessary.
  - Quick check question: What are the two types of bias measured by VisoGender, and how do they relate to representational and allocational harms?

- Concept: Winograd Schemas
  - Why needed here: VisoGender is inspired by Winograd schemas, so familiarity with this framework is important for understanding the dataset's design and purpose.
  - Quick check question: How does VisoGender adapt the Winograd schema framework to the vision-language domain, and what specific aspect of models' reasoning abilities does it aim to stress-test?

## Architecture Onboarding

- Component map:
  - Dataset: 690 images of professionals in 23 occupations, balanced by gender
  - Annotations: Ground truth gender labels for subjects in each image
  - Templates: Captions containing pronoun relationships, based on Winograd schemas
  - Tasks: Pronoun resolution (single image to multiple captions) and image retrieval (single caption to multiple images)
  - Models: CLIP-like models and captioning models
  - Metrics: Resolution accuracy, resolution bias (accuracy gap), retrieval bias (Bias@K, MaxSkew@K, NDKL)

- Critical path:
  1. Curate balanced dataset of professional images with ground truth gender annotations
  2. Construct templated captions with pronoun relationships
  3. Implement pronoun resolution and image retrieval tasks
  4. Evaluate models on tasks and compute bias metrics
  5. Analyze results to identify biases and areas for improvement

- Design tradeoffs:
  - Dataset size vs. balance: Larger datasets may be harder to balance across genders and occupations
  - Template complexity vs. clarity: More complex templates may better stress-test models' reasoning, but could also introduce ambiguity
  - Task difficulty vs. model capabilities: Tasks should be challenging enough to reveal biases, but not so difficult that models consistently fail

- Failure signatures:
  - High resolution accuracy but large accuracy gap between genders indicates capability bias
  - Low overall resolution accuracy indicates insufficient visio-linguistic reasoning abilities
  - Retrieval bias metrics significantly different from zero indicate representational bias
  - Correlation between model bias and societal occupational skew suggests biases are reflecting real-world disparities

- First 3 experiments:
  1. Evaluate a CLIP model on the pronoun resolution task and compute resolution accuracy and bias metrics
  2. Evaluate a captioning model on the pronoun resolution task and compare results to the CLIP model
  3. Evaluate a CLIP model on the image retrieval task and compute retrieval bias metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of vision-language models on the VisoGender dataset correlate with their performance on other benchmarks like Winoground or COCO?
- Basis in paper: [explicit] The paper mentions that VisoGender is inspired by Winoground and Winogender but applies it to the vision-language domain. It also mentions that models struggle with pronoun resolution, especially in complex scenes with multiple people.
- Why unresolved: The paper does not provide a direct comparison between VisoGender and other benchmarks like Winoground or COCO.
- What evidence would resolve it: A comparative study of model performance on VisoGender and other relevant benchmarks would provide insights into the strengths and weaknesses of different evaluation datasets.

### Open Question 2
- Question: What are the specific biases present in the images and captions used in the VisoGender dataset, and how do they impact the evaluation results?
- Basis in paper: [explicit] The paper acknowledges that the dataset is based on images from the internet, which may contain inherent biases. It also mentions that the dataset is balanced by gender representation in professional roles.
- Why unresolved: The paper does not provide a detailed analysis of the specific biases present in the images and captions.
- What evidence would resolve it: A thorough analysis of the images and captions, including an examination of potential biases in terms of race, ethnicity, age, and other factors, would help understand their impact on the evaluation results.

### Open Question 3
- Question: How can the VisoGender dataset be extended to include more diverse gender identities and occupations, and what are the challenges in doing so?
- Basis in paper: [explicit] The paper mentions that the dataset is designed to be flexible to include neopronouns in the future and acknowledges the limitations of binary gender classification.
- Why unresolved: The paper does not provide specific strategies or discuss the challenges in expanding the dataset to include more diverse gender identities and occupations.
- What evidence would resolve it: A discussion of potential approaches to expand the dataset, along with an analysis of the challenges and trade-offs involved, would provide valuable insights for future research.

## Limitations
- The dataset is limited to binary gender classification, which may not capture the full spectrum of gender identities.
- The pronoun resolution task relies on templated captions, which may not fully reflect the complexity of real-world pronoun usage.
- The evaluation focuses on vision-language models, but other types of models (e.g., text-only or image-only) may also exhibit gender bias.

## Confidence

### High Confidence
- The dataset construction and task design are well-documented and follow established best practices for bias evaluation.

### Medium Confidence
- The results and conclusions are generally supported by the data, but some details are missing that would strengthen the analysis.

### Low Confidence
- The lack of implementation specifications and confidence label information limits the reproducibility and interpretability of the results.

## Next Checks
1. Implement the pronoun resolution task with both zero-shot classification and next-token prediction approaches, and compare the results to ensure consistency.
2. Analyze the confidence distributions of the models' predictions to identify potential reliability issues or patterns in uncertainty.
3. Conduct additional failure mode analysis to identify confounding factors, such as image quality or caption complexity, that may influence the models' performance and bias measurements.