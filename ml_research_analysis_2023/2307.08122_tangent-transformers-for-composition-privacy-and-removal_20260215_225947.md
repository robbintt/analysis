---
ver: rpa2
title: Tangent Transformers for Composition, Privacy and Removal
arxiv_id: '2307.08122'
source_url: https://arxiv.org/abs/2307.08122
tags:
- fine-tuning
- training
- tangent
- show
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Tangent Attention Fine-Tuning (TAFT), a method
  for fine-tuning linearized transformers by computing a first-order Taylor expansion
  around pre-trained weights. The Jacobian-Vector Product (JVP) resulting from linearization
  is computed efficiently in a single forward pass, reducing training and inference
  cost to the same order of magnitude as the original non-linear transformer while
  using the same number of parameters.
---

# Tangent Transformers for Composition, Privacy and Removal

## Quick Facts
- arXiv ID: 2307.08122
- Source URL: https://arxiv.org/abs/2307.08122
- Reference count: 40
- Primary result: Linearized transformers via first-order Taylor expansion achieve comparable fine-tuning performance to non-linear models while enabling efficient composition, privacy, and data removal.

## Executive Summary
Tangent Transformers for Composition, Privacy and Removal (TAFT) introduces a method for fine-tuning linearized transformers by computing a first-order Taylor expansion around pre-trained weights. The key innovation is an efficient computation of the Jacobian-Vector Product (JVP) through directional derivatives in a single forward pass, making training and inference as efficient as the original non-linear transformer. This linearization approach enables several advantages: constant-cost model composition through weight averaging, stronger privacy guarantees for differential privacy mechanisms, and efficient data removal by simply subtracting weight perturbations. When applied to visual classification tasks, TAFT performs comparably to traditional fine-tuning while offering these additional benefits.

## Method Summary
TAFT works by linearizing a pre-trained transformer around its weights using first-order Taylor expansion, creating a model of the form f(x) + ∇f(x)·∆w where only the perturbation ∆w is learned during fine-tuning. The critical computational insight is that the Jacobian-Vector Product can be computed efficiently through directional derivatives in a single modified forward pass, avoiding expensive gradient computations. Training uses a Rescaled Square Loss with L2 regularization, and the resulting convex optimization problem enables stronger theoretical guarantees for privacy and data removal. During inference, the linearized model is evaluated with the combined weights w+∆w using the same efficient forward pass.

## Key Results
- TAFT achieves comparable accuracy to non-linear fine-tuning on vision tasks (Caltech-256, MIT-67, Oxford Pets, Stanford Dogs, CUB-200, FGVC-Aircrafts, Stanford Cars)
- Model composition through weight averaging achieves constant inference cost versus linear scaling for output ensembling
- Data removal becomes a simple subtraction of weight perturbations rather than full retraining
- DP-SGD on the convex TAFT loss achieves better utility-privacy trade-offs than non-convex fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The first-order Taylor expansion around pre-trained weights yields a linearized model whose Jacobian-Vector Product (JVP) can be computed efficiently in a single forward pass.
- Mechanism: The linearization approximates the original transformer as a linear operator in the space of weight perturbations (∆w), where the JVP term ∇_w f_w(x) · ∆w is computed via directional derivatives instead of explicit gradient computation.
- Core assumption: The directional derivative formulation allows closed-form expressions for the linearized versions of attention, normalization, and fully-connected layers that can be evaluated in a single modified forward pass.
- Evidence anchors:
  - [abstract]: "We show that the Jacobian-Vector Product resulting from linearization can be computed efficiently in a single forward pass, reducing training and inference cost to the same order of magnitude as its original non-linear counterpart."
  - [section 3.1]: "By computing the directional derivative, we can derive closed form equations for the linearized versions of the key building blocks of transformer networks...computed in a single modified forward pass."
  - [corpus]: Weak - corpus neighbors discuss related topics (model composition, unlearning) but do not provide direct computational evidence for the JVP efficiency claim.
- Break condition: The first-order Taylor approximation becomes invalid when weight perturbations ∆w are large, causing the linearization to diverge significantly from the original non-linear behavior.

### Mechanism 2
- Claim: Linearity in weight space enables theoretical equivalence between weight averaging and output ensembling, allowing efficient model composition with constant inference cost.
- Mechanism: Since the linearized model f_lin_w(x) = f_w(x) + ∇_w f_w(x) · ∆w is linear in ∆w, composing N models through weight averaging (Σ λ_i ∆w_i) produces outputs equivalent to ensembling their individual outputs, but with only one forward pass required.
- Core assumption: The superposition principle holds for linear operators, allowing linear combinations of weight perturbations to produce corresponding linear combinations of outputs.
- Evidence anchors:
  - [abstract]: "Linearity yields equivalence between composition in weight space and composition in activations, i.e., ensembling."
  - [section 3.2]: "Given N models linearized about pre-trained weights w and a query x, the ensemble of these models...is equivalent to evaluating a single tangent model composed by taking the same linear combination of component models in weight space."
  - [corpus]: Moderate - corpus neighbors discuss model composition techniques, but TAFT provides the specific theoretical guarantee of constant inference cost through linearity.
- Break condition: When models are trained on disjoint datasets with high variance in learned representations, simple weight averaging may not capture the optimal combination of features, reducing the effectiveness of the equivalence.

### Mechanism 3
- Claim: The convex nature of the fine-tuning loss for linearized models enables stronger privacy guarantees and deterministic data removal capabilities.
- Mechanism: Since the linearized model's loss function becomes convex quadratic (ridge regression), differential privacy mechanisms designed for convex optimization (like DP-SGD) achieve better utility-privacy trade-offs, and data removal becomes a simple subtraction of weight perturbations.
- Core assumption: Convex optimization problems admit stronger theoretical guarantees for differential privacy and data removal compared to non-convex problems.
- Evidence anchors:
  - [abstract]: "Since Tangent Transformers are linear with respect to the new set of weights, and the resulting fine-tuning loss is convex, we show that TAFT enjoys several advantages...when it comes to...differential privacy."
  - [section 3.4]: "Most theoretical results as well as practical methods concerning Differential Privacy...provide much better utility-privacy trade-offs when the optimization problem being solved is convex."
  - [corpus]: Weak - corpus neighbors discuss privacy mechanisms but do not specifically address the convexity advantage for linearized models.
- Break condition: If the pre-trained initialization point is poorly chosen relative to the target task, the convex loss may still converge to suboptimal solutions, limiting the privacy and removal benefits.

## Foundational Learning

- Concept: First-order Taylor expansion and linearization of non-linear functions
  - Why needed here: The entire TAFT method relies on approximating the non-linear transformer with its first-order Taylor expansion around pre-trained weights, making this mathematical foundation essential.
  - Quick check question: What is the mathematical condition under which a first-order Taylor expansion provides a good approximation of a function?

- Concept: Jacobian-Vector Product (JVP) computation and directional derivatives
  - Why needed here: Efficient JVP computation via directional derivatives is the key computational innovation that makes TAFT practical, replacing expensive gradient computations with single forward passes.
  - Quick check question: How does computing a directional derivative differ from computing a full gradient, and why is it more efficient in this context?

- Concept: Convex optimization and its relationship to differential privacy
  - Why needed here: The convex nature of the linearized fine-tuning loss enables stronger theoretical guarantees for differential privacy and data removal mechanisms that are designed for convex problems.
  - Quick check question: Why do differential privacy mechanisms typically achieve better utility-privacy trade-offs when applied to convex optimization problems versus non-convex ones?

## Architecture Onboarding

- Component map:
  Pre-trained transformer backbone (e.g., ViT-L/16) -> Linearization layer that computes JVP during forward pass -> Tangent weight parameters (∆w) -> Loss function (typically Rescaled Square Loss with L2 regularization) -> Optional initialization strategies (resetting later layers or CLS token)

- Critical path:
  1. Load pre-trained weights w
  2. For each input, compute forward pass with JVP tracking
  3. Compute loss using linearized outputs
  4. Backpropagate through linearized layers only (w remains fixed)
  5. Update ∆w parameters
  6. During inference, compute outputs using w + ∆w with modified forward pass

- Design tradeoffs:
  - Expressiveness vs. efficiency: Linearized models are less expressive but much more efficient to train and compose
  - Initialization sensitivity: Performance depends heavily on the choice of pre-trained weights and which layers to linearize
  - Task proximity: TAFT works best on tasks close to the pre-training distribution; distant tasks may require additional techniques

- Failure signatures:
  - Large degradation on tasks far from pre-training distribution
  - Instability when weight perturbations ∆w become too large
  - Poor performance when using standard cross-entropy loss instead of Rescaled Square Loss
  - Ineffectiveness of simple weight averaging when models are trained on highly diverse or disjoint datasets

- First 3 experiments:
  1. Fine-tune a linearized ViT on a dataset close to ImageNet (e.g., MIT-67) and compare accuracy to non-linear fine-tuning
  2. Compose two TAFT models trained on disjoint shards of the same dataset and verify constant inference cost vs. output ensembling
  3. Implement data removal by subtracting the weight perturbations of a shard and measure accuracy degradation compared to full retraining

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the maximum distance from pre-trained weights that linearization remains a valid approximation for Tangent Transformers, and how does this vary across different vision tasks?
- Basis in paper: [explicit] The paper states that "how good a local approximation of the network about initial weights w depends on the 'distance' the network moves from its initial point" and mentions that divergence increases for tasks far from ImageNet pre-training.
- Why unresolved: The paper doesn't provide quantitative analysis of the approximation error as a function of distance from initialization across different datasets or define clear thresholds for when linearization breaks down.
- What evidence would resolve it: Empirical studies measuring approximation error (e.g., MSE between linearized and original model outputs) as a function of L2 distance from initialization, across multiple vision datasets with varying similarity to ImageNet.

### Open Question 2
- Question: How does the choice of linearization point (e.g., full pre-trained weights vs. re-initialized layers) affect downstream task performance across the full spectrum of task distances from ImageNet?
- Basis in paper: [explicit] The paper shows that re-initializing the last attention block improves performance on some downstream tasks, particularly those "far" from ImageNet, but doesn't systematically characterize this relationship.
- Why unresolved: The paper only demonstrates this effect on a few datasets and doesn't establish a general framework for selecting optimal linearization points based on task similarity to pre-training.
- What evidence would resolve it: A comprehensive study mapping task distance from ImageNet (using established metrics) to optimal linearization strategies, with ablation studies showing performance as a function of which layers are re-initialized.

### Open Question 3
- Question: What are the theoretical guarantees for differential privacy when using Tangent Transformers with DP-SGD, and how do these compare to non-convex fine-tuning under the same privacy budget?
- Basis in paper: [explicit] The paper mentions that "convex models have much better convergence and utility guarantees with trained differentially private convex optimization algorithms" and shows empirical results, but doesn't provide theoretical analysis.
- Why unresolved: The paper only provides empirical comparisons of utility under different privacy budgets without theoretical analysis of privacy guarantees (e.g., Renyi DP bounds) or convergence rates for the convex optimization problem.
- What evidence would resolve it: Theoretical analysis deriving privacy amplification bounds and convergence rates for Tangent Transformer fine-tuning with DP-SGD, compared to analogous bounds for non-convex fine-tuning under the same assumptions.

## Limitations

- The linearization approximation becomes increasingly inaccurate as weight perturbations grow large, limiting effectiveness on tasks far from the pre-training distribution
- The theoretical privacy guarantees, while promising, lack comprehensive empirical validation across different privacy budgets and threat models
- The method's benefits for model composition are reduced when component models are trained on highly diverse or disjoint datasets with high variance in learned representations

## Confidence

- **High Confidence**: The mathematical framework for linearization and JVP computation is well-established and correctly applied. The theoretical advantages for compositionality and constant inference cost are rigorously proven.
- **Medium Confidence**: The empirical performance claims on downstream vision tasks are supported by experiments, but the results may be sensitive to hyperparameter choices and initialization strategies not fully explored in the paper.
- **Low Confidence**: The claims about differential privacy and data removal benefits are theoretically grounded but lack extensive empirical validation, particularly regarding the trade-offs in practical utility-privacy scenarios.

## Next Checks

1. **Cross-Domain Generalization Test**: Fine-tune a TAFT model on a language task (e.g., GLUE benchmark) and compare performance to non-linear fine-tuning, specifically examining whether the linearization assumption holds for transformer-based language models.

2. **Privacy Budget Analysis**: Implement the differential privacy mechanism for TAFT and empirically measure the achieved (ε,δ)-privacy guarantees on a standard dataset (e.g., CIFAR-10), comparing the utility-privacy trade-off against non-linear fine-tuning with DP-SGD.

3. **JVP Implementation Verification**: Reimplement the efficient JVP computation using only the mathematical formulation provided in the paper, without access to the original code, and benchmark against the claimed single-forward-pass efficiency on a standard transformer architecture.