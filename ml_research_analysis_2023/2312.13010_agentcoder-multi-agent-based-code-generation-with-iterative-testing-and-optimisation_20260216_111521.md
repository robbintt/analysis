---
ver: rpa2
title: 'AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation'
arxiv_id: '2312.13010'
source_url: https://arxiv.org/abs/2312.13010
tags:
- test
- code
- agent
- agentcoder
- elements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AgentCoder is a multi-agent framework for code generation that
  improves upon existing large language model (LLM) approaches by separating the tasks
  of code generation, test case generation, and test execution into three specialized
  agents. The programmer agent generates code based on requirements, the test designer
  agent generates comprehensive test cases, and the test executor agent runs the code
  against the tests and provides feedback for iterative refinement.
---

# AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation

## Quick Facts
- arXiv ID: 2312.13010
- Source URL: https://arxiv.org/abs/2312.13010
- Reference count: 19
- Key outcome: AgentCoder achieves 96.3% and 91.8% pass@1 accuracy on HumanEval and MBPP datasets respectively, outperforming state-of-the-art methods while using fewer tokens.

## Executive Summary
AgentCoder introduces a three-agent framework that separates code generation, test case generation, and test execution into specialized components, achieving superior performance on code generation benchmarks compared to single-agent approaches. By using GPT-4 and GPT-3.5 with a modular design, the system generates code that passes comprehensive test suites with 96.3% accuracy on HumanEval and 91.8% on MBPP, significantly outperforming existing methods like CodeCoT. The iterative refinement process, driven by detailed test feedback, enables progressive code improvement while maintaining efficiency through fewer total tokens.

## Method Summary
AgentCoder implements a three-agent system where a programmer agent generates code, a test designer agent creates comprehensive test cases, and a test executor agent runs the code and provides feedback for iterative refinement. The programmer agent uses Chain-of-Thought reasoning to break down coding tasks systematically, while the test designer agent generates basic, edge, and large-scale test cases. The test executor agent runs the code locally and communicates error feedback back to the programmer agent, which then refines the code through up to five iterations. This modular approach separates concerns and enables specialized focus for each agent, improving both code quality and test reliability compared to integrated approaches.

## Key Results
- Achieves 96.3% pass@1 accuracy on HumanEval dataset, outperforming CodeCoT (90.2%) and other baselines
- Achieves 91.8% pass@1 accuracy on MBPP dataset, outperforming CodeCoT (78.9%) and other baselines
- Demonstrates superior code coverage (84.7/87.5% vs 74.7/77.2% for CodeCoT) while using fewer total tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular agent separation improves both code quality and test reliability
- Mechanism: Specialized agents focus on distinct tasks (code generation, test case generation, test execution) without cognitive trade-offs present in single-agent approaches
- Core assumption: Programmer agent's focus on code generation doesn't interfere with test designer agent's ability to generate comprehensive, unbiased test cases
- Evidence anchors:
  - [abstract] "This modular design enables more thorough testing and iterative improvement compared to single-agent or integrated approaches"
  - [section] "The programmer agent will focus on the code generation and refinement based on the test executor agent's feedback"
- Break condition: Test designer agent generates biased test cases too similar to code patterns

### Mechanism 2
- Claim: Iterative refinement with test feedback creates progressive improvement
- Mechanism: Test executor agent provides concrete feedback about code failures, which programmer agent uses to iteratively refine code until tests pass
- Core assumption: Programmer agent can effectively interpret and act on test feedback to improve code quality
- Evidence anchors:
  - [abstract] "The test executor agent will run the code with the test cases and write the feedback to the programmer"
  - [section] "The iteration then continues, with the programmer agent regenerating code snippets to address the issues identified in the feedback"
- Break condition: Test feedback is ambiguous or misleading

### Mechanism 3
- Claim: Comprehensive test case generation improves code coverage and reliability
- Mechanism: Test designer agent generates basic, edge, and large-scale test cases ensuring thorough evaluation across diverse scenarios
- Core assumption: Test designer agent can generate test cases that adequately cover both common and edge scenarios
- Evidence anchors:
  - [section] "We carefully designed the prompts for the test designer agent to satisfy the following three expectations: 1) to generate basic test cases, 2) to cover edge test cases, and 3) to cover large-size inputs"
  - [section] "AgentCoder obtains 84.7 / 87.5% and 85.3 / 89.5% code coverage compared with CodeCoT"
- Break condition: Test designer agent fails to generate edge cases or test cases don't align with actual code behavior

## Foundational Learning

- Concept: Chain-of-Thought reasoning for code generation
  - Why needed here: Enables systematic breakdown of complex coding tasks into manageable steps (problem understanding, algorithm selection, pseudocode creation, code generation)
  - Quick check question: How does Chain-of-Thought approach differ from direct code generation in terms of error prevention?

- Concept: Test-driven development principles
  - Why needed here: Framework relies on test cases to validate and iteratively improve code, similar to how developers write tests alongside code
  - Quick check question: Why is it beneficial to separate test generation from code generation in this framework?

- Concept: Multi-agent system coordination
  - Why needed here: Agents must communicate effectively through well-defined interfaces to achieve collaborative goals
  - Quick check question: What communication protocol ensures test executor agent can accurately interpret and relay error information?

## Architecture Onboarding

- Component map: Programmer agent (code generation) -> Test designer agent (test case generation) -> Test executor agent (test execution and feedback)
- Critical path: Code generation → Test case generation → Code execution → Feedback collection → Code refinement (iterative)
- Design tradeoffs: Modularity vs. communication overhead, comprehensive testing vs. iteration budget, specialized agents vs. unified agent complexity
- Failure signatures: Test designer bias (tests too similar to code), programmer misinterpretation of feedback, executor script failures, excessive iteration cycles
- First 3 experiments:
  1. Single-agent baseline: Programmer agent generates code and test cases together, measure pass@1 and test accuracy
  2. Two-agent validation: Programmer + test executor agent, compare against single-agent performance
  3. Full three-agent implementation: Measure pass@1 improvement and test coverage across HumanEval and MBPP datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the modular design of AgentCoder affect its scalability when integrating newer, more advanced LLMs as they become available?
- Basis in paper: [explicit] The paper mentions that "its modular design provides the flexibility and scalability crucial for adapting to technological advancements" and that "Agents within AgentCoder can be individually updated or replaced with more sophisticated models."
- Why unresolved: The paper does not provide empirical data or case studies demonstrating how AgentCoder's modular design has been or could be scaled with new models.
- What evidence would resolve it: Empirical studies showing AgentCoder's performance with various LLMs over time, or a detailed analysis of the integration process and any limitations encountered when updating individual agents.

### Open Question 2
- Question: What is the impact of the iterative refinement process on the overall time and computational resources required for code generation in AgentCoder?
- Basis in paper: [inferred] The paper discusses iterative refinement and mentions "code refinement iterations" but does not quantify the impact on time or resources.
- Why unresolved: The paper lacks detailed metrics on the computational overhead or time taken for each iteration, which is crucial for understanding the practical efficiency of the system.
- What evidence would resolve it: Performance benchmarks comparing the time and computational resources used by AgentCoder with different numbers of iterations, and a cost-benefit analysis of the iterative process.

### Open Question 3
- Question: How does AgentCoder handle complex dependencies and interactions between different code modules or functions?
- Basis in paper: [inferred] The paper focuses on individual function generation and testing but does not address how AgentCoder manages multi-module or interdependent code scenarios.
- Why unresolved: The evaluation primarily focuses on single functions, leaving uncertainty about AgentCoder's effectiveness in more complex, real-world software development scenarios involving multiple interacting components.
- What evidence would resolve it: Case studies or experiments demonstrating AgentCoder's performance on projects with multiple interdependent modules, or an analysis of how the framework adapts to handle complex codebases.

## Limitations
- Exact prompt engineering templates and communication protocols between agents are not fully specified, making exact reproduction challenging
- Reported improvements depend on implementation details that are not transparent
- Modular approach introduces communication overhead that may not scale efficiently to more complex coding tasks

## Confidence
- High confidence: The modular agent architecture concept and its potential benefits for code quality and test reliability are well-supported by experimental results
- Medium confidence: The iterative refinement mechanism's effectiveness depends on quality of test feedback interpretation, which is not fully validated
- Low confidence: Specific implementation details for agent coordination and exact prompt templates are not sufficiently detailed for reliable replication

## Next Checks
1. Implement and compare single-agent baseline where programmer agent generates both code and test cases together, measuring pass@1 and test accuracy to quantify the benefit of separation
2. Validate the test designer agent's ability to generate edge cases by manually reviewing test cases from both AgentCoder and baseline approaches for coverage and bias
3. Measure communication overhead and iteration efficiency by tracking token usage and refinement cycles, comparing against the claimed efficiency benefits