---
ver: rpa2
title: Probing the Moral Development of Large Language Models through Defining Issues
  Test
arxiv_id: '2309.13356'
source_url: https://arxiv.org/abs/2309.13356
tags:
- moral
- language
- ethical
- development
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using the Defining Issues Test (DIT) as a psychometric
  tool to evaluate the moral development of large language models (LLMs). DIT measures
  moral reasoning stages using moral dilemmas followed by ethical considerations.
---

# Probing the Moral Development of Large Language Models through Defining Issues Test

## Quick Facts
- arXiv ID: 2309.13356
- Source URL: https://arxiv.org/abs/2309.13356
- Reference count: 40
- Models like GPT-4 and ChatGPT achieve post-conventional moral reasoning scores comparable to adult humans, while older models like GPT-3 perform at random baseline levels

## Executive Summary
This paper evaluates the moral reasoning capabilities of large language models using the Defining Issues Test (DIT), a psychometric tool that measures moral development through principled selection of ethical considerations rather than binary right/wrong answers. The study finds that newer RLHF-tuned models (GPT-4, ChatGPT, Llama2-Chat, PaLM-2) achieve significantly higher post-conventional moral reasoning scores comparable to adult humans, while older models like GPT-3 perform no better than random guessing. However, models show inconsistent performance across different dilemmas, revealing gaps in their moral reasoning abilities. The work highlights both the potential of using psychological assessment tools for AI evaluation and the limitations of current models in handling diverse moral contexts.

## Method Summary
The study applies the Defining Issues Test (DIT) framework to evaluate LLM moral reasoning by presenting moral dilemmas followed by 12 ethical considerations mapped to Kohlberg's stages of cognitive moral development. LLMs answer three sequential questions: (1) how the dilemma should be resolved, (2) rate the importance of each ethical consideration, and (3) rank-order the four most important considerations. A principled score (P-score) is calculated based on points assigned to statements aligned with post-conventional moral reasoning stages (Stages 5-6). The researchers tested 10 LLMs including GPT-3, GPT-4, ChatGPT, Llama2-Chat, and PaLM-2 using 9 moral dilemmas (6 from DIT-1, 4 newly curated), comparing performance against random baseline and human benchmarks.

## Key Results
- GPT-4 achieved the highest post-conventional moral reasoning score, equivalent to graduate students
- GPT-3 performed at random baseline levels (P-score of 29.13), while newer models significantly outperformed baseline
- Models show inconsistent performance across different dilemmas, revealing gaps in moral reasoning abilities
- Newer models like ChatGPT and GPT-4 demonstrate moral reasoning capabilities comparable to adult humans

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The DIT framework captures moral reasoning stages via principled selection of ethical considerations, not binary right/wrong answers
- **Mechanism:** The test presents a moral dilemma followed by 12 statements linked to Kohlberg's stages. Subjects rate and rank statements by importance, producing a P-score that reflects emphasis on post-conventional (Stages 5-6) moral reasoning
- **Core assumption:** Ranking reveals the subject's cognitive moral framework more accurately than forced-choice judgments
- **Evidence anchors:**
  - [abstract] "DIT uses moral dilemmas followed by a set of ethical considerations that the respondent has to judge for importance in resolving the dilemma, and then rank-order them by importance"
  - [section 3.3] "P-score... is determined by assigning points to the four most pivotal statements chosen by the individual, aligning with Stages 5 and 6 of Cognitive Moral Development"
  - [corpus] Weak: corpus shows similar works but lacks direct validation of ranking vs. binary methods
- **Break condition:** If subjects (or models) choose statements randomly, P-score collapses to baseline (~29.13 for GPT-3), showing no moral reasoning

### Mechanism 2
- **Claim:** Moral reasoning in LLMs emerges from fine-tuning with human feedback (RLHF), not from pre-training alone
- **Mechanism:** Pre-trained models like GPT-3 show random or near-random P-scores, while RLHF-tuned models (ChatGPT, GPT-4) exhibit structured moral reasoning comparable to adult humans
- **Core assumption:** Human feedback during fine-tuning embeds culturally relevant moral priorities into model outputs
- **Evidence anchors:**
  - [section 5.1.1] "GPT3... shows that GPT3 lacks the ability to comprehend the ethical implications... only answered the second question... overall pscore obtained for GPT3 to be 29.13 which is nearly equivalent to the random baseline"
  - [section 5.2] "Text-davinci-003, ChatGPT (both v1 & v2), GPT-4... achieve significantly better score than a random baseline"
  - [corpus] Moderate: neighboring papers cite alignment and RLHF as drivers of moral performance, but empirical cross-model comparison is sparse
- **Break condition:** If RLHF is removed or replaced with unsupervised fine-tuning, moral reasoning scores drop to baseline levels

### Mechanism 3
- **Claim:** Models show culturally bounded moral reasoning, performing inconsistently across diverse dilemmas
- **Mechanism:** While models score well on average, their P-scores vary dramatically across individual dilemmas, indicating gaps in cross-cultural moral generalization
- **Core assumption:** Models trained predominantly on English/web data lack robust moral reasoning in non-Western contexts unless explicitly exposed
- **Evidence anchors:**
  - [abstract] "models do not perform consistently across all dilemmas, revealing gaps in their understanding and reasoning abilities"
  - [section 4.1] "We do not use the Doctor's dilemma... because of the model developer safety policy"
  - [corpus] Weak: corpus neighbors highlight cross-linguistic misalignment but lack direct empirical support from this paper
- **Break condition:** If all dilemmas are drawn from a single cultural context, inconsistency disappears but generalizability suffers

## Foundational Learning

- **Concept:** Cognitive Moral Development (CMD) stages (pre-conventional, conventional, post-conventional)
  - **Why needed here:** DIT maps statements to these stages; understanding them is essential to interpret P-scores and stage-wise results
  - **Quick check question:** What distinguishes a Stage 5 from a Stage 4 moral consideration in DIT?

- **Concept:** Psychometric assessment vs. binary classification
  - **Why needed here:** DIT's rating/ranking method avoids oversimplification inherent in binary ethical datasets
  - **Quick check question:** How does ranking importance differ from selecting a single "correct" answer in evaluating moral reasoning?

- **Concept:** Reinforcement Learning from Human Feedback (RLHF)
  - **Why needed here:** The performance jump from GPT-3 to GPT-4/ChatGPT is attributed to RLHF fine-tuning
  - **Quick check question:** Why might RLHF improve moral reasoning but also introduce cultural bias?

## Architecture Onboarding

- **Component map:** Prompt → Model output → Statement rating/ranking extraction → P-score computation → Stage-wise aggregation → Baseline comparison
- **Critical path:** Dilemma → 12 statements → Question 2 (rating) → Question 3 (ranking) → P-score calculation
- **Design tradeoffs:** Fixed 12-statement format limits adaptability; prompt reordering mitigates positional bias but increases experiment complexity
- **Failure signatures:** Random statement selection → P-score ≈ baseline; non-responses to question 1 → incomplete evaluation; safety filters → missing dilemmas
- **First 3 experiments:**
  1. Run base prompt on GPT-4; verify non-random P-score > baseline
  2. Shuffle statements and options; confirm consistency across orderings
  3. Test Llama-70b-chat; compare P-score to GPT-3 baseline

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we effectively mitigate cultural bias in psychometric tools like DIT when applied to LLMs?
- **Basis in paper:** [explicit] The paper discusses cultural bias as a criticism of DIT, noting that it was developed in a Western context and may not be relevant for non-Western cultures
- **Why unresolved:** The authors mention curating new dilemmas to cover non-Western scenarios, but do not provide a comprehensive solution to address cultural bias in the DIT framework itself
- **What evidence would resolve it:** A study comparing LLM performance on DIT dilemmas from various cultural contexts, demonstrating consistent performance across diverse scenarios

### Open Question 2
- **Question:** What are the long-term implications of RLHF training on LLM moral reasoning abilities?
- **Basis in paper:** [explicit] The paper notes that ChatGPT's newer version performs worse than its older version, possibly due to frequent RLHF training, which may constrain reasoning capabilities
- **Why unresolved:** The paper does not explore the long-term effects of RLHF training on LLM moral reasoning in depth, nor does it propose alternative training methods
- **What evidence would resolve it:** Longitudinal studies comparing LLM moral reasoning performance before and after multiple RLHF iterations, alongside evaluations using different training techniques

### Open Question 3
- **Question:** How can we develop a universal framework for assessing LLM ethical reasoning that accounts for value pluralism?
- **Basis in paper:** [explicit] The paper emphasizes the challenge of value pluralism, stating that a universal set of values is impractical and that models should be adaptable to diverse circumstances
- **Why unresolved:** The authors propose using DIT as a starting point but do not provide a comprehensive framework that fully addresses the complexities of value pluralism in LLM ethical reasoning
- **What evidence would resolve it:** A proposed framework that incorporates diverse cultural and ethical perspectives, validated through testing on a wide range of LLM models and ethical scenarios

## Limitations

- The study uses a limited set of 9 moral dilemmas, with 4 newly curated dilemmas lacking detailed documentation of their statement-to-stage mappings
- Prompt engineering varies across models with different statement orderings and option labelings, but exact methodology is not fully specified
- The DIT framework itself has been criticized for cultural bias, being developed in a Western context that may not generalize to non-Western moral frameworks

## Confidence

- **High confidence**: The core finding that GPT-4 and ChatGPT achieve significantly higher P-scores than GPT-3, demonstrating that RLHF fine-tuning improves moral reasoning performance
- **Medium confidence**: The claim that newer models reach post-conventional reasoning levels comparable to graduate students, given the limited sample size of tested models
- **Medium confidence**: The observation of inconsistent performance across different dilemmas, though the underlying causes (cultural bias vs. domain specificity) remain unclear

## Next Checks

1. **Cross-cultural validation**: Test models on dilemmas translated into non-English languages to assess whether moral reasoning scores remain consistent across cultural contexts
2. **Prompt robustness analysis**: Systematically vary prompt ordering and formatting to quantify the impact of prompt engineering on P-score variability
3. **Stage-specific breakdown**: Examine which specific ethical considerations models consistently misidentify to identify systematic reasoning gaps at each developmental stage