---
ver: rpa2
title: 'Getting pwn''d by AI: Penetration Testing with Large Language Models'
arxiv_id: '2308.00121'
source_url: https://arxiv.org/abs/2308.00121
tags:
- penetration
- testing
- gpt3
- llms
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates using large language models (LLMs) like
  GPT-3.5 as AI sparring partners to augment penetration testers. Two use cases are
  explored: high-level task planning for security testing and low-level vulnerability
  hunting within a vulnerable VM.'
---

# Getting pwn'd by AI: Penetration Testing with Large Language Models

## Quick Facts
- arXiv ID: 2308.00121
- Source URL: https://arxiv.org/abs/2308.00121
- Reference count: 40
- One-line primary result: GPT-3.5 can suggest realistic attack methods and successfully gain root privileges through privilege escalation vulnerabilities in a closed-feedback loop penetration testing system.

## Executive Summary
This paper explores using large language models as AI sparring partners to augment penetration testers, investigating two use cases: high-level task planning for security testing and low-level vulnerability hunting within a vulnerable VM. The authors implement a closed-feedback loop system where GPT-3.5 suggests attack vectors that are automatically executed in a target VM, with outputs fed back to the LLM for refined suggestions. Promising results demonstrate GPT-3.5 can suggest realistic attack methods and successfully achieve privilege escalation, though the study notes ethical concerns and calls for future work on integration, model options, and improved memory.

## Method Summary
The study implements a closed-feedback loop system where GPT-3.5 generates penetration testing commands based on system state, which are automatically executed on a vulnerable Linux VM via SSH. The system captures command outputs and feeds them back to GPT-3.5 for analysis and next-step suggestions, iterating until privilege escalation is achieved. For high-level planning, the LLM decomposes security objectives into specific tactics and techniques aligned with the MITRE ATT&CK framework. The experiments focus on a single vulnerable VM scenario, demonstrating the feasibility of AI-augmented penetration testing while acknowledging limitations in generalizability and ethical considerations.

## Key Results
- GPT-3.5 successfully suggested realistic attack methods and achieved root privileges through privilege escalation vulnerabilities in a vulnerable VM
- The closed-feedback loop system enabled iterative vulnerability discovery and exploitation by processing command outputs for refined suggestions
- High-level task planning generated attack methodologies including realistic techniques like password spraying and Kerberoasting aligned with MITRE ATT&CK

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM can act as a penetration testing sparring partner by suggesting realistic attack vectors based on pattern recognition from training data.
- Mechanism: The LLM processes system information (e.g., sudoers file, user accounts) and matches it against learned security patterns to suggest specific exploitation commands.
- Core assumption: Security write-ups and exploit descriptions in the training corpus provide sufficient examples for the LLM to recognize and suggest realistic attack patterns.
- Evidence anchors:
  - [abstract] "promising initial results show GPT-3.5 can suggest realistic attack methods and successfully gain root privileges through privilege escalation vulnerabilities"
  - [section 4.1] "After retrieving the list of sudoers, GPT3.5 consistently suggested various vulnerable sudo commands for privilege escalation"
  - [corpus] Weak evidence - neighboring papers focus on automation but don't validate realism of LLM-generated attack vectors
- Break condition: If the LLM encounters novel system configurations not represented in training data, or if security patterns have changed significantly since training cutoff.

### Mechanism 2
- Claim: Closed feedback loop between LLM suggestions and VM execution enables iterative vulnerability discovery and exploitation.
- Mechanism: The system executes LLM-generated commands on the target VM, captures output, and feeds it back to the LLM for refined suggestions until privilege escalation is achieved.
- Core assumption: The LLM can reason about command outputs to identify new attack vectors and suggest appropriate follow-up actions.
- Evidence anchors:
  - [abstract] "A closed-feedback loop system is implemented where GPT-3.5 suggests attack vectors which are automatically executed in the VM"
  - [section 3.2] "Their output is presented back to GPT3.5 when prompted for the next command"
  - [corpus] Moderate evidence - neighboring papers discuss automated pen-testing but focus on web applications rather than VM-based approaches
- Break condition: If command execution fails repeatedly due to syntax errors, permission issues, or the LLM fails to adapt its strategy based on feedback.

### Mechanism 3
- Claim: High-level task planning with LLMs can generate comprehensive attack methodologies that align with established frameworks like MITRE ATT&CK.
- Mechanism: The LLM decomposes high-level security objectives into specific tactics, techniques, and procedures that security testers can execute.
- Core assumption: The LLM's training data includes sufficient security domain knowledge to map objectives to realistic attack methodologies.
- Evidence anchors:
  - [section 3.1] "we asked AgentGPT to 'Become domain admin in an Active Directory'. The generated document contained highly realistic attack vectors such as password spraying, Kerberoasting"
  - [abstract] "We explore the feasibility of supplementing penetration testers with AI models for two distinct use cases: high-level task planning for security testing assignments"
  - [corpus] Weak evidence - corpus neighbors focus on automation but don't validate high-level planning quality against frameworks
- Break condition: If generated methodologies contain unrealistic steps or fail to align with established security frameworks and best practices.

## Foundational Learning

- MITRE ATT&CK framework
  - Why needed here: Provides the hierarchical structure (Tactics, Techniques, Procedures) that guides both high-level planning and low-level execution in the system
  - Quick check question: Can you list the three levels of the TTP hierarchy and give one example of each?

- Prompt engineering
  - Why needed here: Controls how effectively the LLM interprets security testing scenarios and generates appropriate responses
  - Quick check question: What prompt modifications were found to reduce ethical moderation denials while maintaining functionality?

- SSH-based command execution
  - Why needed here: Enables the closed feedback loop between LLM suggestions and actual system state
  - Quick check question: What security considerations must be addressed when automatically executing LLM-generated commands on a target system?

## Architecture Onboarding

- Component map: LLM (GPT-3.5) ↔ Prompt generator ↔ SSH executor ↔ Target VM ↔ Output parser ↔ LLM context updater
- Critical path: Prompt generation → Command execution → Output capture → Vulnerability analysis → Next prompt generation
- Design tradeoffs: Cloud-based LLM (GPT-3.5) offers strong performance but raises ethical concerns and costs vs. local models (privacy, customization, but potentially weaker performance)
- Failure signatures: Repeated command failures, LLM refusal to respond, unexpected command outputs, privilege escalation not achieved within reasonable iterations
- First 3 experiments:
  1. Test LLM's ability to suggest basic enumeration commands (whoami, id, ls) on a known-good system
  2. Validate feedback loop by executing simple commands and checking if LLM adapts suggestions based on output
  3. Attempt privilege escalation on a deliberately vulnerable VM with known weaknesses to establish baseline effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can GPT-3.5-based systems reliably identify and exploit privilege escalation vulnerabilities in real-world systems without causing unintended damage?
- Basis in paper: [explicit] The paper discusses using GPT-3.5 to suggest attack vectors and exploit vulnerabilities in a deliberately vulnerable VM, but notes the need for further investigation into stability and reproducibility.
- Why unresolved: The experiments were conducted in a controlled environment with a vulnerable VM, and the stability and reproducibility of results were not thoroughly evaluated in real-world scenarios.
- What evidence would resolve it: Conducting experiments on real-world systems with varying levels of security and measuring the success rate, stability, and potential for unintended damage.

### Open Question 2
- Question: How effective are locally run models like LLaMA or StableLM compared to cloud-based models like GPT-3.5 for penetration testing tasks?
- Basis in paper: [explicit] The paper suggests investigating model options and mentions the potential benefits of locally run models, such as reduced costs and increased privacy.
- Why unresolved: The paper does not provide a direct comparison between locally run models and cloud-based models for penetration testing tasks.
- What evidence would resolve it: Conducting experiments comparing the performance, cost, and privacy implications of locally run models and cloud-based models for penetration testing tasks.

### Open Question 3
- Question: Can the integration of high-level task planning and low-level attack execution systems improve the efficiency and effectiveness of penetration testing?
- Basis in paper: [explicit] The paper discusses the potential benefits of integrating high-level task planning and low-level attack execution systems, but does not provide experimental evidence.
- Why unresolved: The paper does not provide experimental evidence on the effectiveness of integrating high-level and low-level systems for penetration testing.
- What evidence would resolve it: Conducting experiments comparing the performance of integrated high-level and low-level systems with traditional penetration testing methods.

## Limitations

- Limited to controlled vulnerable VM environment, not validated against real-world systems with complex defenses
- Single-system privilege escalation focus doesn't address multi-system penetration testing scenarios
- Ethical concerns around automated command execution and potential for unintended system modifications

## Confidence

- **Medium Confidence**: Claims about GPT-3.5 suggesting realistic attack methods and achieving privilege escalation are supported by experimental results but limited to a single VM scenario. The demonstration of specific exploitation techniques (e.g., sudo vulnerability exploitation) shows practical feasibility but doesn't generalize to broader security testing contexts.

- **Medium Confidence**: The closed-feedback loop system's effectiveness is validated through successful root access achievement, but the study doesn't address potential failure modes like command execution errors, LLM refusals, or the system's behavior when encountering novel system configurations not represented in training data.

- **Low Confidence**: Claims about high-level task planning generating comprehensive attack methodologies aligned with MITRE ATT&CK are supported by limited qualitative examples. The study shows GPT-3.5 can suggest realistic-sounding techniques (password spraying, Kerberoasting) but doesn't validate their completeness or alignment with established security frameworks against real-world targets.

## Next Checks

1. Test the closed-feedback loop system against multiple vulnerable VM scenarios with varying complexity to assess reliability across different system configurations and vulnerability types.

2. Conduct a comparative analysis of LLM-generated attack methodologies against established penetration testing frameworks to evaluate completeness, realism, and alignment with industry standards.

3. Implement defensive monitoring during LLM-guided penetration testing to identify potential adversarial exploitation vectors or unintended system modifications resulting from automated command execution.