---
ver: rpa2
title: On the Consistency and Robustness of Saliency Explanations for Time Series
  Classification
arxiv_id: '2309.01457'
source_url: https://arxiv.org/abs/2309.01457
tags:
- time
- saliency
- series
- explanations
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the consistency and robustness of saliency
  explanations for time series classification. The authors analyze both perturbation-based
  (Feature Permutation, Feature Ablation) and gradient-based (Integrated Gradients)
  explanation methods on three neural network architectures (LSTM, TCN, Transformer)
  using five real-world datasets.
---

# On the Consistency and Robustness of Saliency Explanations for Time Series Classification

## Quick Facts
- arXiv ID: 2309.01457
- Source URL: https://arxiv.org/abs/2309.01457
- Reference count: 31
- Primary result: Saliency explanation methods for time series classification show significant consistency and robustness issues across overlapping windows and feature reordering

## Executive Summary
This paper investigates the consistency and robustness of saliency explanations for time series classification, analyzing both perturbation-based (Feature Permutation, Feature Ablation) and gradient-based (Integrated Gradients) methods. Through experiments using artificial padding and feature swapping on three neural network architectures (LSTM, TCN, Transformer) across five real-world datasets, the authors demonstrate that these explanation methods frequently produce inconsistent saliency maps across overlapping time windows and are not robust to feature reordering. The analysis reveals that current saliency explanation approaches suffer from significant consistency and robustness issues, motivating the need for more reliable explanation methods in this domain.

## Method Summary
The authors conduct experiments using three neural network architectures (LSTM, TCN, Transformer) on five univariate time series datasets from the UCR Archive. They evaluate three explanation methods: Feature Permutation, Feature Ablation, and Integrated Gradients. To test consistency, they introduce artificial padding to create overlapping time windows and measure the stability of importance scores across these windows using Kendall's tau and Pearson correlation. For robustness testing, they swap feature dimensions in the time series data and examine whether saliency maps correspondingly swap important areas. The experiments also include Recall@k metrics to measure the quality of saliency explanations in detecting important features and timestamps.

## Key Results
- Saliency explanations often produce inconsistent importance scores across overlapping time windows, violating expectations of temporal consistency
- Explanation methods fail to maintain robustness when features are reordered in time series data, with importance rankings not correspondingly swapping
- The consistency and robustness issues persist across different neural network architectures (LSTM, TCN, Transformer) and explanation methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Saliency explanations fail to maintain stable importance scores across overlapping time windows.
- Mechanism: When using sliding windows on time series data, adjacent windows share overlapping temporal regions. If saliency explanation methods are inconsistent, the importance scores assigned to the same time points and features will differ between windows, violating the expectation of temporal consistency.
- Core assumption: Saliency explanations should provide stable importance rankings for overlapping temporal regions since these represent the same underlying data points.
- Evidence anchors:
  - [abstract]: "they demonstrate that these explanation methods often produce inconsistent saliency maps across overlapping time windows"
  - [section 3.1]: "We claim that explanations over the intersection of sliding windows should exhibit consistent behaviors"
  - [corpus]: Weak evidence - the corpus contains papers on time series explanation but none specifically address consistency across sliding windows.

### Mechanism 2
- Claim: Saliency explanations are not robust to feature reordering in time series data.
- Mechanism: When features (input variables) are swapped in the time series data, the saliency map should correspondingly swap the importance scores for those features. If the explanation method fails to do this, it indicates a lack of feature robustness.
- Core assumption: Time series data is semantically different from images - swapping features (input variables) should maintain the same semantic meaning, unlike swapping rows/columns in images which changes meaning.
- Evidence anchors:
  - [abstract]: "these explanation methods frequently fail to maintain stable importance scores across different window positions" and "are not robust to feature reordering"
  - [section 3.2]: "When feature columns in the time series frame are swapped, important areas in the saliency map should stay salient in the corresponding swapped areas"
  - [corpus]: Weak evidence - the corpus mentions explanation methods for time series but doesn't specifically discuss feature reordering robustness.

### Mechanism 3
- Claim: Treating time series windows as images for explanation purposes fails due to temporal dependencies.
- Mechanism: Time series data has temporal dependencies that images don't have. When explanation methods designed for images (treating each timestamp as a pixel) are applied to time series, they fail to capture these temporal relationships, leading to poor explanations.
- Core assumption: Time series data requires explanation methods that account for temporal structure, unlike image data where pixels are independent.
- Evidence anchors:
  - [abstract]: "Saliency maps have been applied to interpret time series windows as images. However, they are not naturally designed for sequential data, thus suffering various issues"
  - [section 1]: "Images and time series represent fundamentally different types of data" and "explanation approaches are often not directly applicable to time series models with recurrent- or attention-based components"
  - [corpus]: Moderate evidence - the corpus contains papers discussing the challenges of applying image-based explanation methods to time series.

## Foundational Learning

- Concept: Sliding window mechanism in time series analysis
  - Why needed here: The paper evaluates consistency across overlapping windows, which is fundamental to understanding the temporal structure of time series data
  - Quick check question: How does the choice of window size and step size affect the overlap between consecutive windows in time series analysis?

- Concept: Kendall's tau and Pearson correlation for ranking consistency
  - Why needed here: The paper uses these metrics to quantify the consistency of importance rankings across different windows
  - Quick check question: What does a Kendall's tau value of 0.5 indicate about the agreement between two rankings compared to a value of 0.9?

- Concept: Shapley values in feature attribution
  - Why needed here: The paper mentions Shapley values as an alternative approach to saliency maps, which is important background for understanding the landscape of explanation methods
  - Quick check question: How do Shapley values differ from gradient-based and perturbation-based methods in terms of computational complexity and theoretical properties?

## Architecture Onboarding

- Component map: Data preparation -> Neural network training -> Saliency explanation generation -> Consistency/robustness evaluation
- Critical path: 1) Prepare padded data with artificial noise, 2) Train neural network models, 3) Generate saliency explanations using three methods, 4) Evaluate consistency using Kendall's tau and Pearson correlation, 5) Evaluate robustness through feature swapping
- Design tradeoffs: Using artificial padding allows precise control over feature importance but may not fully capture real-world temporal dependencies. Evaluating only three explanation methods may miss other important approaches.
- Failure signatures: Inconsistent saliency rankings across overlapping windows (low Kendall's tau values), lack of robustness to feature reordering (low correlation between swapped and unswapped feature importance), poor detection of important features/timestamps (low Recall@k)
- First 3 experiments:
  1. Reproduce the inconsistency analysis by generating padded data, training an LSTM model, and computing Kendall's tau between importance rankings for different padding positions.
  2. Implement the robustness analysis by swapping features in the input data, retraining the model, and comparing the importance rankings before and after swapping.
  3. Visualize the saliency maps for a specific dataset and model architecture to qualitatively assess the consistency and robustness issues described in the paper.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the fundamental reasons behind the inconsistency and lack of robustness in current saliency explanation methods for time series data?
- Basis in paper: [explicit] The authors demonstrate that saliency explanations often produce inconsistent results across overlapping time windows and are not robust to feature reordering.
- Why unresolved: The paper identifies the problem but does not provide a theoretical explanation for why these methods fail on time series data.
- What evidence would resolve it: Mathematical proofs or empirical studies showing why perturbation-based and gradient-based methods inherently struggle with temporal dependencies in time series.

### Open Question 2
- Question: How can saliency explanation methods be adapted or redesigned to maintain consistency across overlapping sliding windows while preserving temporal information?
- Basis in paper: [inferred] The authors show that current methods produce different importance scores in overlapping regions, suggesting the need for new approaches that respect temporal continuity.
- Why unresolved: The paper identifies the consistency problem but does not propose specific solutions or modifications to existing methods.
- What evidence would resolve it: New explanation algorithms that demonstrate consistent importance scores across sliding windows, validated through experiments similar to those in the paper.

### Open Question 3
- Question: What alternative explanation frameworks could better capture the temporal dependencies in time series data while providing robust and consistent attributions?
- Basis in paper: [explicit] The authors motivate the need for developing consistent and robust explanations for time series classification, acknowledging the limitations of current approaches.
- Why unresolved: The paper concludes with a call for new methods but does not explore specific alternative frameworks or architectures.
- What evidence would resolve it: Novel explanation methods specifically designed for time series that outperform current saliency approaches in terms of consistency and robustness metrics.

## Limitations
- The analysis focuses on a specific set of explanation methods and may not generalize to all saliency-based approaches
- Artificial padding approach may not fully capture the complexity of real-world temporal dependencies in time series data
- The paper does not extensively explore whether the identified issues stem from the explanation methods themselves or from how they are applied to time series data

## Confidence
- **High confidence**: The experimental methodology for testing consistency across overlapping windows is sound, and the observation that saliency methods fail to maintain stable importance scores is well-supported by the reported correlation metrics.
- **Medium confidence**: The feature swapping robustness analysis effectively demonstrates vulnerability to feature reordering, though the real-world relevance of this specific attack scenario warrants further investigation.
- **Medium confidence**: The claim that time series require fundamentally different explanation approaches than images is well-motivated but would benefit from direct comparison with methods specifically designed for temporal data.

## Next Checks
1. Test the consistency analysis using alternative correlation metrics (e.g., Spearman correlation) to verify that Kendall's tau results are not method-dependent artifacts.
2. Apply the same consistency and robustness tests to time series-specific explanation methods (such as CRITS or WinTSR mentioned in the corpus) to determine if these approaches mitigate the identified issues.
3. Conduct ablation studies to isolate whether the consistency problems arise from the explanation methods, the neural network architectures, or the interaction between them.