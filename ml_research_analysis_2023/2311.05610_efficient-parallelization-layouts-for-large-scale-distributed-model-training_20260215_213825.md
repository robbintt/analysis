---
ver: rpa2
title: Efficient Parallelization Layouts for Large-Scale Distributed Model Training
arxiv_id: '2311.05610'
source_url: https://arxiv.org/abs/2311.05610
tags:
- flash
- error
- disabled
- attn2
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive study of parallelization strategies
  for efficient large-scale distributed training of large language models (LLMs).
  The authors conduct an ablation study over various configurations, including tensor,
  pipeline, and data parallelism, as well as micro-batch sizes, activation checkpointing,
  fused kernels, FlashAttention, and sequence parallelism.
---

# Efficient Parallelization Layouts for Large-Scale Distributed Model Training

## Quick Facts
- **arXiv ID**: 2311.05610
- **Source URL**: https://arxiv.org/abs/2311.05610
- **Reference count**: 40
- **Primary result**: Micro-batch size of 1 enables most efficient training layouts in most scenarios for large language models

## Executive Summary
This paper presents a comprehensive study of parallelization strategies for efficient large-scale distributed training of large language models (LLMs). Through systematic ablation studies, the authors identify that micro-batch size 1 typically provides the best Model FLOPs Utilization (MFU) by minimizing model parallelization requirements and avoiding activation checkpointing. The study evaluates various configurations including tensor, pipeline, and data parallelism, while examining optimizations like FlashAttention-2 and sequence parallelism across Llama model architectures ranging from 13B to 65B parameters.

## Method Summary
The authors conducted extensive experiments training Llama models (13B, 30B, 65B parameters) with 2k and 8k sequence lengths on up to 256 A100 GPUs. They performed systematic ablation studies varying micro-batch sizes (1, 2, 4, 8), tensor parallelism (1, 2, 4, 8), pipeline parallelism (1, 2, 4, 8), and the use of activation checkpointing, fused kernels, FlashAttention-2, and sequence parallelism. The primary metric was Model FLOPs Utilization (MFU), measuring the ratio of achieved throughput to theoretical peak throughput. Experiments were conducted using the AA-Scaling framework on NVIDIA A100 GPUs with Infiniband networking.

## Key Results
- Micro-batch size of 1 consistently achieves the best MFU across all model types by minimizing model parallelization and avoiding activation checkpointing
- FlashAttention-2 outperforms other attention implementations through improved IO-aware SRAM cache utilization and reduced memory requirements
- Sequence parallelism only provides significant benefits for models exceeding 30B parameters or 2k sequence length
- The study achieved state-of-the-art training efficiency with 70.5% MFU when training a Llama 13B model

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Micro-batch size of 1 minimizes model parallelization, avoiding activation checkpointing and reducing pipeline bubbles.
- **Mechanism**: Smaller micro-batch sizes allow for the most efficient parallelization layout because they require the least amount of model (tensor or pipeline) parallelization, which is achieved when the micro-batch size is smallest. This avoids the need for activation checkpointing and reduces pipeline bubble time.
- **Core assumption**: The computational overhead of processing smaller micro-batches is outweighed by the benefits of reduced model parallelization and pipeline bubbles.
- **Evidence anchors**:
  - [abstract]: "We find that using a micro-batch size of 1 usually enables the most efficient training layouts. Larger micro-batch sizes necessitate activation checkpointing or higher degrees of model parallelism and also lead to larger pipeline bubbles."
  - [section]: "We see that for all model types, a micro-batch size of 1 achieves the best MFU. In general, we find: the smaller the micro-batch size, the better the MFU."
  - [corpus]: No direct corpus evidence found. Weak evidence: Only 1 neighbor paper mentions micro-batch size considerations in distributed training.
- **Break Condition**: If the overhead of processing smaller micro-batches becomes significant compared to the benefits of reduced model parallelization and pipeline bubbles.

### Mechanism 2
- **Claim**: FlashAttention-2 outperforms other attention implementations by improving IO-aware SRAM cache utilization and reducing memory requirements through activation recomputation.
- **Mechanism**: FlashAttention-2's improved tiling method for efficient IO-aware SRAM cache utilization and reduced memory requirements through its activation recomputation approach in the attention block lead to superior performance compared to other attention implementations.
- **Core assumption**: The benefits of FlashAttention-2's improved tiling and activation recomputation outweigh any potential drawbacks, such as increased computational complexity.
- **Evidence anchors**:
  - [abstract]: "We further compare with a more optimized baseline, the Megatron-LM softmax attention kernel. Additionally, we evaluate the use of an optimized RMSNorm kernel from the FLASH ATTENTION repository."
  - [section]: "It is important to note that FLASH ATTENTION's improvements are two-fold: FLASH ATTENTION's improved tiling method for an efficient IO-aware SRAM cache utilization and reduced memory requirements through its activation recomputation approach in the attention block."
  - [corpus]: No direct corpus evidence found. Weak evidence: Only 1 neighbor paper mentions FlashAttention in the context of distributed training.
- **Break Condition**: If the overhead of FlashAttention-2's improved tiling and activation recomputation becomes significant compared to the benefits.

### Mechanism 3
- **Claim**: Sequence parallelism provides significant benefits for models exceeding 30B parameters or 2k sequence length by reducing activation memory usage.
- **Mechanism**: Sequence parallelism builds on tensor parallelism by further parallelizing normalization and dropout operations along the sequence dimension, reducing activation memory usage, especially for longer sequences.
- **Core assumption**: The benefits of reduced activation memory usage outweigh any potential overhead introduced by sequence parallelism.
- **Evidence anchors**:
  - [abstract]: "Sequence parallelism [8] builds on tensor parallelism [19] by further parallelizing normalization and dropout operations along the sequence dimension. This reduces activation memory usage, especially for longer sequences."
  - [section]: "Therefore, we conclude that the use of sequence parallelism, when paired with several other optimizations explored in this work, only facilitates a notable difference in training efficiency for model sizes exceeding 30B parameters or 2k sequence length."
  - [corpus]: No direct corpus evidence found. Weak evidence: Only 1 neighbor paper mentions sequence parallelism in the context of distributed training.
- **Break Condition**: If the overhead of sequence parallelism becomes significant compared to the benefits of reduced activation memory usage.

## Foundational Learning

- **Concept**: Understanding of parallelization strategies (data, tensor, pipeline, and sequence parallelism)
  - **Why needed here**: To grasp the core findings of the paper and how different parallelization strategies interact with each other and with other optimizations.
  - **Quick check question**: Can you explain the difference between tensor parallelism and pipeline parallelism, and when each might be preferred?

- **Concept**: Knowledge of activation checkpointing and its tradeoffs
  - **Why needed here**: To understand the role of activation checkpointing in the context of the paper's findings and how it interacts with other optimizations like FlashAttention.
  - **Quick check question**: What are the benefits and drawbacks of using activation checkpointing in distributed training, and when might it be necessary?

- **Concept**: Familiarity with attention mechanisms and their implementations
  - **Why needed here**: To comprehend the significance of FlashAttention-2's improvements and how it compares to other attention implementations.
  - **Quick check question**: What are the key differences between FlashAttention-2 and other attention implementations, and how do these differences impact training efficiency?

## Architecture Onboarding

- **Component map**: Distributed training of LLMs using parallelization strategies (data, tensor, pipeline, sequence parallelism) + memory optimizations (activation checkpointing, fused kernels, FlashAttention) + Llama architecture with RMSNorm, SwiGLU activation, and rotary positional embeddings
- **Critical path**: Optimization of parallelization layout, particularly micro-batch size selection, tensor and pipeline parallelism sizes, and sequence parallelism usage
- **Design tradeoffs**: Balancing reduced model parallelization and pipeline bubbles (achieved through smaller micro-batch sizes) against potential overhead of processing smaller batches, and weighing memory savings of sequence parallelism against computational overhead
- **Failure signatures**: Out-of-memory errors when model size or sequence length exceeds GPU capacity, suboptimal MFU due to poor parallelization layouts, errors between different optimizations (e.g., activation checkpointing and RMSNorm kernel)
- **First 3 experiments**:
  1. Verify impact of micro-batch size on training efficiency by running experiments with different micro-batch sizes (1, 2, 4) while keeping other parameters constant
  2. Compare performance of FlashAttention-2 with other attention implementations (native PyTorch, Megatron-LM kernel) to confirm superior efficiency
  3. Assess benefits of sequence parallelism for larger models (>30B parameters) or longer sequence lengths (>2k) by running experiments with and without sequence parallelism enabled

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the efficacy of sequence parallelism change when using different attention mechanisms or model architectures beyond LLAMA?
  - **Basis in paper**: [explicit] The paper states that sequence parallelism only provides significant benefits for models exceeding 30B parameters or 2k sequence length, but does not explore its effects on other architectures or attention mechanisms.
  - **Why unresolved**: The study is limited to LLAMA models and does not explore other model architectures or attention mechanisms that might interact differently with sequence parallelism.
  - **What evidence would resolve it**: Experimental results comparing sequence parallelism efficacy across various model architectures (e.g., BERT, GPT) and attention mechanisms (e.g., Performer, Longformer) with different parameter sizes and sequence lengths.

- **Open Question 2**: What is the impact of using fp8 precision on the training efficiency and model parallelization strategies discussed in the paper?
  - **Basis in paper**: [explicit] The paper mentions that NVIDIA's H100 GPUs with more efficient support for fp8 precision might enable new training strategies, but does not explore this in the study.
  - **Why unresolved**: The study is conducted on A100 GPUs with bfloat16 precision, and the potential benefits of fp8 precision are not explored.
  - **What evidence would resolve it**: Experimental results comparing training efficiency and parallelization strategies using fp8 precision on H100 GPUs versus bfloat16 precision on A100 GPUs for various model sizes and configurations.

- **Open Question 3**: How do the recommended parallelization strategies and optimizations perform when scaling to models with more than 100 billion parameters?
  - **Basis in paper**: [inferred] The paper focuses on models up to 65B parameters and provides recommendations based on this scale, but does not explore the effectiveness of these strategies for larger models.
  - **Why unresolved**: The study is limited to models up to 65B parameters, and the performance of the recommended strategies for larger models is unknown.
  - **What evidence would resolve it**: Experimental results demonstrating the training efficiency and effectiveness of the recommended parallelization strategies and optimizations for models exceeding 100B parameters, including comparisons with other large-scale training frameworks.

## Limitations
- Focus on Llama models with specific configurations (13B, 30B, 65B parameters, 2k/8k sequence lengths) may limit generalizability to other architectures
- Experiments conducted exclusively on NVIDIA A100 GPUs, potentially limiting hardware transferability
- Does not explore impact of different learning rate schedules or optimizer configurations on training efficiency

## Confidence
- **High Confidence**: Core finding that micro-batch size 1 provides optimal MFU in most scenarios is well-supported by systematic ablation studies across multiple model sizes
- **Medium Confidence**: Performance benefits of FlashAttention-2 are demonstrated but could benefit from comparison against additional attention implementations
- **Medium Confidence**: Recommendation for sequence parallelism on models exceeding 30B parameters is supported but based on limited parameter space exploration

## Next Checks
1. **Generalization Test**: Validate the micro-batch size findings across additional model architectures (GPT-style, OPT, or BLOOM) to confirm if the size-1 advantage holds universally
2. **Hardware Transferability**: Reproduce key experiments on H100 GPUs or AMD Instinct accelerators to assess hardware dependency of the optimization recommendations
3. **Edge Case Boundary**: Systematically explore the transition point where sequence parallelism becomes beneficial by testing models in the 20B-30B parameter range with varying sequence lengths to precisely identify the 30B parameter threshold