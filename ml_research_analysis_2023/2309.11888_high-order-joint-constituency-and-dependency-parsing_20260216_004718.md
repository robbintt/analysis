---
ver: rpa2
title: High-order Joint Constituency and Dependency Parsing
arxiv_id: '2309.11888'
source_url: https://arxiv.org/abs/2309.11888
tags:
- parsing
- joint
- dependency
- constituency
- trees
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work revisits the problem of jointly parsing constituency
  and dependency trees, which aims to produce compatible trees simultaneously for
  input sentences. The authors propose a more efficient decoding algorithm of O(n^4)
  time complexity compared to the previous O(n^5) approach.
---

# High-order Joint Constituency and Dependency Parsing

## Quick Facts
- arXiv ID: 2309.11888
- Source URL: https://arxiv.org/abs/2309.11888
- Reference count: 5
- Primary result: Joint constituency and dependency parsing with O(n^4) decoding achieves modest LAS/F1 gains over separate models while substantially improving tree compatibility matching.

## Executive Summary
This paper revisits joint constituency and dependency parsing by proposing an efficient O(n^4) decoding algorithm and exploring joint modeling at the training phase. The authors introduce high-order scoring components to better capture constituent-dependency interactions, including headed-span and hooked-span scores in addition to first-order scores. Experiments on seven languages demonstrate that joint modeling leads to modest overall performance improvements while substantially increasing the complete matching ratio of whole trees, validating the effectiveness of explicit tree compatibility modeling.

## Method Summary
The authors propose a joint parsing framework that uses a shared encoder with separate decoders for constituency and dependency parsing under a multi-task learning setup. They employ the Eisner-Satta algorithm (O(n^4)) for efficient decoding of lexicalized trees, which was previously computationally prohibitive with the O(n^5) algorithm. The model incorporates high-order scoring components including headed-span and hooked-span scores to capture higher-order interactions between constituents and dependencies. Training uses max-margin loss with beam search to optimize for compatible tree pairs, and head-binarization is applied to convert unlexicalized constituency trees to lexicalized form.

## Key Results
- JOINT 2O achieves 0.23 LAS improvement on PTB and 0.52 LAS on CTB5 compared to separate dependency parsing
- Complete matching ratio improves substantially (e.g., 0.59 vs JOINT 1O, 0.89 vs MTL on LCMlex)
- Joint modeling shows consistent gains across seven languages tested
- The O(n^4) Eisner-Satta algorithm enables practical joint training previously infeasible with O(n^5) approaches

## Why This Works (Mechanism)

### Mechanism 1
Joint modeling at training phase improves dependency parsing performance over separate modeling. By training constituency and dependency parsers together under a multi-task learning framework, the model can leverage shared encoder representations that capture syntactic information beneficial to both tasks, leading to better feature learning and regularization. The core assumption is that the two tasks share underlying syntactic structures that can be jointly learned. Evidence shows joint parsing models (including MTL, JOINT 1O and JOINT 2O) all showed absolute performance gains compared to DEP, especially JOINT 2O which achieves 0.23 LAS improvement on PTB test and 0.52 LAS on CTB5 test.

### Mechanism 2
Second-order scoring components improve parsing by capturing higher-order interactions between constituents and dependencies. By incorporating headed-span and hooked-span scores in addition to first-order constituent and dependency scores, the model can better model the relationship between a constituent's head and its dependency structure, leading to more accurate tree structures. The core assumption is that higher-order interactions between heads and spans provide useful syntactic information not captured by first-order scores alone. Evidence shows JOINT 2O exhibited a substantial improvement of 0.59 compared to JOINT 1O and 0.89 compared to MTL on LCMlex.

### Mechanism 3
Using the Eisner-Satta algorithm (O(n^4)) instead of the original O(n^5) decoding algorithm enables joint modeling at training time. The improved decoding efficiency allows for the computational overhead of joint decoding during training, which was previously prohibitive with the slower algorithm. This enables the model to optimize for compatible tree pairs during training. The core assumption is that the efficiency gain from O(n^5) to O(n^4) is sufficient to make joint training computationally feasible. Evidence shows the Eisner-Satta algorithm of O(n^4) time complexity is employed for finding optimal lexicalized c-trees, which runs very efficiently on GPUs after proper batchification.

## Foundational Learning

- **Dynamic programming-based decoding algorithms for syntactic parsing**: The paper relies on the Eisner-Satta algorithm, a dynamic programming approach, to efficiently find optimal compatible tree pairs during both training and inference. Quick check: How does the Eisner-Satta algorithm achieve O(n^4) complexity compared to the original O(n^5) approach, and what are the key data structures used in its implementation?

- **Multi-task learning (MTL) framework**: The paper explores joint modeling at the training phase using an MTL framework with one shared encoder and separate decoders for constituency and dependency parsing. Quick check: In an MTL setup for joint parsing, how are the gradients from the two parsing tasks combined, and what strategies can be used to balance their contributions?

- **Lexicalized tree representation and head-binarization**: The paper uses lexicalized trees as the joint representation of constituency and dependency trees, requiring head-binarization to convert unlexicalized constituency trees into a form suitable for the parsing model. Quick check: What is the purpose of head-binarization in converting constituency trees to lexicalized form, and how does it ensure compatibility with the corresponding dependency tree?

## Architecture Onboarding

- **Component map**: Input sentence -> Shared encoder (LSTM or BERT) -> Scoring components (first and second-order) -> Eisner-Satta decoding -> Labeled output trees
- **Critical path**: Input sentence → Shared encoder → Scoring components (first and second-order) → Eisner-Satta decoding → Labeled output trees
- **Design tradeoffs**: Using a shared encoder allows for joint learning but may lead to interference if the tasks are not sufficiently related. Incorporating second-order features improves modeling power but increases computational complexity. Head-binarization ensures compatibility but may introduce additional preprocessing steps.
- **Failure signatures**: Degraded performance on one or both parsing tasks compared to separate models, incompatible constituency and dependency trees in the output, slow training or inference times due to inefficient decoding or overly complex models.
- **First 3 experiments**: 1) Compare joint modeling (JOINT 1O) vs separate modeling (DEP/CON) on development set to assess joint training impact. 2) Evaluate second-order scoring components (JOINT 2O vs JOINT 1O) on parsing accuracy and complete matching ratio. 3) Analyze performance on long-distance dependencies and wide constituents to understand strengths and weaknesses.

## Open Questions the Paper Calls Out

### Open Question 1
Does joint modeling provide consistent improvements in parsing performance across different languages and treebanks? The authors conduct experiments on both English (PTB) and Chinese (CTB5.1) datasets and observe varying degrees of improvement. This remains unresolved because the paper presents results for only two languages, and the degree of improvement differs between them. Conducting joint modeling experiments on a diverse set of languages and treebanks would determine if observed trends hold for other languages or treebanks.

### Open Question 2
What is the impact of joint modeling on parsing performance for specific syntactic phenomena, such as long-range dependencies and wide constituents? The authors analyze performance on different constituent widths and dependency lengths, observing that joint modeling improves recall for wide constituents and precision/recall for long dependencies. This remains unresolved because the analysis focuses on general trends, and it's unclear if joint modeling consistently improves performance for specific syntactic phenomena across different languages and treebanks. A detailed analysis of parsing performance on specific syntactic phenomena across different languages and treebanks would resolve this question.

### Open Question 3
How does the choice of decoding algorithm affect the performance of joint constituency and dependency parsing? The authors compare the O(n^5) decoding algorithm used in Zhou and Zhao (2019) with the O(n^4) Eisner-Satta algorithm, observing improved efficiency with the latter. This remains unresolved because the paper doesn't explore the impact of different decoding algorithms on parsing performance, focusing instead on efficiency gains. Conducting experiments with different decoding algorithms would determine their impact on parsing performance and identify the most effective algorithm for joint constituency and dependency parsing.

## Limitations
- Performance gains are modest (e.g., 0.23 LAS on PTB) and may be task- or dataset-dependent
- O(n^4) decoding efficiency relies on GPU batching optimizations that may not translate to all hardware configurations
- Head-binarization preprocessing required for lexicalized tree representation introduces complexity affecting reproducibility

## Confidence

- **High** for improved decoding efficiency enabling joint training (directly verifiable through complexity analysis and runtime measurements)
- **Medium** for MTL performance improvements (gains are consistent but relatively small, and exact contribution of shared representations versus joint optimization is difficult to isolate)
- **Low** for second-order scoring components' impact (substantial LCM improvements may be partially attributed to increased model capacity rather than specific high-order interactions)

## Next Checks

1. **Ablation on encoder sharing**: Train constituency and dependency parsers with separate encoders but joint decoding to isolate the contribution of shared representations versus joint optimization.

2. **Scaling analysis**: Measure training and inference times on datasets of varying sizes (e.g., PTB, CTB, UD) to verify the claimed O(n^4) efficiency gains hold across different scales.

3. **Generalization test**: Evaluate the model on out-of-domain data (e.g., biomedical or social media text) to assess whether joint modeling provides robustness benefits beyond in-domain performance.