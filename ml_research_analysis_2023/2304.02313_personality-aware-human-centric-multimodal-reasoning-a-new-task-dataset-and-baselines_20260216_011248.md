---
ver: rpa2
title: 'Personality-aware Human-centric Multimodal Reasoning: A New Task, Dataset
  and Baselines'
arxiv_id: '2304.02313'
source_url: https://arxiv.org/abs/2304.02313
tags:
- personality
- task
- multimodal
- reasoning
- behavior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Personality-aware Human-centric Multimodal
  Reasoning (PHMR), a new task that aims to predict the future behavior of a specific
  individual using multimodal information from past instances, while incorporating
  personality traits. The authors construct a new dataset based on six television
  shows, encompassing 225 characters and 12k samples.
---

# Personality-aware Human-centric Multimodal Reasoning: A New Task, Dataset and Baselines

## Quick Facts
- arXiv ID: 2304.02313
- Source URL: https://arxiv.org/abs/2304.02313
- Authors: 
- Reference count: 25
- Primary result: Incorporating personality traits improves human-centric multimodal reasoning performance by up to 1.88% and averaging 1.58% improvement

## Executive Summary
This paper introduces Personality-aware Human-centric Multimodal Reasoning (PHMR), a novel task that predicts future individual behavior using multimodal information while incorporating personality traits. The authors construct a new dataset based on six television shows with 225 characters and 12k samples. They propose seven baseline methods and demonstrate that incorporating personality traits enhances reasoning performance. Additionally, they introduce an extended task called Personality-predicted Human-centric Multimodal Reasoning (PHMRD) that predicts personality traits from multimodal data when explicit annotations are unavailable.

## Method Summary
The method involves constructing a new PHMRD dataset from six television shows, with each sample containing multimodal information (video, dialogue, behavior) and personality traits represented by MBTI. Two models are proposed: PRM for Task 1 (when personality is known) and PRM Pred. for Task 2 (when personality must be predicted). The models use feature extractors for each modality, personality processors with self-attention and cross-modal attention, and fusion modules that combine personality with multimodal features. The answer prediction uses answer attention to score multiple choice options.

## Key Results
- Incorporating personality traits improves multimodal reasoning performance with maximum improvement of 1.88% and average improvement of 1.58%
- Behavioral information is the most essential modality, followed by dialogue, with video being the least important
- The Personality-predicted HMR task successfully predicts MBTI personalities using multimodal information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating personality traits into multimodal reasoning models improves prediction accuracy.
- Mechanism: Personality traits act as additional context that biases behavioral predictions toward patterns consistent with known psychological tendencies.
- Core assumption: Individuals with similar personality profiles exhibit consistent behavioral patterns in similar situations.
- Evidence anchors:
  - [abstract]: "experimental results demonstrate that incorporating personality traits enhances human-centric multimodal reasoning performance"
  - [section]: "By incorporating personality, multimodal information brings more improvements compared to unimodal information, with a maximum improvement of 1.88% and an average improvement of 1.58%"
  - [corpus]: Weak. No direct mention of personality reasoning in corpus results.

### Mechanism 2
- Claim: Multimodal information can be used to predict personality traits when explicit annotations are unavailable.
- Mechanism: Language patterns, facial expressions, and behaviors contain implicit signals of personality that can be learned from large datasets.
- Core assumption: Personality manifests in observable behaviors and linguistic cues that are consistent enough to be learned.
- Evidence anchors:
  - [abstract]: "we introduce an extended task called Personality-predicted HMR... to predict the MBTI personality at first, and then use the predicted personality to help multimodal reasoning"
  - [section]: "we discovered huge disparities in the linguistic habits of persons with distinct personalities by observing statistics on the conversations"
  - [corpus]: Weak. No direct evidence of personality prediction using multimodal signals.

### Mechanism 3
- Claim: The combination of visual, textual, and behavioral modalities provides complementary information for reasoning tasks.
- Mechanism: Each modality captures different aspects of human behavior, and their integration through attention mechanisms improves overall prediction quality.
- Core assumption: Visual, dialogue, and behavioral information are conditionally independent given the true behavior, allowing their features to be effectively combined.
- Evidence anchors:
  - [abstract]: "We benchmark the task by proposing seven baseline methods: three adapted from related tasks, two pre-trained model, and two multimodal large language models"
  - [section]: "behavioral information is the most essential, followed by dialogue, and video is the least important"
  - [corpus]: No direct evidence of modality importance in reasoning tasks.

## Foundational Learning

- Concept: Myers-Briggs Type Indicator (MBTI) personality dimensions
  - Why needed here: The task uses MBTI as the personality annotation framework, requiring understanding of its four dimensions (E/I, S/N, T/F, J/P) and 16 personality types
  - Quick check question: What are the four dimensions of MBTI and what does each measure?

- Concept: Multimodal feature extraction and fusion
  - Why needed here: The model extracts features from video, dialogue, and behavior modalities and fuses them with personality features
  - Quick check question: How does the model fuse personality features with multimodal features?

- Concept: Attention mechanisms for cross-modal reasoning
  - Why needed here: The model uses cross-modal attention to integrate personality information with visual and textual features
  - Quick check question: What type of attention mechanism is used to incorporate personality information?

## Architecture Onboarding

- Component map: Video feature extractor (2D+3D CNN) -> Dialogue feature extractor (WordPiece embeddings) -> Behavior feature extractor (text processing) -> Personality processor (self-attention + cross-modal attention) -> Fusion modules (Personality Attention, Answer Attention) -> Classifier (softmax over answer options)
- Critical path: Feature extraction → Personality fusion → Answer prediction
- Design tradeoffs: Simple concatenation vs learned attention for personality fusion; separate vs joint training of personality prediction
- Failure signatures: Performance drops when personality annotations are removed; attention weights become uniform; training instability
- First 3 experiments:
  1. Test unimodal baselines (dialogue only, video only, behavior only) to establish modality importance
  2. Add personality features to unimodal models to measure improvement from personality alone
  3. Test multimodal fusion with and without personality to measure complementary benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the incorporation of personality traits affect the performance of human-centric multimodal reasoning tasks across different datasets and domains?
- Basis in paper: [explicit] The authors demonstrate that incorporating personality traits enhances human-centric multimodal reasoning performance in their proposed dataset and tasks.
- Why unresolved: The paper focuses on a specific dataset and task, limiting the generalizability of the findings to other datasets and domains.
- What evidence would resolve it: Conducting experiments with various datasets and domains to compare the performance of human-centric multimodal reasoning tasks with and without personality traits.

### Open Question 2
- Question: What are the most effective methods for predicting personality traits using multimodal information, and how do these methods compare to existing personality prediction approaches?
- Basis in paper: [explicit] The authors propose a method for predicting MBTI personalities using multimodal information and demonstrate its effectiveness in their dataset.
- Why unresolved: The paper does not compare the proposed method to existing personality prediction approaches, nor does it explore the most effective methods for predicting personality traits using multimodal information.
- What evidence would resolve it: Conducting experiments to compare the proposed method with existing personality prediction approaches and exploring various methods for predicting personality traits using multimodal information.

### Open Question 3
- Question: How can the incorporation of personality traits be extended to other multimodal reasoning tasks, and what impact would this have on the performance of these tasks?
- Basis in paper: [explicit] The authors propose a new task (Personality-aware Human-centric Multimodal Reasoning) and demonstrate the effectiveness of incorporating personality traits in this task.
- Why unresolved: The paper does not explore the potential impact of incorporating personality traits on other multimodal reasoning tasks or discuss how this approach can be extended to other tasks.
- What evidence would resolve it: Investigating the impact of incorporating personality traits on various multimodal reasoning tasks and exploring methods for extending this approach to other tasks.

## Limitations

- The dataset is constructed from television shows rather than real-world scenarios, potentially limiting generalizability to authentic human interactions
- The paper relies on MBTI personality annotations, which have been criticized in psychology for lacking scientific validity compared to the Big Five model
- Performance improvements from personality incorporation are modest (1.88% maximum, 1.58% average), suggesting the approach is beneficial but not transformative

## Confidence

- **High Confidence**: The core finding that incorporating personality traits improves multimodal reasoning performance is well-supported by experimental results across multiple baseline methods and metrics.
- **Medium Confidence**: The proposed architecture and training methodology appear sound, though specific implementation details for feature extractors and attention mechanisms are not fully specified, requiring some assumptions for reproduction.
- **Low Confidence**: The claim that personality prediction from multimodal signals can effectively substitute for ground-truth personality annotations in real-world applications remains largely theoretical, with limited empirical validation on actual personality-predicted scenarios.

## Next Checks

1. **Generalization Testing**: Evaluate the model on a real-world dataset (not constructed from TV shows) to assess whether personality-aware reasoning maintains performance benefits in authentic human interaction scenarios.

2. **Personality Model Comparison**: Re-run experiments using Big Five personality traits instead of MBTI to determine if the observed benefits are specific to the MBTI framework or generalize to other personality models with stronger psychological validity.

3. **Ablation on Personality Signal Quality**: Systematically degrade the quality of personality annotations (adding noise, using predicted personalities from the auxiliary task) to determine the threshold at which personality incorporation ceases to provide benefits, establishing robustness requirements.