---
ver: rpa2
title: Graph Positional Encoding via Random Feature Propagation
arxiv_id: '2303.02918'
source_url: https://arxiv.org/abs/2303.02918
tags:
- graph
- propagation
- features
- node
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Random Feature Propagation (RFP), a novel
  positional encoding scheme that combines random node features with iterative propagation
  to approximate dominant eigenvectors of graph operators. RFP alternates between
  propagation using predefined (e.g., adjacency/Laplacian matrices) or learned operators
  and normalization steps starting from random features.
---

# Graph Positional Encoding via Random Feature Propagation

## Quick Facts
- arXiv ID: 2303.02918
- Source URL: https://arxiv.org/abs/2303.02918
- Authors: 
- Reference count: 40
- Key outcome: RFP outperforms natural baselines like random features and Laplacian eigenvectors on node and graph classification tasks, with orthonormalization consistently outperforming ℓ2 normalization.

## Executive Summary
This paper introduces Random Feature Propagation (RFP), a novel positional encoding scheme that combines random node features with iterative propagation to approximate dominant eigenvectors of graph operators. RFP alternates between propagation using predefined (e.g., adjacency/Laplacian matrices) or learned operators and normalization steps starting from random features. The resulting feature trajectory is used as input to GNNs, capturing both random and spectral information. Theoretical analysis shows RFP can implement triangle counting and inherits universal approximation properties. Empirically, RFP outperforms natural baselines like random features and Laplacian eigenvectors on both node and graph classification tasks, with the DSS-GNN architecture further improving results. Orthonormalization consistently outperforms ℓ2 normalization across experiments.

## Method Summary
RFP starts with random node features and performs P iterations of propagation (using a matrix S like adjacency or Laplacian) followed by normalization, creating a trajectory that approximates dominant eigenvectors while retaining random feature diversity. The method supports both predefined operators (adjacency, Laplacian) and learned operators with attention mechanisms. Multiple random initializations (trajectories) are concatenated to improve expressiveness and enable substructure counting. The final RFP features are combined with input features, embedded, and processed by a GNN or DSS-GNN architecture for classification tasks.

## Key Results
- RFP with orthonormalization consistently outperforms ℓ2 normalization and natural baselines across node and graph classification tasks
- Multiple random trajectories enable substructure counting (e.g., triangle counting) through the Hutchinson trace estimator
- The DSS-GNN architecture further improves performance when combined with RFP features
- RFP inherits universal approximation properties while capturing both random and spectral information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RFP combines random features and spectral positional encoding by iteratively propagating random node features through a propagation operator and normalizing them, creating a trajectory that approximates dominant eigenvectors.
- Mechanism: The RFP process starts with random node features and performs P iterations of propagation (using a matrix S like adjacency or Laplacian) followed by normalization. This is equivalent to the power method or subspace iteration method for computing dominant eigenvectors, so the final features approximate the eigenvectors while retaining random feature diversity.
- Core assumption: The propagation operator S has a simple spectrum (distinct eigenvalues in magnitude) and the random initialization has non-zero projection onto the dominant eigenvector subspace.
- Evidence anchors:
  - [abstract] "Random Feature Propagation (RFP), is inspired by the power iteration method and its generalizations. It concatenates several intermediate steps of an iterative algorithm for computing the dominant eigenvectors of a propagation matrix, starting from random node features."
  - [section] "When the input matrix represents either the graph adjacency or the Laplacian matrix, the output of these algorithms is the spectral PE mentioned above."
  - [corpus] Weak - no direct corpus evidence linking power iteration to graph PE specifically.
- Break condition: If S has repeated eigenvalues or the initial random features lie entirely in the orthogonal complement of the dominant eigenvector subspace, convergence to the dominant eigenvectors fails.

### Mechanism 2
- Claim: Using multiple random initializations (trajectories) improves expressiveness and allows counting substructures like triangles.
- Mechanism: Each trajectory starts from independent random features. The downstream GNN processes these trajectories, and the combination allows the model to approximate the Hutchinson trace estimator for counting triangles (Trace(A³)/6). Multiple trajectories provide Monte Carlo averaging that reduces variance.
- Core assumption: The random features are drawn from a distribution with sufficient support (e.g., Rademacher or normal) and the GNN can memorize the required products of propagated features.
- Evidence anchors:
  - [abstract] "we provide theoretical justifications for using random features, for incorporating early propagation steps, and for using multiple random initializations."
  - [section] "we prove that MPNNs can use these early-stage features in order to extract structural information, such as the number of triangles in the graph."
  - [corpus] Weak - no corpus papers directly validate the triangle counting claim via RFP.
- Break condition: If the number of trajectories is too small relative to the variance of the estimator, or if the GNN architecture cannot learn the required memorization, the substructure counting fails.

### Mechanism 3
- Claim: Orthonormalization normalization outperforms ℓ2 normalization in practice and theoretically ensures convergence to the dominant k eigenvectors.
- Mechanism: ℓ2 normalization normalizes each feature channel independently, mimicking the power method for a single eigenvector. Orthonormalization (via QR decomposition) jointly normalizes across all k channels, mimicking the subspace iteration method, which converges to the k dominant eigenvectors simultaneously. This richer trajectory is more informative for the GNN.
- Core assumption: The initial random features are continuous i.i.d. with a density, ensuring full rank after orthonormalization with probability 1.
- Evidence anchors:
  - [abstract] "Orthonormalization consistently outperforms ℓ2 normalization across experiments."
  - [section] "Normalization according to Equation (6) offers a key difference compared to the ℓ2 normalization from Equation (5): in the former, the iterations described in Equation (1) take the name of Subspace Iteration Method... In Section 4, we show that starting with RNF drawn from a continuous distribution, this process almost surely converges to the dominant k eigenvectors of the propagation operator S."
  - [corpus] Weak - no corpus evidence comparing orthonormalization vs ℓ2 in graph PE contexts.
- Break condition: If the number of eigenvectors k ≥ n (full rank), orthonormalization cannot proceed; also if the propagation operator is defective (non-diagonalizable), convergence guarantees fail.

## Foundational Learning

- Concept: Power iteration method for eigenvalue computation
  - Why needed here: RFP is explicitly based on power iteration and its generalizations; understanding this links the propagation steps to spectral properties.
  - Quick check question: What is the update rule in the power iteration method for finding the dominant eigenvector of a matrix S?

- Concept: Subspace iteration method (generalization of power iteration)
  - Why needed here: Orthonormalization in RFP implements subspace iteration, which computes multiple dominant eigenvectors simultaneously; this explains why orthonormalization is theoretically justified.
  - Quick check question: How does subspace iteration differ from power iteration in terms of normalization and convergence properties?

- Concept: Random feature augmentation in GNNs
  - Why needed here: RFP builds on prior work using random node features to improve GNN expressiveness; knowing this context explains why starting from random features (not raw features) is beneficial.
  - Quick check question: Why do random node features improve the distinguishing power of message-passing GNNs?

## Architecture Onboarding

- Component map: Random feature sampler -> Propagation operator (predefined or learned) -> Normalization (ℓ2 or QR) -> Trajectory concatenation -> Embedding layer -> DSS-GNN or standard GNN backbone -> Readout
- Critical path: Random features -> Propagation steps -> Normalization -> Embedding -> GNN processing -> Task-specific output
- Design tradeoffs:
  - Predefined vs learned propagation operator: Predefined is simpler and interpretable but may not capture task-specific structure; learned can capture long-range dependencies but adds parameters and complexity.
  - ℓ2 vs QR normalization: ℓ2 is cheaper and mimics power method; QR ensures convergence to eigenvectors but is computationally heavier.
  - Single vs multiple trajectories: Multiple trajectories improve expressiveness and enable substructure counting but increase memory and computation.
- Failure signatures:
  - Poor performance despite correct implementation: Likely due to insufficient number of propagation steps P or eigenvectors k, or inappropriate normalization choice.
  - Training instability: Could arise from learned propagation operator producing ill-conditioned scores; check attention score magnitudes.
  - No improvement over baselines: May indicate the graph operator chosen does not capture relevant structure for the task.
- First 3 experiments:
  1. Implement RFP with predefined adjacency operator, ℓ2 normalization, P=4, k=4 on a small node classification dataset (e.g., Cora) using a simple GNN backbone; compare with raw features and RNF baselines.
  2. Switch to QR normalization while keeping other settings; verify if performance improves as claimed.
  3. Add a second trajectory (B=2) and process with DSS-GNN; measure impact on accuracy and check if substructure counting emerges on synthetic triangle-counting tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of normalization function (ℓ2 vs QR) impact the convergence rate of RFP trajectories to dominant eigenvectors?
- Basis in paper: [explicit] The paper compares ℓ2 normalization and QR orthonormalization, showing QR consistently outperforms ℓ2 in experiments, but doesn't analyze convergence rates theoretically.
- Why unresolved: The paper proves QR orthonormalization converges almost surely to dominant eigenvectors, but doesn't quantify the speed of convergence compared to ℓ2 normalization.
- What evidence would resolve it: Empirical studies measuring eigenvector approximation error at each iteration for both normalization methods on various graph types.

### Open Question 2
- Question: What is the optimal frequency of normalization steps (w) in RFP for different graph operators and tasks?
- Basis in paper: [explicit] The paper uses w=1 in most experiments but mentions w as a hyperparameter without systematically exploring its impact.
- Why unresolved: While the paper shows RFP works well with normalization at every step, it doesn't investigate whether less frequent normalization could be equally effective or computationally beneficial.
- What evidence would resolve it: Ablation studies varying w across different graph operators (adjacency, Laplacian, learned) and task types.

### Open Question 3
- Question: How does the dimensionality of random node features (k) relate to the spectral gap of the propagation operator?
- Basis in paper: [inferred] The paper shows k affects performance but doesn't explore the theoretical relationship between k and the spectral properties of the operator.
- Why unresolved: The paper proves convergence to k dominant eigenvectors when eigenvalues are distinct, but doesn't address how to choose k based on the operator's spectral gap.
- What evidence would resolve it: Theoretical analysis connecting k to spectral gap width, and empirical validation on graphs with varying spectral gaps.

### Open Question 4
- Question: Can learned propagation operators capture long-range dependencies more effectively than predefined operators on specific graph types?
- Basis in paper: [explicit] The paper introduces learned operators with attention mechanisms but doesn't systematically compare their effectiveness across different graph families.
- Why unresolved: While the paper shows learned operators work well on molecular datasets, it doesn't analyze when they provide advantages over predefined operators for different graph structures.
- What evidence would resolve it: Comparative studies on graphs with varying locality properties (grid-like, scale-free, community-structured) measuring the performance gap between learned and predefined operators.

## Limitations

- The core mechanisms rely on spectral properties of the propagation operator that are only guaranteed under restrictive conditions (distinct eigenvalues, non-defective matrices)
- The theoretical justification for substructure counting via multiple trajectories assumes idealized conditions (infinite trajectories, perfect memorization by the GNN) that may not hold in practice
- Hyperparameter sensitivity, particularly for the number of trajectories B and propagation steps P, is not fully characterized

## Confidence

- **High confidence**: RFP as a practical method for combining random features with spectral information; orthonormalization consistently outperforming ℓ2 normalization empirically
- **Medium confidence**: The power iteration interpretation of RFP's mechanics; theoretical connection to universal approximation properties
- **Low confidence**: The practical effectiveness of substructure counting (triangle counting) via multiple random trajectories; the extent to which learned propagation operators improve performance over predefined ones

## Next Checks

1. Implement RFP on synthetic graphs with known triangle counts to verify whether multiple trajectories enable accurate triangle counting as predicted by the theoretical analysis
2. Systematically vary the number of trajectories B and propagation steps P to characterize the trade-off between performance and computational cost, and identify the point of diminishing returns
3. Compare RFP with learned propagation operators against RFP with predefined operators across diverse graph types to quantify the practical benefit of learning versus the simplicity of predefined matrices