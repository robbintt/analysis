---
ver: rpa2
title: Unsupervised Adaptation of Polyp Segmentation Models via Coarse-to-Fine Self-Supervision
arxiv_id: '2308.06665'
source_url: https://arxiv.org/abs/2308.06665
tags:
- adaptation
- domain
- target
- learning
- rpanet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses source-free domain adaptation (SFDA) for
  polyp segmentation, a practical setting where source data is unavailable due to
  privacy concerns. The proposed Region-to-Pixel Adaptation Network (RPANet) learns
  discriminative representations in a coarse-to-fine manner through two modules: Foreground-aware
  Contrastive Learning (FCL) and Confidence-Calibrated Pseudo-Labeling (CCPL).'
---

# Unsupervised Adaptation of Polyp Segmentation Models via Coarse-to-Fine Self-Supervision

## Quick Facts
- arXiv ID: 2308.06665
- Source URL: https://arxiv.org/abs/2308.06665
- Reference count: 39
- Primary result: RPANet achieves mean Dice scores of 80.0%, 63.2%, and 85.8% on ClinicDB, ETIS-LARIB, and Kvasir-SEG datasets respectively, outperforming state-of-the-art SFDA methods by up to 14.3% absolute improvement

## Executive Summary
This paper addresses source-free domain adaptation (SFDA) for polyp segmentation, a practical setting where source data is unavailable due to privacy concerns. The proposed Region-to-Pixel Adaptation Network (RPANet) learns discriminative representations in a coarse-to-fine manner through two modules: Foreground-aware Contrastive Learning (FCL) and Confidence-Calibrated Pseudo-Labeling (CCPL). FCL contrasts region centroids across images using supervised contrastive learning, while CCPL refines pseudo-labels via a fusion strategy to reduce overconfidence. Evaluated on three cross-domain polyp segmentation tasks, RPANet demonstrates significant performance improvements without access to source data.

## Method Summary
RPANet adapts a pre-trained polyp segmentation model to target domains using only unlabeled target images. The method consists of two complementary modules: FCL performs supervised contrastive learning at the region level by computing centroids weighted by prediction confidence, while CCPL generates refined pseudo-labels through a confidence-calibrated fusion strategy. The network is trained using a combination of FCL and CCPL losses, with the target model initialized from source-trained weights. The coarse-to-fine approach first establishes region-level discriminative representations, then refines pixel-level predictions.

## Key Results
- RPANet achieves 80.0% mean Dice on ClinicDB (14.3% absolute improvement over best baseline)
- RPANet achieves 63.2% mean Dice on ETIS-LARIB (12.5% absolute improvement over best baseline)
- RPANet achieves 85.8% mean Dice on Kvasir-SEG (10.2% absolute improvement over best baseline)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised contrastive learning with region centroids improves robustness to noisy pseudo-labels by averaging over regions instead of individual pixels.
- Mechanism: The Foreground-aware Contrastive Learning (FCL) module computes region centroids weighted by prediction confidence, then contrasts these centroids across images. This averaging reduces noise impact since each centroid represents multiple pixels rather than a single pixel.
- Core assumption: Region centroids computed from noisy pseudo-labels still preserve meaningful semantic information about the target domain structure.
- Evidence anchors:
  - [abstract]: "FCL introduces a supervised contrastive learning paradigm in the region level to contrast different region centroids across different target images, which efficiently involves all pseudo labels while robust to noisy samples"
  - [section]: "we propose the FCL module to contrastively learn region-level discriminative representations in a fully supervised way. Here, we use the region centroid to stand for the very region to reduce computational cost."
  - [corpus]: Weak evidence - corpus papers discuss contrastive learning but don't specifically address region-level contrastive approaches in source-free domain adaptation
- Break condition: If pseudo-label noise is so severe that region centroids become meaningless representations of foreground/background regions

### Mechanism 2
- Claim: Confidence-calibrated pseudo-label fusion reduces overconfident predictions by smoothing probability distributions through normalization.
- Mechanism: The CCPL module fuses current and previous predictions using a normalization term ϕ(a,b) = √Σ(a²ᵢ + b²ᵢ) that smooths the pseudo-label probability distribution, preventing any single pixel from dominating the prediction.
- Core assumption: Smoothing probability distributions through fusion of current and historical predictions reduces overconfidence without sacrificing accuracy.
- Evidence anchors:
  - [abstract]: "CCPL designs a novel fusion strategy to reduce the overconfidence problem of pseudo labels by fusing two different target predictions without introducing any additional network modules"
  - [section]: "The functionϕ(a, b) = √Σ|a|(a²ᵢ + b²ᵢ) is a normalization term, which could smooth the pseudo label probability distribution to avoid overconfident predictions (dominate pixels)"
  - [corpus]: No direct evidence in corpus - corpus papers focus on different aspects of source-free domain adaptation
- Break condition: If the smoothing becomes too aggressive and loses discriminative power between foreground and background regions

### Mechanism 3
- Claim: Coarse-to-fine self-supervision progressively refines discriminative representations from region-level to pixel-level.
- Mechanism: FCL provides region-level discriminative initialization that CCPL uses to refine pixel-level pseudo-labels. This two-stage approach first establishes coarse structure then refines details.
- Core assumption: Region-level representations learned by FCL provide meaningful initialization that improves CCPL's ability to generate accurate pixel-level pseudo-labels.
- Evidence anchors:
  - [abstract]: "which learns the region-level and pixel-level discriminative representations through coarse-to-fine self-supervision"
  - [section]: "FCL and CCPL are complementary to each other, i.e., FCL enhances the robustness of CCPL by providing region-level discriminative representations as an initialization, while CCPL refines the target pseudo labels to mitigate the bias introduced by FCL"
  - [corpus]: Weak evidence - corpus papers discuss self-supervision but don't specifically address coarse-to-fine approaches
- Break condition: If the region-level initialization from FCL is too coarse to provide meaningful guidance for pixel-level refinement

## Foundational Learning

- Concept: Supervised contrastive learning
  - Why needed here: Provides a way to learn discriminative representations without source data by contrasting region centroids from target domain pseudo-labels
  - Quick check question: How does supervised contrastive learning differ from standard contrastive learning in this context?

- Concept: Pseudo-labeling with confidence calibration
  - Why needed here: Enables self-training in source-free setting while addressing overconfidence and error accumulation problems
  - Quick check question: What specific problem does the normalization term ϕ(a,b) solve in pseudo-label generation?

- Concept: Coarse-to-fine representation learning
  - Why needed here: Allows progressive refinement of representations from coarse (region-level) to fine (pixel-level) details
  - Quick check question: Why might learning region-level representations first be more effective than directly learning pixel-level representations?

## Architecture Onboarding

- Component map:
  Source model fs -> Target model ft (initialized with fs weights) -> FCL module (region centroid computation and contrastive learning) -> CCPL module (pseudo-label fusion) -> Loss functions (LFCL, LCCPL, LRPANet)

- Critical path:
  1. Source model generates initial predictions on target images
  2. FCL computes region centroids and performs contrastive learning
  3. CCPL generates pseudo-labels using fusion strategy
  4. Target model is trained using both FCL and CCPL losses
  5. Process iterates for multiple epochs

- Design tradeoffs:
  - Region vs pixel-level contrastive learning: Region-level reduces noise but may miss fine details
  - Fusion strategy parameters: α=0.5 balances current and historical predictions
  - Loss weighting: β and γ control importance of FCL vs CCPL

- Failure signatures:
  - Poor performance despite training: Check if pseudo-labels are too noisy for FCL to extract meaningful centroids
  - Overfitting to target domain: Check if FCL loss weight is too high
  - Underfitting: Check if CCPL fusion is too aggressive and smoothing out useful information

- First 3 experiments:
  1. Test FCL alone with fixed pseudo-labels to verify contrastive learning works
  2. Test CCPL alone with ground-truth labels to verify pseudo-label fusion improves over single predictions
  3. Test complete RPANet with reduced training epochs to verify coarse-to-fine progression

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can RPANet be extended to handle multi-source domain adaptation scenarios where source data from multiple hospitals is available?
- Basis in paper: [inferred] The paper focuses on single-source SFDA but mentions the practical setting of adapting models trained on data from "a small number of hospitals" to other hospitals.
- Why unresolved: The paper only evaluates single-source adaptation and does not explore scenarios with multiple source domains.
- What evidence would resolve it: Experimental results comparing multi-source SFDA variants of RPANet against single-source approaches on datasets with multiple source domains.

### Open Question 2
- Question: What is the impact of different temperature values (τ) in the FCL module on RPANet's performance across various polyp segmentation datasets?
- Basis in paper: [explicit] The paper sets τ=0.1 but does not provide sensitivity analysis for this hyperparameter.
- Why unresolved: Only mean Dice scores are reported for different β and γ values, but τ sensitivity is not explored.
- What evidence would resolve it: Systematic experiments varying τ values and reporting performance metrics across multiple datasets.

### Open Question 3
- Question: How does RPANet's performance scale with increasingly noisy pseudo-labels during the adaptation process?
- Basis in paper: [inferred] The paper claims FCL is "robust to noisy samples" but doesn't empirically validate this under varying noise levels.
- Why unresolved: The experiments use pseudo-labels from a pre-trained source model without introducing controlled noise levels to test robustness claims.
- What evidence would resolve it: Experiments introducing different levels of label noise and measuring degradation in RPANet's performance compared to baseline methods.

### Open Question 4
- Question: Can the coarse-to-fine self-supervision approach be generalized to other medical image segmentation tasks beyond polyp detection?
- Basis in paper: [explicit] The authors state their results "reveal the potential of SFDA in medical applications" suggesting broader applicability.
- Why unresolved: The paper only validates on polyp segmentation datasets without testing on other medical segmentation tasks.
- What evidence would resolve it: Successful application and performance improvements of RPANet on other medical segmentation tasks like organ segmentation or tumor detection.

## Limitations
- Performance relies heavily on quality of pseudo-labels generated by source model, which may not transfer well to domains with significantly different polyp appearances
- Method only validated on three specific polyp segmentation datasets; generalizability to other medical imaging tasks or non-medical domains untested
- Coarse-to-fine framework is conceptually appealing but lacks ablation studies showing why this specific two-stage approach outperforms alternatives

## Confidence
- High Confidence: FCL module's region centroid contrastive learning is technically sound and well-supported by contrastive learning literature
- Medium Confidence: CCPL module's confidence-calibrated fusion strategy addresses real overconfidence problem, but exact impact not clearly isolated
- Low Confidence: Claimed "coarse-to-fine" progression between FCL and CCPL is more conceptual framing than rigorously demonstrated mechanism

## Next Checks
1. **Ablation on Region vs Pixel Contrastive Learning**: Remove FCL entirely and replace with pixel-level supervised contrastive learning using the same pseudo-labels to directly test whether region-level averaging provides meaningful noise robustness.

2. **Pseudo-Label Quality Analysis**: Quantitatively measure the entropy and confidence distribution of pseudo-labels generated by FCL vs CCPL across training epochs to reveal whether the method actually reduces overconfident predictions as claimed.

3. **Cross-Domain Generalizability Test**: Apply RPANet to a different medical segmentation task (e.g., cardiac MRI segmentation) with similar source-free constraints to validate whether the coarse-to-fine self-supervision approach generalizes beyond polyp segmentation.