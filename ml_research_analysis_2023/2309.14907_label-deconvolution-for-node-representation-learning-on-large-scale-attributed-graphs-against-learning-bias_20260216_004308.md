---
ver: rpa2
title: Label Deconvolution for Node Representation Learning on Large-scale Attributed
  Graphs against Learning Bias
arxiv_id: '2309.14907'
source_url: https://arxiv.org/abs/2309.14907
tags:
- node
- labels
- graph
- training
- gnns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the learning bias issue in separate training
  of node encoders (NEs) and graph neural networks (GNNs) on large-scale attributed
  graphs. The bias arises because NEs are trained without considering feature convolutions
  in GNNs.
---

# Label Deconvolution for Node Representation Learning on Large-scale Attributed Graphs against Learning Bias

## Quick Facts
- arXiv ID: 2309.14907
- Source URL: https://arxiv.org/abs/2309.14907
- Reference count: 40
- One-line primary result: LD significantly outperforms state-of-the-art methods in node classification on OGB datasets by addressing learning bias through inverse label generation.

## Executive Summary
This paper addresses the learning bias issue in separate training of node encoders (NEs) and graph neural networks (GNNs) on large-scale attributed graphs. The bias arises because NEs are trained without considering feature convolutions in GNNs. To alleviate this, the authors propose Label Deconvolution (LD), which approximates the inverse mapping of GNNs to generate inverse labels for NEs. This allows NEs to be trained with supervision signals that incorporate both node labels and graph structures, similar to joint training. Theoretically, LD converges to the optimal objective function values of joint training under mild assumptions. Experiments on Open Graph Benchmark datasets show that LD significantly outperforms state-of-the-art methods in node classification tasks, achieving higher accuracy with lower computational costs.

## Method Summary
Label Deconvolution (LD) is a two-phase training approach that addresses learning bias between node encoders and GNNs. In the pre-processing phase, LD computes i-hop label propagations to approximate the inverse mapping of GNNs. During the training phase, NEs are fine-tuned using these inverse labels as supervision, allowing them to learn node features that are compatible with subsequent GNN convolutions. The method uses spectral-based GNNs and proposes an efficient approximation to the inverse mapping. LD decouples feature convolutions from non-linear transformations, enabling memory and time-efficient pre-processing. The approach is evaluated on large-scale attributed graphs from the Open Graph Benchmark (OGB) datasets, demonstrating improved accuracy and reduced computational costs compared to joint training and other state-of-the-art methods.

## Key Results
- LD achieves higher node classification accuracy than joint training and existing methods (GIANT, GLEM) on OGB datasets
- LD is significantly faster than GLEM, requiring only one fine-tuning of pre-trained NEs and two feature inferences
- LD converges to the optimal objective function values of joint training under mild assumptions, as proven theoretically

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Label Deconvolution (LD) approximates the inverse mapping of GNNs to generate inverse labels that preserve both node labels and graph structural information.
- Mechanism: LD uses a spectral-based GNN formulation to decouple linear feature convolutions from non-linear transformations. It then parameterizes the inverse labels using a linear combination of i-hop label propagations (weighted sum of the true labels and their i-hop neighbors' labels).
- Core assumption: The node labels depend on both node attributes and graph structures, i.e., Y = ϕ(Â)ψ(F*), where ϕ is an invertible polynomial graph signal filter and ψ is a non-linear encoder.
- Evidence anchors:
  - [abstract] "LD converges to the optimal objective function values by the joint training under mild assumptions."
  - [section 4.2] "We show minθ L(GNN (F(βLD), A; θ), Y) = 0 by the following theorem."
  - [corpus] Weak: Related papers focus on generalization and expressivity of GNNs but do not directly validate LD's inverse label approximation mechanism.
- Break condition: If the graph signal filter ϕ(Â) is not invertible or the node labels do not depend on both attributes and structures, LD cannot recover the optimal objective.

### Mechanism 2
- Claim: LD's pre-processing of inverse labels is memory and time efficient compared to joint training.
- Mechanism: By decoupling feature convolutions and using fixed node labels, LD pre-computes i-hop label propagations once and reuses them during training, avoiding repeated expensive graph convolutions.
- Core assumption: The node labels are fixed during training, allowing pre-computation of i-hop labels.
- Evidence anchors:
  - [section 3.2] "As the node labels are fixed during training, LD pre-processes the memory and time-consuming inverse mapping of the feature convolution only once based on the fixed node labels."
  - [section 5.2] "LD is significantly faster than GLEM... LD only fine-tunes pre-trained NEs once and infers node features twice."
  - [corpus] Weak: Related work on scalable GNNs discusses memory efficiency but does not specifically address LD's pre-processing strategy.
- Break condition: If node labels change during training (e.g., in dynamic graphs), the pre-computation advantage disappears.

### Mechanism 3
- Claim: LD's inverse labels reduce label noise from graph structures by preserving attribute semantics.
- Mechanism: The inverse labels incorporate both true labels and i-hop neighbor labels, providing supervision that reflects both node attributes and local graph topology, reducing noise from heterogeneous neighborhoods.
- Core assumption: Nodes with similar attributes should have similar labels even if their graph neighborhoods differ.
- Evidence anchors:
  - [section 5.5] "Fig. 4 shows that the inverse labels provide similar supervision signals for the nodes with similar texts and different supervision signals for the nodes with different texts."
  - [section 5.4] "XLD significantly improves the prediction performance on all datasets against XWO/LD."
  - [corpus] Weak: Related papers on graph representation learning do not directly test label noise reduction via inverse labels.
- Break condition: If the graph structure is highly heterophilic (connected nodes have different labels), the i-hop label propagation may introduce more noise than it removes.

## Foundational Learning

- Concept: Spectral graph theory and polynomial graph filters
  - Why needed here: LD relies on representing GNNs as polynomial spectral filters ϕ(Â) to enable efficient inversion and label deconvolution.
  - Quick check question: Can you explain how the Cayley-Hamilton theorem allows expressing the inverse of a polynomial matrix as a linear combination of lower powers?

- Concept: Graph neural network expressivity and 1-WL test
  - Why needed here: LD's theoretical analysis assumes that spectral-based GNNs are as expressive as the 1-WL test, which bounds the representational power of many GNN architectures.
  - Quick check question: What is the relationship between the expressiveness of spectral-based GNNs and the 1-WL test under the assumptions in the paper?

- Concept: Pre-trained language models for node attribute encoding
  - Why needed here: LD integrates large pre-trained models (e.g., ESM2 for protein sequences, DeBERTa for text) as node encoders to extract high-quality node features before graph convolutions.
  - Quick check question: How does the choice of pre-trained model (e.g., ESM2 vs. DeBERTa) affect the quality of node features and subsequent label deconvolution performance?

## Architecture Onboarding

- Component map: Pre-processing (compute i-hop label propagations) -> NE fine-tuning (with inverse labels) -> GNN training (with fixed features)
- Critical path: Pre-processing → NE fine-tuning with inverse labels → GNN training with fixed features
- Design tradeoffs:
  - Larger N increases expressiveness but risks overfitting and over-smoothing
  - Higher α emphasizes inverse labels but may destabilize training if inverse labels are noisy
  - Using fixed vs. learnable node labels during pre-processing affects generalization
- Failure signatures:
  - Degraded performance when N is too large (overfitting)
  - Unstable training when α is too high (label noise amplification)
  - Memory overflow if i-hop label propagations are not properly batched
- First 3 experiments:
  1. Ablation study: Compare LD with and without inverse labels (α = 0) on a small dataset
  2. Hyperparameter sensitivity: Test different N values on a validation set to find optimal expressiveness
  3. Runtime comparison: Measure pre-processing time and training time per epoch vs. joint training baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the convergence of LD to the optimal objective function values depend on the properties of the graph (e.g., degree distribution, sparsity)?
- Basis in paper: [explicit] The authors state that LD converges to the optimal objective function values by the joint training under mild assumptions, but do not provide a detailed analysis of how graph properties affect convergence.
- Why unresolved: The proof of Theorem 1 assumes invertibility of the graph signal filter, but does not explore how graph topology impacts this assumption or convergence rate.
- What evidence would resolve it: Empirical studies varying graph properties (e.g., random graphs vs. real-world graphs, varying average degrees) and analyzing LD's convergence behavior.

### Open Question 2
- Question: What is the impact of the number of inverse label layers (N) on the performance and stability of LD?
- Basis in paper: [explicit] The authors mention that N impacts the expressiveness of label deconvolution and can lead to overfitting with large N, but do not provide a systematic study of its effect.
- Why unresolved: While the authors provide some empirical results for different N values, they do not analyze the trade-off between expressiveness and overfitting in detail.
- What evidence would resolve it: A comprehensive study varying N and analyzing its impact on performance, stability, and computational complexity.

### Open Question 3
- Question: How does LD compare to other methods that incorporate graph structures into node feature learning (e.g., GIANT, GLEM) in terms of scalability and performance on different types of attributed graphs?
- Basis in paper: [inferred] The authors compare LD to GIANT and GLEM on specific datasets but do not provide a comprehensive analysis of their relative strengths and weaknesses across different graph types and scales.
- Why unresolved: The paper focuses on LD's performance on OGB datasets but does not explore its behavior on other types of attributed graphs (e.g., social networks, biological networks) or its scalability to extremely large graphs.
- What evidence would resolve it: Empirical studies comparing LD to other methods on a diverse set of attributed graphs with varying scales and characteristics.

## Limitations
- The theoretical convergence proof assumes invertibility of the graph signal filter, which may not hold for sparse graphs or when node labels have limited dependency on attributes.
- LD's effectiveness is demonstrated on specific OGB datasets but not extensively tested on graphs with different homophily levels or dynamic graphs where node labels change during training.
- The paper does not provide a detailed analysis of how graph properties (e.g., degree distribution, sparsity) affect LD's convergence and performance.

## Confidence

- **High:** LD's computational efficiency advantage over joint training and its improved accuracy over existing methods on OGB datasets
- **Medium:** The mechanism by which inverse labels preserve attribute semantics and reduce label noise
- **Medium:** The theoretical convergence to joint training's optimal objective under mild assumptions

## Next Checks

1. Test LD on graphs with varying homophily levels to assess performance when i-hop label propagation introduces noise
2. Evaluate memory and runtime benefits on graphs where node labels change during training to verify pre-computation advantages
3. Conduct ablation studies varying N and α to determine sensitivity and optimal hyperparameter ranges across different GNN architectures