---
ver: rpa2
title: Fast Locality Sensitive Hashing with Theoretical Guarantee
arxiv_id: '2309.15479'
source_url: https://arxiv.org/abs/2309.15479
tags:
- fastlsh
- hashing
- ndmaxmin
- random
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational bottleneck in Locality-Sensitive
  Hashing (LSH) when dealing with high-dimensional data and many hash functions. The
  proposed FastLSH method combines random sampling and random projection to reduce
  the hashing time complexity from O(n) to O(m) (where m < n is the number of sampled
  dimensions).
---

# Fast Locality Sensitive Hashing with Theoretical Guarantee

## Quick Facts
- arXiv ID: 2309.15479
- Source URL: https://arxiv.org/abs/2309.15479
- Reference count: 40
- Primary result: Achieves up to 80x speedup in hash function evaluation while maintaining comparable accuracy to E2LSH

## Executive Summary
This paper introduces FastLSH, a novel approach to accelerate Locality-Sensitive Hashing (LSH) for high-dimensional approximate nearest neighbor search. The method combines random sampling and random projection to reduce the hashing time complexity from O(n) to O(m) where m < n is the number of sampled dimensions. FastLSH maintains provable LSH properties through theoretical analysis, showing asymptotic equivalence to the classic E2LSH scheme. Experiments on 12 real and synthetic datasets demonstrate that FastLSH achieves comparable accuracy and query efficiency to E2LSH while providing significant speedup in hash function evaluation.

## Method Summary
FastLSH addresses the computational bottleneck in LSH by replacing full inner product computation with random sampling and projection. For each hash function, FastLSH samples m dimensions uniformly at random to form a reduced vector, then computes a single random projection on this smaller vector instead of the full n-dimensional vector. This reduces per-hash computation from O(n) to O(m). The method maintains the LSH property through asymptotic equivalence to E2LSH, with the probability of collision converging to E2LSH's collision probability as m increases. The approach is simple to implement and can be extended to other similarity metrics, making it a promising alternative for high-dimensional approximate nearest neighbor search.

## Key Results
- Achieves up to 80x speedup in hash function evaluation compared to E2LSH
- Maintains comparable accuracy and query efficiency to E2LSH across 12 real and synthetic datasets
- Reduces time complexity from O(n) to O(m) where m < n is the number of sampled dimensions
- Simple to implement and extensible to other similarity metrics

## Why This Works (Mechanism)

### Mechanism 1
- FastLSH achieves sublinear hashing time by replacing full inner product computation with random sampling and projection
- For each hash function, FastLSH samples m dimensions uniformly at random to form a reduced vector, then computes a single random projection on this smaller vector instead of the full n-dimensional vector
- Core assumption: The sampled subset of dimensions sufficiently preserves the distance distribution of the original high-dimensional vectors
- Break condition: If the sampled dimensions are highly correlated or concentrated in subspaces that don't capture the original vector's structure, the distance preservation will fail

### Mechanism 2
- FastLSH maintains the locality-sensitive hashing property through asymptotic equivalence to E2LSH
- The probability of collision for FastLSH is derived using truncated normal distributions and shown to converge to the E2LSH collision probability as m increases
- Core assumption: The sum of squared distances in the sampled dimensions converges to a normal distribution via the Central Limit Theorem, and the truncated normal distribution adequately models the non-negative squared distances
- Break condition: For very small m relative to n, the CLT approximation breaks down and the truncated normal assumption may not hold accurately

### Mechanism 3
- FastLSH maintains competitive accuracy while providing speedup because the impact of the sampling variance σ is controlled by choosing appropriate m
- The analysis shows that the first four moments of FastLSH's collision probability match E2LSH's moments, and the deviation terms decrease with m
- Core assumption: Matching the first four moments is sufficient to preserve the essential shape of the collision probability distribution for practical purposes
- Break condition: If the data distribution has very high variance in certain dimensions, the sampling variance σ may remain significant even for moderate m, degrading accuracy

## Foundational Learning

- Concept: Locality-Sensitive Hashing (LSH) and the (R, cR, p1, p2)-sensitive property
  - Why needed here: FastLSH must prove it maintains the LSH property to guarantee theoretical performance bounds
  - Quick check question: What conditions must a hash family satisfy to be considered (R, cR, p1, p2)-sensitive?

- Concept: p-stable distributions and their use in LSH for l2 norm
  - Why needed here: Understanding why E2LSH uses Gaussian projections is key to grasping FastLSH's modifications
  - Quick check question: Why does the Gaussian distribution enable E2LSH to work for l2 distance?

- Concept: Central Limit Theorem and truncated normal distributions
  - Why needed here: The theoretical analysis relies on CLT convergence and truncated normals to model sampled distances
  - Quick check question: How does the truncated normal distribution handle the non-negativity constraint of squared distances?

## Architecture Onboarding

- Component map: Sampling module -> Projection module -> Hashing wrapper -> Integration layer
- Critical path:
  1. Sample m indices from n dimensions
  2. Extract corresponding dimensions from input vector
  3. Generate m-dimensional Gaussian projection vector
  4. Compute inner product and add random offset
  5. Apply floor division with bucket width to produce hash
- Design tradeoffs:
  - Speed vs accuracy: Smaller m gives faster hashing but potentially lower accuracy; trade-off controlled by m selection
  - Memory vs speed: Sampling requires storing m indices per hash; negligible compared to storing full vectors
  - Implementation simplicity vs performance: FastLSH is simpler than complex LSH variants but may not outperform them in all scenarios
- Failure signatures:
  - Accuracy degradation: If recall drops significantly compared to E2LSH with same parameters
  - Sampling artifacts: If certain dimensions are systematically excluded due to sampling bias
  - Numerical instability: If bucket width is too small relative to projection values
- First 3 experiments:
  1. Compare recall vs query time curves for FastLSH with different m values on a small dataset to find optimal m
  2. Verify that FastLSH maintains the LSH property by computing ρ(c) curves and comparing to E2LSH
  3. Benchmark hashing time per vector for FastLSH vs E2LSH on high-dimensional datasets to confirm O(m) vs O(n) complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact theoretical relationship between the sampling parameter m and the collision probability p(s,σ) for FastLSH, beyond the asymptotic analysis?
- Basis in paper: [explicit] The paper states that FastLSH's asymptotic behavior matches E2LSH but also notes that for limited m, the impact of σ is not negligible and needs to be controlled within a reasonable range
- Why unresolved: While the paper provides asymptotic analysis and empirical evidence, it doesn't give an exact closed-form expression for how m affects p(s,σ) for finite values
- What evidence would resolve it: A rigorous mathematical proof deriving the exact expression for p(s,σ) as a function of m, or extensive empirical measurements showing the precise relationship across different m values

### Open Question 2
- Question: How does FastLSH perform compared to other approximate nearest neighbor search methods beyond E2LSH and ACHash, such as HNSW or product quantization?
- Basis in paper: [inferred] The paper only compares FastLSH with E2LSH and ACHash, leaving out other popular ANN methods
- Why unresolved: The experimental results only show comparisons with two specific LSH-based methods, not providing a comprehensive view of FastLSH's performance relative to the broader field of ANN algorithms
- What evidence would resolve it: Experiments comparing FastLSH against other state-of-the-art ANN methods like HNSW, product quantization, or graph-based approaches on the same datasets and evaluation metrics

### Open Question 3
- Question: What is the optimal strategy for choosing the sampling parameter m for FastLSH in different application scenarios?
- Basis in paper: [explicit] The paper mentions that m is set to 30 throughout experiments but doesn't provide guidance on how to choose m for different data characteristics or performance requirements
- Why unresolved: While the paper demonstrates that FastLSH works well with m=30, it doesn't offer a principled approach to determine the optimal m value for different datasets or use cases
- What evidence would resolve it: A theoretical framework or empirical study showing how to choose m based on factors like dataset size, dimensionality, target accuracy, or query time constraints

## Limitations
- Theoretical analysis assumes asymptotic behavior (m → ∞) while practical experiments use m=30, creating a gap between theory and practice
- Claim that matching first four moments ensures adequate distance preservation lacks empirical validation across diverse data distributions
- Sampling strategy may underperform on data with high variance concentrated in non-uniform dimensions, though this scenario is not extensively tested

## Confidence

**High Confidence**: O(m) complexity improvement over O(n) - directly verifiable through time measurements
**Medium Confidence**: Asymptotic equivalence to E2LSH - theoretical derivation appears sound but relies on CLT convergence assumptions
**Medium Confidence**: 80x speedup claim - depends on specific hardware and implementation details not fully disclosed
**Low Confidence**: Universality across all data distributions - limited testing on highly skewed or sparse data

## Next Checks

1. **Statistical Validation**: Perform permutation tests comparing distance distributions between FastLSH and E2LSH outputs across multiple datasets to empirically verify asymptotic equivalence claims for finite m values

2. **Extreme Case Testing**: Evaluate FastLSH on high-variance and sparse datasets (e.g., text embeddings, binary features) to identify performance degradation thresholds and establish practical limits

3. **Parameter Sensitivity Analysis**: Systematically vary m across several orders of magnitude (1 to 1000) while measuring recall, query time, and hashing time to map the complete accuracy-speed tradeoff curve and identify optimal operating points