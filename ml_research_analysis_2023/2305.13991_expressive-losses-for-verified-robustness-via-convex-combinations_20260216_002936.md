---
ver: rpa2
title: Expressive Losses for Verified Robustness via Convex Combinations
arxiv_id: '2305.13991'
source_url: https://arxiv.org/abs/2305.13991
tags:
- adversarial
- cc-ibp
- mtl-ibp
- training
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a simple yet effective method for training
  neural networks that are both accurate and provably robust to adversarial attacks.
  The key idea is to use a convex combination of adversarial attacks and IBP bounds
  within the loss function, allowing for a single parameter to control the trade-off
  between standard and verified robustness.
---

# Expressive Losses for Verified Robustness via Convex Combinations

## Quick Facts
- arXiv ID: 2305.13991
- Source URL: https://arxiv.org/abs/2305.13991
- Reference count: 40
- Key outcome: Achieves state-of-the-art verified robustness by using convex combinations of adversarial and IBP bounds, improving standard and verified accuracy by 1.98% to 3.92% on TinyImageNet and downscaled ImageNet

## Executive Summary
This paper introduces a method for training neural networks that are both accurate and provably robust to adversarial attacks by using convex combinations of adversarial attacks and IBP bounds within the loss function. The approach allows for smooth interpolation between standard and verified robustness objectives through a single tunable parameter α. The resulting algorithms, CC-IBP and MTL-IBP, achieve state-of-the-art results on various benchmarks, particularly excelling on TinyImageNet and downscaled ImageNet where they significantly outperform existing methods. The use of computationally efficient single-step adversarial attacks makes these methods practical for large-scale applications.

## Method Summary
The method employs convex combinations (CC) of adversarial loss and IBP bounds to create expressive loss functions that smoothly transition between standard accuracy and verified robustness. The CC-IBP algorithm directly uses this convex combination in the loss, while MTL-IBP interprets it as a multi-task learning problem balancing adversarial and verified objectives. Both approaches use single-step FGSM attacks for computational efficiency and IBP for tractable verified bound computation. Training involves a warm-up phase, ramp-up period, and fine-tuning with ℓ1 regularization. Post-training verification uses branch-and-bound with α-β-CROWN and IBP.

## Key Results
- CC-IBP and MTL-IBP achieve state-of-the-art performance on TinyImageNet and downscaled ImageNet
- Improves upon best standard and verified accuracies by 1.98% to 3.92% points
- Single-step adversarial attacks provide comparable performance to multi-step methods while being computationally efficient
- The convex combination approach allows for flexible trade-offs between standard accuracy and verified robustness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Expressive losses enable smooth interpolation between adversarial and verified robustness objectives, improving overall performance.
- **Mechanism:** By defining a parametrized family of losses Lα that monotonically transitions from adversarial loss (α=0) to verified loss (α=1), the model can be trained to balance standard accuracy and verified robustness based on a single tunable parameter.
- **Core assumption:** The loss function is translation-invariant and monotonic as assumed in Assumption 2.1, which allows the convex combination to preserve the ordering of losses.
- **Evidence anchors:**
  - [abstract]: "The key to state-of-the-art performance lies in the expressivity of the employed loss function, which should be able to match the tightness of the verifiers to be employed post-training."
  - [section]: "A family of expressive losses can be easily obtained by taking Convex Combinations (CC) of adversarial and lower bounds to logit differences within the loss function."
  - [corpus]: No direct evidence found; this is an inference from the paper's theoretical framework.
- **Break condition:** If the underlying loss function does not satisfy translation-invariance or monotonicity, the expressive property breaks down.

### Mechanism 2
- **Claim:** CC-IBP and MTL-IBP achieve state-of-the-art results by using inexpensive single-step adversarial attacks combined with IBP bounds.
- **Mechanism:** The algorithms use FGSM (single-step) attacks to compute adversarial points, reducing computational overhead while maintaining strong performance. The combination with IBP bounds provides a tractable way to approximate verified robustness.
- **Core assumption:** Single-step attacks are sufficient to approximate the adversarial loss without significant loss in robustness quality.
- **Evidence anchors:**
  - [abstract]: "The use of single-step adversarial attacks makes these methods computationally efficient and easy to tune."
  - [section]: "In order to minimize computational costs and to yield favorable optimization problems, we employ IBP to compute the lower bounds."
  - [corpus]: No direct evidence; assumption based on ablation study results showing FGSM performs comparably to PGD.
- **Break condition:** If stronger multi-step attacks are required for the specific dataset or perturbation radius, single-step attacks may be insufficient.

### Mechanism 3
- **Claim:** MTL-IBP interprets the convex combination as a multi-task learning problem, improving optimization stability.
- **Mechanism:** By treating adversarial robustness and verified robustness as two tasks, the convex combination acts as a scalarization that balances both objectives, leveraging insights from multi-task learning literature.
- **Core assumption:** Scalarization with an appropriately tuned coefficient is effective for balancing multiple objectives in deep learning.
- **Evidence anchors:**
  - [section]: "The above loss lends itself to a Multi-Task Learning (MTL) interpretation, with empirical and verified adversarial robustness as the tasks."
  - [section]: "We instead opt for scalarizations, which, when appropriately tuned and regularized, have recently been shown to yield state-of-the-art results on multi-task benchmarks."
  - [corpus]: No direct evidence; assumption based on cited multi-task learning literature.
- **Break condition:** If the tasks are too conflicting or the scalarization coefficient is poorly chosen, performance may degrade.

## Foundational Learning

- **Concept: Interval Bound Propagation (IBP)**
  - Why needed here: IBP is used to compute inexpensive lower bounds on network outputs over perturbation regions, enabling tractable verified training.
  - Quick check question: What is the computational cost of IBP compared to exact verification methods?

- **Concept: Convex combinations in loss functions**
  - Why needed here: Convex combinations allow the loss to smoothly interpolate between adversarial and verified objectives, controlled by a single parameter.
  - Quick check question: How does the choice of α affect the trade-off between standard and verified accuracy?

- **Concept: Multi-task learning scalarization**
  - Why needed here: MTL-IBP uses scalarization to balance adversarial and verified robustness as separate tasks within a single optimization problem.
  - Quick check question: Why might scalarization be preferred over specialized multi-task optimizers in this context?

## Architecture Onboarding

- **Component map:** Input -> CNN7 with BatchNorm -> IBP bounds computation -> FGSM attack -> Convex combination loss -> Parameter update

- **Critical path:**
  1. Forward pass with IBP to compute bounds
  2. FGSM attack to compute adversarial point
  3. Loss computation using convex combination
  4. Backward pass and parameter update
  5. Periodic verification with branch-and-bound

- **Design tradeoffs:**
  - Single-step vs multi-step attacks: Trade-off between computational efficiency and attack strength
  - IBP vs tighter relaxations: IBP is faster but less tight, affecting verified accuracy
  - Fixed α vs scheduled α: Fixed α simplifies tuning but may miss optimal trade-offs during training

- **Failure signatures:**
  - Low verified accuracy despite high standard accuracy: Indicates loose bounds or insufficient emphasis on verification
  - High training time with minimal gains: Suggests overly tight bounds or unnecessary multi-step attacks
  - Unstable training: May result from poor α choice or lack of regularization

- **First 3 experiments:**
  1. Train with α=0 (pure adversarial training) and compare verified accuracy to baseline
  2. Train with α=1 (pure IBP training) and observe standard accuracy drop
  3. Sweep α in [0,1] and plot standard vs verified accuracy to find optimal trade-off

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond the general need for further research on the expressivity of loss functions and their impact on the robustness-accuracy trade-off.

## Limitations
- Limited empirical validation scope - results are less conclusive on downscaled ImageNet where improvements are modest (1.98-3.92%)
- Assumes translation-invariance and monotonicity of the loss function without extensive validation across different architectures and datasets
- Computational efficiency claims rely on single-step FGSM attacks, but scenarios requiring multi-step attacks aren't thoroughly explored

## Confidence
- Mechanism 1 (Expressive losses): Medium - Theoretical framework is sound, but empirical validation across diverse architectures is limited
- Mechanism 2 (Single-step efficiency): Medium-High - Ablation studies support this, but corner cases aren't explored
- Mechanism 3 (MTL interpretation): Low-Medium - Conceptual connection to MTL literature exists, but specific advantages over direct optimization aren't rigorously established

## Next Checks
1. **Architecture Generalization Test**: Apply CC-IBP to non-CNN architectures (e.g., ResNets, Vision Transformers) on CIFAR-10 to verify if expressive losses maintain their effectiveness across different model families.

2. **Perturbation Regime Stress Test**: Evaluate CC-IBP with ϵ values significantly larger than standard (e.g., ϵ=16/255 on CIFAR-10) to determine if single-step attacks remain sufficient when the verification problem becomes more challenging.

3. **Loss Function Property Validation**: Systematically test the translation-invariance and monotonicity assumptions of the loss function across different datasets and network architectures by measuring how the convex combination parameter affects the ordering of losses during training.