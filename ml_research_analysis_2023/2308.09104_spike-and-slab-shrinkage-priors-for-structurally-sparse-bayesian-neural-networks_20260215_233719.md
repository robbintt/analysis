---
ver: rpa2
title: Spike-and-slab shrinkage priors for structurally sparse Bayesian neural networks
arxiv_id: '2308.09104'
source_url: https://arxiv.org/abs/2308.09104
tags:
- variational
- neural
- node
- network
- ss-gl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes two structured pruning methods for Bayesian
  neural networks using spike-and-slab priors with group shrinkage slab components
  - Group Lasso and Group Horseshoe. The methods automatically select important nodes
  in each layer through continuous relaxation of Bernoulli variables and Log-Normal
  approximation of scale variables, enabling tractable variational inference.
---

# Spike-and-slab shrinkage priors for structurally sparse Bayesian neural networks

## Quick Facts
- arXiv ID: 2308.09104
- Source URL: https://arxiv.org/abs/2308.09104
- Authors: 
- Reference count: 12
- Key outcome: Proposed spike-and-slab methods with group shrinkage priors achieve competitive prediction accuracy while significantly reducing model size and computational complexity compared to baselines.

## Executive Summary
This paper introduces two structured pruning methods for Bayesian neural networks using spike-and-slab priors with group shrinkage slab components - Group Lasso and Group Horseshoe. The methods automatically select important nodes in each layer through continuous relaxation of Bernoulli variables and Log-Normal approximation of scale variables, enabling tractable variational inference. Theoretical analysis establishes posterior consistency and derives optimal contraction rates, showing the shrinkage priors achieve better rates than Gaussian priors under certain network conditions. Experiments on image classification tasks demonstrate the proposed SS-GL and SS-GHS methods achieve competitive prediction accuracy while significantly reducing model size and computational complexity compared to baseline methods, with SS-GHS generally providing the most compact networks.

## Method Summary
The paper proposes structurally sparse Bayesian neural networks using spike-and-slab priors with group shrinkage slab components. Two variants are introduced: Spike-and-Slab Group Lasso (SS-GL) and Spike-and-Slab Group Horseshoe (SS-GHS). These methods automatically select important nodes in each layer through continuous relaxation of Bernoulli variables and Log-Normal approximation of scale variables, enabling tractable variational inference. The spike part is a Dirac delta at zero, while the slab part follows a zero-mean Gaussian with a scale parameter that follows a shrinkage prior. The paper establishes theoretical guarantees on posterior consistency and contraction rates, showing these shrinkage priors outperform Gaussian priors under certain network conditions. Experiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets demonstrate competitive prediction accuracy with significant model compression.

## Key Results
- SS-GL and SS-GHS achieve 93.5% and 93.59% test accuracy on MNIST with 2.7× and 3.2× model compression respectively
- SS-GHS provides the most compact networks with 98.87% sparsity in first layer and 86.49% in second layer on MNIST
- Theoretical analysis establishes optimal contraction rates, showing shrinkage priors outperform Gaussian priors when the best approximating network has specific patterns of exploding or shrinking values

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spike-and-slab priors with group shrinkage slab components enable automated node selection by assigning each node an indicator variable that is zero if the node is pruned.
- Mechanism: The spike part is a Dirac delta at zero, while the slab part is a zero-mean Gaussian with a scale parameter that follows a shrinkage prior (Group Lasso or Group Horseshoe). During variational inference, the scale parameter induces heavier tails and higher mass at zero, improving sparsity over a simple Gaussian slab.
- Core assumption: The spike-and-slab structure with continuous relaxation of Bernoulli variables and Log-Normal approximation of scale variables allows tractable optimization while maintaining exact node sparsity through variational approximation.
- Evidence anchors:
  - [abstract] "The methods automatically select important nodes in each layer through continuous relaxation of Bernoulli variables and Log-Normal approximation of scale variables, enabling tractable variational inference."
  - [section] "We propose structurally sparse Bayesian neural networks which systematically prune excessive nodes with (i) Spike-and-Slab Group Lasso (SS-GL), and (ii) Spike-and-Slab Group Horseshoe (SS-GHS) priors, and develop computationally tractable variational inference including continuous relaxation of Bernoulli variables."
  - [corpus] Weak - no direct mentions of spike-and-slab or group shrinkage in neighbor abstracts.
- Break condition: If the Log-Normal approximation fails to capture the tail behavior of the Gamma or Inverse-Gamma prior on the scale parameter, the sparsity effect may degrade.

### Mechanism 2
- Claim: Group shrinkage priors (Group Lasso and Group Horseshoe) improve posterior contraction rates over Gaussian slab priors under certain network conditions.
- Mechanism: The shrinkage priors impose a stronger penalty on the effective strength of weights (B²_l/(k_l+1)) through the scale parameter, leading to faster concentration of the variational posterior around the true function. Theoretical analysis shows that Group Lasso and Group Horseshoe achieve better contraction rates when the best approximating network has exploding or shrinking values of B²_l/(k_l+1) respectively.
- Core assumption: The network topology, layer-wise node cardinalities, and bounds on network weights satisfy the regularity conditions for posterior consistency and optimal contraction rates.
- Evidence anchors:
  - [abstract] "Theoretical analysis establishes posterior consistency and derives optimal contraction rates, showing the shrinkage priors achieve better rates than Gaussian priors under certain network conditions."
  - [section] "We establish the contraction rates of the variational posterior of our proposed models as a function of the network topology, layer-wise node cardinalities, and bounds on the network weights."
  - [corpus] Weak - no direct mentions of contraction rates or theoretical guarantees in neighbor abstracts.
- Break condition: If the regularity assumptions on network topology or weight bounds are violated, the theoretical guarantees may not hold.

### Mechanism 3
- Claim: Non-centered parameterization of the slab component stabilizes training of Bayesian neural networks with shrinkage priors.
- Mechanism: The non-centered parameterization reparameterizes the prior and variational family to decouple the scale parameter from the weight distribution, avoiding the pathological funnel-shaped geometries associated with coupled posteriors. This leads to more efficient posterior inference without changing the form of the prior.
- Core assumption: The pathological geometries in the posterior distribution can be mitigated by reparameterizing the slab component.
- Evidence anchors:
  - [section] "Non-Centered Parameterization: We adopt non-centered parameterization for the slab component in both the prior setups to circumvent the pathological funnel-shaped geometries associated with the coupled posterior."
  - [corpus] Weak - no direct mentions of non-centered parameterization in neighbor abstracts.
- Break condition: If the reparameterization introduces numerical instability or fails to capture the true posterior geometry, the training may still be unstable.

## Foundational Learning

- Concept: Spike-and-slab priors
  - Why needed here: Spike-and-slab priors provide a principled way to perform model selection by assigning each parameter a probability of being exactly zero (spike) or following a continuous distribution (slab). This is crucial for node selection in Bayesian neural networks.
  - Quick check question: What is the difference between the spike and slab components in a spike-and-slab prior?

- Concept: Variational inference
  - Why needed here: Variational inference is used to approximate the intractable posterior distribution over the network weights and hyperparameters. It enables efficient training of Bayesian neural networks with spike-and-slab priors.
  - Quick check question: How does variational inference differ from Markov chain Monte Carlo in terms of computational complexity and accuracy?

- Concept: Group shrinkage priors (Group Lasso and Group Horseshoe)
  - Why needed here: Group shrinkage priors impose sparsity at the group level (e.g., all weights associated with a node) rather than individual weights. This leads to more interpretable and computationally efficient sparse models.
  - Quick check question: What is the main difference between Group Lasso and Group Horseshoe priors in terms of their shrinkage behavior?

## Architecture Onboarding

- Component map: Spike-and-slab prior -> Continuous relaxation of Bernoulli variables -> Log-Normal approximation of scale variables -> Non-centered parameterization -> Variational inference with ELBO optimization

- Critical path:
  1. Define the spike-and-slab prior with group shrinkage slab
  2. Implement continuous relaxation of Bernoulli variables
  3. Approximate the scale variable distribution with Log-Normal
  4. Set up the variational family and ELBO optimization
  5. Train the model using stochastic gradient descent
  6. Perform inference using the variational posterior mean

- Design tradeoffs:
  - Using Log-Normal approximation vs. exact Gamma/Inverse-Gamma distributions for the scale parameter
  - Non-centered parameterization for stability vs. centered parameterization for simplicity
  - Group Lasso vs. Group Horseshoe for different shrinkage behaviors

- Failure signatures:
  - Poor convergence or unstable training
  - Sparsity not achieved or excessive pruning
  - Degradation in predictive performance

- First 3 experiments:
  1. Train a simple MLP on MNIST using SS-GL and SS-GHS with different hyperparameters for the scale distribution
  2. Compare the sparsity and predictive performance of SS-GL and SS-GHS with a baseline using Gaussian slab
  3. Investigate the effect of non-centered parameterization on training stability and convergence speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions do spike-and-slab group horseshoe priors outperform spike-and-slab group lasso priors in structurally sparse Bayesian neural networks?
- Basis in paper: [explicit] The paper discusses theoretical comparison of the two priors in the discussion section, noting that group lasso may be better when the best approximating network has exploding values of B²ₗ/(kₗ+1), while group horseshoe may be better when these values are shrinking.
- Why unresolved: The paper provides theoretical conditions but does not provide extensive empirical comparison across diverse network architectures and datasets to definitively establish when each prior performs better.
- What evidence would resolve it: Systematic empirical studies comparing the two methods across various network depths, widths, activation functions, and datasets, with statistical analysis of when each method dominates.

### Open Question 2
- Question: How does the choice of activation function affect the performance and theoretical guarantees of structured sparse Bayesian neural networks?
- Basis in paper: [inferred] The paper uses SiLU activation functions in experiments but mentions that other smoother activation functions could be used. The theoretical framework assumes 1-Lipschitz continuous activation functions.
- Why unresolved: The paper does not explore the impact of different activation functions on both theoretical properties (like contraction rates) and empirical performance.
- What evidence would resolve it: Comparative studies using different activation functions (ReLU, Tanh, ELU, etc.) with both theoretical analysis of how they affect the regularity conditions and empirical evaluation of their impact on sparsity patterns and prediction accuracy.

### Open Question 3
- Question: What are the optimal hyperparameter settings for spike-and-slab group shrinkage priors across different network architectures and datasets?
- Basis in paper: [explicit] The paper discusses hyperparameter choices (like ς² for group lasso and c for group horseshoe) and their impact on performance, noting that values need careful tuning. It provides theoretical guidance but acknowledges practical challenges.
- Why unresolved: The paper provides some empirical guidance and theoretical bounds but does not establish a systematic framework for hyperparameter selection across diverse settings.
- What evidence would resolve it: A comprehensive study establishing relationships between network architecture characteristics, dataset properties, and optimal hyperparameter settings, potentially through automated tuning methods or theoretically-derived rules.

## Limitations
- Theoretical guarantees assume ideal regularity conditions that may not hold in practice, particularly for deeper networks or non-standard architectures
- The Log-Normal approximation for scale variables may introduce approximation errors that affect the sparsity patterns
- Computational complexity of variational inference with structured priors is not thoroughly analyzed, and practical scalability to larger models remains unclear

## Confidence

**High Confidence**: The mechanism of using spike-and-slab priors with group shrinkage for node selection is well-established theoretically and the variational inference approach with continuous relaxation is a standard technique. The experimental setup and evaluation metrics are clearly defined.

**Medium Confidence**: The claims about achieving competitive prediction accuracy while significantly reducing model size are supported by experimental results, but the comparison is limited to specific architectures and datasets. The theoretical analysis of contraction rates, while rigorous, applies to idealized conditions.

**Low Confidence**: The scalability claims to larger networks and the practical advantages over existing pruning methods are not fully substantiated with comprehensive experiments across diverse architectures and tasks.

## Next Checks
1. **Theoretical Validation**: Verify the regularity conditions required for the posterior contraction rates hold for deeper networks (beyond 3 layers) and different activation functions beyond ReLU.

2. **Practical Scalability**: Implement and test the method on larger architectures (ResNet-18/50) and more challenging datasets (ImageNet) to assess computational efficiency and memory usage compared to state-of-the-art pruning methods.

3. **Ablation Study**: Conduct a comprehensive ablation study to isolate the contributions of the continuous relaxation, Log-Normal approximation, and non-centered parameterization to the overall performance and training stability.