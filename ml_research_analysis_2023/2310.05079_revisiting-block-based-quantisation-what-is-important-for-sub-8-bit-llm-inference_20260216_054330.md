---
ver: rpa2
title: 'Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference?'
arxiv_id: '2310.05079'
source_url: https://arxiv.org/abs/2310.05079
tags:
- quantisation
- block
- density
- llms
- fp32
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits block-based quantization for sub-8-bit LLM
  inference, addressing the challenge of activation outliers that necessitate different
  scaling factors at fine granularity. The authors attribute this to numerical scaling
  offsets and propose using block quantizations that share scaling factors across
  packed numbers, efficiently reducing these offsets without additional computational
  treatments.
---

# Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference?

## Quick Facts
- **arXiv ID**: 2310.05079
- **Source URL**: https://arxiv.org/abs/2310.05079
- **Reference count**: 40
- **Key outcome**: Block Floating Point (BFP) achieves nearly-lossless 6-bit LLMs, outperforming prior 8-bit methods, with mixed-precision search enabling 4-bit quantization on downstream tasks.

## Executive Summary
This paper revisits block-based quantization for sub-8-bit LLM inference, addressing the challenge of activation outliers that necessitate different scaling factors at fine granularity. The authors attribute this to numerical scaling offsets and propose using block quantizations that share scaling factors across packed numbers, efficiently reducing these offsets without additional computational treatments. They evaluate various block-based arithmetic methods, finding that Block Floating Point (BFP) achieves nearly-lossless 6-bit LLMs, outperforming prior 8-bit methods in both arithmetic and memory density. Further, they demonstrate that fine-tuning and mixed-precision search can enable nearly-lossless 4-bit LLMs on downstream tasks, suggesting potential advantages for cost-effective ASIC inference.

## Method Summary
The authors explore block-based quantization methods including Block Floating Point (BFP), Block Minifloat (BM), Block Logarithm (BL), and others for sub-8-bit LLM inference. They implement these arithmetics using PyTorch and evaluate them on pre-trained OPT models (125M to 6.7B parameters). The quantization configurations are specified by exponent (E), mantissa (M), and shared bias (B) bits. They use Tree-structured Parzen Estimator (TPE) search to optimize per-tensor precision assignments, focusing on Post-Training Quantization (PTQ) and Training-After-Quantization (TAQ). Mixed-precision search identifies sensitive layers requiring higher precision, while variance-aware block sizing adjusts block sizes per tensor based on statistical properties.

## Key Results
- BFP achieves nearly-lossless 6-bit quantization on Wikitext2 with minimal perplexity degradation
- Mixed-precision search enables 4-bit quantization on downstream tasks with accuracy recovery through fine-tuning
- BFP outperforms prior 8-bit methods (LLM.int8, SmoothQuant) in both arithmetic and memory density metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Block quantizations with shared scaling factors reduce numerical scaling offsets in LLM inference by packing numbers into blocks that share exponent information, thus lowering per-element quantization granularity.
- Mechanism: In block-based formats like BFP, all numbers in a block share a common exponent. This reduces the number of distinct scaling factors needed across the tensor, mitigating quantization error caused by outliers with different numerical ranges.
- Core assumption: The statistical distribution of activations and weights in LLMs can be approximated well enough by shared exponents within a block to preserve accuracy.
- Evidence anchors:
  - [abstract] "adapt block quantisations for LLMs, a family of methods that share scaling factors across packed numbers."
  - [section] "Block quantisations efficiently reduce the numerical scaling offsets solely from an arithmetic perspective, without additional treatments in the computational path."
- Break condition: If the variance within a block is too large, the shared exponent becomes a poor fit and quantization error increases.

### Mechanism 2
- Claim: Fine-grained quantization search at the tensor level captures the sensitivity of different LLM layers to quantization, allowing precision to be tailored per tensor.
- Mechanism: The authors use Tree-structured Parzen Estimator (TPE) search to find per-tensor precision assignments. Layers with high variance or outlier presence are assigned higher precision to preserve accuracy.
- Core assumption: Different tensor groups in LLM layers have different tolerances to quantization error, which can be effectively modeled and searched.
- Evidence anchors:
  - [abstract] "optimal fine-tuning strategies, and a lower quantisation granularity inherent in the statistical properties of LLMs."
  - [section] "apply Tree-structured Parzen Estimator (TPE) to conduct a fine-grained search for quantisation precision multiple times."
- Break condition: If the search space is too large or computational budget is limited, the model may miss the optimal configuration.

### Mechanism 3
- Claim: Mixed-precision and variance-aware block sizing can recover accuracy for aggressive quantization (e.g., 4-bit) without fine-tuning.
- Mechanism: By lowering block size granularity to the tensor level and using larger blocks for weights (which have smaller variance) and smaller blocks for activations (which have larger variance), the model reduces quantization error while keeping memory density.
- Core assumption: Weight variance is stable and smaller than activation variance, allowing safe block size increases for weights without accuracy loss.
- Evidence anchors:
  - [section] "we observe that the weight variance remains stable and much smaller, suggesting that we can increase the weight block size while decreasing the activation block size."
  - [section] "variance-aware block size and mixed precision allow aggressive quantisation beyond 6-bit without fine-tuning."
- Break condition: If the variance structure changes across layers or tasks, the fixed block size strategy may degrade accuracy.

## Foundational Learning

- Concept: IEEE floating-point representation and quantization theory
  - Why needed here: Understanding how floating-point numbers are stored and how quantization affects precision is essential to grasp why block-based methods work.
  - Quick check question: What is the difference between a shared exponent (BFP) and a shared exponent bias (BM) in block quantization?

- Concept: Variance and outlier detection in tensor statistics
  - Why needed here: Recognizing that activation outliers cause scaling offset issues is the foundation for the proposed block quantization approach.
  - Quick check question: How does the variance of activations change with depth in transformer layers, and why does this matter for quantization?

- Concept: Search algorithms (e.g., TPE) and mixed-precision strategies
  - Why needed here: The ability to assign different bit widths to different tensors is central to achieving low-bit models without accuracy loss.
  - Quick check question: What is the objective function used in the TPE search, and how does it balance accuracy and memory density?

## Architecture Onboarding

- Component map:
  - Block quantization kernel -> Precision search engine -> Variance-aware block sizing module -> Mixed-precision evaluation pipeline -> Hardware density estimator

- Critical path:
  1. Profile model to collect tensor variances and sizes
  2. Run TPE search with per-tensor precision granularity
  3. Apply variance-aware block sizing
  4. Evaluate mixed-precision configs on downstream tasks
  5. Select config that maximizes accuracy and density

- Design tradeoffs:
  - Larger block sizes → higher arithmetic density but risk of quantization error
  - More aggressive mixed-precision → better accuracy but higher search cost
  - Block size granularity vs. model flexibility (layer-wise vs. tensor-wise)

- Failure signatures:
  - High perplexity on Wikitext2 → block size too large or shared exponent poorly fitted
  - Accuracy drop on downstream tasks → insufficient precision in sensitive layers
  - Search timeout → too many candidates or poor objective function design

- First 3 experiments:
  1. Run fixed-precision BFP (e.g., W6A6) on Wikitext2 to verify baseline perplexity vs FP32
  2. Profile OPT-6.7B to confirm variance trend and outlier presence in activations
  3. Execute TPE search on a single layer group to validate per-tensor precision assignment

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but the research raises several important unresolved issues regarding the scalability of block-based quantization to larger models, the optimal configuration for different transformer architectures, and the hardware implementation challenges of mixed-precision quantization.

## Limitations

- The effectiveness of block-based quantization has only been demonstrated on OPT-family models, not on other transformer architectures like LLaMA or PaLM
- The variance-aware block sizing strategy assumes stable weight variance patterns that may not generalize across all model types
- The computational overhead of TPE-based search at tensor granularity could become prohibitive for very large models or resource-constrained deployment scenarios

## Confidence

**High Confidence:** The core mechanism of block-based quantization with shared exponents (BFP) reducing numerical scaling offsets is well-supported by both theoretical arguments and empirical results. The demonstration of nearly-lossless 6-bit BFP performance on Wikitext2 provides strong evidence for this claim.

**Medium Confidence:** The effectiveness of TPE-based mixed-precision search for identifying sensitive layers shows promise but relies on the assumption that layer sensitivity patterns are consistent across different downstream tasks. The variance-aware block sizing strategy is supported by observed patterns in OPT models but may require adaptation for other architectures.

**Low Confidence:** The claim that variance-aware block sizing alone can enable 4-bit quantization without fine-tuning on downstream tasks appears overly optimistic. The paper shows this works in some cases but doesn't fully characterize when this approach will fail or what the failure modes look like across different task types.

## Next Checks

1. **Cross-Architecture Validation:** Test the proposed block-based quantization methods (particularly BFP with variance-aware block sizing) on non-OPT architectures like LLaMA or GPT-Neo to verify the generalizability of the variance patterns and quantization effectiveness across different model families.

2. **Search Efficiency Analysis:** Conduct a systematic study of TPE search runtime and convergence across different model sizes (e.g., 1B, 6.7B, 13B parameters) to quantify the computational overhead and identify practical limits for tensor-granularity precision search in production scenarios.

3. **Failure Mode Characterization:** Design experiments specifically targeting edge cases where the proposed methods might fail - such as models with highly irregular activation distributions, tasks requiring extreme numerical precision, or scenarios with severe memory constraints - to establish clear boundaries for the approach's applicability.