---
ver: rpa2
title: 'Tutorials on Stance Detection using Pre-trained Language Models: Fine-tuning
  BERT and Prompting Large Language Models'
arxiv_id: '2307.15331'
source_url: https://arxiv.org/abs/2307.15331
tags:
- bert
- prompt
- stance
- tweet
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents two self-contained tutorials on stance detection
  using Twitter data, covering both fine-tuning BERT models and prompting large language
  models (LLMs). The first tutorial explains BERT architecture and tokenization, guiding
  users through training, tuning, and evaluating standard and domain-specific BERT
  models using HuggingFace transformers.
---

# Tutorials on Stance Detection using Pre-trained Language Models: Fine-tuning BERT and Prompting Large Language Models

## Quick Facts
- arXiv ID: 2307.15331
- Source URL: https://arxiv.org/abs/2307.15331
- Reference count: 0
- This paper presents tutorials on stance detection using Twitter data, covering both fine-tuning BERT models and prompting large language models (LLMs).

## Executive Summary
This paper presents two self-contained tutorials on stance detection using Twitter data, covering both fine-tuning BERT models and prompting large language models (LLMs). The first tutorial explains BERT architecture and tokenization, guiding users through training, tuning, and evaluating standard and domain-specific BERT models using HuggingFace transformers. The second tutorial focuses on constructing prompts and few-shot examples to elicit stances from ChatGPT and open-source FLAN-T5 without fine-tuning. Various prompting strategies are implemented and evaluated using confusion matrices and macro F1 scores. The tutorials provide code, visualizations, and insights revealing the strengths of few-shot ChatGPT and FLAN-T5, which outperform fine-tuned BERTs.

## Method Summary
The paper presents two approaches for stance detection: fine-tuning BERT models and prompting LLMs. For BERT, the method involves pre-processing tweets, tokenizing using WordPiece, fine-tuning pre-trained BERT models on labeled stance data, and evaluating performance. For LLMs, the approach constructs prompts with zero-shot, few-shot, and chain-of-thought techniques, then generates stance labels without additional training. The methods are demonstrated on the SemEval-2016 stance detection dataset with tweets labeled as in-favor, against, or neutral towards Abortion.

## Key Results
- Few-shot ChatGPT and FLAN-T5 outperform fine-tuned BERT models on stance detection tasks
- Domain-specific models like BERTweet show improved performance on tweet data compared to general BERT
- Prompt quality significantly impacts LLM performance, with carefully crafted prompts yielding better results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning BERT on stance detection data adapts its pre-trained language understanding to the specific task.
- Mechanism: BERT's pre-training on massive unlabeled text provides general language representations. Fine-tuning updates these weights using labeled stance data, aligning the model's outputs with stance categories.
- Core assumption: The stance detection task shares enough linguistic patterns with the pre-training corpus for effective transfer.
- Evidence anchors:
  - [abstract]: "The first tutorial explains BERT architecture and tokenization, guiding users through training, tuning, and evaluating standard and domain-specific BERT models..."
  - [section]: "Fine-tuning a BERT model involves training the model on a specific task using a labeled dataset, which adapts the model's pre-existing knowledge to the nuances of the task."
  - [corpus]: Weak - no direct citations about BERT fine-tuning for stance detection in the corpus.
- Break condition: If the stance detection dataset is too different from BERT's pre-training data, fine-tuning may yield poor performance.

### Mechanism 2
- Claim: Prompting LLMs like ChatGPT or FLAN-T5 enables stance detection without additional training.
- Mechanism: LLMs are pre-trained on massive text with next-word prediction, learning general language patterns. Carefully crafted prompts guide these models to output stance labels directly based on their pre-existing knowledge.
- Core assumption: The LLM's pre-training data includes sufficient exposure to stance-related language and reasoning patterns.
- Evidence anchors:
  - [abstract]: "The second focuses on constructing prompts and few-shot examples to elicit stances from ChatGPT and open-source FLAN-T5 without fine-tuning."
  - [section]: "This method does not require additional training, thus significantly reducing the amount of labeled data needed."
  - [corpus]: Weak - no direct citations about prompting for stance detection in the corpus.
- Break condition: If the prompt is poorly designed or the LLM lacks relevant pre-training data, the generated stance labels may be inaccurate.

### Mechanism 3
- Claim: Domain-specific pre-trained models (e.g., BERTweet, polibertweet-mlm) improve stance detection on tweets.
- Mechanism: These models are pre-trained on tweet corpora, learning the unique language patterns, slang, and abbreviations common in social media. This domain-specific knowledge helps them better understand and classify tweet stances.
- Core assumption: The target stance detection dataset shares similar linguistic characteristics with the model's pre-training corpus.
- Evidence anchors:
  - [section]: "BERTweet is a pre-trained model specifically designed for processing and understanding Twitter data...BERTweet is expected to perform better on stance detection tasks involving tweets compared to the general-purpose BERT model."
  - [corpus]: Weak - no direct citations about domain-specific models for stance detection in the corpus.
- Break condition: If the tweet dataset contains language or topics significantly different from the pre-training corpus, domain-specific models may not provide an advantage.

## Foundational Learning

- Concept: Transformer architecture with self-attention mechanism
  - Why needed here: BERT and LLMs are both based on transformers, which use self-attention to learn relationships between words. Understanding this is crucial for grasping how these models work.
  - Quick check question: How does self-attention help a model understand the meaning of a word in context?

- Concept: Tokenization (WordPiece for BERT, BPE for GPT-3)
  - Why needed here: Both BERT and LLMs require special tokenization to convert text into numerical representations they can process. Understanding this is essential for preparing data and interpreting model outputs.
  - Quick check question: Why is subword tokenization useful for handling out-of-vocabulary words?

- Concept: Pre-training vs. fine-tuning vs. prompting
  - Why needed here: These are the three main approaches for adapting pre-trained models to specific tasks. Understanding the differences is key to choosing the right method for stance detection.
  - Quick check question: What is the main difference between fine-tuning a BERT model and prompting an LLM like ChatGPT?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Model selection -> Prompt engineering -> Training/evaluation -> Metrics analysis

- Critical path:
  1. Preprocess tweets and split into train/validation/test sets
  2. For BERT: Fine-tune on training set, evaluate on validation set, select best model, evaluate on test set
  3. For LLMs: Create prompts, generate stance labels on validation set, select best prompt/model, evaluate on test set
  4. Compare results and analyze confusion matrices

- Design tradeoffs:
  - BERT fine-tuning: Requires labeled data, computationally intensive, but can achieve high performance with sufficient data
  - LLM prompting: No additional training needed, quick to implement, but performance depends on prompt quality and pre-training data relevance
  - Domain-specific models: Better suited for tweet language, but may require more resources and may not generalize well to other domains

- Failure signatures:
  - BERT: Overfitting to training data, poor generalization to test set, slow convergence during fine-tuning
  - LLMs: Incorrect stance labels, failure to follow prompt instructions, sensitivity to prompt wording

- First 3 experiments:
  1. Fine-tune a standard BERT model on the stance detection dataset and evaluate on the validation set
  2. Prompt ChatGPT with a zero-shot prompt and evaluate the generated labels on the validation set
  3. Fine-tune a domain-specific BERTweet model and compare its performance to the standard BERT model on the validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of few-shot examples to include in prompts for stance detection?
- Basis in paper: [explicit] The paper tested one example per class but noted "There may be more effective ways to select the few-shot examples, but this is beyond the scope of this tutorial."
- Why unresolved: The paper only tested one example per class without exploring the impact of different numbers of examples.
- What evidence would resolve it: Systematic experiments varying the number of few-shot examples (0, 1, 3, 5, 10) across multiple stance detection datasets.

### Open Question 2
- Question: How do different prompt formulations affect stance detection performance across model types?
- Basis in paper: [explicit] The paper tested zero-shot, few-shot, and chain-of-thought prompts but noted "It may sound trivial, but when evaluating the performance of the LLMs with few-shot prompting, it is important to make sure that the examples provided to the model are not included in the test set to avoid data leakage."
- Why unresolved: The paper only tested a limited set of prompt formulations without exploring variations in wording, structure, or content.
- What evidence would resolve it: Systematic ablation studies testing different prompt structures (e.g., question-answer pairs vs. instructions, explicit vs. implicit task descriptions) across multiple stance detection datasets.

### Open Question 3
- Question: What is the performance trade-off between model size and domain specificity for stance detection?
- Basis in paper: [explicit] The paper compared general models (BERT-base, ChatGPT) with domain-specific models (BERTweet, polibertweet-mlm, FLAN-T5) but did not systematically analyze the trade-offs.
- Why unresolved: The paper only compared a limited set of models without exploring the impact of different model sizes or domain specificity on performance.
- What evidence would resolve it: Systematic experiments comparing models of different sizes (small, medium, large) across multiple domains (general, social media, political) on stance detection tasks.

## Limitations
- Lack of explicit hyperparameter specifications for BERT fine-tuning impacts reproducibility
- Dataset scope limited to the Abortion topic from SemEval-2016, limiting generalizability
- Limited exploration of different prompt formulations and their impact on performance

## Confidence
- **High Confidence**: The general superiority of few-shot ChatGPT and FLAN-T5 over fine-tuned BERT models for stance detection
- **Medium Confidence**: The specific performance advantages of domain-specific models like BERTweet and polibertweet-mlm
- **Low Confidence**: The generalizability of findings across different stance detection datasets and domains

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary learning rates, batch sizes, and training epochs for BERT fine-tuning to establish optimal configurations and assess their impact on performance.

2. **Cross-Dataset Validation**: Apply the same fine-tuning and prompting approaches to stance detection datasets covering different topics (e.g., Climate Change, Vaccination) to evaluate generalizability across domains.

3. **Prompt Template Benchmarking**: Create and test multiple variations of prompt templates for LLMs, systematically comparing zero-shot, few-shot, and chain-of-thought approaches to identify which prompt characteristics most strongly influence performance.