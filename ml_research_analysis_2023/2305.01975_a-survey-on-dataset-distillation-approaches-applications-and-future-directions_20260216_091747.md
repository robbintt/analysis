---
ver: rpa2
title: 'A Survey on Dataset Distillation: Approaches, Applications and Future Directions'
arxiv_id: '2305.01975'
source_url: https://arxiv.org/abs/2305.01975
tags:
- dataset
- distillation
- learning
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys the field of dataset distillation, which aims
  to synthesize small, highly informative datasets that can replace large-scale training
  data. The authors propose a novel taxonomy categorizing approaches into meta-learning
  methods (using back-propagation through time or kernel ridge regression) and surrogate
  objective methods (parameter matching or distribution matching).
---

# A Survey on Dataset Distillation: Approaches, Applications and Future Directions

## Quick Facts
- arXiv ID: 2305.01975
- Source URL: https://arxiv.org/abs/2305.01975
- Reference count: 2
- Primary result: Dataset distillation can achieve competitive performance with significantly reduced data size through meta-learning and surrogate objective methods.

## Executive Summary
Dataset distillation is a technique that synthesizes small, highly informative datasets capable of replacing large-scale training data while maintaining model performance. This survey provides a comprehensive overview of current approaches, categorizing them into meta-learning methods (back-propagation through time and kernel ridge regression) and surrogate objective methods (parameter matching and distribution matching). The authors discuss enhancement techniques including parameterization, augmentation, and label distillation, while highlighting applications in continual learning, neural architecture search, privacy protection, and robustness improvements.

## Method Summary
The survey analyzes dataset distillation approaches through a proposed taxonomy distinguishing between meta-learning frameworks (BPTT and KRR) and surrogate objective methods (parameter and distribution matching). Meta-learning approaches optimize synthetic data through bi-level optimization problems, with BPTT computing gradients through training time and KRR transforming the problem into a convex optimization. Surrogate methods optimize proxy objectives such as parameter trajectories or feature distributions. Enhancement techniques like augmentation, parameterization, and label distillation can be integrated with these frameworks to improve performance. The survey covers empirical evidence across image datasets while acknowledging limitations for discrete data like text and graphs.

## Key Results
- Dataset distillation can synthesize condensed datasets that enable models to achieve similar performance to those trained on full datasets
- Kernel Ridge Regression provides computational efficiency advantages over back-propagation through time while maintaining effectiveness
- Performance degradation occurs when the number of images per class increases beyond 50, limiting scalability
- Current research primarily focuses on image classification tasks, with limited exploration of complex domains like object detection or text processing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dataset distillation achieves competitive performance by synthesizing small datasets with high information density that capture essential patterns from the original data.
- Mechanism: The process involves optimizing synthetic data through meta-learning frameworks (BPTT or KRR) or surrogate objectives (parameter matching or distribution matching) to ensure models trained on the condensed dataset perform similarly to those trained on the full dataset.
- Core assumption: High information density in synthetic data can effectively replace large-scale training data without significant performance loss.
- Evidence anchors:
  - [abstract] "By synthesizing datasets with high information density, dataset distillation offers a range of potential applications, including support for continual learning, neural architecture search, and privacy protection."
  - [section 2.1] "Dataset distillation aims to reduce the size of large-scale training input and label pairs T by creating smaller synthetic pairs S, so that models trained on both T and S can achieve similar performance."
  - [corpus] Found 25 related papers, indicating active research and validation of this mechanism.
- Break condition: Performance degradation occurs when the number of images per class (IPC) increases beyond a certain threshold, causing distilled data to become less effective.

### Mechanism 2
- Claim: Kernel Ridge Regression (KRR) provides an efficient alternative to BPTT by transforming dataset distillation into a convex optimization problem.
- Mechanism: KRR uses the synthetic set as support vectors and the original set as target values, computing exact outputs of an infinite-width neural network trained on synthetic data, bypassing gradient computation.
- Core assumption: The kernel method can effectively capture the essential relationships in the data without requiring expensive gradient-based optimization.
- Evidence anchors:
  - [section 3.1] "Nguyen et al. transform dataset distillation into a kernel ridge regression (KRR) problem, where the synthetic set is used as the support set and the original set as the target set."
  - [section 3.1] "KRR loss function for a given kernel and batch data from synthetic set (XS,yS) evaluated on batch data from real set (XT,yT)"
  - [corpus] RFAD uses random feature approximation of the Neural Network Gaussian Process kernel, showing practical implementation of KRR variants.
- Break condition: Computational complexity becomes prohibitive when dealing with very large synthetic datasets, despite being more efficient than BPTT.

### Mechanism 3
- Claim: Parameter matching methods optimize synthetic data by ensuring models trained on both datasets have similar parameter trajectories.
- Mechanism: The approach minimizes the distance between model parameters trained on synthetic and real data across training iterations, making the distilled dataset agnostic to initialization.
- Core assumption: Similar parameter trajectories indicate that the synthetic dataset captures the essential learning dynamics of the original data.
- Evidence anchors:
  - [section 3.2] "Parameter matching aims to make the model approximate the original model in the parameter space, i.e. θS ≈ θT."
  - [section 3.2] "DC synthesizes images by minimizing the gradient matching loss at each training step."
  - [corpus] MTT (Trajectory Matching) extends this concept by matching entire training trajectories rather than just final parameters.
- Break condition: High bias toward samples with large gradients reduces generalization capability, especially for unseen architectures.

## Foundational Learning

- Concept: Meta-learning frameworks (BPTT and KRR)
  - Why needed here: Understanding how synthetic data optimization works through bi-level optimization problems and kernel methods.
  - Quick check question: How does back-propagation through time differ from kernel ridge regression in terms of computational complexity and memory requirements?

- Concept: Surrogate objectives (parameter and distribution matching)
  - Why needed here: These approaches optimize proxy objectives rather than direct model performance, which is crucial for understanding alternative distillation strategies.
  - Quick check question: What is the key difference between parameter matching and distribution matching in terms of what they optimize?

- Concept: Enhancement techniques (parameterization, augmentation, label distillation)
  - Why needed here: These methods can be integrated with learning frameworks to improve distillation performance and address specific challenges.
  - Quick check question: How does label distillation differ from traditional one-hot encoding, and what advantages does it provide?

## Architecture Onboarding

- Component map: Learning framework (meta-learning or surrogate objectives) → Enhancement methods (parameterization, augmentation, label distillation) → Data modality-specific adaptations → Application-specific optimizations
- Critical path: Data preprocessing → Synthetic data generation → Model training on distilled data → Performance evaluation → Iterative refinement
- Design tradeoffs: Computational efficiency vs. performance (KRR vs. BPTT), bias vs. generalization (parameter matching), and storage vs. fidelity (augmentation techniques)
- Failure signatures: Performance degradation with larger IPC, bias toward specific data samples, computational bottlenecks in optimization
- First 3 experiments:
  1. Implement basic dataset distillation using BPTT on MNIST to verify framework functionality
  2. Compare KRR-based approach with BPTT on a small image dataset to evaluate efficiency gains
  3. Test augmentation techniques on CIFAR10 to assess performance improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can dataset distillation methods maintain competitive performance on larger datasets with increased images per class (IPC), beyond the current limitations?
- Basis in paper: [explicit] "According to Cui et al. [2022], current dataset distillation methods perform well only when the number of images per class (IPC) is relatively small. As the IPC increases, the performance of most distillation methods deteriorates and becomes similar to that of random sampling."
- Why unresolved: The paper identifies this as a key limitation but does not propose solutions or demonstrate methods that can overcome this performance degradation.
- What evidence would resolve it: Comparative studies showing dataset distillation methods maintaining performance superiority over random sampling as IPC scales from tens to hundreds or thousands per class.

### Open Question 2
- Question: How can dataset distillation techniques be effectively extended to complex tasks beyond image classification, such as object detection, segmentation, or sequence-to-sequence problems?
- Basis in paper: [explicit] "Currently, research on dataset distillation primarily focuses on classification tasks. However, its potential for more complex tasks, such as image detection and segmentation, named entity recognition, summarization, and machine translation, remains untapped."
- Why unresolved: The paper acknowledges the limitation to classification tasks and the lack of exploration in more complex domains, without providing frameworks or preliminary results.
- What evidence would resolve it: Successful application of dataset distillation methods to at least one complex task, demonstrating comparable performance to full dataset training while maintaining the condensation benefits.

### Open Question 3
- Question: What computational efficiency improvements are possible for dataset distillation methods to handle larger-scale datasets without GPU memory bottlenecks?
- Basis in paper: [explicit] "Methods like MTT [Cazenavette et al., 2022], KIP [Nguyen et al., 2020], and FRePo [Zhou et al., 2022] can cause GPU memory bottlenecks when the number of images per class (IPC) increases."
- Why unresolved: While the paper identifies computational efficiency as a critical challenge, it only mentions existing approaches like distribution matching and random feature approximation without comprehensive solutions for large-scale applications.
- What evidence would resolve it: Implementation of a dataset distillation method that can process datasets with millions of samples efficiently, demonstrating both reduced computational requirements and maintained or improved performance relative to full dataset training.

## Limitations
- Dataset distillation performance degrades significantly when the number of images per class increases beyond 50, limiting scalability
- Current methods primarily focus on image classification tasks with limited exploration of discrete data modalities like text and graphs
- Computational efficiency remains a critical bottleneck, particularly for gradient-based optimization methods requiring training process unrolling

## Confidence
- High Confidence: The core premise that dataset distillation can achieve competitive performance with reduced data size is well-supported by multiple empirical studies cited in the survey.
- Medium Confidence: Claims about computational efficiency gains from KRR over BPTT are supported by theoretical analysis but require more extensive empirical validation across diverse datasets.
- Low Confidence: Assertions about the general applicability of dataset distillation to non-image domains remain largely theoretical, with minimal concrete evidence provided for text and graph datasets.

## Next Checks
1. Implement a controlled experiment comparing KRR-based and BPTT-based dataset distillation on the same image dataset to quantify the computational efficiency claims and verify the convex optimization advantage.
2. Test dataset distillation methods on a non-image domain (such as text classification) to validate the scalability challenges and effectiveness limitations mentioned for discrete data modalities.
3. Conduct an ablation study on augmentation techniques to measure their individual contributions to performance improvement and determine optimal combinations for different dataset sizes and IPC values.