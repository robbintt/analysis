---
ver: rpa2
title: Prompting Large Language Models for Topic Modeling
arxiv_id: '2312.09693'
source_url: https://arxiv.org/abs/2312.09693
tags:
- topic
- topics
- datasets
- modeling
- promptt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PromptTopic, a novel topic modeling approach
  leveraging large language models (LLMs) to extract coherent and diverse topics from
  textual data. Unlike traditional models, it uses prompts to generate topics at the
  sentence level, eliminating the need for manual parameter tuning.
---

# Prompting Large Language Models for Topic Modeling

## Quick Facts
- arXiv ID: 2312.09693
- Source URL: https://arxiv.org/abs/2312.09693
- Reference count: 36
- One-line primary result: Novel approach using LLMs to extract coherent topics from text at sentence level without manual parameter tuning.

## Executive Summary
This paper introduces PromptTopic, a novel topic modeling approach that leverages large language models (LLMs) to extract coherent and diverse topics from textual data. Unlike traditional models that focus on token-level semantics, PromptTopic operates at the sentence level, eliminating the need for manual parameter tuning. The approach uses prompts to guide LLMs in generating topics, followed by two methods (PBM and WSM) to collapse overlapping topics. Evaluated on three datasets, PromptTopic shows comparable or better performance than state-of-the-art baselines in terms of topic coherence and diversity metrics.

## Method Summary
PromptTopic extracts topics at the sentence level using LLMs prompted with demonstration examples, then aggregates and condenses these topics using two collapsing methods: Prompt-Based Matching (PBM) and Word Similarity Matching (WSM). The approach eliminates hyperparameter tuning by using in-context learning via prompts. For topic representation, it employs c-TF-IDF to generate top words, followed by LLM filtering to select the final top 10 words per topic. The method is evaluated on 20 NewsGroup, Yelp Reviews, and Twitter Tweet datasets.

## Key Results
- Achieved higher topic coherence (NPMI) and diversity scores compared to traditional baselines like LDA and BERTopic
- Demonstrated effectiveness particularly on short-text datasets through qualitative and human evaluations
- Showed comparable or better performance than state-of-the-art models without requiring manual parameter tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PromptTopic generates coherent topics by leveraging sentence-level semantics rather than token-level
- Mechanism: It extracts topics from individual sentences using LLM prompts, then aggregates and condenses them into a predefined quantity
- Core assumption: Sentence-level semantics contain richer contextual information than bag-of-words token-level analysis
- Evidence anchors: [abstract] "Moreover, these models often neglect sentence-level semantics, focusing primarily on token-level semantics." [section] "It involves extracting topics at the sentence level from individual documents, then aggregating and condensing these topics into a predefined quantity, ultimately providing coherent topics for texts of varying lengths."

### Mechanism 2
- Claim: The collapsing mechanism (PBM/WSM) improves topic diversity by merging semantically similar topics
- Mechanism: Prompt-Based Matching uses LLM prompts to iteratively merge overlapping topics; Word Similarity Matching merges topics based on shared top words
- Core assumption: LLM can accurately judge semantic similarity between topic names without full context
- Evidence anchors: [section] "We introduced two approaches to collapse topics: Prompt-Based Matching (PBM) and Word Similarity Matching (WSM)." [section] "Topic similarity is measured by counting the number of common words between the top words of each topic pair."

### Mechanism 3
- Claim: Using in-context learning via demonstrations in prompts eliminates hyperparameter tuning
- Mechanism: Each prompt includes N demonstration examples of input-article → topic-list pairs, guiding the LLM without fine-tuning
- Core assumption: Few-shot prompting is sufficient for the LLM to understand the task without parameter updates
- Evidence anchors: [section] "The prompt input comprises N demonstration examples, each with prompt inputs and their associated annotated answers." [section] "Using prompts, the model effectively captures underlying themes and concepts, enhances topic clustering, and improves the quality of topic representations."

## Foundational Learning

- Concept: Sentence-level semantics vs. token-level semantics
  - Why needed here: Understanding why PromptTopic extracts topics at the sentence level rather than the traditional token level is crucial for grasping its innovation
  - Quick check question: What advantage does sentence-level analysis provide over token-level in topic modeling?

- Concept: Topic coherence metrics (NPMI, topic diversity)
  - Why needed here: These metrics are used to evaluate PromptTopic's performance against baselines
  - Quick check question: How do NPMI and topic diversity scores reflect the quality of generated topics?

- Concept: Prompt engineering and few-shot learning
  - Why needed here: The core of PromptTopic is using prompts with demonstrations to guide LLM topic extraction
  - Quick check question: Why is in-context learning via demonstrations preferred over fine-tuning in this approach?

## Architecture Onboarding

- Component map: Document → Sentence-level topic extraction → Overlapping topic collapse → Topic representation generation
- Critical path: Document → Sentence-level topic extraction → Overlapping topic collapse → Topic representation generation
- Design tradeoffs:
  - PBM vs. WSM: PBM leverages LLM understanding but is slower; WSM is faster but relies on coarse word overlap
  - N demonstrations: Higher N may improve accuracy but increases prompt size and cost
- Failure signatures:
  - Poor coherence: LLM misinterprets sentences or demonstrations
  - Over-merging: PBM merges unrelated topics due to lack of context
  - Under-merging: WSM threshold too strict, leaving redundant topics
- First 3 experiments:
  1. Run PromptTopic on a small dataset with N=4 and observe generated topics
  2. Compare PBM vs. WSM on the same dataset for merging quality
  3. Test PromptTopic on short-text vs. long-text datasets to validate sentence-level advantage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PromptTopic scale with the size and complexity of the input datasets, and what are the computational limits of using LLMs for topic modeling?
- Basis in paper: [explicit] The paper mentions that using LLMs for topic generation can be resource-intensive and requires GPU devices with significant memory capacity, especially for large datasets
- Why unresolved: The paper does not provide a detailed analysis of the scalability of PromptTopic with respect to dataset size and complexity, nor does it explore the computational limits of LLMs in this context
- What evidence would resolve it: Conducting experiments with varying dataset sizes and complexities to measure the performance and resource usage of PromptTopic, and identifying the point at which computational resources become a bottleneck

### Open Question 2
- Question: Can PromptTopic be effectively adapted for multilingual or cross-lingual topic modeling tasks, and how does it compare to existing multilingual topic modeling approaches?
- Basis in paper: [inferred] The paper focuses on English datasets and does not explore the potential of PromptTopic for multilingual or cross-lingual applications, which could be an interesting extension
- Why unresolved: The paper does not provide any experiments or discussions on the applicability of PromptTopic to languages other than English, leaving its effectiveness in multilingual scenarios untested
- What evidence would resolve it: Testing PromptTopic on multilingual datasets and comparing its performance to existing multilingual topic modeling methods to assess its adaptability and effectiveness

### Open Question 3
- Question: What are the potential biases introduced by using LLMs in PromptTopic, and how can these biases be mitigated to ensure fair and unbiased topic modeling?
- Basis in paper: [explicit] The paper does not address the issue of biases that may be inherent in LLMs and how they could affect the topic modeling results
- Why unresolved: The paper does not discuss the potential for bias in the topics generated by PromptTopic or propose methods for mitigating such biases, which is a critical consideration for ethical AI applications
- What evidence would resolve it: Analyzing the topics generated by PromptTopic for potential biases and developing strategies to reduce or eliminate these biases, such as incorporating bias detection and correction mechanisms into the model

## Limitations
- Resource intensity: Heavy reliance on LLM API calls for both topic generation and collapsing may be prohibitively expensive for large datasets or production use
- Topic merging quality: Limited evaluation of merged topic quality; PBM method may merge unrelated topics due to lack of context
- Evaluation scope: Primarily uses NPMI and topic diversity metrics; lacks extensive human evaluation and qualitative analysis of generated topics

## Confidence

High confidence: The core mechanism of using sentence-level semantics for topic extraction is technically sound and well-explained. The methodology for topic representation using c-TF-IDF and LLM filtering is clearly specified.

Medium confidence: The effectiveness of the two topic collapsing methods (PBM and WSM) is supported by the evaluation, but the quality of merged topics and potential for over-merging or under-merging needs further validation.

Low confidence: Claims about the approach being "more coherent" and "diverse" than baselines are based on limited metrics and would benefit from more extensive human evaluation and qualitative analysis.

## Next Checks

1. Conduct a thorough human evaluation study comparing topics generated by PromptTopic with those from traditional models, focusing on coherence, diversity, and relevance.

2. Test the sensitivity of the approach to different values of N (number of demonstration examples) and different sentence lengths to understand the robustness of the method.

3. Implement a cost analysis comparing the computational and API expenses of PromptTopic against traditional topic modeling approaches for datasets of varying sizes.