---
ver: rpa2
title: Vehicle Lane Change Prediction based on Knowledge Graph Embeddings and Bayesian
  Inference
arxiv_id: '2312.06336'
source_url: https://arxiv.org/abs/2312.06336
tags:
- vehicle
- lane
- left
- vehicles
- preceding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of predicting vehicle lane changes
  by considering contextual information and surrounding vehicle risk, rather than
  just trajectory data. The core idea is to use Knowledge Graph Embeddings (KGE) trained
  on the HighD dataset and apply Bayesian inference on top of the learned embeddings
  to predict lane change intentions two to three seconds in advance.
---

# Vehicle Lane Change Prediction based on Knowledge Graph Embeddings and Bayesian Inference

## Quick Facts
- arXiv ID: 2312.06336
- Source URL: https://arxiv.org/abs/2312.06336
- Reference count: 15
- Key outcome: Predicts lane changes 2-3 seconds in advance with 97.95% f1-score (2s) and 93.60% f1-score (3s)

## Executive Summary
This paper introduces a novel approach to vehicle lane change prediction that leverages Knowledge Graph Embeddings (KGE) and Bayesian inference. Unlike traditional trajectory-based methods, this approach incorporates contextual information and surrounding vehicle risk by constructing a knowledge graph from HighD dataset features. The model achieves state-of-the-art performance with 97.95% f1-score at two seconds and 93.60% at three seconds before lane change, while maintaining interpretability through linguistic inputs.

## Method Summary
The method constructs a knowledge graph from HighD dataset features including vehicle lateral velocity, acceleration, and Time To Collision (TTC) with surrounding vehicles. These numerical features are discretized into linguistic categories (e.g., "highRiskPreceding", "movingLeft") based on threshold limits. The TransE model is used to train KGE on the generated graph triples, which are then used in Bayesian inference to calculate lane change prediction probabilities. The approach combines structured reasoning from the knowledge graph with probabilistic inference to predict lane change intentions.

## Key Results
- Achieves 97.95% f1-score predicting lane changes 2 seconds in advance
- Achieves 93.60% f1-score predicting lane changes 3 seconds in advance
- Outperforms state-of-the-art methods by incorporating contextual information and surrounding vehicle risk
- Demonstrates superior performance with TransE embeddings compared to ComplEx model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge Graph Embeddings (KGE) trained on HighD dataset preserve semantic meaning of vehicle state and surrounding risk relationships, enabling structured reasoning.
- Mechanism: The TransE model maps triples `<subject, relation, object>` into low-dimensional vectors where connected entities are close in embedding space. This encodes contextual linguistic information (e.g., "highRiskPreceding" linked to "LLC") as spatial proximity, which Bayesian inference can later exploit.
- Core assumption: Semantic relationships in the graph are stable enough that embeddings generalize across different vehicles and traffic conditions.
- Evidence anchors:
  - [abstract] "KG is trained on the HighD dataset using the TransE model to obtain the Knowledge Graph Embeddings (KGE)"
  - [section IV-D] "TransE model worked better and all the upcoming experiments and discussions will be based on using the TransE model"
  - [corpus] Weak: related papers focus on deployment and alternative embeddings but lack direct TransE validation evidence.
- Break condition: If surrounding vehicle dynamics vary too much across scenarios, embeddings may not capture the risk semantics consistently, causing poor Bayesian inference.

### Mechanism 2
- Claim: Bayesian inference on KGEs allows probabilistic prediction of lane-change intention by combining prior knowledge with current linguistic evidence.
- Mechanism: The model computes P(hypothesis|evidence) using Bayes' rule, where hypothesis is lane-change class and evidence is reified linguistic triples (e.g., "vehicle has highRiskPreceding"). Embeddings provide P(e|h) and P(h) terms, while P(e) is factored into product of individual evidence probabilities.
- Core assumption: Conditional independence of evidence terms given the hypothesis is reasonable for the highD driving scenarios.
- Evidence anchors:
  - [abstract] "apply Bayesian inference on top of the KG using the embeddings learned during training"
  - [section IV-E] "P (e∥h) = P (e1 and e2 and . . . and en∥h) = P (e1|h) × . . . × P (en|h)"
  - [corpus] Missing: No direct comparison to non-Bayesian or alternative inference baselines in related works.
- Break condition: Violation of conditional independence (e.g., simultaneous high risk from multiple sides) could cause overconfident or biased predictions.

### Mechanism 3
- Claim: Linguistic input generation from numerical features (TTC, lateral velocity, acceleration) captures human-interpretable risk context that improves prediction explainability.
- Mechanism: Numerical sensor data is discretized into categories (e.g., "highRiskPreceding", "movingLeft") using threshold limits derived from statistical analysis. These categories feed the KG as reified triples, making predictions traceable to interpretable conditions.
- Core assumption: The chosen thresholds effectively separate safe vs. risky driving states across diverse scenarios.
- Evidence anchors:
  - [section IV-C] "TTC ∈ [0, 4] is high-risk, TTC ∈ (4, 10) is medium-risk, and any other positive or negative TTC value is low-risk"
  - [section IV-C] "most vehicles change lanes to the left at TTC approximate range of [4, 10] seconds"
  - [corpus] Weak: Related papers mention risk but do not validate the specific TTC threshold choices against human driving behavior.
- Break condition: If thresholds are misaligned with actual driver risk perception, predictions will not match real-world lane-change triggers.

## Foundational Learning

- Concept: Knowledge Graph Embeddings (KGE) and TransE scoring function
  - Why needed here: KGEs convert structured graph relationships into vector space, enabling similarity-based reasoning and downstream Bayesian inference.
  - Quick check question: How does TransE represent a triple `<s,r,o>` in embedding space, and why does it use vector arithmetic?

- Concept: Bayesian inference and Bayes' rule
  - Why needed here: Bayesian reasoning combines prior knowledge from embeddings with observed evidence to compute probability of lane-change classes.
  - Quick check question: What are the roles of P(h), P(e|h), and P(e) in Bayes' rule, and how are they computed from the KG?

- Concept: Time-to-collision (TTC) calculation and risk categorization
  - Why needed here: TTC quantifies risk from surrounding vehicles; discretizing TTC into linguistic categories creates interpretable inputs for the KG.
  - Quick check question: How is TTC computed for preceding vs. following vehicles, and why are different threshold ranges used?

## Architecture Onboarding

- Component map: Linguistic Input Generator -> Knowledge Graph Builder -> TransE KGE Training -> Bayesian Inference Engine -> Prediction Output
- Critical path: Input generation -> KG triple formation -> KGE training -> Bayesian probability computation -> final prediction
- Design tradeoffs: Using linguistic categories improves interpretability but may lose fine-grained numeric distinctions; TransE is efficient but may oversimplify complex relationships compared to ComplEx or RotatE.
- Failure signatures: Low f1-score despite high embedding quality suggests threshold miscalibration or violated conditional independence; sudden drops in accuracy over time indicate concept drift in driving behavior.
- First 3 experiments:
  1. Vary TTC threshold ranges and measure impact on f1-score to validate linguistic categorization.
  2. Compare TransE with ComplEx embeddings on same KG to confirm embedding choice.
  3. Test Bayesian inference with and without conditional independence assumption to quantify its effect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the model change when using different knowledge graph embedding models (e.g., TransE vs. RotatE) for the same linguistic input categories?
- Basis in paper: [explicit] The paper compares TransE and ComplEx models, showing TransE achieves significantly higher F1-scores (93.60% vs. 12% at 3 seconds).
- Why unresolved: The paper only tests TransE and ComplEx, leaving the potential of other models like RotatE unexplored.
- What evidence would resolve it: Testing and comparing the performance of TransE, RotatE, and other embedding models using the same linguistic input categories and evaluation metrics.

### Open Question 2
- Question: How does the model's performance vary with different threshold limits for converting numerical inputs to linguistic categories?
- Basis in paper: [explicit] The paper mentions that threshold limits for linguistic categories are chosen based on data distribution and standard deviation, but does not explore the impact of varying these thresholds.
- Why unresolved: The sensitivity of the model to different threshold values is not investigated, which could affect the robustness and generalizability of the predictions.
- What evidence would resolve it: Conducting experiments with different threshold limits for linguistic categories and analyzing the resulting changes in model performance metrics.

### Open Question 3
- Question: How does the model's performance change when considering additional contextual information, such as weather conditions or road type?
- Basis in paper: [inferred] The paper focuses on TTC with surrounding vehicles and vehicle kinematic data, but does not incorporate other contextual factors that might influence lane change decisions.
- Why unresolved: The impact of external factors like weather or road conditions on lane change prediction is not explored, which could provide a more comprehensive understanding of the driving environment.
- What evidence would resolve it: Incorporating additional contextual information into the model and evaluating its performance compared to the baseline model without such information.

## Limitations

- The conditional independence assumption in Bayesian inference is critical but not empirically validated across diverse driving scenarios
- TransE performance superiority lacks broader comparative validation against other embedding models in the literature
- TTC threshold values may not generalize to different traffic cultures or driving styles despite statistical derivation

## Confidence

- High confidence in the core methodology of combining KGE with Bayesian inference
- Medium confidence in the specific TTC thresholds and linguistic categories
- Medium confidence in TransE as the optimal embedding model without broader comparative validation

## Next Checks

1. Conduct cross-dataset validation using different traffic environments (e.g., urban vs. highway) to test threshold robustness
2. Compare TransE performance against ComplEx and RotatE embeddings on the same KG to verify optimal choice
3. Test Bayesian inference with relaxed conditional independence assumptions to measure impact on prediction accuracy