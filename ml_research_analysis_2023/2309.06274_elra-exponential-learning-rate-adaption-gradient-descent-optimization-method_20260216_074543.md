---
ver: rpa2
title: 'ELRA: Exponential learning rate adaption gradient descent optimization method'
arxiv_id: '2309.06274'
source_url: https://arxiv.org/abs/2309.06274
tags:
- gradient
- rate
- learning
- c2min
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ELRA, a novel gradient-based optimizer that
  dynamically adapts the learning rate using local gradient information. The key innovation
  is the use of cosine similarity between successive gradients to adjust the learning
  rate, ensuring near-optimal step sizes without requiring hand-tuned parameters.
---

# ELRA: Exponential learning rate adaption gradient descent optimization method

## Quick Facts
- arXiv ID: 2309.06274
- Source URL: https://arxiv.org/abs/2309.06274
- Reference count: 22
- Primary result: Novel optimizer using cosine similarity between successive gradients to dynamically adapt learning rates, showing faster convergence on MNIST than Adam and AdaDelta

## Executive Summary
ELRA introduces a novel gradient-based optimization method that adapts learning rates using cosine similarity between successive gradients. The method dynamically adjusts step sizes based on whether gradients are aligned or orthogonal, eliminating the need for hand-tuned learning rates. Two variants, c2min and p2min, are rotation invariant and scale linearly with problem dimension. Experiments on MNIST demonstrate faster convergence and better performance compared to established optimizers like Adam and AdaDelta.

## Method Summary
ELRA is a gradient-based optimizer that dynamically adapts learning rates using local gradient information. The method computes cosine similarity between successive gradients to determine learning rate adjustments - when gradients are orthogonal, the learning rate is near-optimal. Two variants are presented: c2min (bounded updates, more stable) and p2min (unbounded updates, faster adaptation). Both use momentum to dampen oscillations and are rotation invariant. The approach starts with a small initial learning rate and exponentially adapts it based on the cosine of the angle between consecutive gradients.

## Key Results
- MNIST experiments show ELRA variants converge faster than Adam and AdaDelta
- Both c2min and p2min are rotation invariant and scale linearly with problem dimension
- Method excels at handling saddle points and non-convex optimization landscapes
- No hand-tuned learning rate parameters required due to exponential adaptation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cosine similarity between successive gradients drives learning rate adaptation.
- Mechanism: Uses cos(∠(Gt, Gt-1)) to adjust learning rate - orthogonal gradients indicate optimal step size. Positive cosine increases rate; negative decreases it.
- Core assumption: Optimal step size leads to orthogonal successive gradients in smooth functions.
- Evidence anchors: Abstract states key innovation is cosine similarity for near-optimal step sizes; equation (1) shows optimal learning rate yields orthogonal gradients.

### Mechanism 2
- Claim: Exponential adaptation removes need for manual tuning.
- Mechanism: Updates follow αt = αt-1 * (1 + cost * g) with g > 0, causing exponential growth/decay toward optimal rate from small α0.
- Core assumption: Fast exponential adaptation makes initial guess unimportant.
- Evidence anchors: Abstract mentions no hand-tuned parameters; section describes starting with α0 ~ 10^-5 to prevent instability.

### Mechanism 3
- Claim: Rotation invariance improves saddle point performance.
- Mechanism: Updates use only Euclidean norms and cosines, making them invariant under orthogonal transformations.
- Core assumption: Coordinate invariance benefits general optimization landscapes.
- Evidence anchors: Abstract states both variants are rotation invariant; section confirms optimization path independent of coordinate choices.

## Foundational Learning

- Concept: Gradient descent and its convergence conditions
  - Why needed here: ELRA is a gradient-based optimizer; understanding step size effects on convergence is crucial.
  - Quick check question: What happens to convergence if learning rate is too large or too small in standard gradient descent?

- Concept: Cosine similarity and orthogonality in vector spaces
  - Why needed here: Core mechanism relies on cosine between gradients to adapt learning rate.
  - Quick check question: When are two vectors orthogonal in terms of their dot product and cosine similarity?

- Concept: Momentum in optimization
  - Why needed here: ELRA includes momentum updates to dampen oscillations and improve stability.
  - Quick check question: How does adding momentum change update direction compared to plain gradient descent?

## Architecture Onboarding

- Component map: Gradient computation -> Cosine similarity calculation -> Learning rate update (c2min/p2min formula) -> Parameter update with momentum -> Soft restart logic -> Bounds enforcement

- Critical path: 1) Compute current gradient Gt 2) Calculate cosine similarity with Gt-1 3) Update learning rate α using chosen formula 4) Apply momentum and update parameters 5) Check for soft restart conditions

- Design tradeoffs:
  - c2min: Bounded updates (±50% per step), more stable but slower adaptation
  - p2min: Unbounded updates, faster adaptation but needs soft restart and bounds
  - Rotation invariance vs potential benefits of coordinate-dependent methods

- Failure signatures:
  - Oscillations in test loss (especially for c2min)
  - Learning rate exploding or vanishing (for p2min without bounds)
  - Poor performance on non-differentiable functions

- First 3 experiments:
  1. 2D saddle point test: Verify rotation invariance and compare to Adam/AdaDelta
  2. Simple convex bowl: Check convergence speed and robustness to initialization
  3. MNIST with small network: Compare test loss curves and final accuracy against Adam with tuned learning rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal function g(x) in the general update scheme αt = αt-1 · (1 + cost · g(x)) for different types of optimization problems?
- Basis in paper: Authors mention g can be any function with g(x) > 0 and suggest different answers might be optimal for different problems.
- Why unresolved: Paper only tests specific values of g and acknowledges this as future research direction.
- What evidence would resolve it: Systematic experiments comparing various functions g(x) across diverse optimization landscapes to determine optimal functions for different problem classes.

### Open Question 2
- Question: How can momentum parameters β1 and β2 in c2min be dynamically adapted during training to improve performance?
- Basis in paper: Authors note current choice of β1 = 0.8 and β2 = 0.7 is "purely phenomenological" and leaves "room for future improvement."
- Why unresolved: Paper uses fixed momentum parameters based on empirical observation rather than theoretical justification.
- What evidence would resolve it: Development and validation of algorithms that adjust β1 and β2 based on local landscape features, with comparative experiments showing improved convergence.

### Open Question 3
- Question: What are the theoretical convergence guarantees for ELRA algorithms, particularly for non-convex optimization problems?
- Basis in paper: Authors acknowledge giving convergence estimates is intractable using state-of-the-art methods due to adaptive learning rate nature.
- Why unresolved: Adaptive learning rate makes ELRA incompatible with existing convergence analysis techniques.
- What evidence would resolve it: Mathematical proofs establishing convergence rates for ELRA under various assumptions, or rigorous empirical studies with statistical analysis of success rates.

## Limitations
- Claims rest heavily on single MNIST experiment with tiny network, limiting generalizability
- No comparisons against modern optimizers like Ranger, Lion, or LionW
- Rotation invariance advantage only demonstrated in abstract 2D examples without rigorous real-world validation
- Theoretical grounding connecting cosine orthogonality to optimal step size lacks rigorous proof

## Confidence
- High confidence: Mathematical framework for cosine-based learning rate adaptation is internally consistent
- Medium confidence: Experimental results on MNIST are reproducible and show ELRA outperforming Adam/AdaDelta in that specific setting
- Low confidence: Claims about advantages on complex non-convex landscapes and general applicability to larger networks remain unverified

## Next Checks
1. **Extended benchmarking**: Test ELRA against modern optimizers (Ranger, Lion, LionW) on diverse tasks including CIFAR-10, language modeling, and larger MNIST architectures
2. **Saddle point analysis**: Design controlled experiments with known saddle points to empirically measure ELRA's advantage over Adam in these scenarios
3. **Robustness testing**: Evaluate ELRA's sensitivity to initialization, learning rate starting values, and noise levels to determine practical usability constraints