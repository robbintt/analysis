---
ver: rpa2
title: Rethinking Variational Inference for Probabilistic Programs with Stochastic
  Support
arxiv_id: '2311.00594'
source_url: https://arxiv.org/abs/2311.00594
tags:
- variational
- inference
- program
- slps
- support
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SDVI addresses variational inference for probabilistic programs
  with stochastic support by decomposing the program into sub-programs with static
  support, building separate sub-guides for each, and optimizing a mixture of these
  sub-guides. This avoids the discontinuities and complex dependencies caused by stochastic
  control flow in traditional variable-by-variable guide construction.
---

# Rethinking Variational Inference for Probabilistic Programs with Stochastic Support

## Quick Facts
- arXiv ID: 2311.00594
- Source URL: https://arxiv.org/abs/2311.00594
- Reference count: 40
- Key outcome: SDVI achieves substantially higher ELBO scores and more accurate posterior approximations than existing methods like BBVI and AutoGuide, especially in high-dimensional settings with stochastic support.

## Executive Summary
Stochastic control flow in probabilistic programs causes discontinuities in the support of variables, making traditional variational inference methods ineffective. The authors introduce Stochastic Decomposition Variational Inference (SDVI), which decomposes the program into straight-line programs (SLPs) with static support, builds separate sub-guides for each, and optimizes a mixture of these sub-guides. This approach avoids the discontinuities and complex dependencies caused by stochastic control flow in traditional variable-by-variable guide construction.

## Method Summary
SDVI decomposes a probabilistic program with stochastic support into straight-line programs (SLPs), each with fixed variable support. For each SLP, it constructs a separate variational guide using standard VI techniques. The method then estimates local ELBOs for each SLP and computes mixture weights via softmax of exponentiated ELBOs. Resource allocation is handled through Successive Halving, which iteratively allocates more computational budget to promising SLPs while pruning low-performing ones. The final variational approximation is a mixture of the sub-guides weighted by their estimated mixture probabilities.

## Key Results
- SDVI produces orders-of-magnitude higher posterior predictive density than existing methods on a Gaussian Mixture Model with Poisson prior on clusters
- On GP kernel structure inference, SDVI yields better predictive performance compared to baseline methods
- The method correctly recovers the number of clusters in high-dimensional GMM settings where other methods fail

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Decomposing the variational guide over program paths (SLPs) avoids discontinuities caused by stochastic control flow.
- **Mechanism**: Instead of building a single global guide with stochastic support, SDVI constructs separate sub-guides for each deterministic SLP, then mixes them. This isolates each sub-problem to a fixed variable support.
- **Core assumption**: The program density can be rewritten as a sum over disjoint SLPs, each with fixed support.
- **Evidence anchors**:
  - [abstract]: "SDVI instead breaks the program down into sub-programs with static support, before automatically building separate sub-guides for each."
  - [section 4.1]: Shows that program density decomposes into sum of SLP densities.
  - [corpus]: Weak — corpus focuses on other PPL topics, no direct evidence on SLP decomposition benefits.
- **Break condition**: If there are infinitely many SLPs or the number is so large that learning separate guides becomes intractable.

### Mechanism 2
- **Claim**: The ELBO naturally decomposes into independent local ELBO terms for each SLP.
- **Mechanism**: By defining the guide as a mixture over SLPs, the global ELBO rewrites into a sum of local ELBOs, each depending only on its own parameters.
- **Core assumption**: Optimal mixture weights are softmax of the local ELBOs (Proposition 1).
- **Evidence anchors**:
  - [section 4.2]: Shows the decomposition and proves Proposition 1 about optimal mixture weights.
  - [abstract]: Mentions "substantial improvements in inference performance" tied to this decomposition.
  - [corpus]: No direct evidence; corpus papers don't address ELBO decomposition for stochastic support.
- **Break condition**: If the number of SLPs is too large or the local ELBOs are expensive to estimate accurately.

### Mechanism 3
- **Claim**: Successive Halving (SH) enables effective resource allocation across SLPs.
- **Mechanism**: Allocates more computational budget to SLPs with higher exponentiated ELBOs, pruning low-performing ones iteratively.
- **Core assumption**: The distribution over exponentiated ELBOs is concentrated on a small subset of SLPs.
- **Evidence anchors**:
  - [section 4.4]: Describes SH-based resource allocation and justifies it by concentration assumption.
  - [section 6.2]: Shows SDVI outperforms BBVI and DCC, consistent with effective resource allocation.
  - [corpus]: No direct evidence; corpus papers don't discuss resource allocation for VI in stochastic support.
- **Break condition**: If SLPs have similar ELBOs or if posterior mass is spread over many SLPs, making SH inefficient.

## Foundational Learning

- **Concept: Stochastic support in probabilistic programs**
  - Why needed here: Understanding why traditional VI struggles when variable existence and support change across executions.
  - Quick check question: What happens to the guide if a variable only exists on some paths? How does this affect conditioning?

- **Concept: Straight-line programs (SLPs) and program path decomposition**
  - Why needed here: SDVI relies on rewriting the program into a mixture of deterministic SLPs to enable static-support VI techniques.
  - Quick check question: Given a program with if-else branching, how many SLPs does it have? What defines each SLP's support?

- **Concept: Evidence Lower Bound (ELBO) and KL divergence in VI**
  - Why needed here: SDVI optimizes a decomposed ELBO; understanding how it relates to KL divergence is key to grasping why the method works.
  - Quick check question: Why does maximizing ELBO minimize KL divergence between guide and true posterior?

## Architecture Onboarding

- **Component map**: SLP Discovery -> Local Guide Construction -> Resource Allocation (SH) -> Mixture Weight Estimation -> Truncation & Evaluation
- **Critical path**: Discover SLPs → Build local guides → Optimize each guide independently → Estimate local ELBOs → Compute mixture weights → Return final guide
- **Design tradeoffs**:
  - Discovering all SLPs upfront vs. online discovery: upfront is simpler but can miss SLPs with low prior probability; online is more adaptive but more complex.
  - Using reparameterized vs score-function gradients: reparameterized is lower variance but requires careful construction of guides with correct support.
  - Allocating uniform vs SH-based resources: uniform is simpler but wasteful if some SLPs are negligible.
- **Failure signatures**:
  - If acceptance rates for truncated proposals are very low, the guide isn't concentrating mass on the SLP.
  - If many SLPs have similar ELBOs, SH will waste resources.
  - If the number of SLPs is huge, learning separate guides becomes intractable.
- **First 3 experiments**:
  1. Simple branching program (Fig. 1 style): verify SDVI recovers correct branch probabilities vs BBVI.
  2. GMM with Poisson prior on clusters: test scalability and accuracy in high-dimensional settings.
  3. GP kernel inference: test on structured stochastic support (grammar-based program paths).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively learn shared model parameters across different straight-line programs (SLPs) in SDVI without causing imbalanced updates or interfering with resource allocation?
- Basis in paper: [explicit] The paper discusses challenges of parameter learning for models with stochastic support, particularly when parameters are shared across SLPs.
- Why unresolved: The paper identifies that shared parameters break the separability of inference problems for individual SLPs, creating difficulties in balancing updates from different SLPs.
- What evidence would resolve it: Empirical results comparing different parameter sharing strategies across SLPs, demonstrating effective balancing mechanisms and resource allocation schemes.

### Open Question 2
- Question: Can program analysis techniques be integrated into SDVI to automatically identify models with discrete branching, reducing the need for user annotations?
- Basis in paper: [explicit] The paper mentions that while automatic identification is possible, formalizing and implementing such a tool is out of scope and would be a significant contribution.
- Why unresolved: Current implementation requires user annotations for sample statements that influence branching, limiting automation potential.
- What evidence would resolve it: A prototype implementation showing successful automatic identification of discrete branching in various probabilistic programs, with performance comparable to user-annotated versions.

### Open Question 3
- Question: What is the maximum number of SLPs beyond which SDVI becomes computationally infeasible, and how can this limitation be addressed?
- Basis in paper: [explicit] The paper acknowledges that when there are too many SLPs with significant posterior mass, learning effective variational approximations for all becomes challenging.
- Why unresolved: The paper does not provide empirical thresholds or explore hybrid approaches that might combine SDVI with other methods for very high-dimensional problems.
- What evidence would resolve it: Systematic experiments mapping performance degradation as the number of SLPs increases, and comparative studies with alternative methods like reversible jump MCMC or customized VI approaches.

## Limitations
- Scalability with large numbers of SLPs remains unclear; the method may become intractable if the program has exponentially many execution paths
- Parameter learning for shared parameters across SLPs is acknowledged as an open problem without a concrete solution
- The effectiveness of resource allocation depends critically on concentration of exponentiated ELBOs, which may not hold for all programs

## Confidence
- **High confidence**: The core mechanism of SLP decomposition and its benefits for avoiding discontinuities in guide construction
- **Medium confidence**: The effectiveness of Successive Halving for resource allocation, as it relies on the assumption of ELBO concentration
- **Medium confidence**: The overall experimental results, though the specific metrics (ELBO, LPPD) are standard and verifiable

## Next Checks
1. **Scalability test**: Implement a probabilistic program with controlled branching complexity and measure how the number of SLPs affects training time and memory usage
2. **Shared parameter scenario**: Design a simple experiment where parameters are shared across SLPs (e.g., a prior on observation noise) and verify if the current implementation handles this correctly or fails
3. **Resource allocation sensitivity**: Vary the number of SLP candidates in Successive Halving and observe how this affects final ELBO and training efficiency to understand the method's robustness to this hyperparameter