---
ver: rpa2
title: Elevating Code-mixed Text Handling through Auditory Information of Words
arxiv_id: '2310.18155'
source_url: https://arxiv.org/abs/2310.18155
tags:
- adversarial
- samlm
- code-mixed
- smlm
- bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes incorporating auditory phonetic (AP) features
  into language models to improve their robustness and performance on code-mixed text
  classification tasks. The authors propose two novel pre-training approaches, SMLM
  (SOUNDEX Masked Language Modeling) and SAMLM (SOUNDEX Aligned Masked Language Modeling),
  which use the SOUNDEX algorithm to encode the AP properties of words.
---

# Elevating Code-mixed Text Handling through Auditory Information of Words

## Quick Facts
- arXiv ID: 2310.18155
- Source URL: https://arxiv.org/abs/2310.18155
- Reference count: 19
- Primary result: SOUNDEX-based pre-training improves adversarial robustness on code-mixed text by 10-15% over vanilla BERT/RoBERTa

## Executive Summary
This paper addresses the challenge of handling code-mixed text in low-resource languages by incorporating auditory phonetic features into language models. The authors propose two novel pre-training approaches, SMLM and SAMLM, which use the SOUNDEX algorithm to encode the phonetic properties of words alongside their semantic features. These approaches are applied to BERT and RoBERTa models and evaluated on Hinglish and Benglish classification tasks, demonstrating improved robustness against phonetic perturbation-based adversarial attacks while maintaining or improving accuracy on original test sets.

## Method Summary
The paper proposes incorporating SOUNDEX phonetic encodings into BERT and RoBERTa pre-training to improve robustness on code-mixed text classification. Two approaches are introduced: SMLM (SOUNDEX Masked Language Modeling) concatenates word tokens with their SOUNDEX encodings, while SAMLM interleaves them for tighter alignment. Models are pre-trained on 33,014 Hinglish and 6,149 Benglish sentences, then fine-tuned on sentiment and offensive classification tasks. Robustness is evaluated using phonetic perturbation attacks, and SHAP explainability is used to analyze how phonetic features influence model decisions.

## Key Results
- SMLM and SAMLM achieve 10-15% higher after-attack-accuracy compared to vanilla BERT/RoBERTa on adversarial test sets
- SAMLM shows better alignment between semantic and phonetic features than SMLM, particularly for visually similar but phonetically distinct words
- SHAP analysis reveals that SOUNDEX encodings help models focus on phonetically stable tokens during adversarial attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating SOUNDEX phonetic encodings into BERT/RoBERTa pre-training improves adversarial robustness against spelling perturbations.
- Mechanism: SOUNDEX maps phonetically similar words (e.g., "acha", "achha", "acchha") to the same encoding (A200), allowing the model to treat spelling variations as equivalent during prediction.
- Core assumption: Adversarial perturbations in code-mixed text often preserve phonetic similarity, so aligning semantic and phonetic representations will reduce model confusion.
- Evidence anchors: [abstract] "incorporates the auditory phonetic (AP) features of words along with their semantic features in language models" and "phonetically similar spelling variations of a word are often imperceptible to humans."
- Break condition: If adversarial perturbations intentionally alter phonetic similarity (e.g., "movie" → "moovee" is fine, but "movie" → "movi" is not), SOUNDEX will not help.

### Mechanism 2
- Claim: SAMLM's interleaved word+SOUNDEX token sequence better aligns semantic and phonetic information than SMLM's appended sequence.
- Mechanism: By interleaving tokens (t1, s1, t2, s2, …), SAMLM forces the model to learn a tighter coupling between each word and its phonetic encoding, improving robustness to spelling variations.
- Core assumption: Tighter alignment between word and SOUNDEX tokens improves the model's ability to recognize spelling variants as equivalent.
- Evidence anchors: [abstract] "SOUNDEX Aligned Masked Language Modeling (SAMLM)" is introduced to "better alignment between word and SOUNDEX tokens."
- Break condition: If the SOUNDEX algorithm fails to encode words (e.g., numbers or non-standard scripts), the interleaving may introduce gaps that degrade alignment.

### Mechanism 3
- Claim: SHAP explainability reveals that SOUNDEX encodings help the model focus on phonetically stable tokens rather than visually perturbed ones during adversarial attacks.
- Mechanism: SHAP values highlight which tokens drive predictions; when SOUNDEX tokens are present, the model's attention shifts from visually altered words to their phonetic equivalents.
- Core assumption: Visual perturbations shift model focus away from semantically relevant tokens; phonetic encoding realigns attention.
- Evidence anchors: [abstract] "We use the explainability technique, SHAP (SHapley Additive exPlanations) to explain how the auditory features incorporated through SAMLM assist the model to handle the code-mixed text effectively."
- Break condition: If SHAP values are dominated by other confounding features (e.g., context words), phonetic alignment may not be observable.

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: Both SMLM and SAMLM rely on MLM to jointly predict word and SOUNDEX tokens, enabling bidirectional learning of semantic+phonetic features.
  - Quick check question: In standard BERT MLM, what percentage of tokens are masked during pre-training? (Answer: 15%)

- Concept: Phonetic encoding (SOUNDEX)
  - Why needed here: SOUNDEX provides a way to represent the "sound" of a word independent of its spelling, crucial for handling spelling variations in code-mixed text.
  - Quick check question: What is the SOUNDEX encoding for the words "movie", "moovee", and "movi"? (Answer: M100 for the first two, different for the last if it changes sound)

- Concept: Adversarial robustness evaluation
  - Why needed here: The paper measures model robustness by applying phonetic perturbations and comparing accuracy before/after attack.
  - Quick check question: What is the formula for Percentage Drop in Accuracy (PDA)? (Answer: (BA - AA) / BA)

## Architecture Onboarding

- Component map: Tokenizer -> WordPiece splits text into subwords -> SOUNDEX encoder -> Converts each word to phonetic code -> Pre-training module -> SMLM or SAMLM applies masked prediction on (word, SOUNDEX) pairs -> Fine-tuning module -> Adds classification head on [CLS] representation -> SHAP explainer -> Uses masked combinations of words+SOUNDEX to compute relevance scores

- Critical path: 1. Tokenize input -> 2. Generate SOUNDEX sequence -> 3. Interleave (SAMLM) or append (SMLM) -> 4. Apply MLM masking -> 5. Predict masked tokens -> 6. Fine-tune for classification

- Design tradeoffs:
  - SMLM: Simpler implementation, but weaker alignment when WordPiece splits words
  - SAMLM: Better alignment, but requires careful handling of missing SOUNDEX codes (e.g., numbers)
  - SHAP integration: Adds explainability but increases computation during evaluation

- Failure signatures:
  - Low robustness scores: Likely due to adversarial perturbations that change phonetic similarity
  - Poor accuracy on original test: May indicate SOUNDEX limitation with numbers or non-standard scripts
  - High variance in SHAP scores: Could mean model is not consistently using phonetic features

- First 3 experiments:
  1. Run SMLM pre-training on Hinglish sentiment dataset; evaluate BA/BF1 on original test set
  2. Apply phonetic perturbation attack to SMLM model; record AA/AF1 and PDA
  3. Repeat steps 1-2 with SAMLM; compare alignment quality via SHAP visualization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do SMLM and SAMLM pre-training approaches perform on code-mixed languages written in their original scripts (e.g., Hindi in Devanagari or Bengali in Bengali script)?
- Basis in paper: [inferred] The paper mentions that the proposed approach cannot handle code-mixed languages written in their original scripts, and this is listed as a limitation in the "Limitations" section.
- Why unresolved: The authors have not conducted experiments or provided evidence on how SMLM and SAMLM would perform on code-mixed languages in their original scripts.
- What evidence would resolve it: Conducting experiments with code-mixed datasets in their original scripts and comparing the performance of SMLM and SAMLM to other baselines would provide evidence on the effectiveness of the proposed approach in this context.

### Open Question 2
- Question: How can the proposed approach be improved to handle implicit sentiment in sentences more effectively?
- Basis in paper: [inferred] The paper mentions that the proposed approach does not fix the issue of implicit sentiment in sentences, which is a limitation listed in the "Limitations" section.
- Why unresolved: The authors have not provided any suggestions or experiments on how to address the issue of implicit sentiment in sentences using the proposed approach.
- What evidence would resolve it: Proposing and testing modifications to the SMLM and SAMLM pre-training approaches that specifically target implicit sentiment detection, along with evaluating their performance on datasets with implicit sentiment, would provide evidence on the effectiveness of these improvements.

### Open Question 3
- Question: How can the encoding of auditory features be improved to better handle words containing numeric digits?
- Basis in paper: [explicit] The paper explicitly states that SOUNDEX does not give encoding for numeric digits, resulting in the same representation for different words containing such digits, and this is listed as a limitation in the "Limitations" section.
- Why unresolved: The authors have not provided any suggestions or experiments on how to improve the encoding of auditory features to handle words containing numeric digits more effectively.
- What evidence would resolve it: Proposing and testing alternative encoding methods for auditory features that can handle words containing numeric digits, along with evaluating their performance on datasets with such words, would provide evidence on the effectiveness of these improvements.

## Limitations

- The approach cannot handle code-mixed languages written in their original scripts (e.g., Hindi in Devanagari), limiting its applicability to Romanized text only
- SOUNDEX does not provide encodings for numeric digits, causing different words containing digits to receive the same representation
- The approach does not address implicit sentiment in sentences, which remains a challenge for sentiment classification

## Confidence

**High confidence**: The core mechanism of using SOUNDEX to encode phonetic similarity is well-established and technically sound. The implementation details for SMLM and SAMLM are clearly specified, and the experimental framework for measuring adversarial robustness is appropriate.

**Medium confidence**: The claim that SAMLM's interleaving strategy provides better alignment than SMLM's concatenation is supported by the results but lacks theoretical justification. The explainability analysis via SHAP is methodologically sound but the interpretation of what the visualizations mean for model behavior remains speculative.

**Low confidence**: The generalization claims to other code-mixed language pairs beyond Hinglish and Benglish are unsupported. The paper doesn't investigate whether the approach works for languages with different phonetic properties or writing systems.

## Next Checks

1. **Phonetic perturbation robustness test**: Create adversarial examples where perturbations break phonetic similarity but preserve visual similarity (e.g., "movie" → "movi" changes pronunciation). Evaluate whether SMLM and SAMLM maintain their robustness advantage, or if they become vulnerable to attacks that preserve visual but not phonetic similarity.

2. **Pre-training data ablation study**: Train SMLM and SAMLM on the same amount of pre-training data but without phonetic features, then compare to versions with phonetic features but less data. This would isolate whether improvements come from the auditory features specifically or simply from additional training.

3. **Cross-linguistic generalization test**: Apply the SMLM and SAMLM approaches to a different code-mixed language pair (e.g., Spanish-English or Mandarin-English) and evaluate whether the robustness patterns replicate. This would test whether the phonetic encoding strategy generalizes beyond the specific languages studied.