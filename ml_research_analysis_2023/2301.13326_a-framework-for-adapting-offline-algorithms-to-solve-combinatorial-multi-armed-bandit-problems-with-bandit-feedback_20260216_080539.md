---
ver: rpa2
title: A Framework for Adapting Offline Algorithms to Solve Combinatorial Multi-Armed
  Bandit Problems with Bandit Feedback
arxiv_id: '2301.13326'
source_url: https://arxiv.org/abs/2301.13326
tags:
- greedy
- algorithm
- approximation
- submodular
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a general framework for adapting offline approximation
  algorithms to solve combinatorial multi-armed bandit (CMAB) problems with bandit
  feedback. The framework is based on a robustness property of the offline algorithms
  and uses an explore-then-commit approach.
---

# A Framework for Adapting Offline Algorithms to Solve Combinatorial Multi-Armed Bandit Problems with Bandit Feedback

## Quick Facts
- arXiv ID: 2301.13326
- Source URL: https://arxiv.org/abs/2301.13326
- Reference count: 40
- The paper proposes a framework for adapting offline approximation algorithms to solve combinatorial multi-armed bandit problems with bandit feedback, achieving sublinear α-regret.

## Executive Summary
This paper introduces a general framework for adapting offline approximation algorithms to solve combinatorial multi-armed bandit (CMAB) problems with bandit feedback. The key innovation is leveraging the robustness property of offline algorithms, which allows them to tolerate small errors in function evaluation. By using an explore-then-commit approach with carefully chosen exploration parameters, the framework achieves sublinear α-regret without requiring explicit knowledge of the offline algorithm's internal structure. The authors apply their framework to submodular maximization problems with cardinality and knapsack constraints, obtaining the first sublinear α-regret CMAB algorithms for the latter case.

## Method Summary
The method involves using an explore-then-commit (ETC) framework where each candidate action is sampled m times during exploration to estimate empirical mean rewards. An (α,δ)-robust offline approximation algorithm is then used to select the best action based on these perturbed function evaluations. The algorithm commits to this action for the remaining time horizon. The number of samples m is optimized to balance exploration and exploitation costs, achieving O(T^(2/3) log(T)^(1/3)) expected cumulative α-regret. The framework treats offline algorithms as black-box subroutines and only requires them to be robust to small perturbations in function evaluation.

## Key Results
- First sublinear α-regret CMAB algorithms for submodular maximization with knapsack constraints
- O(T^(2/3) log(T)^(1/3)) expected cumulative α-regret bound for the explore-then-commit framework
- Empirical validation showing superior performance compared to full-bandit methods on budgeted influence maximization and song recommendation problems

## Why This Works (Mechanism)

### Mechanism 1
The explore-then-commit (ETC) adaptation achieves sublinear α-regret despite only having bandit feedback. The algorithm explores by sampling each candidate action m times, estimates its empirical mean reward, and uses an (α,δ)-robust offline algorithm to select the best action. During exploitation, it commits to this action for the remainder of the horizon. Core assumption: The offline algorithm is (α,δ)-robust to small errors in function evaluation, meaning it still achieves α-approximation performance even with bounded perturbations to reward evaluations.

### Mechanism 2
The robustness property alone is sufficient to guarantee sublinear regret without requiring special structure in the offline algorithm. By treating the offline algorithm as a black box subroutine, the adaptation procedure does not need to understand or replicate the internal workings of the algorithm. It simply feeds perturbed function evaluations (empirical means) to the offline algorithm. Core assumption: The offline algorithm's approximation guarantee degrades gracefully with the size of perturbations, controlled by the δ parameter.

### Mechanism 3
The specific choice of m = O(T^(2/3) log(T)^(1/3)) balances exploration and exploitation to achieve O(T^(2/3) log(T)^(1/3)) regret. The algorithm optimizes the number of samples per action to minimize the sum of exploration regret (proportional to N·m) and exploitation regret (proportional to T·δ·rad), where rad is the concentration radius. Core assumption: The horizon T is sufficiently large (T ≥ 2√2N/δ) to justify the exploration cost.

## Foundational Learning

- Concept: Hoeffding's inequality for concentration of empirical means
  - Why needed here: To bound the probability that empirical estimates deviate from true means by more than the concentration radius rad
  - Quick check question: What is the probability that an empirical mean of m i.i.d. bounded [0,1] variables deviates from the true mean by more than √(log(T)/(2m))?

- Concept: Submodular function properties (diminishing returns, monotonicity)
  - Why needed here: The framework is applied to submodular maximization problems, so understanding these properties is essential for analyzing both offline algorithms and their adaptations
  - Quick check question: If f is submodular and A ⊆ B ⊆ Ω, what is the relationship between f(e|A) and f(e|B) for any e ∈ Ω \ B?

- Concept: Approximation algorithms and their robustness analysis
  - Why needed here: The framework requires analyzing whether existing offline approximation algorithms satisfy the (α,δ)-robustness property
  - Quick check question: For a greedy algorithm that achieves (1-1/e)-approximation for submodular maximization, what perturbation size can it tolerate while still guaranteeing a constant fraction of the optimal value?

## Architecture Onboarding

- Component map: Exploration controller -> Empirical mean calculator -> Black box offline algorithm -> Clean event checker -> Exploitation executor
- Critical path: Exploration → Empirical mean calculation → Black box algorithm execution → Action selection → Exploitation
- Design tradeoffs:
  - Larger m → better concentration but higher exploration cost
  - Smaller m → lower exploration cost but higher risk of concentration failure
  - The O(T^(2/3) log(T)^(1/3)) scaling represents the optimal balance
- Failure signatures:
  - High variance in empirical means despite large m → possible noisy reward distribution
  - Clean event fails frequently → m too small or δ too large
  - Regret grows faster than O(T^(2/3)) → exploration not properly balanced with exploitation
- First 3 experiments:
  1. Test concentration bounds by running exploration with varying m values and measuring empirical deviation from true means
  2. Verify robustness property by adding synthetic noise to offline algorithm evaluations and measuring degradation in approximation ratio
  3. Compare cumulative regret against theoretical O(T^(2/3) log(T)^(1/3)) bound across different problem sizes (n, N, δ)

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal expected cumulative α-regret for stochastic CMAB problems with general non-linear rewards under bandit feedback? The paper states that it remains an open question whether tilde-O(T^1/2) expected cumulative α-regret is possible for the general setting of stochastic CMAB with non-linear rewards and only bandit feedback. Why unresolved: The paper only establishes a tilde-O(T^2/3) bound for the specific case of submodular rewards using their framework. What evidence would resolve it: A lower bound proof showing that tilde-O(T^2/3) is indeed the optimal rate, or an algorithm achieving tilde-O(T^1/2) regret for general non-linear rewards.

### Open Question 2
Can the C-ETC framework be extended to achieve sublinear α-regret for stochastic CMAB problems with non-submodular rewards under bandit feedback? Why unresolved: The paper focuses on applying the framework to submodular maximization problems, but does not explore other reward function classes. What evidence would resolve it: A proof that the robustness condition can be satisfied for a broader class of reward functions, or experimental results showing sublinear α-regret for specific non-submodular reward classes.

### Open Question 3
Is it possible to achieve sublinear α-regret for stochastic CMAB problems with submodular rewards under bandit feedback without requiring knowledge of the time horizon T? Why unresolved: The paper mentions that the C-ETC algorithm requires knowledge of the horizon T to optimize the number of samples per action, and only briefly mentions the geometric doubling trick as a potential solution for unknown horizons. What evidence would resolve it: A rigorous analysis of the doubling trick applied to C-ETC, showing that it achieves sublinear α-regret with only logarithmic overhead in the time horizon.

## Limitations
- The framework requires the offline algorithm to be (α,δ)-robust to perturbations, but verifying this property for existing algorithms may be challenging or impossible for some cases
- The explore-then-commit approach inherently suffers from O(T^(2/3)) regret scaling, which is suboptimal compared to fully adaptive methods when the objective function has special structure
- The analysis assumes i.i.d. rewards within each arm and bounded reward distributions, which may not hold in all real-world applications

## Confidence
- High confidence: The regret bound O(T^(2/3) log(T)^(1/3)) follows from standard concentration inequalities and is mathematically rigorous
- Medium confidence: The robustness property requirements are well-defined but may be difficult to verify for all candidate offline algorithms
- Medium confidence: Experimental results demonstrate practical utility but are limited to specific problem domains

## Next Checks
1. Verify robustness property: Test the Greedy+Max algorithm with synthetic perturbations to evaluate how its approximation ratio degrades with perturbation size, comparing against the theoretical (1/2, 1/2 + β + 2K)-robustness bound
2. Evaluate concentration bounds empirically: Run the exploration phase with varying numbers of samples m and measure the empirical probability that concentration fails, comparing against theoretical bounds
3. Stress-test parameter sensitivity: Systematically vary the robustness parameter δ and time horizon T to identify regimes where the regret bound breaks down or the method becomes impractical