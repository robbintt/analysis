---
ver: rpa2
title: 'The Parrot Dilemma: Human-Labeled vs. LLM-augmented Data in Classification
  Tasks'
arxiv_id: '2304.13861'
source_url: https://arxiv.org/abs/2304.13861
tags:
- data
- social
- gpt-4
- text
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study investigates whether synthetic data augmentation using
  GPT-4 and ChatGPT can match or outperform human-labeled data in three computational
  social science classification tasks: sentiment analysis, hate speech detection,
  and social dimensions classification. It employs two augmentation strategies (proportional
  and balanced) and compares them against human-labeled data and zero-shot classification
  with GPT-4 and ChatGPT.'
---

# The Parrot Dilemma: Human-Labeled vs. LLM-augmented Data in Classification Tasks

## Quick Facts
- arXiv ID: 2304.13861
- Source URL: https://arxiv.org/abs/2304.13861
- Reference count: 9
- Human-labeled data consistently shows strong predictive power, overtaking synthetic data in two out of three tasks.

## Executive Summary
This study investigates whether synthetic data augmentation using GPT-4 and ChatGPT can match or outperform human-labeled data in three computational social science classification tasks: sentiment analysis, hate speech detection, and social dimensions classification. The research employs two augmentation strategies (proportional and balanced) and compares them against human-labeled data and zero-shot classification with GPT-4 and ChatGPT. Synthetic data augmentation proves beneficial, especially for rare classes in multi-class tasks, but human-labeled data consistently shows strong predictive power, overtaking synthetic data in two out of three tasks.

## Method Summary
The study uses three datasets: SemEval-2017 Task 4 for sentiment analysis (English Twitter), DKHATE for Danish hate speech detection, and a social dimensions dataset with 7,855 texts. For each task, researchers hold out 20% as test set and sample 500 texts as a base set for augmentation. They generate synthetic data using GPT-4 and ChatGPT with two strategies (proportional and balanced), prompting the models to create 10 examples per base sample. Models are trained using the intfloat/e5-base (110M parameter) model with AdamW optimizer (lr=2e-5, batch=32, 10 epochs). The study compares human-labeled data, synthetic augmentation strategies, and zero-shot classification performance of GPT-4 and ChatGPT across all tasks.

## Key Results
- Human-labeled data performs best in hate speech detection with macro F1 of 0.76, while zero-shot GPT-4 achieves 0.72
- For sentiment analysis, human-labeled data slightly outperforms synthetic data, with GPT-4 achieving macro F1 of 0.71 in zero-shot classification
- In the complex social dimensions classification task, both synthetic and human-labeled data perform comparably, with ChatGPT achieving the highest macro F1 of 0.32 in zero-shot classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data augmentation using GPT-4 and ChatGPT can effectively improve model performance in low-resource settings.
- Mechanism: LLMs generate new training examples that resemble the base set, increasing training data size and diversity.
- Core assumption: Generated examples maintain semantic similarity to original data and preserve target labels.
- Evidence anchors: Synthetic data augmentation proves beneficial, especially for rare classes in multi-class tasks; data augmented with synthetic samples yields good downstream performance, particularly aiding low-resource settings.
- Break condition: If generated examples deviate significantly from semantic meaning or introduce bias/hallucinations, augmented data could degrade model performance.

### Mechanism 2
- Claim: Zero-shot classification with GPT-4 and ChatGPT can achieve competitive performance without requiring labeled training data.
- Mechanism: LLMs leverage pre-trained knowledge to classify texts based on brief label explanations.
- Core assumption: LLMs have been exposed to similar data during pre-training or fine-tuning.
- Evidence anchors: GPT-4 achieves macro F1 score of 0.71 in zero-shot classification for sentiment analysis; GPT-4 and ChatGPT have strong zero-shot performance across all tasks.
- Break condition: If task domain is highly specialized or labels are complex and not well-represented in LLM training data, zero-shot performance may degrade.

### Mechanism 3
- Claim: Human-labeled data consistently outperforms synthetic data in tasks of varying complexity.
- Mechanism: Human annotators provide high-quality, diverse, and unbiased labels that capture task nuances.
- Core assumption: Human annotators are diverse and representative of general perception of tasks and classes.
- Evidence anchors: Human-labeled data consistently shows strong predictive power, overtaking synthetic data in two out of three tasks; human-annotated data exhibits strong predictive power, overtaking synthetic data in two of three tasks.
- Break condition: If human annotators are not diverse or annotation process is not rigorous, quality of human-labeled data may degrade.

## Foundational Learning

- Concept: Computational Social Science (CSS)
  - Why needed here: The study investigates use of LLMs in three CSS classification tasks.
  - Quick check question: What are some examples of tasks in CSS that could benefit from LLM-based data augmentation?

- Concept: Natural Language Processing (NLP)
  - Why needed here: The study uses NLP techniques to classify texts in sentiment analysis, hate speech detection, and social dimensions classification.
  - Quick check question: What are some common NLP techniques used for text classification?

- Concept: Large Language Models (LLMs)
  - Why needed here: The study uses GPT-4 and ChatGPT to generate synthetic data and perform zero-shot classification.
  - Quick check question: How do LLMs differ from traditional language models in terms of size and capabilities?

## Architecture Onboarding

- Component map: SemEval-2017 Task 4 -> DKHATE -> social dimensions dataset -> GPT-4/ChatGPT -> intfloat/e5-base model -> Huggingface Trainer interface -> macro F1 and accuracy evaluation
- Critical path: 1) Data collection and preprocessing, 2) LLM-based data augmentation (proportional and balanced strategies), 3) Model training on human-labeled and synthetic data, 4) Zero-shot classification with GPT-4 and ChatGPT, 5) Performance evaluation and comparison
- Design tradeoffs: Data augmentation vs. human annotation (cost vs. quality), model size vs. performance (110M parameter model vs. larger LLMs), prompt engineering complexity vs. zero-shot performance
- Failure signatures: Synthetic data degrades model performance due to hallucinations or bias, zero-shot classification fails for complex or specialized tasks, human-labeled data is not diverse or representative
- First 3 experiments: 1) Compare performance of model trained on human-labeled data vs. synthetic data for sentiment analysis, 2) Evaluate zero-shot classification performance of GPT-4 and ChatGPT on hate speech detection task, 3) Investigate impact of augmentation strategy (proportional vs. balanced) on social dimensions classification task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific prompt engineering techniques could be developed to improve synthetic data quality and make it consistently surpass human-generated data?
- Basis in paper: [explicit] The paper states "This finding highlights the need for more complex prompts for synthetic datasets to consistently surpass human-generated ones" and discusses how simple prompts were used
- Why unresolved: The study only tested minimal prompt engineering and found synthetic data underperformed human-labeled data in 2 of 3 tasks. The authors acknowledge that more complex prompts could yield better results but didn't explore this systematically.
- What evidence would resolve it: A systematic comparison of different prompt engineering approaches (e.g., few-shot prompting, chain-of-thought prompting, persona-based prompting) across multiple tasks, showing which techniques produce synthetic data that consistently outperforms human-labeled data.

### Open Question 2
- Question: How do the synthetic examples generated by LLMs differ from human-annotated data in terms of lexical, semantic, and stylistic features?
- Basis in paper: [explicit] The paper mentions "a thorough investigation is needed in order to understand how the synthetic examples differ from the original in a number of ways including length and style as well as lexical and semantic similarity"
- Why unresolved: The authors call for detailed analysis of differences between synthetic and human-annotated data but didn't perform this analysis in their study.
- What evidence would resolve it: A comprehensive linguistic analysis comparing synthetic and human-annotated data across multiple dimensions (e.g., vocabulary diversity, sentence complexity, topical coverage, stylistic features) to identify systematic differences that could inform better prompt design.

### Open Question 3
- Question: Can safety protocols in LLMs be effectively strengthened to prevent generation of harmful content while still allowing for legitimate research use cases?
- Basis in paper: [explicit] The authors found that "we were able to easily bypass this safety protocol for both ChatGPT and GPT-4" when trying to generate hate speech for research purposes
- Why unresolved: The study demonstrates vulnerability in current safety protocols but doesn't explore solutions or quantify how widespread this issue is across different types of harmful content.
- What evidence would resolve it: A systematic evaluation of different safety protocol implementations, testing various prompt circumvention techniques, and identifying architectural or training approaches that could make LLMs more resistant to generating harmful content while maintaining research utility.

## Limitations

- The comparison between human-labeled and synthetic data relies on a single set of human annotators, limiting generalizability across different annotation teams or cultural contexts
- The synthetic data generation process depends heavily on prompt engineering quality, which may not transfer seamlessly to other domains or languages
- The use of a relatively small model (intfloat/e5-base with 110M parameters) may constrain the upper bounds of performance achievable with augmented data

## Confidence

**High Confidence**: The core finding that human-labeled data consistently outperforms synthetic data in two of three tasks is well-supported by experimental results and aligns with established understanding of data quality. The zero-shot performance of GPT-4 and ChatGPT across all tasks is also robustly demonstrated.

**Medium Confidence**: The claim that synthetic data augmentation is particularly beneficial for rare classes requires further validation, as the study only examines this across three specific tasks. The mechanism by which LLM-generated examples improve model performance could vary significantly across different data distributions and task complexities.

**Low Confidence**: The comparative advantage of proportional versus balanced augmentation strategies is not thoroughly explored, with limited discussion of when each approach would be preferable in practice.

## Next Checks

1. **Cross-annotator validation**: Replicate the human-labeled data collection with a different team of annotators to assess the stability of the human advantage over synthetic data.

2. **Domain transfer testing**: Apply the same augmentation strategies and evaluation framework to a different domain (e.g., medical text classification or legal document categorization) to test the generalizability of the findings.

3. **Prompt sensitivity analysis**: Systematically vary the augmentation prompts and measure how changes affect the quality of synthetic data and downstream model performance, identifying which prompt elements are most critical for success.