---
ver: rpa2
title: Quantifying Deep Learning Model Uncertainty in Conformal Prediction
arxiv_id: '2306.00876'
source_url: https://arxiv.org/abs/2306.00876
tags:
- uncertainty
- prediction
- data
- xval
- conformal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel probabilistic approach to quantify
  model uncertainty in conformal prediction for classification tasks. The method leverages
  the prediction sets generated by conformal prediction and provides certified boundaries
  for the computed uncertainty.
---

# Quantifying Deep Learning Model Uncertainty in Conformal Prediction

## Quick Facts
- **arXiv ID**: 2306.00876
- **Source URL**: https://arxiv.org/abs/2306.00876
- **Reference count**: 6
- **Primary result**: Novel probabilistic approach to quantify model uncertainty in conformal prediction with certified boundaries for classification tasks

## Executive Summary
This paper proposes a method to quantify model uncertainty in conformal prediction by transforming prediction set sizes into normalized uncertainty measures with certified bounds. The approach enables meaningful comparison of CP-based uncertainty with other methods like Bayesian and Evidential approaches. The technique enhances reliability and accuracy of uncertainty estimation while maintaining conformal prediction's validity guarantees, which is crucial for high-stakes applications like medical AI.

## Method Summary
The paper presents a probabilistic approach to quantify model uncertainty derived from prediction sets generated by conformal prediction. It transforms the size of prediction sets into a normalized uncertainty measure bounded between 0 and 1, providing certified upper and lower bounds. The method uses calibration data to compute a quantile threshold, generates prediction sets by including labels with scores below this threshold, and then applies Theorem 2 to quantify uncertainty bounds for each prediction set.

## Key Results
- Provides certified boundaries for uncertainty computed from conformal prediction prediction sets
- Enables comparison of CP-based uncertainty with Bayesian and Evidential approaches through standardized bounded metrics
- Maintains validity guarantees of conformal prediction while quantifying model uncertainty for classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Conformal Prediction (CP) provides formal marginal coverage guarantees for prediction sets that include the true label with probability at least 1-δ.
- **Mechanism**: CP constructs prediction sets by computing a quantile threshold (ˆq) from calibration data conformal scores. Labels with scores below this threshold are included in the set. The marginal coverage property ensures statistical validity across all data points.
- **Core assumption**: Calibration data are i.i.d. and exchangeable with the test data distribution.
- **Evidence anchors**:
  - [abstract] "Conformal Prediction (CP) has emerged as a promising framework for representing the model uncertainty by providing well-calibrated confidence levels for individual predictions."
  - [section] "CP uses this calibration data and an arbitrary conformal score function to generate the prediction sets... the probability of the true label being covered in the prediction set is guaranteed in the following bounds: 1 - δ ≤ P(yval ∈ C(xval, ˆq)) ≤ 1 - δ + 1/(n+1)."
  - [corpus] Weak evidence - neighboring papers mention CP coverage guarantees but don't provide specific theorem references.
- **Break condition**: If calibration and test data are not exchangeable, the coverage guarantee fails.

### Mechanism 2
- **Claim**: The proposed uncertainty quantification method transforms prediction set size into a normalized uncertainty measure bounded between 0 and 1.
- **Mechanism**: The baseline uncertainty is computed as the ratio of prediction set size to total possible labels (m/K). This baseline is then adjusted using the coverage error level δ and calibration set size n to produce upper and lower bounds on the actual uncertainty.
- **Core assumption**: The size of the prediction set directly correlates with model uncertainty.
- **Evidence anchors**:
  - [abstract] "We propose a probabilistic approach in quantifying the model uncertainty derived from the produced prediction sets in conformal prediction and provide certified boundaries for the computed uncertainty."
  - [section] "Theorem 2... the conformal model uncertainty UC(xval) associated with xval is quantified to be 0 < UC(xval) ≤ 1, and guaranteed in the following marginal upper bound HC and lower bound LC"
  - [corpus] Moderate evidence - neighboring papers discuss prediction set size as uncertainty indicator but don't formalize the bounding approach.
- **Break condition**: When prediction set size is 0 (no labels selected), the method treats this as maximum uncertainty (UC=1) which may not reflect true model behavior.

### Mechanism 3
- **Claim**: The uncertainty quantification method allows comparison between CP-based uncertainty and other methods like Bayesian and Evidential approaches.
- **Mechanism**: By scaling uncertainty to the [0,1] interval and providing certified bounds, the method creates a standardized metric that can be directly compared with other uncertainty quantification approaches that also produce bounded values.
- **Core assumption**: Different uncertainty quantification methods can be meaningfully compared when normalized to the same scale.
- **Evidence anchors**:
  - [abstract] "By doing so, we allow model uncertainty measured by CP to be compared by other uncertainty quantification methods such as Bayesian (e.g., MC-Dropout and DeepEnsemble) and Evidential approaches."
  - [section] "We can compute the model uncertainty as the probability of the model being uncertain denoted by Pu... UC(xval) = Pu|P1).P(P1) + Pu|P0).P(P0)"
  - [corpus] Weak evidence - no direct comparisons with Bayesian or Evidential methods in neighboring papers.
- **Break condition**: If other methods don't produce comparable bounded uncertainty values, the comparison may be invalid.

## Foundational Learning

- **Conformal Prediction fundamentals**: Why needed here: Understanding how CP generates prediction sets and guarantees coverage is essential for implementing the uncertainty quantification method. Quick check question: What is the relationship between coverage error level δ and the probability that the true label is in the prediction set?
- **Probability theory and bounds**: Why needed here: The uncertainty quantification relies on constructing and interpreting upper and lower bounds for uncertainty values. Quick check question: How does the calibration set size n affect the tightness of the uncertainty bounds?
- **Bayesian and Evidential uncertainty methods**: Why needed here: The paper aims to compare CP-based uncertainty with these alternative methods, requiring understanding of their principles. Quick check question: What is the key difference between how Bayesian methods and CP methods quantify uncertainty?

## Architecture Onboarding

- **Component map**: Pretrained classifier -> Calibration data processor -> Conformal score function -> Quantile calculator -> Prediction set generator -> Uncertainty quantifier
- **Critical path**: Pretrained model → Calibration scores → Quantile threshold → Prediction set → Uncertainty bounds. Each step depends on the previous one, with the calibration phase being foundational.
- **Design tradeoffs**: 
  - Larger calibration sets (n) improve bound tightness but increase computation
  - Different conformal score functions affect prediction set quality and uncertainty estimation
  - Coverage error level δ trades off between set size and coverage guarantee strength
- **Failure signatures**: 
  - Prediction sets always contain all labels (m=K) → indicates model uncertainty is consistently high
  - Prediction sets frequently empty (m=0) → suggests calibration threshold is too high
  - Uncertainty bounds are very wide → suggests small calibration set or high δ
- **First 3 experiments**:
  1. Test with synthetic data where true uncertainty is known (e.g., varying softmax entropy) to validate bound accuracy
  2. Compare uncertainty quantification across different pretrained models (CNN, transformer) on same dataset
  3. Vary calibration set size and coverage error level to study their effects on bound tightness and uncertainty estimates

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the proposed conformal uncertainty quantification method compare in accuracy and reliability to existing Bayesian and Evidential approaches across diverse real-world datasets?
- **Basis in paper**: [explicit] The paper aims to allow model uncertainty measured by conformal prediction to be compared with Bayesian and Evidential approaches, but does not provide empirical comparisons.
- **Why unresolved**: The paper focuses on theoretical foundations and proposes a novel method but lacks experimental validation against other methods.
- **What evidence would resolve it**: Empirical studies comparing the proposed method with Bayesian (e.g., MC-Dropout, DeepEnsemble) and Evidential approaches on various datasets, evaluating accuracy, reliability, and computational efficiency.

### Open Question 2
- **Question**: How can the proposed uncertainty quantification method be extended to handle high-dimensional data and imbalanced datasets effectively?
- **Basis in paper**: [inferred] The paper acknowledges opportunities for further exploration, including challenges such as high-dimensional data and imbalanced datasets.
- **Why unresolved**: The paper does not address the scalability and robustness of the method for complex data scenarios.
- **What evidence would resolve it**: Experimental results demonstrating the method's performance on high-dimensional and imbalanced datasets, along with adaptations or modifications to improve handling of such data.

### Open Question 3
- **Question**: How can domain knowledge be incorporated into the uncertainty quantification process to improve interpretability and decision-making in specialized fields like medical AI?
- **Basis in paper**: [inferred] The paper mentions the importance of interpretability and explainability of uncertainty measures but does not provide specific approaches for incorporating domain knowledge.
- **Why unresolved**: The paper does not explore methods for integrating domain-specific information into the uncertainty quantification framework.
- **What evidence would resolve it**: Case studies or theoretical frameworks showing how domain knowledge can be integrated into the proposed method, along with evaluations of the impact on interpretability and decision-making.

## Limitations
- The method's effectiveness heavily depends on calibration data quality and appropriate conformal score function selection
- Lack of empirical validation against Bayesian and Evidential approaches limits comparison claims
- Assumes prediction set size directly correlates with true model uncertainty, which may not always hold

## Confidence

| Claim | Confidence Level | Basis |
|-------|------------------|-------|
| CP coverage guarantees (Mechanism 1) | High | Directly follows established CP theory |
| Uncertainty quantification bounds (Mechanism 2) | Medium | Theoretical framework presented but practical utility depends on assumptions |
| Comparison with other uncertainty methods (Mechanism 3) | Low | No empirical validation or direct comparisons provided |

## Next Checks
1. Validate uncertainty bounds on synthetic data with known uncertainty distributions to test accuracy of the upper and lower bound calculations
2. Compare CP-based uncertainty quantification with Bayesian MC-Dropout and DeepEnsemble methods on the same datasets to assess cross-method validity
3. Test sensitivity to calibration set size and coverage error level by systematically varying n and δ to identify optimal parameter ranges