---
ver: rpa2
title: 'Improved Best-of-Both-Worlds Guarantees for Multi-Armed Bandits: FTRL with
  General Regularizers and Multiple Optimal Arms'
arxiv_id: '2302.13534'
source_url: https://arxiv.org/abs/2302.13534
tags:
- logt
- step
- lemma
- where
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of designing adaptive multi-armed
  bandit algorithms that perform optimally in both the stochastic and adversarial
  settings simultaneously. The authors significantly improve and generalize a recent
  result that shows the Follow-the-Regularized-Leader (FTRL) algorithm can optimally
  adapt to the stochastic setting without requiring a uniqueness assumption.
---

# Improved Best-of-Both-Worlds Guarantees for Multi-Armed Bandits: FTRL with General Regularizers and Multiple Optimal Arms

## Quick Facts
- arXiv ID: 2302.13534
- Source URL: https://arxiv.org/abs/2302.13534
- Reference count: 40
- Primary result: Achieves best-of-both-worlds regret bounds for general regularizers without uniqueness assumption

## Executive Summary
This paper presents significant improvements to best-of-both-worlds (BOBW) guarantees for multi-armed bandits using Follow-the-Regularized-Leader (FTRL) algorithms. The authors develop a new arm-dependent learning rate schedule and a general monotonicity theorem for Bregman divergences that together remove the need for the uniqueness assumption on optimal arms. Their analysis provides improved regret bounds for log-barrier, Shannon entropy, and Tsallis entropy regularizers while maintaining optimal performance in both stochastic and adversarial settings.

## Method Summary
The method employs FTRL with three regularizers (log-barrier, β-Tsallis entropy, Shannon entropy) and an arm-dependent learning rate schedule γ_{t,i} = θ√{1 + ∑_{τ<t}(max{p_{τ,i},1/T})^{1-2α}}. The analysis decomposes regret into sub-optimal arm regret (REGSUB), optimal arm regret (REGOPT), residual regret (RESREG), and prediction cost (PRECOST). A key innovation is the monotonicity theorem showing Bregman divergences can be bounded without uniqueness assumptions, and a residual regret analysis that relates this term to self-bounding quantities.

## Key Results
- Achieves O(√KT) adversarial regret and O(log T) stochastic regret without uniqueness assumption
- Removes √logT factor from adversarial regret for log-barrier and Shannon entropy regularizers
- Extends BOBW guarantees to general regularizers beyond Tsallis entropy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The arm-dependent learning rate schedule balances stability and penalty terms for sub-optimal arms to achieve self-bounding regret.
- Mechanism: The learning rate γt_i = θ√(1 + Στ<t (max{pτ_i, 1/T})^(1-2α)) is designed so that for sub-optimal arms, the stability term and penalty term in the Bregman divergence are of the same order, specifically O(∑t (pt_i)^(1-β)/(βγt_i+1)). This creates a self-bounding quantity that can be related back to the regret itself.
- Core assumption: The stability and penalty terms can be balanced through careful learning rate design, and the resulting sum can be bounded using Lemma 6.
- Evidence anchors:
  - [abstract]: "A new arm-dependent learning rate schedule that balances stability and penalty terms for sub-optimal arms"
  - [section 4.1]: "our particular learning rate schedule makes sure that the penalty term is of the same order as the stability term"
  - [corpus]: Weak - the corpus papers focus on different algorithmic approaches (stability-penalty matching, perturbed leader) without this specific learning rate mechanism
- Break condition: If the learning rate cannot maintain the balance between stability and penalty terms, or if the conditions for Lemma 6 fail to hold for the resulting sequence.

### Mechanism 2
- Claim: The monotonicity theorem for Bregman divergences allows regret on optimal arms to be non-positive without uniqueness assumption.
- Mechanism: The general monotonicity theorem (Theorem 7) shows that under two mild conditions on the regularizer - differentiability/concavity of the derivative and non-increasing regularizer - the Bregman divergence Dt,t+1_U(pt, qt+1) ≤ Dt,t+1_U(qt, qt+1). This means pulling optimal arms incurs no regret even when multiple optimal arms exist.
- Core assumption: The regularizer satisfies the two conditions of Theorem 7 (differentiable concave derivative and non-increasing function) for all arms in U.
- Evidence anchors:
  - [abstract]: "A general monotonicity theorem for Bregman divergences that applies to a broad spectrum of regularizers"
  - [section 4.2]: "we develop the following general monotonicity theorem which applies to a broad spectrum of regularizers as long as they satisfy two mild conditions"
  - [corpus]: Weak - the corpus papers don't discuss this specific monotonicity property or its application to removing uniqueness assumptions
- Break condition: If the regularizer violates either condition of Theorem 7 (e.g., becomes non-monotonic or has non-concave derivatives), the regret on optimal arms could become positive.

### Mechanism 3
- Claim: The residual regret can be decomposed and bounded by relating it to self-bounding quantities through careful case analysis.
- Mechanism: The residual regret Dt+1(¯pt+1, pt+1) is decomposed into three parts using intermediate points ¯pt+1 and ¯pt+1_V. The analysis shows that under certain conditions, this term can be bounded by ∑i∈V (pt_i)^(2-β)/(γt_i wt_i), which relates to the self-bounding term. When conditions fail, the probability of selecting optimal arms must be bounded by the total probability of selecting sub-optimal arms, allowing conversion of dependence on U to V.
- Core assumption: The multiplicative stability relation between pt, ¯pt+1, and pt+1 holds (pt_i ≤ 2¯pt+1_i and pt_i ≤ 2pt+1_i), and the case analysis can be completed successfully.
- Evidence anchors:
  - [abstract]: "A new analysis that relates the residual regret to self-bounding quantities"
  - [section 4.3]: "we carefully consider two cases and show that in both cases it can be related to some self-bounding quantities"
  - [corpus]: Weak - the corpus papers don't discuss this specific residual regret decomposition or case analysis approach
- Break condition: If the multiplicative stability relation fails (due to lack of log-barrier or other reasons), or if the case analysis cannot establish the required bounds.

## Foundational Learning

- Concept: Bregman divergences and their properties
  - Why needed here: The entire analysis framework relies on decomposing regret into Bregman divergence terms and manipulating them through various inequalities and monotonicity properties
  - Quick check question: Can you explain why Dt,t+1(x,y) = φt(x) - φt+1(y) - ⟨∇φt+1(y), x-y⟩ represents a "skewed" divergence and how it differs from the standard Dt(x,y)?

- Concept: Self-bounding quantities and their role in achieving gap-dependent regret
- Why needed here: The key to achieving instance-optimal regret in the stochastic setting is showing that certain quantities can be bounded in terms of the regret itself, allowing interpolation between O(log T) and O(√T) regret depending on the corruption level
  - Quick check question: Given S1(x) = E[√(x · Σt∈T Σi∈V pt_i)], can you show how under Condition (2) this can be bounded by z(RegT + C) + x/(4zΔ_MIN) for any z > 0?

- Concept: KKT conditions and Lagrangian multipliers in constrained optimization
  - Why needed here: The analysis requires understanding the optimality conditions for the benchmark qt and intermediate points ¯pt+1, which involve Lagrange multipliers that need to be carefully bounded
  - Quick check question: For the optimization problem qt = argmin_{p∈Ω_U} ⟨p, Στ<t ℓ̂τ⟩ + φt(p), can you write out the KKT conditions and explain what they tell us about the relationship between pt and qt for i ∈ U?

## Architecture Onboarding

- Component map: FTRL framework -> Arm-dependent learning rate schedule -> Bregman divergence decomposition -> Monotonicity analysis -> Residual regret decomposition -> Final regret bound synthesis
- Critical path: Learning rate design → Self-bounding for sub-optimal arms → Monotonicity for optimal arms → Residual regret decomposition → Final regret bound synthesis
- Design tradeoffs:
  - Arm-dependent vs arm-independent learning rates: More complex analysis but removes uniqueness assumption
  - Extra log-barrier: Ensures multiplicative stability but adds complexity and extra log terms
  - Choice of regularizer: Different regularizers (log-barrier, Shannon, Tsallis) offer different tradeoffs between adversarial and stochastic regret bounds
- Failure signatures:
  - Instability in multiplicative relation: Suggests log-barrier coefficient too small or learning rate schedule inappropriate
  - Positive regret on optimal arms: Indicates monotonicity conditions violated or KKT analysis incorrect
  - Sub-optimal regret scaling: Suggests self-bounding quantities not properly bounded or learning rate balance incorrect
- First 3 experiments:
  1. Implement Algorithm 1 with log-barrier regularizer and test multiplicative stability relation on synthetic data with known optimal arms
  2. Verify monotonicity property empirically by computing Bregman divergences for various pt, qt pairs under different regularizers
  3. Test self-bounding property by computing S1(x) and S2(x) for synthetic sequences and verifying they can be bounded by z(RegT + C) + constants

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the extra log-barrier regularizer necessary for achieving best-of-both-worlds guarantees with general regularizers?
- Basis in paper: [explicit] The paper notes that various places of the analysis require the learner's distribution over arms to be stable in a multiplicative sense between two consecutive rounds, and the authors achieve this by adding an extra small amount of log-barrier. They state that "while we do not know how to prove the same results without this extra tweak, we conjecture that it is indeed unnecessary."
- Why unresolved: The authors conjecture the log-barrier is unnecessary but do not prove it. Removing the log-barrier would simplify the algorithms and potentially lead to tighter regret bounds.
- What evidence would resolve it: Proving regret bounds for the multi-armed bandit problem with general regularizers and the proposed learning rate schedule, without adding any extra log-barrier, would demonstrate the conjecture is correct.

### Open Question 2
- Question: Can the √logT factor in the adversarial regret bound for β-Tsallis entropy regularizers with β≠1/2 be removed?
- Basis in paper: [explicit] The authors conjecture that the √logT factor in the adversarial regret bound for β-Tsallis entropy regularizers when β≠1/2 can be removed, noting "it is the case when using a fixed learning rate (Audibert and Bubeck, 2009; Abernethy et al., 2015)".
- Why unresolved: The authors do not provide a proof or counterexample for this conjecture. Removing the √logT factor would improve the adversarial regret bounds for β-Tsallis entropy with β≠1/2.
- What evidence would resolve it: Proving a regret bound of order O(√KT) (without the √logT factor) for the adversarial setting when using β-Tsallis entropy with any β∈(0,1) and the proposed learning rate schedule would confirm the conjecture.

### Open Question 3
- Question: Is the proposed arm-dependent learning rate schedule necessary for achieving best-of-both-worlds guarantees without the uniqueness assumption?
- Basis in paper: [inferred] The authors propose a new arm-dependent learning rate schedule that balances stability and penalty terms for sub-optimal arms. They state that "this schedule is not only conceptually simpler than those used in (Ito, 2021; Ito et al., 2022) and important for removing the uniqueness assumption, but also leads to better bounds in some cases."
- Why unresolved: The authors do not provide a proof or counterexample showing that an arm-independent learning rate cannot achieve the same results. Determining the necessity of the arm-dependent learning rate would help understand the fundamental requirements for best-of-both-worlds algorithms without uniqueness.
- What evidence would resolve it: Proving that an arm-independent learning rate schedule cannot achieve the same regret bounds (especially for general regularizers without the uniqueness assumption) would confirm the necessity of the proposed arm-dependent learning rate. Alternatively, showing an arm-independent learning rate that achieves the same results would demonstrate it is not necessary.

## Limitations
- The analysis is highly complex and may be difficult to implement in practice
- The extra log-barrier term adds implementation complexity without clear empirical validation
- The arm-dependent learning rate schedule requires careful tuning and may not generalize well to all problem settings

## Confidence
- Theoretical framework: High - The core mathematical results appear sound and build on established FTRL theory
- Practical implementation: Medium - The complex analysis suggests potential numerical stability issues and implementation challenges
- Empirical validation: Low - The paper lacks comprehensive experiments demonstrating the practical benefits of the theoretical improvements

## Next Checks
1. Implement Algorithm 1 with all three regularizers and verify the multiplicative stability relation (pt_i ≤ 2¯pt+1_i) holds empirically across a range of stochastic and adversarial environments with varying numbers of optimal arms.

2. Conduct a systematic ablation study removing the extra log-barrier term to quantify its impact on regret bounds and identify the minimum coefficient needed to maintain the multiplicative stability relation.

3. Develop and test a simplified version of the algorithm that approximates the arm-dependent learning rate schedule with a more practical, computationally efficient alternative while preserving the theoretical guarantees.