---
ver: rpa2
title: 'MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning'
arxiv_id: '2311.02303'
source_url: https://arxiv.org/abs/2311.02303
tags:
- code
- tasks
- task
- mftcoder
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MFTCoder, a multitask fine-tuning framework
  for enhancing code large language models (Code LLMs) by training them on multiple
  code-related tasks simultaneously. The approach addresses data imbalance, varying
  difficulty levels, and inconsistent convergence speeds through balanced loss functions
  and efficient training techniques like dynamic padding and parameter-efficient fine-tuning.
---

# MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning

## Quick Facts
- arXiv ID: 2311.02303
- Source URL: https://arxiv.org/abs/2311.02303
- Authors: 
- Reference count: 37
- CodeFuse-CodeLLama-34B achieves 74.4% pass@1 on HumanEval, surpassing GPT-4's zero-shot performance

## Executive Summary
MFTCoder introduces a multitask fine-tuning framework for code large language models that addresses common challenges in multi-task learning through balanced loss functions, efficient training techniques, and parameter-efficient fine-tuning. The framework enables simultaneous training on multiple code-related tasks while maintaining or improving performance across all tasks. By incorporating dynamic padding, pack tokenization, and LoRA/QLoRA adapters, MFTCoder achieves superior performance compared to individual fine-tuning and mixed-task approaches while being more resource-efficient. The approach demonstrates strong generalization capabilities and can maintain or enhance NLP capabilities when fine-tuned with code data.

## Method Summary
MFTCoder is a multitask fine-tuning framework that trains code LLMs on 35 downstream code-related tasks simultaneously. The method employs balanced loss functions to address data imbalance and varying task difficulties, dynamic padding and pack tokenization to improve training efficiency, and parameter-efficient fine-tuning using LoRA or QLoRA adapters. The framework uses a weighted loss approach where each task's contribution is inversely proportional to its dataset size or token count, ensuring no single task dominates training. Training is conducted using 16 A100 GPUs with batch size 128 and learning rate 2e-4, with early stopping based on validation loss.

## Key Results
- CodeFuse-CodeLLama-34B fine-tuned with MFTCoder achieves 74.4% pass@1 on HumanEval, surpassing GPT-4's zero-shot performance of 67%
- MFTCoder outperforms both individual fine-tuning and mixed-task fine-tuning methods on multiple benchmarks
- The framework demonstrates strong generalization to unseen tasks and maintains or improves NLP capabilities when fine-tuned with code data
- Achieves 71.1% pass@1 on MBPP benchmark while using significantly fewer parameters than full fine-tuning approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Balanced loss functions prevent over-optimization of tasks with larger datasets
- Mechanism: Tasks are weighted inversely to their total samples or valid tokens, ensuring equal convergence progress
- Core assumption: Equal convergence progress across tasks is necessary for optimal multi-task performance
- Evidence anchors: [abstract] addresses data imbalance and inconsistent convergence speeds; [section] describes weight assignment strategy
- Break condition: Extreme task heterogeneity may still lead to sub-optimal convergence despite equal weighting

### Mechanism 2
- Claim: Dynamic padding and pack tokenization improve training speed
- Mechanism: Dynamic padding adjusts micro-batch window size to maximum sample length; pack mode concatenates samples to maximize input utilization
- Core assumption: Reducing padding tokens directly improves training efficiency without affecting performance
- Evidence anchors: [section] explains dynamic padding reduces padding token proportion; [section] describes pack mode benefits
- Break condition: Highly variable sample lengths or excessive truncation in pack mode may degrade training effectiveness

### Mechanism 3
- Claim: PEFT techniques enable efficient fine-tuning with minimal resource requirements
- Mechanism: LoRA adds low-rank adapter matrices; QLoRA combines LoRA with 4-bit quantization
- Core assumption: Low-rank adaptations can capture task-specific knowledge without full fine-tuning
- Evidence anchors: [section] lists supported PEFT methods and models; [section] describes LoRA and QLoRA implementation
- Break condition: Tasks requiring significant parameter updates may suffer compared to full fine-tuning

## Foundational Learning

- Concept: Multitask learning tradeoffs
  - Why needed here: Understanding task interference vs. knowledge sharing is critical for effective MFT framework design
  - Quick check question: What happens to task performance when adding too many heterogeneous tasks to a multi-task model?

- Concept: Parameter-efficient fine-tuning
  - Why needed here: PEFT techniques are essential for making large-scale multi-task training feasible on limited hardware
  - Quick check question: How does the number of trainable parameters in LoRA scale with the rank parameter r?

- Concept: Loss function design for imbalanced data
  - Why needed here: Proper weighting of task losses is crucial for preventing data imbalance from skewing training
  - Quick check question: What is the mathematical difference between sample-based and token-based weighting schemes?

## Architecture Onboarding

- Component map: Data pipeline → Tokenizer (dynamic padding/pack mode) → Model (base LLM + LoRA/QLoRA adapters) → Loss computation (balanced loss functions) → Optimizer → Validation

- Critical path: Data loading → Tokenization → Forward pass → Loss computation → Backward pass (adapter parameters only) → Parameter update

- Design tradeoffs:
  - Memory vs. performance: Higher rank in LoRA adapters improves performance but increases memory usage
  - Speed vs. effectiveness: Pack mode is faster but may cause information loss if sequences are truncated
  - Task balance vs. convergence: Too aggressive balancing may slow down overall training

- Failure signatures:
  - Training stalls: Check if pack mode is causing excessive truncation
  - One task dominates: Verify balanced loss weights are being applied correctly
  - Memory overflow: Reduce rank in LoRA adapters or switch to QLoRA

- First 3 experiments:
  1. Single task fine-tuning with dynamic padding vs. standard padding to measure speed improvement
  2. Two-task MFT with balanced loss vs. unweighted loss to verify convergence balance
  3. LoRA vs. QLoRA on same task to compare performance vs. memory tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can convergence speed imbalance among different tasks in multi-task learning be effectively addressed?
- Basis in paper: [inferred] Existing solutions like FAMO require dual back-propagation, doubling training time and increasing epoch requirements
- Why unresolved: Current approaches significantly increase resource requirements without optimal convergence speed control
- What evidence would resolve it: Development of more adaptive multi-task optimization that addresses convergence imbalance without doubling training time

### Open Question 2
- Question: What are the precise criteria for task delineation in multi-task learning?
- Basis in paper: [inferred] Tasks with distinct main desired abilities are more suitable for task splitting and MFT training
- Why unresolved: Paper provides practical experience but lacks precise criteria for task selection
- What evidence would resolve it: Establishment of more precise criteria for task delineation to guide multi-task learning selection

### Open Question 3
- Question: Why do MFT-trained models produce more concise results while task-mixed SFT produces more Chain-of-Thought information?
- Basis in paper: [inferred] MFT models show higher similarity to reference answers and more concise content
- Why unresolved: Paper is researching underlying mechanisms behind these performance differences
- What evidence would resolve it: Investigation of mechanisms leading to differences in inference results between MFT and task-mixed SFT

## Limitations
- Scalability concerns beyond tested tasks and datasets with extreme heterogeneity
- Limited evaluation beyond pass@1 metrics for code generation tasks
- Claims about maintaining NLP capabilities require more rigorous testing across multiple benchmarks
- Balanced loss approach may not generalize well to domains with fundamentally different learning objectives

## Confidence
- **High confidence**: Core architectural approach (PEFT + balanced loss + efficient tokenization) is technically sound; 74.4% HumanEval score provides strong empirical evidence
- **Medium confidence**: Generalization claims to unseen tasks supported by single-point validation but lack comprehensive testing; convergence-balancing mechanism needs broader validation
- **Low confidence**: Claims about maintaining NLP capabilities based on limited testing; require more rigorous evaluation across multiple NLP benchmarks

## Next Checks
1. **Cross-domain generalization test**: Apply MFTCoder to heterogeneous task set including non-code tasks (e.g., summarization, question answering) to verify balanced loss mechanism scales beyond code-related tasks

2. **Convergence rate analysis**: Conduct ablation studies comparing convergence speeds across individual tasks in MFTCoder versus standard multi-task training, measuring whether convergence-balancing loss accelerates overall training

3. **Resource efficiency validation**: Measure actual training speed improvement from dynamic padding and pack mode across different batch sizes and sequence length distributions, particularly testing edge cases with dramatic length variation