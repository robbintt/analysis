---
ver: rpa2
title: SurreyAI 2023 Submission for the Quality Estimation Shared Task
arxiv_id: '2312.00525'
source_url: https://arxiv.org/abs/2312.00525
tags:
- language
- quality
- translation
- estimation
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the task of quality estimation (QE) for machine
  translation, which is important when no reference translation is available to assess
  the quality of a translation. The proposed approach builds on the TransQuest framework
  and explores using various autoencoder pre-trained language models (PTLMs) - XLMV,
  InfoXLM-large, and XLMR-large - within the MonoTransQuest architecture for sentence-level
  QE.
---

# SurreyAI 2023 Submission for the Quality Estimation Shared Task

## Quick Facts
- arXiv ID: 2312.00525
- Source URL: https://arxiv.org/abs/2312.00525
- Reference count: 7
- Key outcome: MonoTQ-InfoXLM-large achieved Spearman correlation scores of 0.713 (En-Hi), 0.668 (En-Gu), 0.623 (En-Te), 0.600 (En-Mr), and 0.479 (En-Ta), significantly improving over baseline for most language pairs

## Executive Summary
This paper presents the SurreyAI 2023 submission for the WMT23 sentence-level quality estimation shared task, focusing on predicting translation quality without reference translations for five low-resource Indian language pairs. The approach builds on the TransQuest framework, fine-tuning three pre-trained language models (XLMV, InfoXLM-large, and XLMR-large) within the MonoTransQuest architecture using CLS-token pooling. The MonoTQ-InfoXLM-large configuration emerged as the best single model, achieving competitive performance with the top systems in the shared task. The study also explored ensemble methods but found that simple averaging of model predictions did not consistently outperform the best individual model.

## Method Summary
The method employs MonoTransQuest architecture with three pre-trained autoencoder models: XLMV-base, InfoXLM-large, and XLMR-large. Source and target sentences are concatenated with a [SEP] token and passed through the pre-trained transformer encoder. The CLS-token embedding is extracted and passed through a softmax layer to predict quality scores. Models are fine-tuned for 3 epochs using Adam optimizer (learning rate 2e-5) with early stopping based on validation loss. An ensemble approach averages predictions from all three models. Training uses batch size 8 on the WMT23 QE dataset with approximately 7,000 sentence pairs per language pair.

## Key Results
- MonoTQ-InfoXLM-large achieved highest Spearman correlation scores across all language pairs (0.713 En-Hi, 0.668 En-Gu, 0.623 En-Te, 0.600 En-Mr, 0.479 En-Ta)
- Single-model configuration outperformed ensemble approach for most language pairs despite ensemble's higher memory requirements
- InfoXLM-large's information-theoretic framework with mutual information maximization between multilingual texts contributed to superior cross-lingual performance
- CLS-token pooling strategy proved effective for capturing holistic translation quality information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MonoTQ-InfoXLM-large achieves superior performance by leveraging InfoXLM's information-theoretic framework that maximizes mutual information between multilingual texts at different granularities.
- Mechanism: The InfoXLM architecture enhances cross-lingual representation learning through contrastive learning, treating bilingual sentence pairs as views of the same meaning. This allows better capture of shared information across languages, particularly beneficial for low-resource language pairs.
- Core assumption: Cross-lingual representations learned through mutual information maximization transfer effectively to quality estimation tasks across diverse language pairs.
- Evidence anchors:
  - [abstract]: "InfoXLM-large introduces an innovative strategy addressing this issue by achieving scalability to extensive multilingual vocabularies"
  - [section]: "InfoXML-large introduces a novel pre-training task based on contrastive learning, treating bilingual sentence pairs as views of the same meaning"
  - [corpus]: Weak evidence - corpus shows related work on InfoXLM but no direct QE applications
- Break condition: If the pre-training data does not adequately represent the target languages or if the QE task requires language-specific rather than cross-lingual features.

### Mechanism 2
- Claim: The CLS-token strategy provides more effective pooling than MEAN or MAX strategies for QE tasks.
- Mechanism: Using the [CLS] token's embedding as input for the softmax layer captures the holistic sequence representation, which is more suitable for predicting overall translation quality than averaging or max-pooling token-level embeddings.
- Core assumption: The [CLS] token embedding adequately represents the semantic and quality-related information of the entire source-target sentence pair.
- Evidence anchors:
  - [section]: "Ranasinghe et al. (2020b) highlights the superiority of the CLS-strategy over the MEAN-strategy... and MAX-strategy for pooling within the MonoTransQuest framework"
  - [abstract]: "We have used the CLS-strategy (using the output of the [CLS] token) to extract the output from the transformer model"
  - [corpus]: Weak evidence - corpus neighbors focus on different QE approaches without specific pooling strategy comparisons
- Break condition: If the [CLS] token fails to capture task-relevant information for specific language pairs or if the translation quality assessment requires more granular token-level analysis.

### Mechanism 3
- Claim: Single-model configurations can outperform ensemble approaches despite higher memory requirements for ensembles.
- Mechanism: Each pre-trained model captures different aspects of translation quality, but averaging their outputs may dilute strong individual signals. The InfoXLM-large and XLMV models may have complementary strengths that don't necessarily benefit from ensembling.
- Core assumption: The strongest individual model captures sufficient information for high-quality predictions, and ensemble averaging doesn't consistently add value.
- Evidence anchors:
  - [section]: "it's notable that the single-model configuration of MonoTQ-InfoXLM-large and MonoTQ-XLMV outperform ensemble-TQ for the majority of the language pairs"
  - [abstract]: "the MonoTQ-InfoXLM-large approach emerges as a robust strategy, surpassing all other individual models proposed in this study"
  - [corpus]: Weak evidence - corpus neighbors discuss ensemble methods but don't directly compare single vs ensemble performance
- Break condition: If ensemble methods are implemented with weighted averaging based on model confidence or if the task benefits from capturing diverse error patterns that individual models miss.

## Foundational Learning

- Concept: Spearman correlation coefficient
  - Why needed here: The primary evaluation metric for measuring monotonic relationship between predicted and human quality scores
  - Quick check question: What does a Spearman correlation of 0.713 between predictions and human judgments indicate about the model's performance?

- Concept: Pre-trained transformer architectures (BERT, RoBERTa, XLM-R)
  - Why needed here: Understanding the base models (XLMV, InfoXLM-large, XLMR-large) and how they process input sequences
  - Quick check question: How does the input format "[source][SEP][target]" affect the model's ability to learn translation quality features?

- Concept: Cross-lingual representation learning
  - Why needed here: Essential for understanding why InfoXLM-large performs well on low-resource Indian languages
  - Quick check question: How does maximizing mutual information between multilingual texts improve performance on low-resource language pairs?

## Architecture Onboarding

- Component map: Input sentences → Pre-trained encoder (XLMV/InfoXLM-large/XLMR-large) → CLS-token extraction → Softmax prediction layer → MSE loss calculation

- Critical path: Input → Pre-trained encoder → CLS-token extraction → Softmax prediction → MSE loss calculation

- Design tradeoffs:
  - Single model vs ensemble: Higher memory usage for ensembles (3x) vs marginal performance gains
  - Model selection: InfoXLM-large offers best balance of cross-lingual capability and task performance
  - Training duration: 3 epochs with early stopping based on validation loss

- Failure signatures:
  - Overfitting: Validation loss stops improving after 10 consecutive evaluations
  - Underperformance: Correlation scores below baseline indicate architecture mismatch
  - Memory issues: Out-of-memory errors during training with larger batch sizes

- First 3 experiments:
  1. Baseline comparison: Run MonoTQ-XLMR-large with default settings to establish performance baseline
  2. Model ablation: Test each pre-trained model (XLMV, InfoXLM-large, XLMR-large) separately with identical hyperparameters
  3. Ensemble evaluation: Implement ensembleTQ and compare against best single model performance across all language pairs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed models perform on other low-resourced language pairs not included in this study, such as English-Oriya or English-Assamese?
- Basis in paper: [inferred] The paper states that the models were tested on 5 language pairs, all of which are low-resourced except for English-Hindi. The paper also mentions that the models could be tested on a "broader spectrum of low-resourced language pairs" in future research.
- Why unresolved: The paper only tested the models on 5 language pairs, and it is unclear how the models would perform on other low-resourced language pairs.
- What evidence would resolve it: Testing the models on a wider range of low-resourced language pairs and comparing the results.

### Open Question 2
- Question: How does the performance of the proposed models compare to other state-of-the-art models in quality estimation tasks, such as those using large language models?
- Basis in paper: [inferred] The paper mentions that the proposed models achieved competitive performance with the best systems in the WMT23 sentence-level QE shared task, but it does not provide a direct comparison with other state-of-the-art models.
- Why unresolved: The paper does not provide a direct comparison with other state-of-the-art models, and it is unclear how the proposed models would perform in comparison.
- What evidence would resolve it: Conducting a direct comparison between the proposed models and other state-of-the-art models in quality estimation tasks.

### Open Question 3
- Question: How does the memory usage of the proposed models compare to other models in quality estimation tasks, and is there a way to reduce the memory usage without sacrificing performance?
- Basis in paper: [explicit] The paper mentions that ensemble models demand significantly more memory space than single-model setups, despite only offering a marginal boost in performance.
- Why unresolved: The paper does not provide a direct comparison of memory usage with other models, and it is unclear how the memory usage of the proposed models compares to other models in quality estimation tasks.
- What evidence would resolve it: Conducting a comparison of memory usage between the proposed models and other models in quality estimation tasks, and exploring ways to reduce memory usage without sacrificing performance.

## Limitations

- The ensemble approach using simple averaging failed to consistently outperform the best single model, suggesting the need for more sophisticated ensemble strategies
- The study lacks detailed error analysis and failure case investigation across different language pairs
- Limited dataset sizes for low-resource languages raise concerns about model generalization and potential overfitting

## Confidence

- **High Confidence**: The superiority of MonoTQ-InfoXLM-large as the best single model configuration is well-supported by experimental results showing consistent improvements across all five language pairs
- **Medium Confidence**: The claim that CLS-token pooling outperforms MEAN and MAX strategies is supported by prior work but lacks direct experimental validation within this study
- **Low Confidence**: The assertion that ensemble methods consistently underperform single models is premature, as the ensemble implementation used simple averaging without exploring more sophisticated combination strategies

## Next Checks

1. **Ensemble Method Enhancement**: Implement weighted ensemble averaging based on individual model confidence scores or validation performance, then compare against the simple averaging approach to determine if ensemble methods can be optimized to outperform single models

2. **Ablation Study on Pooling Strategies**: Conduct controlled experiments comparing CLS-token, MEAN, and MAX pooling strategies within the same model architecture and training setup to empirically validate the claimed superiority of CLS-token pooling for QE tasks

3. **Error Analysis and Failure Case Investigation**: Perform detailed analysis of predictions where the model underperforms (e.g., language pairs with correlation below 0.5), examining specific error patterns, linguistic features of failing instances, and potential model biases to understand limitations and guide architectural improvements