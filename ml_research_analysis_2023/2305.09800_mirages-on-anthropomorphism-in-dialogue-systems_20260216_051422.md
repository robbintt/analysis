---
ver: rpa2
title: 'Mirages: On Anthropomorphism in Dialogue Systems'
arxiv_id: '2305.09800'
source_url: https://arxiv.org/abs/2305.09800
tags:
- systems
- dialogue
- human
- language
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Anthropomorphism in dialogue systems can lead to over-trust and
  misuse. Developers and users may unknowingly design systems to be more human-like,
  which can reinforce harmful stereotypes and norms.
---

# Mirages: On Anthropomorphism in Dialogue Systems

## Quick Facts
- arXiv ID: 2305.09800
- Source URL: https://arxiv.org/abs/2305.09800
- Reference count: 19
- One-line primary result: Linguistic factors in dialogue systems contribute to anthropomorphism, leading to over-trust and reinforcement of harmful stereotypes.

## Executive Summary
This paper examines how linguistic and design features in dialogue systems contribute to anthropomorphism, creating perceptions of human-like qualities that can lead to over-trust and misuse. The authors identify voice characteristics, content style, and register as key factors that cumulatively build human-like impressions. They argue that developers must be aware of these effects and avoid anthropomorphic language when describing their systems to create safer, more effective dialogue systems.

## Method Summary
The paper conducts a literature review across psychology, linguistics, and human-computer interaction to identify linguistic factors contributing to anthropomorphism in dialogue systems. The authors synthesize findings from multiple disciplines and provide examples from commercial systems to support their analysis. The approach is primarily qualitative, examining design choices and their effects on user perception rather than conducting controlled experiments.

## Key Results
- Anthropomorphism in dialogue systems can lead to over-trust and misuse of technology
- Cumulative linguistic features (voice, content, style) create anthropomorphic impressions rather than single attributes
- Developer language and framing significantly influence user expectations about system capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Anthropomorphism is cognitively automatic and driven by psychological needs.
- Mechanism: Humans apply their own mental models to non-human entities when those entities display interactive, language-using, and role-fulfilling features. This process is mindless and reinforced by effectance (need to predict and control) and sociality (need for connection).
- Core assumption: People cannot easily suppress this attribution process when conversational systems exhibit human-like cues.
- Evidence anchors:
  - [abstract] "Anthropomorphism in dialogue systems can lead to over-trust and misuse...linguistic factors that contribute to this anthropomorphism"
  - [section 2.1] "the process of anthropomorphising is mostly mindless...automatic and encouraged by cues in their interfaces"
  - [corpus] "From Pixels to Personas: Investigating and Modeling Self-Anthropomorphism in Human-Robot Dialogues" - supports the link between self-anthropomorphic cues and human-like perception
- Break condition: If developers eliminate interactive and language cues that trigger mental model mapping, or if systems are explicitly framed as tools rather than social agents.

### Mechanism 2
- Claim: Linguistic and design features act as cumulative "strokes" that build human-like perception.
- Mechanism: Voice quality (prosody, disfluencies, accent), content style (first-person pronouns, claims of sentience, empathy), and register (phatic expressions, hedging) each add anthropomorphic signals. Collectively, these features create a "portrait" of humanity that users interpret.
- Core assumption: Each feature independently contributes to the overall anthropomorphic impression, and removing them reduces the "painting" of humanity.
- Evidence anchors:
  - [abstract] "linguistic factors that contribute to the anthropomorphism of dialogue systems...reinforce stereotypes of gender roles and notions of acceptable language"
  - [section 3] "no single attribute...makes a system anthropomorphic...each contributes to the painting until 'the face' emerges"
  - [corpus] "Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally" - supports the idea that cumulative design choices influence user perception
- Break condition: If all anthropomorphic cues are systematically removed and replaced with functional, non-human signals.

### Mechanism 3
- Claim: Developers' language and framing amplify anthropomorphism beyond system behavior.
- Mechanism: Using terms like "know," "think," "learn," "understand," or "intelligence" in describing systems leads users to over-attribute cognitive capabilities. This anthropomorphic framing interacts with system outputs to create misleading impressions.
- Core assumption: Users rely heavily on developer language to form expectations about system capabilities.
- Evidence anchors:
  - [abstract] "avoid using anthropomorphic language when describing their systems"
  - [section 5] "of even more influence is the language used by system developers to describe their work...anthropomorphic terminology is deeply rooted in the argot of computer scientists"
  - [corpus] "A Taxonomy of Linguistic Expressions That Contribute To Anthropomorphism of Language Technologies" - provides structured evidence for how language choices shape perception
- Break condition: If developers consistently use precise, non-anthropomorphic terminology in documentation, marketing, and public communication.

## Foundational Learning

- Concept: Cognitive anchoring to human experience
  - Why needed here: Explains why people automatically map their own mental models onto interactive systems, making anthropomorphism inevitable without deliberate mitigation.
  - Quick check question: What psychological mechanism causes people to treat dialogue systems as if they have human-like thoughts?

- Concept: Cumulative signal integration
  - Why needed here: Shows how individual design choices (voice, content, style) combine to create an overall impression of humanity, rather than relying on a single feature.
  - Quick check question: How do multiple small anthropomorphic cues combine to create a strong perception of humanness?

- Concept: Developer framing effects
  - Why needed here: Demonstrates that how systems are described influences user expectations as much as the systems' actual behavior, requiring careful communication choices.
  - Quick check question: Why does the language developers use to describe AI systems affect how users perceive their capabilities?

## Architecture Onboarding

- Component map: User interface (voice/UI cues) -> Natural language generation (content style) -> System description (developer framing) -> User perception (anthropomorphism)
- Critical path: Design choices -> System outputs -> User interaction -> Trust/expectations
- Design tradeoffs: More human-like features increase engagement but risk over-trust; more functional features reduce engagement but improve transparency
- Failure signatures: Users attributing emotions/intentions to system outputs, over-reliance on system advice, misunderstanding of system limitations
- First 3 experiments:
  1. Test user trust levels with systems that have vs. don't have first-person pronouns in responses
  2. Measure engagement differences between systems with vs. without disfluencies in synthetic speech
  3. Compare user understanding of system capabilities when described with vs. without anthropomorphic terminology

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do linguistic cues in dialogue systems differ across languages, and how does this impact anthropomorphic perceptions?
- Basis in paper: [explicit] The paper acknowledges that it primarily focuses on English language dialogue systems due to the authors' backgrounds and the dominance of English in NLP research, but notes that other languages have features such as grammatical ways of denoting animacy and gender that could influence users' personification of systems.
- Why unresolved: The paper does not explore how linguistic cues differ across languages and their impact on anthropomorphic perceptions.
- What evidence would resolve it: Comparative studies across multiple languages to identify linguistic cues that influence anthropomorphic perceptions, and empirical data on how users from different linguistic backgrounds perceive and interact with dialogue systems.

### Open Question 2
- Question: What are the long-term effects of interacting with anthropomorphic dialogue systems on users' social behavior and relationships?
- Basis in paper: [inferred] The paper discusses the potential consequences of anthropomorphism, such as reinforcing stereotypes and trust issues, but does not explore the long-term effects on users' social behavior and relationships.
- Why unresolved: The paper focuses on the immediate consequences of anthropomorphism and does not investigate the potential long-term impacts on users' social interactions and relationships.
- What evidence would resolve it: Longitudinal studies tracking users' social behavior and relationships over time, comparing those who frequently interact with anthropomorphic dialogue systems to those who do not.

### Open Question 3
- Question: How can developers create effective dialogue systems that minimize anthropomorphism while maintaining user engagement and satisfaction?
- Basis in paper: [explicit] The paper recommends that developers consider the appropriateness of anthropomorphic tools, reassess their research goals, and avoid using anthropomorphic language when describing their systems. However, it does not provide specific guidelines for creating effective dialogue systems that balance these concerns.
- Why unresolved: The paper identifies the need for developers to minimize anthropomorphism but does not offer concrete strategies for achieving this goal while maintaining user engagement and satisfaction.
- What evidence would resolve it: Empirical studies comparing the effectiveness of different design strategies in creating dialogue systems that minimize anthropomorphism while maintaining user engagement and satisfaction, along with best practices and guidelines for developers.

## Limitations

- Major uncertainties and limitations: This paper presents a qualitative synthesis of anthropomorphism in dialogue systems, but relies heavily on literature review rather than controlled empirical studies.
- No specific datasets or empirical experiments are described; unclear how findings are quantified or validated.
- Lack of clear operational definitions for anthropomorphism, leading to inconsistent interpretations across studies.

## Confidence

- High confidence in the psychological mechanisms of anthropomorphism (automatic mental model mapping, effectance, sociality)
- Medium confidence in the cumulative effect of linguistic features creating anthropomorphic impressions
- Medium confidence in developer framing effects on user expectations
- Low confidence in specific quantitative relationships between design choices and user outcomes

## Next Checks

1. Conduct controlled experiments measuring user trust and understanding when dialogue systems use first-person pronouns versus third-person or functional descriptions.

2. Test the hypothesis that removing all anthropomorphic cues (voice quality, disfluencies, persona language) systematically reduces user personification while measuring impact on engagement and task completion.

3. Compare user expectations and system use patterns when developers describe AI capabilities using anthropomorphic versus functional terminology in documentation and marketing materials.