---
ver: rpa2
title: Learning Curves for Noisy Heterogeneous Feature-Subsampled Ridge Ensembles
arxiv_id: '2307.03176'
source_url: https://arxiv.org/abs/2307.03176
tags:
- ensembling
- error
- where
- learning
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper develops a theory of feature bagging for linear ridge\
  \ regression ensembles, deriving learning curves via the replica method from statistical\
  \ physics. The analysis shows that subsampling shifts the double-descent peak of\
  \ linear predictors, and introduces heterogeneous feature ensembling\u2014where\
  \ predictors access different numbers of features\u2014as a computationally efficient\
  \ method to mitigate double-descent."
---

# Learning Curves for Noisy Heterogeneous Feature-Subsampled Ridge Ensembles

## Quick Facts
- arXiv ID: 2307.03176
- Source URL: https://arxiv.org/abs/2307.03176
- Reference count: 0
- One-line primary result: Feature subsampling shifts double-descent peaks, and heterogeneous ensembling mitigates double-descent in ridge regression.

## Executive Summary
This paper develops a theory of feature bagging for linear ridge regression ensembles using the replica method from statistical physics. The analysis shows that subsampling shifts the double-descent peak location and that heterogeneous feature ensembling—where predictors access different numbers of features—can effectively mitigate double-descent. The framework reveals how noise levels, data correlations, and task alignment determine optimal ensemble size, with experiments confirming these predictions on both synthetic and CIFAR-10 datasets.

## Method Summary
The authors analyze ensembles of ridge regression models trained on feature-subsampled data, deriving learning curves analytically via the replica method. Each ensemble member is trained on a different subset of features, with heterogeneous connectivity meaning members access varying numbers of features. The analysis focuses on equicorrelated data models and derives explicit formulas for generalization error, showing how subsampling fraction controls double-descent peak location and how heterogeneous connectivity smooths out these peaks through averaging.

## Key Results
- Subsampling fraction ν controls the sample complexity at which double-descent peaks occur
- Heterogeneous ensembling mitigates double-descent by averaging over predictors with different interpolation thresholds
- Optimal ensemble size depends on the interplay between data correlations, readout noise, and task alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subsampling shifts the double-descent peak of a linear predictor.
- Mechanism: By reducing the number of features available to each ensemble member, subsampling effectively reduces the interpolation threshold, causing the double-descent peak to occur at a smaller sample size.
- Core assumption: The feature noise and label noise levels remain constant, and the data correlations are sufficiently strong to allow meaningful subsampling.
- Evidence anchors:
  - [abstract]: "subsampling shifts the double-descent peak of a linear predictor"
  - [section II D]: "As Eg ∼ (α − ν)−1, we see that the generalization error diverges when α = ν. The subsampling fraction ν thus controls the sample complexity α at which the double-descent peak occurs."
  - [corpus]: Weak (no direct mention of double-descent or subsampling effects in neighbor papers)
- Break condition: If the data features are uncorrelated or if the noise levels are too high, subsampling may not effectively shift the double-descent peak.

### Mechanism 2
- Claim: Heterogeneous feature ensembling mitigates double-descent by averaging over predictors with different interpolation thresholds.
- Mechanism: By training ensemble members on varying numbers of features, each member will experience double-descent at a different sample size. Averaging their predictions smooths out the peaks, reducing worst-case error.
- Core assumption: The ensemble members are sufficiently diverse in their feature access and the averaging process effectively combines their predictions.
- Evidence anchors:
  - [abstract]: "introduces heterogeneous feature ensembling... as a computationally efficient method to mitigate double-descent"
  - [section II E]: "if the predictors are heterogeneous in the number of features they see, they will go through double-descent at different sample-sizes. Therefore, bagging them will lead a mitigation of double-descent"
  - [corpus]: Weak (no direct mention of heterogeneous ensembling in neighbor papers)
- Break condition: If the ensemble members are too similar in their feature access or if the averaging process is not effective, heterogeneous ensembling may not mitigate double-descent.

### Mechanism 3
- Claim: The optimal ensemble size depends on the interplay between data correlations, readout noise, and task alignment.
- Mechanism: In equicorrelated data, the optimal number of ensemble members k* is determined by balancing the signal-to-noise ratio, the correlation strength, and the alignment between the data and the task. Higher readout noise or stronger correlations favor larger ensembles.
- Core assumption: The data follows the equicorrelated model and the readout noise is isotropic.
- Evidence anchors:
  - [abstract]: "noise level, data correlations, and task alignment determine optimal ensemble size"
  - [section II F]: "an increase in H causes an increase in k*. This can occur because of a decrease in the signal-to-readout noise ratio s/η2, or through an increase in the correlation strength c"
  - [corpus]: Weak (no direct mention of equicorrelated data or task alignment in neighbor papers)
- Break condition: If the data does not follow the equicorrelated model or if the readout noise is not isotropic, the optimal ensemble size may not be determined by these factors.

## Foundational Learning

- Concept: Replica method from statistical physics
  - Why needed here: The replica method is used to derive analytical expressions for the generalization error of the feature-subsampled ridge ensembles.
  - Quick check question: What is the key idea behind the replica method, and how does it help in calculating the generalization error?

- Concept: Equicorrelated data model
  - Why needed here: The equicorrelated data model simplifies the analysis by assuming a specific correlation structure among the features, allowing for analytical solutions.
  - Quick check question: How does the equicorrelated data model differ from other correlation structures, and why is it useful for this analysis?

- Concept: Double-descent phenomenon
  - Why needed here: Understanding double-descent is crucial for interpreting the results and the motivation behind heterogeneous ensembling.
  - Quick check question: What causes double-descent in linear regression, and how does it relate to the interpolation threshold?

## Architecture Onboarding

- Component map:
  Feature subsampling -> Ridge regression -> Ensemble averaging -> Heterogeneous connectivity

- Critical path:
  1. Generate equicorrelated data with specified noise levels and task alignment.
  2. Implement feature subsampling by creating linear masks for each ensemble member.
  3. Train each ensemble member using ridge regression on their respective feature subsets.
  4. Calculate the generalization error using the replica method or numerical experiments.
  5. Analyze the results to understand the effects of subsampling, ensembling, and heterogeneous connectivity.

- Design tradeoffs:
  - Homogeneous vs. heterogeneous connectivity: Homogeneous connectivity is simpler but may not mitigate double-descent as effectively as heterogeneous connectivity.
  - Number of ensemble members: Increasing the number of members can improve performance but also increases computational cost.
  - Subsampling fraction: The optimal subsampling fraction depends on the data correlations and noise levels.

- Failure signatures:
  - Poor performance: If the ensemble members are too similar or if the averaging process is not effective, the performance may not improve.
  - Overfitting: If the ensemble members are not sufficiently regularized or if the data is too noisy, overfitting may occur.
  - Underfitting: If the ensemble members are too simple or if the data is too complex, underfitting may occur.

- First 3 experiments:
  1. Compare the performance of homogeneous and heterogeneous connectivity on equicorrelated data with varying noise levels and task alignments.
  2. Investigate the effects of different subsampling fractions on the double-descent peak location.
  3. Analyze the impact of data correlations and readout noise on the optimal ensemble size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the double-descent peak shift behavior in heterogeneous ensembling extend to non-linear models, such as deep neural networks?
- Basis in paper: [explicit] The paper mentions that in the nonlinear setting of CIFAR-10 classification with a deep MLP feature map, heterogeneous ensembling prevents catastrophic overfitting and leads to monotonic learning curves without regularization.
- Why unresolved: The analysis is limited to linear ridge regression models, and the behavior in non-linear models is only briefly mentioned with a single experiment.
- What evidence would resolve it: Systematic experiments on a variety of deep learning architectures and datasets, comparing homogeneous vs. heterogeneous ensembling strategies and their effects on double-descent.

### Open Question 2
- Question: What is the optimal strategy for selecting the distribution of feature subsets in heterogeneous ensembling to minimize worst-case error across diverse tasks?
- Basis in paper: [explicit] The paper explores Gamma-distributed feature subset sizes for heterogeneous ensembling but doesn't provide a general method for task-specific optimization.
- Why unresolved: The analysis focuses on a specific distribution (Gamma) and a single task alignment parameter (ρ), without exploring other distributions or task structures.
- What evidence would resolve it: Theoretical analysis of the optimal distribution of feature subset sizes for different data correlation structures, task alignments, and noise levels, potentially using information-theoretic or game-theoretic approaches.

### Open Question 3
- Question: How does the performance of heterogeneous ensembling compare to other regularization techniques, such as early stopping, dropout, or data augmentation, in mitigating double-descent?
- Basis in paper: [inferred] The paper introduces heterogeneous ensembling as an alternative to regularization for mitigating double-descent, but doesn't compare its performance to other established techniques.
- Why unresolved: The paper focuses on the theoretical analysis of heterogeneous ensembling without benchmarking it against other regularization methods.
- What evidence would resolve it: Empirical comparisons of heterogeneous ensembling with other regularization techniques across a range of tasks, datasets, and model architectures, measuring their effectiveness in reducing double-descent and improving generalization.

## Limitations

- Analysis is primarily confined to the high-dimensional linear regime with specific data structures (equicorrelated Gaussian features), limiting direct applicability to general real-world datasets.
- The replica method derivations assume specific distributional properties and infinite limit behaviors that may not hold in finite-sample regimes.
- The theoretical framework does not account for potential interactions between feature subsampling and other forms of regularization beyond ridge penalty.

## Confidence

**High Confidence**: The analytical framework using replica method for feature-subsampled ridge ensembles is mathematically rigorous within the assumed model. The characterization of double-descent peak shifting through subsampling is well-supported by the saddle-point analysis.

**Medium Confidence**: The claims about heterogeneous ensembling mitigating double-descent are supported by both theory and experiments, but the extent of improvement may depend heavily on the specific data distribution and noise structure not fully explored in the paper.

**Low Confidence**: The optimal ensemble size determination based on the interplay of correlation strength, readout noise, and task alignment relies on specific equicorrelated data assumptions that may not generalize to more complex correlation structures.

## Next Checks

1. Conduct numerical experiments comparing analytical predictions with finite-sample performance on synthetic data with varying correlation structures (beyond equicorrelated) to validate the robustness of the learning curve predictions.

2. Systematically vary the distribution parameters for feature subsampling fractions in heterogeneous ensembling and measure the impact on double-descent mitigation across different noise levels and correlation strengths.

3. Apply the heterogeneous ensembling approach to diverse real-world datasets (e.g., different image classification tasks, tabular data) to assess generalizability beyond the equicorrelated Gaussian assumption and CIFAR-10 with deep features.