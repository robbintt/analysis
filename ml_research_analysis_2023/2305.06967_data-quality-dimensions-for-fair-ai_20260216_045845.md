---
ver: rpa2
title: Data quality dimensions for fair AI
arxiv_id: '2305.06967'
source_url: https://arxiv.org/abs/2305.06967
tags:
- data
- bias
- label
- time
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that fairness in AI systems, particularly those
  using face recognition and gender classification, cannot be adequately addressed
  by focusing solely on accuracy. The authors propose that fairness should also consider
  data quality dimensions like completeness, consistency, timeliness, and reliability.
---

# Data quality dimensions for fair AI

## Quick Facts
- arXiv ID: 2305.06967
- Source URL: https://arxiv.org/abs/2305.06967
- Reference count: 10
- Primary result: Fairness in AI systems requires explicit consideration of data quality dimensions like completeness, consistency, timeliness, and reliability, especially for gender classification tasks.

## Executive Summary
This paper argues that current approaches to fairness in AI, which focus primarily on accuracy, are insufficient for addressing bias in systems that classify gender identities. The authors propose that fairness should be evaluated using data quality dimensions—completeness, consistency, timeliness, and reliability—especially when dealing with identities that evolve over time. Through two case studies (non-binary individuals and transgender individuals), the paper demonstrates how static label sets and classification methods fail to capture the dynamic nature of gender identity. A theoretical framework is introduced, linking these data quality dimensions to fairness, and basic results are formulated to support the argument. The paper emphasizes the need for a paradigm shift in how fairness is defined and measured in AI systems.

## Method Summary
The paper presents a theoretical framework for fairness in AI that incorporates data quality dimensions. It uses formal methods to analyze classification systems in changing environments, extending Cleanlab's confident learning approach by adding temporal parameters. The method involves defining completeness as the sufficiency of label breadth, depth, and scope for the task; consistency as the stability of labels over time; timeliness as the relevance of labels to current contexts; and reliability as the boundedness of error rate changes. The authors formulate fairness in terms of these dimensions and propose that a fair AI system must maintain bounded error change (reliability) while adapting its label set (completeness). The approach is illustrated through case studies on gender classification, but the paper does not provide full empirical implementation or validation.

## Key Results
- Fairness in AI systems, especially for gender classification, cannot be achieved by focusing solely on accuracy.
- Data quality dimensions (completeness, consistency, timeliness, reliability) are necessary for addressing bias in AI systems.
- The paper proposes a theoretical framework linking these dimensions to fairness and formulates basic results supporting this approach.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating data quality dimensions like completeness, consistency, and timeliness into AI fairness frameworks can improve classification accuracy for underrepresented or shifting identities.
- Mechanism: Completeness ensures label sets cover all real-world identities; consistency tracks changes in correct labels over time; timeliness reduces stale or mismatched labels.
- Core assumption: Temporal dynamics of identity (e.g., gender fluidity) mean static labels become incomplete or inconsistent, causing bias.
- Evidence anchors:
  - [abstract] "fairness should also consider data quality dimensions like completeness, consistency, timeliness, and reliability"
  - [section 4] "Completeness is the level at which data have the sufficient breadth, depth, and scope for their task"
  - [corpus] Weak — no corpus entries mention temporal data quality explicitly.
- Break condition: If identity categories are truly static or label changes are negligible over the system's operational period.

### Mechanism 2
- Claim: Extending Cleanlab's confident learning with temporal parameters can detect and correct label errors arising from shifting identities.
- Mechanism: Augment the confident joint to include time indices; compute label error probabilities conditioned on both class and time.
- Core assumption: The joint distribution p(ỹ, y*) changes over time in predictable ways for certain populations.
- Evidence anchors:
  - [section 5.4] "the probability of assigning a label may change over time"
  - [section 6] "we have suggested considering temporal accuracy as a function of the error rate over time"
  - [corpus] Missing — no corpus support for time-conditioned confident learning.
- Break condition: If the time-dependence of label errors is too complex or non-stationary for the current modeling approach.

### Mechanism 3
- Claim: Defining fairness in terms of reliability and completeness over time provides necessary conditions for fair AI classification.
- Mechanism: FairnessT(X) iff RelT(X) and ComplT(L(X)), where reliability bounds error rate change and completeness ensures labels match evolving identities.
- Core assumption: A fair system must maintain bounded error change (reliability) while adapting its label set (completeness).
- Evidence anchors:
  - [section 6] "Deﬁnition 3 (Fairness for AI systems). FairT (X) only ifRelT (X) andComplT (L(X))"
  - [section 6] "Theorem 1. Given a label setL complete at timet, a classiﬁcation algorithm guarantees a fair classiﬁcation at time t′ >t if and only if the change rate determined with respect toL isϵ<π"
  - [corpus] No direct support for formal fairness theorems combining reliability and completeness.
- Break condition: If the system's operational context does not allow for label set updates or error rate tracking.

## Foundational Learning

- Concept: Temporal data quality dimensions (completeness, consistency, timeliness, reliability)
  - Why needed here: These dimensions address bias sources ignored by accuracy-only approaches, especially for identities that shift over time.
  - Quick check question: How would you define completeness for a label set in a system tracking gender identity?

- Concept: Confident learning and the confident joint
  - Why needed here: Cleanlab's method for finding label errors is the base algorithm being extended; understanding it is essential for implementing temporal extensions.
  - Quick check question: What inputs does Cleanlab's confident joint require, and what does it output?

- Concept: Fairness definitions and bias mitigation strategies
  - Why needed here: The paper proposes a new fairness definition based on data quality; understanding existing definitions helps contextualize the contribution.
  - Quick check question: What is the difference between individual and group fairness?

## Architecture Onboarding

- Component map:
  - Data ingestion → Label set validation (completeness check) → Temporal labeling module → Cleanlab confident learning → Reliability monitoring → Fairness evaluation
- Critical path: Label set completeness → Temporal labeling accuracy → Error rate tracking → Reliability threshold check → Fairness decision
- Design tradeoffs:
  - Broader label sets improve completeness but may increase classification complexity and training data needs.
  - Real-time label updates improve timeliness but may introduce noise or instability.
  - Strict reliability thresholds ensure fairness but may reject otherwise useful models.
- Failure signatures:
  - High completeness but low reliability → Label set is complete but classification accuracy drifts over time.
  - Low completeness but high reliability → Model is stable but misses entire identity categories.
  - Poor timeliness → Labels become stale, causing misclassification of identity shifts.
- First 3 experiments:
  1. Evaluate completeness of a static gender label set against a ground truth of non-binary identities.
  2. Measure change rate (ε) of classification accuracy for a dataset with known label drift over time.
  3. Implement a temporal extension to Cleanlab's confident joint and test on a synthetic dataset with simulated label changes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively implement temporal parameters in AI classification algorithms to account for changing gender identities and label sets?
- Basis in paper: [explicit] The paper discusses the need for temporal parametrization in classification tasks to account for changing gender identities and label sets.
- Why unresolved: The paper suggests the need for temporal parametrization but does not provide specific implementation strategies or technical details.
- What evidence would resolve it: Successful implementation of temporal parameters in a real-world AI classification system, demonstrating improved fairness and accuracy for non-binary and transgender individuals.

### Open Question 2
- Question: What is the optimal threshold value (π) for determining the reliability of AI classification systems over time?
- Basis in paper: [explicit] The paper proposes using a threshold value (π) to determine the reliability of classification systems based on change rates in accuracy over time.
- Why unresolved: The paper suggests this concept but does not provide guidance on how to determine an appropriate threshold value.
- What evidence would resolve it: Empirical studies testing various threshold values across different AI classification systems and datasets, demonstrating their impact on system reliability and fairness.

### Open Question 3
- Question: How can we develop a comprehensive framework for measuring and mitigating all relevant types of bias in AI systems, including those related to data quality dimensions?
- Basis in paper: [explicit] The paper argues for the importance of considering data quality dimensions (completeness, consistency, timeliness, reliability) in addressing bias in AI systems.
- Why unresolved: The paper introduces this concept but does not provide a complete framework or methodology for measuring and mitigating all relevant biases.
- What evidence would resolve it: A validated, comprehensive framework that successfully identifies and mitigates multiple types of bias in AI systems across various applications and datasets.

## Limitations
- The paper remains largely theoretical with minimal empirical validation.
- Specific implementation details for temporal extensions to Cleanlab are not provided.
- Optimal threshold values for reliability (π) are not defined or justified.

## Confidence
- Mechanism 1 (Data quality importance): Medium — concept is well-argued but lacks corpus support.
- Mechanism 2 (Temporal confident learning): Low — no empirical or corpus evidence for feasibility.
- Mechanism 3 (Formal fairness theorems): Low — theoretical but untested in practice.

## Next Checks
1. Implement and test the temporal extension to Cleanlab on a dataset with known label drift.
2. Define and validate criteria for determining the reliability threshold π.
3. Conduct a case study to evaluate completeness of a gender label set against real-world identity distributions.