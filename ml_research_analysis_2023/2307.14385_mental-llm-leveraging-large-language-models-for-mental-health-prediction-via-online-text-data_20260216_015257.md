---
ver: rpa2
title: 'Mental-LLM: Leveraging Large Language Models for Mental Health Prediction
  via Online Text Data'
arxiv_id: '2307.14385'
source_url: https://arxiv.org/abs/2307.14385
tags:
- mental
- health
- tasks
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study evaluates multiple large language models (LLMs) including
  Alpaca, Alpaca-LoRA, and GPT-3.5 on mental health prediction tasks using online
  text data. Three approaches are tested: zero-shot prompting, few-shot prompting,
  and instruction fine-tuning.'
---

# Mental-LLM: Leveraging Large Language Models for Mental Health Prediction via Online Text Data

## Quick Facts
- arXiv ID: 2307.14385
- Source URL: https://arxiv.org/abs/2307.14385
- Reference count: 40
- Primary result: Instruction fine-tuning significantly improves LLM performance on mental health prediction tasks, with Mental-Alpaca outperforming GPT-3.5 by 16.7% on balanced accuracy

## Executive Summary
This study evaluates large language models (LLMs) for mental health prediction using online text data from social media platforms. The researchers test three approaches: zero-shot prompting, few-shot prompting, and instruction fine-tuning across multiple mental health datasets. While zero-shot and few-shot methods show promising but limited performance, instruction fine-tuning significantly boosts LLM performance across all tasks simultaneously. The best finetuned model, Mental-Alpaca, outperforms GPT-3.5 by 16.7% on balanced accuracy and performs comparably to state-of-the-art task-specific models. The study provides practical guidelines for using LLMs in mental health prediction and emphasizes the importance of fine-tuning with diverse datasets.

## Method Summary
The researchers conducted a three-stage evaluation of LLMs for mental health prediction tasks using four datasets from social media platforms. They first tested zero-shot and few-shot prompting approaches with various prompt templates, then implemented instruction fine-tuning by merging all datasets and training on instruction-based prompts. The finetuning process used Adam optimizer with learning rate 2e-5 for 3 epochs. Models were evaluated using balanced accuracy across six binary and multi-class classification tasks including stress, depression, and suicide risk assessment.

## Key Results
- Instruction fine-tuning significantly improves LLM performance across all mental health tasks simultaneously
- Mental-Alpaca outperforms GPT-3.5 by 16.7% on balanced accuracy
- Mental-Alpaca performs on par with state-of-the-art task-specific models
- Contextual information about social media source and mental health domain in prompts consistently improves LLM performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction finetuning across multiple mental health datasets significantly improves LLM performance on mental health prediction tasks.
- Mechanism: By exposing the LLM to diverse mental health tasks and datasets in a single finetuning round, the model learns generalized mental health domain knowledge and task-specific instructions simultaneously, enabling it to perform well across tasks without needing task-specific retraining.
- Core assumption: Mental health concepts and linguistic patterns are transferable across different tasks and datasets, and the LLM can effectively learn from instruction-based finetuning.
- Evidence anchors: [abstract] "More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously."
- Break condition: If mental health tasks are too dissimilar for the model to learn useful cross-task patterns, or if finetuning data is too small or noisy to establish reliable mental health knowledge.

### Mechanism 2
- Claim: Providing contextual information about the social media source and mental health domain in prompts consistently improves LLM performance.
- Mechanism: The contextual prompts help the LLM better understand the nature of the input text (social media posts) and the domain expertise required (mental health), leading to more accurate predictions.
- Core assumption: LLMs can leverage additional contextual information in prompts to improve task performance, and social media context and mental health domain knowledge are relevant for mental health prediction.
- Evidence anchors: [abstract] "We propose a general zero-shot prompt template... PromptPart1-S provides specifications for a mental health recognition target."
- Break condition: If the LLM already has sufficient knowledge to understand the context from the text itself, or if the contextual information provided is irrelevant or misleading.

### Mechanism 3
- Claim: Few-shot prompting improves LLM performance, especially for smaller models on complex tasks.
- Mechanism: By providing a few examples of the mental health prediction task in the prompt, the LLM can quickly learn the task format and expectations, leading to better performance compared to zero-shot prompting.
- Core assumption: LLMs can effectively learn from a small number of examples in the prompt, and the examples provided are representative of the task.
- Evidence anchors: [abstract] "few-shot prompting... show promising but limited performance compared to task-specific models."
- Break condition: If the examples provided are not representative of the task or are misleading, or if the LLM is too large and already has sufficient knowledge to perform the task without examples.

## Foundational Learning

- Concept: Mental health prediction tasks (binary and multi-class classification) using online text data
  - Why needed here: Understanding the nature of the tasks and the data is crucial for designing effective prompts and finetuning strategies.
  - Quick check question: What are the main categories of mental health prediction tasks evaluated in this study, and what type of data is used?

- Concept: Large language models (LLMs) and their capabilities
  - Why needed here: Knowing the strengths and limitations of LLMs is essential for selecting appropriate models and designing effective experiments.
  - Quick check question: What are the key characteristics of the LLMs evaluated in this study, and how do they differ in terms of size and performance?

- Concept: Prompt engineering and instruction finetuning techniques
  - Why needed here: These techniques are the main methods used to improve LLM performance on mental health prediction tasks, so understanding them is crucial for implementing the study's findings.
  - Quick check question: What are the main prompt engineering and instruction finetuning strategies explored in this study, and how do they differ in terms of their goals and implementation?

## Architecture Onboarding

- Component map:
  Datasets (Dreaddit, DepSeverity, SDCNL, CSSRS-Suicide) -> Models (Alpaca, Alpaca-LoRA, GPT-3.5) -> Methods (Zero-shot, Few-shot, Instruction fine-tuning) -> Evaluation (Balanced accuracy)

- Critical path:
  1. Prepare mental health datasets and split into training and test sets
  2. Design prompt templates for zero-shot and few-shot prompting
  3. Implement instruction finetuning on selected LLMs using merged datasets
  4. Evaluate model performance on each task using balanced accuracy
  5. Analyze results and compare to baselines and other LLMs

- Design tradeoffs:
  - Model size vs. performance: Larger models generally perform better but are more computationally expensive
  - Prompt design vs. performance: More complex prompts may improve performance but require more careful design
  - Finetuning data size vs. performance: More data generally leads to better performance but is more expensive to collect

- Failure signatures:
  - Poor performance on specific tasks or datasets
  - Overfitting to training data
  - Biases in predictions based on demographics or language patterns

- First 3 experiments:
  1. Zero-shot prompting with basic prompt template on all LLMs and tasks
  2. Few-shot prompting with best-performing prompt template from experiment 1
  3. Instruction finetuning on Alpaca using merged datasets and evaluation on all tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on mental health tasks with time-series sensor data compared to textual data?
- Basis in paper: [inferred] The paper mentions future work on leveraging LLMs for time-series data and sensor data from mobile phones and wearables.
- Why unresolved: The paper only evaluates LLMs on textual data from social media. Time-series sensor data has different characteristics and may require different approaches.
- What evidence would resolve it: Experimental results comparing LLM performance on mental health prediction tasks using sensor data versus textual data from the same population.

### Open Question 2
- Question: What is the optimal balance between finetuning on multiple datasets versus a single large dataset for mental health tasks?
- Basis in paper: [explicit] The paper shows that finetuning on multiple datasets improves performance, but also notes that finetuning on a single dataset can be beneficial for some tasks.
- Why unresolved: The paper doesn't systematically explore the tradeoff between dataset diversity and size. It's unclear how to optimize this balance.
- What evidence would resolve it: Experiments varying both the number of datasets and the size of each dataset, measuring performance on both seen and unseen tasks.

### Open Question 3
- Question: How do different prompt design strategies (context enhancement, mental health expertise framing) interact with specific mental health conditions?
- Basis in paper: [explicit] The paper shows varied effectiveness of prompt strategies across different tasks and conditions.
- Why unresolved: The paper only tests a limited number of prompt variations and doesn't systematically analyze which strategies work best for which conditions.
- What evidence would resolve it: A comprehensive study testing multiple prompt variations across all mental health conditions, identifying which strategies are most effective for each.

## Limitations

- Evaluation limited to social media text data from Reddit and similar platforms, with unknown generalizability to clinical text or other communication modalities
- Instruction fine-tuning merges all datasets into single training round, potentially obscuring task-specific nuances
- Study does not address potential ethical concerns around privacy, misdiagnosis risks, or need for human oversight in clinical applications

## Confidence

**High Confidence**: Instruction finetuning significantly improves LLM performance across mental health tasks (well-supported by experimental results and consistent improvements over baselines).

**Medium Confidence**: Mental-Alpaca performs "on par with state-of-the-art task-specific models" (based on comparisons to previously published results, but exact baselines not fully specified).

**Low Confidence**: Generalizability to non-social media text domains and clinical applications (not established; study focuses exclusively on social media data).

## Next Checks

1. **Cross-domain validation**: Test the finetuned Mental-Alpaca model on clinical text data from electronic health records or therapy transcripts to assess generalizability beyond social media text.

2. **Bias and fairness audit**: Conduct systematic analysis of model predictions across demographic variables (age, gender, race/ethnicity) to identify potential biases and ensure equitable performance across populations.

3. **Real-world deployment study**: Implement a controlled pilot deployment of the model in a clinical setting with human oversight to evaluate practical performance, user trust, and clinical utility compared to existing screening methods.