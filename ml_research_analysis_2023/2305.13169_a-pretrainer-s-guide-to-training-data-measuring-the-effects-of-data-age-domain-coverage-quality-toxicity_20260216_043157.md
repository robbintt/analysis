---
ver: rpa2
title: 'A Pretrainer''s Guide to Training Data: Measuring the Effects of Data Age,
  Domain Coverage, Quality, & Toxicity'
arxiv_id: '2305.13169'
source_url: https://arxiv.org/abs/2305.13169
tags:
- data
- pretraining
- toxicity
- quality
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study quantifies the effects of pretraining data curation
  choices on language model performance. Using 28 1.5B parameter models, it measures
  how dataset age, toxicity/quality filtering, and domain composition impact model
  behavior.
---

# A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity

## Quick Facts
- arXiv ID: 2305.13169
- Source URL: https://arxiv.org/abs/2305.13169
- Reference count: 40
- Key outcome: This study quantifies the effects of pretraining data curation choices on language model performance using 28 1.5B parameter models, showing that temporal misalignment causes persistent performance degradation, quality filtering improves results despite data removal, and heterogeneous web/books data provides the strongest benefits.

## Executive Summary
This empirical study systematically measures how pretraining data curation decisions affect language model performance across 28 1.5B parameter models. The research investigates three key dimensions: temporal alignment between pretraining and evaluation data, the impact of quality and toxicity filtering on model outputs, and the contribution of different domain sources to downstream task performance. Results demonstrate that temporal misalignment creates persistent performance degradation that cannot be overcome through finetuning, quality filtering improves performance despite removing data, and including heterogeneous sources like web and books provides the strongest benefits. The study also reveals important trade-offs, such as toxicity filtering reducing toxic generation at the cost of worse generalization and toxicity identification ability.

## Method Summary
The study pretrains 28 1.5B parameter decoder-only language models using the C4 and Pile datasets, systematically varying pretraining data through controlled interventions including temporal misalignment, quality/toxicity filtering, and domain ablation. Models are evaluated on downstream tasks including MRQA and UnifiedQA for question answering, along with specialized temporal and toxicity identification benchmarks. The research employs both quantitative performance metrics and observational corpus analysis to understand how different data curation choices affect model behavior. Quality and toxicity filtering use classifier thresholds to remove data, while domain composition experiments ablate specific sources from the Pile dataset.

## Key Results
- Temporal misalignment between pretraining and evaluation data causes performance degradation that persists through finetuning, with stronger effects in larger models
- Quality filtering improves downstream performance across nearly all tasks despite removing 10%+ of training data
- Including heterogeneous data sources (Books and Web) provides the strongest positive effects on downstream performance compared to more focused domain sources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal misalignment between pretraining and evaluation data degrades model performance, and this degradation is not overcome by finetuning.
- Mechanism: Models implicitly encode temporal information during pretraining, and mismatched temporal distributions between pretraining and evaluation data create distributional shifts that affect downstream task performance.
- Core assumption: The pretraining corpus contains measurable temporal signals that influence model representations in a way that persists through finetuning.
- Evidence anchors:
  - [abstract] "A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning."
  - [section 4] "Temporal misalignment during pretraining persists even with temporally-relevant finetuning data."
  - [corpus] Weak evidence - the corpus analysis shows date mentions but doesn't directly prove temporal encoding in representations.
- Break condition: If finetuning data is sufficiently large and temporally aligned with evaluation data, it could potentially overcome pretraining misalignment.

### Mechanism 2
- Claim: Quality filtering improves downstream performance despite reducing training data quantity.
- Mechanism: Quality filtering removes low-quality text that introduces noise or distributional shifts during pretraining, leading to better generalization on downstream tasks.
- Core assumption: The quality classifier effectively identifies and removes text that would negatively impact model learning.
- Evidence anchors:
  - [abstract] "Quality filtering improves downstream performance across tasks we tested, despite removing data."
  - [section 5] "Quality filters significantly improve performance across nearly all tasks, despite removing 10%+ of the training data."
  - [corpus] Weak evidence - the corpus analysis shows quality filtering removes low-quality text but doesn't prove downstream benefits.
- Break condition: If the quality classifier is poorly aligned with the characteristics that matter for downstream tasks, filtering could harm performance.

### Mechanism 3
- Claim: Including heterogeneous data sources like books and web data provides the strongest positive effects on downstream performance.
- Mechanism: Heterogeneous data sources provide diverse linguistic patterns, topical coverage, and writing styles that improve model generalization across tasks.
- Core assumption: Diversity in pretraining data sources translates to better generalization on diverse downstream tasks.
- Evidence anchors:
  - [abstract] "The best performing domains comprise high-quality (Books) and heterogeneous (Web) data."
  - [section 6] "The best performing domains comprise high-quality (Books) and heterogeneous (Web) data, corroborating Brown et al. (2020); Chowdhery et al. (2022); Xie et al. (2023a)."
  - [corpus] Strong evidence - the corpus analysis shows Books and Web have the most diverse and high-quality text characteristics.
- Break condition: If downstream tasks are highly specialized and benefit more from focused domain data than general heterogeneity.

## Foundational Learning

- Concept: Distributional shift and its impact on model performance
  - Why needed here: The paper's core findings revolve around how changes in pretraining data distribution affect downstream performance
  - Quick check question: If you train on data from 2019 and evaluate on 2022 data, what kind of distributional shift are you encountering?

- Concept: Trade-offs in data curation decisions
  - Why needed here: The paper demonstrates that different filtering strategies involve balancing multiple objectives (toxicity vs performance, quality vs quantity)
  - Quick check question: If toxicity filtering reduces toxic generation but also reduces toxicity identification ability, what does this tell you about the relationship between these two objectives?

- Concept: Domain adaptation and transfer learning
  - Why needed here: The paper shows how pretraining data domain composition affects performance on downstream tasks from different domains
  - Quick check question: If removing a specific domain from pretraining hurts performance on related downstream tasks, what does this suggest about domain transfer?

## Architecture Onboarding

- Component map: Data curation pipelines (age filtering, quality/toxicity filtering, domain composition) → Pretraining infrastructure (1.5B parameter decoder-only models) → Finetuning modules for various downstream tasks → Evaluation frameworks for toxicity, QA, and temporal tasks
- Critical path: Data curation → Pretraining → Finetuning → Evaluation → Analysis. Each stage must complete successfully for the next to proceed.
- Design tradeoffs: Large-scale pretraining vs computational cost, comprehensive evaluation vs time constraints, controlled experiments vs real-world applicability
- Failure signatures: Degraded downstream performance, unexpected correlations between filtering choices and outcomes, computational resource exhaustion, evaluation metric inconsistencies
- First 3 experiments:
  1. Replicate the temporal misalignment experiment with C4 data from different years to verify the core finding
  2. Test quality filtering at different thresholds on a small subset of data to observe the relationship between data removal and performance
  3. Ablate one domain from the Pile (e.g., remove Books) and evaluate on QA tasks to confirm domain composition effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does temporal misalignment between pretraining and evaluation datasets affect larger language models differently than smaller models?
- Basis in paper: [explicit] The paper states "Temporal Degradation is greater for larger models" and shows that temporal degradation effects are not significant for LM-Small models but are for LM-XL models.
- Why unresolved: The paper only tests two model sizes (20M and 1.5B parameters), so the relationship between model size and temporal sensitivity is not fully characterized.
- What evidence would resolve it: Experiments testing a broader range of model sizes would establish the scaling relationship between model size and temporal sensitivity.

### Open Question 2
- Question: What is the optimal quality filtering threshold that balances performance improvement with minimal data loss?
- Basis in paper: [explicit] The paper shows quality filtering improves performance but doesn't identify an optimal threshold, stating "While the average performance peaks at T = 0.975 for the QA tasks, greater quality filtering still outperforms the unﬁltered baseline on average."
- Why unresolved: The paper only tests discrete threshold values (0.975, 0.95, 0.9, 0.7) without exploring intermediate values or the full tradeoff curve.
- What evidence would resolve it: A comprehensive sweep of quality filtering thresholds with fine-grained performance measurements would identify the optimal balance point.

### Open Question 3
- Question: How do toxicity and quality filters affect model performance on underrepresented or minority community data?
- Basis in paper: [inferred] The paper mentions that "Pretraining dataset analysis" work has found models perform worse on underrepresented communities after filtering, but doesn't test this directly.
- Why unresolved: The evaluation datasets used (MRQA, UnifiedQA, etc.) are not specifically designed to measure performance disparities across demographic groups.
- What evidence would resolve it: Evaluation on benchmark datasets designed to measure demographic performance gaps would reveal whether filtering exacerbates or mitigates these disparities.

### Open Question 4
- Question: What is the relative contribution of dataset diversity versus dataset quality to model performance?
- Basis in paper: [explicit] The paper states "Data source heterogeneity is often more beneficial than targeted data" and finds that removing heterogeneous web domains hurts performance more than removing targeted academic domains.
- Why unresolved: The experiments ablate domains but don't directly compare the effects of adding more diverse vs. higher quality data to an existing corpus.
- What evidence would resolve it: Controlled experiments that vary diversity and quality independently would establish their relative contributions to performance.

### Open Question 5
- Question: How does the time distribution within pretraining datasets affect model performance on temporally misaligned evaluation data?
- Basis in paper: [explicit] The paper notes that C4 versions contain data from multiple years and that "we estimate the temporal information in the pretraining data by counting instances of dates."
- Why unresolved: The experiments only consider pretraining data collected in single years, not datasets with mixed temporal distributions.
- What evidence would resolve it: Experiments training on datasets with controlled temporal distributions (e.g., uniform distribution vs. recent-heavy distribution) would reveal how temporal spread affects temporal generalization.

## Limitations
- Findings are based on controlled experiments with 1.5B parameter models that may not generalize to larger models or different architectures
- The study uses specific datasets (C4 and Pile) and evaluation tasks that may not represent the full diversity of real-world applications
- Quality and toxicity classifiers are treated as black boxes with potential implementation biases not fully explored

## Confidence
- High Confidence: The core finding that heterogeneous data sources (Books and Web) provide the strongest positive effects on downstream performance
- Medium Confidence: The quality filtering results showing improved downstream performance despite data removal
- Low Confidence: The temporal misalignment findings, particularly the claim that finetuning cannot overcome pretraining misalignment

## Next Checks
1. Replicate with larger models: Test the temporal misalignment and quality filtering effects on 8B or 70B parameter models to verify if the findings scale with model size.
2. Controlled temporal intervention: Design a controlled experiment where training data is explicitly split by year and evaluate performance on held-out future data to isolate the temporal effects.
3. Quality classifier ablation study: Test multiple quality classifiers with different criteria to understand how classifier choice affects downstream performance and identify potential failure modes.