---
ver: rpa2
title: Do VSR Models Generalize Beyond LRS3?
arxiv_id: '2311.14063'
source_url: https://arxiv.org/abs/2311.14063
tags:
- test
- lrs3
- wildvsr
- performance
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Visual Speech Recognition (VSR) models suffer from overfitting
  to the LRS3 test set, which is only 1 hour long. To address this, a new VSR test
  set called WildVSR is created by closely following the LRS3 dataset creation process.
---

# Do VSR Models Generalize Beyond LRS3?

## Quick Facts
- arXiv ID: 2311.14063
- Source URL: https://arxiv.org/abs/2311.14063
- Reference count: 40
- Key outcome: Visual Speech Recognition models fail on a new test set (WildVSR) despite high performance on LRS3.

## Executive Summary
This paper addresses the overfitting problem of Visual Speech Recognition (VSR) models to the limited LRS3 test set by creating a new test set called WildVSR. WildVSR is designed to closely follow the LRS3 dataset creation process but includes more diverse speakers and longer video durations. The authors evaluate several state-of-the-art VSR models on both LRS3 and WildVSR, observing significant drops in Word Error Rate (WER) on the new test set. The study highlights the need for more robust VSR models that can generalize better to real-world scenarios and proposes a new metric called Rankwer to capture model consistency across test samples.

## Method Summary
The paper creates a new VSR test set called WildVSR by closely following the LRS3 dataset creation process. WildVSR has 4.8 hours of duration, 2,854 utterances, and 1.5× more unique speakers than LRS3. The authors evaluate a broad range of VSR models on both LRS3 and WildVSR test sets, calculating WER and a new metric called Rankwer to compare performance and consistency. The study aims to assess the generalization capabilities of VSR models beyond the limited LRS3 test set.

## Key Results
- All VSR models experience significant drops in WER on WildVSR compared to their LRS3 results, with an average drop of 30 points.
- The comparative ranking of models remains consistent between LRS3 and WildVSR, suggesting that LRS3 performance is indicative of model robustness.
- Self-supervised learning (SSL) approaches require significantly higher computational resources but only achieve moderate performance gains compared to supervised models like Auto-A VSR.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Overfitting to LRS3 test set causes models to fail on WildVSR.
- Mechanism: Models trained primarily on LRS3 learn patterns specific to its limited one-hour duration and homogeneous speaker distribution, leading to poor generalization when faced with the broader visual diversity and spoken vocabulary of WildVSR.
- Core assumption: The LRS3 test set distribution is significantly narrower than real-world visual speech data.
- Evidence anchors:
  - [abstract]: "Visual Speech Recognition (VSR) models suffer from overfitting to the LRS3 test set, which is only 1 hour long."
  - [section]: "As expected, we observe that all VSR SoTA models fail to reach their reported Word Error Rate (WER) from LRS3 on the new test set."
  - [corpus]: Weak—corpus neighbors focus on different VSR aspects, no direct evidence of overfitting.
- Break condition: If models are tested on a dataset that closely follows LRS3 creation process but still show performance drops, then overfitting to LRS3 alone is not the full explanation.

### Mechanism 2
- Claim: The drop in WER is proportional to the original LRS3 performance.
- Mechanism: Better performing models on LRS3 experience smaller relative declines on WildVSR, suggesting that LRS3 performance is indicative of model robustness.
- Core assumption: Models that perform well on LRS3 have learned more generalizable features.
- Evidence anchors:
  - [section]: "Specifically, the models that exhibit the highest accuracy on the original test sets also demonstrate the highest accuracy on the new test sets."
  - [section]: "Let's denote a pair of scores for all models (werLRS3, werWildVSR), then calculate the respective means... werWildVSR ≈ 1.31 · werLRS3 + 14.05."
  - [corpus]: Weak—corpus neighbors do not discuss performance scaling between test sets.
- Break condition: If a model with high LRS3 WER shows unexpectedly small drop on WildVSR, the linear relationship breaks.

### Mechanism 3
- Claim: Self-supervised learning (SSL) is less efficient for VSR than supervised learning.
- Mechanism: SSL requires significantly higher computational resources (3.6×) but only achieves moderate performance gains compared to supervised models like Auto-A VSR.
- Core assumption: The complexity of video data and the specific nature of VSR make SSL pretraining less effective than direct supervised training.
- Evidence anchors:
  - [section]: "On the other hand, self-supervised methods (A V-HuBERT and RA Ven) demand a significantly higher compute budget ( ≈ 3.6× the compute of Auto-A VSR), while achieving only a moderate performance in terms of WER."
  - [section]: "While Wav2vec2.0 fails in matching the exact target speech, the predictions are closer and represent reasonable errors, like homophones... In comparison, the state-of-the-art VSR framework Auto-A VSR predictions deviate significantly from the target speech and often appear to be random."
  - [corpus]: Weak—corpus neighbors do not compare SSL vs. supervised learning efficiency.
- Break condition: If SSL pretraining is shown to provide significant benefits on a different task or with different data, this mechanism's assumption may not hold for VSR.

## Foundational Learning

- Concept: Generalization in machine learning
  - Why needed here: Understanding why models perform well on one test set but poorly on another requires knowledge of how models generalize beyond their training distribution.
  - Quick check question: What is the difference between interpolation and extrapolation in the context of model generalization?

- Concept: Word Error Rate (WER) metric
  - Why needed here: WER is the primary metric used to evaluate and compare VSR models; understanding its calculation and limitations is crucial.
  - Quick check question: How is WER calculated, and what are its limitations in comparing models with varying test set characteristics?

- Concept: Tucker decomposition
  - Why needed here: Tucker decomposition is used to analyze the variability of model representations across different test sets, providing insights into model behavior.
  - Quick check question: What is Tucker decomposition, and how can it be used to analyze tensor data in machine learning models?

## Architecture Onboarding

- Component map: Video frames -> ResNet-3D frontend -> Transformer encoder -> Transformer decoder -> Text prediction
- Critical path:
  1. Input video frames → ResNet-3D frontend
  2. Frontend output → Transformer encoder
  3. Encoder output → Transformer decoder (with cross-attention)
  4. Decoder output → Text prediction
- Design tradeoffs:
  - SSL vs. supervised learning: Higher compute cost for SSL with moderate performance gains
  - Model size vs. performance: Larger models may overfit to LRS3, smaller models may underfit
  - Finetuning duration: Longer finetuning may improve WildVSR performance but increase computational cost
- Failure signatures:
  - High WER on WildVSR but low WER on LRS3: Overfitting to LRS3
  - Low WER on both but high variance: Inconsistent model performance across samples
  - High WER on short videos: Lack of contextual information in short sequences
- First 3 experiments:
  1. Evaluate model performance on subsets of WildVSR with similar characteristics to LRS3 to isolate overfitting effects
  2. Test model robustness to head pose variations by creating synthetic data with extreme poses
  3. Compare SSL pretraining benefits on different amounts of labeled data to optimize compute-performance tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we quantify the exact contribution of each component (adaptivity gap, distribution gap, generalization gap) to the performance drop on WildVSR?
- Basis in paper: [explicit] Section 5 discusses decomposing the error difference into three components: adaptivity gap, distribution gap, and generalization gap.
- Why unresolved: These components are hard to track and distinguish in practice.
- What evidence would resolve it: Developing a methodology to isolate and measure each gap component's contribution to the overall performance drop.

### Open Question 2
- Question: What specific characteristics of the "hard samples" in WildVSR are causing the significant performance drop across all models?
- Basis in paper: [explicit] Section 5 mentions analyzing "hard samples" but does not provide detailed insights into their specific characteristics.
- Why unresolved: While some characteristics like head poses and video length are mentioned, the exact features causing the drop are not fully explored.
- What evidence would resolve it: Detailed analysis of hard samples to identify common features (e.g., vocabulary, speaking style, acoustic conditions) that correlate with performance drops.

### Open Question 3
- Question: How can we design a more effective self-supervised learning approach for VSR that balances performance and computational efficiency?
- Basis in paper: [inferred] Section 6 discusses the trade-off between performance and compute for self-supervised models (A V-HuBERT and RA Ven) compared to fully-supervised models (Auto-A VSR).
- Why unresolved: Current self-supervised approaches require significantly higher compute budgets for only moderate performance gains.
- What evidence would resolve it: Developing and testing new self-supervised architectures or training strategies that achieve comparable performance to fully-supervised models with lower computational costs.

## Limitations
- WildVSR still follows LRS3 dataset creation process, potentially introducing some bias.
- Evaluation focuses solely on WER and Rankwer metrics, overlooking other aspects of model performance.
- The exact mechanisms behind overfitting to LRS3 remain unclear.

## Confidence
- High confidence: The observed WER drops across all evaluated models on WildVSR compared to LRS3.
- Medium confidence: The claim that better performing models on LRS3 experience smaller relative declines on WildVSR, indicating some level of robustness.
- Low confidence: The exact mechanisms behind the overfitting to LRS3 and the full extent to which the WildVSR dataset captures real-world visual speech variability.

## Next Checks
1. Evaluate model performance on additional test sets with different creation processes to isolate the effects of dataset-specific biases.
2. Conduct ablation studies by training models on subsets of LRS3 and testing on corresponding subsets of WildVSR to better understand the overfitting phenomenon.
3. Investigate the impact of fine-tuning duration and data augmentation techniques on model generalization to WildVSR.