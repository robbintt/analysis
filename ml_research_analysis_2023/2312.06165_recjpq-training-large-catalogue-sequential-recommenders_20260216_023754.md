---
ver: rpa2
title: 'RecJPQ: Training Large-Catalogue Sequential Recommenders'
arxiv_id: '2312.06165'
source_url: https://arxiv.org/abs/2312.06165
tags:
- embeddings
- item
- recjpq
- items
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the memory bottleneck of large embedding tensors
  in sequential recommendation models. It introduces RecJPQ, a novel adaptation of
  Joint Product Quantisation (JPQ) for training large-scale sequential recommenders.
---

# RecJPQ: Training Large-Catalogue Sequential Recommenders

## Quick Facts
- arXiv ID: 2312.06165
- Source URL: https://arxiv.org/abs/2312.06165
- Reference count: 40
- Key outcome: RecJPQ achieves up to 47.94x model size reduction on Gowalla dataset with no effectiveness degradation and sometimes improves performance through regularisation

## Executive Summary
This paper introduces RecJPQ, a novel adaptation of Joint Product Quantisation (JPQ) for training large-scale sequential recommender systems. The method addresses the memory bottleneck of large embedding tensors by replacing item embeddings with a concatenation of shared sub-embeddings (centroids), drastically reducing trainable parameters. Experiments on MovieLens-1M, Booking.com, and Gowalla datasets show RecJPQ maintains or improves recommendation quality while achieving significant compression (up to 48√ó reduction). The approach is particularly effective for long-tail items, acting as a regularisation mechanism that prevents overfitting on rare items.

## Method Summary
RecJPQ modifies Joint Product Quantisation for end-to-end training of sequential recommenders. Instead of learning a full embedding matrix for all items, each item is assigned a code of length m where each code element indexes into a small set of shared centroid embeddings. During training, item embeddings are reconstructed as the concatenation of these centroids, and only the centroids are updated via backpropagation. The method supports three centroid assignment strategies: random, discrete truncated SVD, and discrete BPR. This approach reduces the number of trainable parameters from |I|¬∑d to b¬∑d (where b is the number of centroids), enabling training on large catalogues that would otherwise exceed memory limits.

## Key Results
- RecJPQ achieves 47.94√ó model size reduction on Gowalla dataset with no NDCG@10 degradation
- On Booking.com, RecJPQ improves NDCG@10 by +0.96% compared to baseline models
- For datasets with long-tail items (75.8% on Gowalla), RecJPQ with random assignments achieves +57% NDCG@10 improvement over base models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RecJPQ enables training of sequential recommenders on large catalogues by replacing the full item embedding tensor with a shared centroid-based representation, drastically reducing trainable parameters.
- Mechanism: Each item is assigned a code of length ùëö, where each code element indexes into a small set of shared centroid embeddings. The full item embedding is reconstructed as the concatenation of these centroids during forward pass. Since only centroid embeddings are learned, the number of trainable parameters drops from |ùêº|¬∑ùëë to ùëè¬∑ùëë (where ùëè is the number of centroids).
- Core assumption: The reconstructed embeddings from centroid concatenation can approximate the performance of full learned embeddings for ranking tasks.
- Evidence anchors:
  - [abstract] states that RecJPQ "replaces item embeddings with a concatenation of a limited number of shared sub-embeddings" and achieves "48√ó reduction for the Gowalla dataset with no effectiveness degradation."
  - [section 4.1] describes the codebook-based reconstruction process with example parameters (ùëö=3, ùëë=12, ùëè=256).
  - [corpus] shows no directly comparable prior work, but related papers like "Efficient Inference of Sub-Item Id-based Sequential Recommendation Models" suggest the same general direction of parameter sharing.
- Break condition: If the codebook cannot represent diverse item characteristics due to insufficient centroid diversity or poor centroid assignment, model performance will degrade.

### Mechanism 2
- Claim: RecJPQ acts as a regularisation mechanism that improves generalisation on long-tail items by forcing parameter sharing across items.
- Mechanism: By constraining items to share sub-embedding components, RecJPQ prevents the model from learning overly specific embeddings for rare items. This reduces overfitting on items with few interactions.
- Core assumption: Long-tail items benefit from shared representation structure, and the shared centroids can capture commonalities across similar items.
- Evidence anchors:
  - [section 4.2] explicitly states "RecJPQ forces different items to share parts of their embeddings" and "prevents the model from learning item embeddings that are too specific to only a few training sequences."
  - [section 5.2.2] shows that on Gowalla (75.8% long-tail items), RecJPQ with random assignments achieves +57% NDCG@10 improvement over base, attributed to stronger regularisation.
  - [corpus] shows no direct evidence, but the concept aligns with "Balancing Accuracy and Novelty with Sub-Item Popularity" suggesting shared representations help long-tail performance.
- Break condition: If the dataset has few long-tail items (e.g., MovieLens-1M with 0% long-tail), the regularisation effect may be neutral or slightly harmful.

### Mechanism 3
- Claim: RecJPQ enables end-to-end training without requiring separate embedding compression steps, avoiding memory bottlenecks during model training.
- Mechanism: By assigning codes to items before training and making centroids learnable parameters, RecJPQ eliminates the need to store and update a full |ùêº|¬∑ùëë embedding tensor during training. The only memory required is for the smaller centroid tensor.
- Core assumption: Gradient flow through the centroid parameters is sufficient to learn effective item representations without the full embedding matrix.
- Evidence anchors:
  - [section 3.2] explains that "JPQ generates item codes before training the model" and "the model can be trained end-to-end without training full item embeddings."
  - [section 5.2.1] states "RecJPQ only increased the model size for SASRec on MovieLens-1M due to the dataset's small item count" implying overhead is negligible for large datasets.
  - [corpus] shows no direct evidence, but aligns with "Efficient Inference of Sub-Item Id-based Sequential Recommendation Models" which also targets memory efficiency.
- Break condition: If the number of centroids ùëè is too small relative to item diversity, the model cannot learn effective representations despite end-to-end training.

## Foundational Learning

- Concept: Product Quantisation (PQ) and its adaptation to learning scenarios
  - Why needed here: RecJPQ builds directly on PQ concepts but modifies them for end-to-end learning rather than post-training compression
  - Quick check question: What is the key difference between standard PQ and RecJPQ's approach to centroid learning?

- Concept: Codebook-based vector reconstruction
  - Why needed here: Understanding how item embeddings are reconstructed from centroid indices is fundamental to grasping RecJPQ's memory efficiency
  - Quick check question: How does the reconstruction process work when an item has code [25, 7, 2] and what dimensions does each centroid contribute?

- Concept: Matrix factorisation for centroid assignment
  - Why needed here: RecJPQ uses discrete truncated SVD and BPR to create meaningful initial code assignments that group similar items
  - Quick check question: Why does adding small Gaussian noise after min-max normalisation help ensure unique embeddings after truncated SVD?

## Architecture Onboarding

- Component map: Item IDs ‚Üí Codebook lookup ‚Üí Centroid concatenation ‚Üí Model input; Centroids are learnable parameters updated via backprop
- Critical path: Code assignment ‚Üí Centroid initialisation ‚Üí Model training with reconstructed embeddings
- Design tradeoffs: Larger code length ùëö provides more representation capacity but less parameter sharing (weaker regularisation); smaller ùëö increases regularisation but may limit expressiveness
- Failure signatures: Performance degradation when similar items receive dissimilar codes; training instability if centroids collapse to similar values
- First 3 experiments:
  1. Verify code assignment strategy produces distinct codes for all items using a small synthetic dataset
  2. Test centroid reconstruction accuracy by comparing reconstructed embeddings to original item embeddings on a validation set
  3. Benchmark memory usage during training with different code lengths on a medium-sized dataset to confirm the memory efficiency claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of centroid assignment strategy (random, discrete truncated SVD, discrete BPR) affect the trade-off between model compression and recommendation quality across different types of datasets and recommendation scenarios?
- Basis in paper: [explicit] The paper experiments with three centroid assignment strategies and observes that the optimal choice depends on the model and dataset characteristics, with SVD generally being a safe choice.
- Why unresolved: The paper shows that the optimal strategy varies by dataset and model, but does not provide a comprehensive framework for selecting the best strategy without exhaustive hyperparameter search. The underlying reasons for why certain strategies work better in specific scenarios are not fully explored.
- What evidence would resolve it: Systematic experiments across diverse datasets (varying in size, sparsity, and item popularity distributions) and recommendation models, combined with analysis of the learned centroid embeddings to understand how different strategies capture item relationships.

### Open Question 2
- Question: Can RecJPQ be effectively combined with other model compression techniques, such as quantization or knowledge distillation, to achieve even greater model size reduction without compromising recommendation quality?
- Basis in paper: [inferred] The paper focuses on RecJPQ as a standalone compression technique but does not explore its potential synergy with other methods like post-training quantization or model pruning.
- Why unresolved: The paper demonstrates significant compression using RecJPQ alone, but does not investigate whether combining it with other compression techniques could yield additional benefits or if there are trade-offs to consider.
- What evidence would resolve it: Experiments comparing the combined use of RecJPQ with other compression techniques (e.g., quantization of centroid embeddings, knowledge distillation) on the same datasets, measuring both model size and recommendation quality.

### Open Question 3
- Question: How does RecJPQ affect the model's ability to generalize to new items or adapt to changing item popularity patterns in dynamic recommendation environments?
- Basis in paper: [inferred] The paper mentions that RecJPQ acts as a regularizer, particularly beneficial for datasets with many long-tail items, but does not explore its impact on the model's adaptability to new or evolving item catalogs.
- Why unresolved: While the paper shows RecJPQ's effectiveness on static datasets, real-world recommender systems often need to handle new items and changing user preferences. The paper does not address how RecJPQ performs in these dynamic scenarios or how it might be adapted for online learning.
- What evidence would resolve it: Experiments evaluating RecJPQ's performance on streaming data or datasets with temporal splits, comparing its ability to recommend new items or adapt to popularity shifts against baseline models.

## Limitations
- Evaluation focuses primarily on ranking metrics (NDCG@10) without investigating downstream effects like diversity or fairness
- SVD and BPR assignment strategies are described at a high level without full algorithmic specifications, making exact reproduction challenging
- Memory savings claims rely on checkpoint size comparisons rather than detailed GPU memory profiling during training

## Confidence

**High Confidence**: The core mechanism of replacing item embeddings with concatenated centroids works as described, supported by consistent experimental results across three datasets and multiple model architectures.

**Medium Confidence**: The regularisation effect on long-tail items is supported by Gowalla results but lacks ablation studies to isolate the specific contribution of parameter sharing versus other factors.

**Low Confidence**: The claim that RecJPQ enables training on datasets "too large for memory" is based on checkpoint size reduction rather than actual training feasibility tests with constrained hardware.

## Next Checks
1. Measure actual GPU memory usage during training (not just checkpoint size) to verify RecJPQ enables training on hardware-limited scenarios
2. Conduct ablation studies isolating the regularisation effect by comparing RecJPQ with random codes against models with explicit regularisation terms
3. Test the SVD and BPR assignment strategies on synthetic datasets where the ground truth item similarities are known to verify they produce meaningful code assignments