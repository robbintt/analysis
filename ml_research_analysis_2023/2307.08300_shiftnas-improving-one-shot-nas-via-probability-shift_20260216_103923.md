---
ver: rpa2
title: 'ShiftNAS: Improving One-shot NAS via Probability Shift'
arxiv_id: '2307.08300'
source_url: https://arxiv.org/abs/2307.08300
tags:
- training
- subnets
- search
- supernet
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the performance gap in one-shot neural architecture
  search (NAS) due to uniform sampling, which concentrates training on subnets with
  intermediate computational resources. The authors propose ShiftNAS, which dynamically
  adjusts sampling probability based on subnet complexity to improve training sufficiency.
---

# ShiftNAS: Improving One-shot NAS via Probability Shift

## Quick Facts
- **arXiv ID:** 2307.08300
- **Source URL:** https://arxiv.org/abs/2307.08300
- **Reference count:** 40
- **Primary result:** ShiftNAS achieves 76.0% top-1 accuracy on ImageNet with 1.3 GFLOPs, outperforming AutoFormer-tiny (74.7%) and FocusFormer-T (75.1%) under similar constraints.

## Executive Summary
ShiftNAS addresses the performance gap in one-shot NAS caused by uniform sampling, which leads to insufficient training of subnets with extreme computational resources. The method dynamically adjusts sampling probability based on subnet complexity using a probability shift mechanism and an LSTM-based architecture generator. This approach improves training sufficiency across the entire computational resource spectrum, achieving state-of-the-art or competitive results on ImageNet for both CNN and ViT models without requiring retraining.

## Method Summary
ShiftNAS introduces a probability shift mechanism that dynamically adjusts the sampling probability of subnets based on their training sufficiency, as measured by gradient analysis. An LSTM-based architecture generator (AG) is trained end-to-end with the supernet using differentiable architecture search (DNAS) to efficiently generate subnets with desired computational constraints. The method eliminates the need for retraining by inheriting weights from a well-trained supernet, making it model-agnostic and computationally efficient. The approach is evaluated on ImageNet across CNN and ViT models, demonstrating significant improvements over existing one-shot NAS methods.

## Key Results
- ShiftFormer-T achieves 76.0% top-1 accuracy with 1.3 GFLOPs, outperforming AutoFormer-tiny (74.7%) and FocusFormer-T (75.1%)
- ShiftNAS consistently improves ranking correlation between inherited and finetuned subnet performance
- The method is model-agnostic and eliminates the need for retraining by inheriting weights from a well-trained supernet

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Uniform sampling in one-shot NAS leads to insufficient training of subnets with extreme computational resources.
- Mechanism: Uniform sampling concentrates training resources on subnets with intermediate computational resources, which are sampled with high probability. This leaves subnets with extreme (very low or very high) computational resources under-trained, resulting in a performance gap when these subnets are deployed.
- Core assumption: Subnets with different computational complexities require different optimal training strategies for best performance.
- Evidence anchors:
  - [abstract] "Uniform sampling concentrates training resources on subnets with intermediate computational resources, which are sampled with high probability. However, subnets with different complexity regions require different optimal training strategies for optimal performance."
  - [section 3.1] "However, in practice, subnets with different computational resources may require different training strategies, and the uniform sampling strategy used in previous one-shot NAS methods may lead to insufficient training of subnets in certain computational regions."
  - [corpus] No direct evidence in the corpus for this specific claim.

### Mechanism 2
- Claim: The probability shift mechanism dynamically adjusts the sampling probability based on the training sufficiency of subnets.
- Mechanism: By evaluating the performance variation of subnets under different computational constraints, subnets that show high-performance variance (indicating under-training) have their sampling probabilities increased. This dynamically shifts training resources towards these under-trained subnets, improving their performance.
- Core assumption: The gradient of the validation loss with respect to the subnet architecture can be used as a proxy for training sufficiency. A large gradient indicates that the subnet is not yet converged and needs more training.
- Evidence anchors:
  - [abstract] "To address the problem of uniform sampling, we propose ShiftNAS, a method that can adjust the sampling probability based on the complexity of subnets."
  - [section 3.2] "For any subnet, we judge whether the subnet has converged by calculating the following gradient of the subnet. When the subnet with b computational resource converges, âˆ‡w will tend to zero."
  - [corpus] No direct evidence in the corpus for this specific claim.

### Mechanism 3
- Claim: The LSTM-based architecture generator (AG) efficiently generates subnets with desired computational constraints.
- Mechanism: The AG is trained end-to-end with the supernet using a differentiable architecture search (DNAS) approach. It takes a desired computational constraint as input and generates a subnet architecture that satisfies this constraint. The AG uses Gumbel Softmax to generate one-hot policies for each operation, which are then mapped to differentiable masks to obtain the final subnet.
- Core assumption: The AG can learn to generate subnet architectures that not only satisfy the computational constraint but also have good performance when trained.
- Evidence anchors:
  - [abstract] "We achieve this by evaluating the performance variation of subnets with different complexity and designing an architecture generator that can accurately and efficiently provide subnets with the desired complexity."
  - [section 3.3] "To address this issue, we propose an architecture generator (AG) that can provide the corresponding subnet architecture according to any resource constraint."
  - [corpus] No direct evidence in the corpus for this specific claim.

## Foundational Learning

- Concept: Neural Architecture Search (NAS)
  - Why needed here: ShiftNAS is a method for improving one-shot NAS, so understanding the basics of NAS is crucial.
  - Quick check question: What is the main goal of Neural Architecture Search (NAS) in the context of deep learning?

- Concept: Weight Sharing in One-shot NAS
  - Why needed here: ShiftNAS is a one-shot NAS method that relies on weight sharing, so understanding how weight sharing works is important.
  - Quick check question: How does weight sharing in one-shot NAS reduce the computational cost compared to traditional NAS methods?

- Concept: Differentiable Architecture Search (DNAS)
  - Why needed here: ShiftNAS uses DNAS to train the architecture generator, so understanding the principles of DNAS is necessary.
  - Quick check question: What is the key difference between Differentiable Architecture Search (DNAS) and other NAS methods like reinforcement learning-based or evolutionary-based methods?

## Architecture Onboarding

- Component map:
  - Supernet -> Architecture Generator (AG) -> Probability Shift Mechanism -> Gumbel Softmax -> Matrix Mapping

- Critical path:
  1. Train the supernet with uniform sampling for a few epochs.
  2. Initialize the sampling distribution vector and the architecture generator.
  3. For each training iteration:
     a. Sample a computational resource constraint from the sampling distribution.
     b. Use the architecture generator to generate a subnet architecture that satisfies the constraint.
     c. Train the supernet with the generated subnet.
     d. Update the sampling distribution based on the gradients of the validation loss.
     e. Update the architecture generator jointly with the supernet.
  4. After training, the supernet and architecture generator can be used to directly obtain optimal subnets for any given computational constraint.

- Design tradeoffs:
  - The choice of the number of groups to discretize the computational resource range affects the granularity of the search and the computational cost.
  - The frequency of updating the sampling distribution vector balances the adaptation to changing training dynamics and the computational overhead.
  - The use of Gumbel Softmax introduces some randomness in the architecture generation process, which may affect the stability of the search.

- Failure signatures:
  - If the sampling distribution does not shift towards the subnets that need more training, the probability shift mechanism may not be effective.
  - If the architecture generator fails to generate subnets that satisfy the desired computational constraints consistently, the overall performance may suffer.
  - If the supernet is not well-trained due to insufficient epochs or inappropriate hyperparameters, the performance of the subnets inherited from the supernet may be suboptimal.

- First 3 experiments:
  1. Implement the supernet training with uniform sampling and observe the distribution of computational resources of the sampled subnets.
  2. Implement the probability shift mechanism and verify that the sampling distribution shifts towards the subnets with high-performance variance.
  3. Implement the architecture generator and test its ability to generate subnets that satisfy given computational constraints.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ShiftNAS models change when trained with different computational resource split steps (e.g., 0.05 GFLOPs vs 0.2 GFLOPs)?
- Basis in paper: [explicit] The paper mentions that the search space is split from 1.3 GFLOPs to 1.9 GFLOPs with 0.2, 0.1, and 0.05 GFLOPs steps in the ablation study.
- Why unresolved: The paper only provides results for 0.1 step, leaving the performance impact of other split steps unexplored.
- What evidence would resolve it: Conducting experiments with different split steps and comparing the resulting model performance would provide clarity.

### Open Question 2
- Question: What is the impact of the update frequency of the sampling distribution vector on the final model performance?
- Basis in paper: [explicit] The paper conducts an experiment with different update frequencies (50, 100, 500, and 1000 iterations) and observes a decrease in performance with lower update frequencies.
- Why unresolved: The paper does not provide a detailed analysis of the relationship between update frequency and model performance, nor does it suggest an optimal update frequency.
- What evidence would resolve it: A more comprehensive study varying the update frequency and analyzing its effect on model performance would help determine the optimal update frequency.

### Open Question 3
- Question: How does the proposed ShiftNAS method compare to other state-of-the-art methods in terms of computational efficiency and model accuracy on various tasks beyond image classification?
- Basis in paper: [inferred] The paper focuses on image classification tasks on ImageNet and provides comparisons with other methods on CNNs and ViTs. However, it does not explore the method's effectiveness on other tasks like object detection or semantic segmentation.
- Why unresolved: The paper's scope is limited to image classification, and it does not provide evidence of the method's generalizability to other tasks.
- What evidence would resolve it: Evaluating ShiftNAS on different computer vision tasks and comparing its performance with other state-of-the-art methods would provide insights into its generalizability and effectiveness across various applications.

## Limitations

- The core mechanism relies on the assumption that subnet training sufficiency can be accurately measured through gradient-based analysis, which requires further empirical validation.
- While the paper demonstrates strong results on ImageNet, the generalization to other datasets and tasks remains unverified.
- The computational overhead of the probability shift mechanism, particularly during the gradient calculation and sampling distribution update phases, could be significant for very large search spaces.

## Confidence

- **High Confidence**: The overall framework design and the experimental results on ImageNet, particularly the improvements over AutoFormer and FocusFormer under similar computational constraints.
- **Medium Confidence**: The effectiveness of the probability shift mechanism in addressing the uniform sampling issue, as the theoretical justification is sound but the practical implementation details could affect performance.
- **Low Confidence**: The generalizability of the method to other datasets and tasks beyond ImageNet, and the computational efficiency of the method for very large search spaces.

## Next Checks

1. **Gradient Signal Stability**: Conduct experiments to verify the stability and reliability of the gradient-based training sufficiency measure across different training stages and architectures.
2. **Generalization Study**: Evaluate the performance of ShiftNAS on a diverse set of datasets and tasks, including object detection, semantic segmentation, and natural language processing, to assess its generalizability.
3. **Computational Efficiency Analysis**: Perform a detailed analysis of the computational overhead introduced by the probability shift mechanism, particularly for large-scale search spaces, and explore optimization strategies to reduce the overhead.