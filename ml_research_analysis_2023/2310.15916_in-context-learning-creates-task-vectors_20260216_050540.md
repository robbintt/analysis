---
ver: rpa2
title: In-Context Learning Creates Task Vectors
arxiv_id: '2310.15916'
source_url: https://arxiv.org/abs/2310.15916
tags:
- task
- tasks
- letter
- simple
- vector
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates the mechanism behind in-context learning\
  \ (ICL) in large language models (LLMs) by proposing a hypothesis-class view of\
  \ ICL. The authors argue that ICL can be broken down into two components: a \"learning\
  \ algorithm\" (A) that maps a set of demonstrations (S) into a task vector (\u03B8\
  ), and a \"rule application\" (f) that applies the rule defined by \u03B8 to a query\
  \ (x)."
---

# In-Context Learning Creates Task Vectors

## Quick Facts
- arXiv ID: 2310.15916
- Source URL: https://arxiv.org/abs/2310.15916
- Reference count: 5
- This work proposes a hypothesis-class view of in-context learning (ICL) in LLMs, showing that ICL can be decomposed into a learning algorithm (A) that computes task vectors from demonstrations, followed by a rule application (f) that applies the task to queries.

## Executive Summary
This paper investigates the mechanism behind in-context learning (ICL) in large language models (LLMs) by proposing a hypothesis-class view of ICL. The authors argue that ICL can be broken down into two components: a "learning algorithm" (A) that maps a set of demonstrations (S) into a task vector (θ), and a "rule application" (f) that applies the rule defined by θ to a query (x). To validate this view, the authors conduct comprehensive experiments across various LLMs and tasks, demonstrating that separating the forward pass into these two components maintains high accuracy. They also show that the task vectors are interpretable and correspond to learned tasks, forming distinct clusters for each task category. The study provides insights into the underlying mechanism of ICL and may have practical implications for adapting LLMs to specific tasks.

## Method Summary
The authors propose a hypothesis-class framework where ICL is decomposed into two stages: first, a "learning algorithm" (A) processes demonstrations S to extract a task vector θ at an intermediate layer L; second, a "rule application" (f) uses this θ to process the query x. To extract θ, they introduce a "dummy query" x′ and calculate representations using only the first L layers. This θ is then patched into a second forward pass on the real query. The framework is tested across 18 tasks and multiple open LLMs (LLaMA 7B/13B/30B, GPT-J 6B, Pythia 2.8B/6.9B/12B) by comparing accuracy when separating A and f versus regular ICL, and by analyzing the geometry and interpretability of extracted θ vectors.

## Key Results
- Separating the forward pass into A (task vector extraction) and f (rule application) maintains high accuracy across multiple models and tasks.
- Task vectors extracted from demonstrations form interpretable clusters corresponding to task categories when projected to vocabulary space.
- Conflicting task vector experiments show that injected θ vectors dominate over direct demonstration attention, guiding the model's output.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning can be decomposed into a two-stage process: a learning algorithm (A) that computes a task vector θ from demonstrations S, followed by a rule application (f) that applies the task to query x using only θ.
- Mechanism: The transformer first processes the demonstration set S plus a dummy query to extract θ at an intermediate layer L. This θ is then patched into a second forward pass on the real query x, where subsequent layers use θ as a task-specific modulation without direct access to S.
- Core assumption: The transformer's intermediate activations can encode the mapping defined by S into a fixed-length vector that remains stable across different S and x inputs.
- Evidence anchors:
  - [abstract] "we make progress on this problem by showing that the functions learned by ICL often have a very simple structure: they correspond to the transformer LLM whose only inputs are the query x and a single 'task vector' calculated from the training set"
  - [section] "we propose the following procedure to tackle these challenges: to solve the first problem, we introduce a 'dummy query' x′ and calculate the representations of → using that query"
  - [corpus] Weak - only 5 citations in 8 neighbor papers, suggesting limited independent verification
- Break condition: If θ becomes highly sensitive to the specific choice of S or x′, the decomposition fails because the task vector is no longer stable.

### Mechanism 2
- Claim: The task vector θ captures task semantics, as evidenced by its geometry forming distinct clusters for each task and projecting to interpretable vocabulary terms.
- Mechanism: After extracting θ, dimensionality reduction (t-SNE) shows that vectors from the same task cluster tightly together, while cross-task distances are larger. Vocabulary projection reveals task-relevant tokens that never appeared in the prompt.
- Core assumption: The transformer's hidden states at layer L contain sufficient semantic information about the task to be captured in a single vector.
- Evidence anchors:
  - [abstract] "we also show that the task vectors are interpretable and correspond to learned tasks, forming distinct clusters for each task category"
  - [section] "Fig. 9 shows proximity between tasks of the same category, strengthening the idea that they encapsulate task understanding"
  - [corpus] Missing - no neighbor papers specifically address interpretability of task vectors
- Break condition: If θ vectors from the same task do not cluster together or vocabulary projections are nonsensical, the interpretability claim fails.

### Mechanism 3
- Claim: The rule application f dominates over direct demonstration attention, as demonstrated by injecting conflicting task vectors while keeping S fixed.
- Mechanism: When the model is given demonstrations for task A but a θ from task B is injected at layer L, the output follows task B's pattern rather than A's, showing θ drives behavior more than S.
- Core assumption: The model's forward pass can be guided predominantly by θ when it is available at the appropriate layer, even if S is still present.
- Evidence anchors:
  - [abstract] "we show that the θ vectors are interpretable and correspond to learned tasks, forming distinct clusters for each task category"
  - [section] "Tab.2, the 'Regular' forward pass shows high accuracy on task A (90%+), as anticipated. However, the 'Conflicting' forward pass yields high accuracy on task B, corresponding to the injected task vector θ"
  - [corpus] Weak - only indirect support from 3 neighbor papers discussing task vectors
- Break condition: If accuracy on the conflicting task drops significantly, the dominance of θ is not established.

## Foundational Learning

- Concept: Hypothesis class
  - Why needed here: The paper reframes ICL as operating within a specific hypothesis class H = {f(·; θ) | θ}, where each θ parameterizes a function that maps queries to outputs based on demonstrations
  - Quick check question: If ICL truly operates in a hypothesis class, what would happen to performance if we fix θ to a random vector instead of extracting it from S?

- Concept: Vector arithmetic in neural networks
  - Why needed here: The task vector θ is manipulated like a parameter vector in standard ML, enabling arithmetic operations (though not explicitly shown here) and understanding how tasks are encoded
  - Quick check question: If we average θ vectors from two similar tasks, would the resulting vector perform a reasonable interpolation between the two tasks?

- Concept: Causal mediation analysis
  - Why needed here: To understand how information flows from S through intermediate layers to the final output, particularly which heads and layers are responsible for extracting θ
  - Quick check question: If we ablate attention heads at layer L, would the model still be able to extract meaningful θ vectors?

## Architecture Onboarding

- Component map: Input prompt → Transformer layers 0 to L → θ extraction → Patch θ at layer L → Transformer layers L+1 to end → Output prediction
- Critical path: S + dummy x′ → layers 0 to L → θ → layers L+1 with patched θ → output
- Design tradeoffs: Extracting θ at layer L trades off between having enough information to capture the task versus preventing the query from influencing θ; choosing L requires balancing these constraints
- Failure signatures: Poor performance when θ is extracted too early (insufficient task information) or too late (query contamination); inconsistent θ across different S for the same task; vocabulary projections that are random
- First 3 experiments:
  1. Run the two-stage forward pass (A then f) and measure accuracy drop compared to regular ICL
  2. Extract θ from multiple S for the same task and visualize clustering with t-SNE
  3. Perform the conflicting tasks experiment by injecting θ from task B while keeping S from task A

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mechanism by which the task vector is formed in the transformer's forward pass?
- Basis in paper: [inferred] The paper proposes that the task vector is calculated by the first L layers of the transformer, but does not explain the specific mechanism or how the transformer's parameters contribute to this calculation.
- Why unresolved: The paper focuses on validating the hypothesis that a task vector exists and can be used to modulate the transformer, but does not delve into the specific mechanism of its formation.
- What evidence would resolve it: Detailed analysis of the transformer's attention patterns and weight matrices during the formation of the task vector, possibly using techniques like probing or mechanistic interpretability.

### Open Question 2
- Question: How does the task vector interact with the transformer's subsequent layers to produce the final output?
- Basis in paper: [inferred] The paper proposes that the task vector modulates the transformer to produce the output, but does not explain the specific interaction between the task vector and the transformer's layers.
- Why unresolved: The paper focuses on validating the hypothesis that the task vector can be used to modulate the transformer, but does not delve into the specific interaction between the task vector and the transformer's layers.
- What evidence would resolve it: Detailed analysis of the transformer's hidden states and attention patterns after the task vector is injected, possibly using techniques like probing or mechanistic interpretability.

### Open Question 3
- Question: Can the task vector approach be extended to more complex tasks, such as multi-token outputs or tasks requiring reasoning?
- Basis in paper: [explicit] The paper acknowledges that it focuses on relatively simple tasks with single-token outputs and notes that future work could explore the extension to more complex tasks.
- Why unresolved: The paper's experiments are limited to simple tasks, and the authors explicitly state that extending the approach to more complex tasks is an open question.
- What evidence would resolve it: Experiments applying the task vector approach to a diverse set of more complex tasks, such as multi-hop reasoning, arithmetic, or code generation, and comparing the results to baseline methods.

## Limitations

- The study focuses on 18 tasks across 4 categories, which may not represent the full diversity of real-world ICL scenarios.
- The hypothesis-class framework makes simplifying assumptions about the linearity and stability of task vector representations.
- The vocabulary projection analysis provides suggestive evidence of interpretability but lacks systematic validation through downstream tasks or human evaluation.

## Confidence

**High Confidence Claims:**
- The two-stage decomposition of ICL into learning algorithm (A) and rule application (f) is technically sound and produces measurable results
- Task vectors can be extracted and reused to maintain reasonable accuracy across multiple demonstrations
- The basic experimental methodology for testing task vector separation is reproducible

**Medium Confidence Claims:**
- Task vectors form interpretable clusters corresponding to task categories
- The conflicting task vector experiments demonstrate θ dominance over S
- The hypothesis-class view provides genuine insight into ICL mechanisms

**Low Confidence Claims:**
- Universal applicability of the hypothesis-class framework across all LLM architectures
- Complete interpretability of task vectors without further validation
- The extracted θ vectors capture all task-relevant information

## Next Checks

1. **Cross-Model Consistency Test**: Extract task vectors from LLaMA 7B and test them on LLaMA 13B and 30B models to assess cross-model transferability and determine whether θ representations are model-specific or task-general.

2. **Robustness to Demonstration Variation**: Systematically vary the number, order, and diversity of demonstrations S for the same task and measure the stability of extracted θ vectors using cosine similarity and clustering metrics.

3. **Human Interpretability Validation**: Conduct human evaluations where annotators predict task categories from projected vocabulary tokens and rate the semantic coherence of task vector clusters, providing empirical validation beyond t-SNE visualization.