---
ver: rpa2
title: Does a Neural Network Really Encode Symbolic Concepts?
arxiv_id: '2302.13080'
source_url: https://arxiv.org/abs/2302.13080
tags:
- concepts
- interaction
- samples
- concept
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether deep neural networks (DNNs) encode
  symbolic concepts by analyzing interaction effects between input variables. The
  authors examine four aspects: (1) sparsity of concepts in DNN inference, (2) transferability
  of concepts across samples, (3) transferability across different DNN architectures,
  and (4) discrimination power of concepts.'
---

# Does a Neural Network Really Encode Symbolic Concepts?

## Quick Facts
- arXiv ID: 2302.13080
- Source URL: https://arxiv.org/abs/2302.13080
- Reference count: 40
- Key outcome: Well-trained DNNs typically encode sparse, transferable, and discriminative interaction concepts across various datasets

## Executive Summary
This paper investigates whether deep neural networks encode symbolic concepts by analyzing interaction effects between input variables. Through extensive experiments on tabular, image, and point-cloud datasets, the authors demonstrate that DNN inference scores can be decomposed into sparse, transferable interaction concepts. The results show that well-trained DNNs extract meaningful symbolic concepts that are consistent across samples and architectures, supporting the idea that DNNs can learn symbolic representations beyond pure connectionist approaches.

## Method Summary
The authors use Harsanyi dividends to extract interaction concepts from DNN inference scores. For a given input sample x, they compute the interaction effect I(S|x) = Σ_{T⊆S} (-1)^{|S|-|T|} · v(x_T), where v(x) is the network output and x_T is the input with variables in T unchanged and others masked. The method analyzes four aspects: sparsity of concepts in DNN inference, transferability across samples, transferability across architectures, and discrimination power. Various DNN architectures are trained on multiple datasets including tabular, image, and point-cloud data.

## Key Results
- Well-trained DNNs encode sparse interaction concepts, with most concepts having negligible effects while a few salient concepts explain inference
- Concepts are transferable across different samples in the same category, with a small concept dictionary explaining most salient concepts
- Same salient concepts appear across different DNN architectures trained on the same task
- Extracted concepts consistently push predictions in the same direction across samples (high discrimination power)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DNN inference scores can be decomposed into sparse, transferable interaction concepts
- Mechanism: The Harsanyi dividend I(S|x) mathematically ensures v(x) = Σ I(S|x), and empirical results show most concepts have negligible effects while a few salient concepts explain the inference
- Core assumption: The decomposition is not just mathematical trickery but represents meaningful learned concepts
- Evidence anchors: [abstract] "well-trained DNNs typically encode sparse, transferable, and discriminative interaction concepts"; [section] "the DNN's inference score on a specific sample can be sparsely disentangled into effects of a few concepts"

### Mechanism 2
- Claim: Concepts learned by DNNs are transferable across different samples in the same category
- Mechanism: A small concept dictionary D_k can explain most salient concepts extracted from different samples, showing shared knowledge
- Core assumption: Similar samples in a category share common underlying concepts that the DNN learns
- Evidence anchors: [abstract] "concepts are supposed to satisfy... transferability of concepts across different samples"; [section] "we aim to verify whether concepts extracted from a sample can be transferred to other samples"

### Mechanism 3
- Claim: Concepts learned by DNNs are transferable across different architectures trained on same task
- Mechanism: Same salient concepts appear across different DNN architectures, suggesting they represent fundamental task knowledge rather than architecture-specific features
- Core assumption: Different architectures converge to similar conceptual representations when trained on same task
- Evidence anchors: [abstract] "concepts are supposed to satisfy... transferability across different DNN architectures"; [section] "we aim to verify whether concepts extracted from a DNN can be transferred to another DNN trained for the same task"

## Foundational Learning

- Concept: Interaction concepts as Harsanyi dividends
  - Why needed here: Forms the mathematical foundation for claiming DNNs learn symbolic concepts
  - Quick check question: Can you explain why I(S|x) satisfies the efficiency axiom?

- Concept: Transferability measurement metrics
  - Why needed here: Quantifies whether concepts are truly transferable across samples and architectures
  - Quick check question: How does the concept dictionary size affect the explanation ratio?

- Concept: Discrimination power measurement
  - Why needed here: Validates that concepts consistently push predictions in same direction
  - Quick check question: What does a high discrimination power value indicate about concept quality?

## Architecture Onboarding

- Component map: Input preprocessing -> Interaction extraction (I(S|x) computation) -> Concept selection (salient concept identification) -> Transferability analysis (dictionary construction and overlap calculation) -> Visualization (semantic part annotation and effect histograms)
- Critical path: Input → I(S|x) computation → Salient concept extraction → Transferability analysis → Results interpretation
- Design tradeoffs:
  - Semantic part annotation vs. computational cost (annotating reduces input variables)
  - Threshold selection for salient concepts (higher threshold = more significant but fewer concepts)
  - Dictionary size vs. explanation ratio (larger dictionary = better coverage but less concise)
- Failure signatures:
  - Low explanation ratio indicates poor transferability
  - High variance in concept effects across samples indicates poor discrimination
  - Concept overlap similar to random indicates poor cross-architecture transferability
- First 3 experiments:
  1. Verify sparsity by computing normalized interaction strength distribution for a simple dataset
  2. Test transferability by constructing concept dictionary for MNIST-3 dataset and measuring explanation ratio
  3. Validate discrimination by computing β(S) for top concepts in wi-fi dataset and checking consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mathematical relationship between interaction concepts and human-interpretable symbolic concepts?
- Basis in paper: [inferred] The authors acknowledge that while interaction concepts show desirable properties, there is no theoretical proof that they represent "meaningful and transferable concepts" in the human sense.
- Why unresolved: The paper focuses on quantitative properties of interaction concepts (sparsity, transferability, discrimination) but does not evaluate whether these concepts align with human cognitive concepts or symbolic reasoning frameworks.
- What evidence would resolve it: Human studies comparing the alignment between extracted interaction concepts and human-defined symbolic concepts, or formal proofs establishing the equivalence between interaction concepts and classical symbolic representations.

### Open Question 2
- Question: How does the concept-emerging phenomenon scale with increasing input dimensionality?
- Basis in paper: [explicit] "Given an input sample x with n input variables, the DNN may encode at most 2^n interaction concepts" and the authors note that "the computational cost for extracting salient concepts is high, when the number of input variables n is large."
- Why unresolved: While the paper shows successful concept extraction on moderate-dimensional data, it does not analyze how the phenomenon behaves as n approaches very large values or explore theoretical bounds on scalability.
- What evidence would resolve it: Systematic experiments on high-dimensional datasets (n >> 1000), theoretical analysis of computational complexity scaling, and exploration of dimensionality reduction techniques that preserve concept emergence.

### Open Question 3
- Question: What is the minimal network architecture complexity required for concept emergence?
- Basis in paper: [inferred] The paper tests various architectures (MLPs, CNNs, ResNets, PointNets) and finds concept emergence, but does not conduct ablation studies to identify which architectural components are essential for concept learning.
- Why unresolved: While concept emergence is demonstrated to be common across architectures, the paper does not isolate the architectural features that enable this phenomenon or establish whether simpler models could achieve similar results.
- What evidence would resolve it: Systematic ablation studies varying architectural components (depth, skip connections, activation functions), comparison with classical symbolic models, and analysis of the relationship between architectural complexity and concept emergence quality.

## Limitations
- Results are based on well-trained models, limiting conclusions about whether concept learning is an emergent property of training
- Concept extraction depends on specific masking strategies, which may influence the discovered concepts
- The generalizability of findings beyond the studied datasets remains unclear

## Confidence
- Sparsity of concepts: High confidence
- Discrimination power: High confidence  
- Cross-sample transferability: Medium confidence
- Cross-architecture transferability: Low confidence

## Next Checks
1. Verify concept extraction by computing normalized interaction strength distribution for a simple dataset and checking if most concepts have negligible effects
2. Test concept transferability by constructing a concept dictionary for MNIST-3 dataset and measuring explanation ratio
3. Validate discrimination power by computing β(S) for top concepts in wi-fi dataset and checking consistency of effects across samples