---
ver: rpa2
title: The performance of multiple language models in identifying offensive language
  on social media
arxiv_id: '2312.11504'
source_url: https://arxiv.org/abs/2312.11504
tags:
- data
- figure
- learning
- language
- offensive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluated the performance of multiple machine learning
  models, including traditional classifiers, deep learning models, and pre-trained
  transformers, for identifying offensive language in tweets. The research addressed
  the challenge of classifying offensive content across three hierarchical tasks:
  detecting offensive language, identifying whether it targets specific entities,
  and categorizing the target type (individual, group, or other).'
---

# The performance of multiple language models in identifying offensive language on social media

## Quick Facts
- arXiv ID: 2312.11504
- Source URL: https://arxiv.org/abs/2312.11504
- Reference count: 0
- Primary result: ALBERT achieved 79.59% accuracy and 0.9227 F1-macro on binary offensive language detection

## Executive Summary
This study evaluated multiple machine learning models for identifying offensive language in tweets across three hierarchical classification tasks: detecting offensive language, identifying if it targets entities, and categorizing target types. The research compared traditional classifiers, deep learning models, and pre-trained transformers on the OLID dataset. ALBERT demonstrated superior performance on binary classification tasks, while Bi-LSTM excelled at multi-class target classification. The study highlights the effectiveness of hierarchical approaches and pre-trained transformers for offensive language detection in imbalanced datasets.

## Method Summary
The study evaluated eight algorithms (Naive Bayes, KNN, SVM, Decision Tree, Random Forest, Logistic Regression, Bi-LSTM, and ALBERT) on the OLID dataset of 14,200 tweets. Data preprocessing included emoji translation, noise removal, and text normalization using NLTK and open-source code. Models were assessed using accuracy, F1-macro, precision, recall, and MCC metrics across three hierarchical classification tasks. The experimental pipeline involved data preprocessing, tokenization with Word2Vec embedding, model training, evaluation, and result comparison.

## Key Results
- ALBERT achieved highest binary classification performance: 79.59% accuracy and 0.9227 F1-macro for detecting offensive language
- Bi-LSTM outperformed other models on multi-class target classification tasks
- Hierarchical classification approach showed improved accuracy by progressively refining task complexity
- Pre-trained transformers demonstrated significant advantages over traditional classifiers for offensive language detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical classification with progressive task refinement improves offensive language detection accuracy.
- Mechanism: The study divides the task into three levels: (1) detect if language is offensive, (2) determine if offensive language targets someone, and (3) classify the target type (individual, group, or other). This staged approach enables specialized modeling for each subtask, reducing confusion between broad and fine-grained categories.
- Core assumption: Subtasks are independent enough that solving earlier ones simplifies later classification.
- Evidence anchors:
  - [abstract]: "This study evaluated the performance of multiple machine learning models... for identifying offensive language in tweets... across three hierarchical tasks: detecting offensive language, identifying whether it targets specific entities, and categorizing the target type."
  - [section]: "Level A: Offensive language Detection... Level B: Categorization of Offensive Language... Level C: Offensive Language Target Identification"
  - [corpus]: Weak—no corpus-level metadata about task hierarchy.
- Break condition: If target identification in Level B is ambiguous or context-dependent, the staged assumption breaks and errors propagate.

### Mechanism 2
- Claim: Pre-trained transformers (ALBERT) outperform traditional classifiers and deep learning models on binary classification tasks for offensive language detection.
- Mechanism: ALBERT leverages massive pre-training on diverse text, capturing rich contextual embeddings that help distinguish subtle offensive vs. non-offensive language more effectively than shallow or less context-aware models.
- Core assumption: Pre-training provides general language understanding that transfers well to the specific domain of offensive language.
- Evidence anchors:
  - [abstract]: "The pre-trained ALBERT model achieved the highest performance on binary classification tasks, with an accuracy of 79.59% and F1-macro of 0.9227 for detecting offensive language."
  - [section]: "Albert... Through extensive text training in advance, ALBERT achieved the best results in 11 NLP tasks."
  - [corpus]: No direct corpus evidence; relies on cross-study generalization.
- Break condition: If the pre-training corpus lacks offensive language patterns, transfer benefit diminishes.

### Mechanism 3
- Claim: Bi-LSTM models excel at multi-class target classification where context and sequence order matter.
- Mechanism: Bidirectional LSTM processes text forwards and backwards, capturing dependencies that help differentiate individual, group, and other target types, which are context-sensitive.
- Core assumption: Sequential context is critical for distinguishing target types.
- Evidence anchors:
  - [abstract]: "For multi-class classification, Bi-LSTM outperformed other models."
  - [section]: "Bi-LSTM... can better capture two-way semantic dependency... It can remember the sequence information in both directions, which is helpful to understand the meaning of the context, which is very helpful to identify the offensive in the sentence."
  - [corpus]: Weak—no corpus examples of Bi-LSTM applied to target type classification.
- Break condition: If target type cues are explicit keywords rather than context-dependent, Bi-LSTM advantage shrinks.

## Foundational Learning

- **Text preprocessing and noise removal**
  - Why needed here: Social media text contains emojis, URLs, @mentions, hashtags, and informal language that can obscure offensive content cues.
  - Quick check question: What preprocessing steps convert "This is great! #awesome @user" into clean, analyzable tokens?

- **Word embedding (Word2Vec vs. one-hot)**
  - Why needed here: Dense embeddings capture semantic similarity (e.g., "hate" close to "dislike") whereas one-hot vectors are sparse and lose relational info.
  - Quick check question: Why does Word2Vec outperform one-hot encoding in tasks where synonymy matters?

- **Evaluation metrics for imbalanced data**
  - Why needed here: Offensive vs. non-offensive classes are often imbalanced; accuracy alone is misleading.
  - Quick check question: How does F1-macro differ from accuracy when one class dominates?

## Architecture Onboarding

- **Component map**: Data ingestion -> Text preprocessing pipeline -> Tokenization & Word2Vec embedding -> Model training (multiple classifiers) -> Evaluation (accuracy, F1-macro, MCC, confusion matrix) -> Result comparison
- **Critical path**: Preprocess -> Embed -> Train -> Evaluate -> Compare. Any delay in preprocessing (e.g., emoji translation) stalls the whole pipeline.
- **Design tradeoffs**:
  - Emoji translation preserves sentiment but adds preprocessing overhead; removing them speeds processing but loses nuance.
  - Using ALBERT gives best accuracy but is computationally heavy; Bi-LSTM is lighter but slightly less accurate on binary tasks.
  - Class imbalance handled via F1-macro/MCC rather than re-sampling to preserve real-world distribution.
- **Failure signatures**:
  - Overfitting: Training accuracy >> validation accuracy; loss curves diverge.
  - Underfitting: All models perform near random; high bias evident in confusion matrix.
  - Data leakage: If preprocessing removes tokens that differentiate classes, accuracy drops sharply.
- **First 3 experiments**:
  1. Baseline: Train all eight models on raw text (no preprocessing) to quantify preprocessing impact.
  2. Ablation: Remove emoji translation step; compare ALBERT accuracy drop.
  3. Hyperparameter sweep: For Bi-LSTM, vary embedding size and LSTM units; plot F1-macro vs. complexity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance metrics of ALBERT compare to other transformer-based models (e.g., BERT, RoBERTa) in the context of offensive language detection on social media?
- Basis in paper: [explicit] The paper mentions that ALBERT achieved the highest performance in binary classification tasks with an accuracy of 79.59% and F1-macro of 0.9227 for detecting offensive language, but does not compare its performance to other transformer-based models.
- Why unresolved: The study focuses on comparing traditional classifiers, deep learning models, and pre-trained transformers, but does not include a direct comparison with other transformer-based models like BERT or RoBERTa.
- What evidence would resolve it: Conducting experiments with BERT and RoBERTa models on the same dataset and comparing their performance metrics (accuracy, F1-macro, precision, recall, and MCC) with ALBERT would provide a clearer understanding of ALBERT's relative performance.

### Open Question 2
- Question: What is the impact of emoji translation on the performance of offensive language detection models?
- Basis in paper: [explicit] The paper mentions that emoji translation was part of the data preprocessing, converting emojis into text to maintain their semantic meaning, but does not analyze its impact on model performance.
- Why unresolved: The paper does not provide a comparative analysis of model performance with and without emoji translation, leaving the impact of this preprocessing step unclear.
- What evidence would resolve it: Running experiments with and without emoji translation on the same dataset and comparing the performance metrics of the models would determine the impact of this preprocessing step.

### Open Question 3
- Question: How does the imbalance in the dataset affect the performance of different classification algorithms, and what strategies can be employed to mitigate this issue?
- Basis in paper: [inferred] The paper acknowledges the imbalance in the dataset and mentions the use of evaluation metrics like MCC to assess performance, but does not explore strategies to address the imbalance.
- Why unresolved: The paper does not investigate techniques such as oversampling, undersampling, or synthetic data generation to handle the imbalance, nor does it analyze how this imbalance affects different algorithms.
- What evidence would resolve it: Implementing imbalance mitigation strategies and comparing their effectiveness across different algorithms would provide insights into the best approaches for handling imbalanced datasets in offensive language detection.

### Open Question 4
- Question: How do different word embedding techniques (e.g., Word2Vec, GloVe, FastText) affect the performance of offensive language detection models?
- Basis in paper: [explicit] The paper mentions the use of Word2Vec for word embedding but does not compare its performance with other embedding techniques.
- Why unresolved: The study does not include experiments with alternative embedding techniques, leaving the impact of different embeddings on model performance unexplored.
- What evidence would resolve it: Conducting experiments with various embedding techniques and comparing their impact on the performance metrics of the models would clarify the influence of word embeddings on offensive language detection.

## Limitations

- Findings based on single dataset (OLID) with 14,200 tweets, limiting generalizability to other platforms or languages
- Hierarchical task approach assumes clear boundaries between subtasks, but real-world offensive language often exhibits overlapping characteristics
- Preprocessing pipeline impact is significant but not fully quantified—removing emojis and noise could eliminate context that helps identify offensive content
- Computational cost and training time for pre-trained transformers create practical tradeoffs not fully addressed

## Confidence

**High confidence**: ALBERT's superior performance on binary classification tasks (accuracy 79.59%, F1-macro 0.9227) is well-supported by direct metrics from the study. The observation that Bi-LSTM excels at multi-class target classification is also strongly evidenced by comparative results.

**Medium confidence**: The hierarchical classification approach's effectiveness relies on the assumption that subtasks are independent enough for staged modeling to work. While the study demonstrates improved performance, it doesn't test whether joint or end-to-end models might perform better by capturing task interdependencies.

**Low confidence**: The claim that pre-training provides general language understanding transferable to offensive language detection lacks direct evidence from the study's corpus. The benefit could be specific to ALBERT's architecture rather than pre-training per se, and the OLID dataset's characteristics may not represent all offensive language contexts.

## Next Checks

1. Cross-dataset validation: Test the same models on different offensive language datasets (e.g., HateXplain, DIRT) to verify whether ALBERT maintains superior performance across varied contexts and distributions.

2. Ablation study on preprocessing: Systematically remove each preprocessing step (emoji translation, noise removal, normalization) and measure the impact on model performance to quantify which steps are most critical for offensive language detection.

3. Task interdependence analysis: Compare the hierarchical approach against joint multi-task learning models that predict all three classification levels simultaneously, measuring whether staged classification truly outperforms integrated modeling.