---
ver: rpa2
title: 'ViCGCN: Graph Convolutional Network with Contextualized Language Models for
  Social Media Mining in Vietnamese'
arxiv_id: '2309.02902'
source_url: https://arxiv.org/abs/2309.02902
tags:
- uni00000013
- graph
- social
- media
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ViCGCN, a novel approach for social media mining
  in Vietnamese that integrates contextualized language models (PhoBERT) with Graph
  Convolutional Networks (GCN). The method addresses the challenges of imbalanced
  and noisy data commonly found in Vietnamese social media text.
---

# ViCGCN: Graph Convolutional Network with Contextualized Language Models for Social Media Mining in Vietnamese

## Quick Facts
- arXiv ID: 2309.02902
- Source URL: https://arxiv.org/abs/2309.02902
- Reference count: 40
- Outperforms 13 powerful baseline models on Vietnamese social media text classification

## Executive Summary
This paper introduces ViCGCN, a novel approach that combines PhoBERT contextualized embeddings with Graph Convolutional Networks (GCN) for Vietnamese social media mining. The method addresses the challenges of imbalanced and noisy data common in Vietnamese social media text by creating a heterogeneous graph structure that represents both document and word nodes. Extensive experiments on three Vietnamese benchmark datasets demonstrate significant performance improvements over state-of-the-art models, achieving up to 6.21% improvement over the best contextualized language models.

## Method Summary
ViCGCN integrates PhoBERT's contextualized embeddings with GCN's ability to capture syntactic and semantic dependencies through a heterogeneous graph structure. The model constructs a bipartite graph connecting document nodes to word nodes using TF-IDF weights, and word-word pairs using PPMI weights. A hyperparameter λ controls the weighted combination of GCN and PhoBERT embeddings in the final classification. The approach is evaluated on three Vietnamese benchmark datasets (UIT-VSMEC, UIT-ViCTSD, and UIT-VSFC) using Macro F1-score and Weighted F1-score metrics.

## Key Results
- Achieves improvements of up to 6.21%, 4.61%, and 2.63% over best contextualized language models on three Vietnamese benchmark datasets
- Demonstrates significant performance improvements compared to 13 baseline models including mBERT, RoBERTa, XLM-R, PhoBERT, viBERT, and vELECTRA
- Successfully addresses imbalanced and noisy data issues common in Vietnamese social media text

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ViCGCN's heterogeneous graph structure enables joint learning across document and word nodes, improving classification by capturing both local and global context
- Mechanism: The model builds a bipartite graph where document nodes are connected to word nodes via TF-IDF and word-word nodes via PPMI, allowing GCN to propagate information between words and documents
- Core assumption: TF-IDF and PPMI measures accurately capture meaningful relationships between words and documents in Vietnamese social media text
- Evidence anchors: Abstract states joint training captures syntactic and semantic dependencies; section describes using TF-IDF and PPMI for graph construction
- Break condition: If TF-IDF or PPMI fail to capture meaningful relationships in Vietnamese social media context

### Mechanism 2
- Claim: Weighted combination of GCN and PhoBERT embeddings allows flexible trade-off between graph-derived and contextualized language model features
- Mechanism: Hyperparameter λ controls contribution of GCN and PhoBERT outputs, with higher values favoring graph-based features and lower values emphasizing language model context
- Core assumption: Both GCN and PhoBERT features contain complementary information valuable for Vietnamese social media classification
- Evidence anchors: Abstract mentions λ controls trade-off; section reports λ = 0.6 optimal across all datasets
- Break condition: If one component consistently provides redundant or noisy information

### Mechanism 3
- Claim: Graph Convolutional Networks mitigate imbalanced and noisy data issues by leveraging graph structure to capture relationships that raw text features miss
- Mechanism: GCN aggregates information from neighboring nodes, effectively smoothing out noise and balancing class representations through message passing
- Core assumption: Graph structure created from social media text contains meaningful relationships that can be exploited to overcome class imbalance and noise
- Evidence anchors: Abstract states GCN addresses imbalanced and noisy data problems; section claims significant performance improvements
- Break condition: If social media text lacks sufficient structural relationships or noise overwhelms the signal

## Foundational Learning

- Concept: Graph Neural Networks and message passing
  - Why needed here: Understanding how GCN aggregates neighbor information is crucial for grasping ViCGCN's ability to capture contextual relationships
  - Quick check question: How does a GCN layer update node representations using information from neighboring nodes?

- Concept: Language model pretraining and fine-tuning
  - Why needed here: PhoBERT's contextualized embeddings form the foundation that GCN builds upon, so understanding transfer learning is essential
  - Quick check question: What is the difference between pretraining objectives (MLM) and fine-tuning objectives in transformer-based models?

- Concept: Heterogeneous graph construction and embedding fusion
  - Why needed here: ViCGCN combines document and word nodes with different edge types, requiring understanding of multi-relational graph structures
  - Quick check question: How would you represent a document-word bipartite graph in matrix form for GCN processing?

## Architecture Onboarding

- Component map: Vietnamese tokenizer → PhoBERT encoder → Document/word node creation → Heterogeneous graph construction (TF-IDF + PPMI edges) → GCN layers → Weighted fusion (λ parameter) → Classification head
- Critical path: PhoBERT → Graph construction → GCN → Fusion → Classification
- Design tradeoffs: Vietnamese-specific PhoBERT vs multilingual alternatives; heterogeneous graph complexity vs performance gains; λ parameter tuning overhead vs improved accuracy
- Failure signatures: Poor performance on rare classes (GCN not handling imbalance well); degraded results on short texts (graph structure less meaningful); overfitting on small datasets (too many parameters)
- First 3 experiments:
  1. Compare ViCGCN with PhoBERT alone on a small subset to measure GCN contribution
  2. Vary λ from 0 to 1 in increments of 0.2 to find optimal trade-off point
  3. Test on documents of varying lengths to understand when graph structure becomes beneficial

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal lambda (λ) value for balancing the contributions of GCN and PhoBERT in the ViCGCN model?
- Basis in paper: [explicit] The paper conducted experiments on the dev set to determine the optimal value of λ, finding that setting λ to a value from 0.6 to 0.8 is more desirable, with λ = 0.6 being optimal on all datasets
- Why unresolved: While the paper provides a recommended range, the optimal value may vary depending on the specific dataset and task, requiring further experimentation to determine the best λ for different scenarios
- What evidence would resolve it: Conducting extensive experiments on a wide range of Vietnamese social media datasets with varying characteristics to determine the optimal λ value for each dataset and task

### Open Question 2
- Question: How does the performance of ViCGCN compare to other graph neural network models, such as Graph Attention Networks (GATs), on Vietnamese social media text classification tasks?
- Basis in paper: [inferred] The paper suggests that experimenting with other graph neural network models, such as GATs, to compare their performance with ViCGCN could be a direction for future research
- Why unresolved: The paper only evaluates the performance of ViCGCN against baseline models and does not compare it to other graph neural network models, leaving the question of its relative performance unanswered
- What evidence would resolve it: Conducting experiments comparing the performance of ViCGCN to other graph neural network models, such as GATs, on Vietnamese social media text classification tasks

### Open Question 3
- Question: How can automatic pre-processing techniques for text normalization, such as converting slang or informal language to standard text, improve the accuracy and robustness of ViCGCN on Vietnamese social media text?
- Basis in paper: [inferred] The paper mentions that automatic pre-processing techniques for text normalization could improve the accuracy and robustness of the system, but does not explore this further
- Why unresolved: The paper does not investigate the impact of pre-processing techniques on the performance of ViCGCN, leaving the question of their effectiveness unanswered
- What evidence would resolve it: Implementing and evaluating different pre-processing techniques for text normalization on Vietnamese social media text and measuring their impact on the performance of ViCGCN

## Limitations
- Evaluation relies on only three Vietnamese benchmark datasets, limiting generalizability to other social media mining tasks
- No ablation analysis provided to isolate individual contributions of GCN versus PhoBERT components
- Computational complexity of constructing heterogeneous graphs for large-scale social media data is not addressed

## Confidence

- **High confidence**: Experimental methodology and comparison with 13 baseline models is well-structured and follows standard NLP evaluation practices. Reported performance improvements over PhoBERT on all three datasets are consistent and statistically significant
- **Medium confidence**: Mechanism claims about GCN addressing imbalanced and noisy data issues are plausible based on general GNN literature but lack direct Vietnamese social media validation. Optimal λ value (0.6) is empirically supported but not theoretically justified
- **Low confidence**: Claims about specific effectiveness of TF-IDF and PPMI for Vietnamese social media text construction are assumed rather than empirically validated for this specific context

## Next Checks

1. **Ablation study**: Remove the GCN component to measure PhoBERT's standalone performance, then add GCN back to quantify its specific contribution across different λ values and document lengths

2. **Cross-dataset generalization**: Test ViCGCN on additional Vietnamese social media datasets (beyond the three benchmarks) to verify robustness and identify failure modes with different text characteristics

3. **Noise robustness evaluation**: Systematically inject varying levels of noise into clean Vietnamese text to measure how ViCGCN's performance degrades compared to baseline models, specifically testing the GCN's noise-mitigation claims