---
ver: rpa2
title: 'Silkie: Preference Distillation for Large Visual Language Models'
arxiv_id: '2312.10665'
source_url: https://arxiv.org/abs/2312.10665
tags:
- visual
- preference
- gpt-4v
- lvlms
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Silkie, a method for improving the alignment
  of large vision-language models (LVLMs) with human preferences. The core idea is
  to leverage AI feedback for preference distillation.
---

# Silkie: Preference Distillation for Large Visual Language Models

## Quick Facts
- arXiv ID: 2312.10665
- Source URL: https://arxiv.org/abs/2312.10665
- Reference count: 35
- Key outcome: Silkie achieves 6.9% and 9.5% relative improvements on MME benchmark perception and cognition tasks respectively, and sets new state-of-the-art on MMHal-Bench with score 3.02.

## Executive Summary
This paper introduces Silkie, a method for improving the alignment of large vision-language models with human preferences through preference distillation. The approach leverages AI feedback by using GPT-4V to annotate responses from multiple LVLMs across three aspects: helpfulness, visual faithfulness, and ethical considerations. This creates a large-scale vision-language feedback dataset that is then used to fine-tune a Qwen-VL-Chat model using direct preference optimization. The resulting model demonstrates significant improvements over the base model on various benchmarks, showing more consistent improvements compared to human-annotated preference datasets, particularly for fine-grained perception and complex cognition tasks.

## Method Summary
The method involves collecting 80k multi-modal instructions from diverse sources, generating responses from 12 different LVLMs, and having GPT-4V rate these responses across three aspects. These ratings are converted into pairwise preference data using the Bradley-Terry model and used to fine-tune Qwen-VL-Chat-v1.1-7B via direct preference optimization (DPO). The training runs for 3 epochs with AdamW optimizer, cosine learning rate schedule, and LoRA tuning across 16 NVIDIA-A100 GPUs. The approach bypasses traditional reward modeling by directly optimizing the model's likelihood on preference pairs.

## Key Results
- 6.9% relative improvement on MME benchmark perception tasks
- 9.5% relative improvement on MME benchmark cognition tasks
- New state-of-the-art score of 3.02 on MMHal-Bench hallucination evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct Preference Optimization (DPO) with AI-annotated preference data improves LVLM alignment by bypassing the reward modeling stage and directly optimizing the model's likelihood on preference pairs.
- Mechanism: DPO uses the Bradley-Terry model to construct a binary preference ranking from multi-aspect annotations. The model parameters are updated via backpropagation of the preference log-likelihood, avoiding the instability of two-stage RLHF pipelines.
- Core assumption: GPT-4V's multi-aspect ratings are sufficiently correlated with human preferences and can serve as a reliable proxy for preference annotation at scale.
- Evidence anchors: [abstract] "We adopt GPT-4V to assess the generated outputs regarding helpfulness, visual faithfulness, and ethical considerations." [section 2.3] "We define evaluation templates to evaluate LVLM outputs from three aspects... GPT-4V is then queried with the annotation templates..." [corpus] Weak – neighbor papers mention AI feedback but do not validate GPT-4V's rating consistency directly.
- Break condition: If GPT-4V ratings are systematically biased toward its own outputs or fail to correlate with human preferences, the preference ranking will be unreliable and harm alignment.

### Mechanism 2
- Claim: Multi-aspect AI feedback (helpfulness, visual faithfulness, ethical considerations) provides a richer supervision signal than single-aspect human annotation, enabling more comprehensive improvements across perception and cognition tasks.
- Mechanism: Averaging scores across three curated aspects into a single preference score creates a composite reward that captures multiple alignment dimensions simultaneously, allowing DPO to optimize for a holistic quality metric.
- Core assumption: The three aspects are sufficiently independent and their combination better reflects user preferences than any single dimension alone.
- Evidence anchors: [abstract] "The resulting model Silkie, achieves 6.9% and 9.5% relative improvement on the MME benchmark regarding the perception and cognition capabilities, respectively." [section 2.4] "Scores for visual faithfulness closely mirror the distribution observed in the helpfulness evaluation, implying a potential correlation between these two aspects during the annotation process." [corpus] Weak – no direct comparison with single-aspect datasets in the corpus.
- Break condition: If one aspect dominates the combined score or the aspects are highly correlated, the multi-aspect supervision may not provide additional benefit over single-aspect feedback.

### Mechanism 3
- Claim: Scaling AI-annotated preference data via automatic generation from a diverse pool of LVLMs yields more consistent improvements than smaller human-annotated datasets.
- Mechanism: By generating responses from 12 LVLMs across diverse instruction sources and having GPT-4V rank them, the resulting dataset captures a wide distribution of model behaviors and instruction types, providing dense supervision for fine-grained perception and complex reasoning tasks.
- Core assumption: A large, automatically generated dataset can cover the instruction space more comprehensively than human annotation, leading to more stable alignment gains.
- Evidence anchors: [abstract] "Further analysis shows that DPO with our VLFeedback dataset mainly boosts the fine-grained perception and complex cognition abilities of LVLMs, leading to more comprehensive improvements compared to human-annotated preference datasets." [section 4.3] "Our GPT-4V annotated preference dataset brings more consistent improvements on four benchmarks." [corpus] Weak – corpus neighbors discuss preference optimization but do not directly compare AI vs human annotation scale effects.
- Break condition: If the diversity of generated responses does not translate to meaningful coverage of user instruction space, the scale advantage will not materialize into performance gains.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO provides a stable, efficient alternative to RLHF by directly optimizing the model on preference pairs without an intermediate reward model, which is critical when scaling to large LVLM datasets.
  - Quick check question: What is the key difference between DPO and the standard RLHF pipeline that makes DPO more stable for large-scale preference learning?

- Concept: Bradley-Terry Preference Model
  - Why needed here: The Bradley-Terry model is used to convert multi-response rankings into pairwise comparisons, which is the input format required for DPO training.
  - Quick check question: How does the Bradley-Terry model transform a ranked list of K responses into pairwise comparison data for training?

- Concept: Multi-Modal Instruction Tuning
  - Why needed here: LVLMs require aligned visual and language representations; instruction tuning on diverse multi-modal datasets (LLaVA, SVIT, M3IT, etc.) establishes the base capabilities that preference distillation then refines.
  - Quick check question: Why is it important to use diverse instruction sources (general, academic, domain-specific) when building the base LVLM before preference distillation?

## Architecture Onboarding

- Component map: Instruction Collection -> 4 LVLM responses -> GPT-4V rating -> pairwise preference pairs -> DPO fine-tuning -> evaluation
- Critical path: Instruction → 4 LVLM responses → GPT-4V rating → pairwise preference pairs → DPO fine-tuning → evaluation
- Design tradeoffs:
  - Using GPT-4V as annotator trades human quality for scale and consistency but introduces potential bias toward GPT-4V's own characteristics.
  - Averaging three aspects into one score simplifies training but may mask important dimension-specific issues.
  - Random sampling of 4 LVLMs per instruction ensures diversity but may miss rare failure modes.
- Failure signatures:
  - Training divergence or collapse: indicates poor preference signal quality or inappropriate DPO hyperparameters.
  - Improved perception but degraded cognition scores: suggests the preference data is biased toward perceptual tasks.
  - No improvement over baseline: indicates either weak preference signal or insufficient diversity in the instruction set.
- First 3 experiments:
  1. Run DPO with Length-as-Best heuristic to verify whether reward hacking occurs and establish a baseline for comparison.
  2. Train DPO with GPT-4V-as-Best to isolate the effect of using GPT-4V's own outputs as positive examples.
  3. Perform ablation on the three annotation aspects (helpfulness, visual faithfulness, ethical) to determine which contributes most to performance gains.

## Open Questions the Paper Calls Out

- Question: How does the choice of base model affect the performance of Silkie? Would a different LVLM backbone, such as BLIP-2 or IDEFICS, yield similar or superior results compared to Qwen-VL-Chat?
  - Basis in paper: [inferred] The paper uses Qwen-VL-Chat as the base model for preference distillation, but does not explore the impact of different base models on performance.
  - Why unresolved: The paper focuses on demonstrating the effectiveness of the VLFeedback dataset and preference distillation method using a single base model. The choice of base model could influence the final performance, and exploring different options would provide valuable insights.
  - What evidence would resolve it: Conducting experiments with various LVLM backbones and comparing their performance on the same benchmarks would reveal the impact of the base model choice.

- Question: How does the quality and diversity of the VLFeedback dataset affect the performance of Silkie? Would expanding the dataset to include more instructions, LVLMs, or annotation aspects lead to further improvements?
  - Basis in paper: [explicit] The paper acknowledges the limitations of the current VLFeedback dataset, including the lack of safety-oriented feedback and the limited range of LVLMs and instruction datasets.
  - Why unresolved: The authors recognize the potential for improvement by expanding the dataset but do not explore this direction in the current work.
  - What evidence would resolve it: Creating an expanded version of the VLFeedback dataset with additional instructions, LVLMs, and annotation aspects, and evaluating the performance of Silkie on this enhanced dataset would provide insights into the impact of dataset quality and diversity.

- Question: How does the preference distillation method compare to other alignment techniques, such as RLHF or SFT, in terms of effectiveness and efficiency for improving LVLMs?
  - Basis in paper: [inferred] The paper focuses on preference distillation using DPO but does not directly compare its performance to other alignment methods like RLHF or SFT.
  - Why unresolved: While the paper demonstrates the effectiveness of preference distillation, a direct comparison with other alignment techniques would provide a more comprehensive understanding of the strengths and weaknesses of each approach.
  - What evidence would resolve it: Conducting experiments that compare the performance of Silkie (using preference distillation) with models trained using RLHF or SFT on the same benchmarks would reveal the relative effectiveness and efficiency of each alignment technique.

## Limitations

- The core claims rely heavily on GPT-4V's ability to provide reliable preference annotations, but this assumption lacks direct human validation evidence.
- The multi-aspect rating system is assumed to provide richer supervision than single-aspect annotation, though the paper does not demonstrate whether all three aspects are independently necessary.
- The claimed advantages over human-annotated datasets are based on observed consistency rather than direct comparative experiments.

## Confidence

- GPT-4V preference quality: Medium
- Multi-aspect annotation advantage: Low-Medium
- Scale benefits over human annotation: Medium
- Benchmark performance claims: High

## Next Checks

1. Conduct human preference evaluation on a subset of the VLFeedback dataset to validate GPT-4V annotation reliability and correlation with human judgments.
2. Perform ablation studies on the three annotation aspects to determine which dimensions drive the most performance improvements and whether all are necessary.
3. Test Silkie on additional evaluation protocols beyond MME and MMHal-Bench to identify potential capability regressions or domain-specific weaknesses.