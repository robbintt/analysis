---
ver: rpa2
title: 'DoGE: Domain Reweighting with Generalization Estimation'
arxiv_id: '2310.15393'
source_url: https://arxiv.org/abs/2310.15393
tags:
- domain
- generalization
- weights
- training
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of optimizing domain weights in
  pretraining corpora for Large Language Models (LLMs) to improve their generalization
  ability. Current methods rely on heuristics or trial and error to determine domain
  weights, which can be suboptimal.
---

# DoGE: Domain Reweighting with Generalization Estimation

## Quick Facts
- arXiv ID: 2310.15393
- Source URL: https://arxiv.org/abs/2310.15393
- Reference count: 8
- Key outcome: DOGE improves perplexity and few-shot reasoning accuracies across 6 tasks compared to baseline methods on SlimPajama dataset.

## Executive Summary
This paper addresses the problem of optimizing domain weights in pretraining corpora for Large Language Models (LLMs) to improve their generalization ability. Current methods rely on heuristics or trial and error to determine domain weights, which can be suboptimal. The authors propose DOmain reweighting with Generalization Estimation (DOGE), a principled approach to optimize domain weights based on their contribution to the final generalization objective.

## Method Summary
DOGE consists of a two-stage process: (1) training a proxy model to obtain domain weights using a bi-level optimization algorithm, and (2) training a larger base model by sampling training domains according to the learned domain weights. The proxy model training iteratively updates domain weights to maximize overall generalization gain using mirror descent. The base model is then trained with domain sampling according to the final weights.

## Key Results
- DOGE improves perplexity and few-shot reasoning accuracies across 6 tasks compared to baseline methods.
- DOGE effectively identifies inter-domain dependencies and consistently achieves better test perplexity on out-of-domain target tasks.
- DOGE achieves 2.87 perplexity improvement on average across 6 tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The domain weight update in DOGE is equivalent to maximizing alignment of gradients across domains, thereby encouraging the model to learn shared representations.
- Mechanism: DOGE computes W(t) as the inner product between the gradient for domain j and the sum of gradients from all other domains. High W(t) indicates that improving the model on Dj also improves performance on other domains, so DOGE increases the weight of Dj.
- Core assumption: The inner product of gradients accurately reflects how much learning in one domain will help in another.
- Evidence anchors: [abstract], [section] - Weak corpus evidence.
- Break condition: If gradients are dominated by noise or the cancellation effect is not well mitigated, the influence estimates become unreliable.

### Mechanism 2
- Claim: DOGE uses a single proxy model instead of DOREMI's two-model setup, avoiding dependency on a separately trained reference model and making the method more robust to reference model capacity.
- Mechanism: In the first stage, DOGE trains only a small proxy model while iteratively updating domain weights to greedily minimize average loss across all domains.
- Core assumption: A small proxy model is sufficient to capture the gradient structure of the full model.
- Evidence anchors: [abstract], [section] - Weak corpus evidence.
- Break condition: If the proxy model is too small or if the greedy minimization does not approximate the full objective well, the weights may not generalize effectively.

### Mechanism 3
- Claim: The mirror descent update with Bregman divergence stabilizes weight updates by preventing drastic changes and smoothing over noisy gradients.
- Mechanism: The update α(t) = α(t-1) ⊙ exp(η(t)W(t)/µ) is a multiplicative weights update that softly increases or decreases domain weights according to their generalization estimation score.
- Core assumption: The mirror descent step with exponential smoothing is effective in maintaining a stable distribution over domains despite noisy gradient estimates.
- Evidence anchors: [abstract], [section] - Weak corpus evidence.
- Break condition: If the step size η(t) or regularization µ is not tuned properly, the update can be either too aggressive or too slow.

## Foundational Learning

- Concept: Bregman divergence and mirror descent
  - Why needed here: To stabilize the multiplicative update of domain weights and ensure they remain within the probability simplex while smoothing noisy estimates.
  - Quick check question: Why is a Bregman divergence (e.g., KL divergence) used instead of a Euclidean projection when updating α(t)?

- Concept: Influence functions
  - Why needed here: To interpret the inner product of gradients as a measure of how much learning one domain helps in learning another, thereby guiding domain weighting.
  - Quick check question: What does a high value of W(t) imply about the relationship between two domains?

- Concept: Gradient cancellation effect
  - Why needed here: To filter out parameters that contribute little to learning and whose gradients can mislead the influence estimation, ensuring that W(t) is computed only on reliable gradients.
  - Quick check question: How does filtering parameters with high cancellation effect improve the discriminative power of gradient-based influence scores?

## Architecture Onboarding

- Component map: Proxy model training loop -> Domain weight optimizer (mirror descent) -> Parameter selection module -> Base model training loop -> Evaluation pipeline

- Critical path: 1. Initialize proxy model and domain weights. 2. Iterate: forward/backward pass → compute gradients → filter by cancellation → compute W(t) → update α(t) → update θ(t). 3. Average α(t) over all steps to get final domain weights. 4. Train base model with domain sampling according to final weights. 5. Evaluate on held-out tasks.

- Design tradeoffs: Small proxy model reduces computation but may not fully capture gradient structure of the large model. Parameter filtering improves gradient quality but adds a preprocessing step and reduces gradient information. Mirror descent update stabilizes training but introduces hyperparameters that require tuning.

- Failure signatures: Domain weights oscillate or collapse to a single domain → check learning rate and mirror descent regularization. Poor generalization on held-out tasks → verify that the proxy model is large enough and that cancellation filtering is not too aggressive. Training becomes unstable → check for exploding gradients and adjust step sizes.

- First 3 experiments: 1. Train proxy model with uniform weights, plot domain weight evolution, verify it stabilizes over iterations. 2. Compare average perplexity with uniform weights vs. DOGE weights on a small subset of SlimPajama. 3. Run ablation: train with all parameters vs. only low-cancellation parameters, measure impact on final domain weights and base model perplexity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can domain weights be adapted to different training phases to improve generalization?
- Basis in paper: The authors note that language models demonstrate similar learning patterns as children, acquiring linguistic skills in a systematic order, and that distinct data mixtures are required in various training phases.
- Why unresolved: The current DOGE method uses a fixed set of domain weights throughout the entire training process of the base model, which fails to adapt to different learning phases.
- What evidence would resolve it: Developing a method to detect phase transitions and design a more adaptive reweighting algorithm that adjusts domain weights dynamically during training.

### Open Question 2
- Question: How can the vulnerability to noisy domains be mitigated in the generalization estimation function?
- Basis in paper: The authors mention that the generalization estimation function is largely dependent on the gradient, and a domain with a larger magnitude of gradient would have higher scores even when its alignment with other domains is poor.
- Why unresolved: Noisy domains are likely upweighed solely due to their high gradient magnitude, which can negatively impact the model's performance.
- What evidence would resolve it: Developing a method to filter out or downweight noisy domains based on their gradient magnitude and alignment with other domains.

### Open Question 3
- Question: How can in-domain variances and fine-grained clusters be detected and utilized for domain reweighting?
- Basis in paper: The authors suggest that applying reweighting on more fine-grained domains could help detect in-domain variances and distinguish among sub-populations within one diverse domain.
- Why unresolved: Both DOGE and DOREMI assign a single weight on all samples within one domain, ignoring the potential heterogeneity within a domain.
- What evidence would resolve it: Developing a method to automatically identify and cluster sub-populations within a domain, and then apply domain reweighting at a more fine-grained level.

## Limitations

- The evaluation focuses primarily on perplexity and a limited set of reasoning tasks, with no investigation of how the learned domain weights generalize to completely unseen domains or tasks.
- The gradient cancellation effect filtering mechanism may discard informative parameters and could introduce bias in the weight estimation.
- The paper does not provide sufficient ablation studies on the impact of different divergence choices or step sizes for the mirror descent update.

## Confidence

**High confidence**: The overall two-stage framework architecture is clearly specified and reproducible. The comparison against baseline methods on SlimPajama shows consistent improvements in perplexity and few-shot reasoning accuracies.

**Medium confidence**: The mechanism by which gradient inner products capture domain inter-dependencies is theoretically sound but not empirically validated. The claim that mirror descent with Bregman divergence provides stable updates is supported by optimization theory but lacks direct experimental comparison to alternative update methods.

**Low confidence**: The assertion that DOGE effectively identifies inter-domain dependencies across diverse tasks is based on limited evaluation and may not generalize to other datasets or model architectures. The parameter selection through cancellation effect filtering is described but not thoroughly evaluated for its impact on final performance.

## Next Checks

1. **Gradient influence validation**: Conduct controlled experiments where domains are known to have specific relationships (e.g., synthetic domains with overlapping vs. disjoint vocabularies) and verify that DOGE's gradient inner product estimates correctly identify these relationships.

2. **Ablation on parameter filtering**: Train DOGE with different thresholds for the gradient cancellation effect filter and with no filtering at all, then compare the stability of domain weight evolution and final model performance to quantify the impact of this preprocessing step.

3. **Cross-domain generalization**: Evaluate the base models trained with DOGE weights on completely out-of-distribution tasks or domains not seen during proxy training to test whether the learned weights provide robust generalization beyond the SlimPajama corpus.