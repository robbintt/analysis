---
ver: rpa2
title: 'BLEURT Has Universal Translations: An Analysis of Automatic Metrics by Minimum
  Risk Training'
arxiv_id: '2307.03131'
source_url: https://arxiv.org/abs/2307.03131
tags:
- metrics
- training
- translation
- metric
- bleurt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Automatic metrics are crucial for training machine translation
  models, but they are often considered to be black boxes with potential biases that
  are difficult to detect. In this paper, we systematically analyze and compare various
  mainstream and cutting-edge automatic metrics from the perspective of their guidance
  for training machine translation systems.
---

# BLEURT Has Universal Translations: An Analysis of Automatic Metrics by Minimum Risk Training

## Quick Facts
- arXiv ID: 2307.03131
- Source URL: https://arxiv.org/abs/2307.03131
- Reference count: 10
- Automatic metrics exhibit robustness defects revealed through Minimum Risk Training, producing universal adversarial translations

## Executive Summary
This paper systematically analyzes mainstream and cutting-edge automatic metrics (BLEU, BERTScore, BARTScore, BLEURT, COMET, UniTE) for machine translation by using Minimum Risk Training (MRT) as a diagnostic tool. The key finding is that certain metrics, particularly BLEURT and BARTScore, exhibit significant robustness defects manifested as universal adversarial translations - degenerate translations that score highly under the metric but poorly under others. The analysis reveals these defects stem from distribution biases in training corpora and the tendency of metric modeling paradigms. The paper proposes solutions including ensemble metrics and incorporating token-level constraints to enhance robustness.

## Method Summary
The paper uses a two-stage training approach: first pre-training translation models with Maximum Likelihood Estimation (MLE) using NLL loss, then fine-tuning with Minimum Risk Training (MRT) using automatic metrics as loss functions. MRT generates translation candidates using beam search (beam size 12) and minimizes expected loss. The method systematically tests each metric individually, then in ensemble combinations, and finally with integrated NLL loss to assess robustness. Experiments are conducted on WMT14 En-De, LDC En-Zh, and WMT17 En-Fi datasets using Transformer Base models with BPE encoding.

## Key Results
- BLEURT and BARTScore exhibit robustness defects, producing universal adversarial translations during MRT training
- Distribution biases in training corpora (particularly hotel review patterns) contribute to metric vulnerabilities
- Ensemble metrics combining BERTScore and BLEURT improve robustness and translation quality
- Incorporating token-level constraints and NLL loss integration enhances metric robustness and model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MRT training reveals robustness defects in metrics by exposing their failure modes through training collapse.
- Mechanism: MRT uses metrics as loss functions to guide translation model training. When a metric has robustness flaws, the training process will collapse, producing degenerate translations that score highly under that metric but poorly under others.
- Core assumption: Metrics that exhibit robustness defects will cause training collapse visible as universal adversarial translations.
- Evidence anchors:
  - [abstract] "Through Minimum Risk Training (MRT), we find that certain metrics exhibit robustness defects, such as the presence of universal adversarial translations in BLEURT and BARTScore."
  - [section 2.4] "We find that MRT reveals the robustness defects in some metrics: the training collapses and the generated translations, despite getting high metric scores, show poor translation quality."
  - [corpus] Weak - corpus only shows related papers but doesn't directly support this mechanism
- Break condition: If metrics are robust enough that training doesn't collapse even when used as MRT loss functions

### Mechanism 2
- Claim: Universal adversarial translations emerge from distribution biases in training data combined with metric optimization.
- Mechanism: High-frequency patterns in translation training data become universal translations when metrics like BLEURT or BARTScore optimize for them. The metrics reward these patterns regardless of input content.
- Core assumption: Translation training corpora contain high-frequency patterns that can become universal translations when metrics are optimized.
- Evidence anchors:
  - [section 2.5.1] "Further analysis shows that the robustness defects are rooted in the distribution biases of the training corpora, as well as in the tendency of the metric modeling paradigm."
  - [section 2.5.1] "We examine the WMT14 En⇔De parallel corpora, and find that there are many sentences with similar semantics in the training set, including a large corpus of hotel reviews that are semantically similar to universal translations of BLEURT."
  - [corpus] Weak - related papers don't directly support this mechanism
- Break condition: If training corpora don't contain high-frequency patterns or if metrics are designed to be robust to such patterns

### Mechanism 3
- Claim: Ensemble metrics improve robustness by compensating for individual metric weaknesses.
- Mechanism: Combining different metrics in ensemble approach creates a more robust evaluation by leveraging complementary strengths and avoiding individual metric biases.
- Core assumption: Different metrics have different failure modes that can be compensated when combined.
- Evidence anchors:
  - [section 3.2.2] "Optimizing BERTScore alone does not change the remaining metrics much, while only optimizing BLEURT reveals robustness problems. However, optimizing the ensemble of BERTScore and BLEURT works well."
  - [section 3.2.2] "Combining two sentence-level supervised metrics can also provide a boost... the ensemble metric can build on the strengths of both metrics."
  - [corpus] Weak - related papers don't directly support this mechanism
- Break condition: If ensemble metrics share common weaknesses or if combination method doesn't properly balance different metrics

## Foundational Learning

- Concept: Minimum Risk Training (MRT)
  - Why needed here: MRT is the core method used to analyze metric robustness by using metrics as loss functions for translation model training
  - Quick check question: How does MRT differ from Maximum Likelihood Estimation in terms of loss function and training objectives?

- Concept: Universal adversarial translations
  - Why needed here: Understanding this concept is crucial for grasping how metric robustness defects manifest in practice
  - Quick check question: What characteristics make a translation a "universal adversarial translation" rather than just a bad translation?

- Concept: Metric evaluation paradigms
  - Why needed here: Different metrics use different paradigms (matching, regression, generation) which affect their robustness properties
  - Quick check question: How do the different evaluation paradigms (matching vs regression vs generation) influence metric behavior and potential robustness issues?

## Architecture Onboarding

- Component map: Translation model (Transformer-base) -> MRT training module with metric loss functions -> Metric evaluation components (BLEU, BERTScore, BARTScore, BLEURT, COMET, UniTE) -> Ensemble metric combination logic -> NLL loss integration module

- Critical path:
  1. MLE pre-training of translation model
  2. MRT fine-tuning with individual metrics
  3. Detection of training collapse and universal translations
  4. Analysis of metric behavior and robustness
  5. Implementation of improvement strategies (ensemble, NLL integration)

- Design tradeoffs:
  - MRT vs MLE: MRT can expose metric robustness issues but requires more computation and may lead to training collapse
  - Single vs ensemble metrics: Ensembles are more robust but increase complexity and computational cost
  - Word-level vs sentence-level constraints: Word-level constraints improve fine-grained control but add complexity to the training process

- Failure signatures:
  - Training collapse indicated by universal adversarial translations
  - Metrics showing extreme score drops when not being optimized
  - Translation model generating repetitive or nonsensical output
  - High-frequency decoded sentences with low entropy

- First 3 experiments:
  1. Implement MRT with a single metric (e.g., BLEURT) on a small translation task and observe for training collapse
  2. Create an ensemble of two complementary metrics (e.g., BLEURT + BERTScore) and compare training stability
  3. Add NLL loss constraint to MRT training and measure impact on translation quality and metric consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different pre-trained models used in metrics affect their robustness to adversarial examples?
- Basis in paper: [explicit] The paper mentions that metrics based on the same pre-trained model (e.g., BERTScore and BLEURT based on BERT) have higher correlations, suggesting similar evaluation criteria. It also notes that the vulnerability of BARTScore may be due to its generation-based paradigm.
- Why unresolved: The paper does not conduct a systematic comparison of metrics using different pre-trained models to isolate the effect of the model architecture on robustness.
- What evidence would resolve it: Experiments comparing metrics with identical evaluation paradigms but different pre-trained models (e.g., BERT vs. RoBERTa) on the same datasets, measuring their susceptibility to universal adversarial translations.

### Open Question 2
- Question: What is the optimal balance between MRT and NLL loss weights for different language pairs and metric combinations?
- Basis in paper: [explicit] The paper experiments with combining MRT and NLL loss, finding that λMRT = 0.6 or 0.4 gives optimal results for BLEURT on En→De. It suggests this improves training stability and metric balance.
- Why unresolved: The optimal λMRT values may vary across language pairs and metrics. The paper only tests one language pair and metric combination.
- What evidence would resolve it: Systematic experiments varying λMRT across multiple language pairs and metric combinations, measuring translation quality and metric stability to find optimal weights for each scenario.

### Open Question 3
- Question: Can incorporating subword-level or character-level information improve metric robustness beyond word-level constraints?
- Basis in paper: [inferred] The paper discusses robustness deficits due to distribution biases and suggests incorporating word-level information constraints. It mentions subword encoding is used in training.
- Why unresolved: The paper only experiments with word-level information constraints and does not explore finer-grained linguistic units.
- What evidence would resolve it: Experiments comparing metrics with word-level, subword-level, and character-level constraints during MRT training, measuring their resistance to universal adversarial translations and overall translation quality.

## Limitations

- Corpus analysis supporting distribution bias claims is weak, relying on related papers rather than direct statistical validation
- Mechanism explaining why specific metrics converge to particular universal translations remains partially speculative
- Ensemble metric approach lacks comprehensive analysis of optimal metric combinations and underlying reasons for effectiveness

## Confidence

**High Confidence**: MRT reveals training collapse with certain metrics (BLEURT, BARTScore) is well-supported by experimental results showing universal translations and metric score drops.

**Medium Confidence**: Distribution biases causing universal translations has moderate support from corpus analysis but lacks comprehensive statistical validation.

**Medium Confidence**: Ensemble metric approach showing improved robustness is supported by experiments but doesn't fully explain the underlying mechanism.

## Next Checks

1. **Corpus Pattern Validation**: Perform detailed statistical analysis of the WMT14 En⇔De corpus to quantify the frequency of hotel review-like patterns and test whether these patterns correlate with the universal translations observed during BLEURT optimization.

2. **Metric Pair Ablation**: Systematically test different metric ensemble combinations (not just BERTScore + BLEURT) to identify which pairs provide optimal robustness and understand the characteristics that make certain combinations more effective.

3. **Constraint Impact Isolation**: Conduct ablation experiments isolating the effects of token-level constraints versus NLL loss integration to determine their individual contributions to improved translation quality and metric consistency.