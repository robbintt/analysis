---
ver: rpa2
title: 'Large Language Models are Few-Shot Training Example Generators: A Case Study
  in Fallacy Recognition'
arxiv_id: '2311.09552'
source_url: https://arxiv.org/abs/2311.09552
tags:
- fallacy
- data
- examples
- shot
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of fallacy recognition, which
  is crucial for ensuring the quality and validity of arguments across various domains.
  The authors aim to enhance existing models for fallacy recognition by incorporating
  additional context and leveraging large language models to generate synthetic data,
  thus increasing the representation of infrequent classes.
---

# Large Language Models are Few-Shot Training Example Generators: A Case Study in Fallacy Recognition

## Quick Facts
- arXiv ID: 2311.09552
- Source URL: https://arxiv.org/abs/2311.09552
- Reference count: 12
- One-line primary result: Large language models can generate synthetic training examples that improve fallacy recognition models, with 1-shot prompts showing the best performance.

## Executive Summary
This paper addresses the challenge of fallacy recognition by leveraging large language models to generate synthetic training data. The authors propose using GPT3.5 to create diverse examples that increase representation of infrequent fallacy types, thereby improving model performance. Through experiments with different prompt settings (zero-shot, 1-shot, 2-shot, 5-shot), they demonstrate that synthetic data augmentation consistently improves fallacy recognition across multiple datasets and fallacy types.

## Method Summary
The authors use GPT3.5 to generate synthetic examples for five fallacy datasets (ARGOTARIO, PROPAGANDA, LOGIC, COVID, and CLIMATE) with varying prompt settings. They fine-tune a T5-3B model on original data and augmented data combinations, evaluating performance using accuracy and macro-F1 scores. The study explores both zero-shot and few-shot scenarios, analyzes the similarity between synthetic and original data using BLEURT scores, and investigates the impact of providing supplementary context for context-dependent fallacy types.

## Key Results
- 1-shot prompts generate the most effective synthetic examples, outperforming zero-shot and other few-shot settings
- Synthetic data augmentation significantly improves recognition of infrequent fallacy types
- Context inclusion shows mixed results, with some fallacy types benefiting more than others
- Consistent performance improvements observed across all datasets and fallacy types

## Why This Works (Mechanism)

### Mechanism 1
Few-shot prompts generate synthetic examples that are sufficiently different from training data while still being task-relevant, improving generalization. By providing one example per fallacy type, GPT3.5 produces novel variations rather than close copies, increasing diversity without losing task relevance. This works because diversity in synthetic examples helps the model generalize better to unseen instances of the same fallacy type.

### Mechanism 2
Synthetic data augmentation addresses class imbalance by increasing representation of infrequent fallacy types. By generating synthetic examples for underrepresented classes, the model sees more balanced training data, improving its ability to recognize rare fallacies. This works because increasing the number of examples for infrequent classes leads to better model performance on those classes.

### Mechanism 3
Context-aware prompts can improve fallacy recognition for types that require external information. Providing context (previous/next sentence) helps the model understand the broader argument structure, which is crucial for detecting certain fallacies like Strawman or Red Herring. This works because some fallacies require understanding of the broader context to be accurately identified.

## Foundational Learning

- Concept: Large Language Models as Data Generators
  - Why needed here: Understanding how LLMs can be prompted to generate task-specific synthetic data
  - Quick check question: How does the number of examples in a prompt affect the diversity and quality of generated synthetic data?

- Concept: Fallacy Recognition and Classification
  - Why needed here: Familiarity with different types of fallacies and their characteristics is crucial for understanding the task and evaluating model performance
  - Quick check question: What are some common fallacies that require external context for accurate detection?

- Concept: Class Imbalance in Machine Learning
  - Why needed here: Understanding the challenges posed by imbalanced datasets and techniques to address them, such as data augmentation
  - Quick check question: How does class imbalance affect model performance, and what are some common techniques to mitigate its effects?

## Architecture Onboarding

- Component map: GPT3.5 → Synthetic data generation → Data augmentation → T5 training → Model evaluation
- Critical path: GPT3.5 generates synthetic data → Data augmentation pipeline combines original and synthetic data → T5 model is trained on augmented data → Model evaluation using accuracy and macro-F1 scores
- Design tradeoffs: Number of examples in prompts vs. diversity of generated data; Context inclusion vs. model complexity and potential noise; Synthetic data quality vs. quantity for class balancing
- Failure signatures: Poor model performance on infrequent fallacy types despite augmentation; Synthetic data too similar to or too different from original data; Context inclusion not improving recognition of context-dependent fallacies
- First 3 experiments: 1) Compare model performance with no augmentation, zero-shot, 1-shot, 2-shot, and 5-shot settings. 2) Evaluate the impact of context inclusion on fallacy recognition for context-dependent fallacy types. 3) Analyze the similarity between original and synthetic data using BLEURT scores to understand the effect of different prompt settings.

## Open Questions the Paper Calls Out

### Open Question 1
How does the similarity between synthetic and original examples vary across different fallacy types?
- Basis in paper: The paper mentions analyzing the similarity between original and synthetic data using BLEURT scores and notes variations across different fallacy types and datasets.
- Why unresolved: While the paper provides some insights into similarity variations, a comprehensive analysis across all fallacy types and datasets is not fully explored.
- What evidence would resolve it: A detailed statistical analysis of BLEURT scores for each fallacy type across all datasets would provide a clearer understanding of similarity variations.

### Open Question 2
What is the impact of synthetic data on the model's ability to generalize to unseen fallacy schemes or domains?
- Basis in paper: The paper suggests testing the value of synthetic data on unseen fallacy schemes or domains as a potential avenue for future work.
- Why unresolved: The current study focuses on known fallacy schemes and domains, leaving the generalization capability to unseen scenarios untested.
- What evidence would resolve it: Conducting experiments with synthetic data on entirely new fallacy schemes or domains would provide evidence of the model's generalization ability.

### Open Question 3
How does the complexity of a fallacy affect the effectiveness of data augmentation in improving model performance?
- Basis in paper: The paper discusses the varying effectiveness of data augmentation across different fallacy types, such as the challenges with Strawman and Hasty Generalization.
- Why unresolved: While the paper highlights differences in effectiveness, it does not systematically analyze the relationship between fallacy complexity and augmentation success.
- What evidence would resolve it: A structured study comparing model performance improvements across fallacies of varying complexity levels would clarify this relationship.

## Limitations

- Data Quality Dependency: The effectiveness of synthetic data generation heavily relies on the quality and diversity of examples generated by GPT3.5, with uncertainty about whether synthetic data truly captures the nuances of different fallacy types.
- Context Sensitivity: Uncertainty exists about determining the optimal context window and whether provided context is always relevant and helpful for improving fallacy recognition.
- Class Imbalance Resolution: While synthetic data augmentation claims to address class imbalance, the long-term effectiveness of this approach remains uncertain across different datasets and settings.

## Confidence

- High Confidence: The overall approach of using large language models to generate synthetic data for fallacy recognition is sound and well-motivated; The use of accuracy and macro-F1 scores as evaluation metrics is appropriate for the task.
- Medium Confidence: The effectiveness of few-shot prompts in generating diverse and relevant synthetic data; The impact of context inclusion on fallacy recognition for context-dependent fallacy types.
- Low Confidence: The long-term effectiveness of synthetic data augmentation in addressing class imbalance; The quality and relevance of the synthetic data generated by GPT3.5.

## Next Checks

1. Analyze Synthetic Data Quality: Conduct a detailed analysis of the synthetic data generated by GPT3.5, including its similarity to the original data and its ability to capture the nuances of different fallacy types. This can be done by manually inspecting a sample of the synthetic data and comparing it to the original data.

2. Evaluate Context Sensitivity: Investigate the impact of context inclusion on fallacy recognition for different fallacy types. This can be done by training models with and without context for different fallacy types and comparing their performance. Additionally, analyze the optimal context window for different fallacy types.

3. Assess Long-term Effectiveness: Evaluate the long-term effectiveness of synthetic data augmentation in addressing class imbalance. This can be done by training models on different subsets of the data with varying levels of class imbalance and analyzing their performance on infrequent fallacy types.