---
ver: rpa2
title: Globally Interpretable Graph Learning via Distribution Matching
arxiv_id: '2306.10447'
source_url: https://arxiv.org/abs/2306.10447
tags:
- graphs
- interpretation
- graph
- training
- interpretive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of understanding the high-level
  patterns that Graph Neural Networks (GNNs) learn during training. The authors propose
  a novel approach called Graph Distribution Matching (GDM) to provide in-process
  global interpretation.
---

# Globally Interpretable Graph Learning via Distribution Matching

## Quick Facts
- arXiv ID: 2306.10447
- Source URL: https://arxiv.org/abs/2306.10447
- Reference count: 40
- Key outcome: Proposed GDM method achieves high utility and fidelity scores while being significantly faster than post-hoc global interpretation baselines

## Executive Summary
This paper addresses the challenge of understanding high-level patterns that Graph Neural Networks learn during training by proposing Graph Distribution Matching (GDM), an in-process global interpretation method. GDM generates interpretable graphs for each class by matching the distribution of original and interpretive graphs in the GNN's feature space as training progresses. The method is evaluated on multiple synthetic and real-world graph classification datasets, demonstrating strong performance in both quantitative metrics (utility, fidelity, efficiency) and qualitative analysis.

The proposed approach provides a valuable contribution to interpretable machine learning for graphs by offering a novel way to understand GNN behavior during training. Through distribution matching in the GNN feature space and practical constraints on interpretive graphs, GDM effectively captures discriminative patterns while maintaining interpretability. The in-process nature of the method provides more stable and less biased interpretations compared to post-hoc approaches.

## Method Summary
The GDM method generates interpretable graphs by optimizing a set of interpretive graphs to match the distribution of training graphs in the GNN's feature space. The approach starts by initializing interpretive graphs through subgraph sampling from training data, then iteratively optimizes them using distribution matching measured by Maximum Mean Discrepancy (MMD) in the GNN feature space. Practical constraints ensure the interpretive graphs have discrete structures, match edge sparsity of training graphs, and maintain feature distribution. The method is evaluated through utility (training accuracy on interpretations), fidelity (classification accuracy of pre-trained model on interpretations), and efficiency metrics.

## Key Results
- GDM achieves high utility and fidelity scores, demonstrating effective capture of informative patterns
- The method is significantly faster than post-hoc global interpretation baselines
- Qualitative analysis shows GDM can identify human-interpretable patterns matching ground-truth rules (e.g., chemical groups in MUTAG dataset)
- In-process interpretation provides more stable results compared to post-hoc methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Matching distributions between original and interpretive graphs in GNN feature space captures most informative patterns
- Mechanism: GDM optimizes interpretive graphs by minimizing MMD distance between interpretive and original data distributions in GNN feature space
- Core assumption: GNN feature extractor captures essential information about learned patterns
- Evidence anchors: Abstract states GDM matches distributions in GNN feature space; section describes optimization using MMD in embedding spaces
- Break condition: If feature extractor doesn't capture essential information or distribution matching doesn't align with learning process

### Mechanism 2
- Claim: In-process interpretation provides more stable, less biased interpretations than post-hoc methods
- Mechanism: GDM inspects whole training trajectory rather than analyzing single pre-trained model
- Core assumption: Training trajectory contains more comprehensive information than single model snapshot
- Evidence anchors: Abstract mentions in-process interpretation is less biased to single pre-trained model; section discusses training trajectory revealing more information
- Break condition: If training trajectory doesn't provide meaningful insights or model behavior isn't stable throughout training

### Mechanism 3
- Claim: Practical constraints ensure generation of solid, interpretable graph explanations
- Mechanism: Optimization includes constraints on adjacency matrix and node feature matrix for discrete structure, edge sparsity matching, and feature distribution
- Core assumption: Constraints during optimization result in interpretable and representative graphs
- Evidence anchors: Section introduces practical constraints on As and Xs; section discusses optimizing adjacency and node features
- Break condition: If constraints are too restrictive or not properly aligned with data

## Foundational Learning

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: GNNs are the core model being interpreted; understanding their structure is essential
  - Quick check question: What are the main components of a GNN, and how do they contribute to graph learning?

- Concept: Maximum Mean Discrepancy (MMD)
  - Why needed here: MMD measures distance between distributions of original and interpretive graphs
  - Quick check question: How does MMD quantify difference between two distributions, and why is it suitable for this task?

- Concept: Distribution Matching
  - Why needed here: Principle behind GDM ensuring interpretive graphs follow similar distribution as original graphs
  - Quick check question: What is the intuition behind distribution matching, and how does it help capture learned patterns?

## Architecture Onboarding

- Component map: GNN model (feature extractor + classifier) -> Interpretive graphs (optimized via distribution matching) -> Constraints (discrete structure, edge sparsity, feature distribution) -> Evaluation metrics (utility, fidelity, efficiency)

- Critical path: 1) Initialize interpretive graphs by sampling subgraphs from training data 2) Optimize interpretive graphs by minimizing MMD between their embeddings and training graphs in GNN feature space 3) Apply constraints to ensure interpretability 4) Evaluate interpretive graphs based on utility, fidelity, and efficiency

- Design tradeoffs: Balancing number of interpretive graphs per class versus interpretation quality; choosing regularization term strengths; deciding whether to update GNN during optimization

- Failure signatures: Too sparse or dense interpretive graphs (edge sparsity constraint issues); low utility/fidelity scores (patterns not captured); high variance in results (instability)

- First 3 experiments: 1) Evaluate utility by training GNN from scratch on generated graphs and measuring test performance 2) Assess fidelity by checking if pre-trained GNN correctly classifies interpretive graphs 3) Compare efficiency with post-hoc baselines by measuring generation time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does distribution matching objective in GDM compare to other potential objectives for in-process interpretation?
- Basis in paper: [explicit] Paper constructs ablation replacing distribution matching with label space matching showing worse performance
- Why unresolved: Paper demonstrates distribution matching outperforms label space matching but doesn't explore other potential objectives
- What evidence would resolve it: Experiments with different objective functions for in-process interpretation and comparing their performance

### Open Question 2
- Question: How does GDM perform when applied to dynamic graphs or graph streams?
- Basis in paper: [inferred] Paper focuses on static graph classification datasets, doesn't address dynamic graph interpretation challenges
- Why unresolved: Dynamic graphs introduce additional complexity due to changing structures and temporal dependencies
- What evidence would resolve it: Evaluating GDM on dynamic graph datasets and comparing with existing dynamic graph interpretation methods

### Open Question 3
- Question: Can GDM be extended to provide explanations for individual graph instances in addition to global patterns?
- Basis in paper: [inferred] Paper focuses on global interpretation, doesn't explore local explanations for specific instances
- Why unresolved: While global interpretation provides overall model behavior insights, local explanations help understand decision-making for individual instances
- What evidence would resolve it: Modifying GDM framework to generate both global and local interpretations and evaluating effectiveness

## Limitations

- Exact hyperparameter values for α and β (feature matching and sparsity matching strengths) are not specified, making reproduction challenging
- Specific implementation details for reparameterization trick for discrete adjacency matrices are not provided
- Generalizability to other graph learning tasks and architectures beyond tested datasets remains to be validated

## Confidence

- High confidence: Core mechanism of distribution matching in GNN feature space (Mechanism 1) is well-supported by mathematical formulation and evaluation results
- Medium confidence: In-process interpretation being more stable than post-hoc methods (Mechanism 2) is supported but could benefit from additional ablation studies
- Medium confidence: Practical constraints ensuring interpretable graphs (Mechanism 3) are theoretically sound but effectiveness depends on specific constraint values

## Next Checks

1. Reproduce experimental results by implementing GDM algorithm with same datasets (MUTAG, BA-Motif, BA-LRP) and comparing utility and fidelity scores against reported baselines

2. Conduct ablation studies to quantify impact of each constraint (discrete structure, edge sparsity, feature distribution) on interpretation quality

3. Test generalizability by applying GDM to additional graph classification datasets and different GNN architectures to assess performance beyond specific experimental setup