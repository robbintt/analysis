---
ver: rpa2
title: Textbooks Are All You Need
arxiv_id: '2306.11644'
source_url: https://arxiv.org/abs/2306.11644
tags:
- self
- list
- arxiv
- dataset
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces phi-1, a small language model for code that
  achieves strong performance on coding benchmarks despite its small size. The key
  innovation is training on high-quality "textbook" data, including synthetic textbooks
  and exercises, rather than the usual large web code datasets.
---

# Textbooks Are All You Need

## Quick Facts
- arXiv ID: 2306.11644
- Source URL: https://arxiv.org/abs/2306.11644
- Reference count: 31
- Primary result: 1.3B parameter phi-1 achieves 50.6% pass@1 accuracy on HumanEval using high-quality synthetic data

## Executive Summary
This paper introduces phi-1, a small language model for code that achieves strong performance on coding benchmarks despite its small size. The key innovation is training on high-quality "textbook" data, including synthetic textbooks and exercises, rather than the usual large web code datasets. Phi-1 attains 50.6% pass@1 accuracy on HumanEval and 55.5% on MBPP, outperforming much larger models. The authors show that finetuning on coding exercises leads to emergent capabilities like using external libraries and improved reasoning. Data pruning experiments confirm that the strong HumanEval performance is not due to dataset contamination. Overall, this work demonstrates the power of high-quality data in training efficient and capable code models.

## Method Summary
The authors trained phi-1, a 1.3B parameter Transformer decoder, using a two-stage approach. First, they pretrain on CodeTextbook (6B tokens of filtered Python code + 1B tokens of synthetic textbooks) for 36,000 steps. Then they finetune on CodeExercises (180M tokens of synthetic coding exercises) for 6,000 steps. The synthetic data is generated using GPT-3.5 with constrained prompts to ensure diversity. The model uses a standard decoder-only architecture with FlashAttention, rotary position embeddings, and a CodeGen-350M-mono tokenizer.

## Key Results
- Phi-1 achieves 50.6% pass@1 accuracy on HumanEval and 55.5% on MBPP, outperforming much larger models
- Finetuning on synthetic exercises unlocks emergent capabilities like using external libraries (PyGame, Tkinter) not present in the finetuning data
- Data pruning experiments show performance gains are not due to dataset contamination, with phi-1 maintaining strong results after removing 40% of similar exercises

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: High-quality synthetic data generation using GPT-3.5 can create diverse, non-repetitive coding exercises that outperform standard web-scraped code datasets.
- **Mechanism**: By constraining GPT-3.5 with randomized topics and target audiences during generation, the model produces a more diverse distribution of coding concepts and solutions compared to simply prompting for exercises. This diversity reduces overfitting and improves generalization.
- **Core assumption**: GPT-3.5 can generate high-quality, diverse coding exercises when given appropriate constraints that prevent repetitive output patterns.
- **Evidence anchors**:
  - [abstract] states phi-1 attains 50.6% pass@1 accuracy on HumanEval and 55.5% on MBPP, outperforming much larger models.
  - [section 2.2] describes the synthetic textbook dataset generation process and its focus on diversity through constrained prompts.
  - [corpus] shows related work on high-quality data pretraining, though specific evidence for GPT-3.5 generation diversity is limited.
- **Break condition**: If GPT-3.5-generated data becomes too homogeneous or contains systematic errors that propagate to phi-1, the performance advantage would diminish.

### Mechanism 2
- **Claim**: Finetuning on a small dataset of synthetic coding exercises (180M tokens) can unlock emergent capabilities not present in the base model.
- **Mechanism**: The finetuning process helps the model reorganize and consolidate knowledge acquired during pretraining, enabling it to use external libraries (like PyGame and Tkinter) and handle more complex algorithmic tasks despite these not being explicitly present in the finetuning data.
- **Core assumption**: The base model contains latent knowledge that can be activated through finetuning, even when the finetuning data doesn't directly cover the target capabilities.
- **Evidence anchors**:
  - [section 3] provides multiple examples where phi-1 after finetuning successfully uses external libraries and handles complex tasks that phi-1-base cannot.
  - [abstract] notes phi-1 displays "surprising emergent properties compared to phi-1-base."
  - [corpus] lacks direct evidence for this specific emergent behavior mechanism.
- **Break condition**: If the emergent capabilities are actually due to data contamination or if the base model lacks the necessary latent knowledge, finetuning would not produce these improvements.

### Mechanism 3
- **Claim**: Aggressive data pruning based on embedding and AST similarity can demonstrate that performance gains are not due to dataset contamination.
- **Mechanism**: By removing exercises from the finetuning dataset that are similar to HumanEval problems using embedding distance and AST edit distance, the model can still achieve high performance, proving the gains come from genuine learning rather than memorization.
- **Core assumption**: The pruning process effectively removes contaminated data while preserving the beneficial aspects of the dataset that contribute to learning.
- **Evidence anchors**:
  - [section 5.2] describes the pruning methodology and shows phi-1 still outperforms StarCoder even after removing 40% of the CodeExercises dataset.
  - [table 3] demonstrates that phi-1 maintains strong performance on non-similar HumanEval problems after pruning.
  - [corpus] shows related work on data quality but limited evidence for this specific pruning approach.
- **Break condition**: If the pruning process is too aggressive and removes beneficial data, or if the similarity metrics fail to capture true contamination, the performance gains could be artifacts of the methodology.

## Foundational Learning

- **Concept**: Transformer architecture and attention mechanisms
  - Why needed here: The paper uses a standard Transformer-based model with FlashAttention implementation, so understanding attention mechanisms and their implementation is crucial for replicating or extending the work.
  - Quick check question: How does FlashAttention improve the efficiency of multi-head attention compared to standard implementations?

- **Concept**: Data quality and diversity in training datasets
  - Why needed here: The core contribution relies on curating high-quality synthetic data rather than using standard web-scraped code, making it essential to understand what constitutes "textbook quality" data and how to achieve diversity in synthetic generation.
  - Quick check question: What specific constraints did the authors use during GPT-3.5 generation to ensure diversity in the synthetic textbook dataset?

- **Concept**: Emergent capabilities in language models
  - Why needed here: The paper demonstrates that finetuning on a small dataset can unlock capabilities not present in the base model, which requires understanding the phenomenon of emergent behaviors in LLMs and how they relate to model size and training approaches.
  - Quick check question: What evidence does the paper provide that the emergent capabilities (like using external libraries) are not simply due to data contamination?

## Architecture Onboarding

- **Component map**: Transformer decoder with 24 layers, hidden dimension 2048, MLP-inner dimension 8192, 32 attention heads -> FlashAttention implementation -> Rotary position embedding with dimension 32 -> Code-specific tokenizer from CodeGen-350M-mono -> AdamW optimizer with fp16 training
- **Critical path**: Data generation → Base model pretraining (CodeTextbook) → Finetuning (CodeExercises) → Evaluation on HumanEval/MBPP
- **Design tradeoffs**: Small model size (1.3B parameters) vs. performance; synthetic data generation cost vs. dataset quality; aggressive data pruning vs. potential loss of beneficial training signals
- **Failure signatures**:
  - If performance degrades significantly after data pruning, contamination might be the primary source of gains
  - If emergent capabilities disappear when using different finetuning data, the improvements might be dataset-specific
  - If synthetic data generation produces too much repetitive content, diversity benefits will be lost
- **First 3 experiments**:
  1. Generate a small synthetic textbook dataset with different constraint parameters and measure diversity metrics
  2. Train phi-1-base with varying amounts of CodeTextbook data to find optimal data quality vs. quantity tradeoff
  3. Apply different similarity thresholds in the pruning process and measure impact on HumanEval performance to validate contamination analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much can model performance improve by using GPT-4 instead of GPT-3.5 to generate the synthetic training data?
- Basis in paper: Explicit
- Why unresolved: The paper only used GPT-3.5 for data generation and noted that the generated data had a high error rate. It's unclear how much of a performance boost could be gained by using the higher quality GPT-4.
- What evidence would resolve it: Train phi-1 using GPT-4 generated synthetic data instead of GPT-3.5 and compare performance on benchmarks like HumanEval.

### Open Question 2
- Question: What is the minimum model size and dataset size needed to achieve strong coding performance, and how do they scale?
- Basis in paper: Inferred
- Why unresolved: The paper showed that phi-1, a 1.3B parameter model trained on 7B tokens, achieved strong results. But it's unclear if even smaller models and datasets could suffice, and how performance scales with size.
- What evidence would resolve it: Train models of various sizes (e.g. 100M, 500M, 1B parameters) on datasets of different sizes (e.g. 1B, 3B, 7B tokens) and measure coding performance.

### Open Question 3
- Question: How much does the quality of the data matter vs the size of the dataset?
- Basis in paper: Explicit
- Why unresolved: The paper emphasized the importance of high-quality "textbook" data, but it's unclear if this matters more than simply having a large dataset.
- What evidence would resolve it: Train models on datasets of varying quality (e.g. filtered vs unfiltered code) but the same size, and compare performance.

### Open Question 4
- Question: How robust are the model's emergent capabilities to different finetuning datasets and methods?
- Basis in paper: Inferred
- Why unresolved: The paper showed that finetuning on CodeExercises led to emergent capabilities like using external libraries. But it's unclear if these capabilities are robust to different finetuning approaches.
- What evidence would resolve it: Finetune models using different datasets and methods (e.g. different datasets, different data augmentation) and measure the presence of emergent capabilities.

### Open Question 5
- Question: What are the limitations of phi-1 and how can they be addressed?
- Basis in paper: Explicit
- Why unresolved: The paper discussed several limitations of phi-1 like sensitivity to prompts and lack of robustness. But it's unclear the extent of these limitations and how they could be overcome.
- What evidence would resolve it: Conduct a thorough analysis of phi-1's limitations by testing it on a wide range of tasks and inputs, and explore methods to mitigate the limitations.

## Limitations
- Data generation bottleneck: The paper relies heavily on GPT-3.5 for synthetic data generation, which may not be accessible to all researchers and limits reproducibility
- Limited generalizability: The 1.3B parameter size may not scale well to more complex coding tasks that benefit from larger models
- Uncertainty about emergent capabilities: While pruning experiments provide some validation, it's difficult to fully rule out data contamination as the source of emergent behaviors

## Confidence
- Data quality impact: High confidence - Strong empirical evidence from pruning experiments
- Synthetic data generation diversity: Medium confidence - Reasonable approach but limited evidence for GPT-3.5 diversity claims
- Emergent capability mechanism: Low confidence - Difficult to fully validate that improvements aren't from contamination

## Next Checks
1. Replicate the synthetic data generation process with different GPT-3.5 prompts and constraints to verify that diversity gains are robust to generation parameters
2. Apply the same data pruning methodology to other code models (like StarCoder) to determine if the contamination detection approach generalizes beyond phi-1
3. Test phi-1 on additional benchmarks beyond HumanEval and MBPP, particularly on tasks requiring complex library usage or algorithmic reasoning not represented in the training data