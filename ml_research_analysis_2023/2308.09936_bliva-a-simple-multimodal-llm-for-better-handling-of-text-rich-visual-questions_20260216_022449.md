---
ver: rpa2
title: 'BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions'
arxiv_id: '2308.09936'
source_url: https://arxiv.org/abs/2308.09936
tags:
- visual
- embeddings
- image
- bliv
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BLIVA addresses the challenge of interpreting text-rich visual
  content by enhancing multimodal LLMs. It combines learned query embeddings from
  InstructBLIP with encoded patch embeddings projected into the LLM, inspired by LLaVA.
---

# BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions

## Quick Facts
- arXiv ID: 2308.09936
- Source URL: https://arxiv.org/abs/2308.09936
- Reference count: 33
- Primary result: Dual-encoder multimodal LLM achieving up to 17.76% improvement on OCR-VQA and strong zero-shot performance on text-rich visual tasks

## Executive Summary
BLIVA addresses the challenge of interpreting text-rich visual content by enhancing multimodal LLMs with a dual-encoder approach. It combines learned query embeddings from InstructBLIP with encoded patch embeddings projected into the LLM, inspired by LLaVA. This design captures both global context and local visual detail missed by single encoder designs. Evaluated on OCR-VQA, Visual Spatial Reasoning, and MME benchmarks, BLIVA shows significant gains: up to 17.76% on OCR-VQA, 7.9% on Visual Spatial Reasoning, and 17.72% overall on MME compared to InstructBLIP.

## Method Summary
BLIVA leverages both learned query embeddings from InstructBLIP's Q-former and encoded patch embeddings from a frozen vision encoder (ViT-G/14). A projection layer maps patch embeddings to the LLM space, and both embeddings are concatenated with the question text as input to the frozen LLM (Vicuna-7B). The model follows a two-stage training paradigm: first pre-training the projection layer on image-text pairs, then fine-tuning the Q-former and projection layer while keeping the vision encoder and LLM frozen to prevent catastrophic forgetting.

## Key Results
- 17.76% improvement on OCR-VQA benchmark over InstructBLIP
- 7.9% improvement on Visual Spatial Reasoning benchmark
- 17.72% overall improvement on MME benchmark
- Strong zero-shot performance on general VQA tasks and real-world YouTube thumbnails

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-encoder approach (query embeddings + patch embeddings) captures both global context and local visual detail missed by single encoder designs.
- Mechanism: Query embeddings (from Q-former) are instruction-aware, but limited in spatial resolution; patch embeddings provide pixel-level information. Concatenating both allows the LLM to attend to fine-grained text and visual cues.
- Core assumption: The LLM can integrate multimodal embeddings of different semantic granularities without catastrophic interference.
- Evidence anchors: [abstract] "incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM"; [section] "We present BLIVA, which leverages both learned query embeddings and encoded patch embeddings"

### Mechanism 2
- Claim: Two-stage training (pre-training + instruction tuning) aligns vision encoder and LLM without forgetting prior knowledge.
- Mechanism: First stage aligns vision encoder and LLM using image-text pairs; second stage fine-tunes Q-former and projection layers while keeping vision encoder and LLM frozen to preserve learned representations.
- Core assumption: Freezing the vision encoder and LLM during fine-tuning prevents catastrophic forgetting and maintains alignment.
- Evidence anchors: [abstract] "two-stage training paradigm, keeping the vision encoder and LLM frozen while fine-tuning projection layers"; [section] "unfreezing the vision encoder results in catastrophic forgetting of prior knowledge"

### Mechanism 3
- Claim: Using instruction tuning data in second stage refines visual embeddings to respond to human questions, improving zero-shot performance.
- Mechanism: Instruction tuning data (image-based QA pairs) teaches the model to map visual features to answer generation, not just captioning.
- Core assumption: The model generalizes from instruction tuning data to unseen tasks without task-specific fine-tuning.
- Evidence anchors: [abstract] "Empirical evidence demonstrates that our model, BLIVA, significantly enhances performance in processing text-rich VQA benchmarks"; [section] "After pre-training, the LLM becomes familiar with the visual embedding space and can generate descriptions of images. However, it still lacks the capability to discern the finer details of images and respond to human questions"

## Foundational Learning

- Concept: Multimodal embeddings and attention mechanisms
  - Why needed here: BLIVA concatenates query and patch embeddings and feeds them into an LLM that uses cross-attention to fuse vision and language. Understanding how attention operates on multimodal inputs is key to grasping why the dual-encoder helps.
  - Quick check question: What happens to the relative weighting of query vs. patch embeddings in the cross-attention if one is much higher-dimensional?

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The paper freezes the vision encoder and LLM during fine-tuning to avoid losing prior knowledge. Knowing how fine-tuning can overwrite earlier representations explains why this design choice is critical.
  - Quick check question: If you unfreeze the vision encoder during fine-tuning, what specific degradation would you expect to see on OCR-VQA vs. general VQA?

- Concept: Instruction tuning and zero-shot generalization
  - Why needed here: BLIVA uses instruction tuning data to improve its ability to answer questions about images without task-specific training. Understanding how instruction tuning works helps explain why zero-shot results improve.
  - Quick check question: Why might instruction tuning improve performance on text-rich images more than on general images?

## Architecture Onboarding

- Component map: Image → Vision Encoder (frozen ViT-G/14) → Encoded Patch Embeddings → Projection Layer (fine-tuned) → Concatenation → LLM (frozen Vicuna-7B) ← Q-former (fine-tuned) → Learned Query Embeddings + Question Text → Answer
- Critical path: Image → Vision Encoder → Patch Embeddings → Projection Layer → Concatenation → LLM → Answer
- Design tradeoffs:
  - Freezing vision encoder and LLM simplifies training and avoids forgetting, but limits end-to-end adaptation.
  - Dual-encoder adds complexity and memory overhead but captures richer visual detail.
  - Using a frozen LLM assumes it already has sufficient language capability, which may not hold for niche domains.
- Failure signatures:
  - Patch embeddings ignored: Check if projection layer weights are too small or if attention weights heavily favor query embeddings.
  - Catastrophic forgetting: Monitor performance drop on original InstructBLIP tasks after fine-tuning.
  - Poor zero-shot generalization: Evaluate on held-out tasks not in instruction tuning data.
- First 3 experiments:
  1. Ablation: Remove patch embeddings → observe drop on text-rich tasks but stability on general VQA.
  2. Freeze/unfreeze vision encoder → measure catastrophic forgetting on OCR-VQA.
  3. Compare instruction tuning vs. captioning-only pre-training → evaluate zero-shot gains on new QA datasets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different image resolutions impact BLIVA's performance on text-rich VQA tasks, particularly for small text characters?
- Basis in paper: [inferred] The paper mentions that DocVQA is particularly challenging due to "documents with minuscule text characters, which become further illegible when the image resolution is down-scaled to 224."
- Why unresolved: The paper only evaluates at 224x224 resolution and doesn't explore the impact of varying resolutions on performance.
- What evidence would resolve it: A systematic study comparing BLIVA's performance across different image resolutions (e.g., 224x224, 336x336, 448x448) on text-rich benchmarks like DocVQA and OCR-VQA.

### Open Question 2
- Question: Would unfreezing and fine-tuning the vision encoder improve BLIVA's performance on text-rich VQA tasks, and if so, under what conditions?
- Basis in paper: [explicit] The authors state they keep the vision encoder frozen because "unfreezing the vision encoder results in catastrophic forgetting of prior knowledge."
- Why unresolved: The paper only tests keeping the vision encoder frozen and doesn't explore whether fine-tuning could provide benefits without catastrophic forgetting.
- What evidence would resolve it: Experiments comparing BLIVA with and without vision encoder fine-tuning, potentially using techniques like adapter layers or low-rank adaptation to mitigate forgetting.

### Open Question 3
- Question: How does BLIVA's performance on text-rich VQA tasks compare to OCR-specialized models when both have access to OCR tokens?
- Basis in paper: [inferred] The paper notes that BLIVA outperforms LLaVA-R on OCR tasks despite LLaVA-R using additional OCR training data, but doesn't compare against models using OCR tokens as input.
- Why unresolved: The paper focuses on "OCR-free" methods and doesn't evaluate BLIVA against models that explicitly use OCR-extracted text as input.
- What evidence would resolve it: A direct comparison of BLIVA against state-of-the-art OCR-token-based models on benchmarks like TextVQA and ST-VQA.

### Open Question 4
- Question: What is the impact of using different types of patch embeddings (e.g., from different vision transformers) on BLIVA's performance?
- Basis in paper: [explicit] The authors mention they use "ViT-G/14 from EV A-CLIP" as their vision encoder and discuss the potential of exploring "more visual embeddings."
- Why unresolved: The paper only tests one specific vision transformer architecture and doesn't explore how different architectures might affect performance.
- What evidence would resolve it: A comparison of BLIVA's performance using different vision transformer architectures (e.g., ViT, Swin, ConvNeXt) as the image encoder.

### Open Question 5
- Question: How does BLIVA's two-stage training approach compare to a single-stage training approach in terms of final performance and training efficiency?
- Basis in paper: [explicit] The authors mention they use a two-stage training approach and note that "PandaGPT, which utilizes a one-stage training method, has also demonstrated commendable results."
- Why unresolved: The paper only evaluates the two-stage approach and doesn't compare it directly to a one-stage approach.
- What evidence would resolve it: A direct comparison of BLIVA's two-stage training against a one-stage training approach, measuring both final performance and total training time.

## Limitations
- Dual-encoder design increases memory and computational overhead compared to single-encoder approaches
- Freezing vision encoder and LLM prevents end-to-end optimization and may limit adaptation to specific tasks
- Projection layer connecting patch embeddings to LLM is not fully specified, impacting reproducibility

## Confidence
- **High confidence**: Dual-encoder mechanism for capturing global context and local detail (17.76% improvement on OCR-VQA)
- **Medium confidence**: Two-stage training approach and freezing strategy (limited ablation studies)
- **Medium confidence**: Instruction tuning benefits for zero-shot generalization (strong performance but limited held-out evaluation)

## Next Checks
1. **Ablation Study**: Remove the patch embedding pathway and retrain BLIVA to quantify the exact contribution of the dual-encoder approach on text-rich vs. general VQA tasks.
2. **Catastrophic Forgetting Test**: Unfreeze the vision encoder during fine-tuning and measure performance degradation on OCR-VQA and general VQA to validate the freezing strategy.
3. **Zero-Shot Generalization**: Evaluate BLIVA on a held-out visual QA dataset not included in the instruction tuning data to assess true zero-shot capabilities beyond reported benchmarks.