---
ver: rpa2
title: Learning Minimalistic Tsetlin Machine Clauses with Markov Boundary-Guided Pruning
arxiv_id: '2309.06315'
source_url: https://arxiv.org/abs/2309.06315
tags:
- markov
- boundary
- clauses
- probability
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Type III Feedback for Tsetlin Machines (TMs)
  to prune literals outside a clause's context-specific Markov boundary. The method
  uses a Context-Specific Independence Automaton (CS-IA) to identify and remove conditionally
  independent features during learning.
---

# Learning Minimalistic Tsetlin Machine Clauses with Markov Boundary-Guided Pruning

## Quick Facts
- **arXiv ID**: 2309.06315
- **Source URL**: https://arxiv.org/abs/2309.06315
- **Reference count**: 18
- **Primary result**: Reduces average clause size from ~40 to under 5 literals while maintaining accuracy

## Executive Summary
This paper introduces Type III Feedback for Tsetlin Machines (TMs) to prune literals outside a clause's context-specific Markov boundary. The method uses a Context-Specific Independence Automaton (CS-IA) to identify and remove conditionally independent features during learning. Applied to MNIST, the approach reduces average clause size from ~40 to under 5 literals while maintaining accuracy. On a toy Bayesian network, the CS-IA successfully isolates the Markov boundary, reducing clauses containing noise from 80% to 0% after sufficient training. Theoretical analysis proves convergence to optimal clauses when the Markov boundary is correctly identified.

## Method Summary
The paper introduces Type III Feedback to complement Type I and II Feedback in Tsetlin Machines. The Context-Specific Independence Automaton (CS-IA) implements this feedback by waiting for two scenarios: (1) the clause evaluates to 1 with the literal present, and (2) the clause evaluates to 1 with the literal removed. If the prediction target Y is the same in both scenarios, the literal is conditionally independent and can be pruned. The CS-IA updates its state based on these observations, eventually converging to clauses containing only Markov boundary variables. The approach is tested on both MNIST and a toy Bayesian network with known Markov boundaries.

## Key Results
- Clause size reduced from ~40 to under 5 literals on MNIST while maintaining accuracy
- CS-IA successfully identified Markov boundary in toy Bayesian network, reducing noise-containing clauses from 80% to 0%
- Theoretical analysis proves convergence to optimal clauses when Markov boundary is correctly identified

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CS-IA successfully identifies literals outside the Markov boundary by exploiting context-specific independence
- Mechanism: The CS-IA waits for two scenarios: (1) the clause evaluates to 1 with the literal present, and (2) the clause evaluates to 1 with the literal removed. If the prediction target Y is the same in both scenarios, the literal is conditionally independent and can be pruned
- Core assumption: Context-specific independence can be reliably detected through repeated scenario comparison during online learning
- Evidence anchors:
  - [abstract] "The automaton learns which features are outside the Markov boundary of the target, allowing them to be pruned from the TM during learning"
  - [section II-B] "If they are equal, Scenario 1 and Scenario 2 will look the same when it comes to predicting Y"
  - [corpus] Weak - no direct comparison to other pruning methods in neighbors

### Mechanism 2
- Claim: Type III Feedback complements Type I and II Feedback to create more efficient and interpretable clauses
- Mechanism: While Type I and II Feedback build descriptive and discriminative clauses, Type III Feedback prunes unnecessary literals, reducing clause size from ~40 to under 5 literals while maintaining accuracy
- Core assumption: The combination of all three feedback types leads to optimal clause structure
- Evidence anchors:
  - [abstract] "The method uses a Context-Specific Independence Automaton (CS-IA) to identify and remove conditionally independent features during learning"
  - [section II-A] "Type III Feedback interacts with Type I and Type II Feedback, with the goal of isolating the mechanism that governs the target variable"
  - [corpus] Weak - no direct evidence of feedback interaction in neighbors

### Mechanism 3
- Claim: The theoretical analysis guarantees convergence to optimal clauses when the Markov boundary is correctly identified
- Mechanism: The formal proof shows that under certain conditions (balanced dataset, small d parameter), the system will almost surely converge to clauses containing only Markov boundary variables
- Core assumption: The mathematical conditions for convergence are met in practical applications
- Evidence anchors:
  - [abstract] "Theoretical analysis proves convergence to optimal clauses when the Markov boundary is correctly identified"
  - [section IV] "if the system configuration is correct... the system will almost surely converge to the correct clause with proper pruning and keeping"
  - [corpus] Weak - no mention of theoretical guarantees in neighbors

## Foundational Learning

- Concept: Markov Blanket and Markov Boundary
  - Why needed here: Understanding that a Markov boundary contains all information needed to predict a target variable, and variables outside it are conditionally independent
  - Quick check question: If variable X8 is outside the Markov boundary of Y given X1-X7, what is the relationship between P(Y|X1,X2,...,X7,X8) and P(Y|X1,X2,...,X7)?

- Concept: Context-Specific Independence
  - Why needed here: Recognizing that independence can hold only for specific values of other variables, allowing more granular pruning
  - Quick check question: How does context-specific independence differ from regular conditional independence in terms of when the independence relationship holds?

- Concept: Bayesian Networks (BN) and Directed Acyclic Graphs (DAG)
  - Why needed here: Understanding how BNs represent dependencies and how TM clauses can be interpreted from a BN perspective
  - Quick check question: In a BN, if node X8 is d-separated from node Y given nodes X1-X7, what does this imply about the conditional independence of X8 and Y?

## Architecture Onboarding

- Component map:
  - Tsetlin Automata -> Type I and II Feedback
  - CS-IA -> Type III Feedback for pruning
  - Clause Pool -> Collection of AND-rules representing patterns
  - Feedback Integration Layer -> Coordinates the three feedback types

- Critical path:
  1. Clause evaluation produces Y prediction
  2. CS-IA waits for specific scenarios to assess independence
  3. CS-IA updates state and potentially prunes literals
  4. Type I/II Feedback continues building discriminative clauses

- Design tradeoffs:
  - Higher d parameter (more conservative) vs. lower d (more aggressive pruning)
  - More states in CS-IA (better detection) vs. fewer states (faster learning)
  - Larger clause pool (better coverage) vs. smaller pool (faster evaluation)

- Failure signatures:
  - Over-pruning: Accuracy drops significantly after initial improvement
  - Under-pruning: Clause size remains large with minimal reduction
  - Oscillation: CS-IA repeatedly adds and removes the same literal
  - Stalling: No change in clause structure despite continued training

- First 3 experiments:
  1. Run MNIST classification with Type III Feedback enabled, measure clause size reduction and accuracy impact
  2. Test toy Bayesian network with known Markov boundary, verify CS-IA correctly identifies and prunes non-boundary variables
  3. Perform ablation study: compare results with only Type I/II Feedback vs. all three feedback types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Type III Feedback be optimized for different datasets beyond MNIST and the toy Bayesian network to ensure consistent performance improvements?
- Basis in paper: [explicit] The paper demonstrates the effectiveness of Type III Feedback on MNIST and a toy Bayesian network, suggesting potential applicability to other datasets.
- Why unresolved: The paper does not explore the adaptability of Type III Feedback to diverse datasets, which is crucial for understanding its generalizability.
- What evidence would resolve it: Empirical studies on various datasets with different characteristics, showing consistent improvements in clause efficiency and accuracy with Type III Feedback.

### Open Question 2
- Question: What are the computational trade-offs between the increased complexity of the Context-Specific Independence Automaton (CS-IA) and the benefits of reduced clause sizes?
- Basis in paper: [inferred] The introduction of the CS-IA adds complexity to the Tsetlin Machine's learning process, but the paper does not discuss the computational cost-benefit analysis.
- Why unresolved: The paper focuses on the effectiveness of pruning literals but does not address the computational resources required for the CS-IA.
- What evidence would resolve it: A detailed analysis comparing the computational overhead of the CS-IA with the performance gains in terms of accuracy and interpretability.

### Open Question 3
- Question: Can the integration of Markov boundary-guided pruning with other machine learning models enhance their interpretability and efficiency?
- Basis in paper: [explicit] The paper connects Tsetlin Machines with Bayesian Networks, suggesting potential synergies in inference and learning.
- Why unresolved: The paper does not explore the integration of Markov boundary-guided pruning with other machine learning models.
- What evidence would resolve it: Experimental studies showing improved interpretability and efficiency in other models when incorporating Markov boundary-guided pruning techniques.

## Limitations

- Theoretical convergence guarantees rely on specific conditions (balanced dataset, appropriate d parameter) that may not hold in real-world scenarios
- Implementation details of CS-IA state transitions and scenario waiting strategy are somewhat abstract
- Paper doesn't provide comprehensive ablation studies showing individual contribution of Type III Feedback

## Confidence

- **High Confidence**: The empirical results showing clause size reduction from ~40 to under 5 literals on MNIST, and the successful identification of Markov boundaries in the toy Bayesian network
- **Medium Confidence**: The theoretical analysis of convergence guarantees, as it depends on assumptions that may not fully translate to practical applications
- **Medium Confidence**: The mechanism by which CS-IA identifies context-specific independence through scenario comparison, as the waiting strategy's effectiveness depends on proper hyperparameter tuning

## Next Checks

1. **Ablation Study**: Run experiments isolating Type III Feedback to measure its individual contribution versus the combined three-feedback approach
2. **Robustness Testing**: Evaluate performance across datasets with varying class imbalances and feature correlations to test the theoretical assumptions
3. **State Transition Analysis**: Instrument the CS-IA to log state transitions and scenario comparisons, verifying that the independence detection mechanism works as intended across different data distributions