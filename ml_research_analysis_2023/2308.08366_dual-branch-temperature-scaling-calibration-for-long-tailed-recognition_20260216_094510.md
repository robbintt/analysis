---
ver: rpa2
title: Dual-Branch Temperature Scaling Calibration for Long-Tailed Recognition
arxiv_id: '2308.08366'
source_url: https://arxiv.org/abs/2308.08366
tags:
- calibration
- temperature
- samples
- each
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses miscalibration in deep neural networks, particularly
  under long-tailed data distributions where minority and majority classes exhibit
  different confidence levels, leading to overconfidence. To tackle this, the authors
  propose a dual-branch temperature scaling calibration model (Dual-TS) that integrates
  class-adaptive temperature scaling (CA-TS) and equal size bin temperature scaling
  (Esbin-TS).
---

# Dual-Branch Temperature Scaling Calibration for Long-Tailed Recognition

## Quick Facts
- arXiv ID: 2308.08366
- Source URL: https://arxiv.org/abs/2308.08366
- Authors: 
- Reference count: 5
- Key outcome: Dual-TS achieves state-of-the-art calibration performance on long-tailed CIFAR datasets, improving ECE and accuracy compared to existing methods.

## Executive Summary
This paper addresses miscalibration in deep neural networks under long-tailed data distributions, where minority and majority classes exhibit different confidence levels leading to overconfidence. The authors propose a dual-branch temperature scaling calibration model (Dual-TS) that combines class-adaptive temperature scaling (CA-TS) and equal size bin temperature scaling (Esbin-TS). Experiments on CIFAR-10 and CIFAR-100 with long-tailed distributions demonstrate that Dual-TS achieves state-of-the-art calibration performance, significantly reducing ECE and improving accuracy compared to existing calibration methods.

## Method Summary
The Dual-TS method employs two temperature scaling branches: CA-TS assigns class-specific temperature parameters to address overconfidence in head classes, while Esbin-TS uses equal-sized confidence bins to provide more robust temperature estimates for tail classes. These temperature parameters are fused through a weighted geometric mean to produce calibrated probabilities. The model is trained using a two-stage approach: first training a baseline classifier, then optimizing temperature parameters using NLL loss with LBFGS. The authors also introduce Esbin-ECE, a new calibration metric that addresses limitations of traditional ECE under long-tailed distributions.

## Key Results
- Dual-TS achieves significant improvements in calibration accuracy and ECE reduction on CIFAR-10 and CIFAR-100 long-tailed datasets
- The dual-branch architecture outperforms both single-branch CA-TS and Esbin-TS individually
- Dual-TS maintains competitive classification accuracy while improving calibration metrics
- Esbin-ECE provides more reliable evaluation of calibration performance in long-tailed settings compared to traditional ECE

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Class-Adaptive Temperature Scaling (CA-TS) addresses miscalibration by assigning different temperature parameters to different classes, compensating for the overconfidence bias toward head classes in long-tailed distributions.
- Mechanism: For each class, CA-TS learns a temperature parameter that scales the logits of all samples belonging to that class. Higher temperature parameters are assigned to head classes to reduce overconfidence, while lower parameters are used for tail classes.
- Core assumption: The degree of overconfidence correlates with the class imbalance ratio and can be captured by learning class-specific temperature parameters.
- Evidence anchors:
  - [abstract] "some current research have designed diverse temperature coefficients for different categories based on temperature scaling (TS) method"
  - [section] "The theoretical temperature coefficient for each class, which is the average logit divided by how much it equals the accuracy rate ratio"
  - [corpus] Weak or missing - no direct evidence from neighbors about CA-TS mechanism
- Break condition: If the learned temperature parameters for minority classes are not generalizable due to insufficient samples, leading to poor calibration generalization.

### Mechanism 2
- Claim: Equal Size Bin Temperature Scaling (Esbin-TS) mitigates the lack of generalizability of temperature parameters for rare samples by grouping samples into equal-sized bins based on confidence.
- Mechanism: Samples are sorted by confidence and divided into bins with equal sample sizes. Each bin is assigned a temperature parameter, allowing more robust temperature estimation for tail classes by leveraging larger, more representative sample groups.
- Core assumption: Samples with similar confidence levels share similar miscalibration patterns, and grouping them into equal-sized bins provides more stable temperature estimates.
- Evidence anchors:
  - [abstract] "we noticed that the traditional calibration evaluation metric, Excepted Calibration Error (ECE), gives a higher weight to low-confidence samples in the minority classes, which leads to inaccurate evaluation of model calibration"
  - [section] "we propose Equal Size Bin Temperature Scaling(Esbin-TS), which divides all samples into intervals based on confidence and trains a temperature coefficient for each interval"
  - [corpus] Weak or missing - no direct evidence from neighbors about Esbin-TS mechanism
- Break condition: If the confidence-based binning does not capture meaningful patterns of miscalibration, leading to ineffective temperature scaling.

### Mechanism 3
- Claim: The dual-branch architecture of Dual-TS combines the strengths of CA-TS and Esbin-TS by fusing their temperature parameters through a weighted geometric mean, achieving better calibration performance than either method alone.
- Mechanism: Each sample's final temperature parameter is computed as a weighted geometric mean of its class temperature (from CA-TS) and its bin temperature (from Esbin-TS). The weight α controls the balance between the two branches.
- Core assumption: CA-TS and Esbin-TS complement each other by addressing different aspects of miscalibration, and their combination can capture both class-specific and confidence-level-specific calibration needs.
- Evidence anchors:
  - [abstract] "we design a dual-branch structure that can independently learn two temperature coefficients for the same sample based on two different settings, and fuse them by class geometric mean"
  - [section] "we designed a dual-branch temperature calibration model... we believe that the simplest and most effective way is to average all the temperature coefficients obtained from each branch"
  - [corpus] Weak or missing - no direct evidence from neighbors about dual-branch mechanism
- Break condition: If the fusion of temperature parameters from both branches does not lead to improved calibration, or if the optimal α value is difficult to determine.

## Foundational Learning

- Concept: Temperature Scaling
  - Why needed here: Temperature scaling is a post-hoc calibration technique that adjusts the confidence of a model's predictions by scaling the logits before applying softmax. It is essential for addressing miscalibration in deep neural networks.
  - Quick check question: How does temperature scaling modify the output probabilities of a model, and what effect does it have on calibration?

- Concept: Long-Tailed Distribution
  - Why needed here: Long-tailed distributions occur when the data has a few head classes with many samples and many tail classes with few samples. This imbalance leads to miscalibration, as the model becomes overconfident on head classes and underconfident on tail classes.
  - Quick check question: Why does a long-tailed data distribution cause miscalibration in deep neural networks, and how does it affect the confidence levels of different classes?

- Concept: Expected Calibration Error (ECE)
  - Why needed here: ECE is a metric used to evaluate the calibration of a model by measuring the difference between predicted confidence and actual accuracy across different confidence bins. It is crucial for assessing the effectiveness of calibration techniques.
  - Quick check question: How is ECE calculated, and what are its limitations when evaluating model calibration under long-tailed distributions?

## Architecture Onboarding

- Component map: Input layer -> CA-TS branch -> Esbin-TS branch -> Fusion layer -> Output layer
- Critical path:
  1. Obtain logits from the classification layer.
  2. Determine the predicted class and confidence bin for each sample.
  3. Apply the class temperature parameter (CA-TS) and bin temperature parameter (Esbin-TS) to the logits.
  4. Fuse the temperature parameters using the weighted geometric mean.
  5. Apply the fused temperature parameter to the logits and pass through softmax.

- Design tradeoffs:
  - Complexity vs. performance: The dual-branch architecture increases model complexity but can achieve better calibration performance than single-branch methods.
  - Number of temperature parameters: CA-TS requires one temperature parameter per class, while Esbin-TS requires one per bin. The choice of bin size and number affects the granularity of calibration.
  - Hyperparameter tuning: The fusion weight α needs to be tuned to balance the contributions of CA-TS and Esbin-TS, which may require additional experimentation.

- Failure signatures:
  - Poor calibration performance: If the model's calibration does not improve or worsens after applying Dual-TS, it may indicate issues with the temperature parameter learning or fusion process.
  - Overfitting: If the model overfits to the training data, especially for tail classes, it may lead to poor generalization of the temperature parameters.
  - Hyperparameter sensitivity: If the calibration performance is highly sensitive to the choice of α or other hyperparameters, it may indicate a lack of robustness in the dual-branch architecture.

- First 3 experiments:
  1. Implement CA-TS alone and evaluate its calibration performance on a long-tailed dataset (e.g., CIFAR-10-LT) using ECE and accuracy metrics.
  2. Implement Esbin-TS alone and compare its calibration performance to CA-TS on the same dataset.
  3. Combine CA-TS and Esbin-TS using the dual-branch architecture and tune the fusion weight α to optimize calibration performance. Compare the results to the single-branch methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the value of the hyperparameter alpha influence the calibration performance in the Dual-TS model?
- Basis in paper: [explicit] The paper discusses the role of alpha in the fusion of the dual branches in the Dual-TS model, and conducts experiments with different alpha values to analyze its impact on calibration performance.
- Why unresolved: The paper does not provide a definitive optimal value for alpha, and the impact of different alpha values on the calibration performance is not fully explored.
- What evidence would resolve it: A comprehensive study examining the effect of various alpha values on the calibration performance of the Dual-TS model would provide insights into the optimal choice of alpha.

### Open Question 2
- Question: Can the Dual-TS model be extended to other types of neural network architectures beyond ResNet18?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of the Dual-TS model on CIFAR-10 and CIFAR-100 datasets using ResNet18. However, it does not explore the applicability of the model to other neural network architectures.
- Why unresolved: The paper does not investigate the performance of the Dual-TS model on different neural network architectures, leaving open the question of its generalizability.
- What evidence would resolve it: Experiments evaluating the performance of the Dual-TS model on various neural network architectures, such as VGG, DenseNet, or EfficientNet, would provide insights into its generalizability.

### Open Question 3
- Question: How does the Dual-TS model perform on datasets with different levels of class imbalance?
- Basis in paper: [inferred] The paper focuses on long-tailed data distributions with two types of imbalance factors. However, it does not explore the performance of the Dual-TS model on datasets with varying levels of class imbalance.
- Why unresolved: The paper does not investigate the performance of the Dual-TS model on datasets with different levels of class imbalance, leaving open the question of its effectiveness in handling varying degrees of imbalance.
- What evidence would resolve it: Experiments evaluating the performance of the Dual-TS model on datasets with different levels of class imbalance, such as CIFAR-10-LT with varying imbalance factors, would provide insights into its effectiveness in handling different degrees of imbalance.

## Limitations

- Poor generalization of temperature parameters for minority classes due to limited sample sizes may lead to overfitting in CA-TS.
- The effectiveness of Esbin-TS depends heavily on the binning strategy, which may not capture meaningful miscalibration patterns if confidence distributions are highly skewed.
- The method's performance on real-world long-tailed datasets beyond synthetic CIFAR variants remains unexplored.

## Confidence

- High confidence in the Dual-TS architecture and experimental methodology
- Medium confidence in the effectiveness of the dual-branch fusion approach
- Medium confidence in the proposed Esbin-ECE metric's superiority over traditional ECE

## Next Checks

1. Evaluate Dual-TS on real-world long-tailed datasets (e.g., iNaturalist, Places-LT) to assess generalizability beyond synthetic CIFAR variants.
2. Conduct ablation studies varying the number of bins B in Esbin-TS to determine optimal granularity and its impact on calibration performance.
3. Perform cross-validation experiments to assess the stability of temperature parameters across different data splits, particularly for minority classes.