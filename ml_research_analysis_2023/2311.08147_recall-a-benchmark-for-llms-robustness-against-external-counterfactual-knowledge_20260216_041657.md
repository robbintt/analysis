---
ver: rpa2
title: 'RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge'
arxiv_id: '2311.08147'
source_url: https://arxiv.org/abs/2311.08147
tags:
- counterfactual
- knowledge
- information
- answer
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RECALL, a benchmark for evaluating large
  language models' robustness against external counterfactual knowledge. The benchmark
  is constructed from existing datasets by adding counterfactual information through
  ChatGPT.
---

# RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge

## Quick Facts
- arXiv ID: 2311.08147
- Source URL: https://arxiv.org/abs/2311.08147
- Reference count: 5
- Key outcome: Existing LLMs are susceptible to interference from unreliable external knowledge with counterfactual information, and simple intervention methods fail to effectively alleviate the problem.

## Executive Summary
This paper introduces RECALL, a benchmark designed to evaluate large language models' robustness against external counterfactual knowledge. The benchmark is constructed by adding counterfactual information to existing datasets through ChatGPT, creating challenging scenarios where models must distinguish between reliable and unreliable context. The study evaluates two tasks (Question Answering and Text Generation) across multiple model architectures and finds that LLMs are significantly vulnerable to counterfactual interference, with performance dropping notably when faced with misleading external information. The research also explores prompting and inference intervention methods, but these fail to provide substantial improvements in model robustness.

## Method Summary
The RECALL benchmark is constructed from EventKG (historical events) and UJ (scientific terms) datasets. For each sample, the method involves generating question-answer pairs and then systematically editing contexts by replacing either answer-relevant or non-answer text with counterfactual information using ChatGPT. The evaluation uses three conditions: original contexts, edited counterfactual contexts, and no contexts. Models are assessed using Accuracy (response quality), Misleading Rate (M-Rate, robustness to edited answers), and Mistake Reappearance Rate (R-Rate, robustness to edited non-answer texts). Two intervention methods are explored: prompting and inference intervention (DoLa).

## Key Results
- Existing LLMs show significant performance degradation when exposed to counterfactual external knowledge
- Simple intervention methods (prompting and inference intervention) fail to effectively improve model robustness
- Model confidence significantly decreases when facing counterfactual contexts, especially in text generation tasks
- The study reveals a fundamental vulnerability in how LLMs integrate external knowledge with their internal reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs are vulnerable to counterfactual information because they lack robust reasoning to distinguish reliable from unreliable context
- Mechanism: Models treat all external information equally without verifying its truthfulness against intrinsic knowledge
- Core assumption: Counterfactual information appears coherent and plausible to the model
- Evidence anchors:
  - [abstract]: "existing LLMs are susceptible to interference from unreliable external knowledge with counterfactual information"
  - [section]: "external information from the Internet may include counterfactual information that will confuse the model"
  - [corpus]: Related work on robustness against misinformation in biomedical QA (FMR 0.565)

### Mechanism 2
- Claim: Prompting and inference intervention fail because they don't address fundamental reasoning limitations
- Mechanism: Simple interventions are insufficient to overcome inherent vulnerability to misleading context
- Core assumption: Model's reasoning capabilities aren't enhanced by these interventions
- Evidence anchors:
  - [abstract]: "simple intervention methods make limited contributions to the alleviation of this issue"
  - [section]: "explore two existing methods...but they fail to effectively alleviate the problem"
  - [corpus]: Related work on robust fine-tuning for RAG (FMR 0.616)

### Mechanism 3
- Claim: Decreased model confidence with counterfactual contexts suggests potential research avenue
- Mechanism: Internal uncertainty signals could guide response generation to mitigate counterfactual impact
- Core assumption: Confidence is a reliable indicator of response truthfulness
- Evidence anchors:
  - [section]: "model's confidence in its answers will significantly drop with the interference of counterfactual contexts"
  - [corpus]: No direct evidence, but related work on uncertainty estimation is promising

## Foundational Learning

- Concept: Counterfactual reasoning
  - Why needed here: Understanding how to identify and handle counterfactual information is crucial for developing robust LLMs
  - Quick check question: What is the difference between a counterfactual statement and a factual statement?

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: RAG is a common approach for incorporating external knowledge into LLMs, but it can also introduce counterfactual information
  - Quick check question: How does RAG work, and what are its potential vulnerabilities?

- Concept: Hallucination in LLMs
  - Why needed here: Hallucination is a related problem where LLMs generate information not grounded in provided context or their own knowledge
  - Quick check question: What are some common causes of hallucination in LLMs?

## Architecture Onboarding

- Component map: Retriever -> Context processor -> LLM -> Confidence estimator
- Critical path:
  1. User query is received
  2. Relevant external knowledge is retrieved
  3. Retrieved context is integrated with the query
  4. LLM generates a response
  5. Confidence in the response is estimated
- Design tradeoffs:
  - Accuracy vs. efficiency: More thorough verification of external knowledge can improve accuracy but may slow down response generation
  - Trust in external knowledge vs. trust in intrinsic knowledge: Balancing reliance on external and internal knowledge is crucial
- Failure signatures:
  - Low confidence in responses that are actually correct
  - High confidence in responses that contain counterfactual information
  - Inconsistency between response and provided context
- First 3 experiments:
  1. Evaluate model performance on RECALL with and without simple prompt intervention
  2. Test impact of different confidence thresholds on response quality
  3. Compare model performance on RECALL with and without pre-trained fact-checking component

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively improve the robustness of LLMs against external counterfactual knowledge?
- Basis in paper: Explicit - The paper states that existing methods fail to effectively alleviate the problem
- Why unresolved: The issue is challenging and requires effective solutions that provide steady and significant improvements
- What evidence would resolve it: Development and evaluation of new methods that consistently and significantly improve robustness

### Open Question 2
- Question: How does the confidence of LLMs in their answers relate to their susceptibility to counterfactual knowledge?
- Basis in paper: Explicit - The paper shows confidence significantly drops with counterfactual interference
- Why unresolved: While the phenomenon is observed, the causal relationship between confidence and susceptibility is not explored
- What evidence would resolve it: Systematic experiments investigating the relationship between model confidence and susceptibility across different tasks and datasets

### Open Question 3
- Question: What are the underlying mechanisms that make LLMs susceptible to counterfactual knowledge?
- Basis in paper: Inferred - The paper demonstrates vulnerability but doesn't explain underlying mechanisms
- Why unresolved: Understanding root causes would require deeper analysis of LLM architectures and training processes
- What evidence would resolve it: Comprehensive studies examining internal workings of LLMs when processing counterfactual information

## Limitations
- Reliance on ChatGPT for counterfactual knowledge generation may introduce bias in how counterfactual information is constructed
- Evaluation focuses on only two specific domains (historical events and scientific terms), limiting generalizability
- Only two intervention methods are explored, without investigating more sophisticated approaches that might be more effective

## Confidence

**High Confidence**: LLMs show decreased performance when exposed to counterfactual external knowledge (supported by empirical results across multiple model architectures and tasks)

**Medium Confidence**: Simple intervention methods are insufficient to address the problem (supported by experimental results, but alternative approaches not explored)

**Low Confidence**: Generalizability of findings beyond tested domains (paper doesn't provide evidence that vulnerability patterns hold for other knowledge types)

## Next Checks
1. Test the RECALL benchmark across additional knowledge domains (medical knowledge, current events, technical domains) to assess generalizability beyond historical and scientific knowledge

2. Evaluate more sophisticated intervention strategies (multi-step reasoning prompts, fact-checking modules, confidence-aware decoding) to determine whether simple interventions' ineffectiveness generalizes or if complex approaches might succeed

3. Conduct human evaluations of ChatGPT-generated counterfactual information to verify it represents realistic misinformation scenarios and assess whether human-generated counterfactuals produce different results than AI-generated ones