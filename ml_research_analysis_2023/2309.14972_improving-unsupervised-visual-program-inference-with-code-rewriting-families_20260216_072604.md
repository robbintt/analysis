---
ver: rpa2
title: Improving Unsupervised Visual Program Inference with Code Rewriting Families
arxiv_id: '2309.14972'
source_url: https://arxiv.org/abs/2309.14972
tags:
- siri
- programs
- program
- learning
- rewriters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of unsupervised visual program
  inference, where the goal is to infer structured programs that generate visual data
  without direct supervision. The authors propose Sparse Intermittent Rewrite Injection
  (SIRI), a framework that integrates a family of code rewriters into a bootstrapped
  learning paradigm to improve program reconstruction accuracy and convergence.
---

# Improving Unsupervised Visual Program Inference with Code Rewriting Families

## Quick Facts
- **arXiv ID**: 2309.14972
- **Source URL**: https://arxiv.org/abs/2309.14972
- **Reference count**: 40
- **Primary result**: Sparse Intermittent Rewrite Injection (SIRI) framework improves unsupervised visual program inference by integrating code rewriters, achieving better reconstruction accuracy and convergence on 2D/3D CSG and ShapeAssembly domains.

## Executive Summary
This paper introduces Sparse Intermittent Rewrite Injection (SIRI), a framework that integrates a family of code rewriters into bootstrapped learning to improve unsupervised visual program inference. The rewriters—Parameter Optimization, Code Pruning, and Code Grafting—target different aspects of program improvement. Evaluated on 2D/3D CSG and ShapeAssembly domains, SIRI achieves better reconstruction performance and faster convergence compared to methods without rewriters. At test time, SIRI with test-time rewriting matches or surpasses domain-specific neural architectures while producing more parsimonious programs.

## Method Summary
SIRI is a bootstrapped learning framework that integrates code rewriters into the training loop. It alternates between Search (finding good programs using a VPI model with beam search), Rewrite (applying Parameter Optimization, Code Pruning, and Code Grafting to a subset of programs), and Train (learning from the best programs). The framework uses a best-program data structure to record the best program found so far for each shape, enabling efficient training data construction. Sparse rewriting is employed to avoid overfitting and maintain diversity in the training set.

## Key Results
- SIRI achieves better reconstruction performance (e.g., 0.22 CD on 2D CSG) and faster convergence compared to methods without rewriters or naive integration.
- At test time, SIRI combined with test-time rewriting matches or surpasses domain-specific neural architectures (e.g., CSG-Stump) while producing significantly more parsimonious programs with fewer primitives.
- The three rewriters (Parameter Optimization, Code Pruning, and Code Grafting) address different aspects of program improvement, including parameter tuning, removing spurious code, and substituting sub-expressions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse intermittent rewriting avoids overfitting and local minima in bootstrapped learning.
- Mechanism: SIRI applies rewrites to only a subset of programs and only overwrites entries from the same source, keeping the dataset diverse and preventing overfitting to a narrow set of rewritten programs.
- Core assumption: Rewritten programs are useful training targets if they come from a diverse set and are not too frequent.
- Evidence anchors:
  - [abstract]: "SIRI sparsely applies code rewrite operations over a dataset of training programs, injecting the improved programs back into the training set."
  - [section]: "SIRI resolves this issue by its frugal usage of the rewriters and by training pθ on a diverse set of programs obtained from both the Search and Rewrite phases."
- Break condition: If the subset of programs chosen for rewriting is too small or too biased, the diversity benefit is lost and performance degrades.

### Mechanism 2
- Claim: Test-time rewriting with a cached program library improves reconstruction accuracy without increasing program size.
- Mechanism: The Code Grafting rewriter builds a cache of sub-programs during training. At test time, it uses execution equivalence to find and substitute cached sub-programs that improve the match to the target shape, yielding better reconstructions with fewer primitives.
- Core assumption: Execution equivalence is a good proxy for functional equivalence in the program space, and the cache contains useful sub-programs.
- Evidence anchors:
  - [abstract]: "we demonstrate that our family of rewriters can be effectively used at test time to improve the output of SIRI predictions."
  - [section]: "CG uses such ternary desired-executions to search for cache entries that are likely to improve reconstruction accuracy and substitutes the most suitable candidate."
- Break condition: If the cache is too small or the desired-execution inversion fails frequently, the CG rewriter cannot find useful substitutions.

### Mechanism 3
- Claim: Parameter optimization via differentiable execution improves continuous parameters without requiring full differentiability of the program executor.
- Mechanism: For programs with continuous parameters, gradients are propagated from the reconstruction metric to the parameters using a piecewise differentiable executor, allowing local optimization of those parameters.
- Core assumption: The program executor can be made piecewise differentiable with respect to the continuous parameters of interest.
- Evidence anchors:
  - [abstract]: "we design a family of rewriters for visual programming domains: parameter optimization, code pruning, and code grafting."
  - [section]: "PO requires that the program executor E is partially differentiable: one or more continuous parameters should have well-defined derivatives for a given program structure."
- Break condition: If the executor cannot be made piecewise differentiable for the continuous parameters, gradient-based optimization fails.

## Foundational Learning

- Concept: Bootstrapped learning with a best-program data structure
  - Why needed here: The framework alternates between search (finding good programs) and train (learning from those programs). The best-program data structure records the best program found so far for each shape, enabling efficient training data construction.
  - Quick check question: What information is stored in each entry of the best-program data structure, and how is it used during the train phase?

- Concept: Program rewriting as a search operator
  - Why needed here: Rewriting modifies programs to improve an objective without changing the underlying visual target. Different types of rewrites (parameter optimization, code pruning, code grafting) address different aspects of program improvement.
  - Quick check question: How does the Code Pruning rewriter determine which sub-programs to remove, and what metric does it use?

- Concept: Execution equivalence and caching for efficient rewriting
  - Why needed here: Code Grafting needs to find replacement sub-programs that are functionally equivalent. Using execution equivalence (comparing output shapes) allows fast lookup in a cache rather than expensive structural comparison.
  - Quick check question: How does the Code Grafting rewriter compute the "desired execution" for a sub-program, and why is this necessary?

## Architecture Onboarding

- Component map:
  - VPI model (Transformer) -> Best-program data structure -> Search phase -> Rewrite phase (PO, CP, CG) -> Train phase -> Test-time rewriting (interleaved PO, CP, CG)

- Critical path:
  1. Pretrain VPI model on synthetic data
  2. Finetune on target dataset with SIRI loop (Search → Rewrite → Train)
  3. At test time, run VPI model to get initial program
  4. Apply test-time rewriting (interleaved PO, CP, CG)

- Design tradeoffs:
  - Sparse rewriting vs. dense rewriting: Sparse is more efficient and prevents overfitting, but may miss some improvement opportunities
  - Cache size for CG: Larger cache increases chances of finding good substitutions but increases memory and lookup time
  - Number of beam search candidates: More candidates increase search quality but slow down training

- Failure signatures:
  - Training divergence: Check if rewrite frequency is too high or if BP entries are being overwritten too aggressively
  - Poor reconstruction: Verify that the executor is properly differentiable for PO, and that the CG cache contains relevant sub-programs
  - Memory issues: Monitor BP size and cache growth during long training runs

- First 3 experiments:
  1. Run SIRI with only Parameter Optimization rewriter enabled; measure improvement over baseline PLAD
  2. Enable all three rewriters with default frequencies; compare reconstruction metrics and program lengths
  3. Apply test-time rewriting to SIRI outputs; measure change in reconstruction accuracy and program size compared to CSG-Stump outputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can code rewriting families be integrated into reinforcement learning and end-to-end learning paradigms for visual program inference?
- Basis in paper: [explicit] The authors state that while SIRI is effective for bootstrapped learning, it remains unclear how code rewriting families can best aid RL and end-to-end learning paradigms.
- Why unresolved: The paper focuses on demonstrating the effectiveness of SIRI within a bootstrapped learning framework and does not explore its application to other learning paradigms.
- What evidence would resolve it: Experiments comparing the performance of RL and end-to-end learning methods with and without the integration of code rewriting families on various visual program inference tasks.

### Open Question 2
- Question: What additional code-rewriting operations could be effectively integrated into the family of rewrites for SIRI + TTR paradigms?
- Basis in paper: [explicit] The authors suggest that future work could explore how additional code-rewriting operations could be integrated into their family of rewrites for SIRI + TTR paradigms.
- Why unresolved: The paper only presents three specific rewrite operations (Parameter Optimization, Code Pruning, and Code Grafting) and does not exhaustively explore all possible rewrite operations.
- What evidence would resolve it: Development and evaluation of new code-rewriting operations that improve the performance of SIRI + TTR on visual program inference tasks compared to the current set of rewrite operations.

### Open Question 3
- Question: How does the effectiveness of SIRI and its family of rewriters scale to more complex and diverse visual programming domains beyond the three shape-program DSLs evaluated in the paper?
- Basis in paper: [inferred] The paper evaluates SIRI on three specific shape-program DSLs (2D CSG, 3D CSG, and ShapeAssembly) and does not explore its scalability to other domains.
- Why unresolved: The effectiveness of SIRI and its rewriters on other visual programming domains is not investigated, and it is unclear whether the current design would generalize well.
- What evidence would resolve it: Application of SIRI and its rewriters to a broader range of visual programming domains, including those with different levels of complexity and structure, and evaluation of their performance relative to domain-specific methods.

## Limitations
- The mechanism claims lack direct empirical validation in the provided corpus.
- The effectiveness of sparse rewriting versus dense alternatives is not directly compared.
- The claims about test-time rewriting benefits are supported only by abstract statements rather than detailed experimental results.

## Confidence
- **High**: The core SIRI framework design and its integration with bootstrapped learning
- **Medium**: The effectiveness of individual rewriters (PO, CP, CG) and sparse rewriting strategy
- **Low**: The specific implementation details of the program executor's partial differentiability and the test-time rewriting cache mechanisms

## Next Checks
1. Conduct an ablation study comparing SIRI with sparse rewriting versus dense rewriting applied to all programs, measuring both reconstruction accuracy and training stability.
2. Implement and test the Code Grafting rewriter's cache mechanism with varying cache sizes, measuring the trade-off between cache size, lookup time, and reconstruction improvement.
3. Verify the partial differentiability of the program executor by implementing gradient checks for the Parameter Optimization rewriter and measuring the impact on continuous parameter optimization.