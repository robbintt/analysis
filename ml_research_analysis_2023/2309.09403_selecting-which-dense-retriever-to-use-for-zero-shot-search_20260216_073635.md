---
ver: rpa2
title: Selecting which Dense Retriever to use for Zero-Shot Search
arxiv_id: '2309.09403'
source_url: https://arxiv.org/abs/2309.09403
tags:
- dense
- retrieval
- dataset
- target
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the problem of selecting the best dense
  retriever model for a target dataset in a zero-shot setting, where no labeled data
  is available. It adapts methods from computer vision and machine learning, such
  as uncertainty-based and activation-based approaches, to rank and select pre-trained
  state-of-the-art DR models.
---

# Selecting which Dense Retriever to use for Zero-Shot Search

## Quick Facts
- arXiv ID: 2309.09403
- Source URL: https://arxiv.org/abs/2309.09403
- Reference count: 40
- Primary result: Uncertainty-based and activation-based methods can rank dense retriever models in zero-shot settings, with Query Similarity and Query Alteration showing strongest correlations to true effectiveness.

## Executive Summary
This paper investigates the challenge of selecting the best dense retriever model for a target dataset in a zero-shot setting where no labeled data is available. The authors adapt methods from computer vision and machine learning, such as uncertainty-based and activation-based approaches, to rank and select pre-trained state-of-the-art DR models. Through evaluation on the BEIR benchmark dataset, the study finds that while in-domain effectiveness is a strong indicator of model generalizability, it fails to consistently identify the best model across all datasets. The proposed methods provide varying degrees of success in ranking DR models, highlighting the need for further research to improve model selection techniques for practical applications of dense retrieval in domain-specific tasks with low labeled data availability.

## Method Summary
The paper adapts computer vision and machine learning techniques for zero-shot model selection to the dense retrieval setting. Six methods are proposed: Query Similarity (measuring similarity between query representations), Corpus Similarity (evaluating similarity of corpus representations using Fréchet distance), Extracted Document Similarity (comparing document representations extracted from queries), Binary Entropy (measuring prediction uncertainty), Query Alteration (comparing representations after query modification), and In-Domain Performance (using source dataset effectiveness). These methods are evaluated on 10 subsets of the BEIR benchmark using Kendall Tau correlation, ΔNDCG@10, and %ΔNDCG@10 metrics compared to true rankings based on nDCG@10 scores. The study uses pre-trained dense retrievers from the BEIR leaderboard and Huggingface hub, including models like Contriever, CoCondenser, SimLM, ANCE, and various DistilBERT variants.

## Key Results
- In-domain effectiveness varies significantly across dense retriever models and datasets, making it an unreliable sole indicator of generalizability
- Query Similarity and Query Alteration methods achieve the highest correlation with true rankings, with Kendall Tau scores up to 0.96
- Binary Entropy shows inconsistent correlation patterns across datasets due to unknown numbers of relevant documents per query
- Corpus Similarity and Extracted Document Similarity struggle with domain shifts between source and target datasets

## Why This Works (Mechanism)

### Mechanism 1
Dense retrievers exhibit dataset-specific performance variations, making in-domain effectiveness an unreliable sole indicator of generalizability. Different datasets have varying data distributions, query types, and document characteristics. A dense retriever trained on one dataset (e.g., MS MARCO) may perform well on similar domains but poorly on datasets with different characteristics (e.g., biomedical or scientific literature). This domain shift causes effectiveness variations depending on the target domain.

Evidence anchors:
- [abstract] "Each model however is characterized by very differing search effectiveness – not just on the test portion of the datasets in which the dense representations have been learned but, importantly, also across different datasets for which data was not used to learn the dense representations."
- [section 1] "The issue of data distribution shifting has gained increasing attention in the field of information retrieval (IR). This is largely due to the recognition that dense retrievers (DRs) based on pretrained language models (PLMs) have shown exceptional effectiveness on in-domain data. However, recent studies [38, 41, 53, 54] have also highlighted the problem of domain shift for these DRs, whereby the effectiveness of a DR varies depending on the target domain, which may differ from the domain of the data it was trained on."

### Mechanism 2
Uncertainty-based methods can estimate a model's generalization ability by analyzing its predictions on target datasets. Methods like Binary Entropy measure the uncertainty of a model's predictions on a target dataset. A model with lower uncertainty is assumed to have better generalization ability and is more likely to produce accurate predictions. The hypothesis is that models with lower uncertainty are more likely to produce accurate predictions on the target dataset.

Evidence anchors:
- [abstract] "An example of such a criterion is the evaluation of the model's level of uncertainty, with the hypothesis that models with lower uncertainty are more likely to produce accurate predictions."
- [section 3.7] "In the context of classification tasks, entropy is commonly employed to measure the level of uncertainty in a model's predictions. However, in our scenario, it cannot be directly applied due to the possibility of multiple relevant documents per query."

### Mechanism 3
Activation-based methods can approximate the domain gap between source and target distributions by comparing the similarity of their hidden representations. Methods like Corpus Similarity and Extracted Document Similarity measure the similarity between the activations of a model on source and target datasets. A model with more similar activations is assumed to have better generalization ability. The intuition is that the ranker should generalize well if the source and the target corpora are encoded in a similar way.

Evidence anchors:
- [abstract] "The availability of reliable methods for the selection of dense retrieval models in zero-shot settings that do not require the collection of labels for evaluation would allow to streamline the widespread adoption of dense retrieval."
- [section 3.5] "Corpus Similarity Score evaluates the similarity of corpus representations. The intuition is that the ranker should generalize well if the source and the target corpora are encoded in a similar way."

## Foundational Learning

- **Concept: Domain shift**
  - Why needed here: Understanding domain shift is crucial for recognizing why dense retrievers may perform differently on various datasets and why model selection in zero-shot settings is challenging.
  - Quick check question: What is domain shift, and how does it affect the performance of dense retrievers across different datasets?

- **Concept: Uncertainty estimation**
  - Why needed here: Uncertainty estimation methods are used to assess a model's generalization ability by analyzing its predictions on target datasets. Understanding these methods is essential for implementing and evaluating model selection techniques.
  - Quick check question: How can uncertainty be measured in the context of information retrieval, and what assumptions are made about its relationship with model performance?

- **Concept: Activation similarity**
  - Why needed here: Activation similarity methods compare the hidden representations of a model on source and target datasets to estimate the domain gap. Understanding these methods is crucial for implementing and evaluating model selection techniques.
  - Quick check question: How can activation similarity be measured, and what assumptions are made about its relationship with model performance?

## Architecture Onboarding

- **Component map:**
  Dense retriever models -> Model selection methods -> Evaluation metrics -> BEIR benchmark datasets

- **Critical path:**
  1. Load and preprocess dense retriever models and target datasets.
  2. Implement model selection methods.
  3. Evaluate model selection methods on target datasets.
  4. Analyze results and draw conclusions.

- **Design tradeoffs:**
  - Accuracy vs. computational cost: More complex model selection methods may be more accurate but require more computational resources.
  - Generalizability vs. specificity: Methods that are too specific to a particular dataset or domain may not generalize well to other datasets.
  - Interpretability vs. performance: Some methods may be more interpretable but less performant than others.

- **Failure signatures:**
  - Low correlation between model selection rankings and ground truth rankings.
  - High variance in model selection performance across different datasets.
  - Inability to handle large datasets or high-dimensional activations.

- **First 3 experiments:**
  1. Implement and evaluate Query Similarity method on a small subset of BEIR datasets.
  2. Compare the performance of Corpus Similarity and Extracted Document Similarity methods on the same subset of datasets.
  3. Analyze the impact of different uncertainty measures (e.g., Binary Entropy) on model selection performance.

## Open Questions the Paper Calls Out

### Open Question 1
How can uncertainty estimation methods be adapted for dense retrievers in a way that is both theoretically sound and practically effective across diverse domains? The paper highlights that uncertainty estimation in information retrieval is largely under-explored, especially for rankers using pre-trained language models. It notes the challenges of applying entropy-based methods due to the lack of a single correct answer per query and the potential for high entropy to reflect equally relevant documents rather than uncertainty.

### Open Question 2
Can activation-based methods be improved by developing NLP-oriented distance approximations between activation spaces, rather than relying on assumptions like normal distribution? The paper discusses the use of Fréchet distance between network activations as a method for corpus similarity but notes that this approach assumes normal distribution, which may not hold for NLP-induced hidden representations. It suggests that designing NLP-oriented distance approximations could be a promising direction.

### Open Question 3
How can weight-based and performance-based methods, such as those involving unsupervised fine-tuning on target datasets, be made computationally feasible for dense retrievers while maintaining or improving model selection accuracy? The paper mentions recent work on weight-based performance estimation involving fine-tuning networks on target datasets with unsupervised loss, but notes that these approaches are computationally expensive and have not been explored in the context of dense retrievers.

## Limitations

- Binary Entropy method shows inconsistent correlation patterns across datasets due to unknown numbers of relevant documents per query and varying dataset characteristics
- Activation-based methods (Corpus Similarity and Extracted Document Similarity) rely on approximations like Fréchet distance that may not fully capture the complex relationship between source and target distributions
- Methods struggle with significant domain shifts between source and target datasets, limiting their generalizability

## Confidence

- **High Confidence:** The observation that in-domain effectiveness varies significantly across different dense retriever models and datasets is well-supported by empirical results
- **Medium Confidence:** The effectiveness of the proposed model selection methods is demonstrated through empirical results, but their performance varies significantly across different datasets
- **Low Confidence:** The generalizability of these methods to datasets beyond BEIR or to real-world applications with more extreme domain shifts remains uncertain

## Next Checks

1. Evaluate the proposed model selection methods on additional benchmark datasets beyond BEIR, particularly those with more extreme domain shifts or different characteristics (e.g., social media, news articles, or domain-specific corpora).

2. Implement the model selection methods in a practical information retrieval system with limited labeled data to assess their effectiveness in real-world scenarios and identify any practical challenges not captured by the benchmark evaluation.

3. Explore and compare additional uncertainty estimation techniques, such as Monte Carlo dropout or ensemble methods, to determine if they provide more consistent and reliable model selection across diverse datasets and domain shifts.