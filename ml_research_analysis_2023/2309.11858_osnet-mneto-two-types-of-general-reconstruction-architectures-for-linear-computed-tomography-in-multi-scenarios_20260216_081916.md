---
ver: rpa2
title: 'OSNet & MNetO: Two Types of General Reconstruction Architectures for Linear
  Computed Tomography in Multi-Scenarios'
arxiv_id: '2309.11858'
source_url: https://arxiv.org/abs/2309.11858
tags:
- image
- images
- reconstruction
- osnet
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of achieving high-quality interior\
  \ reconstruction in linear computed tomography (LCT) systems, which often suffer\
  \ from projection truncation and blurring due to multiple rotation operations during\
  \ backprojection filtration. The authors propose two general reconstruction architectures\u2014\
  Overlay-Single Network (OSNet) and Multiple Networks Overlaying (MNetO)\u2014that\
  \ leverage deep learning to learn the Hilbert filtering function without requiring\
  \ rotation operations."
---

# OSNet & MNetO: Two Types of General Reconstruction Architectures for Linear Computed Tomography in Multi-Scenarios

## Quick Facts
- arXiv ID: 2309.11858
- Source URL: https://arxiv.org/abs/2309.11858
- Reference count: 40
- Primary result: OSNet and MNetO architectures achieve superior interior reconstruction quality in LCT systems with higher PSNR and SSIM metrics

## Executive Summary
This paper addresses the challenge of achieving high-quality interior reconstruction in linear computed tomography (LCT) systems, which often suffer from projection truncation and blurring due to multiple rotation operations during backprojection filtration. The authors propose two general reconstruction architectures—Overlay-Single Network (OSNet) and Multiple Networks Overlaying (MNetO)—that leverage deep learning to learn the Hilbert filtering function without requiring rotation operations. OSNet overlays multiple differentiated backprojection (DBP) images and trains a single network, while MNetO trains separate models for different directional Hilbert filtering and then overlays the results. A Swin Transformer block is introduced into pix2pixGAN to extract both local and global features from DBP images, improving reconstruction quality.

## Method Summary
The paper introduces two deep learning architectures for LCT reconstruction. OSNet overlays multiple DBP images from different linear trajectories and uses a single pix2pixGAN network with Swin Transformer blocks to learn the Hilbert filtering function. MNetO trains separate ST-pix2pixGAN models for each directional Hilbert filtering and then overlays the reconstructed results. Both architectures are trained on the 2DeteCT dataset using Adam optimizer (learning rate 0.0002, batch size 16, 200 epochs) and evaluated using PSNR, SSIM, and RMSE metrics. The Swin Transformer integration helps address global pixel misalignment issues in DBP images.

## Key Results
- OSNet outperforms traditional BPF method in various scenarios, achieving higher PSNR and SSIM
- MNetO exhibits some artifacts but excels in exterior edge reconstruction
- Both architectures demonstrate enhanced efficiency for large-size image reconstructions
- ST-pix2pixGAN shows superior performance compared to traditional pix2pixGAN and CycleGAN

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OSNet achieves superior image quality by overlaying multiple DBP images first and then applying a single network to learn the Hilbert filtering function.
- Mechanism: By combining DBP images from multiple linear scans into a complete image before network training, OSNet avoids the multiple rotation-Hilbert filtering-inverse rotation operations that blur the final image in traditional BPF methods.
- Core assumption: Overlaying DBP images from multiple linear scans creates a complete DBP image without limited-angle artifacts.

### Mechanism 2
- Claim: MNetO can provide high-quality exterior edge reconstruction by training separate models for different directional Hilbert filtering and then overlaying the results.
- Mechanism: By training multiple networks, each specializing in a specific directional Hilbert filtering, MNetO can better capture the nuances of the filtering process for each direction.
- Core assumption: Training separate models for different directional Hilbert filtering can capture the nuances of the filtering process better than a single model.

### Mechanism 3
- Claim: Introducing a Swin Transformer block into pix2pixGAN helps extract both local and global features from DBP images, improving reconstruction quality.
- Mechanism: The Swin Transformer block uses self-attention and multi-scale processing to capture both local and global features in the DBP images, addressing the "global pixel misalignment" problem.
- Core assumption: The Swin Transformer block can effectively capture both local and global features in DBP images, addressing the "global pixel misalignment" problem.

## Foundational Learning

- Concept: Backprojection Filtration (BPF) algorithm for linear computed tomography (LCT)
  - Why needed here: Understanding the BPF algorithm is crucial for understanding the problem that OSNet and MNetO are trying to solve.
  - Quick check question: What are the main steps of the BPF algorithm for LCT, and what are its limitations in terms of interior reconstruction and blurring?

- Concept: Differentiated Backprojection (DBP) images and Hilbert filtering
  - Why needed here: DBP images are an intermediate step in the BPF algorithm, and Hilbert filtering is a key operation in the reconstruction process.
  - Quick check question: How are DBP images created in LCT, and what is the role of Hilbert filtering in the reconstruction process?

- Concept: Deep learning-based image reconstruction
  - Why needed here: OSNet and MNetO are both deep learning-based approaches to image reconstruction in LCT.
  - Quick check question: What are the key components of a deep learning-based image reconstruction system, and how do they differ from traditional image reconstruction methods like BPF?

## Architecture Onboarding

- Component map:
  - OSNet: Overlay DBP images → pix2pixGAN network with Swin Transformer block → final reconstructed image
  - MNetO: Multiple DBP images → multiple ST-pix2pixGAN networks (one for each direction) → overlay results → final reconstructed image

- Critical path:
  - OSNet: DBP image generation → overlay → pix2pixGAN training → final image generation
  - MNetO: DBP image generation → multiple pix2pixGAN training → individual image generation → overlay → final image

- Design tradeoffs:
  - OSNet vs. MNetO: OSNet uses a single network but requires proper alignment of overlaid DBP images, while MNetO uses multiple networks but may have issues with artifacts due to differences between models.
  - Swin Transformer vs. traditional CNN: Swin Transformer can capture both local and global features but may be more computationally intensive.
  - Paired vs. unpaired training: Paired training (OSNet) may provide more stable results but requires aligned data, while unpaired training (MNetO) may be more flexible but may lead to artifacts.

- Failure signatures:
  - OSNet: Blurry or misaligned final images, poor learning of Hilbert filtering function
  - MNetO: Artifacts in final image, poor performance of individual directional models
  - General: Overfitting, underfitting, slow convergence during training

- First 3 experiments:
  1. Test OSNet and MNetO on a simple synthetic dataset to verify that the architectures can learn the Hilbert filtering function and produce reasonable reconstructions.
  2. Compare the performance of OSNet and MNetO on a real LCT dataset, using metrics such as PSNR and SSIM to quantify the quality of the reconstructions.
  3. Experiment with different network architectures (e.g., different numbers of layers, different attention mechanisms) to optimize the performance of OSNet and MNetO.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the OSNet architecture generalize to different types of images beyond the 2DeteCT dataset, particularly for complex medical images with varying structures?
- Basis in paper: The authors mention that OSNet demonstrates good generalizability when tested on out-of-distribution (OOD) slices from the 2DeteCT dataset, but they do not explore its performance on other types of medical images.
- Why unresolved: The paper focuses on a specific dataset and does not provide evidence of OSNet's performance on a broader range of medical images with different structures and complexities.
- What evidence would resolve it: Testing OSNet on diverse medical imaging datasets, such as CT scans of different body parts or other imaging modalities, would provide insights into its generalizability and robustness across various image types.

### Open Question 2
- Question: What are the computational and memory requirements of the OSNet and MNetO architectures when applied to large-scale CT image reconstruction tasks?
- Basis in paper: The paper mentions that OSNet and MNetO demonstrate enhanced efficiency for large-size image reconstructions, but it does not provide detailed information on their computational and memory requirements.
- Why unresolved: The authors do not discuss the specific computational resources needed for OSNet and MNetO, such as GPU memory usage, processing time, or scalability to very large images.
- What evidence would resolve it: Conducting experiments to measure the computational and memory requirements of OSNet and MNetO on different hardware setups and image sizes would provide valuable insights into their practical applicability and limitations.

### Open Question 3
- Question: How does the performance of OSNet and MNetO compare to other deep learning-based reconstruction methods, such as U-Net or Transformer-based approaches, in terms of image quality and reconstruction time?
- Basis in paper: The authors compare OSNet and MNetO to BPF and other networks like CycleGAN and pix2pixGAN, but they do not provide a comprehensive comparison with other state-of-the-art deep learning methods.
- Why unresolved: The paper focuses on the proposed architectures and their performance relative to a limited set of baselines, but it does not explore how they stack up against other advanced deep learning methods in the field.
- What evidence would resolve it: Conducting experiments to compare OSNet and MNetO with other state-of-the-art deep learning-based reconstruction methods, such as U-Net, Transformer-based approaches, or other GAN variants, would provide a more comprehensive understanding of their strengths and weaknesses.

## Limitations

- The performance claims rely heavily on a single dataset (2DeteCT), with no ablation studies on different LCT geometries or noise levels.
- The MNetO architecture's artifact issues are acknowledged but not thoroughly analyzed.
- The computational efficiency trade-offs between OSNet and MNetO are not clearly quantified.

## Confidence

- **High Confidence**: The core architectural innovations (OSNet and MNetO frameworks) are well-defined and the experimental setup is reproducible. The improvements over BPF are demonstrated with clear metrics.
- **Medium Confidence**: The Swin Transformer integration claims are supported by results but lack detailed implementation specifics. The artifact analysis in MNetO is acknowledged but not rigorously quantified.
- **Low Confidence**: The generalization claims across different LCT scenarios are based on limited experimental evidence. The computational complexity analysis is superficial.

## Next Checks

1. **Dataset Diversity Test**: Validate OSNet and MNetO on at least two additional LCT datasets with varying noise levels, patient anatomies, and scan parameters to assess generalization capability.

2. **Ablation Study on Swin Transformer**: Remove the Swin Transformer block from both architectures and compare performance to quantify its exact contribution to feature extraction and reconstruction quality.

3. **Computational Efficiency Analysis**: Conduct detailed timing experiments comparing OSNet, MNetO, and BPF across different image sizes (512×512, 1024×1024, 2048×2048) to quantify the claimed efficiency improvements and identify bottlenecks.