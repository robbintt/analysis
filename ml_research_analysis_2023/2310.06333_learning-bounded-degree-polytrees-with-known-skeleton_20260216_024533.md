---
ver: rpa2
title: Learning bounded-degree polytrees with known skeleton
arxiv_id: '2310.06333'
source_url: https://arxiv.org/abs/2310.06333
tags:
- learning
- pxpy
- lemma
- algorithm
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper provides the first finite-sample guarantee for proper
  learning of bounded-degree polytrees (a class of high-dimensional distributions
  and a subclass of Bayesian networks) when the skeleton is known. The authors give
  an efficient algorithm that learns d-polytrees in polynomial time and sample complexity
  for any bounded degree d, using conditional mutual information tests to orient edges.
---

# Learning bounded-degree polytrees with known skeleton

## Quick Facts
- arXiv ID: 2310.06333
- Source URL: https://arxiv.org/abs/2310.06333
- Reference count: 40
- Primary result: First finite-sample guarantee for proper learning of bounded-degree polytrees when skeleton is known

## Executive Summary
This paper presents the first efficient algorithm with finite-sample guarantees for learning bounded-degree polytrees when the skeleton is known. The algorithm uses conditional mutual information tests to orient edges in three phases, achieving polynomial time complexity and sample complexity of ~O(n/ε) for constant degree d. The authors complement this with an information-theoretic lower bound showing the dependence on dimension and accuracy parameters is nearly tight. The approach addresses a fundamental problem in high-dimensional distribution learning and Bayesian network structure learning.

## Method Summary
The method is a three-phase algorithm that learns a d-polytree from samples when the skeleton is known. It first identifies "strong v-structures" using conditional mutual information tests, then performs local orientation checks using Meek rules and forced orientations, and finally orients remaining edges as a 1-polytree. The algorithm relies on distinguishing zero from non-zero conditional mutual information values rather than exact estimation, and maintains degree bounds to control KL divergence. The approach assumes either known skeleton or a distributional gap for skeleton recovery via Chow-Liu.

## Key Results
- First algorithm with polynomial time and sample complexity for learning bounded-degree polytrees with known skeleton
- Sample complexity of ~O(n/ε) for constant degree d, matching a lower bound of Ω(n/ε)
- Three-phase approach that orients edges using conditional mutual information tests without requiring exact estimation
- KL divergence bound of ε achieved through careful degree control and orientation strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm orients edges by testing conditional mutual information (CMI) terms for being either zero or sufficiently large.
- Mechanism: In Phase 1, the algorithm checks for "strong v-structures" by verifying that for a center vertex v with a set T of neighbors, each subset relationship I(u; T\{u}|v) is at least C·ε. In Phase 2, it uses local checks to orient edges based on whether I(u; N_in(v)|v) or I(u; N_in(v)) exceeds C·ε. The key insight is that these tests distinguish ground truth orientations from incorrect ones without needing to estimate CMI values exactly.
- Core assumption: The conditional mutual information tests from Corollary 4 reliably distinguish zero from non-zero values within the specified tolerance.
- Evidence anchors:
  - [abstract]: "Our algorithm relies on estimating MI and conditional MI (CMI) terms involving subsets of variables... we show that it suffices to have access to a tester that can distinguish between a CMI term being 0 or at least η"
  - [section]: "In Phase 1, we orient 'strong v-structures'... In Phase 2, we locally check if an edge is 'forced' to orient in a specific direction"
- Break condition: If the CMI tests from Corollary 4 fail to reliably distinguish zero from non-zero values due to insufficient samples or noise.

### Mechanism 2
- Claim: The algorithm maintains a bound on KL divergence by controlling the number of incoming edges per vertex.
- Mechanism: After Phase 2, the algorithm has a partially oriented graph where each vertex v has at most d* incoming edges identified (π_in(v)). Phase 3 freely orients remaining edges as a 1-polytree, allowing each vertex to gain at most one additional incoming edge. This ensures the KL divergence bound ∑I(v;π(v)) - ∑I(v;π̂(v)) ≤ n(d*+1)ε.
- Core assumption: Increasing the incoming degree of any vertex by at most one in Phase 3 doesn't significantly increase the KL divergence.
- Evidence anchors:
  - [abstract]: "We complement our algorithm with an information-theoretic sample complexity lower bound, showing that the dependence on the dimension and target accuracy parameters are nearly tight."
  - [section]: "In Phase 3, we increase the incoming edges to any vertex by at most one. The following lemma tells us that we lose at most an additive ε error per vertex."
- Break condition: If the ground truth graph has vertices with in-degree close to d* and the remaining edges cannot be oriented without violating the degree constraint.

### Mechanism 3
- Claim: The algorithm correctly identifies skeleton edges under distributional assumptions.
- Mechanism: Under Assumption 11, the Chow-Liu algorithm recovers the true skeleton by finding edges with mutual information at least ε_P, distinguishing them from non-edges. This works because there's a detectable gap between edges in G* and edges outside G*.
- Core assumption: There exists a constant ε_P such that edges in G* have mutual information at least ε_P larger than non-edges.
- Evidence anchors:
  - [abstract]: "We complement our algorithm with an information-theoretic sample complexity lower bound, showing that the dependence on the dimension and target accuracy parameters are nearly tight."
  - [section]: "Suppose there is a large enough gap of ε_P between edges in G* and edges outside of G*. Then, with O(1/ε_P^2) samples, each estimated mutual information ˆI(a; b) will be sufficiently close to the true mutual information I(a; b)."
- Break condition: If the gap ε_P is too small relative to sample complexity requirements, making it impossible to distinguish edges from non-edges.

## Foundational Learning

- Concept: Conditional Mutual Information (CMI) and its finite-sample testing
  - Why needed here: The algorithm relies on distinguishing zero from non-zero CMI values to orient edges correctly without exact estimation.
  - Quick check question: What does it mean for a CMI test to distinguish I(X;Y|Z) = 0 from I(X;Y|Z) ≥ η, and why is this useful for polytree learning?

- Concept: Chow-Liu algorithm for skeleton recovery
  - Why needed here: The algorithm assumes the skeleton is known, but in practice it can be recovered using Chow-Liu with sufficient samples under distributional assumptions.
  - Quick check question: How does the Chow-Liu algorithm find a maximum spanning tree based on mutual information, and what assumptions make it recover the true skeleton?

- Concept: KL divergence and its relationship to graph structure
  - Why needed here: The algorithm's success is measured by KL divergence between the true distribution and the learned one, which depends on the mutual information terms in the graph.
  - Quick check question: Why does the KL divergence between two distributions on different DAGs depend only on the difference of mutual information terms ∑I(v;π_G(v))?

## Architecture Onboarding

- Component map:
  - Input: Samples from d-polytree P, skeleton skel(G*), max in-degree d
  - Phase 1: Strong v-structure detection using CMI tests
  - Phase 2: Local search with Meek rules and forced orientations
  - Phase 3: Free orientation of remaining edges as 1-polytree
  - Output: Oriented graph ˆG close to G* in KL divergence

- Critical path:
  1. Phase 1 identifies strong v-structures (O(nd^(d+1)) time)
  2. Phase 2 performs local checks and Meek R1(d*) rules (O(n^3) time)
  3. Phase 3 orients remaining edges (O(n) time via DFS)
  4. Final parameter learning on ˆG (polynomial time)

- Design tradeoffs:
  - Testing vs estimating: Testing CMI for zero/non-zero is more sample-efficient than exact estimation
  - Degree bounds: Allowing one extra incoming edge per vertex simplifies Phase 3 but adds ε error
  - Distributional assumptions: Requiring Assumption 11 for skeleton recovery limits applicability

- Failure signatures:
  - High KL divergence: Indicates incorrect edge orientations in ˆG
  - Incorrect skeleton recovery: Suggests Assumption 11 is violated or sample complexity is insufficient
  - Runtime issues: May indicate problematic graph structure or implementation bugs

- First 3 experiments:
  1. Test on small polytrees (n=5, d=2) with known skeletons to verify Phase 1 correctly identifies v-structures
  2. Verify Phase 2 doesn't make orientation mistakes by checking all local tests against ground truth
  3. Test Phase 3 on various 1-polytree orientations to confirm KL divergence bound holds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the algorithm be extended to learn bounded-degree polytrees when the skeleton is unknown?
- Basis in paper: [explicit] The paper assumes knowledge of the skeleton and focuses on orientation; it mentions "sufﬁcient assumptions (Assumption 11) under which the Chow-Liu algorithm will recover the true skeleton even with ﬁnite samples."
- Why unresolved: The paper only provides a sample complexity lower bound for learning with an unknown skeleton, but does not provide an algorithm that achieves this bound.
- What evidence would resolve it: A polynomial-time algorithm that learns both the skeleton and orientation of bounded-degree polytrees with optimal sample complexity.

### Open Question 2
- Question: Is the sample complexity lower bound of Ω(n/ε) tight for constant degree d?
- Basis in paper: [explicit] "For constant d, our hardness result shows that our proposed algorithm is sample-optimal up to logarithmic factors."
- Why unresolved: The paper establishes a lower bound of Ω(n/ε) but does not prove whether this is achievable for learning algorithms.
- What evidence would resolve it: A matching upper bound algorithm that learns bounded-degree polytrees with O(n/ε) sample complexity for constant d.

### Open Question 3
- Question: Can the algorithm be extended to learn polytrees with higher in-degrees (d > constant)?
- Basis in paper: [inferred] The paper focuses on bounded-degree polytrees with constant d, and the sample complexity grows exponentially with d.
- Why unresolved: The algorithm's efficiency relies on the bounded degree assumption, and extending it to higher degrees would require new techniques.
- What evidence would resolve it: An algorithm that learns polytrees with in-degree d = O(log n) or similar sublinear bounds in polynomial time and sample complexity.

## Limitations
- The algorithm requires either known skeleton or strong distributional assumptions (Assumption 11) for skeleton recovery via Chow-Liu
- The sample complexity grows exponentially with the degree d, limiting applicability to polytrees with higher in-degrees
- The algorithm's practical performance depends on the reliability of conditional mutual information testers in finite-sample settings

## Confidence
- **High Confidence**: The three-phase algorithm structure and its polynomial time complexity are well-defined and theoretically sound.
- **Medium Confidence**: The sample complexity bounds are derived rigorously, but their tightness depends on the practical performance of CMI testers and the validity of distributional assumptions.
- **Low Confidence**: The practical implementation details for combining oriented skeleton with parameter learning are not fully specified in the paper.

## Next Checks
1. Implement and test the CMI tester from Corollary 4 on synthetic data with known conditional independence relationships to verify it can reliably distinguish zero from non-zero values at the claimed tolerance levels.
2. Run the complete three-phase algorithm on small, hand-crafted polytree examples with known skeletons to validate that each phase produces the expected intermediate results and final orientations.
3. Compare the empirical sample complexity on synthetic d-polytrees against the theoretical bounds, particularly examining how the constant factors affect practical sample requirements.