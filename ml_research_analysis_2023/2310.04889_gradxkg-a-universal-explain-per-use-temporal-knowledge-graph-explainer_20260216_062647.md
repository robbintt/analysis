---
ver: rpa2
title: 'GradXKG: A Universal Explain-per-use Temporal Knowledge Graph Explainer'
arxiv_id: '2310.04889'
source_url: https://arxiv.org/abs/2310.04889
tags:
- graph
- knowledge
- rgcn
- temporal
- tkgr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of explainability in Temporal Knowledge
  Graph (TKG) reasoning models by proposing GradXKG, a universal two-stage gradient-based
  approach. GradXKG first uses a Grad-CAM-inspired RGCN explainer to quantify node
  contributions across timesteps efficiently, then employs an integrated gradients
  explainer to consolidate importance scores for RGCN outputs, ensuring compatibility
  with diverse TKGR architectures.
---

# GradXKG: A Universal Explain-per-use Temporal Knowledge Graph Explainer

## Quick Facts
- arXiv ID: 2310.04889
- Source URL: https://arxiv.org/abs/2310.04889
- Reference count: 39
- This paper addresses the lack of explainability in Temporal Knowledge Graph (TKG) reasoning models by proposing GradXKG, a universal two-stage gradient-based approach.

## Executive Summary
This paper tackles the critical challenge of explainability in Temporal Knowledge Graph (TKG) reasoning models by introducing GradXKG, a universal two-stage gradient-based explainer. GradXKG first uses a Grad-CAM-inspired RGCN explainer to quantify node contributions across timesteps efficiently, then employs an integrated gradients explainer to consolidate importance scores for RGCN outputs. This approach ensures compatibility with diverse TKGR architectures while highlighting critical nodes at each timestep for a given prediction. Extensive experiments demonstrate that GradXKG provides insightful, timely explanations grounded in the model's logic, significantly improving fidelity (e.g., 0.73-0.81) and stability (e.g., 0.77-0.85) over baselines while reducing time complexity.

## Method Summary
GradXKG is a two-stage gradient-based approach that generates explanations for RGCN-based TKGR models. The first stage uses a Grad-CAM-inspired RGCN explainer to track gradients and quantify each node's contribution across timesteps. The second stage employs an integrated gradients explainer to consolidate these importance scores for RGCN outputs, making it compatible with diverse TKGR architectures. This method decomposes the explanation process into node-level RGCN explanation and top-layer explanation, allowing it to handle various TKGR architectures that build on RGCN. The approach leverages gradient information to highlight the most influential nodes across timesteps, providing model-agnostic explanations for diverse TKGR architectures.

## Key Results
- GradXKG significantly improves fidelity (0.73-0.81) and stability (0.77-0.85) over baseline explainers
- Reduces time complexity compared to perturbation-based methods while maintaining explanation quality
- Human evaluations confirm the validity, relevance, and sufficiency of explanations generated by GradXKG
- Demonstrates universal compatibility across different RGCN-based TKGR architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GradXKG provides universal explanations for most RGCN-based TKGR models by decomposing the explanation into two stages: node-level RGCN explainer and top-layer explainer.
- Mechanism: The node-level RGCN explainer uses gradients from the RGCN output to quantify each node's contribution across timesteps, while the top-layer explainer uses integrated gradients to consolidate these contributions into final predictions. This two-stage approach allows GradXKG to handle various TKGR architectures that build on RGCN.
- Core assumption: RGCN-based TKGR models can be decomposed into RGCN encoding and subsequent reasoning layers, and gradients from both stages provide meaningful attribution for explanations.
- Evidence anchors:
  - [abstract] "First, a Grad-CAM-inspired RGCN explainer tracks gradients to quantify each node's contribution across timesteps... Second, an integrated gradients explainer consolidates importance scores for RGCN outputs..."
  - [section] "Our method leverages gradient information to highlight the most influential nodes across timesteps. First, a Grad-CAM-inspired RGCN explainer tracks gradients flowing into each node at each timestep..."

### Mechanism 2
- Claim: The RGCN explainer extends Grad-CAM to handle RGCN's relation-specific adjacency matrices by computing gradients for each relation separately.
- Mechanism: Since RGCN uses different weight matrices for each relation type, the explainer computes gradients for each relation-specific feature map at each layer, then aggregates these to get node importance scores. This handles RGCN's unique architecture while preserving Grad-CAM's effectiveness.
- Core assumption: Relation-specific adjacency matrices in RGCN can be handled by computing gradients separately for each relation and aggregating them.
- Evidence anchors:
  - [section] "However, despite the success of this simple yet effective gradient-based approach, it cannot be directly applied to the RGCN model... Therefore, Eq. 3 cannot be directly applied to the RGCN model."
  - [section] "To extend the compatibility of GCN Grad-CAM to RGCN, we first convert Eq. 4 into matrix graph representation form..."

### Mechanism 3
- Claim: The integrated gradients top-layer explainer provides model-agnostic explanations for diverse TKGR architectures.
- Mechanism: By using integrated gradients on the RGCN outputs before they enter model-specific reasoning layers, GradXKG can provide consistent explanations regardless of the specific TKGR architecture. This makes GradXKG truly universal across different RGCN-based models.
- Core assumption: Integrated gradients can effectively attribute importance from RGCN outputs to final predictions across diverse model architectures.
- Evidence anchors:
  - [abstract] "Second, an integrated gradients explainer consolidates importance scores for RGCN outputs, extending compatibility across diverse TKGR architectures based on RGCN."
  - [section] "This proposed explainer layer tracks the contribution of RGCN output tensors toward the final prediction in the top layer of these TKGR models. Moreover, the top-layer explainer associates RGCN output tensors with prediction scores regardless of the model specifics."

## Foundational Learning

- Concept: Temporal Knowledge Graphs (TKGs)
  - Why needed here: GradXKG specifically addresses explainability for TKGR models, which operate on TKGs that evolve over time.
  - Quick check question: How does a TKG differ from a standard knowledge graph in terms of representation and reasoning tasks?

- Concept: Graph Neural Networks (GNNs) and Relational GNNs (RGCNs)
  - Why needed here: GradXKG is built on RGCN as the base model for TKGR, and understanding RGCN's architecture is crucial for implementing the explainer.
  - Quick check question: What distinguishes RGCN from standard GNNs in terms of handling multi-relational graphs?

- Concept: Gradient-based explainability methods
  - Why needed here: GradXKG uses Grad-CAM and integrated gradients, which are gradient-based methods for generating explanations.
  - Quick check question: How do gradient-based explainability methods differ from perturbation-based methods in terms of computational efficiency and the type of information they provide?

## Architecture Onboarding

- Component map: RGCN explainer (Grad-CAM-based) -> Top-layer explainer (integrated gradients) -> Explanation output (saliency map of node importance)
- Critical path: Input TKG -> RGCN encoding -> RGCN explainer (gradient tracking) -> Top-layer explainer (integrated gradients) -> Final explanation
- Design tradeoffs: The two-stage approach trades some explanation specificity for universality across different TKGR architectures. The use of gradients trades some accuracy for computational efficiency compared to perturbation methods.
- Failure signatures: If explanations are not faithful to the model's predictions, check whether the RGCN explainer or top-layer explainer is not properly capturing gradient information. If explanations are computationally expensive, check whether the integrated gradients approximation is using too many sampling points.
- First 3 experiments:
  1. Test the RGCN explainer on a simple RGCN-only TKGR model to verify node importance scores align with expected patterns.
  2. Test the top-layer explainer with integrated gradients on a multi-layer TKGR architecture to verify it properly consolidates RGCN contributions.
  3. Compare GradXKG explanations against perturbation-based methods on a standard dataset to verify fidelity and stability metrics.

## Open Questions the Paper Calls Out

- Open Question 1: How does the computational complexity of GradXKG scale with the size of the temporal knowledge graph, particularly in terms of the number of nodes and timestamps?
  - Basis in paper: [inferred] The paper mentions that perturbation-based methods require O(n*m) runs, while GradXKG aims to reduce this complexity.
  - Why unresolved: The paper provides some comparisons but does not offer a detailed analysis of GradXKG's computational complexity scaling.
  - What evidence would resolve it: A detailed complexity analysis of GradXKG, showing how its time and space requirements grow with graph size.

- Open Question 2: How well does GradXKG generalize to different types of temporal knowledge graph reasoning tasks beyond link prediction, such as question answering or event forecasting?
  - Basis in paper: [inferred] The paper focuses on link prediction but mentions potential applications in question answering and event forecasting.
  - Why unresolved: The experiments are limited to link prediction, leaving the performance on other tasks unexplored.
  - What evidence would resolve it: Experiments applying GradXKG to diverse TKG reasoning tasks and comparing its performance to task-specific methods.

- Open Question 3: How does the choice of hyperparameters, such as the number of sampling points in the integrated gradients calculation or the number of basis functions in RGCN, affect the quality and stability of GradXKG's explanations?
  - Basis in paper: [explicit] The paper mentions the sampling parameter p in the integrated gradients approximation but does not explore its impact.
  - Why unresolved: The paper does not provide a sensitivity analysis of GradXKG's performance to different hyperparameter settings.
  - What evidence would resolve it: Experiments varying key hyperparameters and analyzing their impact on explanation quality metrics like fidelity and stability.

## Limitations

- The scalability of GradXKG to very large TKGs with millions of nodes remains untested, as current experiments use datasets with thousands of nodes
- Claims about universal compatibility across all RGCN-based architectures are supported by theoretical arguments but limited empirical validation on truly diverse architectures
- The paper does not address potential biases in gradient-based explanations when the underlying model has learned spurious correlations in training data

## Confidence

- **High**: The core mechanism of using two-stage gradient attribution (RGCN explainer + integrated gradients) is well-founded and technically sound
- **Medium**: Claims about universal compatibility across RGCN-based architectures, as they are supported by theoretical arguments but limited empirical validation
- **Medium**: Fidelity and stability improvements over baselines, as these are demonstrated on specific datasets but may not generalize to all TKG reasoning scenarios

## Next Checks

1. **Scalability test**: Evaluate GradXKG on a TKG with 10x more nodes than ICEWS datasets to measure time complexity and memory requirements
2. **Architecture diversity validation**: Apply GradXKG to at least two additional RGCN-based TKGR architectures not mentioned in the paper to verify universal compatibility claims
3. **Bias analysis**: Test whether GradXKG explanations are consistent when the underlying TKGR model is trained on datasets with known spurious correlations, to assess robustness to learned biases