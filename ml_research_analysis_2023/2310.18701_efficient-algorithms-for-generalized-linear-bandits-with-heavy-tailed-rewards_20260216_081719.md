---
ver: rpa2
title: Efficient Algorithms for Generalized Linear Bandits with Heavy-tailed Rewards
arxiv_id: '2310.18701'
source_url: https://arxiv.org/abs/2310.18701
tags:
- rewards
- algorithms
- lemma
- linear
- heavy-tailed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates generalized linear bandits with heavy-tailed\
  \ rewards whose (1+\u03B5)-th moment is bounded for some \u03B5 \u2208 (0,1]. It\
  \ proposes two algorithms based on truncation and mean of medians that achieve an\
  \ almost optimal regret bound of \xD5(dT^(1/(1+\u03B5))), where d is the dimension\
  \ of contextual information and T is the time horizon."
---

# Efficient Algorithms for Generalized Linear Bandits with Heavy-tailed Rewards

## Quick Facts
- arXiv ID: 2310.18701
- Source URL: https://arxiv.org/abs/2310.18701
- Reference count: 40
- Primary result: Two algorithms achieving Õ(dT^(1/(1+ε))) regret for generalized linear bandits with (1+ε)-th moment bounded heavy-tailed rewards

## Executive Summary
This paper addresses the challenge of learning in generalized linear bandit models when rewards follow heavy-tailed distributions with only bounded (1+ε)-th moments. The authors propose two novel algorithms - CRTM (truncation-based) and CRMM (mean-of-medians-based) - that achieve near-optimal regret bounds while being computationally efficient. Unlike previous approaches, CRTM supports online learning without requiring batch processing, and CRMM needs only O(log T) reward samples per epoch. Both algorithms significantly improve upon existing methods, particularly when ε=1 where they reduce regret by a logarithmic factor.

## Method Summary
The paper introduces two algorithms for generalized linear bandits with heavy-tailed rewards: CRTM uses uniform truncation of rewards at threshold Γ = O(T^(1-ε)/(2(1+ε))) combined with online Newton step updates, while CRMM plays each arm r = O(log T) times and uses the mean of medians as a robust estimator. Both algorithms construct confidence regions around parameter estimates and select arms to maximize expected reward within these regions. The key innovation is handling heavy tails through either truncation (for asymmetric distributions) or median-based estimation (for symmetric distributions) while maintaining theoretical guarantees through careful analysis of the (1+ε)-th moment condition.

## Key Results
- CRTM and CRMM achieve Õ(dT^(1/(1+ε))) regret bounds, matching the lower bound up to logarithmic factors
- CRTM supports online learning with O(T) computational complexity, unlike existing O(T²) truncation methods
- CRMM requires only O(log T) rewards per epoch, making it more practical for high-frequency applications
- Both algorithms improve regret bounds by a logarithmic factor compared to existing methods when ε=1

## Why This Works (Mechanism)

### Mechanism 1: Truncation reduces extreme reward impact
- Truncation threshold Γ = O(T^(1-ε)/(2(1+ε))) applied to rewards prevents heavy tails from dominating gradient computation
- Core assumption: Bounded (1+ε)-th moment ensures controlled probability mass removal
- Evidence: Abstract and section 3.2 description of CRTM truncation mechanism
- Break condition: ε → 0 or u → ∞ makes truncation ineffective

### Mechanism 2: Mean of medians for robust estimation
- CRMM plays each arm r = O(log T) times, computes medians, then uses mean of medians
- Core assumption: Symmetric reward distributions ensure unbiased median estimation
- Evidence: Abstract mentions O(log T) rewards, section 3.3 describes median computation
- Break condition: Asymmetric distributions cause systematic bias

### Mechanism 3: Online parameter updates
- CRTM updates parameters using only current truncated reward, avoiding batch storage
- Core assumption: Generalized linear model structure enables sequential updates
- Evidence: Section 3.2 compares O(T) complexity to O(T²) batch methods
- Break condition: Link function convexity loss after truncation causes ONS instability

## Foundational Learning

- Concept: Heavy-tailed distributions and bounded (1+ε)-th moments
  - Why needed: Traditional sub-Gaussian assumptions fail; understanding when truncation works
  - Quick check: Would truncation work for distributions with finite variance but infinite third moment?

- Concept: Generalized linear models and link functions
  - Why needed: Algorithms must handle arbitrary link functions while maintaining guarantees
  - Quick check: For logistic link, what minimum curvature κ ensures strong convexity?

- Concept: Online Newton Step (ONS) algorithm
  - Why needed: Both algorithms use ONS variants for parameter updates
  - Quick check: How does regularization parameter λ affect regret in ONS?

## Architecture Onboarding

- Component map: Data processing -> Estimation -> Exploration -> Configuration
- Critical path: 1) Receive context xt and select arm 2) Observe reward yt 3) Apply heavy-tailed strategy 4) Compute gradient 5) Update parameters 6) Construct confidence region 7) Select next arm
- Design tradeoffs: CRTM handles asymmetric rewards but needs O(T) computation vs CRMM's O(log T) efficiency but symmetry requirement
- Failure signatures: CRTM - increasing regret suggests ε too small or threshold mis-tuned; CRMM - systematic bias indicates asymmetric rewards; Both - unstable updates suggest ONS numerical issues
- First 3 experiments: 1) Test CRTM on Student's t-distributions with varying degrees of freedom 2) Compare CRMM on symmetric vs asymmetric Pareto distributions 3) Measure computational scaling for both algorithms as T increases

## Open Questions the Paper Calls Out

- Extending CRTM to more general link functions beyond current assumptions
- Applying CRMM to non-symmetric reward distributions
- Impact of truncation threshold Γ choice on CRTM's practical performance

## Limitations

- CRMM requires symmetric reward distributions, limiting applicability to asymmetric heavy-tailed cases
- Hyperparameter sensitivity, particularly for confidence region width and truncation threshold tuning
- Bounded (1+ε)-th moment assumption may not hold for extreme outliers in real-world applications

## Confidence

- Theoretical regret bounds: High confidence - rigorous proofs provided
- Computational complexity improvements: High confidence - clear analysis of O(T) vs O(T²) scaling
- Practical performance: Medium confidence - limited empirical validation across diverse scenarios
- Asymmetric reward handling: Medium confidence - CRTM claims coverage but limited testing shown

## Next Checks

1. Systematically vary confidence region width parameter c across orders of magnitude to identify robustness ranges
2. Compare algorithm performance on symmetric (Student's t) vs asymmetric (Pareto) heavy-tailed distributions
3. Test ONS stability as ε approaches 0 with increasingly heavy-tailed distributions to identify practical limits