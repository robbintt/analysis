---
ver: rpa2
title: Banach-Tarski Embeddings and Transformers
arxiv_id: '2311.09387'
source_url: https://arxiv.org/abs/2311.09387
tags:
- embedding
- vectors
- data
- algorithm
- tree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a new family of embeddings, called BT embeddings,
  for arbitrary recursive data structures into high-dimensional vectors. These embeddings
  are constructed using random vectors and matrices, and provide an interpretable
  model for the latent state vectors of transformers.
---

# Banach-Tarski Embeddings and Transformers

## Quick Facts
- arXiv ID: 2311.09387
- Source URL: https://arxiv.org/abs/2311.09387
- Reference count: 23
- The paper introduces BT embeddings that map recursive data structures to high-dimensional vectors using random vectors and orthogonal matrices, with empirical evidence showing linear scaling of embedding dimension with data structure size.

## Executive Summary
This paper introduces BT embeddings, a new family of vector embeddings for arbitrary recursive data structures. The embeddings use random unit vectors for tokens and orthogonal matrices for attributes, constructing the embedding of a tree as the sum of token vectors transformed by path-specific matrix products. The key theoretical result is that BT embeddings are invertible with high probability when the embedding dimension is sufficiently large, with empirical evidence suggesting this dimension scales linearly with the size of the data structure. The paper demonstrates that these embeddings can be manipulated directly to perform computations without decoding, and presents a transformer implementation that parses embedded token sequences using only vector operations in embedding space.

## Method Summary
The method constructs BT embeddings by representing each token as a random unit vector and each attribute as a random orthogonal matrix. For a tree, the embedding is computed as the sum of all token vectors transformed by the product of attribute matrices along their path from the root. Decoding uses a recursive nearest-neighbor search that identifies tokens by finding the vector with the largest inner product to the current embedding vector. The paper also introduces a parsing algorithm that operates directly on embeddings by detecting production rule matches through inner product comparisons and constructing new embeddings through vector addition and transformation. A transformer model is implemented that uses attention mechanisms to copy and transform embedded tokens during parsing.

## Key Results
- BT embeddings are invertible with high probability when embedding dimension scales linearly with data structure size
- The embedding of a tree is the sum of token vectors transformed by path-specific orthogonal matrix products
- Direct manipulation of embeddings enables parsing without decoding through inner product matching and vector operations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The BT embedding preserves tree structure through a sum of transformed token vectors.
- Mechanism: Each node's token is embedded as a unit vector, and the edge attributes are embedded as orthogonal matrices. The embedding of a tree is the sum of all token vectors transformed by the product of edge matrices along their path from the root.
- Core assumption: The transformations by orthogonal matrices and the random nature of the token vectors ensure that different trees map to distinguishable vectors in high dimensions.
- Evidence anchors:
  - [abstract] "The BT embedding is constructed using random vectors and matrices."
  - [section 2.4] "In other words, the BT encoding of a tree is the sum of the embeddings of the tokens at the nodes, transformed by the product of matrices corresponding to the attributes of the path of each node."
  - [corpus] Weak: Corpus lacks direct discussion of tree embeddings or orthogonal transformations in this context.
- Break condition: If the dimension is too low relative to the tree size, the JL lemma no longer guarantees distinguishability, causing decoding failures.

### Mechanism 2
- Claim: Decoding is possible via nearest-neighbor search in embedding space using inner products.
- Mechanism: The algorithm decodes by finding the token vector with the largest inner product to the current vector, then recursively applying inverse transformations for each attribute.
- Core assumption: The inner products between distinct terms in the BT sum are small enough that the correct token can be identified by comparing to a threshold (e.g., 1/2).
- Evidence anchors:
  - [section 3] "Lemma 1. If |⟨v1,v2⟩| < 1/2l for all v1 ≠ v2 ∈ V, then Algorithm 1 decodes all trees with ≤ l nodes."
  - [section 3] "This algorithm runs in linear time with respect to the number of nodes in τ, assuming that we can correctly decode all tokens (or lack thereof) with the comparison in step 3."
  - [corpus] Missing: No corpus evidence of similar decoding methods for tree structures.
- Break condition: When inner products exceed the threshold due to insufficient embedding dimension, the algorithm may select incorrect tokens.

### Mechanism 3
- Claim: BT embeddings can be directly manipulated for parsing without decoding.
- Mechanism: Production rules are embedded as token sequences. Matching is detected by checking if the inner product between a pattern and a substring exceeds a threshold. Replacements are formed by adding attribute-transformed token vectors.
- Core assumption: Inner products can reliably indicate matches because non-matching terms contribute negligibly.
- Evidence anchors:
  - [section 5.1] "Test for a match by comparing ⟨pi, x⟩ > m − 1/2."
  - [section 5.1] "Assuming that the non-matching terms have inner products < 1/2C√ml, summing up to ml terms, the result will be < 1/2 with high probability."
  - [corpus] Weak: Corpus lacks examples of direct parsing using inner product matching on embeddings.
- Break condition: If embedding dimension is too low, non-matching terms may contribute enough to cause false positives in pattern matching.

## Foundational Learning

- Concept: Johnson-Lindenstrauss (JL) Lemma
  - Why needed here: The JL lemma justifies why random high-dimensional embeddings preserve distances and inner products, which is essential for both decoding and direct manipulation.
  - Quick check question: If we embed n vectors into d dimensions with d > 16 log(n)/ε², what is the maximum distortion in pairwise distances with high probability?

- Concept: Orthogonal matrices and free groups
  - Why needed here: Attribute matrices are orthogonal, and their products form free groups, ensuring unique transformations for different paths in the tree, preventing collisions.
  - Quick check question: Why does the group generated by two random orthogonal matrices in dimension ≥ 3 contain a free subgroup?

- Concept: Recursive data structures and tree traversal
  - Why needed here: The BT embedding and decoding algorithm rely on the recursive nature of trees and path-based transformations to encode and decode hierarchical information.
  - Quick check question: How does the path of a node in a tree relate to the sequence of attribute transformations applied to its token vector?

## Architecture Onboarding

- Component map:
  - Random token embeddings (unit vectors in Rd) -> Random attribute embeddings (orthogonal matrices in Od) -> BT encoding function (sums of transformed token vectors) -> Decoding algorithm (recursive nearest-neighbor search) -> Parsing algorithm (inner product matching for production rules) -> Transformer implementation (attention-based copying and transformation)

- Critical path:
  1. Generate random token and attribute embeddings.
  2. Encode tree using BT embedding.
  3. Decode tree using Algorithm 1 or Transformer decoder.
  4. (Optional) Parse sequences using Algorithm 2.

- Design tradeoffs:
  - Embedding dimension vs. tree size: Higher dimensions allow larger trees to be embedded and decoded reliably.
  - Random vs. learned embeddings: Random embeddings avoid training but may be less efficient than optimized ones.
  - Direct manipulation vs. decoding: Parsing without decoding is more efficient but requires sufficient dimension for reliable matching.

- Failure signatures:
  - Decoding fails when embedding dimension is too low relative to tree size.
  - Parsing produces incorrect results if inner product thresholds are not met due to insufficient dimension.
  - Transformer implementation may fail if attention mechanisms do not correctly propagate transformations.

- First 3 experiments:
  1. Encode and decode small random trees with varying dimensions to find the scaling threshold.
  2. Test direct parsing on simple grammars (e.g., balanced parentheses) with different embedding dimensions.
  3. Implement and test the Transformer decoder on encoded trees to verify correctness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal random embedding scheme for BT embeddings that balances computational efficiency and decoding success rate?
- Basis in paper: [explicit] The paper mentions that the current construction uses uniform measure on Sd-1 and Haar measure on Od, but suggests that other random embedding schemes, such as [1], could make the constructions substantially more efficient.
- Why unresolved: The paper does not explore alternative random embedding schemes beyond the basic uniform and Haar measures.
- What evidence would resolve it: Empirical comparisons of different random embedding schemes (e.g., using methods from [1]) on decoding success rates and computational efficiency for various data structures and schema sizes.

### Open Question 2
- Question: How does the BT embedding scale to more complex data structures with constraints on which attributes can be attached to particular tokens?
- Basis in paper: [inferred] The paper mentions that in practice there are typically constraints on which attributes can be attached to a particular token, but ignores this detail in the present discussion.
- Why unresolved: The paper focuses on the general case without considering practical constraints on attribute-token relationships.
- What evidence would resolve it: Experiments applying BT embeddings to real-world data structures with attribute constraints, comparing decoding success rates and computational efficiency to the unconstrained case.

### Open Question 3
- Question: Can the BT embedding be extended to handle non-reflexive schemas, where attributes are not also tokens?
- Basis in paper: [explicit] The paper assumes that all schemas are reflexive, allowing attributes to be used as values and enabling self-describing structures.
- Why unresolved: The paper does not explore the implications or potential extensions of BT embeddings to non-reflexive schemas.
- What evidence would resolve it: Theoretical analysis and empirical experiments demonstrating the feasibility and performance of BT embeddings for non-reflexive schemas, including any modifications required to the encoding and decoding algorithms.

## Limitations
- The theoretical analysis relies on the Johnson-Lindenstrauss lemma but does not provide explicit bounds on required embedding dimension for arbitrary schemas
- The decoding algorithm assumes inner products between distinct terms are small enough for correct nearest-neighbor identification, but this is only guaranteed with high probability
- The parsing algorithm's performance depends on non-matching terms contributing negligibly to inner products, which may break down for complex schemas or insufficient embedding dimension

## Confidence
**High Confidence**: The core mathematical framework of BT embeddings using random vectors and orthogonal matrices is sound and well-established through the Johnson-Lindenstrauss lemma and properties of free groups. The encoding process and its theoretical guarantees are clearly specified.

**Medium Confidence**: The decoding algorithm's correctness follows from the theoretical analysis, but empirical validation is limited to relatively small trees. The linear scaling relationship between embedding dimension and tree size requires more extensive testing across diverse schemas.

**Low Confidence**: The parsing algorithm that operates directly on embeddings without decoding is the most speculative component. While the theoretical justification is provided, the empirical evidence is minimal and the algorithm's robustness to real-world data remains unproven.

## Next Checks
1. **Dimensionality Scaling Analysis**: Systematically vary embedding dimension across multiple orders of magnitude for different schema types (linked lists, trees, graphs) to empirically determine the exact scaling relationship and identify the point where decoding accuracy drops below 95%.

2. **Robustness Testing**: Evaluate the parsing algorithm on noisy data where token vectors have been perturbed or where attribute matrices have small errors to assess sensitivity to numerical imprecision and determine practical limits of direct manipulation.

3. **Comparative Benchmarking**: Implement alternative embedding schemes (such as standard positional encodings or learned embeddings) for the same recursive data structures and compare decoding accuracy, computational efficiency, and robustness to determine whether BT embeddings offer practical advantages over existing methods.