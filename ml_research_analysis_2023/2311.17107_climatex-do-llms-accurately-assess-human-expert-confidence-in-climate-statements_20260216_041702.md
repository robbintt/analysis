---
ver: rpa2
title: 'ClimateX: Do LLMs Accurately Assess Human Expert Confidence in Climate Statements?'
arxiv_id: '2311.17107'
source_url: https://arxiv.org/abs/2311.17107
tags:
- confidence
- climate
- statements
- dataset
- high
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Expert Confidence in Climate Statements
  (CLIMATE X) dataset, a novel, curated, expert-labeled dataset of 8,094 climate statements
  from the latest IPCC reports, labeled with their associated confidence levels. The
  dataset is used to evaluate how accurately recent LLMs assess human expert confidence
  in climate-related statements.
---

# ClimateX: Do LLMs Accurately Assess Human Expert Confidence in Climate Statements?

## Quick Facts
- **arXiv ID**: 2311.17107
- **Source URL**: https://arxiv.org/abs/2311.17107
- **Reference count**: 36
- **Key outcome**: Recent LLMs can classify human expert confidence in climate statements with limited accuracy (up to 47%) in few-shot learning settings, consistently overestimating confidence in low and medium confidence statements.

## Executive Summary
This paper introduces the CLIMATE X dataset, containing 8,094 climate statements from IPCC reports labeled with expert-assessed confidence levels. The study evaluates how accurately recent LLMs (GPT-3.5-turbo, GPT-4, and Cohere Command-XL) can assess human expert confidence in climate-related statements. Results show that while LLMs can perform this task with limited accuracy, they consistently overestimate confidence in low and medium confidence statements. The study highlights the need for improved LLM calibration in communicating knowledge limitations, particularly in domains like climate science where nuanced communication is crucial.

## Method Summary
The study constructs the CLIMATE X dataset from IPCC AR6 reports, containing 8,094 statements labeled with four confidence levels (low, medium, high, very high). Models are evaluated using both zero-shot and few-shot prompting with the Demonstrate-Search-Predict (DSPy) library, using 4 demonstrations from the training set. Performance is assessed through regression analysis, mapping confidence levels to numerical scores (0-3) and calculating metrics including accuracy, slope, bias, and support. The test set contains 300 statements balanced across confidence levels and report sources.

## Key Results
- LLMs achieve up to 47% accuracy in classifying human expert confidence in climate statements using few-shot learning
- Models consistently overestimate confidence in low and medium confidence statements, showing mean scores > 1.0 for these categories
- Models perform significantly better on statements from pre-knowledge cutoff reports (WGI) than post-cutoff reports (WGII/WGIII)

## Why This Works (Mechanism)

### Mechanism 1
Few-shot prompting improves LLMs' ability to discern confidence levels by providing in-context examples that act as calibration data. The demonstrations provide a reference distribution of confidence labels, allowing models to adjust predictions based on demonstrated patterns. Few-shot learning significantly improves performance for all models, with slopes improving towards 1 (perfect classifier) after demonstrations are added.

### Mechanism 2
LLMs exhibit systematic overconfidence on low and medium confidence statements due to training data bias toward definitive answers and lack of explicit uncertainty modeling. Models have been trained on data that likely emphasizes confident assertions, leading them to default to higher confidence predictions when uncertain. All three models consistently over-estimate confidence in the 'low' and 'medium' categories.

### Mechanism 3
Models perform better on statements from pre-knowledge cutoff reports (WGI) than post-cutoff reports (WGII/WGIII) due to ability to recall training data rather than genuine reasoning capability. The models can retrieve and reproduce confidence patterns from statements they have seen during pretraining, but cannot generalize to new statements from reports published after their knowledge cutoff.

## Foundational Learning

- **Concept**: Understanding of IPCC confidence assessment framework
  - **Why needed here**: Models are evaluated on their ability to predict human expert confidence levels as defined by IPCC guidelines, which assess confidence based on "quantity and quality of available evidence and agreement among their peers"
  - **Quick check**: What are the four confidence levels used by IPCC reports, and how are they determined by human experts?

- **Concept**: Few-shot learning mechanics and in-context learning
  - **Why needed here**: The study compares zero-shot vs few-shot performance, where few-shot involves providing 4 demonstrations from the training set as context for the model
  - **Quick check**: How many demonstrations were used in the few-shot setting, and from which dataset split were they drawn?

- **Concept**: Regression analysis for classification evaluation
  - **Why needed here**: The study maps categorical confidence levels to numerical scores (0-3) and analyzes model performance using regression metrics like slope and bias
  - **Quick check**: What numerical mapping is used for the confidence levels, and what do slope and bias measure in this context?

## Architecture Onboarding

- **Component map**: Dataset construction (CLIMATE X) -> Prompt engineering with DSPy library -> Model API calls to OpenAI and Cohere models -> Statistical analysis of predictions vs ground truth
- **Critical path**: (1) Retrieve and preprocess IPCC statements, (2) Construct few-shot demonstrations from training split, (3) Format prompts with statements and demonstrations, (4) Send API requests to models, (5) Collect and parse model responses, (6) Calculate regression metrics and accuracy scores, (7) Analyze results by confidence class and report source
- **Design tradeoffs**: Zero-shot vs few-shot involves tradeoff between generalizability and performance - zero-shot tests true model capability but performs worse, while few-shot improves accuracy but may reflect memorization of demonstration patterns
- **Failure signatures**: Consistent overconfidence across all confidence classes indicates model bias toward certainty; significantly better performance on WGI vs WGII/WGIII suggests reliance on memorized patterns rather than reasoning; refusal to provide confidence labels indicates model uncertainty about knowledge limitations
- **First 3 experiments**:
  1. Reproduce zero-shot results for GPT-3.5-turbo on the full test set to establish baseline performance
  2. Test few-shot performance with different numbers of demonstrations (1, 2, 4, 8) to find optimal demonstration count
  3. Evaluate open-source models (Llama, Mistral) on the same test set to compare commercial vs open-source performance

## Open Questions the Paper Calls Out

- **Open Question 1**: What specific retrieval methods could improve model performance on confidence classification tasks? The paper suggests exploring better retrieval methods to select examples or context to include in the prompt as an area for future work.

- **Open Question 2**: How do open-source LLMs perform on the CLIMATE X test set before and after fine-tuning? The paper suggests assessing open-source LLMs on the CLIMATE X test set, before and after fine-tuning them on the train set, as an area for future work.

- **Open Question 3**: Do natural language cues and qualifiers impact model performance on confidence classification tasks? The paper suggests probing whether natural language cues and qualifiers impact model performance on this task as an area for future work.

## Limitations

- Use of commercial LLM APIs limits reproducibility and transparency compared to open-source models
- Dataset construction may not capture full diversity of confidence assessment patterns beyond IPCC reports
- Evaluation methodology relies on regression metrics that assume linear relationships between confidence levels

## Confidence

- Finding that LLMs show limited accuracy in confidence assessment: **High confidence**
- Observation of systematic overconfidence on low/medium confidence statements: **High confidence**
- Interpretation that this reflects training data bias: **Medium confidence**
- Performance difference between WGI and WGII/WGIII statements: **Low confidence**

## Next Checks

1. Replicate the study using open-source models (Mistral, Llama) with fine-tuning on the CLIMATE X dataset to determine if API-based models are uniquely challenged by this task
2. Conduct ablation studies varying the number of few-shot demonstrations to identify optimal demonstration count and test whether performance improvements stem from genuine learning or pattern matching
3. Expand the dataset to include confidence-labeled statements from additional climate science sources beyond IPCC reports to test generalizability and reduce potential dataset-specific biases