---
ver: rpa2
title: Optimal Transport for Measures with Noisy Tree Metric
arxiv_id: '2310.13653'
source_url: https://arxiv.org/abs/2310.13653
tags:
- tree
- metric
- robust
- measures
- latexit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies optimal transport (OT) for probability measures
  supported on a tree metric space, specifically addressing the issue of noisy or
  perturbed tree structures due to real-world measurements. It proposes novel uncertainty
  sets of tree metrics based on edge deletion/addition to capture diverse tree structures.
---

# Optimal Transport for Measures with Noisy Tree Metric

## Quick Facts
- arXiv ID: 2310.13653
- Source URL: https://arxiv.org/abs/2310.13653
- Reference count: 40
- Primary result: Proposes novel uncertainty sets for tree metrics based on edge deletion/addition, enabling closed-form max-min robust OT with applications in document classification and TDA

## Executive Summary
This paper addresses the challenge of optimal transport (OT) for probability measures supported on tree metric spaces when the tree structure is noisy or perturbed due to real-world measurements. The authors propose uncertainty sets of tree metrics defined through edge deletion and addition, which elegantly capture diverse tree structures while maintaining computational tractability. By leveraging the tree structure, they derive closed-form expressions for the max-min robust OT, enabling fast computation and scalability. The robust OT satisfies metric properties and negative definiteness, allowing the construction of positive definite kernels for use in kernel methods. Experiments on document classification and topological data analysis demonstrate that the proposed robust OT kernels outperform standard OT kernels while being computationally efficient.

## Method Summary
The method introduces uncertainty sets of tree metrics defined via edge deletion/addition rules and weight perturbations within a p-norm ball. For probability measures supported on a tree, the robust OT problem is reformulated to maximize over edge weights within uncertainty bounds while minimizing the OT cost. The key insight is that tree structure allows decomposition of the OT problem into a sum over edges, enabling a closed-form expression. The method handles noisy tree metrics by considering edge lengths of zero as deleted, then perturbing weights within uncertainty bounds. For kernel construction, the robust OT distances are transformed using exponential functions to create positive definite kernels suitable for SVM classification.

## Key Results
- Novel uncertainty sets based on edge deletion/addition capture diverse tree structures while enabling closed-form robust OT computation
- Robust OT satisfies metric properties and negative definiteness, enabling positive definite kernel construction
- Experiments show robust OT kernels outperform standard OT kernels on document classification and TDA tasks while being computationally efficient

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Closed-form expression for robust OT is possible due to tree structure allowing decomposition over edges
- Mechanism: By leveraging the tree structure, the optimal transport problem decomposes into a sum over edges where each term depends only on the mass difference across that edge's cut. This allows analytical optimization of edge weights within uncertainty bounds.
- Core assumption: The tree structure ensures that OT between measures can be written as a sum of edge contributions independent of other edges' weights.
- Evidence anchors: Abstract states "closed-form expression"; section shows reformulation as sum over edges; weak corpus evidence.
- Break condition: If the tree structure is lost through excessive edge deletions or if mass distributions are too complex for edge-cut decomposition.

### Mechanism 2
- Claim: Uncertainty sets defined via edge deletion/addition capture diverse tree structures while maintaining computational tractability
- Mechanism: By allowing edges to have zero length (deletion) or adding new edges with zero length, the uncertainty set can represent various tree topologies. This is computationally tractable because zero-length edges don't affect the OT calculation.
- Core assumption: Edges with zero length can be collapsed or added without changing the OT distance, as stated in Theorem 3.1.
- Evidence anchors: Abstract mentions "diversity of tree structures"; section notes zero-length edges don't affect OT computation; weak corpus evidence.
- Break condition: If the number of possible tree structures becomes too large due to combinatorial explosion.

### Mechanism 3
- Claim: Negative definiteness of robust OT enables construction of positive definite kernels for use in kernel methods
- Mechanism: The robust OT metric satisfies negative definiteness properties, which by Berg et al.'s theorem guarantees that exponentiated distances form positive definite kernels.
- Core assumption: The robust OT satisfies mathematical conditions for negative definiteness as proven in Theorem 3.6.
- Evidence anchors: Abstract states negative definiteness and kernel construction; section provides Theorem 3.6 proof; weak corpus evidence.
- Break condition: If robust OT fails to satisfy required mathematical properties under certain tree structures.

## Foundational Learning

- Concept: Tree metrics and their properties
  - Why needed here: The entire approach relies on probability measures supported on tree metric spaces, and the closed-form expression depends on the tree structure.
  - Quick check question: Given a tree with nodes A, B, C where A is root, B is child of A, and C is child of B, what is the path [A,C] and which edges does it contain?

- Concept: Optimal transport and Wasserstein distances
  - Why needed here: The paper builds upon standard optimal transport theory and extends it to robust versions for noisy tree metrics.
  - Quick check question: For two probability measures µ = 0.5δA + 0.5δB and ν = 0.3δA + 0.7δB on a tree with edge length w between A and B, what is the standard tree-Wasserstein distance WT(µ,ν)?

- Concept: Uncertainty sets and robust optimization
  - Why needed here: The robust OT approach requires defining uncertainty sets of tree metrics to account for noisy measurements.
  - Quick check question: If we have an edge with original length 2.0 and uncertainty bounds ±0.5, what is the range of possible edge lengths in the uncertainty set?

## Architecture Onboarding

- Component map: Input measures -> Tree processing (γe sets) -> Uncertainty handling (edge rules) -> Core computation (closed-form formulas) -> Kernel construction (exponential) -> Output distance/kernel

- Critical path:
  1. Parse input measures and tree structure
  2. Precompute γe sets for all edges
  3. Apply uncertainty transformations to tree
  4. Compute edge weight contributions using closed-form formulas
  5. Sum contributions for final robust OT distance
  6. (Optional) Apply exponential transformation for kernel

- Design tradeoffs:
  - Computational efficiency vs. expressiveness of uncertainty sets
  - Choice of p-norm for uncertainty ball (affects robustness properties)
  - Trade-off between tree structure flexibility and closed-form solvability

- Failure signatures:
  - Negative edge lengths in uncertainty set (violates metric properties)
  - Non-convergence in kernel SVM when using robust OT kernel
  - Degenerate tree structures leading to zero distances between distinct measures

- First 3 experiments:
  1. Implement the basic closed-form computation for RTU with simple binary tree and synthetic measures
  2. Test uncertainty handling by perturbing edge lengths within bounds and verifying distance changes
  3. Verify negative definiteness property by computing kernel matrices and checking positive semi-definiteness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed robust OT kernels (kRTU and kRTU2) perform compared to other robust OT approaches like subspace robust Wasserstein (SRW) or min-max robust OT in terms of both accuracy and computational efficiency?
- Basis in paper: The paper discusses related work on max-min/min-max robust OT approaches and highlights that their proposed method is the first to yield a closed-form expression for fast computation. However, it does not provide direct comparisons with these specific approaches.
- Why unresolved: The paper focuses on comparing the proposed robust OT kernels with the standard OT kernel (kTW) on document classification and TDA tasks. It does not include experiments comparing with other robust OT approaches.
- What evidence would resolve it: Conducting experiments on the same datasets used in the paper, comparing the performance of kRTU and kRTU2 with SRW, min-max robust OT, and other relevant approaches in terms of accuracy and computational time.

### Open Question 2
- Question: How does the choice of tree structure impact the performance of the robust OT kernels, and can the method be extended to handle more complex tree structures beyond binary trees?
- Basis in paper: The paper mentions that the robust OT approach can be extended to handle more general structures like graphs, but it does not explore this direction. It also focuses on binary tree structures in the experiments.
- Why unresolved: The paper does not investigate the impact of different tree structures on the performance of the robust OT kernels or explore extensions beyond binary trees.
- What evidence would resolve it: Conducting experiments with different tree structures (e.g., ternary trees, general graphs) and analyzing the impact on the performance of kRTU and kRTU2. Additionally, developing theoretical extensions of the method to handle more complex tree structures.

### Open Question 3
- Question: How does the proposed method handle missing data or outliers in the input probability measures, and can it be adapted to robustify the OT problem against such perturbations?
- Basis in paper: The paper discusses the impact of noisy tree metrics on OT problem and proposes a robust approach. However, it does not explicitly address the issue of missing data or outliers in the input measures.
- Why unresolved: The paper focuses on handling noise in the tree metric, but does not consider the robustness of the method against missing data or outliers in the input probability measures themselves.
- What evidence would resolve it: Extending the theoretical framework to incorporate robustness against missing data or outliers in the input measures. Conducting experiments on datasets with missing data or outliers to evaluate the performance of the robust OT kernels compared to standard OT approaches.

## Limitations

- Computational scalability: While claiming O(|E|) complexity, practical scalability for large trees with many edges remains uncertain, particularly with combinatorial explosion of possible tree structures.
- Hyperparameter sensitivity: Performance appears sensitive to λ and t parameters, but sensitivity analysis is limited and guidance on selection is lacking.
- Generalization beyond trees: The approach is specifically designed for tree metrics, limiting its applicability to general metric spaces or graphs.

## Confidence

**High confidence**: Closed-form expression derivation for robust OT on tree metrics, computational efficiency claims based on tree structure, and negative definiteness proof enabling kernel construction.

**Medium confidence**: Practical effectiveness in document classification and TDA applications, though experimental details are somewhat sparse.

**Low confidence**: Claim about uncertainty sets capturing "diverse tree structures" lacks quantitative validation, and break conditions for mechanisms are not thoroughly explored.

## Next Checks

1. **Scalability test**: Implement the robust OT algorithm and benchmark its performance on trees of increasing size (100, 1000, 10000 edges) to verify the claimed O(|E|) complexity and identify any practical bottlenecks.

2. **Hyperparameter sensitivity analysis**: Systematically vary λ and t parameters in the kernel construction and measure their impact on SVM performance across multiple datasets to identify stable operating regimes.

3. **Robustness verification**: Create controlled synthetic datasets where the ground truth tree structure is known but measurements are noisy. Compare robust OT performance against standard OT in recovering the true distances under various noise levels.