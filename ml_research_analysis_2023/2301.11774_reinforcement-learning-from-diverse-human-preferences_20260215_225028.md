---
ver: rpa2
title: Reinforcement Learning from Diverse Human Preferences
arxiv_id: '2301.11774'
source_url: https://arxiv.org/abs/2301.11774
tags:
- learning
- reward
- preferences
- human
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper tackles the challenge of learning reward functions from
  diverse and inconsistent human preferences, a limitation in current preference-based
  reinforcement learning. The core method introduces a latent space approach to stabilize
  reward learning: it maps inputs to a latent space, imposes a strong prior-based
  constraint to ensure temporal consistency, and employs a confidence-based ensemble
  method to aggregate predictions.'
---

# Reinforcement Learning from Diverse Human Preferences

## Quick Facts
- arXiv ID: 2301.11774
- Source URL: https://arxiv.org/abs/2301.11774
- Reference count: 9
- Key outcome: Proposed method improves RL performance from diverse human preferences, often approaching single-oracle performance.

## Executive Summary
This paper addresses the challenge of learning reward functions from diverse and inconsistent human preferences in preference-based reinforcement learning. The authors introduce a method that stabilizes reward learning by mapping inputs to a latent space with a strong prior-based constraint, ensuring temporal consistency. Additionally, a confidence-based ensemble method aggregates predictions from multiple reward models, improving stability and reliability. Experiments on locomotion and robotic manipulation tasks show consistent performance improvements over existing algorithms when learning from diverse feedback.

## Method Summary
The method uses an encoder-decoder structure to map state-action pairs to a latent space, where rewards are generated by decoding sampled latent variables. A KL divergence constraint forces the latent distribution toward a prior, ensuring temporal consistency in reward predictions. Multiple reward models are trained, and their predictions are aggregated using a confidence-weighted sum based on each model's divergence from the prior. This approach stabilizes learning from diverse human preferences and prevents policy collapse.

## Key Results
- Consistent performance improvements over existing algorithms on DMControl and Meta-world tasks.
- Approaches performance of learning from a single perfect oracle.
- Ablation study confirms importance of latent space constraint and ensemble method.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Imposing a strong constraint on the latent space ensures temporal consistency by preventing reward predictions from fluctuating with new preference data.
- Mechanism: The encoder maps inputs to a latent space where a KL divergence constraint is applied, forcing the latent distribution toward a prior. This makes reward predictions less sensitive to new, diverse preference labels.
- Core assumption: Temporal inconsistency in reward predictions is the main cause of policy collapse when learning from diverse preferences.
- Evidence anchors:
  - [abstract] "To ensure temporal consistency, a strong constraint is imposed on the reward model that forces its latent space to be close to the prior distribution."
  - [section 3.2] "We found that it is the temporal inconsistency that causes the collapse of a policy. To alleviate the problem, we propose to impose a constraint on the latent space so that the reward model has a fixed optimization direction throughout the training process."
- Break condition: If the prior distribution does not adequately represent the true reward structure, the constraint could mislead the reward model.

### Mechanism 2
- Claim: Confidence-based reward model ensembling improves prediction stability by weighting models according to their divergence from the prior.
- Mechanism: Each reward model's confidence is measured via KL divergence between its latent space and the prior; predictions are aggregated using confidence-weighted sums rather than simple averaging.
- Core assumption: Diverse preferences introduce outliers and noise; models with higher confidence (larger KL divergence) are more reliable on a given input.
- Evidence anchors:
  - [abstract] "Additionally, a confidence-based reward model ensembling method is designed to generate more stable and reliable predictions."
  - [section 3.3] "We measure the confidence of a reward model in its prediction by calculating the divergence between its latent space and the prior distribution. Based on this divergence, we design a confidence-based reward model ensembling method to generate more stable and reliable predictions."
- Break condition: If all models have similar confidence scores, the weighting provides no benefit and may add unnecessary complexity.

### Mechanism 3
- Claim: Learning rewards in a latent space allows manipulation of predictions without altering raw outputs, enabling smoother policy updates.
- Mechanism: An encoder-decoder structure maps state-action pairs to a latent representation; rewards are generated by decoding sampled latent variables. This separation allows controlling reward fluctuations via latent space regularization.
- Core assumption: Direct manipulation of reward outputs is more prone to instability than indirect control via latent representations.
- Evidence anchors:
  - [section 3.1] "We propose to manipulate the rewards within a latent space, which avoids directly changing the predictions. We adopt an encoder-decoder structure where the encoder is used to map the input, i.e., a state-action pair, to latent space."
  - [section 3.2] "Concretely, we first map the inputs of the reward model to a latent space so that the predicted reward can be easily manipulated by modifying it in the latent space."
- Break condition: If the latent space becomes too constrained (due to strong prior), the decoder may lose expressiveness, limiting reward granularity.

## Foundational Learning

- Concept: Preference-based reinforcement learning
  - Why needed here: The method builds on preference-based RL, learning rewards from pairwise human preferences rather than predefined reward functions.
  - Quick check question: How does the Bradley-Terry model relate to preference-based RL reward learning?
- Concept: KL divergence and distribution matching
  - Why needed here: The constraint uses KL divergence to align the latent space distribution with a prior, ensuring consistency across training updates.
  - Quick check question: What is the effect of minimizing KL divergence between two distributions?
- Concept: Ensemble methods and confidence weighting
  - Why needed here: The method aggregates multiple reward models using confidence scores derived from KL divergence to reduce overfitting and improve robustness to noisy labels.
  - Quick check question: How does confidence-weighted averaging differ from simple averaging in ensemble methods?

## Architecture Onboarding

- Component map: (s,a) -> Encoder -> Latent z -> Decoder -> Reward; Prior distribution r(z) serves as regularization target; Multiple reward models with shared architecture but different parameters; RL policy optimizer uses aggregated reward predictions.
- Critical path: (1) Sample trajectory pairs -> (2) Obtain human preferences -> (3) Update reward models via loss L = φLc + Ls -> (4) Compute confidence scores -> (5) Ensemble predictions -> (6) Update policy.
- Design tradeoffs: Strong φ stabilizes training but may over-constrain latent space; multiple reward models increase robustness but add computational cost; confidence weighting improves stability but relies on accurate KL divergence estimation.
- Failure signatures: Policy collapse due to reward instability; overfitting to noisy preferences; slow convergence if φ too large; poor performance if confidence weighting misestimates reliability.
- First 3 experiments:
  1. Train with single reward model, no constraint (φ=0) -> observe reward instability.
  2. Add constraint with φ=100 -> measure improvement in temporal consistency.
  3. Add ensemble with confidence weighting -> compare against simple averaging.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed method scale with the number of reward models in the ensemble, and is there an optimal number beyond which additional models provide diminishing returns?
- Basis in paper: [inferred] The paper mentions using N reward models and a confidence-based ensembling method, but does not explore the impact of varying N.
- Why unresolved: The paper does not conduct experiments to determine the optimal number of reward models or analyze the scalability of the ensemble method.
- What evidence would resolve it: Experiments comparing performance with different numbers of reward models in the ensemble, showing the trade-off between performance gains and computational cost.

### Open Question 2
- Question: Can the proposed method be effectively applied to environments with continuous action spaces and high-dimensional state representations, such as those found in real-world robotics applications?
- Basis in paper: [explicit] The paper evaluates the method on locomotion and robotic manipulation tasks from DMControl and Meta-world, but does not explore more complex, high-dimensional environments.
- Why unresolved: The experiments are limited to relatively simple environments, and it is unclear how the method would perform in more complex, real-world scenarios.
- What evidence would resolve it: Experiments applying the method to more complex, high-dimensional environments, such as those found in real-world robotics applications, to demonstrate its scalability and effectiveness.

### Open Question 3
- Question: How sensitive is the proposed method to the choice of prior distribution in the latent space, and are there alternative priors that could lead to improved performance?
- Basis in paper: [explicit] The paper uses a standard Gaussian prior for the latent space, but does not explore the impact of different prior distributions.
- Why unresolved: The paper does not conduct experiments to evaluate the sensitivity of the method to the choice of prior distribution or explore alternative priors.
- What evidence would resolve it: Experiments comparing performance with different prior distributions in the latent space, showing the impact on the method's effectiveness and stability.

## Limitations

- Dependence on a well-chosen prior distribution for the latent space; an inappropriate prior could misguide the reward model and degrade performance.
- Computational overhead of maintaining multiple reward models and computing confidence scores could limit scalability.
- All experiments use scripted annotators; performance with real human preferences remains untested.

## Confidence

- Mechanism 1 (Latent space constraint for temporal consistency): High
- Mechanism 2 (Confidence-based ensembling): Medium
- Mechanism 3 (Latent space for smoother updates): High
- Generalization to real-world human preferences: Low

## Next Checks

1. **Prior Sensitivity Analysis**: Systematically vary the prior distribution and measure impact on reward stability and policy performance.
2. **Real Human Preference Test**: Validate the method using actual human preference data from diverse annotators, comparing against scripted baselines.
3. **Ablation of Confidence Weighting**: Compare confidence-weighted ensembling against uniform averaging and Bayesian model averaging to isolate the specific benefit of the proposed weighting scheme.