---
ver: rpa2
title: Counterfactual Explainer Framework for Deep Reinforcement Learning Models Using
  Policy Distillation
arxiv_id: '2305.16532'
source_url: https://arxiv.org/abs/2305.16532
tags:
- learning
- input
- policy
- teacher
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a counterfactual explanation framework for
  deep reinforcement learning (DRL) models using policy distillation. The framework
  addresses the challenge of generating meaningful and plausible counterfactual explanations
  for DRL models with high-dimensional inputs by distilling the teacher's learned
  policies to a student network with low-dimensional inputs.
---

# Counterfactual Explainer Framework for Deep Reinforcement Learning Models Using Policy Distillation

## Quick Facts
- arXiv ID: 2305.16532
- Source URL: https://arxiv.org/abs/2305.16532
- Reference count: 40
- Key outcome: Proposed framework generates plausible and meaningful counterfactual explanations for DRL models by distilling high-dimensional policies to low-dimensional student networks

## Executive Summary
This paper introduces a novel framework for generating counterfactual explanations for deep reinforcement learning (DRL) models operating on high-dimensional inputs. The core innovation lies in using policy distillation to transfer knowledge from a high-dimensional black-box teacher model to a low-dimensional student model, enabling gradient-based counterfactual generation without adversarial optimization in high-dimensional spaces. The framework is evaluated on automated driving systems and Atari Pong, demonstrating its ability to produce valid and interpretable counterfactual explanations that reflect the underlying decision-making process.

## Method Summary
The framework trains a high-dimensional DRL teacher model on raw inputs (e.g., images), then distills its policy to a low-dimensional student model using soft labels (softmax Q-values). For a given state, counterfactuals are generated by applying the Wachter method to the student model, finding minimal perturbations in the low-dimensional space that change the predicted action. These counterfactuals are then transformed back to high-dimensional space and validated by checking if the teacher model's output changes accordingly. The approach leverages the interpretability of low-dimensional features while maintaining fidelity to the original high-dimensional model's decisions.

## Key Results
- Framework successfully generates valid counterfactual explanations for both driving and Atari Pong environments
- Low-dimensional state representations enable meaningful perturbations without adversarial optimization
- Validation step ensures counterfactuals are faithful to the teacher model's decision boundaries
- Policy distillation with KL divergence loss shows robust performance across different DRL algorithms

## Why This Works (Mechanism)

### Mechanism 1
Policy distillation enables counterfactual explanations by transferring a high-dimensional black-box DRL policy to a low-dimensional student model. The teacher model learns a policy from high-dimensional inputs (e.g., raw images). Its soft labels (softmax of Q-values) are used to train a student model that receives low-dimensional, human-understandable inputs (e.g., object positions). This transfer simplifies the input space, making gradient-based counterfactual generation feasible without adversarial noise. Core assumption: Both teacher and student models can operate in the same environment and produce comparable outputs for the same underlying state.

### Mechanism 2
The Wachter method applied to the low-dimensional student model generates sparse counterfactuals that preserve plausibility. The student model's loss function balances the squared difference between current and target actions with Euclidean distance in the low-dimensional space. Iterative optimization finds the minimal perturbation to achieve the desired action change. Core assumption: The student model's decision boundary in the low-dimensional space maps meaningfully to the teacher's boundary in the high-dimensional space.

### Mechanism 3
Validation by comparing teacher and student outputs ensures counterfactuals are both valid and faithful to the original model. After generating a counterfactual for the student, it is transformed back to the high-dimensional space and fed to the teacher. Validity requires the teacher's action to change correspondingly, confirming the counterfactual reflects the true decision boundary. Core assumption: The transformation between high- and low-dimensional spaces is invertible or at least preserves decision-relevant features.

## Foundational Learning

- Concept: Counterfactual explanations
  - Why needed here: The paper's core contribution is generating counterfactuals for DRL models, which requires understanding what counterfactuals are and how they differ from adversarial examples.
  - Quick check question: What distinguishes a counterfactual example from an adversarial attack in the context of high-dimensional inputs?

- Concept: Policy distillation
  - Why needed here: The framework relies on transferring knowledge from a complex teacher to a simpler student; understanding the mechanics of distillation (soft labels, loss functions) is essential.
  - Quick check question: How does the Kullback-Leibler divergence loss function encourage the student to imitate the teacher's policy?

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: DRL models are trained on MDPs; understanding states, actions, rewards, and policies is foundational to interpreting the experiments.
  - Quick check question: In an MDP formulation, what role does the discount factor Î³ play in shaping the agent's policy?

## Architecture Onboarding

- Component map: High-dimensional teacher DRL model -> Low-dimensional state extractor -> Student model (policy distillation) -> Counterfactual generator (Wachter method) -> High-dimensional transformer -> Teacher model validator

- Critical path: 1. Train teacher DRL model on high-dimensional inputs. 2. Extract low-dimensional states alongside high-dimensional ones. 3. Distill teacher policy to student using soft labels. 4. For a given state, generate counterfactual via Wachter method on student. 5. Transform counterfactual to high-dimensional space. 6. Validate by checking teacher output change.

- Design tradeoffs: High-dimensional vs low-dimensional inputs: High-dimensional inputs capture full detail but make counterfactual generation adversarial; low-dimensional inputs are interpretable but may lose nuance. Sparsity vs validity: More perturbed features increase chance of valid counterfactuals but reduce interpretability. Loss function choice: MSE, NLL, and KL divergence each emphasize different aspects of policy imitation.

- Failure signatures: Student model fails to imitate teacher (low validity scores). Generated counterfactuals are implausible (e.g., two paddles in Pong). Transformation between input spaces loses critical features (teacher doesn't change action). Optimization fails to converge (flat loss landscape).

- First 3 experiments: 1. Train teacher DQN on highway driving images; distill to student using MSE loss; generate and validate counterfactuals. 2. Repeat experiment with PPO teacher and KL divergence loss; compare validity and sparsity. 3. Apply framework to Atari Pong; extract low-dimensional state (paddle positions, ball position); generate counterfactuals and inspect plausibility visually.

## Open Questions the Paper Calls Out

- How can the proposed framework be extended to handle environments that do not provide both high-dimensional and low-dimensional observation spaces? The paper acknowledges this as a limitation but does not propose a specific solution.

- How does the choice of KD loss function (MSE, NLL, KL divergence) impact the validity and sparsity of generated counterfactual explanations? The paper conducts a comparison among the loss functions but does not provide a comprehensive analysis of their impact on counterfactual explanation quality.

- How can the framework be adapted to generate counterfactual explanations for multi-agent DRL systems? The current framework focuses on single-agent DRL models, but there is no discussion of its applicability to multi-agent scenarios.

## Limitations
- High dependence on state representation quality and availability of low-dimensional features that capture decision-relevant information
- Restricted to discrete action spaces, limiting applicability to continuous control tasks
- Manual feature selection requires domain expertise and may miss critical features
- Performance may vary significantly depending on DRL algorithm and environment complexity

## Confidence
- High confidence: The core mechanism of using policy distillation to transfer from high-dimensional to low-dimensional input spaces is well-established and theoretically sound
- Medium confidence: Empirical validation on two domains demonstrates feasibility but may not generalize to all DRL applications
- Medium confidence: The framework addresses the sparsity-validity tradeoff through low-dimensional feature perturbation, but optimal balance depends heavily on specific environment and representation

## Next Checks
1. Cross-environment validation: Test the framework on additional environments (e.g., MuJoCo continuous control tasks) to assess generalizability beyond discrete action spaces and visual inputs
2. Ablation study on representation quality: Systematically vary the completeness of low-dimensional state representations to quantify the impact on counterfactual validity and plausibility
3. Comparison with alternative explanation methods: Benchmark against other XRL approaches (e.g., saliency maps, influence functions) to evaluate the relative effectiveness of counterfactual explanations in practice