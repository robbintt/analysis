---
ver: rpa2
title: 'Large Learning Rates Improve Generalization: But How Large Are We Talking
  About?'
arxiv_id: '2311.11303'
source_url: https://arxiv.org/abs/2311.11303
tags:
- regime
- training
- pre-training
- fine-tuning
- second
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how large initial learning rates affect
  neural network generalization. Prior research suggests starting training with large
  LRs improves generalization, but there is no clear answer on the optimal LR range.
---

# Large Learning Rates Improve Generalization: But How Large Are We Talking About?

## Quick Facts
- arXiv ID: 2311.11303
- Source URL: https://arxiv.org/abs/2311.11303
- Reference count: 25
- The paper investigates how large initial learning rates affect neural network generalization.

## Executive Summary
The paper investigates how large initial learning rates affect neural network generalization. Prior research suggests starting training with large LRs improves generalization, but there is no clear answer on the optimal LR range. The authors conduct detailed empirical analysis to clarify this.

The authors train scale-invariant models on the unit sphere to precisely control the learning rate. They divide training into two stages: pre-training with different fixed LRs, followed by fine-tuning with small LRs or weight averaging. They analyze the final solutions' generalization and geometry.

## Method Summary
The authors investigate the effect of large learning rates on generalization by training scale-invariant ResNet-18 and ConvNet models on CIFAR-10/CIFAR-100 datasets. They use a two-stage training approach: pre-training with various fixed learning rates (PLRs) for 200 epochs, followed by fine-tuning with small learning rates (FLRs) or stochastic weight averaging (SWA). The pre-training is conducted on the unit sphere to ensure precise control over the learning rate, and the last layer is fixed to avoid issues related to scale-invariance. The authors analyze the generalization performance and geometry of the final solutions using test accuracy, angular distance, and linear connectivity metrics.

## Key Results
- Pre-training in the first regime (small LRs) leads to monotonic increase in test accuracy, but fine-tuning cannot significantly improve the solution.
- Pre-training in the second regime (large LRs) provides the best results for fine-tuning and weight averaging, especially in the lower subregime.
- Pre-training in the third regime (very large LRs) leads to poor performance, but can be beneficial for fine-tuning due to uneven distribution of parameter norms.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training with large learning rates in the second regime (specifically subregime 2A) enables access to a "bowl" of high-quality minima that are otherwise unreachable from standard initialization.
- Mechanism: Large initial learning rates destabilize the optimization landscape sufficiently to escape poor local minima and locate a broader basin containing better generalizing solutions. Subsequent fine-tuning or weight averaging then converge to these superior solutions.
- Core assumption: The loss landscape contains distinct basins of attraction, some of which yield better generalization but are inaccessible from standard initialization without prior large-LR pre-training.
- Evidence anchors:
  - [abstract] "The key findings are: Pre-training in the first regime (small LRs) leads to monotonic increase in test accuracy, but fine-tuning cannot significantly improve the solution."
  - [section] "Pre-training in subregime 2A...results in significantly better fine-tuning and SWA results compared to the first regime, while pre-training in subregime 2B...loses this advantage and leads to poor fine-tuning and SWA quality."
  - [corpus] Weak: Corpus neighbors show related work on learning rates and generalization, but lack direct evidence about "bowls" in loss landscape.
- Break condition: If the loss landscape is convex or has very shallow basins, the "bowl" mechanism would not explain the observed improvements.

### Mechanism 2
- Claim: Pre-training in the third regime (very large learning rates) provides an initialization with uneven distribution of parameter norms, which benefits subsequent fine-tuning.
- Mechanism: Very large learning rates during pre-training create a heterogeneous norm distribution across scale-invariant parameter groups. This results in varying effective learning rates for different parameter groups during fine-tuning, allowing some groups to learn faster despite a small overall learning rate.
- Core assumption: Scale-invariant parameters with different norms experience different effective learning rates, and this heterogeneity can be beneficial for fine-tuning.
- Evidence anchors:
  - [abstract] "Pre-training in the third regime (very large LRs) leads to poor performance, but can be beneficial for fine-tuning due to uneven distribution of parameter norms."
  - [section] "We conjecture that the point obtained by pre-training in regime 3...has a very uneven distribution of norms of individual scale-invariant parameter groups...which promotes convergence to overall better optima."
  - [corpus] Weak: No direct evidence in corpus about parameter norm distributions from large-LR pre-training.
- Break condition: If scale-invariance is removed or if parameter norms become uniform during fine-tuning, the mechanism would not hold.

### Mechanism 3
- Claim: Training instabilities during fine-tuning with FLR > PLR can lead to convergence to better minima, but only when the FLR is substantially larger than the PLR.
- Mechanism: When fine-tuning with a learning rate substantially larger than the pre-training learning rate, the optimization dynamics are destabilized, allowing escape from the pre-trained minimum and potential discovery of better generalizing solutions.
- Core assumption: There exist minima that are better than the pre-training minimum but require optimization instabilities to reach.
- Evidence anchors:
  - [abstract] "Pre-training in the first regime (small LRs) leads to monotonic increase in test accuracy, but fine-tuning cannot significantly improve the solution."
  - [section] "FLR > PLR may lead to training instabilities and subsequent convergence to a new better minimum...However, the training instabilities only happen when FLR is substantially larger than PLR."
  - [corpus] Weak: No direct evidence in corpus about optimization instabilities leading to better minima.
- Break condition: If the loss landscape is smooth or if fine-tuning always converges to the nearest minimum, this mechanism would not explain the observations.

## Foundational Learning

- Concept: Scale-invariance and effective learning rates
  - Why needed here: The paper's main experiments use scale-invariant models on the unit sphere to precisely control learning rates. Understanding how scale-invariance affects effective learning rates is crucial for interpreting the results.
  - Quick check question: How does the norm of scale-invariant parameters affect their effective learning rate during training?

- Concept: Three-regimes taxonomy of training with fixed learning rates
  - Why needed here: The paper's analysis is built around the observation that training with fixed learning rates typically occurs in three regimes (convergence, chaotic equilibrium, divergence). Understanding this taxonomy is essential for interpreting the pre-training results.
  - Quick check question: What distinguishes the three training regimes in terms of parameter dynamics and loss behavior?

- Concept: Linear connectivity and angular distance in weight space
  - Why needed here: The paper uses these metrics to analyze the geometry between different solutions obtained through various training strategies. Understanding these concepts is crucial for interpreting the geometric analysis in the results.
  - Quick check question: How do linear connectivity and angular distance differ as measures of solution similarity in weight space?

## Architecture Onboarding

- Component map:
  Scale-invariant ResNet-18 model on unit sphere -> Two-stage training (pre-training with fixed PLR, fine-tuning or SWA) -> Metrics (test accuracy, angular distance, linear connectivity)

- Critical path:
  1. Pre-train model with various fixed learning rates (PLRs) in three regimes
  2. Analyze pre-training results (test accuracy, regime classification)
  3. Fine-tune pre-trained models with small FLRs or apply SWA
  4. Compare final solutions using test accuracy, angular distance, and linear connectivity
  5. Validate findings in practical setting with standard ResNet and SGD

- Design tradeoffs:
  - Scale-invariant setup vs. practical setup: Scale-invariance provides precise LR control but may not fully represent standard training dynamics
  - Fixed LR vs. schedule: Fixed LRs simplify analysis but may not reflect optimal practical training strategies
  - Pre-training duration: Sufficient pre-training is needed to stabilize dynamics, but excessive pre-training may lead to overfitting

- Failure signatures:
  - If pre-training test accuracy is monotonic across all regimes, the three-regimes taxonomy may not apply
  - If fine-tuning always improves pre-training solutions regardless of PLR, the proposed mechanisms may not hold
  - If angular distance and linear connectivity do not correlate with test accuracy differences, the geometric analysis may be flawed

- First 3 experiments:
  1. Train scale-invariant ResNet-18 on CIFAR-10 with various fixed PLRs and plot test accuracy to verify three-regimes taxonomy
  2. Pre-train with PLRs from each regime, then fine-tune with small FLRs and compare final test accuracies
  3. Apply SWA to pre-trained models from different regimes and analyze the quality of resulting solutions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal learning rate range for the second regime (subregime 2A) that consistently provides the best results for fine-tuning and weight averaging?
- Basis in paper: [explicit] The authors found that pre-training in subregime 2A results in significantly better fine-tuning and SWA results compared to the first regime, but the exact LR range is not specified.
- Why unresolved: The authors did not provide specific LR values or a clear boundary between subregimes 2A and 2B, making it difficult to determine the optimal LR range.
- What evidence would resolve it: Conducting experiments with a finer grid of learning rates in the second regime and identifying the LR range that consistently yields the best results for fine-tuning and weight averaging would help determine the optimal range.

### Open Question 2
- Question: How does the scale-invariant property of the models affect the optimal learning rate range and the benefits of pre-training with large learning rates?
- Basis in paper: [explicit] The authors used scale-invariant models in their experiments to precisely control the learning rate and avoid issues related to normalization.
- Why unresolved: The authors did not explore the impact of scale-invariance on the optimal learning rate range and the benefits of pre-training with large learning rates, which may differ in non-scale-invariant models.
- What evidence would resolve it: Conducting experiments with non-scale-invariant models and comparing the results with the scale-invariant setup would help understand the role of scale-invariance in determining the optimal learning rate range and the benefits of pre-training with large learning rates.

### Open Question 3
- Question: What is the mechanism behind the uneven distribution of norms of individual scale-invariant parameter groups after pre-training in the third regime, and how does it contribute to better fine-tuning results?
- Basis in paper: [inferred] The authors suggested that the uneven distribution of norms leads to an uneven distribution of effective learning rates, promoting convergence to better optima during fine-tuning.
- Why unresolved: The authors did not provide a detailed explanation of the mechanism behind the uneven distribution of norms or how it contributes to better fine-tuning results.
- What evidence would resolve it: Conducting further analysis on the distribution of norms and their impact on the effective learning rates during fine-tuning would help understand the mechanism and its contribution to better results.

## Limitations
- The three-regimes taxonomy is empirically observed but the theoretical justification for sharp boundaries remains unclear.
- The scale-invariant setup on the unit sphere may not fully capture the dynamics of standard training.
- The proposed mechanisms for why large LRs improve generalization are largely speculative and lack direct empirical evidence.

## Confidence
- **High confidence**: The empirical observation that pre-training with large LRs in subregime 2A leads to better subsequent fine-tuning/SWA results, and that pre-training in regimes 1 and 3 has limitations.
- **Medium confidence**: The three-regimes taxonomy of training with fixed LRs, as it is well-supported by empirical evidence but theoretical justification for sharp boundaries is lacking.
- **Low confidence**: The specific mechanisms proposed to explain why large LRs improve generalization, as these are largely speculative and lack direct empirical validation.

## Next Checks
1. **Landscape analysis**: Perform empirical analysis of the loss landscape around solutions obtained from different pre-training regimes to directly test the "bowl" hypothesis and understand basin geometry.

2. **Ablation study**: Remove scale-invariance constraints and train standard models with large initial LRs to verify if the observed benefits generalize beyond the unit sphere setup.

3. **Parameter norm analysis**: Conduct detailed analysis of parameter norm distributions during pre-training and fine-tuning to empirically validate the claim that norm heterogeneity drives the benefits of large-LR pre-training.