---
ver: rpa2
title: Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via
  Translation-based Alignment
arxiv_id: '2305.03510'
source_url: https://arxiv.org/abs/2305.03510
tags:
- language
- english
- text
- languages
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose a parameter-efficient framework to reduce multilingual
  disparity in Multilingual-CLIP. Our approach uses translation-based alignment to
  map text embeddings into better initialization and apply parameter-efficient tuning
  methods (Adapter, LoRA, hard prompt) to avoid full-model fine-tuning.
---

# Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via Translation-based Alignment

## Quick Facts
- **arXiv ID**: 2305.03510
- **Source URL**: https://arxiv.org/abs/2305.03510
- **Reference count**: 11
- **Primary result**: Translation-based alignment + PET methods (Adapter/LoRA/hard prompt) reduce multilingual disparity in Multilingual-CLIP by >5 points range and >1.5 points std while only tuning 0.05%-0.45% extra parameters per language

## Executive Summary
This paper addresses the multilingual disparity problem in Multilingual-CLIP by proposing a parameter-efficient framework that combines translation-based alignment with parameter-efficient tuning methods. The approach maps target language embeddings to a better initialization through machine translation and alignment loss, then applies Adapter, LoRA, or hard prompts to fine-tune without full-model updates. Extensive experiments on XTD and Multi30K datasets across 11 languages demonstrate significant reductions in multilingual disparity while maintaining performance with minimal parameter overhead.

## Method Summary
The method uses machine translation to map target language text into English, creating an embedding distribution closer to English before applying alignment loss (MSE or contrastive) between pivot (English) and target embeddings. Parameter-efficient tuning methods (Adapter, LoRA, Compacter, hard prompt) are then applied, updating only small trainable modules while freezing the base model. The framework is trained with Adam optimizer and cosine decay learning rate scheduler, evaluating on zero-shot, few-shot, and full-dataset scenarios using Recall@1 metrics and multilingual disparity measures.

## Key Results
- Multilingual disparity reduction: Range ↓ >5 points, standard deviation ↓ >1.5 points across all scenarios
- Performance improvement: Higher average Recall@1 scores across 11 languages in zero-shot, few-shot, and full-dataset settings
- Parameter efficiency: Only 0.05%-0.45% extra parameters per language tuned compared to full-model fine-tuning
- Method effectiveness: Hard prompts with English outperform multilingual prompts in zero-shot; LoRA most effective in few-shot scenarios

## Why This Works (Mechanism)

### Mechanism 1
Translation-based alignment improves multilingual performance by mapping target language embeddings to a better initialization distribution closer to English. Machine translation creates an embedding distribution that is partially aligned with English, and the alignment loss further reduces the distance between these distributions, narrowing the gap Multilingual-CLIP experiences during training. The core assumption is that translated text embedding distribution is closer to English than natural target language distribution, making optimization easier.

### Mechanism 2
Parameter-efficient tuning methods reduce resource consumption while maintaining performance by updating only a small subset of parameters per language. PET methods like Adapter, LoRA, and Compacter introduce small trainable modules that are updated during fine-tuning while freezing the bulk of the pre-trained model. This allows separate parameter sets for each language without storing full models, based on the assumption that updating a small, strategically chosen subset of parameters is sufficient to adapt the model to new languages without significant performance loss.

### Mechanism 3
Hard prompts in English improve zero-shot multilingual performance by providing a consistent, high-quality textual signal across languages. Adding English hard prompts (e.g., "a photo of [text]") before input text provides a strong semantic cue that works well even when the input text is in another language, especially after machine translation to English. The core assumption is that the English prompt provides a more effective semantic framing than prompts in the target language or soft prompts.

## Foundational Learning

- **Multilingual representation learning**: Understanding how models like Multilingual-CLIP handle multiple languages and why disparities arise is crucial for designing alignment methods. Quick check: Why does Multilingual-CLIP perform worse on low-resource languages compared to English?
- **Parameter-efficient fine-tuning**: PET methods are the core technique for achieving efficiency; understanding their mechanisms and trade-offs is essential for implementation. Quick check: What is the key difference between Adapter and LoRA in terms of how they modify model parameters?
- **Machine translation as a tool for alignment**: The alignment method relies on translation quality; understanding how translation affects embedding distributions is key to the approach. Quick check: How does machine-translated text differ from naturally written text in terms of embedding distribution?

## Architecture Onboarding

- **Component map**: Multilingual-CLIP base model (fixed visual encoder, multilingual text encoder) -> Translation module (Google Translate, pre-translated datasets) -> Alignment module (MSE/contrastive loss between pivot and target embeddings) -> PET modules (Adapter/Compacter/LoRA/hard prompt per language) -> Training pipeline (optimizer, scheduler, evaluation)
- **Critical path**: Translate data → align embeddings (pivot-target) → apply PET → evaluate multilingual performance
- **Design tradeoffs**: Translation quality vs. computational cost (pre-translation saves compute but locks in translation errors); PET method choice vs. performance/resource balance (Adapter good for full-dataset, LoRA for few-shot); Alignment routine choice vs. effectiveness (routine 3 with MSE loss performs best)
- **Failure signatures**: Poor multilingual performance → check translation quality, alignment loss coefficients, PET module updates; High resource usage → verify PET method parameters, check if full model is being updated; Disparity not reduced → inspect embedding distributions, verify alignment routine, check if English consistently outperforms
- **First 3 experiments**: 1) Compare CLIP vs. Multilingual-CLIP with machine translation on XTD to confirm baseline disparity; 2) Test alignment routine 3 (pivot-target) with MSE loss to verify best configuration; 3) Evaluate PET methods (Adapter, LoRA, hard prompt) in zero-shot to determine optimal method per scenario

## Open Questions the Paper Calls Out

1. **Translation Quality Dependence**: How does the quality of the machine translation tool affect the alignment effectiveness and subsequent reduction in multilingual disparity? The paper hypothesizes that higher-quality translation tools could yield better results but does not empirically compare different tools.

2. **Visual Encoder Enhancement**: Would incorporating visual data from diverse cultures and languages during the pre-training process of the visual encoder help in reducing multilingual disparity? The paper suggests this as future work but does not explore this avenue.

3. **Generalizability of PET Methods**: How do the results of parameter-efficient tuning methods like Adapter, Compacter, and LoRA compare in terms of effectiveness and efficiency for cross-lingual transfer across different types of tasks and datasets? The paper's experiments are limited to specific datasets and scenarios.

## Limitations

- **Translation Quality Dependence**: The approach's effectiveness heavily depends on translation quality, which varies across language pairs and is not addressed in terms of error propagation or handling poor translations.
- **Unreported Hyperparameters**: Critical hyperparameters including learning rates, alignment coefficients, and exact prompt templates are determined through grid search but specific optimal values are not reported.
- **Limited Dataset Scope**: Experiments cover 11 languages across two datasets but do not address whether improvements generalize to other vision-language tasks or more diverse multilingual settings.

## Confidence

- **High Confidence**: The core claim that translation-based alignment combined with parameter-efficient tuning reduces multilingual disparity is well-supported by experimental results showing consistent improvements across all tested scenarios.
- **Medium Confidence**: The claim that English hard prompts outperform multilingual prompts in zero-shot settings is supported by specific results but may be task-dependent.
- **Medium Confidence**: The assertion that LoRA is most effective in few-shot scenarios is based on experimental results but lacks theoretical justification for why this specific PET method would outperform others in low-data regimes.

## Next Checks

1. **Cross-Dataset Generalization Test**: Validate the approach on additional vision-language datasets (e.g., COCO, Flickr30K) with the same 11 languages to assess whether improvements generalize beyond the original experimental setup.

2. **Translation Quality Impact Analysis**: Systematically evaluate model performance using different translation quality levels (professional translation vs. machine translation) to quantify the impact of translation errors on multilingual disparity reduction.

3. **Resource Usage Benchmarking**: Conduct comprehensive measurements of training time, inference latency, and memory consumption for each PET method compared to full fine-tuning to verify claimed efficiency benefits in practical deployment scenarios.