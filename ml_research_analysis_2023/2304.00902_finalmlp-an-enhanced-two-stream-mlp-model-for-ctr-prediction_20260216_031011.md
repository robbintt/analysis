---
ver: rpa2
title: 'FinalMLP: An Enhanced Two-Stream MLP Model for CTR Prediction'
arxiv_id: '2304.00902'
source_url: https://arxiv.org/abs/2304.00902
tags:
- feature
- two-stream
- finalmlp
- interactions
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of click-through rate (CTR) prediction
  in online advertising and recommendation. While multi-layer perceptron (MLP) is
  a core component in deep CTR models, it is inefficient in learning multiplicative
  feature interactions.
---

# FinalMLP: An Enhanced Two-Stream MLP Model for CTR Prediction

## Quick Facts
- arXiv ID: 2304.00902
- Source URL: https://arxiv.org/abs/2304.00902
- Reference count: 10
- The paper proposes FinalMLP, an enhanced two-stream MLP model that achieves state-of-the-art CTR prediction performance on four benchmark datasets and shows 1.6% CTR improvement in online A/B testing.

## Executive Summary
This paper addresses the challenge of click-through rate (CTR) prediction in online advertising and recommendation systems. While traditional multi-layer perceptron (MLP) models struggle to efficiently learn multiplicative feature interactions, the authors propose FinalMLP, an enhanced two-stream MLP architecture that achieves surprisingly strong performance without explicit interaction networks. The model incorporates stream-specific feature selection through gating mechanisms and uses multi-head bilinear fusion to effectively combine stream-level interactions. Extensive experiments on four benchmark datasets and an industrial online A/B test demonstrate that FinalMLP outperforms many sophisticated two-stream CTR models while maintaining computational efficiency.

## Method Summary
FinalMLP is a two-stream MLP architecture that processes feature embeddings through parallel MLP networks with different configurations. The model introduces stream-specific feature selection using gating networks that condition on different features (global, user-specific, or item-specific) to create differentiated importance weights for each stream. A multi-head bilinear fusion layer then captures second-order interactions between the stream outputs, with the multi-head decomposition reducing computational complexity while maintaining expressiveness. The model is trained using binary cross-entropy loss with standard CTR prediction hyperparameters including batch size 4096, embedding dimension 10, and learning rate 1e-3 or 5e-4.

## Key Results
- FinalMLP achieves an average AUC of 0.7669 on the Avazu dataset and 0.9720 on the MovieLens dataset
- The model outperforms many sophisticated two-stream CTR models without requiring explicit interaction networks
- Online A/B testing shows a 1.6% improvement in CTR compared to the production system

## Why This Works (Mechanism)

### Mechanism 1
Two independent MLP streams can achieve strong CTR prediction performance without explicit interaction networks by implicitly learning complementary feature interactions through different network configurations. This demonstrates that properly tuned MLPs can match or exceed the performance of specialized interaction networks.

### Mechanism 2
Stream-specific feature selection enhances model performance by enabling differentiated feature inputs for each stream through gating networks that condition on different features. This reduces homogeneous learning between streams and enables more complementary learning of feature interactions.

### Mechanism 3
Multi-head bilinear fusion captures stream-level interactions more efficiently than simple concatenation or summation by modeling second-order interactions between stream outputs with computational savings from the multi-head decomposition approach.

## Foundational Learning

- **Feature interaction modeling in CTR prediction**: Understanding how different architectures capture feature interactions is central to appreciating why FinalMLP works. Quick check: What's the difference between implicit and explicit feature interaction learning, and why might both be valuable?

- **Multi-layer perceptron (MLP) limitations and capabilities**: The paper's core contribution relies on demonstrating MLP's effectiveness despite known limitations in learning multiplicative interactions. Quick check: Why has MLP been considered inefficient for learning multiplicative feature interactions, and what might make it effective despite this limitation?

- **Two-stream model architecture patterns**: FinalMLP builds on the two-stream pattern but innovates within it, so understanding the pattern is essential. Quick check: How do typical two-stream CTR models differentiate their streams, and what advantage does this provide over single-stream approaches?

## Architecture Onboarding

- **Component map**: Feature embeddings → Stream-specific gating → Two MLPs → Multi-head bilinear fusion → Sigmoid output

- **Critical path**: Feature embeddings flow through stream-specific gating networks to produce importance weights, then through two parallel MLPs with different configurations, followed by multi-head bilinear fusion to combine stream outputs, and finally through a sigmoid activation for CTR prediction.

- **Design tradeoffs**: The architecture balances simplicity vs. expressiveness by using two MLPs instead of specialized interaction networks, computational efficiency vs. interaction modeling through multi-head bilinear fusion, and stream differentiation vs. parameter efficiency through the gating mechanism.

- **Failure signatures**: Poor performance with identical MLP configurations indicates stream differentiation is crucial; overfitting with high head counts suggests multi-head bilinear fusion may be too expressive; no improvement over single MLP indicates gating networks or fusion may not be adding value.

- **First 3 experiments**: 1) Compare DualMLP with different MLP configurations to find optimal stream diversity, 2) Test different conditioning features for gating networks to find most effective differentiation, 3) Evaluate multi-head bilinear fusion with varying head counts to balance performance and computational efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
How do stream-specific feature selection and multi-head bilinear fusion compare in effectiveness when applied to different two-stream CTR models beyond FinalMLP? The paper mentions these components are "pluggable" but doesn't provide empirical comparisons with other architectures.

### Open Question 2
What is the optimal number of heads (k) for multi-head bilinear fusion across different dataset characteristics and model architectures? The paper tests only a few values without analyzing the relationship between k and dataset properties.

### Open Question 3
How does FinalMLP performance compare to state-of-the-art models that use attention mechanisms or graph neural networks for CTR prediction? The experimental evaluation focuses on traditional two-stream models without benchmarking against more recent architectures.

## Limitations

- The paper lacks comprehensive ablation studies showing the individual contributions of each component (gating, bilinear fusion, MLP diversity) to performance gains.
- The theoretical connection between MLP's implicit learning and explicit interaction networks remains somewhat hand-wavy without mathematical justification.
- The online A/B test results lack statistical significance testing and confidence intervals for the reported improvements.

## Confidence

**High Confidence**: Experimental methodology is sound with proper train/test splits and standard evaluation metrics. Baseline comparisons include well-established models.

**Medium Confidence**: Claims about MLP effectiveness are supported by empirical evidence but lack theoretical grounding. Online A/B test shows directional improvements without statistical rigor.

**Low Confidence**: The assertion that this is the "first work" to demonstrate two-stream MLP effectiveness is difficult to verify given the vast literature in CTR prediction.

## Next Checks

1. Conduct systematic ablation studies removing each component (gating, bilinear fusion, MLP diversity) to quantify their individual contributions to performance gains across all benchmark datasets.

2. Perform paired statistical tests on online A/B test results to verify that the 1.6% CTR improvement is statistically significant, and provide confidence intervals for all offline metrics.

3. Evaluate FinalMLP on additional CTR datasets with different characteristics (sparse vs. dense features, different domain types) to assess the robustness of the "two-stream MLP effectiveness" claim beyond the four benchmark datasets.