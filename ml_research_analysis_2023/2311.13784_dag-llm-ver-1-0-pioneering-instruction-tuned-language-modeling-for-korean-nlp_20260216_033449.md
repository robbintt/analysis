---
ver: rpa2
title: 'DaG LLM ver 1.0: Pioneering Instruction-Tuned Language Modeling for Korean
  NLP'
arxiv_id: '2311.13784'
source_url: https://arxiv.org/abs/2311.13784
tags:
- language
- korean
- datasets
- instruction
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DaG LLM ver 1.0 addresses the lack of high-quality, instruction-tuned
  Korean language models by developing a model trained on 41 tasks across 13 categories
  using balanced, culturally relevant datasets. The model is based on Polyglot-Ko-5.8b
  and fine-tuned using full fine-tuning with a batch size of 2048 and learning rate
  of 3e-5 on H-100 GPUs.
---

# DaG LLM ver 1.0: Pioneering Instruction-Tuned Language Modeling for Korean NLP

## Quick Facts
- arXiv ID: 2311.13784
- Source URL: https://arxiv.org/abs/2311.13784
- Reference count: 2
- Primary result: Developed instruction-tuned Korean language model trained on authentic datasets rather than translated materials

## Executive Summary
DaG LLM ver 1.0 addresses the critical gap in high-quality, instruction-tuned Korean language models by developing a model trained on 41 tasks across 13 categories using balanced, culturally relevant datasets. The model, based on Polyglot-Ko-5.8b, is fine-tuned using full fine-tuning with a batch size of 2048 and learning rate of 3e-5 on H-100 GPUs. The instruction datasets are constructed through a three-step process involving selection from open-source corpora, expansion of task categories, and refinement with template construction. This work represents a significant advancement in Korean NLP by providing an open, instruction-tuned model trained on authentic Korean datasets rather than translated materials, with deployment via a web interface offering services including question answering, summarization, and specialized legal reasoning through KATALOG.

## Method Summary
The model is developed by fine-tuning the Polyglot-Ko-5.8b base model on 41 instruction-tuned tasks across 13 categories. Training employs full fine-tuning with batch size 2048 (achieved through gradient accumulation of 256) and learning rate 3e-5 on H-100 GPUs. The instruction datasets are constructed through a three-step process: selecting from open-source corpora, expanding task categories, and refining with template construction. Dataset balancing is implemented by capping entries per task to prevent overfitting. The resulting model is deployed via a web interface at https://dag.snu.ac.kr offering various NLP services including question answering, summarization, and legal reasoning through KATALOG.

## Key Results
- DaG LLM ver 1.0 demonstrates proficiency in both natural language understanding and generation tasks
- The model shows specialized capabilities in legal reasoning through its KATALOG service for traffic accident liability analysis
- Instruction datasets are constructed using a systematic three-step process ensuring cultural relevance and task diversity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction tuning with culturally balanced Korean datasets improves task-specific performance over translated data
- Mechanism: The model learns task patterns from native Korean language structures and cultural context, avoiding noise from translation artifacts
- Core assumption: Korean-specific datasets preserve linguistic nuance better than translated ones, leading to more accurate task generalization
- Evidence anchors:
  - [abstract] "trained on authentic Korean datasets rather than translated materials"
  - [section 3.1.8] "Topic classification tasks... guided by datasets such as klue_tc and category_find"
  - [corpus] Weak - neighbor papers focus on hardware design or model scaling, not language dataset quality
- Break condition: If datasets are not sufficiently diverse or balanced, cultural and linguistic nuances may be lost, reducing instruction-following accuracy

### Mechanism 2
- Claim: Full fine-tuning with large batch sizes and gradient accumulation effectively adapts Polyglot-Ko-5.8b to instruction datasets
- Mechanism: The model updates all parameters using the full instruction dataset, leveraging batch size 2048 via gradient accumulation to stabilize learning across diverse tasks
- Core assumption: Sufficient computational resources and batch size allow stable convergence when adapting a base model to many instruction tasks
- Evidence anchors:
  - [section 5.2] "training regimen for DaG LLM is executed by deploying a batch size of 8, bolstered by a Gradient Accumulation setting of 256. This arrangement culminates in an effective batch size of 2048"
  - [abstract] "fine-tuned using full fine-tuning with a batch size of 2048 and learning rate of 3e-5 on H-100 GPUs"
  - [corpus] Weak - neighbor papers discuss model size and efficiency but not fine-tuning batch strategies
- Break condition: If batch size is too small or gradient accumulation too aggressive, training instability or poor convergence may occur

### Mechanism 3
- Claim: Balancing datasets across 41 tasks prevents overfitting to overrepresented categories and improves generalization
- Mechanism: Capping dataset entries per task ensures the model does not bias toward frequent task types, promoting balanced skill development
- Core assumption: Task distribution imbalance in pretraining data leads to overfitting; balancing mitigates this
- Evidence anchors:
  - [section 4.3] "During balancing, we placed a cap on dataset entries per task to prevent overfitting"
  - [section 3.1] Lists 13 categories with specific task counts, implying structured balancing
  - [corpus] Weak - neighbor papers do not discuss dataset balancing strategies
- Break condition: If balancing is too strict, rare but important tasks may be underrepresented, reducing model capability in those areas

## Foundational Learning

- Concept: Instruction tuning methodology
  - Why needed here: Enables the model to follow user-provided instructions across diverse Korean NLP tasks
  - Quick check question: What is the difference between full fine-tuning and LoRA in the context of instruction tuning?

- Concept: Dataset balancing and bias mitigation
  - Why needed here: Ensures the model does not overfit to overrepresented tasks and maintains fairness across task types
  - Quick check question: How does capping dataset entries per task help prevent overfitting?

- Concept: Korean linguistic and cultural context
  - Why needed here: Native Korean datasets preserve cultural nuance and linguistic structure better than translated data
  - Quick check question: Why might translated instruction datasets introduce noise compared to native Korean datasets?

## Architecture Onboarding

- Component map:
  - Polyglot-Ko-5.8b (5.8B parameters) -> Full fine-tuning layer -> Balanced instruction datasets (41 tasks, 13 categories) -> Web deployment interface

- Critical path:
  1. Load base Polyglot-Ko-5.8b model
  2. Prepare balanced instruction datasets
  3. Configure training: batch size 2048 (via gradient accumulation), learning rate 3e-5
  4. Run full fine-tuning on H-100 GPUs
  5. Deploy via web interface

- Design tradeoffs:
  - Full fine-tuning vs. parameter-efficient methods (LoRA): Full fine-tuning offers better task adaptation but is more resource-intensive
  - Large batch size vs. memory constraints: Achieves stable training but requires significant GPU memory
  - Balanced datasets vs. natural distribution: Prevents overfitting but may underrepresent rare tasks

- Failure signatures:
  - Poor instruction-following: Likely due to insufficient dataset diversity or translation artifacts
  - Training instability: Could indicate batch size too large or learning rate too high
  - Overfitting to certain tasks: Suggests imbalance in dataset distribution

- First 3 experiments:
  1. Fine-tune on a single task category (e.g., QA) to verify basic instruction-following capability
  2. Test model on translated vs. native Korean datasets to measure performance impact
  3. Evaluate task-specific performance after full fine-tuning to identify overfitting or underfitting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific sources of bias in the DaG LLM ver 1.0 model, and how can they be effectively measured and mitigated in future iterations?
- Basis in paper: [explicit] The paper discusses balancing processing to ensure equitable representation and mitigate biases, but does not specify the exact sources of bias or detailed methods for measurement and mitigation
- Why unresolved: The paper acknowledges the presence of bias but does not provide a comprehensive analysis of its sources or propose a systematic approach for bias measurement and mitigation
- What evidence would resolve it: Detailed studies on bias sources, bias measurement metrics, and effective mitigation strategies specific to the DaG LLM and Korean language models

### Open Question 2
- Question: How does the performance of DaG LLM ver 1.0 compare to other Korean language models, particularly those that rely on translated datasets, in terms of accuracy and cultural relevance?
- Basis in paper: [explicit] The paper claims that DaG LLM ver 1.0 is trained on authentic Korean datasets, which should provide better cultural and linguistic nuances compared to models using translated materials
- Why unresolved: The paper does not provide comparative performance metrics against other Korean language models, particularly those using translated datasets
- What evidence would resolve it: Empirical studies comparing the performance of DaG LLM ver 1.0 with other Korean language models on various tasks, including accuracy and cultural relevance assessments

### Open Question 3
- Question: What are the long-term implications of using instruction-tuned models like DaG LLM ver 1.0 for the preservation and evolution of the Korean language, and how can these models be adapted to reflect contemporary linguistic practices?
- Basis in paper: [explicit] The paper discusses the importance of culturally relevant datasets and the potential of DaG LLM ver 1.0 to capture linguistic nuances, but does not address the long-term implications or adaptation strategies
- Why unresolved: The paper does not explore the future impact of instruction-tuned models on language preservation or provide strategies for adapting models to evolving linguistic practices
- What evidence would resolve it: Longitudinal studies on the impact of instruction-tuned models on language evolution, and frameworks for updating models to reflect contemporary linguistic trends

## Limitations

- The paper lacks detailed performance metrics and quantitative evaluation results comparing DaG LLM ver 1.0 against existing Korean language models or international baselines
- The claim that native Korean datasets outperform translated materials lacks empirical validation through controlled experiments
- The KATALOG service for legal reasoning is presented without detailed accuracy metrics or legal domain-specific evaluation

## Confidence

- High confidence: The technical training methodology (batch size 2048, learning rate 3e-5, full fine-tuning on H-100 GPUs) is clearly specified and follows standard practices
- Medium confidence: The dataset balancing approach and task distribution claims are plausible but lack quantitative validation
- Low confidence: Performance superiority claims over translated datasets and specific task capabilities (particularly KATALOG) lack empirical support

## Next Checks

1. **Benchmark Comparison**: Conduct head-to-head comparisons of DaG LLM ver 1.0 against established Korean models (KoGPT, HyperCLOVA) and international models on standard Korean NLP benchmarks like KLUE, with specific focus on instruction-following tasks

2. **Translation Artifact Analysis**: Design controlled experiments comparing model performance on native Korean instructions versus machine-translated Korean instructions to empirically validate the claimed superiority of authentic datasets

3. **Legal Domain Evaluation**: Perform detailed evaluation of KATALOG service accuracy using real Korean legal cases, comparing its liability analysis performance against expert human judgment and existing legal AI tools