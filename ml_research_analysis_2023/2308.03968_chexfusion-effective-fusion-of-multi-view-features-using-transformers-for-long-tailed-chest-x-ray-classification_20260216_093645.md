---
ver: rpa2
title: 'CheXFusion: Effective Fusion of Multi-View Features using Transformers for
  Long-Tailed Chest X-Ray Classification'
arxiv_id: '2308.03968'
source_url: https://arxiv.org/abs/2308.03968
tags:
- cation
- classi
- performance
- image
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multi-label long-tailed chest X-ray classification
  using CheXFusion, a transformer-based fusion module that integrates multi-view features
  via self-attention and cross-attention mechanisms. It combines weighted asymmetric
  loss and Ml-decoder to handle class imbalance and label co-occurrence.
---

# CheXFusion: Effective Fusion of Multi-View Features using Transformers for Long-Tailed Chest X-Ray Classification

## Quick Facts
- arXiv ID: 2308.03968
- Source URL: https://arxiv.org/abs/2308.03968
- Reference count: 40
- Primary result: Achieves 0.372 mAP on MIMIC-CXR test set, winning first place in ICCV CVAMD 2023 CXR-LT shared task

## Executive Summary
This paper addresses the challenge of multi-label long-tailed chest X-ray classification by proposing CheXFusion, a transformer-based fusion module that integrates multi-view features using self-attention and cross-attention mechanisms. The approach combines weighted asymmetric loss and Ml-decoder to handle class imbalance and label co-occurrence. Extensive experiments on four major chest X-ray datasets demonstrate state-of-the-art performance, with CheXFusion achieving 0.372 mAP on the MIMIC-CXR test set.

## Method Summary
CheXFusion uses a two-stage approach: first pre-training a ConvNeXt backbone with Ml-Decoder head using weighted asymmetric loss and self-training with soft pseudo-labels, then training the CheXFusion transformer fusion module on multi-view features. The fusion module employs self-attention to model intra-view dependencies and cross-attention to model inter-view dependencies, while segment embeddings and shuffling prevent positional bias. The method handles both inter-class and intra-class imbalance through a combination of weighted binary cross-entropy and asymmetric loss.

## Key Results
- Achieves 0.372 mAP on MIMIC-CXR test set, winning first place in ICCV CVAMD 2023 CXR-LT shared task
- Outperforms existing methods on NIH ChestX-ray14, CheXpert, and VinDr-CXR datasets
- Ablation studies confirm benefits of data balancing, self-training, segment embeddings, and shuffling for improved performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer-based self-attention and cross-attention mechanisms enable effective integration of multi-view features by dynamically weighting contributions from each view based on relevance to each label.
- Mechanism: The transformer encoder uses self-attention to model intra-view dependencies and cross-attention to model inter-view dependencies, creating a learned fusion that accounts for label co-occurrence.
- Core assumption: Attention mechanisms can capture complex relationships between features from different views that correlate with disease presence and co-occurrence.
- Evidence anchors:
  - [abstract] "fusion module, guided by self-attention and cross-attention mechanisms, efficiently aggregates multi-view features while considering label co-occurrence"
  - [section 2.2] "We propose CheXFusion. This transformer-based fusion model does not require modality information, is invariant to the number of samples, and is easily compatible with existing solutions to multi-label classification."

### Mechanism 2
- Claim: Weighted asymmetric loss function effectively addresses both inter-class and intra-class imbalance in multi-label long-tailed classification.
- Mechanism: The weighted binary cross-entropy component increases gradient updates for tail classes, while asymmetric loss mitigates the dominance of negative labels through class-specific focusing parameters.
- Core assumption: Different loss components can independently address distinct forms of imbalance without interfering with each other's effectiveness.
- Evidence anchors:
  - [section 3.4] "By combining the weighted binary cross-entropy loss with asymmetric loss, we can effectively handle both inter-class and intra-class imbalances in the multi-label long-tailed classification task"
  - [section 4.6] "Using wBCE loss alone improves performance for the tail group... Using ASL loss mitigates intra-class imbalances in all classes"

### Mechanism 3
- Claim: Segment embeddings and shuffling prevent the model from learning spurious correlations between image order and label predictions.
- Mechanism: Learnable segment embeddings encode which view each feature map belongs to, while shuffling removes positional bias, forcing the model to learn view-invariant features.
- Core assumption: The order of input views should not contain predictive information for disease classification.
- Evidence anchors:
  - [section 3.3] "we add a different learnable segment embedding to each feature map to indicate which image it belongs to. Since the order of the feature maps is irrelevant to the task, we shuffle them along the first index"
  - [section 4.7] "shuffling leads to the highest performance... This indicates that shuffling is crucial in preventing the model from overfitting to the training image order distribution"

## Foundational Learning

- Concept: Multi-label classification with label co-occurrence
  - Why needed here: Medical images often contain multiple simultaneous pathologies that have known correlations (e.g., cardiomegaly often co-occurs with pulmonary edema)
  - Quick check question: How does multi-label classification differ from multi-class classification in terms of output structure and loss functions?

- Concept: Long-tailed distribution handling
  - Why needed here: Rare diseases in chest X-rays occur far less frequently than common ones, requiring special techniques to prevent the model from ignoring tail classes
  - Quick check question: What are the two main types of imbalance in long-tailed classification, and how do they affect model training differently?

- Concept: Transformer attention mechanisms
  - Why needed here: Self-attention captures intra-view feature relationships while cross-attention enables view-to-view information flow, essential for integrating multi-modal medical data
  - Quick check question: In a transformer encoder, how does self-attention differ from cross-attention in terms of query-key-value relationships?

## Architecture Onboarding

- Component map: Pre-trained ConvNeXt backbone → Feature extraction → Positional encoding + segment embeddings → Shuffled feature sequence → Transformer encoder (self-attention) → Ml-Decoder (cross-attention) → Classification heads
- Critical path: Input images → Backbone feature extraction → Fusion module → Final classification; the backbone is frozen during fusion training
- Design tradeoffs: Using frozen backbone speeds training but prevents joint optimization; segment embeddings add parameters but enable view discrimination
- Failure signatures: Uniform attention weights across views, poor performance on tail classes despite loss weighting, degradation when shuffling is disabled
- First 3 experiments:
  1. Replace CheXFusion with simple weighted average fusion and compare mAP on validation set
  2. Remove segment embeddings and shuffling, retrain, and measure performance drop
  3. Replace asymmetric loss with standard focal loss and evaluate impact on tail class performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does joint training of the backbone and CheXFusion fusion module affect performance compared to the two-stage approach?
- Basis in paper: [explicit] The paper mentions "Future Work: Joint training" as an area for exploration.
- Why unresolved: The current approach freezes the pre-trained backbone during fusion module training for faster training and focused learning, but joint training could potentially optimize overall performance.
- What evidence would resolve it: A controlled experiment comparing performance metrics (mAP, AUC-ROC) between the current two-stage approach and a joint training approach.

### Open Question 2
- Question: How would view-specific backbones with Mixture of Experts (MoE) improve multi-view classification compared to the single shared backbone?
- Basis in paper: [explicit] The paper suggests "view-specific backbone" as a future direction using MoE.
- Why unresolved: While a single backbone reduces complexity, it may not capture unique view characteristics as effectively as specialized backbones.
- What evidence would resolve it: Performance comparison experiments showing mAP/AUC-ROC differences between single backbone and MoE-based view-specific backbone approaches.

### Open Question 3
- Question: What is the optimal number of images (N) to use per patient in the CheXFusion model?
- Basis in paper: [explicit] The paper mentions "By default, we set N = 4" but doesn't explore different values.
- Why unresolved: The paper only uses N=4 by default without investigating whether this is optimal or how performance varies with different N values.
- What evidence would resolve it: An ablation study testing various N values (e.g., 2, 4, 6, 8) and their impact on mAP and AUC-ROC performance.

## Limitations

- The paper leaves several implementation details unspecified, particularly regarding the exact architecture of the Ml-Decoder head and the CheXFusion transformer module
- The two-stage training approach prevents end-to-end optimization and may miss potential performance gains from joint backbone-fusion training
- The model's performance on extremely rare conditions (tail classes with very few training examples) remains uncertain, as the paper doesn't provide per-class analysis for the rarest diseases

## Confidence

- **High confidence**: The core mechanism of using transformer-based fusion for multi-view medical images is well-supported by ablation studies and achieves state-of-the-art results on multiple benchmarks
- **Medium confidence**: The weighted asymmetric loss function effectively handles both inter-class and intra-class imbalance, though the optimal balance between wBCE and ASL components may be dataset-dependent
- **Medium confidence**: Segment embeddings and shuffling prevent overfitting to view order, though the paper doesn't explore alternative view discrimination methods

## Next Checks

1. Conduct per-class analysis on the rarest disease categories to verify that tail classes genuinely benefit from the proposed fusion and loss mechanisms, not just aggregate metrics
2. Test the model's robustness to missing views by systematically dropping individual views during inference and measuring performance degradation across different disease categories
3. Compare against an end-to-end trainable alternative where the backbone is fine-tuned jointly with the fusion module, to quantify the potential performance gap from the two-stage approach