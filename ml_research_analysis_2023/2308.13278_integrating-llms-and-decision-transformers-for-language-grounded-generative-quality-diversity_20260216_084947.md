---
ver: rpa2
title: Integrating LLMs and Decision Transformers for Language Grounded Generative
  Quality-Diversity
arxiv_id: '2308.13278'
source_url: https://arxiv.org/abs/2308.13278
tags:
- behavior
- which
- language
- descriptor
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to address the limitation of Quality-Diversity
  (QD) optimization in generating trajectories that are both diverse and customizable
  beyond a single behavior descriptor. The approach leverages Large Language Models
  (LLMs) to augment QD repertoires with natural language descriptions of trajectories,
  and trains a causal transformer policy conditioned on both behavior descriptors
  and textual prompts.
---

# Integrating LLMs and Decision Transformers for Language Grounded Generative Quality-Diversity

## Quick Facts
- **arXiv ID**: 2308.13278
- **Source URL**: https://arxiv.org/abs/2308.13278
- **Reference count**: 40
- **Primary result**: Proposes a method combining LLMs and Decision Transformers to generate diverse, customizable trajectories conditioned on behavior descriptors and textual prompts, evaluated on semantic maze navigation.

## Executive Summary
This paper addresses the challenge of generating diverse and customizable trajectories in Quality-Diversity (QD) optimization by leveraging Large Language Models (LLMs) and Decision Transformers. The proposed method augments QD repertoires with natural language descriptions of trajectories and trains a causal transformer policy conditioned on both behavior descriptors and textual prompts. This allows users to specify arbitrary target behavior descriptors and provide high-level textual instructions to shape the generated trajectory. The approach is evaluated on a semantic maze navigation benchmark, demonstrating the ability to generate trajectories that align with both the target behavior descriptor and the textual prompt.

## Method Summary
The method combines QD optimization with LLMs and causal transformers to generate diverse, customizable trajectories. It first builds a QD repertoire of policies with behavior descriptors and trajectories. An LLM then generates natural language descriptions for each trajectory based on semantic annotations. A causal transformer is trained on this augmented repertoire, conditioned on both behavior descriptors and text tokens to predict action sequences. To handle multimodal action distributions, the action space is partitioned using k-means clustering, and the transformer predicts both cluster indices and offsets. At test time, the model samples from this distribution to generate trajectories matching target behavior descriptors and textual prompts.

## Key Results
- The proposed method generates trajectories that align with both target behavior descriptors and textual prompts in semantic maze navigation tasks.
- LLM-based evaluation shows moderate correlation (r=0.49) with human assessments of trajectory-prompt alignment.
- The method successfully handles multimodal action distributions through k-means clustering and offset prediction.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The causal transformer can condition on both behavior descriptors and textual prompts simultaneously to generate diverse trajectories.
- Mechanism: The transformer architecture takes token embeddings for text (ET), behavior descriptors (EB), and observation-action sequences (EO, EA) combined with positional and timestamp embeddings. This multi-modal conditioning allows the model to learn joint representations of the desired behavior and textual instructions.
- Core assumption: The LLM-generated natural language descriptions contain sufficient semantic information about trajectories to guide the transformer during training.
- Evidence anchors:
  - [abstract] "leveraging a Large Language Model to augment the repertoire with natural language descriptions of trajectories, and training a policy conditioned on those descriptions"
  - [section] "Our proposed solution, detailed in §3, leverages Large Language Models (LLMs) along a causal transformer architecture to solve those problems jointly"
  - [corpus] Weak - corpus shows related work on behavior-conditioned transformers but not simultaneous language+behavior conditioning
- Break condition: If LLM descriptions are incoherent, hallucinated, or don't capture essential trajectory semantics, the transformer cannot learn meaningful associations between text and behavior.

### Mechanism 2
- Claim: The k-means clustering approach enables modeling of multimodal action distributions in continuous action spaces.
- Mechanism: Actions are decomposed into cluster centers plus offsets. The transformer predicts both the cluster index and the offset for each timestep, allowing it to represent multiple valid action sequences for the same state.
- Core assumption: The action space can be reasonably partitioned into discrete clusters that capture the essential modes of the distribution.
- Evidence anchors:
  - [section] "Handling multimodal action distributions...We use the solution proposed by Shafiullah et al. [19] for multimodal behavior cloning: the set of all action vectors present in the training split is partitioned into K clusters using k-means"
  - [section] "Assuming d−dimensional actions, we map each observation embedding M(oi) to 1) a 1d array of length k representing cluster indexes and 2) a k × d matrix ∆A"
  - [corpus] Weak - corpus doesn't contain specific evidence about k-means clustering for action distributions
- Break condition: If the action distribution has too many modes or the clusters don't capture meaningful groupings, the model performance degrades significantly.

### Mechanism 3
- Claim: LLM-based evaluation provides a reasonable proxy for human assessment of trajectory-prompt alignment.
- Mechanism: The evaluation prompt is optimized through trial and error to maximize correlation with human scores across multiple metrics (MSE, MAE, Kendall's tau, Spearman's rho, Pearson correlation).
- Core assumption: An appropriately engineered prompt can steer the LLM to produce scores that correlate well with human judgments.
- Evidence anchors:
  - [section] "As a surrogate function Ssurrogate(x) ∈ [0, 1]...we consider the use of LLMs for that evaluation, casting the problem as a prompt engineering problem that maximizes a number of similarity/correlation metrics between human made evaluations and those made by a language model"
  - [section] "Figure 5: (a) Histogram of MSE between scores given by gpt4-0314 and human subjects...indicate moderate positive correlation between S and Sq∗ eval both in terms of values and rankings"
  - [corpus] Weak - corpus shows related work on LLM evaluation but not specifically for trajectory-prompt alignment
- Break condition: If the LLM has different biases than human evaluators or the prompt engineering fails to capture the relevant aspects, the correlation breaks down.

## Foundational Learning

- Concept: Quality-Diversity optimization and behavior descriptor spaces
  - Why needed here: Understanding how QD algorithms build diverse repertoires of policies is fundamental to grasping the motivation for this work
  - Quick check question: What is the key difference between traditional RL (finding one optimal policy) and QD optimization (building a repertoire of diverse policies)?

- Concept: Causal transformers and attention mechanisms
  - Why needed here: The proposed model is based on a decoder-only causal transformer that conditions on multiple input modalities
  - Quick check question: In a causal transformer, can the attention for token i attend to tokens j > i? Why or why not?

- Concept: Multimodal action distribution modeling
  - Why needed here: The paper addresses the challenge of continuous action spaces that often have multimodal distributions
  - Quick check question: Why would using a simple MSE loss be problematic for modeling action distributions in navigation tasks?

## Architecture Onboarding

- Component map: LLM text generation -> Semantic annotation function -> Causal transformer -> K-means clustering -> Prediction heads -> Evaluation LLM
- Critical path: LLM descriptions → Transformer training → Test-time sampling → LLM evaluation
- Design tradeoffs:
  - K-means vs. diffusion models for multimodal action modeling
  - Number of clusters vs. action space granularity
  - Prompt complexity vs. evaluation LLM performance
  - Training data diversity vs. model generalization
- Failure signatures:
  - Poor correlation between LLM scores and human scores
  - High variance in behavior descriptor errors across rollouts
  - Generated trajectories that contradict target behavior descriptors
  - LLM descriptions that are incoherent or hallucinate objects
- First 3 experiments:
  1. Train the transformer with k-means clustering disabled (use MSE loss) to establish baseline performance
  2. Vary the number of clusters (e.g., 4, 8, 16) to find optimal granularity for the action space
  3. Test different LLM models (gpt-3.5 vs gpt-4) for description generation to assess impact on training quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed method perform in more complex, realistic robotic environments with dynamic objects and uncertainty?
- Basis in paper: The paper mentions this as a future direction, noting that the current method uses pre-computed, frozen environment representations which won't capture states of dynamic objects. It also doesn't consider uncertainty mitigation strategies used in QD literature.
- Why unresolved: The current experiments are limited to a simplified simulated environment with static objects and deterministic transitions. Real-world applications would require handling dynamic objects and uncertainty.
- What evidence would resolve it: Experiments in more realistic robotic environments with dynamic objects, non-deterministic transitions, and partial observability would demonstrate the method's robustness and limitations in practical settings.

### Open Question 2
- Question: What are the optimal sampling strategies to reduce the considerable stochasticity observed in test-time rollouts?
- Basis in paper: The authors note considerable stochasticity among five rollouts for the same example, with mean and standard deviation of 11.2% and 14.6% relative to the maze diagonal length. They conjecture this is due to noise from LLM-generated descriptions and suboptimal test-time sampling.
- Why unresolved: The paper uses simple sampling from the estimated multimodal action distribution, which introduces significant variability in generated trajectories.
- What evidence would resolve it: Comparing different sampling strategies (e.g., beam search, top-k sampling, nucleus sampling) and their impact on trajectory consistency and quality would identify optimal approaches.

### Open Question 3
- Question: How can the noise introduced by LLM-generated descriptions be quantified and mitigated?
- Basis in paper: The authors acknowledge that LLM-generated descriptions sometimes contain hallucinated objects, miss parts of the trajectory, or reference non-exploitable information, leading to suboptimal supervision. They note this as an area for future work.
- Why unresolved: The current approach relies on noisy LLM-generated descriptions for both training and evaluation, which can introduce errors and inconsistencies.
- What evidence would resolve it: Developing metrics to quantify the quality and relevance of LLM-generated descriptions, and implementing techniques to filter or correct erroneous descriptions, would demonstrate the impact of noise reduction on model performance.

## Limitations
- The method relies on the quality of LLM-generated natural language descriptions, which can introduce noise and inconsistencies in training and evaluation.
- Evaluation is limited to a single semantic maze navigation benchmark, and performance on other domains or more complex environments remains untested.
- The k-means clustering approach for handling multimodal action distributions may break down for highly complex or high-dimensional action spaces.

## Confidence
- **High Confidence**: The core mechanism of using a causal transformer to condition on both behavior descriptors and textual prompts is technically sound and well-supported by the architecture description.
- **Medium Confidence**: The effectiveness of LLM-generated descriptions for guiding policy learning, as the evaluation shows only moderate correlation with human judgments.
- **Medium Confidence**: The k-means clustering approach for multimodal action distributions, as the paper provides theoretical justification but limited empirical validation of this specific component.

## Next Checks
1. **Cross-Domain Validation**: Test the method on a different domain (e.g., robotic manipulation or locomotion) to assess generalization beyond semantic maze navigation. This would reveal whether the approach scales to more complex action spaces and environments.

2. **Alternative Description Methods**: Compare LLM-generated descriptions against human-annotated descriptions and rule-based systems to quantify the impact of description quality on final performance. This would help determine if the moderate correlation with human judgments represents a fundamental limitation or an optimization opportunity.

3. **Cluster Sensitivity Analysis**: Systematically vary the number of clusters (k) and evaluate the impact on both training stability and final performance. This would reveal whether the method is robust to hyperparameter choices in the action space partitioning or requires careful tuning for each new environment.