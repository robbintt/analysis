---
ver: rpa2
title: 'Asking Before Acting: Gather Information in Embodied Decision Making with
  Language Models'
arxiv_id: '2305.15695'
source_url: https://arxiv.org/abs/2305.15695
tags:
- drawer
- think
- diningtable
- countertop
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes "Asking Before Acting" (ABA), a method that
  enables language model agents to gather essential information in unfamiliar environments
  by proactively querying external sources in natural language. When deployed to new
  environments, LLM agents struggle to efficiently collect necessary information,
  leading to suboptimal performance.
---

# Asking Before Acting: Gather Information in Embodied Decision Making with Language Models

## Quick Facts
- arXiv ID: 2305.15695
- Source URL: https://arxiv.org/abs/2305.15695
- Authors: 
- Reference count: 40
- Key outcome: ABA improves success rates by over 40% compared to baseline LLM agents in embodied decision making tasks

## Executive Summary
The paper proposes "Asking Before Acting" (ABA), a method that enables language model agents to gather essential information in unfamiliar environments by proactively querying external sources in natural language. When deployed to new environments, LLM agents struggle to efficiently collect necessary information, leading to suboptimal performance. ABA addresses this by allowing agents to ask pertinent questions and act upon the answers, mimicking human behavior of seeking information before taking action. The method uses in-context learning with annotated examples or imitation learning to learn when and what to ask. Evaluated on ALFWorld and its variants, ABA significantly improves success rates by over 40% compared to baseline LLM agents.

## Method Summary
ABA enables LLM agents to gather information before acting in embodied decision making tasks through two complementary approaches: in-context learning and imitation learning. In in-context learning, the agent is provided with K annotated example trajectories that demonstrate proper question-asking behavior. For imitation learning, the agent is fine-tuned using expert trajectories collected via a PDDL-based expert policy, with noisy actions injected during training to improve robustness. The agent maintains memory of previously acquired information to avoid redundant queries. The method is evaluated on ALFWorld tasks including pick, go, and ambiguous instruction scenarios.

## Key Results
- ABA improves success rates by over 40% compared to baseline LLM agents on ALFWorld tasks
- ABA-IC (in-context learning) achieves 45% success rate while ABA-IL (imitation learning) achieves 54% for ID tasks
- The approach demonstrates ability to remember and reuse acquired information, avoiding redundant queries in subsequent tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning enables LLM agents to acquire the skill of asking relevant questions without parameter updates
- Mechanism: By prepending human-annotated trajectories containing proper question-asking examples to the input, the LLM learns to generalize the pattern of when and what to ask
- Core assumption: LLMs can effectively learn new skills through in-context examples without fine-tuning
- Evidence anchors:
  - [abstract]: "ABA can learn to ask proper questions even only with a modest modification of existing agents by providing in-context examples"
  - [section 3.2.1]: "Recent works focus on embodied planning... also leverage in-context learning to learn the policy. Therefore, an intuitive and natural way is to provide the agent with examples which show the ability to ask appropriate questions at appropriate time"
- Break condition: When task complexity exceeds the token limitations of LLMs, making it impossible to provide sufficient in-context examples

### Mechanism 2
- Claim: Imitation learning with expert trajectories improves performance on complex tasks by learning explicit question-asking policies
- Mechanism: The agent is trained via cross-entropy loss on expert trajectories that include both asking actions and environment interactions, with noisy actions injected to improve robustness
- Core assumption: Expert demonstrations containing proper question-asking behavior can be collected and used to fine-tune the LLM
- Evidence anchors:
  - [abstract]: "Further finetuning ABA with reformulated metadata (ABA-FT) facilitates learning the rationale for asking and allows for additional enhancements especially in tasks that baselines struggle to solve"
  - [section 3.2.2]: "We collect a dataset of N trajectories using expert policy... Then, the policy is trained to maximize the probability of actions across trajectories via the cross-entropy loss"
- Break condition: When expert trajectories are difficult to collect or the task space is too diverse for expert policies to generalize

### Mechanism 3
- Claim: The agent can remember and reuse previously acquired information to avoid redundant queries
- Mechanism: The agent maintains a memory of object locations discovered through previous asking actions, and consults this memory before deciding to ask again
- Core assumption: The agent has access to its historical interactions and can track what information has been gathered
- Evidence anchors:
  - [abstract]: "The approach also demonstrates the ability to remember and reuse acquired information, avoiding redundant queries in subsequent tasks"
  - [section 4.2]: "This adjustment enables the agent to familiarize itself with the object placement and provides an opportunity to test its capability to remember and refrain from repetitive questioning"
- Break condition: When memory becomes too large or when information needs to be updated due to environmental changes

## Foundational Learning

- Concept: Contextual Markov Decision Processes (CMDPs)
  - Why needed here: The problem formulation requires understanding how context affects state transitions and rewards in embodied decision making
  - Quick check question: How does the context c parameterize the transition function p(Â·|s, a, c) in CMDPs?

- Concept: In-context learning
  - Why needed here: The primary mechanism for teaching the LLM to ask questions without fine-tuning relies on providing examples in the prompt
  - Quick check question: What are the token limitations that constrain how many in-context examples can be provided?

- Concept: Imitation learning
  - Why needed here: The fine-tuning approach requires understanding how to train from expert demonstrations with cross-entropy loss
  - Quick check question: How does the noisy action injection (with probability p) help with the distribution shift problem?

## Architecture Onboarding

- Component map:
  - LLM agent (Vicuna-7B) -> Human/External information source (H) implemented as another LLM -> Environment simulator (TextWorld/ALFWorld) -> Memory module for storing acquired information -> Prompt construction module for in-context examples

- Critical path:
  1. Environment provides initial state and task instruction
  2. Agent constructs prompt with in-context examples and current history
  3. LLM generates action (either ask or environment interaction)
  4. If ask, H provides answer based on environment state
  5. Agent updates memory and continues until task completion

- Design tradeoffs:
  - In-context learning vs fine-tuning: In-context is faster but limited by token constraints; fine-tuning is more robust but requires data collection
  - Memory vs computation: Maintaining memory of all previous queries increases efficiency but adds computational overhead
  - Question specificity vs generality: More specific questions get better answers but may require more examples to learn

- Failure signatures:
  - Stuck in loops: Agent repeatedly visits same locations without progress (seen in ReAct baseline)
  - Hallucination: Agent claims to have found objects that don't exist (observed in ReAct examples)
  - Token truncation: Prompt exceeds LLM token limits, causing information loss
  - Memory inconsistency: Agent forgets previously acquired information and asks redundant questions

- First 3 experiments:
  1. ALFWorld pick task: Test basic question-asking and action-following capabilities
  2. Ambiguous ALFWorld task: Test ability to ask clarifying questions when task description is unclear
  3. Multiround ALFWorld: Test ability to remember and reuse information across multiple tasks in same environment

## Open Questions the Paper Calls Out
None explicitly identified in the paper.

## Limitations
- ABA is currently confined to language environments and has not been validated in multimodal settings with visual inputs
- The approach requires either substantial annotated examples for in-context learning or expert trajectories for imitation learning, which may not scale to more complex tasks
- No analysis of how ABA's performance scales with increasing environmental complexity or state space size

## Confidence
- Central claim (>40% improvement): High confidence
- In-context learning approach: Medium confidence
- Imitation learning variant: High confidence
- Memory mechanism: Medium confidence

## Next Checks
1. **Token budget stress test**: Systematically measure how ABA performance degrades as the number of in-context examples approaches Vicuna's token limit, and compare this degradation rate to baseline methods.

2. **Memory consistency validation**: Create a multiround task where object locations change between rounds to test whether the agent correctly updates its memory rather than relying on stale information from previous tasks.

3. **Cross-domain generalization**: Evaluate ABA on a different embodied environment (e.g., ALFRED or Habitat) to assess whether the asking patterns learned from ALFWorld transfer to new physical layouts and object configurations.