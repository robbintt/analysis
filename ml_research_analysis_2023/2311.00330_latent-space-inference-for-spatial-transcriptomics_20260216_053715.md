---
ver: rpa2
title: Latent Space Inference For Spatial Transcriptomics
arxiv_id: '2311.00330'
source_url: https://arxiv.org/abs/2311.00330
tags:
- latent
- data
- spatial
- space
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates a probabilistic machine learning approach
  to obtain full genetic expression information for tissue samples while preserving
  spatial coordinates. This is achieved through mapping single-cell RNA sequencing
  (scRNA-seq) and image-based spatial transcriptomics datasets to a joint latent space
  representation using variational machine learning methods.
---

# Latent Space Inference For Spatial Transcriptomics

## Quick Facts
- arXiv ID: 2311.00330
- Source URL: https://arxiv.org/abs/2311.00330
- Authors: 
- Reference count: 0
- Key outcome: Variational autoencoder-based pipeline maps scRNA-seq and spatial transcriptomics data to joint latent space for spatial gene expression inference

## Executive Summary
This paper presents a probabilistic machine learning approach to obtain full genetic expression information for tissue samples while preserving spatial coordinates. The method maps single-cell RNA sequencing (scRNA-seq) and image-based spatial transcriptomics datasets to a joint latent space representation using variational machine learning methods. The project benchmarks five VAE methods (scPhere, scDHA, VASC, DRA, scVI) and selects scVI for its performance and PyTorch compatibility. A discriminator is implemented to minimize adversarial loss between the latent spaces of scRNA and spatial transcriptomics data. The modified SEDR architecture is used to generate a shared latent space for gene expression and spatial information.

## Method Summary
The method involves training a VAE (scVI) on scRNA-seq data to obtain a latent representation, then training a VGAE (SEDR) on spatial transcriptomics data with adversarial alignment to the scRNA latent space. The pipeline selects top 2000 highly variable genes from scRNA data and top 500 shared genes between scRNA and spatial transcriptomics data. The VAE is trained on scRNA data to obtain fixed latent representations, then the VGAE is trained on spatial transcriptomics data with adversarial loss against the scRNA latent space. The final output is a concatenated latent space that combines both gene expression and spatial information.

## Key Results
- scVI selected as optimal VAE implementation for PyTorch compatibility and performance
- Adversarial training effectively aligns latent spaces from different data modalities
- Combined latent space provides good representation of ground truth cell groupings in tissue
- Functional probabilistic machine learning pipeline delivered with published codebase and documentation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Variational Autoencoders (VAEs) enable optimal low-dimensional latent space representation for high-dimensional single-cell RNA sequencing data by learning probabilistic mappings between data and latent variables.
- **Mechanism:** The encoder network maps high-dimensional gene expression data to a low-dimensional latent space distribution (typically Gaussian), while the decoder reconstructs the original data from the latent space. The VAE jointly optimizes the evidence lower bound (ELBO), which balances reconstruction accuracy with regularization to prevent overfitting.
- **Core assumption:** The high-dimensional gene expression data can be effectively represented in a lower-dimensional latent space without significant loss of biological information.
- **Evidence anchors:**
  - [section] "Variational autoencoders (VAE) have been proven effective in obtaining the optimal latent space for high-dimensional RNA sequencing data."
  - [corpus] "scVI is a popular software package that uses a variational autoencoder (VAE) for the analysis of single-cell RNA sequencing (scRNA-seq) data."
- **Break condition:** If the gene expression data contains highly nonlinear relationships that cannot be captured by the VAE's probabilistic framework, or if the latent dimension is too small to preserve essential biological variability.

### Mechanism 2
- **Claim:** Adversarial training between latent spaces of single-cell RNA sequencing and spatial transcriptomics data enables effective cross-modal mapping by aligning distributions.
- **Mechanism:** A discriminator network is trained to distinguish between latent representations from the two different data modalities. The VAE for spatial transcriptomics data is then trained to generate latent representations that "fool" the discriminator, effectively aligning the two distributions in a shared latent space.
- **Core assumption:** The latent representations of the two data modalities can be aligned through adversarial training, even though they originate from different distributions and capture different types of information.
- **Evidence anchors:**
  - [section] "This loss function is adversarial instead of plain euclidean since we are trying make two latent spaces from different datasets (and thus different distributions) similar."
  - [section] "We first train the discriminator to maximizeL2 so that our discriminator can correctly differentiate the two latent spaces and identify their labels correctly."
- **Break condition:** If the two datasets contain fundamentally incompatible information (e.g., different cell types or tissue origins), the adversarial alignment may fail to produce meaningful mappings.

### Mechanism 3
- **Claim:** Variational Graph Autoencoders (VGAEs) effectively capture spatial relationships in spatial transcriptomics data by encoding graph-structured information.
- **Mechanism:** The VGAE takes an adjacency matrix representing spatial relationships between spots and a feature matrix containing gene expression data. Graph convolutional networks within the encoder learn to aggregate information from neighboring spots, capturing local spatial patterns in the latent representation.
- **Core assumption:** Spatial relationships in tissue can be effectively represented as graph structures where nodes correspond to spots and edges represent spatial proximity.
- **Evidence anchors:**
  - [section] "the encoder of the VGAE consists of graph convolutional networks (GCNs) that is able to encode and decode graph-structured data, which is precisely useful for our spatial transcriptomics datasets."
  - [corpus] "The SEDR framework performs latent mapping of spatial transcriptomics datasets and produces a latent representation of both the spatial graph and gene expression."
- **Break condition:** If the spatial resolution is too coarse or the tissue architecture is too complex to be captured by the graph representation, the VGAE may fail to preserve important spatial information.

## Foundational Learning

- **Concept:** Variational Autoencoders (VAEs)
  - Why needed here: VAEs provide a principled probabilistic framework for dimensionality reduction that is essential for handling the high-dimensional gene expression data while preserving uncertainty and biological variability.
  - Quick check question: What is the key difference between a standard autoencoder and a variational autoencoder in terms of the latent space representation?

- **Concept:** Graph Neural Networks (GNNs)
  - Why needed here: GNNs are crucial for processing the spatial transcriptomics data because they can naturally capture local spatial relationships between spots in tissue samples through message passing.
  - Quick check question: How does a graph convolutional network differ from a standard convolutional neural network in terms of the data structure it processes?

- **Concept:** Adversarial training
  - Why needed here: Adversarial training is necessary for aligning the latent spaces of two different data modalities (scRNA-seq and spatial transcriptomics) by making them indistinguishable to a discriminator network.
  - Quick check question: In an adversarial setup, what is the objective of the generator versus the discriminator?

## Architecture Onboarding

- **Component map:** Data preprocessing pipeline -> VAE training (scVI) -> VGAE training (SEDR) -> Discriminator training -> Adversarial alignment -> Inference pipeline -> Evaluation framework

- **Critical path:** The critical path for this system involves: data preprocessing → VAE training for scRNA-seq → VGAE training for spatial transcriptomics → adversarial alignment → inference. The adversarial alignment step is particularly critical as it directly affects the quality of the cross-modal mapping.

- **Design tradeoffs:** The choice between scVI and other VAE implementations involves balancing PyTorch compatibility with performance (scVI was chosen for easier integration with VGAE). The decision to use adversarial training versus direct alignment methods involves a tradeoff between alignment quality and training stability.

- **Failure signatures:** Poor performance in the kNN evaluation metrics suggests the VAE is not learning meaningful latent representations. High discriminator accuracy during adversarial training indicates poor alignment between modalities. Reconstruction errors in the VGAE suggest issues with capturing spatial relationships.

- **First 3 experiments:**
  1. Run kNN evaluation on the scVI latent representations to verify they capture biological structure.
  2. Train the discriminator to distinguish between scRNA-seq and spatial transcriptomics latent spaces and verify it achieves high accuracy.
  3. Implement and test the adversarial alignment phase, monitoring both discriminator loss and VAE reconstruction loss.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of latent space geometry (e.g., Euclidean vs. hyperbolic) impact the accuracy of spatial inference in the proposed pipeline?
- Basis in paper: [explicit] The paper mentions that scPhere, which uses hyperspherical and hyperbolic spaces, performs better than scVI (which uses Euclidean space). However, scVI was chosen for implementation due to its PyTorch compatibility.
- Why unresolved: The paper does not directly compare the performance of the proposed pipeline using different latent space geometries. The choice of scVI was made based on compatibility rather than optimal performance for spatial inference.
- What evidence would resolve it: Conducting experiments using the proposed pipeline with both scVI and scPhere (or other methods with different latent space geometries) and comparing their performance in terms of spatial inference accuracy would provide evidence to resolve this question.

### Open Question 2
- Question: How does the proposed pipeline handle batch effects and biological variability in the input data?
- Basis in paper: [explicit] The paper mentions that single-cell profiles are impacted by batch effects and biological factors, but notes that these are handled outside of the VAE, scVI. It also mentions that scPhere implements multi-level batch correction.
- Why unresolved: The paper does not provide details on how the proposed pipeline addresses batch effects and biological variability. It only mentions that these factors are considered but does not elaborate on the specific methods used.
- What evidence would resolve it: Investigating and documenting the specific methods used within the proposed pipeline to handle batch effects and biological variability, such as data normalization techniques or batch correction algorithms, would provide evidence to resolve this question.

### Open Question 3
- Question: What is the impact of varying the number of highly variable genes (HVG) selected for the analysis on the performance of the proposed pipeline?
- Basis in paper: [inferred] The paper selects the top 2000 HVGs for the scRNA dataset and the top 500 HVGs shared between the scRNA and spatial transcriptomics datasets. However, it does not explore the impact of varying these numbers on the pipeline's performance.
- Why unresolved: The paper does not investigate how different numbers of HVGs affect the quality of the latent space representation and the accuracy of spatial inference. It only uses fixed numbers based on common practices.
- What evidence would resolve it: Conducting experiments using the proposed pipeline with different numbers of HVGs (e.g., 1000, 1500, 2500) and comparing the performance in terms of latent space quality and spatial inference accuracy would provide evidence to resolve this question.

## Limitations
- Limited scalability testing on small datasets (10,000 cells and 1,000 spots) without validation on larger, more complex tissue samples
- Lack of comprehensive ablation studies to quantify individual contributions of VAE, VGAE, and adversarial training components
- No extensive validation of whether aligned latent spaces capture meaningful cross-modal relationships versus spurious correlations

## Confidence
- **High confidence**: Core VAE and VGAE methodologies for dimensionality reduction and spatial relationship encoding
- **Medium confidence**: Adversarial alignment mechanism for cross-modal mapping
- **Low confidence**: Scalability claims and biological validation of predicted spatial gene expression

## Next Checks
1. **Ablation study**: Remove the adversarial component and directly concatenate latent spaces to quantify the contribution of adversarial training to alignment quality.
2. **Scale testing**: Apply the pipeline to larger datasets (e.g., >100,000 cells) to evaluate scalability and computational efficiency.
3. **Biological validation**: Perform differential expression analysis on the predicted spatial gene expression to verify biological plausibility against known tissue markers.