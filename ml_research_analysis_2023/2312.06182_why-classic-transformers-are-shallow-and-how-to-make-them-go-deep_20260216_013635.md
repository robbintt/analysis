---
ver: rpa2
title: Why "classic" Transformers are shallow and how to make them go deep
arxiv_id: '2312.06182'
source_url: https://arxiv.org/abs/2312.06182
tags:
- similarity
- token
- transformer
- where
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why classic Transformers become ineffective
  when deepened and provides theoretical and empirical evidence that the root cause
  is token similarity escalation (TSE) - tokens become increasingly similar through
  repeated self-attention operations. The authors prove that TSE occurs at a linear
  rate driven by the invariant leading eigenspace and large spectral gaps of attention
  matrices.
---

# Why "classic" Transformers are shallow and how to make them go deep

## Quick Facts
- **arXiv ID**: 2312.06182
- **Source URL**: https://arxiv.org/abs/2312.06182
- **Reference count**: 40
- **Primary result**: Classic Transformers suffer from Token Similarity Escalation (TSE) that prevents effective depth scaling, and a simple de-escalation strategy can mitigate this issue.

## Executive Summary
This paper investigates why classic Transformers become ineffective when deepened, identifying Token Similarity Escalation (TSE) as the root cause. Through theoretical analysis, the authors prove that TSE occurs at a linear rate driven by the invariant leading eigenspace and large spectral gaps of attention matrices. They propose a simple de-escalation strategy that surgically removes excessive similarity without diminishing self-attention's role. Preliminary experiments on moderate-scale post-norm Transformer models show the proposed strategy substantially improves training quality compared to pre-norm models, while post-norm models without de-escalation fail.

## Method Summary
The authors propose a de-escalation strategy that removes excessive token similarity by projecting representations onto the orthogonal complement of the invariant leading eigenspace. The method involves adding a projection step Y = (I - τΠ₁)X after self-attention operations, where Π₁ is the projection onto the all-ones vector. The strategy is tested on Vision Transformer for CIFAR-10 and Transformer-XL for WikiText-103, comparing post-norm, pre-norm, and de-escalated post-norm variants. Training uses AdamW optimizer with specific learning rates and multi-step learning rate scheduling.

## Key Results
- Token similarity provably escalates at a linear rate in classic Transformers due to invariant leading eigenspace and large spectral gaps
- Post-norm Transformers suffer severely from TSE while pre-norm Transformers naturally mitigate it
- The proposed de-escalation strategy substantially improves training quality on moderate-scale models
- Pure post-norm models fail to train effectively at depth 80, while de-escalated variants succeed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token similarity escalation (TSE) occurs due to invariant leading eigenspace and large spectral gaps in self-attention matrices
- Mechanism: Self-attention matrices have leading eigenvalue 1 with eigenvector being the all-ones vector. Repeated application of self-attention operations projects token representations onto this eigenspace, causing tokens to become increasingly similar
- Core assumption: Attention matrices computed via softmax formula maintain these spectral properties across layers
- Evidence anchors:
  - [abstract]: "Our analysis reveals that, driven by the invariant leading eigenspace and large spectral gaps of attention matrices, token similarity provably escalates at a linear rate"
  - [section 2.2.1]: Power method intuition showing convergence to span{1} for stochastic matrices with spectral gaps
  - [corpus]: Weak evidence - related papers discuss depth issues but don't explicitly address spectral properties of attention matrices
- Break condition: If attention matrices lose their stochastic structure or spectral gaps become negligible, TSE would slow or stop

### Mechanism 2
- Claim: The escalation rate depends on the parameter α controlling self-attention strength relative to residual connections
- Mechanism: Larger α values amplify the self-attention contribution, accelerating TSE. The escalation rate formula shows it scales linearly with α²
- Core assumption: Standard initialization schemes maintain the relationship between α and escalation dynamics
- Evidence anchors:
  - [abstract]: "Our analysis reveals that, driven by the invariant leading eigenspace and large spectral gaps of attention matrices, token similarity provably escalates at a linear rate"
  - [section 2.2.2]: Theorem 2.6 provides explicit formula showing escalation rate proportional to α²
  - [corpus]: No direct evidence in related papers about α parameter's role in TSE
- Break condition: If α is reduced sufficiently or residual connections dominate, escalation rate approaches zero

### Mechanism 3
- Claim: Layer normalizations and FFN operations do not interfere with TSE progression
- Mechanism: Layer normalizations preserve the column space structure, and FFN operations only perform right-multiplications that don't change column spaces. These operations maintain the conditions for TSE to continue
- Core assumption: Standard layer normalization and FFN architectures maintain these properties
- Evidence anchors:
  - [section 2.3]: "no discernible reason is seen to expect any impact on TSE from the usual element-wise activation functions" and "layer normalizations do not de-escalate high token similarity"
  - [abstract]: Contrast with mitigation strategies that "reduce, explicitly or implicitly, the role of self-attention"
  - [corpus]: Weak evidence - related papers discuss normalization but don't analyze their impact on TSE specifically
- Break condition: If modified normalization schemes or FFN architectures alter the column space preservation property

## Foundational Learning

- Concept: Eigenvalue decomposition and spectral properties of matrices
  - Why needed here: Understanding how attention matrices' spectral properties drive TSE requires familiarity with eigenvalue analysis and power method convergence
  - Quick check question: Why does the leading eigenvalue of a stochastic matrix being 1 with eigenvector 1 cause convergence to uniform distributions?

- Concept: Stochastic matrices and doubly stochastic matrices
  - Why needed here: Attention matrices are row-stochastic by construction, and their properties as stochastic matrices determine TSE behavior
  - Quick check question: What properties must a matrix have to be considered row-stochastic, and how does this relate to attention matrix construction?

- Concept: Concentration inequalities and sub-gaussian random variables
  - Why needed here: The theoretical analysis relies on concentration bounds to show that TSE occurs with high probability under random initialization
  - Quick check question: How do concentration inequalities help establish that random matrix properties hold across multiple layers with high probability?

## Architecture Onboarding

- **Component map**: Self-attention computation -> residual addition -> layer normalization -> FFN computation -> residual addition -> layer normalization -> optional de-escalation projection
- **Critical path**: Self-attention computation → residual addition → layer normalization → FFN computation → residual addition → layer normalization → optional de-escalation projection
- **Design tradeoffs**: Post-norm vs pre-norm architectures affect how TSE progresses; post-norm allows TSE to accumulate while pre-norm progressively diminishes self-attention's role
- **Failure signatures**: Token similarity approaching 1 (measured by tsim metric), loss plateauing or diverging, gradient norms becoming unstable
- **First 3 experiments**:
  1. Implement the base Transformer with depth 20 and measure tsim values across layers to observe TSE progression
  2. Add the de-escalation operation at different locations in the block and compare tsim trajectories
  3. Test both post-norm and pre-norm variants to verify the theoretical predictions about TSE behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal de-escalation location within the Transformer block for different model scales and tasks?
- Basis in paper: [explicit] The paper mentions that "There are several possible locations in Algorithm 1 to insert the proposed de-escalation step" and that "It remains to be determined whether different locations can make meaningful differences in the performance of large-scale models."
- Why unresolved: The paper only tested one location (after each layer normalization) and found it effective, but didn't systematically explore different locations or their impact on large-scale models.
- What evidence would resolve it: Comparative experiments testing different de-escalation locations (before/after self-attention, before/after FFN, etc.) across various model scales and tasks, measuring performance metrics like perplexity, accuracy, and training stability.

### Open Question 2
- Question: How does the proposed de-escalation strategy perform on large-scale language models compared to pre-norm architectures?
- Basis in paper: [inferred] The paper only tested the strategy on "moderate-scale" transformers (Vision Transformer on CIFAR10 and Transformer-XL on WikiText-103) and states "The potential of the proposed strategy in large language models remains to be assessed."
- Why unresolved: Large-scale language models have different training dynamics and optimization challenges compared to the moderate-scale models tested in the paper.
- What evidence would resolve it: Experiments applying the de-escalation strategy to large-scale language models (e.g., BERT, GPT variants) comparing training stability, convergence speed, and final performance metrics against pre-norm baselines.

### Open Question 3
- Question: What is the theoretical relationship between spectral gap properties of attention matrices and the rate of token similarity escalation?
- Basis in paper: [explicit] The paper shows that "the escalation process can be characterized" by spectral properties of attention matrices, particularly "large spectral gaps commonly present in computed attention matrices," and provides Corollary 2.7 showing the relationship between |λ₂(P)| and escalation rate.
- Why unresolved: While the paper demonstrates the connection empirically and provides bounds, it doesn't establish a rigorous theoretical relationship or explain why attention matrices tend to have large spectral gaps.
- What evidence would resolve it: A theoretical proof establishing the relationship between spectral gap properties (beyond just |λ₂(P)|) and escalation rate, along with analysis of why attention matrices computed from typical initialization schemes exhibit these spectral properties.

## Limitations

- Theoretical analysis assumes random matrix properties that may not hold in practice
- Empirical validation limited to moderate-scale experiments without extensive ablation studies
- Optimal de-escalation location and strength parameter τ remain undetermined
- Generalization to large-scale models and diverse tasks untested

## Confidence

- **High Confidence**: The core mechanism of TSE - that repeated self-attention operations cause token representations to become increasingly similar - is well-established through both theoretical proof and empirical observation. The spectral analysis of attention matrices is mathematically rigorous.
- **Medium Confidence**: The claim that post-norm architectures specifically suffer from TSE while pre-norm architectures naturally mitigate it is supported by experimental evidence but lacks comprehensive ablation studies across different depth ranges and architectures.
- **Medium Confidence**: The de-escalation strategy's effectiveness in improving training quality is demonstrated on moderate-scale models, but the generalization to large-scale models and different tasks remains untested. The choice of τ=1 for complete de-escalation may not be optimal for all settings.

## Next Checks

1. **Scale Validation**: Test the de-escalation strategy on large-scale models (e.g., LLaMA-7B or GPT-2 sized models) across diverse tasks including language modeling, machine translation, and code generation to verify that the TSE mitigation generalizes beyond the moderate-scale experiments presented.

2. **Ablation on De-escalation Parameters**: Systematically vary the de-escalation strength τ and test different insertion points within the Transformer block to determine optimal configurations. Compare against alternative TSE mitigation strategies like modified initialization schemes or attention normalization techniques.

3. **Long-range Dependencies Analysis**: Evaluate whether the de-escalation strategy preserves or enhances the model's ability to capture long-range dependencies by testing on tasks requiring distant information integration (e.g., pronoun resolution in long documents, retrieval-augmented generation) and comparing against both post-norm and pre-norm baselines.