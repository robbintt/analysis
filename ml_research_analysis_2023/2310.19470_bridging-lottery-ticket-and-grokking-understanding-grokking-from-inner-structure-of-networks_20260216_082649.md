---
ver: rpa2
title: 'Bridging Lottery Ticket and Grokking: Understanding Grokking from Inner Structure
  of Networks'
arxiv_id: '2310.19470'
source_url: https://arxiv.org/abs/2310.19470
tags:
- grokking
- tickets
- pruning
- weight
- lottery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the grokking phenomenon by connecting it to
  the lottery ticket hypothesis. It demonstrates that lottery tickets identified after
  perfect generalization (called "grokking tickets") significantly accelerate the
  grokking process across various tasks and architectures.
---

# Bridging Lottery Ticket and Grokking: Understanding Grokking from Inner Structure of Networks

## Quick Facts
- arXiv ID: 2310.19470
- Source URL: https://arxiv.org/abs/2310.19470
- Reference count: 17
- Primary result: Grokking tickets accelerate the grokking phenomenon across multiple tasks and architectures

## Executive Summary
This paper bridges the lottery ticket hypothesis and the grokking phenomenon by demonstrating that lottery tickets identified after perfect generalization (grokking tickets) can significantly accelerate the transition from memorization to generalization. The authors show that these subnetworks possess beneficial structural properties like periodic weight patterns and improved graph properties that facilitate faster learning. The study reveals that weight decay primarily helps discover these tickets rather than maintain them, and that grokking tickets achieve better generalization than dense networks with equivalent weight norms.

## Method Summary
The authors identify grokking tickets by training dense networks until perfect generalization occurs, then applying magnitude pruning to obtain sparse subnetworks. These tickets are reset to their initialization and retrained to compare generalization speed against the original dense model. Experiments span modular addition and MNIST classification tasks using both MLP and Transformer architectures, with AdamW optimization and global magnitude pruning at critical generalization points.

## Key Results
- Grokking tickets accelerate the grokking process across multiple tasks and architectures
- Grokking tickets achieve faster generalization than dense networks with equivalent weight norms
- Grokking tickets exhibit periodic weight patterns and beneficial graph properties
- Weight decay primarily helps discover grokking tickets rather than maintain them

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grokking tickets accelerate grokking by providing subnetworks with beneficial structural properties.
- Mechanism: Lottery tickets identified after perfect generalization (grokking tickets) possess periodic weight patterns and improved graph properties (increased average path lengths, reduced clustering coefficients). These structural properties facilitate the transition from memorization to generalization.
- Core assumption: The structure of the subnetwork, not just weight norms or sparsity, is critical for achieving generalization.
- Evidence anchors:
  - [abstract] "utilizing lottery tickets obtained during the generalizing phase (termed grokked tickets) significantly reduces delayed generalization across various tasks"
  - [section 2] "lottery tickets hypothesis states that randomly initialized over-parameterized networks include sparse subnetworks that reach good performance after train"
  - [corpus] Weak evidence - only 25 related papers found, suggesting this is a niche connection
- Break condition: If structural properties can be shown to be irrelevant to generalization speed, this mechanism fails.

### Mechanism 2
- Claim: Weight decay is primarily important for discovering grokking tickets rather than maintaining them.
- Mechanism: Weight decay during training helps explore the space of subnetworks to find grokking tickets. Once found, these tickets can achieve generalization without weight decay.
- Core assumption: The transition between memorization and generalization corresponds to exploring good subnetworks.
- Evidence anchors:
  - [abstract] "grokking tickets achieve faster generalization than dense networks with equivalent weight norms"
  - [section 4.3] "we discovered that at an appropriate pruning rate, grokking can be achieved even without weight decay"
  - [corpus] Weak evidence - limited citations in related papers
- Break condition: If weight decay remains necessary for maintaining generalization even after grokking tickets are found, this mechanism fails.

### Mechanism 3
- Claim: Grokking tickets acquire good representations faster than base models through periodic patterns.
- Mechanism: Grokking tickets exhibit periodic weight patterns from early training stages, which are associated with good representation learning and faster generalization.
- Core assumption: Good representations are characterized by specific frequency patterns in weight matrices.
- Evidence anchors:
  - [section 6.2] "we investigated the periodicity of each neuron by plotting input-side weight and output-side weight"
  - [section E.1] "grokking tickets contribute to the acquisition of a good representation"
  - [corpus] Weak evidence - limited discussion of representation learning in related papers
- Break condition: If periodic patterns are shown to be incidental rather than causal for good representation, this mechanism fails.

## Foundational Learning

- Concept: Lottery Ticket Hypothesis
  - Why needed here: Understanding that sparse subnetworks can achieve comparable performance to dense networks is crucial for the paper's core argument about grokking tickets.
  - Quick check question: What is the key claim of the Lottery Ticket Hypothesis regarding sparse subnetworks?

- Concept: Magnitude Pruning
  - Why needed here: The method used to identify grokking tickets relies on magnitude pruning, making it essential to understand this technique.
  - Quick check question: How does magnitude pruning determine which weights to keep in a network?

- Concept: Weight Decay
  - Why needed here: Weight decay is a key hyperparameter in the experiments, and its role in discovering grokking tickets is a central claim of the paper.
  - Quick check question: What is the primary effect of weight decay on neural network training?

## Architecture Onboarding

- Component map:
  - Modular addition and MNIST datasets
  - MLP (4-layer for MNIST, 2-layer for modular addition) and Transformer architectures
  - Magnitude pruning for identifying grokking tickets
  - Weight decay regularization
  - AdamW optimizer

- Critical path:
  1. Train dense network until perfect generalization
  2. Apply magnitude pruning to obtain grokking tickets
  3. Reset pruned weights to initialization
  4. Retrain grokking tickets and compare to base model

- Design tradeoffs:
  - Pruning rate selection: Higher rates may remove important connections, lower rates may not sufficiently accelerate generalization
  - Weight decay strength: Affects discovery of grokking tickets but may be unnecessary once found
  - Architecture choice: MLP vs. Transformer may impact grokking dynamics

- Failure signatures:
  - No improvement in generalization speed with grokking tickets
  - Grokking tickets require weight decay to maintain generalization
  - Random pruning performs as well as grokking tickets

- First 3 experiments:
  1. Compare grokking speed of base model vs. grokking tickets on modular addition task using MLP
  2. Test grokking tickets with different pruning rates to find optimal setting
  3. Verify that grokking tickets achieve better generalization than dense models with equal weight norms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific structural properties of grokking tickets make them particularly effective at accelerating the transition from memorization to generalization?
- Basis in paper: [explicit] The paper mentions that grokking tickets exhibit periodic weight patterns and beneficial graph properties such as increased average path lengths and reduced clustering coefficients, but doesn't fully explain why these properties are critical for generalization.
- Why unresolved: While the paper identifies these structural properties, it doesn't establish a clear causal relationship between these properties and the acceleration of grokking. The mechanism by which these structural characteristics enable better generalization remains unclear.
- What evidence would resolve it: Systematic ablation studies that modify individual structural properties (e.g., periodicity, path length, clustering) and measure their impact on grokking speed would help identify which properties are most critical.

### Open Question 2
- Question: Why do pruning at initialization (PaI) methods fail to find effective grokking tickets, while magnitude pruning after generalization succeeds?
- Basis in paper: [explicit] The paper shows that PaI methods (random pruning, Grasp, SNIP, Synflow) perform worse than base models, while magnitude pruning after generalization finds effective tickets.
- Why unresolved: The paper demonstrates the failure of PaI methods but doesn't explain the fundamental reason why pruning after training is necessary. The key difference between the information available at initialization versus after training remains unexplained.
- What evidence would resolve it: Comparative analysis of the weight distributions and importance scores from PaI methods versus post-training pruning, combined with theoretical analysis of what information becomes available only after training.

### Open Question 3
- Question: What is the relationship between the critical pruning rate that enables grokking without weight decay and the underlying network architecture?
- Basis in paper: [inferred] The paper shows that with appropriate pruning rates, grokking can occur without weight decay, suggesting weight decay's primary role is discovering good subnetworks rather than maintaining them.
- Why unresolved: While the paper identifies critical pruning rates for specific architectures (MLP and Transformer), it doesn't explain how these rates relate to architectural properties like depth, width, or attention mechanisms.
- What evidence would resolve it: Systematic experiments varying network architectures while measuring critical pruning rates and analyzing how architectural parameters influence the discovery of effective subnetworks.

### Open Question 4
- Question: How do grokking tickets achieve faster generalization than dense networks with equivalent weight norms?
- Basis in paper: [explicit] The paper shows that grokking tickets with the same L1 and L2 norms as controlled dense models still generalize faster, suggesting structure matters beyond just norm.
- Why unresolved: The paper demonstrates this phenomenon but doesn't provide a mechanistic explanation for why sparse subnetworks with identical norms outperform dense networks. The specific advantages of the sparse structure remain unexplained.
- What evidence would resolve it: Detailed analysis of gradient flow, optimization dynamics, and loss landscape geometry comparing dense networks and grokking tickets with matched norms.

## Limitations
- The connection between structural properties like periodicity and generalization speed is primarily correlational
- Evidence base is relatively narrow with only 25 related papers found
- Limited mechanistic explanation of why periodic patterns facilitate the memorization-to-generalization transition

## Confidence
- **High**: Claims about grokking tickets accelerating generalization across different tasks and architectures
- **Medium**: Claims about the role of weight decay in discovering versus maintaining grokking tickets
- **Low**: Claims about periodic weight patterns being the causal mechanism for faster generalization

## Next Checks
1. **Ablation study on structural properties**: Systematically remove periodic patterns from grokking tickets while preserving other properties to test if they are necessary for accelerated generalization.

2. **Cross-dataset generalization**: Test whether grokking tickets discovered on one task (e.g., modular addition) transfer to related but unseen tasks to validate their representation quality.

3. **Scaling analysis**: Evaluate grokking ticket effectiveness across different model scales (width, depth) to determine if the phenomenon persists beyond the specific architectures tested.