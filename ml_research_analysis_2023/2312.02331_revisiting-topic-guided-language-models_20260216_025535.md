---
ver: rpa2
title: Revisiting Topic-Guided Language Models
arxiv_id: '2312.02331'
source_url: https://arxiv.org/abs/2312.02331
tags:
- language
- topic
- topic-guided
- lstm-lm
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares topic-guided language models with standard
  LSTM language models in a unified setting. Four topic-guided models (TopicRNN, VRTM,
  TDLM, rGBN-RNN) are evaluated on four document-level corpora.
---

# Revisiting Topic-Guided Language Models

## Quick Facts
- arXiv ID: 2312.02331
- Source URL: https://arxiv.org/abs/2312.02331
- Reference count: 12
- One-line primary result: Topic-guided language models fail to outperform standard LSTM baselines in held-out perplexity, with most failing to learn good topics.

## Executive Summary
This paper challenges the prevailing assumption that incorporating explicit topic models into neural language models improves performance. Through systematic evaluation of four topic-guided language models (TopicRNN, VRTM, TDLM, rGBN-RNN) against standard LSTM baselines on four document-level corpora, the authors find that none of the topic-guided models outperform the LSTM in held-out perplexity. Surprisingly, a probe experiment reveals that LSTM hidden states already encode topic information, rendering the additional topic model component redundant. The findings suggest that neural language models are sufficiently expressive to capture topic information without explicit topic modeling.

## Method Summary
The paper evaluates four topic-guided language models (TopicRNN, VRTM, TDLM, rGBN-RNN) against LSTM baselines on four document-level corpora (APNEWS, IMDB, BNC, WikiText-2). All models are trained using truncated BPTT with a sequence length of 30, Adam optimizer (learning rate 0.001), and dropout of 0.4. During evaluation, topic-guided models are carefully conditioned only on previous words rather than the entire document to ensure fair comparison with standard language models. The evaluation includes held-out perplexity, topic coherence measured by NPMI scores, and a probe experiment using linear regression to test whether LSTM hidden states encode topic information. Google News word2vec embeddings are used for initialization across all models.

## Key Results
- None of the four topic-guided language models (TopicRNN, VRTM, TDLM, rGBN-RNN) outperform their corresponding LSTM baselines in held-out perplexity.
- The probe experiment reveals that LSTM hidden states already encode topic information, making explicit topic modeling redundant.
- Topic-guided language models do not learn topics that are more coherent than a standard LDA topic model.

## Why This Works (Mechanism)

### Mechanism 1
LSTM language models already encode topic information in their hidden states, making explicit topic modeling redundant. The hidden state at each timestep summarizes not just local syntax but also global topic information from the entire document context seen so far. The core assumption is that topic information is extractable from hidden states using a simple linear probe trained to predict topic proportions. Evidence comes from the probe experiment showing that LSTM hidden states predict TDLM topic proportions better than chance, with weak additional support from the corpus section. This mechanism would break if the probe fails to predict topic proportions from hidden states better than chance, or if topic-guided models consistently outperform LSTM baselines.

### Mechanism 2
Topic-guided language models fail to improve predictive performance because they condition on future words during evaluation. Models that use future document context to estimate topic proportions at test time have an unfair advantage, violating the autoregressive principle. The core assumption is that proper evaluation should only use past words to estimate topic proportions, consistent with language modeling's causal nature. Evidence comes from the abstract stating that the evaluation conditions the topic model component on only previous words, with weak support from the corpus section. This mechanism would break if all topic-guided models are evaluated using only past context and still fail to outperform LSTM baselines.

### Mechanism 3
The quality of learned topics in topic-guided models is not superior to standard LDA topics. Neural topic-guided models either fail to learn coherent topics or produce topics of similar quality to classical LDA, failing to justify their additional complexity. The core assumption is that topic coherence can be measured using NPMI scores, which correlate with human judgments. Evidence comes from the abstract stating that topics learned by topic-guided models are not better than LDA, with weak support from the corpus section. This mechanism would break if topic-guided models learn significantly more coherent topics than LDA, or if topic coherence strongly correlates with predictive performance.

## Foundational Learning

- Concept: Language modeling and perplexity calculation
  - Why needed here: The paper's core evaluation metric is perplexity, which measures how well a model predicts held-out data.
  - Quick check question: What is the mathematical definition of perplexity, and why is lower perplexity better?

- Concept: Topic modeling with LDA and topic coherence
  - Why needed here: Understanding how LDA works and how topic coherence is measured is crucial for interpreting the paper's comparison between neural and classical topic models.
  - Quick check question: How does LDA generate documents, and what does NPMI measure in the context of topic coherence?

- Concept: Variational inference and ELBO
  - Why needed here: Topic-guided language models are trained using variational inference, and understanding the ELBO objective is key to grasping their training process.
  - Quick check question: What is the ELBO, and how does it relate to the KL divergence between the variational distribution and the true posterior?

## Architecture Onboarding

- Component map: Input words -> LSTM hidden states -> (topic model) -> topic proportions -> next word prediction
- Critical path: For each model, the critical path is: input words → LSTM hidden states → (topic model) → topic proportions → next word prediction
- Design tradeoffs: Topic-guided models add complexity and parameters but may capture document-level semantics better; LSTM-LM is simpler but may already encode sufficient topic information
- Failure signatures: Topic-guided models fail if they cannot learn coherent topics (low NPMI), cannot improve perplexity over LSTM-LM, or if their hidden states do not encode topic information (probe fails)
- First 3 experiments:
  1. Reproduce the perplexity comparison between all four topic-guided models and their corresponding LSTM-LM baselines on APNEWS corpus
  2. Implement and run the probe experiment to test whether LSTM-LM hidden states predict TDLM topic proportions
  3. Compute NPMI topic coherence scores for all learned topics and compare against LDA topics on the same corpus

## Open Questions the Paper Calls Out

### Open Question 1
Do more powerful neural architectures (transformers) inherently encode topic information like LSTMs do, making explicit topic modeling unnecessary? The paper only tests topic-guided models with LSTM architectures, not transformers. Evidence to resolve this would include experiments comparing topic-guided models vs transformer baselines on language modeling tasks, plus probe experiments testing if transformer hidden states encode topic information.

### Open Question 2
Would using Dirichlet priors instead of Gaussian priors for topic proportions in topic-biased language models improve topic coherence? The paper mentions that using Gaussian priors leads to low topic coherence while Dirichlet with reparameterization gradients is prone to posterior collapse. Evidence to resolve this would include training topic-biased models with Dirichlet priors and comparing topic coherence to Gaussian-based versions.

### Open Question 3
Can topic-guided language models provide benefits in controllable text generation that aren't captured by standard perplexity metrics? The paper suggests future work should investigate controllability in topic-guided language models for conditional generation. Evidence to resolve this would include human evaluations of topic-guided model generations conditioned on specific topics versus standard language model outputs.

## Limitations

- The probe experiment's validity depends on the linear regression model's ability to capture the complex relationship between hidden states and topic proportions, which may be oversimplified.
- The paper assumes that held-out perplexity is the definitive metric for evaluating these models, but this metric alone may not capture all aspects of topic-guided models' potential benefits for downstream tasks.
- The conclusion that topic-guided models are fundamentally flawed or unnecessary is based only on predictive performance on four document-level corpora and may not generalize to other applications or domains.

## Confidence

**High Confidence:** The empirical finding that topic-guided models do not outperform LSTM baselines in held-out perplexity. The experimental setup is clearly described, and the results are straightforward to verify through reproduction.

**Medium Confidence:** The interpretation that LSTM hidden states already encode topic information, making explicit topic modeling redundant. While the probe results support this claim, alternative explanations exist, such as the probe's limited capacity or the possibility that topic information is distributed across multiple layers.

**Low Confidence:** The conclusion that topic-guided models are fundamentally flawed or unnecessary. The paper's scope is limited to predictive performance on four document-level corpora, and topic-guided models may still provide benefits for other applications or domains not tested.

## Next Checks

1. **Probe Capacity Analysis:** Conduct ablation studies on the probe model architecture (varying hidden layer sizes, non-linearities) to determine if simple linear regression is sufficient to extract topic information from LSTM hidden states.

2. **Cross-Corpus Evaluation:** Test the topic-guided models on additional corpora with different characteristics (e.g., scientific articles, social media posts) to determine if the negative results generalize across domains or are specific to the tested datasets.

3. **Downstream Task Evaluation:** Evaluate topic-guided models on downstream tasks that explicitly benefit from topic information (e.g., document classification, information retrieval) to assess whether predictive performance on language modeling translates to practical utility.