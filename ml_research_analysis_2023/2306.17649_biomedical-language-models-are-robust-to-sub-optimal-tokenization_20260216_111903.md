---
ver: rpa2
title: Biomedical Language Models are Robust to Sub-optimal Tokenization
arxiv_id: '2306.17649'
source_url: https://arxiv.org/abs/2306.17649
tags:
- biomedical
- language
- which
- pubmedbert
- segmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effect of sub-optimal tokenization
  on biomedical language models. It finds that standard tokenizers like BERT and PubMedBERT
  perform poorly on segmenting biomedical terms into meaningful morphemes.
---

# Biomedical Language Models are Robust to Sub-optimal Tokenization

## Quick Facts
- arXiv ID: 2306.17649
- Source URL: https://arxiv.org/abs/2306.17649
- Reference count: 12
- This paper finds that biomedical language model pre-training is robust to sub-optimal tokenization, as evidenced by BioVocabBERT's superior tokenization performance not translating to better downstream task results compared to PubMedBERT.

## Executive Summary
This paper investigates whether sub-optimal tokenization affects the performance of biomedical language models. Despite creating a superior tokenizer (BioVocabBERT) that segments biomedical terms into more meaningful morphemes, the authors find that pre-training a language model with this tokenizer does not improve downstream task performance compared to PubMedBERT. Through a combination of quantitative experiments and case studies, the research demonstrates that biomedical language models are remarkably robust to tokenization errors, as the pre-training process and model architecture can overcome sub-optimal segmentation decisions.

## Method Summary
The authors create BioVocabBERT, a new tokenizer that uses a vocabulary derived from a fine-tuned morpheme segmentation model and biomedical domain knowledge. They pre-train language models using both BioVocabBERT and PubMedBERT on a 4-billion-word biomedical corpus, then fine-tune these models on named entity recognition (NER) and entity linking (EL) tasks. The evaluation includes tokenization performance metrics (SIGMORPHON F1), downstream task performance (F1 scores), and zero-shot EL performance (R@1, R@5).

## Key Results
- BioVocabBERT tokenizer significantly outperforms PubMedBERT on tokenization metrics, achieving 92.3% vs 60.7% F1 on the biomedical SIGMORPHON subset
- Despite superior tokenization, BioVocabBERT-pretrained model shows no performance improvement on NER and EL tasks compared to PubMedBERT
- Zero-shot entity linking experiments show PubMedBERT performing slightly better than BioVocabBERT, suggesting the model can overcome tokenization errors
- MLM-only pre-training objective appears better suited for entity representation quality than MLM+NSP

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The biomedical language model pre-training process is robust to sub-optimal tokenization.
- Mechanism: Even when tokenizer segments biomedical terms incorrectly (e.g., splitting "neuromod-ulation" into meaningless subwords), the model's parameters can recover the correct semantic representation through contextual embeddings and the [CLS] token.
- Core assumption: The language model architecture can integrate information across subwords to reconstruct meaningful concepts.
- Evidence anchors:
  - [abstract] "These quantitative findings, along with a case study which explores entity representation quality more directly, suggest that the biomedical pre-training process is quite robust to instances of sub-optimal tokenization."
  - [section] "We first observe that the bias we expected to find in the word embedding neighborhoods is evidently present... Fortunately, we observe that the language model is able to readily overcome the bias observed in the word embeddings when it comes to the final '[CLS]' representations."
- Break condition: If the subword errors are so severe that the model cannot infer the correct meaning from context, or if the architecture doesn't have mechanisms to integrate subword information.

### Mechanism 2
- Claim: Domain-specific pre-training with a large biomedical corpus compensates for tokenization errors.
- Mechanism: Pre-training on 4 billion words of biomedical text provides enough context for the model to learn the correct semantic associations between subwords, even when individual subwords are incorrectly segmented.
- Core assumption: The large-scale pre-training data contains enough examples of correctly formed biomedical terms to allow the model to learn the correct semantics.
- Evidence anchors:
  - [abstract] "recent work on domain-specific language models has demonstrated fairly conclusively that using domain-specific data for pre-training significantly improves language model performance on in-domain downstream tasks."
  - [section] "We use the easily accessible and readily preprocessed corpus used for BlueBERT (Peng et al., 2020) pre-training which contains around 4 billion words."
- Break condition: If the pre-training corpus is too small or lacks sufficient examples of the biomedical terms, the model may not be able to learn the correct semantics.

### Mechanism 3
- Claim: The masked language modeling (MLM) objective is better suited for learning entity representations than next-sentence prediction (NSP).
- Mechanism: The MLM objective forces the model to learn the semantics of individual words by predicting masked tokens, while NSP is more focused on sentence-level coherence.
- Core assumption: Entity representation quality is more important for biomedical NLP tasks than sentence-level coherence.
- Evidence anchors:
  - [section] "We note that the main difference between our pre-training setup and the original PubMedBERT setting is the use of the masked language modeling (MLM) objective alone instead of both MLM and next-sentence prediction (NSP) objectives. This suggests that the use of the MLM objective only might be better aligned with obtaining high quality entity representations."
  - [section] "We select named entity recognition (NER) and entity linking (EL), also referred to as concept normalization, as the two biomedical NLP tasks which most closely meet this criterion."
- Break condition: If the downstream tasks require sentence-level coherence rather than entity representation quality, the MLM-only objective might not be optimal.

## Foundational Learning

- Concept: Tokenization and subword segmentation
  - Why needed here: The paper's core argument is about how different tokenization strategies affect language model performance. Understanding tokenization is essential to grasp why the authors' BioVocabBERT tokenizer is designed the way it is.
  - Quick check question: What is the difference between BPE (Byte-Pair Encoding) and WordPiece tokenization? How do they handle out-of-vocabulary words?

- Concept: Biomedical terminology structure
  - Why needed here: The paper argues that biomedical terms are designed to be agglutinating, combining morphemes to create new concepts. This is the foundation for why better morpheme segmentation could improve language models.
  - Quick check question: What is the difference between a morpheme and a word? Can you give an example of a biomedical term that combines multiple morphemes?

- Concept: Pre-training and fine-tuning in NLP
  - Why needed here: The paper's experiments involve pre-training language models on biomedical corpora and then fine-tuning them on specific tasks. Understanding this process is crucial for interpreting the results.
  - Quick check question: What is the difference between pre-training and fine-tuning? Why do we pre-train language models on large corpora before fine-tuning them on specific tasks?

## Architecture Onboarding

- Component map:
  - Tokenizer (BioVocabBERT vs PubMedBERT) -> Pre-training (MLM objective on biomedical corpus) -> Fine-tuning (NER and EL tasks) -> Evaluation (tokenization performance and downstream task performance)

- Critical path:
  1. Build BioVocabBERT tokenizer using supervised morpheme segmentation and UMLS domain knowledge
  2. Pre-train language models (PubMedBERT replica and BioVocabBERT) using MLM objective on biomedical corpus
  3. Fine-tune models on NER and EL tasks
  4. Evaluate tokenization performance and downstream task performance

- Design tradeoffs:
  - Larger vocabulary size for BioVocabBERT (80,181 tokens) vs PubMedBERT (28,895 tokens) may increase pre-training computational cost but could improve tokenization accuracy
  - MLM-only objective vs MLM+NSP objective may improve entity representation quality but could reduce sentence-level coherence
  - Zero-shot EL evaluation vs fully-supervised NER evaluation provides a more direct measure of entity representation quality but may be less representative of real-world performance

- Failure signatures:
  - If BioVocabBERT tokenizer significantly outperforms PubMedBERT tokenizer on tokenization performance but not on downstream tasks, it suggests that tokenization is not the bottleneck for biomedical language models
  - If BioVocabBERT model significantly underperforms PubMedBERT model on downstream tasks, it suggests that the larger vocabulary size or MLM-only objective is detrimental
  - If both models perform similarly on downstream tasks but BioVocabBERT tokenizer significantly outperforms PubMedBERT tokenizer, it suggests that the biomedical pre-training process is robust to tokenization errors

- First 3 experiments:
  1. Evaluate tokenization performance of BioVocabBERT and PubMedBERT tokenizers on SIGMORPHON biomedical subset
  2. Pre-train BioVocabBERT and PubMedBERT models using MLM objective on biomedical corpus
  3. Fine-tune both models on NER and EL tasks and compare performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust are biomedical LMs to tokenization errors across different tasks and model architectures?
- Basis in paper: [explicit] The paper focuses on NER and entity linking, finding robustness to sub-optimal tokenization. However, it acknowledges that the study's scope is limited to these specific tasks.
- Why unresolved: The paper only evaluates robustness on NER and entity linking. It does not explore other biomedical NLP tasks like relation extraction, question answering, or summarization, which might be more sensitive to tokenization errors.
- What evidence would resolve it: Conducting experiments on a wider range of biomedical NLP tasks with various model architectures (e.g., different Transformer variants) would reveal if the observed robustness generalizes across tasks and models.

### Open Question 2
- Question: Does the robustness to tokenization errors depend on the scale of pre-training data?
- Basis in paper: [inferred] The paper uses a relatively large pre-training corpus (4 billion words). It does not explore how the robustness might change with smaller or larger datasets.
- Why unresolved: The paper does not investigate the relationship between pre-training data size and robustness to tokenization errors. It is possible that models trained on smaller datasets might be more sensitive to tokenization errors.
- What evidence would resolve it: Pre-training models with varying amounts of biomedical text and evaluating their robustness to tokenization errors would reveal the impact of pre-training data size on this phenomenon.

### Open Question 3
- Question: Can we leverage the robustness to sub-optimal tokenization to develop more efficient tokenization strategies for biomedical LMs?
- Basis in paper: [inferred] The paper finds that models are robust to tokenization errors, even when the tokenizer is not well-aligned with human judgments. This suggests that there might be room for more efficient tokenization strategies that do not require perfect alignment with human segmentation.
- Why unresolved: The paper does not explore the implications of this robustness for developing new tokenization strategies. It is possible that we can design tokenizers that prioritize efficiency over perfect alignment with human segmentation, relying on the model's ability to overcome tokenization errors.
- What evidence would resolve it: Developing and evaluating tokenization strategies that prioritize efficiency (e.g., using fewer subwords) and measuring their impact on downstream performance would reveal if we can leverage the model's robustness to improve tokenization efficiency.

## Limitations

- The study focuses exclusively on tokenization quality rather than exploring whether improved tokenization could yield performance gains above current state-of-the-art levels
- The evaluation is limited to two specific biomedical NLP tasks (NER and entity linking), which may not generalize to all downstream applications
- The analysis assumes that entity representation quality is the primary factor determining downstream performance, which may not hold for all biomedical NLP tasks

## Confidence

- **High confidence**: The finding that BioVocabBERT tokenizer outperforms PubMedBERT tokenizer on tokenization metrics (SIGHMORPHON F1 scores) - this is directly measurable and clearly demonstrated
- **Medium confidence**: The claim that pre-training is robust to tokenization errors - while the zero-shot entity linking results support this, the effect could be task-dependent
- **Medium confidence**: The assertion that MLM-only objective is better aligned with obtaining high quality entity representations - this is supported by evidence but could be influenced by other factors like pre-training data size

## Next Checks

1. Test the robustness finding across a broader range of biomedical NLP tasks beyond NER and entity linking, including relation extraction and document classification
2. Evaluate whether improved tokenization (BioVocabBERT) can provide performance gains when pre-training data is limited (e.g., 10M vs 4B words)
3. Conduct a more comprehensive ablation study examining the individual contributions of vocabulary size, tokenization quality, and pre-training objectives to downstream performance