---
ver: rpa2
title: One-shot Empirical Privacy Estimation for Federated Learning
arxiv_id: '2302.03098'
source_url: https://arxiv.org/abs/2302.03098
tags:
- privacy
- training
- canary
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a method for empirically estimating the privacy\
  \ loss of differentially private federated learning algorithms without requiring\
  \ multiple training runs or strong assumptions about the adversary. The key idea\
  \ is to inject multiple random \"canary\" clients into the training process and\
  \ use the distribution of cosines between their updates and the final model to estimate\
  \ the privacy parameter \u03B5."
---

# One-shot Empirical Privacy Estimation for Federated Learning

## Quick Facts
- arXiv ID: 2302.03098
- Source URL: https://arxiv.org/abs/2302.03098
- Authors: 
- Reference count: 12
- Key outcome: Proposes method to estimate privacy loss ε for DP federated learning using single training run with canary clients

## Executive Summary
This paper introduces a novel method for empirically estimating the privacy loss parameter ε in differentially private federated learning without requiring multiple training runs or strong adversary assumptions. The approach injects random "canary" clients with isotropic Gaussian updates during training and analyzes the distribution of cosines between these updates and the final model. By comparing observed and unobserved canaries, the method estimates how much information the model has memorized, which directly relates to its privacy guarantees. The technique works with a single training run and requires no a priori knowledge about model architecture or DP algorithm details.

## Method Summary
The method works by generating random canary clients whose updates are sampled from isotropic Gaussian distributions. These canaries are integrated into the federated learning process alongside real clients. After training, the cosine of the angle between each canary's update and the final model is computed. Observed canaries (those that participated in training) will have higher cosine values than unobserved ones due to partial model memorization. By analyzing the distribution of these cosines and comparing them to theoretical distributions under the Gaussian mechanism, the method estimates the privacy parameter ε. The approach can be applied during the same single training run used to fit the model parameters.

## Key Results
- Provides provably correct estimates for privacy loss under the Gaussian mechanism in the limit of high dimensionality
- Demonstrates tight empirical estimates for DP-FedAvg on large-scale federated learning benchmarks
- Shows significantly lower privacy estimates when only final model is released versus intermediate updates
- Enables privacy auditing without multiple training runs or strong adversary assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random canary updates provide an orthogonal basis for measuring memorization.
- Mechanism: Each canary update is sampled from an isotropic Gaussian distribution, making it nearly orthogonal to real client updates and other canaries in high dimensions. The cosine between a canary and the final model indicates whether that canary was observed during training.
- Core assumption: In high-dimensional spaces, isotropically sampled vectors are nearly orthogonal to fixed vectors.
- Evidence anchors:
  - [abstract] "The intuition behind the approach comes from the elementary result that in a high-dimensional space, isotropically sampled vectors are nearly orthogonal with high probability."
  - [section 4] "Asymptotically, this choice of generating the canary model updates is in some sense the worst case... the expected squared cosine between an isotropically sampled vector and an arbitrary fixed vector is 1/d."
- Break condition: If the dimensionality d is too low, the orthogonality assumption breaks down and canary cosines become less discriminative.

### Mechanism 2
- Claim: Cosine angles between canary updates and final model provide a test statistic for membership inference.
- Mechanism: For each canary, compute the cosine of the angle between its update and the final model. Observed canaries should have higher cosine values than unobserved ones, allowing discrimination based on a threshold.
- Core assumption: Models partially overfit to training data, so final parameters are better aligned with observed updates.
- Evidence anchors:
  - [abstract] "We expect that the final model will be better aligned with canary updates that have been observed during training."
  - [section 4] "A canary update that was not observed in training will have vanishing cosine with the final model, while the cosine with one that was observed will be positive."
- Break condition: If the model doesn't overfit or the noise level is too high, the cosine distributions may overlap significantly.

### Mechanism 3
- Claim: The method provides provably correct estimates for the Gaussian mechanism and tight empirical estimates for DP-FedAvg.
- Mechanism: For a single Gaussian mechanism, the method exactly recovers the analytical ε in the limit of high dimensionality. For DP-FedAvg, it provides empirical estimates by comparing cosine distributions between observed and unobserved canaries.
- Core assumption: The Gaussian mechanism's privacy can be inverted from the observed noise level, and the cosine distributions are approximately Gaussian.
- Evidence anchors:
  - [abstract] "We show that our method provides provably correct estimates for privacy loss under the Gaussian mechanism."
  - [section 3] "In the limit as d and k both become large... εest converges to the true, analytical ε."
- Break condition: If the underlying mechanism deviates significantly from Gaussian, or the cosine distributions are not well-approximated by Gaussians, the estimates may be inaccurate.

## Foundational Learning

- Concept: Differential Privacy and its formal definition
  - Why needed here: The paper estimates the privacy parameter ε, which quantifies DP guarantees.
  - Quick check question: What is the difference between user-level and example-level differential privacy?

- Concept: Membership inference attacks and their connection to DP
- Why needed here: The method uses membership inference (distinguishing observed vs unobserved canaries) to estimate privacy loss.
  - Quick check question: How does a successful membership inference attack relate to the privacy parameter ε?

- Concept: Federated learning and DP-FedAvg
  - Why needed here: The method is specifically designed for privacy estimation in federated learning settings.
  - Quick check question: What is the main difference between standard FedAvg and DP-FedAvg?

## Architecture Onboarding

- Component map:
  - Canary client generator -> Training loop integration -> Cosine calculator -> Privacy estimator -> Reporting module

- Critical path:
  1. Generate random canary updates
  2. Integrate canaries into federated training (Algorithm 1 or 2)
  3. Collect cosine values between canaries and final model
  4. Analyze cosine distributions (mean, variance)
  5. Compute privacy estimate using Gaussian comparison

- Design tradeoffs:
  - Number of canaries (k) vs. training overhead: More canaries provide better estimates but increase computation
  - Canary participation pattern: Same as real clients vs. worst-case scenarios
  - Access to intermediate updates vs. final model only: Affects adversary strength and estimate accuracy

- Failure signatures:
  - Cos angles not well-separated: May indicate insufficient noise or model not memorizing
  - High variance in estimates: Could suggest too few canaries or poor approximation of Gaussian distributions
  - Extremely high ε estimates: Might indicate vulnerability to membership inference

- First 3 experiments:
  1. Verify cosine distributions with synthetic data (known observed/unobserved canaries)
  2. Test on a simple federated learning task (e.g., logistic regression) with varying noise levels
  3. Compare estimates with and without intermediate model access on a small neural network task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the method's privacy estimate scale with model dimensionality and number of canaries in non-asymptotic regimes?
- Basis in paper: [explicit] The paper states "While this result is asymptotic, running the algorithm with moderate values of d and k already yields a close approximation."
- Why unresolved: The paper provides empirical results for one specific configuration (d=106, k=103) but doesn't systematically study the scaling behavior.
- What evidence would resolve it: Experiments varying both d and k across multiple orders of magnitude while measuring estimation error.

### Open Question 2
- Question: What is the optimal tradeoff between number of canaries and privacy-accuracy tradeoff in practical deployments?
- Basis in paper: [explicit] "We use 1k canaries for each set of cosines" but don't explore the sensitivity to this choice.
- Why unresolved: The paper fixes 1000 canaries but doesn't investigate how this number affects either privacy estimation quality or model accuracy.
- What evidence would resolve it: Systematic experiments varying canary counts and measuring both ε estimation error and model accuracy degradation.

### Open Question 3
- Question: How robust is the method to violations of the isotropy assumption in real-world federated learning?
- Basis in paper: [inferred] The method assumes canary updates are isotropically distributed, but real client updates may have structure that violates this.
- Why unresolved: The paper doesn't test scenarios where client updates have non-uniform distributions or correlations.
- What evidence would resolve it: Experiments with synthetic structured client distributions or real datasets with known correlation structure.

### Open Question 4
- Question: Can the method be extended to estimate privacy parameters for non-Gaussian mechanisms like DP-SGD with sub-Gaussian noise?
- Basis in paper: [explicit] "We show that our method provides provably correct estimates for the privacy loss under the Gaussian mechanism"
- Why unresolved: The proof relies specifically on Gaussian properties and doesn't generalize to other noise distributions.
- What evidence would resolve it: Extension of the analytical framework to sub-Gaussian noise and experimental validation on DP-SGD implementations.

## Limitations
- The method's effectiveness depends on the choice of canary participation patterns, which may not reflect realistic adversary capabilities
- The theoretical guarantees are asymptotic and may not hold in practical non-asymptotic regimes
- The approach assumes the underlying DP mechanism is approximately Gaussian, which may not hold for all implementations

## Confidence
- High confidence: The orthogonality property of isotropically sampled vectors in high dimensions, and the analytical relationship between cosine distributions and ε for the Gaussian mechanism.
- Medium confidence: The empirical estimates for DP-FedAvg, as they depend on the specific characteristics of the training data and model architecture.
- Low confidence: The generalizability of the method to all federated learning scenarios, particularly those with different data distributions or model architectures.

## Next Checks
1. Test the method on a broader range of federated learning tasks and datasets to assess its generalizability.
2. Compare the privacy estimates with those obtained from established membership inference attack methods to validate the approach.
3. Investigate the impact of different canary participation patterns and numbers on the accuracy and reliability of the privacy estimates.