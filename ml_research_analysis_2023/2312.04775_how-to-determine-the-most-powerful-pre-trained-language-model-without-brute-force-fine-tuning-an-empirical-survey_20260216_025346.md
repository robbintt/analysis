---
ver: rpa2
title: How to Determine the Most Powerful Pre-trained Language Model without Brute
  Force Fine-tuning? An Empirical Survey
arxiv_id: '2312.04775'
source_url: https://arxiv.org/abs/2312.04775
tags:
- methods
- target
- task
- features
- pre-trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys transferability estimation (TE) methods for
  selecting pre-trained language models (PLMs) for downstream tasks without exhaustive
  fine-tuning. TE methods are categorized into model similarity-based methods, which
  require a target model and measure inter-model similarity, and training-free methods,
  which estimate transferability by examining the compatibility between pre-trained
  features and target labels.
---

# How to Determine the Most Powerful Pre-trained Language Model without Brute Force Fine-tuning? An Empirical Survey

## Quick Facts
- arXiv ID: 2312.04775
- Source URL: https://arxiv.org/abs/2312.04775
- Reference count: 21
- Key outcome: H-Score generally performs well among 14 transferability estimation methods evaluated on GLUE benchmark

## Executive Summary
This paper surveys transferability estimation (TE) methods for selecting pre-trained language models (PLMs) for downstream tasks without exhaustive fine-tuning. The authors categorize TE methods into model similarity-based methods (requiring a target model to measure inter-model similarity) and training-free methods (estimating transferability by examining compatibility between pre-trained features and target labels). Through evaluation on the GLUE benchmark, H-Score emerges as the best overall performer, with model similarity-based methods showing better adaptability to different tasks but slower execution, while training-free methods are faster but less adaptable.

## Method Summary
The paper evaluates 14 transferability estimation methods across two categories: model similarity-based methods (DSE, DDS, RSA, kNN, MSC, PARC, GBC) and training-free methods (Logistic, H-Score, Reg. H-Score, NLEEP, TransRate, LogME, SFDA, PACTran). The evaluation uses 6 PLMs (BERT, RoBERTa, and their distilled versions) on 8 GLUE tasks. Methods are assessed using MRR, Spearman correlation, mean training time, and mean estimating time. The minimum viable reproduction plan involves preprocessing GLUE datasets, implementing all 14 TE methods with provided code links, and evaluating them using the specified metrics.

## Key Results
- H-Score achieves superior performance in both effectiveness and efficiency across GLUE tasks
- Model similarity-based methods are more adaptable to different target tasks but require longer computation due to target model training
- Training-free methods offer faster estimation but show reduced adaptability across diverse task types
- Task type, sample size, and feature dimensions significantly impact TE method effectiveness and efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training-free methods estimate transferability by directly comparing pre-trained features with target labels without needing a target model
- Mechanism: These methods use metrics such as class separability or loss approximation to assess how well the pre-trained features align with the target labels. For example, H-Score approximates optimal log-loss by computing inter-class variance and feature redundancy
- Core assumption: Better separation or alignment between pre-trained features and target labels indicates higher transferability
- Evidence anchors:
  - [abstract]: "training-free methods that estimate transferability by examining the compatibility between pre-trained features and target labels"
  - [section 3.3]: "Training-free Methods...try to directly compare the pre-trained features {ϕ(xi)}Ni=1 with the true target labels {yi}Ni=1 by cheap metrics"
- Break condition: If pre-trained features are not informative for the target task or if the target labels are noisy, the estimated transferability may be inaccurate

### Mechanism 2
- Claim: Model similarity-based methods estimate transferability by measuring the similarity between candidate PLMs and a target model trained on the target task
- Mechanism: These methods assume that higher inter-model similarity reflects higher transferability. They compute the similarity between pre-trained features of the candidate PLMs and the target model's features using sample-wise or graph-wise similarity functions
- Core assumption: The similarity between two models' representations correlates with their ability to perform the target task
- Evidence anchors:
  - [abstract]: "Model Similarity-based Methods that assume the inter-model similarity reflects the transferability which require the model trained on target task"
  - [section 3.2]: "the model similarity-based methods are designed based on the assumption that a high similarity between two models correlates with a high degree of transferability between the tasks bonded to the models"
- Break condition: If the target model does not capture the essential features of the target task or if the similarity measure is not representative, the estimated transferability may be misleading

### Mechanism 3
- Claim: Some training-free methods consider the fine-tuning dynamics of pre-trained features to estimate transferability
- Mechanism: These methods simulate the adjustment of pre-trained features during fine-tuning by assuming a linear transformation can be applied. They then assess how well the transformed features align with the target labels
- Core assumption: Pre-trained features can be linearly transformed to better fit the target task, and this transformation reflects the fine-tuning process
- Evidence anchors:
  - [abstract]: "training-free methods...examine the compatibility between pre-trained features and target labels" and "the training dynamics of pre-trained features are considered" for some methods
  - [section 3.3]: "LogME...approximates the marginalized likelihood of label given pre-trained features over all possible linear transformation"
- Break condition: If the actual fine-tuning process involves non-linear transformations or if the linear assumption does not hold, the estimated transferability may not be accurate

## Foundational Learning

- Concept: Transferability Estimation (TE)
  - Why needed here: TE is the core task of this paper, aiming to estimate the performance of a pre-trained model on a target task without exhaustive fine-tuning
  - Quick check question: What are the two main categories of TE methods discussed in the paper?

- Concept: Pre-trained Language Models (PLMs)
  - Why needed here: PLMs are the source models being evaluated for their transferability to target tasks
  - Quick check question: What are some examples of PLMs mentioned in the paper?

- Concept: Target Task
  - Why needed here: The target task is the specific task to which the transferability of PLMs is being evaluated
  - Quick check question: What benchmark dataset is used in the paper to evaluate TE methods?

## Architecture Onboarding

- Component map: PLMs (BERT, RoBERTa, distilled versions) -> GLUE datasets (CoLA, SST-2, MRPC, QQP, MNLI, QNLI, RTE, WNLI) -> TE methods (14 total, 7 model similarity-based, 7 training-free) -> Evaluation metrics (MRR, Spearman correlation, training time, estimating time)
- Critical path: Select TE method → Apply to estimate transferability of candidate PLMs → Evaluate effectiveness and efficiency using MRR, Spearman correlation, training time, and estimating time
- Design tradeoffs: Model similarity-based methods are more adaptable to different target tasks but require training a target model, making them slower. Training-free methods are faster but may be less adaptable and rely on assumptions about feature-label compatibility
- Failure signatures: Inaccurate transferability estimation can occur if the TE method does not capture the true relationship between pre-trained features and target task performance, or if the assumptions underlying the method are violated
- First 3 experiments:
  1. Implement and test the H-Score method on a simple classification task to understand its mechanism
  2. Compare the performance of model similarity-based and training-free methods on a diverse set of GLUE tasks
  3. Analyze the sensitivity of different TE methods to variations in task type, sample size, and feature dimensions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can transferability estimation methods be adapted to account for different fine-tuning strategies beyond fully fine-tuning and parameter-efficient tuning?
- Basis in paper: [explicit] The paper identifies the need to make transferability estimation approaches aware of fine-tuning strategies and experimental hyper-parameters, noting that current approaches usually consider the situation of one strategy whose effectiveness can't be guaranteed in other situations
- Why unresolved: Current TE methods don't consider how different fine-tuning strategies (like adapter tuning or prompt tuning) affect the loss landscape of PLMs and subsequent transferability
- What evidence would resolve it: Empirical studies showing how TE methods perform across various fine-tuning strategies, and development of new metrics that incorporate strategy-specific information

### Open Question 2
- Question: How can transferability estimation methods be adapted for text generation tasks, which have variable output lengths and one-to-many output spaces?
- Basis in paper: [explicit] The paper identifies the need to adapt TE methods to text generation tasks, noting that current methods that consider label information are challenging for generation tasks due to variable output lengths and one-to-many issues
- Why unresolved: Current TE methods are primarily designed for classification tasks and don't adequately capture the complexity of text generation tasks where output space is more complex
- What evidence would resolve it: Development and validation of TE methods specifically designed for text generation tasks, showing improved correlation with actual performance metrics

### Open Question 3
- Question: How can transferability estimation methods be made consistent with specific evaluation metrics, especially when tasks have multiple metrics with varying importance?
- Basis in paper: [explicit] The paper identifies the need to make estimation results consistent with specific evaluation metrics, noting that current methods correlate with single metrics but may be confusing when tasks have multiple metrics with varying importance
- Why unresolved: Current TE methods typically correlate with one evaluation metric, but real-world tasks often have multiple metrics with different importance levels
- What evidence would resolve it: Development of TE methods that can be tuned to prioritize specific evaluation metrics, with empirical validation showing improved correlation with user-specified metrics

## Limitations
- Claims about TE method effectiveness are based on a limited evaluation set (14 methods on 8 GLUE tasks)
- Weak external validation with only 25 related papers identified and minimal citation overlap
- Evaluation metrics (MRR, Spearman correlation) provide relative rankings but don't directly measure actual model performance when fine-tuned
- Limited ablation studies on why specific methods work better under different conditions

## Confidence
- High confidence: The categorization of TE methods into model similarity-based and training-free approaches is well-supported and logically consistent. The observed tradeoff between adaptability and efficiency for different method types is clearly demonstrated through empirical results
- Medium confidence: The identification of H-Score as the best overall performer is supported by the reported metrics but could vary significantly with different task distributions or larger model sets. The factors affecting TE effectiveness (task type, sample size, feature dimensions) are plausible but not extensively validated across diverse conditions
- Low confidence: The mechanism claims for individual TE methods, particularly the fine-tuning dynamics assumption in training-free methods, lack direct empirical validation. The break conditions identified are theoretical rather than demonstrated through systematic failure analysis

## Next Checks
1. Test H-Score and top-performing TE methods on out-of-distribution GLUE tasks or entirely different NLP benchmarks to verify generalizability beyond the original evaluation set
2. Conduct ablation studies varying task complexity, sample sizes, and feature dimensions systematically to quantify the impact of each factor on TE method performance as claimed in the paper
3. Implement a direct validation where the top-ranked models from each TE method are actually fine-tuned and compared against their estimated transferability scores to measure prediction accuracy