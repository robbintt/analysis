---
ver: rpa2
title: A Bayesian framework for discovering interpretable Lagrangian of dynamical
  systems from data
arxiv_id: '2310.06241'
source_url: https://arxiv.org/abs/2310.06241
tags:
- lagrangian
- systems
- data
- bayesian
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a sparse Bayesian framework for discovering
  interpretable Lagrangian models of dynamical systems directly from limited single
  time-series data. Unlike prior neural network approaches, it uses a spike-and-slab
  prior to enforce sparsity, ensuring interpretable mathematical expressions and quantifying
  epistemic uncertainty.
---

# A Bayesian framework for discovering interpretable Lagrangian of dynamical systems from data

## Quick Facts
- arXiv ID: 2310.06241
- Source URL: https://arxiv.org/abs/2310.06241
- Authors: 
- Reference count: 40
- Primary result: Sparse Bayesian framework discovers interpretable Lagrangians from single time-series data with small relative L2 errors (0.6% for cubic-quintic Duffing oscillator)

## Executive Summary
This paper introduces a sparse Bayesian framework for discovering interpretable Lagrangian models of dynamical systems directly from limited single time-series data. Unlike prior neural network approaches, it uses a spike-and-slab prior to enforce sparsity, ensuring interpretable mathematical expressions and quantifying epistemic uncertainty. The method identifies the Lagrangian, derives the Hamiltonian via Legendre transformation, and provides ODE/PDE descriptions of the system. Six examples, including nonlinear oscillators, charged particle dynamics, and PDEs like Navier-Stokes, demonstrate accurate recovery of ground truth models with small relative L2 errors.

## Method Summary
The framework constructs a library of basis functions representing potential terms in the Lagrangian, applies the Euler-Lagrange operator to create a constrained regression problem, and solves it using sparse Bayesian regression with spike-and-slab priors. Gibbs sampling estimates the posterior distribution over coefficients, selecting basis functions with posterior inclusion probability above 0.5. The discovered Lagrangian yields the Hamiltonian through Legendre transformation, enabling ODE/PDE generation. The approach quantifies uncertainty due to limited data and produces interpretable mathematical expressions for the governing equations.

## Key Results
- Recovered cubic-quintic Duffing oscillator Lagrangian with 0.6% relative L2 error
- Discovered Navier-Stokes equations with 5.8% relative L2 error from single time series
- Accurately identified Hamiltonian for Penning trap with 1.5% relative L2 error
- Framework generalizes to high-dimensional systems while maintaining predictive accuracy

## Why This Works (Mechanism)

### Mechanism 1
The spike-and-slab prior enforces sparsity in discovered Lagrangian coefficients, enabling interpretable physical laws. The spike component (Dirac delta at zero) prunes irrelevant basis functions while the slab component (Student-t) allows non-zero coefficients for physically meaningful terms. This creates a discontinuous prior that promotes exact zero coefficients for unnecessary terms. Core assumption: The true Lagrangian can be expressed as a sparse linear combination of pre-selected basis functions.

### Mechanism 2
The Euler-Lagrange operator applied to the Lagrangian library creates constrained regression that biases toward physically valid models. By using the Euler-Lagrange equation as a constraint, the framework ensures discovered Lagrangians produce equations of motion satisfying energy conservation and physical symmetries. Core assumption: The Euler-Lagrange operator can be effectively applied to a pre-defined library of basis functions to create meaningful constraint.

### Mechanism 3
Bayesian inference with Gibbs sampling provides uncertainty quantification for discovered Lagrangian parameters. The posterior distribution over coefficients captures epistemic uncertainty due to limited data, allowing confidence intervals on predictions and model parameters. Core assumption: The posterior distribution can be adequately sampled using Gibbs sampling despite the discontinuous spike-and-slab prior.

## Foundational Learning

- Concept: Lagrangian mechanics and the Euler-Lagrange equation
  - Why needed here: The entire framework is built on discovering the Lagrangian and deriving equations of motion from it
  - Quick check question: What is the relationship between the Lagrangian L = T - V and the equations of motion?

- Concept: Sparse regression and basis function selection
  - Why needed here: The method relies on selecting relevant basis functions from a large dictionary to represent the Lagrangian
  - Quick check question: How does the spike-and-slab prior differ from L1 regularization for inducing sparsity?

- Concept: Bayesian inference and Gibbs sampling
  - Why needed here: The framework uses Bayesian methods to quantify uncertainty and sample from the posterior distribution
  - Quick check question: What are the conditional distributions needed for Gibbs sampling in a spike-and-slab model?

## Architecture Onboarding

- Component map: Data preprocessing → Basis function library construction → Euler-Lagrange operator application → Sparse Bayesian regression → Posterior sampling → Model selection → Hamiltonian derivation → ODE/PDE generation
- Critical path: 1) Generate training data (single time series), 2) Construct design matrix with basis functions, 3) Apply Euler-Lagrange operator to create constraint, 4) Perform sparse Bayesian regression with spike-and-slab prior, 5) Sample posterior via Gibbs sampling, 6) Select basis functions with PIP > 0.5, 7) Derive Hamiltonian via Legendre transformation, 8) Generate equations of motion
- Design tradeoffs: Library size vs. computational cost (larger libraries increase expressiveness but exponential growth in regression coefficients), prior hyperparameters vs. sparsity (stronger spike component increases sparsity but may miss relevant terms), sampling iterations vs. convergence (more iterations improve posterior approximation but increase computation time)
- Failure signatures: PIP values clustering around 0.5 for all basis functions (model cannot distinguish relevant terms), Hamiltonian not conserved in predictions (incorrect Lagrangian discovered), very wide confidence intervals (insufficient data or poor prior specification)
- First 3 experiments: 1) Simple harmonic oscillator: Verify framework recovers L = ½mẋ² - ½kx² from noisy position data, 2) Double pendulum: Test ability to discover coupled Lagrangian with trigonometric terms, 3) Heat equation: Validate performance on a PDE with spatial derivatives in the Lagrangian

## Open Questions the Paper Calls Out

1. How can we design an optimal strategy for selecting the most probable basis functions in the design matrix? The authors note this as a limitation and suggest including all possible kinetic and potential energy functions while using basis functions' uncertainties to create parsimonious models, but don't provide a concrete solution.

2. How can the proposed framework be extended to learn interpretable Lagrangian densities of dissipative and constrained systems? The authors mention this as a future endeavor since the current framework focuses on conservative systems.

3. How does the proposed framework perform in the presence of measurement noise, and what are the limits of its robustness? While the authors conducted sensitivity analysis reporting relative L2 errors for different noise levels, a more comprehensive study is needed to fully understand robustness limitations.

## Limitations
- Framework relies on pre-defined basis function libraries, potentially missing critical physics outside the selected dictionary
- Spike-and-slab prior may be overly aggressive in eliminating relevant terms when true Lagrangian requires dense combinations
- Performance on highly chaotic systems or those with strong non-linearities remains untested
- Computational cost scales poorly with library size, limiting practical application

## Confidence

- **High Confidence**: The sparse Bayesian framework with spike-and-slab prior can recover simple Lagrangian systems from clean data, as demonstrated by the 0.6% error in cubic-quintic Duffing oscillator recovery
- **Medium Confidence**: The framework generalizes to more complex systems like the Penning trap and Navier-Stokes equations, though basis function selection becomes more critical
- **Low Confidence**: Uncertainty quantification via Gibbs sampling is reliable for all tested systems, particularly for high-dimensional or chaotic dynamics where posterior distributions may be highly multimodal

## Next Checks

1. Test the framework on a system with known dense Lagrangian (not sparse) to verify that the spike-and-slab prior doesn't incorrectly eliminate necessary terms, potentially comparing against L1 regularization approaches

2. Conduct systematic ablation studies on the Gibbs sampler convergence by varying library sizes and measuring the stability of posterior estimates across multiple runs with different random seeds

3. Evaluate the framework's performance on noisy, real-world experimental data from physical systems (e.g., pendulum with friction) to assess robustness beyond synthetic data and verify the claimed uncertainty quantification provides meaningful confidence intervals