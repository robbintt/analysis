---
ver: rpa2
title: Negative Sampling with Adaptive Denoising Mixup for Knowledge Graph Embedding
arxiv_id: '2310.09781'
source_url: https://arxiv.org/abs/2310.09781
tags:
- triples
- negative
- sampling
- mixup
- triple
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of false-negative triples in knowledge
  graph embedding (KGE) training, where non-existent triples with high scores might
  actually be true facts due to KG incompleteness. The authors propose DeMix, a framework
  that generates high-quality triples by refining negative triples using an adaptive
  denoising mixup approach.
---

# Negative Sampling with Adaptive Denoising Mixup for Knowledge Graph Embedding

## Quick Facts
- arXiv ID: 2310.09781
- Source URL: https://arxiv.org/abs/2310.09781
- Reference count: 34
- Primary result: DeMix improves MRR by 7.8% on FB15K237 compared to best baseline method

## Executive Summary
This paper addresses the false-negative problem in knowledge graph embedding (KGE) training, where sampled negative triples might actually be true facts due to KG incompleteness. The authors propose DeMix, a self-supervised framework that refines negative triples using adaptive denoising mixup. The framework consists of two modules: MPNE estimates potentially noisy triples using the KGE model's scoring ability, while AdaMix generates higher quality training samples through mixup operations. Experiments on WN18RR and FB15K237 datasets show DeMix outperforms existing methods with significant improvements in link prediction metrics.

## Method Summary
DeMix is a pluggable framework that refines sampled negative triples to address false-negative issues in KGE training. The method works by first generating corrupted triples via standard negative sampling, then classifying them into marginal pseudo-negative and true-negative subsets using the MPNE module based on the KGE model's scoring patterns. The AdaMix module then generates refined triples through mixup operations - partially positive samples for marginal pseudo-negatives and harder negatives for true-negatives. The framework operates in a self-supervised manner without requiring external knowledge or manual labeling, making it generalizable across different KGE models and datasets.

## Key Results
- DeMix achieves 7.8% MRR improvement on FB15K237 compared to best baseline
- The framework is pluggable and can be combined with other negative sampling methods for additional performance gains
- Significant improvements across all link prediction metrics (Hits@1, Hits@3, Hits@10) on both benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
MPNE uses the KGE model's own score predictions to identify potentially false-negative triples by dividing corrupted triples into marginal pseudo-negative (high-score, likely true facts) and true-negative (low-score, clearly false) subsets based on score thresholds relative to positive triples with the same pattern.

### Mechanism 2
AdaMix generates higher quality training triples by mixing entities in embedding space - for marginal pseudo-negatives, mixing with positive triples near decision boundary to create partially positive samples; for true-negatives, mixing with other true-negatives to create harder negatives.

### Mechanism 3
The self-supervised approach works without external knowledge or manual labeling by using only the KGE model's own scoring ability and training data patterns to identify and refine negative triples, making it pluggable and generalizable.

## Foundational Learning

- Concept: Knowledge Graph Embedding (KGE) fundamentals - Understanding how KGE models score triples and learn embeddings is essential to grasp why false negatives are problematic and how DeMix addresses them
- Concept: Negative sampling in KGE training - The entire DeMix framework is built around improving negative sampling quality, so understanding standard negative sampling approaches is crucial
- Concept: Mixup data augmentation technique - AdaMix module uses mixup to generate new triples, so understanding the core mixup concept is necessary to grasp how synthetic triples are created

## Architecture Onboarding

- Component map: KGE Model <- DeMix (MPNE -> AdaMix)
- Critical path: 1) Generate corrupted triples via standard negative sampling, 2) MPNE classifies into marginal pseudo-negative and true-negative subsets, 3) AdaMix selects mixup partners and generates refined triples, 4) KGE model trains on refined triples, 5) MPNE and AdaMix update adaptively as KGE model improves
- Design tradeoffs: Trades computational overhead for improved training quality - MPNE and AdaMix add processing steps but generate more informative training samples
- Failure signatures: Poor MPNE classification leads to wrong supervision signals; poor AdaMix generation creates triples with incompatible semantic relationships; poorly tuned mixup balance parameter creates ineffective training samples
- First 3 experiments: 1) Verify MPNE classification accuracy by injecting known false negatives, 2) Test AdaMix triple generation quality by visualizing entity embeddings, 3) Benchmark convergence speed by comparing training curves with and without DeMix

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several remain unresolved based on the analysis of the work.

## Limitations

- Computational overhead introduced by MPNE and AdaMix modules is not thoroughly analyzed
- Limited validation on larger knowledge graphs beyond benchmark datasets
- Lack of ablation studies to isolate individual contributions of the two modules

## Confidence

- Core claims about DeMix effectiveness: Medium
- Confidence in MPNE module's false-negative detection: Medium
- Confidence in AdaMix's mixup quality: Medium

## Next Checks

1. Conduct ablation studies to quantify individual contributions of MPNE and AdaMix modules
2. Perform sensitivity analysis on critical hyperparameters (estimation threshold µ, mixup balance α) across different KGE models and datasets
3. Evaluate DeMix's scalability and performance on larger knowledge graphs (e.g., Wikidata, DBpedia) to assess practical applicability beyond benchmark datasets