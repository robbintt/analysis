---
ver: rpa2
title: Learning to Predict Concept Ordering for Common Sense Generation
arxiv_id: '2309.06363'
source_url: https://arxiv.org/abs/2309.06363
tags:
- ordering
- concepts
- concept
- sentences
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically investigates how concept ordering affects
  sentence generation quality in generative commonsense reasoning. The authors evaluate
  five language models (BERT-gen, BART-base, T5-base, BART-large, T5-large) fine-tuned
  with three different concept ordering strategies: Original (as in training data),
  Random, and Example (using human-written sentence orderings).'
---

# Learning to Predict Concept Ordering for Common Sense Generation

## Quick Facts
- arXiv ID: 2309.06363
- Source URL: https://arxiv.org/abs/2309.06363
- Reference count: 13
- This paper systematically investigates how concept ordering affects sentence generation quality in generative commonsense reasoning.

## Executive Summary
This paper investigates how concept ordering strategies affect the quality of sentence generation in generative commonsense reasoning tasks. The authors evaluate five language models (BERT-gen, BART-base, T5-base, BART-large, T5-large) fine-tuned with four different concept ordering strategies: Original, Random, Example, and Probabilistic. Experiments on the CommonGen dataset demonstrate that BART-large consistently outperforms other models when fine-tuned with human-written sentence orderings. The study also compares these fine-tuned models against GPT-3 and GPT-3.5, finding that smaller fine-tuned models can outperform larger LLMs on this task. The results highlight the importance of appropriate concept ordering for improving commonsense sentence generation quality.

## Method Summary
The study employs five pre-trained language models (BERT-gen, BART-base, T5-base, BART-large, T5-large) fine-tuned on the CommonGen dataset using four different concept ordering strategies. The Original strategy uses the concept order as provided in the training data, Random shuffles the concepts, Example uses the order from human-written reference sentences, and Probabilistic uses ConceptNet co-occurrence statistics to determine ordering. The models are trained using next-token prediction, and generated sentences are evaluated using multiple metrics including BLEU, ROUGE, METEOR, CIDEr, SPICE, and Coverage. Kendall's τ correlation is used to measure alignment between generated and reference concept orderings. The study also includes experiments with GPT-3 and GPT-3.5 models using prompt-based generation.

## Key Results
- BART-large fine-tuned with Example ordering consistently outperforms other models across multiple evaluation metrics
- The Example ordering strategy significantly improves generation quality compared to Original and Random orderings
- Human-written sentence orderings show only weak correlation with original input concept orderings
- GPT-3 and GPT-3.5 models underperform compared to fine-tuned smaller models, though Example ordering still helps
- Probabilistic ordering using ConceptNet co-occurrence statistics outperforms Original ordering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BART-large fine-tuned with Example ordering consistently outperforms other models across multiple evaluation metrics.
- Mechanism: When input concepts are ordered as they appear in human-written sentences, BART-large leverages its pre-trained syntactic knowledge to generate more natural sentences that better match reference distributions.
- Core assumption: The ordering of concepts in human-written sentences contains valuable contextual clues that can guide generation quality.
- Evidence anchors:
  - [abstract] "BART-large consistently outperforms other models across several evaluation metrics when fine-tuned using the ordering of concepts as they appear in CommonGen training data"
  - [section] "BART-large model consistently outperforms all other LMs considered in this study when fine-tuned using the ordering of concepts as they appear in CommonGen training data"
  - [corpus] Weak evidence - no direct citations to syntactic knowledge in BART
- Break condition: If the human-written sentences themselves contain suboptimal orderings, or if BART's syntactic knowledge doesn't generalize well to concept sets with different properties than training data.

### Mechanism 2
- Claim: Concept ordering strategies significantly improve generation quality compared to Original and Random orderings.
- Mechanism: Proper concept ordering reduces the search space for coherent sentence generation by providing syntactic scaffolding that guides the model toward more natural arrangements.
- Core assumption: The order in which concepts appear constrains the possible sentence structures and improves generation fluency.
- Evidence anchors:
  - [abstract] "Example ordering strategy significantly improves generation quality compared to Original and Random orderings"
  - [section] "Both Probabilistic and Example ordering outperform the Original Ordering presented in CommonGen"
  - [corpus] No direct citations to concept ordering improving search space
- Break condition: If the concept sets are too diverse or the relationships between concepts are too complex for ordering to provide meaningful constraints.

### Mechanism 3
- Claim: Larger LLMs (GPT-3 variants) do not necessarily outperform much smaller fine-tuned models on this task.
- Mechanism: Fine-tuning on task-specific data provides more relevant knowledge than relying solely on general pre-training, especially when the task requires specific reasoning patterns.
- Core assumption: Task-specific fine-tuning provides more relevant knowledge than general pre-training for specialized generation tasks.
- Evidence anchors:
  - [abstract] "the larger GPT3-based large language models (LLMs) variants do not necessarily outperform much smaller LMs on this task, even when fine-tuned on task-specific training data"
  - [section] "we find that the three LLMs underperform compared against the smaller fine-tuned LMs"
  - [corpus] No direct citations comparing fine-tuning vs pre-training effectiveness
- Break condition: If the fine-tuning dataset is too small or the pre-training of larger models already captures the necessary task-specific patterns.

## Foundational Learning

- Concept: Kendall's tau (τ) correlation coefficient
  - Why needed here: Used to measure the correlation between concept orderings in generated sentences versus reference sentences
  - Quick check question: What does a τ value of 0.697 indicate about the relationship between two orderings?

- Concept: BLEU, ROUGE, METEOR, CIDEr, SPICE evaluation metrics
  - Why needed here: These metrics evaluate the quality of generated sentences against reference sentences
  - Quick check question: Which metric specifically evaluates semantic propositional content in image captions?

- Concept: ConceptNet graph traversal and co-occurrence statistics
  - Why needed here: Used to compute probabilistic concept ordering based on ConceptNet path frequencies
  - Quick check question: How are transition probabilities estimated from ConceptNet path counts?

## Architecture Onboarding

- Component map: Fine-tuning pipeline → BART-large/T5-base → Concept ordering strategy → Evaluation metrics
- Critical path: Concept ordering → Model fine-tuning → Sentence generation → Quality evaluation
- Design tradeoffs: Model size vs. fine-tuning effectiveness; deterministic vs. probabilistic ordering; training vs. inference ordering strategies
- Failure signatures: Low τ values indicate poor concept ordering alignment; inconsistent BLEU scores across metrics suggest generation quality issues
- First 3 experiments:
  1. Fine-tune BART-large with Original ordering on CommonGen training data
  2. Fine-tune BART-large with Example ordering using [ORDERING] tokens
  3. Compare Kendall's τ between generated and reference concept orderings for both models

Assumption: The [ORDERING] token format provides meaningful separation between unordered and ordered concept sets for the model.

## Open Questions the Paper Calls Out
The paper identifies several important open questions:

1. What is the optimal method for incorporating external knowledge (like ConceptNet) into language models for concept ordering in generative commonsense reasoning tasks? The authors mention using ConceptNet co-occurrence statistics but note it's just one approach and don't explore other methods of incorporating external knowledge.

2. How do different concept ordering strategies scale with increasing numbers of input concepts beyond the typical 3-5 concepts in CommonGen? The paper focuses on CommonGen where m ≈ 5 concepts but doesn't explore scenarios with larger concept sets.

3. What is the relationship between concept ordering quality and cross-lingual generalization in generative commonsense reasoning? The authors acknowledge this as a limitation, noting they only evaluated on English and the CommonGen dataset.

4. How can prompt engineering be optimized to improve LLM performance on concept ordering tasks without fine-tuning? While the paper shows that fine-tuned smaller models outperform LLMs with simple prompts, it doesn't explore the space of prompt engineering techniques that might improve LLM performance.

## Limitations
- The study's evaluation is limited to the CommonGen dataset, which may not generalize to other commonsense reasoning tasks with different concept distributions or relationship patterns
- The analysis of GPT-3/GPT-3.5 performance relies on comparisons with fine-tuned smaller models, but the exact prompting strategies and few-shot examples used are not fully specified
- The concept ordering strategies are evaluated only with next-token prediction fine-tuning, leaving open questions about their effectiveness with other fine-tuning approaches or few-shot prompting

## Confidence
- **High confidence**: BART-large fine-tuned with Example ordering consistently outperforms other models across multiple evaluation metrics when using CommonGen training data orderings
- **Medium confidence**: Concept ordering strategies significantly improve generation quality compared to Original and Random orderings, based on observed improvements in multiple evaluation metrics
- **Medium confidence**: Larger LLMs (GPT-3 variants) do not necessarily outperform much smaller fine-tuned models on this task, though the gap in performance is notable

## Next Checks
1. **Concept ordering generalizability test**: Evaluate the four concept ordering strategies (Original, Random, Example, Probabilistic) on at least two additional commonsense reasoning datasets to assess whether the observed improvements transfer beyond CommonGen

2. **Decoding strategy ablation study**: Systematically vary decoding parameters (temperature, top-k, nucleus sampling) across all concept ordering strategies to determine whether improvements are robust to different generation settings

3. **Cross-model ordering consistency analysis**: Measure Kendall's τ correlation between concept orderings generated by different fine-tuned models (BART-large, T5-large, etc.) to determine whether the Example ordering strategy produces consistent structural patterns across model architectures