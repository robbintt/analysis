---
ver: rpa2
title: 'Combatting Human Trafficking in the Cyberspace: A Natural Language Processing-Based
  Methodology to Analyze the Language in Online Advertisements'
arxiv_id: '2311.13118'
source_url: https://arxiv.org/abs/2311.13118
tags:
- trafficking
- human
- page
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis addresses the problem of human trafficking in online
  C2C marketplaces through Natural Language Processing techniques. The core method
  involves creating pseudo-labeled datasets with minimal supervision for training
  state-of-the-art NLP models.
---

# Combatting Human Trafficking in the Cyberspace: A Natural Language Processing-Based Methodology to Analyze the Language in Online Advertisements

## Quick Facts
- arXiv ID: 2311.13118
- Source URL: https://arxiv.org/abs/2311.13118
- Reference count: 40
- Primary result: Achieved 0.966 F1 in NER, 0.85 AUC in HTRP, 0.92 AUC in OAD tasks

## Executive Summary
This thesis addresses the problem of human trafficking in online C2C marketplaces through Natural Language Processing techniques. The core method involves creating pseudo-labeled datasets with minimal supervision for training state-of-the-art NLP models. The primary results include achieving an F1 score of 0.966 in Named Entity Recognition, 0.85 AUC in Human Trafficking Risk Prediction, and 0.92 AUC in Organized Activity Detection tasks. The study also demonstrates a methodology for interpreting model predictions to discover new insights in the language used by offenders, contributing to combating human exploitation online.

## Method Summary
The approach creates pseudo-labeled datasets using heuristics based on shared hard identifiers (phone numbers, emails) in escort advertisements to connect related posts. Transformer models (BERT, RoBERTa) are fine-tuned on these datasets for two tasks: Human Trafficking Risk Prediction and Organized Activity Detection. A Named Entity Recognition system extracts identifiers from text, which are used to construct a Relatedness Graph. Integrated Gradients attribution analyzes model predictions to identify language patterns associated with trafficking risk.

## Key Results
- Named Entity Recognition achieved 0.966 F1 score for extracting hard identifiers from escort ad text
- Human Trafficking Risk Prediction model reached 0.85 AUC for classifying high-risk posts
- Organized Activity Detection model achieved 0.92 AUC for identifying connected advertisements
- Attribution analysis revealed language patterns correlating with trafficking risk

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuned transformer models can learn to predict human trafficking risk and organized activity from escort advertisement data.
- **Mechanism:** Pretrained transformer architectures (BERT, RoBERTa) are fine-tuned on task-specific datasets derived from online escort advertisements. The models learn to extract meaningful features from noisy text and metadata to classify posts as high or low risk for trafficking and detect connections between posts.
- **Core assumption:** Pretrained transformer models have sufficient language understanding to adapt to the specific domain of escort advertisements, despite noise and adversarial language.
- **Evidence anchors:**
  - [abstract] "Train and evaluate state-of-the-art NLP models on the datasets defined."
  - [section] "We explored two Transformer models, BERT and RoBERTa. BERT was selected because of its widespread use and compatibility with libraries for interpretability purposes. RoBERTa, in turn, was one of the best-performing models in the Named Entity Recognition system."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.384, average citations=0.0."
- **Break condition:** If the noise and adversarial language in escort ads overwhelms the model's ability to extract useful features, performance will degrade significantly.

### Mechanism 2
- **Claim:** Named Entity Recognition (NER) is crucial for extracting hard identifiers (phone numbers, emails, social media handles) that connect related advertisements.
- **Mechanism:** A robust NER system is developed to identify entities in escort ad text, which are then used to construct a "Relatedness Graph" connecting posts with shared identifiers. This graph is used for pseudo-labeling datasets.
- **Core assumption:** Hard identifiers in escort ads are reliable connectors between related posts, even if other information is obfuscated.
- **Evidence anchors:**
  - [abstract] "Our contribution in this area is an evaluation of state-of-the-art Transformer models to solve the Named Entity Recognition problem in the adversarial context of online escort ads text."
  - [section] "As social media handles, we utilized the usernames of Onlyfans, Snapchat, and Twitter platforms. Instagram was ignored due to low performance in the NER task."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.384, average citations=0.0."
- **Break condition:** If NER performance is too low, the Relatedness Graph will be sparse and unreliable, leading to poor pseudo-labels.

### Mechanism 3
- **Claim:** Integrated Gradients attribution can identify language patterns in escort ads that correlate with human trafficking risk.
- **Mechanism:** After training models on the pseudo-labeled datasets, Integrated Gradients is used to compute attribution scores for tokens and n-grams. High-scoring n-grams are analyzed by domain experts to discover new indicators of trafficking.
- **Core assumption:** The model learns to associate specific language patterns with underlying criminal activity, and these patterns can be identified through attribution analysis.
- **Evidence anchors:**
  - [abstract] "A methodology to analyze the predictions of a trained neural network with the purpose of understanding and finding new insights in the language utilized by offenders."
  - [section] "We analyze the top-scoring n-grams. Figure 4.7 shows a list of the top 10 6-grams with the most positive contribution to the positive-class logit."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.384, average citations=0.0."
- **Break condition:** If the pseudo-labeling process introduces bias, the model may focus on spurious patterns, leading to misleading attribution results.

## Foundational Learning

- **Concept: Transformer Architecture**
  - Why needed here: Transformers are the foundation of modern NLP and are used for both the NER and classification tasks in this work.
  - Quick check question: What is the key innovation of the Transformer architecture that distinguishes it from previous models like RNNs and CNNs?

- **Concept: Named Entity Recognition (NER)**
  - Why needed here: NER is used to extract hard identifiers from escort ad text, which are essential for connecting related posts and creating pseudo-labels.
  - Quick check question: What are the challenges of applying standard NER models to the adversarial context of online escort advertisements?

- **Concept: Integrated Gradients**
  - Why needed here: Integrated Gradients is used to interpret the predictions of the trained models and identify language patterns that contribute to the classification of posts as high risk for trafficking.
  - Quick check question: How does Integrated Gradients address the issue of sensitivity in gradient-based attribution methods?

## Architecture Onboarding

- **Component map:** Data Scraping -> Data Processing -> NER -> Relatedness Graph Construction -> Pseudo-labeling -> Model Training -> Attribution Analysis
- **Critical path:** Data Scraping -> Data Processing -> Relatedness Graph Construction -> Pseudo-labeling -> Model Training -> Attribution Analysis
- **Design tradeoffs:**
  - Using pseudo-labels instead of expert-labeled data allows for larger datasets but introduces potential bias
  - Masking entities in the ad text prevents the model from relying on specific values but may remove important context
  - The choice of threshold for connecting posts in the Relatedness Graph affects the sparsity and quality of the graph
- **Failure signatures:**
  - Low NER performance leading to a sparse Relatedness Graph
  - Model overfitting to spurious patterns in the pseudo-labeled data
  - Attribution analysis revealing biased or misleading language patterns
- **First 3 experiments:**
  1. Evaluate the NER system on a held-out test set to assess its performance on the adversarial context of escort ads
  2. Train a simple classifier (e.g., logistic regression) on the pseudo-labeled data to establish a baseline for the more complex transformer models
  3. Apply Integrated Gradients to a trained model and analyze the top-scoring n-grams to identify potential indicators of human trafficking

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would removing the hard identifiers heuristic entirely be for pseudo-labeling in the HTRP task?
- Basis in paper: explicit
- Why unresolved: The authors hypothesized that the current pseudo-labeling process induces bias toward positive class predictions when phone numbers appear, but did not experiment with removing the hard identifiers heuristic entirely.
- What evidence would resolve it: Conduct experiments comparing model performance and attribution scores with and without the hard identifiers heuristic in the pseudo-labeling process.

### Open Question 2
- Question: Would training transformer models on a joint HTRP and OAD task improve performance compared to individual tasks?
- Basis in paper: explicit
- Why unresolved: The authors only trained models on individual tasks and suggest future work could attempt joint training.
- What evidence would resolve it: Train and evaluate transformer models on a joint HTRP and OAD task and compare performance metrics to individual task models.

### Open Question 3
- Question: How does the choice of similarity metric and threshold impact the quality of the OAD task dataset?
- Basis in paper: explicit
- Why unresolved: The authors selected a similarity metric and threshold for creating the OAD dataset but did not comprehensively evaluate all possible scenarios.
- What evidence would resolve it: Experiment with different similarity metrics and thresholds to create OAD datasets and evaluate model performance and attribution scores for each.

### Open Question 4
- Question: How effective would incorporating additional entity types, such as physical characteristics and services offered, be for the NER system?
- Basis in paper: explicit
- Why unresolved: The authors only targeted location and identifiable information for the NER system and identified other valuable information that could be incorporated as new entity types.
- What evidence would resolve it: Expand the NER system to include additional entity types and evaluate performance on the new entity types compared to the original system.

## Limitations
- Pseudo-labeling introduces uncertainty and potential bias in the results
- NER system performance on highly obfuscated or intentionally misleading text is not fully characterized
- The choice of thresholds for connecting posts in the Relatedness Graph (15-50 distance) is somewhat arbitrary

## Confidence
- **High Confidence**: The NER task results (F1 score of 0.966) are well-supported by standard evaluation metrics and the use of established transformer architectures
- **Medium Confidence**: The HTRP and OAD results (0.85 and 0.92 AUC respectively) are promising but depend heavily on the quality of pseudo-labels, which are not independently verified
- **Medium Confidence**: The attribution analysis methodology is sound, but the interpretation of language patterns requires domain expertise that may introduce subjective bias

## Next Checks
1. **Independent Label Validation**: Obtain a small sample of expert-labeled data to compare against pseudo-labels and assess labeling bias in the HTRP and OAD tasks
2. **Adversarial Robustness Test**: Evaluate NER and classification model performance on intentionally obfuscated or adversarial examples to assess real-world applicability
3. **Cross-platform Generalization**: Test the trained models on data from different online platforms or geographic regions to evaluate generalizability beyond the SkipTheGames dataset