---
ver: rpa2
title: On the Uses of Large Language Models to Interpret Ambiguous Cyberattack Descriptions
arxiv_id: '2306.14062'
source_url: https://arxiv.org/abs/2306.14062
tags:
- descriptions
- tactics
- llms
- gpt-3
- securebert-st
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares the effectiveness of Large Language Models
  (LLMs) like GPT-3.5 versus supervised fine-tuned small-scale LLMs like BERT for
  interpreting cyberattack descriptions into ATT&CK tactics. The researchers found
  that while BaseLLMs with supervised training (SecureBERT-ST) provided clearer and
  more accurate differentiation between ATT&CK tactics, direct use of LLMs like GPT-3.5
  offered broader but less precise interpretations.
---

# On the Uses of Large Language Models to Interpret Ambiguous Cyberattack Descriptions

## Quick Facts
- **arXiv ID**: 2306.14062
- **Source URL**: https://arxiv.org/abs/2306.14062
- **Reference count**: 40
- **Primary result**: SecureBERT-ST achieved 85% accuracy and 92% micro F1, significantly outperforming GPT-3.5's 44% accuracy and 67% micro F1 for ATT&CK tactic classification

## Executive Summary
This study compares the effectiveness of Large Language Models (LLMs) like GPT-3.5 versus supervised fine-tuned small-scale LLMs like BERT for interpreting cyberattack descriptions into ATT&CK tactics. The researchers found that while BaseLLM-STs with supervised training (SecureBERT-ST) provided clearer and more accurate differentiation between ATT&CK tactics, direct use of LLMs like GPT-3.5 offered broader but less precise interpretations. SecureBERT-ST achieved an average accuracy of 85% and micro F1 of 92%, outperforming GPT-3.5's 44% accuracy and 67% micro F1. However, inherent ambiguity in TTP descriptions limited the predictive power of both approaches, especially for CAPEC descriptions.

## Method Summary
The study compares two approaches for classifying cyberattack descriptions into ATT&CK tactics: (1) BaseLLM-STs (BERT and SecureBERT) fine-tuned with supervised learning on ATT&CK data, and (2) direct LLM use (GPT-3.5 and Bard) with engineered prompts. The researchers used 618 ATT&CK descriptions, 177 CAPEC descriptions, and 100 IMDB reviews for baseline testing. BaseLLM-STs were trained using 5-fold cross-validation with AdamW optimizer (learning rate 2e-5) and Sigmoid activation for multi-label classification. LLM approaches used iterative prompt engineering to guide predictions. Performance was evaluated using micro-F1 score and accuracy metrics across all three datasets.

## Key Results
- SecureBERT-ST achieved 85% accuracy and 92% micro F1, significantly outperforming GPT-3.5's 44% accuracy and 67% micro F1
- GPT-3.5 provided broader but less precise interpretations, while BaseLLM-STs offered more focused differentiation between ATT&CK tactics
- Inherent ambiguity in TTP descriptions limited predictive power of both approaches, particularly for CAPEC descriptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BaseLLM-STs with supervised fine-tuning provide more accurate and focused classification of ATT&CK tactics compared to direct LLM use.
- Mechanism: Supervised fine-tuning allows BaseLLMs like BERT and SecureBERT to learn specific patterns and relationships between cybersecurity descriptions and ATT&CK tactics through labeled training data, while direct LLM use relies on broader language understanding without task-specific training.
- Core assumption: The ambiguity in TTP descriptions can be better handled through task-specific training rather than general language understanding.
- Evidence anchors:
  - [abstract] "BaseLLM-STs provide a more focused and clearer differentiation between the ATT&CK tactics (if such differentiation exists)"
  - [section 4.3] "SecureBERT-ST, when trained with labeled data, outperforms GPT-3.5 in predicting ATT&CK Tactics"
  - [corpus] Weak evidence - corpus shows related work on LLM applications but lacks direct comparison of supervised vs. direct approaches
- Break Condition: If TTP descriptions contain patterns that cannot be captured through supervised training, or if the labeled training data is insufficient or biased.

### Mechanism 2
- Claim: GPT-3.5 and Bard provide broader but less precise interpretations of cyberattack techniques compared to BaseLLM-STs.
- Mechanism: Large Language Models like GPT-3.5 and Bard have been trained on extensive internet data, giving them broader language understanding capabilities, but lack the task-specific focus needed for precise TTP classification.
- Core assumption: General language understanding capabilities do not necessarily translate to accurate cybersecurity-specific task performance.
- Evidence anchors:
  - [abstract] "LLMs offer a broader interpretation of cyberattack techniques"
  - [section 4.2] "GPT-3.5 was able to predict almost all the descriptions with the specified format"
  - [section 4.3] "GPT-3.5 tends to predict much more loosely and broadly when interpreting the TTP descriptions"
- Break Condition: If task-specific training data becomes available for LLMs, or if LLMs are fine-tuned for cybersecurity tasks.

### Mechanism 3
- Claim: Inherent ambiguity in TTP descriptions limits the predictive power of both LLM approaches.
- Mechanism: The ambiguity in cybersecurity descriptions, where a single description can map to multiple ATT&CK tactics or contain both approach and intended effect, creates challenges for both supervised and direct LLM approaches.
- Core assumption: The ambiguity in TTP descriptions is a fundamental challenge that cannot be completely eliminated through current LLM approaches.
- Evidence anchors:
  - [abstract] "inherent ambiguity in TTP descriptions used in various cyber operations"
  - [section 4.5] "Case 3: The CAPEC description...both failed to predict 'Impact', the human expert assigned ATT&CK tactic"
  - [section 5.1] "Type 2 Ambiguity can arise when a description contains both the approach (i.e., how) and the intended effect (i.e., why)"
- Break Condition: If new methods are developed to handle ambiguity, such as improved prompt engineering or reinforcement learning with human feedback.

## Foundational Learning

- Concept: Multi-label classification
  - Why needed here: The task involves classifying cybersecurity descriptions into multiple ATT&CK tactics simultaneously, requiring understanding of how to handle multiple labels per instance.
  - Quick check question: How does the model handle cases where a description maps to more than one ATT&CK tactic?

- Concept: Supervised fine-tuning of pre-trained models
  - Why needed here: The study compares the performance of BaseLLM-STs (BERT and SecureBERT) that have been fine-tuned with labeled ATT&CK data against direct LLM use.
  - Quick check question: What is the difference between pre-training and fine-tuning in the context of transformer models?

- Concept: Prompt engineering
  - Why needed here: The study uses engineered prompts to guide GPT-3.5 and Bard in predicting ATT&CK tactics, highlighting the importance of prompt design in direct LLM use.
  - Quick check question: What are the key components of an effective prompt for guiding LLM predictions in cybersecurity tasks?

## Architecture Onboarding

- Component map: Data collection and preprocessing pipeline -> BaseLLM-ST training pipeline -> LLM prediction pipeline -> Evaluation and comparison framework -> Visualization tools for overlap analysis

- Critical path: 1) Collect and preprocess ATT&CK and CAPEC descriptions 2) Train BaseLLM-STs with supervised fine-tuning 3) Engineer prompts for direct LLM use 4) Run predictions on test data 5) Evaluate and compare performance using micro-F1 and accuracy metrics 6) Analyze and visualize overlap patterns

- Design tradeoffs:
  - BaseLLM-STs vs. direct LLM use: focused accuracy vs. broader interpretation
  - Training data size and quality vs. model performance
  - Computational resources for fine-tuning vs. API costs for direct LLM use
  - Model interpretability vs. performance

- Failure signatures:
  - High overlap in predictions for BaseLLM-STs may indicate insufficient differentiation in training data
  - Inconsistent predictions from direct LLM use may suggest prompt engineering issues
  - Low performance on CAPEC descriptions may indicate domain-specific challenges

- First 3 experiments:
  1. Compare BaseLLM-ST performance with different training data sizes
  2. Test different prompt engineering strategies for direct LLM use
  3. Analyze the impact of domain-specific fine-tuning on LLM performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can specialized fine-tuning of LLMs on cybersecurity-specific data improve their ability to distinguish between ATT&CK tactics compared to direct use without fine-tuning?
- Basis in paper: [explicit] The paper suggests that BaseLLM-STs with supervised training on ATT&CK data outperformed direct use of LLMs like GPT-3.5, indicating potential benefits of fine-tuning for specialized tasks.
- Why unresolved: The paper did not experiment with fine-tuning GPT-3.5 or other LLMs on cybersecurity data, only comparing direct use versus fine-tuning of smaller BaseLLMs.
- What evidence would resolve it: Experimental results comparing fine-tuned LLMs against direct use and BaseLLM-STs on the same datasets.

### Open Question 2
- Question: What specific types of inherent ambiguity exist in TTP descriptions, and can these be categorized to improve LLM interpretation?
- Basis in paper: [explicit] The paper identifies Type 1 Ambiguity (multiple tactics implied) and Type 2 Ambiguity (approach vs effect confusion), suggesting these categories exist but require further exploration.
- Why unresolved: The paper only begins to identify ambiguity types without comprehensive categorization or testing how addressing these specific types would improve LLM performance.
- What evidence would resolve it: A systematic study that categorizes all ambiguity types in TTP descriptions and tests LLM performance when these specific types are addressed through targeted prompt engineering or training.

### Open Question 3
- Question: How would continual learning approaches affect LLM performance on TTP interpretation as new vulnerabilities and exploits emerge?
- Basis in paper: [explicit] The paper recommends continual learning as a future direction to handle evolving vulnerabilities, noting that the current approach has limitations with new/unseen vulnerabilities.
- Why unresolved: The paper does not implement or test continual learning approaches, only suggesting it as a potential improvement area.
- What evidence would resolve it: Experimental comparison of static LLMs versus continually updated models on datasets containing both historical and newly introduced vulnerabilities.

## Limitations
- Inherent ambiguity in TTP descriptions fundamentally limits the performance of both LLM approaches regardless of training methodology
- Relatively small dataset size (618 ATT&CK descriptions, 177 CAPEC descriptions) may not fully represent real-world diversity
- Findings may not generalize well to other cybersecurity tasks beyond ATT&CK tactic classification

## Confidence
- **High Confidence**: Comparative performance metrics between SecureBERT-ST and GPT-3.5 are well-supported by reported results
- **Medium Confidence**: Conclusions about impact of inherent ambiguity on model performance are supported but may not capture all ambiguity types
- **Low Confidence**: Generalizability to other cybersecurity tasks and long-term effectiveness as LLMs and cybersecurity landscapes evolve

## Next Checks
1. **Dataset Expansion Test**: Validate findings by testing models on larger, more diverse dataset of cyberattack descriptions from multiple sources
2. **Prompt Engineering Optimization**: Systematically test variations in prompt engineering for GPT-3.5 and Bard to determine if performance gap can be reduced
3. **Cross-Validation with Human Experts**: Conduct blind evaluation where human cybersecurity experts classify subset of descriptions and compare with both model approaches