---
ver: rpa2
title: 'MCTS: A Multi-Reference Chinese Text Simplification Dataset'
arxiv_id: '2306.02796'
source_url: https://arxiv.org/abs/2306.02796
tags:
- simplification
- text
- sentence
- chinese
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MCTS, a multi-reference Chinese text simplification
  dataset to address the lack of evaluation data for Chinese text simplification.
  MCTS contains 723 original sentences and 5 human simplifications for each sentence,
  covering various rewriting transformations.
---

# MCTS: A Multi-Reference Chinese Text Simplification Dataset

## Quick Facts
- **arXiv ID**: 2306.02796
- **Source URL**: https://arxiv.org/abs/2306.02796
- **Reference count**: 11
- **Key outcome**: MCTS dataset contains 723 original sentences with 5 human simplifications each, showing GPT-3.5-turbo outperforms other methods but still lags human performance.

## Executive Summary
This paper introduces MCTS, a multi-reference Chinese text simplification dataset designed to address the lack of evaluation data for Chinese text simplification. The dataset contains 723 original sentences from the Penn Chinese Treebank, each with 5 human-written simplifications covering paraphrasing, compression, and structural changes. The paper evaluates several unsupervised methods and large language models on MCTS, finding that while GPT-3.5-turbo achieves the best performance among automated methods, it still falls short of human-level simplification quality.

## Method Summary
The MCTS dataset was constructed by selecting 723 sentences from the Penn Chinese Treebank and having 5 native Chinese speakers with linguistics or computer science backgrounds provide simplifications for each sentence. The annotation process explicitly instructed annotators to apply three types of rewriting transformations: paraphrasing, compression, and structural change. The dataset was split into 366 validation and 357 test sentences. Various unsupervised methods were implemented including direct back-translation using Google Translate, translated Wiki-Large data, and cross-lingual pseudo data generation. Large language models (GPT-3.5-turbo and text-davinci-003) were evaluated using zero-shot prompting with translated simplification instructions.

## Key Results
- MCTS contains 723 original sentences with 5 human simplifications each, covering diverse rewriting transformations
- GPT-3.5-turbo achieves SARI score of 42.39, outperforming unsupervised methods (direct back-translation: 40.37) but below human references (48.11)
- All automatic methods show lower HSK levels than human references, indicating insufficient lexical simplification
- BLEU scores poorly correlate with simplification quality, highlighting the need for specialized evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-reference data improves evaluation quality by capturing multiple valid simplification strategies
- Mechanism: Multiple human-written simplifications for each sentence expose different ways to simplify (paraphrasing, compression, structural change), making evaluation more robust than single-reference methods
- Core assumption: Different annotators naturally choose different rewriting transformations, providing coverage of all simplification types
- Evidence anchors:
  - [abstract]: "MCTS contains 723 original sentences and 5 human simplifications for each sentence, covering various rewriting transformations"
  - [section 3.2]: Describes annotation instructions that explicitly define three types of rewriting transformations (paraphrasing, compression, structure changing)
- Break condition: If annotators converge on similar simplification strategies due to instruction bias or lack of diversity in the annotator pool

### Mechanism 2
- Claim: Unsupervised methods can serve as effective baselines without requiring parallel training data
- Mechanism: Leveraging back-translation and pseudo-data generation creates reasonable simplifications by exploiting the tendency of neural machine translation to produce high-frequency tokens
- Core assumption: Translation systems naturally simplify language by default frequency, and pseudo-data from English simplification models transfers knowledge to Chinese
- Evidence anchors:
  - [section 5.1]: Describes direct back-translation and cross-lingual pseudo data methods based on Lu et al. (2021) findings about translation frequency effects
  - [section 6.1]: Shows direct back-translation achieves SARI score of 40.37, outperforming other unsupervised methods
- Break condition: If translation systems prioritize accuracy over simplicity, or if English-to-Chinese transfer is ineffective due to language differences

### Mechanism 3
- Claim: Large language models can outperform unsupervised methods but still lag behind human performance
- Mechanism: Zero-shot prompting leverages the general language understanding capabilities of models like GPT-3.5-turbo to generate simplifications without task-specific training
- Core assumption: LLMs have learned simplification patterns from their pretraining data and can apply them when prompted appropriately
- Evidence anchors:
  - [section 5.1]: Describes zero-shot prompting using translated simplification instructions
  - [section 6.1]: Shows GPT-3.5-turbo achieves SARI of 42.39, outperforming unsupervised methods but below gold reference (48.11)
- Break condition: If LLMs prioritize other aspects of text generation over simplification, or if prompt engineering is insufficient to elicit simplification behavior

## Foundational Learning

- Concept: Text simplification rewriting transformations
  - Why needed here: Understanding the three types (paraphrasing, compression, structural change) is essential for designing evaluation metrics and interpreting results
  - Quick check question: What distinguishes "structural changing" from "sentence splitting" in the context of Chinese simplification?

- Concept: Evaluation metrics for simplification
  - Why needed here: SARI, BLEU, and HSK levels measure different aspects of simplification quality, and understanding their strengths/weaknesses is crucial for result interpretation
  - Quick check question: Why does BLEU perform poorly for simplification evaluation despite being designed for machine translation?

- Concept: Cross-lingual knowledge transfer
  - Why needed here: The pseudo-data method relies on transferring simplification knowledge from English to Chinese via translation
  - Quick check question: What are the key challenges in transferring simplification patterns between English and Chinese?

## Architecture Onboarding

- Component map: Data preparation → Annotation pipeline → Feature extraction → Method implementation → Evaluation framework
- Critical path: Original sentence selection → Multi-reference annotation → Automatic metric calculation → Human evaluation
- Design tradeoffs: Multi-reference increases annotation cost but improves evaluation quality; unsupervised methods are cheaper but less effective than LLMs
- Failure signatures: Low SARI scores despite high BLEU scores indicate superficial changes; poor HSK performance suggests lack of lexical simplification
- First 3 experiments:
  1. Compare single-reference vs multi-reference evaluation using same simplification outputs
  2. Test different bridge languages for back-translation (English vs other languages)
  3. Vary the number of simplification references per sentence (2, 3, 5) to find optimal balance between cost and quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Chinese text simplification models be evaluated more comprehensively beyond the current metrics?
- Basis in paper: [explicit] The paper evaluates Chinese text simplification models using automatic metrics (SARI, BLEU, HSK Level) and human evaluation, but acknowledges that even advanced models are still inferior to human simplification
- Why unresolved: Current evaluation methods may not fully capture the nuances of Chinese text simplification, and there may be other important aspects of simplification that are not being measured
- What evidence would resolve it: Development and validation of new evaluation metrics or methods that can better assess the quality of Chinese text simplification, including aspects such as semantic preservation, fluency, and readability

### Open Question 2
- Question: How can unsupervised Chinese text simplification methods be improved to better preserve semantic information while simplifying text?
- Basis in paper: [explicit] The paper compares the performance of unsupervised methods (direct back translation, translated Wiki-Large, cross-lingual pseudo data) with large language models, finding that unsupervised methods tend to perform worse in terms of semantic preservation
- Why unresolved: Unsupervised methods rely on pseudo data and machine translation, which may introduce noise and errors that affect the quality of the simplified text
- What evidence would resolve it: Development and evaluation of new unsupervised methods that can better preserve semantic information while simplifying text, possibly by incorporating more advanced techniques or leveraging additional resources

### Open Question 3
- Question: How can large language models be further fine-tuned or adapted to improve their Chinese text simplification capabilities?
- Basis in paper: [explicit] The paper evaluates the performance of large language models (gpt-3.5-turbo, text-davinci-003) on Chinese text simplification and finds that even the best model (gpt-3.5-turbo) is still inferior to human simplification
- Why unresolved: Large language models may require additional fine-tuning or adaptation to better understand the nuances of Chinese text simplification and generate high-quality simplified text
- What evidence would resolve it: Development and evaluation of fine-tuning strategies or adaptation techniques that can improve the Chinese text simplification capabilities of large language models, possibly by incorporating domain-specific knowledge or leveraging additional resources

## Limitations

- Dataset size (723 sentences) is relatively small for training robust simplification models, though appropriate for evaluation purposes
- Annotation quality depends on limited pool of annotators with specific linguistic backgrounds, which may introduce bias
- Evaluation framework relies heavily on automatic metrics that may not fully capture human judgments of simplification quality, particularly for structural changes in Chinese text

## Confidence

- **High confidence**: The dataset construction methodology and annotation process are well-documented and reproducible. The multi-reference design effectively captures diverse simplification strategies.
- **Medium confidence**: The performance rankings of different methods (GPT-3.5-turbo > unsupervised methods > human references) are reliable, though absolute metric values may vary depending on implementation details.
- **Low confidence**: The transferability of findings from Chinese to other languages, and the effectiveness of cross-lingual pseudo data generation, require further validation due to the unique characteristics of Chinese syntax and vocabulary.

## Next Checks

1. Conduct ablation studies on the number of simplification references per sentence (2, 3, 5) to quantify the trade-off between annotation cost and evaluation quality.
2. Test the unsupervised methods with different bridge languages in the back-translation pipeline to determine if English is optimal for Chinese simplification.
3. Perform detailed error analysis comparing LLM-generated simplifications with human references to identify systematic weaknesses in current approaches.