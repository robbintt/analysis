---
ver: rpa2
title: Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation
arxiv_id: '2307.08875'
source_url: https://arxiv.org/abs/2307.08875
tags:
- robust
- function
- policy
- approximation
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces two novel uncertainty set formulations\u2014\
  one based on double sampling and the other on an integral probability metric\u2014\
  that make large-scale robust reinforcement learning computationally tractable under\
  \ function approximation. The authors propose a robust natural actor-critic (RNAC)\
  \ algorithm that leverages these uncertainty sets and provide finite-time convergence\
  \ guarantees to the optimal robust policy within function approximation error."
---

# Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation

## Quick Facts
- arXiv ID: 2307.08875
- Source URL: https://arxiv.org/abs/2307.08875
- Reference count: 40
- Primary result: Introduces tractable uncertainty set formulations enabling robust RL with function approximation, achieving O(1/ε²) sample complexity

## Executive Summary
This paper addresses the challenge of robust reinforcement learning under model mismatch between training and testing environments. The authors propose two novel uncertainty set formulations - one based on double sampling and another using integral probability metrics - that make large-scale robust RL computationally tractable even with function approximation. The Robust Natural Actor-Critic (RNAC) algorithm is developed to incorporate these uncertainty sets, with theoretical guarantees for convergence to optimal robust policies within function approximation error. The method is evaluated on multiple MuJoCo environments and a real-world TurtleBot navigation task, demonstrating superior performance under various perturbations compared to non-robust baselines.

## Method Summary
The method introduces two uncertainty set formulations to enable tractable robust RL with function approximation. The double sampling (DS) uncertainty set uses multiple simulator queries per state-action pair to construct an inner optimization problem that can be solved efficiently. The integral probability metric (IPM) uncertainty set adds a regularization term to the Bellman operator that can be computed directly. These are integrated into a Robust Natural Actor-Critic framework where a robust critic learns the value function using stochastic approximation with the empirical robust Bellman operator, while the robust natural actor updates policy parameters using a compatible Q-function approximation. The algorithm alternates between these steps to achieve robust policy improvement.

## Key Results
- Proposes RNAC algorithm with finite-time convergence guarantees to optimal robust policy within function approximation error
- Achieves O(1/ε²) sample complexity with geometrically increasing step sizes, or O(1/ε³) with constant step size under linear function approximation
- Demonstrates robust performance on MuJoCo environments (Hopper-v3, Walker2d-v3, HalfCheetah-v3) under model mismatch
- Validates effectiveness on real-world TurtleBot navigation task under unbalanced actuation noise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Double-sampling uncertainty sets enable tractable robust Bellman operator evaluation under function approximation by allowing unbiased estimation using only nominal model samples.
- Mechanism: By sampling multiple next states for each state-action pair from the nominal model and perturbing the sampling distribution within a divergence constraint, the robust Bellman operator can be computed as an inner optimization over a small finite set of samples rather than over the entire state space.
- Core assumption: The nominal model can be queried multiple times for the same state-action pair to generate the required next state samples.
- Evidence anchors:
  - [abstract]: "one based on double sampling and the other on an integral probability metric. Both make large-scale robust RL tractable even when one only has access to a simulator."
  - [section]: "Let {s′₁, s′₂, . . . , s′ₘ} be m samples drawn i.i.d. according to the nominal model p◦s,a. Then, for any given divergence measure d(·, ·) and radius δ > 0, there exists an uncertainty set P = ⊗s,aPs,a such that inf p∈Ps,a p⊤V = Es′₁:m i.i.d.∼p◦s,a [inf α∈∆[m]:d(α,Unif([m]))≤δ Pm i=1 αiV(s′i)]"
  - [corpus]: Weak - no direct evidence of double-sampling in related papers.
- Break condition: If the simulator cannot provide multiple samples for the same state-action pair, or if m becomes too large making the inner optimization computationally expensive.

### Mechanism 2
- Claim: Integral probability metric (IPM) uncertainty sets provide tractable robust Bellman operator evaluation by regularizing the value function approximation with a norm penalty term.
- Mechanism: The robust Bellman operator under IPM uncertainty sets can be expressed as the nominal Bellman operator minus a regularization term proportional to the norm of the value function weights, which can be computed directly without solving an inner optimization problem.
- Core assumption: The feature matrix Ψ has full column rank and the first coordinate of ψ(s) is 1 for any s (bias term).
- Evidence anchors:
  - [abstract]: "the other on an integral probability metric"
  - [section]: "For the IPM with F in (5), we have inf q∈Ps,a q⊤Vw = (p◦s,a)⊤Vw − δ∥w2:d∥"
  - [corpus]: Weak - no direct evidence of IPM-based uncertainty sets in related papers.
- Break condition: If the feature matrix Ψ is not full rank or the regularization parameter δ is too large, breaking the theoretical guarantees.

### Mechanism 3
- Claim: The robust natural actor-critic algorithm converges to the optimal robust policy within function approximation error by alternating between robust value function approximation and policy improvement steps.
- Mechanism: The robust critic learns the robust value function using stochastic approximation with the empirical robust Bellman operator, while the robust natural actor updates the policy parameters along an ascent direction that improves the robust value function, with each step providing approximate policy improvement.
- Core assumption: The projected robust Bellman operator is a contraction mapping, which is guaranteed for both uncertainty sets with appropriate parameters.
- Evidence anchors:
  - [abstract]: "We propose a robust natural actor-critic (RNAC) approach that incorporates the new uncertainty sets and employs function approximation. We provide finite-time convergence guarantees for the proposed RNAC algorithm to the optimal robust policy within the function approximation error."
  - [section]: "The update procedure essentially minimizes the Mean Square Projected Robust Bellman Error MSPRBEπ(w) = ∥Ππ(TπPVw) − Vw∥2νπ"
  - [corpus]: Moderate - related papers on robust actor-critic methods exist but focus on tabular settings.
- Break condition: If the contraction property of the projected robust Bellman operator fails, or if the step sizes are not properly tuned.

## Foundational Learning

- Concept: Markov decision processes (MDPs) and robust MDPs
  - Why needed here: The paper builds on MDP theory and extends it to the robust setting where the transition model is uncertain.
  - Quick check question: What is the key difference between an MDP and a robust MDP in terms of the transition model?

- Concept: Function approximation in reinforcement learning
  - Why needed here: The paper uses function approximation to scale robust RL to large state spaces, which is a core technique in modern RL.
  - Quick check question: How does linear function approximation represent the value function, and what is the role of the feature matrix Ψ?

- Concept: Policy gradient methods and natural policy gradient
  - Why needed here: The robust natural actor-critic algorithm is based on natural policy gradient, which requires understanding of policy gradient theory and its extensions.
  - Quick check question: What is the key idea behind the natural policy gradient compared to the standard policy gradient?

## Architecture Onboarding

- Component map:
  - Robust critic -> Compatible Q-function approximation -> Robust natural actor -> Policy improvement

- Critical path: Robust critic updates → Compatible Q-function approximation → Robust natural actor updates → Policy improvement

- Design tradeoffs:
  - DS vs IPM uncertainty sets: DS requires multiple samples from simulator but works for any divergence measure, while IPM adds regularization but is limited to linear approximation
  - Linear vs general function approximation: Linear is more tractable with theoretical guarantees, while general allows more flexible representation
  - Step size tuning: Geometrically increasing step sizes give linear convergence but larger constants, while constant step sizes are more practical but give sublinear convergence

- Failure signatures:
  - Critic failure: If the projected robust Bellman operator is not a contraction, RLTD will not converge
  - Actor failure: If the compatible Q-function approximation is poor, RQNPG will not provide good policy improvement
  - Uncertainty set failure: If the uncertainty set parameters (m, δ) are not properly chosen, the robust Bellman operator may be intractable or too conservative

- First 3 experiments:
  1. Implement RLTD with DS uncertainty set on a simple gridworld environment to verify convergence to robust value function
  2. Implement RQNPG with linear function approximation on the same environment to verify policy improvement
  3. Combine RLTD and RQNPG in RNAC on a continuous control environment (e.g., CartPole) to verify end-to-end performance

## Open Questions the Paper Calls Out
None explicitly mentioned in the provided material.

## Limitations
- Double sampling approach requires multiple simulator queries per state-action pair, which may be computationally expensive for complex simulators
- IPM formulation is limited to linear function approximation due to its regularization structure
- Theoretical guarantees assume specific conditions on step sizes and feature matrix properties that may not hold in practice

## Confidence
- High: The theoretical framework connecting uncertainty sets to tractable robust Bellman operators is sound
- Medium: The sample complexity bounds (O(1/ε²) and O(1/ε³)) are valid under stated assumptions
- Low: The practical performance claims across diverse environments are supported by experiments but may not generalize to all domains

## Next Checks
1. **Simulator query efficiency**: Measure the actual number of simulator calls required per algorithm step and analyze the trade-off between computational cost and robustness benefits.
2. **Non-linear approximation**: Test the algorithm with non-linear function approximators (e.g., neural networks) to assess whether the theoretical advantages extend beyond linear cases.
3. **Uncertainty set parameter sensitivity**: Systematically vary uncertainty set parameters (m for DS, δ for IPM) to determine their impact on both computational efficiency and robust performance.