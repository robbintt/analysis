---
ver: rpa2
title: Discriminative Entropy Clustering and its Relation to K-means and SVM
arxiv_id: '2301.11405'
source_url: https://arxiv.org/abs/2301.11405
tags:
- clustering
- loss
- entropy
- network
- k-means
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits discriminative entropy clustering and proposes
  a new self-labeling formulation to address limitations of standard approaches. The
  authors disprove prior claims of equivalence between entropy clustering and K-means,
  showing fundamental differences and susceptibility to narrow margins.
---

# Discriminative Entropy Clustering and its Relation to K-means and SVM

## Quick Facts
- arXiv ID: 2301.11405
- Source URL: https://arxiv.org/abs/2301.11405
- Authors: [Not specified in source]
- Reference count: 19
- Primary result: Proposes a new self-labeling formulation of entropy clustering that improves state-of-the-art results on deep clustering benchmarks, achieving 92.2% accuracy on STL10 and 73.48% on CIFAR10.

## Executive Summary
This paper revisits discriminative entropy clustering and proposes a new self-labeling formulation to address limitations of standard approaches. The authors disprove prior claims of equivalence between entropy clustering and K-means, showing fundamental differences and susceptibility to narrow margins. They propose a new loss function using reverse cross-entropy and a strong fairness constraint, along with an efficient EM algorithm for pseudo-label estimation. The method improves state-of-the-art results on standard deep clustering benchmarks.

## Method Summary
The method introduces a new self-labeling formulation of discriminative entropy clustering using reverse cross-entropy and a strong fairness constraint via auxiliary splitting variables. The approach addresses the susceptibility to narrow decision margins and pseudo-label errors in standard entropy clustering. The authors propose an efficient EM algorithm for pseudo-label estimation that scales linearly with dataset size. The loss function combines reverse cross-entropy, strong fairness through KL divergence, and margin maximization via classifier norm regularization.

## Key Results
- Achieves 92.2% accuracy on STL10, 73.48% on CIFAR10, and 93.58% on MNIST using fixed features
- Demonstrates robustness to label noise in weakly-supervised settings
- Disproves prior claims of equivalence between entropy clustering and K-means
- Shows the importance of classifier norm regularization for margin maximization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing forward cross-entropy H(y, σ) with reverse cross-entropy H(σ, y) makes the loss robust to pseudo-label errors.
- Mechanism: Reverse cross-entropy treats network predictions σ as the primary distribution and pseudo-labels y as targets, ensuring uncertain or incorrect pseudo-labels don't drive the model to copy the noise.
- Core assumption: Pseudo-labels y contain errors but the model predictions σ are more reliable estimates of the true class distribution.
- Evidence anchors:
  - [abstract]: "We propose a new self-labeling formulation of entropy clustering... robust to pseudo-labeling errors..."
  - [section]: "We replace standard forward cross-entropy H(y, σ)... by the reverse cross-entropy H(σ, y) that is significantly more robust to errors in estimated soft pseudo-labels..."
  - [corpus]: No direct evidence in corpus. The claim is unique to this paper.
- Break condition: If pseudo-labels are consistently more accurate than the model predictions, reverse cross-entropy may lose its robustness advantage.

### Mechanism 2
- Claim: The combination of decisiveness (H(σ)) and strong fairness (KL(u||σ)) via auxiliary splitting variables y enables efficient optimization.
- Mechanism: By introducing auxiliary variables y that split the fairness and decisiveness terms, the optimization problem decouples into simpler subproblems. The consistency constraint y = σ is enforced through a Lagrangian with forward KL divergence.
- Core assumption: The splitting into decisiveness sub-problem and fairness sub-problem via auxiliary variables y is tractable and improves convergence.
- Evidence anchors:
  - [abstract]: "We propose a new self-labeling formulation of entropy clustering... The derived self-labeling loss includes the reverse cross-entropy robust to pseudo-label errors and allows an efficient EM solver for pseudo-labels."
  - [section]: "Optimization of losses (1) or (4) during network training is mostly done with standard gradient descent... However, the difference between the two entropy terms implies non-convexity... This motivates alternative formulations and optimization approaches... self-labeling approaches to unsupervised network training iterate optimization of the loss over pseudo-labels and network parameters..."
  - [corpus]: No direct evidence in corpus. The claim is unique to this paper.
- Break condition: If the consistency constraint y = σ cannot be satisfied or the EM algorithm fails to converge, the splitting approach may not provide benefits.

### Mechanism 3
- Claim: Classifier norm regularization γ||v||² is essential for margin maximization in entropy clustering.
- Mechanism: Without norm regularization, the softmax model can produce narrow decision margins by scaling the logits arbitrarily while maintaining hard decisiveness on training points. The norm regularization term γ||v||² prevents this by constraining the classifier parameters.
- Core assumption: Narrow margins in softmax models without norm regularization lead to poor generalization, and adding γ||v||² explicitly addresses this.
- Evidence anchors:
  - [abstract]: "Moreover, we show the susceptibility of standard entropy clustering to narrow decision margins and motivate an explicit margin maximization term."
  - [section]: "Interestingly, regularization of the norm for all network parameters [v, w] is motivated in (4) differently... But, since the classifier parameters v are included, coincidentally, it also leads to margin maximization... Thus, the norm regularization term ||v||² should be explicitly present in any clustering loss for pseudo-posterior models."
  - [corpus]: No direct evidence in corpus. The claim is unique to this paper.
- Break condition: If the data is perfectly separable with large margins naturally, the norm regularization may be unnecessary.

## Foundational Learning

- Concept: Entropy and mutual information in clustering
  - Why needed here: The paper relies on maximizing mutual information between input and output, which is approximated by the difference between average entropy and entropy of average predictions.
  - Quick check question: What is the difference between mutual information and cross-entropy, and how does this difference motivate the entropy-based clustering loss?

- Concept: Softmax models and pseudo-posteriors
  - Why needed here: The paper uses softmax outputs as pseudo-posteriors for clustering.
  - Quick check question: How does the softmax function convert logits into probability distributions, and what role does this play in the clustering process?

- Concept: Self-labeling and auxiliary variables
  - Why needed here: The paper introduces pseudo-labels as auxiliary variables to split the optimization of decisiveness and fairness.
  - Quick check question: How do auxiliary variables like pseudo-labels help in splitting complex optimization problems, and what are the trade-offs of this approach?

## Architecture Onboarding

- Component map:
  Input data -> Feature extraction (optional) -> Classifier -> Softmax layer -> Loss computation -> EM algorithm for pseudo-label updates

- Critical path:
  1. Forward pass: Compute features fw(X) and classifier output v⊤fw(X)
  2. Softmax: Convert logits to probability distributions σ
  3. Loss computation: Calculate H(σ, y) + λKL(u||y) + γ||v||²
  4. Backward pass: Compute gradients and update [v, w]
  5. EM update: Use EM algorithm to update pseudo-labels y based on current σ

- Design tradeoffs:
  - Using reverse cross-entropy vs forward cross-entropy: Reverse is more robust to pseudo-label errors but may converge slower
  - Strong fairness via KL(u||y) vs weak fairness via H(y): Strong fairness enforces better cluster balance but may be harder to optimize
  - Explicit margin maximization via γ||v||² vs implicit margin maximization: Explicit control provides better generalization but adds a hyperparameter

- Failure signatures:
  - Poor cluster quality: Check if pseudo-labels are being updated correctly via EM algorithm
  - Slow convergence: Verify learning rate and batch size settings; consider using the batch version of EM
  - Overfitting: Monitor norm of classifier v; increase γ if overfitting is observed

- First 3 experiments:
  1. Implement the linear classifier version on MNIST with fixed features and compare accuracy with K-means and standard MI loss
  2. Test the batch version of EM algorithm on MNIST and compare convergence speed with Newton's method
  3. Vary the margin maximization weight γ on CIFAR10 and observe its effect on decision boundaries and accuracy

## Open Questions the Paper Calls Out

- Open Question 1: How does the proposed reverse cross-entropy loss (H(σ, y)) perform compared to standard forward cross-entropy (H(y, σ)) in other noisy label scenarios beyond the experiments presented in the paper?
- Open Question 2: What is the theoretical justification for the choice of the fairness constraint (KL(u ∥ σ)) in the proposed loss function, and how does it compare to other potential fairness constraints?
- Open Question 3: How does the proposed EM algorithm for estimating pseudo-labels compare to other optimization methods, such as gradient descent or alternating minimization, in terms of convergence speed and final clustering performance?

## Limitations
- The empirical results show strong performance but lack detailed experimental setup and hyperparameter values, limiting reproducibility
- The claims about K-means equivalence are disproven, but alternative formulations and their advantages are not fully explored or compared systematically
- Several key aspects remain uncertain, including the effectiveness of the EM algorithm and the impact of margin maximization without detailed empirical validation

## Confidence
- High confidence in the theoretical analysis of entropy clustering and its relation to K-means
- Medium confidence in the proposed reverse cross-entropy loss and its robustness to pseudo-label errors
- Low confidence in the effectiveness of the EM algorithm and the impact of margin maximization without detailed empirical validation

## Next Checks
1. Implement the proposed loss function (10) and the EM algorithm for pseudo-label estimation on a standard dataset (e.g., MNIST with fixed features) and compare the results with K-means and standard MI loss.
2. Conduct a systematic study of the proposed method's sensitivity to hyperparameters, such as learning rate, batch size, and regularization coefficients, on CIFAR10 and CIFAR100.
3. Analyze the decision boundaries learned by the proposed method and compare them with those of K-means and other state-of-the-art clustering methods on STL10 and CIFAR10.