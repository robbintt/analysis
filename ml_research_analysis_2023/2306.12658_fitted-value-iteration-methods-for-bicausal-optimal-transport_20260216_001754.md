---
ver: rpa2
title: Fitted value iteration methods for bicausal optimal transport
arxiv_id: '2306.12658'
source_url: https://arxiv.org/abs/2306.12658
tags:
- function
- assumption
- bicausal
- time
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a fitted value iteration (FVI) method to compute
  bicausal optimal transport, where couplings have an adapted structure. The method
  leverages neural networks to approximate value functions in the dynamic programming
  formulation.
---

# Fitted value iteration methods for bicausal optimal transport

## Quick Facts
- arXiv ID: 2306.12658
- Source URL: https://arxiv.org/abs/2306.12658
- Authors: 
- Reference count: 16
- Key outcome: FVI achieves estimation error of 1.311 (SD: 0.049) with average runtime of 10.592 seconds for T=1, outperforming LP (error: 1.217, SD: 0.102, runtime: 0.102s) in scalability as horizon increases

## Executive Summary
This paper introduces a fitted value iteration (FVI) method for computing bicausal optimal transport, where couplings must have an adapted structure. The approach uses neural networks to approximate value functions in a dynamic programming formulation, enabling scalable computation compared to traditional linear programming and adapted Sinkhorn methods. Under concentrability conditions and approximate completeness assumptions, the authors prove sample complexity bounds using local Rademacher complexity. Numerical experiments demonstrate that FVI maintains acceptable accuracy while significantly outperforming baseline methods in scalability as the time horizon increases.

## Method Summary
The FVI method approximates value functions using neural networks in a dynamic programming framework for bicausal optimal transport. Data is generated from the true distribution, and the algorithm iteratively updates neural network parameters to minimize Bellman errors using Adam optimization with smooth L1 loss. The method employs a sampling distribution as the product of marginals and uses empirical optimal transport to estimate value functions. For comparison, linear programming is implemented with Gurobi solver using a backward induction approach on non-recombining binomial trees, while adapted Sinkhorn methods use similar tree structures to approximate continuous distributions.

## Key Results
- FVI achieves estimation error of 1.311 (SD: 0.049) with runtime of 10.592 seconds for T=1
- FVI scales significantly better than LP and Sinkhorn methods as time horizon increases
- Multilayer neural networks with appropriate structures satisfy the assumptions required for sample complexity proofs
- Entropic regularization improves sample complexity under smoothness conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural networks enable scalable computation by approximating value functions instead of enumerating scenarios
- Mechanism: FVI learns value functions from sampled data, avoiding exponential complexity growth in time horizons
- Core assumption: Neural networks can adequately approximate bicausal OT value functions
- Evidence anchors: [abstract], [section] on FVI adoption, weak evidence from corpus
- Break condition: Poor function class approximation or rapid sample complexity growth

### Mechanism 2
- Claim: Concentrability condition and approximate completeness enable sample complexity bounds
- Mechanism: These assumptions connect value suboptimality to Bellman errors, enabling Rademacher complexity analysis
- Core assumption: Finite concentrability coefficient and function class satisfies approximate completeness
- Evidence anchors: [abstract], [section] on sample complexity analysis, weak evidence from corpus
- Break condition: Infinite concentrability coefficient or incomplete function class

### Mechanism 3
- Claim: Entropic regularization improves sample complexity
- Mechanism: Regularization makes the OT problem smoother, leading to better bounds
- Core assumption: Cost function and domains satisfy smoothness conditions
- Evidence anchors: [abstract], [section] on entropic regularization, weak evidence from corpus
- Break condition: Insufficient smoothness for regularization benefits

## Foundational Learning

- Concept: Optimal Transport (OT)
  - Why needed here: Bicausal OT is the main problem being solved
  - Quick check question: What is the difference between classical OT and bicausal OT?

- Concept: Dynamic Programming (DP)
  - Why needed here: FVI is based on the DP formulation of bicausal OT
  - Quick check question: How does the DP principle apply to bicausal transport?

- Concept: Neural Networks as Function Approximators
  - Why needed here: FVI uses neural networks to approximate value functions
  - Quick check question: What are the key properties of neural networks that make them suitable for function approximation?

## Architecture Onboarding

- Component map: Data generation -> Value function approximator -> Bellman operator -> Optimization module -> Evaluation module

- Critical path:
  1. Generate data from true distribution
  2. Initialize neural network
  3. For each time step:
    a. Sample mini-batch of states
    b. Compute empirical OT to approximate value function
    c. Update network parameters to minimize loss
  4. Evaluate learned value function

- Design tradeoffs:
  - Accuracy vs. scalability: FVI trades some accuracy for better scalability
  - Function class complexity: More complex classes may improve approximation but increase sample complexity
  - Regularization: Entropic regularization improves sample complexity but adds hyperparameters

- Failure signatures:
  - High variance in estimation error
  - Slow convergence or failure to converge
  - Poor scalability with increasing horizon or dimension

- First 3 experiments:
  1. Test FVI on simple bicausal OT problem with known solution (Gaussian data)
  2. Vary function class complexity and measure impact on error and runtime
  3. Test scalability with increasing horizon and dimension, comparing with LP and Sinkhorn

## Open Questions the Paper Calls Out

- How does the choice of sampling distribution Î² affect the concentrability condition and subsequent sample complexity bounds in FVI for bicausal OT?
- Can the FVI algorithm be extended to handle non-adapted transport plans while maintaining computational advantages?
- What is the optimal balance between gradient steps G and mini-batch size B in the FVI algorithm for different time horizons T?

## Limitations

- Theoretical analysis relies on restrictive assumptions (finite concentrability coefficient, approximate completeness) that may not hold in practice
- Empirical validation limited to Gaussian data with linear dynamics, leaving performance on complex distributions unclear
- Accuracy-runtime trade-off needs more thorough examination, particularly at longer time horizons

## Confidence

- High confidence: Neural networks enable scalable computation through value function approximation
- Medium confidence: Theoretical sample complexity bounds under stated assumptions
- Medium confidence: Empirical scalability results, though limited to specific distributions

## Next Checks

1. Test FVI on non-Gaussian, non-linear data distributions to assess robustness beyond Gaussian linear dynamics
2. Conduct comprehensive analysis of accuracy-runtime trade-off across different problem scales and distributions
3. Investigate sensitivity to hyperparameter choices and develop guidelines for different scenarios