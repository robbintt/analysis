---
ver: rpa2
title: Data-Efficient Multimodal Fusion on a Single GPU
arxiv_id: '2312.10144'
source_url: https://arxiv.org/abs/2312.10144
tags:
- multimodal
- learning
- data
- fusion
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FuseMix, a multimodal augmentation scheme that
  operates on the latent spaces of arbitrary pre-trained unimodal encoders to achieve
  competitive performance in both image-text and audio-text retrieval tasks. The method
  is designed to address the computational and data efficiency challenges of existing
  multimodal alignment methods by leveraging pre-trained unimodal encoders and minimal
  paired data.
---

# Data-Efficient Multimodal Fusion on a Single GPU

## Quick Facts
- arXiv ID: 2312.10144
- Source URL: https://arxiv.org/abs/2312.10144
- Reference count: 40
- Primary result: Achieves competitive multimodal retrieval performance using orders of magnitude less compute and data than state-of-the-art methods

## Executive Summary
This paper introduces FuseMix, a novel multimodal augmentation scheme that operates on the latent spaces of pre-trained unimodal encoders to achieve data-efficient multimodal fusion. The method leverages existing pre-trained encoders (like DINOv2 for images, BGE for text, and Whisper for audio) as semantic bootstrap representations, requiring only lightweight fusion adapters to be trained on paired multimodal data. FuseMix demonstrates strong performance on both image-text and audio-text retrieval tasks while using dramatically less computational resources - achieving better results than CLIP on Flickr30K with ~600× fewer GPU days and ~80× fewer image-text pairs.

## Method Summary
FuseMix is a multimodal augmentation method that operates on latent spaces of pre-trained unimodal encoders rather than raw inputs or fine-tuned encoders. The approach pre-computes latent encodings from frozen pre-trained unimodal encoders, applies mixup-style interpolation with shared mixing coefficients across modalities to ensure semantic consistency, and trains lightweight fusion adapters using contrastive learning with symmetric InfoNCE loss. The method is designed to be plug-and-play, allowing easy substitution of different pre-trained unimodal encoders while maintaining the same fusion framework. FuseMix achieves data efficiency by leveraging the semantic richness of pre-trained encoders and requiring minimal paired multimodal data for training.

## Key Results
- Outperforms CLIP on Flickr30K text-to-image retrieval with ~600× fewer GPU days and ~80× fewer image-text pairs
- Achieves competitive performance on image-text retrieval benchmarks (COCO, Flickr30K, SBU Captions, Conceptual Captions)
- Demonstrates successful application to audio-text retrieval on AudioCaps and Clotho datasets
- Enables conversion of pre-trained text-to-image generative models into audio-to-image models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained unimodal encoders provide semantically rich bootstrap representations that reduce the need for large paired multimodal datasets
- Mechanism: By using frozen pre-trained unimodal encoders and only training lightweight fusion adapters, the method leverages existing semantic knowledge encoded in the unimodal models, avoiding redundant learning from scratch
- Core assumption: The latent spaces of pre-trained unimodal encoders already contain rich semantic information relevant to multimodal alignment
- Evidence anchors: Abstract states pre-trained encoders "should provide an effective bootstrap"; Section 3.1 explains latent spaces provide "more homogeneous alternative"; Corpus analysis shows moderate relatedness to data-efficient multimodal fusion

### Mechanism 2
- Claim: FuseMix data augmentation on latent space creates semantically meaningful synthetic multimodal pairs by sharing interpolation coefficients across modalities
- Mechanism: Mixup-inspired interpolation with shared λ across both latent spaces ensures augmented latent pairs remain semantically consistent as valid positive pairs
- Core assumption: Linear interpolations in semantically rich latent spaces preserve semantic relationships better than in raw input space
- Evidence anchors: Section 5.2 explains shared λ ensures "semantically consistent" augmentation; Same section expects linear interpolations to be "more semantically meaningful"; Section 6.5 shows data augmentations are beneficial

### Mechanism 3
- Claim: The plug-and-play architecture allows modular replacement of unimodal components without retraining the entire system
- Mechanism: Decoupling unimodal encoders (kept frozen) from fusion adapters (trained) enables easy substitution of improved unimodal encoders while maintaining the same fusion framework
- Core assumption: Fusion adapters can adapt to different pre-trained unimodal encoders without requiring architectural changes
- Evidence anchors: Section 5.1 states approach is "agnostic to both the choice of unimodal encoders"; Section 6.2 mentions quickly incorporating better unimodal encoders; Same section shows combination of DINOv2+BGE achieves highest performance

## Foundational Learning

- Concept: Contrastive learning with InfoNCE loss
  - Why needed here: Used as the core alignment objective to learn a shared latent space
  - Quick check question: What is the purpose of the temperature parameter τ in the InfoNCE loss formulation?

- Concept: Mixup data augmentation
  - Why needed here: Inspires FuseMix but adapted for multimodal latent spaces by sharing the mixing coefficient across modalities
  - Quick check question: How does mixup differ from simple data augmentation like random crops or color jitter?

- Concept: Determinantal Point Processes (DPPs)
  - Why needed here: Used to measure dataset diversity by finding maximally diverse subsets
  - Quick check question: What property of DPP sampling makes it useful for selecting diverse data points?

## Architecture Onboarding

- Component map: Pre-trained unimodal encoders (frozen) -> Latent space extraction (penultimate layer) -> FuseMix augmentation (shared λ) -> Lightweight fusion adapters (inverted bottleneck MLPs) -> Symmetric InfoNCE loss
- Critical path: Precompute unimodal latents → Apply FuseMix augmentation → Train fusion adapters → Evaluate on retrieval tasks
- Design tradeoffs: Computational efficiency vs. potential loss of fine-tuning benefits from the unimodal encoders
- Failure signatures: Poor retrieval performance suggests issues with either the unimodal encoders' semantic quality or the FuseMix augmentation strategy
- First 3 experiments:
  1. Verify that pre-computed unimodal latents can be successfully loaded and processed
  2. Test FuseMix augmentation with a small batch size to confirm semantic consistency
  3. Train fusion adapters with a subset of data to validate the learning pipeline before full-scale training

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the methodology and results, several important questions remain:

### Open Question 1
- Question: How does FuseMix perform when applied to modality pairs beyond image-text and audio-text, such as video-text or multi-modal fusion with more than two modalities?
- Basis in paper: The paper demonstrates FuseMix's effectiveness for image-text and audio-text retrieval but does not explore its performance with other modality combinations or multi-modal fusion.
- Why unresolved: The paper focuses on two modality pairs and does not provide experimental results or analysis for other combinations, leaving the generalizability of FuseMix to other modalities unexplored.
- What evidence would resolve it: Experimental results showing FuseMix's performance on video-text retrieval or fusion tasks involving three or more modalities would clarify its applicability and limitations in broader contexts.

### Open Question 2
- Question: What is the impact of using different pre-trained unimodal encoders on FuseMix's performance, and how sensitive is the method to the choice of encoders?
- Basis in paper: The paper mentions that FuseMix is agnostic to the choice of unimodal encoders and demonstrates performance with various combinations, but does not systematically analyze the sensitivity to encoder selection.
- Why unresolved: While the paper shows competitive results with different encoders, it does not provide a detailed analysis of how encoder choice affects performance or whether certain encoder combinations are more effective.
- What evidence would resolve it: A comprehensive study comparing FuseMix's performance across a wide range of pre-trained encoders and their combinations would reveal the method's sensitivity and optimal encoder pairings.

### Open Question 3
- Question: How does FuseMix's performance scale with increasing dataset size, and is there a point of diminishing returns?
- Basis in paper: The paper emphasizes data efficiency and shows strong performance with limited data, but does not explore how FuseMix performs as dataset size increases to levels comparable to internet-scale datasets.
- Why unresolved: The paper focuses on data efficiency and performance with limited data, but does not investigate whether FuseMix maintains its advantages or encounters limitations when scaled to very large datasets.
- What evidence would resolve it: Experiments evaluating FuseMix's performance across a wide range of dataset sizes, including very large datasets, would clarify its scalability and potential limitations in data-rich scenarios.

## Limitations
- The method relies heavily on the semantic richness of pre-trained unimodal encoders, which may not generalize well to all multimodal tasks or domains
- The approach assumes linear interpolations in latent space preserve semantic relationships, but this may not hold for all encoder architectures or data distributions
- The lightweight fusion adapters may have limited capacity to capture complex multimodal relationships compared to end-to-end trained models

## Confidence
- **High Confidence**: The computational efficiency claims and comparison to CLIP are well-supported by the presented data (600× fewer GPU days, 80× fewer image-text pairs). The modular plug-and-play architecture is clearly demonstrated and validated.
- **Medium Confidence**: The semantic consistency of FuseMix augmentations is theoretically sound but could benefit from more ablation studies on different latent space distributions and encoder combinations. The performance gains over baselines are statistically significant but may vary across different datasets and tasks.
- **Low Confidence**: The generalizability of the approach to modalities beyond image, text, and audio has not been tested. The long-term stability of the fusion adapters when paired with rapidly evolving pre-trained unimodal models is unknown.

## Next Checks
1. **Latent Space Semantic Validation**: Test FuseMix augmentation on latent spaces from multiple different unimodal encoder architectures to verify semantic consistency is preserved across models, not just the specific ones used in the paper.
2. **Cross-Domain Generalization**: Apply the FuseMix framework to a novel modality pair (e.g., text-video or text-point cloud) to test the method's generality beyond the evaluated modalities.
3. **Adversarial Robustness**: Evaluate the fusion model's performance when exposed to out-of-distribution or adversarial multimodal inputs to assess the robustness of the lightweight adapter architecture.