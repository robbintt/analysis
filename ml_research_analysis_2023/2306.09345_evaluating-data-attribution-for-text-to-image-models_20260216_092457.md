---
ver: rpa2
title: Evaluating Data Attribution for Text-to-Image Models
arxiv_id: '2306.09345'
source_url: https://arxiv.org/abs/2306.09345
tags:
- images
- training
- style
- object
- attribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of data attribution in large-scale
  text-to-image generative models, specifically identifying which training images
  most influence a given generated output. The core method leverages "customization"
  techniques to efficiently create exemplar-influenced synthetic images, enabling
  evaluation of data attribution algorithms and feature spaces.
---

# Evaluating Data Attribution for Text-to-Image Models

## Quick Facts
- arXiv ID: 2306.09345
- Source URL: https://arxiv.org/abs/2306.09345
- Reference count: 40
- Key outcome: Achieves Recall@10 scores up to 0.55 for object-centric models and 0.23 for artistic style models in identifying training images that influence generated outputs

## Executive Summary
This paper tackles the challenging problem of data attribution in text-to-image generative models—specifically identifying which training images most influence a given generated output. The authors propose a novel approach using "customization" techniques to efficiently create synthetic exemplar-influenced images that serve as controlled ground truth for training attribution algorithms. By curating a large dataset of exemplar-synthesized image pairs and applying contrastive learning, they develop feature spaces that can effectively retrieve training images responsible for influencing generated outputs. The method demonstrates strong performance on object-centric models and shows generalization to artistic styles and out-of-domain data.

## Method Summary
The method leverages Custom Diffusion fine-tuning to create exemplar-influenced generators by training on single exemplar images. These generators produce synthetic images that are computationally influenced by the exemplar by construction, enabling controlled creation of ground truth attribution pairs. The authors curate a dataset of such exemplar-synthesized pairs from ImageNet, BAM-FG, and Artchive datasets. They then evaluate various pretrained feature spaces on retrieval tasks using this dataset and apply contrastive learning with NT-Xent loss to train linear mappings that improve attribution performance. Finally, they calibrate similarity scores using a thresholded softmax to produce probabilistic soft influence scores over training images.

## Key Results
- Achieves Recall@10 scores of 0.55 for object-centric models and 0.23 for artistic style models
- Demonstrates strong generalization to larger sets of unrelated images and produces calibrated soft influence scores
- Shows feature spaces trained on exemplar-synthesized pairs significantly outperform pretrained features alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Customization creates exemplar-influenced synthetic images that serve as noisy but informative ground truth for data attribution.
- Mechanism: The method uses "add-one-in" training to fine-tune a pretrained generative model on a single exemplar image, creating a new model G+. Sampling from G+ generates images that are computationally influenced by the exemplar by construction, enabling controlled creation of ground truth attribution pairs.
- Core assumption: Fine-tuning on a single exemplar image will produce generated images that are primarily influenced by that exemplar, creating a useful proxy for ground truth attribution.
- Evidence anchors:
  - [abstract] "Our key insight is that this allows us to efficiently create synthetic images that are computationally influenced by the exemplar by construction."
  - [section] "With our new dataset of such exemplar-influenced images, we are able to evaluate various data attribution algorithms and different possible feature spaces."
- Break condition: If the fine-tuning process introduces significant influences from other training images, or if the generated images are too diverse to be reliably attributed to the exemplar.

### Mechanism 2
- Claim: Contrastive learning on the exemplar-synthesized pairs improves feature spaces for data attribution.
- Mechanism: The method applies contrastive learning with NT-Xent loss on paired views of training exemplars and synthesized images. This encourages the learned feature space to place exemplars and their influenced images close together while pushing unrelated images apart.
- Core assumption: The contrastive learning objective will effectively learn feature embeddings that capture attribution-relevant similarity between exemplars and influenced images.
- Evidence anchors:
  - [section] "Our dataset consists of paired views of training and synthesized sets. This lends itself naturally to contrastive learning [47, 71] to capture the association between the two views."
  - [section] "We apply the commonly used NT-Xent (normalized temperature cross-entropy) loss [10]..."
- Break condition: If the contrastive learning overfits to the exemplar-synthesized pairs without generalizing to broader attribution scenarios.

### Mechanism 3
- Claim: Soft influence scores calibrated from feature similarities provide probabilistic attribution over training images.
- Mechanism: The method takes a thresholded softmax over similarity scores between synthesized images and training images, with learned temperature and threshold parameters to produce calibrated probability scores.
- Core assumption: The calibrated softmax transformation will produce meaningful probability scores that reflect the likelihood of each training image being influential.
- Evidence anchors:
  - [section] "Finally, by taking into account the inherent uncertainty of the problem, we can assign soft attribution scores over a set of training images."
  - [section] "We optimize the KL divergence... This resembles our retrieval objective (Equation 3)."
- Break condition: If the calibration fails to produce meaningful probability distributions, or if the scores are not discriminative enough between influential and non-influential images.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: The method uses contrastive learning to train feature spaces that can distinguish between influential and non-influential training images for a given generated output.
  - Quick check question: What is the purpose of using contrastive learning in this attribution method, and how does it differ from using contrastive learning for standard representation learning?

- Concept: Data attribution
  - Why needed here: The method aims to identify which training images most influenced a generated output, which is the core problem of data attribution in generative models.
  - Quick check question: Why is data attribution in generative models more challenging than in discriminative models, and what makes the proposed approach tractable?

- Concept: Fine-tuning for customization
  - Why needed here: The method relies on fine-tuning pretrained generative models on exemplar images to create controlled synthetic data for training and evaluation.
  - Quick check question: How does the "add-one-in" training approach used here differ from standard fine-tuning, and why is it suitable for creating ground truth attribution pairs?

## Architecture Onboarding

- Component map: Exemplar selection → Customization → Synthetic data generation → Feature space evaluation → Contrastive learning → Soft influence scoring

- Critical path: Exemplar selection → Customization → Synthetic data generation → Contrastive learning → Soft influence scoring

- Design tradeoffs:
  - Using single exemplars vs. multiple exemplars for customization
  - Training feature mappers vs. using pretrained features directly
  - Calibrating influence scores vs. using raw similarity scores

- Failure signatures:
  - Poor retrieval performance on exemplar-synthesized pairs
  - Overfitting during contrastive learning (performance gap between train and test)
  - Influence scores that are too uniform or not discriminative

- First 3 experiments:
  1. Test retrieval performance of various pretrained feature spaces on a small exemplar-synthesized dataset
  2. Train a linear mapper on top of a promising feature space and evaluate performance improvement
  3. Calibrate influence scores and compare against raw similarity scores on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the learned attribution features perform on datasets with completely different distributions than ImageNet and LAION, such as medical imaging or satellite imagery?
- Basis in paper: [inferred] The paper shows generalization to out-of-domain test cases (ImageNet unseen classes, Artchive) but these are still within the realm of natural images. The authors note that "plenty of headroom remains for improving attributing MSCOCO models" when using larger sets of unrelated images, suggesting limitations in generalization.
- Why unresolved: The experiments only tested generalization within natural image domains. There is no evaluation on radically different image distributions where the feature spaces may fail to capture relevant attribution patterns.
- What evidence would resolve it: Evaluating the attribution method on completely different domains (e.g., X-ray images, satellite imagery, microscopy images) and comparing performance to in-domain attribution would show the limits of generalization.

### Open Question 2
- Question: Can the attribution method identify compositional influences, such as attributing specific objects or styles within a complex scene to their corresponding training images?
- Basis in paper: [explicit] The paper states "it is difficult to collect customized models of arbitrary images" and that "tackling attribution in a compositional manner remains an open challenge." The method currently assesses attribution for full images, not individual components within them.
- Why unresolved: The current approach treats images holistically and cannot decompose them into constituent elements for separate attribution analysis. The "add-one-in" training process only guarantees exemplar influence for the whole image, not for specific regions or elements.
- What evidence would resolve it: Developing a method to segment images into compositional elements (objects, styles, backgrounds) and attributing each element separately to training data, then validating against ground truth compositional attribution data.

### Open Question 3
- Question: How does the attribution performance change when using different customization methods (e.g., textual inversion, dreambooth) compared to Custom Diffusion?
- Basis in paper: [inferred] The paper exclusively uses Custom Diffusion for creating the attribution dataset and states it's an "efficient approximation" but doesn't explore alternative customization methods. The authors note that "training behavior can be different in the original pretraining procedure."
- Why unresolved: Different customization methods may introduce different patterns of influence and noise into the attribution signal. The paper doesn't investigate whether the learned attribution features are specific to Custom Diffusion or generalize across customization approaches.
- What evidence would resolve it: Creating attribution datasets using multiple customization methods and comparing the performance of features trained on each dataset would reveal method-specific biases and generalization capabilities.

## Limitations

- The method assumes that customization via add-one-in training produces reliably traceable attribution chains, but this assumption lacks thorough empirical validation.
- Performance on artistic style attribution (Recall@10=0.23) is notably weaker than object-centric attribution, suggesting fundamental limitations in handling abstract influences.
- The soft influence score calibration relies on hyperparameters optimized on synthetic data that may not transfer well to real attribution tasks.

## Confidence

- **High confidence**: The core methodology of using customization to create exemplar-synthesized pairs is technically sound and well-implemented. The retrieval performance on object-centric models is robust and reproducible.
- **Medium confidence**: The contrastive learning improvements and soft influence score calibration show promise but require more rigorous validation on truly independent attribution tasks.
- **Low confidence**: The generalization claims to arbitrary image sets and the calibration of probabilistic attribution scores need substantial additional validation.

## Next Checks

1. **Attribution Chain Validation**: Create a controlled experiment where known image manipulations (e.g., cropping, color adjustments) are applied to exemplars before customization, then verify whether these modifications are reflected in the attribution scores of generated images.

2. **Cross-Domain Generalization**: Test the trained attribution models on completely different generative models (e.g., DALL-E, Midjourney) and datasets not seen during training to assess true generalization beyond the LAION-based evaluation.

3. **Influence Attribution Granularity**: Analyze the attribution scores at different levels of granularity (object parts, textures, compositional elements) to determine whether the method captures fine-grained influences or only coarse-level similarities.