---
ver: rpa2
title: 'Eliminating Reasoning via Inferring with Planning: A New Framework to Guide
  LLMs'' Non-linear Thinking'
arxiv_id: '2310.12342'
source_url: https://arxiv.org/abs/2310.12342
tags:
- reasoning
- arxiv
- llms
- human
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Inferential Elimination Prompting (IEP),
  a novel prompting method that guides large language models to perform non-linear
  reasoning by combining elimination and inference principles. Unlike Chain-of-Thought
  methods that follow linear thinking, IEP instructs models to first generate possible
  answers, then evaluate each option's entailment with context or facts through Natural
  Language Inference, and finally eliminate inconsistent options to identify the correct
  answer.
---

# Eliminating Reasoning via Inferring with Planning: A New Framework to Guide LLMs' Non-linear Thinking

## Quick Facts
- arXiv ID: 2310.12342
- Source URL: https://arxiv.org/abs/2310.12342
- Reference count: 9
- Primary result: IEP outperforms Chain-of-Thought prompting by using elimination-based reasoning with NLI

## Executive Summary
This paper introduces Inferential Elimination Prompting (IEP), a novel prompting method that guides large language models to perform non-linear reasoning by combining elimination and inference principles. Unlike Chain-of-Thought methods that follow linear thinking, IEP instructs models to first generate possible answers, then evaluate each option's entailment with context or facts through Natural Language Inference, and finally eliminate inconsistent options to identify the correct answer. The method was evaluated on various reasoning tasks and consistently outperformed Chain-of-Thought prompting. Additionally, the authors introduced MARB, a comprehensive benchmark containing 9,115 questions across six reasoning subtasks, with 1,685 hand-crafted rationale references to evaluate human-like reasoning abilities. The results demonstrate IEP's effectiveness in simulating complex human thinking processes and highlight the importance of non-linear reasoning in enhancing language model performance.

## Method Summary
The paper proposes IEP as a three-step prompting framework: planning (generating possible answers), inferring (evaluating each option's entailment with context using NLI), and eliminating (removing inconsistent options). The method is compared against Zero-shot Standard Prompting, Chain-of-Thought (CoT) Prompting, and their combination (IEP∪CoT) using PaLM2-540B and GPT4 models. The evaluation includes existing benchmarks (CommonsenseQA, OpenbookQA, StrategyQA, LogiQA) and a newly introduced MARB benchmark with 9,115 questions across six reasoning subtasks. The core innovation lies in leveraging NLI to systematically eliminate incorrect options rather than following a single linear reasoning chain.

## Key Results
- IEP consistently outperforms Chain-of-Thought prompting across multiple reasoning benchmarks
- Integrating IEP with CoT further improves performance on certain tasks
- MARB benchmark demonstrates the importance of non-linear reasoning in complex problem-solving
- Human rationale references in MARB provide valuable context for improving LLM reasoning performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IEP outperforms CoT by reducing error propagation through elimination-based reasoning.
- Mechanism: Instead of following a single linear chain where errors compound, IEP generates multiple candidate answers and systematically eliminates those contradicted by context using NLI, creating a "backward elimination" process that isolates correct answers.
- Core assumption: The LLM's NLI capabilities are sufficiently reliable to accurately judge entailment relationships between options and context.
- Evidence anchors:
  - [abstract]: "IEP guides LLMs to plan and then utilize Natural Language Inference (NLI) to deduce each possible solution's entailment relation with context, commonsense, or facts"
  - [section 3.2]: "Using the PLM, we assess its consistency with the context C and obtain a binary score sij"
  - [corpus]: Average neighbor FMR=0.399 suggests moderate relatedness to reasoning frameworks in the corpus
- Break condition: If NLI judgments become unreliable (e.g., ambiguous premises or context), elimination becomes noisy and may discard correct answers.

### Mechanism 2
- Claim: Combining IEP with CoT yields further improvements on certain tasks.
- Mechanism: CoT provides detailed step-by-step reasoning while IEP provides global perspective through elimination. The combination leverages both linear exploration and non-linear verification.
- Core assumption: Linear and non-linear reasoning processes are complementary rather than redundant.
- Evidence anchors:
  - [abstract]: "we observe that integrating IEP and CoT further improves the LLMs' performance on certain tasks"
  - [section 5.1]: "The combination of IEP and CoT generally shows improvements, but it does not always outperform IEP alone"
  - [corpus]: No direct evidence, marked as weak
- Break condition: When tasks don't benefit from step-by-step reasoning (like parajumbles), CoT integration may introduce confusion.

### Mechanism 3
- Claim: IEP better simulates complex human thinking by incorporating both forward planning and backward elimination.
- Mechanism: Humans often start with desired outcomes and work backward to prerequisites (reverse thinking), while also exploring multiple solutions. IEP mimics this by generating candidates first, then validating them against context.
- Core assumption: Human reasoning is inherently non-linear and benefits from considering multiple solutions simultaneously.
- Evidence anchors:
  - [abstract]: "IEP is inherently capable of forward planning several possible answers to make a comprehensive deduction and then backward checking those options"
  - [section 1]: "the human cognitive system is sophisticated and multifaceted... which does not only rely on such linear thinking to solve problems"
  - [corpus]: Weak correlation with reasoning complexity literature
- Break condition: If the task domain doesn't support multiple valid approaches or requires strict sequential logic.

## Foundational Learning

- Concept: Natural Language Inference (NLI)
  - Why needed here: IEP relies on NLI to judge whether each candidate answer is entailed by or contradicts the given context
  - Quick check question: Given context "All birds can fly" and hypothesis "Penguins can fly", what would NLI classify this as?

- Concept: Disjunctive syllogism in logic
  - Why needed here: IEP's elimination process is based on disjunctive syllogism - if we have options A or B, and we prove A is false, then B must be true
  - Quick check question: If we have options "The answer is 2" or "The answer is 3" and prove "The answer is not 2", what can we conclude?

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: IEP is compared against CoT, so understanding its linear, step-by-step reasoning approach is essential for grasping IEP's improvements
  - Quick check question: What is the main vulnerability of CoT prompting when reasoning through multi-step problems?

## Architecture Onboarding

- Component map: Input processing -> Candidate generation -> Premise extraction -> NLI evaluation -> Elimination logic -> Final answer selection

- Critical path: Candidate generation → Premise extraction → NLI evaluation → Elimination → Answer selection

- Design tradeoffs:
  - Number of candidates (k): More candidates increase coverage but computational cost
  - Premise quality: Better premise extraction improves NLI accuracy but may require additional prompting
  - NLI granularity: Binary vs. graded entailment scores affect elimination precision

- Failure signatures:
  - Poor premise extraction leading to weak NLI judgments
  - Ambiguous context causing unreliable entailment scoring
  - Too few candidates missing the correct answer entirely
  - Over-aggressive elimination removing correct answers

- First 3 experiments:
  1. Baseline test: Run IEP with k=3 candidates on CommonsenseQA and compare to CoT
  2. Ablation test: Run IEP with and without premise extraction to measure NLI contribution
  3. Integration test: Combine IEP with CoT on StrategyQA to verify performance gains mentioned in the paper

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does IEP's performance compare to other reasoning frameworks like Chain-of-Thought (CoT) when applied to complex reasoning tasks that require both linear and non-linear thinking?
- Basis in paper: [explicit] The paper mentions that IEP outperforms CoT in various tasks and that integrating IEP with CoT further improves performance on certain tasks, highlighting the necessity of equipping LLMs with mixed logic processes.
- Why unresolved: The paper provides some comparison between IEP and CoT, but a more comprehensive evaluation across a wider range of tasks and benchmarks would be needed to fully understand the relative strengths and weaknesses of each approach.
- What evidence would resolve it: Conducting experiments that compare IEP and CoT on a diverse set of reasoning tasks, including both linear and non-linear thinking, and analyzing the results in terms of accuracy, efficiency, and other relevant metrics.

### Open Question 2
- Question: How does the inclusion of human rationale references in the MARB benchmark impact the performance of LLMs in complex reasoning tasks?
- Basis in paper: [explicit] The paper introduces MARB, a comprehensive dataset with hand-crafted rationale references, and conducts experiments to evaluate the utility of these references in k-shot In-Context Learning (ICL) settings.
- Why unresolved: While the paper provides initial results on the utility of human rationale, further investigation is needed to understand the specific ways in which these references aid LLM performance and how they can be effectively utilized in different reasoning tasks.
- What evidence would resolve it: Conducting more experiments with varying numbers of shots and rationale references, analyzing the impact on different types of reasoning tasks, and exploring techniques for effectively incorporating human rationale into LLM training and inference processes.

### Open Question 3
- Question: How can IEP be extended to handle reasoning tasks that require domain knowledge and active retrieval of information?
- Basis in paper: [explicit] The paper mentions that IEP is limited to reasoning without domain knowledge and suggests that extending the framework with retrieval-augmented skills could be helpful in the correctness of both inferring and eliminating.
- Why unresolved: The paper does not provide specific details on how IEP can be extended to incorporate domain knowledge and retrieval, and the challenges and potential solutions for doing so remain unexplored.
- What evidence would resolve it: Developing and evaluating extensions of IEP that incorporate retrieval-augmented skills, testing them on reasoning tasks that require domain knowledge, and analyzing the impact on performance and efficiency.

## Limitations
- IEP's performance heavily depends on the reliability of NLI judgments, which may vary across different domains and contexts
- The method is limited to reasoning without domain knowledge and would require extensions for specialized knowledge tasks
- The paper focuses primarily on general reasoning benchmarks, with limited testing on highly specialized or technical domains

## Confidence
- High confidence: IEP's effectiveness compared to standard prompting (direct empirical evidence across multiple benchmarks)
- Medium confidence: IEP's superiority over CoT prompting (consistent improvements but with task-dependent variations)
- Low confidence: The claimed simulation of "complex human thinking" (largely theoretical assertion without direct human reasoning comparison studies)

## Next Checks
1. **Ablation study on NLI reliability**: Systematically vary the quality of NLI judgments (e.g., by introducing controlled noise) to quantify how much IEP's performance depends on NLI accuracy versus the elimination framework itself.

2. **Cross-domain generalization test**: Evaluate IEP on specialized domains (medical diagnosis, legal reasoning, scientific problem-solving) where the contextual knowledge differs significantly from commonsense reasoning to assess robustness beyond the tested benchmarks.

3. **Human-AI reasoning comparison**: Conduct a controlled study where human subjects solve the same reasoning tasks using elimination strategies, then compare their reasoning patterns and success rates against IEP's approach to validate the claim about simulating human thinking processes.