---
ver: rpa2
title: Training With "Paraphrasing the Original Text" Teaches LLM to Better Retrieve
  in Long-context Tasks
arxiv_id: '2312.11193'
source_url: https://arxiv.org/abs/2312.11193
tags:
- text
- data
- long
- training
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies the "lost in the middle" issue in long-context
  tasks as a key challenge for LLMs. The core idea is to design training data that
  forces the model to extract key information from the middle of long contexts by
  adding a "paraphrasing the original text" component to the answer.
---

# Training With "Paraphrasing the Original Text" Teaches LLM to Better Retrieve in Long-context Tasks

## Quick Facts
- arXiv ID: 2312.11193
- Source URL: https://arxiv.org/abs/2312.11193
- Reference count: 24
- Key outcome: Up to 8.48% improvement in long-context retrieval accuracy

## Executive Summary
This paper addresses the "lost in the middle" problem where LLMs struggle to extract information from the middle of long contexts. The proposed solution involves training models with data that requires "paraphrasing the original text" as part of the answer format. This forces models to adopt a W-shaped attention pattern, paying attention to both the beginning/end (for instructions) and the middle (for relevant information). Experiments on LongBench and NaturalQuestions datasets demonstrate significant improvements in long-context retrieval tasks, with up to 8.48% accuracy gains.

## Method Summary
The method involves fine-tuning LLMs using specially constructed training data that requires paraphrasing relevant portions of the original text before answering. The approach uses QLoRA fine-tuning on models like Qwen-14B-chat with specific hyperparameters (LoRA rank=16, alpha=64, learning rate=5e-5). The key innovation is the data construction pipeline that creates "W-shaped" samples where the output format mandates paraphrasing of relevant documents followed by the final answer, ensuring a high percentage of "effective" tokens in the output that require genuine attention to the context.

## Key Results
- Up to 8.48% improvement in average scores on PassageRetrieval-zh dataset
- Up to 4.48% improvement on NaturalQuestions dataset
- Demonstrates effectiveness in addressing "lost in the middle" issue in long-context tasks
- Shows model can extract relevant information from middle portions of long contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The "paraphrasing the original text" task forces the model to adopt a W-shaped attention pattern.
- Mechanism: By requiring the model to paraphrase relevant portions of the original text before answering, the training data ensures that the model must attend to both the beginning/end of the context (for instructions/questions) and the middle (for relevant information), creating the necessary W-shaped attention distribution.
- Core assumption: Models trained on typical long-context data develop U-shaped attention patterns due to the "closer is more relevant" and "emphasis at the beginning and end" characteristics of human language.
- Evidence anchors: [section] "To address this bias, one straightforward approach is to use data that does not possess the aforementioned characteristics..." [abstract] "Specially, we incorporate an additional part named 'paraphrasing the original text' when constructing the answer of training samples..."

### Mechanism 2
- Claim: Increasing the percentage of "effective" tokens in the output improves long-context retrieval accuracy.
- Mechanism: Traditional multi-document QA outputs have low percentages of effective tokens (tokens requiring W-shaped attention). By making the paraphrased original text a significant portion of the output (usually >50%), the model receives stronger gradients that guide it toward attending to relevant middle context.
- Core assumption: The optimization process can be steered by ensuring that a sufficient percentage of output tokens require the model to attend to relevant context rather than relying on common sense or reasoning alone.
- Evidence anchors: [section] "Paraphrasing the original text ensures that the model pays attention to the correct documents..." [section] "Traditional multi-document tasks have outputs with a low percentage of effective tokens..."

### Mechanism 3
- Claim: The method addresses the "lost in the middle" problem by preventing reliance on common sense and reasoning for answers.
- Mechanism: By forcing the model to include a paraphrase of the original relevant text in its output, it cannot generate answers based solely on its own knowledge or reasoning. It must attend to the reference documents to produce the correct paraphrase.
- Core assumption: Models without this training approach tend to rely on their own knowledge and reasoning abilities rather than the provided context, especially when output tokens are mostly "ineffective."
- Evidence anchors: [section] "This 'ineffective' data causes the model to rely on its own knowledge..." [section] "The model trained with 'original text paraphrasing' provides completely correct original text and answers..."

## Foundational Learning

- Concept: Positional encoding and attention mechanisms in transformers
  - Why needed here: Understanding how transformers process long sequences and why they develop U-shaped attention patterns is crucial for grasping why this method works
  - Quick check question: Why do transformers naturally develop U-shaped attention patterns when processing typical human language data?

- Concept: The difference between effective and ineffective tokens in training
  - Why needed here: The method's success depends on understanding how the percentage of effective tokens in training data affects model behavior and optimization
  - Quick check question: What distinguishes an "effective" token from an "ineffective" token in the context of long-context training?

- Concept: Fine-tuning vs. pretraining strategies for LLMs
  - Why needed here: The paper uses fine-tuning with specific data design rather than pretraining, and understanding this distinction is important for implementation
  - Quick check question: Why might fine-tuning with carefully designed data be more effective than pretraining for addressing the "lost in the middle" problem?

## Architecture Onboarding

- Component map: Data construction → Fine-tuning with QLoRA → Evaluation on long-context tasks
- Critical path: Data construction → Fine-tuning with QLoRA → Evaluation on long-context tasks
- Design tradeoffs: The method increases training data complexity and output length (due to paraphrasing requirements) but doesn't require architectural changes
- Failure signatures: If the model still shows "lost in the middle" behavior despite using this method
- First 3 experiments:
  1. Compare attention patterns (U-shaped vs W-shaped) between models trained with and without the paraphrasing task on the same multi-document QA dataset
  2. Test model performance on PassageRetrieval-zh with varying percentages of paraphrasing in the output (e.g., 25%, 50%, 75%) to find the optimal threshold
  3. Evaluate whether the method generalizes to non-QA tasks like long-text summarization by measuring attention patterns and output quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the "original text paraphrasing" approach improve performance in non-sparse long-context scenarios like document comparison or full-text summarization?
- Basis in paper: [explicit] The paper acknowledges limitations in covering non-sparse scenarios where "multi-W" or "multimodal" attention is needed
- Why unresolved: The theory and experiments focus on sparse scenarios (multi-doc QA) where "W-shaped" attention is sufficient
- What evidence would resolve it: Experiments testing the approach on non-sparse long-context tasks like document comparison or full-text summarization

### Open Question 2
- Question: What is the optimal percentage of "effective" tokens in the output for maximizing model performance in long-context tasks?
- Basis in paper: [inferred] The paper discusses the importance of "effective" tokens but notes that this is a qualitative and subjective indicator
- Why unresolved: The paper treats "effectiveness" as a qualitative concept without detailed quantitative analysis
- What evidence would resolve it: Systematic experiments varying the percentage of "effective" tokens in the output and measuring the impact on model performance

### Open Question 3
- Question: Does using "original text paraphrasing" data lead to overfitting and a decline in the model's other capabilities?
- Basis in paper: [explicit] The paper mentions this as a limitation, noting that the approach may alter the model's answering habits
- Why unresolved: The paper acknowledges this potential issue but does not provide experimental evidence to support or refute it
- What evidence would resolve it: Experiments comparing the model's performance on various tasks before and after training with "original text paraphrasing" data

## Limitations

- The method increases computational overhead due to longer outputs with paraphrasing components
- Generalizability to non-QA long-context tasks remains unproven and uncertain
- Potential for overfitting to the paraphrasing task, potentially degrading performance on other capabilities

## Confidence

**High Confidence**: The core observation that LLMs show "lost in the middle" behavior in long-context tasks is well-supported by existing literature and the paper's analysis of attention patterns.

**Medium Confidence**: The theoretical mechanism explaining why paraphrasing forces W-shaped attention patterns is plausible but relies on indirect evidence.

**Low Confidence**: The generalizability of the method to non-QA long-context tasks remains uncertain, as the paper focuses exclusively on multi-document QA scenarios.

## Next Checks

1. **Attention Pattern Analysis**: Conduct a detailed visualization study comparing attention distributions (U-shaped vs W-shaped) between models trained with and without the paraphrasing task across multiple layers and attention heads.

2. **Generalization Testing**: Evaluate the method on at least two different long-context task types (e.g., document summarization and multi-hop reasoning) beyond multi-document QA to assess whether the paraphrasing approach transfers to other domains.

3. **Ablation Study on Paraphrasing Percentage**: Systematically vary the percentage of paraphrased content in the output (e.g., 25%, 50%, 75%, 100%) to determine the optimal threshold where benefits plateau.