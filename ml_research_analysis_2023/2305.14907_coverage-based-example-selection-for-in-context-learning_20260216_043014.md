---
ver: rpa2
title: Coverage-based Example Selection for In-Context Learning
arxiv_id: '2305.14907'
source_url: https://arxiv.org/abs/2305.14907
tags:
- coverage
- demonstrations
- bm25
- test
- split
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for selecting demonstrations
  for in-context learning (ICL) that focuses on covering salient aspects of the test
  input, such as reasoning patterns. It proposes using contextual token embeddings,
  measured via BERTScore-Recall (BSR), to capture these aspects and extends BSR and
  other metrics to set-level versions for optimal demonstration selection.
---

# Coverage-based Example Selection for In-Context Learning

## Quick Facts
- arXiv ID: 2305.14907
- Source URL: https://arxiv.org/abs/2305.14907
- Authors: 
- Reference count: 31
- Key outcome: BERTScore-Recall (BSR) consistently outperforms standard metrics like BM25 and cosine similarity for selecting ICL demonstrations, with set-level selection (SET-BSR) providing additional gains especially for compositional tasks.

## Executive Summary
This paper introduces a framework for selecting demonstrations for in-context learning (ICL) that focuses on covering salient aspects of the test input, such as reasoning patterns. It proposes using contextual token embeddings, measured via BERTScore-Recall (BSR), to capture these aspects and extends BSR and other metrics to set-level versions for optimal demonstration selection. Experiments on 15 datasets across 6 tasks and 7 LLMs show that BSR consistently outperforms standard metrics like BM25 and cosine similarity, and that set-level selection using SET-BSR further improves performance—especially for compositional tasks—achieving up to 17 points higher accuracy and matching or exceeding task-specific learning-based methods.

## Method Summary
The framework computes BERTScore-Recall (BSR) between test inputs and candidate demonstrations using contextual token embeddings, then applies greedy set optimization to select demonstrations that maximize coverage of salient aspects. The set-level metric is submodular, enabling efficient optimization. SET-BSR extends this by considering coverage across sets rather than individual demonstrations, selecting complementary examples that together cover all salient aspects of the test input.

## Key Results
- BSR consistently outperforms standard metrics like BM25 and cosine similarity across 15 datasets and 7 LLMs
- SET-BSR provides additional improvements, especially for compositional tasks, achieving up to 17 points higher accuracy
- SET-BSR matches or exceeds task-specific learning-based methods despite being learning-free
- The approach scales effectively with larger LLMs, with gains particularly pronounced for compositional splits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BERTScore-Recall (BSR) better captures salient aspects of test inputs than cosine similarity or BM25.
- Mechanism: BSR uses contextual token embeddings to measure recall of salient aspects, capturing reasoning patterns and meaning rather than just surface form.
- Core assumption: Contextual embeddings effectively represent salient aspects like reasoning patterns, entities, etc.
- Evidence anchors: [abstract] "Using this framework, we show that contextual token embeddings effectively capture these salient aspects, and their recall measured using BERTScore-Recall (BSR) yields a reliable measure of informativeness." [section 4] "A better way to capture salient aspects is to use contextualized token embeddings, the idea behind the BERTScore (Zhang* et al., 2020) metric."

### Mechanism 2
- Claim: Set-level selection using SET-BSR improves coverage of salient aspects compared to independent selection.
- Mechanism: SET-BSR extends BSR to measure coverage across sets of demonstrations, selecting demonstrations that together cover all salient aspects rather than redundant similar examples.
- Core assumption: Salient aspects of test inputs are unlikely to be fully covered by a single demonstration.
- Evidence anchors: [abstract] "We further extend BSR and many standard metrics to measure the informative-ness of sets of demonstrations." [section 5] "Given the combinatorial space of sets of demonstrations, for a measure on sets to be practical, it needs to be efficiently optimizable. Fortunately, the set-level metric, as defined above, is also sub-modular for any definition of c(x, z)."

### Mechanism 3
- Claim: SET-BSR scales effectively with larger LLMs and outperforms learning-based methods without requiring task-specific training.
- Mechanism: As LLMs become more powerful, their ability to compose information from diverse demonstrations increases, making comprehensive coverage more valuable than similarity.
- Core assumption: Larger LLMs have better compositional reasoning capabilities that benefit from diverse, complementary demonstrations.
- Evidence anchors: [abstract] "With the 175B parameter Codex LLM, SET-BSR outperforms the standard cosine-similarity by upto 16% on average with upto 49% improvement in some splits, and, despite being learning-free, is on par or better than learning-based methods that require task and/or LLM-specific training." [section 7.1] "SET-BSR is more effective with larger LLMs... The trend is particularly pronounced in compositional splits."

## Foundational Learning

- Concept: Submodularity in set optimization
  - Why needed here: Enables efficient greedy optimization of demonstration sets while guaranteeing near-optimal coverage
  - Quick check question: What property makes greedy algorithms effective for set selection in this framework?

- Concept: Contextual embeddings and their role in semantic similarity
  - Why needed here: Core to understanding why BSR captures salient aspects better than traditional metrics
  - Quick check question: How do contextual embeddings differ from static embeddings in measuring semantic similarity?

- Concept: Information retrieval metrics (BM25, cosine similarity)
  - Why needed here: Understanding baseline metrics helps appreciate why BSR and SET-BSR offer improvements
  - Quick check question: What limitations do BM25 and cosine similarity have that make them less effective for ICL demonstration selection?

## Architecture Onboarding

- Component map: Input processing -> Embedding computation (BSR) -> Set optimization (greedy) -> Output generation
- Critical path: 1) Compute BSR scores between test input and all demonstrations 2) Apply greedy set optimization to maximize coverage 3) Return top-k demonstrations ordered by instance-level scores
- Design tradeoffs: Embedding quality vs. computational cost (deberta-large vs. base), coverage completeness vs. prompt length constraints, greedy approximation vs. optimal but expensive set selection
- Failure signatures: Poor performance despite high BSR scores (may indicate BSR not capturing relevant aspects), no improvement with set selection (could mean demonstrations already cover all aspects individually), degradation with larger models (might suggest over-reliance on diverse demonstrations)
- First 3 experiments: 1) Compare BSR vs. cosine similarity on a simple compositional task to verify core mechanism 2) Test set-level vs. independent selection on a dataset with redundant demonstrations 3) Evaluate performance scaling with LLM size using the same demonstration selection method

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does coverage-based set selection with BERTScore-Recall consistently improve ICL performance across all types of tasks, including non-compositional ones?
- Basis in paper: [explicit] The paper states that set selection is particularly effective in compositional settings but less so in IID splits.
- Why unresolved: The paper only shows aggregate results and specific examples, but doesn't provide a comprehensive analysis across all task types.
- What evidence would resolve it: A detailed breakdown of performance improvements for each task type (IID vs compositional) would clarify the generalizability of the method.

### Open Question 2
- Question: How does the choice of contextual embedding model (e.g., deberta-large-mnli vs deberta-base-mnli) impact the effectiveness of BERTScore-Recall for demonstration selection?
- Basis in paper: [explicit] The paper mentions using deberta-large-mnli by default but doesn't extensively analyze the impact of different models.
- Why unresolved: The paper only provides limited ablation studies on embedding model choice, leaving uncertainty about the optimal configuration.
- What evidence would resolve it: A systematic comparison of different contextual embedding models' impact on ICL performance would clarify the best choice.

### Open Question 3
- Question: Can the proposed framework be extended to handle more complex demonstration selection scenarios, such as when the candidate pool is extremely large or when the test input is ambiguous?
- Basis in paper: [inferred] The paper focuses on standard demonstration selection scenarios but doesn't address scalability or ambiguity.
- Why unresolved: The paper's experimental setup and analysis don't explore edge cases or extreme conditions that might arise in real-world applications.
- What evidence would resolve it: Experiments testing the framework's performance under large candidate pools and ambiguous test inputs would demonstrate its robustness.

## Limitations
- The effectiveness relies heavily on the assumption that contextual embeddings capture salient aspects like reasoning patterns, which is not directly validated across all task types
- The computational cost of BSR, which requires running a BERT model for each test-demonstration pair, could become prohibitive at scale
- The submodularity assumption for efficient set optimization lacks extensive empirical validation in the ICL context

## Confidence

High confidence: The empirical results showing BSR outperforming standard metrics (BM25, cosine similarity) across 15 datasets are robust and well-supported. The consistent improvements and the logical extension from instance-level to set-level selection provide strong evidence for the framework's effectiveness.

Medium confidence: The claim that SET-BSR scales effectively with larger LLMs is supported by experimental results but lacks theoretical grounding or ablation studies to isolate why larger models benefit more from diverse demonstrations.

Low confidence: The assumption that contextual embeddings specifically capture "salient aspects" like reasoning patterns is asserted but not directly tested. There's limited evidence showing what aspects of test inputs are actually being captured by BSR.

## Next Checks

1. **Ablation study on embedding types**: Compare BSR performance using different embedding sources (contextual vs. static, different pre-trained models) on a subset of tasks to isolate whether contextual embeddings specifically drive improvements, or if the benefits stem from other aspects of the scoring mechanism.

2. **Qualitative analysis of selected demonstrations**: Manually analyze demonstration sets selected by SET-BSR versus baseline methods on 2-3 representative test instances to verify that selected demonstrations actually cover complementary reasoning patterns rather than redundant content, directly testing the coverage hypothesis.

3. **Computational cost analysis**: Measure the wall-clock time and memory usage of BSR-based selection versus baseline methods across different dataset sizes and candidate pool sizes, comparing against the reported accuracy improvements to determine if the computational overhead is justified for practical deployment.