---
ver: rpa2
title: Efficient Subclass Segmentation in Medical Images
arxiv_id: '2307.00257'
source_url: https://arxiv.org/abs/2307.00257
tags:
- subclass
- superclass
- segmentation
- medical
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of fine-grained subclass segmentation
  in medical images when only limited subclass annotations and abundant superclass
  annotations are available. The proposed approach leverages the hierarchical structure
  of categories by introducing three key components: (1) a Prior Concatenation module
  that incorporates superclass predictions as prior knowledge to aid subclass segmentation,
  (2) a Separate Normalization module that enhances intra-class diversity within superclasses,
  and (3) a HierarchicalMix module that generates high-quality pseudo labels by fusing
  only similar superclass regions from labeled and unlabeled images.'
---

# Efficient Subclass Segmentation in Medical Images

## Quick Facts
- **arXiv ID**: 2307.00257
- **Source URL**: https://arxiv.org/abs/2307.00257
- **Reference count**: 31
- **Primary result**: Achieves 87.3% average Dice score on ACDC and 75.4% on BraTS2021 using only limited subclass and sufficient superclass annotations

## Executive Summary
This paper addresses the challenge of fine-grained subclass segmentation in medical images when only limited subclass annotations and abundant superclass annotations are available. The proposed approach leverages the hierarchical structure of categories by introducing three key components: Prior Concatenation, Separate Normalization, and HierarchicalMix modules. Experiments on BraTS2021 and ACDC datasets show that this method achieves comparable accuracy to models trained with full subclass annotations while using only limited subclass and sufficient superclass annotations. Specifically, the approach achieves 87.3% average Dice score on ACDC and 75.4% on BraTS2021, outperforming competing methods by 5.0% and 1.4% respectively.

## Method Summary
The proposed method introduces three novel components to leverage hierarchical structure for efficient subclass segmentation. Prior Concatenation concatenates superclass logits with feature maps to provide contextual priors for subclass discrimination. Separate Normalization applies distinct batch normalization to foreground and background regions to enhance intra-class diversity. HierarchicalMix generates high-quality pseudo labels by fusing only similar superclass regions from labeled and unlabeled images. The approach is evaluated on BraTS2021 and ACDC datasets using Dice coefficient and Hausdorff Distance metrics.

## Key Results
- Achieves 87.3% average Dice score on ACDC dataset
- Achieves 75.4% average Dice score on BraTS2021 dataset
- Outperforms competing methods by 5.0% and 1.4% respectively
- Matches performance of models trained with full subclass annotations using only limited subclass and sufficient superclass annotations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Prior Concatenation module leverages superclass logits as contextual priors to improve subclass discrimination.
- Mechanism: By concatenating the superclass predicted logits with the feature map before subclass classification, the network receives explicit guidance about which superclass region is being processed, allowing it to focus on fine-grained subclass distinctions within that context.
- Core assumption: The superclass classifier provides reliable coarse-level predictions that can serve as meaningful priors for subclass segmentation.
- Evidence anchors:
  - [abstract]: "a Prior Concatenation module that enhances confidence in subclass segmentation by concatenating predicted logits from the superclass classifier"
  - [section 2]: "we propose a simple yet effective method calledPrior Concatenation (PC): as shown in Fig. 1 (a), we concatenate predicted superclass logit scoresSc(xl) onto the feature mapsF (xl) and then perform subclass segmentation"
  - [corpus]: Weak - no direct evidence about concatenation effectiveness in related works
- Break condition: If superclass predictions are unreliable or noisy, concatenating them could degrade subclass segmentation by introducing misleading priors.

### Mechanism 2
- Claim: Separate Normalization improves intra-class diversity within superclasses to facilitate subclass discrimination.
- Mechanism: By applying separate batch normalization to foreground and background regions defined by superclass labels, the network can learn class-specific transformations that preserve discriminative features within superclasses while maintaining separation between different superclasses.
- Core assumption: The original batch normalization applied uniformly across all features reduces the distance between subclass samples within the same superclass, creating an optimization conflict.
- Evidence anchors:
  - [abstract]: "a Separate Normalization module that stretches the intra-class distance within the same superclass to facilitate subclass segmentation"
  - [section 2]: "we proposeSeparate Normalization(SN) to separately process feature maps belonging to hierarchical foreground and background divided by superclass labels"
  - [corpus]: Weak - no direct evidence about separate normalization in related works
- Break condition: If the foreground/background separation is not meaningful or if the dataset has very different characteristics than medical images.

### Mechanism 3
- Claim: HierarchicalMix generates high-quality pseudo labels by fusing only similar superclass regions from labeled and unlabeled images.
- Mechanism: The method crops and overlays foreground regions from a labeled image with the corresponding foreground region in an unlabeled image, creating a mixed image with pseudo subclass labels. This targeted mixing preserves semantic coherence while transferring knowledge.
- Core assumption: Mixing only similar semantic regions (same superclass) provides more effective knowledge transfer than whole-image mixing.
- Evidence anchors:
  - [abstract]: "a HierarchicalMix model that generates high-quality pseudo labels for unlabeled samples by fusing only similar superclass regions from labeled and unlabeled images"
  - [section 2]: "We address this limitation by exploiting the additional superclass information for a more targeted mixup. This information allows us to fuse only the semantic foreground regions"
  - [corpus]: Moderate - GuidedMix [23] mentioned as inspiration, but the specific targeted mixing approach is novel
- Break condition: If the assumption about similar regions being beneficial is incorrect, or if the resizing/cropping operations introduce artifacts that confuse the model.

## Foundational Learning

- Concept: Semi-supervised learning with limited labeled data
  - Why needed here: The approach operates with only a small fraction of subclass-labeled images (n ≪ N), requiring techniques to leverage the abundant superclass labels effectively
  - Quick check question: What are the key differences between this setting and standard semi-supervised learning where we have unlabeled data?

- Concept: Hierarchical classification and segmentation
  - Why needed here: The problem explicitly defines a hierarchy (superclasses → subclasses), and the solution leverages this structure through specialized modules
  - Quick check question: How does the hierarchical structure differ from multi-task learning where tasks are independent?

- Concept: Data augmentation through mixup and pseudo-labeling
  - Why needed here: With limited subclass annotations, the approach needs to generate additional supervision signals through mixing and pseudo-labeling techniques
  - Quick check question: What are the risks of confirmation bias in pseudo-labeling, and how does the approach attempt to mitigate them?

## Architecture Onboarding

- Component map: U-Net backbone -> Feature extraction -> Prior Concatenation -> Separate Normalization -> Classification
- Critical path: Feature extraction → Prior Concatenation → Separate Normalization → Classification
- Design tradeoffs:
  - Prior Concatenation vs. Negative Learning: Concatenation is simpler and doesn't introduce additional loss terms, but relies on reliable superclass predictions
  - Separate Normalization vs. standard normalization: Provides better subclass discrimination but adds architectural complexity
  - HierarchicalMix vs. standard mixup: More targeted but requires careful implementation of region detection and resizing
- Failure signatures:
  - Poor superclass classifier performance → Prior Concatenation becomes harmful
  - Incorrect foreground/background separation → Separate Normalization degrades performance
  - Poorly matched regions in HierarchicalMix → noisy pseudo labels that confuse the model
- First 3 experiments:
  1. Implement U-Net with superclass and subclass heads only (baseline)
  2. Add Prior Concatenation module and evaluate impact
  3. Add Separate Normalization and evaluate impact (can be done independently of HierarchicalMix)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal weighting factor α for the HierarchicalMix module across different medical imaging tasks and datasets?
- Basis in paper: [explicit] The paper mentions α is sampled according to a uniform distribution on [0.5, 1], but does not investigate optimal values or task-specific tuning.
- Why unresolved: The paper uses a fixed uniform distribution without exploring how different α values affect performance across datasets or imaging modalities.
- What evidence would resolve it: Systematic ablation studies varying α across different ranges and datasets, with performance comparisons to identify optimal values for specific medical imaging tasks.

### Open Question 2
- Question: How does the proposed method scale to more than two hierarchical levels (e.g., super-superclass, superclass, subclass)?
- Basis in paper: [inferred] The paper assumes R=2 (background and foreground) and extends foreground to multiple subclasses, but doesn't explore deeper hierarchical structures that may exist in medical imaging.
- Why unresolved: The method is designed for a two-level hierarchy, and extending it to deeper hierarchies would require modifications to the Prior Concatenation and Separate Normalization modules.
- What evidence would resolve it: Experimental validation on datasets with deeper hierarchical structures, along with architectural modifications to handle multiple hierarchical levels.

### Open Question 3
- Question: What is the impact of the confidence threshold τ on the quality of pseudo labels generated by HierarchicalMix, and how should it be adjusted during training?
- Basis in paper: [explicit] The paper mentions τ starts at 1 and linearly decays to 0.4 throughout training, but doesn't investigate the sensitivity of performance to this parameter or alternative scheduling strategies.
- Why unresolved: The linear decay schedule is arbitrary, and the paper doesn't explore whether different initial values, decay rates, or adaptive schemes would improve performance.
- What evidence would resolve it: Systematic experiments varying τ initialization, decay schedules, and adaptive schemes, with analysis of their impact on pseudo label quality and final segmentation performance.

## Limitations

- Implementation details of the three proposed modules are not fully specified, affecting reproducibility
- No ablation studies isolating the contribution of each module to overall performance
- Limited experimental validation across diverse medical imaging datasets or tasks
- Comparison with only a few baseline methods limits generalizability of conclusions

## Confidence

- **High confidence**: The overall problem formulation and the general approach of leveraging hierarchical structure are well-established concepts in the field. The reported quantitative results on the two datasets appear reasonable for medical image segmentation tasks.
- **Medium confidence**: The specific implementation details of the three proposed modules are described conceptually but lack precise architectural specifications. The effectiveness of these modules relative to simpler alternatives is not thoroughly evaluated.
- **Low confidence**: The paper does not provide extensive experimental validation across diverse medical imaging datasets or tasks. The comparison with only a few baseline methods limits the generalizability of the conclusions.

## Next Checks

1. **Module Isolation Testing**: Implement each proposed module (Prior Concatenation, Separate Normalization, HierarchicalMix) independently in a controlled setting to measure their individual contributions to segmentation performance.
2. **Cross-Dataset Generalization**: Evaluate the method on additional medical imaging datasets beyond ACDC and BraTS2021 to assess its generalizability across different imaging modalities and anatomical structures.
3. **Robustness Analysis**: Test the method's performance under varying levels of superclass annotation quality to determine the sensitivity of the approach to noisy or imperfect superclass predictions.