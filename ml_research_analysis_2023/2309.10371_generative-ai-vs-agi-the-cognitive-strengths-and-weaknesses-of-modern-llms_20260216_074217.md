---
ver: rpa2
title: 'Generative AI vs. AGI: The Cognitive Strengths and Weaknesses of Modern LLMs'
arxiv_id: '2309.10371'
source_url: https://arxiv.org/abs/2309.10371
tags:
- llms
- page
- intelligence
- human
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares the cognitive capabilities of modern Large
  Language Models (LLMs) like GPT-4 with human-like Artificial General Intelligence
  (AGI). It finds that while LLMs excel at certain tasks like language processing
  and ethical reasoning, they fall short in areas requiring complex multi-step reasoning,
  robust world-modeling, and fundamental creativity.
---

# Generative AI vs. AGI: The Cognitive Strengths and Weaknesses of Modern LLMs

## Quick Facts
- arXiv ID: 2309.10371
- Source URL: https://arxiv.org/abs/2309.10371
- Reference count: 23
- Primary result: LLMs excel at language processing and ethical reasoning but fall short in complex multi-step reasoning, robust world-modeling, and fundamental creativity, suggesting they are not on a path to AGI without significant architectural changes.

## Executive Summary
This paper compares the cognitive capabilities of modern Large Language Models (LLMs) like GPT-4 with human-like Artificial General Intelligence (AGI). It finds that while LLMs excel at certain tasks like language processing and ethical reasoning, they fall short in areas requiring complex multi-step reasoning, robust world-modeling, and fundamental creativity. The author argues that incremental improvements to LLMs are unlikely to lead to true AGI due to their limited cognitive architectures. Instead, the paper suggests incorporating LLMs as components within broader AGI architectures that more closely mimic human cognitive processes. It concludes that while LLMs are powerful tools, they are not on a path to achieve human-level AGI without significant architectural changes.

## Method Summary
The paper reviews literature on AGI, analyzes LLM capabilities and limitations, and compares them against cognitive science frameworks like the Standard Model of Mind. It examines performance data on key cognitive tasks including reasoning, world-modeling, and creativity. The analysis draws from existing AI benchmarks, cognitive psychology research, and technical analysis of LLM behavior patterns to identify systematic limitations in current architectures.

## Key Results
- LLMs demonstrate strong language processing and ethical reasoning but struggle with complex multi-step reasoning requiring consistent world models
- Current LLMs lack fundamental creativity and world-modeling capabilities, relying instead on pattern matching and statistical combinations
- Incremental improvements to LLM architectures are unlikely to achieve human-level AGI without significant architectural changes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs fail at multi-step reasoning because their internal representations are patchwork and lack cross-consistent world models
- Mechanism: LLMs learn patterns from massive datasets but without grounding in real-world experience, their representations are fragments that don't form coherent, cross-consistent models
- Core assumption: World modeling requires more than pattern matching from text data - it needs grounding in experience and internal consistency
- Evidence anchors:
  - "Failures on fairly basic examples of temporal and spatial reasoning...highlight the inability to construct and leverage models of time and space"
  - "ChatGPT's performance on physics problems...it will often correctly explain the detailed process for solving or applying a certain sort of equation – but then, when given a specific example of this process, it will fail to execute it correctly"
- Break condition: If LLMs develop better grounding mechanisms or hybrid architectures that connect pattern matching to real-world experience

### Mechanism 2
- Claim: LLMs hallucinate because they are trained to maximize probability of next token, not truth
- Mechanism: The fundamental training objective of predicting most probable next token naturally leads to generating plausible-sounding but fabricated content when true information isn't sufficiently represented in training data
- Core assumption: Probability maximization is fundamentally misaligned with truth-seeking behavior
- Evidence anchors:
  - "The core cause of this hallucination phenomenon is easy to see at a conceptual level: The underlying metric for which transformer neural nets are typically trained is to output the character most likely to come next in a given sequence"
- Break condition: If training objectives shift from probability maximization to truth-seeking metrics

### Mechanism 3
- Claim: LLMs lack creativity because they're fundamentally combinatory rather than generative in a deeper sense
- Mechanism: LLMs combine patterns from training data in statistically plausible ways but cannot generate truly novel concepts or structures because they lack the ability to form abstract representations and conceptual blends
- Core assumption: True creativity requires abstract conceptual representation and the ability to form novel combinations at the conceptual level, not just pattern matching
- Evidence anchors:
  - "The more interesting-in-context artistic quality of these results as compared to GPT-4 poetry illustrates that a well-constructed small model can outperform large models for particular aesthetic purposes. But this also is not fundamental inventiveness – it's bald imitation of the style and concepts of particular inventive writers"
- Break condition: If LLMs develop architectures that enable true conceptual abstraction and blending

## Foundational Learning

- Concept: World modeling and symbol grounding
  - Why needed here: LLMs lack grounded world models, leading to failures in reasoning and hallucination
  - Quick check question: How would you test whether an LLM has a grounded understanding of a concept versus just pattern matching?

- Concept: Multi-step reasoning and plan execution
  - Why needed here: LLMs struggle with complex reasoning tasks requiring multiple steps and consistent world models
  - Quick check question: What architectural changes would enable an LLM to execute complex plans reliably?

- Concept: Creativity and conceptual blending
  - Why needed here: LLMs can only combine existing patterns rather than generate truly novel concepts
  - Quick check question: How would you measure whether an AI system is truly creative versus just combining existing patterns?

## Architecture Onboarding

- Component map: Input → LLM processing → pattern matching → external memory lookup → reasoning module → output
- Critical path: Input → LLM processing → pattern matching → external memory lookup → reasoning module → output
- Design tradeoffs: Complexity vs performance, training data vs generalization, probability vs truth
- Failure signatures: Hallucination, inability to execute complex plans, lack of true creativity
- First 3 experiments:
  1. Test LLM performance on multi-step reasoning tasks with and without external memory
  2. Compare LLM output quality when trained on probability vs truth-seeking objectives
  3. Evaluate LLM creativity by measuring novelty of generated content versus training data patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How far can current LLM architectures be pushed in terms of complex multi-step reasoning before hitting fundamental limitations?
- Basis in paper: [explicit] The paper discusses the limited capacity of LLMs for complex multi-step reasoning and notes that while strategies like chain-of-thought prompting can help, LLMs still follow crude "greedy" inference heuristics. It states that it's hard to tell when the performance improvements achievable within the GPT architecture will top out.
- Why unresolved: The paper notes that while LLMs show interesting success at basic math and science reasoning, their capability falls apart as problems get more advanced. The exact point at which current architectures will fail for more complex reasoning is unknown.
- What evidence would resolve it: Systematic testing of current LLMs on increasingly complex reasoning tasks, pushing the boundaries of what they can handle. Comparing performance to human experts on these tasks would provide a benchmark.

### Open Question 2
- Question: Can LLMs be effectively integrated into broader AGI architectures to overcome their cognitive limitations?
- Basis in paper: [explicit] The paper discusses potential strategies for incorporating LLMs into broader AGI architectures, such as Bengio and Hu's proposal to couple an LLM with a reinforcement learning module and an MDL-based pattern learner. It also mentions the possibility of integrating LLMs into cognitive architectures like OpenCog Hyperon.
- Why unresolved: While the paper outlines potential approaches, it doesn't provide concrete evidence of how well these integrations would work in practice. The effectiveness of these strategies remains to be seen.
- What evidence would resolve it: Implementation and testing of these integrated architectures, comparing their performance on various tasks to that of pure LLMs and other AGI approaches. Success would be demonstrated by the integrated systems surpassing the limitations of LLMs alone.

### Open Question 3
- Question: To what extent can incremental improvements to current LLM architectures lead to systems with human-level general intelligence?
- Basis in paper: [explicit] The paper argues that incremental improvement of current LLMs is not a viable approach to working toward human-level AGI, given the substantial differences between the cognitive systems LLMs are and the sort of cognitive systems human beings are. It states that while LLMs are powerful tools, they are not on a path to achieve human-level AGI without significant architectural changes.
- Why unresolved: The paper presents a strong argument against incremental improvements leading to AGI, but doesn't definitively rule out the possibility. It's possible that a series of significant improvements could eventually bridge the gap.
- What evidence would resolve it: Tracking the progress of LLM capabilities over time, comparing them to the cognitive requirements for AGI. If LLMs continue to fall short in key areas like world-modeling, creativity, and complex reasoning despite improvements, it would support the paper's argument. Conversely, if LLMs suddenly leap forward in these areas, it could indicate a path to AGI.

## Limitations
- Rapidly evolving LLM capabilities may overcome identified limitations before research can be completed
- Difficulty distinguishing between temporary implementation issues and fundamental architectural constraints
- Challenge of defining precise benchmarks for human-like creativity and reasoning

## Confidence
- High confidence in identifying current architectural constraints based on robust evidence from multiple reasoning benchmarks and cognitive science frameworks
- Medium confidence regarding whether these limitations are fundamental or can be overcome through architectural innovations
- High confidence in assessment of LLMs as "powerful tools but not on path to AGI" based on documented performance gaps

## Next Checks
1. **Benchmark tracking**: Establish systematic tracking of LLM performance on complex reasoning tasks (multi-step problem solving, causal reasoning) to detect whether identified limitations persist across model generations
2. **Hybrid architecture evaluation**: Design experiments testing whether LLM integration with explicit world models and reasoning modules significantly improves performance on tasks requiring cross-consistent knowledge
3. **Creativity quantification**: Develop metrics to distinguish between pattern combination and genuine conceptual novelty in LLM outputs, then test whether architectural modifications can generate more novel combinations