---
ver: rpa2
title: 'Thought Propagation: An Analogical Approach to Complex Reasoning with Large
  Language Models'
arxiv_id: '2310.03965'
source_url: https://arxiv.org/abs/2310.03965
tags:
- reasoning
- arxiv
- problems
- node
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Thought Propagation addresses the limitations of existing large
  language model reasoning approaches that reason from scratch without leveraging
  insights from similar problems and suffer from accumulated errors in multi-step
  reasoning. The core idea is to explore analogous problems related to the input problem
  and use their solutions to enhance reasoning.
---

# Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models

## Quick Facts
- arXiv ID: 2310.03965
- Source URL: https://arxiv.org/abs/2310.03965
- Reference count: 6
- Primary result: Thought Propagation improves reasoning performance across three tasks with 12-15% absolute gains

## Executive Summary
Thought Propagation (TP) addresses limitations in existing large language model reasoning approaches by leveraging insights from analogous problems rather than reasoning from scratch. The framework proposes and solves a set of related problems, then aggregates their solutions to enhance the final answer. This analogical approach shows substantial improvements across diverse reasoning tasks including shortest-path finding, creative writing, and agent planning.

## Method Summary
Thought Propagation is a three-module framework that enhances LLM reasoning through analogical problem-solving. The LLM Propose module generates analogous problems related to the input problem. The LLM Solve module then solves both the original and analogous problems using existing prompting methods like Chain-of-Thought. Finally, the LLM Aggregate module combines insights from these solutions to either directly yield a refined solution or derive a knowledge-intensive plan for execution.

## Key Results
- 12% absolute increase in finding optimal solutions in shortest-path reasoning tasks
- 13% improvement in human preference for creative writing outputs
- 15% enhancement in task completion rate for LLM-agent planning tasks

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Reuse from Analogous Problems
TP enables knowledge reuse by having LLMs propose and solve related problems, then transfer their solutions to improve the original problem-solving. This approach leverages the insight that similar problems often share reusable solutions and strategies.

### Mechanism 2: Error Mitigation in Multi-Step Reasoning
Instead of reasoning from scratch through intermediate steps where errors can accumulate, TP uses solutions from analogous problems to provide a more robust foundation. This reduces the impact of errors in intermediate stages that typically misguide subsequent planning.

### Mechanism 3: Plug-and-Play Generalization
TP is designed to be compatible with existing prompting approaches, allowing it to enhance various tasks without extensive task-specific prompt engineering. This makes it broadly applicable across different reasoning domains.

## Foundational Learning

- **Analogical Reasoning**: The core principle behind TP - understanding how transferring insights from similar problems can improve solving new ones. Quick check: How does analogical reasoning differ from reasoning from scratch, and why might it be beneficial?

- **Prompt Engineering**: TP builds upon existing methods like Chain-of-Thought. Quick check: What are the key differences between few-shot prompting, Chain-of-Thought prompting, and Tree-of-Thought prompting?

- **Graph Neural Networks (GNNs)**: Mentioned in context of graph-related tasks and LLM integration. Quick check: How do GNNs aggregate neighborhood node messages, and how might this relate to propagating insights?

## Architecture Onboarding

- **Component map**: LLM Propose -> LLM Solve -> LLM Aggregate
- **Critical path**: 1) LLM Propose generates analogous problems, 2) LLM Solve solves all problems using existing prompting methods, 3) LLM Aggregate refines solution or derives plan
- **Design tradeoffs**: Balancing number of analogous problems (more insights vs. higher computational cost), choosing appropriate prompting methods (simplicity vs. effectiveness), deciding between direct solution vs. plan derivation
- **Failure signatures**: Irrelevant analogous problems, ineffective prompting methods for specific tasks, poor aggregation leading to ineffective insight combination
- **First 3 experiments**: 1) Implement TP on shortest path finding and compare to baselines, 2) Vary number of analogous problems to find optimal balance, 3) Compare performance across different prompting methods (IO, CoT, ToT)

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the methodology and results, several key questions remain: How does TP performance scale with the number of analogous problems? How well does TP generalize to tasks beyond those tested? How does the choice of prompting method for LLM Solve impact overall performance?

## Limitations
- Quality of generated analogous problems is critical but not thoroughly evaluated
- Computational overhead from solving multiple problems is not fully analyzed
- Limited testing across diverse reasoning task types

## Confidence
- **High Confidence**: Core mechanism of using analogous problems shows consistent 12-15% performance gains across three tasks
- **Medium Confidence**: Claim of plug-and-play generalization across tasks, though broader validation needed
- **Low Confidence**: Assertion that TP specifically mitigates accumulated errors, lacking direct comparative evidence

## Next Checks
1. Conduct human evaluation of generated analogous problems to assess quality and relevance
2. Measure token usage and inference time of TP versus baselines to quantify computational overhead
3. Apply TP to a fourth, qualitatively different reasoning task to test cross-domain generalization