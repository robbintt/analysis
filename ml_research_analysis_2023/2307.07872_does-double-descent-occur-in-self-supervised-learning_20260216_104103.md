---
ver: rpa2
title: Does Double Descent Occur in Self-Supervised Learning?
arxiv_id: '2307.07872'
source_url: https://arxiv.org/abs/2307.07872
tags:
- double
- descent
- loss
- settings
- occur
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether double descent occurs in self-supervised
  learning settings, focusing on autoencoders (AEs). While double descent is a well-known
  phenomenon in supervised learning, previous works suggested it might not appear
  in self-supervised settings.
---

# Does Double Descent Occur in Self-Supervised Learning?

## Quick Facts
- arXiv ID: 2307.07872
- Source URL: https://arxiv.org/abs/2307.07872
- Authors: 
- Reference count: 16
- Primary result: Empirical evidence shows double descent does not occur in self-supervised autoencoder settings

## Executive Summary
This paper investigates whether double descent occurs in self-supervised learning by testing autoencoders on artificially generated data. While double descent is well-documented in supervised learning, previous work suggested it might not appear in self-supervised settings. The authors conducted controlled experiments varying latent and hidden dimensions of both standard and linear autoencoders. Their results consistently showed that test loss either followed a classical U-shape or monotonically decreased, but never exhibited the characteristic double-descent curve seen in supervised learning. This finding suggests double descent may not be a universal phenomenon across all learning paradigms.

## Method Summary
The authors generated artificial data using a latent variable model with specified noise levels and signal-to-noise ratios. They trained both standard and linear autoencoders with varying latent and hidden dimensions, using MSE reconstruction loss optimized via Adam. The experimental setup systematically explored different parameterization regimes to identify interpolation thresholds. Test loss was tracked across the parameter sweep to detect double-descent behavior. The controlled artificial data environment allowed precise manipulation of model capacity relative to data complexity.

## Key Results
- Test loss never exhibited double-descent curves across all tested autoencoder configurations
- Loss either followed classical U-shape or monotonically decreased with increasing overparameterization
- The interpolation regime was identified when latent dimension approached the true data dimension (20) or hidden width reached approximately 28
- Results were consistent across both standard and linear autoencoder architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Double descent does not occur because self-supervised reconstruction has different parameterization requirements than supervised classification
- Mechanism: The reconstruction task ties output dimensionality to input dimensionality, preventing the typical double descent condition where parameters ≈ dataset size × output dimension
- Core assumption: The interpolation peak requires exact training data fitting, which has different requirements in self-supervised settings
- Evidence anchors:
  - [abstract] "The test loss never exhibits double descent"
  - [section] "The few works that asked this found that double descent is not as universal as it seemed"
  - [corpus] "No double descent in PCA"

### Mechanism 2
- Claim: Different noise assumptions between supervised and self-supervised learning affect generalization dynamics
- Mechanism: Supervised learning assumes noisy labels while self-supervised reconstruction assumes clean inputs, changing where interpolation peaks occur
- Core assumption: Noise characteristics fundamentally alter the generalization landscape
- Evidence anchors:
  - [abstract] "We hope that further work on this will help elucidate the theoretical underpinnings"
  - [section] "the discrepancy between supervised and self-supervised settings may be because of their different formulations"
  - [corpus] Weak evidence - no direct corpus support

### Mechanism 3
- Claim: Different optimization dynamics prevent double descent in self-supervised learning
- Mechanism: Reconstruction loss has different landscape properties than classification loss, lacking critical points that create double descent
- Core assumption: Loss function geometry differs fundamentally between task types
- Evidence anchors:
  - [abstract] "The few works that asked this found that double descent is not as universal as it seemed"
  - [section] "The autoencoder transitions to the interpolation regime in a large part of the parameter space"
  - [corpus] "Double Descent Demystified"

## Foundational Learning

- Concept: Bias-variance tradeoff
  - Why needed here: Understanding classical U-shape behavior helps recognize when double descent does or doesn't occur
  - Quick check question: In classical learning theory, what happens to bias and variance as model capacity increases?

- Concept: Overparameterization
  - Why needed here: Double descent specifically occurs in overparameterized regime, so understanding this concept is crucial
  - Quick check question: What defines the "interpolation threshold" in double descent?

- Concept: Self-supervised learning
  - Why needed here: The paper specifically investigates self-supervised settings, so understanding these is fundamental
  - Quick check question: How does self-supervised learning differ from supervised learning in terms of data requirements?

## Architecture Onboarding

- Component map:
  Data generation module -> Autoencoder model -> Training loop -> Evaluation module -> Analysis tools

- Critical path:
  1. Generate synthetic data with known latent structure
  2. Configure autoencoder with varying latent and hidden dimensions
  3. Train model and monitor loss
  4. Identify whether interpolation peak appears
  5. Analyze results to determine presence/absence of double descent

- Design tradeoffs:
  - Artificial vs real data: Artificial data provides controlled experiments but may not capture real-world complexities
  - Linear vs nonlinear autoencoders: Linear models are easier to analyze but may miss phenomena present in nonlinear settings
  - Fixed architecture vs varied architecture: Keeping architecture constant isolates effects but may miss regime-specific phenomena

- Failure signatures:
  - Double descent appears: This would contradict the hypothesis and require explanation
  - No clear trend: May indicate insufficient parameterization range or experimental issues
  - High variance in results: Could indicate need for more runs or hyperparameter tuning

- First 3 experiments:
  1. Replicate the linear autoencoder experiment with varying dataset sizes to confirm no double descent
  2. Test a variational autoencoder to see if the reconstruction objective matters
  3. Add noise to the input data to test whether noise characteristics affect double descent emergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What theoretical mechanisms prevent double descent from occurring in self-supervised learning tasks?
- Basis in paper: [explicit] "The discrepancy between supervised and self-supervised settings may be because of their different formulations—while the former assumes noisy data, in the latter case, the model tries to learn a distribution."
- Why unresolved: The authors only speculate about potential reasons without providing rigorous theoretical analysis
- What evidence would resolve it: A formal proof showing under what conditions self-supervised learning tasks cannot exhibit double descent

### Open Question 2
- Question: Does double descent ever occur in any self-supervised learning architecture, or is it universally absent?
- Basis in paper: [explicit] "We hope that our findings will help elucidate the theory of overparameterized models" and acknowledges experiments were limited to autoencoders
- Why unresolved: Experiments were limited to autoencoders, and authors state "A fundamental analysis and more extensive experiments are necessary"
- What evidence would resolve it: Systematic testing across diverse self-supervised architectures (VAEs, contrastive learning, masked autoencoders, etc.)

### Open Question 3
- Question: How does the definition of "output size" affect the expected location of the interpolation peak in self-supervised models?
- Basis in paper: [explicit] "it is unclear what is the output's size. For example, one could argue it is the AE's latent dimension or its number of features."
- Why unresolved: Ambiguity in defining output size makes it impossible to predict where interpolation peak should occur
- What evidence would resolve it: Experiments that systematically vary both latent dimension and input/output dimensions while tracking interpolation peak locations

### Open Question 4
- Question: Does the relationship between data dimension, latent dimension, and hidden width create a regime where double descent could emerge in autoencoders?
- Basis in paper: [explicit] "This transition happens when the latent dimension is about 20, which is the latent dimension of the data, or when the hidden width is about 28"
- Why unresolved: Paper only tested limited range and doesn't explore whether crossing specific dimensional thresholds might trigger double descent
- What evidence would resolve it: Experiments systematically exploring full parameter space around critical dimensions

## Limitations

- Limited to autoencoders: Results may not generalize to other self-supervised architectures like contrastive learning or masked language models
- Artificial data only: Findings based on synthetically generated data may not reflect real-world dataset behavior
- Single task type: Only reconstruction-based self-supervised learning was investigated, leaving open whether other self-supervised tasks might exhibit double descent

## Confidence

Our confidence in the claim that double descent does not occur in self-supervised learning is Medium. While the empirical results with autoencoders are clear and reproducible, the analysis is limited to a specific self-supervised task and artificial data.

## Next Checks

1. Test the autoencoder setup on real-world datasets to verify the absence of double descent is not an artifact of artificial data generation.
2. Explore different self-supervised tasks (e.g., contrastive learning, masked reconstruction) to determine if the phenomenon is specific to reconstruction-based approaches.
3. Analyze the loss landscape geometry for different self-supervised tasks to understand the optimization dynamics that may prevent double descent.