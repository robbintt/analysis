---
ver: rpa2
title: SHAP@k:Efficient and Probably Approximately Correct (PAC) Identification of
  Top-k Features
arxiv_id: '2307.04850'
source_url: https://arxiv.org/abs/2307.04850
tags:
- shap
- features
- samplingshap
- tkip
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Top-k Identification Problem (TkIP) for
  identifying the k most important features based on SHAP values, motivated by finance
  applications requiring explanations for adverse actions. The authors frame TkIP
  as an Explore-m problem from multi-armed bandits literature and leverage this connection
  to develop more sample-efficient variants of KernelSHAP and SamplingSHAP.
---

# SHAP@k:Efficient and Probably Approximately Correct (PAC) Identification of Top-k Features

## Quick Facts
- arXiv ID: 2307.04850
- Source URL: https://arxiv.org/abs/2307.04850
- Reference count: 30
- Primary result: 1.2-14.7x improvements in sample efficiency and runtime for identifying top-k features with PAC guarantees

## Executive Summary
This paper introduces the Top-k Identification Problem (TkIP) for efficiently identifying the k most important features based on SHAP values, motivated by finance applications requiring explanations for adverse actions. The authors frame TkIP as an Explore-m problem from multi-armed bandits literature and develop more sample-efficient variants of KernelSHAP and SamplingSHAP. Their key innovations are an overlap-based stopping condition that avoids reducing all confidence intervals below epsilon and a greedy sampling scheme that allocates samples to features most likely to change the top-k subset. The methods are evaluated on four credit-related datasets and demonstrate significant improvements in sample efficiency and runtime compared to baseline approaches.

## Method Summary
The paper develops two main innovations: an overlap-based stopping condition and a greedy sampling scheme. The stopping condition terminates sampling when the lower confidence bound of the k-th highest feature is sufficiently separated from the upper confidence bound of the (k+1)-th feature, avoiding the need to reduce all confidence intervals below epsilon. The greedy sampling scheme identifies a "critical pair" - the lowest feature in the current top-k and the highest feature outside top-k - and allocates additional samples only to these two features. These methods are applied to modify both KernelSHAP and SamplingSHAP algorithms, with the greedy sampling scheme only applicable to SamplingSHAP due to its sequential nature.

## Key Results
- Demonstrated 1.2-14.7x improvements in sample efficiency compared to baseline methods
- Achieved significant runtime reductions across four credit-related datasets
- Successfully identified top-k features with PAC guarantees using fewer function evaluations
- The greedy sampling scheme provides additional efficiency gains when applicable to SamplingSHAP

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The overlap-based stopping condition improves sample efficiency by stopping early when the top-k subset is already identified with PAC guarantees.
- Mechanism: Instead of requiring all confidence intervals to be narrower than epsilon, the method only requires that the lower confidence bound of the k-th highest feature is sufficiently separated from the upper confidence bound of the (k+1)-th feature.
- Core assumption: The confidence intervals provide accurate probability bounds on the true SHAP values, and the stopping condition correctly captures when the top-k identification is PAC-guaranteed.
- Evidence anchors:
  - [abstract]: "a better stopping-condition (to stop sampling) that identifies when PAC (Probably Approximately Correct) guarantees have been met"
  - [section]: "By only considering the overlap between the CIs, the improved stopping condition avoids the need to reduce all the CIs widths to below ϵ as shown in Fig. 1b."
- Break condition: If the confidence intervals are systematically misestimated (e.g., due to non-Gaussian distributions of SHAP estimates), the overlap-based stopping condition may terminate too early or too late.

### Mechanism 2
- Claim: The greedy sampling scheme allocates samples more efficiently by focusing on features that are likely to change the top-k subset.
- Mechanism: Instead of allocating samples based on feature variance, the method identifies the "critical pair" - the lowest feature in the current top-k and the highest feature outside top-k - and allocates additional samples only to these two features.
- Core assumption: The current top-k ranking provides a good approximation of the true ranking, and the critical pair is indeed the most likely to change the top-k subset.
- Evidence anchors:
  - [abstract]: "a greedy sampling scheme that judiciously allocates samples between different features"
  - [section]: "This scheme improves sample efficiency by allocating more samples to (h, ℓ), which are exactly the features that can potentially affect what is inside TOPK."
- Break condition: If the initial feature ranking is highly unstable or if there are many features with similar SHAP values, the greedy sampling scheme may miss important features outside the critical pair.

### Mechanism 3
- Claim: Framing TkIP as an Explore-m problem enables the application of well-established multi-armed bandit techniques.
- Mechanism: By mapping features to arms and SHAP estimates to rewards, the method leverages existing PAC algorithms for identifying the top-m arms to solve the feature identification problem.
- Core assumption: The SHAP estimation process can be treated as independent sampling from each "arm" (feature), and the multi-armed bandit framework applies.
- Evidence anchors:
  - [abstract]: "TkIP can be framed as an Explore-m problem [14]–a well-studied problem related to multi-armed bandits (MAB)"
  - [section]: "This connection allows us to improve sample efficiency by using (1) an overlap-based stopping-condition and (2) a greedy sampling scheme that efficiently allocates samples between different features."
- Break condition: If SHAP estimates for different features are highly correlated or if the sampling process introduces dependencies between features, the multi-armed bandit framework may not apply.

## Foundational Learning

- Concept: Multi-armed bandit theory and PAC guarantees
  - Why needed here: The entire efficiency improvement relies on adapting bandit algorithms for feature selection
  - Quick check question: What is the difference between regret minimization and PAC identification in multi-armed bandits?

- Concept: Shapley value computation and sampling-based approximation
  - Why needed here: Understanding how KernelSHAP and SamplingSHAP work is essential to modify them for TkIP
  - Quick check question: Why is exact Shapley value computation exponential in the number of features?

- Concept: Confidence interval construction using Central Limit Theorem
  - Why needed here: Both the stopping condition and sampling scheme rely on accurate confidence intervals
  - Quick check question: How does the confidence level (1-δ) affect the width of the confidence intervals?

## Architecture Onboarding

- Component map:
  - SHAP estimation engine (KernelSHAP or SamplingSHAP) -> Confidence interval calculator -> Feature ranking module -> Overlap-based stopping condition checker -> Greedy sampling allocator (for SamplingSHAP@k only)

- Critical path:
  1. Initial SHAP estimation for all features
  2. Confidence interval calculation
  3. Feature ranking and identification of critical pair
  4. Greedy sampling allocation (if using SamplingSHAP@k)
  5. Stopping condition evaluation
  6. Repeat 4-5 until stopping condition met

- Design tradeoffs:
  - KernelSHAP@k vs SamplingSHAP@k: KernelSHAP@k cannot use greedy sampling but may have lower per-sample cost; SamplingSHAP@k can use greedy sampling but has higher per-sample cost
  - Accuracy vs sample efficiency: Tighter epsilon values require more samples but provide better approximations
  - Feature independence assumption: The method assumes features are independent, which may not hold in practice

- Failure signatures:
  - Premature stopping: The overlap condition is met but the true top-k features are incorrect
  - Excessive sampling: The algorithm continues sampling even after the top-k features are clearly identified
  - Unstable rankings: Small changes in SHAP estimates cause large changes in the top-k subset

- First 3 experiments:
  1. Run baseline KernelSHAP on a small credit dataset and measure the number of samples needed to achieve PAC guarantees
  2. Implement overlap-based stopping condition and verify it stops earlier while maintaining PAC guarantees
  3. Add greedy sampling scheme to SamplingSHAP and compare sample efficiency against baseline SamplingSHAP

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed methods perform when applied to GroupSHAP for handling feature correlations?
- Basis in paper: [explicit] The paper mentions that SHAP assumes features are not correlated, and suggests GroupSHAP as a potential solution, leaving evaluation as future work.
- Why unresolved: The paper does not evaluate KernelSHAP@k or SamplingSHAP@k with GroupSHAP, which is designed to handle correlated features by grouping them.
- What evidence would resolve it: Experimental results comparing the performance of KernelSHAP@k and SamplingSHAP@k when applied to GroupSHAP on datasets with correlated features.

### Open Question 2
- Question: What is the impact of the ordering of top-k features on the effectiveness of the proposed methods?
- Basis in paper: [explicit] The paper notes that their methods identify the top-k features but do not guarantee their correct order, suggesting repeated application of Kernel/SamplingSHAP@k with different k values as a potential solution.
- Why unresolved: The paper does not evaluate the repeated application method or its effectiveness in ordering top-k features correctly.
- What evidence would resolve it: Empirical studies comparing the ordered top-k features obtained by repeated application of Kernel/SamplingSHAP@k with the true order, assessing accuracy and efficiency.

### Open Question 3
- Question: How does the performance of KernelSHAP@k and SamplingSHAP@k scale with the number of features (d) in the dataset?
- Basis in paper: [inferred] The paper demonstrates significant improvements in sample efficiency and runtime for credit-related datasets but does not explicitly analyze scalability with increasing feature dimensions.
- Why unresolved: The paper focuses on specific datasets with a limited number of features and does not explore the scalability of the methods as the feature dimension grows.
- What evidence would resolve it: Experiments varying the number of features in datasets and measuring the sample cost and runtime of KernelSHAP@k and SamplingSHAP@k, providing insights into their scalability.

## Limitations

- The methods assume feature independence, which may not hold in practice for correlated features
- The greedy sampling scheme may miss important features if the initial ranking is unstable or if multiple features have similar SHAP values
- Confidence intervals constructed using CLT may be inaccurate for small datasets or non-Gaussian distributions

## Confidence

**High confidence**: The core theoretical connection between TkIP and Explore-m problems is well-established in multi-armed bandit literature. The sample efficiency improvements (1.2-14.7x) are directly measured and reported.

**Medium confidence**: The PAC guarantees hold under the stated assumptions, but real-world performance may vary depending on feature correlations and distribution characteristics. The greedy sampling scheme's effectiveness is demonstrated empirically but lacks theoretical guarantees.

**Low confidence**: The specific implementation details of the greedy sampling scheme and the exact parameter settings (epsilon, delta) are not fully specified, making exact reproduction challenging.

## Next Checks

1. **Validation of PAC Guarantees**: Run the proposed methods on a synthetic dataset where the true top-k features are known, and verify that the PAC guarantees hold across multiple runs with different random seeds.

2. **Sensitivity to Feature Correlation**: Test the methods on datasets with varying levels of feature correlation to assess how correlation affects sample efficiency and the accuracy of the greedy sampling scheme.

3. **Comparison with Alternative Stopping Conditions**: Implement and compare the overlap-based stopping condition with alternative approaches (such as fixed-width confidence intervals) to quantify the specific contribution of this innovation to overall sample efficiency.