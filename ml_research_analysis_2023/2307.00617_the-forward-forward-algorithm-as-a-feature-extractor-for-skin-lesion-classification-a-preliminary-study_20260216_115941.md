---
ver: rpa2
title: 'The Forward-Forward Algorithm as a feature extractor for skin lesion classification:
  A preliminary study'
arxiv_id: '2307.00617'
source_url: https://arxiv.org/abs/2307.00617
tags:
- skin
- classification
- lesion
- algorithm
- backpropagation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using the Forward-Forward Algorithm (FFA) as
  a feature extractor for skin lesion classification. The FFA is a neural network
  training method that does not require backpropagation, aiming to reduce computational
  costs and hardware constraints.
---

# The Forward-Forward Algorithm as a feature extractor for skin lesion classification: A preliminary study

## Quick Facts
- arXiv ID: 2307.00617
- Source URL: https://arxiv.org/abs/2307.00617
- Reference count: 8
- This paper explores using the Forward-Forward Algorithm (FFA) as a feature extractor for skin lesion classification, achieving lowest error rates (23.31% on ISIC 2016, 30.44% on HAM10000) with the FFA+BP approach.

## Executive Summary
This study investigates the Forward-Forward Algorithm (FFA) as an alternative to backpropagation for training neural networks in skin lesion classification. The authors propose using FFA as a feature extractor followed by a backpropagation-based classifier, comparing this hybrid approach against FFA-only and BP-only methods. Experimental results on two skin lesion datasets (ISIC 2016 and HAM10000) demonstrate that the FFA+BP combination achieves lower error rates and higher ROC-AUC scores than either method alone, suggesting FFA can serve as an effective feature extraction technique when combined with traditional backpropagation.

## Method Summary
The study compares three training approaches (FFA-only, BP-only, and FFA+BP) using fully connected neural networks with 3 hidden layers (784, 500, 500 units) on two skin lesion datasets. FFA is implemented by training networks with positive data (real images) and negative data (real images with added noise), using mean squared error loss. The FFA+BP approach first trains the network with FFA to extract features, then uses these features as input to a BP classifier. All models are trained using Adam optimizer (learning rate 1e-3) and evaluated using error rate and ROC-AUC metrics.

## Key Results
- FFA+BP achieved the lowest error rates: 23.31% on ISIC 2016 and 30.44% on HAM10000
- FFA+BP produced highest ROC-AUC scores: 0.6495 on ISIC 2016 and 0.8483 on HAM10000
- FFA-only and BP-only approaches showed higher error rates and lower ROC-AUC scores compared to the hybrid method
- Results suggest FFA can serve as an effective feature extractor when combined with backpropagation for classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FFA can serve as an effective feature extractor when combined with backpropagation
- Mechanism: FFA trains networks by using two forward passes—one with positive (real) data and one with negative (generated) data—forcing the network to learn noise-robust internal representations. These learned representations can then be used as input features for a subsequent BP-based classifier.
- Core assumption: The representations learned by FFA are sufficiently discriminative for the downstream classification task
- Evidence anchors: [abstract] "our experimental results suggest that the combination of FFA and BP can be a better alternative to achieve a more accurate prediction"; [section 3.2] "Each layer possesses its individual objective function, which essentially aims to maximize the goodness for positive data while minimizing the goodness for negative data"
- Break condition: If FFA-generated features do not improve BP classifier accuracy compared to BP-only training

### Mechanism 2
- Claim: FFA reduces computational cost and hardware constraints compared to BP
- Mechanism: FFA eliminates the backward pass required in BP, thus removing gradient computation and reducing memory and power requirements, making it suitable for low-power analog hardware
- Core assumption: The absence of gradient computation does not severely degrade the quality of learned features
- Evidence anchors: [abstract] "FFA is claimed to use very low-power analog hardware"; [section 3.2] "the FFA has been claimed to be superior in hardware efficiency, through low power consumption when compared with the backpropagation and the gradient computation"
- Break condition: If the accuracy drop from FFA-only training outweighs the computational savings

### Mechanism 3
- Claim: Combining FFA with BP leverages strengths of both methods
- Mechanism: FFA provides robust feature learning without gradient computation, while BP refines classification performance using standard optimization. This hybrid approach aims to balance efficiency and accuracy
- Core assumption: Early-stage feature learning with FFA does not corrupt useful representations before BP fine-tuning
- Evidence anchors: [section 3.3] "Our approach means to provide a baseline on the performance of DL architecture using the FFA strategy in contrast with backpropagation, in addition to exploring the combination of the two techniques"; [section 5.2.3] "reported results suggest a better performance can be achieved when both techniques are combined"
- Break condition: If the FFA+BP combination fails to outperform BP-only in accuracy or efficiency

## Foundational Learning

- Concept: Forward-Forward Algorithm (FFA)
  - Why needed here: FFA is the core alternative to backpropagation proposed in this study; understanding its mechanics is essential for reproducing or extending the experiments
  - Quick check question: How does FFA generate "negative" data during training?

- Concept: Backpropagation (BP)
  - Why needed here: BP is the standard training method used for comparison and as the second stage in the hybrid approach
  - Quick check question: What are the two main stages of BP, and how do they differ from FFA's two forward passes?

- Concept: Skin lesion classification datasets (ISIC 2016, HAM10000)
  - Why needed here: These are the benchmark datasets used; knowing their structure and class distributions is critical for proper data handling and evaluation
  - Quick check question: How many classes are in the HAM10000 dataset, and what is the train-test split used in the paper?

## Architecture Onboarding

- Component map:
  Input preprocessing (64×64 RGB images → flatten → normalize) → FFA stage (3 fully connected layers with ReLU + batch norm) → BP stage (same architecture, softmax output) → Evaluation (error rate and ROC-AUC)

- Critical path:
  1. Load and preprocess images
  2. Generate positive/negative data for FFA
  3. Train FFA stage for 250 epochs
  4. Extract FFA features
  5. Train BP classifier on extracted features
  6. Evaluate with error rate and ROC-AUC

- Design tradeoffs:
  - Using only fully connected layers limits model capacity but simplifies comparison
  - Downsampling to 64×64 reduces computational load but may lose fine detail important for lesion classification
  - Overlay labeling in FFA is a simple but potentially noisy encoding scheme

- Failure signatures:
  - FFA training loss plateaus early or diverges
  - BP classifier performs worse on FFA-extracted features than on raw images
  - ROC-AUC scores remain near 0.5 (random chance)

- First 3 experiments:
  1. Train FFA-only model on ISIC 2016; record error rate and ROC-AUC
  2. Train BP-only model on same dataset with identical architecture; compare performance
  3. Train FFA+BP pipeline; compare final metrics to both single-method results

## Open Questions the Paper Calls Out

- Question: How does the FFA's performance scale with more complex neural network architectures (e.g., convolutional layers)?
  - Basis in paper: [inferred] The paper mentions that the architecture was limited to fully connected layers due to computational constraints, suggesting this as a direction for future work
  - Why unresolved: The current study only tested FFA with fully connected layers, not convolutional layers which are more commonly used in image classification
  - What evidence would resolve it: Comparative experiments using convolutional neural networks with FFA, BP, and FFA+BP on the same datasets

- Question: What is the energy efficiency of FFA in terms of hardware implementation compared to BP?
  - Basis in paper: [explicit] The authors state that energy efficiency was not included in the study and will be addressed in future work
  - Why unresolved: The paper focuses on classification accuracy but does not provide data on computational resource usage or energy consumption
  - What evidence would resolve it: Measurements of power consumption, training time, and memory usage for FFA vs BP implementations on comparable hardware

- Question: Can FFA achieve better results than BP on larger, more diverse datasets?
  - Basis in paper: [explicit] The authors note that FFA "has not yet been tested on large-scale problems"
  - Why unresolved: The experiments were conducted on two relatively small skin lesion datasets (900-10,000 images)
  - What evidence would resolve it: Experiments on larger datasets like ImageNet or medical imaging datasets with millions of samples

## Limitations

- FFA implementation details for generating negative samples are underspecified, making exact reproduction difficult
- Limited comparison to state-of-the-art skin lesion classification methods (which typically achieve error rates below 20%)
- Small test set sizes (379 images for ISIC 2016) may lead to high variance in performance estimates

## Confidence

- High confidence in the methodology for comparing FFA, BP, and FFA+BP approaches
- Medium confidence in the claim that FFA+BP outperforms both individual methods, given the modest performance improvements
- Low confidence in the computational efficiency claims, as hardware metrics were not reported

## Next Checks

1. Implement and verify the FFA negative sample generation process using different noise types and magnitudes
2. Compare FFA+BP performance against modern CNN-based skin lesion classifiers on the same datasets
3. Conduct statistical significance testing on the error rate differences between methods across multiple random seeds