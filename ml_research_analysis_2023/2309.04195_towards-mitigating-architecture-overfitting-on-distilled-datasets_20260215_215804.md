---
ver: rpa2
title: Towards Mitigating Architecture Overfitting on Distilled Datasets
arxiv_id: '2309.04195'
source_url: https://arxiv.org/abs/2309.04195
tags:
- training
- distillation
- performance
- dataset
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the architecture overfitting problem in dataset
  distillation, where distilled datasets synthesized by one network architecture perform
  poorly when used to train other architectures. The authors propose several methods
  to mitigate this issue: a DropPath variant with three-phase keep rate scheduling
  and improved shortcut connections to enable implicit model ensemble; knowledge distillation
  using the training network as teacher for the test network; and improved optimization
  and data augmentation strategies.'
---

# Towards Mitigating Architecture Overfitting on Distilled Datasets

## Quick Facts
- arXiv ID: 2309.04195
- Source URL: https://arxiv.org/abs/2309.04195
- Authors: 
- Reference count: 40
- One-line primary result: DropPath with three-phase scheduling and knowledge distillation significantly reduces architecture overfitting, enabling larger networks to outperform shallow training networks on distilled datasets

## Executive Summary
This paper addresses architecture overfitting in dataset distillation, where models trained on distilled datasets perform poorly when tested on architectures different from the one used for synthesis. The authors propose a comprehensive approach combining DropPath with three-phase keep rate scheduling, knowledge distillation from the training network to the test network, periodical learning rates, Lion optimizer, and stronger data augmentation. Their experiments demonstrate significant improvements across multiple architectures, dataset distillation methods, and IPC values, with ResNet50 achieving 73.2% accuracy on CIFAR10 versus 44.3% for the 3-layer CNN training network.

## Method Summary
The authors propose four key methods to mitigate architecture overfitting: (1) a DropPath variant with three-phase keep rate scheduling (warmup → exploration → convergence) and improved shortcut connections, (2) knowledge distillation using the training network as teacher for the test network, (3) periodical learning rates with cosine annealing and warmup, and (4) stronger data augmentation (2-fold for IPC 10/50, 4-fold for IPC 1) combined with the Lion optimizer. These methods are evaluated on CIFAR10, CIFAR100, and Tiny-ImageNet datasets using distilled data generated by FRePo and MTT methods with IPC values of 1, 10, and 50.

## Key Results
- DropPath with three-phase scheduling significantly reduces architecture overfitting across all test architectures
- Knowledge distillation from the training network improves performance of larger test networks
- The full method enables ResNet50 to achieve 73.2% accuracy on CIFAR10 versus 44.3% for the 3-layer CNN training network
- Performance improvements are consistent across FRePo and MTT distillation methods with IPC values of 1, 10, and 50

## Why This Works (Mechanism)

### Mechanism 1: DropPath as Implicit Ensemble of Subnetworks
DropPath mitigates architecture overfitting by training an implicit ensemble of subnetworks, each using a subset of layers. Randomly pruning entire branches during training forces the network to learn robust representations across different layer subsets, addressing the generalization gap between shallow training networks and deep test networks.

### Mechanism 2: Knowledge Distillation with Reversed Roles
Using a smaller training network as teacher and larger test network as student improves generalization by regularizing the larger network. The student model predictions are forced to match teacher model predictions via KL divergence, encouraging similar decision boundaries despite architectural differences.

### Mechanism 3: Three-Phase Keep Rate Scheduling
Gradual adjustment of DropPath keep rate optimizes the exploration-exploitation tradeoff. Starting with p=1 for warmup, gradually decreasing to minimum value for exploration, then increasing to final high value for convergence balances optimization challenges from large architecture variance in early training with controlled variance for exploring better local minima.

## Foundational Learning

- Concept: Dropout and DropPath regularization
  - Why needed here: Understanding how stochastic layer pruning creates model ensembles and improves generalization
  - Quick check question: What's the key difference between Dropout (masking feature maps) and DropPath (pruning entire branches)?

- Concept: Knowledge distillation principles
  - Why needed here: Understanding how matching teacher and student predictions regularizes the student model
  - Quick check question: In standard knowledge distillation, why is the teacher typically larger than the student, and how does this paper reverse that relationship?

- Concept: Learning rate scheduling and optimization
  - Why needed here: Understanding how periodical learning rate adjustment and optimizer choice (Lion vs AdamW) affect training dynamics
  - Quick check question: How does cosine annealing with warmup differ from standard step decay, and why is it combined with DropPath's three-phase scheduling?

## Architecture Onboarding

- Component map: Distilled dataset → Test network training with DropPath → Knowledge distillation from training network → Periodical learning rate adjustment → Final evaluation
- Critical path: Distilled dataset → Test network training with DropPath → Knowledge distillation from training network → Periodical learning rate adjustment → Final evaluation
- Design tradeoffs: DropPath vs dropout (branch-level vs feature-level pruning), knowledge distillation with reversed roles (small teacher, large student), stronger augmentation vs computational cost
- Failure signatures: Training divergence (keep rate too low early), poor convergence (keep rate too high late), negligible improvement (teacher too weak or architectural mismatch too large)
- First 3 experiments:
  1. Baseline test: Train ResNet18 on FRePo distilled CIFAR10 with IPC=10 using standard training (no DropPath, no KD)
  2. DropPath only: Same setup but add three-phase DropPath with p_min=0.5
  3. Full method: Add knowledge distillation from 3-layer CNN, periodical learning rate, Lion optimizer, and 2-fold augmentation

## Open Questions the Paper Calls Out

### Open Question 1
How do the proposed methods perform on larger-scale datasets beyond Tiny-ImageNet, such as ImageNet-1K? The paper only evaluates on CIFAR10, CIFAR100, and Tiny-ImageNet. Performance on larger-scale datasets with more classes and higher resolution images remains unknown.

### Open Question 2
What is the impact of different teacher model architectures on knowledge distillation performance in dataset distillation? The paper only experiments with a 3-layer CNN as teacher. Performance with other teacher architectures (e.g., ResNet18, ResNet50) is not explored.

### Open Question 3
How do the proposed methods affect the robustness of models trained on distilled datasets to adversarial attacks? While the paper focuses on architecture overfitting and generalization, it does not evaluate model robustness to adversarial attacks.

### Open Question 4
What is the relationship between the minimum keep rate in DropPath and the final test accuracy across different network depths? The paper only tests one minimum keep rate value. The optimal minimum keep rate may vary with network depth.

### Open Question 5
How does the performance of the proposed methods change when using different dataset distillation algorithms beyond FRePo and MTT? The paper only tests two dataset distillation algorithms. Performance with other algorithms is unknown.

## Limitations

- Limited evaluation to small-scale datasets (CIFAR10, CIFAR100, Tiny-ImageNet) without testing on larger datasets like ImageNet-1K
- No ablation studies to isolate contributions of individual components (DropPath, knowledge distillation, learning rate scheduling)
- Lack of evaluation on model robustness to adversarial attacks

## Confidence

- DropPath effectiveness: Medium
- Knowledge distillation with reversed roles: Low
- Three-phase keep rate scheduling: Medium
- Overall performance claims: Medium

## Next Checks

1. **Ablation study on DropPath phases**: Conduct experiments to isolate the contribution of each phase in the three-phase keep rate scheduling by testing different combinations of warmup, exploration, and convergence phases.

2. **Knowledge distillation sensitivity analysis**: Evaluate the impact of varying the teacher model's strength and the knowledge distillation hyperparameters (temperature factor, weight factor) on the final performance.

3. **Generalization to other datasets and architectures**: Test the proposed methods on additional datasets and architectures not covered in the paper to assess their robustness and scalability.