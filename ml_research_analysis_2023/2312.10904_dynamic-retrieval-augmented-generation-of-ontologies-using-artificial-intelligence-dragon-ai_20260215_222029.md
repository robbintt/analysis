---
ver: rpa2
title: Dynamic Retrieval Augmented Generation of Ontologies using Artificial Intelligence
  (DRAGON-AI)
arxiv_id: '2312.10904'
source_url: https://arxiv.org/abs/2312.10904
tags:
- https
- dragon-ai
- page
- arxiv
- mungall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DRAGON-AI is an LLM-based system for assisting ontology term completion.
  It uses Retrieval Augmented Generation (RAG) to find similar terms and context,
  then prompts a large language model to generate missing term components like relationships
  and definitions.
---

# Dynamic Retrieval Augmented Generation of Ontologies using Artificial Intelligence (DRAGON-AI)

## Quick Facts
- arXiv ID: 2312.10904
- Source URL: https://arxiv.org/abs/2312.10904
- Reference count: 13
- Primary result: RAG-based LLM system achieves 0.89 precision for ontology relationship generation, with expert evaluation showing AI definitions close to human-authored ones

## Executive Summary
DRAGON-AI is a system that leverages Retrieval Augmented Generation (RAG) to assist in ontology construction by generating missing components like relationships and definitions. It uses semantic similarity search to retrieve relevant ontology terms and GitHub issue context, which are then injected into LLM prompts to guide term completion. Evaluation on 10 diverse ontologies demonstrates high precision for relationship generation and definition quality approaching human standards, though expert oversight remains important for catching AI-generated flaws.

## Method Summary
DRAGON-AI employs a RAG approach where ontology terms and GitHub issues are embedded into a vector database. When given a partial ontology term, the system retrieves semantically similar terms and relevant issue discussions, constructs a structured JSON prompt, and passes it to an LLM to generate missing components. The LLM output is parsed and validated against ontology schemas, with relationship filtering applied to remove invalid entries. The method was evaluated on 10 ontologies using both automated metrics and expert human evaluation.

## Key Results
- Relationship generation achieved precision of 0.89 on evaluated ontologies
- AI-generated definitions scored close to but lower than human-authored ones
- Expert evaluators with higher domain confidence were better at detecting flaws in AI-generated definitions
- Including GitHub issue context improved definition quality across all tested models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DRAGON-AI uses retrieval-augmented generation to find similar ontology terms and inject them as in-context examples for the LLM.
- Mechanism: By embedding ontology terms into vector space, the system retrieves the most semantically similar terms and passes them to the LLM alongside the partial input. This guides the LLM to produce terms consistent with existing ontology style and structure.
- Core assumption: Semantic similarity in vector space corresponds to functional similarity in ontology structure and terminology.
- Evidence anchors: [abstract] "The prompt is generated from a template, incorporating the most similar items in the vector database"; [section] "We use a Retrieval Augmented Generation (RAG) as the general approach to retrieve the most relevant information"
- Break condition: If the embedding model poorly captures semantic nuance or the ontology has highly variable term structures, retrieved examples may mislead rather than guide.

### Mechanism 2
- Claim: Translating identifiers from non-semantic numeric IDs to camel-case labels improves LLM performance.
- Mechanism: The LLM is trained primarily on natural language text; providing identifiers in label-like form makes them more recognizable and reduces hallucination.
- Core assumption: LLMs perform better on natural language identifiers than arbitrary numeric codes.
- Evidence anchors: [section] "These can confound LLMs, which have a tendency to hallucinate identifiers. In our initial experiments, we found LLMs tend to perform best if presented with information in the same way that information is presented to humans"
- Break condition: If the LLM is fine-tuned on ontology-specific data containing numeric IDs, the benefit may vanish or reverse.

### Mechanism 3
- Claim: Including GitHub issue context via RAG improves definition quality.
- Mechanism: GitHub issues often contain natural language requests or discussions about new terms. Retrieving and injecting this context helps the LLM generate definitions that better match user intent.
- Core assumption: GitHub issue text contains useful semantic hints that align with desired ontology content.
- Evidence anchors: [section] "We investigated whether providing background knowledge from GitHub issue trackers would improve the quality of generated definitions"; [section] "Including the GitHub issues improved performance of all models"
- Break condition: If GitHub issues are sparse, noisy, or irrelevant, including them may degrade performance or introduce noise.

## Foundational Learning

- Concept: Vector embeddings and semantic search
  - Why needed here: DRAGON-AI relies on retrieving semantically similar terms via vector embeddings to guide LLM generation.
  - Quick check question: What is the role of the Hierarchical Navigable Small World (HNSW) algorithm in the retrieval process?

- Concept: JSON-based structured prompt construction
  - Why needed here: The system serializes ontology terms into JSON and uses this format to create structured prompts and parse LLM outputs.
  - Quick check question: How does the system ensure the LLM output conforms to the expected JSON schema?

- Concept: OWL reasoning as a baseline for relationship prediction
  - Why needed here: The paper compares AI-generated relationships against OWL reasoning to benchmark precision.
  - Quick check question: Why is OWL reasoning always perfectly precise for SubClassOf relationships?

## Architecture Onboarding

- Component map: Vector DB indexer -> RAG retriever -> Prompt generator -> LLM interface -> Parser/validator -> Evaluator
- Critical path:
  1. Input partial ontology term
  2. Vector DB retrieval of top-k similar terms
  3. Prompt generation with retrieved examples
  4. LLM inference and JSON response
  5. Parsing and post-processing
  6. Output completed term
- Design tradeoffs:
  - Embedding model choice: OpenAI text-embedding-ada-002 offers strong semantic capture but incurs API cost and latency
  - k in RAG: Larger k increases diversity but risks noise and prompt length limits
  - LLM model: GPT-4 yields highest precision but is expensive; smaller models like gpt-3.5-turbo or open models trade quality for speed/cost
  - GitHub context inclusion: Improves definition quality but adds retrieval complexity and potential noise
- Failure signatures:
  - Low precision in relationship generation: Likely due to poor similarity retrieval or over-generalization by LLM
  - High hallucination rate: Possibly from insufficient context or numeric ID formatting issues
  - Slow response times: Could be caused by large prompt size or remote LLM latency
  - Invalid JSON parsing: Often due to malformed LLM output or schema mismatch
- First 3 experiments:
  1. Test embedding retrieval with a known query term and inspect top-5 results for semantic relevance
  2. Generate a term with only label provided; verify JSON output structure and relationship filtering
  3. Include a GitHub issue as context; compare definition quality with and without it using a simple accuracy check

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DRAGON-AI performance compare when generating ontologies from scratch versus completing existing ontologies?
- Basis in paper: [inferred] The paper notes that DRAGON-AI relies heavily on existing terms via RAG when generating new terms, and states "We consider it a strength of the DRAGON-AI method that it is able to leverage this prior work via RAG when generating new terms; we consider this task relevant to the day-to-day efforts of biological ontology developers. However, it also means we did not address the question of how well the approach would perform on constructing an ontology from scratch, or from an early state."
- Why unresolved: The paper only evaluated DRAGON-AI on completing existing ontologies, not creating new ones from scratch.
- What evidence would resolve it: Testing DRAGON-AI on creating a new ontology from scratch and comparing results to manually created ontologies or existing automated ontology learning methods.

### Open Question 2
- Question: How does the performance of DRAGON-AI vary across different ontology domains (e.g. biomedical vs environmental vs food science)?
- Basis in paper: [explicit] The paper tested DRAGON-AI on 10 diverse ontologies from different domains, but did not analyze performance differences between domains.
- Why unresolved: The paper provides aggregate results across all ontologies but does not break down performance by domain.
- What evidence would resolve it: Analyzing DRAGON-AI performance metrics separately for ontologies from different domains to identify which domains it performs better or worse in.

### Open Question 3
- Question: How does the quality of AI-generated definitions change as the ontology becomes more complex or specialized?
- Basis in paper: [inferred] The paper found that expert evaluators with higher domain confidence were better at detecting flaws in AI-generated definitions, suggesting that AI performance may vary with ontology complexity.
- Why unresolved: The paper does not explore how definition quality changes with ontology complexity or specialization.
- What evidence would resolve it: Evaluating AI-generated definitions on ontologies of varying complexity and specialization, and correlating quality scores with ontology characteristics.

## Limitations
- Performance may degrade on ontologies with different structural conventions or highly specialized vocabularies
- System relies on continuous updates to vector embeddings as ontologies evolve; drift handling is unclear
- Identifier formatting benefit (numeric to camel-case) is based on internal experiments not shared publicly

## Confidence

- High confidence: DRAGON-AI can generate ontology relationships with precision of 0.89 using RAG-based prompting; this is directly supported by experimental results
- Medium confidence: Including GitHub issue context improves definition quality; while reported, this is based on a limited evaluation set and lacks statistical significance testing
- Low confidence: Identifier formatting (numeric to camel-case) consistently improves LLM output; this is based on internal experiments not shared publicly, and contradicts findings in some ontology engineering contexts

## Next Checks

1. Cross-ontology validation: Test DRAGON-AI on 5 additional ontologies with distinct domain vocabularies (e.g., legal, medical, industrial) to assess robustness
2. A/B testing for GitHub context: Run a controlled experiment comparing definition quality with and without GitHub context across multiple LLM models, measuring inter-annotator agreement
3. Identifier formatting ablation study: Systematically compare LLM performance with numeric IDs, camel-case labels, and mixed formats across multiple ontology datasets to isolate the effect of formatting