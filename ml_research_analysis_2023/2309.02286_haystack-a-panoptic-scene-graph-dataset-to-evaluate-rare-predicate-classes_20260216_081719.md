---
ver: rpa2
title: 'Haystack: A Panoptic Scene Graph Dataset to Evaluate Rare Predicate Classes'
arxiv_id: '2309.02286'
source_url: https://arxiv.org/abs/2309.02286
tags:
- predicate
- scene
- classes
- graph
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of evaluating rare predicate classes
  in scene graph generation, which current datasets struggle with due to long-tail
  distributions. The authors propose a model-assisted annotation pipeline to efficiently
  construct a new panoptic scene graph dataset called Haystack, focusing on rare predicates.
---

# Haystack: A Panoptic Scene Graph Dataset to Evaluate Rare Predicate Classes

## Quick Facts
- arXiv ID: 2309.02286
- Source URL: https://arxiv.org/abs/2309.02286
- Reference count: 19
- Key outcome: New dataset and metrics for evaluating rare predicate classes in scene graph generation

## Executive Summary
This paper addresses the problem of evaluating rare predicate classes in scene graph generation, which current datasets struggle with due to long-tail distributions. The authors propose a model-assisted annotation pipeline to efficiently construct a new panoptic scene graph dataset called Haystack, focusing on rare predicates. The pipeline uses a pretrained model to propose relation candidates, which are then annotated by humans, resulting in a dataset with explicit negative annotations. They also introduce new metrics (Predicate ROC-AUC, Predicate Dominance Overestimation, and Predicate Discrimination Disadvantage) to better evaluate model performance on rare predicates. Experiments show that existing models have a good understanding of rare predicates but struggle with ranking them against others. The Haystack dataset and code are publicly available for further research.

## Method Summary
The method involves constructing a new panoptic scene graph dataset called Haystack to evaluate rare predicate classes. The process uses a model-assisted annotation pipeline where a pretrained VCTree scene graph model proposes probable relation candidates to human annotators. The dataset contains explicit negative annotations where annotators label whether a given relation does not have a certain predicate class. The authors also introduce three new metrics: Predicate ROC-AUC (P-AUC), Predicate Dominance Overestimation (PDO), and Predicate Discrimination Disadvantage (PDD) to better evaluate model performance on rare predicates.

## Key Results
- Haystack dataset successfully captures rare predicate classes that are typically underrepresented in existing scene graph datasets
- New metrics (P-AUC, PDO, PDD) provide more nuanced evaluation of model performance on rare predicates compared to traditional metrics
- Existing models show good understanding of rare predicates but struggle with ranking them against other predicates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model-assisted annotation pipeline enables efficient discovery of rare predicate classes by focusing human effort on promising candidates.
- Mechanism: A pretrained scene graph model generates scores for all possible relations across a large image set; these scores are used to rank and prioritize relations for human annotation, ensuring annotators focus on rare predicate instances rather than common ones.
- Core assumption: The pretrained model's proposals are sufficiently correlated with rare predicates to guide annotators effectively.
- Evidence anchors:
  - [abstract]: "We propose a model-assisted annotation pipeline that efficiently finds rare predicate classes that are hidden in a large set of images like needles in a haystack."
  - [section 3.1]: "We use a scene graph generation model to propose probable relation candidates to the annotators... We train it on the original PSG dataset and then calculate all possible relations... Given these scores, we can rank the processed relation candidates."
  - [corpus]: Weak/no direct evidence; no citations mention model-assisted pipeline efficiency.
- Break condition: If the model's proposals are biased toward frequent predicates, annotators will spend time on common classes and miss rare ones.

### Mechanism 2
- Claim: Negative annotations improve model evaluation by providing explicit information about absent predicates, enabling fine-grained metrics.
- Mechanism: For each relation, annotators explicitly label whether a fixed predicate is correct, incorrect, or not applicable; these labels become negative annotations used in metric calculation.
- Core assumption: Explicit negative annotations are more reliable than implicit ones derived from missing labels in existing datasets.
- Evidence anchors:
  - [abstract]: "Contrary to prior scene graph datasets, Haystack contains explicit negative annotations, i.e. annotations that a given relation does not have a certain predicate class."
  - [section 3.3]: "A fundamental requirement of the proposed metrics is the availability of negative annotations... With the Haystack dataset, explicit negative annotations become available and our new metrics can be calculated without the risk of noisy ground truths due to implicit negative annotations."
  - [corpus]: Weak/no direct evidence; no citations discuss explicit negatives in scene graph datasets.
- Break condition: If annotators are inconsistent in labeling negatives, the dataset quality and metrics will degrade.

### Mechanism 3
- Claim: Per-predicate annotation structure (vs. per-image) increases coverage of rare predicates at the cost of annotation density per image.
- Mechanism: Annotators are shown one relation at a time with a fixed predicate, reducing cognitive load and increasing focus on rare predicates rather than common ones.
- Core assumption: Annotators are less likely to default to common predicates when only one is shown at a time.
- Evidence anchors:
  - [abstract]: "Contrary to most prior scene graph datasets, our dataset is not a subset of Visual Genome... Instead of focusing only on positive annotations, we include negative annotations as well."
  - [section 3.1]: "We actively prevent this phenomenon by fixing the predicate and showing potential relation candidates one after the other... The annotator can label the relation with one of three choices."
  - [corpus]: Weak/no direct evidence; no citations discuss per-predicate vs per-image annotation structures.
- Break condition: If annotators find the fixed-predicate interface confusing or slow, annotation throughput and dataset size will suffer.

## Foundational Learning

- Concept: Long-tail predicate distribution
  - Why needed here: Scene graph datasets are dominated by a few frequent predicates, making evaluation on rare ones unreliable.
  - Quick check question: In the PSG dataset, what fraction of annotations are covered by the top 3 predicates ("on", "beside", "over")?

- Concept: Recall@k vs. Mean Recall@k
  - Why needed here: Recall@k favors frequent predicates; Mean Recall@k balances per-predicate performance.
  - Quick check question: If a model gets all "on", "beside", and "over" relations correct but none else, what Recall@k score does it achieve?

- Concept: ROC-AUC in imbalanced classification
  - Why needed here: ROC-AUC is robust to label imbalance, making it suitable for rare predicate evaluation.
  - Quick check question: Why is ROC-AUC preferred over accuracy when positive and negative samples are highly unbalanced?

## Architecture Onboarding

- Component map: SA-1B images -> MaskDINO segmentation -> DINOv2 clustering -> VCTree proposals -> Web annotation UI -> JSON storage
- Critical path: 1. Cluster images -> 2. Segment objects -> 3. Generate proposals -> 4. Annotate relations -> 5. Store negatives -> 6. Evaluate
- Design tradeoffs:
  - Per-predicate vs per-image annotation: higher rare predicate coverage vs lower image-level completeness
  - Model-assisted vs random sampling: faster rare predicate discovery vs potential bias
  - Explicit negatives vs implicit: reliable metrics vs annotation overhead
- Failure signatures:
  - Low positive annotation rate -> model proposals misaligned with human judgment
  - High PDO scores -> model overestimates predicate relevance
  - Low P-AUC scores -> model cannot distinguish predicate applicability
- First 3 experiments:
  1. Measure positive annotation rate per predicate to validate model proposal quality
  2. Compute PDO/PDD/P-AUC for a baseline model to confirm metrics detect ranking issues
  3. Integrate Haystack into an existing SGG pipeline and verify mR@50 improves on rare predicates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Haystack dataset's explicit negative annotations impact the performance of scene graph generation models compared to implicit negative annotations?
- Basis in paper: [explicit] The paper introduces explicit negative annotations and states they are helpful for evaluating rare predicate classes and open up new possibilities for improving models.
- Why unresolved: The paper demonstrates the existence and potential benefits of explicit negative annotations but does not provide direct comparisons with implicit negative annotations on model performance.
- What evidence would resolve it: Experiments comparing model performance using explicit vs. implicit negative annotations on the same tasks would clarify the impact.

### Open Question 2
- Question: What is the optimal balance between focusing on rare predicates and maintaining dataset diversity in scene graph generation?
- Basis in paper: [inferred] The paper emphasizes the importance of rare predicates but also mentions the need for dataset diversity to improve model robustness.
- Why unresolved: The paper doesn't provide specific guidelines on how to balance the focus on rare predicates with the need for diverse data.
- What evidence would resolve it: Studies analyzing the trade-off between rare predicate coverage and dataset diversity, and their respective impacts on model performance, would help determine the optimal balance.

### Open Question 3
- Question: How can the proposed metrics (Predicate ROC-AUC, Predicate Dominance Overestimation, and Predicate Discrimination Disadvantage) be further refined to better evaluate scene graph generation models?
- Basis in paper: [explicit] The paper introduces new metrics and discusses their benefits, but also mentions they require reliable annotations and could be improved.
- Why unresolved: The paper presents initial versions of these metrics but doesn't explore their full potential or limitations.
- What evidence would resolve it: Further research on the effectiveness of these metrics in various scenarios and comparisons with other evaluation methods would help refine their use and interpretation.

## Limitations
- The model-assisted annotation pipeline relies heavily on the quality of the pretrained model's proposals, with no quantitative evaluation of proposal quality provided
- New metrics (PDO, PDD, P-AUC) lack extensive validation across multiple SGG models beyond the two baseline models tested
- Dataset construction requires significant computational resources for MaskDINO segmentation and DINOv2 clustering across 11M images

## Confidence
- **High Confidence**: The problem statement (rare predicate evaluation in SGG) and the dataset construction methodology are well-established and clearly described.
- **Medium Confidence**: The effectiveness of the model-assisted annotation pipeline in discovering rare predicates, as no quantitative evaluation of proposal quality is provided.
- **Medium Confidence**: The utility of explicit negative annotations and the new metrics, as validation is limited to two baseline models.

## Next Checks
1. Measure positive annotation rates per predicate across the Haystack dataset to quantify how effectively the model-assisted pipeline discovers rare predicates compared to random sampling.
2. Evaluate a diverse set of SGG models (beyond VCTree) on Haystack to validate whether PDO, PDD, and P-AUC consistently identify model weaknesses that mR@50 misses.
3. Conduct ablation studies removing explicit negative annotations to measure their impact on metric reliability and model evaluation quality.