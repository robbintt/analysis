---
ver: rpa2
title: 'E2E-AT: A Unified Framework for Tackling Uncertainty in Task-aware End-to-end
  Learning'
arxiv_id: '2312.10587'
source_url: https://arxiv.org/abs/2312.10587
tags:
- training
- adversarial
- learning
- input
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified framework for tackling uncertainties
  in task-aware end-to-end learning. The core method idea is to treat multiple sources
  of uncertainty, such as input feature space of ML models and constraints of COs,
  uniformly as a robust optimization problem.
---

# E2E-AT: A Unified Framework for Tackling Uncertainty in Task-aware End-to-end Learning

## Quick Facts
- arXiv ID: 2312.10587
- Source URL: https://arxiv.org/abs/2312.10587
- Reference count: 40
- One-line primary result: E2E-AT unifies handling of input and CO parameter uncertainties via robust optimization, improving robustness in task-aware E2E learning for power system operations.

## Executive Summary
This paper introduces E2E-AT, a unified framework that addresses uncertainties in both input features and CO parameters within task-aware end-to-end learning. By treating these uncertainties uniformly as a robust optimization problem, E2E-AT employs end-to-end adversarial training to improve robustness against worst-case scenarios. The framework is validated on a real-world power system operation problem, demonstrating significant cost reductions and enhanced robustness compared to natural E2E learning approaches.

## Method Summary
The framework formulates end-to-end learning with uncertainties as a min-max optimization problem, where the inner maximization accounts for perturbations in both input features and CO parameters. This is solved via end-to-end adversarial training, with perturbations bounded by uncertainty sets. For COs expressible as affine-parametric QPs, certified robustness is achieved through exact MILP reformulation of the KKT conditions. The method is applied to a load forecasting and sequential scheduling problem on an IEEE bus-14 power system.

## Key Results
- E2E-AT effectively reduces task-aware cost under input and CO parameter uncertainties compared to natural E2E learning.
- Adversarial training on CO parameters (AT-PARA) improves robustness against parameter perturbations in the NCED formulation.
- Integrated uncertainty handling (AT-BOTH) provides better overall robustness than addressing input or parameter uncertainty separately.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: E2E-AT treats uncertainties in both input space and CO parameters uniformly by embedding them into a single robust optimization framework.
- Mechanism: The unified framework reformulates the E2E learning problem as a min-max optimization: minimize the task-aware loss over the model parameters while maximizing the loss over both input perturbations and CO parameter perturbations.
- Core assumption: Both input uncertainty and CO parameter uncertainty can be modeled as bounded perturbations within uncertainty sets (e.g., ℓ∞-bounded for inputs, proportional bounds for CO parameters).
- Evidence anchors:
  - [abstract]: "We propose a unified framework that covers the uncertainties emerging in both the input feature space of the ML models and the COs. The framework is described as a robust optimization problem and is practically solved via end-to-end adversarial training (E2E-AT)."
  - [section]: "Consider the uncertainty of the input x + δx ∈ X and the unpredictable parameter ϕ + δϕ ∈ Φ. Denote ψ = (x, ϕ) ∈ Ψ := X ×Φ. The worst scenario, which maximizes the task-aware objective, can be formulated as..."
- Break condition: If the COs cannot be represented as parametric functions of (θ, x, y, ϕ) or if the uncertainty sets are non-convex and intractable for robust optimization.

### Mechanism 2
- Claim: E2E-AT improves robustness against CO parameter uncertainty by adversarial training on the CO parameter space.
- Mechanism: During training, E2E-AT samples perturbations δϕ within the uncertainty set of CO parameters (e.g., susceptance b in power grid dispatch) and updates the NN forecaster to minimize the worst-case task-aware cost.
- Core assumption: The unpredictable CO parameters have a bounded uncertainty set that can be sampled or optimized over during training.
- Evidence anchors:
  - [abstract]: "We extend this idea to E2E learning and prove that there is a robustness certification procedure by solving augmented integer programming."
  - [section]: "When treating the E2E framework as an integrated model, the data source includes both conventionally defined data samples (x, y) ∈ D and the unpredictable parameter ϕ of COs."
- Break condition: If the CO parameter uncertainty set is unknown or too large to sample effectively during training.

### Mechanism 3
- Claim: Certified robustness for affine-parametric QPs can be achieved by exact MILP reformulation.
- Mechanism: The KKT conditions of affine-parametric QPs are linear in the decision variables and dual variables, enabling mixed-integer linear reformulation. This allows exact worst-case cost computation via MILP solvers.
- Core assumption: COs are restricted to affine-parametric QPs (linear in uncertain parameters) so that KKT conditions remain linear.
- Evidence anchors:
  - [section]: "Proposition 2 The affine-parametric QP: ... can be equivalently written as the set of mixed integer linear constraints..."
  - [section]: "Due to the complexity of integer programming, we restrict the original settings in (Amos and Kolter 2017) by assuming linearity in uncertain terms for certified robustness."
- Break condition: If COs involve nonlinear uncertain parameters or non-convex constraints that cannot be linearized.

## Foundational Learning

- Concept: Robust optimization (min-max formulation)
  - Why needed here: Provides the theoretical foundation to model and optimize against worst-case uncertainties in both input and CO parameters.
  - Quick check question: Can you write the robust optimization problem for a supervised learning task with ℓ∞-bounded input perturbations?

- Concept: Implicit function theorem and OptNet
  - Why needed here: Enables gradient flow through the solution of parametric QPs, allowing end-to-end training of the ML model with COs as differentiable layers.
  - Quick check question: Given a QP with KKT conditions g(z; θ) = 0, what is the formula for ∂z/∂θ via the implicit function theorem?

- Concept: Adversarial training (PGD, Danskin's theorem)
  - Why needed here: Practical algorithm to solve the inner maximization of the robust optimization problem during E2E-AT.
  - Quick check question: In adversarial training, why is Danskin's theorem used when the inner maximization is solved approximately (e.g., via PGD)?

## Architecture Onboarding

- Component map:
  - Data loader → NN forecaster → CO solver (dispatch/redispatch) → task-aware loss
  - E2E-AT adds: perturbation generator (input + CO params) → robust loss computation
  - Certified robustness adds: MILP solver for exact worst-case cost

- Critical path:
  1. Forward pass: NN → CO solution → task loss
  2. Backward pass: Gradients through CO KKT conditions → NN parameter update
  3. E2E-AT: During training, generate perturbations → compute worst-case loss → backprop

- Design tradeoffs:
  - Tradeoff between clean and robust accuracy controlled by α in (20)
  - Certified robustness requires affine-parametric QPs, limiting CO expressiveness
  - Gradient reuse (free AT) reduces training time but may affect convergence

- Failure signatures:
  - High clean cost but low robust cost → overfitting to adversarial examples
  - High robust cost but low clean cost → underfitting or poor optimization
  - MILP solver timeout → overly large CO or NN linearization

- First 3 experiments:
  1. Train NAT (natural E2E) vs AT-INPUT (adversarial on inputs) on synthetic load forecasting; compare clean vs adversarial cost.
  2. Train AT-PARA (adversarial on CO parameters) vs NAT; test on random CO parameter perturbations.
  3. Train AT-BOTH (integrated uncertainties) vs AT-INPUT/AT-PARA; evaluate robustness on combined input+CO attacks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different sources of uncertainty in end-to-end learning interact, and can their impacts be modeled by gradients of the neural network?
- Basis in paper: [explicit] The paper discusses the relationship between input space uncertainty and CO uncertainty, suggesting that their impacts can be modeled by the gradients of the neural network.
- Why unresolved: While the paper provides insights into the interaction between these uncertainties, it does not offer a comprehensive model or theory explaining their relationship and impact.
- What evidence would resolve it: A theoretical framework or empirical study demonstrating how gradients of the neural network can effectively model the impacts of different uncertainty sources in end-to-end learning.

### Open Question 2
- Question: How can multi-uncertainties be effectively handled in end-to-end learning, and can techniques from multi-task learning be applied?
- Basis in paper: [explicit] The paper suggests that handling multi-uncertainties in end-to-end learning is an interesting future work and mentions that techniques from multi-task learning could be borrowed to handle multi-uncertainties.
- Why unresolved: The paper does not provide a detailed approach or methodology for handling multi-uncertainties in end-to-end learning.
- What evidence would resolve it: A proposed method or algorithm that effectively handles multi-uncertainties in end-to-end learning, possibly incorporating techniques from multi-task learning.

### Open Question 3
- Question: How can certified and tractable end-to-end adversarial training be developed for security purposes?
- Basis in paper: [explicit] The paper mentions that solving the exact attack vector is an MILP problem which cannot be used for adversarial training due to its high complexity, and developing certified and tractable E2E-AT is important for security purposes.
- Why unresolved: The paper does not provide a solution or approach to develop certified and tractable end-to-end adversarial training.
- What evidence would resolve it: A proposed method or algorithm that can efficiently solve the MILP problem for exact attack vector computation, enabling certified and tractable end-to-end adversarial training.

## Limitations

- The framework assumes bounded uncertainty sets for both input features and CO parameters, which may not hold in practice.
- Certified robustness via MILP reformulation is restricted to affine-parametric QPs, limiting applicability to more complex CO formulations.
- The effectiveness of E2E-AT depends heavily on proper tuning of adversarial attack budgets and uncertainty set sizes, which are problem-specific and not universally optimal.

## Confidence

- **High confidence**: The core theoretical framework (robust optimization formulation, OptNet integration) is mathematically sound and well-supported by the literature.
- **Medium confidence**: The practical implementation of E2E-AT on the power system case study shows improvements, but results are limited to a single problem instance.
- **Low confidence**: The scalability claims to larger systems and different CO formulations are not empirically validated beyond the IEEE bus-14 system.

## Next Checks

1. Test E2E-AT on a non-linear CO (e.g., AC optimal power flow) to evaluate the limitations of the affine-parametric assumption.
2. Conduct ablation studies varying uncertainty set sizes to quantify the tradeoff between robustness and clean performance.
3. Benchmark E2E-AT against alternative uncertainty handling methods (e.g., scenario-based optimization, chance constraints) on the same power system problem.