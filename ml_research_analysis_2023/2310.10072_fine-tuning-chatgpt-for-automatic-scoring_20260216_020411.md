---
ver: rpa2
title: Fine-tuning ChatGPT for Automatic Scoring
arxiv_id: '2310.10072'
source_url: https://arxiv.org/abs/2310.10072
tags:
- scoring
- gpt-3
- automatic
- education
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the effectiveness of fine-tuned ChatGPT
  (GPT-3.5-turbo) for automatically scoring student written responses in science education,
  comparing it to fine-tuned BERT. Six assessment tasks comprising two multi-label
  and four multi-class items were used with datasets of middle and high school student
  responses.
---

# Fine-tuning ChatGPT for Automatic Scoring

## Quick Facts
- arXiv ID: 2310.10072
- Source URL: https://arxiv.org/abs/2310.10072
- Authors: 
- Reference count: 13
- Key outcome: Fine-tuned GPT-3.5-turbo significantly outperformed fine-tuned BERT in automatic scoring of student responses in science education, with an average accuracy increase of 9.1% across all tasks.

## Executive Summary
This study investigates the effectiveness of fine-tuned ChatGPT (GPT-3.5-turbo) for automatically scoring student written responses in science education, comparing it to fine-tuned BERT. Six assessment tasks comprising two multi-label and four multi-class items were used with datasets of middle and high school student responses. Results show that fine-tuned GPT-3.5-turbo significantly outperformed fine-tuned BERT across all tasks, with an average accuracy increase of 9.1% (mean = 9.15, SD = 0.042, p = 0.001). For multi-label tasks, GPT-3.5-turbo achieved higher accuracy across all labels, with a 7.1% increase on the second task. For multi-class tasks, GPT-3.5-turbo showed an average scoring increase of 10.6% compared to BERT.

## Method Summary
The study collected two multi-label assessment tasks (5 and 10 labels) from Next Generation Science Standards and four multi-class assessment tasks (4-5 classes) from Mathematical Thinking in Science, using middle school and high school student responses with expert scoring. GPT-3.5-turbo was fine-tuned on these datasets and compared to a fine-tuned BERT baseline. Performance was evaluated using accuracy metrics and paired-sample t-tests to determine statistical significance. The fine-tuning process included data cleaning, tokenization, and supervised training with regression-based or classification loss depending on the task type.

## Key Results
- Fine-tuned GPT-3.5-turbo achieved an average accuracy increase of 9.1% over fine-tuned BERT across all six assessment tasks.
- For multi-label tasks, GPT-3.5-turbo showed higher accuracy across all labels, with a 7.1% increase on the second task with unbalanced labels.
- For multi-class tasks, GPT-3.5-turbo demonstrated an average scoring increase of 10.6% compared to BERT.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-3.5-turbo outperforms BERT in domain-specific educational scoring due to its autoregressive generative architecture and few-shot learning capability.
- Mechanism: GPT-3.5-turbo's generative, autoregressive design allows it to produce sequence outputs conditioned on task-specific prompts, whereas BERT's bidirectional masked language modeling is optimized for classification but less flexible for open-ended scoring.
- Core assumption: The educational scoring task benefits more from generative sequence modeling than fixed-length embeddings.
- Evidence anchors:
  - [abstract]: "GPT-3.5 has been trained over enormous online language materials such as journals and Wikipedia; however, direct usage of pre-trained GPT-3.5 is insufficient for automatic scoring as students do not utilize the same language as journals or Wikipedia, and contextual information is required for accurate scoring."
  - [section 4.2]: "GPT-3.5-Turbo is an improved version of GPT-3 and GPT-3.5 to balance performance and efficiency. GPT-3.5-Turbo has capabilities comparable to GPT-3 but with fewer settings."
  - [corpus]: Found 5 related papers with FMR scores 0.0-0.58, suggesting moderate corpus relevance for domain-specific LLMs in education.
- Break condition: If the scoring task requires only fixed-length embeddings or lacks rich contextual input, BERT's bidirectional embeddings may be equally effective.

### Mechanism 2
- Claim: Fine-tuning GPT-3.5-turbo on domain-specific corpora significantly improves accuracy over generic pre-trained models.
- Mechanism: Domain-specific fine-tuning aligns the model's internal representations with the linguistic patterns and rubric criteria of science education, reducing mismatch between generic training data and student response language.
- Core assumption: Student responses in science education have distinct linguistic features not well-represented in general web corpora.
- Evidence anchors:
  - [abstract]: "These imply that a domain-specific model fine-tuned using data for specific tasks can enhance model performance."
  - [section 5.1.4]: "Compare the fine-tuned model's performance with the original GPT-3.5-turbo to quantify the benefits of domain-specific fine-tuning."
  - [corpus]: 2 related papers focus explicitly on domain-specific fine-tuning for science education, supporting this claim.
- Break condition: If fine-tuning data is too small or not representative of the target task, gains may not materialize.

### Mechanism 3
- Claim: GPT-3.5-turbo handles unbalanced multi-label scoring tasks better than BERT, improving minority label accuracy.
- Mechanism: GPT-3.5-turbo's generative, sequence-aware architecture allows better handling of sparse or unbalanced labels by leveraging contextual relationships across labels rather than relying solely on token-level classification.
- Core assumption: Unbalanced label distributions in multi-label tasks create challenges that sequence models can address via contextual coherence.
- Evidence anchors:
  - [section 6.1]: "For the second task, which has more unbalanced labels (Mean Difference = 0.065... t(9) = 4.67 and p = 0.001 < 0.05). Upon examining the multi-label dataset... GPT-3.5-turbo consistently achieved higher accuracy on certain unbalanced labels than BERT."
  - [section 4.3]: "Text data augmentation is one creative way to use GPT-3.5-turbo's capabilities... GPT-3.5 to supplement textual datasets... enhancing the model's comprehension and offering a wider variety of sample responses."
  - [corpus]: No direct corpus evidence; weak support for this specific claim.
- Break condition: If the label distribution is extremely skewed, even sequence-aware models may fail without additional balancing strategies.

## Foundational Learning

- Concept: Bidirectional vs. Autoregressive Language Modeling
  - Why needed here: Understanding why GPT-3.5-turbo (autoregressive) outperforms BERT (bidirectional) in scoring requires grasping how each architecture processes context and outputs predictions.
  - Quick check question: What is the main difference in how BERT and GPT-3.5 generate predictions from input text?

- Concept: Fine-tuning vs. Zero-shot/Few-shot Learning
  - Why needed here: The study contrasts fine-tuned GPT-3.5-turbo with BERT, so understanding how fine-tuning adapts pre-trained models to specific tasks is essential.
  - Quick check question: How does fine-tuning differ from few-shot learning in adapting a pre-trained model?

- Concept: Multi-label vs. Multi-class Classification
  - Why needed here: The experiments involve both task types, and results differ; knowing the distinction is key to interpreting performance differences.
  - Quick check question: In multi-label tasks, can a single instance be assigned more than one label?

## Architecture Onboarding

- Component map: Pre-trained GPT-3.5-turbo model -> Domain-specific fine-tuning pipeline -> Evaluation harness -> Comparison baseline (fine-tuned BERT) -> Output interface for scoring predictions

- Critical path:
  1. Load and preprocess student response dataset with expert rubrics.
  2. Fine-tune GPT-3.5-turbo on this data.
  3. Fine-tune BERT on the same data.
  4. Evaluate both models on held-out test sets.
  5. Perform statistical comparison of accuracies.

- Design tradeoffs:
  - GPT-3.5-turbo: higher accuracy, generative flexibility, but more expensive API calls and potentially slower inference.
  - BERT: lower accuracy in this domain, faster and cheaper for fixed-class classification, but less adaptable to unbalanced labels.

- Failure signatures:
  - GPT-3.5-turbo: Overfitting to training rubric language, poor generalization to unseen response styles, higher latency/ cost.
  - BERT: Inability to handle unbalanced labels, lower accuracy on multi-label tasks, struggles with open-ended responses.

- First 3 experiments:
  1. Fine-tune GPT-3.5-turbo on a small balanced subset of the dataset and evaluate accuracy.
  2. Compare GPT-3.5-turbo vs. BERT on a single multi-label task with balanced labels.
  3. Introduce an unbalanced label subset and measure per-label accuracy differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does fine-tuning GPT-3.5-turbo with different data augmentation strategies impact its automatic scoring accuracy compared to traditional fine-tuning approaches?
- Basis in paper: [inferred] The paper mentions text data augmentation as a creative way to use GPT-3.5-turbo's capabilities for domain-specific training, but does not explore different augmentation strategies.
- Why unresolved: The paper does not provide a comparative analysis of various data augmentation techniques and their effects on fine-tuning performance.
- What evidence would resolve it: Empirical results comparing GPT-3.5-turbo's performance using different data augmentation methods versus standard fine-tuning on the same datasets.

### Open Question 2
- Question: What is the long-term effectiveness of fine-tuned GPT-3.5-turbo in maintaining scoring accuracy as student response patterns evolve over time?
- Basis in paper: [inferred] The paper demonstrates current accuracy but does not address model performance over extended periods or with changing response patterns.
- Why unresolved: The study's timeframe and dataset do not account for temporal changes in student writing styles or assessment formats.
- What evidence would resolve it: Longitudinal studies tracking GPT-3.5-turbo's scoring accuracy across multiple academic years with updated datasets.

### Open Question 3
- Question: How does the performance of fine-tuned GPT-3.5-turbo vary across different educational levels and subject domains beyond science education?
- Basis in paper: [explicit] The paper focuses specifically on science education tasks but suggests potential applications in other educational contexts.
- Why unresolved: The study's scope is limited to science education, and results may not generalize to other subjects or educational levels.
- What evidence would resolve it: Comparative studies applying fine-tuned GPT-3.5-turbo to assessment tasks in mathematics, humanities, and different grade levels, with accuracy measurements across domains.

## Limitations
- The study does not report hyperparameter tuning details for either GPT-3.5-turbo or BERT fine-tuning, which may significantly affect performance comparisons.
- Limited dataset sizes (1,200-1,150 training samples) may constrain generalization, particularly for the multi-label tasks with unbalanced labels.
- No analysis of computational cost or inference latency differences between the two models is provided, despite GPT-3.5-turbo's higher operational costs.

## Confidence
- High confidence: GPT-3.5-turbo outperforms fine-tuned BERT in this specific science education scoring context, based on statistically significant accuracy improvements across all tasks.
- Medium confidence: The mechanisms explaining why GPT-3.5-turbo performs better (generative architecture, sequence modeling) are plausible but not empirically validated within this study.
- Medium confidence: The claim that domain-specific fine-tuning improves performance over generic models is supported by comparison with the original GPT-3.5-turbo baseline, but the magnitude of improvement could vary with different datasets.

## Next Checks
1. Replicate the experiment with larger, more diverse student response datasets to verify if accuracy improvements hold at scale and across different educational domains.
2. Conduct ablation studies comparing few-shot learning versus fine-tuning for both GPT-3.5-turbo and BERT to isolate the contribution of fine-tuning versus architectural advantages.
3. Measure and compare inference latency, computational cost, and environmental impact between the two approaches to provide a complete cost-benefit analysis for educational deployment.