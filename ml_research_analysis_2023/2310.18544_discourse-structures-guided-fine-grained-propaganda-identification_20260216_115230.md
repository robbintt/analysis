---
ver: rpa2
title: Discourse Structures Guided Fine-grained Propaganda Identification
arxiv_id: '2310.18544'
source_url: https://arxiv.org/abs/2310.18544
tags:
- discourse
- propaganda
- news
- sentence
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper focuses on identifying propaganda in political news
  at fine-grained levels: sentence-level and token-level. It observes that propaganda
  content is more likely to be embedded in sentences that attribute causality or assert
  contrast to nearby sentences, as well as seen in opinionated evaluation, speculation
  and discussions of future expectation.'
---

# Discourse Structures Guided Fine-grained Propaganda Identification

## Quick Facts
- arXiv ID: 2310.18544
- Source URL: https://arxiv.org/abs/2310.18544
- Reference count: 16
- Key outcome: Incorporates discourse structures (local relations and global roles) using knowledge distillation to improve propaganda identification at sentence and token levels

## Executive Summary
This paper addresses fine-grained propaganda identification in political news by incorporating discourse structures at both local (sentence-to-sentence) and global (sentence role) levels. The authors propose using teacher models to identify PDTB-style discourse relations between adjacent sentences and common discourse roles within news articles. Two methods - feature concatenation and knowledge distillation - are used to incorporate these discourse structures into propaganda detection. Experiments demonstrate that leveraging discourse structure guidance significantly improves both precision and recall of propaganda content identification.

## Method Summary
The approach constructs teacher models for discourse relations (using PDTB 2.0) and discourse roles (using NewsDiscourse dataset), then incorporates their predictions into propaganda detection through either feature concatenation or knowledge distillation. The method operates at sentence and token levels, using Longformer as the base encoder. Discourse structures are leveraged through two mechanisms: local relations (Comparison, Contingency) between adjacent sentences, and global discourse roles (Evaluation, Expectation, Historical Event, etc.) within the article. Knowledge distillation is implemented through both response-based (mimicking teacher probabilities) and feature relation-based (learning from teacher embeddings) approaches.

## Key Results
- Knowledge distillation models outperform baseline Longformer models by significant margins in both precision and recall
- Feature concatenation of teacher predictions improves performance but is less effective than knowledge distillation
- Response-based and feature relation-based distillation methods mutually complement each other, achieving enhanced guidance from discourse structures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Propaganda sentences are more likely to be embedded in sentences that attribute causality or assert contrast to nearby sentences.
- Mechanism: The model leverages local discourse relations (Comparison, Contingency) between adjacent sentences to identify propaganda. By training a teacher model to predict these relations, the system can flag sentences with causal or contrasting connections as potential propaganda carriers.
- Core assumption: Discourse relations between adjacent sentences provide meaningful signals for propaganda detection beyond the sentence content itself.
- Evidence anchors:
  - [abstract]: "propaganda content is more likely to be embedded in sentences that attribute causality or assert contrast to nearby sentences"
  - [section]: "sentences that exhibit contingency and comparison relations with adjacent sentences are more prone to containing propaganda"
  - [corpus]: Weak evidence. No direct correlation data provided in corpus analysis; relies on PDTB-style relation classification performance.
- Break condition: If propaganda uses different rhetorical strategies not captured by Comparison/Contingency relations, or if discourse relation classifier fails to capture these relations accurately.

### Mechanism 2
- Claim: Propaganda content is more likely to be embedded into opinionated evaluations, speculation about future expectations, or fabrication of historical background.
- Mechanism: The model uses global discourse roles (Evaluation, Expectation, Historical Event) to identify sentences that carry propaganda. A teacher model predicts these roles, and the student model learns to associate these roles with propaganda likelihood.
- Core assumption: Specific discourse roles correlate strongly with propaganda content, independent of the actual semantic content.
- Evidence anchors:
  - [abstract]: "propaganda content is more likely to be embedded in... opinionated evaluation, speculation and discussions of future expectation"
  - [section]: "propaganda is more likely to be embedded into sentences expressing opinions or evaluations (D3), speculating future expectations (D4), or fabricating historical background (D1)"
  - [corpus]: Weak evidence. Table 2 shows correlation but doesn't establish causation or predictive power.
- Break condition: If propaganda shifts to other discourse roles not captured by the teacher model, or if non-propaganda content frequently appears in these roles.

### Mechanism 3
- Claim: Knowledge distillation effectively transfers discourse structure knowledge from teacher models to the propaganda detection task.
- Mechanism: The model uses both response-based distillation (mimicking teacher prediction probabilities) and feature relation-based distillation (learning from teacher-generated sentence embeddings) to incorporate discourse structure guidance.
- Core assumption: The teacher models' predictions and embeddings contain valuable information that can improve propaganda detection performance.
- Evidence anchors:
  - [section]: "We further devise two methods to incorporate the two types of discourse structures for propaganda identification by either using teacher predicted probabilities as additional features or soliciting guidance in a knowledge distillation framework"
  - [section]: "The response-based and feature relation-based distillation mutually complement each other, acquiring an enhanced guidance from discourse structures"
  - [corpus]: Strong evidence. Table 5 shows knowledge distillation models outperform feature concatenation and baseline models.
- Break condition: If teacher model performance degrades, or if the distillation process fails to preserve useful information during transfer.

## Foundational Learning

- Concept: Discourse relations (PDTB-style)
  - Why needed here: Understanding how sentences relate to each other locally is crucial for identifying propaganda that leverages causal or contrasting arguments.
  - Quick check question: Can you identify whether a sentence pair has Comparison, Contingency, Temporal, or Expansion relation?

- Concept: Discourse roles in news articles
  - Why needed here: Recognizing the global function of sentences (Main Event, Evaluation, Expectation, etc.) helps detect propaganda that appears in opinionated or speculative contexts.
  - Quick check question: Given a news article, can you classify each sentence into one of the eight discourse role types?

- Concept: Knowledge distillation
  - Why needed here: Transferring knowledge from teacher models trained on discourse structure to the propaganda detection task improves performance by leveraging complementary information.
  - Quick check question: What's the difference between response-based and feature relation-based distillation, and when would you use each?

## Architecture Onboarding

- Component map: Input news article → Longformer encoder → Three learning heads (propaganda, discourse relation, discourse role) → Distillation losses → Output
- Critical path: Longformer embedding → Propaganda learning head → Cross-entropy loss
- Design tradeoffs: Knowledge distillation adds complexity but improves performance; simpler feature concatenation is faster but less effective
- Failure signatures: Performance degradation when teacher models perform poorly; knowledge doesn't transfer effectively; increased training instability
- First 3 experiments:
  1. Train baseline Longformer without discourse structures, establish performance floor
  2. Add feature concatenation of teacher predictions, measure improvement
  3. Implement full knowledge distillation with both loss types, compare against feature concatenation approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is the proposed method for identifying propaganda in other types of misinformation, such as fake news, conspiracy theories, and more?
- Basis in paper: [inferred] The paper mentions that the designed discourse structures method has demonstrated its usefulness in identifying propaganda, but its effectiveness for other types of misinformation remains unknown.
- Why unresolved: The paper focuses on the detection of propaganda and does not provide evidence or experiments for other types of misinformation.
- What evidence would resolve it: Conducting experiments to evaluate the performance of the proposed method on datasets containing different types of misinformation.

### Open Question 2
- Question: How does the proposed method compare to other state-of-the-art approaches for fine-grained propaganda identification, such as (Fadel et al., 2019) and (Vlad et al., 2019)?
- Basis in paper: [explicit] The paper mentions these approaches as baselines for comparison and reports their performance in the experimental results.
- Why unresolved: While the paper provides a comparison, it does not provide a detailed analysis of the strengths and weaknesses of the proposed method compared to these approaches.
- What evidence would resolve it: Conducting a comprehensive analysis of the proposed method and other state-of-the-art approaches, including their strengths, weaknesses, and limitations.

### Open Question 3
- Question: How does the proposed method handle long news articles that may contain multiple propaganda sentences and tokens?
- Basis in paper: [explicit] The paper mentions that the model takes the entire news article as input and predicts the label for each sentence or token.
- Why unresolved: The paper does not provide specific details on how the proposed method handles long news articles and whether it can effectively identify propaganda in such cases.
- What evidence would resolve it: Conducting experiments to evaluate the performance of the proposed method on long news articles and analyzing its ability to identify propaganda in different sections of the article.

## Limitations
- Weak evidence that causal and contrasting discourse relations strongly predict propaganda, based only on PDTB-style relation classification performance
- Assumes discourse patterns captured by PDTB relations and NewsDiscourse roles encompass all relevant propaganda strategies
- Knowledge distillation framework adds significant complexity without clear ablation studies identifying key contributing components

## Confidence

**High confidence:** The experimental results showing knowledge distillation outperforms baseline models (Table 5)

**Medium confidence:** The claim that specific discourse roles correlate with propaganda, based on Table 2 correlations

**Low confidence:** The assertion that causal and contrasting relations between adjacent sentences are strong predictors of propaganda, due to weak supporting evidence

## Next Checks

1. Conduct ablation studies to isolate which discourse structure components (local vs global, feature concatenation vs distillation) contribute most to performance gains
2. Test model robustness by introducing adversarial examples where propaganda appears in discourse roles or relations not captured by the teacher models
3. Evaluate whether the approach generalizes beyond political news to other propaganda domains like social media or advertising texts