---
ver: rpa2
title: Understanding the Robustness of Randomized Feature Defense Against Query-Based
  Adversarial Attacks
arxiv_id: '2310.00567'
source_url: https://arxiv.org/abs/2310.00567
tags:
- attacks
- adversarial
- input
- defense
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a randomized feature defense against black-box
  adversarial attacks by injecting noise into hidden features of a pre-trained model
  during inference. The method introduces stochasticity into intermediate layers,
  which disrupts the attacker's ability to approximate gradients and find adversarial
  directions.
---

# Understanding the Robustness of Randomized Feature Defense Against Query-Based Adversarial Attacks

## Quick Facts
- arXiv ID: 2310.00567
- Source URL: https://arxiv.org/abs/2310.00567
- Reference count: 11
- This paper proposes a randomized feature defense against black-box adversarial attacks by injecting noise into hidden features of a pre-trained model during inference.

## Executive Summary
This paper introduces a randomized feature defense that injects Gaussian noise into hidden layer activations during inference to defend against query-based adversarial attacks. The method disrupts gradient estimation by attackers without requiring adversarial training and maintains clean accuracy. Theoretical analysis shows effectiveness depends on the ratio between defense noise and attack noise, as well as gradient norms at different layers. Empirical results demonstrate significant robustness improvements across multiple models and datasets against both score-based and decision-based attacks.

## Method Summary
The proposed method injects Gaussian noise into hidden features at intermediate layers during inference. The noise is sampled from N(0, Σ) and added to selected layer outputs in the forward pass. The defense leverages the attacker's reliance on finite difference gradient estimation, where noise in hidden features increases variance in the gradient approximation. The method is model-agnostic and can be applied to any pre-trained model without retraining. Key parameters include noise scale and layer selection, which are optimized to balance robustness gains with clean accuracy preservation.

## Key Results
- The randomized feature defense significantly improves robustness against both score-based (NES, Square, SignHunt) and decision-based (RayS, SignFlip) attacks
- The defense achieves better robustness-accuracy trade-offs compared to input-noise defenses
- Combining the proposed defense with adversarial training further enhances robustness against score-based attacks with 1000 queries
- Selectively adding noise to layers with higher gradient norms maximizes robustness per unit accuracy loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Injecting Gaussian noise into hidden layer activations disrupts gradient estimation by the attacker.
- Mechanism: The attacker estimates gradients via finite differences. When noise δ is added to features h(x), the observed loss becomes stochastic, causing the finite difference approximation to include an additional variance term. When this variance is large, the attacker's gradient estimate is unreliable, leading to wrong update directions.
- Core assumption: The defense noise variance dominates the attack's finite difference noise, and the attacker cannot distinguish between signal and noise.
- Evidence anchors:
  - [abstract]: "by injecting noise into hidden features of a pre-trained model during inference" and "the method introduces stochasticity into intermediate layers, which disrupts the attacker's ability to approximate gradients"
  - [section]: Theorem 1 and Section 3.3 derive the robustness condition, showing higher defense noise or feature-layer gradient norm improves robustness.
- Break condition: If attack step size is large enough that noise variance is negligible relative to gradient signal, or if attacker uses EOT to average out randomness.

### Mechanism 2
- Claim: Selectively adding noise to layers with higher gradient norms at hidden features maximizes robustness per unit accuracy loss.
- Mechanism: The clean accuracy loss from noise injection depends on the gradient norm of the adversarial objective function. Theorem 1 and Section 3.5 show that accuracy drop is proportional to the gradient norm. Thus, adding noise to layers where adversarial gradients are large yields more robustness for the same accuracy drop.
- Core assumption: The adversarial objective function's gradient norm varies significantly across layers, and the defender can identify these layers.
- Evidence anchors:
  - [abstract]: "Our analysis also reveals the significance of selectively adding noise to different parts of the model based on the gradient of the adversarial objective function"
  - [section]: Section 3.5 derives accuracy degradation and Section 4.4 analyzes gradient norms at each layer, showing higher norms at deeper layers improve robustness.
- Break condition: If gradient norms are uniformly distributed across layers, selective noise provides no advantage.

### Mechanism 3
- Claim: Randomized feature noise increases the variance of decision-based attack loss, causing misjudgments in binary search.
- Mechanism: Decision-based attacks solve min_d g(d) where g(d) is the minimal perturbation to flip the decision. When noise δ is added to h(x), the linearized loss includes a variance term that can dominate and flip the sign of L, causing the attacker to misjudge the direction.
- Core assumption: The attacker's decision boundary search relies on consistent loss values, and variance in L(frand(x), y) breaks this consistency.
- Evidence anchors:
  - [abstract]: "Our theoretical analysis confirms that this method effectively enhances the model's resilience against both score-based and decision-based black-box attacks"
  - [section]: Section 3.4 analyzes the effect of noise on g(d) and shows that large gradient norms at hidden layers increase variance and flip signs.
- Break condition: If attacker uses EOT to average over multiple queries, reducing the variance of L(frand(x), y).

## Foundational Learning

- Concept: Finite difference gradient estimation and its variance.
  - Why needed here: The defense exploits the attacker's reliance on finite differences. Understanding how noise variance affects these estimates is critical to grasping why hidden-feature noise works.
  - Quick check question: If the attacker's gradient estimate has variance σ² and the defense adds noise with variance ν², under what condition does the defense noise dominate the attacker's signal?

- Concept: Backpropagation through hidden layers and Jacobian matrices.
  - Why needed here: The robustness condition involves ‖∇h(x)(L ◦ g)‖₂, the gradient of the loss with respect to hidden features. Understanding how this relates to the overall gradient is essential for interpreting Theorem 1.
  - Quick check question: Given f = g ◦ h, write the chain rule for ∇x(L ◦ f) in terms of ∇h(x)(L ◦ g) and Jh(x).

- Concept: Trade-offs between robustness and clean accuracy in randomized defenses.
  - Why needed here: Section 3.5 and the experiments show that noise injection reduces clean accuracy. Understanding how the magnitude of the gradient norm affects this trade-off is key to designing effective defenses.
  - Quick check question: If a layer has a very small gradient norm for a correctly classified sample, what happens to clean accuracy when noise is added there?

## Architecture Onboarding

- Component map:
  Pre-trained model f with layers h₁, h₂, ..., hₙ -> Noise injection module (samples δ ~ N(0, Σ)) -> Inference pipeline with noise injection at specified layers

- Critical path:
  1. Load pre-trained model
  2. For each input x:
     a. Forward pass through layers up to target layer hl
     b. Sample noise δ and add to hl(x)
     c. Continue forward pass to output
     d. Return prediction

- Design tradeoffs:
  - Noise scale ν: Larger ν → more robustness but more accuracy drop
  - Layer selection: Noise at deeper layers → more robustness (higher gradient norms) but may affect different features
  - Noise distribution: Gaussian N(0, Σ) is simple; other distributions could be explored

- Failure signatures:
  - High clean accuracy but low robustness → noise scale too small or wrong layers
  - Low clean accuracy → noise scale too large or too many layers
  - Robustness saturates → ratio in Theorem 1 is already high enough

- First 3 experiments:
  1. Baseline: Evaluate clean accuracy and robustness (Square attack, 1000 queries) on VGG19 without defense
  2. Uniform input noise: Add N(0, νI) to input, sweep ν, record accuracy/robustness trade-off
  3. Hidden-layer noise: Add N(0, νI) to layer 4 of VGG19, sweep ν, compare robustness to input noise at same accuracy drop

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed randomized feature defense compare to adversarial training in terms of robustness against adaptive attacks?
- Basis in paper: [explicit] The paper mentions that combining the proposed defense with adversarial training significantly improves robustness against score-based attacks with 1000 queries. However, it does not provide a direct comparison between the two methods in terms of robustness against adaptive attacks.
- Why unresolved: The paper focuses on evaluating the proposed defense against various types of attacks, but does not explicitly compare its performance to adversarial training in terms of robustness against adaptive attacks.
- What evidence would resolve it: Conducting experiments to compare the robustness of the proposed defense and adversarial training against adaptive attacks, such as EOT (Expectation Over Transformation), would provide insights into their relative effectiveness.

### Open Question 2
- Question: How does the proposed randomized feature defense perform against black-box attacks that transfer adversarial samples from a surrogate model to the target model?
- Basis in paper: [inferred] The paper mentions that future work will be directed towards analyzing black-box attacks that transfer adversarial samples from a surrogate model to the target model. This implies that the current evaluation does not include such attacks.
- Why unresolved: The paper focuses on evaluating the proposed defense against query-based attacks, but does not explicitly consider the transfer-based black-box attacks.
- What evidence would resolve it: Conducting experiments to evaluate the performance of the proposed defense against transfer-based black-box attacks, such as those that transfer adversarial samples from a surrogate model to the target model, would provide insights into its effectiveness in this scenario.

### Open Question 3
- Question: How does the proposed randomized feature defense affect the interpretability of the model's predictions?
- Basis in paper: [inferred] The paper introduces noise into the hidden features of the model during inference, which could potentially affect the interpretability of the model's predictions. However, the paper does not explicitly discuss the impact on interpretability.
- Why unresolved: The paper focuses on evaluating the proposed defense in terms of robustness and clean accuracy, but does not explicitly consider its impact on the interpretability of the model's predictions.
- What evidence would resolve it: Conducting experiments to analyze the impact of the proposed defense on the interpretability of the model's predictions, such as through feature visualization or attribution methods, would provide insights into its effects on interpretability.

## Limitations

- The theoretical analysis assumes perfect knowledge of gradient norms and noise distributions, but in practice these are estimated and may vary across samples.
- The defense's reliance on high-variance noise may be partially mitigated by attack methods using Expectation over Transformation (EOT) to average out randomness.
- The clean accuracy drop trade-off may become prohibitive for very high noise levels needed against stronger attacks.

## Confidence

- Mechanism 1 (Gradient disruption): High - well-supported by Theorem 1 and finite difference analysis
- Mechanism 2 (Layer selection): Medium - theoretical basis solid but empirical validation limited to specific architectures
- Mechanism 3 (Decision-based attack disruption): Low - theoretical analysis exists but limited experimental validation against real decision-based attacks

## Next Checks

1. Test EOT-based attacks to assess if averaging reduces defense effectiveness
2. Evaluate defense transferability to models not seen during attack generation
3. Measure clean accuracy under distribution shift (different test data than training)