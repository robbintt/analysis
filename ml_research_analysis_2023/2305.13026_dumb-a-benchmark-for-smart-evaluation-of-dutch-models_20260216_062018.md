---
ver: rpa2
title: 'DUMB: A Benchmark for Smart Evaluation of Dutch Models'
arxiv_id: '2305.13026'
source_url: https://arxiv.org/abs/2305.13026
tags:
- e-05
- tasks
- language
- dutch
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DUMB introduces a comprehensive Dutch language benchmark with eight
  tasks spanning word, word-pair, sentence-pair, and document-level challenges. It
  introduces Relative Error Reduction (RER) as a comparative metric to evaluate model
  performance beyond simple averages.
---

# DUMB: A Benchmark for Smart Evaluation of Dutch Models

## Quick Facts
- arXiv ID: 2305.13026
- Source URL: https://arxiv.org/abs/2305.13026
- Reference count: 34
- Key outcome: DUMB introduces a comprehensive Dutch language benchmark with eight tasks spanning word, word-pair, sentence-pair, and document-level challenges.

## Executive Summary
DUMB (Dutch Understanding Multitask Benchmark) introduces a comprehensive evaluation framework for Dutch language models across eight diverse NLP tasks. The benchmark employs a novel Relative Error Reduction (RER) metric to fairly compare model performance across tasks with varying difficulty levels. Testing 14 language models reveals that current Dutch monolingual models underperform compared to multilingual alternatives, particularly those based on the DeBERTaV3 architecture. The findings suggest that future Dutch language models should adopt DeBERTaV3 architecture and ELECTRA-style pretraining objectives to achieve optimal performance.

## Method Summary
DUMB evaluates 14 pre-trained language models (monolingual and multilingual, base and large sizes) across eight Dutch NLP tasks. Models are fine-tuned using hyperparameter grid search with 5 random seeds per model-task combination. Performance is measured using task-specific metrics and aggregated using Relative Error Reduction (RER) compared to a BERTje baseline. The benchmark covers word-level (POS, NER), word-pair (WSD), sentence-pair (PR, CR, NLI), and document-level (SA, AL) tasks, using both existing and newly translated datasets.

## Key Results
- Current Dutch monolingual models underperform compared to multilingual and English models
- DeBERTaV3-based models achieve the highest overall performance across all tasks
- Relative Error Reduction scores range from 6.9 to 37.3, with DeBERTaV3-large achieving the highest score of 37.3
- Low-resource tasks show particular challenges for non-Dutch models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dutch monolingual models underperform because they are only available in base size and use outdated BERT/RoBERTa architectures.
- Mechanism: Model size and architecture directly influence downstream task performance. Larger models and newer architectures (DeBERTaV3) achieve higher relative error reduction (RER) scores.
- Core assumption: Task performance scales predictably with model size and architecture within a language.
- Evidence anchors:
  - [abstract]: "Our results indicate that current Dutch monolingual models under-perform and suggest training larger Dutch models with other architectures and pre-training objectives."
  - [section]: "According to this analysis, a Dutch DeBERTaV3 large model could potentially achieve more than a 3.5 times higher error reduction than the current best model (42.7 vs. 11.6)."

### Mechanism 2
- Claim: Multilingual and English models can achieve high performance on Dutch tasks due to language contamination during pre-training and effective cross-lingual transfer.
- Mechanism: Pre-training data contains Dutch text, and models like DeBERTaV3 use ELECTRA-style objectives and gradient-disentangled embeddings that facilitate cross-lingual generalization.
- Core assumption: Cross-lingual transfer effectiveness is sufficient to compensate for not being natively trained on Dutch.
- Evidence anchors:
  - [abstract]: "Moreover, DeBERTaV3 and mDeBERTaV3 are pre-trained with an ELECTRA-style generator-discriminator training with gradient-disentangled word embeddings."
  - [section]: "English and multilingual CR performance is still high for DeBERTaV3-based models."

### Mechanism 3
- Claim: Relative Error Reduction (RER) is a better comparative metric than mean score because it accounts for varying task difficulties and baseline performances.
- Mechanism: RER normalizes performance differences relative to a strong baseline, making improvements comparable across tasks with different scales.
- Core assumption: Absolute score differences are not equally meaningful across tasks with different expected performance ranges.
- Evidence anchors:
  - [abstract]: "Instead of relying on a mean score across tasks, we propose Relative Error Reduction (RER), which compares the DUMB performance of models to a strong baseline."
  - [section]: "We assume that a single point improvement for POS (effectively reducing the error by 20%) would then be more meaningful than a single point improvement on CR (effectively reducing the error by about 3%)."

## Foundational Learning

- Concept: Relative Error Reduction (RER) metric
  - Why needed here: To compare model performance across tasks with different scales and difficulties fairly.
  - Quick check question: If a baseline achieves 80% accuracy and a model achieves 85%, what is the RER score? (Answer: 25% reduction in error rate)

- Concept: Cross-lingual transfer and language contamination
  - Why needed here: To understand why non-Dutch models can perform well on Dutch tasks.
  - Quick check question: What pre-training objective does DeBERTaV3 use that helps with cross-lingual transfer? (Answer: ELECTRA-style generator-discriminator training)

- Concept: Transformer encoder architectures (BERT, RoBERTa, DeBERTaV3)
  - Why needed here: To understand architectural differences that lead to performance variations.
  - Quick check question: What is the key architectural difference between DeBERTaV3 and BERT/RoBERTa? (Answer: DeBERTaV3 uses ELECTRA-style pretraining with gradient-disentangled embeddings)

## Architecture Onboarding

- Component map: Data preprocessing -> Model fine-tuning with hyperparameter search -> Task evaluation -> RER calculation -> Leaderboard update
- Critical path: Data preprocessing → Model fine-tuning with hyperparameter search → Task evaluation → RER calculation → Leaderboard update
- Design tradeoffs:
  - Using classification tasks only for standardization vs. more complex task types
  - Focusing on Transformer encoders vs. including generative models
  - Providing download scripts vs. hosting data directly due to licensing
- Failure signatures:
  - Low RER scores despite high absolute performance on individual tasks
  - High variance across test runs indicating instability
  - Poor performance on low-resource tasks suggesting overfitting or cross-lingual limitations
- First 3 experiments:
  1. Fine-tune BERTje on POS tagging task with default hyperparameters to verify basic functionality
  2. Run hyperparameter search for RobBERT2022 on sentiment analysis to understand optimization process
  3. Compute RER scores for all models and verify aggregation logic matches paper results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would Dutch monolingual DeBERTaV3 models perform on DUMB compared to current Dutch models?
- Basis in paper: [explicit] The paper explicitly states that "Dutch DeBERTaV3 models could potentially achieve more than a 3.5 times higher error reduction than the current best model (42.7 vs. 11.6)" and that "our findings...suggest that a Dutch DeBERTaV3 would outperform RoBERTa models"
- Why unresolved: No Dutch DeBERTaV3 models exist yet, preventing direct experimental comparison with current Dutch models

### Open Question 2
- Question: Does ELECTRA-style pretraining with GDES specifically explain DeBERTaV3's superior cross-lingual transfer to Dutch tasks, or are there other architectural factors at play?
- Basis in paper: [explicit] The paper notes that "English DeBERTalarge model...achieves -33.7 average RER, and -31.6 and 0.0 RER on CR and NLI, as opposed to 35.4 and 24.1 with DeBERTaV3large, suggesting that high cross-lingual DeBERTaV3 performance is primarily due to ELECTRA-style pretraining and GDES"
- Why unresolved: The comparison is between DeBERTa and DeBERTaV3, but doesn't isolate the effect of ELECTRA-style pretraining from other potential architectural differences

### Open Question 3
- Question: Why do low-resource tasks (PR and CR) show particularly poor performance for non-Dutch models, and what strategies could improve their performance?
- Basis in paper: [explicit] The paper observes that "Low-resource task performance seems more random than the other tasks, possibly due to model failure of non-Dutch models" and notes that "non-Dutch PLMs, and especially larger variants have a disadvantage at low-resource tasks"
- Why unresolved: The paper hypothesizes about model failure but doesn't experimentally investigate specific causes or solutions for the poor low-resource task performance

## Limitations

- Benchmark coverage limited to 8 specific tasks, which may not represent all Dutch NLP applications
- Results depend on quality and size of available Dutch training data, which is relatively limited
- Extrapolation of performance for larger Dutch models is speculative without actual large Dutch models to test

## Confidence

- **Medium** on DeBERTaV3 architecture superiority claim - experimental results support this but benchmark scope is limited
- **Low** on scalability predictions for large Dutch DeBERTaV3 models - based on extrapolation from existing models
- **Medium** on language contamination explanation - supported by results but not directly measured

## Next Checks

1. **Cross-task validation**: Evaluate the best-performing models (DeBERTaV3 variants) on additional Dutch NLP tasks not included in DUMB to verify whether the benchmark's findings generalize beyond its specific task set.

2. **Scaling experiment**: Train a medium-sized Dutch DeBERTaV3 model (between base and large) to test whether the performance scaling follows the predicted linear relationship before investing in truly large models.

3. **Data contamination analysis**: Analyze the pre-training data of top-performing multilingual models to quantify the amount of Dutch text present and establish whether language contamination explains their strong Dutch performance.