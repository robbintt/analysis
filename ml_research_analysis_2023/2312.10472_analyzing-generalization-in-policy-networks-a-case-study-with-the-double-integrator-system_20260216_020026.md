---
ver: rpa2
title: 'Analyzing Generalization in Policy Networks: A Case Study with the Double-Integrator
  System'
arxiv_id: '2312.10472'
source_url: https://arxiv.org/abs/2312.10472
tags:
- division
- state
- network
- policy
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes performance degradation in deep reinforcement
  learning (DRL) policy networks when operating in expanded state spaces beyond those
  encountered during training. The authors introduce a novel analysis technique called
  state division, which reveals that expansion of the state space causes the tanh
  activation function to saturate, transforming the state division boundary from nonlinear
  to linear.
---

# Analyzing Generalization in Policy Networks: A Case Study with the Double-Integrator System

## Quick Facts
- arXiv ID: 2312.10472
- Source URL: https://arxiv.org/abs/2312.10472
- Reference count: 16
- Key outcome: This paper analyzes performance degradation in deep reinforcement learning (DRL) policy networks when operating in expanded state spaces beyond those encountered during training.

## Executive Summary
This paper investigates why DRL policy networks experience performance degradation when tested in expanded state spaces. The authors introduce a novel analysis technique called state division to reveal that expansion of the state space causes the tanh activation function to saturate, transforming the state division boundary from nonlinear to linear. This transformation imparts a control behavior reminiscent of bang-bang control, but the inherent linearity of the division boundary prevents attainment of ideal bang-bang control, introducing unavoidable overshooting. Experimental investigations with diverse RL algorithms demonstrate that this performance degradation stems from inherent attributes of the DRL policy network, remaining consistent across various optimization algorithms.

## Method Summary
The authors analyze DRL policy network generalization by training agents on the double-integrator system using DDPG, TD3, SAC, and PPO algorithms with a 3-layer MLP policy network (32 nodes each, tanh activation) in a limited state space. They then apply state division analysis to reveal how division boundaries transform as state norms increase, mapping division regions and visualizing state-action patterns. The significance of weight vectors is measured using a strip function, and performance is evaluated in expanded state spaces to quantify overshooting behavior and its relationship to division line linearity.

## Key Results
- Tanh activation saturation in large-state regions transforms policy network state division boundaries from nonlinear to linear
- Linear division boundaries prevent approximation of nonlinear ideal bang-bang control, leading to unavoidable overshoot
- Performance degradation is consistent across multiple DRL algorithms (DDPG, TD3, SAC, PPO) and stems from inherent network attributes
- Only division lines with high significance (measured by the strip function) meaningfully affect control policy in expanded state spaces

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Saturation of the tanh activation function in large-state regions transforms the policy network's state division boundary from nonlinear to linear.
- Mechanism: As the norm of the input state grows, the tanh activation function saturates at ±1. This saturation causes the output of the first layer to depend only on the sign of the weighted sum, not its magnitude. Consequently, the division boundary in the state space becomes determined solely by the sign patterns of the weight vectors, which are linear hyperplanes.
- Core assumption: The first layer's weight vectors are fixed and their directions are not altered by training in the expanded state regime.
- Evidence anchors:
  - [abstract]: "expansion of state space induces the activation function tanh to exhibit saturability, resulting in the transformation of the state division boundary from nonlinear to linear."
  - [section]: "When the norm of the state goes to infinity, the first layer of the policy network encodes the state into a permutation consisting of −1, 0, and 1."
  - [corpus]: Weak evidence; no direct mention of tanh saturation in neighboring papers.
- Break condition: If the policy network uses a different activation function (e.g., ReLU) or a normalization layer that prevents saturation, the division boundary may remain nonlinear in the expanded state space.

### Mechanism 2
- Claim: The linear state division boundary prevents the policy network from approximating the nonlinear ideal bang-bang control, leading to unavoidable overshoot.
- Mechanism: The ideal bang-bang control for the double-integrator system requires a nonlinear division boundary (Eq. 16). However, the saturated tanh layers force the policy network to use a linear boundary. This mismatch means the network cannot time the deceleration optimally, causing overshoot when the initial error is large.
- Core assumption: The policy network's output is piecewise constant or nearly constant on either side of the division boundary.
- Evidence anchors:
  - [abstract]: "this gradual shift towards linearity imparts a control behavior reminiscent of bang-bang control. However, the inherent linearity of the division boundary prevents the attainment of an ideal bang-bang control, thereby introducing unavoidable overshooting."
  - [section]: "Consequently, time-optimal control cannot be achieved using the policy network. When the initial position error is relatively large, the agent will commence decelerating later, leading to unavoidable overshoots."
  - [corpus]: No evidence; neighboring papers do not discuss bang-bang control or overshoot.
- Break condition: If the network architecture or training objective is modified to explicitly model the nonlinear boundary (e.g., by using a different network structure or reward shaping), the overshoot may be reduced.

### Mechanism 3
- Claim: The significance of each division line, determined by the subsequent layers, dictates the magnitude of control changes as the state crosses the boundary.
- Mechanism: The significance ρi measures the difference in network output between adjacent regions divided by line i. Only significant division lines (large ρi) meaningfully affect the control policy. Insignificant lines have negligible impact on the output, even though they may be present in the state space.
- Core assumption: The network's subsequent layers can amplify or attenuate the effect of the first layer's output, thereby modulating the significance of each division line.
- Evidence anchors:
  - [section]: "if we assume the direction d is perpendicular to w1 i, and denote δi = tanh(||w1 i||x) as the offset on the feature vector, then the strip function of Eq. (9) can be rewritten as ψ(d, δi), where ... the significance of the division line Di is defined as ρi = |ψ(d, 1) − ψ(d, −1)|."
  - [section]: "Fig. 4 (b), only the division line of D1 is significant. The influence of D2 on the action is unobservable."
  - [corpus]: No evidence; neighboring papers do not discuss division line significance.
- Break condition: If the subsequent layers are removed or their weights are constrained to be small, all division lines may become equally significant, potentially leading to erratic control behavior.

## Foundational Learning

- Concept: tanh activation saturation
  - Why needed here: Understanding how tanh saturation in large states transforms the state division boundary is central to the paper's analysis.
  - Quick check question: What happens to the output of tanh(x) as |x| becomes very large?

- Concept: State division and division lines
  - Why needed here: The concept of dividing the state space into regions based on the network's output is fundamental to analyzing policy network behavior.
  - Quick check question: How does the direction of a weight vector in the first layer relate to the orientation of the corresponding division line in the state space?

- Concept: Bang-bang control and its optimality
  - Why needed here: The paper uses the ideal bang-bang control as a benchmark to show the limitations of the policy network's linear division boundary.
  - Quick check question: What is the key feature of a bang-bang control law that makes it time-optimal for the double-integrator system?

## Architecture Onboarding

- Component map: State → First layer (tanh) → Feature vector → Subsequent layers (tanh) → Control output
- Critical path: State → First layer (tanh) → Feature vector → Subsequent layers (tanh) → Control output. The first layer's saturation in large states is the critical point where the division boundary becomes linear.
- Design tradeoffs: Using tanh allows for smooth gradients during training but leads to saturation in large states. Alternative activations (e.g., ReLU) may avoid saturation but introduce discontinuities.
- Failure signatures: Large overshoot in the control response, especially when the initial state error is large. The state trajectory may cross division lines late, leading to delayed deceleration.
- First 3 experiments:
  1. Train a policy network on the double-integrator system with varying initial state magnitudes and observe the division boundary shape and control performance.
  2. Replace tanh with ReLU in the first layer and analyze how the division boundary changes and whether overshoot is reduced.
  3. Modify the network architecture to include a normalization layer before the first layer and assess its impact on saturation and control performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the state division theory be effectively extended to high-dimensional state spaces, where division lines become division hyperplanes?
- Basis in paper: [inferred] The paper discusses the transition from division lines to division strips and mentions future work on analyzing how this transition affects policy network performance in high-dimensional spaces.
- Why unresolved: The paper primarily focuses on two-dimensional state spaces (double-integrator system) and does not provide empirical evidence or theoretical analysis for higher dimensions.
- What evidence would resolve it: Experimental results demonstrating the behavior of division hyperplanes in high-dimensional reinforcement learning tasks, showing how network performance is affected by the dimensionality of the state space.

### Open Question 2
- Question: How does the linear state space division phenomenon affect the performance of DRL policy networks in nonlinear control systems beyond the double-integrator example?
- Basis in paper: [explicit] The paper states that future work will discuss how the degradation impact affects nonlinear systems, indicating this is currently unexplored.
- Why unresolved: The paper only provides analysis for the double-integrator system, which is a linear system, and does not extend the findings to nonlinear systems.
- What evidence would resolve it: Comparative studies of DRL policy network performance in various nonlinear control systems, analyzing how the linear division boundary impacts control effectiveness and generalization.

### Open Question 3
- Question: What alternative network architectures or activation functions could mitigate the performance degradation caused by state space expansion?
- Basis in paper: [inferred] The paper identifies the tanh activation function's saturation as a key factor in the linear division phenomenon and suggests that this is an inherent limitation of the network structure, implying that alternative designs might help.
- Why unresolved: The paper focuses on analyzing the problem but does not propose or test alternative network designs that could overcome the limitations identified.
- What evidence would resolve it: Empirical comparison of different network architectures (e.g., varying activation functions, additional layers, or different topologies) showing improved generalization performance in expanded state spaces compared to standard MLP with tanh.

## Limitations

- The analysis relies on the assumption that network weights remain fixed in expanded state regimes, which may not hold if continued training occurs
- Findings are based exclusively on the double-integrator system, limiting generalizability to other control tasks
- The paper does not explore alternative network architectures or activation functions that could mitigate the identified limitations

## Confidence

- **High confidence**: Core observation that tanh saturation transforms division boundaries from nonlinear to linear in large-state regions
- **Medium confidence**: Link between linear boundaries and unavoidable overshoot, as this depends on specific properties of the double-integrator system
- **Low confidence**: Generality of findings across different activation functions, network architectures, and control problems

## Next Checks

1. Test the analysis with ReLU activation to determine if division boundary linearity persists without saturation effects
2. Evaluate performance across multiple control tasks (pendulum, cart-pole) to assess generalizability beyond the double-integrator
3. Implement a normalization layer before the first network layer to test whether preventing state norm growth mitigates saturation and maintains nonlinear boundaries