---
ver: rpa2
title: Assessing Robustness via Score-Based Adversarial Image Generation
arxiv_id: '2310.04285'
source_url: https://arxiv.org/abs/2310.04285
tags:
- adversarial
- examples
- images
- scoreag
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces Score-Based Adversarial Generation (ScoreAG),\
  \ a novel framework for generating unrestricted adversarial examples using score-based\
  \ generative models. Unlike traditional methods constrained by \u2113p-norm perturbations,\
  \ ScoreAG leverages diffusion models to create realistic adversarial examples that\
  \ maintain core semantics while being outside common perturbation norms."
---

# Assessing Robustness via Score-Based Adversarial Image Generation

## Quick Facts
- arXiv ID: 2310.04285
- Source URL: https://arxiv.org/abs/2310.04285
- Reference count: 15
- Primary result: ScoreAG generates unrestricted adversarial examples that preserve semantics while outperforming traditional ℓp-norm constrained methods

## Executive Summary
This paper introduces Score-Based Adversarial Generation (ScoreAG), a framework that leverages diffusion models to create unrestricted adversarial examples beyond traditional ℓp-norm constraints. The approach generates semantically preserving adversarial examples by combining unconditional score functions with task-specific guidance terms. ScoreAG includes three tasks: generating adversarial examples from scratch (GAS), transforming existing images into adversarial ones (GAT), and purifying images to enhance classifier robustness (GAP). Experiments demonstrate that ScoreAG outperforms existing methods in generating realistic adversarial examples while maintaining core image semantics across CIFAR-10, CIFAR-100, and TinyImagenet datasets.

## Method Summary
ScoreAG uses score-based generative models and diffusion processes to generate unrestricted adversarial examples. The framework employs Bayes' theorem to combine unconditional score functions with task-specific guidance terms, enabling conditional generation beyond ℓp constraints. It includes three tasks: GAS for generating adversarial examples from scratch, GAT for transforming existing images, and GAP for purifying images to enhance robustness. The method uses pre-trained classifiers and diffusion models without requiring retraining, approximating the original image using a one-step Euler method for guidance term computation.

## Key Results
- ScoreAG outperforms traditional ℓp-norm constrained methods in generating realistic adversarial examples
- The framework achieves superior performance in both adversarial attacks and defenses across multiple datasets
- Semantic preservation is maintained while generating unrestricted adversarial examples beyond conventional perturbation bounds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ScoreAG generates unrestricted adversarial examples by leveraging diffusion guidance on unconditional score-based generative models
- Mechanism: The framework uses Bayes' theorem to combine the unconditional score function ∇xt log pt(xt) with a task-specific guidance term ∇xt log p(c|xt), enabling generation of semantically-preserving perturbations beyond ℓp constraints
- Core assumption: The guidance term can be approximated using a one-step Euler method from the diffusion process without requiring time-dependent classifiers
- Evidence anchors:
  - [abstract] "ScoreAG leverages the advancements in score-based generative models to generate unrestricted adversarial examples that overcome the limitations of ℓp-norm constraints"
  - [section 3.1] "While the term ∇x log pt,y∗(xt) can be approximated using a class-conditional neural network sθ(xt, t, y), ∇xt log pt,y∗(f(x0) = ˜y|xt) requires access to a time-dependent classifier or the original imagex0. Considering that we are given a pre-trained classifier and cannot retrain it on perturbed images xt, we propose to approximate x0 using a one-step Euler method"
- Break condition: The approximation fails when the one-step Euler prediction ˆx0 deviates significantly from the true x0, making the guidance term inaccurate

### Mechanism 2
- Claim: ScoreAG maintains image semantics while generating adversarial examples through class-conditional score networks
- Mechanism: The class-conditional score network sθ(xt, t, y) ensures that generated images preserve the core semantics of the target class y* while being classified as a different class ˜y
- Core assumption: The class-conditional score network has learned to generate realistic images that preserve class semantics
- Evidence anchors:
  - [abstract] "Unlike traditional methods, ScoreAG maintains the core semantics of images while generating realistic adversarial examples"
  - [section 3.1] "While the term ∇x log pt,y∗(xt) can be approximated using a class-conditional neural network sθ(xt, t, y)"
- Break condition: The score network fails to learn realistic class semantics or overfits to training data, leading to unrealistic or semantically incorrect adversarial examples

### Mechanism 3
- Claim: ScoreAG enhances classifier robustness through generative adversarial purification
- Mechanism: By leveraging the generative capability of score-based models, ScoreAG can purify adversarial perturbations by sampling from the distribution p(x0|xADV) that resembles the semantics of the adversarial image
- Core assumption: The purified image sampled from p(x0|xADV) will be more robust to adversarial attacks than the original adversarial image
- Evidence anchors:
  - [abstract] "We further exploit the generative capability of ScoreAG to purify images, empirically enhancing the robustness of classifiers"
  - [section 3.3] "Generative Adversarial Purification (GAP) extends the capability of ScoreAG to counter adversarial attacks. It is designed to purify adversarial images, i.e., remove adversarial perturbations by leveraging the generative capability of the model"
- Break condition: The purification process introduces artifacts or fails to remove adversarial perturbations effectively, leaving the classifier vulnerable

## Foundational Learning

- Concept: Diffusion processes and score-based generative models
  - Why needed here: ScoreAG is built on diffusion models and score-based generative models, which form the foundation for generating unrestricted adversarial examples
  - Quick check question: What is the relationship between the forward diffusion process and the reverse-time SDE in score-based generative models?

- Concept: Bayes' theorem and conditional probability
  - Why needed here: ScoreAG uses Bayes' theorem to combine the unconditional score function with task-specific guidance terms for conditional generation
  - Quick check question: How does Bayes' theorem enable the use of unconditional models for conditional tasks in ScoreAG?

- Concept: ℓp-norm bounded vs. unrestricted adversarial examples
  - Why needed here: Understanding the limitations of ℓp-norm constraints is crucial for appreciating the significance of ScoreAG's approach to generating unrestricted adversarial examples
  - Quick check question: What are the key differences between ℓp-norm bounded and unrestricted adversarial examples in terms of their impact on classifier robustness?

## Architecture Onboarding

- Component map: Score-based generative model (sθ) -> Classifier (f) -> Guidance term (task-specific) -> Diffusion process (forward and reverse-time SDE)

- Critical path:
  1. Select task-specific guidance term
  2. Adapt reverse-time SDE with conditional score function
  3. Solve adapted SDE to generate adversarial or purified image

- Design tradeoffs:
  - Tradeoff between attack strength (sy) and preservation of image semantics
  - Computational cost of solving the SDE vs. effectiveness of adversarial examples
  - Generalization of the score network vs. specificity to target classes

- Failure signatures:
  - Generated images are unrealistic or do not preserve class semantics
  - Classifier accuracy remains high despite large sy values
  - Purification process introduces artifacts or fails to remove adversarial perturbations

- First 3 experiments:
  1. Generate synthetic adversarial examples using GAS and evaluate classifier accuracy and FID score
  2. Transform existing images into adversarial examples using GAT and compare with traditional ℓp-norm constrained attacks
  3. Purify adversarial images using GAP and evaluate the robust accuracy of the classifier

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a metric to quantify semantic similarity between original images and generated adversarial examples?
- Basis in paper: [inferred] The paper mentions that human studies are currently required to evaluate unrestricted adversarial attacks due to the lack of automated metrics for semantic similarity.
- Why unresolved: Existing metrics like ℓp-norm constraints fail to capture semantic changes, and automated methods for assessing semantic preservation are currently lacking.
- What evidence would resolve it: A new metric that reliably quantifies semantic similarity between original and adversarial images, validated against human judgment.

### Open Question 2
- Question: Can we develop an efficient method to compute gradients of the generative process for use in adversarial purification?
- Basis in paper: [explicit] The paper states that the current purification approach is limited to a preprocessor-blackbox setting because gradients of the generative process cannot be computed efficiently.
- Why unresolved: The stochastic nature of diffusion models makes gradient computation challenging, limiting the applicability of ScoreAG to certain defense scenarios.
- What evidence would resolve it: A method to efficiently approximate or compute gradients of the diffusion process that enables ScoreAG to be used in white-box or gray-box defense settings.

### Open Question 3
- Question: Is it possible to integrate certifiable robustness with ScoreAG's unrestricted threat model?
- Basis in paper: [explicit] The paper identifies integrating certifiable robustness as an open challenge due to the stochastic nature of the generative process and the unbounded threat model.
- Why unresolved: Current certification methods are designed for ℓp-bounded perturbations, and the unrestricted nature of ScoreAG's perturbations makes traditional certification approaches inapplicable.
- What evidence would resolve it: A framework that provides provable guarantees against unrestricted adversarial examples generated by ScoreAG or similar methods.

## Limitations
- The approach's effectiveness depends heavily on the quality and generalization capability of the pre-trained score network
- Computational cost of solving reverse-time SDEs is substantially higher than traditional gradient-based attacks
- The framework's performance may degrade with out-of-distribution inputs or underrepresented classes

## Confidence
- **High confidence**: The core mechanism of using diffusion guidance for adversarial generation is well-founded and demonstrates superior performance on standard benchmarks
- **Medium confidence**: The semantic preservation claims, while supported by FID scores and human evaluations, require further validation across diverse datasets and threat models
- **Medium confidence**: The purification mechanism's effectiveness in real-world scenarios needs additional empirical validation, particularly against adaptive attacks

## Next Checks
1. Evaluate ScoreAG's performance against adaptive defenses that specifically target diffusion-based attack methods, including gradient obfuscation techniques
2. Test the framework's generalization across different score-based generative models (e.g., DDPM, NCSN) and diffusion process configurations
3. Conduct extensive human studies to validate semantic preservation claims across diverse image categories and cultural contexts