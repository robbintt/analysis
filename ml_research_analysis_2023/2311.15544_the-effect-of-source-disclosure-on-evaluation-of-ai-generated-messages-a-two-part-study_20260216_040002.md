---
ver: rpa2
title: 'The effect of source disclosure on evaluation of AI-generated messages: A
  two-part study'
arxiv_id: '2311.15544'
source_url: https://arxiv.org/abs/2311.15544
tags:
- messages
- source
- message
- negative
- ai-generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how source disclosure (AI vs. human) influences
  the evaluation of AI-generated health messages.
---

# The effect of source disclosure on evaluation of AI-generated messages: A two-part study

## Quick Facts
- arXiv ID: 2311.15544
- Source URL: https://arxiv.org/abs/2311.15544
- Reference count: 16
- Key outcome: Source disclosure significantly reduced the difference in evaluation between AI and human-generated messages

## Executive Summary
This two-part study examines how source disclosure (AI vs. human) influences the evaluation of AI-generated health messages. Using vaping prevention messages, participants rated messages with or without source labels across two experiments. Results showed that source disclosure significantly reduced the difference in evaluation between AI and human-generated messages. Negative attitudes toward AI moderated this effect, with moderate negative attitudes decreasing preference for AI-generated messages when the source was disclosed. The findings suggest a slight bias against AI-generated messages once the source is known, contributing to source effects theory and highlighting implications for public health campaigns using AI-generated content.

## Method Summary
The study employed a two-part experimental design using vaping prevention messages. In both studies, 30 messages (15 AI-generated using Bloom, 15 human-generated tweets) were evaluated by participants who were randomly assigned to see either labeled or unlabeled messages. Study 1 used a ranking task where participants ordered all 30 messages, while Study 2 used a selection task where participants chose their top 5 messages. Both studies measured effects perception scores and included a negative attitudes toward AI scale (GAAIS) in Study 2. Data was analyzed using mixed ANOVA, Wilcoxon Rank Sum Tests, and Poisson regression to examine main effects and interactions with negative attitudes toward AI.

## Key Results
- Source disclosure significantly reduced the difference in evaluation between AI and human-generated messages (p < .001 in both studies)
- Negative attitudes toward AI moderated the effect of source disclosure on message evaluation
- Source disclosure decreased the preference for AI-generated messages among those with moderate negative attitudes toward AI
- No significant difference in effects perception scores between AI and human-generated messages when source was undisclosed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Source disclosure reduces the difference in evaluation between AI-generated and human-generated messages.
- Mechanism: When the source is disclosed, participants apply their general negative attitudes toward AI to the evaluation of AI-generated messages, narrowing the gap between AI and human-generated messages.
- Core assumption: People have pre-existing negative attitudes toward AI that influence how they evaluate AI-generated content.
- Evidence anchors:
  - [abstract] "Results showed that source disclosure significantly reduced the difference in evaluation between AI and human-generated messages."
  - [section] "source disclosure significantly decreased the ratings difference between AI and human-generated messages."
  - [corpus] Weak - corpus focuses on AI vs human comparison but not source disclosure specifically.
- Break condition: If source disclosure has no effect on evaluation differences, or if disclosure actually increases the difference in evaluation.

### Mechanism 2
- Claim: Negative attitudes toward AI moderate the effect of source disclosure on message evaluation.
- Mechanism: People with higher negative attitudes toward AI are more likely to rate AI-generated messages lower when the source is disclosed, while those with lower negative attitudes are less affected.
- Core assumption: Individual differences in negative attitudes toward AI influence how people evaluate AI-generated messages.
- Evidence anchors:
  - [abstract] "Negative attitudes toward AI moderated this effect, with moderate negative attitudes decreasing preference for AI-generated messages when the source was disclosed."
  - [section] "The influence of source disclosure on the evaluation of the AI-generated messages vs. human-generated messages differed by the level of negative attitudes towards AI."
  - [corpus] Weak - corpus focuses on AI vs human comparison but not negative attitudes as moderator.
- Break condition: If negative attitudes toward AI do not moderate the effect of source disclosure on message evaluation.

### Mechanism 3
- Claim: Source disclosure decreases the preference for AI-generated messages among those with moderate negative attitudes toward AI.
- Mechanism: When the source is disclosed, people with moderate negative attitudes toward AI are less likely to select AI-generated messages as their top choices.
- Core assumption: Source disclosure affects not only evaluation but also preference for AI-generated messages.
- Evidence anchors:
  - [abstract] "source disclosure decreased the preference for AI-generated messages."
  - [section] "source disclosure significantly decreased the number of AI-generated messages selected for those with moderate levels of negative attitudes towards AI."
  - [corpus] Weak - corpus focuses on AI vs human comparison but not source disclosure and moderate negative attitudes specifically.
- Break condition: If source disclosure does not decrease preference for AI-generated messages among those with moderate negative attitudes toward AI.

## Foundational Learning

- Concept: Source effects theory
  - Why needed here: Understanding how source characteristics influence message evaluation is crucial for interpreting the study's findings.
  - Quick check question: What are some key factors that influence source effects in communication research?
- Concept: Large language models (LLMs)
  - Why needed here: Understanding how LLMs work is important for grasping the context of AI-generated messages.
  - Quick check question: How do LLMs generate text, and what are their limitations?
- Concept: Vaping prevention messaging
  - Why needed here: The study uses vaping prevention as the context for testing AI-generated messages.
  - Quick check question: What are some key strategies used in vaping prevention campaigns?

## Architecture Onboarding

- Component map: Experiment design (source disclosure manipulation, message evaluation, negative attitudes toward AI measurement) -> Data analysis (mixed effects modeling, regression analysis) -> Computational analysis (textual feature extraction)
- Critical path: Design experiment → Collect data → Analyze data → Interpret results → Draw conclusions
- Design tradeoffs: Using tweets as messages (real-world context but limited length) vs. longer messages; ranking vs. selection tasks for measuring preference
- Failure signatures: No significant effect of source disclosure; negative attitudes toward AI not moderating the effect; computational analysis not revealing meaningful differences between AI and human-generated messages
- First 3 experiments:
  1. Replicate the source disclosure manipulation with different types of messages (e.g., longer health messages, political messages)
  2. Test the moderating effect of negative attitudes toward AI with a larger sample size
  3. Conduct a longitudinal study to examine how attitudes toward AI-generated messages change over time as AI technology advances

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the preference for human-generated messages depend on the specific health topic or the perceived severity of the topic?
- Basis in paper: [inferred] The paper mentions that psychometric models of risk perception could predict that certain critical topics could be particularly prone to AI-source effects, but the authors chose a straightforward and widely applicable topic (vaping prevention) for their study.
- Why unresolved: The study only examined one health topic, so it's unclear whether the results would generalize to other health topics or topics perceived as more or less severe.
- What evidence would resolve it: Conducting similar studies across various health topics, including those perceived as more or less severe, would help determine if the preference for human-generated messages varies by topic.

### Open Question 2
- Question: How do different aspects of the AI message generation process (e.g., the initiator, the level of human involvement) influence the evaluation of AI-generated messages?
- Basis in paper: [explicit] The paper discusses the complexity of the "source" for AI-generated messages, highlighting that humans initiate the message creation sequence and that the level of human involvement varies.
- Why unresolved: The study did not manipulate different aspects of the AI message generation process, so it's unclear how these factors might influence evaluations.
- What evidence would resolve it: Experimental studies that manipulate different aspects of the AI message generation process (e.g., the initiator, the level of human involvement) and examine their effects on message evaluations would help clarify this.

### Open Question 3
- Question: How do people's attitudes towards AI evolve over time as they become more exposed to AI-generated content, and how does this impact their evaluations of AI-generated messages?
- Basis in paper: [inferred] The paper mentions that the dynamically evolving landscape of AI-language generation systems and the potential for increasing exposure to AI-generated content make it difficult to draw final conclusions about evaluations.
- Why unresolved: The study only examined attitudes towards AI at a single point in time, so it's unclear how these attitudes might change with increased exposure to AI-generated content.
- What evidence would resolve it: Longitudinal studies that track people's attitudes towards AI and their evaluations of AI-generated messages over time would help understand how these attitudes and evaluations evolve.

## Limitations
- Small effect sizes with the primary interaction effect explaining approximately 1-2% of variance
- Predominantly young, well-educated sample from a single geographic region
- Limited to one content domain (vaping prevention messages)

## Confidence
- High confidence in basic finding that source disclosure affects message evaluation differences (p < .001 in both studies)
- Medium confidence in moderating role of negative attitudes toward AI (interaction p-values range from .02 to .07)
- Low confidence in specific pattern for moderate negative attitudes groups due to smaller subsample sizes and less consistent effects

## Next Checks
1. Replicate with larger, more diverse samples across different age groups and cultural contexts to test generalizability
2. Test the mechanism by experimentally manipulating awareness of AI capabilities rather than just source labels
3. Conduct longitudinal studies to assess whether attitudes toward AI-generated messages change as AI technology becomes more familiar to the public