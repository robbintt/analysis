---
ver: rpa2
title: Are Large Language Models Really Robust to Word-Level Perturbations?
arxiv_id: '2309.11166'
source_url: https://arxiv.org/abs/2309.11166
tags:
- robustness
- level
- reward
- arxiv
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TREVal, a novel evaluation approach for assessing
  the robustness of large language models (LLMs) to word-level perturbations. The
  method leverages pre-trained reward models to evaluate the quality of generated
  text responses to open-ended questions under various types of perturbations (synonym
  substitution, swapping, and misspelling).
---

# Are Large Language Models Really Robust to Word-Level Perturbations?

## Quick Facts
- arXiv ID: 2309.11166
- Source URL: https://arxiv.org/abs/2309.11166
- Authors: 
- Reference count: 40
- Primary result: LLMs frequently exhibit vulnerability to word-level perturbations, with robustness decreasing as fine-tuning (SFT and RLHF) is conducted

## Executive Summary
This paper introduces TREVal, a novel evaluation approach for assessing the robustness of large language models (LLMs) to word-level perturbations. The method leverages pre-trained reward models to evaluate the quality of generated text responses to open-ended questions under various types of perturbations (synonym substitution, swapping, and misspelling). The authors conduct extensive experiments on multiple LLMs across different stages of development (pre-trained, SFT, RLHF) and parameter sizes (7B-70B). They find that LLMs frequently exhibit vulnerability to word-level perturbations, with robustness decreasing as fine-tuning (SFT and RLHF) is conducted. Specifically, they observe significant performance drops in reward scores when models are presented with perturbed prompts, indicating a lack of robustness. The authors also note that while larger models tend to have higher overall performance, their robustness to perturbations is not necessarily better than smaller models.

## Method Summary
The authors propose TREVal, a novel evaluation approach that leverages pre-trained reward models as diagnostic tools to evaluate the longer conversation generated from more challenging open questions by LLMs. The method focuses on word-level perturbations such as word swapping, synonym substitution, and common misspellings, which frequently arise in everyday language use. The evaluation process involves generating responses to both clean and perturbed prompts using the target LLMs, then assessing these responses with pre-trained reward models. The drop rate in reward scores between clean and perturbed responses serves as a measure of robustness. The study conducts experiments on multiple LLMs across different stages of development (pre-trained, SFT, RLHF) and parameter sizes (7B-70B), using open-ended questions from the Natural Questions dataset.

## Key Results
- LLMs exhibit significant vulnerability to word-level perturbations, with performance drops observed across all tested models and perturbation types
- Robustness to perturbations tends to decrease as models progress through fine-tuning stages (pre-trained → SFT → RLHF)
- Larger models (70B) do not necessarily demonstrate better robustness to perturbations compared to smaller models (7B)
- Synonym substitution perturbations generally cause less performance drop than swapping and misspelling perturbations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using a pre-trained reward model as the evaluator provides a more aligned assessment of LLM robustness than accuracy-based methods.
- Mechanism: Reward models trained on human preferences can evaluate the quality of generated text in a way that captures the nuances of open-ended responses, which is more representative of LLM capabilities than single-word or letter outputs.
- Core assumption: Reward models can accurately capture human intent and preferences for evaluating open-ended responses.
- Evidence anchors:
  - [abstract] "we propose a novel rational evaluation approach that leverages pre-trained reward models as diagnostic tools to evaluate the longer conversation generated from more challenging open questions by LLMs"
  - [section 2] "we innovatively introduce trained reward models as a judge"
- Break condition: If the reward model is not well-aligned with human preferences or fails to capture the nuances of open-ended responses, the evaluation will be inaccurate.

### Mechanism 2
- Claim: Word-level perturbations that do not change semantic intent from a human perspective can still mislead LLMs, revealing their vulnerability.
- Mechanism: By introducing common perturbations like synonym substitution, swapping, and misspelling, the study tests if LLMs can maintain performance despite minor changes that humans would easily understand.
- Core assumption: These perturbations are representative of real-world usage and do not alter the semantic meaning of the prompts.
- Evidence anchors:
  - [section 2] "we concentrate on word-level perturbations such as word swapping, synonym substitution, and common misspellings, which frequently arise in everyday language use"
  - [section 3.2] "Importantly, these attacks do not alter the semantic labels of the prompts from a human-centric perspective"
- Break condition: If the perturbations significantly alter the semantic meaning of the prompts, the evaluation will not accurately reflect LLM robustness to minor errors.

### Mechanism 3
- Claim: Fine-tuning processes like SFT and RLHF can actually decrease LLM robustness to word-level perturbations, despite improving performance on clean inputs.
- Mechanism: The study observes that as LLMs progress through stages of fine-tuning, their robustness decreases, suggesting that the fine-tuning process may introduce instability.
- Core assumption: The observed decrease in robustness is due to the fine-tuning process rather than other factors.
- Evidence anchors:
  - [abstract] "our results demonstrate that LLMs frequently exhibit vulnerability to word-level perturbations that are commonplace in daily language usage. Notably, we are surprised to discover that robustness tends to decrease as fine-tuning (SFT and RLHF) is conducted"
  - [section 4.2.2] "We observe a noticeable decline in the robustness of LLMs as they progress from the Pretrained to the RLHF stages, particularly against word-level attacks"
- Break condition: If other factors (e.g., model architecture, training data) are responsible for the observed decrease in robustness, the conclusion about fine-tuning may be incorrect.

## Foundational Learning

- Concept: Robustness evaluation
  - Why needed here: To ensure LLMs can maintain performance despite minor input errors that do not alter semantic meaning.
  - Quick check question: What is the primary metric used to evaluate LLM robustness in this study?

- Concept: Word-level perturbations
  - Why needed here: To simulate common errors in real-world usage and test LLM resilience to minor input changes.
  - Quick check question: What are the three types of word-level perturbations used in this study?

- Concept: Fine-tuning stages
  - Why needed here: To understand how different stages of model development affect robustness to perturbations.
  - Quick check question: What is the observed trend in LLM robustness as they progress through fine-tuning stages?

## Architecture Onboarding

- Component map:
  Datasets -> LLMs -> Perturbations -> Reward Models -> Robustness Scores

- Critical path:
  1. Select open-ended questions from NQ dataset
  2. Generate clean responses using the LLM under evaluation
  3. Introduce word-level perturbations to the prompts
  4. Generate perturbed responses using the same LLM
  5. Evaluate both clean and perturbed responses using the reward model
  6. Calculate the drop rate in reward scores as a measure of robustness

- Design tradeoffs:
  - Using open-ended questions vs. closed questions: Open-ended questions better reflect LLM capabilities but are more challenging to evaluate
  - Reward model vs. human evaluation: Reward models are more scalable but may not perfectly align with human preferences
  - Three levels of perturbation severity: Allows for a nuanced understanding of LLM resilience but increases experimental complexity

- Failure signatures:
  - High drop rates in reward scores indicate low robustness to word-level perturbations
  - Inconsistent performance across different stages of fine-tuning suggests instability introduced by the fine-tuning process
  - Significant differences in robustness between models of the same family and size may indicate other factors affecting robustness

- First 3 experiments:
  1. Evaluate the robustness of Llama2-7B to synonym substitution perturbations at level 1
  2. Compare the robustness of Alpaca-7B and Beavor-7B to misspelling perturbations at level 2
  3. Assess the impact of increasing model size (7B to 70B) on the robustness of Llama2-chat to swapping perturbations at level 3

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different levels of perturbation (Level 1, 2, 3) specifically affect the robustness of LLMs in terms of helpfulness and harmlessness?
- Basis in paper: [explicit] The paper discusses perturbation levels and their impact on LLM robustness.
- Why unresolved: While the paper mentions that higher perturbation levels lead to greater score decline, it does not provide a detailed analysis of how each specific level affects helpfulness versus harmlessness robustness.
- What evidence would resolve it: Detailed comparative analysis of score drops at each perturbation level for both helpfulness and harmlessness metrics.

### Open Question 2
- Question: What is the impact of model size (7B vs 13B vs 70B parameters) on the robustness of LLMs against word-level perturbations?
- Basis in paper: [explicit] The paper evaluates LLMs of different sizes and notes fluctuations in robustness.
- Why unresolved: The paper observes fluctuations in robustness with increasing model size but does not provide a clear explanation for these variations or a definitive conclusion on the relationship between model size and robustness.
- What evidence would resolve it: A comprehensive study correlating model size with robustness metrics across various perturbation types and levels.

### Open Question 3
- Question: How does the fine-tuning process (SFT vs RLHF) specifically impact the robustness of LLMs to word-level perturbations?
- Basis in paper: [explicit] The paper finds that robustness tends to decrease as fine-tuning is conducted.
- Why unresolved: The paper identifies a decline in robustness during fine-tuning but does not explore the underlying reasons or mechanisms causing this decrease.
- What evidence would resolve it: In-depth analysis of parameter changes during fine-tuning stages and their correlation with robustness metrics.

## Limitations

- The choice of reward models for evaluation may introduce bias, as different models may have varying interpretations of response quality.
- The perturbations used, while common, may not fully represent all real-world error patterns.
- The study focuses on open-ended questions, which may not generalize to other types of tasks or domains.
- The observed decrease in robustness through fine-tuning stages could be influenced by factors not controlled for in the study, such as changes in training data or hyperparameters.

## Confidence

**High Confidence:**
- LLMs exhibit vulnerability to word-level perturbations
- There is a noticeable decline in robustness as models progress through fine-tuning stages

**Medium Confidence:**
- Larger models do not necessarily show better robustness to perturbations
- The proposed TREVal method provides a more aligned assessment of LLM robustness than accuracy-based methods

**Low Confidence:**
- The specific mechanisms by which fine-tuning reduces robustness
- The generalizability of findings to other types of tasks or domains

## Next Checks

1. **Cross-Reward Model Validation**: Evaluate the same set of LLMs using multiple reward models to assess the consistency and reliability of the robustness findings. This will help determine if the observed vulnerabilities are consistent across different evaluation criteria.

2. **Error Pattern Expansion**: Introduce a wider variety of perturbation types and real-world error patterns (e.g., grammatical errors, idiomatic expressions) to the prompts and assess their impact on LLM robustness. This will provide a more comprehensive understanding of LLM vulnerabilities in practical scenarios.

3. **Fine-Tuning Stability Analysis**: Conduct a controlled study isolating the effects of different fine-tuning techniques (SFT, RLHF) on LLM robustness. This should include varying hyperparameters and training data to determine if the observed decrease in robustness is inherent to the fine-tuning process or dependent on specific implementation choices.