---
ver: rpa2
title: 'LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores'
arxiv_id: '2311.09766'
source_url: https://arxiv.org/abs/2311.09766
tags:
- evaluation
- metrics
- language
- generated
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Language model-based evaluation metrics such as BARTScore, T5Score,
  and GPTScore are increasingly popular for assessing generated text. However, this
  paper investigates whether these metrics exhibit a bias toward texts generated by
  the same underlying language model.
---

# LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores

## Quick Facts
- arXiv ID: 2311.09766
- Source URL: https://arxiv.org/abs/2311.09766
- Reference count: 12
- Key outcome: Language model-based evaluation metrics exhibit significant bias toward texts generated by the same underlying model, particularly in reference-free settings.

## Executive Summary
This paper investigates a critical bias in popular language model-based evaluation metrics (BARTScore, T5Score, GPTScore) that causes them to favor summaries generated by the same underlying model architecture. Through systematic experiments across multiple model families and configurations, the authors demonstrate that evaluators score their own generated outputs significantly higher than those from other models. This bias is particularly pronounced when generator and evaluator share matching fine-tuning configurations and is amplified in reference-free evaluation settings. The findings reveal that current evaluation protocols may be unreliable for comparing different summarization systems.

## Method Summary
The study systematically evaluates 18 different language model generators (BART, T5, GPT-2, GPT-3 variants) and their corresponding evaluation metrics across two summarization datasets (CNN/DM and XSUM). Summaries are generated for 500 documents per dataset and scored using both reference-free and reference-based evaluation modes. The authors normalize all scores to a 0-1 scale and compute Spearman and Kendall correlations with human evaluations from the RoSE and SummEval benchmarks. Heatmaps are used to visualize bias patterns across different generator-evaluator combinations.

## Key Results
- Evaluators assign significantly higher scores to summaries generated by the same model family and fine-tuning configuration
- The bias is 2-3× more pronounced in reference-free evaluation compared to reference-based evaluation
- Evaluators show strong preference for longer summaries, contributing to the observed bias
- Matching model sizes and fine-tuning datasets between generator and evaluator amplifies the bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative evaluators assign higher scores to summaries generated by the same underlying model.
- Mechanism: The evaluation metrics compute scores based on the conditional probability of the summary being generated by the model. When the evaluator and generator share the same architecture, training data, and fine-tuning configuration, the evaluator implicitly recognizes and favors the statistical patterns it produced during generation.
- Core assumption: The conditional probability estimation inherently captures model-specific generation patterns, not just text quality.
- Evidence anchors:
  - [abstract] "Our findings unveil a latent bias, particularly pronounced when such evaluation metrics are used in a reference-free manner"
  - [section 4.1] "we observe darker cells along the diagonal line... This indicates potential evaluator bias towards their respective model"
- Break condition: If the evaluation uses different architectures, training datasets, or fine-tuning configurations, the bias diminishes significantly.

### Mechanism 2
- Claim: The bias is more pronounced when generator and evaluator have matching fine-tuning configurations and model sizes.
- Mechanism: Matching fine-tuning configurations create identical statistical distributions in the generated text and the evaluator's probability space, amplifying recognition bias.
- Core assumption: Fine-tuning on the same dataset creates overlapping statistical representations that the evaluator can detect.
- Evidence anchors:
  - [abstract] "This bias becomes more pronounced when the fine-tuning configuration and model size match for both the generator and evaluator"
  - [section 4.1] "Notably, this pattern differs for T5 generators and evaluators that are fine-tuned on XSUM"
- Break condition: Using evaluators fine-tuned on different datasets (e.g., CNN vs XSUM) reduces the bias.

### Mechanism 3
- Claim: Reference-free setting amplifies the same-model bias compared to reference-based setting.
- Mechanism: In reference-free evaluation, the evaluator scores summaries based solely on their compatibility with the source document and the evaluator's own generation patterns. Without gold references to anchor against, the evaluator's internal model preferences dominate.
- Core assumption: Reference-free evaluation lacks external quality anchors, making internal model bias more influential.
- Evidence anchors:
  - [abstract] "particularly pronounced when such evaluation metrics are used in a reference-free manner without leveraging gold summaries"
  - [section 4.1] "this bias is notably more pronounced in the reference-free setting"
- Break condition: Reference-based evaluation with gold summaries provides external quality anchors that mitigate the bias.

## Foundational Learning

- Concept: Conditional probability in text generation
  - Why needed here: The evaluation metrics are based on computing the conditional probability of generated text, which is central to understanding how bias emerges
  - Quick check question: If a summary has higher conditional probability under model A, does this necessarily mean it's higher quality, or could it reflect model-specific patterns?

- Concept: Fine-tuning and statistical alignment
  - Why needed here: Understanding how fine-tuning on specific datasets creates statistical patterns that evaluators can recognize is crucial for grasping the matching configuration bias
  - Quick check question: If two models are fine-tuned on different summarization datasets, how would their generated summaries statistically differ?

- Concept: Reference-free vs reference-based evaluation paradigms
  - Why needed here: The paper shows that bias manifests differently in these two settings, so understanding their fundamental differences is essential
  - Quick check question: In reference-free evaluation, what external quality signals are absent that are present in reference-based evaluation?

## Architecture Onboarding

- Component map: Generators (BART, T5, GPT-2, GPT-3 variants) -> Summaries -> Evaluators (BARTScore, T5Score, GPTScore) -> Normalized scores -> Bias analysis
- Critical path: Generate summaries → Normalize scores → Analyze bias patterns → Validate with human correlation
- Design tradeoffs: Using the same model family for both generation and evaluation simplifies implementation but introduces bias; using different model families reduces bias but may decrease evaluation relevance
- Failure signatures: High correlation between evaluator scores and summary length indicates length bias; diagonal patterns in heatmaps indicate same-model bias
- First 3 experiments:
  1. Generate summaries using different model variants (base, large, fine-tuned) and evaluate them with their corresponding evaluators
  2. Compare reference-free vs reference-based evaluation scores to quantify bias amplification
  3. Compute Spearman correlation between summary length and evaluator scores to detect length bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design evaluation metrics that are robust to biases towards both the same model and summary length?
- Basis in paper: [explicit] The paper discusses biases towards the same model and longer summaries, but does not propose solutions to mitigate these biases.
- Why unresolved: While the paper identifies these biases, it does not explore potential methods or approaches to address them in future evaluation metrics.
- What evidence would resolve it: Experiments comparing new evaluation metrics designed to be robust to these biases against existing metrics on the same datasets would provide evidence.

### Open Question 2
- Question: What are the underlying mechanisms causing language models to prefer summaries generated by the same model?
- Basis in paper: [inferred] The paper observes this bias but does not investigate the reasons behind it.
- Why unresolved: The paper identifies the bias but does not delve into the theoretical or empirical reasons for why language models exhibit this preference.
- What evidence would resolve it: Detailed analyses of model outputs and evaluation scores, possibly including ablation studies or probing tasks, could reveal the mechanisms behind this bias.

### Open Question 3
- Question: How does the same model bias affect the evaluation of other natural language generation tasks beyond summarization?
- Basis in paper: [explicit] The paper focuses on summarization but mentions that the same model bias is observed in other tasks.
- Why unresolved: The study is limited to summarization, and the extent of the bias in other tasks is not explored.
- What evidence would resolve it: Conducting similar experiments on a variety of natural language generation tasks, such as machine translation or dialogue generation, would provide insights into the generalizability of the bias.

### Open Question 4
- Question: What are the implications of the same model bias for the development and deployment of language models in real-world applications?
- Basis in paper: [inferred] The paper discusses the need for more reliable evaluation metrics but does not explore the broader implications of the bias.
- Why unresolved: The paper does not address how this bias might impact the trustworthiness and effectiveness of language models in practical applications.
- What evidence would resolve it: Case studies or experiments evaluating the performance of language models in real-world scenarios, considering the presence of the same model bias, would highlight its implications.

## Limitations

- The study focuses primarily on summarization tasks and may not generalize to all NLG domains
- The term "narcissistic" is metaphorical and not empirically validated as a psychological mechanism
- The paper does not provide concrete solutions for mitigating the identified biases
- Evaluation is limited to specific model families and sizes available in the public domain

## Confidence

- High confidence: Same-model bias exists and is measurable across multiple model families
- Medium confidence: Fine-tuning matching amplifies bias (based on observed patterns but requires controlled ablation)
- Low confidence: The "narcissistic" characterization as an explanatory mechanism (metaphorical framing, not empirically validated)

## Next Checks

1. Ablation study: Compare bias magnitude when using evaluators fine-tuned on different datasets versus same dataset as generators
2. Statistical control: Measure bias after controlling for summary length to isolate pure model preference effects
3. Cross-architecture validation: Test whether bias persists when comparing models from different families (e.g., BART generator with T5 evaluator)