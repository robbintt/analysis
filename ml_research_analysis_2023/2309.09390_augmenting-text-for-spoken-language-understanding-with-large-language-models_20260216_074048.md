---
ver: rpa2
title: Augmenting text for spoken language understanding with Large Language Models
arxiv_id: '2309.09390'
source_url: https://arxiv.org/abs/2309.09390
tags:
- data
- text
- speech
- generate
- unpaired
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of training spoken semantic\
  \ parsing models without requiring expensive matched speech-transcript-semantic\
  \ parse triplets. The authors propose leveraging unpaired text data\u2014either\
  \ from existing corpora or generated by large language models (LLMs)\u2014to augment\
  \ training."
---

# Augmenting text for spoken language understanding with Large Language Models

## Quick Facts
- arXiv ID: 2309.09390
- Source URL: https://arxiv.org/abs/2309.09390
- Authors: 
- Reference count: 0
- Primary result: Unpaired text data, whether from corpora or LLM-generated, significantly improves spoken semantic parsing performance by 1.4-30% in Exact Match accuracy

## Executive Summary
This paper addresses the challenge of training spoken semantic parsing models without requiring expensive matched speech-transcript-semantic parse triplets. The authors propose leveraging unpaired text data—either from existing corpora or generated by large language models (LLMs)—to augment training. They compare two methods for representing unpaired text: Joint Audio-Text (JAT), which uses mean speech embeddings, and Text-to-Speech (TTS), which synthesizes speech from text. Experiments on the STOP dataset show that unpaired text improves Exact Match (EM) by 2% for existing domains and 30% for new domains. Additionally, they prompt Llama 2.0 to generate unpaired text, achieving a 1.4% EM improvement for existing domains and 2.6% for new domains when combined with JAT or TTS. TTS outperforms JAT in new domains, likely due to better handling of domain-specific terms.

## Method Summary
The method involves training a deliberation model that combines ASR and SLU in a two-pass system. For unpaired text representation, JAT computes mean speech embeddings from paired data while TTS synthesizes speech directly from text. The authors also use Llama 2.0 to generate unpaired text data for existing and new domains using intent-word-based and exemplar-based prompting. The generated text is then represented using either JAT or TTS embeddings. The model is trained on paired data combined with unpaired text data, and performance is evaluated using Exact Match accuracy on the STOP dataset.

## Key Results
- Unpaired text from existing domains improves EM by 2% for existing domains and 30% for new domains
- LLM-generated text with JAT/TTS improves EM by 1.4% for existing domains and 2.6% for new domains
- TTS outperforms JAT in new domains, likely due to better handling of domain-specific terms
- JAT is computationally cheaper but less effective for new domains compared to TTS

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint Audio-Text (JAT) training improves spoken semantic parsing by providing speech representations for unpaired text using average embeddings from paired data.
- Mechanism: JAT leverages the statistical similarity between speech embeddings from paired data to create approximate speech representations for unpaired text. These approximate representations can be used in end-to-end models that require both speech and text inputs.
- Core assumption: The distribution of speech embeddings from paired data is representative enough of the speech space to serve as proxies for unseen text.
- Evidence anchors: Experiments on the STOP dataset show that unpaired text from existing and new domains improves performance by 2% and 30% in absolute Exact Match (EM) respectively.
- Break condition: If the paired speech data does not cover the acoustic space of the unpaired text (e.g., different accents, noise conditions), the average embeddings become poor proxies.

### Mechanism 2
- Claim: Text-to-Speech (TTS) synthesis provides more accurate speech representations for unpaired text than JAT, especially for new domains with domain-specific terms.
- Mechanism: TTS converts unpaired text directly into synthetic speech, preserving phonetic and acoustic properties that are lost in averaging. This produces more informative speech embeddings for the semantic parser.
- Core assumption: The TTS model can accurately pronounce domain-specific terms and handle the linguistic characteristics of the unpaired text.
- Evidence anchors: TTS outperforms JAT in new domains, likely due to better handling of domain-specific terms.
- Break condition: If the TTS model mispronounces domain-specific terms or produces unnatural prosody, the resulting embeddings may confuse the parser.

### Mechanism 3
- Claim: LLM-generated text data, when combined with pseudo-labeling, expands the training distribution and improves model robustness.
- Mechanism: LLMs generate diverse utterances matching target intents, and pseudo-labeling provides semantic parses without manual annotation. This synthetic data augments the limited paired dataset.
- Core assumption: The LLM can generate utterances that are both semantically meaningful and diverse enough to improve generalization.
- Evidence anchors: Using generated text with JAT and TTS for spoken semantic parsing improves EM on STOP by 1.4% and 2.6% absolute for existing and new domains respectively.
- Break condition: If the LLM generates unrealistic or out-of-distribution utterances, the pseudo-labels may be incorrect, introducing noise rather than signal.

## Foundational Learning

- Concept: Semantic parsing
  - Why needed here: The task requires converting speech to structured meaning representations (semantic parses), not just text transcripts.
  - Quick check question: What is the difference between SLU and SLU with semantic parsing?

- Concept: Deliberation models
  - Why needed here: The architecture combines ASR and SLU in a two-pass system, requiring understanding of both components and their interaction.
  - Quick check question: How does the fusion module in a deliberation model combine audio and text embeddings?

- Concept: Text-to-speech synthesis
  - Why needed here: TTS is used to generate speech representations for unpaired text, requiring knowledge of how to extract useful features from synthetic audio.
  - Quick check question: What speech features are most important for semantic parsing when using TTS-generated audio?

## Architecture Onboarding

- Component map: ASR module (frozen RNNT encoder and predictor) -> Fusion module (Multi-Head Attention) -> Decoder module (Transformer with pointer-generator) -> JAT/TTS module (for unpaired text representation) -> LLM generation pipeline (for synthetic data)
- Critical path: ASR → Fusion → Decoder, with JAT/TTS providing speech embeddings for unpaired text
- Design tradeoffs:
  - JAT: computationally cheap but less accurate for new domains
  - TTS: more accurate but computationally expensive
  - LLM generation: scalable but quality depends on prompting strategy
- Failure signatures:
  - JAT fails when paired data doesn't cover unpaired text domain
  - TTS fails when domain-specific terms are mispronounced
  - LLM generation fails when prompts don't constrain output diversity
- First 3 experiments:
  1. Compare JAT vs TTS performance on a small subset of STOP with controlled unpaired text
  2. Test different LLM prompting strategies (IWP vs EP) on a held-out validation set
  3. Measure the impact of unpaired text quantity on EM for both existing and new domains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the different prompt strategies for LLM-generated data compare in terms of Intent Match Accuracy (IMA) and Exact Match (EM) for new domains?
- Basis in paper: The paper mentions that EP-based prompting has a lower IMA compared to IWP, but combining data from both strategies improves EM.
- Why unresolved: The paper does not provide a detailed comparison of the impact of IWP and EP on new domains, especially in terms of IMA and EM.
- What evidence would resolve it: Experiments comparing the performance of IWP and EP on new domains, including IMA and EM metrics, would provide clarity on their effectiveness.

### Open Question 2
- Question: What is the optimal amount of unpaired text data required to achieve maximum performance gains in spoken semantic parsing?
- Basis in paper: The paper shows that performance improves with increasing unpaired text data but saturates at a certain point.
- Why unresolved: The paper does not specify the exact point of saturation or the optimal amount of unpaired text data for maximum performance gains.
- What evidence would resolve it: Experiments varying the amount of unpaired text data and measuring performance would identify the optimal amount for maximum gains.

### Open Question 3
- Question: How does the performance of LLM-generated data compare to real data in terms of Exact Match (EM) for new domains?
- Basis in paper: The paper shows that LLM-generated data improves EM by 2.3 points absolute over a baseline but lags behind the topline using real data.
- Why unresolved: The paper does not provide a detailed comparison of the performance of LLM-generated data versus real data in terms of EM for new domains.
- What evidence would resolve it: Experiments comparing the EM of LLM-generated data and real data for new domains would provide insights into their relative performance.

## Limitations

- The effectiveness of Joint Audio-Text (JAT) depends entirely on the paired speech data adequately representing the acoustic space of unpaired text - this distributional assumption is never validated
- The paper doesn't address critical practical concerns like computational overhead of TTS synthesis at scale or quality control mechanisms for LLM-generated pseudo-labels
- Claims about why TTS outperforms JAT in new domains (better handling of domain-specific terms) are speculative and lack empirical validation

## Confidence

**High Confidence (5/5)**: The experimental methodology and evaluation metrics are clearly specified. The baseline results on STOP dataset are reproducible and the comparison between JAT and TTS for paired vs unpaired text scenarios is methodologically sound.

**Medium Confidence (3/5)**: The claims about LLM-generated data improvements are moderately supported but limited by the narrow scope of prompting strategies tested. The 1.4% and 2.6% EM improvements are statistically significant but may not generalize to other datasets or LLMs.

**Low Confidence (2/5)**: The paper's claims about why TTS outperforms JAT in new domains (better handling of domain-specific terms) are speculative and lack empirical validation. The distributional assumptions underlying JAT are stated but never tested.

## Next Checks

1. **Distributional Coverage Validation**: Design an experiment that systematically measures the distributional mismatch between paired speech embeddings used in JAT and the actual speech space of unpaired text. Use techniques like t-SNE visualization or KL divergence to quantify this mismatch across domains.

2. **TTS Pronunciation Robustness Test**: Create a controlled experiment with domain-specific terms that have multiple valid pronunciations. Compare TTS performance when using domain-specific lexicons versus generic pronunciation models to isolate the effect of pronunciation accuracy on semantic parsing performance.

3. **LLM Generalization Study**: Replicate the LLM generation experiments using at least three different LLMs (e.g., Llama 2.0, GPT-3.5, Claude) with varied prompting strategies. Analyze which aspects of the LLM architecture and prompting most strongly correlate with successful semantic parsing improvements.