---
ver: rpa2
title: 'MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading Comprehension'
arxiv_id: '2310.18167'
source_url: https://arxiv.org/abs/2310.18167
tags:
- prompt
- prompts
- arxiv
- context
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multi-level prompt tuning (MPrompt) approach
  for machine reading comprehension that utilizes task-specific, domain-specific,
  and context-specific prompts to improve answer generation. MPrompt incorporates
  context-related knowledge from a small-scale PLM into prompt generation to enhance
  contextual relevance.
---

# MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading Comprehension

## Quick Facts
- arXiv ID: 2310.18167
- Source URL: https://arxiv.org/abs/2310.18167
- Reference count: 13
- Primary result: Multi-level prompt tuning achieves 1.94% average improvement over soft prompt methods on 12 benchmark QA datasets

## Executive Summary
MPrompt introduces a multi-level prompt tuning approach for machine reading comprehension that utilizes task-specific, domain-specific, and context-specific prompts to improve answer generation. The method incorporates context-related knowledge from a small-scale PLM into prompt generation to enhance contextual relevance. Experimental results demonstrate that MPrompt achieves state-of-the-art performance across 12 benchmark datasets, matching and even surpassing fine-tuning performance on most tasks.

## Method Summary
MPrompt employs three levels of prompts: task-specific (shared across all examples), domain-specific (shared within clustered contexts), and context-specific (unique per context). The method uses K-means clustering to group contexts into domains, with domain-specific prompts constrained by an independence measure (CKA) to avoid redundancy. A prompt generator based on a small-scale PLM enriches context-specific prompts with external semantic knowledge. All prompts are integrated into the encoder of a generative QA model, with only prompt parameters being trained.

## Key Results
- Achieves 1.94% average improvement over state-of-the-art soft prompt methods
- Matches and surpasses fine-tuning performance on most benchmark tasks
- Demonstrates effectiveness of multi-level prompts and context-awareness for enhanced comprehension and semantics

## Why This Works (Mechanism)

### Mechanism 1
Multi-level prompts (task, domain, context) allow fine-grained semantic control over the input to the language model, improving answer quality. Task-specific prompts provide general task direction; domain-specific prompts encode shared knowledge within clustered contexts; context-specific prompts add per-instance fine-tuning. Combined, they give the model multiple semantic lenses, which improves contextual understanding.

### Mechanism 2
Independence constraint via CKA reduces redundancy between domain-specific prompts, forcing each prompt to focus on intra-domain knowledge. HSIC/CKA measures statistical dependence between prompt embeddings from different domains. Minimizing CKA forces the prompts to be more independent, thereby encouraging each to encode unique domain-specific semantics rather than overlapping information.

### Mechanism 3
The prompt generator, using a small PLM, enriches context-specific prompts with external semantic knowledge, improving contextual sensitivity. The encoder of the small PLM processes the context and passes its hidden states to the decoder, which conditions the context-specific prompt generation on the domain-specific prompt and the context. This injects external semantic cues into the prompt.

## Foundational Learning

- Concept: Task-specific vs. input-dependent prompts
  - Why needed here: The paper contrasts standard input-independent prompts with dynamic, context-aware ones. Understanding the difference is essential to see why MPrompt's multi-level approach works.
  - Quick check question: What is the main limitation of input-independent prompt tuning in reading comprehension tasks?

- Concept: Clustering for domain identification
  - Why needed here: MPrompt clusters contexts into domains to assign domain-specific prompts. Understanding clustering and its impact on semantic separation is key to grasping the domain-specific mechanism.
  - Quick check question: Why does the paper use Kmeans clustering on context embeddings instead of labeling domains manually?

- Concept: Independence constraints (HSIC, CKA)
  - Why needed here: These metrics are used to enforce independence between domain-specific prompts. Knowing what they measure and how they differ is crucial to understanding the independence constraint mechanism.
  - Quick check question: What does a CKA value of 0 signify about two domain-specific prompts?

## Architecture Onboarding

- Component map:
  - Prompt Generator (small PLM encoder-decoder) -> Encodes context -> passes knowledge to decoder -> Decoder combines domain-specific prompt + context-specific prompt -> generates key-value pairs
  - Multi-level Prompts (task, domain, context) -> Task-specific: shared across all examples in task -> Domain-specific: shared within domain cluster -> Context-specific: unique per context
  - Generative QA Model (e.g., T5-base/large/XL) -> Receives augmented key-values from multi-level prompts -> generates answers conditioned on input

- Critical path:
  1. Cluster contexts -> obtain domain labels
  2. Initialize task-specific, domain-specific, context-specific prompts
  3. For each batch:
     - Generate domain-specific and context-specific prompts via prompt generator
     - Augment attention layers with prompt key-values
     - Compute loss (NLL + 位 * Lidp)
     - Update only prompt parameters
  4. Evaluate on test set

- Design tradeoffs:
  - Using small PLM in prompt generator reduces cost but may limit knowledge richness
  - Independence constraint adds regularization but increases computation (sampling m domain pairs)
  - Multi-level prompts increase parameter count but keep it far below full fine-tuning

- Failure signatures:
  - Performance drops when independence constraint 位 is too high (over-regularization)
  - Prompt length tuning is critical; too short may underfit, too long may overfit or hurt generalization
  - If clustering yields too few or too many domains, prompt specificity may degrade

- First 3 experiments:
  1. Run with only task-specific prompts (baseline) -> measure drop vs full MPrompt
  2. Vary 位 (independence constraint weight) -> find optimal value per dataset
  3. Test different domain cluster counts (e.g., 3, 6, 9) -> observe impact on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt lengths affect the performance of MPrompt across various QA tasks, and what is the optimal prompt length for each task?
- Basis in paper: The paper mentions that the length of prompts is a key factor that affects model performance and provides some evaluation of prompt length on the UnifiedQA-base model.
- Why unresolved: The paper does not provide a comprehensive analysis of how different prompt lengths affect the performance across all tasks and model scales. It also does not determine the optimal prompt length for each task.
- What evidence would resolve it: A systematic study varying prompt lengths across all tasks and model scales, identifying the optimal prompt length for each task and analyzing the trade-offs between prompt length and performance.

### Open Question 2
- Question: How does the independence constraint in MPrompt prevent information redundancy in domain-specific prompts, and can this constraint be further improved?
- Basis in paper: The paper introduces the independence constraint to steer domain-specific prompts to focus on intra-domain information and avoid information redundancy. It also mentions that computing the pair-wise independence requires n(n-1)/2 iterations, which is slow for large n.
- Why unresolved: The paper does not provide a detailed analysis of how the independence constraint prevents information redundancy and whether there are other ways to improve this constraint. It also does not address the computational cost of the independence constraint.
- What evidence would resolve it: An analysis of the effectiveness of the independence constraint in preventing information redundancy and a comparison with other potential constraints. Additionally, an exploration of more efficient methods to compute the independence constraint.

### Open Question 3
- Question: How does the prompt generator in MPrompt integrate context-related knowledge from a small-scale PLM, and what is the impact of using different scales of PLMs in the prompt generator?
- Basis in paper: The paper mentions that the prompt generator extracts context-related knowledge from a small-scale PLM and integrates it into the prompt generation process. It also evaluates the impact of using different scales of PLMs in the prompt generator.
- Why unresolved: The paper does not provide a detailed explanation of how the prompt generator integrates context-related knowledge from the small-scale PLM. It also does not explore the impact of using different scales of PLMs in the prompt generator on the performance of MPrompt.
- What evidence would resolve it: A detailed analysis of the integration process of context-related knowledge from the small-scale PLM into the prompt generation process. Additionally, a comparison of the performance of MPrompt using different scales of PLMs in the prompt generator.

## Limitations
- Lack of statistical significance tests makes it difficult to assess whether the 1.94% average improvement is consistent across datasets
- Sparse implementation details for the prompt generator's integration with the small PLM
- Strong simplification of using only 3 clusters for domain clustering may not capture full semantic diversity
- Computationally expensive independence constraint requiring sampling of domain pairs

## Confidence
- High confidence in the theoretical framework and architectural design
- Medium confidence in the empirical results due to lack of statistical significance testing
- Low confidence in the practical scalability of the approach

## Next Checks
1. Conduct statistical significance tests (e.g., paired t-tests) on the 12 benchmark datasets to determine if the 1.94% average improvement is statistically reliable
2. Perform an ablation study varying the number of domain clusters (e.g., 3, 6, 9) and the independence constraint weight 位
3. Test MPrompt on a held-out dataset not used in the original experiments to assess generalization beyond the 12 benchmark datasets