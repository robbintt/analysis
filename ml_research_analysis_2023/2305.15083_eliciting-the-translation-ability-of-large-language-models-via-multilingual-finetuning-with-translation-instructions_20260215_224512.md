---
ver: rpa2
title: Eliciting the Translation Ability of Large Language Models via Multilingual
  Finetuning with Translation Instructions
arxiv_id: '2305.15083'
source_url: https://arxiv.org/abs/2305.15083
tags:
- language
- translation
- pairs
- mfti
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) have shown remarkable multilingual
  translation abilities without explicit parallel training. This work investigates
  their translation potential by fine-tuning a multilingual LLM (XGLM-7B) using multilingual
  translation instructions.
---

# Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions

## Quick Facts
- **arXiv ID:** 2305.15083
- **Source URL:** https://arxiv.org/abs/2305.15083
- **Reference count:** 7
- **Primary result:** mFTI improves LLM translation ability, outperforming 8-shot ICL by 3+ BLEU on average

## Executive Summary
This paper investigates how multilingual large language models can be better utilized for translation tasks through multilingual fine-tuning with translation instructions (mFTI). The authors demonstrate that XGLM-7B, when fine-tuned on parallel sentences organized as translation instructions, significantly outperforms traditional in-context learning approaches. The method not only improves translation quality for seen language pairs but also enables the model to follow translation instructions for unseen language pairs, achieving up to 1.7 BLEU improvement. The study reveals that translation performance correlates with both pretraining data amount and language similarity to English, with similarity being the more significant factor.

## Method Summary
The authors fine-tune XGLM-7B using multilingual translation instructions by organizing parallel sentences from WikiMatrix and MultiCCAligned into instruction templates. The model is trained using next-token prediction with Adam optimizer (lr=5e-6, batch size=80) for 1 epoch or 2000 steps. Evaluation is performed on FLORES-101 test set, comparing mFTI against 8-shot in-context learning baseline. The approach uses 1000 parallel sentences per language pair across 13 languages, organized in instruction format to teach both task recognition and execution.

## Key Results
- mFTI outperforms 8-shot ICL by 3+ BLEU on average across 13 languages
- Improves instruction-following for unseen language pairs by up to 1.7 BLEU
- Translation performance correlates with language similarity to English more than pretraining data amount
- Successfully addresses common translation issues: source copy, off-target translation, over/under translation, and oscillatory hallucination
- Approach scales effectively with model size and training examples

## Why This Works (Mechanism)

### Mechanism 1: Explicit Alignment Learning
Fine-tuning on thousands of parallel sentences enables robust cross-lingual alignment learning, outperforming the limited pattern matching from few in-context examples.

### Mechanism 2: Meta-Task Generalization
Training on translation instructions across multiple language pairs teaches the model to recognize and execute translation as a meta-task, enabling zero-shot generalization to unseen pairs.

### Mechanism 3: Pivot-Based Direct Alignment
Training on X→En and En→Y pairs enables the model to learn direct X→Y translation without explicit pivoting, improving zero-shot translation quality.

## Foundational Learning

- **Multilingual language modeling with cross-lingual alignment**
  - Why needed: XGLM's pretraining provides foundation for mFTI; understanding multilingual representations is crucial
  - Quick check: How does XGLM's 500B token pretraining across 30 languages enable subsequent translation finetuning?

- **Instruction tuning and meta-learning**
  - Why needed: mFTI is instruction tuning applied to translation; understanding this framework is key to its success
  - Quick check: What's the difference between in-context learning and instruction tuning, and why does instruction tuning work better for translation?

- **Zero-shot and few-shot learning in LLMs**
  - Why needed: Analysis of unseen language pairs requires understanding LLM generalization from limited training data
  - Quick check: Why can mFTI improve translation for language pairs never seen during finetuning?

## Architecture Onboarding

- **Component map:** XGLM-7B (multilingual LLM) → WikiMatrix/MultiCCAligned (parallel corpus) → Instruction templates → Adam optimizer (lr=5e-6) → FLORES-101 evaluation
- **Critical path:** Parallel corpus extraction → Instruction template formatting → mFTI fine-tuning → BLEU evaluation
- **Design tradeoffs:** Larger finetuning corpus improves quality but increases cost; more language pairs improve generalization but may dilute per-language performance
- **Failure signatures:** Poor BLEU on high-similarity languages (template issues); large gap between seen/unseen pairs (poor generalization)
- **First 3 experiments:**
  1. Run mFTI with 1000 sentences per language pair on 5 language pairs, compare to 8-shot ICL
  2. Test instruction-following on held-out language pairs to measure zero-shot generalization
  3. Vary number of language pairs to find sweet spot between breadth and depth

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do mFTI scaling laws compare to other instruction-tuning approaches, and at what point does improvement plateau?
- Basis: Paper shows log-linear scaling but lacks comparative analysis with other methods
- Resolution needed: Comparative scaling studies with varying model sizes and training data

### Open Question 2
- Question: What specific linguistic features in pretraining data most strongly correlate with successful translation performance?
- Basis: Paper identifies correlations but doesn't analyze specific linguistic patterns
- Resolution needed: Detailed linguistic analysis of pretraining data features

### Open Question 3
- Question: How does mFTI perform on extremely low-resource language pairs, and what techniques could enhance effectiveness?
- Basis: Paper focuses on 13 languages but doesn't address extremely low-resource scenarios
- Resolution needed: Experiments on extremely low-resource languages with enhancement techniques

## Limitations
- Parallel corpus quality varies due to random selection without detailed filtering criteria
- Instruction template design space not systematically explored beyond Appendix variations
- Evaluation relies primarily on BLEU scores, potentially missing nuanced quality aspects
- Results limited to XGLM-7B architecture, raising generalizability questions

## Confidence

**High Confidence:**
- mFTI outperforms 8-shot ICL by 3+ BLEU (direct experimental comparison)
- Language similarity more significant than pretraining data volume (correlation analysis)
- Approach scales with model size and training examples (ablation studies)

**Medium Confidence:**
- Improves instruction-following for unseen pairs (limited experiments, mechanism unclear)
- Pivot-based alignment achieved (improvement shown, causal mechanism inferred)
- Adding more language pairs improves performance (experiments support but ratios unclear)

**Low Confidence:**
- Exact template variations' impact (limited exploration in Appendix)
- Generalizability to other architectures (only tested on XGLM-7B)
- Long-term stability of improvements (no temporal analysis)

## Next Checks

1. **Corpus Quality Impact Study:** Systematically vary parallel sentence quality during mFTI and measure relationship between corpus quality and BLEU improvements across language pairs.

2. **Template Design Space Exploration:** Conduct comprehensive ablation study testing 10+ instruction template variations to identify optimal characteristics and their relative impact on translation quality.

3. **Cross-Architecture Generalization Test:** Apply mFTI approach to at least two additional multilingual LLM architectures and compare translation performance gains to establish generalizability.