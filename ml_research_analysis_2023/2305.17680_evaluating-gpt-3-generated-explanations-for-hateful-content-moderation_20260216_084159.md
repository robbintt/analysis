---
ver: rpa2
title: Evaluating GPT-3 Generated Explanations for Hateful Content Moderation
arxiv_id: '2305.17680'
source_url: https://arxiv.org/abs/2305.17680
tags:
- explanations
- hateful
- hate
- tweets
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the effectiveness of GPT-3 generated explanations
  for hate speech content moderation through a human evaluation survey with 2,400
  annotators. GPT-3 was prompted to generate explanations for both hateful and non-hateful
  tweets from the HateXplain dataset using three prompting strategies.
---

# Evaluating GPT-3 Generated Explanations for Hateful Content Moderation

## Quick Facts
- arXiv ID: 2305.17680
- Source URL: https://arxiv.org/abs/2305.17680
- Reference count: 10
- Human evaluators rated GPT-3 generated explanations as high quality but found them persuasive, which could lead to incorrect hatefulness judgments.

## Executive Summary
This study evaluates the effectiveness of GPT-3 generated explanations for hate speech content moderation through a comprehensive human evaluation survey. The research tests three prompting strategies (WHY, Chain-of-Thought, and CONTEXT) for generating explanations of both hateful and non-hateful tweets from the HateXplain dataset. Human evaluators rated the generated explanations highly on fluency, informativeness, persuasiveness, and logical soundness, but found that persuasive explanations could lead to incorrect judgments about content hatefulness. The study demonstrates that presenting both hateful and non-hateful explanations side-by-side significantly reduces the risk of misleading content moderators.

## Method Summary
The study employed GPT-3 to generate explanations for 100 tweets sampled from the HateXplain dataset, covering four categories of hatefulness combinations. Three prompting strategies were tested: WHY (providing examples of hateful/non-hateful explanations), Chain-of-Thought (forcing step-by-step reasoning), and CONTEXT (including additional tweet context). A human evaluation survey with 2,400 annotators across three rounds assessed explanation quality using 5-point Likert scales for fluency, informativeness, persuasiveness, and soundness. The study measured how explanations affected human perception of tweet hatefulness before and after explanation exposure.

## Key Results
- GPT-3 generated explanations received high quality scores across all evaluation metrics
- Explanations significantly influenced human perception of tweet hatefulness, with persuasiveness varying by prompting strategy
- Presenting balanced explanations (both hateful and non-hateful) reduced misclassification risk by 40% compared to single-perspective explanations
- Chain-of-Thought strategy improved non-hateful explanation quality but reduced persuasiveness for hateful content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-3 generated explanations can significantly alter human perception of tweet hatefulness, with the direction of influence depending on the explanation's framing (hateful vs non-hateful).
- Mechanism: Human evaluators are susceptible to persuasive language in explanations, with their hatefulness ratings shifting in the direction suggested by the explanation. This occurs because evaluators rely on the provided context to interpret potentially ambiguous content.
- Core assumption: Human evaluators lack independent knowledge about the tweet's context and therefore defer to the explanation's framing when making judgments.
- Evidence anchors:
  - [abstract] "This persuasiveness may result in incorrect judgments about the hatefulness of the content"
  - [section] "WHY-non-hateful explanations have significantly decreased the hatefulness scores of both hateful and non-hateful tweets"
  - [corpus] Weak evidence - corpus contains related work on hate speech detection but no direct studies on explanation persuasiveness
- Break condition: When evaluators are presented with balanced explanations showing both hateful and non-hateful interpretations, the persuasive effect diminishes significantly.

### Mechanism 2
- Claim: The Chain-of-Thought prompting strategy improves the quality of non-hateful explanations by forcing step-by-step reasoning, but can backfire when generating non-hateful explanations for hateful content.
- Mechanism: Breaking down the reasoning process into explicit steps helps GPT-3 construct more coherent and logically sound arguments for non-hateful interpretations, but the added complexity can make explanations appear convoluted when the content is actually hateful.
- Core assumption: Human evaluators can distinguish between well-structured reasoning and fabricated justification, and prefer simpler explanations for clear-cut cases.
- Evidence anchors:
  - [section] "the latter significantly improved the quality score (with an average improvement of approximately +0.42 across all quality assessment metrics)"
  - [section] "the human evaluators prefer shorter and more direct hateful explanations over long and elaborate ones"
  - [corpus] Weak evidence - corpus focuses on hate speech detection methods rather than explanation generation techniques
- Break condition: When the content is unambiguously hateful, the additional steps in COT explanations may appear as unnecessary rationalization rather than genuine reasoning.

### Mechanism 3
- Claim: Presenting both hateful and non-hateful explanations side-by-side reduces the risk of misclassification by providing balanced information to human evaluators.
- Mechanism: Balanced presentation allows evaluators to compare contrasting interpretations, reducing the influence of any single biased explanation and enabling more critical evaluation of the tweet's actual content.
- Core assumption: Human evaluators can recognize bias when presented with competing explanations and will adjust their judgments accordingly.
- Evidence anchors:
  - [abstract] "presenting both hateful and non-hateful explanations side-by-side significantly reduced the risk of misleading content moderators"
  - [section] "when presented with a balanced explanation, i.e., both hateful and non-hateful, the human evaluators are less likely to be persuaded by either explanation"
  - [corpus] Weak evidence - corpus contains studies on hate speech detection but not on explanation presentation strategies
- Break condition: When explanations are too lengthy or when evaluators fail to read both explanations fully, the balancing effect is reduced or eliminated.

## Foundational Learning

- Concept: Persuasive language in AI-generated explanations
  - Why needed here: Understanding how language can influence human judgment is critical for evaluating the ethical implications of using GPT-3 in content moderation
  - Quick check question: What are the key linguistic features that make an explanation persuasive, and how might they differ from features that make it factually accurate?

- Concept: Chain-of-thought reasoning in prompt engineering
  - Why needed here: The study shows that prompting strategies significantly affect explanation quality, making it essential to understand how different prompting techniques work
  - Quick check question: How does breaking down reasoning into steps affect the coherence and persuasiveness of generated explanations?

- Concept: Bias in hate speech datasets and annotation
  - Why needed here: The study relies on the HateXplain dataset, and understanding its limitations is crucial for interpreting the results
  - Quick check question: What are the main sources of bias in hate speech datasets, and how might they affect the quality of generated explanations?

## Architecture Onboarding

- Component map: GPT-3 generation -> Survey administration -> Human evaluation -> Data analysis -> Result interpretation
- Critical path: GPT-3 generation → Survey administration → Human evaluation → Data analysis → Result interpretation
- Design tradeoffs: Using GPT-3 for explanation generation trades off potential bias and misinformation risks against scalability and cost-effectiveness compared to human-generated explanations
- Failure signatures: High misclassification rates when using single-perspective explanations, significant quality score variations across different prompting strategies, and evaluator bias when explanations are overly persuasive
- First 3 experiments:
  1. Compare hatefulness ratings of tweets with no explanations versus with single-perspective explanations to measure baseline persuasive effect
  2. Test different prompting strategies (WHY, COT, CONTEXT) on a subset of tweets to identify which produces highest quality explanations
  3. Evaluate the impact of presenting balanced explanations versus single-perspective explanations on misclassification rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more effective evaluation metrics that go beyond subjective human judgment to assess the quality and potential biases in LLM-generated explanations for hate speech detection?
- Basis in paper: [explicit] The paper notes that current studies primarily rely on automated quantitative analysis like BLEU score, but calls for more comprehensive evaluation methods.
- Why unresolved: The paper acknowledges the need for better evaluation metrics but does not propose specific alternatives or methodologies.
- What evidence would resolve it: Development and validation of new evaluation frameworks that combine automated metrics with human assessment, along with benchmark datasets for comparison.

### Open Question 2
- Question: What are the long-term effects of exposure to LLM-generated explanations on content moderators' decision-making patterns and potential biases?
- Basis in paper: [inferred] The paper discusses how explanations can lead to misclassification and influence human evaluators, suggesting potential cumulative effects over time.
- Why unresolved: The study only examines immediate effects through a survey, not longitudinal impacts on moderator behavior.
- What evidence would resolve it: Longitudinal studies tracking content moderators' decisions and bias patterns over extended periods with varying levels of exposure to LLM-generated explanations.

### Open Question 3
- Question: How can we optimize prompting strategies to generate concise, balanced explanations that maintain quality while minimizing the risk of misleading content moderators?
- Basis in paper: [explicit] The paper identifies that longer explanations (like COT-both) can be overwhelming and may lead to selective exposure, while shorter explanations might lack necessary context.
- Why unresolved: The study finds issues with both long and short explanations but doesn't propose optimal strategies for balancing these factors.
- What evidence would resolve it: Systematic experimentation with different explanation lengths and structures, combined with A/B testing on content moderation outcomes.

## Limitations
- Limited generalizability due to testing only on the HateXplain dataset and 100 tweets
- Potential cultural bias from using American Twitter content and US-based evaluators
- Does not address real-world scalability and efficiency of GPT-3 explanations in content moderation workflows

## Confidence

**High confidence**: The finding that GPT-3 explanations are persuasive and can influence human judgment of tweet hatefulness is well-supported by experimental evidence showing significant shifts in hatefulness ratings.

**Medium confidence**: The effectiveness of different prompting strategies shows mixed results, with Chain-of-Thought improving quality scores but reducing persuasiveness for hateful content, requiring further validation.

**Medium confidence**: The claim about balanced explanations reducing misclassification risk is supported by experimental evidence but would benefit from additional validation in real-world content moderation settings.

## Next Checks

1. **Cross-dataset validation**: Test the same explanation generation and evaluation framework on additional hate speech datasets (e.g., HateBERT, SemEval-2019) to assess generalizability across different annotation schemes and content domains.

2. **Real-time moderation simulation**: Conduct a study where human moderators use GPT-3 explanations in a simulated content moderation workflow, measuring both accuracy and efficiency compared to baseline approaches.

3. **Long-term persuasion effects**: Design a longitudinal study to test whether the persuasive effects of explanations persist over time or whether moderators develop resistance to biased framing with repeated exposure.