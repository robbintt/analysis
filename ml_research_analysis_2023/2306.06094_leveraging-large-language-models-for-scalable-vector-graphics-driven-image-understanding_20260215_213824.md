---
ver: rpa2
title: Leveraging Large Language Models for Scalable Vector Graphics-Driven Image
  Understanding
arxiv_id: '2306.06094'
source_url: https://arxiv.org/abs/2306.06094
tags:
- image
- query
- code
- example
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  understand and process images when given Scalable Vector Graphics (SVG) representations.
  The authors convert raster images into SVG format, which describes images using
  XML-based text that details shapes, colors, and relationships between elements.
---

# Leveraging Large Language Models for Scalable Vector Graphics-Driven Image Understanding

## Quick Facts
- arXiv ID: 2306.06094
- Source URL: https://arxiv.org/abs/2306.06094
- Reference count: 33
- One-line primary result: LLMs can perform visual tasks using SVG representations, showing robustness to distribution shifts and effectiveness in in-context learning

## Executive Summary
This paper investigates whether large language models (LLMs) can understand and process images when given Scalable Vector Graphics (SVG) representations. The authors convert raster images into SVG format, which describes images using XML-based text that details shapes, colors, and relationships between elements. They evaluate the LLM's capabilities across three main tasks: visual reasoning and question answering, image classification under distribution shift and few-shot learning, and generating new images using visual prompting. Their results show that LLMs can often perform well on these tasks, even without explicit visual components. Notably, SVG representations demonstrate robustness to distribution shifts and improved performance with in-context learning. The method also enables image generation and editing through interactive chat-based feedback. While the approach has limitations with photographic content, it offers promising initial results for integrating LLMs with SVG in visual tasks.

## Method Summary
The authors convert raster images to SVG format using curve tracing or segmentation-based approaches, then feed the SVG text into pre-trained LLMs (primarily GPT-4) as prompts. They employ zero-shot and in-context learning strategies, where example input-output pairs are included in the prompt to improve performance. For classification tasks, they fine-tune Vicuna-7B on MNIST. The approach leverages the structured, textual nature of SVG to bridge visual and textual modalities without requiring vision-specific model components.

## Key Results
- LLMs achieve competitive classification accuracy on MNIST variants using SVG representations, with in-context learning improving performance
- SVG representations demonstrate strong robustness to color distribution shifts in Colored-MNIST compared to traditional CNNs
- The method enables image generation and editing through interactive chat-based feedback using SVG prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can understand images when given SVG format because SVG is a textual, XML-based representation of visual primitives.
- Mechanism: SVG encodes shapes, colors, and spatial relationships as structured text, which LLMs already excel at processing due to their training on diverse textual data. This allows LLMs to "see" images through a language-like interface without requiring vision-specific components.
- Core assumption: The semantic meaning of visual elements (e.g., a circle or polygon) is preserved and interpretable when described in SVG text format.
- Evidence anchors:
  - [abstract] "By leveraging the XML-based textual descriptions of SVG representations instead of raster images, we aim to bridge the gap between the visual and textual modalities"
  - [section 2.1] "SVG format stores images as XML-based text files that define geometrical objects and their properties"
  - [corpus] Weak evidence: no direct comparisons of LLM performance on SVG vs. plain text descriptions of shapes.
- Break condition: If SVG abstraction loses critical visual information (e.g., texture, fine-grained details), LLM performance degrades significantly.

### Mechanism 2
- Claim: In-context learning improves image classification performance with SVG inputs.
- Mechanism: LLMs can leverage example SVG-image-label pairs provided in the prompt to infer patterns and generalize to new inputs, similar to few-shot learning in NLP.
- Core assumption: The LLM's learned reasoning and pattern-matching abilities transfer from language to visual domain when SVG is used.
- Evidence anchors:
  - [section 3.4] "With 1 sample per class, the test accuracy climbs to 28%, which is 8% higher than the zero-shot classification performance"
  - [section 2.2] "in-context learning has only recently been introduced to the vision domain... our paper directly applies in-context learning by feeding SVG data into the LLM"
  - [corpus] No direct evidence on whether in-context learning works better with SVG than with raster image embeddings.
- Break condition: If the LLM cannot extract relevant features from SVG or if the context examples are not representative.

### Mechanism 3
- Claim: SVG's shape-color disentanglement leads to robustness against distribution shifts like colored MNIST.
- Mechanism: SVG encodes shape and color in separate structural components, allowing the LLM to focus on shape for classification even when color varies, unlike CNNs which are biased toward color.
- Core assumption: The XML structure of SVG naturally separates shape primitives from color attributes, and the LLM can exploit this separation.
- Evidence anchors:
  - [section 3.4] "SVG can naturally be better at debiasing color from shape. In contrast, convolutional neural networks (CNN) are known to be biased toward color"
  - [section 3.4] "the performance of our model is much stronger, showing strong robustness" on Colored-MNIST
  - [corpus] No direct comparison with other disentangled representations (e.g., shape masks + color channels).
- Break condition: If color information is essential for the task or if the LLM does not attend to shape over color.

## Foundational Learning

- Concept: Scalable Vector Graphics (SVG) as a textual image format.
  - Why needed here: Understanding that SVG is not an image file but structured XML describing shapes, colors, and relationships is crucial to grasp how LLMs can process it.
  - Quick check question: What is the fundamental difference between SVG and raster image formats like PNG or JPEG?
- Concept: In-context learning in LLMs.
  - Why needed here: The paper relies on providing example input-output pairs in the prompt to improve performance, a key mechanism for few-shot learning.
  - Quick check question: How does in-context learning differ from traditional fine-tuning in terms of model adaptation?
- Concept: Distribution shift and debiasing.
  - Why needed here: The paper demonstrates robustness to color shifts in MNIST, which requires understanding how models can be biased and how representations like SVG can mitigate this.
  - Quick check question: Why might CNNs be more sensitive to color changes than shape in image classification?

## Architecture Onboarding

- Component map: Raster image → SVG converter → Prompt construction → Pre-trained LLM → Output parsing
- Critical path:
  1. Convert image to SVG (preprocessing)
  2. Construct prompt with task and examples
  3. Feed prompt to LLM
  4. Parse and interpret LLM output
- Design tradeoffs:
  - SVG detail vs. sequence length: More detail improves fidelity but may exceed LLM context limits.
  - Manual prompt engineering vs. automated: Handcrafted prompts work but are not scalable.
  - Generalist LLM vs. fine-tuned: Fine-tuning improves performance but requires labeled SVG data.
- Failure signatures:
  - Low accuracy on tasks requiring fine-grained visual details (e.g., texture, shading).
  - Performance drops when SVG representation is too abstract or loses key features.
  - Inconsistent results due to prompt sensitivity or example selection in in-context learning.
- First 3 experiments:
  1. Zero-shot classification: Convert a few MNIST digits to SVG, prompt GPT-4 to classify, measure accuracy.
  2. In-context learning: Same as above but add 1-3 example SVG-label pairs in the prompt, compare performance.
  3. Colored MNIST robustness: Convert colored MNIST to SVG, test classification accuracy, compare to PNG baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs effectively understand and process photographic images when converted to SVG format?
- Basis in paper: [inferred] The paper acknowledges that SVG representations are not as effective for photographic content due to loss of fine-grained details, and that incorporating more details can lead to prohibitively long sequences for current LLMs.
- Why unresolved: The paper only provides preliminary results and acknowledges this as a limitation without exploring potential solutions or hybrid representations.
- What evidence would resolve it: Experiments comparing LLM performance on photographic images using various SVG conversion methods (different levels of detail, segmentation approaches) and measuring accuracy against raster-based methods.

### Open Question 2
- Question: How can we develop hybrid representations that combine the advantages of discrete SVG and continuous pixel data while preserving fine details?
- Basis in paper: [explicit] The authors explicitly state that "Developing hybrid representations that can retain the advantages of both discrete and continuous data, while preserving finer details, is an area for future exploration."
- Why unresolved: This represents a fundamental research direction that the paper identifies but does not attempt to address, as it would require new representation algorithms beyond the scope of the current work.
- What evidence would resolve it: A proposed hybrid representation framework with experiments demonstrating improved performance on both graphic and photographic content compared to pure SVG or raster approaches.

### Open Question 3
- Question: What is the optimal way to tokenize SVG primitives for efficient processing by LLMs?
- Basis in paper: [inferred] The authors mention that "in LLMs, the processing unit is the token, which can correspond to one or several words. However, in SVG, we would prefer to have a specific embedding module for each geometric primitive in SVG, such as circles, polygons, and so on."
- Why unresolved: While the paper identifies this as a desirable approach, it does not implement or evaluate specific tokenization strategies for SVG primitives.
- What evidence would resolve it: A comparative study of different tokenization schemes for SVG elements (geometric primitives, attributes, styles) measuring their impact on LLM performance across various vision tasks.

## Limitations

- The approach is less effective for photographic content due to loss of fine-grained details in SVG representation
- The SVG conversion process is not fully specified, making reproducibility challenging
- No direct comparison with established visual-language models using raster embeddings

## Confidence

**High Confidence**: The claim that SVG is a textual, XML-based representation of visual primitives. This is well-established and uncontroversial.

**Medium Confidence**: The claim that LLMs can perform visual reasoning and classification tasks using SVG inputs. While results are positive, the experiments are limited in scope and lack robust baselines.

**Medium Confidence**: The claim that SVG provides robustness to distribution shifts like colored MNIST. The evidence is suggestive but not definitive, and the mechanism (shape-color disentanglement) needs further validation.

**Low Confidence**: The claim that LLMs can generate new images via visual prompting using SVG. This is demonstrated but the quality and practicality of generated images are not thoroughly evaluated.

## Next Checks

1. **Direct modality comparison**: Implement a controlled experiment comparing LLM performance on the same image classification tasks using SVG, raster image embeddings (CLIP), and plain text descriptions. This would isolate the contribution of SVG as an intermediate representation.

2. **Quality of SVG conversion**: Systematically vary the parameters of the SVG conversion process (detail level, simplification) and measure the impact on LLM performance across different tasks. This would reveal whether the approach is robust to conversion quality or requires careful tuning.

3. **Generalization to complex datasets**: Extend the experiments to include more challenging image classification datasets like CIFAR-10/100 or ImageNet subsets. This would test whether the approach scales beyond simple synthetic datasets and identify failure modes with more complex visual content.