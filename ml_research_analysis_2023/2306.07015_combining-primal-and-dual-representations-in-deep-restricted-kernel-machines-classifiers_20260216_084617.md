---
ver: rpa2
title: Combining Primal and Dual Representations in Deep Restricted Kernel Machines
  Classifiers
arxiv_id: '2306.07015'
source_url: https://arxiv.org/abs/2306.07015
tags:
- classi
- level
- drkm
- kpca
- cation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new deep restricted kernel machine (DRKM)
  classifier that combines multiple levels of kernel PCA in the dual representation
  with a classification level in the primal. The primal classifier can be either an
  LSSVM or an MLP, allowing the model to leverage both the nonlinear feature extraction
  capabilities of KPCA and the computational efficiency of primal optimization.
---

# Combining Primal and Dual Representations in Deep Restricted Kernel Machines Classifiers

## Quick Facts
- **arXiv ID**: 2306.07015
- **Source URL**: https://arxiv.org/abs/2306.07015
- **Reference count**: 22
- **Primary result**: DRKM classifier combining dual KPCA with primal LSSVM/MLP outperforms standard baselines on high-dimensional, small-sample datasets

## Executive Summary
This paper introduces a novel deep restricted kernel machine (DRKM) classifier that integrates multiple levels of kernel PCA (KPCA) in the dual representation with a classification level in the primal. The approach combines the nonlinear feature extraction capabilities of KPCA with the computational efficiency of primal optimization. The authors formulate the problem with orthogonality constraints on hidden features and solve it using Projected Gradient Descent on the Stiefel manifold. Experiments demonstrate that DRKM outperforms traditional LSSVM and MLP/CNN classifiers when training data is limited, particularly on high-dimensional datasets, while maintaining competitive energy efficiency.

## Method Summary
DRKM combines multiple KPCA layers operating in dual representation with orthogonality constraints on hidden features, followed by a primal classifier (either LSSVM or MLP). The dual KPCA levels extract nonlinear features using the kernel trick, while the primal classifier operates efficiently in the reduced feature space. The model is trained jointly using Projected Gradient Descent with Euclidean projection onto the Stiefel manifold for the KPCA levels. The approach can be initialized with unsupervised DKPCA or trained end-to-end. The orthogonality constraints promote better regularization and more diverse feature representations.

## Key Results
- DRKM outperforms LSSVM and MLP/CNN classifiers when training data is limited, particularly on high-dimensional datasets
- Multiple KPCA levels improve classification accuracy compared to single-level models
- DRKM demonstrates comparable or lower energy consumption than CNN while maintaining competitive performance
- Increased depth via multiple KPCA levels can improve classification accuracy in kernel methods, especially with small training sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual representation in KPCA allows nonlinear feature extraction independent of input dimensionality
- Mechanism: KPCA uses kernel trick to implicitly map data to high-dimensional space without computing explicit features, then performs PCA in that space
- Core assumption: Kernel function k0(x,y) is positive semi-definite and enables computation of inner products in feature space
- Evidence anchors:
  - [abstract]: "The dual representation is better suited for the kernel trick and feature extraction"
  - [section]: "the dual formulation is independent of the dimension of the inputs"
- Break condition: If kernel matrix K(0) is not positive semi-definite or computationally infeasible for large N

### Mechanism 2
- Claim: Primal formulation in LSSVM/MLP enables efficient optimization in low-dimensional space after KPCA embedding
- Mechanism: After KPCA reduces dimensionality to s << d, primal optimization in weight space w becomes computationally tractable
- Core assumption: Hidden features h(j) from KPCA are low-dimensional (s << d) making primal optimization efficient
- Evidence anchors:
  - [abstract]: "the primal setting is parametric, which makes the proposed method computationally efficient for both high-dimensional inputs and large datasets"
  - [section]: "h(1)_i is usually low-dimensional, as the number of principal components s1 is generally small"
- Break condition: If KPCA does not sufficiently reduce dimensionality or if number of components approaches input dimension

### Mechanism 3
- Claim: Orthogonality constraints on hidden features improve generalization and prevent co-adaptation
- Mechanism: Enforcing H^T H = I constrains hidden features to lie on Stiefel manifold, promoting diversity in learned representations
- Core assumption: Orthogonal hidden features lead to better regularized and more interpretable representations
- Evidence anchors:
  - [section]: "we use orthogonality constraints on the hidden features of the kernel PCA level"
  - [section]: "orthogonality constraints can lead to better performance [2] and better regularization [5,4]"
- Break condition: If orthogonality constraint prevents model from fitting complex decision boundaries or causes numerical instability

## Foundational Learning

- Concept: Kernel trick and kernel PCA
  - Why needed here: Understanding how KPCA performs nonlinear dimensionality reduction using kernel functions
  - Quick check question: How does kernel PCA differ from standard PCA, and why is the dual formulation advantageous for high-dimensional data?

- Concept: Primal vs dual optimization formulations
  - Why needed here: The paper combines dual representation for unsupervised KPCA levels with primal representation for supervised classification
  - Quick check question: When would you choose primal over dual formulation, and what are the computational trade-offs?

- Concept: Stiefel manifold and orthogonality constraints
  - Why needed here: Hidden features are constrained to lie on Stiefel manifold St(s,N) requiring specialized optimization techniques
  - Quick check question: What is the geometric interpretation of the Stiefel manifold, and how does projecting onto it affect the optimization dynamics?

## Architecture Onboarding

- Component map:
  - KPCA levels (dual representation): Multiple kernel PCA layers with orthogonality constraints on hidden features H(1:T)H = I
  - Classification level (primal representation): Either LSSVM (primal SVM) or MLP classifier operating on KPCA outputs
  - Optimization: Projected Gradient Descent with Euclidean projection onto Stiefel manifold for KPCA levels, standard gradient descent for classifier

- Critical path:
  1. Forward pass: Input → KPCA level 1 → ... → KPCA level n → Classification level → Output
  2. Backward pass: Compute gradients → Project hidden features onto Stiefel manifold → Update weights in classifier

- Design tradeoffs:
  - Depth vs width: More KPCA levels vs more components per level
  - Dual vs primal: Computational efficiency vs model expressiveness
  - Orthogonality constraints: Regularization benefits vs potential restriction of representation capacity

- Failure signatures:
  - Training instability: May indicate learning rate issues or numerical problems with manifold projection
  - Poor performance on simple datasets: Could suggest over-complexity or that KPCA levels are not learning useful features
  - Memory issues with large datasets: Dual formulation requires storing NxN kernel matrix

- First 3 experiments:
  1. Train 1-level DRKM on MNIST with small training set (N=50) to verify basic functionality
  2. Compare 1-level vs 2-level DRKM on same dataset to observe depth benefits
  3. Test end-to-end training vs DKPCA initialization + fine-tuning on ARCENE dataset

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but several arise from the research:

- How does the depth of KPCA levels affect generalization on real-world high-dimensional datasets beyond MNIST and UCI datasets?
- What is the optimal trade-off between the number of KPCA levels and the number of components per level for maximizing classification accuracy?
- How does the kernel choice for different KPCA levels affect the final classification performance, and should different kernels be used at different levels?

## Limitations

- Dual formulation requires storing NxN kernel matrix, making it computationally prohibitive for very large datasets
- Orthogonality constraints may limit the model's capacity to learn complex representations in certain domains
- Energy consumption comparisons with CNN lack rigorous benchmarking across diverse hardware configurations

## Confidence

- **High**: The theoretical foundation combining primal-dual formulations and the orthogonality constraints on the Stiefel manifold
- **Medium**: The empirical performance improvements on benchmark datasets, particularly for small training sets
- **Low**: The generalizability of results to domains outside the tested image and bioinformatics datasets

## Next Checks

1. **Scalability test**: Evaluate DRKM performance on datasets with N > 10,000 to quantify the dual formulation bottleneck and assess whether primal-only alternatives could match performance with better scalability.

2. **Ablation study**: Systematically remove orthogonality constraints and additional KPCA levels to quantify their individual contributions to performance gains versus computational overhead.

3. **Cross-domain validation**: Apply DRKM to text classification and time-series prediction tasks to test whether the architecture's benefits extend beyond the image and bioinformatics domains where it was primarily validated.