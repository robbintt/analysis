---
ver: rpa2
title: 'DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge Graphs'
arxiv_id: '2307.04090'
source_url: https://arxiv.org/abs/2307.04090
tags:
- debate
- which
- graphs
- evidence
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DebateKG, a system for automatically generating
  Policy Debate cases using semantic knowledge graphs built from the DebateSum dataset.
  The authors significantly expand DebateSum with 53,180 new examples and additional
  metadata.
---

# DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge Graphs

## Quick Facts
- arXiv ID: 2307.04090
- Source URL: https://arxiv.org/abs/2307.04090
- Reference count: 11
- Authors: None listed in source
- Primary result: Demonstrates automatic generation of Policy Debate cases using constrained shortest path traversals on semantic knowledge graphs

## Executive Summary
This paper introduces DebateKG, a system that automatically generates Policy Debate cases by leveraging semantic knowledge graphs built from the DebateSum dataset. The authors significantly expand DebateSum with 53,180 new examples and additional metadata, then construct 9 different semantic knowledge graphs using various language models and indexing strategies. The core innovation lies in using constrained shortest path traversals to connect relevant evidence into coherent debate arguments, with a novel evaluation method that measures the average length of extracted evidence chains. The system is demonstrated through a publicly available Hugging Face demo that allows users to input arguments and automatically generate complete debate cases.

## Method Summary
DebateKG builds semantic knowledge graphs from the DebateSum dataset using the txtai toolchain, with 9 different graph configurations combining various language models (MPNet, LegalBERT, Longformer) and indexing strategies (abstractive summary, extractive summary, sentence-level). The system takes user-provided input arguments and uses constrained shortest path algorithms to find chains of semantically related evidence that form coherent debate cases. Metadata constraints (topic, argument type, source) can be applied to filter subgraphs. The effectiveness of each graph is evaluated based on the average length of evidence chains it produces, with shorter chains being preferable in Policy Debate contexts.

## Key Results
- Successfully generates coherent debate cases by connecting evidence through constrained shortest path traversals on semantic knowledge graphs
- Demonstrates that different language models and indexing strategies produce knowledge graphs with varying effectiveness for debate case generation
- Shows that metadata constraints can be used to filter knowledge graph subgraphs and improve argument relevance
- Achieves practical demonstration through a publicly available Hugging Face space for user interaction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Constrained shortest path traversals on semantic knowledge graphs can generate coherent debate cases by connecting related evidence.
- **Mechanism**: The system uses txtai to build semantic knowledge graphs where nodes represent debate evidence and edges represent semantic similarity between pieces of evidence. By finding shortest paths between user-provided input arguments, the system chains together evidence that flows logically from one point to another, creating a coherent debate case.
- **Core assumption**: Semantic similarity between evidence pieces correlates with their argumentative coherence and relevance in a debate context.
- **Evidence anchors**:
  - [abstract] "We show that effective debate cases can be constructed using constrained shortest path traversals on Argumentative Semantic Knowledge Graphs."
  - [section] "DebateKG extracts the evidence closest to the given arguments which meets the given constraints, and then connects these evidence examples together by calculating the constrained weighted shortest path between each evidence example."
  - [corpus] Weak - the corpus shows related work on knowledge graphs and argumentation but doesn't directly validate this specific mechanism.
- **Break condition**: If semantic similarity doesn't align with argumentative coherence, or if the graph structure becomes too sparse to find meaningful paths between input arguments.

### Mechanism 2
- **Claim**: Different language models and indexing strategies produce knowledge graphs with varying effectiveness for debate case generation.
- **Mechanism**: The system builds 9 different knowledge graphs using different combinations of language models (mpnet, legalbert, longformer) and indexing strategies (abstract, extract, sentence-level). The effectiveness of each graph is evaluated based on the average length of evidence chains it produces, with shorter chains being preferable in Policy Debate.
- **Core assumption**: The choice of language model and indexing granularity significantly impacts the quality of semantic relationships captured in the knowledge graph.
- **Evidence anchors**:
  - [abstract] "We create a unique method for evaluating which knowledge graphs are better in the context of producing policy debate cases."
  - [section] "Thus, we evaluate each graph based on how long the created Debate Cases extracts are... Table 1 shows the results of this experiment."
  - [corpus] Weak - the corpus contains related work on knowledge graphs but doesn't provide direct evidence about this specific evaluation method.
- **Break condition**: If all graphs produce similarly long evidence chains, or if longer chains prove more effective for certain types of debate arguments.

### Mechanism 3
- **Claim**: Metadata constraints (topic, argument type, source) can be used to filter knowledge graph subgraphs and improve argument relevance.
- **Mechanism**: The system leverages metadata added to DebateSum (source camp, argument type/tag, topic-year) to create constrained subgraphs. Users can specify constraints like "only include evidence about the environment" or "only use counterplan arguments," and the system finds paths within these filtered subgraphs.
- **Core assumption**: Metadata labels accurately capture the argumentative content and context of evidence, and filtering by these labels improves argument relevance.
- **Evidence anchors**:
  - [abstract] "We significantly improve upon DebateSum by introducing 53180 new examples, as well as further useful metadata for every example"
  - [section] "We also add further metadata columns, indicating the source DebateCamp, the broad type of argument, and the topic-year, for all documents within DebateSum."
  - [corpus] Weak - the corpus mentions related work on knowledge graphs and argumentation but doesn't specifically address metadata-constrained filtering.
- **Break condition**: If metadata labels are inconsistent or don't capture argumentative content well, leading to irrelevant or overly restrictive subgraphs.

## Foundational Learning

- **Concept**: Graph theory and pathfinding algorithms
  - Why needed here: The system fundamentally relies on finding shortest paths in weighted graphs to connect evidence pieces into coherent arguments
  - Quick check question: How would Dijkstra's algorithm differ from A* in this context, and why might one be preferred over the other?

- **Concept**: Semantic similarity and embedding models
  - Why needed here: The system uses language models to create embeddings and calculate semantic similarity between evidence pieces, which forms the edge weights in the knowledge graph
  - Quick check question: What's the difference between using cosine similarity versus Euclidean distance on embeddings, and how might this choice affect the knowledge graph structure?

- **Concept**: Information retrieval and query processing
  - Why needed here: Users provide input arguments that the system uses to find relevant starting and ending points in the knowledge graph
  - Quick check question: How would you modify the system to handle ambiguous or underspecified user inputs more effectively?

## Architecture Onboarding

- **Component map**: DebateSum dataset -> txtai toolchain -> 9 semantic knowledge graphs -> DebateKG web interface -> Hugging Face space deployment
- **Critical path**: User input -> semantic search in knowledge graph -> constrained shortest path calculation -> evidence chain extraction -> debate case presentation
- **Design tradeoffs**:
  - Memory vs. granularity: Sentence-level indexing creates larger graphs but may capture finer-grained semantic relationships
  - Model choice: Domain-specific models (legalbert) vs. general-purpose models (mpnet) for semantic similarity
  - Extractive vs. generative: The system uses extractive methods for reliability but may miss creative argument connections
- **Failure signatures**:
  - Empty results: Input arguments too specific or knowledge graph too sparse
  - Irrelevant results: Semantic similarity not aligned with argumentative relevance
  - Performance issues: Large knowledge graphs causing slow path calculations
- **First 3 experiments**:
  1. Test with simple input arguments (e.g., "climate change is real" â†’ "we need action") to verify basic functionality
  2. Compare output quality between different language models for the same input
  3. Test metadata filtering by constraining to specific topics or argument types and measuring relevance improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DebateKG compare to generative approaches for debate case creation in terms of quality and relevance of arguments?
- Basis in paper: [explicit] The paper mentions that DebateKG is an extractive system and notes that extractive systems have lower abuse potential compared to generative systems, but does not provide a direct comparison of performance.
- Why unresolved: The paper focuses on the development and evaluation of DebateKG using its own dataset and methods, without benchmarking against generative models that could produce debate cases.
- What evidence would resolve it: A head-to-head comparison study where both DebateKG and a state-of-the-art generative model are tasked with creating debate cases on the same topics, with human judges evaluating the quality, coherence, and persuasiveness of the outputs.

### Open Question 2
- Question: Can the DebateKG approach be effectively adapted for other languages or debate formats beyond American Policy Debate?
- Basis in paper: [explicit] The paper acknowledges that American Policy Debate is almost always performed in English and notes the limitation regarding data availability in other languages.
- Why unresolved: The current system is built on a dataset specific to American Policy Debate and English, and there is no exploration of how it might generalize to other linguistic or cultural contexts.
- What evidence would resolve it: Experiments where DebateKG or its core methodology is applied to datasets from other debate formats (e.g., British Parliamentary, Asian Parliamentary) or translated into other languages, with evaluation of the system's performance in those new contexts.

### Open Question 3
- Question: What is the impact of different semantic similarity thresholds on the quality and diversity of generated debate cases in DebateKG?
- Basis in paper: [explicit] The paper mentions that DebateKG computes semantic similarity between entities and connects those whose similarity is above a user-defined threshold, using a default threshold of 0.10.
- Why unresolved: The paper does not explore how varying this threshold affects the resulting debate cases, such as their coherence, length, or the diversity of perspectives included.
- What evidence would resolve it: A systematic study where the threshold is varied across a range of values, generating debate cases at each setting, and then analyzing and comparing the resulting cases in terms of their characteristics and quality metrics.

## Limitations

- The evaluation methodology focuses on measuring average evidence chain length rather than direct assessment of argument quality or persuasiveness
- The core assumption that semantic similarity equates to argumentative coherence lacks empirical validation through user studies or debate judge evaluations
- The system's effectiveness is limited to the domain of American Policy Debate and English language content, with no exploration of generalization to other formats or languages

## Confidence

- High confidence: The technical implementation of constrained shortest path traversals on knowledge graphs is well-specified and reproducible
- Medium confidence: The effectiveness of different language models and indexing strategies for debate contexts is supported by quantitative metrics but not qualitative evaluation
- Low confidence: The assumption that shorter evidence chains produce better debate cases is asserted but not empirically validated

## Next Checks

1. Conduct a user study with debate judges or experienced debaters to evaluate whether automatically generated cases using different knowledge graphs actually perform better in competitive settings
2. Test the system's robustness by providing deliberately ambiguous or underspecified input arguments and measuring how well it can still generate coherent debate cases
3. Compare the system's output quality against human-generated debate cases on the same topics to establish baseline performance expectations