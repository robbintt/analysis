---
ver: rpa2
title: 'Teach me with a Whisper: Enhancing Large Language Models for Analyzing Spoken
  Transcripts using Speech Embeddings'
arxiv_id: '2311.07014'
source_url: https://arxiv.org/abs/2311.07014
tags:
- language
- audio
- dataset
- text
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of incorporating rich acoustic
  and paralinguistic information from speech data into large language models, which
  traditionally only process text. The authors propose an audio-language knowledge
  distillation framework that transfers information from a pre-trained speech embedding
  model (OpenAI Whisper) to a text-based language model.
---

# Teach me with a Whisper: Enhancing Large Language Models for Analyzing Spoken Transcripts using Speech Embeddings

## Quick Facts
- arXiv ID: 2311.07014
- Source URL: https://arxiv.org/abs/2311.07014
- Authors: 
- Reference count: 11
- Key outcome: Audio-language knowledge distillation improves sentiment analysis and emotion recognition on spoken transcripts by 1.69-2.74% over text-only baselines

## Executive Summary
This paper addresses the challenge of incorporating rich acoustic and paralinguistic information from speech data into large language models, which traditionally only process text. The authors propose an audio-language knowledge distillation framework that transfers information from a pre-trained speech embedding model (OpenAI Whisper) to a text-based language model. This allows the text model to learn from spoken language audio data without requiring audio input during inference.

The core method involves using Whisper's encoder as a teacher model to generate audio embeddings, which are then used to guide the training of a student BERT model through knowledge distillation objectives (either Neuron Selectivity Transfer or Contrastrastive Representation Distillation). The student model is trained on an audio-text dataset (People's Speech) and evaluated on sentiment analysis and emotion recognition tasks using the CMU-MOSEI dataset. Key results show that the student models (trained on 66M tokens) outperform traditional text-based models trained on the same dataset across all tasks, demonstrating that incorporating audio information during training enhances language models' ability to analyze spoken transcripts.

## Method Summary
The method uses a pre-trained Whisper encoder as a teacher model to extract audio embeddings from spoken transcripts. These embeddings are temporally averaged and used in knowledge distillation objectives (NST or CRD) to guide a BERT student model trained on paired audio-text data from the People's Speech dataset. The student model learns both through masked language modeling and alignment with audio representations, enabling it to capture acoustic and paralinguistic cues without requiring audio input at inference time. After pre-training, the student model is fine-tuned on CMU-MOSEI for sentiment analysis and emotion recognition tasks.

## Key Results
- Student models achieve 1.69-1.67% higher accuracy in sentiment classification compared to text-only baselines
- Sentiment score prediction shows 2.62-2.74% higher Pearson correlation with ground truth
- Emotion recognition accuracy improves by 1.06-1.02% across 6 binary emotion categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge distillation from speech embeddings to text-only model transfers acoustic and paralinguistic cues that improve sentiment and emotion understanding.
- Mechanism: Whisper encoder generates audio embeddings that capture acoustic features (tone, pitch, emphasis). These embeddings are averaged and used as teacher representations in KD objectives (NST or CRD) to guide BERT student training on audio-text pairs.
- Core assumption: Mean audio embedding across time steps preserves sufficient acoustic information to guide token-level text representations despite lack of alignment between audio frames and text tokens.
- Evidence anchors:
  - [abstract] "We achieve this via an audio-language knowledge distillation framework, where we transfer acoustic and paralinguistic information from a pre-trained speech embedding (OpenAI Whisper) teacher model to help train a student language model"
  - [section] "We get the final audio representation ha by temporally averaging... we use an average of the MMD distances over all valid text tokens in the batch when computing the loss per minibatch."
  - [corpus] Weak - no direct evidence that mean pooling preserves paralinguistic cues; assumption based on prior work in multimodal KD.
- Break condition: If audio-text alignment existed, using frame-level audio tokens instead of mean pooling would be more effective, potentially making this averaging suboptimal.

### Mechanism 2
- Claim: Training on spoken language transcriptions (informal, spontaneous) provides richer linguistic diversity than formal text corpora, improving model performance on spoken transcripts.
- Mechanism: People's Speech dataset contains 66M tokens of conversational English, which captures informal vocabulary and speech patterns not present in formal text (e.g., Wikipedia). This domain match improves downstream performance on CMU-MOSEI (spoken YouTube reviews).
- Core assumption: Token count matters less than domain relevance and linguistic diversity when training transformer models for spoken language tasks.
- Evidence anchors:
  - [section] "we observe that a model trained on 44 times fewer tokens can perform competitively... this dataset only has 1.65% of the total number of tokens compared to... BERT... However, this is still the largest dataset publicly available for spoken audio clips and transcription."
  - [section] "transitioning from BERT to wikiBERT... we observe a decrease... despite a substantial reduction in the total number of tokens (97.72%) from wikiBERT to transcription-BERT, the decline in performance is relatively modest"
  - [corpus] Moderate - evidence shows People's Speech is largest available spoken corpus, but no comparison of linguistic diversity metrics against formal text.
- Break condition: If downstream tasks used formal written text rather than spoken transcripts, this advantage would disappear.

### Mechanism 3
- Claim: Contrastive Representation Distillation (CRD) transfers richer information than Neuron Selectivity Transfer (NST) by maximizing mutual information between teacher and student representations.
- Mechanism: CRD uses contrastive learning to align teacher and student representations through positive/negative pairs, capturing more comprehensive information than NST's MMD-based distribution alignment.
- Core assumption: Maximizing mutual information provides better knowledge transfer than matching activation distributions alone.
- Evidence anchors:
  - [section] "Tian et al. (2019) suggest that the contrastive objective better transfers all the information in the teacher's representation, rather than only assuming the teachers embedding dimensions are conditionally independent"
  - [section] "In our experiments, the student model achieves consistent improvement over traditional language models... Both our student models, StudentNST and StudentCRD, exhibit superior performance"
  - [corpus] Weak - paper states "LCRDKD and LNSTKD demonstrate similar performance without a clear winner," contradicting the assumed superiority of CRD.
- Break condition: If NST and CRD show equivalent performance, the mutual information maximization advantage is not realized in practice.

## Foundational Learning

- Concept: Knowledge Distillation fundamentals
  - Why needed here: Understanding how teacher model knowledge transfers to student through loss functions is essential for implementing and debugging the KD framework.
  - Quick check question: What is the difference between response-based, feature-based, and relation-based knowledge distillation?

- Concept: Speech embedding extraction and temporal pooling
  - Why needed here: The method relies on extracting meaningful audio features from Whisper and appropriately pooling them to match text token dimensions.
  - Quick check question: Why does the method use mean pooling across audio frames rather than using the full sequence of audio embeddings?

- Concept: Contrastive learning and mutual information maximization
  - Why needed here: CRD objective uses contrastive learning principles to align representations, requiring understanding of positive/negative pairs and temperature scaling.
  - Quick check question: How does the temperature parameter τ in CRD affect the concentration of the similarity distribution between positive pairs?

## Architecture Onboarding

- Component map:
  - Whisper encoder (frozen teacher) → audio feature extraction → mean pooling
  - BERT student (trainable) → text processing → masked language modeling
  - KD objectives (NST or CRD) → alignment loss between audio and text representations
  - Combined loss = γ × KD loss + MLM loss
  - People's Speech dataset (audio-text pairs) → pre-training
  - CMU-MOSEI dataset → fine-tuning and evaluation

- Critical path:
  1. Load audio sample and transcript pair
  2. Extract log-Mel spectrogram from audio
  3. Pass through Whisper encoder to get audio embeddings
  4. Mean pool audio embeddings to get single vector
  5. Tokenize text and pass through BERT student
  6. Compute MLM loss on masked positions
  7. Compute KD loss between mean audio vector and text token embeddings
  8. Backpropagate combined loss to update student parameters only

- Design tradeoffs:
  - Using mean audio embedding vs. frame-level alignment (simplicity vs. precision)
  - Whisper-small vs. larger Whisper models (efficiency vs. richer embeddings)
  - NST vs. CRD objectives (computational simplicity vs. potential information transfer)
  - Training on 66M vs. full 8,273 hours of People's Speech (speed vs. coverage)

- Failure signatures:
  - Student model overfits to audio information and loses text understanding (check MLM loss vs KD loss balance)
  - No improvement over text-only baseline (check if audio embeddings are being properly extracted and averaged)
  - Training instability (check learning rates, batch sizes, and gradient norms)
  - Poor downstream performance despite good pre-training loss (check domain mismatch between pre-training and fine-tuning data)

- First 3 experiments:
  1. Train StudentNST with γ=1.0 and MLM only (no KD) to establish baseline improvement from spoken language data alone
  2. Train StudentNST with γ=1.0 using full KD objective to measure audio information contribution
  3. Train StudentCRD with same settings to compare KD objective effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of training tokens for achieving competitive performance on spoken language tasks, and how does this compare to traditional text-based models?
- Basis in paper: [explicit] The paper discusses the trade-off between token quantity and model performance, noting that transcription-BERT trained on 66M tokens performs comparably to BERT trained on 4B tokens.
- Why unresolved: The paper does not conduct an exhaustive search over different token quantities to determine the optimal training set size for spoken language tasks.
- What evidence would resolve it: Systematic experiments training models on varying quantities of spoken language tokens and comparing their performance on downstream tasks.

### Open Question 2
- Question: How do different knowledge distillation objectives (e.g., NST vs CRD) compare in terms of effectiveness for transferring audio information to text models?
- Basis in paper: [explicit] The paper compares two KD objectives (NST and CRD) but notes they perform similarly without a clear winner.
- Why unresolved: The paper does not explore a wider range of KD objectives or conduct a more thorough comparison between the ones tested.
- What evidence would resolve it: Comprehensive experiments comparing multiple KD objectives across various spoken language tasks and datasets.

### Open Question 3
- Question: How does the length of audio sequences (e.g., 15s vs 30s) impact the effectiveness of audio-language knowledge distillation for spoken language understanding?
- Basis in paper: [inferred] The paper uses 15-second audio clips for training but mentions that longer sequences could enable exploration of more downstream tasks.
- Why unresolved: The paper does not investigate the impact of audio sequence length on model performance or downstream task applicability.
- What evidence would resolve it: Experiments training models on different audio sequence lengths and evaluating their performance on various spoken language tasks.

## Limitations

- The paper assumes mean pooling of audio embeddings preserves sufficient acoustic information without empirical validation of this alignment mechanism
- Performance improvements could stem from domain adaptation (training on spoken language) rather than the audio distillation mechanism itself
- CRD and NST objectives show similar performance despite theoretical advantages claimed for CRD's mutual information maximization

## Confidence

- High Confidence: The core empirical finding that student models trained with audio-language distillation outperform text-only baselines on CMU-MOSEI tasks is well-supported by the presented results (1.69-1.67% accuracy gains, 2.62-2.74% ρ improvements)
- Medium Confidence: The claim that incorporating audio information during training improves language models' ability to analyze spoken transcripts is supported, but the specific mechanism (mean pooling of audio embeddings, superiority of CRD) lacks rigorous validation
- Low Confidence: The assertion that CRD transfers richer information than NST due to mutual information maximization is not supported by the experimental results showing equivalent performance between the two objectives

## Next Checks

1. **Alignment Analysis**: Conduct correlation analysis between mean-pooled audio embeddings and corresponding text token representations to empirically validate whether the temporal averaging preserves meaningful audio-text relationships

2. **Domain Adaptation Ablation**: Train a BERT student on the same People's Speech dataset without any KD objectives to isolate whether improvements come from spoken language data exposure versus audio information transfer

3. **Frame-Level KD Experiment**: Implement frame-level audio-text alignment (if possible) or token-level audio embedding matching to test whether the mean pooling approach is suboptimal compared to more precise alignment methods