---
ver: rpa2
title: Self-Evaluation Guided Beam Search for Reasoning
arxiv_id: '2305.00633'
source_url: https://arxiv.org/abs/2305.00633
tags:
- reasoning
- correct
- above
- generation
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces self-evaluation guided stochastic beam search
  for improving multi-step reasoning in large language models (LLMs). The method addresses
  error accumulation in multi-step reasoning chains by decomposing the reasoning process
  into intermediate steps and guiding the beam search with a self-evaluation mechanism
  that acts as a fine-grained, automatic criterion.
---

# Self-Evaluation Guided Beam Search for Reasoning

## Quick Facts
- arXiv ID: 2305.00633
- Source URL: https://arxiv.org/abs/2305.00633
- Authors: 
- Reference count: 40
- Key outcome: Improves accuracy by 6.34% (GSM8K), 9.56% (AQuA), and 5.46% (StrategyQA) in few-shot reasoning settings

## Executive Summary
This paper introduces self-evaluation guided stochastic beam search to improve multi-step reasoning in large language models by addressing error accumulation through intermediate step evaluation. The method decomposes reasoning into steps and uses an LLM to evaluate each step's faithfulness based on prior context, guiding beam search with a combined score of generation probability and evaluation confidence. The approach demonstrates significant accuracy improvements across arithmetic, symbolic, and commonsense reasoning tasks while improving consistency through majority voting.

## Method Summary
The method treats reasoning chain generation as an autoregressive factorization P(R|x) = ∏t P(st|x,s1:t−1) and extends beam search to reason over intermediate reasoning steps rather than tokens. At each step, an LLM evaluates the faithfulness of the current reasoning step based on previous context, producing a probability score that identifies logical failures. Stochastic beam search with temperature-controlled randomness balances exploration and exploitation, while a tunable hyperparameter λ combines generation probability and faithfulness score into a unified decoding objective.

## Key Results
- Achieves 6.34% accuracy improvement on GSM8K dataset
- Achieves 9.56% accuracy improvement on AQuA dataset  
- Achieves 5.46% accuracy improvement on StrategyQA dataset
- Improves consistency through majority voting of multiple reasoning chains
- Identifies logical failures at step level, improving robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-evaluation guidance provides fine-grained error detection in intermediate reasoning steps
- Mechanism: The self-evaluation LLM judges each reasoning step's faithfulness based on prior context, assigning a probability score that identifies logical failures before they accumulate
- Core assumption: LLM evaluation scores are well-calibrated and correlate with actual reasoning correctness
- Evidence anchors: [abstract] "self-evaluation mechanism that acts as a fine-grained, automatic criterion", [section] "evaluation conﬁdence is more discriminatory than generation conﬁdence"

### Mechanism 2
- Claim: Stochastic beam search balances exploration-exploitation tradeoff in reasoning space
- Mechanism: Temperature-controlled randomness in beam pruning allows sampling from accumulated scores rather than deterministic argmax, maintaining diversity while pursuing high-quality paths
- Core assumption: Adding controlled randomness improves coverage of reasoning space without sacrificing too much quality
- Evidence anchors: [abstract] "Stochastic beam search balances exploitation and exploration of the search space with temperature-controlled randomness"

### Mechanism 3
- Claim: Combining generation probability and faithfulness score creates better decoding objective
- Mechanism: The final score E(s1:T) = ∏t PλLMG(st|x,s1:t−1)C1−λ(st) balances language model fluency with logical correctness
- Core assumption: Both generation fluency and logical correctness contribute independently to reasoning quality
- Evidence anchors: [section] "we propose a constrained decoding approach that combines the language model probability and the faithfulness probability as a new decoding objective function"

## Foundational Learning

- Concept: Autoregressive factorization of reasoning chains
  - Why needed here: The paper treats reasoning chain generation as a sequential decision process P(R|x) = ∏t P(st|x,s1:t−1), which is fundamental to understanding how beam search applies to reasoning
  - Quick check question: If a reasoning chain has T steps, how many conditional probability factors are in the autoregressive decomposition?

- Concept: Calibration in probabilistic predictions
  - Why needed here: Self-evaluation relies on the LLM's probability outputs being well-calibrated to actual correctness
  - Quick check question: What does it mean for a model's probability outputs to be "well-calibrated"?

- Concept: Beam search algorithm fundamentals
  - Why needed here: The paper extends traditional beam search from token-level to reasoning-step-level
  - Quick check question: In standard beam search, how many candidates are kept at each step when beam size = k?

## Architecture Onboarding

- Component map: Question → Generation LLM → Evaluation LLM → Beam search controller → Chain completion → Majority vote
- Critical path: Question → Generation LLM produces step → Evaluation LLM scores step → Beam search selects candidates → Repeat until chain complete → Execute/interpret chain → Majority vote if multiple chains
- Design tradeoffs:
  - Beam size vs. computational cost: Larger k explores more paths but increases inference time quadratically
  - Temperature vs. diversity: Higher τ increases diversity but may reduce quality; annealing schedule (α) helps
  - λ weight vs. task type: Different λ values may work better for arithmetic vs. commonsense reasoning
- Failure signatures:
  - Poor λ tuning → either fluent but incorrect chains or correct but unnatural chains
  - Insufficient evaluation examples → unreliable faithfulness scores
  - Too low beam size → premature convergence on suboptimal reasoning paths
  - Temperature too high → search becomes random, loses coherent reasoning
- First 3 experiments:
  1. Validate self-evaluation scores correlate with actual correctness on a held-out validation set
  2. Test different λ values (0.2, 0.5, 0.8) on GSM8K to find optimal balance
  3. Compare deterministic vs. stochastic beam search with same computational budget on StrategyQA

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the self-evaluation guided decoding method scale with model size beyond the Codex backbone, particularly for GPT-4 and larger models?
- Basis in paper: [explicit] The paper discusses the potential of using more powerful LLM backends like GPT-4 but notes compatibility issues since these models don't provide probabilistic generation scores needed for the method.
- Why unresolved: The paper does not provide experimental results for GPT-4 or larger models, only discussing theoretical considerations.
- What evidence would resolve it: Experimental results comparing self-evaluation guided decoding performance across different model sizes, particularly including GPT-4 and other large models.

### Open Question 2
- Question: What is the impact of different self-evaluation prompt formulations on the faithfulness score's effectiveness in identifying reasoning errors?
- Basis in paper: [explicit] The paper mentions using multiple-choice question prompting for self-evaluation based on Kadavath et al. (2022) but doesn't extensively explore alternative prompt formulations.
- Why unresolved: The paper uses a specific prompt formulation without systematic comparison to other potential formulations.
- What evidence would resolve it: Comparative experiments testing different prompt formulations (e.g., free-form vs. multiple-choice, different exemplar sets) and their impact on evaluation accuracy.

### Open Question 3
- Question: How does the self-evaluation guided decoding method perform on more complex reasoning tasks involving external knowledge or multimodal inputs?
- Basis in paper: [inferred] The paper mentions potential future work on applying the method to "more complex scenarios involving multi-modal understanding and reasoning" in the discussion section.
- Why unresolved: The current evaluation focuses on text-based arithmetic, symbolic, and commonsense reasoning tasks without external knowledge or multimodal components.
- What evidence would resolve it: Experimental results applying the method to tasks requiring external knowledge retrieval or multimodal reasoning (e.g., visual question answering, knowledge graph reasoning).

## Limitations

- The approach requires running two LLMs (generation and evaluation) for each reasoning step, introducing significant computational overhead
- Performance depends heavily on the quality and calibration of self-evaluation exemplars, which may not generalize well to all reasoning domains
- The method has only been validated on text-based arithmetic, symbolic, and commonsense reasoning tasks, limiting generalizability to more complex reasoning scenarios

## Confidence

**High Confidence (Experimental Claims):**
- The 6.34% accuracy improvement on GSM8K is well-supported by the experimental results presented
- The consistency improvements demonstrated through majority voting are robust across datasets
- The step-level error detection capability is validated through ablation studies

**Medium Confidence (Mechanism Claims):**
- The effectiveness of stochastic beam search with temperature control is supported but could benefit from more extensive ablation on temperature schedules
- The claim about evaluation confidence being more discriminatory than generation confidence is plausible but needs more rigorous statistical validation
- The assertion that the method is insensitive to decoding depth requires further validation across more diverse reasoning tasks

**Low Confidence (Generalizability Claims):**
- Claims about robustness to decoding depth variations need testing on longer, more complex reasoning chains
- The paper's assertion that this approach is superior to all existing multi-step reasoning methods lacks comparison to some newer techniques
- The scalability of the approach to larger models or more complex reasoning domains hasn't been thoroughly explored

## Next Checks

1. **Self-Evaluation Calibration Test**: Conduct a comprehensive analysis of self-evaluation score calibration by comparing evaluation confidence scores against actual correctness rates across different reasoning difficulty levels and domains.

2. **Computational Overhead Analysis**: Perform detailed benchmarking comparing the wall-clock time and computational resources required for self-evaluation guided beam search versus alternative approaches like iterative refinement or standard beam search with larger beam sizes.

3. **Cross-Domain Generalization Test**: Apply the method to reasoning tasks outside the arithmetic and commonsense domains tested in the paper, such as logical inference, multi-hop question answering, or scientific reasoning.