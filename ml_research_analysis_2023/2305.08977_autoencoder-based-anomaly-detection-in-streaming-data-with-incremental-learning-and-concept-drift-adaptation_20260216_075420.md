---
ver: rpa2
title: Autoencoder-based Anomaly Detection in Streaming Data with Incremental Learning
  and Concept Drift Adaptation
arxiv_id: '2305.08977'
source_url: https://arxiv.org/abs/2305.08977
tags:
- drift
- learning
- data
- straem
- window
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of anomaly detection in streaming
  data under severe class imbalance and nonstationary environments with concept drift.
  It proposes strAEm++DD, an autoencoder-based incremental learning method with explicit
  concept drift detection.
---

# Autoencoder-based Anomaly Detection in Streaming Data with Incremental Learning and Concept Drift Adaptation

## Quick Facts
- arXiv ID: 2305.08977
- Source URL: https://arxiv.org/abs/2305.08977
- Reference count: 38
- Key outcome: strAEm++DD significantly outperforms baselines on streaming anomaly detection under severe class imbalance and concept drift.

## Executive Summary
This paper addresses the challenge of anomaly detection in streaming data under severe class imbalance and nonstationary environments with concept drift. The authors propose strAEm++DD, an autoencoder-based incremental learning method with explicit concept drift detection. The method uses a sliding window to store recent instances and updates the autoencoder incrementally when a percentage of the window is replaced. Concept drift is detected using the Mann-Whitney U test on reconstruction losses between a reference and moving window. Experiments on synthetic and real-world datasets with 1% and 0.1% imbalance rates show that strAEm++DD significantly outperforms baseline and advanced methods, especially when drift causes a change in posterior probability.

## Method Summary
The paper proposes strAEm++DD, an autoencoder-based method for anomaly detection in streaming data with concept drift and severe class imbalance. The method uses incremental learning with a sliding window to adapt to concept drift. When a percentage of the sliding window is replaced, the autoencoder is updated using the recent instances. Concept drift is detected using the Mann-Whitney U test on reconstruction losses between a reference and moving window. The anomaly threshold is recalculated as the b-th percentile of the losses in the sliding window. The method is evaluated on four datasets with severe and extreme imbalance rates, showing significant improvement over baseline and advanced methods.

## Key Results
- strAEm++DD achieves higher geometric mean scores compared to baseline and advanced methods on all datasets.
- The method demonstrates robustness to severe (1%) and extreme (0.1%) class imbalance rates.
- strAEm++DD outperforms competitors, especially when concept drift causes a change in posterior probability.

## Why This Works (Mechanism)

### Mechanism 1
Incremental autoencoder retraining on a sliding window adapts to concept drift in streaming anomaly detection. The autoencoder is updated when p% of the sliding window is replaced, using the recent instances to refine the reconstruction loss threshold. This allows the model to gradually adjust to new data distributions. Core assumption: The sliding window contains mostly normal instances, even after drift, so the autoencoder can learn the new normal pattern.

### Mechanism 2
Mann-Whitney U test on reconstruction losses detects concept drift by comparing reference and moving windows. Reconstruction losses from the reference window and the moving window are compared using the Mann-Whitney U test. If the test indicates a significant difference (p-value below threshold), a drift alarm is raised. Core assumption: Reconstruction loss distribution changes significantly when concept drift occurs, and the test is sensitive enough to detect this change.

### Mechanism 3
Percentile-based anomaly threshold adapts to changing reconstruction loss distributions. The anomaly threshold is recalculated at each training time as the b-th percentile of the losses in the sliding window. This allows the threshold to adapt to changes in the reconstruction loss distribution. Core assumption: The b-th percentile of the reconstruction losses is a good estimate of the anomaly threshold, even as the distribution changes.

## Foundational Learning

- Concept: Autoencoder reconstruction loss as anomaly score
  - Why needed here: The autoencoder learns to reconstruct normal instances well, but anomalies have higher reconstruction loss. This difference is used to detect anomalies.
  - Quick check question: If an autoencoder is trained on normal instances, will it reconstruct an anomaly with high or low reconstruction loss?

- Concept: Concept drift and its types
  - Why needed here: The streaming data may undergo concept drift, which can cause the autoencoder to become outdated. Understanding the types of drift (prior, likelihood, posterior) is crucial for designing effective detection and adaptation mechanisms.
  - Quick check question: If the posterior probability p(y|x) changes, but the class-conditional probability p(x|y) remains the same, what type of concept drift is occurring?

- Concept: Statistical hypothesis testing (Mann-Whitney U test)
  - Why needed here: The Mann-Whitney U test is used to detect concept drift by comparing the reconstruction loss distributions of the reference and moving windows. Understanding the test's assumptions and interpretation is crucial for proper implementation.
  - Quick check question: If the Mann-Whitney U test indicates a significant difference between two samples, what can we conclude about their underlying distributions?

## Architecture Onboarding

- Component map:
  Autoencoder (encoder and decoder) -> Sliding window (movtrain) -> Reference window (refdrif tx) and moving window (movdrif tx) -> Mann-Whitney U test -> Anomaly threshold calculation

- Critical path:
  1. Observe new instance
  2. Predict anomaly using current autoencoder and threshold
  3. Append instance to sliding window
  4. If p% of window replaced, update autoencoder using instances in window
  5. If reference window full, calculate reconstruction losses for reference and moving windows
  6. Apply Mann-Whitney U test to detect drift
  7. If drift detected, raise alarm, train new autoencoder on warning window, update threshold

- Design tradeoffs:
  - Window size: Larger windows provide more stable estimates but slower adaptation to drift; smaller windows adapt faster but may be noisier
  - Number of epochs: More epochs can improve autoencoder performance but increase computation time and risk of overfitting
  - P-value thresholds (Pwarn, Palarm): Higher thresholds reduce false alarms but may miss true drifts; lower thresholds increase sensitivity but also false alarms

- Failure signatures:
  - High false positive rate: Threshold too low or window too small
  - High false negative rate: Threshold too high or window too large
  - Slow adaptation to drift: Insufficient incremental learning or conservative P-value thresholds
  - Drift not detected: P-value thresholds too high or reconstruction loss not sensitive to the type of drift

- First 3 experiments:
  1. Verify autoencoder learns to reconstruct normal instances well and has high reconstruction loss for anomalies on a synthetic dataset with known anomalies
  2. Test incremental learning by introducing concept drift and observing if the autoencoder adapts over time using a sliding window
  3. Validate drift detection by comparing the performance of strAEm++DD with and without the Mann-Whitney U test on a dataset with injected concept drift

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do variational autoencoders (VAEs) compare to standard autoencoders in terms of anomaly detection performance under severe class imbalance and concept drift?
- Basis in paper: The paper mentions investigating different types of autoencoders (e.g., variational) as future work.
- Why unresolved: The paper only uses standard autoencoders and does not explore other variants like VAEs.
- What evidence would resolve it: Experimental comparison of VAEs vs. standard autoencoders on the same datasets with severe imbalance and concept drift.

### Open Question 2
- Question: How does the proposed method perform on more complex, high-dimensional datasets beyond MNIST and synthetic examples?
- Basis in paper: The paper acknowledges applying the method to more complex datasets as future work, implying current experiments are limited.
- Why unresolved: Experiments are conducted only on relatively simple datasets (MNIST, synthetic Sea and Circle).
- What evidence would resolve it: Testing on real-world, high-dimensional streaming datasets (e.g., network traffic, sensor data) with severe imbalance and concept drift.

### Open Question 3
- Question: What is the impact of different drift detection mechanisms (e.g., statistical tests vs. threshold-based) on the performance of autoencoder-based incremental learning?
- Basis in paper: The paper uses the Mann-Whitney U test for drift detection and mentions threshold-based mechanisms as alternatives.
- Why unresolved: Only one drift detection method (Mann-Whitney U test) is evaluated.
- What evidence would resolve it: Comparative analysis of multiple drift detection methods within the same incremental learning framework.

## Limitations
- The exact drift injection mechanism for synthetic datasets (Sea and Circle) is not fully specified, making exact reproduction challenging.
- The composition and size of pre-training data beyond the initial 2000 instances is unspecified.
- The paper does not provide runtime complexity analysis or computational resource requirements.

## Confidence
- **High Confidence**: The core methodology (incremental autoencoder learning with sliding window and Mann-Whitney U test for drift detection) is well-described and theoretically sound. Experimental results showing significant improvement over baselines on multiple datasets are convincing.
- **Medium Confidence**: The specific hyperparameter settings and their sensitivity analysis. While the paper mentions values used, it does not explore the robustness of these choices or provide guidance on tuning for different scenarios.
- **Low Confidence**: The exact procedure for generating concept drift in synthetic datasets and the precise timing of drift occurrences, which are crucial for reproducing the experimental results.

## Next Checks
1. Implement the strAEm++DD algorithm on a small synthetic dataset with known anomalies and manually injected concept drift to verify that the autoencoder adapts to drift and the Mann-Whitney U test correctly detects changes in reconstruction loss distribution.
2. Systematically vary the Pwarn and Palarm thresholds on a controlled dataset to quantify the tradeoff between false alarm rate and detection latency, establishing optimal settings for different drift speeds.
3. Apply strAEm++DD to a publicly available streaming dataset with natural concept drift (such as network traffic or sensor data) to evaluate performance in a less controlled but more realistic setting, comparing against at least one state-of-the-art streaming anomaly detection method.