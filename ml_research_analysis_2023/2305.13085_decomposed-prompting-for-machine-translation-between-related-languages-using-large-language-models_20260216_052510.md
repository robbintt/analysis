---
ver: rpa2
title: Decomposed Prompting for Machine Translation Between Related Languages using
  Large Language Models
arxiv_id: '2305.13085'
source_url: https://arxiv.org/abs/2305.13085
tags:
- translation
- language
- languages
- prompting
- decomt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel approach called Decomposed Prompting
  for Machine Translation (DecoMT) that simplifies the translation process between
  related languages by breaking it down into a sequence of word chunk translations.
  The method leverages the monotonic alignment characteristic of translations between
  related languages and employs a two-stage translation process for word chunks: independent
  translation followed by contextual translation.'
---

# Decomposed Prompting for Machine Translation Between Related Languages using Large Language Models

## Quick Facts
- arXiv ID: 2305.13085
- Source URL: https://arxiv.org/abs/2305.13085
- Authors: [Not specified in source]
- Reference count: 4
- Primary result: DecoMT achieves 8 chrF++ average improvement over few-shot baselines across multiple related language pairs

## Executive Summary
This paper introduces Decomposed Prompting for Machine Translation (DecoMT), a novel approach that simplifies translation between related languages by breaking sentences into word chunks and translating them sequentially. The method leverages the monotonic alignment characteristic of related languages, where word order is largely preserved during translation. By decomposing the translation task into smaller, more manageable components, DecoMT reduces cognitive load on large language models and achieves superior performance compared to established few-shot baseline approaches.

## Method Summary
DecoMT employs a two-stage translation process using mT5-XL (3.7B parameters). First, sentences are chunked into word segments (typically 5 tokens) and translated independently using few-shot prompting. Second, these independently translated chunks undergo contextual translation where surrounding context is incorporated through infill masking. The approach uses human-annotated aligned monotonic translations for prompt templates and was evaluated on FLORES dataset across multiple related language pairs including Hindi-Marathi, Hindi-Malayalam, Indonesian-Malay, Russian-Ukrainian, and Spanish-Portuguese.

## Key Results
- DecoMT achieves an average 8 chrF++ improvement across examined languages compared to BLOOM baseline
- Performance improves with sentence length, demonstrating the value of contextual translation
- Statistical significance testing via paired bootstrap sampling confirms improvements (p<0.05)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing translation into smaller word chunks reduces cognitive load on LLMs by removing the need for simultaneous word ordering decisions
- Core assumption: Related languages have monotonic alignment characteristics that preserve word order
- Evidence anchors: Translations between related languages are predominantly monotonic (abstract, section 1)
- Break condition: When translating between languages with significant syntactic divergence where monotonic alignment doesn't hold

### Mechanism 2
- Claim: Independent chunk translation followed by contextual translation improves accuracy by allowing focused processing
- Core assumption: LLMs can maintain context effectively when given surrounding chunks and previous translations
- Evidence anchors: Contextual translation uses Hi, Hi+1, Hi+2, Ri-1, Mi+1 (section 3.3); performance improves with sentence length (section 4.2)
- Break condition: When chunk size becomes too small to capture meaningful context or too large to benefit from decomposition

### Mechanism 3
- Claim: Using bidirectional LLMs for contextual translation through infill masking leverages their inherent capabilities
- Core assumption: mT5's training objective transfers effectively to translation tasks
- Evidence anchors: mT5 trained with span-corruption objective (section 3.1); employed for contextual translations (section 3.2)
- Break condition: When bidirectional context isn't sufficient to resolve ambiguities in chunk translation

## Foundational Learning

- Concept: Monotonic alignment in related languages
  - Why needed here: Forms the foundational assumption that word order is preserved in translations between related languages
  - Quick check question: Why does monotonic alignment matter for decomposing translation into word chunks?

- Concept: Few-shot prompting mechanics
  - Why needed here: The approach relies on prompting LLMs with examples rather than fine-tuning
  - Quick check question: What distinguishes few-shot prompting from zero-shot prompting in practice?

- Concept: Bidirectional context utilization
  - Why needed here: Contextual translation stage requires using both preceding and following context for chunk refinement
  - Quick check question: How does bidirectional context differ from left-to-right autoregressive generation?

## Architecture Onboarding

- Component map: Input preprocessor -> Independent translation engine -> Contextual translation engine -> Output assembler -> Template generator

- Critical path:
  1. Input sentence → chunk segmentation
  2. Each chunk → independent translation (Mi)
  3. Sequential contextual translation using Hi, Hi+1, Hi+2, Ri-1, Mi+1 → Ri
  4. Output: Concatenation of R1...Rβ

- Design tradeoffs:
  - Chunk size vs. context sufficiency: Smaller chunks reduce complexity but provide less context for refinement
  - Prompt template construction: Manual annotation required vs. automated generation
  - Translation time: Two-stage process takes longer than direct translation
  - Model choice: mT5 chosen for bidirectional capability vs. purely autoregressive models

- Failure signatures:
  - Poor performance on non-monotonic language pairs (symptom: low chrF++ scores)
  - Chunk boundary issues (symptom: disfluent translations at chunk interfaces)
  - Context insufficiency (symptom: contextual translations don't improve over independent translations)

- First 3 experiments:
  1. Vary chunk size m across {3,4,5} on dev set to find optimal balance between translation accuracy and context
  2. Compare independent translation stage alone vs. full DecoMT to quantify contextual translation contribution
  3. Test with alternative LLMs (BLOOM, XGLM) for contextual translation to assess mT5's unique advantages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of DecoMT compare when applied to machine translation between distantly related languages or languages from different families?
- Basis in paper: [inferred] The paper focuses on translations between related languages and demonstrates superior performance in these scenarios. However, it does not explore the model's performance with distantly related languages or languages from different families.
- Why unresolved: The paper does not provide any experiments or results for translations between distantly related languages or languages from different families.
- What evidence would resolve it: Conducting experiments using DecoMT on translations between distantly related languages or languages from different families and comparing the results with the current findings would provide the necessary evidence.

### Open Question 2
- Question: How does the translation quality of DecoMT vary with different chunk sizes (m) in the independent translation stage?
- Basis in paper: [explicit] The paper mentions that the value of m, which represents the token count within a word-chunk for independent translation, is treated as a hyperparameter and tuned using the FLORES development set. However, the paper does not provide a detailed analysis of how different chunk sizes affect the translation quality.
- Why unresolved: The paper does not provide a detailed analysis of the impact of different chunk sizes on the translation quality.
- What evidence would resolve it: Conducting experiments with different chunk sizes and analyzing the impact on the translation quality would provide the necessary evidence.

### Open Question 3
- Question: How does DecoMT perform in terms of translation speed compared to other few-shot prompting methodologies?
- Basis in paper: [inferred] The paper mentions that DecoMT requires a longer duration to generate outputs compared to traditional few-shot prompting methodologies due to its two-stage process. However, it does not provide a detailed comparison of translation speeds.
- Why unresolved: The paper does not provide a detailed comparison of translation speeds between DecoMT and other few-shot prompting methodologies.
- What evidence would resolve it: Conducting experiments to measure and compare the translation speeds of DecoMT and other few-shot prompting methodologies would provide the necessary evidence.

## Limitations

- Fundamental assumption of monotonic alignment may not generalize to all related language pairs, particularly those with significant syntactic divergence
- Requirement for human-annotated aligned translations for prompt template creation introduces scalability concerns and potential bias
- Computational overhead of two-stage translation process may limit practical deployment in resource-constrained settings

## Confidence

**High Confidence (8/10):**
- Overall performance improvement over few-shot baselines
- Two-stage translation process contribution to improved quality
- Monotonic alignment characteristic for examined language pairs

**Medium Confidence (6/10):**
- Generalizability to all related language pairs beyond examined set
- Scalability of prompt template creation through human annotation
- Optimal chunk size parameters across different language pairs

**Low Confidence (4/10):**
- Computational efficiency compared to direct translation approaches
- Performance on extremely long sentences where contextual errors compound
- Effectiveness on low-resource related language pairs

## Next Checks

1. **Cross-linguistic validation**: Test DecoMT on additional related language pairs from underrepresented families (e.g., Dravidian, Sino-Tibetan, or Niger-Congo) to assess generalizability beyond Indo-European and Austronesian languages.

2. **Prompt template automation**: Develop and evaluate automated methods for creating aligned translation examples for prompt templates, comparing performance against human-annotated templates to address scalability concerns.

3. **Chunk size optimization study**: Conduct comprehensive ablation study varying chunk sizes across a wider range (m=3 to m=15) for both translation stages, measuring the trade-off between translation accuracy and computational efficiency.