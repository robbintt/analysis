---
ver: rpa2
title: Leveraging Few-Shot Data Augmentation and Waterfall Prompting for Response
  Generation
arxiv_id: '2308.01080'
source_url: https://arxiv.org/abs/2308.01080
tags:
- knowledge
- data
- dialogue
- reviews
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the DSTC11 challenge for response generation
  in task-oriented dialogue systems, focusing on incorporating subjective knowledge.
  The authors augment the dataset with synthetic reviews generated via few-shot learning
  using ChatGPT, expanding both existing domains and introducing new ones.
---

# Leveraging Few-Shot Data Augmentation and Waterfall Prompting for Response Generation

## Quick Facts
- arXiv ID: 2308.01080
- Source URL: https://arxiv.org/abs/2308.01080
- Authors: 
- Reference count: 5
- Key outcome: This paper tackles the DSTC11 challenge for response generation in task-oriented dialogue systems, focusing on incorporating subjective knowledge.

## Executive Summary
This paper presents a novel approach to response generation in task-oriented dialogue systems by incorporating subjective knowledge from user reviews. The authors augment the dataset with synthetic reviews generated via few-shot learning using ChatGPT, expanding both existing domains and introducing new ones. They explore three main approaches: fine-tuning task-specific models, appending the most frequent question to responses, and using waterfall prompting with GPT-3 and ChatGPT. Results show the augmented data improves knowledge selection, and the flan-t5-large model with post-processing outperforms the baseline on most metrics.

## Method Summary
The paper addresses the DSTC11 challenge for response generation in task-oriented dialogue systems by augmenting the knowledge base with synthetic reviews generated using few-shot learning with ChatGPT. The augmented dataset includes 715 additional reviews across existing and new domains. The authors explore three main approaches: fine-tuning flan-t5-large for response generation, appending the most frequent question to responses, and implementing a waterfall prompting technique combining GPT-3 and ChatGPT. The waterfall approach first summarizes knowledge items using GPT-3, then generates the final response with ChatGPT. Models are evaluated using automatic metrics including BLEU, METEOR, and ROUGE variants.

## Key Results
- Knowledge selection performance improves with augmented data, increasing the knowledge base from 1,430 to 2,145 reviews
- The flan-t5-large model with post-processing outperforms baseline on most metrics
- Waterfall prompting produces promising qualitative outputs despite scoring lower quantitatively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic review generation improves knowledge selection performance by increasing both the diversity and size of the knowledge base.
- Mechanism: Few-shot data augmentation using ChatGPT generates new subjective reviews for existing and new domains, which expands the knowledge base from 1,430 to 2,145 reviews. This richer knowledge base enables the knowledge selection model to retrieve more relevant items.
- Core assumption: The synthetic reviews generated by ChatGPT are sufficiently diverse and contextually appropriate to mimic real user reviews.
- Evidence anchors:
  - [abstract]: "We used few-shot learning to augment the data with newly generated subjective knowledge items"
  - [section 4.1]: "We added 715 reviews to this by adding five different traveller types and producing five sentences for each type"
  - [corpus]: FMR scores show related papers on synthetic data generation, suggesting this is a recognized approach
- Break condition: If synthetic reviews are too generic or biased, they could mislead the knowledge selection model and harm performance.

### Mechanism 2
- Claim: Waterfall prompting with separate summarization and dialogue management steps improves response generation by decomposing the task.
- Mechanism: GPT-3 is used first to summarize the knowledge items, then ChatGPT generates the final response including a follow-up question. This separation allows each model to specialize in one subtask.
- Core assumption: Large language models can effectively decompose the response generation task into summarization and dialogue management subtasks.
- Evidence anchors:
  - [abstract]: "a waterfall prompting technique that integrated a combination of Large Language Models"
  - [section 4.2.2]: "To accommodate CoT answers, which tend to be longer, we double the value of the 'max_tokens' parameter"
  - [corpus]: Related papers on prompting techniques support this decomposition approach
- Break condition: If the intermediate summary is poor, the final response will also suffer, and the additional complexity may not be worth the marginal gains.

### Mechanism 3
- Claim: Appending the most frequent question to responses improves conversation coherence and engagement metrics.
- Mechanism: The most common follow-up question ("Would you like to know more about them?") is appended to responses that don't already contain a question, increasing the proportion of responses with questions from 34.94% to 100%.
- Core assumption: Generic follow-up questions are appropriate and beneficial for most dialogue contexts.
- Evidence anchors:
  - [section 3.3]: "Out of the 2,522 unique questions, 79% (n=1,994) appear only once. The most commonly occurring question is 'Would you like to know more about them?' accounting for 19% of the occurrences"
  - [section 4.2.2]: "we experiment with adding the most frequent question (MFQ) to the responses that did not already contain a question"
  - [corpus]: Weak - no direct evidence in related papers about appending generic questions
- Break condition: If the generic question is contextually inappropriate, it could reduce response quality and user satisfaction.

## Foundational Learning

- Concept: Few-shot learning and prompt engineering
  - Why needed here: The paper relies heavily on using ChatGPT with carefully crafted prompts to generate synthetic data and guide responses
  - Quick check question: What are the key components of an effective few-shot prompt for generating synthetic reviews?

- Concept: Dialogue act recognition and response decomposition
  - Why needed here: The approach separates responses into summary and optional question parts based on dialogue act analysis
  - Quick check question: How do different dialogue act types (questions, statements, commands) influence the structure of generated responses?

- Concept: Automatic evaluation metrics for text generation
  - Why needed here: The paper evaluates models using BLEU, METEOR, ROUGE-1/2/L, requiring understanding of what each metric captures
  - Quick check question: Why might a model with higher ROUGE scores still produce less coherent responses than a lower-scoring model?

## Architecture Onboarding

- Component map: Knowledge base (original + augmented reviews) -> Knowledge Seeking Turn Detection module -> Knowledge Selection module -> Response Generation pipeline (model exploration + post-processing + waterfall prompting) -> Evaluation metrics (BLEU, METEOR, ROUGE variants)
- Critical path: Knowledge base augmentation → Knowledge Selection → Response Generation → Evaluation
- Design tradeoffs: Using larger models (flan-t5-large) improves performance slightly but increases computational cost; waterfall prompting adds complexity but may improve coherence
- Failure signatures: Poor knowledge selection leading to irrelevant responses; generic follow-up questions reducing engagement; API limitations truncating responses
- First 3 experiments:
  1. Compare knowledge selection performance with and without augmented data
  2. Evaluate different model sizes (flan-t5-small vs base vs large) on response generation
  3. Test the impact of appending the most frequent question on response quality metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the synthetic review data quality compare to human-written reviews in terms of bias and factual accuracy?
- Basis in paper: [inferred] from "Ethics Statement" discussing potential biases in synthetic data and lack of public training data for commercial LLMs.
- Why unresolved: The paper mentions potential biases but doesn't provide a quantitative analysis comparing synthetic vs. human reviews for bias or factual accuracy.
- What evidence would resolve it: A systematic evaluation comparing sentiment distributions, topic coverage, and factual consistency between synthetic and human-written reviews, plus bias detection analysis.

### Open Question 2
- Question: What is the optimal balance between review sentiment polarity (positive/negative) for maximizing response generation performance?
- Basis in paper: [explicit] from discussion of selecting challenging examples with "predominantly negative sentiment" and finding that ChatGPT struggles with high numbers of knowledge items and negative sentiment.
- Why unresolved: The paper only explores one extreme (negative sentiment) when selecting examples, without testing whether moderate sentiment balance or other distributions might yield better results.
- What evidence would resolve it: Ablation studies varying the sentiment distribution of synthetic reviews (e.g., 50/50 positive-negative vs 70/30 vs 90/10) and measuring impact on response generation metrics.

### Open Question 3
- Question: How does the waterfall prompting approach perform with human evaluation compared to automatic metrics?
- Basis in paper: [explicit] from "Results" section noting that waterfall prompting "scored lower in quantitative metrics compared to the baseline" but "produced promising qualitative outputs" and that "A full human evaluation would be an interesting step."
- Why unresolved: The paper lacks human evaluation results, relying only on automatic metrics which may not capture qualitative aspects of generated responses.
- What evidence would resolve it: Human evaluation studies comparing waterfall prompting outputs against baseline outputs on dimensions like coherence, appropriateness, informativeness, and naturalness.

## Limitations

- Synthetic data quality may introduce biases that affect downstream performance
- Waterfall prompting approach lacks comprehensive human evaluation
- Prompt engineering sensitivity makes the approach difficult to reproduce

## Confidence

- **High Confidence**: The dataset augmentation process and its quantitative impact on knowledge selection performance are well-documented and reproducible.
- **Medium Confidence**: The fine-tuning approach with flan-t5-large and its superiority over smaller models is supported by clear quantitative results.
- **Low Confidence**: The waterfall prompting approach's effectiveness is primarily supported by qualitative observations rather than comprehensive quantitative evaluation.

## Next Checks

1. Conduct a systematic comparison between synthetic and real reviews using both automatic metrics and human evaluation to assess whether the augmented data introduces systematic biases or quality issues.

2. Perform ablation studies varying the prompts used in waterfall prompting to determine which components are essential versus optional, and test the approach's robustness to prompt variations.

3. Design a comprehensive human evaluation framework to assess response quality dimensions not captured by automatic metrics, including coherence, appropriateness of follow-up questions, and overall user satisfaction across different domains and conversation contexts.