---
ver: rpa2
title: Adversarially Robust Neural Legal Judgement Systems
arxiv_id: '2308.00165'
source_url: https://arxiv.org/abs/2308.00165
tags:
- legal
- adversarial
- training
- bert
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of existing neural legal
  judgment prediction (LJP) systems to adversarial attacks, proposing a method to
  increase their robustness. The authors fine-tune baseline transformer models (BERT,
  Legal-BERT, RoBERTa, H-BERT) on legal datasets and show that their accuracy drops
  significantly under adversarial attacks.
---

# Adversarially Robust Neural Legal Judgement Systems

## Quick Facts
- arXiv ID: 2308.00165
- Source URL: https://arxiv.org/abs/2308.00165
- Reference count: 26
- This paper proposes adversarial training to improve the robustness of neural legal judgment prediction systems against adversarial attacks

## Executive Summary
This paper addresses the vulnerability of neural legal judgment prediction (LJP) systems to adversarial attacks, proposing a method to increase their robustness. The authors fine-tune baseline transformer models (BERT, Legal-BERT, RoBERTa, H-BERT) on legal datasets and show that their accuracy drops significantly under adversarial attacks. They then propose adversarial training by augmenting training data with adversarial examples generated through word substitution based on importance scores and semantic similarity. Experimental results on three legal datasets (ECHR, SCOTUS, ILDC) show that adversarial training improves robustness: models achieve significantly higher accuracy against adversarial attacks compared to naturally trained or data-augmented models.

## Method Summary
The authors implement adversarial training for LJP systems by first fine-tuning baseline transformer models on legal datasets using standard cross-entropy loss. They then generate adversarial examples by identifying important words through deletion-based importance scoring, replacing them with semantically similar words using counter-fitted word embeddings, and filtering by semantic similarity using Universal Sentence Encoder. The final training combines natural training (3 epochs) with adversarial training (7 epochs) using a weighted loss function L = Lnat + γLadv where γ controls adversarial training importance. They also implement H-BERT, a hierarchical architecture that handles long legal documents by processing overlapping chunks of text.

## Key Results
- Naturally trained models show significant accuracy drops under adversarial attacks (e.g., H-BERT drops from 79.32% to 39.18% on ECHR)
- Adversarially trained models maintain much higher accuracy under attacks (e.g., H-BERT maintains 69.32% on ECHR)
- H-BERT architecture handles long legal documents better than standard transformers
- Legal-BERT performs best on ECHR and SCOTUS datasets due to domain-specific pre-training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial training with adversarial examples generated by word substitution based on importance scores and semantic similarity improves robustness.
- Mechanism: The model is fine-tuned on a dataset that includes both original legal examples and adversarial examples created by replacing high-importance words with semantically similar synonyms. The importance of words is determined by their impact on the model's prediction (deletion-based scoring), and synonyms are chosen using cosine similarity and counter-fitted word embeddings. This process creates a more diverse training set that helps the model learn to maintain correct predictions even when input is perturbed.
- Core assumption: Adversarial examples generated through this method are similar enough to original examples that they provide meaningful training signal without being too easy or too hard.
- Evidence anchors:
  - [abstract] The authors propose adversarial training by augmenting training data with adversarial examples generated through word substitution based on importance scores and semantic similarity.
  - [section 4.3] The paper describes generating adversarial examples by finding important words, replacing them with semantically similar words, and checking similarity with Universal Sentence Encoder.
  - [corpus] Weak - the corpus contains related works on legal judgment prediction but none specifically address adversarial training methods.
- Break condition: If the adversarial examples are too dissimilar from original examples (similarity below threshold) or if the word importance scoring fails to identify truly influential words, the training signal becomes ineffective.

### Mechanism 2
- Claim: Hierarchical BERT architecture handles long legal documents better than standard BERT.
- Mechanism: Legal documents are typically much longer than the 512-token limit of standard transformers. The H-BERT architecture splits text into overlapping chunks of 510 tokens, processes each chunk through a BERT encoder, and then combines the chunk representations using CNN, max-pooling, and Bi-LSTM layers before final classification. This allows the model to capture information from the entire document rather than just the first/last 512 tokens.
- Core assumption: Important legal information is distributed throughout the document and not concentrated in any specific 512-token window.
- Evidence anchors:
  - [section 5.1] The paper notes that legal text datasets have substantial length (ECtHR average 1619 words, SCOTUS 5853 words) and implements H-BERT to handle this.
  - [section 4.1] Describes dividing text into chunks with overlapping tokens and using RoBERTa as encoder in H-BERT.
  - [corpus] Weak - related papers mention long text encoders but don't provide detailed architectural comparisons.
- Break condition: If critical legal information is concentrated in specific document regions that get averaged out during chunking, or if the overlapping mechanism fails to preserve document context.

### Mechanism 3
- Claim: Combining natural training with adversarial training (using γ-weighted loss) creates more robust models than either approach alone.
- Mechanism: The training process alternates between natural training (using cross-entropy loss on original data) and adversarial training (using cross-entropy loss on adversarial examples). The final loss is a weighted combination: L = argminθ(Lnat + γLadv), where γ controls the importance of adversarial examples. This balanced approach ensures the model doesn't overfit to either clean or adversarial data.
- Core assumption: A model that performs well on both clean and adversarial data generalizes better than one optimized for only one type.
- Evidence anchors:
  - [section 4.3] The paper defines the final loss function as a combination of natural and adversarial cross-entropy losses with hyperparameter γ.
  - [section 5.3] Shows results comparing naturally trained, data-augmented, and adversarially trained models, with adversarially trained models showing best robustness.
  - [corpus] Weak - related works don't discuss the specific weighted combination approach used here.
- Break condition: If γ is set too high, the model may overfit to adversarial examples and perform poorly on clean data; if too low, it won't gain robustness benefits.

## Foundational Learning

- Concept: Word importance scoring through deletion-based methods
  - Why needed here: To identify which words in legal documents most influence the model's predictions, so adversarial examples can target the most impactful changes
  - Quick check question: How does the deletion-based importance scoring method determine which words are most important for a given prediction?

- Concept: Semantic similarity in adversarial attacks
  - Why needed here: To ensure adversarial examples remain valid legal text that humans can't easily distinguish from original examples while still fooling the model
  - Quick check question: What metrics and thresholds does the paper use to ensure adversarial examples are semantically similar to original legal text?

- Concept: Hierarchical text processing for long documents
  - Why needed here: Legal documents exceed standard transformer token limits, requiring architectures that can process and combine information from multiple document segments
  - Quick check question: How does the H-BERT architecture handle document segmentation and what mechanisms ensure context is preserved across segments?

## Architecture Onboarding

- Component map: Input → Text segmentation (H-BERT) or truncation (BERT variants) → Transformer encoder → Chunk representation aggregation (CNN + max-pooling + Bi-LSTM for H-BERT) → Dense layer → Output classification
- Critical path: Data preprocessing → Model fine-tuning (natural training) → Adversarial example generation → Combined training (adversarial training) → Evaluation against attacks
- Design tradeoffs: H-BERT handles full document context but adds complexity and computational cost; standard BERT is simpler but loses information from truncated text; adversarial training improves robustness but requires generating and storing additional training data
- Failure signatures: Model accuracy drops significantly on adversarial examples (as shown in Table 2), indicating lack of robustness; H-BERT performance lags behind standard BERT on short documents, suggesting overhead isn't justified
- First 3 experiments:
  1. Fine-tune baseline models (BERT, Legal-BERT, RoBERTa, H-BERT) on clean legal datasets and measure baseline accuracy
  2. Generate adversarial examples using the described method and test how much baseline accuracy drops
  3. Implement adversarial training with varying γ values and measure robustness improvement against attacks

## Open Questions the Paper Calls Out

- Open Question 1: How does the performance of adversarially trained LJP systems compare to naturally trained systems when evaluated on out-of-domain legal datasets from different jurisdictions (e.g., non-English legal systems)?
- Open Question 2: What is the optimal balance between natural training epochs and adversarial training epochs for maximizing both accuracy and robustness in LJP systems?
- Open Question 3: How do different adversarial attack methods (beyond the greedy search with word importance ranking used in this paper) affect the robustness of LJP systems, and can models be made robust against a wider range of attack types?

## Limitations

- The robustness claims are limited to the specific attack methodology tested (word substitution based on importance scores)
- Performance improvements vary significantly across datasets, suggesting effectiveness depends on dataset characteristics
- The paper doesn't test against other attack types like paraphrasing, syntactic transformations, or black-box attacks

## Confidence

- High confidence: The baseline finding that existing LJP models are vulnerable to adversarial attacks and that accuracy drops significantly without defense mechanisms
- Medium confidence: The specific adversarial training methodology working as described, though exact hyperparameters remain unclear
- Low confidence: The generalizability of robustness claims beyond the specific attack type tested and across different legal domains

## Next Checks

1. Test robustness against alternative attack methods: Generate adversarial examples using different techniques (e.g., character-level perturbations, semantic-preserving transformations, or black-box attacks) to verify whether the observed robustness extends beyond the specific word substitution attack used in the paper.

2. Validate on additional legal datasets: Evaluate the adversarial training approach on legal datasets from different jurisdictions or with different case types to determine if the robustness gains are consistent across various legal domains or if they're specific to the ECtHR, SCOTUS, and ILDC datasets used.

3. Analyze performance on clean data: Measure how adversarial training affects model accuracy on clean (non-adversarial) legal text to ensure the robustness gains don't come at the cost of reduced performance on legitimate inputs, particularly for the H-BERT architecture which showed the most dramatic improvements.