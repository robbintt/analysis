---
ver: rpa2
title: A Comprehensive Evaluation of Large Language Models on Legal Judgment Prediction
arxiv_id: '2310.11761'
source_url: https://arxiv.org/abs/2310.11761
tags:
- cases
- llms
- similar
- uni00000011
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive evaluation of large language
  models (LLMs) on the task of legal judgment prediction. The authors investigate
  the performance of various LLMs, including GPT-4, ChatGPT, Vicuna, ChatGLM, and
  BLOOMZ, in predicting charges based on case facts.
---

# A Comprehensive Evaluation of Large Language Models on Legal Judgment Prediction

## Quick Facts
- arXiv ID: 2310.11761
- Source URL: https://arxiv.org/abs/2310.11761
- Reference count: 13
- Primary result: Large language models show varying effectiveness in legal judgment prediction, with label candidates improving performance while the combination of LLMs with information retrieval systems sometimes underperforms the retrieval system alone.

## Executive Summary
This paper presents a systematic evaluation of large language models (LLMs) on legal judgment prediction tasks, focusing on criminal charge classification from case facts. The authors investigate five prominent LLMs—GPT-4, ChatGPT, Vicuna, ChatGLM, and BLOOMZ—across four settings combining zero/few-shot learning with and without label candidates and similar cases. Using a Chinese legal dataset (CAIL) with 112 criminal charges, they demonstrate that label candidates significantly improve both accuracy and self-consistency, while similar cases provide mixed benefits. The study reveals an intriguing paradox where information retrieval systems can outperform LLM+IR combinations, suggesting current LLMs struggle to effectively leverage retrieved information for legal reasoning tasks.

## Method Summary
The study evaluates five LLMs (GPT-4, ChatGPT, Vicuna-13B, ChatGLM-6B, BLOOMZ-7B) on a Chinese legal judgment prediction task using the CAIL dataset with 112 criminal charges. Models are tested across four settings: zero-shot open questions, few-shot open questions, zero-shot multi-choice questions (with label candidates), and few-shot multi-choice questions. An information retrieval system retrieves similar cases and label candidates from training data using BM25. Prompts include instruction, label candidates, and demonstrations (truncated to 500 tokens), while test samples are truncated to 1000 tokens. Performance is measured using accuracy, F1 score, and self-consistency (majority prediction frequency across 5 samples per prompt).

## Key Results
- Label candidates improve LLM performance by helping models recall domain knowledge and increasing prediction consistency
- Similar cases introduce greater performance variance compared to label candidates alone
- An IR system can outperform LLM+IR combinations when LLMs fail to effectively utilize retrieved information
- GPT-4 and ChatGPT show strong sensitivity to false similar demonstrations, while smaller models like Vicuna demonstrate greater robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Label candidates and similar cases help LLMs recall domain knowledge critical for legal reasoning
- Mechanism: Explicit legal labels and relevant case demonstrations trigger LLMs to access pre-existing knowledge encoded in their parameters
- Core assumption: LLMs store relevant legal knowledge in parameters, and structured prompts can activate this knowledge
- Evidence anchors:
  - [abstract] "We show that similar cases and multi-choice options, namely label candidates, included in prompts can help LLMs recall domain knowledge that is critical for expertise legal reasoning"
  - [section 4.1] "Label candidates contribute to a higher mean performance, while similar cases introduce greater variance"

### Mechanism 2
- Claim: Label candidates result in more consistent outputs, indicating greater LLM confidence in domain knowledge
- Mechanism: Providing constrained answer sets reduces uncertainty and improves prediction confidence
- Core assumption: Consistency in outputs correlates with higher confidence and better knowledge grasp
- Evidence anchors:
  - [section 4.2] "The incorporation of label candidates leads to more consistent outputs (8 of 10 cases) and higher confidence in LLMs except zero-shot GPT-4 with a slight decrease"
  - [section 4.2] "The self-consistency also correlates with model performances (7 of 10 cases)"

### Mechanism 3
- Claim: IR systems can outperform LLM+IR when LLMs acquire limited gains from IR systems
- Mechanism: If LLMs fail to effectively utilize retrieved information, IR systems alone provide better predictions by leveraging retrieval capability without LLM interference
- Core assumption: LLM+IR effectiveness depends on LLM's ability to leverage retrieved information
- Evidence anchors:
  - [abstract] "We additionally present an intriguing paradox wherein an IR system surpasses the performance of LLM+IR due to limited gains acquired by weaker LLMs from powerful IR systems"
  - [section 5] "ChatGPT is unable to attain 100% accuracy and lags significantly behind the optimal IR system"

## Foundational Learning

- **In-context learning**: LLMs perform legal judgment prediction by conditioning on prompts without fine-tuning; understanding this is crucial for interpreting how demonstrations affect performance
  - Quick check: What distinguishes in-context learning from fine-tuning in LLM behavior?

- **Information retrieval systems**: IR systems retrieve similar cases and label candidates to enhance LLM legal reasoning capabilities
  - Quick check: How does an IR system contribute to LLM performance in legal judgment prediction?

- **Self-consistency**: Measures agreement among multiple LLM outputs, indicating prediction confidence
  - Quick check: Why is self-consistency an important metric for evaluating LLM performance in legal judgment prediction?

## Architecture Onboarding

- **Component map**: Case facts → IR system (BM25 retriever) → Similar cases & label candidates → Prompt templates → LLM (GPT-4/ChatGPT/Vicuna/ChatGLM/BLOOMZ) → BM25 parser → Charge labels

- **Critical path**: 1) Retrieve similar cases and label candidates using IR system; 2) Construct prompts with retrieved information; 3) Generate predictions using LLM; 4) Parse LLM outputs to predefined charge labels; 5) Evaluate performance using accuracy, F1 score, and self-consistency

- **Design tradeoffs**: Balancing number of demonstrations against noise introduction; specificity of label candidates versus LLM generalization ability; tradeoff between LLM language understanding and IR system retrieval quality

- **Failure signatures**: Poor performance from ineffective utilization of retrieved information; decreased self-consistency indicating low confidence; overfitting to training data when using supervised baselines

- **First 3 experiments**: 1) Evaluate LLM performance with and without label candidates to assess impact on recall and confidence; 2) Compare LLM performance with different numbers of demonstrations to understand information-noise tradeoff; 3) Simulate IR systems with varying capabilities to investigate LLM+IR paradox

## Open Questions the Paper Calls Out

- **Cross-lingual and cross-jurisdictional applicability**: How well do these findings generalize to non-Chinese legal systems and languages? The study's confinement to Chinese criminal law with 112 charges limits broader applicability.

- **Optimal integration of LLMs and IR systems**: What methods can improve the synergy between LLMs and IR systems to consistently outperform either component alone? The paradox where IR systems outperform LLM+IR combinations suggests current integration approaches are suboptimal.

- **Improving legal reasoning abilities**: How can we enhance LLMs' capabilities for complex or obscure legal cases where even GPT-4 and ChatGPT struggle to achieve 100% accuracy?

- **Incorporating legal articles**: How does including explicit legal articles that define charges in prompts affect LLM performance, given the substantial difference between LLMs and legal experts in charge distinction precision?

## Limitations

- Evaluation is limited to Chinese legal judgments with 112 criminal charges, raising questions about cross-lingual and cross-jurisdictional applicability
- Dataset sampling methodology for creating balanced test, training, and validation sets is not explicitly specified, potentially introducing selection bias
- Truncation strategy (500 tokens for demonstrations, 1000 tokens for test samples) may disproportionately affect longer, more complex cases
- Reliance on BM25 for information retrieval may not capture semantic similarity as effectively as neural retrieval methods

## Confidence

- **High Confidence**: Label candidates improve mean performance and self-consistency across most models and settings; similar cases introduce greater variance in performance; effectiveness of label candidates in enhancing LLM confidence
- **Medium Confidence**: The paradox where IR systems outperform LLM+IR is demonstrated but may depend on specific model capabilities and retrieval quality; relative contributions of label candidates versus similar cases to performance gains; claim about LLMs storing domain knowledge in parameters
- **Low Confidence**: Generalizability to other legal domains or languages beyond Chinese criminal law; assertion of being first systematic evaluation of LLMs for legal judgment prediction; specific truncation threshold of 500/1000 tokens as optimal

## Next Checks

1. **Cross-linguistic validation**: Evaluate the same models and settings on a non-Chinese legal dataset (e.g., European or American criminal law cases) to test generalizability of findings regarding label candidates and similar cases effectiveness

2. **Retrieval method comparison**: Replace BM25 with a neural retrieval method (e.g., dense retrieval with sentence transformers) to determine if retrieval quality impacts the LLM+IR system performance paradox and whether stronger retrieval changes observed patterns

3. **Token length sensitivity analysis**: Systematically vary truncation lengths (e.g., 300, 500, 700, 1000 tokens) for both demonstrations and test cases to quantify impact of information loss on model performance and determine optimal truncation strategies