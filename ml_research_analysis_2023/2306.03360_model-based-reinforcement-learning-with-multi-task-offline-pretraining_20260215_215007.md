---
ver: rpa2
title: Model-Based Reinforcement Learning with Multi-Task Offline Pretraining
arxiv_id: '2306.03360'
source_url: https://arxiv.org/abs/2306.03360
tags:
- learning
- source
- transfer
- dynamics
- vid2act
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving training efficiency
  for reinforcement learning (RL) models by pretraining on offline video datasets,
  despite challenges from task mismatch across domains. The proposed method, Vid2Act,
  leverages world models not only as simulators for behavior learning but also as
  tools to measure domain relevance for both dynamics representation transfer and
  policy transfer.
---

# Model-Based Reinforcement Learning with Multi-Task Offline Pretraining

## Quick Facts
- arXiv ID: 2306.03360
- Source URL: https://arxiv.org/abs/2306.03360
- Authors: 
- Reference count: 40
- Key outcome: Achieves nearly 50% higher performance on quadruped walk and 100% higher performance on quadruped run compared to DreamerV2 after 400k steps of environment interactions

## Executive Summary
This paper addresses the challenge of improving reinforcement learning training efficiency through offline video pretraining across multiple domains. The proposed Vid2Act method uses world models not only as simulators for behavior learning but also as tools to measure domain relevance for both dynamics representation transfer and policy transfer. By leveraging a time-varying, domain-selective distillation loss and generative action replay, Vid2Act demonstrates significant improvements over state-of-the-art methods on both Meta-World and DeepMind Control Suite benchmarks.

## Method Summary
Vid2Act pretrains teacher world models on action-conditioned videos from multiple source domains, then trains a student world model using domain-selective knowledge distillation with learned similarity weights. These weights are computed by comparing student and teacher predicted states through a distillation network. The method then uses the highest-confidence source domain to train a state-conditioned VAE action generator, which provides guidance during target policy learning using a DreamerV2-style actor-critic algorithm with imagined rollouts.

## Key Results
- Achieves nearly 50% higher performance on quadruped walk task compared to DreamerV2
- Achieves 100% higher performance on quadruped run task compared to DreamerV2
- Demonstrates significant improvements across both Meta-World and DeepMind Control Suite benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-selective distillation improves dynamics learning by adaptively weighting teacher models based on task similarity.
- Mechanism: The student model computes similarity weights by concatenating predicted states from teacher and student models, then applies a softmax to obtain a normalized importance vector. This weight vector is used to scale the distillation loss for each source domain.
- Core assumption: The Euclidean distance between student and teacher predicted states, weighted by domain similarity, provides a meaningful signal for dynamics transfer relevance.
- Evidence anchors:
  - [abstract] "We build a time-varying, domain-selective distillation loss to generate a set of offline-to-online similarity weights."
  - [section] "We then minimize the Euclidean distance between pairs of states as follows, taking into account the corresponding task-similarity weights"
  - [corpus] Weak evidence - related papers discuss multi-task world models but don't directly address domain-selective distillation mechanisms.
- Break condition: If the state representations from different domains are not comparable (e.g., different observation spaces or action spaces), the similarity weights may become meaningless and fail to identify relevant dynamics.

### Mechanism 2
- Claim: Generative action replay provides behavior guidance by selecting source actions based on domain similarity.
- Mechanism: After learning similarity weights during dynamics transfer, the system selects the source domain with highest confidence to train a state-conditioned VAE action generator. This generator produces actions that are then used as guidance during target policy learning.
- Core assumption: The domain with highest similarity weight contains action trajectories that are relevant and useful for the target task.
- Evidence anchors:
  - [abstract] "learning to replay relevant source actions to guide the target policy"
  - [section] "we reuse the offline source datasets to learn a domain-selective action generation model to provide guidance for target behavior learning"
  - [corpus] No direct evidence in corpus papers about generative action replay mechanisms.
- Break condition: If the selected source domain's actions are actually detrimental or misleading for the target task, the policy may learn incorrect behaviors or fail to converge.

### Mechanism 3
- Claim: Action-conditioned pretraining aligns representation learning between pretraining and finetuning stages.
- Mechanism: Unlike APV [48], Vid2Act trains world models on action-conditioned videos, ensuring the learned representations match the action-conditioned dynamics used in downstream tasks.
- Core assumption: Representations learned from action-conditioned data transfer more effectively to action-conditioned downstream tasks than action-free representations.
- Evidence anchors:
  - [abstract] "Unlike APV [48], it transfers action-conditioned dynamics from multiple source domains"
  - [section] "Compared to APV [48], our approach incorporates actions during the pretraining phase, which is more reasonable in learning the consequences of state transitions"
  - [corpus] No corpus evidence about action-conditioned vs action-free pretraining differences.
- Break condition: If the action spaces between source and target domains are fundamentally incompatible (e.g., different dimensionalities or semantics), the alignment benefit may be lost.

## Foundational Learning

- Concept: World models (latent dynamics models)
  - Why needed here: Vid2Act relies on world models both as simulators for behavior learning and as tools to measure domain relevance. The model learns to predict future states from current states and actions.
  - Quick check question: What are the three main components of the world model architecture used in this paper?

- Concept: Knowledge distillation
  - Why needed here: The student world model is trained using knowledge from multiple teacher models through a domain-selective distillation loss that weights each teacher by relevance.
  - Quick check question: How does the domain-selective distillation loss differ from standard knowledge distillation?

- Concept: Variational autoencoders (VAEs)
  - Why needed here: The action generation model uses a VAE architecture to generate source actions conditioned on states, which are then used to guide the target policy.
  - Quick check question: What is the role of the encoder and decoder in the action generation VAE?

## Architecture Onboarding

- Component map:
  - Teacher models (F_i) -> Student model (F_φ) -> Distillation network (f_distill) -> Weight network (f_weight) -> Action generation model (G_θ) -> Policy and value networks

- Critical path: Teacher pretraining → Domain-selective distillation → Action generation → Policy learning → Environment interaction

- Design tradeoffs:
  - Using multiple teacher models increases computational complexity but enables better transfer across diverse domains
  - Action-conditioned pretraining requires action labels but provides better alignment with downstream tasks
  - Domain-selective approach requires computing similarity weights but focuses learning on relevant knowledge

- Failure signatures:
  - Poor performance despite long training: Check if domain similarity weights are meaningful or if all weights collapse to one domain
  - Unstable learning: Verify that the distillation loss is properly balanced with other losses
  - Action generation producing unrealistic actions: Check VAE reconstruction quality and KL divergence term

- First 3 experiments:
  1. Train teacher models on source domains and verify they can predict future frames reasonably well
  2. Implement domain-selective distillation with a single teacher model to validate the basic mechanism
  3. Add action generation module and test if it can reproduce source actions given states

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Vid2Act scale with an increasing number of source domains, and what are the computational trade-offs?
- Basis in paper: [inferred] The paper mentions that training complexity increases with more source domains and that they were limited by computing resources in their experiments.
- Why unresolved: The authors did not conduct experiments with a large number of source domains to evaluate scalability and computational costs.
- What evidence would resolve it: Experimental results comparing Vid2Act's performance and training time with varying numbers of source domains.

### Open Question 2
- Question: How robust is Vid2Act to domain shifts in the dynamics of the source and target tasks, beyond what is captured by the similarity weights?
- Basis in paper: [inferred] The paper assumes that the world models can measure domain relevance, but it's unclear how well this captures complex dynamics shifts.
- Why unresolved: The paper doesn't provide a detailed analysis of the model's robustness to different types of domain shifts in dynamics.
- What evidence would resolve it: Experiments showing Vid2Act's performance on tasks with varying degrees of dynamics similarity, including adversarial cases.

### Open Question 3
- Question: Can Vid2Act effectively transfer knowledge when the source domains have fundamentally different action spaces compared to the target task?
- Basis in paper: [inferred] The paper mentions that actions in offline datasets can be "far from expert policies" and that there might be "distribution shifts" in actions, but doesn't explore cases with entirely different action spaces.
- Why unresolved: The experiments likely used source domains with similar action spaces to the target, not exploring the full extent of potential action space differences.
- What evidence would resolve it: Experiments testing Vid2Act on target tasks with action spaces that are subsets, supersets, or completely different from those in the source domains.

## Limitations

- Limited scalability analysis: The paper doesn't explore how performance scales with an increasing number of source domains or the associated computational trade-offs.
- Unverified similarity weight reliability: The learned domain similarity weights could be unreliable when source domains have overlapping but distinct dynamics, and this is not thoroughly validated.
- Lack of action replay quality evaluation: The paper doesn't provide quantitative evaluation of the action generation quality or analyze failure modes where generated actions might mislead the policy.

## Confidence

- **High Confidence**: The core architecture and training procedure are well-specified and reproducible. The empirical results show consistent improvements across multiple benchmark tasks.
- **Medium Confidence**: The mechanism by which domain-selective distillation improves transfer learning is plausible but not rigorously proven. The assumptions about state representation comparability are reasonable but untested.
- **Low Confidence**: Claims about the relative importance of different components (dynamics transfer vs action replay) lack supporting ablation studies.

## Next Checks

1. **Ablation Study**: Remove the action replay component and retrain to quantify its contribution versus dynamics transfer alone.
2. **Weight Analysis**: Visualize and analyze the learned domain similarity weights across training to verify they capture meaningful task relationships rather than spurious correlations.
3. **Representation Comparison**: Test whether action-conditioned pretraining representations indeed transfer better than action-free pretraining by implementing a direct comparison on the same tasks.