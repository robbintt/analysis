---
ver: rpa2
title: Enhancing Neural Architecture Search with Multiple Hardware Constraints for
  Deep Learning Model Deployment on Tiny IoT Devices
arxiv_id: '2310.07217'
source_url: https://arxiv.org/abs/2310.07217
tags:
- search
- dnas
- memory
- cost
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently deploying deep
  neural networks (DNNs) on resource-constrained IoT devices by proposing a novel
  method called DUCCIO (DNAS Under Combined Constraints In One-shot). DUCCIO extends
  differentiable neural architecture search (DNAS) to incorporate multiple hardware
  constraints, such as memory footprint and latency, directly into the optimization
  process.
---

# Enhancing Neural Architecture Search with Multiple Hardware Constraints for Deep Learning Model Deployment on Tiny IoT Devices

## Quick Facts
- arXiv ID: 2310.07217
- Source URL: https://arxiv.org/abs/2310.07217
- Reference count: 40
- Key outcome: Reduces memory by up to 87.4% and latency by up to 54.2% on GAP8 while maintaining or improving accuracy

## Executive Summary
DUCCIO (DNAS Under Combined Constraints In One-shot) is a novel method that extends differentiable neural architecture search (DNAS) to incorporate multiple hardware constraints such as memory and latency directly into the optimization process. Unlike previous approaches requiring iterative hyperparameter tuning, DUCCIO enables generation of models respecting user-defined constraints in a single training run. The method was evaluated on five IoT-relevant benchmarks using the GAP8 ultra-low-power microcontroller, demonstrating significant reductions in memory and latency while maintaining or improving accuracy compared to state-of-the-art hand-tuned DNNs for TinyML.

## Method Summary
DUCCIO builds upon differentiable neural architecture search by introducing a multi-constraint loss formulation with max(0, R(θ) - T) regularization terms, discretized sampling with Gumbel-Softmax, and progressive λ scheduling. The method operates by optimizing a supernet containing multiple architectural choices, where the search process jointly optimizes network weights and architectural parameters. Discretized sampling ensures that cost constraints are respected at the architecture level, while progressive regularization strength prevents premature convergence to overly conservative architectures. The approach is compatible with both path-based and mask-based DNAS methods and introduces layer-wise constraints to optimize for specific memory hierarchies.

## Key Results
- Achieves up to 87.4% memory reduction and 54.2% latency reduction on GAP8 microcontroller
- Maintains or improves accuracy compared to state-of-the-art hand-tuned DNNs for TinyML
- Demonstrates significant speedups with minimal accuracy loss using layer-wise constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DUCCIO enforces constraints without iterative tuning using max(0, R(θ) - T) regularization terms.
- Mechanism: Penalizes only when cost exceeds target, ignoring sub-target values.
- Core assumption: Gradients can guide NAS parameters toward architectures staying within constraints without over-penalizing under-target models.
- Evidence anchors: Abstract states DUCCIO generates models respecting constraints in time comparable to single standard training; section describes using max function to penalize only when cost metric exceeds constraint.
- Break condition: Poor gradient signals for R(θ) << T may prevent optimizer from improving accuracy.

### Mechanism 2
- Claim: Discretized sampling ensures cost constraints respected at architecture level, not just continuous relaxation.
- Mechanism: Gumbel-Softmax with one-hot discretization samples only one path per module, aligning estimated cost during training with actual cost of final model.
- Core assumption: Cost estimated with discrete sampling correlates with cost of exported architecture, avoiding overestimation from continuous relaxation.
- Evidence anchors: Section claims continuous relaxation should not be applied during search; discretized sampling is fundamental to respect cost constraints.
- Break condition: High discretization noise may cause optimizer to converge to poor local minima.

### Mechanism 3
- Claim: Progressive λ scheduling prevents early convergence to overly conservative architectures, maintaining accuracy.
- Mechanism: Regularization strength starts small and increases linearly during warmup epochs, allowing exploration of higher-accuracy regions before being forced to meet constraints.
- Core assumption: Constant high λ causes search to prematurely shrink architectures below necessary size, sacrificing accuracy for constraint satisfaction.
- Evidence anchors: Section describes implementing scheduling of λj to ensure Rj(θ) < Tj at end of training; results show scheduling yields more stable results than fixed λj.
- Break condition: Schedule too slow may not meet constraints within training epochs.

## Foundational Learning

- Concept: Differentiable Neural Architecture Search (DNAS) and supernet training
  - Why needed here: DUCCIO builds on DNAS; understanding how supernets train architectural parameters is essential.
  - Quick check question: How does the Gumbel-Softmax relaxation enable differentiable sampling in DNAS?

- Concept: Hardware-aware NAS and memory/latency constraints
  - Why needed here: Paper targets IoT devices with strict memory and latency budgets; knowing how to model these is critical.
  - Quick check question: What are the differences between global and layer-wise memory constraints?

- Concept: Path-based vs mask-based NAS architectures
  - Why needed here: DUCCIO applies to both; recognizing their differences informs method choice and interpretation.
  - Quick check question: In mask-based NAS, how does channel pruning translate to memory savings?

## Architecture Onboarding

- Component map: Base DNAS (path-based or mask-based) -> Multi-constraint loss formulation with max(0, R(θ) - T) terms -> Discretized sampling with Gumbel-Softmax -> Progressive λ scheduling
- Critical path: 1) Warmup -> 2) Search (optimize W and θ jointly) -> 3) Export discretized architecture -> 4) Fine-tuning
- Design tradeoffs: Discretized sampling vs continuous relaxation balances constraint accuracy vs search stability; progressive λ vs constant λ balances speed vs accuracy
- Failure signatures: (a) Constraints not met -> λ schedule too slow; (b) Accuracy loss -> λ too aggressive or discretization too coarse; (c) Long search time -> supernet too large for path-based
- First 3 experiments:
  1. Run warmup on seed network, record initial accuracy and cost metrics
  2. Run DUCCIO with one constraint (e.g., model size) and monitor if constraint is met after search
  3. Compare accuracy and cost trade-offs for mask-based vs path-based DNAS on same task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DUCCIO performance scale when applied to different hardware platforms beyond GAP8, such as more complex SoCs or specialized accelerators?
- Basis in paper: [explicit] Evaluates DUCCIO on GAP8 and mentions method is flexible but doesn't explore performance on other hardware platforms.
- Why unresolved: Study limited to one hardware platform; performance on other platforms with different memory hierarchies or computational capabilities is unknown.
- What evidence would resolve it: Testing DUCCIO on variety of hardware platforms with different architectures and constraints, comparing results in terms of accuracy, latency, and memory usage.

### Open Question 2
- Question: What are the limitations of DUCCIO when dealing with extremely large models or those with very deep architectures?
- Basis in paper: [inferred] Paper doesn't discuss scalability with respect to model size or depth, focuses on relatively small models for IoT devices.
- Why unresolved: Method's effectiveness and efficiency in handling larger models with more complex architectures are not explored, crucial for understanding applicability to broader range of deep learning tasks.
- What evidence would resolve it: Applying DUCCIO to progressively larger and deeper models, analyzing performance in terms of search time, accuracy, and constraint satisfaction.

### Open Question 3
- Question: How does DUCCIO compare to other state-of-the-art NAS techniques in terms of computational efficiency and quality of final models?
- Basis in paper: [explicit] Compares DUCCIO to other DNAS methods but doesn't provide comprehensive comparison with all state-of-the-art NAS techniques, especially those not based on DNAS.
- Why unresolved: Relative strengths and weaknesses of DUCCIO compared to other NAS approaches are not fully elucidated, particularly in terms of search efficiency and model performance.
- What evidence would resolve it: Conducting thorough comparison of DUCCIO with various NAS techniques, including those based on reinforcement learning and evolutionary algorithms, on common set of benchmarks and hardware platforms.

## Limitations
- Core claims rely heavily on proposed multi-constraint loss formulation and progressive λ scheduling, with uncertainties around generalization to other IoT platforms and larger-scale models
- Discretization of sampling introduces potential noise that may affect search stability in more complex search spaces
- Method's reliance on accurate hardware cost estimation could be problematic if underlying cost models are not precise for different hardware architectures

## Confidence

- **High confidence**: Empirical results showing 87.4% memory and 54.2% latency reductions on GAP8 microcontroller are well-supported by presented experiments and comparisons with baseline methods
- **Medium confidence**: Claim that DUCCIO achieves constraint satisfaction without iterative tuning is supported by proposed mechanism, but generalization to other constraint types or search spaces requires further validation
- **Medium confidence**: Effectiveness of layer-wise constraints for optimizing DNNs for specific memory hierarchies is demonstrated on provided benchmarks, but applicability to more complex architectures or different memory systems needs additional testing

## Next Checks

1. **Cross-platform validation**: Test DUCCIO on different microcontroller architecture (e.g., ARM Cortex-M) to verify portability of method and accuracy of hardware cost models across platforms

2. **Search space scalability**: Apply DUCCIO to larger-scale image classification tasks (e.g., ImageNet) and evaluate whether progressive λ scheduling and discretized sampling maintain effectiveness in higher-dimensional search spaces

3. **Constraint type generalization**: Extend DUCCIO to incorporate additional hardware constraints such as energy consumption or peak memory usage, and assess whether max(0, R(θ) - T) regularization formulation remains effective for these new constraint types