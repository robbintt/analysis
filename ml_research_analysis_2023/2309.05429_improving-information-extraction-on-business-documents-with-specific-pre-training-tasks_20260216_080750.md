---
ver: rpa2
title: Improving Information Extraction on Business Documents with Specific Pre-Training
  Tasks
arxiv_id: '2309.05429'
source_url: https://arxiv.org/abs/2309.05429
tags:
- documents
- arxiv
- pre-training
- document
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of improving information extraction
  (IE) on business documents, such as invoices and receipts, by enhancing pre-training
  tasks and post-processing methods for LayoutLM, a Transformer-based language model.
  The authors introduce two novel pre-training tasks: Numeric Ordering, which teaches
  the model to compare and order numeric values, and Layout Inclusion, which focuses
  on understanding the 2D positional relationships of text within documents.'
---

# Improving Information Extraction on Business Documents with Specific Pre-Training Tasks

## Quick Facts
- **arXiv ID**: 2309.05429
- **Source URL**: https://arxiv.org/abs/2309.05429
- **Reference count**: 0
- **Primary result**: Introducing Numeric Ordering and Layout Inclusion pre-training tasks, plus ConfOpt post-processing, improves F1 scores on business document IE by 1.62 (SROIE) and 0.49 (BDC-PO).

## Executive Summary
This paper addresses the challenge of improving information extraction (IE) on business documents by enhancing LayoutLM with two novel pre-training tasks and a new post-processing algorithm. The authors introduce Numeric Ordering to teach models to compare numeric values and Layout Inclusion to understand 2D positional relationships in documents. Additionally, they propose ConfOpt, a dynamic programming decoder that maximizes model confidence under sequence constraints. These contributions lead to significant performance improvements on both public and private business document datasets.

## Method Summary
The method involves pre-training LayoutLM on a business document corpus (BDC) with three tasks: Masked Visual Language Modeling (MVLM), Numeric Ordering (NO), and Layout Inclusion (LI). MVLM masks tokens for standard language modeling, NO teaches relative numeric magnitude comparison, and LI focuses on understanding 2D spatial relationships. After pre-training, the model is fine-tuned on target datasets (SROIE and BDC-PO) using BIESO tagging and decoded with the ConfOpt algorithm, which optimizes sequence-level confidence rather than greedy token-wise decoding.

## Key Results
- F1 score on SROIE increases from 93.88 to 95.50
- F1 score on BDC-PO increases from 84.35 to 84.84
- ConfOpt post-processing improves performance by 1.06 F1 on SROIE and 0.24 on BDC-PO

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Numeric Ordering (NO) task improves model understanding of numeric magnitude relationships in documents.
- Mechanism: The model learns to classify numeric values relative to a randomly selected reference number, forcing it to parse and compare numeric magnitudes. Masking 15% of numeric tokens' text and position embeddings ensures the model relies on both textual and spatial context.
- Core assumption: Numeric values in business documents are meaningful for the task and can be reliably parsed from text.
- Evidence anchors:
  - [abstract]: "The second focuses on numeric values and their order of magnitude."
  - [section]: "Numeric Ordering (NO) focuses on numeric figures in the document and their relative values... the model must predict for every numeric figure in the document if its parsed value is smaller, equal or greater than a randomly selected number among the document."
  - [corpus]: Weak/no direct mention of numeric parsing challenges in related work; however, general numeric understanding is often assumed in business document IE.
- Break condition: If numeric parsing fails due to OCR errors or non-standard numeric formats, the task cannot teach meaningful magnitude relationships.

### Mechanism 2
- Claim: The Layout Inclusion (LI) task enhances the model's understanding of 2D document structure and positional relationships.
- Mechanism: The model learns to classify whether each token's midpoint lies inside a randomly placed rectangle, combining textual and spatial embeddings. Masking 15% of positional encodings forces reliance on both modalities.
- Core assumption: Document structure and token positions relative to visual elements (like tables) are crucial for IE.
- Evidence anchors:
  - [abstract]: "The first is aimed at better understanding the complex layout of documents..."
  - [section]: "Layout Inclusion (LI)... focuses on words in the 2D plane and their relative positioning... the model must then classify every token in the document into 2 groups: either inside or outside of the question token."
  - [corpus]: Limited explicit evidence; layout understanding is generally assumed important in document IE literature.
- Break condition: If spatial encodings are inaccurate or if the model overfits to the synthetic rectangle positions, it won't generalize to real document layouts.

### Mechanism 3
- Claim: The ConfOpt post-processing algorithm improves BIESO tag decoding by solving an optimization problem that maximizes model confidence under sequence constraints.
- Mechanism: Instead of greedy token-wise decoding, ConfOpt finds the sequence of BIESO tags that maximizes total model confidence while respecting the (BI*E)|S pattern, reducing errors from ambiguous local predictions.
- Core assumption: Model confidence scores are reliable indicators of correct tag sequences and that the (BI*E)|S pattern covers all valid entity structures.
- Evidence anchors:
  - [abstract]: "...a new post-processing algorithm to decode BIESO tags in Information Extraction that performs better with complex entities."
  - [section]: "We model the Information Extraction task as sequence tagging on tokens... we decided to decode a model's prediction by solving a basic optimization problem... The predicted sequence for a target label is the sequence that maximizes model confidence over the whole input sequence."
  - [corpus]: No direct mention of similar optimization-based decoding; mostly standard CRF or greedy methods in related work.
- Break condition: If model confidence scores are poorly calibrated or if the optimization becomes intractable for very long sequences, the benefits may diminish or performance could degrade.

## Foundational Learning

- Concept: Tokenization and WordPiece algorithm
  - Why needed here: The model operates on subword tokens; understanding tokenization is essential for interpreting model inputs and outputs.
  - Quick check question: What is the maximum sequence length used, and why is it limited?

- Concept: Positional encoding in Transformers
  - Why needed here: LayoutLM uses both 1D and 2D positional encodings; understanding their role is key to grasping how the model integrates spatial information.
  - Quick check question: How does LayoutLM's 2D positional encoding differ from standard 1D positional encoding?

- Concept: Masked Language Modeling (MLM) and self-supervised pre-training
  - Why needed here: Pre-training tasks (MVLM, NO, LI) rely on self-supervision; understanding MLM helps in designing and debugging new pre-training tasks.
  - Quick check question: What percentage of tokens are masked during MVLM pre-training, and why?

## Architecture Onboarding

- Component map: OCR text and position inputs -> LayoutLM Transformer -> Task-specific output heads -> ConfOpt decoder
- Critical path:
  1. Tokenize and encode input document
  2. Feed through LayoutLM Transformer
  3. Apply task-specific output head
  4. For IE: decode with ConfOpt to get entity spans
- Design tradeoffs:
  - Using base (vs large) LayoutLM reduces compute but may limit capacity
  - Limiting sequence length to 512 tokens may truncate long documents
  - Masking strategy in NO/LI forces multimodal reasoning but may slow convergence
  - ConfOpt adds decoding time but improves accuracy over greedy methods
- Failure signatures:
  - Erratic confidence scores → ConfOpt decoding may produce wrong entities
  - Poor numeric parsing → NO task teaches wrong magnitude relationships
  - Inaccurate spatial encodings → LI task cannot learn layout structure
  - Sequence too long → Model may miss entities at the end
- First 3 experiments:
  1. Run fine-tuning on SROIE with Ad-Hoc decoding; verify baseline F1 ≈ 93.88
  2. Replace Ad-Hoc with ConfOpt decoding; verify F1 improvement ≈ +1.06
  3. Pre-train on BDC with MVLM only; fine-tune on BDC-PO; verify F1 ≈ 84.77

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ConfOpt compare to other advanced decoding algorithms like CRF or span-based approaches in information extraction tasks?
- Basis in paper: [explicit] The paper compares ConfOpt to CRF and Ad-Hoc decoding methods, showing significant improvements with ConfOpt.
- Why unresolved: The paper only compares ConfOpt to two other methods (Ad-Hoc and CRF), leaving room for exploration of other advanced decoding techniques.
- What evidence would resolve it: Conducting experiments comparing ConfOpt to other state-of-the-art decoding algorithms like span-based or sequence-to-sequence models in various information extraction benchmarks.

### Open Question 2
- Question: How would incorporating visual information alongside text and layout features impact the performance of information extraction models?
- Basis in paper: [inferred] The paper mentions LayoutLMv2's use of visual information and its superior performance, suggesting potential benefits of multimodal approaches.
- Why unresolved: The paper focuses on text-only models, leaving the question of visual information integration unexplored.
- What evidence would resolve it: Implementing and evaluating models that combine text, layout, and visual features using architectures like LayoutLMv2 or similar multimodal transformers on business document datasets.

### Open Question 3
- Question: What is the impact of different pre-training dataset compositions on model generalization across various document types?
- Basis in paper: [explicit] The paper shows that pre-training on a business-specific dataset (BDC) improves performance compared to a general dataset (RVL-CDIP).
- Why unresolved: The paper only compares two datasets, and the effect of different compositions within business documents is not explored.
- What evidence would resolve it: Conducting experiments with various pre-training datasets composed of different ratios of document types (e.g., invoices, receipts, forms) and evaluating their impact on model performance across diverse document benchmarks.

## Limitations
- Limited dataset size and domain specificity: BDC dataset contains only 50k documents, raising concerns about generalization to other document types or languages.
- Evaluation scope constraints: Focuses on exact string matching for field extraction, which may not reflect practical IE needs where partial matches or semantic equivalence are acceptable.
- Implementation details missing: Critical implementation details are absent, including the specific handcrafted number parser for Numeric Ordering and exact training hyperparameters.

## Confidence
- High confidence: The architectural approach of adding specialized pre-training tasks (Numeric Ordering and Layout Inclusion) to LayoutLM is sound and well-motivated by the need for numeric and spatial understanding in business documents.
- Medium confidence: The reported performance improvements (+1.62 F1 on SROIE, +0.49 on BDC-PO) are statistically significant but may be influenced by dataset-specific factors.
- Low confidence: The claim that these specific pre-training tasks are optimal or necessary for business document IE is not well-supported.

## Next Checks
1. **Ablation study on pre-training tasks**: Run experiments removing Numeric Ordering and Layout Inclusion tasks individually to quantify their individual contributions. Compare against a baseline using only standard MLM pre-training to determine if the specialized tasks provide additive benefits.

2. **Cross-dataset generalization test**: Evaluate the pre-trained model on additional business document datasets (e.g., FUNSD, CORD, or other receipt/invoices datasets) to assess whether improvements generalize beyond the specific BDC and SROIE domains.

3. **Error analysis and failure case identification**: Perform detailed error analysis on both successful and failed extractions, categorizing errors by type (numeric parsing, spatial reasoning, entity boundary detection, etc.). Analyze whether failures correlate with specific document characteristics to identify model limitations and potential improvements.