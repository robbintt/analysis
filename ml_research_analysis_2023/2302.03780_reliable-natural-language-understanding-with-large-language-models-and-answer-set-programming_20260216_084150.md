---
ver: rpa2
title: Reliable Natural Language Understanding with Large Language Models and Answer
  Set Programming
arxiv_id: '2302.03780'
source_url: https://arxiv.org/abs/2302.03780
tags:
- reasoning
- predicates
- knowledge
- language
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework called STAR (Semantic-parsing Transformer
  and ASP Reasoner) that combines large language models (LLMs) with answer set programming
  (ASP) to perform natural language understanding tasks that require reasoning. The
  framework extracts knowledge from text using LLMs and then reasons over it using
  ASP, resulting in improved performance and explainability compared to using LLMs
  alone.
---

# Reliable Natural Language Understanding with Large Language Models and Answer Set Programming

## Quick Facts
- arXiv ID: 2302.03780
- Source URL: https://arxiv.org/abs/2302.03780
- Reference count: 2
- This paper proposes STAR, a framework combining LLMs with ASP for reliable natural language understanding with improved performance and explainability.

## Executive Summary
This paper introduces STAR (Semantic-parsing Transformer and ASP Reasoner), a framework that addresses the reasoning limitations of large language models (LLMs) by combining them with Answer Set Programming (ASP). The framework extracts structured knowledge from text using LLMs, then applies ASP to perform reliable reasoning over this knowledge. Experimental results on three tasks (qualitative reasoning, mathematical reasoning, and goal-directed conversation) demonstrate significant performance improvements, particularly for smaller LLMs. The STAR framework also provides explainability by generating justifications for its responses through proof trees.

## Method Summary
The STAR framework combines fine-tuned LLMs with ASP reasoning to solve natural language understanding tasks. The method involves fine-tuning GPT-3 variants (Davinci and Curie) on the QuaRel dataset for predicate generation, encoding commonsense knowledge in ASP, and using s(CASP) for reasoning. The approach follows a three-step process: (1) LLMs extract predicates from input text, (2) predicates are combined with ASP-encoded knowledge, and (3) s(CASP) reasons over the combined knowledge to produce answers with justifications. The framework was evaluated on three tasks: qualitative reasoning using QuaRel, mathematical reasoning using algebra word problems, and goal-directed conversation using the E2E dataset.

## Key Results
- STAR achieves 93.1% accuracy on the QuaRel dataset, outperforming GPT-3 Davinci (71.6%) and Curie (56.1%)
- The framework shows particular effectiveness with smaller LLMs, bridging the reasoning gap between model sizes
- STAR generates explainable responses through proof trees, demonstrating reliable reasoning over extracted predicates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The STAR framework bridges the reasoning gap in LLMs by offloading reasoning to ASP.
- Mechanism: STAR uses LLMs to extract knowledge from text in the form of predicates, then applies ASP to reason over these predicates. This separation allows LLMs to focus on semantic parsing while ASP handles complex reasoning tasks.
- Core assumption: LLMs are proficient at extracting deep structure from sentences, and ASP is effective at performing reliable reasoning over structured knowledge.
- Evidence anchors:
  - [abstract] "We propose STAR, a framework that combines LLMs with Answer Set Programming (ASP). We show how LLMs can be used to effectively extract knowledge—represented as predicates—from language. Goal-directed ASP is then employed to reliably reason over this knowledge."
  - [section 2.1] "We theorize that given the vast pre-training they go through, LLMs can be used to automatically extract knowledge inherent in the text, just like humans do."
- Break condition: If LLMs fail to accurately extract knowledge as predicates, or if the ASP knowledge base is insufficient for the reasoning task.

### Mechanism 2
- Claim: The STAR framework provides explainability by generating justifications for responses.
- Mechanism: Since reasoning is performed using s(CASP), which can generate justifications as proof trees, the STAR framework can provide explanations for its responses by tracing the reasoning steps.
- Core assumption: s(CASP) is capable of generating proof trees that justify the reasoning process.
- Evidence anchors:
  - [abstract] "NLU applications developed using the STAR framework are also explainable: along with the predicates generated, a justification in the form of a proof tree can be produced for a given output."
  - [section 2.2] "Additionally, s(CASP) system's interface with a constraint solver (over reals) allows for sound non-monotonic reasoning with constraints (useful for solving algebra problems in one of the NLU applications we discuss later)."
- Break condition: If the proof tree generation in s(CASP) is too complex or fails to capture the reasoning steps clearly.

### Mechanism 3
- Claim: The STAR framework improves reliability by preventing LLMs from directly performing reasoning.
- Mechanism: By shifting the reasoning burden to ASP, the STAR framework avoids potential calculation errors or missing reasoning steps that LLMs might make when reasoning directly.
- Core assumption: ASP is more reliable than LLMs for performing complex reasoning tasks.
- Evidence anchors:
  - [abstract] "Our experiments reveal that STAR is able to bridge the gap of reasoning in NLU tasks, leading to significant performance improvements, especially for smaller LLMs, i.e., LLMs with a smaller number of parameters."
  - [section 4] "Since the computation is done by the reasoning process externally (as seen above), it circumvents any potential calculation mistakes the LLM might make, making it reliable."
- Break condition: If the ASP knowledge base is incomplete or incorrect, leading to flawed reasoning despite the LLM's accurate predicate extraction.

## Foundational Learning

- Concept: Semantic parsing
  - Why needed here: Semantic parsing is crucial for converting natural language sentences into structured predicates that can be processed by the reasoning system.
  - Quick check question: How does semantic parsing help in bridging the gap between natural language understanding and formal reasoning?

- Concept: Answer Set Programming (ASP)
  - Why needed here: ASP is used to perform reliable reasoning over the structured knowledge extracted from text, enabling complex inference and decision-making.
  - Quick check question: What are the key features of ASP that make it suitable for commonsense reasoning in natural language understanding?

- Concept: Explainable AI
  - Why needed here: Explainability is essential for understanding the reasoning process and building trust in the system's outputs, especially in critical applications.
  - Quick check question: How does the STAR framework ensure explainability in its reasoning process?

## Architecture Onboarding

- Component map: LLM -> Predicate Extractor -> ASP System -> Answer Generator
- Critical path:
  1. Input text is processed by the LLM to extract predicates.
  2. Extracted predicates are combined with the ASP knowledge base.
  3. A query is generated and executed against the combined knowledge using the ASP system.
  4. The ASP system returns the answer and generates a justification if needed.

- Design tradeoffs:
  - Accuracy vs. Efficiency: Using a more complex LLM might improve predicate extraction accuracy but could increase computational cost.
  - Explainability vs. Performance: Providing detailed justifications can improve explainability but might impact system performance.

- Failure signatures:
  - Incorrect predicate extraction by the LLM, leading to flawed reasoning by the ASP system.
  - Incomplete or incorrect ASP knowledge base, resulting in unreliable reasoning.
  - Query generation issues, causing the ASP system to produce irrelevant or incorrect answers.

- First 3 experiments:
  1. Test the LLM's predicate extraction accuracy on a sample dataset and compare it to human annotations.
  2. Evaluate the ASP system's reasoning performance on a set of problems with known answers and justifications.
  3. Assess the overall STAR framework's performance on a real-world natural language understanding task, comparing it to LLM-only and traditional approaches.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the STAR framework compare to other state-of-the-art approaches for qualitative reasoning, mathematical reasoning, and goal-directed conversation?
- Basis in paper: Explicit
- Why unresolved: The paper only compares the performance of the STAR framework to a few baseline models, and does not provide a comprehensive comparison to other state-of-the-art approaches.
- What evidence would resolve it: Experimental results comparing the STAR framework to other state-of-the-art approaches on the three tasks mentioned in the paper.

### Open Question 2
- Question: How does the performance of the STAR framework scale with the size of the commonsense knowledge base?
- Basis in paper: Inferred
- Why unresolved: The paper does not provide any experiments or analysis on how the performance of the STAR framework scales with the size of the commonsense knowledge base.
- What evidence would resolve it: Experimental results showing the performance of the STAR framework on the three tasks mentioned in the paper, using commonsense knowledge bases of different sizes.

### Open Question 3
- Question: How does the performance of the STAR framework compare to other approaches that use ASP for reasoning, such as the s(CASP) system?
- Basis in paper: Inferred
- Why unresolved: The paper does not provide any comparison between the STAR framework and other approaches that use ASP for reasoning.
- What evidence would resolve it: Experimental results comparing the performance of the STAR framework to other approaches that use ASP for reasoning on the three tasks mentioned in the paper.

## Limitations

- The paper lacks detailed specifications of fine-tuning hyperparameters, making exact reproduction challenging.
- Knowledge base construction methodology is not systematically addressed for broader applications.
- Performance scaling with very large models and different reasoning domains needs further validation.

## Confidence

**High confidence**: The core mechanism of separating semantic parsing (LLM) from reasoning (ASP) is well-supported by evidence. The explainability claim is directly validated through proof tree generation.

**Medium confidence**: Performance improvements are demonstrated but are task-specific. The generalizability to other reasoning domains needs further validation.

**Low confidence**: Claims about reliability improvements are primarily theoretical, based on the assumption that ASP reasoning is inherently more reliable than LLM reasoning.

## Next Checks

1. **Predicate extraction accuracy validation**: Test the fine-tuned LLM's predicate generation accuracy on a held-out validation set with human-annotated predicates, measuring precision and recall.

2. **Knowledge base coverage assessment**: Systematically evaluate the ASP knowledge base completeness by attempting to solve problems requiring knowledge outside the current encoding and measuring failure rates.

3. **Scalability experiment**: Compare STAR framework performance across multiple LLM sizes (small, medium, large) on the same reasoning tasks to quantify the scaling relationship between model size and performance gain.