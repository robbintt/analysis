---
ver: rpa2
title: 'SneakyPrompt: Jailbreaking Text-to-image Generative Models'
arxiv_id: '2305.12082'
source_url: https://arxiv.org/abs/2305.12082
tags:
- safety
- adversarial
- sneakyprompt
- prompt
- filter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes the first automated framework, called SneakyPrompt,
  to evaluate the robustness of existing safety filters via searching the prompt space
  to find adversarial prompts that not only bypasses the filter but also preserve
  the semantics. The paper also categorizes existing safety filters into three types:
  image-based, text-based, and image-text-based, and perform the evaluation.'
---

# SneakyPrompt: Jailbreaking Text-to-image Generative Models

## Quick Facts
- arXiv ID: 2305.12082
- Source URL: https://arxiv.org/abs/2305.12082
- Reference count: 40
- Key outcome: First automated framework to find adversarial prompts that bypass safety filters while preserving semantics, successfully compromising all tested filters including closed-box DALL·E 2

## Executive Summary
This paper introduces SneakyPrompt, an automated framework that searches for adversarial prompts capable of bypassing safety filters in text-to-image generative models while maintaining semantic integrity. The key innovation is treating safety filters as binary classifiers in text embedding space and using reinforcement learning to efficiently navigate the search space for token replacements. SneakyPrompt outperforms existing text-based adversarial methods, achieving a 88.5% bypass rate on Stable Diffusion compared to 1.5% for baselines, and uniquely succeeds against closed-box filters like DALL·E 2 where previous methods fail.

## Method Summary
SneakyPrompt operates by first identifying sensitive tokens in target prompts, then using reinforcement learning to search for semantically similar replacement tokens that bypass safety filters. The approach models filters as binary classifiers in embedding space, with an RL agent guided by rewards based on semantic similarity and filter bypass success. Three search strategies are implemented: BruteForce, GreedySearch, and BeamSearch as baselines, with the RL-based SneakyPrompt-RL as the primary method. The framework expands the search space iteratively when initial attempts fail, and can use either online queries to the target model or offline shadow text encoders for efficiency.

## Key Results
- SneakyPrompt achieves 88.5% bypass rate on Stable Diffusion vs 1.5% for baseline text-based adversarial methods
- Successfully bypasses closed-box DALL·E 2 safety filter where no existing methods work
- Maintains high semantic similarity (FID scores comparable to or better than baselines) while achieving higher bypass rates
- RL-based search is more query-efficient than brute-force approaches, finding valid adversarial prompts with fewer online queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SneakyPrompt finds adversarial prompts by searching for semantically similar tokens that bypass the safety filter's decision boundary
- Mechanism: The approach models the safety filter as a binary classifier in text embedding space and searches for tokens whose embeddings cross the decision boundary while maintaining semantic similarity to the target prompt
- Core assumption: Safety filters operate with a continuous decision boundary in embedding space that can be approximated through query-based sampling
- Evidence anchors:
  - [abstract] "Our key insight is to search for alternative tokens in a prompt that generates NSFW images so that the generated prompt (called an adversarial prompt) bypasses existing safety filters"
  - [section 4.1] "A safety filter—no matter text-, image-, or text-image-based—can be considered as a binary (i.e., sensitive or non-sensitive) classifier with a decision boundary in the text embedding space"
  - [corpus] Weak - neighboring papers focus on jailbreaking but don't explain the mechanism in detail
- Break condition: If the safety filter uses non-differentiable or non-continuous decision boundaries, or if semantic similarity cannot be preserved while crossing the boundary

### Mechanism 2
- Claim: Reinforcement learning guides the search process to find adversarial prompts more efficiently than brute force methods
- Mechanism: An RL agent receives rewards based on semantic similarity between generated images and target prompts, and penalties when the safety filter is not bypassed, iteratively improving the search policy
- Core assumption: The RL agent can learn a policy that effectively navigates the search space to find valid adversarial prompts
- Evidence anchors:
  - [abstract] "Specifically, SneakyPrompt utilizes reinforcement learning (RL) to guide the perturbation of tokens"
  - [section 4.3] "Our implementation of SneakyPrompt uses the Actor-Critic method along with the deterministic policy... The agent's task is to maximize the cumulative reward"
  - [corpus] Weak - neighboring papers mention RL but don't provide detailed implementation
- Break condition: If the reward signal is too sparse or noisy, preventing effective learning

### Mechanism 3
- Claim: Search space expansion strategy increases bypass rate by iteratively adding more sensitive tokens to replace
- Mechanism: When the RL agent fails to find a bypass after multiple attempts, the algorithm expands the search space by including additional sensitive tokens from the prompt
- Core assumption: Adding more tokens to replace increases the chances of finding a successful adversarial prompt
- Evidence anchors:
  - [section 4.3] "Strategy One: Search Space Expansion... SneakyPrompt-RL stops the current search and adds one more sensitive word to replace if the agent searched prompt cannot pass the safety filter continuously"
  - [section 6.2] "Such an expansion strategy not only increases the bypass rate but also decreases the number of queries"
  - [corpus] Weak - no neighboring papers describe this specific strategy
- Break condition: If the expanded search space becomes too large to search effectively within query limits

## Foundational Learning

- **Concept: Reinforcement Learning (RL) fundamentals**
  - Why needed here: SneakyPrompt uses RL to guide the search for adversarial prompts efficiently
  - Quick check question: What are the key components of an RL agent (state, action, reward, policy) and how do they apply to finding adversarial prompts?

- **Concept: Text embeddings and similarity metrics**
  - Why needed here: The approach relies on computing semantic similarity between text embeddings and generated images
  - Quick check question: How do CLIP embeddings capture semantic meaning and how is cosine similarity used to measure semantic similarity?

- **Concept: Adversarial example generation**
  - Why needed here: SneakyPrompt is fundamentally an adversarial attack method targeting safety filters
  - Quick check question: What distinguishes adversarial examples that preserve semantics from those that don't, and why is this distinction important for SneakyPrompt?

## Architecture Onboarding

- **Component map**: Input prompt → Search space generation → RL agent sampling → Online query evaluation → Similarity assessment → Output adversarial prompt
- **Critical path**: Input prompt → Search space generation → RL agent sampling → Online query evaluation → Similarity assessment → Output adversarial prompt
- **Design tradeoffs**:
  - Search space size vs. query efficiency: Larger search spaces increase bypass potential but require more queries
  - Semantic similarity threshold vs. bypass rate: Higher thresholds preserve semantics better but may reduce bypass success
  - Reward function choice: Image-based rewards provide better semantic preservation but require more online queries
- **Failure signatures**:
  - Low bypass rate despite high query count: Indicates ineffective search policy or poor reward shaping
  - High FID scores: Suggests semantic similarity threshold is too low or search is finding semantically distant prompts
  - RL agent not learning: May indicate reward signal is too sparse or search space is too large
- **First 3 experiments**:
  1. Baseline test: Run SneakyPrompt on a simple text-based safety filter with known parameters to verify basic functionality
  2. Parameter sensitivity: Test different semantic similarity thresholds (0.22-0.30) on a fixed dataset to find optimal balance
  3. RL vs. baseline comparison: Compare SneakyPrompt-RL against greedy search on the same dataset to quantify efficiency gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the exact mechanisms of DALL·E 2's closed-box safety filter that SneakyPrompt managed to bypass?
- Basis in paper: [explicit] The paper mentions that SneakyPrompt successfully bypassed DALL·E 2's closed-box safety filter, but does not provide details on how the filter works internally.
- Why unresolved: The filter is closed-box, meaning its internal mechanisms are not publicly documented or accessible for analysis.
- What evidence would resolve it: Detailed documentation or reverse engineering of DALL·E 2's safety filter mechanisms would provide clarity on how it was bypassed.

### Open Question 2
- Question: How does the randomness introduced by the diffusion model affect the long-term effectiveness of adversarial prompts in re-use attacks?
- Basis in paper: [explicit] The paper notes that re-use bypass rates drop due to the random seeds used in the diffusion model, but does not explore the extent or nature of this effect.
- Why unresolved: The paper does not provide a detailed analysis of how randomness affects the stability and effectiveness of adversarial prompts over multiple uses.
- What evidence would resolve it: A comprehensive study analyzing the impact of different random seeds on the generation of adversarial prompts and their bypass rates over multiple iterations would provide insights.

### Open Question 3
- Question: What are the specific differences in the architecture and parameters between the shadow text encoder ˆE and the original text encoder E that affect the performance of SneakyPrompt?
- Basis in paper: [explicit] The paper discusses the use of shadow text encoders and their potential differences from the original encoder, but does not detail how these differences impact the performance of SneakyPrompt.
- Why unresolved: The paper does not provide a comparative analysis of the performance of SneakyPrompt when using different shadow text encoders with varying architectures and parameters.
- What evidence would resolve it: An in-depth comparison of the performance of SneakyPrompt using shadow text encoders with different architectures and parameters would clarify the impact on bypass rates and image quality.

## Limitations

- The evaluation is limited to seven specific safety filters, which may not represent the full diversity of safety mechanisms in practice
- Reliance on FID scores as a proxy for semantic similarity has known limitations in capturing perceptual similarity
- The RL-based approach requires substantial computational resources for training, potentially limiting reproducibility

## Confidence

**High Confidence** - The paper demonstrates clear empirical evidence that SneakyPrompt outperforms existing text-based adversarial methods in bypassing safety filters, with quantitative results showing bypass rates of 88.5% compared to 1.5% for baseline methods on Stable Diffusion. The methodology is well-documented with specific algorithms and evaluation metrics.

**Medium Confidence** - The paper's core insight about modeling safety filters as binary classifiers in embedding space is theoretically sound, but the empirical validation is limited to the specific filters tested. The RL-based search strategy shows promise but may not generalize to all types of safety filters or text-to-image models.

**Low Confidence** - The paper's claims about the closed-box DALL·E 2 filter being completely vulnerable to SneakyPrompt are difficult to verify independently due to the proprietary nature of the filter and lack of access to the actual implementation details.

## Next Checks

1. **Cross-model generalization test**: Evaluate SneakyPrompt on a broader range of text-to-image models beyond Stable Diffusion, including newer models like Midjourney or Google's Imagen, to assess whether the approach generalizes beyond the tested models.

2. **Human evaluation of semantic preservation**: Conduct a human study where participants rate the semantic similarity between original prompts and adversarial prompts, rather than relying solely on FID scores, to validate that the generated adversarial prompts truly preserve intended meaning.

3. **Query efficiency benchmarking**: Systematically measure the number of queries required for successful bypass across different prompt categories and filter types to determine whether the RL-based approach provides consistent efficiency gains over simpler search strategies.