---
ver: rpa2
title: Robust Nonparametric Hypothesis Testing to Understand Variability in Training
  Neural Networks
arxiv_id: '2310.00541'
source_url: https://arxiv.org/abs/2310.00541
tags:
- test
- training
- accuracy
- which
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of quantifying variability in deep
  neural network (DNN) training, where different runs produce models with similar
  test accuracy but potentially different underlying functions. The authors propose
  a new measure based on the distribution of network outputs (logit gaps) before thresholding,
  using a robust nonparametric hypothesis testing framework.
---

# Robust Nonparametric Hypothesis Testing to Understand Variability in Training Neural Networks

## Quick Facts
- arXiv ID: 2310.00541
- Source URL: https://arxiv.org/abs/2310.00541
- Reference count: 0
- Key outcome: A new measure based on logit gap distributions and trimmed Kolmogorov-Smirnov tests that captures variability in DNN training not reflected in test accuracy or churn

## Executive Summary
This paper addresses the fundamental question of whether different training runs of deep neural networks produce models that compute the same underlying function, even when they achieve similar test accuracy. The authors propose a novel approach using robust nonparametric hypothesis testing on the distribution of network outputs (logit gaps) before thresholding. By comparing each model to a leave-one-out ensemble using trimmed Kolmogorov-Smirnov tests, they quantify dissimilarity through the trimming level α that allows acceptance of the null hypothesis. Experiments on CIFAR-10 with a small CNN demonstrate that this measure captures aspects of model variability invisible to traditional metrics.

## Method Summary
The method trains multiple models under different randomness scenarios (initialization, batch ordering, data sampling) and computes logit gap distributions on a held-out test set. For each model, it compares the empirical CDF of logit gaps to a leave-one-out ensemble (LOOE) using a robust two-sample KS test with α-trimming. The largest α level at which the hypothesis test fails to reject similarity serves as a measure of model dissimilarity. This approach leverages robust statistics to make the test resilient to large sample sizes while maintaining statistical validity.

## Key Results
- Models with similar test accuracy can have very different underlying functions, captured by logit gap distributions
- The trimming-based KS test can distinguish model similarity while being robust to sample size effects
- Different sources of randomness (initialization, batch ordering, data sampling) contribute differently to model variability depending on training epoch
- The proposed measure can identify models that are better representatives of the training algorithm's output distribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using logit gap distributions before thresholding captures more granular model variability than test accuracy or churn
- Mechanism: By examining continuous outputs (logits) rather than binary predictions, the method detects differences in model confidence and uncertainty patterns invisible to accuracy-based metrics
- Core assumption: Models with similar accuracy but different logit distributions represent meaningfully different functions
- Evidence anchors: Abstract statement that "models with similar test accuracy may not be computing the same function"; section noting that "Looking at m(x) directly gives us other approaches to assess whether models are similar or not"
- Break condition: If logit gap distributions converge to similar shapes despite different training runs, the measure loses discriminative power

### Mechanism 2
- Claim: The trimming-based robust KS test can distinguish model similarity while being resilient to sample size effects
- Mechanism: Finding the maximum α level at which the hypothesis test fails to reject similarity quantifies dissimilarity in a way robust to large sample sizes and outliers
- Core assumption: The LOOE ensemble represents a meaningful baseline that captures "typical" model behavior
- Evidence anchors: Section stating "We use concepts derived from robust statistics" and explaining the advantage of computing "the closest L∞ approximation to F0"
- Break condition: If the LOOE itself becomes unstable or if trimming doesn't meaningfully filter noise, the test loses validity

### Mechanism 3
- Claim: The measure evolves to capture different aspects of variability at different training stages
- Mechanism: Early in training, initialization dominates variability; later, other sources become more important, and the measure captures this temporal shift
- Core assumption: Different randomness sources contribute differently to model variability depending on training epoch
- Evidence anchors: Abstract noting that "different sources of randomness...contribute differently to model variability depending on training epoch"; section showing that "At smaller epochs, although test accuracy improves, α remains large for most models"
- Break condition: If training dynamics change fundamentally (e.g., different architectures), the epoch-dependent patterns may not hold

## Foundational Learning

- Concept: Kolmogorov-Smirnov test and goodness-of-fit testing
  - Why needed here: Forms the statistical foundation for comparing empirical distributions of logit gaps
  - Quick check question: What does the KS test statistic measure, and how is it used to accept/reject the null hypothesis?

- Concept: Robust statistics and trimming methods
  - Why needed here: Addresses sensitivity of standard KS tests to large sample sizes and outliers
  - Quick check question: How does α-trimming modify the hypothesis test, and what does the optimal α represent?

- Concept: Leave-one-out ensemble (LOOE) methodology
  - Why needed here: Provides practical baseline for comparison when true expected distribution is unknown
  - Quick check question: Why use LOOE instead of full ensemble, and how does this affect statistical properties of the test?

## Architecture Onboarding

- Component map: Data preprocessing -> Model architecture -> Training loop -> Evaluation -> Analysis
- Critical path: 1) Load and preprocess data, 2) Train M models under specified randomness scenario, 3) Compute logit gap distributions, 4) Perform robust KS tests comparing to LOOE, 5) Determine maximum α for each model, 6) Analyze results across scenarios and epochs
- Design tradeoffs: Small model size enables many runs but may not capture full complexity; binary classification simplifies analysis but limits generalizability; fixed test set size affects statistical power; bootstrap resampling adds computational cost but improves robustness
- Failure signatures: α values near 1.0 for all models suggests inability to distinguish them; high variance in α across models indicates instability; α not decreasing over epochs suggests models not converging to common distribution; computational timeout during bootstrap resampling
- First 3 experiments: 1) Replicate results on CIFAR-10 binary classification with small CNN, 2) Compare α measure with test accuracy and churn across training epochs, 3) Isolate effects of different randomness sources by training under Sinit, Sbatch, and Strain scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does the α measure generalize to larger neural network architectures beyond the small CNN used in this study?
- Basis in paper: The authors explicitly state "future extensions of this work include applications to very large deep net models" as a limitation
- Why unresolved: The paper only demonstrates the method on a small CNN architecture, and scaling to larger models may introduce new challenges in computational efficiency or statistical power
- What evidence would resolve it: Experiments showing the α measure performs consistently on various larger architectures (ResNets, Transformers, etc.) with comparable sensitivity to model variability

### Open Question 2
- Question: Can the robust KS testing framework be effectively extended to multi-class classification problems?
- Basis in paper: The authors state "extensions to multi-class classification" as a future direction and acknowledge their current focus on binary classification
- Why unresolved: The logit gap formulation and CDF comparison may not directly translate to multi-class settings where softmax outputs create a more complex probability space
- What evidence would resolve it: A modified robust testing framework that successfully quantifies model variability in multi-class settings with comparable interpretability to the binary case

### Open Question 3
- Question: Would alternative distance metrics like Wasserstein distance provide more informative measures of model variability than the L∞-based KS test?
- Basis in paper: The authors mention "explore two-sample hypothesis testing based on other distance metrics like the Wasserstein metric" as a potential future direction
- Why unresolved: While the L∞-based KS test is sensitive to extreme differences, it may not capture the overall distributional differences as effectively as other metrics considering the entire distribution shape
- What evidence would resolve it: Comparative studies showing whether Wasserstein-based or other distance metrics provide more stable or interpretable measures of model variability across different training scenarios

## Limitations
- The analysis is restricted to binary classification with a small CNN, limiting generalizability
- The assumption that LOOE represents a meaningful baseline is reasonable but untested against alternatives
- The robustness claims depend on the choice of trimming parameter α and specific bootstrap implementation
- Only one dataset (CIFAR-10) and small architecture were tested

## Confidence
- Claim about logit gap distributions capturing additional variability: Medium
- Claim about trimming-based KS test robustness: Medium
- Claim about epoch-dependent variability patterns: Low

## Next Checks
1. Test the method on multi-class CIFAR-10 classification to verify scalability
2. Compare results with alternative ensemble baselines (full ensemble vs LOOE)
3. Validate findings on a different architecture (e.g., ResNet-18) and dataset (e.g., ImageNet subset)