---
ver: rpa2
title: Learning Multi-graph Structure for Temporal Knowledge Graph Reasoning
arxiv_id: '2312.03004'
source_url: https://arxiv.org/abs/2312.03004
tags:
- temporal
- graph
- learning
- historical
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles temporal knowledge graph (TKG) extrapolation
  - predicting future events in knowledge graphs with timestamps. Existing methods
  struggle to capture diverse dependencies in TKGs, particularly correlations across
  timestamps and temporal periodicity.
---

# Learning Multi-graph Structure for Temporal Knowledge Graph Reasoning

## Quick Facts
- arXiv ID: 2312.03004
- Source URL: https://arxiv.org/abs/2312.03004
- Reference count: 40
- This paper proposes Learning Multi-graph Structure (LMS) for temporal knowledge graph extrapolation, achieving up to 2.60% improvement in time-aware filtered MRR and 6.47% improvement in raw MRR over state-of-the-art models.

## Executive Summary
This paper tackles the challenge of temporal knowledge graph (TKG) extrapolation - predicting future events in knowledge graphs with timestamps. Existing methods struggle to capture diverse dependencies in TKGs, particularly correlations across timestamps and temporal periodicity. To address this, the authors propose Learning Multi-graph Structure (LMS), which learns from three graph perspectives: evolutional patterns along timestamps, query-specific correlations across timestamps via a union graph, and temporal semantic dependencies. LMS incorporates an adaptive gate to merge entity representations effectively and integrates timestamp semantics into graph attention and time-aware decoders. Experiments on five benchmarks show LMS outperforms state-of-the-art extrapolation models.

## Method Summary
LMS addresses TKG extrapolation by learning from three distinct graph perspectives. The Evolutional Graph Learning module captures sequential patterns by aggregating recent graph snapshots with GRU. The Union Graph Learning module constructs query-specific union graphs to learn cross-timestamp correlations relevant to specific queries. The Temporal Graph Learning module models periodic temporal patterns using RGCN on timestamp graphs with Time2Vec embeddings. An adaptive gate merges evolutional and union representations, while a historical indicator matrix narrows prediction scope. The model employs two decoders - one incorporating historical statistics and one without - to produce final predictions. Training uses Adam optimizer with a weighted loss function balancing task performance and historical relevance.

## Key Results
- LMS achieves up to 2.60% improvement in time-aware filtered MRR over state-of-the-art models
- LMS shows up to 6.47% improvement in raw MRR metrics
- Ablation studies demonstrate the effectiveness of each module, with union graph learning showing the most significant contribution
- Model demonstrates consistent performance across five benchmark datasets with varying characteristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-graph modeling captures complementary temporal and structural dependencies better than single-graph methods.
- Mechanism: LMS separately learns from evolutional graph (recent sequential patterns), union graph (query-specific cross-timestamp correlations), and temporal graph (periodic timestamp interactions), then adaptively fuses them.
- Core assumption: Different graph perspectives encode distinct but complementary information needed for extrapolation.
- Evidence anchors:
  - [abstract] "LMS comprises three distinct modules... concurrent and evolutional patterns along timestamps, query-specific correlations across timestamps, and semantic dependencies of timestamps"
  - [section] "LMS begins by focusing on the most recent history, capturing concurrent interactions and temporal patterns through the Evolutional Graph Learning... To explicitly model structural correlations among crucial entities across timestamps, LMS further introduces Union Graph Learning... Additionally, from a global perspective, we present Temporal Graph Learning..."
  - [corpus] Weak - related works focus on single-graph approaches, but do not directly test multi-graph combinations
- Break condition: If the three graph perspectives share redundant information or if the adaptive gate cannot effectively combine them.

### Mechanism 2
- Claim: Query-specific union graph construction improves prediction accuracy by focusing on relevant entities.
- Mechanism: LMS extracts facts containing query subject/object from historical snapshots to build a union graph, then applies GAT with temporal embeddings to learn cross-timestamp correlations.
- Core assumption: Events most relevant to queries provide stronger predictive signals than all historical events.
- Evidence anchors:
  - [abstract] "constructs a query-specific union graph to learn structural correlations across time"
  - [section] "We construct a union graph, denoted as UG, using these query-specific facts... we retain temporal information by incorporating the original timestamp t, as an attribute of the relations within TKGs"
  - [corpus] Weak - related works use entire historical context without query-specific filtering
- Break condition: If query subjects/objects do not consistently appear in relevant future events.

### Mechanism 3
- Claim: Historical indicator matrix improves predictions by narrowing prediction scope to historically relevant entities.
- Mechanism: LMS constructs indicator matrix I_s,r_t that marks entities appearing with (s,r) before timestamp t, then uses it to constrain decoder predictions.
- Core assumption: Entities appearing with same subject-relation pairs historically are more likely to appear in future predictions.
- Evidence anchors:
  - [abstract] "incorporates an indicator... to make statistics of whole historical repetitive events and thereby narrow down the scope of predictions"
  - [section] "we construct an indicator matrix I_s,r_t ∈ Z^|E|×|R|×|E|, which encompasses all facts occurring before timestamp t with a subject entity s and relation r"
  - [corpus] Weak - related works do not use historical indicators for scope narrowing
- Break condition: If historical patterns do not generalize to future predictions or if indicator introduces excessive bias.

## Foundational Learning

- Concept: Graph Neural Networks for multi-relational data
  - Why needed here: TKGs are multi-relational graphs where entities connect through various relations, requiring specialized GNN architectures
  - Quick check question: How does CompGCN handle different relation types differently from standard GCN?

- Concept: Temporal graph modeling and periodicity
  - Why needed here: TKGs evolve over time, and capturing periodic patterns (e.g., weekly, monthly events) is crucial for accurate extrapolation
  - Quick check question: What temporal granularities are used in the temporal graph, and why are inverse relations important?

- Concept: Attention mechanisms for relational graphs
  - Why needed here: Learning which entity-relation combinations are most important for specific predictions requires relational-aware attention
  - Quick check question: How does the UGAT attention score incorporate temporal information alongside entity and relation embeddings?

## Architecture Onboarding

- Component map:
  - Input: TKG sequence {G₁, G₂, ..., G_T}, query (s, r, ?, t)
  - EGL: Graph aggregation + GRU for evolutional patterns
  - UGL: Query-specific union graph construction + UGAT
  - TGL: Temporal graph with Time2Vec embeddings + RGCN
  - Adaptive gate: Merges EGL and UGL outputs
  - Indicator: Historical statistics matrix I_s,r_t
  - Decoders: Historical (with indicator) + Raw (without indicator)
  - Output: Entity prediction scores

- Critical path: Query → UGL (most important) → EGL → Adaptive gate → Time-aware decoders → Prediction

- Design tradeoffs:
  - Historical length k: Longer captures more context but increases computation and may introduce noise
  - Union graph scope: Query-specific reduces noise but may miss relevant context
  - Temporal granularity: Finer granularity captures more patterns but increases temporal graph complexity

- Failure signatures:
  - Performance drops on datasets with abstract entities (like GDELT) suggest union graph construction issues
  - Sensitivity to k indicates poor historical context selection
  - Indicator over-reliance suggests model cannot learn temporal patterns independently

- First 3 experiments:
  1. Ablation study: Remove UGL and measure performance drop - should show significant decrease
  2. Vary k (historical length) and plot performance - should show optimal range around 20-30
  3. Compare with and without indicator matrix - should show improvement with indicator

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the LMS model be adapted to dynamically adjust the history length for both the evolutional graph and union graph, as well as tailor periods of the temporal graph to specific datasets?
- Basis in paper: [explicit] The authors mention in the conclusion that future research aims to develop a strategy that can dynamically adapt the history length for both the evolutional graph and union graph, and tailor periods of the temporal graph to specific datasets.
- Why unresolved: This question is explicitly stated as a future research direction by the authors, indicating that they have not yet developed such a strategy.
- What evidence would resolve it: Developing and implementing a dynamic strategy for adjusting history lengths and temporal graph periods, and evaluating its performance compared to the current fixed-length approach.

### Open Question 2
- Question: How does the performance of LMS change when using different graph aggregation methods (e.g., Graph Attention Networks, GraphSAGE) in place of the current Graph Convolutional Network approach?
- Basis in paper: [inferred] The paper uses Graph Convolutional Networks for graph aggregation but does not explore other graph neural network architectures. Comparing different graph aggregation methods could provide insights into the robustness and generalizability of the LMS approach.
- Why unresolved: The authors do not discuss or compare the performance of LMS with different graph aggregation methods.
- What evidence would resolve it: Conducting experiments with LMS using different graph aggregation methods and comparing their performance on the same benchmark datasets.

### Open Question 3
- Question: How does the performance of LMS vary with different temporal relation definitions in the temporal graph, especially for datasets with different time granularities?
- Basis in paper: [explicit] The authors define temporal relations based on the ICEWS datasets and mention that GDELT uses "hour" dimension. They do not explore how different temporal relation definitions might affect performance.
- Why unresolved: The paper does not investigate the impact of varying temporal relation definitions on model performance across different datasets.
- What evidence would resolve it: Experimenting with LMS using different temporal relation definitions for various datasets and analyzing the resulting performance changes.

## Limitations
- Query-specific union graph construction mechanism lacks detailed specification for independent replication
- Performance improvements are modest (1.76-2.60% on filtered metrics) relative to increased model complexity
- Limited analysis of which specific graph perspectives contribute most to performance gains

## Confidence
- **High confidence** in the overall methodology and experimental setup validity
- **Medium confidence** in the claimed mechanisms, particularly the adaptive gate and union graph benefits
- **Low confidence** in exact implementation details necessary for perfect reproduction

## Next Checks
1. Conduct an ablation study focusing specifically on the contribution of each graph perspective (evolutional, union, temporal) to identify which module drives improvements
2. Test model sensitivity to the historical length parameter k across different datasets to verify optimal range claims
3. Evaluate the indicator matrix's impact by comparing performance with and without it across varying temporal granularities