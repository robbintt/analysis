---
ver: rpa2
title: 'Alexpaca: Learning Factual Clarification Question Generation Without Examples'
arxiv_id: '2310.11571'
source_url: https://arxiv.org/abs/2310.11571
tags:
- question
- questions
- information
- primary
- example
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new task called task-oriented asking (TOA)
  that focuses on the ability of language models to ask clarifying questions to elicit
  missing information in multi-hop reasoning tasks. The authors propose a method called
  fact-level masking (FLM) to convert natural language datasets into self-supervised
  TOA datasets by omitting critical facts.
---

# Alexpaca: Learning Factual Clarification Question Generation Without Examples

## Quick Facts
- arXiv ID: 2310.11571
- Source URL: https://arxiv.org/abs/2310.11571
- Reference count: 13
- Zero-shot LLMs generate plausible questions but recover less useful information than humans in task-oriented asking

## Executive Summary
This paper introduces fact-level masking (FLM), a self-supervised method for creating task-oriented asking (TOA) datasets by strategically removing critical facts from complete examples. The authors evaluate several zero-shot language models on the FLM-HotpotQA dataset and find that while humans significantly outperform GPT-4, Llama 3 8B Instruct does not even beat the dummy baseline in some metrics. They demonstrate that fine-tuning Llama 3 8B Instruct on its own filtered generations can improve information recovery by 27.6%.

## Method Summary
The authors propose fact-level masking to convert natural language datasets into self-supervised TOA datasets by omitting critical facts from supporting contexts. They create incomplete examples by randomly removing one supporting fact from HotpotQA examples, then evaluate question quality based on information recovery - the improvement in a primary QA model's performance when answers to generated questions are added to the context. The evaluation pipeline measures how much the primary model's F1 score increases when the oracle's response is provided, compared to the theoretical minimum (incomplete context) and maximum (complete context).

## Key Results
- Humans significantly outperform GPT-4 on information recovery in TOA tasks
- Llama 3 8B Instruct fails to beat dummy baseline in some metrics for zero-shot TOA
- Fine-tuning Llama 3 8B Instruct on its own filtered generations improves information recovery by 27.6%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fact-level masking creates a self-supervised TOA dataset by strategically removing critical facts from complete examples
- Mechanism: FLM operates by selecting one supporting fact from a complete example and removing it, creating an incomplete example. The masked fact, along with distractor facts and other supporting facts, becomes the set of possible oracle responses. This creates a natural information gap that prompts the TOA model to ask questions seeking the missing information.
- Core assumption: The completeness of the original dataset ensures that the masked facts are indeed critical for answering the question
- Evidence anchors:
  - [abstract] "We present fact-level masking (FLM), a procedure for converting natural language datasets into self-supervised TOA datasets by omitting particular critical facts."
  - [section] "To create an incomplete example xi, we create an incomplete example xi by randomly selecting one supporting fact, f* ∈ xc, to be the masked fact and deleting it from the context: xi = xc \f*."

### Mechanism 2
- Claim: The FLM-HotpotQA pipeline evaluates question quality based on information recovery rather than traditional metrics
- Mechanism: The pipeline measures how much the primary model's performance improves when the oracle's response is added to the incomplete context. Information recovery is calculated as the percentage increase in F1 score relative to the theoretical minimum and maximum.
- Core assumption: Information recovery provides a more objective measure of question quality than word overlap metrics because it directly measures whether the question elicits useful information for the downstream task.
- Evidence anchors:
  - [abstract] "The task, HotpotQA-FLM, can be evaluated automatically, making it convenient for benchmarking language models."
  - [section] "We define recovery as: ρ = 100 · R(xr) − R(xi) / R(xc) − R(xi)"

### Mechanism 3
- Claim: Fine-tuning Llama 3 8B Instruct on its own filtered generations improves information recovery by 27.6%
- Mechanism: The model generates questions on the training set, the oracle provides responses, and then the model is fine-tuned on the subset of examples where its questions successfully elicited useful information (positive information recovery).
- Core assumption: The filtering process effectively identifies which generated questions are useful and which are not.
- Evidence anchors:
  - [abstract] "Finally, we find by fine-tuning Llama 3 8B Instruct on its own generations, filtered via rejection sampling, we can improve information recovery by 27.6 percent."
  - [section] No direct evidence provided in the main text - this appears to be a key result mentioned in the abstract but not detailed in the sections.

## Foundational Learning

- Concept: Multi-hop reasoning
  - Why needed here: HotpotQA requires reasoning across multiple supporting facts to answer questions, which creates natural opportunities for missing information to be critical
  - Quick check question: Can you explain how a question like "When was the composer of 'Persian Surgery Dervishes' born?" requires information from multiple facts to answer correctly?

- Concept: Information retrieval evaluation metrics
  - Why needed here: The paper uses F1 score and exact match (EM) to evaluate how well questions retrieve useful information, which differs from standard question generation metrics
  - Quick check question: What is the difference between measuring question quality using BLEU score versus information recovery, and why is information recovery more appropriate for TOA?

- Concept: Self-supervised learning
  - Why needed here: FLM creates training data automatically without human annotation by masking facts from complete examples, enabling large-scale TOA dataset creation
  - Quick check question: How does fact-level masking create a self-supervised learning setup, and what are the advantages of this approach compared to manual annotation?

## Architecture Onboarding

- Component map:
  Primary model (M1) -> Question generation (TOA model M2) -> Oracle (Θ) -> Response evaluation

- Critical path: Incomplete example → TOA model → Oracle → Response example → Primary model evaluation
  The TOA model's question quality is ultimately determined by how much it improves the primary model's performance when the oracle's response is provided.

- Design tradeoffs:
  - Masking one fact vs. multiple facts: Single fact masking creates clearer evaluation but may be too simple; multiple fact masking would be more realistic but harder to evaluate
  - Oracle selection: Using Flan-T5-Base balances accessibility with performance, but stronger oracles might provide more accurate responses
  - Evaluation metric choice: Information recovery directly measures utility but may be noisier than traditional metrics

- Failure signatures:
  - Low information recovery across all models suggests either the task is too difficult or the evaluation pipeline has issues
  - High variance in recovery between similar questions indicates inconsistent oracle behavior or primary model performance
  - Distractor responses that improve performance suggests the primary model has memorized knowledge or the context contains sufficient redundancy

- First 3 experiments:
  1. Run the complete pipeline with a simple baseline (e.g., random question generation) to verify all components work together
  2. Compare different primary model architectures (Flan-T5 sizes, GPT models) to establish which works best for the evaluation pipeline
  3. Test different oracle models to find the best balance between response quality and consistency

## Open Questions the Paper Calls Out

- How does the performance of task-oriented asking (TOA) models change when using an unconstrained generative oracle instead of the current constrained oracle model?
- How does the performance of TOA models change when using multiple oracles, each lacking complete information, instead of a single oracle?
- How does the performance of TOA models change when specifically trained for the task using self-supervised FLM datasets or reinforcement learning?

## Limitations
- Exact prompt templates for different language models are not specified, making reproduction difficult
- The filtering mechanism for fine-tuning lacks clear thresholds or criteria
- Some masked facts may be redundant or unnecessary for answering questions, potentially inflating performance metrics

## Confidence
- **High Confidence**: The core mechanism of fact-level masking creating a self-supervised TOA dataset is well-supported
- **Medium Confidence**: The claim that humans significantly outperform GPT-4 on information recovery is supported by results but lacks detailed methodology
- **Low Confidence**: The fine-tuning improvement claim (27.6% recovery increase) is mentioned in the abstract but not detailed in the main text

## Next Checks
1. Reproduce the baseline comparison by implementing the complete evaluation pipeline with multiple primary models and oracle models
2. Test prompt sensitivity by systematically varying prompt templates and shot settings for zero-shot models
3. Validate the filtering mechanism by analyzing the distribution of information recovery scores across the training set to determine appropriate thresholds for rejection sampling