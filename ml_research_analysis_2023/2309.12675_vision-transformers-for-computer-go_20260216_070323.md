---
ver: rpa2
title: Vision Transformers for Computer Go
arxiv_id: '2309.12675'
source_url: https://arxiv.org/abs/2309.12675
tags:
- residual
- computer
- networks
- efficient
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the application of Vision Transformers (EfficientFormer)
  in Computer Go, comparing them to traditional Residual Networks. The authors adapt
  the EfficientFormer architecture for Go game prediction, replacing the final layers
  with policy and value prediction heads, and modify the embedding layers to preserve
  the 19x19 board size.
---

# Vision Transformers for Computer Go

## Quick Facts
- arXiv ID: 2309.12675
- Source URL: https://arxiv.org/abs/2309.12675
- Authors: 
- Reference count: 36
- Primary result: EfficientFormer (l9) achieves 54.29% accuracy, 0.0405 MSE, and 50.4% win rate against Residual networks on CPU, demonstrating parameter-efficient performance in Computer Go

## Executive Summary
This paper investigates the application of Vision Transformers, specifically EfficientFormer, in Computer Go prediction tasks and compares them to traditional Residual Networks. The authors adapt EfficientFormer's architecture for Go by modifying embedding layers to preserve the 19x19 board size and replacing final layers with policy and value prediction heads. Through extensive experiments on a 1,000,000-game Katago dataset, they demonstrate that EfficientFormer offers a parameter-efficient alternative to Residual Networks, with superior CPU performance while maintaining comparable GPU results. The work establishes that transformer-based architectures can effectively capture Go board relationships while requiring fewer parameters than convolutional approaches.

## Method Summary
The study trains and evaluates various EfficientFormer and Residual Network architectures on 1,000,000 Katago self-play games, using 31 19x19 input planes to represent the Go board state. The EfficientFormer models employ a dimension-consistent plan with local pooling and global self-attention token mixers, while the Residual Networks use standard convolutional layers. Both architectures are trained for 500 epochs with batch size 64 using Adam optimizer and cosine annealing learning rate schedules. The evaluation compares accuracy, MSE, MAE, win rates, memory usage, and inference speed across different network sizes on both CPU and GPU platforms.

## Key Results
- EfficientFormer (l9) achieves 54.29% accuracy and 0.0405 MSE, comparable to larger Residual Networks
- On CPU, EfficientFormer demonstrates superior speed with higher evaluations per second than Residual Networks of similar size
- The l9 EfficientFormer wins 50.4% of matches against the l9 Residual network on CPU
- EfficientFormer shows better parameter efficiency while maintaining competitive performance across all metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EfficientFormer reduces parameter count while maintaining accuracy compared to Residual Networks
- Mechanism: The EfficientFormer architecture uses a dimension-consistent plan and local pooling/global self-attention token mixers, which reduces the number of parameters needed to achieve comparable accuracy
- Core assumption: The self-attention mechanisms in EfficientFormer can effectively capture the relationships between different portions of the Go board image without the need for as many parameters as Residual Networks
- Evidence anchors:
  - [abstract] The paper states that EfficientFormer offers a parameter-efficient alternative to Residual Networks for Computer Go
  - [section] The paper explains that EfficientFormer achieves high performance and matches MobileNet's speed on mobile devices by using a dimension-consistent plan and token mixer configurations
  - [corpus] The corpus includes papers on Vision Transformers and their efficiency, supporting the claim that transformer-based models can achieve high performance with fewer parameters
- Break condition: If the self-attention mechanisms in EfficientFormer fail to capture the relevant relationships in the Go board image, the model's accuracy would degrade despite the reduced parameter count

### Mechanism 2
- Claim: EfficientFormer has better CPU performance than Residual Networks
- Mechanism: The EfficientFormer architecture, with its reduced parameter count and efficient token mixer configurations, allows for faster inference on CPU compared to the larger Residual Networks
- Core assumption: The computational efficiency gained from the EfficientFormer architecture translates to better performance on CPU, which has limited parallel processing capabilities compared to GPU
- Evidence anchors:
  - [section] Table 3 shows that EfficientFormer models have higher evaluations per second on CPU compared to Residual Networks with similar or higher parameter counts
  - [section] The paper states that EfficientFormer is better than Residual Networks on CPU
  - [corpus] The corpus includes papers on the efficiency of transformer-based models, supporting the claim that they can achieve better performance on resource-constrained devices
- Break condition: If the computational efficiency gains from the EfficientFormer architecture are offset by other factors, such as memory access patterns or CPU-specific optimizations, the CPU performance advantage may not materialize

### Mechanism 3
- Claim: EfficientFormer can preserve the 19x19 board size without losing information
- Mechanism: The EfficientFormer architecture modifies the embedding layers to maintain the height and width of the Go board throughout the training process, ensuring that no valuable details are lost
- Core assumption: Preserving the board size is crucial for maintaining the richness of the input data and capturing the relevant features for Go game prediction
- Evidence anchors:
  - [section] The paper states that the input board's dimensions were fixed at 19x19, and it was imperative to preserve this size throughout the training process to avoid losing critical information
  - [section] The paper explains that the height and width of the board were maintained during training to retain the richness of the board data
  - [corpus] The corpus includes papers on the importance of input size preservation in transformer-based models, supporting the claim that maintaining the board size is crucial for capturing relevant features
- Break condition: If the modified embedding layers fail to effectively process the 19x19 board size, the model may lose important information or fail to capture relevant features, leading to degraded performance

## Foundational Learning

- Concept: Transformer architectures and self-attention mechanisms
  - Why needed here: Understanding how transformers work and how self-attention captures relationships between elements is crucial for grasping the EfficientFormer architecture's advantages over Residual Networks
  - Quick check question: How does self-attention in transformers differ from traditional convolutional or recurrent approaches in capturing relationships between elements in a sequence?

- Concept: Computer Go and game state representation
  - Why needed here: Familiarity with the game of Go, its rules, and how the game state is represented as input to the neural network is essential for understanding the problem domain and the specific challenges addressed by the EfficientFormer architecture
  - Quick check question: What are the key components of a Go game state representation, and how do they differ from other board games like chess?

- Concept: Model evaluation metrics and their interpretation
  - Why needed here: Understanding the various evaluation metrics used in the paper (accuracy, MSE, MAE, win rate) and how to interpret their results is crucial for assessing the performance of the EfficientFormer architecture compared to Residual Networks
  - Quick check question: How do the different evaluation metrics (accuracy, MSE, MAE, win rate) provide insights into the strengths and weaknesses of the EfficientFormer and Residual Network models?

## Architecture Onboarding

- Component map: Input 31 19x19 planes -> Modified embedding layers -> EfficientFormer MetaBlock (MB) -> Policy and Value heads
- Critical path: 1) Input Go board state is processed through the modified embedding layers 2) The EfficientFormer architecture processes the embedded representation using self-attention mechanisms 3) The output is passed through the policy and value heads to generate move probabilities and win rate predictions
- Design tradeoffs: Parameter efficiency vs. accuracy: EfficientFormer aims to reduce parameter count while maintaining comparable accuracy to Residual Networks; CPU vs. GPU performance: EfficientFormer is optimized for better CPU performance, while Residual Networks may have advantages on GPU due to parallelization; Input size preservation: EfficientFormer modifies the embedding layers to preserve the 19x19 board size, which may impact the model's ability to capture certain features
- Failure signatures: Degraded accuracy: If the self-attention mechanisms fail to capture relevant relationships in the Go board image, the model's accuracy will suffer; Poor CPU performance: If the computational efficiency gains from the EfficientFormer architecture are offset by other factors, the CPU performance advantage may not materialize; Loss of information: If the modified embedding layers fail to effectively process the 19x19 board size, the model may lose important information or fail to capture relevant features
- First 3 experiments: 1) Train and evaluate the l1 EfficientFormer model on a subset of the Katago dataset, comparing its performance to a small Residual Network (e.g., Residual(10,128)) in terms of accuracy, MSE, MAE, and CPU latency 2) Train and evaluate the l9 EfficientFormer model on the full Katago dataset, comparing its performance to a large Residual Network (e.g., Residual(40,256)) in terms of accuracy, MSE, MAE, and win rate on both CPU and GPU 3) Perform an ablation study by modifying the embedding layers of the EfficientFormer architecture to not preserve the 19x19 board size, and evaluate the impact on model performance and information retention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do EfficientFormer models perform on other board games with different board sizes and rules?
- Basis in paper: [explicit] The paper mentions that EfficientFormer's architecture is not limited to Go and exhibits versatility and applicability to a wide range of other games and domains
- Why unresolved: The paper only evaluates EfficientFormer on the game of Go, leaving its performance on other games unexplored
- What evidence would resolve it: Training and evaluating EfficientFormer models on other board games such as Chess, Shogi, or Hex, and comparing their performance to existing methods

### Open Question 2
- Question: What is the impact of different self-attention mechanisms and their configurations on the performance of Vision Transformers in Computer Go?
- Basis in paper: [inferred] The paper discusses the use of EfficientFormer, which employs self-attention mechanisms, but does not explore the impact of different self-attention configurations on performance
- Why unresolved: The paper focuses on EfficientFormer's overall performance but does not delve into the specifics of how different self-attention mechanisms might affect results
- What evidence would resolve it: Conducting experiments with various self-attention mechanisms and configurations in Vision Transformers and comparing their performance in Computer Go tasks

### Open Question 3
- Question: How does the performance of EfficientFormer models scale with increasing dataset sizes and more complex game scenarios?
- Basis in paper: [inferred] The paper evaluates EfficientFormer on a dataset of 1,000,000 Katago self-play games but does not explore how performance changes with larger datasets or more complex scenarios
- Why unresolved: The paper provides results for a specific dataset size and complexity level, leaving the scalability of EfficientFormer unexplored
- What evidence would resolve it: Training and evaluating EfficientFormer models on progressively larger datasets and more complex game scenarios, then analyzing the performance trends

## Limitations

- The evaluation relies solely on Katago self-play data, potentially introducing domain-specific biases that may not generalize to human-played games
- The comparison between EfficientFormer and Residual Networks is somewhat uneven, as the networks are not directly comparable in size
- The paper focuses on prediction accuracy metrics without implementing actual gameplay scenarios with search algorithms

## Confidence

**High Confidence Claims:**
- EfficientFormer architectures can be successfully adapted for Go prediction tasks with modified embedding layers to preserve board dimensions
- The EfficientFormer models demonstrate parameter efficiency compared to Residual Networks of similar accuracy
- CPU performance advantages for EfficientFormer are clearly demonstrated through timing measurements

**Medium Confidence Claims:**
- The overall performance comparison between EfficientFormer and Residual Networks across different evaluation metrics
- The effectiveness of the cosine annealing learning rate schedule for both architectures
- The generalization capability of models trained on Katago self-play data

**Low Confidence Claims:**
- Claims about the superiority of EfficientFormer for actual gameplay decisions based solely on prediction metrics
- Extrapolations about performance on other games or domains without additional validation

## Next Checks

1. **Cross-domain validation**: Test the trained models on human-played game datasets to verify whether the Katago-specific patterns learned don't limit generalization to real-world gameplay scenarios

2. **Search integration benchmark**: Implement a simple Monte Carlo Tree Search using both EfficientFormer and Residual Network policies to measure actual gameplay strength rather than just prediction accuracy, revealing whether prediction performance translates to playing strength

3. **Ablation on board size preservation**: Systematically vary the board size preservation in the embedding layers to quantify exactly how much information is retained versus lost, and whether the claimed benefits of maintaining 19x19 dimensions are statistically significant