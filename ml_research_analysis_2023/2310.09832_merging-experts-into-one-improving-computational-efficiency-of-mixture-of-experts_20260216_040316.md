---
ver: rpa2
title: 'Merging Experts into One: Improving Computational Efficiency of Mixture of
  Experts'
arxiv_id: '2310.09832'
source_url: https://arxiv.org/abs/2310.09832
tags:
- experts
- computational
- cost
- performance
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Merging Experts into One (MEO), a computation-efficient
  approach for Mixture-of-Experts (MoE) models. MEO reduces computational costs by
  merging selected experts into a single expert, achieving performance comparable
  to MoE with significantly lower FLOPs (e.g., 28.6G vs 72.0G).
---

# Merging Experts into One: Improving Computational Efficiency of Mixture of Experts

## Quick Facts
- arXiv ID: 2310.09832
- Source URL: https://arxiv.org/abs/2310.09832
- Reference count: 6
- Key outcome: MEO reduces FLOPs from 72.0G to 28.6G while achieving 0.7% average GLUE score improvement over vanilla MoE

## Executive Summary
This paper introduces Merging Experts into One (MEO), a novel approach to reduce the computational cost of Mixture-of-Experts (MoE) models. By aggregating selected expert parameters using gating scores before applying activation functions, MEO achieves performance comparable to vanilla MoE with significantly lower computational overhead. The method introduces a token-level attention mechanism that further enhances efficiency by enabling per-token adaptation without the cost of per-token expert selection.

## Method Summary
MEO modifies the standard MoE architecture by merging selected expert parameters before computation. Instead of computing each expert independently and summing outputs (P_k∈T G_k · E_k(x)), MEO aggregates expert weights and biases using gating scores (Ŵ_i = Σ_k∈T G_k · W_k, ŷ_i = σ(Ŵ_i x_i + ˆb_i)), then applies the activation function once. This reduces computation from O(m × E) to O(E). A token-level attention block is introduced to inject token-specific information into the input before MEO application, enabling fine-grained adaptation while maintaining computational efficiency.

## Key Results
- FLOPs reduced from 72.0G (vanilla MoE) to 28.6G (MEO)
- Average GLUE score improvement of 0.7% over vanilla MoE
- Consistent performance improvements across GLUE, XSum, Wikitext, and SQuAD benchmarks
- Inference time remains relatively consistent as number of selected experts increases, while vanilla MoE shows significant increases

## Why This Works (Mechanism)

### Mechanism 1
The core efficiency gain comes from parameter merging before activation. By computing a single merged expert once per input (ŷ_i = σ(Ŵ_i x_i + ˆb_i)), MEO reduces computational cost from O(m × E) to O(E). This works because the weighted sum of expert parameters can approximate the mixture of outputs when activation is applied after merging.

### Mechanism 2
Token-level attention blocks enable per-token adaptation without per-token expert selection. A bottleneck adapter (down-projection, activation, up-projection) injects token-level information into the input before applying sequence-level MEO, capturing token-specific information while maintaining efficiency.

### Mechanism 3
The computational efficiency gain scales with the number of selected experts. Since MEO's computation is constant regardless of how many experts are selected (O(E)), while vanilla MoE scales linearly (O(m × E)), the relative efficiency improvement grows with m.

## Foundational Learning

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: Understanding the baseline MoE mechanism is essential to grasp how MEO modifies it
  - Quick check question: In vanilla MoE, if 4 experts are selected for an input, how does the computational cost compare to selecting 1 expert?

- Concept: Sparse routing and gating mechanisms
  - Why needed here: The gating network determines which experts to merge and how to weight them in MEO
  - Quick check question: What is the difference between token-level and sequence-level gating in MoE models?

- Concept: Parameter merging and weighted averaging
  - Why needed here: MEO fundamentally relies on merging expert parameters using gating scores
  - Quick check question: When merging two experts with weights W1, b1 and W2, b2 using gating scores G1 and G2, what are the resulting merged parameters?

## Architecture Onboarding

- Component map: Input sequence → Token attention block (for token-level MEO) → Gating network → Expert merging (weighted sum of selected experts) → Single expert computation → Output

- Critical path: Token/sequence → Gating scores → Parameter merging (Σ Gk·Wk, Σ Gk·bk) → σ(merged_params × input + merged_bias) → Output

- Design tradeoffs:
  - MEO vs. vanilla MoE: Efficiency vs. potential slight accuracy loss when activation functions are applied before gating
  - Token-level vs. sequence-level MEO: More fine-grained control vs. computational overhead
  - Number of experts: More experts can improve performance but increase memory requirements for storing all expert parameters

- Failure signatures:
  - Performance degradation when expert activation functions are applied before gating (the merging assumption breaks)
  - Inconsistent results across different tasks or datasets
  - Memory issues if too many experts are used without proper gating sparsity

- First 3 experiments:
  1. Implement basic MEO layer and verify computational cost reduction on a small BERT model with m=4 experts
  2. Compare MEO with vanilla MoE on GLUE benchmark for sequence-level MEO to validate performance retention
  3. Implement token-level MEO with attention block and test on a single GLUE task to verify the attention mechanism works as intended

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of experts to select per input for different tasks and architectures?
- Basis in paper: [inferred] The paper discusses how selecting multiple experts can improve performance but also increases computational cost. It mentions the need for further exploration in determining the optimal number of experts for specific layers and tasks.
- Why unresolved: The paper does not provide a definitive answer on the optimal number of experts for different tasks and architectures. It only suggests that this requires additional investigation.
- What evidence would resolve it: Empirical studies comparing the performance of MEO with different numbers of experts across various tasks and architectures would provide insights into the optimal number of experts for different scenarios.

### Open Question 2
- Question: How does the token-level attention mechanism in MEO compare to other attention mechanisms in terms of computational efficiency and performance?
- Basis in paper: [explicit] The paper introduces a token-level attention mechanism to enhance the efficiency and performance of token-level MEO. It mentions that this mechanism captures token identification with minimal extra computational cost.
- Why unresolved: The paper does not provide a direct comparison between the token-level attention mechanism in MEO and other attention mechanisms in terms of computational efficiency and performance.
- What evidence would resolve it: Comparative studies between the token-level attention mechanism in MEO and other attention mechanisms, considering both computational efficiency and performance, would help determine the relative advantages and disadvantages of each approach.

### Open Question 3
- Question: How does MEO perform in tasks that require long-range dependencies or sequential reasoning?
- Basis in paper: [inferred] The paper evaluates MEO on various benchmarks, including natural language understanding and generation tasks. However, it does not specifically address tasks that require long-range dependencies or sequential reasoning.
- Why unresolved: The paper does not provide evidence on how MEO performs in tasks that require long-range dependencies or sequential reasoning.
- What evidence would resolve it: Empirical studies comparing the performance of MEO on tasks that require long-range dependencies or sequential reasoning, such as machine translation or reading comprehension, would help determine its effectiveness in these domains.

## Limitations
- The fundamental assumption that parameter merging preserves model behavior when activation functions are applied before gating lacks mathematical proof
- Experiments are limited to transformer-based architectures on NLP tasks, limiting generalizability to other domains
- No ablation studies on the necessity or effectiveness of the token-level attention mechanism

## Confidence
- Parameter Merging Validity: Low - The mathematical validity of merging expert parameters before activation is not proven, especially when activation functions are non-linear
- Token-Level Attention Effectiveness: Medium - Limited evidence provided on whether the additional complexity provides meaningful benefits beyond sequence-level MEO
- Architecture and Task Generalizability: Medium - All experiments use transformer-based architectures on NLP tasks, limiting applicability to other domains

## Next Checks
1. Conduct ablation studies testing MEO with different activation function positions (before vs. after gating) to quantify the impact on the merging assumption validity
2. Implement MEO on non-transformer architectures (e.g., RNNs or CNN-based models) and different task types to assess generalizability beyond the tested NLP/transformer setup
3. Perform a comprehensive study varying the number of selected experts (m) and total experts (n) to quantify the precise relationship between computational savings, memory usage, and performance across different sparsity levels