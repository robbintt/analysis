---
ver: rpa2
title: When does In-context Learning Fall Short and Why? A Study on Specification-Heavy
  Tasks
arxiv_id: '2311.08993'
source_url: https://arxiv.org/abs/2311.08993
tags:
- arxiv
- tasks
- llms
- https
- specification-heavy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the limitations of in-context learning (ICL)
  on specification-heavy tasks, which require extensive and complex task specifications.
  The authors conduct comprehensive experiments on 18 such tasks with various LLMs
  and find that ICL performance is often far below state-of-the-art results achieved
  by fine-tuned models.
---

# When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks

## Quick Facts
- arXiv ID: 2311.08993
- Source URL: https://arxiv.org/abs/2311.08993
- Reference count: 40
- Key outcome: ICL performance is often far below state-of-the-art results achieved by fine-tuned models on specification-heavy tasks

## Executive Summary
This paper investigates the limitations of in-context learning (ICL) on specification-heavy tasks, which require extensive and complex task specifications. Through comprehensive experiments on 18 such tasks with various LLMs, the authors find that ICL performance is substantially worse than state-of-the-art results achieved by fine-tuned models. They identify three main reasons for ICL's failure: inability to specifically understand context, misalignment in task schema comprehension with humans, and inadequate long-text understanding ability. The authors demonstrate that fine-tuning LLMs can achieve decent performance on these tasks, indicating that the failure of ICL is not an inherent flaw of LLMs, but rather a drawback of existing alignment methods.

## Method Summary
The study evaluates ICL performance on 18 specification-heavy tasks across six types (NER, RE, ED, EAE, ERE, SA) using various LLMs including GPT-4, Vicuna, FLAN-UL2, Alpaca, ChatGPT, and Davinci. Test sets were sampled from each task (1,000 instances or the entire set if smaller), and F1 scores were calculated via string matching with ground-truth annotations. The evaluation used human-written instructions and 8-shot demonstrations (with variations for specific tasks). The paper also demonstrates that fine-tuning these LLMs on the same tasks can achieve state-of-the-art or near-state-of-the-art performance, and performs dedicated instruction tuning to observe improvements in ICL performance.

## Key Results
- ICL performance is substantially worse than fine-tuned models on specification-heavy tasks, with some tasks showing ICL F1 scores below 30% while fine-tuned models achieve over 70%
- Three main failure modes identified: inability to specifically understand context, misalignment in task schema comprehension with humans, and inadequate long-text understanding ability
- Fine-tuning LLMs on specification-heavy tasks achieves results on par with or surpassing previous state-of-the-art, demonstrating that the failure of ICL is not an inherent flaw of LLMs
- Instruction tuning improves ICL performance on these tasks but still falls significantly short of fine-tuned model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICL fails on specification-heavy tasks because it cannot capture the fine-grained semantic context required by these tasks.
- Mechanism: Specification-heavy tasks require precise understanding of contextual details, but ICL relies on implicit pattern matching from few examples, leading to context-agnostic predictions.
- Core assumption: The task's complexity cannot be adequately represented within the token limits of ICL prompts.
- Evidence anchors:
  - [abstract] "LLMs often lack fine-grained context understanding on these tasks, i.e., the inability to specifically understand context."
  - [section 3.1] "In these error cases, LLMs either ignore all the contexts and give predictions only based on their parametric knowledge... or overlook some important specific words within the contexts."
  - [corpus] FMR=0.704 for related paper on "In-Context Learning Learns Label Relationships but Is Not Conventional Learning" supports the idea that ICL relies on pattern matching rather than deep comprehension.
- Break condition: If task specifications can be fully represented within the model's context window, ICL performance would improve.

### Mechanism 2
- Claim: ICL fails because task schema understanding becomes misaligned with human expectations when specifications are underspecified.
- Mechanism: Heavy specifications cannot be fully included in prompts, creating underspecification that causes LLMs to develop schema interpretations that differ from human-defined categories.
- Core assumption: LLMs cannot infer complete task semantics from minimal instruction alone.
- Evidence anchors:
  - [abstract] "LLMs' understanding of tasks, e.g., task schema, may not be aligned with human expectations."
  - [section 3.2] "While human annotators can consult the specifications to understand the differences between the two types, the under-alignment of LLMs cannot be simply solved with ICL."
  - [corpus] FMR=0.571 for paper on "Semantic Anchors in In-Context Learning: Why Small LLMs Cannot Flip Their Labels" supports that label semantics are difficult to override through ICL.
- Break condition: If specifications can be fully encoded in the prompt, alignment would improve.

### Mechanism 3
- Claim: ICL fails because LLMs' inadequate long-text understanding prevents full specification inclusion.
- Mechanism: LLMs struggle with modeling long contexts, making it impossible to include complete task specifications in prompts, which perpetuates underspecification.
- Core assumption: LLMs' context length limitations directly impact their ability to process comprehensive task descriptions.
- Evidence anchors:
  - [abstract] "LLMs are often inadequate for understanding long contexts... For specification-heavy tasks, this not only implies that LLMs would perform worse with longer given contexts."
  - [section 3.3] "As shown in Table 1, LLMs particularly underperform on tasks featuring long contexts: DocRED, MATRES, and MA VEN-ERE."
  - [corpus] FMR=0.657 for paper on "Understanding Emergent In-Context Learning from a Kernel Regression Perspective" suggests theoretical limitations in how LLMs process long contexts.
- Break condition: If context length limitations are overcome through architectural improvements.

## Foundational Learning

- Concept: Task specification complexity
  - Why needed here: Understanding why certain tasks require extensive specifications helps explain why ICL fails when those specifications cannot be included in prompts.
  - Quick check question: Can you identify which tasks in the study required 77-page annotation guidelines?

- Concept: In-context learning mechanics
  - Why needed here: ICL's reliance on demonstration-based pattern matching rather than explicit rule learning explains why it fails on tasks requiring detailed specifications.
  - Quick check question: What is the primary difference between ICL and traditional fine-tuning approaches?

- Concept: Model context window limitations
  - Why needed here: Understanding how context length affects model performance explains why specification-heavy tasks are particularly challenging for ICL.
  - Quick check question: What happens to ICL performance as context length increases according to the study?

## Architecture Onboarding

- Component map: Task specifications → Instruction creation → Demonstration sampling → Prompt construction → LLM inference → Output parsing → Evaluation
- Critical path: Task specification parsing → Prompt construction with demonstrations → LLM inference with context limits → Output parsing and evaluation
- Design tradeoffs: Token budget vs. specification completeness; Demonstration quantity vs. prompt length; Instruction clarity vs. brevity
- Failure signatures: Consistent misclassification of specific categories; Performance degradation with longer contexts; Predictions based solely on parametric knowledge
- First 3 experiments: 1) Test ICL performance with varying context lengths on a single task; 2) Compare ICL vs. fine-tuning performance on specification-heavy tasks; 3) Evaluate instruction-tuned model's zero-shot performance on held-out tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of fine-tuned LLMs on specification-heavy tasks compare to state-of-the-art models specifically designed for those tasks?
- Basis in paper: [explicit] The paper states that fine-tuning FLAN-UL2 on specification-heavy tasks achieves results "on par with or even surpass previous SoTA" but doesn't provide direct comparisons with specialized models.
- Why unresolved: The paper focuses on comparing fine-tuned LLMs to in-context learning (ICL) performance, but doesn't explicitly compare to specialized models that may have been trained extensively on these specific tasks.
- What evidence would resolve it: Direct comparison of fine-tuned LLM performance on specification-heavy tasks against the performance of state-of-the-art models specifically designed for those tasks.

### Open Question 2
- Question: Can instruction tuning be further improved to achieve better performance on specification-heavy tasks compared to the preliminary results shown in the paper?
- Basis in paper: [explicit] The paper demonstrates that instruction tuning improves ICL performance on specification-heavy tasks, but acknowledges that the aligned LLM still falls "significantly short of the existing SoTA."
- Why unresolved: The paper only presents preliminary results using a straightforward instruction tuning approach. More advanced alignment techniques or refined instruction tuning methods may yield better results.
- What evidence would resolve it: Comparative analysis of different instruction tuning approaches or advanced alignment techniques on specification-heavy tasks, demonstrating improved performance beyond the preliminary results.

### Open Question 3
- Question: What is the impact of model scale on the performance of fine-tuned LLMs for specification-heavy tasks beyond the FLAN-T5 model family explored in the paper?
- Basis in paper: [explicit] The paper shows a clear positive scaling effect within the FLAN-T5 model family, but doesn't explore the impact of scaling beyond this family.
- Why unresolved: The paper focuses on a specific model family and doesn't investigate whether the scaling law observed holds true for other LLM architectures or even larger models.
- What evidence would resolve it: Fine-tuning experiments with a wider range of LLM architectures and model scales, demonstrating the impact of scale on specification-heavy task performance beyond the FLAN-T5 family.

## Limitations

- The paper demonstrates ICL limitations but does not fully establish whether these are fundamental to ICL or can be overcome through better prompting strategies
- While instruction tuning shows improvements, the evaluation lacks a comprehensive ablation study of which instruction components contribute most to performance gains
- The study does not test alternative solutions like retrieval-augmented generation or adaptive prompting that might address the identified failure modes

## Confidence

- **High confidence**: ICL performance is substantially worse than fine-tuned models on specification-heavy tasks
- **Medium confidence**: The three identified failure modes are the primary reasons for ICL's limitations
- **Low confidence**: Instruction tuning represents the optimal solution for improving ICL performance on these tasks

## Next Checks

1. Conduct a systematic ablation study on instruction tuning to identify which components contribute most to performance improvements
2. Test alternative prompting strategies beyond instruction tuning, such as retrieval-augmented generation or dynamic prompt construction based on task complexity
3. Evaluate whether the identified failure modes persist when using models with extended context windows (e.g., 32k or 100k tokens) to isolate whether limitations are due to context length constraints versus fundamental ICL mechanisms