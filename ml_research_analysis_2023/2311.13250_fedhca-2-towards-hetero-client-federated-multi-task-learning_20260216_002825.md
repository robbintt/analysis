---
ver: rpa2
title: 'FedHCA$^2$: Towards Hetero-Client Federated Multi-Task Learning'
arxiv_id: '2311.13250'
source_url: https://arxiv.org/abs/2311.13250
tags:
- learning
- task
- multi-task
- clients
- aggregation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Hetero-Client Federated Multi-Task Learning
  (HC-FMTL), a novel problem setting where clients have diverse task setups and model
  architectures. The authors propose FedHCA2, a framework that addresses the challenges
  of model incongruity, data heterogeneity, and task heterogeneity in HC-FMTL.
---

# FedHCA$^2$: Towards Hetero-Client Federated Multi-Task Learning

## Quick Facts
- arXiv ID: 2311.13250
- Source URL: https://arxiv.org/abs/2311.13250
- Reference count: 40
- Key outcome: FedHCA2 achieves 2.18% and 0.75% lower average per-task performance drop compared to baselines on PASCAL-Context and NYUD-v2 datasets respectively

## Executive Summary
This paper addresses Hetero-Client Federated Multi-Task Learning (HC-FMTL), where clients have diverse task setups and model architectures, presenting unique challenges in model incongruity, data heterogeneity, and task heterogeneity. The authors propose FedHCA2, a framework that employs three key mechanisms: Hyper Conflict-Averse Aggregation to mitigate update conflicts among clients' encoders, Hyper Cross Attention Aggregation to enhance decoder interactions using layer-wise cross attention, and learnable Hyper Aggregation Weights for personalized parameter updates. Extensive experiments on PASCAL-Context and NYUD-v2 datasets demonstrate that FedHCA2 outperforms representative methods in various HC-FMTL scenarios.

## Method Summary
FedHCA2 addresses HC-FMTL by implementing three core mechanisms: Hyper Conflict-Averse Aggregation minimizes gradient conflicts during encoder updates by solving an optimization problem that finds aggregated update directions within a convergence constraint; Hyper Cross Attention Aggregation enables task interaction in decoders by computing layer-wise cross attention dependencies among decoder updates; and learnable Hyper Aggregation Weights allow each client to customize how much to incorporate aggregated updates from peers. The framework is trained using AdamW optimizer with learning rate 1e-4, batch size 8, one local epoch for PASCAL-Context and four for NYUD-v2, across 100 communication rounds.

## Key Results
- FedHCA2 achieves 2.18% lower average per-task performance drop on PASCAL-Context dataset compared to baselines
- FedHCA2 achieves 0.75% lower average per-task performance drop on NYUD-v2 dataset compared to baselines
- The framework demonstrates effectiveness across both single-task and multi-task client scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hyper Conflict-Averse Aggregation reduces update conflicts among clients by optimizing the inner product of gradients between tasks
- Mechanism: The server solves an optimization problem that finds an aggregated update direction minimizing conflicts between client updates while staying within a convergence constraint
- Core assumption: The inner product of gradients measures accordance between tasks, and minimizing conflicts improves multi-task learning performance
- Evidence anchors:
  - [abstract]: "Drawing on our theoretical insights into the difference between multi-task and federated optimization, we propose the Hyper Conflict-Averse Aggregation scheme to mitigate conflicts during encoder updates"
  - [section]: "Theorem 1 states the necessity of integrating optimization techniques to mitigate gradient conflicts during encoder aggregation in HC-FMTL"
  - [corpus]: Weak - no corpus evidence found
- Break condition: If the inner product of gradients no longer correlates with task accordance, or if the constraint parameter c is set outside [0,1)

### Mechanism 2
- Claim: Hyper Cross Attention Aggregation enables task interaction in decoders by modeling fine-grained cross-task relationships
- Mechanism: Layer-wise cross attention computes dependencies among decoder updates, allowing each decoder to selectively incorporate information from other decoders based on task relevance
- Core assumption: Task interaction improves multi-task learning by allowing tasks to share complementary information while maintaining task-specific capabilities
- Evidence anchors:
  - [abstract]: "Additionally, inspired by task interaction in MTL, the Hyper Cross Attention Aggregation scheme uses layer-wise cross attention to enhance decoder interactions while alleviating model incongruity"
  - [section]: "We develop a Hyper Cross Attention Aggregation scheme to facilitate task interaction in decoders by modeling the fine-grained cross-task relationships among each decoder layer"
  - [corpus]: Weak - no corpus evidence found
- Break condition: If cross attention weights become uniform across all tasks, indicating no selective information flow

### Mechanism 3
- Claim: Learnable Hyper Aggregation Weights customize personalized parameter updates for each client
- Mechanism: Each client maintains dedicated weights that determine how much to incorporate aggregated updates from peers, updated via chain rule through local training
- Core assumption: Different clients benefit from different amounts of knowledge sharing based on their data domain and task heterogeneity
- Evidence anchors:
  - [abstract]: "Moreover, we employ learnable Hyper Aggregation Weights for each client to customize personalized parameter updates"
  - [section]: "We propose Hyper Aggregation Weights, which adaptively assess the importance of the aggregated parameters from peers"
  - [corpus]: Weak - no corpus evidence found
- Break condition: If aggregation weights converge to extreme values (all 0 or all 1) for all clients, indicating no personalization

## Foundational Learning

- Concept: Federated Multi-Task Learning (FMTL)
  - Why needed here: Understanding the baseline problem where clients have identical model architectures and task sets
  - Quick check question: What is the main difference between FMTL and HC-FMTL?

- Concept: Model Incongruity
  - Why needed here: Recognizing that different task setups lead to incompatible model structures that prevent direct parameter aggregation
  - Quick check question: Why can't we directly aggregate model parameters in HC-FMTL like in standard FMTL?

- Concept: Task Interaction in MTL
  - Why needed here: Understanding how tasks can benefit from sharing information while maintaining task-specific capabilities
  - Quick check question: What is the benefit of cross-task feature sharing in multi-task learning?

## Architecture Onboarding

- Component map: Server coordinates aggregation of N client models, each with shared encoder and task-specific decoders. Server performs independent aggregation of encoders (Hyper Conflict-Averse) and decoders (Hyper Cross Attention).
- Critical path: Client local training → parameter updates sent to server → server disassembles models → encoder aggregation → decoder aggregation → personalized updates sent back → client update
- Design tradeoffs: Model incongruity flexibility vs. aggregation complexity; personalized weights vs. increased communication overhead
- Failure signatures: Performance worse than local training baseline; aggregation weights converging to extreme values; cross attention weights becoming uniform
- First 3 experiments:
  1. Run with only Hyper Conflict-Averse Aggregation enabled to measure encoder-only benefits
  2. Run with only Hyper Cross Attention Aggregation enabled to measure decoder-only benefits
  3. Run with both aggregations but fixed aggregation weights to isolate personalization benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FedHCA2 scale when applied to real-world federated learning scenarios with hundreds or thousands of clients, as opposed to the small-scale experiments presented in the paper?
- Basis in paper: [inferred] The paper demonstrates FedHCA2's effectiveness in scenarios with a limited number of clients, but does not explore its performance in large-scale real-world settings.
- Why unresolved: The experiments conducted in the paper are limited to a small number of clients, and the scalability of the approach to larger, more realistic federated learning environments is not addressed.
- What evidence would resolve it: Empirical results from experiments conducted with a large number of clients in real-world federated learning scenarios would provide insights into the scalability and effectiveness of FedHCA2.

### Open Question 2
- Question: How does FedHCA2 perform when applied to non-vision tasks, such as natural language processing or tabular data analysis, in the HC-FMTL setting?
- Basis in paper: [explicit] The paper focuses on vision tasks and does not explore the applicability of FedHCA2 to other domains or task types.
- Why unresolved: The experiments and evaluations in the paper are limited to vision tasks, and the performance of FedHCA2 on non-vision tasks in the HC-FMTL setting is not investigated.
- What evidence would resolve it: Empirical results from experiments conducted with non-vision tasks in the HC-FMTL setting would demonstrate the generalizability and effectiveness of FedHCA2 across different domains and task types.

### Open Question 3
- Question: How does the choice of aggregation weights (α and β) in FedHCA2 impact the overall performance, and can these weights be optimized dynamically during training to further improve results?
- Basis in paper: [explicit] The paper introduces learnable Hyper Aggregation Weights (α and β) for encoders and decoders, but does not explore the impact of different weight choices or the potential for dynamic optimization during training.
- Why unresolved: The paper presents the use of learnable weights but does not investigate the effects of different weight configurations or the possibility of optimizing these weights dynamically during training.
- What evidence would resolve it: Empirical results from experiments comparing different weight configurations and dynamic optimization strategies would provide insights into the impact of aggregation weights on FedHCA2's performance and the potential for further improvements.

## Limitations
- Limited exploration of the framework's scalability to large-scale federated learning scenarios with hundreds or thousands of clients
- Focus on vision tasks without investigation of the approach's applicability to non-vision domains like NLP or tabular data
- Limited analysis of the learned cross attention weights and their impact on task interactions across different dataset scenarios

## Confidence

- High confidence: Problem identification and framework design
- Medium confidence: Theoretical foundations of Hyper Conflict-Averse Aggregation
- Low confidence: Hyper Cross Attention Aggregation mechanism effectiveness

## Next Checks

1. **Sensitivity Analysis**: Conduct experiments varying the conflict aversion parameter c (e.g., 0.1, 0.5, 0.9) and analyze its impact on both convergence speed and final performance across different dataset pairs to determine optimal parameter ranges.

2. **Attention Weight Analysis**: Examine the learned cross attention weights across decoder layers and tasks to verify that the mechanism is actually learning meaningful task interactions rather than defaulting to uniform attention patterns.

3. **Client Heterogeneity Scaling**: Test the framework with increasing numbers of clients (e.g., 3, 6, 9 clients) having progressively more diverse task setups to evaluate whether the personalization benefits scale or degrade with system complexity.