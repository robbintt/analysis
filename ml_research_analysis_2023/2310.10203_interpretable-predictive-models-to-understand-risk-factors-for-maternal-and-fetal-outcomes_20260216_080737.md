---
ver: rpa2
title: Interpretable Predictive Models to Understand Risk Factors for Maternal and
  Fetal Outcomes
arxiv_id: '2310.10203'
source_url: https://arxiv.org/abs/2310.10203
tags:
- risk
- data
- pregnancy
- maternal
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study applies Explainable Boosting Machines (EBMs) to four
  high-impact maternal and fetal health outcomes: severe maternal morbidity (SMM),
  shoulder dystocia, preterm preeclampsia, and antepartum stillbirth. The EBMs achieve
  performance comparable to complex black-box models (XGBoost, Random Forests, DNNs)
  while outperforming logistic regression, with Area Under the ROC (AUROC) scores
  ranging from 0.700 to 0.767.'
---

# Interpretable Predictive Models to Understand Risk Factors for Maternal and Fetal Outcomes

## Quick Facts
- arXiv ID: 2310.10203
- Source URL: https://arxiv.org/abs/2310.10203
- Reference count: 40
- One-line primary result: Explainable Boosting Machines achieve high accuracy and interpretability for predicting maternal and fetal health outcomes, revealing clinically important risk factors.

## Executive Summary
This study applies Explainable Boosting Machines (EBMs) to predict and understand risk factors for four critical pregnancy outcomes: severe maternal morbidity, shoulder dystocia, preterm preeclampsia, and antepartum stillbirth. EBMs achieve AUROC scores between 0.700 and 0.767, matching the accuracy of complex black-box models like XGBoost and Random Forests while outperforming logistic regression. The key advantage of EBMs is their interpretability: they reveal non-obvious risk factors, such as maternal height for shoulder dystocia, and provide clear, additive feature contribution plots. Robustness is validated through external hospital-level validation and convergence analysis of feature contribution functions, confirming reliability across datasets.

## Method Summary
The study uses clinical data from 20 hospitals (158,629 births, 2016-2021) to train EBMs for predicting four pregnancy outcomes. Models are trained with hyperparameters including 25 outer and inner bags, minimum sample leaf sizes of 25 (or 15 for stillbirth), and 10 interactions. EBMs are compared to XGBoost, Random Forests, DNNs, and logistic regression using AUROC as the primary metric. External validation is performed by splitting hospitals into training (75%) and validation (25%) sets, ensuring equal representation of hospital care levels. Interpretability is assessed through feature importance rankings and shape function visualizations, while robustness is evaluated using discrete Fréchet distance analysis.

## Key Results
- EBMs achieve AUROC scores between 0.700 and 0.767, matching or exceeding black-box models like XGBoost and Random Forests.
- Maternal height emerges as the second most important feature for predicting shoulder dystocia, highlighting non-obvious risk factors.
- External validation confirms that EBMs generalize well across hospitals and settings, with robust performance across different care levels.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EBMs provide predictive accuracy on par with complex black-box models while maintaining interpretability.
- Mechanism: EBMs combine generalized additive models (GAMs) with gradient boosting and bagging, allowing them to learn non-linear relationships without sacrificing transparency. The cyclic training process enables each feature to contribute independently, creating additive shape functions that are easy to interpret.
- Core assumption: The additivity assumption holds—that the joint prediction can be decomposed into independent contributions from each feature.
- Evidence anchors:
  - [abstract] "EBMs match the accuracy of other black-box ML methods such as deep neural networks and random forests, and outperform logistic regression, while being more interpretable."
  - [section] "The Explainable Boosting Machine (EBM) is an algorithm for training the fi by boosting shallow decision trees in a round-robin fashion"
- Break condition: If strong feature interactions exist that cannot be captured by the model's interaction detection mechanism, the additivity assumption breaks and accuracy suffers.

### Mechanism 2
- Claim: EBMs reveal clinically important risk factors that are not traditionally recognized.
- Mechanism: By visualizing the shape functions fi(xi) for each feature, clinicians can identify non-obvious relationships between predictors and outcomes. The model's ability to learn complex non-linear relationships without manual feature engineering uncovers hidden patterns.
- Core assumption: The shape functions learned by the EBM accurately represent the true relationship between features and outcomes.
- Evidence anchors:
  - [abstract] "The interpretability of the EBM models reveals surprising insights into the features contributing to risk (e.g. maternal height is the second most important feature for shoulder dystocia)"
  - [section] "Plotting fi(xi) provides the end-user with a visual representation of the function learned by the tree ensemble"
- Break condition: If the training data is biased or incomplete, the shape functions may reflect data artifacts rather than true clinical relationships.

### Mechanism 3
- Claim: EBMs are robust to variations in training data and generalize well across hospitals.
- Mechanism: The combination of inner-bagging (ensembling trees within each feature) and outer-bagging (ensembling entire models) reduces variance and improves stability. The discrete Fréchet distance analysis shows that shape functions converge as more data is added.
- Core assumption: The model's performance is stable across different subsets of the data and across different hospitals.
- Evidence anchors:
  - [section] "We compute the discrete Fréchet distances from the shape functions trained on subsets of the data to the shape function trained on the largest data set, which shows the convergence of the sequence of shape functions"
  - [section] "We perform external validation to confirm that the models generalize well to hospitals and settings different from the training set"
- Break condition: If there are systematic differences in data collection or patient populations across hospitals that are not captured by the model, external validation performance may degrade.

## Foundational Learning

- Generalized Additive Models (GAMs)
  - Why needed here: Understanding GAMs is essential to grasp how EBMs work and why they are interpretable. GAMs decompose the prediction into additive components, making each feature's contribution transparent.
  - Quick check question: What is the key difference between a GAM and a traditional linear regression model?

- Gradient Boosting
  - Why needed here: EBMs use gradient boosting to train the shape functions for each feature. Understanding this optimization process helps explain how the model learns complex relationships.
  - Quick check question: How does gradient boosting differ from random forests in terms of training process?

- External Validation
  - Why needed here: The paper emphasizes external validation as crucial for ensuring model generalizability in healthcare settings. Understanding this concept is key to interpreting the robustness claims.
  - Quick check question: Why is external validation more informative than cross-validation for assessing model generalizability?

## Architecture Onboarding

- Component map:
  - Data preprocessing -> Feature engineering, missing value imputation, filtering implausible values
  - EBM training -> Cyclic boosting, inner-bagging, outer-bagging, interaction detection
  - Evaluation -> AUROC calculation, calibration analysis, external validation
  - Interpretation -> Shape function visualization, feature importance ranking

- Critical path:
  1. Data preparation and filtering
  2. EBM model training with hyperparameter optimization
  3. External validation across hospital partitions
  4. Interpretation of shape functions and feature importance
  5. Robustness analysis using Fréchet distance

- Design tradeoffs:
  - Accuracy vs. interpretability: EBMs prioritize interpretability but still achieve high accuracy
  - Complexity vs. robustness: The combination of inner and outer bagging increases training time but improves stability
  - Feature selection vs. completeness: Including all available features may improve accuracy but could introduce noise

- Failure signatures:
  - Poor external validation performance indicates lack of generalizability
  - Shape functions that don't converge with increasing data suggest model instability
  - Calibration curves that deviate from diagonal indicate poor probability estimates

- First 3 experiments:
  1. Train EBM on a single outcome with default hyperparameters and visualize the shape functions to understand the interpretability
  2. Compare EBM performance to logistic regression and random forests on a held-out validation set to verify the accuracy claims
  3. Perform a sensitivity analysis by training the model on subsets of the data to observe convergence of shape functions and robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the identified risk factors (e.g., maternal height for shoulder dystocia) suggest new clinical interventions that could improve maternal and fetal outcomes?
- Basis in paper: [explicit] The paper highlights surprising risk factors such as maternal height for shoulder dystocia and emphasizes the potential for clinical application.
- Why unresolved: While the paper identifies important risk factors, it does not test or propose specific clinical interventions based on these findings.
- What evidence would resolve it: Clinical trials or observational studies testing interventions targeting the newly identified risk factors (e.g., pelvic dimension measurements or tailored prenatal care based on maternal height).

### Open Question 2
- Question: How do the EBMs perform when applied to different healthcare systems or international datasets?
- Basis in paper: [inferred] The paper focuses on U.S. data and mentions the importance of external validation but does not test the models on international datasets.
- Why unresolved: The models were validated on U.S. hospitals only, leaving uncertainty about their generalizability to other healthcare systems or populations.
- What evidence would resolve it: External validation studies using datasets from diverse international healthcare systems to assess model performance and robustness.

### Open Question 3
- Question: What is the optimal balance between model interpretability and accuracy for clinical decision-making?
- Basis in paper: [explicit] The paper demonstrates that EBMs achieve high accuracy while maintaining interpretability, but it does not explore the trade-offs between interpretability and other performance metrics.
- Why unresolved: The paper does not investigate whether further improvements in accuracy (e.g., through more complex models) would outweigh the benefits of interpretability in clinical settings.
- What evidence would resolve it: Comparative studies evaluating the impact of model interpretability versus accuracy on clinical outcomes, such as decision-making efficiency or patient safety.

## Limitations
- Data Source Limitations: The study relies on data from a single health system, potentially limiting generalizability despite external validation.
- Temporal Generalization: Models are trained on 2016-2021 data and may not capture evolving risk factors or changes in clinical practice over time.
- Missing Variable Impact: The absence of key risk factors like genetic markers may affect model performance and interpretability.

## Confidence

- **High Confidence**: AUROC performance metrics and external validation results showing EBM accuracy comparable to black-box models.
- **Medium Confidence**: Interpretability claims regarding clinically important non-obvious risk factors, as these depend on the quality and completeness of the underlying clinical data.
- **Medium Confidence**: Robustness claims based on convergence analysis, as the Fréchet distance methodology, while rigorous, has not been widely validated in clinical ML contexts.

## Next Checks

1. Conduct temporal validation by training models on earlier years and testing on later years to assess temporal generalization.
2. Implement sensitivity analysis to evaluate how missing key risk factors (e.g., genetic markers) would impact model performance and interpretability.
3. Perform subgroup analysis across different demographic populations to identify potential bias or performance disparities in the models.