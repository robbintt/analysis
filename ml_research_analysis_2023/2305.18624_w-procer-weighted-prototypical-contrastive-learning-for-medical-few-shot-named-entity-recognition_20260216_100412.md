---
ver: rpa2
title: 'W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot
  Named Entity Recognition'
arxiv_id: '2305.18624'
source_url: https://arxiv.org/abs/2305.18624
tags:
- entity
- contrastive
- learning
- medical
- type
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes W-PROCER, a novel framework for few-shot named
  entity recognition (NER) in the medical domain. It addresses the challenge of class
  collision in contrastive learning, where the label "OUTSIDE (O)" undesirably pushes
  apart entities that are semantically related.
---

# W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition

## Quick Facts
- arXiv ID: 2305.18624
- Source URL: https://arxiv.org/abs/2305.18624
- Authors: 
- Reference count: 12
- Key outcome: W-PROCER achieves micro-F1 scores of 36.91, 40.26, and 38.86 on 1-shot tasks, and 46.43, 56.02, and 40.90 on 5-shot tasks for I2B2'14, BC5CDR, and NCBI respectively

## Executive Summary
This paper proposes W-PROCER, a novel framework for few-shot named entity recognition (NER) in the medical domain. It addresses the challenge of class collision in contrastive learning, where the label "OUTSIDE (O)" undesirably pushes apart entities that are semantically related. W-PROCER introduces a prototype-based contrastive learning approach that constructs prototypes from "O" labeled tokens and employs a weighting network to differentiate negative samples. The method significantly outperforms strong baselines on three medical benchmark datasets.

## Method Summary
W-PROCER combines prototype-based contrastive learning with a weighting network to address class collision in few-shot medical NER. The approach clusters "O" labeled tokens into prototypes, then applies a weighted contrastive loss that differentiates between positive and negative prototypes based on semantic distance to type descriptions. It uses GatorTron for encoding, implements both type-based and prototype-based contrastive losses, and combines these with a cross-entropy layer during training.

## Key Results
- Achieves state-of-the-art performance on I2B2'14, BC5CDR, and NCBI datasets
- Outperforms GPT-3.5 on medical NER tasks, highlighting the need for domain-specific training
- Ablation studies confirm the effectiveness of both weighting network and prototype-based contrastive learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weighted prototypical contrastive learning mitigates class collision by pushing apart prototypes from "O" tokens that share similar semantics with labeled entities
- Mechanism: The approach clusters "O" labeled tokens into prototypes, then applies a weighted contrastive loss that differentiates between positive and negative prototypes based on semantic distance to type descriptions
- Core assumption: Tokens labeled as "O" can be meaningfully clustered based on their semantic similarity to labeled entities
- Evidence anchors: [abstract] "constructs prototypes from 'O' labeled tokens and employs a weighting network to differentiate negative samples"

### Mechanism 2
- Claim: Type-based contrastive learning increases discrimination between different entity types by aligning tokens with their type descriptions
- Mechanism: Embeddings of type descriptions serve as anchors in vector space, and contrastive loss pulls tokens of the same type closer to their anchor while pushing them away from other type anchors
- Core assumption: Entity type descriptions provide meaningful semantic anchors that can guide representation learning
- Evidence anchors: [section 4.3] "Type-based contrastive loss that encourages the reduction of distances between entities with the same type"

### Mechanism 3
- Claim: Weighting network improves contrastive learning by assigning higher importance to harder negative samples
- Mechanism: A neural network computes similarity between type embeddings and cluster centers, producing weights that emphasize more confusable negative examples in the contrastive loss
- Core assumption: Not all negative samples are equally informative for learning discriminative representations
- Evidence anchors: [section 4.3] "introduce a weighting network that is utilized in both of the aforementioned contrastive objectives"

## Foundational Learning

- Concept: Contrastive learning framework
  - Why needed here: Understanding how positive and negative pairs are constructed and how loss functions operate is essential for grasping W-PROCER's approach
  - Quick check question: What is the fundamental difference between standard contrastive loss and prototypical contrastive loss?

- Concept: Prototype-based representation learning
  - Why needed here: W-PROCER relies on clustering and prototype construction to handle "O" labeled tokens, requiring understanding of how prototypes capture semantic structure
  - Quick check question: How does clustering "O" tokens help differentiate between entities that should be semantically close?

- Concept: Few-shot learning paradigms
  - Why needed here: The paper operates in few-shot setting where limited labeled data necessitates special techniques for generalization
  - Quick check question: Why is class collision particularly problematic in few-shot named entity recognition compared to traditional supervised settings?

## Architecture Onboarding

- Component map: Input text → GatorTron encoding → Token clustering → Prototype construction → Weighted contrastive loss → Cross-entropy optimization → Inference

- Critical path: Input text → GatorTron encoding → Token clustering → Prototype construction → Weighted contrastive loss → Cross-entropy optimization → Inference

- Design tradeoffs:
  - Clustering vs. fixed prototypes: Clustering allows adaptive grouping of "O" tokens but adds computational complexity
  - Weighting vs. equal treatment: Weighting focuses on harder negatives but requires additional network parameters
  - Hybrid loss vs. single objective: Combining multiple losses captures different aspects but requires tuning β parameter

- Failure signatures:
  - Poor clustering quality → Ineffective prototype separation → Confused predictions
  - Incorrect distance threshold α → Too few/many positive prototypes → Loss function instability
  - Weighting network failure → Amplified noise → Degraded discrimination
  - Insufficient medical knowledge in encoder → Poor semantic understanding → Limited applicability to medical domain

- First 3 experiments:
  1. Ablation study: Remove weighting network (W-PROCER w/o weight) to verify its contribution to performance
  2. Hyperparameter sensitivity: Test different k values (3, 4, 5) and β settings (0, 0.3, 0.5, 0.9, 1) to find optimal configuration
  3. Masking strategy: Apply different mask ratios (Mask-2, Mask-3) to evaluate robustness to unlabeled data scenarios

## Open Questions the Paper Calls Out
None explicitly mentioned in the provided text.

## Limitations
- Experimental evaluation limited to three medical benchmark datasets, potentially limiting generalizability
- Performance gains show substantial room for improvement, especially in 1-shot scenarios
- Clustering approach for "O" tokens relies heavily on distance threshold α and cluster count k parameters

## Confidence
- High Confidence: Experimental methodology and evaluation metrics are clearly specified and appropriately applied
- Medium Confidence: Claims about class collision and mitigation mechanisms are supported but rely on assumptions about "O" token clustering
- Medium Confidence: Comparison with GPT-3.5 highlights limitations but may not fully capture large language model capabilities

## Next Checks
1. Evaluate W-PROCER performance across a wider range of medical domains and entity types to assess generalizability
2. Conduct comprehensive experiments varying the distance threshold α, cluster count k, and weighting parameters
3. Perform detailed analysis of model predictions to identify specific cases where class collision persists or where the weighting network fails