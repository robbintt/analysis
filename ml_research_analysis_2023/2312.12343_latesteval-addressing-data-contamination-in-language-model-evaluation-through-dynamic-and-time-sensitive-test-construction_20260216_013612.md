---
ver: rpa2
title: 'LatestEval: Addressing Data Contamination in Language Model Evaluation through
  Dynamic and Time-Sensitive Test Construction'
arxiv_id: '2312.12343'
source_url: https://arxiv.org/abs/2312.12343
tags:
- data
- latesteval
- contamination
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LatestEval, an automated method for creating
  reading comprehension evaluations that avoid data contamination by using only the
  most recent texts. The method gathers latest texts from sources like arXiv and news,
  extracts key information as answers, and generates questions targeting that information
  while removing explicit answers to encourage inference.
---

# LatestEval: Addressing Data Contamination in Language Model Evaluation through Dynamic and Time-Sensitive Test Construction

## Quick Facts
- **arXiv ID**: 2312.12343
- **Source URL**: https://arxiv.org/abs/2312.12343
- **Reference count**: 4
- **Primary result**: Automated method creates reading comprehension benchmarks using only recent texts, showing negligible model memorization compared to existing benchmarks

## Executive Summary
LatestEval introduces an automated approach to creating reading comprehension benchmarks that avoid data contamination by using only texts published within a recent time window. The method gathers latest texts from sources like arXiv, BBC, and GitHub, extracts key information as answers, and generates questions targeting that information while removing explicit answers to encourage inference. Experiments demonstrate that language models exhibit negligible memorization on LatestEval compared to previous benchmarks, suggesting significantly reduced contamination risk. Human evaluation confirms the benchmark is generally reliable and effective for assessing model capabilities.

## Method Summary
LatestEval is an automated pipeline that creates reading comprehension benchmarks using only recently published texts (within a 7-day window) to avoid data contamination. The method collects texts from arXiv, BBC, and GitHub APIs, extracts key information as answers using keyword extraction and GPT-4, generates questions targeting that information, and removes explicit answers from the context by replacing answer-containing sentences with placeholders. This forces models to infer answers rather than copy them. The approach uses perplexity comparison against fresh content to detect contamination and LLM-as-a-judge for model evaluation.

## Key Results
- Language models show negligible memorization on LatestEval compared to existing benchmarks
- Human evaluation found 83.6% of documents had faithful answers and 81.4% were answerable
- Perplexity on LatestEval was similar to or higher than fresh Wikipedia content, indicating no prior knowledge

## Why This Works (Mechanism)

### Mechanism 1
Using only the most recent texts avoids contamination by ensuring no overlap with training corpora. By collecting texts published within a recent time window (e.g., 7 days), the method ensures that language models cannot have encountered these texts during pretraining since they were created after training concluded. Core assumption: Training corpora are fixed and do not include future texts beyond a certain cutoff date.

### Mechanism 2
Removing explicit answers from context forces inference rather than memorization. After extracting key information as answers, LatestEval replaces sentences containing these answers with placeholder sentences, requiring models to reason from the remaining context rather than simply retrieving memorized content. Core assumption: Models will attempt to copy answers if they are present verbatim in the passage, but will be forced to reason when answers are removed.

### Mechanism 3
Perplexity-based contamination detection identifies memorization by comparing to fresh content. The method compares model perplexity on benchmark test examples to perplexity on fresh Wikipedia content created after model training, using significantly lower perplexity as evidence of memorization/contamination. Core assumption: Unseen, recent content should have similar perplexity to test data if no contamination exists, while contaminated data will show lower perplexity.

## Foundational Learning

- **Data contamination in language model evaluation**: Understanding what data contamination is and why it's problematic is essential for appreciating the motivation behind LatestEval. Quick check: What is the difference between input contamination and input-and-label contamination?

- **Perplexity as a measure of model uncertainty**: The contamination detection mechanism relies on comparing perplexities between different datasets to identify memorization. Quick check: If a model has memorized a sequence, should its perplexity on that sequence be higher or lower than on unseen content?

- **Reading comprehension benchmark construction**: LatestEval follows the standard passage-query-answer format but with automated construction methods that need to be understood. Quick check: What are the three essential components of a reading comprehension test instance?

## Architecture Onboarding

- **Component map**: Text Collection Module -> Answer Extraction Pipeline -> Question Generation System -> Context Modification -> Evaluation Framework
- **Critical path**: Text Collection → Answer Extraction → Question Generation → Context Modification → Evaluation
- **Design tradeoffs**: Using only recent texts limits the diversity of content but ensures contamination avoidance; automated construction trades human quality control for scalability and timeliness; GPT-4 dependency provides flexibility but introduces cost and potential knowledge leakage
- **Failure signatures**: High perplexity on LatestEval compared to fresh content might indicate poor construction rather than lack of contamination; unanswerable questions due to over-aggressive answer removal; models finding answers elsewhere in the passage despite removal attempts
- **First 3 experiments**:
  1. Run perplexity comparison between LatestEval and existing benchmarks across multiple model sizes to validate contamination detection
  2. Conduct human evaluation on a sample of 200 documents to assess faithfulness, answerability, and copyability issues
  3. Test model performance on LatestEval using LLM-as-a-judge to establish baseline capabilities and compare with existing benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How effective is LatestEval at preventing data contamination for models trained on dynamically updated web data?
- **Basis in paper**: The paper states that LatestEval avoids contamination by using only texts published within a recent time window, ensuring no overlap with training corpora.
- **Why unresolved**: The paper shows that LatestEval reduces contamination risk compared to existing benchmarks, but does not directly test models trained on continuously updated web data.
- **What evidence would resolve it**: Experiments comparing LatestEval performance on models trained on static vs. dynamically updated web data.

### Open Question 2
- **Question**: What is the optimal time window for collecting latest texts to balance data freshness and volume?
- **Basis in paper**: The paper uses a 7-day time window for collecting texts from arXiv, BBC, and GitHub.
- **Why unresolved**: The paper does not explore the impact of different time window lengths on data volume and contamination risk.
- **What evidence would resolve it**: Experiments testing LatestEval performance with different time window lengths.

### Open Question 3
- **Question**: How can the quality and faithfulness of automatically generated questions and answers be further improved?
- **Basis in paper**: The paper identifies issues with faithfulness, answerability, and copyability in the human evaluation section.
- **Why unresolved**: The paper suggests potential solutions like post-processing and few-shot settings, but does not implement or test them.
- **What evidence would resolve it**: Experiments implementing and evaluating proposed quality improvement techniques.

## Limitations

- Automated construction pipeline produces some unusable content (13.4% unfaithful answers, 18.6% unanswerability issues)
- Focus on recent texts may limit diversity and generalizability of benchmark content
- Method cannot prevent models from having memorized similar content or possessing general knowledge

## Confidence

- **High Confidence**: LatestEval provides a method for creating time-sensitive benchmarks that reduces contamination risk (well-supported by methodology and experimental results)
- **Medium Confidence**: LatestEval is "significantly less contaminated" than existing benchmarks (supported by perplexity metrics but relies on indirect evidence)
- **Low Confidence**: LatestEval can fully prevent all forms of data contamination (addresses temporal contamination but cannot prevent models from having memorized similar content)

## Next Checks

1. **Direct Training Data Verification**: Cross-reference a sample of LatestEval passages against the actual training corpora of tested models to empirically confirm absence of contamination, rather than relying solely on temporal assumptions.

2. **Controlled Memorization Test**: Create a subset of LatestEval with known content (artificially inserted) and verify that the perplexity detection mechanism correctly identifies this as contaminated, establishing ground truth for the detection approach.

3. **Generalization Impact Assessment**: Evaluate whether the focus on recent texts creates a bias toward specific domains or writing styles, potentially limiting the benchmark's ability to assess general language model capabilities across diverse content types.