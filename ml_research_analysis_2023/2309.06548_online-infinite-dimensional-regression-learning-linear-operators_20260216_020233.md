---
ver: rpa2
title: 'Online Infinite-Dimensional Regression: Learning Linear Operators'
arxiv_id: '2309.06548'
source_url: https://arxiv.org/abs/2309.06548
tags:
- online
- learning
- class
- operator
- operators
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies online learning of linear operators between\
  \ infinite-dimensional Hilbert spaces under squared loss. It shows that operators\
  \ with uniformly bounded p-Schatten norm are online learnable for any p \u2208 [1,\u221E\
  ), with regret bounds matching known PAC learning rates."
---

# Online Infinite-Dimensional Regression: Learning Linear Operators

## Quick Facts
- arXiv ID: 2309.06548
- Source URL: https://arxiv.org/abs/2309.06548
- Reference count: 6
- Key outcome: Linear operators with uniformly bounded p-Schatten norm are online learnable for any p ∈ [1,∞), while those with bounded operator norm are not; a separation between online learnability and uniform convergence is established.

## Executive Summary
This paper studies online learning of linear operators between infinite-dimensional Hilbert spaces under squared loss. The authors establish a dichotomy: operators with bounded p-Schatten norm are online learnable with regret bounds matching PAC learning rates, while those with bounded operator norm are not learnable. Additionally, they construct a class of operators that is online learnable but fails to satisfy uniform convergence, demonstrating a fundamental separation between these two concepts. The key technical tools are sequential Rademacher complexity analysis and a novel construction inspired by multiclass classification.

## Method Summary
The authors use sequential Rademacher complexity to characterize online learnability. For p-Schatten norm bounded operators, they show the sequential Rademacher complexity scales as O(T^max{1/2, 1-1/p}), yielding the desired regret bounds. For the impossibility result, they construct a hard instance where the adversary can force linear regret by exploiting the infinite-dimensional structure. The separation between learnability and uniform convergence is demonstrated through a specific construction using operators indexed by natural numbers, where online learnability is achieved via multiplicative weights over experts while uniform convergence fails.

## Key Results
- p-Schatten norm bounded operators are online learnable with regret bounds matching PAC learning rates
- Operator norm bounded operators are not online learnable (linear regret lower bound)
- Existence of a class online learnable but failing uniform convergence, establishing a separation between these concepts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: p-Schatten norm bounded operators are online learnable with regret bounds matching PAC learning rates
- **Mechanism**: Sequential Rademacher complexity analysis provides a sufficient condition for online learnability by bounding the expected supremum of Rademacher-weighted losses
- **Core assumption**: Loss function can be normalized to [0,1] for applying Rakhlin et al.'s theorem
- **Evidence anchors**: Abstract and Section 3 establish the regret bound of O(T^max{1/2, 1-1/p})
- **Break condition**: If normalization fails or sequential Rademacher complexity grows faster than O(T^max{1/2, 1-1/p})

### Mechanism 2
- **Claim**: Operator norm bounded operators are not online learnable
- **Mechanism**: Hard instance construction where adversary forces linear regret using rank-1 operators depending on entire history of Rademacher variables
- **Core assumption**: Infinite-dimensional spaces allow selection of orthonormal basis vectors as inputs
- **Evidence anchors**: Abstract and Section 4.2 prove linear regret lower bound
- **Break condition**: If spaces were finite-dimensional or adversary constrained in input selection

### Mechanism 3
- **Claim**: Class exists that is online learnable but uniform convergence fails
- **Mechanism**: Operators indexed by natural numbers with online learnability via multiplicative weights over experts, while uniform convergence fails due to Ω(T) sequential Rademacher complexity
- **Core assumption**: Infinite-dimensional target space allows increasingly complex operator behavior
- **Evidence anchors**: Abstract and Section 5 show both the separation and the algorithmic construction
- **Break condition**: If target space were finite-dimensional or operators uniformly bounded in operator norm

## Foundational Learning

- **Concept**: Hilbert spaces and orthonormal bases
  - **Why needed here**: The paper works with infinite-dimensional Hilbert spaces V and W, and analysis relies on properties of orthonormal bases and inner product structure
  - **Quick check question**: Given an orthonormal basis {en}∞n=1 for a Hilbert space V, what is the expansion of any vector v ∈ V in terms of this basis?

- **Concept**: Schatten norms and compact operators
  - **Why needed here**: Learnability results depend on p-Schatten norm, and hardness results rely on distinction between compact and non-compact operators
  - **Quick check question**: For a compact operator f: V → W, how is the p-Schatten norm related to the singular values of f?

- **Concept**: Sequential Rademacher complexity
  - **Why needed here**: This complexity measure characterizes online learnability by bounding expected supremum of Rademacher-weighted losses
  - **Quick check question**: What is the relationship between sequential Rademacher complexity and online uniform convergence?

## Architecture Onboarding

- **Component map**: Online learner maintains distribution over finite set of experts (operators) → Updates distribution using multiplicative weights based on observed losses → Outputs weighted average prediction
- **Critical path**: 1) Receive input xt from nature, 2) Compute predictions from each expert, 3) Calculate losses based on actual output yt, 4) Update expert weights using multiplicative weights rule, 5) Output weighted average prediction
- **Design tradeoffs**: Finite expert set limits computational complexity but may sacrifice some learning performance compared to continuous distribution over operators
- **Failure signatures**: Sequential Rademacher complexity growing faster than sublinearly causes online learnability failure; operator class containing non-compact operators with unbounded operator norm triggers hardness result
- **First 3 experiments**:
  1. Implement multiplicative weights algorithm for specific operator class in Theorem 8 with small T (e.g., T=10) to verify regret bound empirically
  2. Test algorithm on synthetic dataset where true operator has known singular value decomposition matching construction proof
  3. Modify expert construction to use different basis functions (not just orthonormal bases) and measure effect on regret bound

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal regret of learning Fp for p ∈ [1, 2)?
- **Basis in paper**: Section 6 states gap between upper and lower bounds for p ∈ [1, 2), with upper bound saturating at √T while lower bound worsens as p decreases
- **Why unresolved**: Authors conjecture their upper bound in Theorem 1 is loose for p ∈ [1, 2) and suggest Vovk-Azoury-Warmuth forecaster might achieve faster rates
- **What evidence would resolve it**: Formal proof showing either existing upper bound is tight, or new algorithm with improved regret bounds for p ∈ [1, 2)

### Open Question 2
- **Question**: Is online uniform convergence and online learnability equivalent for every F ⊆ {f ∈ S∞(V, W) | ∥f∥∞ ≤ 1}?
- **Basis in paper**: Section 6 raises this question after showing separation between uniform convergence and learnability for class in S∞(V, W) that is not uniformly bounded
- **Why unresolved**: While authors show separation for non-uniformly bounded classes in S∞(V, W), they don't know if such separation exists for uniformly bounded subsets of S∞(V, W)
- **What evidence would resolve it**: Either proof that every uniformly bounded subset of S∞(V, W) satisfies equivalence between online uniform convergence and online learnability, or counterexample showing uniformly bounded subset where equivalence fails

## Limitations
- Separation results rely heavily on infinite-dimensional structure, limiting practical applicability
- Constructions require careful handling of orthonormal bases and infinite-dimensional properties
- Results may be sensitive to specific choices of basis representations

## Confidence
- **High confidence**: Learnability result for p-Schatten norm bounded operators (Theorem 1) - sequential Rademacher complexity analysis is well-established
- **Medium confidence**: Impossibility result for operator norm bounded classes (Theorem 4) - construction is sound but adversarial proof less robust
- **Medium confidence**: Separation between online learnability and uniform convergence - technically correct but relies on specific infinite-dimensional properties

## Next Checks
1. Implement online learning algorithm for p-Schatten operators in finite-dimensional approximations (d=10, 20, 50) and verify regret bounds scale as predicted when transitioning to infinite-dimensional limit
2. Modify adversary's strategy in Theorem 4 to use different basis selection rules and verify linear regret persists across variations, establishing robustness of impossibility result
3. Test separating class construction with different orthonormal bases and target space dimensions to determine which aspects of infinite-dimensional structure are essential for separation result