---
ver: rpa2
title: '"We care": Improving Code Mixed Speech Emotion Recognition in Customer-Care
  Conversations'
arxiv_id: '2308.03150'
source_url: https://arxiv.org/abs/2308.03150
tags:
- emotion
- speech
- dataset
- recognition
- negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of speech emotion recognition
  in natural, code-mixed customer-care conversations, which are often noisy and involve
  frequent language switching. A key contribution is the Natural Speech Emotion Dataset
  (NSED), a 5000+ utterance dataset annotated for emotion, sentiment, valence, arousal,
  and dominance (VAD).
---

# "We care": Improving Code Mixed Speech Emotion Recognition in Customer-Care Conversations

## Quick Facts
- arXiv ID: 2308.03150
- Source URL: https://arxiv.org/abs/2308.03150
- Reference count: 9
- Primary result: Word-level VAD values improve negative emotion detection by 2% in code-mixed customer-care speech

## Executive Summary
This study addresses the challenge of speech emotion recognition in natural, code-mixed customer-care conversations, which are often noisy and involve frequent language switching. The authors introduce the Natural Speech Emotion Dataset (NSED) with 5000+ utterances annotated for emotion, sentiment, valence, arousal, and dominance (VAD). The core method combines speech features from Wav2Vec2, text features from multilingual BERT, and word-level VAD values through a BiLSTM model. The key contribution is demonstrating that incorporating word-level VAD values improves negative emotion detection by 2% over the baseline.

## Method Summary
The approach uses continual pre-training of Wav2Vec2 on the target domain audio, then fine-tunes it for emotion recognition. ASR-generated transcripts are processed with multilingual BERT to extract text features. Word-level VAD values are obtained from the NRG-VAD lexicon and concatenated with speech and text features. A BiLSTM model processes these multimodal features to predict emotions. The system achieves high precision (>90%) for neutral emotion but lower performance for positive emotions due to dataset imbalance.

## Key Results
- 2% improvement in weighted-average precision for negative emotions by incorporating word-level VAD values
- 90%+ precision for neutral emotion detection
- Dataset imbalance (61% neutral, 14% positive) limits performance on positive emotions

## Why This Works (Mechanism)

### Mechanism 1
Word-level VAD values improve negative emotion detection in code-mixed customer-care speech. VAD values provide granular emotional intensity cues at the word level that speech-only features miss in noisy, code-mixed contexts. The core assumption is that word-level emotional cues are more reliable than utterance-level ones in natural conversational speech with frequent language switching. Evidence shows a 2% improvement in weighted-average precision for negative emotions when VAD values are incorporated. Break condition: If ASR quality degrades so much that VAD word lookups fail, the 2% gain disappears.

### Mechanism 2
Continual pre-training of Wav2Vec2 on the target domain audio improves performance over off-the-shelf models. Fine-tuning Wav2Vec2 on the actual customer-care dataset adapts its speech representations to the domain's acoustic and linguistic characteristics (code-mixing, noise patterns). The core assumption is that speech representations learned on Librispeech do not generalize well to customer-care conversations without domain adaptation. Evidence comes from comparing performance with and without continual pre-training, though direct comparisons are limited in the corpus. Break condition: If the unlabeled audio is too noisy or too small, pre-training yields negligible gains.

### Mechanism 3
Fusing speech, text, and VAD features via BiLSTM outperforms single-modality models for emotion recognition in code-mixed speech. Speech captures prosody, text captures lexical semantics, and VAD captures emotional intensity; BiLSTM models temporal dependencies across modalities. The core assumption is that different modalities encode complementary emotional information that a single modality cannot capture. Evidence shows that concatenating Wav2Vec2 and BERT features achieves 0.64 weighted-average precision for negative emotions, demonstrating that textual features provide additional emotional information absent from speech features alone. Break condition: If modality fusion weights are poorly tuned, performance can degrade below single-modality baselines.

## Foundational Learning

- Code-mixing and code-switching in speech: Customer-care conversations frequently switch between Hindi and English, affecting ASR and feature extraction. Quick check: What happens to ASR accuracy when two languages are spoken in the same utterance without clear boundaries?

- Wav2Vec2 self-supervised speech representation learning: Wav2Vec2 learns rich speech features without labeled data, which is crucial given the limited annotated dataset. Quick check: How does continual pre-training on domain-specific audio change the learned representations compared to pre-training on Librispeech?

- VAD dimensional model of emotion: VAD values (valence, arousal, dominance) provide a continuous representation of emotion intensity useful for regression and classification tasks. Quick check: How do you map discrete emotion labels (e.g., anger, sad) to continuous VAD values for annotation?

## Architecture Onboarding

- Component map: Audio input -> Wav2Vec2 (Indic) -> speech embeddings (768-dim) -> concatenation -> BiLSTM -> FC + softmax -> emotion classification; Whisper ASR -> transcript generation -> multilingual BERT -> text embeddings (768-dim) -> concatenation; NRG-VAD lexicon -> word-level VAD values -> concatenation

- Critical path: 1. Load and preprocess audio (mono, 8kHz) 2. Generate ASR transcript 3. Extract Wav2Vec2 speech features 4. Extract BERT text features from transcript 5. Map words to VAD values 6. Concatenate features 7. Feed into BiLSTM + FC + softmax

- Design tradeoffs: ASR accuracy vs. VAD coverage (better ASR improves text features and VAD lookups but is harder in code-mixed, noisy data); Model complexity vs. dataset size (BiLSTM is lightweight enough for ~5k utterances; transformers would overfit); Continual pre-training time vs. accuracy gain (adapter-based pre-training reduces compute but may limit adaptation depth)

- Failure signatures: Low ASR accuracy → missing or wrong text features → poor emotion predictions; VAD lexicon missing many words → incomplete VAD feature vector → reduced multimodal benefit; Class imbalance (61% neutral) → model biased toward neutral prediction

- First 3 experiments: 1. Train baseline Wav2Vec2-only model; measure precision on negative emotions 2. Add BERT text features; compare performance to baseline 3. Add VAD features; measure improvement over text+speech model

## Open Questions the Paper Calls Out

- How would the proposed model perform on a balanced dataset with equal representation of positive and negative emotions? The current dataset is imbalanced (61% neutral, 25% negative, 14% positive), making it difficult to assess true performance on positive emotions. Testing on a balanced dataset would provide insights into the model's true performance across all emotion categories.

- How would the performance change if a code-mixed Wav2Vec2 model specifically trained on Hindi-English data was used instead of the multilingual Wav2Vec2-xlsr-53 model? The current model uses a multilingual Wav2Vec2 model, which may not be optimal for code-mixed speech data. A model specifically trained on Hindi-English code-mixed data could potentially improve performance.

- How would the performance change if a more accurate ASR model was used to generate transcripts from the speech input? The current ASR model struggles with code-switching and noisy environments, leading to transcription errors that affect textual features and word-level VAD values. A more accurate ASR model could potentially improve the overall performance of the SER system.

## Limitations

- Dataset imbalance (61% neutral, 25% negative, 14% positive) creates significant constraints on model generalizability, particularly for positive emotion detection
- Reliance on ASR-generated transcripts introduces potential cascading errors, particularly problematic in code-mixed contexts where ASR performance degrades substantially
- The 2% improvement for negative emotions, while statistically meaningful, represents a relatively modest gain that may not generalize across different code-mixed domains

## Confidence

**High Confidence**: The multimodal fusion architecture combining speech, text, and VAD features is well-established in emotion recognition literature. The dataset creation methodology and annotation process are clearly described. The 90%+ precision for neutral emotion detection is supported by strong empirical evidence.

**Medium Confidence**: The 2% improvement for negative emotions with word-level VAD incorporation is reported but requires careful interpretation given the dataset imbalance and potential overfitting to the specific corpus characteristics. The continual pre-training approach shows promise but lacks comparison to other domain adaptation methods.

**Low Confidence**: The generalizability of results to other code-mixed language pairs, different conversational domains, or larger datasets remains untested. The specific contribution of the BiLSTM architecture versus simpler temporal models is unclear.

## Next Checks

1. **ASR Quality Assessment**: Measure the correlation between ASR transcription accuracy and emotion recognition performance, particularly focusing on code-mixed segments where language boundaries are ambiguous.

2. **Dataset Balance Impact**: Conduct experiments with balanced class distributions to isolate the effect of dataset imbalance on positive emotion detection performance.

3. **VAD Lexicon Coverage Analysis**: Quantify the proportion of words lacking VAD values in the NRG lexicon and measure the impact on feature completeness and model performance.