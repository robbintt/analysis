---
ver: rpa2
title: A continuous Structural Intervention Distance to compare Causal Graphs
arxiv_id: '2307.16452'
source_url: https://arxiv.org/abs/2307.16452
tags:
- distribution
- causal
- mean
- 'true'
- adjustment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel metric called continuous Structural
  Intervention Distance (contSID) for comparing true and learned causal graphs in
  causal structure learning settings. Unlike existing metrics like Structural Hamming
  Distance (SHD) and Structural Intervention Distance (SID) that rely solely on graph
  properties, contSID incorporates underlying data by computing distances between
  intervention distributions implied by observational data.
---

# A continuous Structural Intervention Distance to compare Causal Graphs

## Quick Facts
- arXiv ID: 2307.16452
- Source URL: https://arxiv.org/abs/2307.16452
- Reference count: 4
- This paper proposes a novel metric called continuous Structural Intervention Distance (contSID) that incorporates underlying data into causal graph comparisons by embedding intervention distributions.

## Executive Summary
This paper introduces contSID, a continuous metric for comparing true and learned causal graphs that goes beyond traditional graph-based measures like SHD and SID. The key innovation is that contSID incorporates the underlying data distribution by computing distances between intervention distributions implied by observational data, rather than just comparing graph structures. The metric uses kernel conditional mean embeddings to represent intervention distributions and estimates their differences using maximum conditional mean discrepancy. Experimental results on synthetic data demonstrate that contSID provides more accurate comparisons between DAGs than existing metrics, particularly when edge weights vary, and shows that ICALiNGAM algorithm outperforms PC and GES algorithms across different node counts.

## Method Summary
The contSID metric embeds intervention distributions as conditional mean embeddings in reproducing kernel Hilbert spaces and estimates their differences using maximum conditional mean discrepancy (MCMD). For each pair of nodes, it computes the distance between their interventional distributions under the true and learned DAGs, using the observational distribution as a mean embedding. The pairwise distances are then aggregated to produce the final contSID score. The method requires valid adjustment sets to compute interventional distributions from observational data via the backdoor formula, and scales distances by the norm of the observational distribution to maintain scale invariance.

## Key Results
- contSID incorporates edge weights into DAG comparison, unlike SHD and SID
- ICALiNGAM algorithm outperforms PC and GES algorithms across different node counts and metrics
- contSID penalizes missing high-weight edges more than low-weight edges through intervention distribution comparison

## Why This Works (Mechanism)

### Mechanism 1
- Claim: contSID incorporates edge weights into DAG comparison, unlike SHD and SID.
- Mechanism: By embedding intervention distributions via conditional mean embeddings in RKHS and using maximum conditional mean discrepancy (MCMD), contSID captures quantitative differences in interventional effects rather than just structural differences.
- Core assumption: The observational data distribution is Markov with respect to both the true and learned DAGs, and causal sufficiency holds.
- Evidence anchors:
  - [abstract] "considers the underlying data in addition to the graph structure for its calculation of the difference between a true and a learnt causal graph."
  - [section 4] "contSID quantifies the pairwise difference in the interventional distributions by using the observational distribution (via the valid adjustment set/backdoor set formula) as a mean embedding."
- Break condition: If the causal Markov assumption fails or hidden confounders exist, the embedding-based distance becomes unreliable.

### Mechanism 2
- Claim: contSID scales the pairwise distances by the norm of the observational distribution to maintain scale invariance.
- Mechanism: Dividing by the norm of the observational embedding (CXj) ensures that distances are comparable across different scales of interventions.
- Core assumption: The kernel used to embed distributions is characteristic and the observational data provides sufficient coverage of the intervention space.
- Evidence anchors:
  - [section 4] "We divide by the norm of the embedding of the observational distribution Xj to make contSID scale-invariant."
- Break condition: If the observational data does not adequately represent the range of possible interventions, the scaling becomes misleading.

### Mechanism 3
- Claim: contSID distinguishes between the importance of different edges by weighting differences in interventional distributions.
- Mechanism: By comparing the full interventional distributions (via MCMD) rather than just edge presence, contSID penalizes missing high-weight edges more than low-weight edges.
- Core assumption: The underlying data follows a linear SEM with additive noise, allowing meaningful comparisons of interventional effects.
- Evidence anchors:
  - [section 1] "Intuitively, missing the edge V1 → V3 should be penalized more than missing the edge V2 → V3 since an intervention on V1 would lead to a larger difference in the distribution of V3 than the same intervention on V2."
- Break condition: If the data generating process is non-linear or contains complex interactions, the linear embedding assumption may break down.

## Foundational Learning

- Conditional Mean Embedding
  - Why needed here: To represent interventional distributions in a Hilbert space where distances can be computed.
  - Quick check question: What property must a kernel have to ensure unique embeddings of distributions?

- Reproducing Kernel Hilbert Space (RKHS)
  - Why needed here: Provides the mathematical framework for embedding and comparing distributions.
  - Quick check question: How does the reproducing property enable computation of inner products in the embedding space?

- Valid Adjustment Sets
  - Why needed here: Required to compute interventional distributions from observational data via the backdoor formula.
  - Quick check question: What graphical condition must a set satisfy to be a valid adjustment set for a given intervention-target pair?

## Architecture Onboarding

- Component map: Data -> DAG Learning Algorithm -> True DAG & Learnt DAG -> contSID computation pipeline (embedding -> MCMD -> aggregation)
- Critical path: Compute conditional mean embeddings -> Estimate MCMD -> Aggregate pairwise distances
- Design tradeoffs: More expressive kernels improve accuracy but increase computational cost; simpler metrics like SHD are faster but less informative
- Failure signatures: High variance in MCMD estimates indicates insufficient data or poor kernel choice; inconsistent distances across runs suggest numerical instability
- First 3 experiments:
  1. Generate synthetic data from a known DAG and compare contSID vs SHD/SID on small graphs
  2. Vary edge weights systematically and verify contSID penalizes high-weight errors more
  3. Test contSID on cyclic graphs to identify where the causal sufficiency assumption breaks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does contSID perform on real-world datasets compared to synthetic data, particularly in domains with known causal relationships?
- Basis in paper: [inferred] The paper only validates the metric on synthetic data and mentions that the underlying data distribution is assumed to be known and Markov with respect to the DAG.
- Why unresolved: Real-world datasets often contain hidden confounders, measurement errors, and complex dependencies that may violate the assumptions made in the synthetic experiments.
- What evidence would resolve it: Empirical studies applying contSID to benchmark real-world causal inference datasets (e.g., from the CauseMe challenge or similar repositories) comparing it with SHD and SID on datasets with ground truth causal structures.

### Open Question 2
- Question: What is the computational complexity of contSID and how does it scale with the number of nodes and samples in the dataset?
- Basis in paper: [inferred] The paper presents empirical results for up to 20 nodes but does not analyze the computational complexity or scalability of the algorithm.
- Why unresolved: The metric involves computing conditional mean embeddings and MMDs for each pair of nodes, which could become computationally prohibitive for large-scale networks.
- What evidence would resolve it: Theoretical analysis of time and space complexity, along with empirical benchmarks showing runtime and memory usage as functions of node count and sample size.

### Open Question 3
- Question: How sensitive is contSID to the choice of kernel functions and hyperparameters (e.g., regularization parameter λ)?
- Basis in paper: [explicit] The authors mention that popular kernels like Gaussian and Laplacian are characteristic, but do not explore the sensitivity to kernel choice or hyperparameters.
- Why unresolved: Different kernels and hyperparameters can significantly affect the performance of kernel-based methods, potentially leading to inconsistent comparisons between DAGs.
- What evidence would resolve it: Systematic experiments varying kernel types (Gaussian, Laplacian, polynomial) and hyperparameters, showing how these choices affect contSID values and comparisons with other metrics.

## Limitations
- Assumes causal sufficiency and linear SEM models, which may not hold in real-world applications
- Computational complexity scales poorly with increasing node counts due to pairwise distance computations
- Kernel choice for conditional mean embeddings is not specified, which could significantly impact performance

## Confidence
- **High confidence**: contSID successfully incorporates edge weights into DAG comparison through intervention distribution embeddings (demonstrated across all experimental settings)
- **Medium confidence**: contSID provides more accurate comparisons than SHD/SID specifically for ICALiNGAM (supported by experiments but not tested across diverse data generating processes)
- **Low confidence**: contSID generalizes well to non-linear causal mechanisms (not tested beyond linear SEM assumption)

## Next Checks
1. Test contSID on synthetic data with non-linear causal relationships (e.g., quadratic or threshold effects) to evaluate performance beyond linear assumptions
2. Evaluate contSID on real-world datasets with known causal structure (e.g., gene regulatory networks) to assess practical applicability
3. Conduct sensitivity analysis on kernel choice and regularization parameters to determine their impact on metric stability and accuracy