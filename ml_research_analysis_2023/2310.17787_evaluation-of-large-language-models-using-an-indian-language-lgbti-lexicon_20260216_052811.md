---
ver: rpa2
title: Evaluation of large language models using an Indian language LGBTI+ lexicon
arxiv_id: '2310.17787'
source_url: https://arxiv.org/abs/2310.17787
tags:
- words
- llms
- lgbti
- language
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates three large language models (LLMs) on their
  ability to handle LGBTI+ terminology in Indian languages. A methodology is proposed
  involving task formulation, prompt engineering, LLM usage, and manual evaluation.
---

# Evaluation of large language models using an Indian language LGBTI+ lexicon

## Quick Facts
- arXiv ID: 2310.17787
- Source URL: https://arxiv.org/abs/2310.17787
- Reference count: 5
- Key outcome: GPT-3 achieves 61-82% accuracy across tasks, but all models struggle with hate speech detection for LGBTI+ terms in Indian languages

## Executive Summary
This paper evaluates three large language models (GPT-Neo, GPT-J, GPT-3, and ChatGPT) on their ability to handle LGBTI+ terminology in Indian languages using a domain-specific lexicon of 38 words. The authors propose a methodology involving task formulation, prompt engineering, zero-shot LLM usage, and manual evaluation across three tasks: question-answering, machine translation, and hate speech detection. The evaluation reveals that while GPT-3 performs best with 61-82% accuracy, all models struggle significantly with hate speech detection and show limitations in translating LGBTI+ terms. The study highlights that translation-based benchmarks are insufficient for evaluating multilingual LLM capability in culturally nuanced domains.

## Method Summary
The methodology involves four steps: (1) task formulation (question-answering, machine translation, hate speech detection), (2) prompt engineering using three prompts per task with code-mixed English-Indian language format, (3) zero-shot LLM usage across GPT-Neo, GPT-J, GPT-3, and ChatGPT, and (4) manual evaluation by human annotators familiar with the lexicon. The evaluation uses a lexicon of 38 LGBTI+ words in Indian languages, with outputs judged as correct, partially correct, or appropriate for hate speech detection. Accuracy is reported as the highest value across all prompts for each task per model.

## Key Results
- GPT-3 achieves highest accuracy at 82% for question-answering, 73% for machine translation, and 61% for hate speech detection
- All models struggle with hate speech detection, achieving only 61% accuracy at best
- Machine translation outputs often fail to capture cultural nuances, suggesting translation alone is insufficient for evaluating multilingual capability
- ChatGPT blocks certain prompts containing LGBTI+ terminology, indicating content filtering issues

## Why This Works (Mechanism)

### Mechanism 1
Using a domain-specific lexicon reveals LLM behavior gaps that general benchmarks miss. General benchmarks like MMLU translate test data into target languages, but translation can distort nuanced meanings. A domain-specific lexicon directly probes the model's native understanding of culturally specific terms.

### Mechanism 2
Prompt engineering with multiple formulations improves LLM performance consistency. Using three prompts per task allows models to interpret the task from different angles, reducing the impact of any single poorly formulated prompt.

### Mechanism 3
Manual evaluation is necessary because automated metrics cannot capture nuanced cultural and contextual understanding. Human evaluators familiar with the lexicon can judge whether answers reflect correct understanding of LGBTI+ terms in their cultural context.

## Foundational Learning

- Concept: Domain-specific lexicon creation and curation
  - Why needed here: The evaluation relies on a carefully selected set of LGBTI+ terms in Indian languages to probe LLM behavior.
  - Quick check question: How would you ensure that a lexicon captures both the linguistic and cultural nuances of a specific domain?

- Concept: Prompt engineering for zero-shot learning
  - Why needed here: The evaluation uses zero-shot prompts to test how well LLMs understand tasks without examples.
  - Quick check question: What are the key differences between zero-shot, one-shot, and few-shot prompting, and when would each be most appropriate?

- Concept: Manual evaluation methodology for NLP tasks
  - Why needed here: The study uses manual evaluation to assess LLM outputs on culturally specific terminology.
  - Quick check question: What criteria would you use to train human evaluators for assessing domain-specific NLP tasks?

## Architecture Onboarding

- Component map:
  Lexicon repository -> Prompt generator -> LLM interface -> Evaluation engine

- Critical path:
  1. Load lexicon
  2. Generate prompts for each task
  3. Run prompts through LLMs
  4. Collect and store outputs
  5. Manual evaluation by domain experts
  6. Analyze and report accuracy metrics

- Design tradeoffs:
  - Using multiple prompts increases evaluation time but improves reliability
  - Manual evaluation ensures accuracy but is resource-intensive
  - Testing multiple models provides comprehensive comparison but increases computational cost

- Failure signatures:
  - Consistent failure across all prompts for specific terms indicates cultural context gap
  - High variance in outputs for the same prompt suggests model instability
  - Complete rejection of certain prompts (e.g., ChatGPT blocking) indicates content filtering issues

- First 3 experiments:
  1. Test each LLM with a single prompt for one task to establish baseline performance
  2. Compare outputs when using different prompts for the same task and term
  3. Analyze error patterns by categorizing incorrect outputs (e.g., cultural misunderstanding vs. translation error)

## Open Questions the Paper Calls Out

### Open Question 1
How can we develop a comprehensive LGBTI+ lexicon for Indian languages that includes context-dependent usage patterns and regional variations? The paper acknowledges the lexicon used is incomplete and notes the challenge of identifying and tagging words as "hateful, discriminatory, or homo-/transphobic" due to contextual layers.

### Open Question 2
What specific strategies can be implemented to improve LLM performance on hate speech detection for LGBTI+ terminology in multilingual contexts? The paper shows all models struggle with hate speech detection, achieving only 61% accuracy at best, and notes the need for domain-specific lexicon evaluation.

### Open Question 3
How can we accurately evaluate multilingual LLM performance beyond simple translation-based benchmarks for domain-specific vocabularies? The paper demonstrates limitations of using machine translation as a means to evaluate natural language understanding and notes that translated benchmark datasets may lead to inaccurate claims about LLM multilingual ability.

## Limitations

- The lexicon contains only 38 words, limiting the evaluation's comprehensiveness and generalizability
- Language coverage is restricted primarily to Hindi and Marathi, not representing the full diversity of Indian languages
- Manual evaluation introduces subjectivity without inter-rater reliability metrics or detailed annotation guidelines
- The study does not explore how results might vary with larger, more diverse lexicons or different cultural contexts

## Confidence

**High confidence**: The finding that GPT-3 outperforms smaller models (GPT-Neo, GPT-J) is well-supported by direct comparison and aligns with expected performance patterns for larger models. The observation that all models struggle with hate speech detection is also robust, given the manual evaluation methodology.

**Medium confidence**: The claim that translation-based benchmarks fail to capture cultural context is supported by the study's methodology but would benefit from direct comparison with translation-based evaluations to quantify the difference. The assertion that manual evaluation is necessary for domain-specific terminology is reasonable but not empirically validated against automated alternatives.

**Low confidence**: The broader claim that this methodology reveals fundamental gaps in LLM behavior beyond what general benchmarks capture is suggestive but not conclusively proven given the limited scope and scale of the evaluation.

## Next Checks

1. **Scale validation**: Replicate the evaluation with a lexicon expanded to 200+ LGBTI+ terms across 5+ Indian languages to test whether the observed performance patterns hold with increased scale and diversity.

2. **Benchmark comparison**: Conduct parallel evaluations using both the domain-specific lexicon approach and traditional translation-based benchmarks on the same models to quantify the differences in performance assessment.

3. **Inter-rater reliability**: Implement a second round of manual evaluation with different annotators and calculate inter-rater agreement scores to assess the subjectivity and consistency of the manual evaluation process.