---
ver: rpa2
title: 'Factored Verification: Detecting and Reducing Hallucination in Summaries of
  Academic Papers'
arxiv_id: '2310.10627'
source_url: https://arxiv.org/abs/2310.10627
tags:
- summary
- gpt-4
- claim
- hallucination
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Factored Verification, a method for detecting
  and reducing hallucinations in LLM-generated summaries of academic papers. The approach
  decomposes summaries into claims, evaluates each claim's correctness using the provided
  source material, and combines these into an overall correctness score.
---

# Factored Verification: Detecting and Reducing Hallucination in Summaries of Academic Papers

## Quick Facts
- arXiv ID: 2310.10627
- Source URL: https://arxiv.org/abs/2310.10627
- Reference count: 3
- Key outcome: Achieves 76.2% accuracy on HaluEval benchmark, reducing hallucinations by 5.50-23.25% through self-correction

## Executive Summary
This paper introduces Factored Verification, a method for detecting and reducing hallucinations in LLM-generated summaries of academic papers. The approach decomposes summaries into individual claims, evaluates each claim's correctness using provided source material, and combines these into an overall correctness score. Tested on the HaluEval benchmark, Factored Verification achieves 76.2% accuracy, setting a new state-of-the-art for hallucination detection in summarization tasks. Applied to summarizing academic papers, it finds hallucination rates of 0.62-1.55 hallucinations per summary across different models. Using Factored Verification to generate critiques and prompt self-correction reduces hallucinations by 5.50-23.25% across models, with ChatGPT showing the lowest initial hallucination rate (0.62) and GPT-4 achieving the lowest rate after correction (0.46).

## Method Summary
Factored Verification works by automatically decomposing a summary into key claims, assigning a model-generated probability to each claim given the relevant sources, and combining these into an overall correctness probability. The method uses chain-of-thought prompting with GPT-4 to evaluate claim correctness, then generates critiques for unsupported claims and uses these to guide summary revision. The approach is tested on both the HaluEval benchmark and a custom dataset of academic paper summaries generated from Elicit search queries.

## Key Results
- Achieves 76.2% accuracy on HaluEval benchmark, outperforming previous state-of-the-art (30.9%) and few-shot prompting (63.3%)
- Reduces hallucination rates by 5.50-23.25% across models (ChatGPT, GPT-3.5, GPT-4) through self-correction
- Finds hallucination rates of 0.62-1.55 hallucinations per summary across different models on academic paper summarization task

## Why This Works (Mechanism)

### Mechanism 1: Factored Verification decomposes complex verification into tractable sub-tasks
Breaking summary verification into individual claim checks makes hallucination detection more accurate than holistic verification. The approach splits a summary into discrete claims, evaluates each claim's correctness independently against source material, then combines results. This factorization allows focused evaluation of each claim's factual basis.

### Mechanism 2: Chain-of-thought verification provides more reliable correctness assessment than few-shot prompting
Using chain-of-thought reasoning for claim verification achieves higher accuracy than direct few-shot prompting. The method uses GPT-4 with chain-of-thought prompting to evaluate whether each claim is supported by the source material, looking up the probability of the final "Yes" token. This allows the model to reason through its verification process.

### Mechanism 3: Factored critiques enable targeted self-correction by focusing revision on specific unsupported claims
Providing models with sentence-wise critiques of unsupported claims enables more effective self-correction than general revision requests. The approach generates critiques for each unsupported claim, then uses these specific critiques to guide the model's revision of the summary. This targeted feedback focuses the model's attention on factual errors.

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: Enables models to reason through verification decisions step-by-step rather than jumping to conclusions
  - Quick check question: How does chain-of-thought prompting differ from standard prompting in terms of model behavior during verification?

- Concept: Factored reasoning decomposition
  - Why needed here: Breaking complex tasks into smaller verifiable components makes the overall task more tractable for models
  - Quick check question: Why might decomposing a summary into individual claims be more effective than evaluating the summary as a whole?

- Concept: Token probability lookup
  - Why needed here: Allows conversion of model reasoning into quantitative confidence scores for each claim
  - Quick check question: What information does looking up the probability of the final "Yes" token provide about the model's confidence?

## Architecture Onboarding

- Component map: Claim decomposition module -> Claim verification engine -> Probability aggregation system -> Critique generation component -> Revision engine -> Evaluation framework

- Critical path: 1. Generate summary from source material 2. Decompose summary into claims 3. Verify each claim using chain-of-thought prompting 4. Aggregate probabilities to determine hallucination presence 5. Generate critiques for unsupported claims 6. Use critiques to guide revision 7. Re-evaluate revised summary

- Design tradeoffs:
  - Accuracy vs. computational cost: More granular claim decomposition improves accuracy but increases processing time
  - Granularity vs. context: Finer claim decomposition may lose contextual understanding
  - Model choice: GPT-4 base provides token probabilities but costs more than ChatGPT

- Failure signatures:
  - High false positive rate: Indicates overly strict claim verification criteria
  - Low detection accuracy: Suggests decomposition process is losing important context
  - Ineffective self-correction: Critiques may be too subtle or poorly formatted for the model

- First 3 experiments:
  1. Compare chain-of-thought vs few-shot prompting accuracy on HaluEval dataset
  2. Test different claim decomposition strategies (sentence-level vs semantic claims)
  3. Evaluate different methods for combining individual claim probabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Factored Verification be extended to handle out-of-context fact checking beyond academic paper summarization?
- Basis in paper: Inferred from the discussion of Dhuliawala et al.'s (2023) Chain-of-Verification method which focuses on out-of-context fact checking
- Why unresolved: The paper primarily evaluates Factored Verification on academic paper summarization tasks. While it mentions potential extension to other domains, it does not explore or validate this possibility.
- What evidence would resolve it: Experiments applying Factored Verification to various out-of-context fact checking tasks, such as news article verification or social media claim checking, would demonstrate its generalizability and effectiveness in broader applications.

### Open Question 2
- Question: Can the Factored Verification method be combined with retrieval augmentation techniques to further reduce hallucination rates?
- Basis in paper: Inferred from the related work section mentioning retrieval augmentation as a strategy to mitigate hallucination
- Why unresolved: The paper focuses on using provided source material but does not explore integrating retrieval augmentation, which could potentially improve the model's ability to ground claims in external knowledge.
- What evidence would resolve it: Comparative studies evaluating hallucination rates using Factored Verification with and without retrieval augmentation across various summarization tasks would show the potential benefits of this combination.

### Open Question 3
- Question: How does the performance of Factored Verification scale with increasing document length and complexity?
- Basis in paper: Inferred from the observation that many claims were wrong in subtle ways requiring careful inspection of sources
- Why unresolved: The paper evaluates Factored Verification on academic paper abstracts but does not test its limits with longer, more complex documents that may contain more nuanced information and require deeper analysis.
- What evidence would resolve it: Experiments applying Factored Verification to progressively longer and more complex documents, measuring hallucination detection accuracy and computational costs, would reveal scalability limitations and potential optimizations.

## Limitations

- The independence assumption in probability aggregation may not hold for interdependent claims, potentially leading to overcounting or undercounting of hallucinations
- The method's effectiveness may vary significantly across different content types, as it assumes summaries contain discrete, verifiable claims
- The claimed accuracy metrics rely heavily on LLM-based evaluation, which may introduce its own biases and should be interpreted cautiously

## Confidence

**High Confidence**: The decomposition mechanism works as described and achieves measurable improvements in hallucination detection accuracy. The chain-of-thought verification consistently outperforms few-shot approaches.

**Medium Confidence**: The self-correction mechanism reduces hallucination rates by 5.50-23.25% across models, though the exact magnitude varies by model and prompt quality.

**Low Confidence**: The long-term effectiveness of self-correction - whether models learn to avoid previously criticized error patterns in subsequent generations without explicit correction prompts.

## Next Checks

1. **Cross-domain validation**: Test Factored Verification on non-academic summarization tasks (news articles, product reviews) to assess generalizability beyond the tested domain.

2. **Human evaluation comparison**: Conduct blind human evaluations comparing Factored Verification's hallucination detection against human judgments to validate the accuracy claims and identify potential systematic biases.

3. **Iterative correction study**: Test whether models that undergo Factored Verification-based correction show reduced hallucination rates in subsequent generations without explicit correction prompts.