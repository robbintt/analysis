---
ver: rpa2
title: A Computational Theory and Semi-Supervised Algorithm for Clustering
arxiv_id: '2306.06974'
source_url: https://arxiv.org/abs/2306.06974
tags:
- data
- clustering
- clusters
- algorithm
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a computational theory for clustering and a
  semi-supervised clustering algorithm. Clustering is defined as obtaining groupings
  of data such that each group contains no anomalies with respect to a chosen grouping
  principle and measure, with all other examples considered anomalies or unknown clusters.
---

# A Computational Theory and Semi-Supervised Algorithm for Clustering

## Quick Facts
- arXiv ID: 2306.06974
- Source URL: https://arxiv.org/abs/2306.06974
- Reference count: 13
- Primary result: Presents a parameter-free, semi-supervised clustering algorithm based on anomaly detection that outperforms k-means and DBSCAN on synthetic and real-world datasets.

## Executive Summary
This paper introduces a computational theory for clustering where clusters are defined as groups containing no anomalies relative to a chosen grouping principle. The proposed semi-supervised algorithm uses a small set of labeled examples as seeds to guide cluster formation, iteratively expanding clusters by ejecting anomalies and absorbing non-anomalous points. Built on Mohammad's Perception anomaly detection algorithm, the method achieves parameter-free operation by automatically adapting thresholds based on the Helmholtz principle. Empirical results demonstrate advantages over traditional unsupervised and semi-supervised clustering approaches on both synthetic and real-world datasets.

## Method Summary
The method defines clustering as obtaining groupings where each group contains no anomalies with respect to a chosen principle. It leverages Perception anomaly detection as its core, using labeled seeds to guide the semi-supervised process. The algorithm iteratively expands clusters by ejecting identified anomalies and testing whether these anomalies should be absorbed into other clusters. This continues until convergence or a maximum iteration count. The approach is parameter-free due to Perception's automatic threshold adaptation, and it handles a minority of mislabeled seeds through its anomaly ejection mechanism.

## Key Results
- Achieves parameter-free clustering through Perception anomaly detection
- Outperforms k-means and DBSCAN on synthetic and real-world datasets
- Demonstrates effectiveness of semi-supervised guidance with minimal labeled examples
- Provides a computational theory framing clustering as anomaly removal

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parameter-free clustering through Perception anomaly detection
- Mechanism: Perception computes anomaly scores using the Helmholtz principle—points are anomalies if their expectation of occurrence under uniform random noise is <1. This automatically adapts thresholds without requiring user-specified parameters.
- Core assumption: Median-based Euclidean distance representation is robust to outliers and captures cluster structure
- Evidence anchors: Abstract states it's parameter-free; clustering approached from anomaly removal perspective
- Break condition: Fails if data doesn't follow hyperspherical distributions or contains high-dimensional noise patterns the median-based representation cannot capture

### Mechanism 2
- Claim: Semi-supervised guidance via small labeled examples constrains clustering search space
- Mechanism: Users provide seed examples from known clusters. Algorithm expands these seeds by iteratively testing nearby anomalies against the cluster's median model; non-anomalous points are absorbed into the cluster.
- Core assumption: Small fraction of correctly labeled examples is sufficient to guide clustering
- Evidence anchors: Abstract mentions reliance on known relationships between examples; section discusses tolerance for mislabelled seeds
- Break condition: Fails if initial seeds are too few, too noisy, or poorly representative of cluster structure

### Mechanism 3
- Claim: Iterative anomaly ejection and cluster expansion converges to stable partition
- Mechanism: For each cluster, anomalies are ejected and re-tested; non-anomalous points are absorbed. Process repeats until no label changes occur or max iterations reached.
- Core assumption: Anomaly detection model remains valid as cluster grows and convergence is achievable
- Evidence anchors: Section describes iterative process over clusters in order of tightness; continues until no further changes
- Break condition: Fails if cluster structure changes drastically during expansion, causing oscillation or misclassification

## Foundational Learning

- Concept: Gestalt principles of perception
  - Why needed here: Clustering defined using Helmholtz principle (Gestalt law), so understanding human visual grouping informs computational theory
  - Quick check question: What is the Helmholtz principle and how does it relate to anomaly detection in clustering?

- Concept: Median-based robustness to outliers
  - Why needed here: Algorithm uses median (not mean) as cluster centroid to avoid distortion by extreme values
  - Quick check question: Why is the median more robust than the mean in presence of outliers, and how does this choice affect Euclidean distance representation?

- Concept: Semi-supervised learning with constraints
  - Why needed here: Algorithm requires small number of labeled examples to guide clustering
  - Quick check question: How does providing labeled seeds change problem from unsupervised to semi-supervised, and what is the trade-off compared to parameter tuning?

## Architecture Onboarding

- Component map: Data input → UMAP dimensionality reduction (optional) → Perception anomaly detector → Seed initialization → Iterative cluster expansion → Output labels + membership scores

- Critical path:
  1. Load and preprocess data (normalize, optional UMAP reduction)
  2. Initialize with labeled seeds, label rest as anomalies
  3. For each cluster (ordered by compactness): Fit Perception model, predict and eject anomalies, retrain on cleaned cluster, test all anomalies for inclusion
  4. Repeat until convergence or max iterations
  5. Final refit and score computation

- Design tradeoffs:
  - Parameter-free vs. need for labeled seeds: Removes user burden of tuning k/epsilon but requires domain knowledge for seeds
  - Median vs. mean: More robust to outliers but may be less efficient in very high dimensions
  - Iterative expansion vs. one-shot clustering: More accurate but potentially slower on large datasets

- Failure signatures:
  - Slow convergence or oscillation: Likely due to poor initial seeds or highly overlapping clusters
  - Many points labeled as anomalies: Could indicate overly strict anomaly threshold or insufficient seed coverage
  - Merging of distinct clusters: Suggests seeds were too few or too close in feature space

- First 3 experiments:
  1. Synthetic 1D Gaussian clusters with added outliers: Verify algorithm recovers main clusters and flags outliers correctly
  2. 2D overlapping Gaussian clusters: Test ability to separate close clusters and handle density variation
  3. MNIST digits subset with UMAP reduction: Confirm scalability and effectiveness on real high-dimensional data with few labeled seeds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed clustering algorithm perform compared to other semi-supervised clustering methods on real-world datasets beyond MNIST and 20 Newsgroups?
- Basis in paper: [inferred] Paper only evaluates on MNIST and 20 Newsgroups datasets
- Why unresolved: Paper does not provide comprehensive evaluation across diverse real-world datasets
- What evidence would resolve it: Empirical results on diverse real-world datasets would provide evidence of generalizability and performance

### Open Question 2
- Question: How does the proposed clustering algorithm handle high-dimensional data with complex distributions?
- Basis in paper: [inferred] Paper focuses on low-dimensional data and doesn't address high-dimensional challenges
- Why unresolved: Paper doesn't provide detailed analysis of performance on high-dimensional data with complex distributions
- What evidence would resolve it: Empirical results on high-dimensional datasets with complex distributions would provide evidence of effectiveness

### Open Question 3
- Question: How does the proposed clustering algorithm scale with increasing dataset size and dimensionality?
- Basis in paper: [inferred] Paper doesn't provide comprehensive analysis of scalability
- Why unresolved: Paper doesn't present empirical results demonstrating scalability on large datasets with high dimensionality
- What evidence would resolve it: Empirical results on large-scale datasets with high dimensionality would provide evidence of scalability and efficiency

## Limitations
- Performance depends critically on quality and quantity of labeled seeds
- No comprehensive evaluation on diverse real-world datasets beyond MNIST and 20 Newsgroups
- Scalability and efficiency on very large, high-dimensional datasets not thoroughly analyzed

## Confidence

### Claim Confidence Labels
- Mechanism 1 (Parameter-free through Perception): Low - Perception algorithm referenced but not fully specified
- Mechanism 2 (Semi-supervised guidance): Medium - Conceptual approach clear but effectiveness depends on seed quality
- Mechanism 3 (Iterative convergence): Medium - Iterative process described but convergence guarantees not proven

### Next Validation Checks
1. Implement and validate Perception anomaly detection: Create reference implementation and test on synthetic data with known anomalies to verify parameter-free threshold adaptation
2. Seed sensitivity analysis: Systematically vary number and quality of labeled seeds on controlled datasets to quantify robustness to seed noise and scarcity
3. Comparison benchmark: Replicate results on cited real-world datasets (MNIST, synthetic mixtures) using proposed method and established baselines (k-means, DBSCAN) with identical evaluation metrics