---
ver: rpa2
title: Adapting Language Models to Compress Contexts
arxiv_id: '2305.14788'
source_url: https://arxiv.org/abs/2305.14788
tags:
- summary
- vectors
- tokens
- language
- autocompressor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose to adapt pre-trained language models into AutoCompressors,
  which compress long contexts into compact summary vectors. These summary vectors
  are used as soft prompts to improve language modeling over long documents.
---

# Adapting Language Models to Compress Contexts

## Quick Facts
- arXiv ID: 2305.14788
- Source URL: https://arxiv.org/abs/2305.14788
- Reference count: 23
- Key outcome: Adapt pre-trained LMs into AutoCompressors that compress long contexts into summary vectors, improving perplexity over long documents and reducing inference costs.

## Executive Summary
AutoCompressors adapt pre-trained language models to compress long contexts into compact summary vectors that serve as soft prompts for subsequent predictions. By training on sequences up to 30,720 tokens with summary accumulation, the model learns to retain essential information while dramatically reducing computational requirements. The approach shows consistent perplexity improvements over long documents, outperforms few-shot learning in in-context tasks while reducing inference costs, and enables efficient retrieval-augmented language modeling through pre-computed summary vectors.

## Method Summary
AutoCompressors use summary accumulation by concatenating summary vectors from all previous segments and prepending them to the current segment input. The model is fine-tuned with an unsupervised objective minimizing cross-entropy loss over entire documents, with summary vectors conditioning predictions. Training uses randomized segment lengths (1024-2048 tokens) and stop gradients after 2 compression steps to manage memory while preserving training dynamics. The approach uses 50 summary tokens by default and evaluates on held-out documents from diverse Pile domains.

## Key Results
- Summary vectors improve perplexity over long documents up to 6144 tokens
- AutoCompressors outperform few-shot learning in in-context tasks while reducing inference costs
- Pre-computed summary vectors enable efficient retrieval-augmented language modeling and passage re-ranking

## Why This Works (Mechanism)

### Mechanism 1
Summary vectors serve as compressed representations that retain high-level semantic information while drastically reducing token count. AutoCompressors learn to transform long document segments into fixed-length summary vectors that can be used as soft prompts for subsequent segments. Core assumption: The summary vectors capture essential information needed for language modeling tasks. Evidence: [abstract] "Summary vectors are short soft prompts...that are obtained from the output states of a language model" and [section 3.1] "We expect that this mechanism has a high capacity for passing information to subsequent segments". Break condition: If summary vectors fail to retain critical information needed for accurate predictions in subsequent segments.

### Mechanism 2
Summary accumulation improves long-range information retention by creating direct pathways between all previous segments. Concatenating summary vectors from all previous segments provides more comprehensive context than only using the immediately preceding segment. Core assumption: Information from distant segments remains useful for current predictions. Evidence: [section 3.1] "We propose summary accumulation, which allows for a direct information pathway between each segment and all segments preceding it" and [section 4.1] "we find that the AutoCompressor benefits from long contexts of up to 6144 tokens and consistently outperforms the RMT model". Break condition: If information becomes too stale or irrelevant over many segments.

### Mechanism 3
Randomized segmenting during training improves generalization to different context lengths. Training with variable segment lengths teaches the model to compress effectively regardless of input size. Core assumption: The model learns compression strategies that are robust to different context lengths. Evidence: [section 3.2] "We segment sequences randomly during training, subject to the condition that each segment fits into the model's context window" and [section 4.3] "we show in Appendix A that randomized segmenting improves performance under evaluation with fixed-length segments". Break condition: If the model overfits to specific segment lengths seen during training.

## Foundational Learning

- **Soft prompts and parameter-efficient fine-tuning**: AutoCompressors use summary vectors as soft prompts to condition subsequent predictions. Quick check: What distinguishes soft prompts from regular token embeddings in terms of learning dynamics?

- **Transformer attention mechanisms**: Understanding why standard Transformers struggle with long contexts is crucial for appreciating AutoCompressors' efficiency. Quick check: How does attention complexity scale with sequence length in standard Transformers?

- **Language modeling objectives**: The training objective directly shapes what information gets preserved in summary vectors. Quick check: What does minimizing cross-entropy loss encourage the model to learn about token prediction?

## Architecture Onboarding

- **Component map**: Input processing -> Compression mechanism -> Accumulation system -> Language modeling head -> Training loop

- **Critical path**: 1. Segment input document into chunks fitting within context window 2. Process each segment through AutoCompressor to generate summary vectors 3. Accumulate summary vectors from all previous segments 4. Concatenate accumulated summaries with current segment input 5. Make predictions and compute loss 6. Backpropagate with stop-gradients after 2 steps

- **Design tradeoffs**: Number of summary tokens (κ): More tokens can capture more information but increase computational cost; Segment length: Longer segments provide more context but reduce compression benefits; Stop-gradient timing: Earlier stopping saves memory but may lose important gradients; Random vs fixed segmenting: Random improves generalization but may complicate optimization

- **Failure signatures**: Perplexity increases over long contexts despite compression; Summary vectors become semantically meaningless (e.g., random noise patterns); Performance degrades when evaluating on out-of-domain data; Memory overflow during training despite stop-gradients

- **First 3 experiments**: 1. Baseline comparison: Fine-tune standard OPT-2.7B on 8192-token sequences without compression 2. Single-segment compression: Train AutoCompressor on fixed 2048-token segments, evaluate on same length 3. Multi-segment evaluation: Test summary accumulation by compressing 4 segments of 2048 tokens each, evaluating on final segment

## Open Questions the Paper Calls Out

- **How does the quality of summary vectors degrade with increasing compression depth and number of segments?**: The paper discusses training AutoCompressors on sequences up to 30,720 tokens with 20 compression steps, but does not analyze performance degradation with more steps or segments. What evidence would resolve it: Systematic evaluation of AutoCompressor performance with varying numbers of compression steps (e.g., 5, 10, 50, 100) and segment counts, measuring perplexity and information retention at each level.

- **What is the optimal balance between summary vector dimensionality and compression ratio for different types of documents or tasks?**: The paper experiments with 20, 50, 70, and 100 summary tokens but finds κ=50 performs best overall, suggesting the relationship between dimensionality and performance is non-monotonic and task-dependent. What evidence would resolve it: Comprehensive analysis varying summary vector dimensions across multiple orders of magnitude (e.g., 10, 50, 100, 500, 1000 tokens) on diverse document types and tasks, measuring both compression efficiency and downstream task performance.

- **Can AutoCompressors be effectively combined with other long-range transformer architectures like sparse attention or retrieval-based methods?**: The paper compares AutoCompressors against RMT and full attention baselines but does not explore hybrid architectures that combine AutoCompressor summary vectors with other long-range mechanisms. What evidence would resolve it: Experimental results showing the performance of hybrid architectures that combine AutoCompressor summary vectors with sparse attention patterns, retrieval mechanisms, or other long-range transformer variants on both language modeling and downstream tasks.

## Limitations

- Evaluation relies heavily on in-distribution data from the Pile dataset with limited testing on truly out-of-domain data or zero-shot generalization scenarios.
- Compression efficiency claims are primarily measured against standard fine-tuned LMs rather than alternative long-context methods, making absolute performance improvements difficult to assess.
- Stop-gradient mechanism may limit the model's ability to optimize summary vectors for maximum information retention.

## Confidence

- **High Confidence**: The core claim that summary vectors can improve perplexity over long documents is well-supported by experimental results, particularly in language modeling and retrieval-augmented tasks.
- **Medium Confidence**: In-context learning improvements over few-shot learning are promising but based on limited task diversity, and compression efficiency gains are demonstrated but not benchmarked against alternatives.
- **Low Confidence**: Generalization claims to out-of-domain data and optimal configuration choices lack sufficient empirical support and could vary significantly based on data characteristics.

## Next Checks

1. **Cross-dataset generalization test**: Evaluate AutoCompressors on truly out-of-domain datasets (e.g., biomedical literature, legal documents) to verify generalization claims and identify domain-specific limitations in the compression mechanism.

2. **Compression efficiency benchmark**: Compare AutoCompressor's perplexity and inference speed against alternative long-context methods (context folding, flash attention, infinite length) on the same datasets to establish relative performance and identify scenarios where AutoCompressors provide the best trade-offs.

3. **Hyperparameter sensitivity analysis**: Systematically vary summary token count (κ), segment lengths, and stop-gradient timing to identify optimal configurations for different document types and task requirements, providing practical guidance for deployment scenarios.