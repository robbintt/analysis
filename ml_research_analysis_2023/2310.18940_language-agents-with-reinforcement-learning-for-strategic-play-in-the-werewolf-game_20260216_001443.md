---
ver: rpa2
title: Language Agents with Reinforcement Learning for Strategic Play in the Werewolf
  Game
arxiv_id: '2310.18940'
source_url: https://arxiv.org/abs/2310.18940
tags:
- player
- werewolf
- agent
- players
- werewolves
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework that combines large language
  models (LLMs) with reinforcement learning (RL) to develop strategic language agents
  for the Werewolf game. The proposed approach uses an LLM for deductive reasoning
  and generating diverse action candidates, followed by an RL policy to select the
  final action from the candidates.
---

# Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game

## Quick Facts
- arXiv ID: 2310.18940
- Source URL: https://arxiv.org/abs/2310.18940
- Reference count: 26
- Key outcome: RL-enhanced LLM agents achieve human-level performance in Werewolf through strategic action selection

## Executive Summary
This paper introduces a novel framework that combines large language models (LLMs) with reinforcement learning (RL) to develop strategic language agents for the Werewolf game. The approach uses an LLM for deductive reasoning and generating diverse action candidates, followed by an RL policy to select the final action from the candidates. Extensive experiments demonstrate that the agent overcomes the intrinsic bias in language actions and achieves the highest win rate against other LLM-based agents. Human-agent experiments further show that the agent achieves human-level performance and demonstrates strong strategic play.

## Method Summary
The framework combines LLMs with RL to create strategic language agents for Werewolf. The process involves three main steps: 1) Using an LLM to perform deductive reasoning on game state and generate diverse action candidates, 2) Training an RL policy through population-based training to select the final action from these candidates, and 3) Applying this RL policy to improve the strategic decision-making of different LLMs through zero-shot transfer.

## Key Results
- RL-enhanced agents achieve the highest win rate against other LLM-based agents in Werewolf
- Human-agent experiments demonstrate human-level performance with strong strategic play
- The learned RL policy exhibits zero-shot transfer ability, improving performance of unseen LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining LLMs with RL overcomes the intrinsic bias in language actions inherited from model training data.
- Mechanism: LLMs generate diverse action candidates through strategic reasoning, then an RL policy selects the final action from these candidates, optimizing decision-making beyond the LLM's inherent biases.
- Core assumption: The RL policy can learn to select actions that are strategically superior to the LLM's default preferences.
- Evidence anchors:
  - [abstract] "Our agent uses an LLM to perform deductive reasoning and generate a diverse set of action candidates. Then an RL policy trained to optimize the decision-making ability chooses an action from the candidates to play in the game."
  - [section 4.3] "To optimize decision-making and achieve strategic play, we use reinforcement learning to train a policy that selects the final action from the candidates."
  - [corpus] Weak evidence - only one related paper directly addresses strategic language agents in Werewolf, and it uses a different approach (iterative latent space policy optimization).

### Mechanism 2
- Claim: Population-based training with an agent pool improves robustness against different opponent styles.
- Mechanism: The RL policy is trained by playing against itself, its past versions, and a fixed pool of LLM-based agents with different styles, leading to more robust and less exploitable behavior.
- Core assumption: Exposure to diverse opponent styles during training improves the agent's ability to handle various gameplay scenarios.
- Evidence anchors:
  - [section 4.3] "To make the final policy more unexploitable, we generate a pool of fixed LLM-based agents with different styles and use population-based training to further improve the RL policy by playing against itself, its past versions, and the agents in the pool."
  - [section 5.1] "We generated three common styles of Werewolf including a quiet follower, an active contributor, and an aggressive accuser."
  - [corpus] Moderate evidence - the related paper on learning strategic language agents uses a different approach (iterative latent space policy optimization) but acknowledges the importance of diverse strategies.

### Mechanism 3
- Claim: Zero-shot transfer of the learned RL policy to unseen LLMs improves their performance in decision-making.
- Mechanism: The RL policy, which takes natural language state and actions as input, can be directly combined with any other LLM to improve its strategic decision-making ability without further training.
- Core assumption: The natural language interface between the RL policy and LLMs is general enough to work across different models.
- Evidence anchors:
  - [section 5.3] "Since our RL policy takes natural language state and actions as input and is decoupled from the LLM used in previous steps, it can be directly combined with any other LLMs and improve the performance of the LLM-based agent."
  - [section 5.3] "We evaluate this zero-shot transfer ability of our RL policy trained with gpt-3.5-turbo by applying it to unseen LLMs including GPT-4, LLaMA-7B, and ChatGLM-6B."
  - [corpus] Weak evidence - no related papers discuss zero-shot transfer of RL policies to different LLMs in the context of strategic language games.

## Foundational Learning

- Concept: Deductive reasoning in the presence of unreliable information
  - Why needed here: Players must distinguish between truths and lies and deduce hidden roles from unreliable information in the Werewolf game.
  - Quick check question: How does the agent categorize information into facts, potential truths, and potential deceptions based on player reliability?

- Concept: Strategic diversity in action selection
  - Why needed here: Players must adopt a diverse range of actions to avoid being exploited by adversarial opponents in the Werewolf game.
  - Quick check question: What are the two methods used to generate N action candidates with strategic diversity, and when is each method used?

- Concept: Reinforcement learning in mixed cooperative-competitive environments
  - Why needed here: The Werewolf game involves both cooperation (within teams) and competition (between teams), requiring RL to optimize decision-making.
  - Quick check question: How does the population-based training approach generalize self-play to handle the mixed cooperative-competitive nature of the Werewolf game?

## Architecture Onboarding

- Component map:
  Deductive Reasoning -> Diverse Action Generation -> RL Policy -> Action Selection

- Critical path: Game observation → Deductive Reasoning → Diverse Action Generation → RL Policy → Action selection → Game outcome → RL policy update

- Design tradeoffs:
  - Using LLMs for deductive reasoning provides strong reasoning capabilities but may introduce biases; RL policy helps mitigate these biases.
  - Generating diverse action candidates improves strategic play but increases computational cost; iterative generation is used for complex actions to balance quality and efficiency.
  - Population-based training improves robustness but requires maintaining an agent pool and training against multiple opponents; this is balanced by the improved performance against diverse strategies.

- Failure signatures:
  - If the agent consistently loses to specific opponent styles, the agent pool may be incomplete or the RL policy may not be learning effectively.
  - If the agent's actions become predictable over multiple games, the diverse action generation may not be producing sufficiently varied candidates.
  - If the agent's reasoning is consistently flawed, the deductive reasoning component may not be effectively organizing or interpreting the game information.

- First 3 experiments:
  1. Evaluate the agent against a fixed set of opponents without population-based training to establish a baseline performance.
  2. Compare the performance of the agent with and without diverse action generation to quantify the impact of strategic diversity.
  3. Test the zero-shot transfer capability by applying the learned RL policy to an unseen LLM and measuring the performance improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance differences between our agent and the atomic agent scale with increasing game complexity (more players, more roles)?
- Basis in paper: [explicit] The paper states "Although both our agent and the atomic agent combine RL with LLMs, our agent achieves much better performance" and explains that the atomic agent's predefined actions are too general.
- Why unresolved: The paper only evaluates on a 7-player version with 5 roles. Scaling to larger games with more complex dynamics would test whether the LLM-based action generation approach maintains its advantage over predefined action sets.
- What evidence would resolve it: Experiments showing win rates of both agents across varying player counts (e.g., 7, 10, 14, 21 players) and role configurations (e.g., adding more special roles like Jailer, Gunner).

### Open Question 2
- Question: Does the learned RL policy exhibit any emergent biases toward specific players or roles during gameplay?
- Basis in paper: [inferred] The paper discusses how vanilla LLM-based agents show clear preferences in action selection (e.g., choosing the Doctor over the Seer 9 out of 10 times). The RL policy is trained to optimize decision-making, which could potentially introduce new biases.
- Why unresolved: While the paper shows the RL policy improves performance, it doesn't analyze whether the learned policy itself develops systematic biases in its action selection patterns.
- What evidence would resolve it: Statistical analysis of action distributions across different game states, showing whether certain players or roles are consistently targeted or protected by the RL policy beyond what's optimal for gameplay.

### Open Question 3
- Question: What is the impact of the RL policy's zero-shot transfer ability on the diversity of strategies exhibited by different LLM models?
- Basis in paper: [explicit] The paper demonstrates that "the RL policy trained with one LLM can be directly deployed to other LLMs to improve their performance in decision-making, showing zero-shot transfer capability."
- Why unresolved: The paper shows improved performance but doesn't investigate whether the same RL policy causes different LLM models to converge toward similar strategies or if model-specific characteristics still emerge.
- What evidence would resolve it: Comparative analysis of strategy distributions (e.g., frequency of bluffing, sacrificing, concealment) across different LLM models both with and without the RL policy applied.

## Limitations

- The framework's effectiveness depends heavily on the quality of the LLM's deductive reasoning and the diversity of generated actions
- Generalizability to other strategic language games remains untested
- Zero-shot transfer capability has only been evaluated on a limited set of LLMs (GPT-4, LLaMA-7B, and ChatGLM-6B)

## Confidence

**High confidence** in the RL policy's ability to select superior actions from the LLM-generated candidates, as evidenced by improved win rates against other agents and human players.

**Medium confidence** in the population-based training approach for improving robustness, given the demonstration of better performance against diverse opponent styles, but without extensive ablation studies isolating the impact of each component.

**Low confidence** in the scalability of the approach to games with more complex strategic interactions or larger action spaces, as the Werewolf game has a relatively constrained set of actions and roles.

## Next Checks

1. **Ablation study on action generation diversity**: Systematically vary the number and diversity of action candidates to quantify their impact on the RL policy's performance and identify the optimal balance between diversity and computational efficiency.

2. **Generalization to other strategic language games**: Evaluate the framework's performance on different strategic language games (e.g., Mafia, Diplomacy) to assess its generalizability and identify any game-specific adaptations needed.

3. **Scaling analysis**: Test the framework on Werewolf variants with more players, roles, and actions to understand its scalability limits and identify potential bottlenecks or performance degradation.