---
ver: rpa2
title: Generative Modelling of Stochastic Actions with Arbitrary Constraints in Reinforcement
  Learning
arxiv_id: '2311.15341'
source_url: https://arxiv.org/abs/2311.15341
tags:
- action
- policy
- actions
- flow
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the problem of reinforcement learning in large,
  unordered, and categorical action spaces with state-dependent constraints. The authors
  propose a novel approach that combines a conditional normalizing flow-based policy
  network with an invalid action rejection mechanism.
---

# Generative Modelling of Stochastic Actions with Arbitrary Constraints in Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2311.15341
- **Source URL:** https://arxiv.org/abs/2311.15341
- **Reference count:** 40
- **Primary result:** Proposed method outperforms prior approaches in learning efficiency and constraint satisfaction in large, unordered, categorical action spaces with state-dependent constraints.

## Executive Summary
This paper addresses the challenge of reinforcement learning in large, unordered, and categorical action spaces with state-dependent constraints. The authors propose a novel approach combining a conditional normalizing flow-based policy network (Argmax Flow) with an invalid action rejection mechanism. This method compactly represents stochastic policies by outputting only sampled actions and their log probabilities, while ensuring constraint satisfaction through a modified policy gradient. The approach is evaluated across multiple environments, demonstrating superior performance compared to prior methods in terms of learning efficiency and constraint satisfaction, while also learning desirable stochastic optimal policies.

## Method Summary
The proposed method uses a conditional normalizing flow (Argmax Flow) to compactly represent stochastic policies, outputting only a sampled action and its log probability rather than the full action distribution. An invalid action rejection (IAR) mechanism samples actions from the flow policy and rejects invalid ones using a constraint oracle. A modified policy gradient accounts for this rejection process. The method employs a sandwich estimator combining Evidence Lower Bound (ELBO) and Conditional Upper Bound (CUBO) to reduce bias in log probability estimation. The approach is implemented within an Advantage Actor-Critic (A2C) framework.

## Key Results
- The proposed method outperforms prior approaches in learning efficiency and constraint satisfaction across multiple environments including CartPole, Acrobot, Pistonball, and a custom Emergency Resource Allocation simulator
- The approach successfully learns stochastic optimal policies, which is particularly valuable for resource allocation problems
- The method demonstrates effective handling of large, unordered, categorical action spaces with state-dependent constraints

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The flow policy network compactly represents stochastic policies by outputting only a sampled action and its log probability, avoiding explicit representation of the full action distribution.
- **Mechanism:** Uses a conditional normalizing flow with Argmax Flow to map a base Gaussian distribution through invertible transformations and an argmax operation, producing a discrete action sample. The ELBO provides a lower bound on the log probability of the sampled action.
- **Core assumption:** The policy distribution can be well-approximated by transforming a simple base distribution through a series of invertible mappings, and the ELBO is a sufficient estimator for policy gradient updates.
- **Evidence anchors:**
  - [abstract]: "applying a (state) conditional normalizing flow to compactly represent the stochastic policy — the compactness arises due to the network only producing one sampled action and the corresponding log probability"
  - [section 3.1]: "our first contribution is a compact policy representation using Argmax Flow [16], which we refer to as the Flow Policy. Argmax Flow is a state-of-the-art discrete normalizing flow model and it has shown great capability of learning categorical data such as in sentence generation. In our context, Flow Policy will output the action (a sample of the flow policy) and its probability, instead of explicitly outputting the entire distribution and sampling from it as in prior work."
  - [corpus]: Weak evidence for this specific claim; corpus mentions related work but not this exact mechanism.
- **Break condition:** If the base distribution cannot adequately represent the policy, or if the ELBO becomes too biased (large gap from true log probability), the policy gradient will be incorrect and learning will fail.

### Mechanism 2
- **Claim:** The sandwich estimator (weighted average of ELBO and CUBO) reduces bias in log probability estimation, improving policy gradient convergence.
- **Mechanism:** ELBO provides a lower bound on log π(a|s), CUBO provides an upper bound. Combining them with weight α yields a low-bias estimate of log π(a|s), which is used in the policy gradient.
- **Core assumption:** The true log probability lies between the ELBO and CUBO bounds, and a convex combination reduces estimation bias sufficiently for stable learning.
- **Evidence anchors:**
  - [section 3.2]: "we propose a sandwich estimator which combines ELBO and CUBO to obtain a low-bias estimation of log π(a|s), thus improving the convergence of policy gradient learning."
  - [section 3.2]: "We then use a weighted average of the upper and lower bounds as a low-bias estimate oflog π(a|s), denoted by dlog pθ,ψ = αˆlπ + (1 − α)ˆluπ where α is a hyperparameter. We call this the sandwich estimator of log probability. We observed in our experiments that an adaptive α(ˆlπ, ˆluπ) as a function of the two bounds provides better results than a static α = 1/2"
  - [corpus]: Weak evidence; related work mentions normalizing flows in RL but not this specific sandwich estimator.
- **Break condition:** If α is poorly chosen (e.g., too close to 0 or 1), or if the bounds are too loose, the estimator will remain biased and policy gradients will be incorrect.

### Mechanism 3
- **Claim:** Invalid action rejection (IAR) ensures constraint satisfaction by sampling actions from the flow policy and rejecting invalid ones until a valid action is found, with a modified policy gradient accounting for the rejection.
- **Mechanism:** Sample a batch of actions from the flow policy, query an oracle to reject invalid actions, and uniformly sample from the remaining valid actions. The policy gradient is modified to account for the renormalized distribution over valid actions (Theorem 1).
- **Core assumption:** The constraint oracle can efficiently determine validity of sampled actions, and the modified policy gradient correctly accounts for the rejection process without introducing bias.
- **Evidence anchors:**
  - [abstract]: "employing an invalid action rejection method (via a valid action oracle) to update the base policy. The action rejection is enabled by a modified policy gradient that we derive."
  - [section 3.2]: "IAR-A2C operates by sampling a set of actions from the current flow-based policy and then leveraging the constraint oracle to reject all invalid actions, enabling the agent to explore safely with only valid actions. We then derive the policy gradient estimator for this algorithm."
  - [section 3.2]: Theorem 1 and Lemma 1 provide the mathematical derivation of the modified policy gradient.
  - [corpus]: Weak evidence; related work mentions constrained RL but not this specific invalid action rejection approach.
- **Break condition:** If the fraction of valid actions is extremely small, sampling will be inefficient and learning will slow down significantly. If the oracle is noisy or slow, the rejection process will be unreliable.

## Foundational Learning

- **Concept:** Normalizing flows and invertible transformations
  - Why needed here: The flow policy network relies on transforming a simple base distribution into a complex action distribution through invertible mappings. Understanding how normalizing flows work is essential to grasp the policy representation.
  - Quick check question: Can you explain how the change of variables formula allows us to compute the density of a transformed variable using the density of the base variable and the determinant of the Jacobian?

- **Concept:** Evidence Lower Bound (ELBO) and variational inference
  - Why needed here: The ELBO provides a lower bound on the log probability of actions sampled from the flow policy, which is used in the policy gradient. Understanding ELBO is crucial for grasping how the policy is trained.
  - Quick check question: What is the relationship between the ELBO and the true log probability, and how does optimizing the ELBO relate to minimizing the KL divergence between the approximate and true posteriors?

- **Concept:** Policy gradients and advantage estimation
  - Why needed here: The modified policy gradient (Theorem 1) is used to update the flow policy parameters while accounting for invalid action rejection. Understanding standard policy gradients and advantage estimation is necessary to follow the derivation and implementation.
  - Quick check question: How does the advantage function reduce the variance of policy gradient estimates compared to using raw returns?

## Architecture Onboarding

- **Component map:** State encoder -> base distribution -> invertible transformations -> argmax -> action sample + log probability (via sandwich estimator)
- **Critical path:** State -> Flow Policy -> Action Sample + Log Prob -> Execute Action -> Reward + Next State -> Update Critic -> Update Flow Policy (via modified policy gradient with invalid action rejection)
- **Design tradeoffs:**
  - Compact policy representation vs. expressiveness: Flow policy avoids representing full distribution but may be limited by base distribution choice
  - Sandwich estimator complexity vs. bias reduction: Adds computation but improves gradient quality
  - Invalid action rejection vs. sampling efficiency: Ensures constraints but can be slow if few valid actions exist
- **Failure signatures:**
  - Flow policy not learning: Check if ELBO is being optimized, if base distribution is appropriate, if invertible transformations are working
  - Invalid actions being executed: Check constraint oracle, IAR implementation, modified policy gradient
  - High variance in gradients: Check advantage estimation, ELBO bias, batch size for sampling
- **First 3 experiments:**
  1. Train flow policy on a simple environment (e.g., CartPole) without constraints to verify basic learning works
  2. Add a simple constraint oracle (e.g., action must be in a specific range) and verify invalid action rejection works
  3. Scale to a larger environment (e.g., Pistonball) with complex constraints to test scalability and efficiency

## Open Questions the Paper Calls Out
- **Question:** How does the proposed method perform in environments where the fraction of valid actions out of all actions is extremely small?
  - **Basis in paper:** [explicit] The authors mention this as a limitation in the conclusion, stating that their sampling-based rejection will need a lot of samples to be effective in such scenarios.
  - **Why unresolved:** The paper does not provide experimental results or analysis for environments with very small fractions of valid actions.
  - **What evidence would resolve it:** Experiments comparing the performance of the proposed method to other approaches in environments with varying fractions of valid actions, particularly those with very small fractions.

- **Question:** Can the proposed method be extended to handle constraints that are not state-dependent but rather depend on the history of actions taken by the agent?
  - **Basis in paper:** [inferred] The current method only considers state-dependent constraints and does not account for action history. The authors mention that handling such constraints is an interesting future research direction.
  - **Why unresolved:** The paper does not explore the extension of the method to handle action history-dependent constraints.
  - **What evidence would resolve it:** A modified version of the proposed method that incorporates action history into the constraint checking process, along with experiments demonstrating its effectiveness in environments with action history-dependent constraints.

- **Question:** How does the performance of the proposed method compare to other approaches in environments with continuous action spaces?
  - **Basis in paper:** [explicit] The authors state that their method is designed for discrete action spaces and does not directly apply to continuous action spaces.
  - **Why unresolved:** The paper does not provide any experimental results or analysis for continuous action space environments.
  - **What evidence would resolve it:** Experiments comparing the performance of the proposed method (adapted for continuous action spaces) to other approaches designed for continuous action spaces, such as DDPG or SAC, in environments with continuous action spaces.

## Limitations
- The method's efficiency degrades significantly when the fraction of valid actions is extremely small, requiring many samples for effective learning
- The approach is designed for discrete action spaces and does not directly apply to continuous action spaces
- The method assumes a perfect and efficient constraint oracle, which may not be realistic in practical applications

## Confidence
- **High confidence:** The mathematical derivation of the modified policy gradient (Theorem 1) and the basic framework of using normalizing flows for policy representation
- **Medium confidence:** The effectiveness of the sandwich estimator for reducing bias, as empirical validation is limited to specific environments
- **Low confidence:** Scalability to extremely large action spaces and the robustness of the approach to imperfect constraint oracles

## Next Checks
1. **Base Distribution Sensitivity:** Systematically evaluate how different base distributions (Gaussian, uniform, etc.) affect the flow policy's learning efficiency and final performance across multiple environments.

2. **Constraint Oracle Robustness:** Introduce noise or delays in the constraint oracle and measure the impact on learning stability and constraint satisfaction rates.

3. **Scalability Test:** Evaluate the method on a synthetic environment with exponentially growing action spaces (e.g., 2^n actions) to identify performance degradation points and determine practical scalability limits.