---
ver: rpa2
title: A Novel Loss Function Utilizing Wasserstein Distance to Reduce Subject-Dependent
  Noise for Generalizable Models in Affective Computing
arxiv_id: '2308.10869'
source_url: https://arxiv.org/abs/2308.10869
tags:
- affective
- distance
- function
- data
- physiological
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of subject-dependent noise in
  physiological data that limits the generalizability of affective computing models.
  The proposed method introduces a novel loss function based on Wasserstein Distance
  from Optimal Transport Theory.
---

# A Novel Loss Function Utilizing Wasserstein Distance to Reduce Subject-Dependent Noise for Generalizable Models in Affective Computing

## Quick Facts
- arXiv ID: 2308.10869
- Source URL: https://arxiv.org/abs/2308.10869
- Reference count: 25
- Primary result: Proposed Wasserstein-based loss function improved classification accuracy by 4.21% and increased class separation distances by 14.75% (minimum) and 17.75% (centroid) compared to baseline

## Executive Summary
This paper addresses the challenge of subject-dependent noise in physiological data that limits the generalizability of affective computing models. The authors propose a novel loss function based on Wasserstein Distance from Optimal Transport Theory that scales the importance of subject-specific data during training. By assigning higher weights to patterns common across subjects and lower weights to subject-dependent noise, the method aims to create more generalizable models. The approach was implemented using an autoencoder with a classifier in the latent space and evaluated on four standard affective computing datasets.

## Method Summary
The proposed method uses an autoencoder architecture with a multi-class classifier attached to the latent space. The key innovation is a novel loss function that incorporates Wasserstein Distance to measure how far each subject's data distribution is from the group distribution. This distance is normalized and used to scale the contribution of each subject's data during training, with subjects whose distributions are closer to the group receiving higher weights. The autoencoder is trained simultaneously with the classifier to optimize both reconstruction quality and classification accuracy while the modified loss function reduces the influence of subject-specific noise patterns.

## Key Results
- Average increase of 14.75% in minimum Euclidean distances between classes in latent space compared to baseline MSE loss
- Average increase of 17.75% in centroid Euclidean distances between classes in latent space compared to baseline MSE loss
- Average accuracy improvement of 4.21% across all datasets when using the proposed loss function compared to MSE baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Wasserstein Distance scales the importance of subject-dependent noise by measuring how far each subject's distribution is from the group distribution
- Mechanism: For each subject, Wasserstein Distance (EMD) is computed between that subject's data distribution and the overall group distribution. This distance is normalized and subtracted from 1 to create a scaling factor that inversely weights subject-specific patterns. Subjects with distributions closer to the group receive higher weights, while those further away receive lower weights
- Core assumption: Subject-specific noise manifests as distributions farther from the group distribution, and this distance is a reliable proxy for noise level
- Evidence anchors:
  - [abstract] "utilizes Wasserstein Distance to scale the importance of subject-dependent data such that higher importance is assigned to patterns in data that are common across all participants while decreasing the importance of patterns that result from subject-dependent noise"
  - [section] "Optimal Transport Theory, specifically, Wasserstein Distance, is used to calculate λs,i by measuring the distance between the group distribution and subject distribution"

### Mechanism 2
- Claim: The autoencoder architecture with simultaneous classifier training creates a latent space that is both compressed and discriminant for affective state classification
- Mechanism: The autoencoder learns to reconstruct the input while the attached classifier forces the latent space to be discriminative for affective state labels. The novel loss function modifies the reconstruction objective to account for subject-dependent noise, resulting in a cleaner latent representation
- Core assumption: The autoencoder can learn meaningful feature representations in the latent space that capture affective state information while the classifier loss ensures these representations are discriminant
- Evidence anchors:
  - [section] "we focus on using an autoencoder architecture to reduce noise along with our novel cost function through dimensionality reduction"
  - [section] "The group reconstruction error, rg, of the autoencoder for the subject-independent model is given as... The classifier head, attached to the latent space, is used to calculate the group classifier error"

### Mechanism 3
- Claim: The combined loss function (reconstruction + classification) with subject-specific scaling factors creates a more generalizable model by focusing learning on cross-subject patterns
- Mechanism: The overall loss is a weighted combination of group reconstruction loss, group classification loss, and subject-specific classification losses. The subject-specific losses are scaled by λs,i values that are inversely proportional to the Wasserstein Distance from the group distribution, effectively reducing the influence of subject-specific noise during training
- Core assumption: By reducing the influence of subject-specific patterns during training, the model will learn features that are more generalizable across subjects
- Evidence anchors:
  - [section] "The overall classification loss is given as... Optimal Transport Theory, specifically, Wasserstein Distance, is used to calculate λs,i by measuring the distance between the group distribution and subject distribution"
  - [section] "The goal is to increase the separation between different affective states in the latent space such that a more generalised classification model can be trained"

## Foundational Learning

- Concept: Optimal Transport Theory and Wasserstein Distance
  - Why needed here: Provides a mathematically principled way to measure distance between probability distributions, used to quantify subject-dependent noise
  - Quick check question: How does Wasserstein Distance differ from other distribution distance metrics like KL divergence or Jensen-Shannon divergence?

- Concept: Autoencoder architecture and latent space representation
  - Why needed here: The autoencoder learns compressed representations of physiological data used for classification while the novel loss function modifies the learning objective
  - Quick check question: What is the role of the reconstruction loss in an autoencoder, and how does it differ from the classification loss?

- Concept: Subject-independent vs subject-dependent classification
  - Why needed here: The paper addresses building models that generalize across subjects, which is harder than building models that work for specific subjects
  - Quick check question: Why does subject-independent classification typically perform worse than subject-dependent classification in affective computing?

## Architecture Onboarding

- Component map: Input physiological data → Encoder → Latent space → Classifier head → Output affective state labels; Reconstruction path: Latent space → Decoder → Reconstructed input; Loss function combines reconstruction error, group classification error, and subject-specific classification errors with Wasserstein-based scaling
- Critical path: Encoder → Latent space → Classifier (for classification accuracy); Encoder → Latent space → Decoder (for reconstruction quality)
- Design tradeoffs: Using Wasserstein Distance adds computational complexity but provides better noise modeling; simultaneous training of autoencoder and classifier may create conflicting objectives; scaling subject-specific losses may reduce overfitting but could lose subject-specific information
- Failure signatures: Poor reconstruction quality (decoder not learning), poor classification accuracy (latent space not discriminative), or unstable training (conflicting loss objectives)
- First 3 experiments:
  1. Train baseline autoencoder with MSE loss only, measure reconstruction quality and classification accuracy
  2. Train with proposed loss function but without classifier head, measure reconstruction quality
  3. Train full model with proposed loss function and classifier, measure classification accuracy and latent space separation metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Wasserstein-based loss function perform on larger-scale physiological datasets with thousands of subjects, and what is its scalability limit?
- Basis in paper: [inferred] The authors acknowledge they were unable to test on large-scale datasets due to their unavailability, and state that future work includes testing on larger datasets
- Why unresolved: The authors only tested on datasets with 15-60 subjects, and did not have access to larger datasets to evaluate scalability
- What evidence would resolve it: Testing the proposed loss function on large-scale physiological datasets with thousands of subjects and measuring performance metrics like accuracy, class separation, and computational efficiency

### Open Question 2
- Question: How does the proposed loss function compare to other domain adaptation techniques like Subspace Alignment Autoencoders or Maximum Mean Discrepancy in reducing subject-dependent noise?
- Basis in paper: [explicit] The authors mention domain adaptation techniques like Subspace Alignment Autoencoders in the literature review but do not compare their approach to these methods
- Why unresolved: The paper focuses on demonstrating the effectiveness of the Wasserstein-based loss function but does not benchmark it against other established domain adaptation methods
- What evidence would resolve it: Conducting experiments comparing the Wasserstein-based loss function with other domain adaptation techniques on the same datasets, measuring metrics like class separation and classification accuracy

### Open Question 3
- Question: What is the optimal architecture for the encoder and decoder components when using the proposed Wasserstein-based loss function for affective computing?
- Basis in paper: [explicit] The authors mention exploring more complex encoder and decoder models like LSTMs and CNNs as future work, indicating that the current architecture may not be optimal
- Why unresolved: The paper uses a standard autoencoder architecture but does not explore or optimize the encoder and decoder designs for the proposed loss function
- What evidence would resolve it: Systematically testing different encoder and decoder architectures (e.g., LSTMs, CNNs, attention mechanisms) with the Wasserstein-based loss function and evaluating their performance on affective computing tasks

## Limitations

- The paper does not specify exact autoencoder architecture details (layer sizes, activation functions, or exact implementation of Wasserstein Distance calculation)
- Hyperparameter values (learning rate, batch size, latent space dimensionality, regularization coefficients) are not provided
- The method was only tested on datasets with 15-60 subjects, limiting assessment of scalability to larger datasets

## Confidence

- **High Confidence:** The core mechanism of using Wasserstein Distance to scale subject-specific contributions is well-founded and mathematically sound
- **Medium Confidence:** The generalizability of the 14.75% and 17.75% distance improvements across different datasets is reasonable but depends on dataset characteristics
- **Low Confidence:** The specific architectural choices and hyperparameter settings that led to the 4.21% average accuracy improvement are uncertain without more implementation details

## Next Checks

1. **Architecture Replication:** Implement the autoencoder and classifier architecture with reasonable defaults, then systematically vary layer sizes and activation functions to identify optimal configurations
2. **Wasserstein Distance Implementation:** Validate the Wasserstein Distance calculation by testing on synthetic distributions with known properties to ensure the scaling factors behave as expected
3. **Cross-Dataset Consistency:** Evaluate the method on additional physiological datasets beyond the four used in the paper to assess generalizability and identify dataset-specific factors affecting performance