---
ver: rpa2
title: Continuous Prompt Generation from Linear Combination of Discrete Prompt Embeddings
arxiv_id: '2312.10323'
source_url: https://arxiv.org/abs/2312.10323
tags:
- prompts
- prompt
- discrete
- continuous
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to construct interpretable continuous
  prompts by linearly combining discrete prompt embeddings. The approach involves
  embedding a set of manually designed discrete prompts, training a neural network
  to predict weights for their linear combination, and using the resulting continuous
  prompt to guide a pre-trained language model like BART.
---

# Continuous Prompt Generation from Linear Combination of Discrete Prompt Embeddings

## Quick Facts
- arXiv ID: 2312.10323
- Source URL: https://arxiv.org/abs/2312.10323
- Reference count: 1
- Key outcome: Method constructs interpretable continuous prompts by linearly combining discrete prompt embeddings, achieving lower cross-entropy loss on ARC dataset compared to BART alone while providing interpretable weights.

## Executive Summary
This paper proposes a method to construct interpretable continuous prompts by linearly combining discrete prompt embeddings. The approach involves embedding a set of manually designed discrete prompts, training a neural network to predict weights for their linear combination, and using the resulting continuous prompt to guide a pre-trained language model like BART. The method is evaluated on the AI2 Reasoning Challenge dataset, where it achieves lower cross-entropy loss compared to BART alone. The learned weights on discrete prompts are interpretable, reflecting the relevance of each prompt's elicited behavior to the task.

## Method Summary
The method constructs interpretable continuous prompts by training a feed-forward neural network to predict weights for linear combinations of discrete prompt embeddings. Discrete prompts are manually designed, tokenized, and embedded into tensors. The neural network learns to assign appropriate weights to these embeddings based on their relevance to the task, creating a task-specific continuous prompt that guides the BART model. The approach is evaluated on the ARC dataset using cross-entropy loss as the primary metric, with interpretability analyzed through the learned weights on discrete prompts.

## Key Results
- The learned continuous prompts achieve lower cross-entropy loss on the ARC dataset compared to BART alone.
- The learned weights on discrete prompts are interpretable, reflecting the relevance of each prompt's elicited behavior to the task.
- Prompts encouraging analytical reasoning received higher weights on the scientific reasoning task, demonstrating task-specific weight assignment.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The linear combination of discrete prompt embeddings generates interpretable continuous prompts by preserving the semantic contribution of each discrete prompt.
- Mechanism: The feed-forward neural network learns weights that linearly combine the embeddings of discrete prompts. Since each discrete prompt is human-interpretable, the resulting continuous prompt inherits interpretability through the weighted combination of these discrete elements.
- Core assumption: The semantic meaning of discrete prompts can be preserved and combined linearly in the embedding space.
- Evidence anchors:
  - [abstract] "The learned weights on discrete prompts are interpretable, reflecting the relevance of each prompt's elicited behavior to the task."
  - [section] "For some basis-defining discrete prompts pDα, pDβ with learned weightings α and β on some task t, if β > α, then it can be concluded that the effective knowledge held within the discrete prompt pDβ is more relevant and beneficial to the downstream pretrained language model when attempting to solve the task, t."
  - [corpus] Weak - corpus papers discuss prompt optimization but don't directly address interpretability through linear combinations.
- Break condition: If the embedding space doesn't preserve semantic relationships between discrete prompts, the linear combination would produce uninterpretable continuous prompts.

### Mechanism 2
- Claim: Training a neural network to predict weights for linear combinations of discrete prompt embeddings improves performance on natural language understanding tasks compared to using BART alone.
- Mechanism: The neural network learns to assign appropriate weights to discrete prompt embeddings based on their relevance to the task, creating a task-specific continuous prompt that guides BART more effectively than generic continuous prompts or no prompts at all.
- Core assumption: Different tasks benefit from different combinations of discrete prompt behaviors, and a neural network can learn these optimal combinations.
- Evidence anchors:
  - [abstract] "The approach involves embedding a set of manually designed discrete prompts, training a neural network to predict weights for their linear combination, and using the resulting continuous prompt to guide a pre-trained language model like BART."
  - [section] "We observed that although the loss improved over time in both cases, it's not clear if further training would yield better results."
  - [corpus] Weak - corpus papers focus on prompt optimization but don't specifically validate performance improvements through linear combinations of discrete prompts.
- Break condition: If the task doesn't benefit from the specific behaviors elicited by the discrete prompts, or if the neural network cannot learn meaningful weight assignments.

### Mechanism 3
- Claim: Restricting the basis of learned continuous prompts to discrete prompt embeddings reduces "waywardness" and improves safety in people-sensitive applications.
- Mechanism: By constraining the continuous prompt to be a linear combination of interpretable discrete prompts, the model avoids generating unpredictable behaviors that might arise from unconstrained continuous prompt learning, particularly in sensitive applications like resume screening.
- Core assumption: Waywardness in continuous prompts arises from unconstrained learning, and constraining the learning space to interpretable discrete prompts prevents this.
- Evidence anchors:
  - [abstract] "The wayward quality of continuous prompts stresses the importance of their interpretability as unexpected and unpredictable behaviors appear following training, especially in the context of large language models automating people-sensitive tasks such as resume screening."
  - [section] "The waywardness of prompts motivates an alternative to particular tasks. For example (Khashabi et al., 2021) consider the scenario where in designing a model to 'solve a target task like 'ranking resumes according to each applicant's qualifications and merits', the wayward quality of continuous prompts 'may maliciously target, for example, a minority group'."
  - [corpus] Weak - corpus papers discuss prompt optimization but don't specifically address safety or waywardness in people-sensitive applications.
- Break condition: If the discrete prompt set doesn't capture the full range of behaviors needed for the task, or if the linear combination constraint is too restrictive.

## Foundational Learning

- Concept: Discrete Prompting
  - Why needed here: Understanding discrete prompts is essential because this method builds continuous prompts from them, and their interpretability is the foundation of the approach.
  - Quick check question: What distinguishes discrete prompts from continuous prompts in terms of their representation and optimization?

- Concept: Embedding Spaces and Linear Combinations
  - Why needed here: The method relies on representing discrete prompts as embeddings and combining them linearly, so understanding how semantic meaning is preserved in embedding spaces is crucial.
  - Quick check question: How does the linear combination of embeddings relate to the semantic combination of the original discrete prompts?

- Concept: Interpretability in Machine Learning
  - Why needed here: The core contribution is improved interpretability of continuous prompts, so understanding what makes a model interpretable and how to measure it is essential.
  - Quick check question: What are the key differences between interpretable and uninterpretable continuous prompts, and how does this method address those differences?

## Architecture Onboarding

- Component map:
  - Discrete Prompt Set -> Tokenizer -> Embedding Layer -> Feed-Forward Neural Network -> Linear Combination Layer -> BART Model -> Loss Function

- Critical path:
  1. Tokenize discrete prompts
  2. Embed prompts into tensor space
  3. Neural network predicts weights
  4. Linearly combine prompt embeddings
  5. Concatenate continuous prompt with input
  6. BART processes input and generates output
  7. Compute loss and backpropagate through neural network

- Design tradeoffs:
  - Prompt selection vs. coverage: More diverse discrete prompts increase the span of the tensor space but may include irrelevant prompts
  - Model complexity vs. interpretability: Simpler neural networks are easier to interpret but may not capture complex relationships
  - Training stability vs. batch size: Larger batch sizes improve stability but require more memory

- Failure signatures:
  - All weights converge to zero: Indicates the model cannot find meaningful combinations
  - Single prompt dominates all weights: Suggests the prompt set lacks diversity or the model overfits
  - Loss plateaus early: May indicate the linear combination constraint is too restrictive

- First 3 experiments:
  1. Baseline: Run BART without any continuous prompt to establish performance floor
  2. Random weights: Apply random weights to discrete prompt embeddings to test if learned weights provide benefit
  3. Varying prompt diversity: Test with different sets of discrete prompts (highly diverse vs. similar) to understand the impact of prompt selection on performance and interpretability

## Open Questions the Paper Calls Out
- Question: How does the mutual orthogonality of the basis-defining discrete prompts affect the performance of the continuous prompt generation method?
- Basis in paper: [explicit] The paper hypothesizes that performance is a function of mutual orthogonality, and mentions it as an interesting area for future exploration.
- Why unresolved: The authors did not conduct experiments to systematically vary the orthogonality of the discrete prompts and measure its impact on performance.
- What evidence would resolve it: Controlled experiments comparing performance with orthogonal vs non-orthogonal discrete prompts, and analysis of the relationship between prompt orthogonality and model performance.

## Limitations
- The experimental validation is constrained to a single task (ARC dataset) with a small batch size, which may not generalize well to other natural language understanding tasks.
- The discrete prompt set is not fully specified, making exact reproduction difficult.
- Safety claims regarding "waywardness" prevention are based on theoretical arguments rather than empirical demonstrations.

## Confidence
- High confidence: The mathematical framework for linear combinations of embeddings is well-defined and the method is implementable given the specifications.
- Medium confidence: The claim that this approach improves interpretability of continuous prompts is supported by qualitative analysis of learned weights, but lacks rigorous quantitative interpretability metrics. The performance improvement claim is based on limited experimental results with unclear statistical significance.
- Low confidence: The safety and waywardness prevention claims lack empirical validation and are primarily theoretical. The generalizability across diverse tasks and the impact of prompt diversity on both performance and interpretability are not thoroughly explored.

## Next Checks
1. Apply established interpretability metrics (such as feature importance scores, attention visualization, or behavioral testing) to compare the interpretability of learned continuous prompts against both discrete prompts and unconstrained continuous prompts across multiple tasks.

2. Evaluate the method on a diverse set of natural language understanding tasks beyond ARC (such as GLUE benchmark, SQuAD, or commonsense reasoning tasks) with larger batch sizes and multiple random seeds to assess robustness and statistical significance of performance improvements.

3. Design controlled experiments that test the method's behavior in simulated people-sensitive applications, comparing the occurrence of biased or unexpected outputs between interpretable continuous prompts and unconstrained continuous prompts to empirically validate the waywardness prevention claims.