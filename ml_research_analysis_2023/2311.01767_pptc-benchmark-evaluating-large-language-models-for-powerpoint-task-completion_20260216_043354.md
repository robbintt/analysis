---
ver: rpa2
title: 'PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion'
arxiv_id: '2311.01767'
source_url: https://arxiv.org/abs/2311.01767
tags:
- file
- llms
- arxiv
- content
- instructions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PPTC, a benchmark for evaluating large language
  models (LLMs) on PowerPoint task completion. PPTC simulates multi-turn dialogues
  where users give instructions to an AI assistant to create and edit PPT files.
---

# PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion

## Quick Facts
- arXiv ID: 2311.01767
- Source URL: https://arxiv.org/abs/2311.01767
- Reference count: 10
- Key outcome: PPTC benchmark evaluates LLMs on PowerPoint task completion, showing GPT-4 achieves 75.1% turn-based accuracy but only 6% session accuracy due to error accumulation and multi-modality challenges.

## Executive Summary
This paper introduces PPTC, a benchmark for evaluating large language models on PowerPoint task completion through multi-turn dialogues. The benchmark contains 279 sessions with diverse topics and hundreds of multi-modal instructions. The authors propose PPTX-Match, an evaluation system that checks if LLM-generated PPT files match ground truth based on attributes and position relations. Experiments on 9 LLMs show GPT-4 outperforms others with 75.1% turn-based accuracy but struggles significantly with entire sessions (6% session accuracy), highlighting challenges with error accumulation, long PPT template processing, and multi-modality perception.

## Method Summary
The PPTC benchmark evaluates LLMs by having them generate API sequences to complete user instructions for creating and editing PPT files. The evaluation process involves prompting LLMs with task instructions, API references, parsed PPT content, and dialogue history to generate API sequences. These sequences are executed to create prediction PPT files, which are then evaluated against ground truth using the PPTX-Match system that extracts attributes and position relations. The benchmark includes content and API selection algorithms to improve efficiency by filtering irrelevant information from prompts.

## Key Results
- GPT-4 achieves 75.1% turn-based accuracy but only 6% session accuracy, demonstrating severe error accumulation issues
- GPT-4 performs exceptionally well on text operations (85.6% accuracy) but poorly on position-related operations (24% accuracy)
- Content and API selection algorithms improve GPT-4's turn-based performance by 1-5%
- The "editing PPT template" task is particularly challenging, with GPT-4 achieving only 22.7% session-based accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-turn dialogue structure forces LLMs to handle error accumulation
- Mechanism: Each turn's API sequence depends on previous turns' outputs. A single error corrupts the PPT file state, leading to cascading failures in subsequent turns.
- Core assumption: LLM-generated API sequences are executed deterministically and affect the PPT file state.
- Evidence anchors:
  - [abstract] "GPT-4 outperforms other LLMs with 75.1% accuracy in single-turn dialogue testing but faces challenges in completing entire sessions, achieving just 6% session accuracy."
  - [section 4.4] "GPT-4 achieves only a 22.7% session-based accuracy for the 'creating new PPT file' task and a mere 6.0% session-based accuracy for the 'editing PPT template' task."
  - [corpus] Weak - no direct citations about error propagation in multi-turn systems.
- Break condition: If the evaluation system resets PPT state between turns or uses oracle corrections.

### Mechanism 2
- Claim: Multi-modality perception is harder than unimodal understanding
- Mechanism: Instructions involving images, charts, tables, and spatial positioning require integrating multiple representation types, which current LLMs struggle to process coherently.
- Core assumption: GPT-4's token limit and embedding space cannot fully represent multi-modal relationships.
- Evidence anchors:
  - [abstract] "GPT-4 and other LLMs also struggle to process long PPT templates (complex file environment)" and "GPT-4 struggles to finish instructions involving non-text modality operations, especially for position-related operations, such as 'Put object A on the top of the slide'. It only achieves 24% accuracy in these instructions."
  - [section 5.1] "GPT-4 performs exceptionally well in the text modality, achieving an accuracy of 85.6%. Its performance becomes poorer when processing structured data (Chart and Table), with 12.4% and 16.2% lower accuracy."
  - [corpus] Moderate - related works on multi-modal LLMs show similar struggles with spatial reasoning.
- Break condition: If a multi-modal model (e.g., GPT-4V) can process PPT content directly.

### Mechanism 3
- Claim: API selection algorithm reduces cognitive load and improves accuracy
- Mechanism: By filtering irrelevant APIs and PPT content before prompting, the LLM focuses on task-relevant information, reducing token consumption and decision complexity.
- Core assumption: Irrelevant content in the prompt distracts the LLM and increases error probability.
- Evidence anchors:
  - [section 4.4] "Content and API selection algorithms can further improve the turn-based performance of GPT-4 by 1∼5 percent. That is because they reduce the task difficulty by filtering irrelevant PPT content/APIs in the input prompt."
  - [section 3.2] "Combining the whole PPT file and the whole API file into the LLM's input can result in an overwhelming amount of redundant information... Filtering the redundant information would improve the efficiency of the LLM."
  - [corpus] Moderate - similar selection techniques used in code generation benchmarks.
- Break condition: If the selection algorithm incorrectly filters necessary information.

## Foundational Learning

- Concept: PPTX library for PPT file parsing
  - Why needed here: The evaluation system must extract attributes and position relations from PPT files to compare LLM outputs against ground truth.
  - Quick check question: Can you write Python code using python-pptx to iterate over all shapes in a slide and print their text and position?

- Concept: Embedding similarity for API selection
  - Why needed here: The API selection algorithm uses cosine similarity between instruction embeddings and API description embeddings to rank relevant APIs.
  - Quick check question: Given two sentences, can you compute their cosine similarity using sentence-transformers?

- Concept: Chain-of-thought prompting
  - Why needed here: CoT helps LLMs decompose complex instructions into intermediate reasoning steps before generating API sequences.
  - Quick check question: How would you modify a prompt to include "Let's think step by step" for chain-of-thought reasoning?

## Architecture Onboarding

- Component map: PPTC Benchmark -> PPTX-Match Evaluation System -> LLM API wrapper -> PPT reader function -> API executor -> Content/API selection modules
- Critical path: Instruction → PPT reader → LLM prompt → API sequence → API executor → PPTX-Match evaluation → Accuracy metric
- Design tradeoffs:
  - Text-based PPT representation vs. direct PPT file input: Simpler but loses visual context
  - Turn-based vs. session-based evaluation: Isolates errors vs. captures error accumulation
  - API reference completeness vs. size: More APIs enable more tasks but increase prompt length
- Failure signatures:
  - Low turn accuracy but high session accuracy: Evaluation bug (state not carried forward)
  - High turn accuracy but low session accuracy: Error accumulation problem
  - High accuracy on text but low on images/charts: Multi-modality weakness
  - High accuracy on simple turns but low on complex turns: Reasoning/planning limitation
- First 3 experiments:
  1. Run GPT-4 on 10 sample turns with debugging enabled to observe prompt content and API generation
  2. Test API selection algorithm with k=5 vs k=15 to find optimal trade-off
  3. Implement oracle PPT state tracking to isolate error accumulation vs. single-turn reasoning failures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do error accumulation patterns in multi-turn sessions vary across different types of instructions (e.g., text vs. position vs. image)?
- Basis in paper: [explicit] The paper identifies error accumulation as a key challenge and mentions that GPT-4's session accuracy is only 6% for editing PPT templates, with errors in previous turns influencing current turn completion.
- Why unresolved: While the paper identifies error accumulation as a key challenge, it doesn't analyze how different types of instructions contribute differently to error accumulation patterns.
- What evidence would resolve it: A detailed breakdown of error types and their frequency across different instruction types within multi-turn sessions, showing how errors compound differently for text, position, and image operations.

### Open Question 2
- Question: What is the relationship between the number of turns in a session and the likelihood of session completion for different LLMs?
- Basis in paper: [explicit] The paper mentions that sessions contain 2-17 turns and that longer sessions are more challenging, but doesn't analyze how session length specifically impacts completion rates across different models.
- Why unresolved: The paper provides turn-based and session-based accuracy metrics but doesn't analyze how session length correlates with completion probability for different models.
- What evidence would resolve it: A statistical analysis showing completion rates as a function of session length for each evaluated LLM, potentially revealing thresholds where performance degrades significantly.

### Open Question 3
- Question: How does the effectiveness of planning algorithms like CoT and ToT vary based on the complexity of the instruction (e.g., number of required APIs)?
- Basis in paper: [explicit] The paper tests CoT and ToT algorithms but notes that ToT, despite being more complex, doesn't outperform CoT, and both only improve performance by 1-2% for turn-based accuracy.
- Why unresolved: The paper applies planning algorithms uniformly but doesn't analyze whether their effectiveness depends on instruction complexity (measured by number of required APIs).
- What evidence would resolve it: An analysis comparing the performance gains of planning algorithms across different instruction complexity levels, potentially showing they're more effective for complex multi-API instructions.

## Limitations

- The PPTC benchmark covers 279 multi-turn sessions but may have uneven distribution across task types, potentially skewing performance evaluations
- PPTX-Match evaluation focuses on attributes and position relations but misses complex PPT features like animations, transitions, and style consistency
- Ground truth PPT files and API sequences were likely created by humans, introducing potential biases or inconsistencies in the evaluation

## Confidence

- High Confidence: GPT-4 outperforms other LLMs on turn-based accuracy (75.1%) - well-supported by direct experimental comparisons
- Medium Confidence: Error accumulation is the primary cause of low session accuracy (6%) - data shows this pattern but other factors may contribute
- Medium Confidence: Multi-modality perception is a key challenge - performance gap between text and chart/table operations supports this
- Low Confidence: Content/API selection algorithms provide consistent 1-5% improvements - paper mentions these improvements without detailed ablation studies

## Next Checks

1. **Ablation Study on Error Sources**: Run controlled experiments where PPT state is reset between turns versus carried forward to isolate whether low session accuracy stems from error accumulation or fundamental multi-turn reasoning limitations.

2. **Ground Truth Inter-Annotator Agreement**: Have multiple human experts independently solve 50 sample tasks to measure ground truth consistency and identify whether the current ground truth represents the only correct solution path.

3. **Direct PPT Manipulation Comparison**: Implement a GPT-4V-based evaluation where the model can directly "see" and manipulate PPT content rather than relying on text-based API generation, to determine if API-based evaluation underestimates true multimodal capabilities.