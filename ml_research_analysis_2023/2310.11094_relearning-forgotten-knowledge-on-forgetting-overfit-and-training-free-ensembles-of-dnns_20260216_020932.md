---
ver: rpa2
title: 'Relearning Forgotten Knowledge: on Forgetting, Overfit and Training-Free Ensembles
  of DNNs'
arxiv_id: '2310.11094'
source_url: https://arxiv.org/abs/2310.11094
tags:
- training
- data
- test
- which
- overfit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel perspective on overfitting in deep
  neural networks by introducing the forget fraction metric, which measures the portion
  of test data that is correctly classified during training but misclassified by the
  final model. The authors show that even when traditional overfitting is not observed
  (i.e., when test accuracy does not decrease), networks still "forget" significant
  portions of the test population during training, indicating a form of local overfitting.
---

# Relearning Forgotten Knowledge: on Forgetting, Overfit and Training-Free Ensembles of DNNs

## Quick Facts
- arXiv ID: 2310.11094
- Source URL: https://arxiv.org/abs/2310.11094
- Reference count: 40
- Key outcome: Introduces KnowledgeFusion (KF) method that improves accuracy by 1-7% through ensemble of checkpoints selected by "forget fraction" metric

## Executive Summary
This paper introduces a novel perspective on overfitting in deep neural networks by introducing the forget fraction metric, which measures the portion of test data that is correctly classified during training but misclassified by the final model. The authors show that even when traditional overfitting is not observed (i.e., when test accuracy does not decrease), networks still "forget" significant portions of the test population during training, indicating a form of local overfitting. Based on this observation, they propose a training-free ensemble method called KnowledgeFusion (KF) that combines checkpoints from mid-training with the final model to recover forgotten knowledge and improve performance.

## Method Summary
The KnowledgeFusion method works by first training a model and saving checkpoints throughout training. It then calculates a "forget fraction" metric on validation data, which represents the portion of test data that the model initially classifies correctly but misclassifies as training proceeds. Checkpoints with high forget fractions are selected and combined with the final model using learned weights. The method delivers a weighted average of the class probability output vector between the final model and a set of checkpoints, optimized to maximize validation accuracy.

## Key Results
- KF consistently improves performance by 1-7% accuracy compared to single models across CIFAR-100, TinyImagenet, and ImageNet
- KF outperforms or matches other ensemble approaches while being simpler to implement and not requiring changes to the training process
- KF adds no training overhead and is especially effective in transfer learning scenarios and with noisy labels
- The forget fraction metric reveals "local overfitting" even when traditional overfitting is not observed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Networks "forget" test data during training even when overall test accuracy improves
- Mechanism: As training progresses, the network shifts its internal representations to fit training data, causing some previously correctly classified test examples to be misclassified in later epochs
- Core assumption: The network's optimization process prioritizes fitting training data over maintaining correct classifications of test data
- Evidence anchors: [abstract] "even when traditional overfitting is not observed (i.e., when test accuracy does not decrease), networks still 'forget' significant portions of the test population during training"
- Break condition: If the network maintains perfect classification of all test data throughout training (which is theoretically impossible with finite capacity)

### Mechanism 2
- Claim: Mid-training checkpoints contain knowledge about test data that the final model has forgotten
- Mechanism: Different training epochs optimize different aspects of the data distribution, with mid-training epochs sometimes capturing patterns that the final model loses
- Core assumption: The optimization trajectory passes through multiple local minima that each capture different aspects of the data manifold
- Evidence anchors: [abstract] "propose a training-free ensemble method called KnowledgeFusion (KF) that combines checkpoints from mid-training with the final model to recover forgotten knowledge"
- Break condition: If all checkpoints perform equally poorly on the forgotten data points

### Mechanism 3
- Claim: Weighted averaging of mid-training and final model predictions recovers forgotten knowledge
- Mechanism: By combining the probability outputs of the final model with selected mid-training checkpoints using learned weights, the ensemble can recover classifications that any single model misses
- Core assumption: The optimal combination of models is a convex combination of their probability outputs
- Evidence anchors: [section] "The method delivers a weighted average of the class probability output vector between the final model and a set of checkpoints"
- Break condition: If the mid-training checkpoints are consistently worse than the final model on all data points

## Foundational Learning

- Concept: Overfitting and generalization gap
  - Why needed here: Understanding traditional overfitting helps distinguish the "local overfitting" the paper identifies
  - Quick check question: What's the difference between global overfitting (test accuracy decreasing) and the local overfitting described here?

- Concept: Ensemble methods and checkpoint averaging
  - Why needed here: The method builds on ensemble techniques but with a novel selection criterion
  - Quick check question: How does KnowledgeFusion differ from simple checkpoint averaging or exponential moving average?

- Concept: Label noise and double descent
  - Why needed here: The paper uses double descent phenomena to illustrate and validate the forgetting metric
  - Quick check question: Why does label noise in training data help reveal the forgetting phenomenon?

## Architecture Onboarding

- Component map: Training pipeline -> Validation pipeline (forget fraction calculation) -> KF pipeline (checkpoint selection, weight learning, ensemble inference) -> Hyperparameter search

- Critical path:
  1. Train model and save checkpoints
  2. Calculate forget fractions on validation data
  3. Select high-forget checkpoints
  4. Learn ensemble weights via validation accuracy
  5. Apply ensemble at inference time

- Design tradeoffs:
  - Checkpoint frequency vs memory usage
  - Number of checkpoints vs inference overhead
  - Validation set size vs hyperparam tuning accuracy
  - Weight learning complexity vs performance gain

- Failure signatures:
  - No improvement over single model
  - Performance degradation on validation set
  - High variance in ensemble weights
  - Forget fractions close to zero across all checkpoints

- First 3 experiments:
  1. CIFAR-100 with Resnet18, no label noise, measure forget fraction
  2. CIFAR-100 with 20% symmetric label noise, compare KF vs horizontal ensemble
  3. Transfer learning on CIFAR-100, fine-tune last layers only, apply KF

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the forgetting fraction metric correlate with specific architectural properties or training dynamics of neural networks beyond what was shown for model size and dataset size?
- Basis in paper: [explicit] The paper shows that larger models and smaller datasets lead to higher forget fractions, but doesn't explore other architectural or training factors.
- Why unresolved: The authors only tested a limited set of architectures (ResNet, ConvNeXt, ViT, MaxViT) and training configurations, leaving open whether other factors like activation functions, normalization schemes, or optimizer choices might influence forgetting.
- What evidence would resolve it: Systematic experiments varying architectural choices (e.g., different activation functions, normalization layers, skip connections) and training parameters (learning rate schedules, optimizer types, batch sizes) while measuring forget fractions would reveal correlations.

### Open Question 2
- Question: Can the KnowledgeFusion method be extended to work effectively with online learning or continual learning scenarios where data distribution changes over time?
- Basis in paper: [inferred] The method relies on comparing predictions across training epochs, which assumes a fixed data distribution. The paper mentions catastrophic forgetting as a separate phenomenon but doesn't explore their interaction.
- Why unresolved: The paper focuses on static dataset scenarios and doesn't investigate whether the method would work when the underlying data distribution shifts or when learning from streaming data.
- What evidence would resolve it: Testing KF on datasets with temporal shifts or in online learning setups where the model encounters new data over time would show whether the method can adapt to changing distributions.

### Open Question 3
- Question: What is the relationship between forgotten examples and their semantic similarity or feature space proximity to other examples in the dataset?
- Basis in paper: [inferred] The paper discusses forgetting as a local phenomenon affecting specific subregions of the data space but doesn't analyze the characteristics of forgotten examples.
- Why unresolved: While the authors mention that forgetting occurs in subregions, they don't investigate whether forgotten examples share common features, belong to specific classes, or occupy particular regions in the feature space.
- What evidence would resolve it: Clustering analysis of forgotten examples, visualization of their feature representations, or analysis of their semantic similarities would reveal patterns in which types of examples are more likely to be forgotten.

## Limitations
- The forget fraction metric may not fully capture the complexity of model behavior during training as it focuses only on examples that transition from correct to incorrect predictions
- The method's effectiveness depends on having sufficient validation data to accurately calculate forget fractions and learn ensemble weights
- The weighted averaging assumption in KF may not be optimal for all architectures or training scenarios

## Confidence
- High confidence: Observation that models "forget" test data during training (directly measurable through forget fraction metric)
- Medium confidence: Mechanism explanation that mid-training checkpoints capture forgotten knowledge (primarily empirical evidence)
- Medium confidence: KF method's superiority over other ensemble approaches (some comparisons could be more comprehensive)

## Next Checks
1. **Ablation study on checkpoint selection**: Test KF with different checkpoint selection criteria (e.g., random selection, highest accuracy vs. highest forget fraction) to isolate the contribution of the forgetting-based selection mechanism.

2. **Cross-architecture forgetting patterns**: Analyze whether the forgetting phenomenon and KF effectiveness are consistent across very different architectures (e.g., transformers vs. convolutional networks) to validate the generality of the findings.

3. **Theoretical analysis of convex combination assumption**: Investigate whether the weighted averaging assumption in KF is theoretically justified or if non-convex combinations might yield better performance, particularly for very high-forget checkpoints.