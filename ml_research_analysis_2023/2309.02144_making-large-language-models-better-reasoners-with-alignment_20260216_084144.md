---
ver: rpa2
title: Making Large Language Models Better Reasoners with Alignment
arxiv_id: '2309.02144'
source_url: https://arxiv.org/abs/2309.02144
tags:
- constraint
- reasoning
- ranking
- alignment
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models fine-tuned with vanilla methods suffer from
  an assessment misalignment problem where they struggle to properly score high-quality
  versus low-quality chain-of-thought reasoning paths. The proposed Alignment Fine-Tuning
  (AFT) paradigm addresses this by calibrating LLM scores on generated COT responses
  using a novel constraint alignment loss.
---

# Making Large Language Models Better Reasoners with Alignment

## Quick Facts
- arXiv ID: 2309.02144
- Source URL: https://arxiv.org/abs/2309.02144
- Authors: Liang Zhang, Yichao Lu, Han Guo, Quanzhi Li, Lu Hou, Zhiqiu Lin, Lifeng Shang, Xin Jiang, Irwin King, Michael R. Lyu, et al.
- Reference count: 39
- Large language models fine-tuned with vanilla methods suffer from an assessment misalignment problem where they struggle to properly score high-quality versus low-quality chain-of-thought reasoning paths. The proposed Alignment Fine-Tuning (AFT) paradigm addresses this by calibrating LLM scores on generated COT responses using a novel constraint alignment loss. This loss ensures positive COT scores exceed negative ones while preventing degradation through a boundary constraint. Experiments on four reasoning benchmarks show AFT improves accuracy by 1.91%-2.57% over vanilla fine-tuning and outperforms concurrent ranking-based alignment methods. The approach also scales well to multi-task and out-of-distribution settings.

## Executive Summary
This paper addresses a fundamental problem in fine-tuning large language models for reasoning tasks: assessment misalignment. When vanilla fine-tuning methods are applied to chain-of-thought reasoning data, models struggle to properly score high-quality versus low-quality reasoning paths, limiting their reasoning ability. The authors propose Alignment Fine-Tuning (AFT), a three-step paradigm that generates multiple COT responses per example, categorizes them as positive or negative based on correctness, and calibrates the model's scoring using a novel constraint alignment loss. This approach significantly improves reasoning accuracy across multiple benchmarks by 1.91%-2.57% over vanilla fine-tuning while also enhancing the model's ability to distinguish between good and poor reasoning paths.

## Method Summary
The AFT method addresses assessment misalignment through a three-step process. First, a vanilla fine-tuned (VFT) model is trained using standard maximum likelihood estimation on chain-of-thought data. Second, for each training example, the VFT model generates k=6 candidate COTs using temperature=1.0 sampling, which are categorized as positive or negative based on whether they lead to correct answers. Third, the model is fine-tuned using a constraint alignment loss that combines the standard VFT loss with an alignment objective. This alignment objective uses contrastive learning to ensure positive COT scores exceed negative ones while preventing degradation through a boundary constraint that keeps negative scores within a reasonable range. The method can be implemented using either a detached constraint (DCL) or boundary constraint (BCL) formulation.

## Key Results
- AFT improves reasoning accuracy by 1.91%-2.57% over vanilla fine-tuning across four benchmarks (GSM8K, AQUA-RAT, ECQA, and GSM8K-RANK)
- AFT significantly outperforms concurrent ranking-based alignment methods (DPO, RRHF, PRO) on all four reasoning benchmarks
- AFT improves the model's assessment accuracy (AAccuracy) by 5.73%-9.23%, demonstrating enhanced ability to distinguish high-quality from low-quality COTs
- The method scales well to multi-task and out-of-distribution settings, showing robust performance across different reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VFT LLMs fail to properly score COT quality because MLE objective allocates probability mass exclusively to the reference COT
- Mechanism: The MLE objective treats all other correct and incorrect COTs as negative examples, preventing LLMs from learning to assess the quality of various COTs
- Core assumption: The reasoning tasks require LLMs to distinguish between high-quality and low-quality COTs beyond just the reference answer
- Evidence anchors:
  - [abstract] "MLE only assigns probability mass to the reference COT, which contradicts reasoning tasks where various reasoning paths can lead to the correct answer"
  - [section 3.2] "This objective uniformly treats all other correct and incorrect COTs as negative examples. As a result, it will impede LLMs from learning to assess the quality of various COTs and degrade their reasoning ability"
- Break condition: If reasoning tasks had only one correct COT path, this mechanism would not apply

### Mechanism 2
- Claim: AFT's constraint alignment loss fixes the scoring misalignment by ensuring positive COT scores exceed negative ones while preventing degradation
- Mechanism: The loss has two objectives - alignment (positive scores > negative scores) and constraint (keeps negative scores confined to reasonable range)
- Core assumption: Without the constraint term, simply forcing positive scores above negative ones could harm model performance by pushing negative scores too low
- Evidence anchors:
  - [abstract] "Specifically, the constraint alignment loss has two objectives: a) Alignment, which guarantees that positive scores surpass negative scores to encourage answers with high-quality COTs; b) Constraint, which keeps the negative scores confined to a reasonable range to prevent the model degradation"
  - [section 4.3] "We find that reducing their scores by Equation 6 without setting any constraint will result in the degradation of the LLMs"
- Break condition: If all COTs were of similar quality, the constraint might be unnecessary

### Mechanism 3
- Claim: Ranking-based alignment methods (DPO, RRHF, PRO) fail without constraint terms because they reduce negative scores too aggressively
- Mechanism: These methods decrease scores of non-optimal COTs without protecting against excessive reduction, leading to model degradation
- Core assumption: Negative COTs still have reasonable quality since they're sampled from fine-tuned LLMs, so completely suppressing their scores is harmful
- Evidence anchors:
  - [abstract] "Furthermore, we also delve deeply into recent ranking-based methods for alignment, such as DPO, RRHF, and PRO, and find that the constraint, which has been overlooked by these approaches, is also crucial for their performance"
  - [section 6.1] "We think the reason is that previous alignment ranking losses will unreasonably decrease the score of non-optimal COTs"
- Break condition: If negative COTs were consistently very poor quality, aggressive reduction might be beneficial

## Foundational Learning

- Concept: Chain of Thought (COT) reasoning
  - Why needed here: The paper's core contribution addresses misalignment in scoring COT quality
  - Quick check question: What distinguishes a positive COT from a negative COT in this framework?

- Concept: Perplexity as a scoring mechanism
  - Why needed here: The paper uses perplexity scores to assess COT quality and alignment
  - Quick check question: Why does lower perplexity indicate a higher score in this context?

- Concept: Contrastive learning principles
  - Why needed here: The alignment loss uses contrastive learning to ensure positive scores exceed negative ones
  - Quick check question: How does the InfoNCE loss formulation relate to the alignment objective?

## Architecture Onboarding

- Component map: Data generation pipeline: VFT → COT sampling → categorization (positive/negative) → Training objectives: LVFT + alignment loss (LDC or LBC) → Evaluation metrics: Accuracy, assessment accuracy, perplexity

- Critical path: Data generation → Fine-tuning with alignment loss → Evaluation on reasoning benchmarks

- Design tradeoffs:
  - Sampling temperature (1.0 used) vs diversity of COTs
  - Number of candidate COTs (k=6 used) vs computational cost
  - Boundary constraint hyperparameter β vs model performance

- Failure signatures:
  - High assessment accuracy but low task accuracy suggests overfitting to COT quality assessment
  - Degraded perplexity on positive COTs indicates constraint is too strong
  - Minimal improvement over VFT suggests alignment loss isn't effective

- First 3 experiments:
  1. Reproduce assessment misalignment: Fine-tune VFT, generate COTs, measure AAccuracy
  2. Test boundary constraint: Vary β parameter, observe impact on performance and perplexity
  3. Compare alignment strategies: Implement both LDC and LBC, evaluate on GSM8K validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does AFT's effectiveness scale to larger language models (e.g., 65B, 70B parameter models) as the paper suggests, or are there diminishing returns for very large models?
- Basis in paper: [inferred] The paper states it did not scale to larger models due to resource limitations, but hypothesizes AFT would improve performance on larger models.
- Why unresolved: The paper explicitly acknowledges this as a limitation and did not test larger models.
- What evidence would resolve it: Experiments running AFT on Llama 65B and 70B parameter models, comparing performance to vanilla fine-tuning and other baselines.

### Open Question 2
- Question: What is the optimal strategy for dynamically setting the boundary constraint hyperparameter β across different reasoning tasks and model sizes, without requiring manual tuning?
- Basis in paper: [explicit] The paper notes that finding the optimal β requires constructing a validation set and search overhead, and suggests this as a meaningful question for future work.
- Why unresolved: The paper uses manual grid search for β and acknowledges this is suboptimal.
- What evidence would resolve it: Development and validation of an automatic method for setting β (e.g., based on COT quality distributions or model uncertainty) that eliminates the need for manual tuning.

### Open Question 3
- Question: How does AFT's performance compare to other advanced reasoning techniques like self-consistency with majority voting when both are applied to the same base model?
- Basis in paper: [explicit] The paper shows AFT enhances self-consistency performance but does not directly compare AFT-tuned models against self-consistency with baseline models.
- Why unresolved: The paper only measures AFT's impact on improving self-consistency, not how AFT-tuned models compare to baseline models with self-consistency.
- What evidence would resolve it: Head-to-head comparison of reasoning accuracy between: (1) AFT-tuned models without self-consistency, (2) baseline models with self-consistency, and (3) AFT-tuned models with self-consistency.

## Limitations
- The paper assumes COT quality directly correlates with final answer correctness, which may not hold in all reasoning scenarios
- The sampling strategy (temperature=1.0, k=6 candidates) appears arbitrary without sensitivity analysis
- The boundary constraint introduces hyperparameters requiring manual tuning, with limited guidance for practitioners

## Confidence
- High confidence: The assessment misalignment problem is well-demonstrated, and core experimental results showing AFT outperforming vanilla fine-tuning are reproducible
- Medium confidence: The claim that boundary constraints are crucial for preventing degradation is supported by ablation studies
- Low confidence: The assertion that AFT scales well to multi-task and out-of-distribution settings is based on limited experiments

## Next Checks
1. Implement ablation studies varying the number of candidate COTs (k=3, 6, 9) and sampling temperature to understand their impact on assessment accuracy and final task performance.

2. Test the method on reasoning tasks where high-quality COTs might lead to incorrect answers (e.g., problems requiring creative thinking or where standard approaches fail), to validate whether the approach generalizes beyond straightforward arithmetic reasoning.

3. Conduct systematic hyperparameter sensitivity analysis for the boundary constraint β across different model sizes and reasoning benchmarks to establish guidelines for practical deployment.