---
ver: rpa2
title: Worth of knowledge in deep learning
arxiv_id: '2307.00712'
source_url: https://arxiv.org/abs/2307.00712
tags:
- data
- rule
- rules
- importance
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework inspired by Shapley value to
  quantitatively measure the importance of prior knowledge (rules) in deep learning.
  The framework evaluates how rules affect model performance by comparing predictions
  with and without specific rules across different data volumes and prediction scenarios
  (in-distribution vs out-of-distribution).
---

# Worth of knowledge in deep learning

## Quick Facts
- arXiv ID: 2307.00712
- Source URL: https://arxiv.org/abs/2307.00712
- Reference count: 40
- This paper introduces a Shapley value-based framework to quantify the importance of prior knowledge (rules) in deep learning models.

## Executive Summary
This paper introduces a Shapley value-based framework to quantify the importance of prior knowledge (rules) in deep learning models. The framework measures how rules affect model performance by comparing predictions with and without specific rules across different data volumes and prediction scenarios. Experiments on four canonical PDEs reveal that rule importance decreases with more data in in-distribution tasks, global rules gain importance in out-of-distribution prediction while local rules lose importance, and rules exhibit complex interactions including dependence, synergism, and substitution effects. The approach successfully identifies improper rules and guides regularization parameter tuning, reducing prediction error by 74% in a multivariable equation example.

## Method Summary
The framework pre-trains a baseline model without rules, then measures marginal contributions by adding rule coalitions to calculate Shapley values that represent rule importance. The approach uses ANN architectures with 3-5 layers and 50 neurons per hidden layer, incorporating rules through modified loss functions. The method is model-agnostic and applicable to various network architectures, requiring training of 2^N models for N rules, though pre-training strategies reduce computational cost.

## Key Results
- Rule importance decreases with more training data in in-distribution tasks
- Global rules gain importance in out-of-distribution prediction while local rules lose importance
- Rules exhibit dependence, synergism, and substitution effects that affect measured importance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Rule importance decreases with more training data in in-distribution tasks because the model learns the sample distribution directly from data, reducing reliance on rules.
- **Mechanism**: As training data volume increases, the model's learned distribution becomes more accurate, making prior knowledge less necessary for constraining the solution space.
- **Core assumption**: Rules primarily function to restrict the optimization space when data is limited.
- **Evidence anchors**:
  - [abstract]: "rule importance decreases with more data in in-distribution tasks"
  - [section]: "In this scenario, a larger volume of data enables the model to learn a more accurate distribution without the need to integrate rules to limit the range of alternative distributions"
- **Break condition**: When rules encode constraints that cannot be learned from data alone (e.g., physical laws with specific functional forms)

### Mechanism 2
- **Claim**: Global rules (like PDEs) gain importance in out-of-distribution prediction while local rules (like ICs) lose importance.
- **Mechanism**: Out-of-distribution prediction requires extrapolation beyond observed data distribution. Global rules constrain the entire domain and remain relevant, while local rules only constrain observed regions and become less useful.
- **Core assumption**: Global rules provide constraints that apply beyond the training distribution boundaries.
- **Evidence anchors**:
  - [abstract]: "global rules gain importance in out-of-distribution prediction while local rules lose importance"
  - [section]: "global rules will play a more critical role in instructing the model by restricting possible distributions globally"
- **Break condition**: When local rules provide critical boundary conditions that prevent model divergence

### Mechanism 3
- **Claim**: Rules exhibit dependence, synergism, and substitution effects that affect their measured importance.
- **Mechanism**: Rules can be interdependent (requiring other rules to function), synergistic (working together to produce effects greater than individual contributions), or substitutable (one rule can be replaced by data or other rules).
- **Core assumption**: Multiple rules interact in complex ways that affect their marginal contributions to model performance.
- **Evidence anchors**:
  - [abstract]: "rules exhibit dependence, synergism, and substitution effects"
  - [section]: "we observed evidence of synergism through the importance of different numbers of other relying rules"
- **Break condition**: When rules are truly independent and additive in their effects

## Foundational Learning

- **Concept**: Shapley value framework
  - Why needed here: Provides a fair method to allocate rule contributions based on marginal contributions across all possible rule coalitions
  - Quick check question: How does Shapley value ensure that the sum of individual rule contributions equals the total coalition benefit?

- **Concept**: Marginal contribution calculation
  - Why needed here: Measures the change in model performance when a rule is added to a coalition versus when it's absent
  - Quick check question: Why is the logarithm of MSE used instead of raw MSE when calculating marginal contributions?

- **Concept**: In-distribution vs out-of-distribution prediction
  - Why needed here: Determines whether rules function primarily as constraints (in-distribution) or as extrapolation guides (out-of-distribution)
  - Quick check question: What distinguishes interpolation from extrapolation in terms of sample distribution coverage?

## Architecture Onboarding

- **Component map**: Pre-trained baseline model → Rule coalition perturbation → Performance measurement → Shapley value calculation → Rule importance ranking
- **Critical path**: Model training → Rule incorporation → MSE measurement → Importance calculation
- **Design tradeoffs**: Computational cost (2^N models) vs. accuracy of importance estimation; rule weight optimization vs. training stability
- **Failure signatures**: Negative rule importance values; convergence issues during training; sensitivity to collocation point selection
- **First 3 experiments**:
  1. Implement baseline model without rules and measure baseline MSE
  2. Add single rule coalitions and measure marginal contribution
  3. Test rule importance sensitivity to collocation point density

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed framework handle conflicting or contradictory prior rules when multiple rules are incorporated into the model?
- Basis in paper: [explicit] The paper discusses interactions between rules including dependence, synergism, and substitution effects, and mentions that incorporating multiple prior rules can lead to a high risk of model collapse due to intricate internal interactions.
- Why unresolved: While the paper mentions that the framework can identify improper rules with negative importance, it doesn't provide a detailed mechanism for resolving conflicts between multiple rules that might have competing or contradictory effects on the model's predictions.
- What evidence would resolve it: A case study demonstrating how the framework handles scenarios where different rules pull the model's predictions in opposing directions, and showing how the framework resolves these conflicts to maintain model stability and accuracy.

### Open Question 2
- Question: What is the theoretical relationship between the proposed rule importance framework and traditional regularization techniques in machine learning?
- Basis in paper: [explicit] The paper mentions that the framework can guide regularization parameter tuning, reducing prediction error by 74% in a multivariable equation example, but doesn't provide a detailed theoretical connection between rule importance and regularization.
- Why unresolved: The paper demonstrates practical benefits of using the framework for regularization but doesn't explore the underlying mathematical relationship between measuring rule importance and traditional regularization approaches like L1/L2 regularization or dropout.
- What evidence would resolve it: A theoretical analysis showing how the rule importance framework relates to existing regularization theory, possibly demonstrating that certain rule importance calculations correspond to specific forms of regularization penalties.

### Open Question 3
- Question: How does the computational complexity of the rule importance framework scale with the number of rules and data volume?
- Basis in paper: [explicit] The paper mentions that calculating rule importance requires training 2^N models and proposes a pre-training strategy to reduce computational cost, but doesn't provide detailed complexity analysis.
- Why unresolved: While the paper addresses computational efficiency through pre-training and mentions that Monte Carlo methods could accelerate calculations, it doesn't provide formal complexity analysis of how the framework scales with increasing numbers of rules or data volume.
- What evidence would resolve it: A comprehensive complexity analysis showing how the framework's computational requirements grow with different parameters (number of rules, data volume, network architecture), along with empirical benchmarks demonstrating actual runtime scaling behavior.

## Limitations
- Computational scalability remains a significant challenge, as calculating Shapley values requires training 2^N models for N rules
- The framework assumes rules are correctly specified; improperly formulated rules could lead to misleading importance estimates
- Results are primarily validated on relatively simple PDEs, raising questions about applicability to more complex real-world problems

## Confidence
- **High Confidence**: The core Shapley value framework for measuring rule importance is mathematically sound and the observed trend that rule importance decreases with more data in in-distribution tasks is well-supported
- **Medium Confidence**: The claim about global vs local rules in out-of-distribution prediction is plausible but requires more diverse test cases for robust validation
- **Medium Confidence**: The 74% error reduction claim is based on a single multivariable equation example and needs replication across different problem types

## Next Checks
1. Test the framework on more complex PDEs with multiple interacting terms to verify scalability and robustness
2. Conduct ablation studies removing individual rules to validate that Shapley-calculated importance correlates with actual performance degradation
3. Evaluate performance on real-world data from multiple domains (e.g., fluid dynamics, materials science) to assess generalizability beyond synthetic problems