---
ver: rpa2
title: 'Establishing Trustworthiness: Rethinking Tasks and Model Evaluation'
arxiv_id: '2310.05442'
source_url: https://arxiv.org/abs/2310.05442
tags:
- language
- knowledge
- arxiv
- llms
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper argues for rethinking NLP evaluation by centering trustworthiness,\
  \ driven by the shift from compartmentalized tasks to general-purpose LLMs. It proposes\
  \ knowledge facets as a framework to understand a model\u2019s functional capacity\
  \ and origin, aiming to establish trust in model predictions."
---

# Establishing Trustworthiness: Rethinking Tasks and Model Evaluation

## Quick Facts
- arXiv ID: 2310.05442
- Source URL: https://arxiv.org/abs/2310.05442
- Reference count: 26
- Primary result: Proposes knowledge facets framework for trustworthy LLM evaluation

## Executive Summary
This paper argues for rethinking NLP evaluation by centering trustworthiness, driven by the shift from compartmentalized tasks to general-purpose LLMs. It proposes knowledge facets as a framework to understand a model's functional capacity and origin, aiming to establish trust in model predictions. The paper outlines desiderata for trustworthy LLMs, including knowledge about model input, behavior, evaluation protocols, and data origin. It advocates for faceted evaluation, emphasizing qualitative analysis, diagnostic benchmarks, and transparency about data provenance.

## Method Summary
The paper reviews existing compartmentalized approaches for understanding the origins of a model's functional capacity and proposes knowledge facets as a framework for evaluating trustworthiness. It recommends multi-faceted evaluation protocols combining quantitative and qualitative analyses, diagnostic benchmarks with standardized protocols, and transparency about data provenance. The method emphasizes understanding both what models can do and how/where they learned these capabilities.

## Key Results
- Traditional NLP tasks with specialized architectures and evaluation protocols are being replaced by general-purpose LLMs that accept natural language instructions
- Trust requires knowledge of both functional capacity AND origin (per Hays 1979 framework)
- Multi-faceted evaluation protocols combining quantitative and qualitative analyses are needed to reveal hidden capabilities and limitations
- Data provenance transparency is essential to avoid overestimation of LLM capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The shift from task-specific to general-purpose LLMs undermines traditional evaluation by breaking the input-output formalization cycle.
- Mechanism: Traditional NLP tasks are formalized into datasets with machine-readable inputs/outputs and corresponding metrics. LLMs bypass this by accepting natural language instructions and generating outputs without explicit task formalization, making evaluation protocols less applicable.
- Core assumption: The "compartmentalized NLP paradigm" (Figure 1) accurately represents how trust was established in traditional systems through clear formalization and evaluation.
- Evidence anchors:
  - [abstract] "Traditionally, facets of linguistic intelligence have been compartmentalized into tasks with specialized model architectures and corresponding evaluation protocols."
  - [section] "In the following, we aim to provide a brief history of central trust desiderata (D1-4), discussing how our knowledge of functional capacity and its origins has changed over time."
  - [corpus] Weak/no direct evidence found in corpus about the breakdown of formalization cycles.

### Mechanism 2
- Claim: Trust requires knowledge of both functional capacity AND origin, as per Hays (1979).
- Mechanism: The paper operationalizes trust through "knowledge facets" - factors that improve understanding of a model's functional capacity (what it can do) and knowledge of origin (how/where it learned this). This dual knowledge enables routing instances to appropriate models and establishing confidence in predictions.
- Core assumption: Hays (1979)'s definition that "Trust arises from knowledge of origin as well as from knowledge of functional capacity" is the correct operational framework for LLM trustworthiness.
- Evidence anchors:
  - [abstract] "we review existing compartmentalized approaches for understanding the origins of a model's functional capacity"
  - [section] "we denote with knowledge facets (henceforth, facets) all factors that improve our knowledge of functional capacity and knowledge of origin."
  - [corpus] No direct corpus evidence found about Hays (1979) definition application.

### Mechanism 3
- Claim: Multi-faceted evaluation protocols combining quantitative and qualitative analyses improve trust by revealing hidden capabilities and limitations.
- Mechanism: The paper argues that standalone metrics obscure findings from negative results and fail to capture which sub-problems are better/worse solved. Diagnostic benchmarks with standardized qualitative protocols enable comparable meta-analysis and reveal linguistic phenomena that drive performance.
- Core assumption: "We therefore cannot trust reported SOTA results as long as the facets that explain how well sub-problems are solved remain hidden."
- Evidence anchors:
  - [abstract] "We advocate for faceted evaluation, emphasizing qualitative analysis, diagnostic benchmarks, and transparency about data provenance."
  - [section] "We therefore cannot trust reported SOTA results as long as the facets that explain how well sub-problems are solved remain hidden."
  - [corpus] Weak/no direct evidence found in corpus about the effectiveness of multi-faceted protocols.

## Foundational Learning

### Task compartmentalization in traditional NLP
- Why needed here: Understanding the shift from compartmentalized tasks to general-purpose LLMs is central to grasping why evaluation protocols need rethinking
- Quick check question: How did traditional NLP tasks differ from the instruction-based approach used by contemporary LLMs?

### Knowledge facets framework
- Why needed here: The paper's central contribution is operationalizing trust through knowledge facets that capture both functional capacity and origin knowledge
- Quick check question: What are the two main types of knowledge that the paper argues are necessary for establishing trust in LLMs?

### Cross-X evaluation setup
- Why needed here: The paper advocates for explicit cross-X setups (data sampling, tasks, or capabilities) to ensure transparency about data provenance and avoid overestimation of LLM capabilities
- Quick check question: Why does the paper call for always employing a cross-X setup when evaluating LLMs?

## Architecture Onboarding

### Component map
- Knowledge facet identification system (descriptive + inferred knowledge)
- Multi-faceted evaluation framework (quantitative + qualitative protocols)
- Data provenance tracking mechanism
- Skill attribution and explainability modules
- Diagnostic benchmark augmentation system

### Critical path
1) Define knowledge facets → 2) Implement evaluation protocols → 3) Track data provenance → 4) Develop explainability tools → 5) Validate through diagnostic benchmarks

### Design tradeoffs
Comprehensive multi-faceted evaluation provides deeper insights but requires more resources than traditional metric-based evaluation; transparency about data provenance may reveal limitations but is essential for trust

### Failure signatures
Overreliance on quantitative metrics alone; failure to track data provenance; inability to explain skill utilization during inference; evaluation protocols that don't capture real-world usage patterns

### First 3 experiments
1. Implement a diagnostic benchmark augmentation system that adds linguistic phenomenon annotations to existing datasets and evaluate how this affects trust assessment
2. Develop a skill attribution module that identifies which model parameters are activated during specific inference tasks and test its effectiveness in explaining predictions
3. Create a cross-X evaluation setup that stratifies test data by capability type (rather than just domain/language) and measure its impact on estimating true LLM performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop evaluation protocols that effectively assess the trustworthiness of large language models (LLMs) across diverse, real-world tasks beyond benchmark datasets?
- Basis in paper: [explicit] The paper discusses the limitations of current evaluation methods and the need for more holistic qualitative evaluation protocols.
- Why unresolved: Current evaluation methods primarily focus on quantitative benchmarks, which may not capture the full range of LLM capabilities or their potential failures in real-world scenarios.
- What evidence would resolve it: Development and validation of comprehensive evaluation frameworks that incorporate qualitative analysis, diagnostic benchmarks, and transparency about data provenance, along with empirical studies demonstrating their effectiveness in assessing LLM trustworthiness.

### Open Question 2
- Question: How can we gain insights into the skills employed by LLMs during inference, and how do these skills relate to the skills required to solve specific tasks?
- Basis in paper: [explicit] The paper emphasizes the importance of understanding both the skills required to solve tasks and the skills actually employed by LLMs during inference.
- Why unresolved: Current methods for analyzing LLM skills are often limited to specific tasks or rely on task-specific data, making it difficult to generalize insights across diverse tasks.
- What evidence would resolve it: Development of novel techniques for analyzing LLM skills that are task-agnostic and can provide insights into the inference process, along with empirical studies demonstrating their effectiveness in understanding LLM behavior.

### Open Question 3
- Question: How can we ensure transparency about data provenance in LLM training and evaluation, and what impact does this have on trust in LLM predictions?
- Basis in paper: [explicit] The paper highlights the importance of knowledge about data origin and its influence on model behavior, emphasizing the need for open access to data and transparency about data provenance.
- Why unresolved: Current practices often lack transparency about data provenance, making it difficult to assess the potential biases or limitations of LLMs trained on such data.
- What evidence would resolve it: Development of standardized practices for documenting and sharing data provenance information, along with empirical studies demonstrating the impact of data provenance transparency on trust in LLM predictions.

## Limitations

- The paper's claims about the breakdown of traditional evaluation paradigms lack direct empirical evidence from the corpus, making the foundational argument partially speculative
- The application of Hays (1979)'s trust framework to LLMs is asserted but not empirically validated
- The effectiveness of proposed multi-faceted evaluation protocols in practice remains untested, with the paper providing theoretical justification rather than experimental validation

## Confidence

- **High confidence**: The observation that LLMs operate differently from traditional task-specific models (accepts natural language instructions vs. formalized inputs/outputs)
- **Medium confidence**: The assertion that understanding both functional capacity AND origin is necessary for trust, based on Hays (1979) but not directly validated in the LLM context
- **Low confidence**: The claim that current SOTA results cannot be trusted without multi-faceted evaluation protocols, as this requires empirical comparison that is not provided

## Next Checks

1. Conduct a controlled experiment comparing trust assessments using traditional quantitative metrics versus multi-faceted protocols (including qualitative analysis) on the same LLM predictions to measure differential trust outcomes.

2. Design a study testing whether knowledge of model origin (training data provenance) independently affects trust calibration, separate from functional capacity knowledge, to validate the dual-knowledge framework.

3. Implement a diagnostic benchmark augmentation system that adds linguistic phenomenon annotations to existing datasets and measure whether this improves practitioners' ability to correctly route instances to appropriate models.