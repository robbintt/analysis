---
ver: rpa2
title: 'ComOM at VLSP 2023: A Dual-Stage Framework with BERTology and Unified Multi-Task
  Instruction Tuning Model for Vietnamese Comparative Opinion Mining'
arxiv_id: '2312.09000'
source_url: https://arxiv.org/abs/2312.09000
tags:
- comparative
- task
- extract
- language
- review
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper describes a winning approach for the ComOM shared task
  at VLSP 2023, focused on extracting comparative opinions from Vietnamese product
  reviews. The task involves two sub-tasks: Comparative Sentence Identification (CSI)
  and Comparative Element Extraction (CEE).'
---

# ComOM at VLSP 2023: A Dual-Stage Framework with BERTology and Unified Multi-Task Instruction Tuning Model for Vietnamese Comparative Opinion Mining

## Quick Facts
- arXiv ID: 2312.09000
- Source URL: https://arxiv.org/abs/2312.09000
- Reference count: 6
- Primary result: Winning approach at VLSP 2023 ComOM shared task with macro F1-score 0.2373 and micro F1-score 0.2952

## Executive Summary
This paper presents a winning approach for the ComOM shared task at VLSP 2023, which focuses on extracting comparative opinions from Vietnamese product reviews. The task involves two sub-tasks: Comparative Sentence Identification (CSI) to classify whether a review is comparative, and Comparative Element Extraction (CEE) to extract quintuplets containing subject, object, aspect, predicate, and label. The proposed method uses a dual-stage framework where CSI filters non-comparative reviews using a BERT-based model, followed by CEE using an end-to-end multi-task instruction tuning model with data augmentation. The approach achieved top performance on the private test set, outperforming other competitors through the combination of multi-task learning and data augmentation strategies.

## Method Summary
The method employs a two-stage framework for Vietnamese comparative opinion mining. Stage 1 uses PhoBERT v2 fine-tuned for binary classification to identify comparative reviews (CSI). Stage 2 employs viT5-large with multi-task instruction tuning and data augmentation to extract quintuplets (CEE). The data augmentation strategy involves replacing subject, object, and aspect elements with values from predefined word sets while swapping predicates and labels together. Training uses Hugging Face Trainer API with AdamW optimizer, linear warmup, batch sizes of 32 for CSI and 16 for CEE, and learning rates of 5e-5 and 2e-5 respectively.

## Key Results
- Achieved macro F1-score of 0.2373 and micro F1-score of 0.2952 on private test set
- Ranked first among all competitors in VLSP 2023 ComOM shared task
- Multi-task instruction tuning with data augmentation outperformed single-task fine-tuning baselines
- viT5-large (monolingual) showed better performance than mT5 (multilingual) for Vietnamese language tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage framework reduces error propagation by isolating comparative sentence identification from element extraction.
- Mechanism: The first stage filters non-comparative reviews using PhoBERT v2, preventing the CEE model from processing irrelevant inputs. This separation ensures that the second-stage generative model only needs to handle true comparative cases.
- Core assumption: Accurate CSI filtering improves CEE performance by reducing noise and ambiguity in the input.
- Evidence anchors: [abstract] The paper explicitly states the two-stage design: "we proposed a two-stage framework, which consists of: (1) the first stage, which identifies comparative sentences, and (2) the second stage, which extracts five comparative elements." [section 2.2] "we implement a binary classification model based on fine-tuning encoder-type BERT pre-trained language models" to identify comparative reviews.
- Break condition: If CSI performance is poor (low recall), relevant comparative reviews may be filtered out, leading to missed extraction opportunities in CEE.

### Mechanism 2
- Claim: Multi-task instruction tuning leverages inter-element correlations in quintuplets to improve extraction accuracy.
- Mechanism: By training viT5-large on nine instruction templates that require predicting different subsets of the quintuple elements, the model learns shared representations and dependencies among subject, object, aspect, predicate, and label.
- Core assumption: There is a strong correlation between the presence and ordering of elements in comparative quintuplets.
- Evidence anchors: [abstract] "we present an end-to-end multi-task instruction tuning model to predict the quintuples" and "we develop a fuzzy string-matching algorithm combined with heuristic rules." [section 2.3] "we hypothesise that there is a correlation information between sub-tasks and comparative elements in the quintuplets." [section 3.2] Comparison with baselines shows multi-task instruction tuning significantly outperforms single-task fine-tuning.
- Break condition: If the correlation assumption is invalid (e.g., elements are conditionally independent), multi-task learning may introduce noise rather than signal.

### Mechanism 3
- Claim: Data augmentation increases model robustness to element variation in comparative reviews.
- Mechanism: The augmentation strategy replaces subject, object, and aspect with values from predefined word sets, while swapping predicates and labels together, thereby diversifying the training distribution without altering the underlying comparative structure.
- Core assumption: The model fails when encountering novel surface forms of elements that were present in training but with different contexts.
- Evidence anchors: [section 2.4] "we introduce a simple data augmentation to diversify the training set" and describe the replacement strategy. [section 3.2] Results show data augmentation improves baseline models and multi-task instruction tuning.
- Break condition: If augmentation introduces unrealistic or contradictory examples, it may confuse the model and degrade performance.

## Foundational Learning

- Concept: Fine-tuning vs. prompt engineering for pre-trained models
  - Why needed here: The approach uses both fine-tuning (BERT models for CSI) and prompt-based instruction tuning (T5 models for CEE). Understanding when each is appropriate is crucial for architecture decisions.
  - Quick check question: When would you choose fine-tuning over prompt engineering for a classification task?

- Concept: Encoder-decoder vs. encoder-only architectures
  - Why needed here: CSI uses encoder-only (PhoBERT), while CEE uses encoder-decoder (T5/viT5). The choice affects task formulation and model capacity.
  - Quick check question: What are the key differences in how encoder-only and encoder-decoder models process input and generate output?

- Concept: Multi-task learning and shared representations
  - Why needed here: The CEE stage trains on nine different instruction templates simultaneously. Understanding how tasks share representations is key to interpreting performance gains.
  - Quick check question: How does multi-task learning potentially improve generalization compared to single-task learning?

## Architecture Onboarding

- Component map: Reviews → CSI stage (PhoBERT v2 binary classification) → CEE stage (viT5-large multi-task instruction tuning) → Quintuple output
- Critical path: 1) Preprocess input review (remove multiple spaces) 2) CSI classification using PhoBERT v2 3) If comparative, pass to CEE stage 4) CEE stage generates quintuplets using multi-task instruction tuning 5) Post-process output using fuzzy matching and heuristics
- Design tradeoffs: Two-stage vs. end-to-end: Two-stage reduces error propagation but adds complexity. Fine-tuning vs. prompt engineering: Fine-tuning offers better control for CSI, while prompts enable flexible generation for CEE. Monolingual vs. multilingual models: viT5 (monolingual) outperforms mT5 (multilingual) for Vietnamese
- Failure signatures: Low CSI recall: Many comparative reviews incorrectly classified as non-comparative. CEE generation errors: Missing elements, incorrect ordering, or hallucinated content. Data augmentation issues: Unrealistic examples or imbalance in augmented data
- First 3 experiments: 1) Baseline CSI: Fine-tune PhoBERT v2 on original training data, evaluate on public test 2) CEE baseline: Fine-tune viT5-large with single-task template, compare to multi-task 3) Augmentation impact: Train CEE with and without augmented data, measure performance difference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed data augmentation strategy compare to other augmentation techniques (e.g., back-translation, synonym replacement) for Vietnamese comparative opinion mining?
- Basis in paper: [inferred] The paper introduces a simple data augmentation strategy but does not compare it to other techniques.
- Why unresolved: The authors only tested their own augmentation method and did not benchmark against alternative approaches.
- What evidence would resolve it: Experiments comparing multiple augmentation techniques on the same dataset would show which method yields the best performance improvements.

### Open Question 2
- Question: Would incorporating more advanced linguistic features, such as dependency parsing or semantic role labeling, further improve the extraction of comparative quintuples?
- Basis in paper: [inferred] The paper focuses on BERTology models and multi-task instruction tuning but does not explore additional linguistic features.
- Why unresolved: The authors did not investigate the impact of incorporating richer linguistic information beyond the BERT representations.
- What evidence would resolve it: Experiments comparing models with and without additional linguistic features would demonstrate their effectiveness.

### Open Question 3
- Question: How does the performance of the proposed method scale with increasing dataset size, and what is the optimal amount of training data for this task?
- Basis in paper: [inferred] The paper applies data augmentation to increase training data but does not analyze the relationship between dataset size and model performance.
- Why unresolved: The authors did not conduct experiments varying the amount of training data to determine the point of diminishing returns.
- What evidence would resolve it: Training and evaluating models on datasets of varying sizes would reveal the impact of dataset size on performance and the optimal training data amount.

## Limitations

- Evaluation relies on a single dataset (VLSP 2023 ComOM) with limited scale, making generalization uncertain
- Two-stage architecture introduces potential error propagation that isn't quantified
- Data augmentation strategy lacks rigorous ablation studies to isolate its specific contribution

## Confidence

- High confidence: Architectural framework and implementation details (Stage 1 and Stage 2 clearly described)
- Medium confidence: Mechanism claims about multi-task instruction tuning benefits, relying on assumed correlations between elements without direct empirical validation
- Low confidence: Data augmentation effectiveness due to limited description of the augmentation pipeline and no statistical significance testing of results

## Next Checks

1. Conduct error analysis on CSI stage to measure false negative rate and quantify how many comparative reviews are incorrectly filtered out
2. Perform ablation studies comparing multi-task instruction tuning against single-task fine-tuning on identical data splits and augmentation settings
3. Test the framework on additional Vietnamese comparative opinion datasets or translated versions of English datasets to assess cross-domain robustness