---
ver: rpa2
title: 'FAMO: Fast Adaptive Multitask Optimization'
arxiv_id: '2306.03792'
source_url: https://arxiv.org/abs/2306.03792
tags:
- task
- famo
- learning
- tasks
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FAMO, a fast and efficient multitask optimization
  method that balances task losses using O(1) space and time complexity. FAMO dynamically
  adjusts task weights based on loss history, avoiding the need to compute and store
  all task gradients.
---

# FAMO: Fast Adaptive Multitask Optimization

## Quick Facts
- arXiv ID: 2306.03792
- Source URL: https://arxiv.org/abs/2306.03792
- Reference count: 40
- The paper introduces FAMO, a fast and efficient multitask optimization method that balances task losses using O(1) space and time complexity.

## Executive Summary
FAMO addresses the conflicting gradients problem in multitask learning by dynamically adjusting task weights based on loss history, avoiding the need to compute and store all task gradients. It achieves comparable or superior performance to state-of-the-art gradient manipulation methods on supervised and reinforcement learning benchmarks while being significantly more computationally efficient. The method shows strong results on standard multitask datasets like NYU-v2, QM-9, and CelebA, and performs well in multitask reinforcement learning tasks.

## Method Summary
FAMO is a multitask optimization method that balances task losses using a fast, memory-efficient approach. It maintains a logit vector w that parameterizes task weights via softmax and updates these weights based on the difference between current and previous task losses. The method includes an ℓ₂ regularization term to stabilize weight updates and renormalizes the weighted gradient to prevent exploding gradients. FAMO operates in O(1) space and time complexity, making it scalable to large models and many tasks.

## Key Results
- FAMO achieves comparable or superior multitask performance to state-of-the-art methods on NYU-v2, QM-9, and CelebA datasets
- The method demonstrates O(1) space and time complexity, making it significantly more efficient than existing gradient manipulation methods
- FAMO performs well in multitask reinforcement learning tasks, achieving strong results on the MetaWorld-10 benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FAMO ensures balanced loss improvement across tasks by updating task weights based on historical loss changes, without needing to compute or store all task gradients.
- Mechanism: FAMO maintains a logit vector w that parameterizes task weights via softmax. At each step, it approximates the gradient of a balancing objective using the difference between current and previous task losses, updating w in O(1) space/time. The weighted loss is then normalized to prevent exploding gradients.
- Core assumption: The change in log-losses over small steps approximates the gradient of the balancing objective; past task weighting can be amortized over the optimization trajectory.
- Evidence anchors:
  - [abstract] "FAMO dynamically adjusts task weights based on loss history, avoiding the need to compute and store all task gradients."
  - [section] "FAMO leverages the change in task losses to update the task weighting iteratively, while ensuring that all task losses drop at approximately equal rates."
  - [corpus] Weak: No explicit mention of FAMO in corpus; related works discuss multitask optimization but not the specific O(1) update trick.
- Break condition: If the approximation error in using loss differences instead of true gradients becomes too large, the balancing may fail and some tasks may be under-optimized.

### Mechanism 2
- Claim: The regularization on w (via ℓ₂ decay) stabilizes task weighting updates over time, preventing oscillations or collapse to extreme weights.
- Mechanism: An ℓ₂ penalty γ||w||² is added to the logit update, shrinking w toward zero each step. This smooths the weighting trajectory and avoids over-reliance on noisy recent gradients.
- Core assumption: Past task weighting should gradually decay in influence so that the method adapts to changing loss landscapes rather than sticking to outdated weights.
- Evidence anchors:
  - [section] "we introduce an ℓ2 regularization term that decays the past updates on w: wt←wt−1−β(δ+ γwt−1)."
  - [abstract] "FAMO shows strong results on standard multitask datasets... while being significantly more computationally efficient."
  - [corpus] Weak: No direct discussion of ℓ₂ regularization in related works; mentions task affinity estimation but not decay-based weighting.
- Break condition: If γ is too large, the weights will be overly suppressed and the method may revert to uniform weighting, losing the balancing benefit.

### Mechanism 3
- Claim: Normalizing the weighted gradient by ct ensures that FAMO's effective step size matches that of standard gradient descent, making it compatible with existing learning rate schedules.
- Mechanism: After computing the weighted gradient (∇log ∆t)z, FAMO divides by ct = (z ⊘ ∆t)ᵀ1 to renormalize the effective step into the probability simplex, ensuring stable per-step progress.
- Core assumption: Without renormalization, small task losses could produce huge gradients, destabilizing learning; normalization preserves the relative geometry of the update.
- Evidence anchors:
  - [section] "we re-normalize the task weighting back into the probability simplex by dividing a constant ct per step."
  - [abstract] "FAMO decreases task losses in a balanced way using O(1) space and time."
  - [corpus] Weak: No mention of normalization tricks in related works; focuses on task affinity or loss weighting but not this specific renormalization.
- Break condition: If normalization is omitted or incorrectly implemented, the method could suffer from gradient explosion on low-loss tasks, breaking convergence.

## Foundational Learning

- Concept: Multitask learning (MTL) and conflicting gradients
  - Why needed here: FAMO is designed to solve the core MTL problem where gradients for different tasks pull the model in incompatible directions, causing some tasks to be under-optimized.
  - Quick check question: What is a conflicting gradient in MTL, and why does naively averaging task gradients fail?

- Concept: Dynamic task weighting via softmax parameterization
  - Why needed here: FAMO represents task weights as unconstrained logits w and uses softmax to map them into the simplex, enabling efficient gradient updates.
  - Quick check question: How does using softmax over logits differ from directly optimizing weights in the simplex?

- Concept: Dual formulation of gradient balancing objectives
  - Why needed here: The method's theoretical grounding relies on viewing the balancing problem as a dual optimization over task weights, ensuring equal rate of loss improvement.
  - Quick check question: What is the dual form of the balancing objective, and why does solving it yield equal loss improvement rates?

## Architecture Onboarding

- Component map:
  - Task loss vector L(θ) ∈ Rᴷ≥0
  - Logit vector w ∈ Rᴷ (unconstrained)
  - Softmax weights z = Softmax(w) ∈ Δᴷ
  - Loss-normalized vector ∆ = L + ε
  - Normalization constant c = (z ⊘ ∆)ᵀ1
  - Weighted loss: (∆.log() * z / c).sum()

- Critical path:
  1. Forward pass: compute all task losses L
  2. Apply FAMO.get_weighted_loss to get scalar loss
  3. Backward pass through scalar loss
  4. Call FAMO.update with prev_losses and curr_losses to adjust w

- Design tradeoffs:
  - O(1) space/time vs. exact gradient balancing: FAMO trades some precision for efficiency.
  - Regularization γ: balances stability vs. adaptability.
  - Small ε: avoids division by zero but can affect numerical precision.

- Failure signatures:
  - Loss collapse on a subset of tasks → check if weights have collapsed or normalization is broken.
  - Unstable training → reduce γ or increase ε.
  - No improvement over LS → check if update frequency or step size is too low.

- First 3 experiments:
  1. Run FAMO on a 2-task synthetic problem (like in Section 5.1) and visualize loss trajectories vs. MGDA to confirm balancing behavior.
  2. Compare training time per epoch vs. LS and NASH MTL on QM-9 (11 tasks) to verify O(1) efficiency claim.
  3. Perform an ablation over γ on NYU-v2 to see stability/robustness trade-off.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the regularization coefficient γ affect FAMO's performance across different MTL benchmarks?
- Basis in paper: [explicit] The paper mentions an ablation study on γ in Section 5.4, showing results with different values of γ on the four supervised MTL benchmarks.
- Why unresolved: The paper states that choosing the right regularization coefficient can be crucial, but the optimal value may vary depending on the specific MTL problem and dataset characteristics.
- What evidence would resolve it: Systematic experiments varying γ across a wider range of MTL problems and analyzing its impact on performance metrics like ∆m% and MR.

### Open Question 2
- Question: Can FAMO's approximation method be extended to other gradient manipulation methods like PCG RAD and IMTL-G?
- Basis in paper: [inferred] The paper discusses in Appendix B that while it's possible to derive fast approximation algorithms for some gradient manipulation methods, it often involves much more complicated computation compared to FAMO.
- Why unresolved: The paper only provides a detailed analysis for CAG RAD and NASH MTL, leaving open the question of whether similar approximations can be derived for other methods.
- What evidence would resolve it: Developing and testing approximation methods for PCG RAD and IMTL-G, comparing their computational efficiency and performance against FAMO and the original methods.

### Open Question 3
- Question: How does FAMO perform in MTL problems with a large number of tasks (K >> 40) compared to existing methods?
- Basis in paper: [explicit] The paper demonstrates FAMO's performance on benchmarks with up to 40 tasks (CelebA) and mentions that existing methods require O(K) space and time, making them slow and memory inefficient for large K.
- Why unresolved: While FAMO is shown to be efficient for moderate numbers of tasks, its scalability and performance for very large K remains untested.
- What evidence would resolve it: Extensive experiments on MTL problems with hundreds or thousands of tasks, comparing FAMO's performance, memory usage, and training time against existing methods.

## Limitations

- The approximation of gradients using loss differences may not accurately capture the balancing objective in complex, high-dimensional optimization landscapes
- The method's performance under extreme task loss disparities or highly conflicting gradients is not fully characterized
- The optimal regularization coefficient γ may vary significantly across different MTL problems and datasets

## Confidence

- **High confidence**: FAMO achieves comparable or superior multitask performance to state-of-the-art methods on standard benchmarks (NYU-v2, QM-9, CelebA).
- **Medium confidence**: The O(1) space/time efficiency claim holds in practice, given the linear complexity of the updates.
- **Medium confidence**: The balancing mechanism (via loss-difference gradients) is effective, but the approximation's accuracy in complex settings is uncertain.
- **Low confidence**: The method's behavior under extreme task loss disparities or highly conflicting gradients is not fully characterized.

## Next Checks

1. **Robustness to loss scale**: Test FAMO on a synthetic MTL problem with deliberately disparate loss scales to see if the approximation breaks down.
2. **Ablation of regularization**: Run FAMO on QM-9 with γ set to 0 and a large value (e.g., 10) to isolate the effect of ℓ₂ regularization on stability and balancing.
3. **Comparison with true gradients**: For a small MTL problem, compare FAMO's task weight updates against those computed using exact gradients of the balancing objective to quantify approximation error.