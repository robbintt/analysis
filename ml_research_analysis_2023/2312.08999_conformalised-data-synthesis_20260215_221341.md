---
ver: rpa2
title: Conformalised data synthesis
arxiv_id: '2312.08999'
source_url: https://arxiv.org/abs/2312.08999
tags:
- data
- samples
- conformal
- prediction
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The proliferation of increasingly complicated Deep Learning architectures
  demands increasingly large datasets, which are not always readily available. To
  address this challenge, we have designed a unique confident data synthesis algorithm
  based on a novel extension of the Conformal Prediction framework.
---

# Conformalised data synthesis

## Quick Facts
- arXiv ID: 2312.08999
- Source URL: https://arxiv.org/abs/2312.08999
- Authors: 
- Reference count: 40
- Key outcome: Algorithm guarantees synthetic data quality to user-specified significance level, improving Deep Learning performance by up to +65% F1-score on benchmark datasets.

## Executive Summary
This paper introduces a novel conformalized data synthesis algorithm that generates high-quality synthetic samples by identifying high-confidence regions in feature space using Conformal Prediction's p-value framework. The method guarantees that generated data maintains statistical similarity to original training data up to a user-specified significance level. By limiting synthesis to these high-confidence regions, the algorithm produces synthetic samples that statistically represent the original data distribution. Extensive experiments on five benchmark datasets demonstrate that training sets extended with this confident synthesized data consistently match or exceed the performance of original training sets.

## Method Summary
The conformalized data synthesis algorithm works by first constructing a grid representation of the feature space and calculating non-conformity scores for each grid point using nearest neighbor distances. Conformal Prediction is then applied to calculate p-values, identifying high-confidence regions where p > ε. Synthetic samples are generated by sampling grid points from these regions and assigning class labels based on the original data distribution. The algorithm is tested by training feedforward neural networks on original training sets, synthetic-only sets, and extended sets, comparing performance using F1-score, precision, and recall metrics across multiple trials.

## Key Results
- Training sets extended with confident synthesized data performed at least as well as original sets across all benchmark datasets
- Significant performance improvements observed, with F1-score improvements up to +65% on certain datasets
- The algorithm effectively addresses data scarcity and class imbalance issues while guaranteeing synthetic data quality
- Five benchmark datasets (MNIST, MSHRM, WINE, USPS, plus toy dataset) validated the approach across diverse data characteristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The conformalized data synthesis algorithm generates synthetic samples with guaranteed quality based on feature space confidence.
- Mechanism: The algorithm identifies high-confidence regions in the feature space using Conformal Prediction's p-value framework. Grid points in these regions are sampled as synthetic data points, ensuring they are statistically similar to the original training data up to a user-specified significance level.
- Core assumption: The feature space grid points can be evaluated for their representativeness of each class using label-conditional non-conformity measures.
- Evidence anchors:
  - [abstract] "We incorporate the confidence of feature space regions into the synthesis process, guaranteeing the generated data's quality to a user-specified significance level ϵ."
  - [section] "By limiting synthesis to the high-confidence regions, we statistically guarantee the generated data's quality to a user-specified significance level ϵ."
  - [corpus] No direct evidence in corpus. The corpus focuses on other topics like generative models, deep learning operators, and data scarcity.
- Break condition: If the feature space is too sparse or the non-conformity measure poorly represents class similarity, the confidence regions may not accurately reflect the data distribution.

### Mechanism 2
- Claim: The significance level ϵ controls the trade-off between the quantity and quality of synthetic samples.
- Mechanism: A larger ϵ tightens the high-confidence regions, resulting in fewer but higher-quality synthetic samples. A smaller ϵ widens the regions, generating more samples but potentially including less representative data.
- Core assumption: The significance level ϵ effectively regulates the inclusion of feature space points in the high-confidence regions.
- Evidence anchors:
  - [abstract] "By limiting synthesis to the high-confidence regions, we statistically guarantee the generated data's quality to a user-specified significance level."
  - [section] "The larger ϵ is, the more tightly controlled the confidence regions are, and the more we exclude low-confidence synthetic data."
  - [corpus] No direct evidence in corpus.
- Break condition: If the original dataset is highly imbalanced or the feature space is complex, the relationship between ϵ and sample quality may not be linear.

### Mechanism 3
- Claim: The algorithm improves Deep Learning performance by extending training sets with high-quality synthetic samples.
- Mechanism: The synthetic samples are generated from high-confidence feature space regions, which are statistically representative of the original data. Adding these samples to the training set strengthens the model's learned feature space representations and improves generalization.
- Core assumption: The Deep Learning model benefits from additional training samples that are statistically similar to the original data.
- Evidence anchors:
  - [abstract] "In all trials, training sets extended with our confident synthesised data performed at least as well as the original, and frequently significantly improved Deep Learning performance by up to +65% F1-score."
  - [section] "In all trials, training sets extended with our confident synthesised data performed at least as well as the original set and frequently significantly improved Deep Learning performance by up to 61 percentage points F1-score."
  - [corpus] No direct evidence in corpus.
- Break condition: If the synthetic samples are too similar to the original data, they may not provide new information and could lead to overfitting.

## Foundational Learning

- Concept: Conformal Prediction framework
  - Why needed here: It provides a statistical foundation for measuring the confidence of feature space regions, which is crucial for generating high-quality synthetic data.
  - Quick check question: What is the key property of Conformal Prediction that guarantees the validity of its confidence measures?
- Concept: Non-conformity measures
  - Why needed here: They evaluate the similarity of grid points to the original training data, determining which points fall into high-confidence regions.
  - Quick check question: How does the choice of non-conformity measure affect the identification of high-confidence feature space regions?
- Concept: Significance level ϵ
  - Why needed here: It controls the trade-off between the quantity and quality of synthetic samples, allowing users to balance the benefits of data augmentation.
  - Quick check question: What happens to the high-confidence regions and the number of synthetic samples as the significance level ϵ increases?

## Architecture Onboarding

- Component map:
  Grid generation -> Non-conformity measure calculation -> Conformal Prediction p-value computation -> Synthetic sample generation -> Deep Learning model training
- Critical path:
  1. Generate a grid representation of the feature space.
  2. Calculate non-conformity scores for grid points.
  3. Apply Conformal Prediction to identify high-confidence regions.
  4. Sample synthetic data points from high-confidence regions.
  5. Extend the training set with synthetic samples.
  6. Train a Deep Learning model on the extended training set.
- Design tradeoffs:
  - Grid step γ: A smaller grid step increases the resolution of the feature space but requires more computational power.
  - Significance level ϵ: A larger ϵ results in fewer but higher-quality synthetic samples, while a smaller ϵ generates more samples but potentially includes less representative data.
  - Non-conformity measure: The choice of measure affects how well the algorithm identifies high-confidence regions and can be tailored to the specific dataset and classification task.
- Failure signatures:
  - Poor performance on the test set: Indicates that the synthetic samples may not be representative of the original data distribution.
  - Overfitting: Suggests that the synthetic samples are too similar to the original data and do not provide new information.
  - High computational cost: Implies that the grid step is too small or the feature space is too large for efficient processing.
- First 3 experiments:
  1. Test the algorithm on a simple 2D toy dataset with clear class separation to verify that it can generate high-quality synthetic samples.
  2. Evaluate the algorithm's performance on a dataset with class imbalance to assess its ability to balance the training set.
  3. Apply the algorithm to a dataset with overlapping classes to examine its effectiveness in handling non-linearly separable data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed algorithm perform on non-tabular data such as images, time series, or text?
- Basis in paper: [explicit] The authors mention that their algorithm is compatible with any size dimensional input and demonstrate results on image data (MNIST, USPS), but do not evaluate on other data types like time series or text.
- Why unresolved: The paper only evaluates on tabular and image datasets. Non-tabular data often have different characteristics (e.g., temporal dependencies, sequential patterns) that may affect the performance of the conformal synthesis approach.
- What evidence would resolve it: Empirical results on diverse non-tabular datasets (e.g., speech, financial time series, text classification) showing the algorithm's effectiveness across different data modalities.

### Open Question 2
- Question: What is the optimal strategy for selecting the grid step size (γ) and significance level (ϵ) for a given dataset?
- Basis in paper: [explicit] The authors systematically investigate the influence of γ and ϵ on the synthesized data quality and model performance, but do not provide a general guideline for selecting these parameters.
- Why unresolved: The optimal values for γ and ϵ depend on the dataset's characteristics (e.g., dimensionality, class separability, sample size) and the desired trade-off between synthetic data quantity and quality.
- What evidence would resolve it: A study comparing different strategies for selecting γ and ϵ (e.g., grid search, cross-validation, domain knowledge) and their impact on the algorithm's performance across various datasets.

### Open Question 3
- Question: How does the proposed algorithm compare to other data synthesis techniques (e.g., GANs, VAEs) in terms of data quality and downstream task performance?
- Basis in paper: [explicit] The authors do not compare their algorithm to other data synthesis techniques, focusing solely on the advantages of their conformal approach.
- Why unresolved: There are many data synthesis techniques available, each with its own strengths and weaknesses. A comparison would help understand the relative merits of the proposed algorithm.
- What evidence would resolve it: Empirical results comparing the proposed algorithm to other data synthesis techniques on the same datasets, using the same evaluation metrics (e.g., data quality, model performance).

## Limitations
- The algorithm's performance on non-tabular data types (time series, text) remains untested despite theoretical compatibility
- Computational complexity may limit scalability to high-dimensional datasets due to feature space grid generation requirements
- The exact implementation details of non-conformity measures are not fully specified, potentially affecting reproducibility

## Confidence
- **High confidence**: The mechanism of using Conformal Prediction's p-value framework to identify high-confidence regions in feature space is theoretically sound and well-established in the literature.
- **Medium confidence**: The claim that significance level ϵ effectively controls the trade-off between synthetic sample quality and quantity is supported by experimental results, but the relationship may be dataset-dependent.
- **Medium confidence**: The improvement in Deep Learning performance through synthetic data augmentation is demonstrated empirically, though the magnitude of improvement (+65% F1-score) appears exceptionally high and may not generalize across all tasks.

## Next Checks
1. Reproduce on a simple 2D dataset: Implement the algorithm on a toy dataset with clear class boundaries to verify the synthetic data generation process and validate that high-confidence regions correspond to class-representative areas.

2. Sensitivity analysis of hyperparameters: Systematically test how different grid steps (γ) and significance levels (ϵ) affect the quality and quantity of synthetic samples across multiple datasets to confirm the claimed trade-offs.

3. Performance on diverse model architectures: Evaluate the algorithm's effectiveness when used to augment training data for different Deep Learning architectures (CNNs, transformers, etc.) to assess generalizability beyond the specific feedforward network used in the paper.