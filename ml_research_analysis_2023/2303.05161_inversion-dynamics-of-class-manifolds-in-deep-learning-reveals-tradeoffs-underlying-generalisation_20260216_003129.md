---
ver: rpa2
title: Inversion dynamics of class manifolds in deep learning reveals tradeoffs underlying
  generalisation
arxiv_id: '2303.05161'
source_url: https://arxiv.org/abs/2303.05161
tags:
- training
- data
- learning
- inversion
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Deep learning models excel at near-zero training error, but the
  geometric dynamics underlying this success remain unclear. This study analyzes the
  evolution of class manifold representations during training using gyration radii
  and inter-manifold distances as geometric observables.
---

# Inversion dynamics of class manifold in deep learning reveals tradeoffs underlying generalisation

## Quick Facts
- **arXiv ID:** 2303.05161
- **Source URL:** https://arxiv.org/abs/2303.05161
- **Reference count:** 40
- **Key outcome:** Deep learning models exhibit non-monotonic class manifold segregation dynamics during training, marked by an inversion epoch where further optimization increases class entanglement to classify remaining "stragglers"

## Executive Summary
Deep learning models excel at near-zero training error, but the geometric dynamics underlying this success remain unclear. This study analyzes the evolution of class manifold representations during training using gyration radii and inter-manifold distances as geometric observables. The key finding is a non-monotonic segregation dynamics: after an initial fast segregation phase, a slower expansion phase increases class entanglement, separated by an inversion epoch. This inversion point is remarkably stable under subsampling, different network initializations, and optimization algorithms. The inversion is triggered by a well-defined set of "straggler" data points—approximately 10-13% of the training set—that remain misclassified until this epoch.

## Method Summary
The authors train shallow two-layer fully connected networks (20 hidden units) on MNIST, KMNIST, fashion-MNIST, and CIFAR-10 datasets using full-batch gradient descent. They track geometric observables during training: gyration radii R±(t) measuring class manifold extension and inter-manifold distance D(t) between class centers. Class manifolds M±(t) are defined using normalized internal representations of points with the same label. The analysis converts epoch-based dynamics to training-error-based dynamics to identify invariant properties, particularly the inversion epoch t* where geometric observables change direction. They identify "stragglers" as the set of misclassified points at t* and measure their stability across different initializations and architectures.

## Key Results
- Class manifold segregation exhibits non-monotonic dynamics: initial fast segregation followed by slower rearrangement that increases class entanglement
- The inversion epoch t* is remarkably stable across different initializations, optimizers, and data subsets
- Stragglers (misclassified points at t*) constitute ~10-13% of training data and are maximally conserved across different runs and architectures
- Removal of stragglers eliminates the non-monotonic behavior, confirming their causal role
- Training error at inversion marks a critical tradeoff between segregation and entanglement affecting generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Class manifold segregation in neural networks exhibits non-monotonic dynamics due to a competition between segregation and entanglement objectives
- Mechanism: Early in training, optimization rapidly separates class manifolds (increasing D, decreasing R±) to minimize training error. Later, remaining misclassified points ("stragglers") trigger a tradeoff where further optimization increases entanglement to improve classification of these difficult points
- Core assumption: The training dynamics can be characterized by geometric observables (gyration radii and inter-manifold distance) that capture the essential trade-off between separation and entanglement
- Evidence anchors:
  - [abstract]: "after a fast segregation phase, a slower rearrangement (conserved across data sets and architectures) increases the class entanglement"
  - [section]: "Just after the segregation period, the model classifies correctly most of the training data. After this point, further optimization of the loss function requires to tradeoff the overall segregation of this bulk for the separability of the few data points that are still misclassified"
  - [corpus]: Weak - corpus neighbors focus on network inversion and uncertainty detection rather than geometric manifold dynamics

### Mechanism 2
- Claim: The "inversion epoch" t* is a data-dependent invariant property that marks the transition between segregation and entanglement phases
- Mechanism: At epoch t*, the derivatives of geometric observables change sign. This point corresponds to a training error that's remarkably stable across different initializations, optimizers, and data subsets, suggesting it's a fundamental property of the data manifold structure
- Core assumption: The geometric observables R± and D have well-defined derivatives that can be used to identify phase transitions in the training dynamics
- Evidence anchors:
  - [abstract]: "The training error at the inversion is stable under subsampling, and across network initialisations and optimisers"
  - [section]: "The location of the inversion epoch t* where the time derivatives of the metric observables change sign...barely fluctuates between runs started from different initialisations"
  - [corpus]: Weak - corpus neighbors don't address training dynamics phase transitions

### Mechanism 3
- Claim: "Stragglers" - the set of misclassified points at t* - are maximally stable across training runs and architectures, and their removal eliminates the non-monotonic behavior
- Mechanism: Stragglers constitute a well-defined subset of the training data that resists classification until t*. Their identity is highly conserved across different network initializations and architectures, and their removal causes the geometric observables to become monotonic
- Core assumption: The set of misclassified points at a specific training error level has a stable identity that's independent of random initialization
- Evidence anchors:
  - [abstract]: "The inversion is the manifestation of tradeoffs elicited by well-defined and maximally stable elements of the training set, coined 'stragglers'"
  - [section]: "Remarkably, among all the sets S(t(ϵtr)), stragglers are maximally conserved...We name 'stragglers' the elements of S(t*)"
  - [corpus]: Weak - corpus neighbors focus on different aspects of model inversion and uncertainty

## Foundational Learning

- Concept: Gyration radii as geometric observables
  - Why needed here: Gyration radii provide a simple, computable measure of class manifold extension that captures the trade-off between segregation and entanglement
  - Quick check question: How does the gyration radius change when class manifolds become more compact vs. more dispersed?

- Concept: Manifold geometry in high-dimensional spaces
  - Why needed here: Understanding how class manifolds evolve in the activation space is crucial for interpreting the non-monotonic dynamics
  - Quick check question: Why is normalization of activations (projection onto unit sphere) important for comparing manifold geometries across different layers?

- Concept: Training error as a function of geometric observables
  - Why needed here: Converting epoch-based dynamics to training-error-based dynamics reveals the data-dependent invariants in the system
  - Quick check question: How does plotting R± and D as functions of training error vs. epoch change our understanding of the dynamics?

## Architecture Onboarding

- **Component map:** Input layer → hidden layer (20 units, tanh) → output layer (2 units)
- **Critical path:** Data standardization → forward pass through network → compute geometric observables → gradient descent optimization → track R±, D, and training error over epochs
- **Design tradeoffs:** Shallow networks show clear non-monotonic behavior but may not capture complex data structures; deeper networks may show similar dynamics but with more complexity
- **Failure signatures:** If radii become monotonic or if the inversion point becomes highly variable across runs, this suggests either data structure changes or optimization issues
- **First 3 experiments:**
  1. Train a 2-layer network on MNIST and plot R+, R-, and D as functions of epoch to observe non-monotonic behavior
  2. Compute the inversion point t* and verify its stability across different initializations and optimizers
  3. Identify stragglers at t* and verify their conservation across different runs and architectures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relationship between the fraction of stragglers (φ) and traditional metrics of dataset difficulty like intrinsic dimensionality or task complexity?
- Basis in paper: [explicit] The paper states "How this measure relates to other metrics of task difficulty, such as the intrinsic dimensions of the data set [31,32] or of the objective landscape [33], and to other specifics of data structure [34,35], is an interesting open question."
- Why unresolved: The study identifies stragglers as a robust measure of dataset complexity but doesn't establish connections to established complexity metrics in the literature.
- What evidence would resolve it: Empirical studies comparing φ across datasets with varying intrinsic dimensionality, examining correlations between φ and other complexity measures, and developing theoretical frameworks linking these concepts.

### Open Question 2
- Question: How does the inversion dynamics phenomenology change in state-of-the-art deep convolutional neural networks?
- Basis in paper: [explicit] The paper acknowledges "However robust, the phenomenology found in small fully-connected architectures should not be expected to arise immediately, or to be as clearcut, in state-of-the-art deep convolutional neural networks."
- Why unresolved: The study focuses on simple fully-connected networks where the phenomenon is most pronounced, leaving uncertainty about whether and how it manifests in more complex architectures.
- What evidence would resolve it: Systematic experiments tracking manifold geometry across training epochs in various CNN architectures (ResNet, EfficientNet, etc.) and identifying analogous phenomena to the inversion dynamics.

### Open Question 3
- Question: What is the theoretical explanation for the non-monotonic segregation dynamics observed in the training process?
- Basis in paper: [inferred] The paper demonstrates the phenomenon exists and identifies stragglers as triggers, but notes "This suggests that theoretical insight into the segregation dynamics of class manifolds may be gained by employing the theory of deep linear networks, which allows for analytical computations."
- Why unresolved: While empirical evidence is strong, the paper suggests theoretical approaches but doesn't provide a complete mathematical explanation for why this specific non-monotonic behavior emerges.
- What evidence would resolve it: Analytical derivations of the inversion dynamics in simplified models (linear networks, mean-field theory) that predict the existence and properties of the inversion epoch and its relationship to stragglers.

## Limitations
- Analysis focuses on relatively simple architectures (shallow networks) and binary classification tasks, raising questions about generalizability to deeper networks and multi-class problems
- Definition of "stragglers" and their maximal stability across architectures needs more rigorous statistical validation
- Geometric observables R± and D may not capture all relevant aspects of manifold geometry in high-dimensional spaces, particularly for non-convex or highly structured manifolds

## Confidence

- **High Confidence:** The non-monotonic segregation dynamics and the existence of an inversion epoch are well-supported by the geometric observables and stable across multiple datasets and initializations
- **Medium Confidence:** The characterization of stragglers as maximally stable across architectures is supported but requires more rigorous statistical validation of the conservation metric
- **Low Confidence:** Generalization to deeper networks and multi-class problems remains untested and could reveal significant limitations in the geometric framework

## Next Checks
1. Test the non-monotonic dynamics on deeper networks (3+ layers) and multi-class classification tasks to assess architectural generalizability
2. Conduct ablation studies systematically removing different subsets of stragglers to quantify the precise contribution of different data points to the inversion behavior
3. Apply alternative manifold learning techniques (t-SNE, UMAP, persistent homology) to verify that the observed geometric dynamics capture the essential structure of class manifolds