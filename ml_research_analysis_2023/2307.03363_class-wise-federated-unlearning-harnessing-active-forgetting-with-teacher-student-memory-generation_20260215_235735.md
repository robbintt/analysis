---
ver: rpa2
title: 'Class-wise Federated Unlearning: Harnessing Active Forgetting with Teacher-Student
  Memory Generation'
arxiv_id: '2307.03363'
source_url: https://arxiv.org/abs/2307.03363
tags:
- unlearning
- uni00000013
- data
- uni00000011
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of federated unlearning, which
  aims to remove the influence of specific target training data from machine learning
  models in a distributed manner. Existing federated unlearning methods suffer from
  imprecise data influence estimation, significant computational burden, or both.
---

# Class-wise Federated Unlearning: Harnessing Active Forgetting with Teacher-Student Memory Generation

## Quick Facts
- **arXiv ID**: 2307.03363
- **Source URL**: https://arxiv.org/abs/2307.03363
- **Reference count**: 40
- **Primary result**: Proposed FedAF framework achieves efficient class-wise federated unlearning with high completeness and utility

## Executive Summary
This paper introduces FedAF, a novel federated unlearning framework that addresses the challenge of removing specific target training data influence from machine learning models in distributed settings. Inspired by neurological active forgetting mechanisms, FedAF employs teacher-student learning to generate knowledge-free fake labels that can overwrite old memories. The method uses refined elastic weight consolidation to preserve non-target data knowledge while efficiently unlearning target data. Experimental results on benchmark datasets demonstrate that FedAF significantly improves unlearning efficiency while maintaining satisfactory completeness and model utility compared to existing methods.

## Method Summary
FedAF implements class-wise federated unlearning through a teacher-student learning framework. Untrained teacher models generate fake labels for target data features by averaging their predictions, creating knowledge-free labels that the student model can learn. These manipulated data pairs are then used to train the student model with an EWC regularization term that preserves non-target data knowledge. The unlearning process integrates with standard FedAvg aggregation, allowing clients to process unlearning requests and send updated parameters to the server for aggregation. The framework specifically addresses the trade-off between unlearning completeness and catastrophic forgetting through its dual-component approach.

## Key Results
- FedAF achieves 10.6% higher unlearning completeness than baseline methods on average
- The framework reduces unlearning time by up to 92% compared to naive retraining approaches
- Maintains model utility with minimal accuracy degradation (<1%) while effectively removing target class influence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Teacher-student learning can generate knowledge-free labels that are easy to unlearn by the model.
- **Mechanism**: Untrained teacher models predict labels for target data features. Averaging predictions produces a fake label. This label contains no effective knowledge from the original training data but is learnable by the student model, enabling overwriting of old memories.
- **Core assumption**: Untrained models have no prior knowledge of the training data distribution, so their predictions are unbiased and suitable for generating knowledge-free labels.
- **Evidence anchors**: [abstract] "These new memories are generated through teacher-student learning." [section 4.2.1] "Specifically, we adopt a teacher-student learning pattern [18]."
- **Break condition**: If teacher models are not truly untrained, they may retain bias and produce labels that leak information about the target data.

### Mechanism 2
- **Claim**: Elastic Weight Consolidation (EWC) can preserve non-target data knowledge while unlearning target data.
- **Mechanism**: During unlearning, EWC adds a regularization term to the loss that penalizes changes to model parameters important for non-target tasks. This prevents catastrophic forgetting of non-target data.
- **Core assumption**: The model parameters important for non-target data have high Fisher information and thus high regularization weights in EWC.
- **Evidence anchors**: [abstract] "We further utilize refined elastic weight consolidation to mitigate catastrophic forgetting of non-target data." [section 4.3] "EWC provides us an approximation of ð¿Dð‘˜ (ðœƒ) when having ðœƒ âˆ—Dð‘˜, but not having access to Dð‘˜."
- **Break condition**: If the Fisher information matrix is poorly estimated or the trade-off coefficient Î» is mis-tuned, EWC may fail to preserve non-target knowledge or hinder unlearning.

### Mechanism 3
- **Claim**: Debiasing teacher labels with propensity scores improves unlearning completeness and stability.
- **Mechanism**: The teacher label is multiplied by a debias vector that downweights the target class propensity score. This reduces algorithmic bias from untrained models while maintaining label learnability.
- **Core assumption**: Untrained models have prediction bias on particular classes, and this bias can be mitigated by adjusting propensity scores.
- **Evidence anchors**: [section 4.2.2] "Our empirical study (Figure 7) shows that the untrained model naturally has prediction bias on particular classes..."
- **Break condition**: If propensity scores are incorrectly set, the debiasing may either fail to remove bias or make labels too difficult to learn.

## Foundational Learning

- **Concept**: Federated learning aggregation (FedAvg)
  - Why needed here: The unlearning process must integrate into the existing federated learning workflow without disrupting the aggregation mechanism.
  - Quick check question: What aggregation function does FedAvg use to combine client updates?

- **Concept**: Influence functions and data influence estimation
  - Why needed here: The paper contrasts its approach with methods that rely on influence estimation, highlighting the computational burden of those approaches.
  - Quick check question: Why are influence functions considered analytically intractable for deep models?

- **Concept**: Catastrophic forgetting in neural networks
  - Why needed here: The unlearning process involves training on new (manipulated) data, which risks forgetting non-target data knowledge. Understanding catastrophic forgetting is key to appreciating the need for EWC.
  - Quick check question: What is the primary cause of catastrophic forgetting when training neural networks sequentially?

## Architecture Onboarding

- **Component map**:
  - Memory Generator: Creates manipulated data (features + fake labels) using teacher models
  - Knowledge Preserver: Trains student model using EWC loss on manipulated data
  - Client Module: Handles unlearning requests, runs memory generator and knowledge preserver
  - Server Module: Performs standard FedAvg aggregation with unlearned client updates

- **Critical path**:
  1. Client receives unlearning request for target class
  2. Memory generator creates fake labels via teacher models
  3. Knowledge preserver trains student model with EWC loss
  4. Updated model parameters sent to server
  5. Server aggregates updates via FedAvg

- **Design tradeoffs**:
  - Number of teacher models (Q): More teachers â†’ better label diversity but higher computation
  - EWC trade-off coefficient (Î»): Higher Î» â†’ better preservation of non-target knowledge but slower unlearning
  - Debias weight (Ïƒ): Higher Ïƒ â†’ less bias but potentially harder to unlearn

- **Failure signatures**:
  - Incomplete unlearning: Backdoor accuracy remains high after unlearning
  - Catastrophic forgetting: Test accuracy drops significantly after unlearning
  - Unstable performance: High variance across trials, especially with uniform/random labels

- **First 3 experiments**:
  1. Overlapping validation: Train on mixed task (D' + M) to verify overlapping solution space exists
  2. Ablation study: Compare debias teacher labels vs uniform/random labels for unlearning completeness
  3. Efficiency benchmark: Measure running time vs baseline retrain and FRR methods

## Open Questions the Paper Calls Out

The paper explicitly notes that comparing during-training unlearning performance would be inconvenient, leaving the more realistic scenario of interleaved training and unlearning requests unexplored. This limitation means the framework's effectiveness when unlearning requests arrive during active federated training, rather than after convergence, remains unknown.

## Limitations

- The paper uses only 4 federated clients, which may not reflect real-world scenarios with many clients
- Experimental evaluation is limited to relatively simple models (ResNet-10, basic CNN) without testing scalability to modern deep learning architectures
- Exact neural network architectures for teacher models are unspecified, which could significantly impact label generation quality

## Confidence

- Mechanism 1 (Teacher-student label generation): Medium confidence - conceptually sound but lacks direct empirical validation against alternatives
- Mechanism 2 (EWC regularization): High confidence - well-established technique with strong theoretical foundation
- Mechanism 3 (Debiasing with propensity scores): Low confidence - empirical validation is limited to single study with no comparison to established debiasing methods

## Next Checks

1. Conduct ablation studies comparing FedAF with alternative label generation methods (random, uniform, human-labeled) to isolate the teacher-student contribution
2. Test FedAF's performance across different neural network architectures to assess robustness beyond the unspecified base models
3. Evaluate the computational overhead of FedAF in highly federated settings (100+ clients) to verify scalability claims