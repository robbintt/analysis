---
ver: rpa2
title: Text Classification via Large Language Models
arxiv_id: '2305.08377'
source_url: https://arxiv.org/abs/2305.08377
tags:
- input
- sentiment
- arxiv
- reasoning
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces CARP (Clue And Reasoning Prompting), a novel
  framework for text classification using large language models (LLMs). CARP addresses
  two main challenges: (1) LLMs'' limited reasoning ability for complex linguistic
  phenomena, and (2) the token constraints in in-context learning.'
---

# Text Classification via Large Language Models

## Quick Facts
- arXiv ID: 2305.08377
- Source URL: https://arxiv.org/abs/2305.08377
- Reference count: 20
- New state-of-the-art performances on 4 out of 5 text-classification benchmarks, achieving 97.39% on SST-2, 96.40% on AGNews, 98.78% on R8, and 96.95% on R52

## Executive Summary
This paper introduces CARP (Clue And Reasoning Prompting), a novel framework for text classification using large language models that addresses two key challenges: LLMs' limited reasoning ability for complex linguistic phenomena and token constraints in in-context learning. CARP employs a progressive reasoning strategy where LLMs first identify superficial clues (keywords, tones, semantic relations) before generating diagnostic reasoning for final decisions. To overcome token limitations, CARP uses a fine-tuned model for kNN demonstration search, allowing LLMs to leverage both their generalization ability and task-specific evidence from the full labeled dataset.

## Method Summary
CARP is a framework that decomposes the reasoning process into three steps: clue identification, reasoning generation, and final classification. The approach uses a fine-tuned retriever for kNN demonstration search to overcome token limitations in in-context learning. For few-shot setups, CARP automatically generates clues and reasoning explanations using LLMs, creating richer demonstrations as (text, clues, reasons, label) tuples. The framework demonstrates strong performance across five text-classification benchmarks and shows impressive capabilities in low-resource and domain-adaptation setups.

## Key Results
- Achieved new state-of-the-art performances on 4 out of 5 widely-used text-classification benchmarks
- Improved SST-2 accuracy from 96.15% to 97.39% (+1.24)
- Improved AGNews accuracy from 95.68% to 96.40% (+0.72)
- Achieved comparable performances to supervised models with only 16 examples per class in low-resource settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CARP's progressive reasoning strategy addresses LLMs' lack of complex linguistic reasoning by decomposing the decision process into three explicit steps.
- Mechanism: The model first identifies superficial clues (keywords, tones, semantic relations), then induces a diagnostic reasoning process based on those clues, and finally makes a decision considering both the clues and reasoning. This mirrors human decision-making and enables the model to handle complex linguistic phenomena like intensification, contrast, and irony.
- Core assumption: LLMs can be effectively guided through multi-step reasoning when prompts are structured to explicitly request clue identification, reasoning generation, and final classification.
- Evidence anchors:
  - [abstract]: "CARP adopts a progressive reasoning strategy tailored to addressing the complex linguistic phenomena involved in text classification: CARP first prompts LLMs to find superficial clues... based on which a diagnostic reasoning process is induced for final decisions."
  - [section 4.1]: "CARP decomposes the reasoning process into three steps, where LLMs are first prompted to find superficial clues... next, CARP treats the clues and input as premises and induce a diagnostic reasoning process; and finally determine the final label considering the above two steps."
  - [corpus]: Weak - corpus neighbors focus on math reasoning and molecule generation, not text classification reasoning mechanisms.
- Break condition: If the LLM fails to generate meaningful clues or reasoning in the intermediate steps, the final classification will be no better than baseline prompting methods.

### Mechanism 2
- Claim: Using a fine-tuned model for kNN demonstration search allows CARP to overcome token limitations by leveraging task-specific evidence from the full labeled dataset.
- Mechanism: Instead of randomly sampling demonstrations, CARP uses a model fine-tuned on the supervised dataset as the kNN encoder. This ensures retrieved samples are semantically close to the input with respect to the classification task, effectively connecting the LLM to the full training set despite context window limitations.
- Core assumption: A model fine-tuned on the task-specific dataset provides better semantic representations for retrieval than general-purpose models like SimCSE.
- Evidence anchors:
  - [abstract]: "CARP uses a fine-tuned model on the supervised dataset for kNN demonstration search in the in-context learning, allowing the model to take the advantage of both LLM's generalization ability and the task-specific evidence provided by the full labeled dataset."
  - [section 3.3]: "Since the fine-tuned model is trained based on task-specific labels, it guarantees that retrieved samples are close to the input sequence with respect to the task."
  - [corpus]: Weak - corpus neighbors don't discuss kNN retrieval strategies for text classification.
- Break condition: If the fine-tuned retriever's representations don't capture task-relevant semantic similarity, the retrieved demonstrations may be irrelevant, negating the benefits of this approach.

### Mechanism 3
- Claim: CARP's clue and reasoning generation in the few-shot setup provides human-readable influence functions that align better with instruction tuning objectives than simple (text, label) pairs.
- Mechanism: For each training example, CARP automatically generates clues and reasoning explanations using LLMs. These are then included in demonstrations as (text, clues, reasons, label) tuples, creating richer prompts that guide the LLM through the reasoning process rather than just showing examples.
- Core assumption: Including intermediate reasoning steps in demonstrations helps LLMs understand the decision process better than showing only input-output pairs.
- Evidence anchors:
  - [section 4.2.1]: "we ask LLMs to generate clues that indicate the label... Based on clues generated clues, the input, and the label, we ask LLMs to generate reasoning details"
  - [section 4.1]: "clues and reasoning serve as a tunnel to let human intervene: in the few-shot setup, where clues and reasons need to be prepared in advance for demonstrations, we can modify them as we see fit."
  - [corpus]: Weak - corpus neighbors don't discuss explanation generation for few-shot learning.
- Break condition: If the automatically generated clues and reasoning are of poor quality or misleading, they could confuse the model rather than help it learn the classification task.

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: Text classification requires reasoning about complex linguistic phenomena (negation, intensification, irony) that simple pattern matching cannot handle. Chain-of-thought prompting enables LLMs to break down complex decisions into manageable steps.
  - Quick check question: What are the three steps in CARP's progressive reasoning strategy?

- Concept: In-context learning limitations
  - Why needed here: LLMs have limited context windows (e.g., 4096 tokens for GPT-3), which restricts the number of demonstration examples that can be provided. Understanding these limitations is crucial for designing effective retrieval strategies.
  - Quick check question: How does CARP overcome the token limitation problem in in-context learning?

- Concept: Semantic similarity for retrieval
  - Why needed here: Effective demonstration selection requires finding examples semantically similar to the test input. Different encoding approaches (SimCSE vs. fine-tuned models) have different strengths for this task.
  - Quick check question: Why does CARP use a fine-tuned model rather than SimCSE for kNN demonstration search?

## Architecture Onboarding

- Component map: Input processor -> Fine-tuned retriever -> kNN demonstration search -> Clue generator -> Reasoning generator -> Decision maker -> Voting mechanism
- Critical path: Input → Fine-tuned retriever → kNN demonstration search → Clue generation → Reasoning generation → Final classification
- Design tradeoffs:
  - Using fine-tuned retriever vs. SimCSE: Better task-specific similarity but requires training a model
  - Including clues/reasoning vs. simple examples: Richer prompts but more tokens consumed
  - Multiple runs with voting vs. single run: Better robustness but higher computational cost
- Failure signatures:
  - Poor retrieval quality: Retrieved demonstrations are irrelevant to the test input
  - Clue generation failure: LLM fails to identify meaningful clues from text
  - Reasoning generation failure: LLM generates illogical or irrelevant reasoning
  - Voting instability: Different runs produce highly inconsistent predictions
- First 3 experiments:
  1. Test CARP with random demonstration sampling vs. kNN sampling to verify retrieval benefits
  2. Test CARP with vs. without clue and reasoning generation to verify the progressive reasoning benefits
  3. Test CARP on low-resource settings (16 examples per class) to verify generalization capabilities

## Open Questions the Paper Calls Out
1. What is the optimal number of demonstration examples (k) for the kNN search in CARP, and how does this vary across different datasets and text classification tasks?
2. How does the quality of automatically generated clues and reasoning compare to human-written ones, and what impact does this have on downstream classification performance?
3. How does CARP perform on text classification tasks with longer input sequences that exceed typical context window limits?
4. What is the computational overhead of CARP compared to traditional fine-tuning approaches, and how does this affect its practical deployment?

## Limitations
- The evaluation focuses on five specific text-classification benchmarks, which may not represent the full diversity of text classification tasks
- The paper doesn't extensively discuss computational costs or inference time implications of the CARP framework
- The approach relies on LLMs with specific capabilities, and results might vary with different model architectures or sizes

## Confidence
- High confidence in the core mechanism of progressive reasoning strategy (CARP's three-step approach)
- Medium confidence in the kNN demonstration search methodology and its effectiveness
- Medium confidence in the generalization capabilities demonstrated in low-resource settings
- Low confidence in the scalability and computational efficiency of the approach for larger-scale applications

## Next Checks
1. Test CARP on additional text classification datasets beyond the five benchmarks to verify generalization across diverse domains and task types.
2. Evaluate the computational overhead and inference time of CARP compared to standard prompting methods to assess practical deployment viability.
3. Conduct ablation studies removing the fine-tuned retriever component to quantify the exact contribution of the kNN demonstration search to overall performance improvements.