---
ver: rpa2
title: Non-Invasive Fairness in Learning through the Lens of Data Drift
arxiv_id: '2303.17566'
source_url: https://arxiv.org/abs/2303.17566
tags:
- data
- fairness
- confair
- constraints
- groups
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper recasts fairness issues in ML as data drift problems
  and addresses them through non-invasive conformance improvements. The authors propose
  two strategies: DifFair, which trains separate models for different groups and deploys
  them based on data conformance constraints, and ConFair, which reweighs training
  data using conformance constraints to build a single model.'
---

# Non-Invasive Fairness in Learning through the Lens of Data Drift

## Quick Facts
- arXiv ID: 2303.17566
- Source URL: https://arxiv.org/abs/2303.17566
- Authors: [Not specified]
- Reference count: 40
- This paper recasts fairness issues in ML as data drift problems and addresses them through non-invasive conformance improvements.

## Executive Summary
This paper proposes a novel approach to improving fairness in machine learning by recasting fairness issues as data drift problems. The authors introduce two strategies - DifFair and ConFair - that leverage conformance constraints to improve fairness without modifying the underlying learning algorithms. DifFair employs model splitting with conformance-based deployment, while ConFair uses conformance-based reweighting for single-model training. Experiments on seven real-world datasets demonstrate significant improvements in fairness metrics while maintaining utility.

## Method Summary
The paper introduces two non-invasive approaches to fairness: DifFair (differential fairness) and ConFair (constrained fairness). Both methods use conformance constraints (CCs) - linear combinations of numerical attributes that capture distributional patterns. DifFair trains separate models for different groups and deploys them based on conformance to group-specific constraints. ConFair reweighs training data using conformance constraints to build a single model. The methods are tested on seven real-world datasets using logistic regression and XGBoost classifiers, evaluating fairness improvements in terms of Disparate Impact and Average Odds Difference while maintaining Balanced Accuracy.

## Key Results
- ConFair outperforms existing reweighing techniques in fairness improvement while maintaining utility
- DifFair shows better performance in scenarios with significant group drift where single models struggle
- Both methods significantly improve fairness metrics compared to baselines, with ConFair showing gains of up to 200% in some cases
- ConFair remains robust when weights are used by different learning algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conformance Constraints (CCs) can identify the densest regions of data for different groups, enabling targeted reweighting that improves fairness without harming utility.
- Mechanism: CCs learn linear combinations of numerical attributes that capture the core distributional patterns of each group. By assigning higher weights to tuples that conform to these patterns, ConFair amplifies the signal from well-represented regions while reducing the influence of outliers.
- Core assumption: The densest regions of data for different groups have sufficiently distinct attribute distributions that can be captured by linear constraints.
- Evidence anchors:
  - [abstract] "ConFair employs conformance constraints in a novel way to derive weights for training data, which are then used to build a single model."
  - [section 2.3] "A conformance constraint is a constraint over arithmetic relationships involving multiple numerical attributes."
  - [corpus] Weak - the corpus contains related fairness papers but none specifically validate the use of conformance constraints for this purpose.
- Break condition: If group attribute distributions are not sufficiently separable or are highly nonlinear, the linear constraints may fail to capture meaningful differences, leading to ineffective reweighting.

### Mechanism 2
- Claim: Model splitting (DifFair) can improve fairness when groups exhibit significant drift that cannot be captured by a single model.
- Mechanism: By training separate models for each group and deploying them based on conformance to group-specific constraints, DifFair ensures each group receives predictions from a model optimized for its characteristics.
- Core assumption: The drift between groups is significant enough that a single model cannot adequately serve both populations.
- Evidence anchors:
  - [abstract] "DifFair shows better performance in scenarios with significant group drift where single models struggle."
  - [section 3.1] "DifFair employs conformance constraints to deploy the appropriate model for serving tuples."
  - [corpus] Weak - the corpus contains related fairness work but lacks direct evidence about model splitting performance under drift.
- Break condition: If groups have overlapping distributions or the drift is minimal, model splitting may actually harm performance by reducing training data per model and introducing unnecessary complexity.

### Mechanism 3
- Claim: Density estimation pre-processing can strengthen conformance constraints by focusing on the most representative data points.
- Mechanism: By filtering training data to retain only the densest points for each group (using kernel density estimation), the resulting conformance constraints become tighter and more discriminative.
- Core assumption: The densest regions of data contain the most representative and informative samples for learning group-specific patterns.
- Evidence anchors:
  - [section 3.3] "The effectiveness of conformance constraints is affected by the variance of attributes in the input."
  - [section 4.3] "Compared to the variants without our optimization, DifFair and ConFair achieve significant boosting in improving model fairness."
  - [corpus] Weak - the corpus does not contain evidence about density-based constraint optimization.
- Break condition: If density estimation incorrectly identifies outliers as dense regions or if the densest points are not representative of the broader group distribution, this could lead to overfitting and poor generalization.

## Foundational Learning

- Concept: Conformance Constraints (CCs)
  - Why needed here: CCs provide a quantitative measure of how well a data point fits the expected distributional patterns, which is essential for both reweighting strategies and model deployment decisions.
  - Quick check question: How does the violation score for a tuple relate to its conformance to a set of CCs?

- Concept: Group Fairness vs Individual Fairness
  - Why needed here: The paper focuses on group fairness (equal treatment across demographic groups) rather than individual fairness (consistent treatment of similar individuals), which influences the choice of evaluation metrics and intervention strategies.
  - Quick check question: What are the key differences between Disparate Impact and Average Odds Difference as fairness metrics?

- Concept: Data Drift
  - Why needed here: Understanding how data distributions change over time or across subpopulations is fundamental to recognizing why fairness issues arise and how the proposed solutions address them.
  - Quick check question: How does covariate shift between groups manifest in ML model performance?

## Architecture Onboarding

- Component map:
  Data preprocessing (normalization, one-hot encoding) -> Conformance constraint derivation (CC learning) -> Density estimation (optional optimization) -> Model training (LR or XGBoost) -> Model deployment (single model with reweighting or multi-model selection) -> Evaluation (fairness and utility metrics)

- Critical path:
  For ConFair: Data → CC derivation → Weight assignment → Model training → Evaluation
  For DifFair: Data → CC derivation → Model training (multiple) → Constraint-based deployment → Evaluation

- Design tradeoffs:
  - Single vs multi-model approach: Single model (ConFair) is more robust to poor group representation but may struggle with significant drift; multi-model (DifFair) handles drift better but requires adequate representation per group
  - Invasive vs non-invasive: The proposed methods modify weights or deployment strategies rather than data or algorithms, preserving interpretability but potentially limiting maximum fairness improvement

- Failure signatures:
  - ConFair fails when group distributions are highly overlapping and cannot be separated by linear constraints
  - DifFair fails when one or more groups have insufficient data for meaningful model training
  - Both methods fail when the target variable is not well-predicted by numerical attributes alone

- First 3 experiments:
  1. Compare ConFair vs baseline on MEPS dataset with LR models to verify fairness improvement while maintaining utility
  2. Run DifFair on synthetic data with controlled drift to validate performance in high-drift scenarios
  3. Test ConFair with varying density thresholds to find optimal point for constraint strength vs data retention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ConFair perform when applied to more than two groups, and what is the optimal strategy for weight adjustment across multiple groups with different levels of representation and label skew?
- Basis in paper: [inferred] The paper mentions that the approach can be easily generalized to more than two groups but does not provide empirical results or discuss the optimal strategy for multi-group scenarios.
- Why unresolved: The current evaluation focuses on binary group settings, leaving the effectiveness and optimization of ConFair for multi-group scenarios unexplored.
- What evidence would resolve it: Experimental results showing ConFair's performance across datasets with multiple groups, analysis of weight adjustment strategies for different group characteristics, and comparison with baseline methods in multi-group settings.

### Open Question 2
- Question: How sensitive is ConFair to the choice of intervention factors (αw and αu), and what automated methods could be developed to optimize these parameters?
- Basis in paper: [explicit] The paper mentions that the intervention factors are adjustable parameters and that automatic search is used to find optimal values, but it does not discuss sensitivity analysis or automated optimization methods.
- Why unresolved: The current approach uses manual tuning or simple automatic search, which may not be scalable or robust across different datasets and domains.
- What evidence would resolve it: Sensitivity analysis showing how different values of intervention factors affect fairness and utility, development and evaluation of automated optimization methods (e.g., Bayesian optimization, reinforcement learning), and comparison of automated methods with manual tuning.

### Open Question 3
- Question: How can ConFair and DifFair be combined or integrated with other fairness intervention techniques to achieve better results?
- Basis in paper: [inferred] The paper mentions that ConFair focuses on numerical attributes and could potentially be combined with methods that work in categorical domains, but does not explore such combinations.
- Why unresolved: The paper evaluates each method independently without exploring synergies or trade-offs that might arise from combining different fairness intervention approaches.
- What evidence would resolve it: Experimental results comparing combined approaches against individual methods, analysis of trade-offs between different fairness metrics when combining techniques, and development of frameworks for integrating multiple fairness interventions.

### Open Question 4
- Question: How does the performance of DifFair and ConFair change when applied to streaming or dynamically changing data where group distributions may shift over time?
- Basis in paper: [inferred] The paper focuses on static datasets and does not address scenarios where data distributions change over time or in streaming settings.
- Why unresolved: Real-world applications often involve dynamic data, and the effectiveness of fairness interventions in such settings is not explored in the current work.
- What evidence would resolve it: Experimental results showing performance on streaming or time-series data, analysis of how group drift affects fairness intervention effectiveness, and development of adaptive algorithms that can handle changing data distributions.

## Limitations

- The paper's claims about conformance constraints capturing group distributions are supported primarily by empirical results rather than theoretical guarantees.
- The effectiveness of linear constraints for highly nonlinear drift patterns remains untested.
- The automatic parameter tuning approach for intervention factors lacks transparency about sensitivity and reproducibility.

## Confidence

- High confidence in the empirical findings showing ConFair improves fairness metrics while maintaining utility across multiple datasets and learning algorithms.
- Medium confidence in the mechanism explanations, particularly around why density-based optimization improves performance, as this lacks theoretical grounding.
- Low confidence in generalizability claims beyond the tested datasets, especially for scenarios with severe class imbalance or extreme drift patterns.

## Next Checks

1. Test ConFair on synthetic datasets with controlled nonlinear drift patterns to validate constraint effectiveness beyond linear separability assumptions.
2. Conduct ablation studies removing the density optimization step to quantify its actual contribution versus the core conformance constraint mechanism.
3. Evaluate performance degradation when applying ConFair to groups with highly imbalanced sample sizes (ratio > 1:10) to identify practical limits of the approach.