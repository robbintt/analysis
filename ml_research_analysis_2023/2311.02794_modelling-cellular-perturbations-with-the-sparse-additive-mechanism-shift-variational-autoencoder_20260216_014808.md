---
ver: rpa2
title: Modelling Cellular Perturbations with the Sparse Additive Mechanism Shift Variational
  Autoencoder
arxiv_id: '2311.02794'
source_url: https://arxiv.org/abs/2311.02794
tags:
- latent
- sams-v
- perturbation
- perturbations
- effects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents the Sparse Additive Mechanism Shift Variational
  Autoencoder (SAMS-VAE), a generative model for modeling cellular perturbations and
  their effects. SAMS-VAE models the latent state of a perturbed sample as the sum
  of a local latent variable capturing sample-specific variation and sparse global
  variables of latent intervention effects.
---

# Modelling Cellular Perturbations with the Sparse Additive Mechanism Shift Variational Autoencoder

## Quick Facts
- arXiv ID: 2311.02794
- Source URL: https://arxiv.org/abs/2311.02794
- Reference count: 29
- Primary result: SAMS-VAE achieves superior predictive capability over baselines, identifies disentangled perturbation-specific latent subspaces, and outperforms other models in average treatment effect estimation and differential expression analysis.

## Executive Summary
SAMS-VAE is a generative model designed to model cellular perturbations and their effects by representing the latent state of perturbed samples as the sum of a local latent variable capturing sample-specific variation and sparse global variables capturing intervention effects. The model explicitly combines additive perturbations in latent space, perturbation-specific sparse effects, and perturbation-independent natural variation in a joint model. The authors evaluate SAMS-VAE on two single-cell RNA sequencing datasets, demonstrating superior generalization across in-distribution and out-of-distribution tasks, including combinatorial reasoning under resource constraints, while yielding interpretable latent structures that correlate strongly with known biological mechanisms.

## Method Summary
SAMS-VAE models cellular perturbations using a variational autoencoder framework where the latent state is decomposed into a basal component and sparse perturbation-specific offsets. The generative model assumes perturbation effects compose additively in latent space with sparsity-inducing Bernoulli priors on perturbation masks. Inference is performed using a correlated variational family that models dependencies between basal states and perturbation embeddings. The model is trained using the IWELBO objective with Adam optimizer, and evaluated on held-out data using importance-weighted log-likelihood estimation, average treatment effect correlation with differential expression, and pathway prediction from inferred perturbation masks.

## Key Results
- SAMS-VAE outperforms baseline models (CPA-VAE, SVAE+, Conditional VAE) on marginal likelihood estimation and combinatorial generalization tasks
- The model achieves strong performance on average treatment effect estimation, with Pearson correlations exceeding baseline models when compared to ground truth differential expression
- Interpretable latent structures identified by SAMS-VAE show strong correlations with known biological pathways, with random forest classifiers achieving high accuracy in pathway prediction from inferred perturbation masks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SAMS-VAE learns interpretable latent representations by modeling perturbations as sparse additive latent variables
- Mechanism: The model assumes the latent state of a perturbed sample is the sum of a local latent variable (capturing sample-specific variation) and sparse global variables (capturing intervention effects), allowing identification of disentangled, perturbation-specific latent subspaces
- Core assumption: Perturbation effects are sparse and additive in the latent space
- Evidence anchors: [abstract] "SAMS-VAE models the latent state of a perturbed sample as the sum of a local latent variable capturing sample-specific variation and sparse global variables of latent intervention effects" [section 2.1] "We propose to model perturbations as inducing sparse latent offsets that compose additively"
- Break condition: If perturbations have dense or non-additive effects in the latent space, the model's assumptions would not hold and performance would degrade

### Mechanism 2
- Claim: The correlated variational inference strategy improves model performance by capturing correlations between latent variables
- Mechanism: SAMS-VAE uses a correlated variational family that models possible correlations between sample latent basal states and the global latent perturbation masks and embeddings, allowing more faithful inversion of the generative model
- Core assumption: There are correlations between the latent basal states and perturbation effects that should be captured during inference
- Evidence anchors: [section 2.3] "We propose two improvements to the mean-field inference scheme... First, we model possible correlations between sample latent basal states zb_i and the global latent perturbation masks and embeddings" [section 4.1] "In addition to comparing model types, we perform an ablation of SAMS-V AE and CPA-V AE inference strategies. We find that the correlated zbasal strategy yields substantial improvements in performance"
- Break condition: If the correlations between latent variables are not significant or the model cannot effectively capture them, the correlated inference strategy may not provide benefits

### Mechanism 3
- Claim: SAMS-VAE's generative model allows for better generalization and compositionality of perturbations compared to models that do not explicitly model sparsity and compositionality
- Mechanism: By modeling perturbations as sparse additive latent variables, SAMS-VAE can learn to compose interventions in the latent space, allowing generalization to new combinations of perturbations by leveraging learned sparse structure
- Core assumption: The sparse additive structure in the latent space captures the true underlying mechanisms of how perturbations interact
- Evidence anchors: [abstract] "SAMS-VAE outperforms comparable models in terms of generalization across in-distribution and out-of-distribution tasks, including a combinatorial reasoning task under resource paucity" [section 4.2] "SAMS-V AE and CPA-V AE both achieve strong performance on the norman-ood task across metrics... These results support the utility of the compositional mechanisms"
- Break condition: If the true underlying mechanisms of perturbation interactions are not sparse or additive, the model's compositional assumptions may not hold and performance may suffer

## Foundational Learning

- Concept: Variational Autoencoders (VAEs)
  - Why needed here: SAMS-VAE is built upon the VAE framework, which allows for learning a generative model of the data distribution in a latent space
  - Quick check question: What is the key difference between a standard autoencoder and a variational autoencoder?

- Concept: Causal inference and interventions
  - Why needed here: SAMS-VAE is designed to model the effects of interventions (perturbations) on cellular systems, which is closely related to causal inference
  - Quick check question: What is the difference between an observational study and an interventional study in causal inference?

- Concept: Disentangled representation learning
  - Why needed here: SAMS-VAE aims to learn disentangled latent representations where different factors of variation are captured in separate dimensions of the latent space
  - Quick check question: What is the benefit of learning disentangled representations in machine learning?

## Architecture Onboarding

- Component map: Encoder -> Latent Space -> Decoder
- Critical path: 1) Encode observations to latent space using the encoder network 2) Sample from the approximate posterior distribution over latent variables using the inference network 3) Decode latent representations to the observation space using the decoder network 4) Compute the ELBO loss and backpropagate gradients to update model parameters
- Design tradeoffs: Sparsity vs. expressiveness - Enforcing sparsity in the perturbation masks may lead to more interpretable representations but could limit the model's ability to capture complex effects; Correlated vs. mean-field inference - Using a correlated variational family can improve performance by capturing dependencies between latent variables, but it may be more computationally expensive
- Failure signatures: Poor generalization to new perturbations - If the model fails to learn a meaningful sparse additive structure, it may not generalize well to new combinations of perturbations; Inability to recover known biological pathways - If the model's latent representations are not disentangled or do not capture the true underlying mechanisms, it may fail to identify factors correlated with known biological pathways
- First 3 experiments: 1) Train SAMS-VAE on a simple dataset with known sparse additive perturbation effects and evaluate its ability to recover the true latent structure 2) Compare the performance of SAMS-VAE with and without the correlated inference strategy on a benchmark dataset to assess the impact of modeling correlations between latent variables 3) Evaluate the interpretability of SAMS-VAE's latent representations by training a classifier to predict known biological pathway annotations from the inferred perturbation masks and assessing the classification accuracy

## Open Questions the Paper Calls Out
1. How does the choice of prior distribution for the perturbation masks (e.g., Bernoulli vs. Beta-Bernoulli) affect the sparsity and interpretability of the learned latent subspaces?
2. How does SAMS-VAE perform on datasets with continuous or multi-dose perturbation levels, rather than binary dosages?
3. How do the proposed inference strategies (correlated zbasal and correlated E) compare to other advanced inference techniques, such as normalizing flows or hierarchical variational families, in terms of capturing complex dependencies between latent variables?

## Limitations
- The core assumption of sparse additive perturbation effects in latent space lacks direct empirical support from single-cell biology literature
- The model's performance improvements are demonstrated primarily through held-out log-likelihood and ATE correlation metrics, which may not fully capture biological relevance
- The computational cost of correlated variational inference is not explicitly characterized

## Confidence
- **High Confidence**: The mathematical formulation of SAMS-VAE and its implementation details are clearly specified and reproducible; the comparison methodology against baseline models is rigorous and well-documented
- **Medium Confidence**: The empirical results demonstrating superior performance on held-out data and combinatorial generalization tasks are convincing, but the biological interpretability of learned representations requires further validation
- **Low Confidence**: The claim that sparse additive perturbation effects accurately capture biological reality lacks direct supporting evidence from the single-cell literature

## Next Checks
1. Test SAMS-VAE on synthetic datasets with known perturbation mechanisms (both sparse additive and non-additive) to quantify the model's ability to recover ground truth latent structures under different intervention patterns
2. Evaluate SAMS-VAE's performance on truly out-of-distribution perturbations not present in training data, using established perturbation combinations from the literature that were excluded during model training
3. Perform controlled experiments varying the sparsity level in the mask prior (currently fixed at Bernoulli(0.001)) to determine the sensitivity of model performance and interpretability to this hyperparameter