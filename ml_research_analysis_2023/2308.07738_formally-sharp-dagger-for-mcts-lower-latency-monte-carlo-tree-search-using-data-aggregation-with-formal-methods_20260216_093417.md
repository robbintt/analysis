---
ver: rpa2
title: 'Formally-Sharp DAgger for MCTS: Lower-Latency Monte Carlo Tree Search using
  Data Aggregation with Formal Methods'
arxiv_id: '2308.07738'
source_url: https://arxiv.org/abs/2308.07738
tags:
- policy
- neural
- advice
- mcts
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes combining formal methods with Monte Carlo Tree
  Search (MCTS) and deep learning to synthesize high-quality, low-latency policies
  for large Markov Decision Processes (MDPs). The method uses model checking to guide
  MCTS in generating high-quality samples offline, which are then used to train a
  neural network.
---

# Formally-Sharp DAgger for MCTS: Lower-Latency Monte Carlo Tree Search using Data Aggregation with Formal Methods

## Quick Facts
- arXiv ID: 2308.07738
- Source URL: https://arxiv.org/abs/2308.07738
- Reference count: 35
- One-line primary result: Combines formal methods with MCTS and deep learning to synthesize high-quality, low-latency policies for large MDPs, achieving 64% win rate in Pac-Man versus 55% for standard MCTS

## Executive Summary
This work proposes a novel approach that combines formal methods with Monte Carlo Tree Search (MCTS) and deep learning to synthesize high-quality, low-latency policies for large Markov Decision Processes (MDPs). The method uses model checking to guide MCTS in generating high-quality samples offline, which are then used to train a neural network. This network can either replace the expensive expert advice in MCTS or serve as a full policy, significantly reducing decision latency while maintaining performance. The approach employs statistical model checking to detect when additional samples are needed and focuses them on states where the learned policy differs from the expert.

## Method Summary
The method combines formal methods with Monte Carlo Tree Search and deep learning to create lower-latency policies for MDPs. It uses probabilistic model checking (Storm) to generate expert policies and advice functions, then employs a Sharp DAgger algorithm to iteratively generate training data by simulating with the current policy and adding counterexamples. Convolutional neural networks learn to approximate expert advice or full policies, which can be used in MCTS instead of expensive expert advice. Statistical model checking evaluates policy performance with confidence bounds. The approach is demonstrated on Frozen Lake and Pac-Man, showing that neural advice can match expert performance with significantly lower latency.

## Key Results
- Neural advice in MCTS achieves 87% win rate in Pac-Man, approaching expert advice performance while reducing decision time from 8 seconds to near-instantaneous
- Sharp DAgger accelerates learning by focusing on states where neural network predictions differ from expert advice
- Best policy achieves 64% win rate in Pac-Man, surpassing standard MCTS (55%) while enabling near-instantaneous decisions
- Exact methods can handle MDPs up to 10^8 states, but current experiments limited to smaller MDPs

## Why This Works (Mechanism)

### Mechanism 1
Sharp DAgger accelerates learning by generating additional samples focused on states where the learned policy differs from the expert policy. The algorithm simulates paths using the current neural network policy, identifies states where predictions diverge from the expert's expected values beyond a threshold, and adds those state-value pairs to the training dataset before retraining. This assumes states with significant prediction differences are most informative for improving policy quality.

### Mechanism 2
Neural advice can replace expensive expert advice in MCTS while maintaining similar performance but with significantly lower latency. A neural network is trained to approximate the expert advice function (e.g., safety probability scores), which can then be used at every node in the MCTS tree instead of computing the expensive expert advice at each step. This assumes the neural network can learn to approximate the expert advice function well enough that resulting policy quality is comparable.

### Mechanism 3
Statistical model checking provides practical performance evaluation that captures policy quality better than traditional machine learning metrics. Instead of measuring accuracy on a test set, the method simulates many game executions using both the learned and expert policies, then compares their average rewards with confidence bounds. This addresses the limitation that in sequential decision making problems, small errors can accumulate over time, making traditional accuracy metrics insufficient for evaluating policy quality.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The entire framework operates on MDPs as the underlying model for sequential decision making
  - Quick check question: What are the key components of an MDP tuple (S, A, P, R, RT, AP, L) and what does each represent?

- Concept: Monte Carlo Tree Search (MCTS)
  - Why needed here: MCTS is the core online planning algorithm that the work aims to accelerate through neural advice
  - Quick check question: What are the four main phases of MCTS and how does each contribute to finding good actions?

- Concept: Statistical Model Checking
  - Why needed here: Used for both detecting when more samples are needed and for evaluating policy performance with confidence guarantees
  - Quick check question: How does statistical model checking differ from exact probabilistic model checking, and when is each appropriate?

## Architecture Onboarding

- Component map:
  Formal methods layer (Storm) -> Data generation layer (Sharp DAgger) -> Neural network layer (CNN) -> MCTS layer -> Evaluation layer (Statistical model checking)

- Critical path:
  1. Generate initial dataset using formal methods
  2. Train initial neural network
  3. Simulate with current policy to find counterexamples
  4. Add counterexamples to dataset
  5. Retrain network
  6. Evaluate performance using statistical model checking
  7. If performance gap is significant, return to step 3

- Design tradeoffs:
  - Perfect data vs. noisy data: Using exact methods gives noise-free expert policies but is computationally expensive; using MCTS gives noisier data but scales to larger problems
  - Local vs. global normalization: Local normalization performed better experimentally but may not capture global patterns
  - Horizon length: Longer horizons give better policies but increase computational cost and make statistical evaluation more expensive

- Failure signatures:
  - Sharp DAgger loop not converging: Dataset keeps growing without performance improvement, suggesting the neural network architecture is insufficient
  - Neural advice performing poorly: Indicates the network cannot learn the expert advice function accurately enough
  - Statistical evaluation showing large confidence intervals: Suggests insufficient simulation budget for reliable performance estimates

- First 3 experiments:
  1. Implement Frozen Lake MDP and verify that exact methods can compute optimal policies; compare win rates of policies learned from perfect data vs. MCTS-generated data
  2. Implement sharp DAgger loop on a simple grid world and verify that it identifies and adds informative counterexamples; measure how quickly performance improves compared to random data generation
  3. Implement neural advice for MCTS on Pac-Man and measure latency improvement vs. expert advice while maintaining similar win rates; verify the break-even point in terms of number of calls

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of neural advice compare to exact methods in larger MDPs beyond Frozen Lake and Pac-Man? The authors mention that exact methods like model checking can handle MDPs up to 10^8 states, but their current experiments are limited to smaller MDPs. No experiments or analysis are presented for larger MDPs where exact methods would be computationally infeasible.

### Open Question 2
What is the optimal threshold t for determining "almost-optimal" actions when using neural advice in MCTS? The authors mention using a threshold t = 0.9 when restricting to actions where NN(s,a) ≥ 0.9 × max_a' NN(s,a), but this appears to be an arbitrary choice without justification.

### Open Question 3
How does the sharp DAgger algorithm compare to other dataset aggregation methods in terms of sample efficiency and convergence speed? The authors present sharp DAgger as an improvement over standard DAgger, using model checking to identify counterexamples where the neural network performs poorly, but they don't compare it to other dataset aggregation approaches.

## Limitations

- Experiments limited to relatively simple MDPs (Frozen Lake and Pac-Man), with scalability to more complex domains unproven
- Strong assumptions about quality of formal methods-generated expert policies and neural network approximation ability
- Evaluation relies on statistical model checking which provides confidence bounds but still involves sampling uncertainty

## Confidence

- High confidence: The basic mechanism of using neural networks to approximate expensive expert advice in MCTS is well-established and demonstrated effectively
- Medium confidence: The sharp DAgger algorithm for focused data generation shows promise but lacks extensive validation across diverse problem domains
- Medium confidence: The use of statistical model checking for policy evaluation is theoretically sound but may have practical limitations in terms of simulation budget requirements

## Next Checks

1. Test sharp DAgger on a wider range of MDPs with varying characteristics (different state spaces, action spaces, reward structures) to validate its generality
2. Compare the performance of sharp DAgger against alternative active learning strategies for identifying informative training samples
3. Conduct ablation studies to determine the impact of different hyperparameters (simulation horizon, threshold for counterexample detection, neural network architecture) on final policy quality