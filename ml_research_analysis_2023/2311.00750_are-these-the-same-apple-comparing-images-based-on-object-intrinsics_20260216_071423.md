---
ver: rpa2
title: Are These the Same Apple? Comparing Images Based on Object Intrinsics
arxiv_id: '2311.00750'
source_url: https://arxiv.org/abs/2311.00750
tags:
- object
- similarity
- images
- image
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of measuring intrinsic object similarity,
  which is the similarity between images based on the identity of the objects in the
  images, ignoring extrinsic factors such as lighting, object poses, and backgrounds.
  To benchmark this measurement, the authors collect a new dataset called CUTE, which
  contains 18,000 images of 180 objects from 50 semantic categories under varying
  extrinsic conditions.
---

# Are These the Same Apple? Comparing Images Based on Object Intrinsics

## Quick Facts
- arXiv ID: 2311.00750
- Source URL: https://arxiv.org/abs/2311.00750
- Authors: Not specified in input
- Reference count: 20
- Key outcome: A new dataset (CUTE) and method (FFA) for measuring intrinsic object similarity, outperforming baselines on multiple metrics.

## Executive Summary
This paper tackles the challenge of measuring intrinsic object similarity—comparing images based on the objects they contain while ignoring extrinsic factors like lighting, pose, and background. To address this, the authors introduce the CUTE dataset, containing 18,000 images of 180 objects from 50 categories under controlled extrinsic variations. They propose a simple yet effective method, Foreground Feature Averaging (FFA), which combines foreground segmentation with features from the self-supervised DINOv2 model. The approach achieves strong performance on intrinsic similarity benchmarks and shows promise for downstream applications such as generalizable re-identification.

## Method Summary
The method combines foreground filtering with visual features learned by the self-supervised DINOv2 model. Images are resized to 336x336, and DINOv2 extracts both global and patch-level features. A foreground segmentation model (CarveKit) generates a binary mask to isolate the object. Patch-level features are masked to remove background patches, then averaged to produce a compact, object-focused embedding. Cosine similarity between these embeddings measures intrinsic object similarity. The method is evaluated on the CUTE dataset and compared against baselines like LPIPS, CLIPScore, and DreamSim using metrics such as top-1 accuracy, mean average precision, and adjusted Rand index.

## Key Results
- FFA achieves 80.2% top-1 accuracy on CUTE’s in-the-wild group, outperforming CLIPScore (73.2%) and LPIPS (70.7%).
- On pose evaluation, FFA scores 72.5%, compared to 69.4% for CLIPScore and 68.4% for LPIPS.
- FFA shows promise in generalizable re-identification, improving upon CLIP-based methods in cross-dataset evaluations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining foreground filtering with DINOv2 features reduces the influence of extrinsic visual factors like lighting and background on object similarity measurement.
- Mechanism: DINOv2 provides instance-level feature representations that are invariant to lighting and pose variations. Foreground filtering (via segmentation) removes background clutter, isolating the object-centric features. Averaging these filtered features yields a compact, object-focused representation.
- Core assumption: Instance-level contrastive learning inherently captures object identity information that is stable across extrinsic variations.
- Evidence anchors:
  - [abstract]: "we find that combining deep features learned from contrastive self-supervised learning with foreground filtering is a simple yet effective approach"
  - [section]: "we find that contrastively-trained models such as DINOv2 [Oquab et al., 2023] produce features that contain a large amount of instance-level information"
  - [corpus]: Weak/no direct evidence; corpus neighbors focus on shape similarity, manipulation, and state encoding rather than foreground/background separation.
- Break condition: If the foreground segmentation fails to accurately isolate the object, background information will leak into the feature averaging, degrading performance on small or occluded objects.

### Mechanism 2
- Claim: Using patch-level features rather than a global feature vector preserves spatial object structure, improving similarity discrimination for objects with subtle differences.
- Mechanism: DINOv2 produces both global and patch-level features. Averaging patch features preserves local spatial relationships within the object, which is crucial for distinguishing objects with similar global appearance but different fine details (e.g., different apple textures).
- Core assumption: Local spatial detail within an object is discriminative for object identity, not just global shape or color.
- Evidence anchors:
  - [abstract]: "patch-level foreground feature pooling"
  - [section]: "We experiment using both the global feature and the patch-level features"
  - [corpus]: No direct evidence; corpus papers do not discuss patch-level vs global feature tradeoffs in this context.
- Break condition: If objects are highly symmetric or lack distinguishing local features, patch-level detail may add noise rather than signal, potentially hurting performance.

### Mechanism 3
- Claim: The CUTE dataset design, with paired objects under controlled extrinsic variations, provides a strong ground truth for benchmarking intrinsic similarity metrics.
- Mechanism: By capturing the same object under different lighting and poses, the dataset isolates intrinsic properties. Including different but similar objects ensures that metrics must focus on identity, not just general category features.
- Core assumption: Controlled variation in extrinsic factors (lighting, pose) combined with semantically similar but distinct objects creates a clean separation between intrinsic and extrinsic influences.
- Evidence anchors:
  - [abstract]: "we collect the Common paired objects Under differenT Extrinsics (CUTE) dataset"
  - [section]: "Our dataset is composed of object instances from each of 50 semantic object categories... These categories range from fruits like 'apples' and 'oranges', to household items like 'plates' and 'forks'"
  - [corpus]: Weak/no direct evidence; corpus neighbors focus on different tasks (sketch retrieval, manipulation, relational similarity) and do not provide evidence about dataset design for intrinsic similarity.
- Break condition: If the extrinsic variation is insufficient (e.g., lighting changes are too subtle) or object pairs are not similar enough, the metric may rely on extrinsic cues, invalidating the benchmark.

## Foundational Learning

- Concept: Contrastive self-supervised learning (e.g., DINOv2)
  - Why needed here: DINOv2 provides rich instance-level features without requiring labeled data, which is essential for capturing object identity under varying extrinsic conditions.
  - Quick check question: What is the key difference between DINOv2's training objective and traditional supervised classification?
- Concept: Foreground segmentation and masking
  - Why needed here: Removing background ensures the metric focuses on the object itself, not its surroundings, which is critical for intrinsic similarity.
  - Quick check question: How does the choice of segmentation model affect the quality of the foreground mask and downstream similarity scores?
- Concept: Cosine similarity for embedding comparison
  - Why needed here: Cosine similarity measures angular distance in feature space, which is effective for comparing high-dimensional embeddings like those from DINOv2.
  - Quick check question: Why might cosine similarity be preferred over Euclidean distance for comparing DINOv2 embeddings?

## Architecture Onboarding

- Component map: Input image → Resize to 336x336 → DINOv2 backbone → Global feature + patch-level features → Foreground segmentation (CarveKit) → Binary mask → Mask patch features → Discard background patches → Average remaining patch features → Compact object embedding → Cosine similarity between embeddings → Similarity score
- Critical path: Image → DINOv2 → Patch features → Segmentation mask → Filtered patches → Averaged embedding → Cosine similarity
- Design tradeoffs:
  - Using global DINOv2 feature is faster but may lose fine-grained object details; patch averaging is slower but more discriminative.
  - Segmentation quality directly impacts foreground filtering; a poor mask degrades performance.
  - Fixed input size (336x336) balances speed and feature resolution.
- Failure signatures:
  - Low segmentation accuracy → Background leakage → Degraded similarity scores.
  - Insufficient extrinsic variation in dataset → Metric may rely on non-intrinsic cues.
  - Patch averaging over too few patches (e.g., small objects) → Noisy embeddings.
- First 3 experiments:
  1. Compare global DINOv2 feature vs patch averaging on a small subset of CUTE; measure mAP.
  2. Test with and without foreground segmentation on the "in-the-wild" group; measure top-1 accuracy.
  3. Sweep input resolutions (224, 336, 512) and measure impact on similarity scores and runtime.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we further improve the clustering performance of embedding-based methods like CLIPScore and the proposed FFA metric, as indicated by their low adjusted Rand index (ARI) scores?
- Basis in paper: [explicit] The paper reports that both CLIPScore and FFA have low ARI scores, suggesting that their embedding spaces are not well-separated according to object identity.
- Why unresolved: The paper does not provide a detailed analysis of why the embedding-based methods perform poorly in terms of clustering, nor does it propose specific solutions to improve this aspect.
- What evidence would resolve it: Experimental results showing the impact of different feature extraction techniques, clustering algorithms, or hyperparameter tuning on the ARI scores of embedding-based methods.

### Open Question 2
- Question: How does the proposed FFA metric generalize to more complex scenes with multiple objects, occlusions, and varying object poses relative to the camera?
- Basis in paper: [inferred] The paper focuses on images containing single objects and does not evaluate the performance of the FFA metric on more complex scenes.
- Why unresolved: The paper does not provide any experiments or analysis on the performance of the FFA metric in scenarios with multiple objects, occlusions, or varying object poses.
- What evidence would resolve it: Experimental results demonstrating the performance of the FFA metric on datasets containing images with multiple objects, occlusions, and varying object poses, along with comparisons to other similarity metrics.

### Open Question 3
- Question: How does the choice of foreground segmentation model affect the performance of the FFA metric, and can we identify more effective segmentation models or techniques for this task?
- Basis in paper: [explicit] The paper uses the CarveKit [Selin, 2022] model for foreground segmentation in the FFA metric but does not explore other segmentation models or techniques.
- Why unresolved: The paper does not provide a comprehensive analysis of the impact of different foreground segmentation models on the performance of the FFA metric, nor does it explore alternative segmentation techniques.
- What evidence would resolve it: Experimental results comparing the performance of the FFA metric using different foreground segmentation models or techniques, along with an analysis of the strengths and weaknesses of each approach.

## Limitations
- The CUTE dataset focuses on rigid objects, limiting generalizability to deformable objects like clothing or animals.
- Foreground segmentation quality is critical; failures on occluded or transparent objects degrade performance.
- The study does not evaluate the method on complex scenes with multiple objects or occlusions.

## Confidence
- Mechanism 1 (DINOv2 + foreground filtering): High confidence based on strong experimental results in Table 1.
- Dataset design (CUTE): Medium confidence; while well-structured, potential ambiguities in object pairs may not fully isolate intrinsic factors.
- Patch-level vs global features: Medium confidence; paper reports differences but lacks ablation studies on why patch averaging helps.

## Next Checks
1. Test FFA on objects with significant occlusion or transparency to validate foreground segmentation robustness.
2. Compare FFA performance on deformable objects (e.g., folded clothes, animals) to assess generalizability beyond rigid objects.
3. Conduct an ablation study on patch vs global features using synthetic data where intrinsic/extrinsic factors are precisely controlled.