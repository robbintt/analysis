---
ver: rpa2
title: Text Retrieval with Multi-Stage Re-Ranking Models
arxiv_id: '2311.07994'
source_url: https://arxiv.org/abs/2311.07994
tags:
- bm25
- documents
- language
- retrieval
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of improving text retrieval accuracy
  while maintaining reasonable search speed, particularly in zero-shot settings. The
  authors propose a three-stage re-ranking model that combines BM25, a language model,
  and either a model ensemble or a larger language model.
---

# Text Retrieval with Multi-Stage Re-Ranking Models

## Quick Facts
- arXiv ID: 2311.07994
- Source URL: https://arxiv.org/abs/2311.07994
- Reference count: 5
- Key outcome: Three-stage re-ranking model achieves higher retrieval accuracy (NDCG@10 up to 0.6244) than BM25 alone while keeping search time increases minimal (1.39-2.59x slower than BM25+LM)

## Executive Summary
This paper proposes a three-stage re-ranking model for text retrieval that combines BM25 with language models and either model ensembles or larger language models. The key innovation is applying computationally expensive models only to a limited subset of highly similar documents, thus maintaining reasonable search speed while improving accuracy. The method is evaluated on MS-MARCO and BEIR benchmarks, demonstrating superior performance compared to BM25 alone or BM25+LM, particularly in zero-shot settings.

## Method Summary
The method employs a three-stage re-ranking architecture: first, BM25 retrieves top candidates from the document collection; second, a language model (MiniLM) re-ranks the top 100 documents; third, either a model ensemble (3 MiniLM models with different random seeds), a larger model (6-layer, 768 hidden size), or a pairwise model re-ranks the top 20 documents. Training uses cross-entropy loss with Adam optimizer, linear warmup, and L2 regularization. The approach allows trading off between accuracy and speed by adjusting the number of documents processed at each stage.

## Key Results
- NDCG@10 scores up to 0.6244 achieved, outperforming BM25-only and BM25+LM baselines
- Search time increases remain minimal (1.39-2.59x slower than BM25+LM)
- Model ensemble and larger model variants outperform pairwise model baseline, especially in zero-shot settings
- Larger model variant shows best overall performance on out-of-domain average score (0.6176)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-stage re-ranking reduces search delay by applying expensive models only to a subset of documents
- Mechanism: First stage uses fast BM25 to retrieve top candidates, then computationally expensive models re-rank only the top a1 documents
- Core assumption: Expensive models only need to process a small subset of documents to significantly improve ranking accuracy
- Evidence anchors: [abstract] "propose a three-stage re-ranking model using model ensembles or larger language models to improve search accuracy while minimizing the search delay"

### Mechanism 2
- Claim: Model ensembles improve accuracy by averaging multiple model predictions
- Mechanism: Multiple models trained with identical architecture and hyperparameters but different random seeds produce predictions that are averaged
- Core assumption: Different random seeds produce sufficiently diverse model predictions to benefit from averaging
- Evidence anchors: [section] "We calculate the similarity using an ensemble of language models... performance can be improved by changing a minimum number of settings"

### Mechanism 3
- Claim: Larger models with more parameters achieve better accuracy than smaller models
- Mechanism: Models with 768 hidden layer size outperform models with 384 hidden layer size on the same re-ranking task
- Core assumption: The increased model capacity translates to better ranking performance for this task
- Evidence anchors: [table] "BM25 + LM + Large 0.3845 0.4141 0.6545 0.7843 0.6176" shows highest out-of-domain average score

## Foundational Learning

- Concept: Text retrieval and ranking fundamentals
  - Why needed here: Understanding how documents are retrieved and ranked based on query similarity is essential to grasp why multi-stage approaches work
  - Quick check question: What is the difference between BM25 and language model-based ranking approaches?

- Concept: Zero-shot evaluation
  - Why needed here: The paper evaluates models on datasets different from training data (MS-MARCO) to test generalization
  - Quick check question: Why is zero-shot evaluation important for information retrieval systems?

- Concept: NDCG@10 metric
  - Why needed here: The paper uses NDCG@10 to evaluate ranking quality by measuring how well the top 10 results match relevance
  - Quick check question: What does NDCG@10 measure and why is it appropriate for this task?

## Architecture Onboarding

- Component map: BM25 retrieval → Language model re-ranking → High-performance model re-ranking
- Critical path: Query → BM25 → Top a1 documents → Language model → Top a2 documents → High-performance model → Final ranked results
- Design tradeoffs: Larger a1/a2 improves accuracy but increases computation; model complexity vs speed; ensemble vs single larger model
- Failure signatures: Low accuracy improvement despite expensive computation; search time increase without accuracy gain; poor zero-shot performance
- First 3 experiments:
  1. Compare BM25-only vs BM25+LM to establish baseline improvement
  2. Test different values of a2 (20, 30, 50) to find optimal tradeoff point
  3. Compare pairwise model vs ensemble vs larger model for third stage

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the analysis, several remain unresolved regarding dataset characteristics, comparison to state-of-the-art methods, and ensemble strategies.

## Limitations
- Model ensemble approach requires training multiple models, increasing computational requirements
- Larger model variant requires significantly more computational resources than baseline models
- Input truncation (512 tokens) in pairwise model limits effectiveness on long documents
- Focus primarily on NDCG@10 metric may not capture all aspects of retrieval quality

## Confidence
- High confidence in the core multi-stage architecture and its effectiveness in balancing accuracy and speed
- Medium confidence in the specific performance numbers, as implementation details for the pairwise model are not fully specified
- Medium confidence in the zero-shot generalization claims, given the limited number of out-of-domain datasets tested

## Next Checks
1. Test the ensemble approach with models trained using different architectures or hyperparameters (not just random seeds) to verify if this provides additional benefits
2. Evaluate the larger model's performance on document sets with varying lengths to quantify the impact of input truncation
3. Conduct ablation studies to determine the individual contribution of each re-ranking stage to the overall performance improvement