---
ver: rpa2
title: On the Vulnerability of Fairness Constrained Learning to Malicious Noise
arxiv_id: '2307.11892'
source_url: https://arxiv.org/abs/2307.11892
tags:
- group
- divides
- fairness
- classi
- adversary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the vulnerability of fairness-constrained
  learning algorithms to malicious noise in the training data. The authors show that
  by allowing randomized classifiers, many fairness notions (e.g., Demographic Parity,
  Equal Opportunity) can be made much more robust to adversarial noise than previously
  thought.
---

# On the Vulnerability of Fairness Constrained Learning to Malicious Noise

## Quick Facts
- arXiv ID: 2307.11892
- Source URL: https://arxiv.org/abs/2307.11892
- Reference count: 40
- Primary result: Shows that fairness-constrained learning algorithms can be made much more robust to malicious noise through randomization than previously thought

## Executive Summary
This paper investigates how fairness-constrained learning algorithms perform under malicious noise in training data, where an adversary can corrupt a small fraction of examples. The authors demonstrate that by allowing randomized classifiers, many fairness notions can achieve significantly better robustness than previously established impossibility results suggested. The key insight is that randomization can bypass simple adversarial tricks that amplify corruption power, particularly when exploiting group size imbalances.

## Method Summary
The authors develop a (P,Q)-Randomized Expansion (PQ(H)) that injects noise into each hypothesis h ∈ H, creating a new class of randomized classifiers. For each fairness constraint, they identify specific noise parameters p and q for different groups that maintain fairness on corrupted data while minimizing accuracy loss. The approach works by smoothing the hypothesis class to make it more resilient against adversarial manipulation.

## Key Results
- Demographic Parity can achieve O(α) accuracy loss under malicious noise through randomization, matching the best possible without fairness constraints
- Equal Opportunity requires O(√α) accuracy loss, with a matching lower bound
- Equalized Odds and Predictive Parity cannot be made robust beyond O(1) accuracy loss regardless of randomization
- Randomization allows bypassing impossibility results by creating improper learners that exploit controlled noise injection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Randomization within the hypothesis class (PQ(H)) allows bypassing adversarial amplification on small groups
- Mechanism: By injecting controlled noise into each hypothesis h∈H through parameterized randomization (p,q), the learner can effectively "smooth out" the hypothesis class, making it more resilient against adversarial manipulation that exploits group size imbalances
- Core assumption: The adversary's corruption budget α is small enough that the noise injection parameters can be chosen to offset the adversarial effects while maintaining fairness
- Evidence anchors:
  - [abstract]: "The key technical novelty of our work is how randomization can bypass simple 'tricks' an adversary can use to amplify his power"
  - [section 1.1]: "We bypass the impossibility results in Konstantinov and Lampert (2021) by allowing the learner to exhibit a randomized improper classifier"

### Mechanism 2
- Claim: The dependency of fairness violation on group size creates vulnerability that randomization can mitigate
- Mechanism: For smaller groups, the adversary can more easily manipulate fairness metrics (Proposition 1 and 2 show the violation scales with 1/(1-α)r where r is group proportion). Randomization allows the learner to compensate for this by adjusting prediction probabilities
- Core assumption: The group size distribution is known or can be estimated, allowing appropriate parameterization of the randomization
- Evidence anchors:
  - [section 4.1]: "The proof shows that this change is bounded by a function of the corruption rate α and the proportion of the data set in the fixed group A, denoted by rA"
  - [section 4.2]: Similar bound for True Positive Rate showing inverse dependence on group size

### Mechanism 3
- Claim: Different fairness constraints have fundamentally different vulnerabilities to malicious noise
- Mechanism: The paper shows that some constraints (Demographic Parity) can be made robust with O(α) loss, others (Equal Opportunity) require O(√α) loss, and some (Equalized Odds, Predictive Parity) cannot be made robust beyond O(1) loss regardless of randomization
- Core assumption: The classification problem and data distribution have specific properties that determine which fairness notion is appropriate
- Evidence anchors:
  - [abstract]: "For Demographic Parity we show we can incur only a Θ(α) loss in accuracy... For Equal Opportunity, we show we can incur an O(√α) loss... For these fairness notions, the excess accuracy clusters into three natural regimes O(α), O(√α), and O(1)"
  - [section 5.1]: Theorem 5 shows Ω(1) error for Predictive Parity with imbalanced groups

## Foundational Learning

- Concept: Malicious noise model and its relationship to fairness constraints
  - Why needed here: The paper builds on understanding how malicious noise (where an adversary can corrupt α fraction of data) interacts with fairness constraints, which is fundamental to the entire analysis
  - Quick check question: If an adversary can corrupt 10% of training data in the malicious noise model, what is the maximum accuracy loss without any fairness constraints?

- Concept: Group fairness metrics (Demographic Parity, Equal Opportunity, Equalized Odds, Calibration)
  - Why needed here: These are the specific fairness notions being analyzed for their vulnerability to malicious noise, and understanding their definitions and relationships is crucial
  - Quick check question: How does Equalized Odds differ from Equal Opportunity in terms of the fairness requirements?

- Concept: Improper learning and hypothesis class expansion
  - Why needed here: The key technical contribution involves allowing improper learning through PQ(H) - understanding why this helps and how it differs from proper learning is essential
  - Quick check question: What is the difference between a proper and improper learner in the context of this paper?

## Architecture Onboarding

- Component map: Data distribution D → Corruption by adversary (α fraction) → Corrupted distribution D̃ → Learning algorithm (PQ(H)) → Randomized classifier → Evaluation on clean test data
- Critical path: 1) Define hypothesis class H and fairness constraint F 2) Apply randomization to create PQ(H) 3) Implement learning algorithm that finds optimal classifier in PQ(H) subject to F 4) Evaluate accuracy and fairness on test data 5) Analyze vulnerability to malicious noise
- Design tradeoffs: Allowing randomization (improper learning) improves robustness to malicious noise but may increase computational complexity and reduce interpretability. The choice of fairness constraint involves tradeoffs between robustness (O(α) vs O(√α) vs O(1) loss) and the specific fairness guarantees provided.
- Failure signatures: High accuracy loss despite randomization suggests either α is too large for the chosen fairness constraint, or the group size imbalance is too extreme. Failure to satisfy fairness constraints despite randomization suggests the adversary's corruption strategy is particularly effective against that constraint.
- First 3 experiments:
  1. Implement the PQ(H) expansion for a simple hypothesis class (e.g., linear classifiers) and verify that randomization parameters can be chosen to maintain fairness on clean data
  2. Simulate malicious noise corruption and test whether the randomized classifier maintains lower accuracy loss compared to the non-randomized version
  3. Compare performance across different fairness constraints (Demographic Parity vs Equal Opportunity vs Equalized Odds) under the same malicious noise conditions to verify the O(α), O(√α), and O(1) loss patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal randomized strategy for Equalized Odds that minimizes accuracy loss under malicious noise?
- Basis in paper: The paper shows that Equalized Odds is highly vulnerable to malicious noise, forcing Ω(1) accuracy loss, but does not explore potential randomized strategies that might mitigate this.
- Why unresolved: The paper focuses on proving impossibility results for Equalized Odds rather than exploring potential mitigation strategies through randomization.
- What evidence would resolve it: A construction showing that a particular randomized strategy can achieve accuracy loss better than Ω(1) for Equalized Odds under malicious noise.

### Open Question 2
- Question: How does the performance of fairness-constrained learning algorithms under malicious noise change when there are more than two groups?
- Basis in paper: The paper primarily focuses on the binary group case and mentions that results extend to multiple groups but does not provide detailed analysis.
- Why unresolved: The analysis becomes significantly more complex with multiple groups, and the paper does not explore these cases in detail.
- What evidence would resolve it: A comprehensive analysis of fairness-constrained learning under malicious noise for cases with 3 or more groups, including both upper and lower bounds.

### Open Question 3
- Question: What is the relationship between malicious noise vulnerability and the strength of fairness constraints?
- Basis in paper: The paper shows different fairness notions have different vulnerabilities (O(α), O(√α), or O(1) accuracy loss), but does not provide a systematic characterization of this relationship.
- Why unresolved: The paper focuses on specific fairness notions rather than developing a general theory relating constraint strength to noise vulnerability.
- What evidence would resolve it: A theoretical framework that characterizes the relationship between the strength of various fairness constraints and their vulnerability to malicious noise.

## Limitations
- The theoretical bounds rely on specific assumptions about data distribution and adversary capabilities that may not hold in practice
- The randomization approach may incur computational overhead and potential interpretability costs not fully explored
- The analysis focuses on binary classification with two groups, limiting generalizability to more complex scenarios

## Confidence

- **High confidence**: The O(α) and O(√α) bounds for Demographic Parity and Equal Opportunity, supported by the core technical results in Theorems 1 and 2
- **Medium confidence**: The Ω(1) lower bounds for Equalized Odds and Predictive Parity, as these rely on specific distribution constructions that may not generalize
- **Low confidence**: The practical effectiveness of the randomization mechanism in real-world scenarios with complex, high-dimensional data and unknown group distributions

## Next Checks

1. Implement the randomization mechanism on a standard fairness benchmark dataset (e.g., COMPAS) and verify the claimed accuracy-fairness tradeoff under various malicious noise rates
2. Test the bounds empirically by systematically varying group size ratios and corruption patterns to see if the theoretical predictions hold
3. Compare the proposed approach against alternative robust fairness methods (e.g., adversarial training, robust optimization) on both synthetic and real datasets