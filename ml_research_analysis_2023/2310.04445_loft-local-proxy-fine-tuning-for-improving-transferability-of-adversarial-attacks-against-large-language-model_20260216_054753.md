---
ver: rpa2
title: 'LoFT: Local Proxy Fine-tuning For Improving Transferability Of Adversarial
  Attacks Against Large Language Model'
arxiv_id: '2310.04445'
source_url: https://arxiv.org/abs/2310.04445
tags:
- harmful
- target
- queries
- attack
- proxy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method called Local Fine-Tuning (LoFT) to
  improve the transferability of adversarial attacks against large language models
  (LLMs). The key idea is to fine-tune proxy models locally around harmful queries
  to better approximate target models in those regions, thus increasing attack success
  rates.
---

# LoFT: Local Proxy Fine-tuning For Improving Transferability Of Adversarial Attacks Against Large Language Model

## Quick Facts
- arXiv ID: 2310.04445
- Source URL: https://arxiv.org/abs/2310.04445
- Authors: 
- Reference count: 40
- This paper proposes a method called Local Fine-Tuning (LoFT) to improve the transferability of adversarial attacks against large language models (LLMs).

## Executive Summary
This paper introduces LoFT, a method for improving the transferability of adversarial attacks against large language models by locally fine-tuning proxy models around harmful queries. The approach generates similar queries using target models, fine-tunes proxies on these query-response pairs, and demonstrates significant improvements in attack success rates compared to non-fine-tuned proxies. The work also highlights critical limitations in existing automatic evaluation metrics, showing that they often overestimate attack success by counting responses that bypass alignment but don't contain actual harmful content.

## Method Summary
LoFT works by generating semantically similar queries to harmful inputs using target models, then fine-tuning proxy models on these query-response pairs to create locally accurate approximations of target behavior. The method uses a Fill-in-the-Blank (FITB) approach to generate similar queries while preserving semantic content. After fine-tuning, the proxy models are used with Greedy Coordinate Gradient-based search (GCG) to compute adversarial suffixes. The approach focuses on achieving accurate local approximation around harmful queries rather than global model similarity.

## Key Results
- LoFT increases attack success rates by 39% absolute on ChatGPT, 7% on GPT-4, and 0.5% on Claude compared to non-fine-tuned proxies
- Automatic metrics inadequately measure attack success, as responses often bypass alignment but don't contain actual harmful content
- Human evaluation reveals significant discrepancies between automatic and true attack success rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local fine-tuning improves adversarial attack transferability by better approximating the target model's behavior in the neighborhood of harmful queries.
- Mechanism: LoFT fine-tunes proxy models on similar queries generated by target models, creating a more accurate local approximation of the target's response function around harmful inputs.
- Core assumption: The proxy model only needs to accurately approximate the target model in the vicinity of harmful queries for successful attack transfer.
- Evidence anchors:
  - [abstract]: "fine-tuning proxy models on similar queries that lie in the lexico-semantic neighborhood of harmful queries to decrease the divergence between the proxy and target models"
  - [section]: "we contend that it is sufficient to ensure that the response of the proxy model mimics that of the target model in the region where we expect to search for attacks"
- Break condition: If the target model's response function changes rapidly in the neighborhood of harmful queries, local approximation may not be sufficient.

### Mechanism 2
- Claim: The fill-in-the-blank (FITB) prompting method generates the most semantically and lexically similar queries to harmful queries.
- Mechanism: FITB masks random non-stop words in harmful queries and asks the target model to generate replacements, preserving semantic content while creating variation.
- Core assumption: Maintaining high semantic and lexical similarity between similar queries and harmful queries is crucial for effective local fine-tuning.
- Evidence anchors:
  - [section]: "The FITB strategy retains the most semantic and lexical similarity to the harmful query, as evidenced by the highest BERTScore, ROUGE-L, and BLEU respectively"
  - [section]: "we find that the FITB approach yielded the best results in terms of the similarity scores achieved"
- Break condition: If target models generate poor quality or irrelevant similar queries, the effectiveness of FITB would diminish.

### Mechanism 3
- Claim: Human evaluation reveals that existing automatic metrics inadequately measure attack success by not capturing whether responses contain actual harmful content.
- Mechanism: The paper identifies that responses bypassing alignment but lacking harmful content are counted as successful by automatic metrics but fail human evaluation.
- Core assumption: Measuring attack success requires both bypassing alignment and delivering harmful content, not just avoiding refusal.
- Evidence anchors:
  - [section]: "we discovered that a sizable fraction of them do not, in fact, contain the requested harmful information, although they do bypass the alignment of the model"
  - [section]: "We observe that the word football is in the attack suffix and thus most of the responses are about football, regardless of what the query was about"
- Break condition: If future automatic metrics evolve to better capture harmful content, the gap between automatic and human evaluation might narrow.

## Foundational Learning

- Concept: Adversarial examples and transferability in machine learning
  - Why needed here: The paper builds on the concept that adversarial examples crafted on one model can transfer to another, but LoFT improves this by better approximating the target model locally
  - Quick check question: What is the fundamental assumption behind transfer-based adversarial attacks?

- Concept: Local approximation and neighborhood analysis in function spaces
  - Why needed here: LoFT's core insight is that accurate global approximation isn't necessary—only local accuracy around harmful queries matters
  - Quick check question: How does local approximation differ from global approximation in terms of computational requirements and effectiveness?

- Concept: Human evaluation methodologies for content safety
  - Why needed here: The paper reveals limitations of automatic metrics and advocates for human evaluation to measure true attack success
  - Quick check question: Why might automatic metrics fail to capture whether harmful content is actually delivered in adversarial attacks?

## Architecture Onboarding

- Component map: Target model (private) -> Similar query generator -> Proxy model (public) -> Local fine-tuning module -> Attack optimization module (GCG) -> Evaluation pipeline
- Critical path: Generate similar queries → Obtain target responses → Fine-tune proxy → Optimize attack suffixes → Evaluate on target
- Design tradeoffs: Local fine-tuning trades global accuracy for computational efficiency and improved local approximation; FITB trades query diversity for semantic preservation
- Failure signatures: Low similarity scores between similar queries and harmful queries; High refusal rates in target responses; Low human evaluation success rates despite high automatic metrics
- First 3 experiments:
  1. Generate similar queries using different methods (FITB, paraphrasing, similar to X) and compare similarity metrics
  2. Fine-tune proxy models on similar query-response pairs and measure improvement in attack success rates
  3. Conduct human evaluation comparing automatic metrics vs human judgments of attack success

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the local fine-tuning approach (LoFT) compare to other methods of generating adversarial attacks in terms of success rate and efficiency?
- Basis in paper: [explicit] The paper introduces LoFT as a method for generating adversarial attacks against LLMs and compares its success rate to non-fine-tuned baselines.
- Why unresolved: While the paper provides results comparing LoFT to non-fine-tuned baselines, it does not compare LoFT to other state-of-the-art methods of generating adversarial attacks.
- What evidence would resolve it: Experiments comparing the success rate and efficiency of LoFT to other state-of-the-art methods of generating adversarial attacks.

### Open Question 2
- Question: How does the quality of the fine-tuning data generated by different target models (e.g., Claude, GPT-3.5, GPT-4) impact the success rate of adversarial attacks?
- Basis in paper: [explicit] The paper mentions that Claude-generated data yields higher lexico-semantic similarity to the harmful query compared to GPT-3.5-generated data.
- Why unresolved: The paper does not provide a comprehensive analysis of how the quality of fine-tuning data impacts the success rate of adversarial attacks across different target models.
- What evidence would resolve it: A detailed analysis comparing the success rate of adversarial attacks generated using fine-tuning data from different target models.

### Open Question 3
- Question: How does the choice of prompting method (e.g., FITB, paraphrasing, similar to X) affect the success rate of adversarial attacks?
- Basis in paper: [explicit] The paper mentions that the FITB approach yields the highest lexico-semantic similarity to the harmful query compared to other prompting methods.
- Why unresolved: The paper does not provide a comprehensive analysis of how the choice of prompting method affects the success rate of adversarial attacks.
- What evidence would resolve it: Experiments comparing the success rate of adversarial attacks generated using different prompting methods.

## Limitations
- The paper's reported success rates rely on automatic metrics that the authors themselves identify as problematic for measuring true attack success
- Results may be specific to the chosen proxy model architecture (Vicuna-based) and may not generalize to other architectures
- The FITB method for generating similar queries may have limited diversity in the query space, potentially affecting robustness

## Confidence
**High Confidence**: The core insight that local fine-tuning can improve proxy model approximation around harmful queries. The experimental results showing increased similarity between proxy and target model responses after fine-tuning provide strong evidence for this claim.

**Medium Confidence**: The effectiveness of FITB as the best method for generating similar queries. While the paper shows strong similarity metrics, the comparison is limited to three methods, and other approaches might yield better results.

**Low Confidence**: The absolute improvement percentages in attack success rates (39%, 7%, and 0.5%) due to the reliance on automatic metrics that the paper itself identifies as problematic. The true effectiveness likely varies significantly based on human evaluation.

## Next Checks
1. **Human Evaluation Validation**: Conduct comprehensive human evaluation across all tested attack scenarios to verify the actual increase in harmful content delivery, not just alignment bypassing. Compare human-evaluated success rates with the reported automatic metric improvements to establish the true effectiveness of LoFT.

2. **Proxy Model Ablation Study**: Test LoFT across multiple proxy model architectures (different sizes, training methods, base models) to determine whether the improvements are specific to Vicuna-based proxies or generalize across different architectures. This would validate the robustness of the approach.

3. **Long-term Stability Assessment**: Evaluate the stability of LoFT improvements over time as target models receive updates and alignment changes. Test whether locally fine-tuned proxies maintain their effectiveness or require periodic retraining, and assess the computational trade-offs of this maintenance requirement.