---
ver: rpa2
title: 'Salespeople vs SalesBot: Exploring the Role of Educational Value in Conversational
  Recommender Systems'
arxiv_id: '2310.17749'
source_url: https://arxiv.org/abs/2310.17749
tags:
- product
- salesbot
- shopper
- preferences
- seller
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SalesOps, a framework for simulating realistic
  conversational recommender systems in complex product domains. The framework provides
  both a product catalog and a buying guide to the seller, while gradually revealing
  shopping preferences to the shopper.
---

# Salespeople vs SalesBot: Exploring the Role of Educational Value in Conversational Recommender Systems

## Quick Facts
- arXiv ID: 2310.17749
- Source URL: https://arxiv.org/abs/2310.17749
- Reference count: 6
- Primary result: SalesBot approaches professional salespeople in fluency and informativeness but lags in recommendation quality

## Executive Summary
This paper introduces SalesOps, a framework for simulating realistic conversational recommender systems in complex product domains. The framework uses LLM-based agents to simulate both salespeople and shoppers, gradually revealing preferences during conversation to mimic underspecified buying experiences. A comprehensive human study compares SalesBot against professional salespeople, revealing that while SalesBot approaches professional performance in terms of fluency and informativeness, it lags behind in recommendation quality. The study also highlights challenges in ensuring truthfulness and faithfulness in educational conversational recommender systems.

## Method Summary
The SalesOps framework simulates conversational recommender systems using two LLM-based agents: SalesBot (seller) and ShopperBot (buyer). SalesBot uses a multi-component architecture including Action Decision, Knowledge Search, Product Search, and Response Generation modules, with retrieval systems for accessing product catalogs and buying guides. ShopperBot employs a simpler prompt-based approach. The framework gradually reveals shopping preferences during conversation rather than upfront disclosure, and evaluates performance across three dimensions: recommendation quality, informativeness, and fluency. Human evaluations compare SalesBot against professional salespeople across multiple product categories.

## Key Results
- SalesBot approaches professional salespeople in fluency and informativeness but lags in recommendation quality
- SalesBot's responses are on average less faithful than human sellers (53.0 vs 73.8), with 21.1% contradictions vs 6.4% for humans
- The educational value mechanism through buying guides shows promise but requires improvement in recommendation accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradual revelation of shopping preferences creates realistic simulation of underspecified buying experience
- Mechanism: Preferences revealed incrementally during conversation rather than upfront
- Core assumption: Shopping for complex products involves iterative learning and preference refinement
- Evidence anchors:
  - [abstract]: "Shopping preferences are gradually revealed to the shopper during the course of conversation in order to simulate the underspecified goal scenario of a typical uninformed shopper."
  - [section]: "Unlike prior work that reveals the shopper preferences in their entirety when the conversation is initiated... we choose to reveal preferences to the Shopper gradually during the conversation"
- Break condition: Users with highly specific upfront needs or non-educational product categories may find gradual revelation frustrating

### Mechanism 2
- Claim: Providing both product catalog and buying guide enables educational dialogue
- Mechanism: Buying guide serves as knowledge source for educating shoppers on product attributes
- Core assumption: Complex product purchases require background knowledge that can be structured into buying guides
- Evidence anchors:
  - [abstract]: "We utilize existing buying guides as a knowledge source for the sales agent"
  - [section]: "Professional salespeople often receive training or rely on technical documentation to effectively sell complex products. We proxy this expert knowledge through leveraging publicly available buying guides."
- Break condition: Poorly structured, outdated, or misaligned buying guides would compromise educational value

### Mechanism 3
- Claim: Multi-dimensional evaluation framework captures unique challenges of educational CRS
- Mechanism: Evaluates recommendation quality, informativeness, and fluency independently
- Core assumption: Educational CRS success criteria differ from traditional recommendation accuracy metrics
- Evidence anchors:
  - [abstract]: "A multi-dimensional evaluation framework is designed to measure sales agent performance in terms of (a) quality of final recommendation, (b) educational value to the shopper, and (c) fluency and professionalism"
  - [section]: "Along with the SalesOps framework, we propose a multi-dimensional evaluation that defines success for the Seller along three axes"
- Break condition: Optimizing one dimension at expense of others (e.g., highly fluent but uninformative responses)

## Foundational Learning

- Concept: Large Language Models (LLMs) for conversational agents
  - Why needed here: SalesOps relies on LLM-based agents to simulate realistic sales interactions
  - Quick check question: What key limitations of LLMs does the paper address regarding faithfulness and hallucinations?

- Concept: Retrieval-augmented generation
  - Why needed here: SalesBot uses retrieval modules to search product catalog and buying guide
  - Quick check question: How does the paper structure retrieval process in SalesBot and what are the two main types?

- Concept: Evaluation of conversational systems
  - Why needed here: Introduces novel multi-dimensional evaluation framework for educational CRS
  - Quick check question: What are the three evaluation dimensions and how does each measure CRS performance?

## Architecture Onboarding

- Component map: Action Decision -> Knowledge Search -> Product Search -> Response Generation (with Regeneration as optional follow-up)
- Critical path: 1) Action Decision selects tool, 2) Query Generation creates search query, 3) Retrieval fetches relevant content, 4) Response Generation creates response using retrieved content and chat history
- Design tradeoffs: Comprehensive product catalogs (30 items) vs manageability for human sellers; gradual preference revelation for realism vs added complexity; multiple LLM components for flexibility vs increased hallucination risk
- Failure signatures: Poor recommendation quality despite high fluency; low informativeness despite good recommendations; suboptimal Action Decision tool selection; retrieval modules returning irrelevant content
- First 3 experiments:
  1. Implement basic SalesBot with all four components and test with simple product category
  2. Conduct ablation study by replacing each LLM component with simpler baselines
  3. Run small-scale human evaluation comparing SalesBot against scripted baseline seller

## Open Questions the Paper Calls Out

- Open Question 1: How does SalesBot's performance compare when preferences are fully revealed at start vs gradually revealed during conversation?
  - Basis: Paper describes gradual revelation approach but doesn't compare to alternative methods
  - Why unresolved: Focuses on gradual revelation without testing alternatives
  - Evidence needed: Controlled experiment comparing fully revealed vs gradually revealed preferences

- Open Question 2: How would SalesBot's performance change with larger and more diverse product catalogs (100+ items vs ~30)?
  - Basis: Paper deliberately limits catalog to ~30 items for realism but notes automated creation could expand it
  - Why unresolved: Focuses on constrained catalog without exploring scaling impact
  - Evidence needed: Experiments comparing performance across different catalog sizes

- Open Question 3: How does SalesBot's faithfulness compare to human salespeople in terms of upselling and unsupported claims?
  - Basis: Paper identifies upselling and answering without true answers as unfaithful behaviors in both types of sellers
  - Why unresolved: Notes both can be unfaithful but doesn't quantify or compare frequency
  - Evidence needed: Comparative analysis measuring frequency of unfaithful behaviors

## Limitations
- Limited sample size: Only 10 professional salespeople per product category in human evaluation
- Simulated scenarios: May not fully capture real-world sales interaction complexity
- Automated faithfulness detection: NLI-based method may have false positives/negatives without human validation

## Confidence
- High confidence: Framework architecture and evaluation methodology are well-specified and reproducible
- Medium confidence: Performance gap between SalesBot and human sellers is real but magnitude may vary
- Low confidence: Specific faithfulness analysis results due to potential limitations in automated detection method

## Next Checks
1. Conduct larger-scale human evaluation with 50+ participants per product category to verify performance gaps
2. Validate NLI-based faithfulness detection with human judges on subset of conversations to establish precision/recall
3. Test SalesBot on broader range of product categories (particularly non-technical ones) to assess generalizability