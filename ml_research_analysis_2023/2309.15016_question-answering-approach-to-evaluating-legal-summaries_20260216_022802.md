---
ver: rpa2
title: Question-Answering Approach to Evaluating Legal Summaries
arxiv_id: '2309.15016'
source_url: https://arxiv.org/abs/2309.15016
tags:
- gpt-4
- legal
- evaluation
- summary
- summaries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel legal summarization evaluation framework
  that utilizes GPT-4 to generate question-answer pairs covering the main points of
  reference summaries, then grades generated summaries by comparing answers derived
  from them against reference answers. The approach incorporates legal argumentative
  structure (Issue, Reason, Conclusion).
---

# Question-Answering Approach to Evaluating Legal Summaries
## Quick Facts
- arXiv ID: 2309.15016
- Source URL: https://arxiv.org/abs/2309.15016
- Reference count: 14
- Key outcome: GPT-4-based QA framework shows strong correlation (Pearson 0.72, Spearman 0.74) with human evaluation for legal summary quality assessment

## Executive Summary
This paper proposes a novel evaluation framework for legal summaries that leverages GPT-4 to generate question-answer pairs capturing the essential Issue-Reason-Conclusion argumentative structure. The framework grades generated summaries by comparing answers derived from them against reference answers, converting GPT-4's 0-10 scale grades to binary assessments. Human evaluation of 48 question-answer pairs showed 42 captured required information and all 48 answers were correct. The approach demonstrated strong correlations with human evaluation across three summarization models (LED, BART, GPT-4), suggesting it can be a useful tool for gauging legal summary quality.

## Method Summary
The method involves using GPT-4 to generate question-answer pairs from reference legal summaries annotated with Issue, Reason, and Conclusion components. These QA pairs are then used to evaluate generated summaries by having GPT-4 answer the questions based on the summary content and grade the answers against reference answers on a 0-10 scale. The grades are converted to binary "YES"/"NO" labels using a threshold of 5.0. The evaluation was conducted on 1,049 Canadian legal case summaries, with 10 summaries per model tested for correlation analysis against human evaluation.

## Key Results
- Human evaluation showed 42 out of 48 questions accurately captured required information
- All 48 answers were correct and appropriately addressed the questions
- Strong correlations with human evaluation: average Pearson 0.72 and Spearman 0.74 across models
- GPT-4 demonstrated stronger Pearson correlation (0.74) compared to BART (0.61)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 can reliably generate question-answer pairs that capture essential legal argumentative structure (Issue, Reason, Conclusion)
- Mechanism: GPT-4 is prompted to generate QA pairs based on annotated legal summaries, leveraging its understanding of legal reasoning patterns
- Core assumption: GPT-4's training data includes sufficient legal reasoning examples to understand and reproduce the Issue-Reason-Conclusion structure
- Evidence anchors: [abstract] "GPT-4 to generate a set of question-answer pairs that cover main points and information in the reference summary"; [section] "The human evaluator assessed whether the generated question-answer pairs adequately captured the necessary information and are being addressed correctly. The evaluation options were limited to 'YES' and 'NO'. Based on the results, 42 out of 48 questions accurately captured the required information, while all 48 answers were correct and appropriately addressed the questions."

### Mechanism 2
- Claim: GPT-4 can accurately grade generated summaries by comparing predicted answers against reference answers
- Mechanism: GPT-4 uses a structured evaluation prompt to assess the similarity between reference and generated answers on a 0-10 scale
- Core assumption: GPT-4 can reliably assess semantic similarity between answers and understand legal reasoning quality
- Evidence anchors: [abstract] "GPT-4 grades the answers from the reference summary and the generated summary"; [section] "We converted GPT-4 grades (0-10 scale) into binary by setting a threshold. Set a threshold at a grade of 5, then label grades 5 and above as 'YES' and grades below 5 as 'NO'"; [section] "The correlation results of BART suggest that the relationship it captures is more consistently monotonic (0.80) than linear (0.61). GPT-4 has a stronger Pearson correlation (0.74)."

### Mechanism 3
- Claim: The question-answering approach correlates strongly with human evaluation of summary quality
- Mechanism: The QA-based evaluation framework captures both factual accuracy and argumentative completeness, which are critical for legal summaries
- Core assumption: Human evaluation of legal summaries prioritizes factual correctness and argumentative structure over lexical overlap
- Evidence anchors: [abstract] "The results suggest that this question-answering approach with GPT-4 can be a useful tool for gauging the quality of the summary"; [section] "Overall, LED exhibits robust linear (0.81) and monotonic (0.83) relationships; The correlation results of BART suggest that the relationship it captures is more consistently monotonic (0.80) than linear (0.61). GPT-4 has a stronger Pearson correlation (0.74). The average correlations across all models are 0.72 (Pearson) and 0.74 (Spearman), indicating generally strong linear and monotonic relationships."

## Foundational Learning

- Concept: Legal argumentative structure (Issue, Reason, Conclusion)
  - Why needed here: The evaluation framework specifically targets these three components as essential for legal summary quality assessment
  - Quick check question: Can you identify the Issue, Reason, and Conclusion in a simple legal case summary?

- Concept: Question answering evaluation metrics
  - Why needed here: The framework converts summary evaluation into a QA task, requiring understanding of how QA metrics work
  - Quick check question: How would you design a question to test whether a summary captures the "Reason" component of a legal argument?

- Concept: Prompt engineering for LLMs
  - Why needed here: The framework relies on carefully constructed prompts for both generating QA pairs and evaluating answers
  - Quick check question: What key elements should be included in a prompt to evaluate legal answers on a 0-10 scale?

## Architecture Onboarding

- Component map: Reference summary processing -> GPT-4 QA generation -> Reference QA pairs -> Generated summary -> GPT-4 answer prediction -> Predicted answers -> Reference QA pairs + Predicted answers -> GPT-4 grading -> Quality score -> Human evaluation -> Correlation analysis -> Framework validation

- Critical path: Reference summary -> GPT-4 QA generation -> Generated summary -> GPT-4 answer prediction -> GPT-4 grading -> Quality score

- Design tradeoffs:
  - GPT-4 vs. fine-tuned models: GPT-4 provides zero-shot capability but at higher cost
  - Threshold selection: Binary conversion at 5.0 balances sensitivity and specificity
  - Question types: Focusing on Issue/Reason/Conclusion may miss other quality aspects

- Failure signatures:
  - Low correlation with human evaluation (below 0.5 Pearson)
  - GPT-4 generating hallucinated answers not present in source
  - Inconsistent grading across similar answer pairs
  - Questions that don't capture essential information (as identified by human evaluators)

- First 3 experiments:
  1. Test GPT-4 QA generation with simplified legal summaries to verify structure understanding
  2. Compare GPT-4 grading against multiple human evaluators on the same answer pairs
  3. Evaluate different threshold values (4.5, 5.0, 5.5) to optimize binary classification accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt constructions affect GPT-4's evaluation performance for legal summaries?
- Basis in paper: [explicit] The paper states "Since GPT-4's performance as an evaluation metric is sensitive to the construction of prompts, we plan to explore various prompts to achieve better performance."
- Why unresolved: The paper only used a single prompt structure for generating questions and evaluating answers, but did not explore variations in prompt construction.
- What evidence would resolve it: Comparative studies testing different prompt formulations and their impact on evaluation accuracy and correlation with human judgments.

### Open Question 2
- Question: How does the QA evaluation approach scale to larger datasets and more complex legal documents?
- Basis in paper: [explicit] The paper notes "We need to scale up the experiment and show more robust comparison results."
- Why unresolved: The current study only evaluated 10 summaries per model, which may not be representative of broader performance.
- What evidence would resolve it: Testing the approach on larger datasets with hundreds or thousands of summaries, including more diverse legal cases and document structures.

### Open Question 3
- Question: How can hallucination and information beyond the generated summary be controlled in GPT-4's answers?
- Basis in paper: [explicit] The paper mentions "The human evaluator assessed whether the generated answer correctly addresses the given question in relation to the model-generated summary. During the assessment, the human evaluator found some of the answers are legally correct, but it goes well beyond the information provided in the generated summary. Hallucination is also found in some answers as well."
- Why unresolved: The current approach doesn't address how to prevent GPT-4 from generating answers that go beyond the content of the summary or hallucinate information.
- What evidence would resolve it: Developing and testing techniques to constrain GPT-4's answers to only information present in the summary, such as using source attribution or answer verification mechanisms.

## Limitations
- Small human evaluation sample size (48 question-answer pairs) may not represent the full 1,049-case dataset
- Binary conversion of GPT-4 grades uses an arbitrary threshold of 5.0 that may not optimize for legal domain
- Reliance on GPT-4 for both QA generation and grading creates potential for compounding errors

## Confidence
**High confidence**: The strong correlations between GPT-4 grades and human evaluation (Pearson 0.72, Spearman 0.74) are well-supported by the data and methodology. The approach of using QA pairs to evaluate legal summaries is conceptually sound given the argumentative nature of legal reasoning.

**Medium confidence**: The claim that GPT-4 can reliably generate QA pairs capturing Issue-Reason-Conclusion structure is supported but limited by the small human evaluation sample. The generalizability to other legal domains or jurisdictions remains untested.

**Low confidence**: The framework's performance across different types of legal cases (criminal, civil, constitutional) is not evaluated. The impact of prompt engineering variations on evaluation quality is not explored.

## Next Checks
1. **Scale human evaluation**: Expand human evaluation from 48 to 200+ question-answer pairs across different legal case types to better assess GPT-4's QA generation reliability.
2. **Cross-jurisdiction validation**: Test the framework on legal summaries from different jurisdictions (US, UK, EU) to evaluate generalizability beyond the Canadian dataset.
3. **Threshold optimization study**: Systematically evaluate different binary threshold values (4.0, 4.5, 5.0, 5.5, 6.0) to determine optimal classification performance for legal summary quality assessment.