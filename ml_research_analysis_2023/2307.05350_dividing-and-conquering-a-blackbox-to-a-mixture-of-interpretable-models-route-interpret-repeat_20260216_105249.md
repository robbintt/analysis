---
ver: rpa2
title: 'Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route,
  Interpret, Repeat'
arxiv_id: '2307.05350'
source_url: https://arxiv.org/abs/2307.05350
tags:
- latexit
- sha1
- base64
- blackbox
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MoIE, a method to iteratively extract interpretable
  models from a trained blackbox by leveraging its flexibility. The core idea is to
  route samples through a mixture of interpretable experts (MoIE) and a residual network,
  repeating the process until the residual is minimal.
---

# Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat

## Quick Facts
- **arXiv ID**: 2307.05350
- **Source URL**: https://arxiv.org/abs/2307.05350
- **Reference count**: 40
- **Key outcome**: Introduces MoIE method to iteratively extract interpretable models from blackbox by routing samples through mixture of interpretable experts and residual network, achieving performance comparable to blackbox while providing local FOL explanations

## Executive Summary
This paper introduces MoIE, a method to iteratively extract interpretable models from a trained blackbox by leveraging its flexibility. The core idea is to route samples through a mixture of interpretable experts (MoIE) and a residual network, repeating the process until the residual is minimal. Each expert provides local explanations in First Order Logic (FOL) using concepts extracted from the blackbox, capturing diverse explanations for different samples. Experiments on vision and medical imaging datasets show MoIE achieves performance comparable to the blackbox while maintaining interpretability. It identifies "harder" samples in residuals and outperforms interpretable-by-design models in test-time interventions. MoIE also successfully eliminates shortcut biases in the blackbox.

## Method Summary
MoIE iteratively extracts interpretable models from a trained blackbox by routing samples through a mixture of interpretable experts and a residual network. Each expert provides local FOL explanations using concepts extracted from the blackbox, while the residual captures "harder" samples. The process repeats until the residual falls below a threshold, producing a final model that combines interpretable experts with the residual network.

## Key Results
- MoIE achieves performance comparable to blackbox on CUB-200, Awa2, and HAM10000 datasets
- Local FOL explanations capture diverse concepts for different samples
- Residual network identifies "harder" samples requiring more complex explanations
- Successfully eliminates shortcut biases in the Waterbirds dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The mixture of interpretable experts (MoIE) can explain a diverse set of samples by learning multiple local FOL explanations, each specialized for a subset of the data.
- Mechanism: MoIE uses a learnable selector πk to route each sample to an interpretable expert gk with probability πk(cj), or to a residual network with probability 1-πk(cj). The expert gk is trained to explain the Blackbox f k-1 using First Order Logic (FOL) based on concepts c extracted from the Blackbox. This allows each expert to specialize in explaining a subset of samples with a different local FOL.
- Core assumption: A single interpretable model cannot capture the full diversity of explanations used by the Blackbox across all samples. Multiple experts are needed to explain different subsets of the data.
- Evidence anchors:
  - [abstract] "Each expert provides local explanations in First Order Logic (FOL) using concepts extracted from the blackbox, capturing diverse explanations for different samples."
  - [section] "As the baseline includes a single interpretable model g, all the FOLs for each sample per class incorporate identical concepts... MoIE employs the concepts on a per-sample basis, so different experts adopt different concepts to classify the samples of a particular class."
- Break condition: If the coverage of the experts is low, or if the experts start explaining the same samples, the diversity assumption breaks down.

### Mechanism 2
- Claim: The residual network rk = f k-1 - gk captures the "harder" samples that the interpretable expert gk cannot explain, and routing these samples through subsequent experts improves overall performance.
- Mechanism: After training an interpretable expert gk, the residual rk is computed as the difference between the Blackbox f k-1 and the expert's predictions gk. This residual is then approximated by a new Blackbox f k, which becomes the input to the next iteration. The final residual covers the hardest samples that require the most complex explanations.
- Core assumption: The Blackbox's predictions can be decomposed into interpretable and uninterpretable components, and the uninterpretable component becomes progressively harder to explain.
- Evidence anchors:
  - [abstract] "We route the remaining samples through a flexible residual network. We repeat the method on the residual network until the proportion of data explained by the residual network falls below a desired threshold."
  - [section] "As shown in Figure 5a, the VIT-derived expert and residual cumulatively achieve an accuracy of approximately 91% for the CUB-200 dataset in iteration 1, where the residual's contribution (black bar) is greater than the expert1's (blue bar). Later iterations reveal that the MoIE's performances cumulatively increase, and the residual worsens."
- Break condition: If the residual's performance does not improve over iterations, or if the residual covers too many samples, the decomposition assumption breaks down.

### Mechanism 3
- Claim: The MoIE model maintains performance comparable to the original Blackbox while providing interpretability, by combining the interpretable experts and the final residual.
- Mechanism: The final MoIE model consists of all the interpretable experts g1,...,gK and the final residual f K. The model routes each sample to the appropriate expert or residual based on the selector probabilities π1,...,πK. The combined model achieves similar accuracy to the Blackbox on the training data, while providing local FOL explanations for the samples covered by the experts.
- Core assumption: The interpretable experts and residual can together approximate the Blackbox's predictions without significant loss of accuracy.
- Evidence anchors:
  - [abstract] "Our extensive experiments show that our route, interpret, and repeat approach (1) identifies a diverse set of instance-specific concepts with high concept completeness via MoIE without compromising in performance..."
  - [section] "Figure 4 displays the performance of our method using a heldout test set. "MoIE" and "MoIE + Residual" denote the mixture of interpretable experts (g) excluding and including the final residual, respectively. Coverage (Cov in the figure) represents the proportion of samples in the test set covered by all the experts throughout iterations. For CUB-200, Awa2, and HAM10000, MoIE achieves similar performance to the Blackbox."
- Break condition: If the MoIE model's accuracy drops significantly compared to the Blackbox, or if the interpretable experts cannot maintain performance on their covered samples.

## Foundational Learning

- **Concept**: First Order Logic (FOL) and concept-based explanations
  - Why needed here: MoIE uses FOL to provide local explanations for each interpretable expert, based on human-interpretable concepts extracted from the Blackbox.
  - Quick check question: What is the difference between FOL and other explanation methods like saliency maps or LIME?

- **Concept**: Residual networks and knowledge distillation
  - Why needed here: MoIE uses residual networks to capture the "harder" samples that the interpretable experts cannot explain, and knowledge distillation to train the experts to approximate the Blackbox.
  - Quick check question: How does knowledge distillation work, and how is it used to train interpretable models to approximate Blackbox models?

- **Concept**: Concept extraction and bottleneck models
  - Why needed here: MoIE extracts human-interpretable concepts from the Blackbox's internal representations, and uses concept bottleneck models to learn the mapping from concepts to predictions.
  - Quick check question: What is a concept bottleneck model, and how does it differ from a regular neural network?

## Architecture Onboarding

- **Component map**: Image → Concept extraction → Selector routing → Expert or residual → Prediction and explanation
- **Critical path**: Image → Concept extractor ϕ → Selector πk → Interpretable expert gk or Residual network f k → Prediction and FOL explanation
- **Design tradeoffs**:
  - Number of experts K vs. complexity of each expert
  - Coverage threshold τ vs. performance of experts
  - Concept extraction method vs. interpretability of explanations
- **Failure signatures**:
  - Low coverage of experts → Experts not learning meaningful partitions
  - Poor performance of experts → Concepts not discriminative enough
  - High complexity of FOL explanations → Concepts not human-interpretable
- **First 3 experiments**:
  1. Train MoIE on CUB-200 with ResNet-101 Blackbox, analyze coverage and performance of experts
  2. Visualize local FOL explanations for a few samples, check for diversity and interpretability
  3. Compare MoIE performance to baseline interpretable models and Blackbox on held-out test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MoIE's interpretability change when applied to datasets with more complex or higher-dimensional concept spaces?
- Basis in paper: [inferred] The paper primarily tests MoIE on datasets with moderate concept complexity (CUB-200, Awa2, HAM10000, MIMIC-CXR). The performance and interpretability may degrade or improve depending on concept space dimensionality and structure.
- Why unresolved: The paper does not explore scalability or performance in higher-dimensional or more abstract concept spaces.
- What evidence would resolve it: Experiments on datasets with 100+ concepts, or high-dimensional concept spaces like those in medical imaging or natural language processing.

### Open Question 2
- Question: Can MoIE's interpretable models be used for direct model correction or retraining, rather than just explanation?
- Basis in paper: [inferred] The paper demonstrates fixing shortcut biases in the Waterbirds dataset using FOL explanations, but doesn't explore systematic retraining or model correction based on MoIE's interpretable components.
- Why unresolved: The paper focuses on explanation and bias detection, not on using MoIE for active model improvement or retraining.
- What evidence would resolve it: Case studies where MoIE's interpretable components are used to retrain or fine-tune the original blackbox model for improved generalization or bias reduction.

### Open Question 3
- Question: How does MoIE's performance and interpretability scale with increasing number of iterations (K)?
- Basis in paper: [inferred] The paper stops iterations when 85% of samples are explained or residual accuracy drops below 70%, but does not explore the trade-offs of more iterations (e.g., diminishing returns, overfitting, interpretability loss).
- Why unresolved: The paper does not analyze the impact of varying K on model performance, interpretability, or computational cost.
- What evidence would resolve it: Experiments with varying K (e.g., 2, 4, 6, 8 iterations) on the same datasets, analyzing performance, interpretability, and computational overhead.

## Limitations
- Coverage threshold of 0.7 appears arbitrary and may not be optimal for all datasets
- Limited validation of extracted FOL explanations' correctness and consistency
- Effectiveness of shortcut bias elimination on complex real-world datasets remains unclear

## Confidence
- **High Confidence**: The core mechanism of iterative routing through interpretable experts and residuals (Mechanism 2) is well-supported by experimental results showing progressive improvement across iterations and decreasing residual accuracy.
- **Medium Confidence**: The diversity claim for local FOL explanations (Mechanism 1) has reasonable theoretical grounding and partial experimental support, though direct evidence of explanation diversity could be stronger.
- **Medium Confidence**: The performance maintenance claim (Mechanism 3) is well-demonstrated on multiple datasets, but the extent to which this holds for more challenging real-world scenarios requires further validation.

## Next Checks
1. **Coverage Sensitivity Analysis**: Systematically vary the coverage threshold τ from 0.5 to 0.9 and measure impacts on performance, interpretability quality, and computational efficiency to establish optimal threshold ranges.

2. **Explanation Consistency Testing**: For a subset of samples covered by multiple experts across iterations, verify whether the extracted FOL explanations remain logically consistent and whether they converge to simpler explanations over successive iterations.

3. **Real-world Generalization Study**: Apply MoIE to a domain with known, complex biases (e.g., medical imaging with demographic confounding) and evaluate both the identification of these biases through residuals and the effectiveness of the extracted explanations in revealing the underlying decision-making process.