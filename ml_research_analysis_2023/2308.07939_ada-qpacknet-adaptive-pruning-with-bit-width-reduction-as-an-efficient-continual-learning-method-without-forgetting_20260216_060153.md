---
ver: rpa2
title: Ada-QPacknet -- adaptive pruning with bit width reduction as an efficient continual
  learning method without forgetting
arxiv_id: '2308.07939'
source_url: https://arxiv.org/abs/2308.07939
tags:
- pruning
- learning
- ada-qpacknet
- quantization
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Ada-QPacknet, a novel continual learning method
  that combines adaptive pruning and adaptive quantization to address catastrophic
  forgetting while efficiently utilizing model capacity. The key innovation is an
  iterative process that first applies adaptive pruning with layer-specific sparsity
  levels identified through a fast lottery ticket search, followed by adaptive quantization
  that splits each weight into components with reduced bit-widths.
---

# Ada-QPacknet -- adaptive pruning with bit width reduction as an efficient continual learning method without forgetting

## Quick Facts
- arXiv ID: 2308.07939
- Source URL: https://arxiv.org/abs/2308.07939
- Reference count: 40
- Primary result: Achieves 97.1% accuracy on p-MNIST while using 81.25% less model capacity than PackNet

## Executive Summary
Ada-QPacknet is a continual learning method that combines adaptive pruning and adaptive quantization to prevent catastrophic forgetting while efficiently utilizing model capacity. The method first applies layer-specific pruning identified through fast lottery ticket search, then performs adaptive quantization that splits each weight into components with reduced bit-widths. This allows multiple tasks to be assigned to a single weight without significant accuracy loss. Experiments on multiple benchmarks demonstrate state-of-the-art performance with significant capacity savings.

## Method Summary
Ada-QPacknet works through an iterative process where each new task is handled by first applying adaptive pruning with layer-specific sparsity levels identified via fast lottery ticket search, followed by adaptive quantization that splits weights into multiple components with reduced bit-widths. The pruning stage uses random lottery ticket masks to identify optimal subnetworks for each task, while the quantization stage employs non-linear K-Means clustering to determine the minimal bit-width needed to preserve accuracy. Each 32-bit weight can be split into multiple components (e.g., four 8-bit components), allowing weight reuse across tasks and reducing overall model capacity requirements.

## Key Results
- Achieves 97.1% accuracy on p-MNIST, outperforming Cumulative (96.45%) and PackNet (95.52%)
- Uses only 81.25% of model capacity on p-MNIST compared to 96.38% for PackNet
- Maintains competitive accuracy (65.07%) on Imagenet100 while using only 49.23% capacity
- Outperforms state-of-the-art methods across multiple benchmarks including s-CIFAR100, 5-datasets, and TinyImagenet scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive pruning with layer-specific sparsity levels prevents catastrophic forgetting while preserving accuracy.
- Mechanism: Each layer is pruned using fast lottery ticket search to find the optimal subset of weights for each task, rather than using a constant sparsity ratio across all layers.
- Core assumption: Different layers have different sensitivities to pruning, and identifying task-specific subnetworks preserves task performance.
- Evidence anchors:
  - [abstract] "Our proposed Ada-QPacknet incorporates adaptive pruning with different sparsity levels for each layer, where pruning ratios are chosen via fast lottery ticket search."
  - [section 3.1] "One important pitfall of existing pruning-based architectural forget-free methods is that they are limited in their ability to efficiently exploit model sparsity since each layer is pruned with the same constant sparsity level."

### Mechanism 2
- Claim: Adaptive quantization splits weights into multiple components with reduced bit-widths, allowing efficient weight reuse across tasks.
- Mechanism: Each 32-bit weight is divided into multiple components (e.g., four 8-bit components), where each component can be assigned to a different task without significant accuracy loss.
- Core assumption: The precision required for each task varies, and adaptive bit-width allocation can preserve accuracy while maximizing weight reuse.
- Evidence anchors:
  - [abstract] "Ada-QPacknet performs an adaptive quantization stage that separates each weight into multiple components, each using a subset of the available 32 bits."
  - [section 3.2] "This separation allows us to reuse a single weight for more than one task, leading to reduced use of models' capacity and improvements in terms of model efficiency."

### Mechanism 3
- Claim: Combining adaptive pruning and quantization creates a synergistic effect that outperforms individual approaches.
- Mechanism: Pruning reduces the number of weights needed per task, while quantization increases the number of tasks that can share each weight, resulting in optimal capacity utilization.
- Core assumption: The combined approach leverages the complementary strengths of both compression techniques.
- Evidence anchors:
  - [section 4.3] "On average, pruning allows to free a significant amount of model capacity (45.0% for p-MNIST, 50.0% for s-CIFAR100, 52% for 5-datasets). On the other hand, quantization frees, on average, a higher amount of capacity (86.58%) for all three scenarios."

## Foundational Learning

- Concept: Lottery Ticket Hypothesis
  - Why needed here: The adaptive pruning mechanism relies on finding "winning tickets" - subnetworks that can solve tasks with minimal weights.
  - Quick check question: What is the key insight of the Lottery Ticket Hypothesis that makes it relevant for continual learning?

- Concept: Quantization-aware training
  - Why needed here: Adaptive quantization requires understanding how bit-width reduction affects model accuracy and how to minimize this impact.
  - Quick check question: How does non-linear quantization differ from linear quantization in terms of preserving model accuracy?

- Concept: Catastrophic forgetting
  - Why needed here: The core problem Ada-QPacknet addresses is preventing catastrophic forgetting while learning new tasks.
  - Quick check question: What distinguishes architectural approaches from regularization approaches in preventing catastrophic forgetting?

## Architecture Onboarding

- Component map:
  Input pipeline → Adaptive Pruning module → Adaptive Quantization module → Output predictions
- Critical path:
  1. Task arrives → Adaptive pruning identifies optimal subnetworks
  2. Subnetworks are quantized to optimal bit-widths
  3. Codebooks are created and weights are split into components
  4. Components are assigned to tasks
- Design tradeoffs:
  - Higher sparsity vs. accuracy retention
  - Lower bit-width vs. model performance
  - Number of tasks vs. component size
  - Search population size vs. computation time
- Failure signatures:
  - Accuracy drops after pruning: indicates too aggressive sparsity
  - Accuracy drops after quantization: indicates insufficient bit-width
  - Model capacity saturation: indicates need for better weight reuse strategy
- First 3 experiments:
  1. Run Ada-QPacknet on p-MNIST with default parameters and verify it achieves ~97% accuracy
  2. Test sensitivity by varying sparsity levels and measuring accuracy degradation
  3. Test quantization sensitivity by varying bit-widths and measuring accuracy impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal trade-off between bit-width reduction and accuracy across different types of datasets and model architectures?
- Basis in paper: [explicit] The paper mentions adaptive quantization identifies optimal bit-width for each task based on trade-off between bits assigned and model performance, but does not provide a general formula or framework for determining this optimal balance across different scenarios.
- Why unresolved: The paper demonstrates adaptive quantization works well empirically but doesn't establish a theoretical framework or systematic method for determining optimal bit-width reduction levels for arbitrary tasks and architectures.
- What evidence would resolve it: Systematic experiments across diverse datasets, architectures, and task distributions showing how bit-width reduction impacts accuracy in predictable ways, potentially leading to a predictive model for optimal quantization.

### Open Question 2
- Question: How does the method scale to very large-scale scenarios like full ImageNet with 1000 classes?
- Basis in paper: [explicit] The paper mentions they did not perform extensive hyperparameter optimization for the ImageNet100 scenario due to computational constraints, suggesting scalability challenges for even larger datasets.
- Why unresolved: The paper only tests on relatively small-scale scenarios (p-MNIST, s-CIFAR100, 5-datasets, TinyImagenet, and ImageNet100 with 100 classes), leaving uncertainty about performance on truly large-scale problems.
- What evidence would resolve it: Comprehensive experiments on full ImageNet and other large-scale benchmarks, including analysis of computational costs and memory requirements at scale.

### Open Question 3
- Question: What is the relationship between task similarity and the effectiveness of weight sharing in the proposed method?
- Basis in paper: [explicit] The paper suggests that weight sharing provides advantage for Packnet and WSN in homogeneous tasks from single dataset, while Ada-QPacknet performs better in heterogeneous scenarios like 5-datasets where task similarity is low.
- Why unresolved: The paper provides some empirical evidence but doesn't develop a theoretical understanding of how task similarity affects the trade-offs between weight sharing and adaptive quantization approaches.
- What evidence would resolve it: Systematic studies varying task similarity across a spectrum, quantifying the relationship between similarity metrics and performance gains from different CL strategies.

## Limitations
- The method's scalability to very large models or long task sequences remains unproven, as experiments were limited to 5-10 tasks
- The effectiveness of the fast lottery ticket search may be limited by the relatively small population size (PS=16) and short search duration (5 epochs)
- The adaptive quantization mechanism's performance heavily depends on the accuracy threshold δ used for bit-width selection, which is not explicitly specified

## Confidence

**High Confidence**: The core mechanism of combining adaptive pruning with layer-specific sparsity levels is well-supported by empirical results showing significant capacity reduction (45-52% pruning gains) while maintaining accuracy.

**Medium Confidence**: The adaptive quantization approach's effectiveness is demonstrated through capacity gains (86.58% on average) but the bit-width adaptation threshold and clustering details are underspecified.

**Low Confidence**: The synergistic claims about combined pruning+quantization outperforming individual approaches are primarily based on aggregate capacity metrics rather than systematic ablation studies isolating each component's contribution.

## Next Checks
1. **Ablation Study**: Systematically disable pruning or quantization separately on p-MNIST to quantify each component's individual contribution to accuracy and capacity metrics.
2. **Threshold Sensitivity Analysis**: Vary the accuracy drop threshold δ across a range of values (e.g., 0.1% to 2%) and measure the impact on final accuracy and bit-width distribution.
3. **Long-sequence Scalability Test**: Extend experiments to 20+ tasks on p-MNIST to evaluate performance degradation and capacity saturation over longer task sequences.