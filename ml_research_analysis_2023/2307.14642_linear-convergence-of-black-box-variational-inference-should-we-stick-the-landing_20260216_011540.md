---
ver: rpa2
title: 'Linear Convergence of Black-Box Variational Inference: Should We Stick the
  Landing?'
arxiv_id: '2307.14642'
source_url: https://arxiv.org/abs/2307.14642
tags:
- u1d740
- ight
- enle
- u1d451
- uni23b4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proves that black-box variational inference (BBVI) with
  the sticking-the-landing (STL) gradient estimator converges at a linear rate when
  the variational family is perfectly specified. The authors establish a quadratic
  bound on the gradient variance of the STL estimator, which encompasses both well-specified
  and misspecified variational families.
---

# Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?

## Quick Facts
- arXiv ID: 2307.14642
- Source URL: https://arxiv.org/abs/2307.14642
- Reference count: 40
- The paper proves that black-box variational inference with the sticking-the-landing gradient estimator converges at a linear rate when the variational family is perfectly specified.

## Executive Summary
This paper establishes linear convergence rates for black-box variational inference using the sticking-the-landing (STL) gradient estimator under perfect variational family specification. The authors prove that STL satisfies a quadratic variance condition, enabling convergence analysis via projected stochastic gradient descent. By establishing a quadratic bound on the gradient variance of STL, the analysis encompasses both well-specified and misspecified variational families, providing explicit non-asymptotic complexity guarantees that show STL achieves $O(d^2 \log(1/\epsilon))$ complexity under perfect specification.

## Method Summary
The paper analyzes black-box variational inference using two gradient estimators: closed-form entropy (CFE) and sticking-the-landing (STL). The analysis relies on the quadratic variance condition framework, which bounds gradient variance quadratically in the distance from optimal parameters. The STL estimator implicitly subtracts a gradient term from current parameters, canceling the entropy gradient at convergence and eliminating gradient variance at the optimum. The authors establish adaptive variance bounds using the Peter-Paul inequality and apply them to projected stochastic gradient descent to derive convergence rates. The analysis covers both full-rank and mean-field variational parameterizations with fixed and decreasing step sizes.

## Key Results
- STL estimator achieves linear convergence ($O(d^2 \log(1/\epsilon))$) under perfect variational specification
- The quadratic variance condition bounds gradient variance quadratically in parameter distance from optimum
- Adaptive bounds with the Peter-Paul inequality tighten gradient variance analysis and automatically adapt to target accuracy
- Analysis improves existing bounds on CFE estimator, enabling fair comparison between STL and CFE methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: STL achieves linear convergence when the variational family perfectly matches the true posterior because it satisfies the interpolation condition with zero gradient variance at the optimum.
- Mechanism: STL acts as a control variate method by implicitly subtracting a gradient term from current parameters, canceling the entropy gradient at convergence and eliminating gradient variance at optimal variational parameters.
- Core assumption: The true posterior is contained within the variational family (perfect specification), making Fisher divergence between optimal variational and true posteriors zero.
- Evidence anchors: [abstract] "We prove that black-box variational inference (BBVI) with control variates, particularly the sticking-the-landing (STL) estimator, converges at a geometric... rate under perfect variational family specification"; [section 3.1.2] "When the variational family is well specified such that DF4 (u1D45E)minute(var)asteriskmath, u1D70B) = 0, we obtain interpolation such that u1D6FDSTL = 0."
- Break condition: Misspecification (DF4 > 0) reintroduces non-zero variance, degrading convergence to u1D4AA(1/u1D716) rate.

### Mechanism 2
- Claim: The quadratic variance condition bounds gradient variance quadratically in parameter distance from optimum, enabling convergence analysis for projected SGD.
- Mechanism: For any gradient estimator, if u1D53C‖u1D65C(u1D740)‖2u2262 ≤ u1D6FC‖u1D740 − u1D740asteriskmath‖2u2262 + u1D6FD, then projected SGD with appropriate stepsize achieves convergence. STL satisfies this bound adaptively with free parameter u1D6FF that trades off u1D6FC and u1D6FD.
- Core assumption: The ELBO is strongly log-concave and u1D43F-log-smooth, ensuring existence of constants in the QV bound.
- Evidence anchors: [section 2.5] "A gradient estimator u1D65C is said to satisfy the quadratic variance condition if the following bound holds: u1D53C‖u1D65C(u1D740)‖2u2262 ≤ u1D6FC‖u1D740 − u1D740asteriskmath‖2u2262 + u1D6FD"; [section 3.1.2] "For the full-rank parameterization, the expected-squared norm of the STL estimator is bounded as [quadratic bound expression]"
- Break condition: Non-log-concave posteriors or improper parameterizations break strong convexity, invalidating the QV condition.

### Mechanism 3
- Claim: Adaptive bounds with the Peter-Paul inequality tighten gradient variance analysis by introducing free parameter that optimizes tradeoff between u1D6FC and u1D6FD.
- Mechanism: Using (u1D44E + u1D44F)u2262 ≤ (1 + u1D6FF)u1D44E2 + (1 + u1D6FFu22121)u1D44F2, analysis becomes adaptive to target accuracy u1D716, automatically balancing convergence speed and variance terms.
- Core assumption: Gradient estimator can be decomposed into components where Peter-Paul inequality applies, and free parameter u1D6FF can be optimized for downstream complexity analysis.
- Evidence anchors: [section 3.1.1] "We call these bounds adaptive QV bounds. Assumption 3 (Adaptive QV). The gradient estimator u1D65C satisfies the bound [adaptive QV expression]"; [section 3.4] "For the complexity guarantees for strongly convex objectives... it is possible to optimize the free parameter u1D6FF in the bounds, such that they automatically adapt to other problem-specific constants."
- Break condition: If gradient decomposition doesn't naturally separate into required form, adaptive bound construction fails.

## Foundational Learning

- Concept: Quadratic Variance Condition (QVC)
  - Why needed here: Provides theoretical foundation for proving convergence of stochastic gradient methods with non-standard gradient estimators like STL.
  - Quick check question: What constants must be bounded in the QVC for projected SGD to converge with rate u1D4AA(1/u1D716)?

- Concept: Fisher-Hyvärinen Divergence
  - Why needed here: Measures mismatch between score functions of variational and true posteriors, crucial for analyzing STL estimator behavior under misspecification.
  - Quick check question: How does DF4 (u1D45E)minute(var)asteriskmath, u1D70B) relate to gradient variance of the STL estimator?

- Concept: Strong Log-Convexity and Log-Smoothness
  - Why needed here: Ensures ELBO has favorable curvature properties required for both QV bounds and complexity analysis.
  - Quick check question: Why does condition number u1D705 = u1D43F/u1D707 appear in iteration complexity bounds?

## Architecture Onboarding

- Component map: Variational family → Gradient estimator selection → QV bound establishment → Projected SGD convergence → Complexity analysis
- Critical path: Variational family → Gradient estimator selection → QV bound establishment → Projected SGD convergence → Complexity analysis
- Design tradeoffs:
  - Full-rank vs mean-field: Full-rank has better dimension dependence (u1D451) but higher per-iteration cost; mean-field has u1D4AA(√u1D451) dependence but is looser
  - Stepsize schedule: Fixed stepsize achieves better u1D716-dependence; decreasing schedule trades u1D716 rate for simplicity
  - Parameterization: Triangular scale matrices reduce gradient variance but break theoretical guarantees due to entropy bounds
- Failure signatures:
  - Non-convergence: Indicates QV constants not properly bounded or stepsize too large
  - Slow convergence: Suggests high Fisher divergence under misspecification or poor stepsize choice
  - Numerical instability: May indicate ill-conditioned scale parameters or inappropriate domain constraints
- First 3 experiments:
  1. Verify STL achieves lower gradient variance than CFE on a well-specified Gaussian posterior
  2. Test convergence rates of STL vs CFE under varying degrees of misspecification (controlled DF4)
  3. Compare fixed vs decreasing stepsize schedules on strongly log-concave posteriors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the STL estimator provide better overall convergence speed compared to the CFE estimator when the variational family is misspecified?
- Basis in paper: Explicit. The paper states that under misspecification, STL has an O(1/ε) dependence on the 4th order Fisher divergence DF4(π, q) > 0, and suggests comparing DF4(π, q) versus ||q - p||² to evaluate performance.
- Why unresolved: The paper provides theoretical bounds but does not empirically compare STL and CFE performance under misspecification, leaving practical impact unclear.
- What evidence would resolve it: Empirical studies comparing STL and CFE convergence rates on various misspecified posteriors with different levels of Fisher divergence would clarify which estimator performs better in practice.

### Open Question 2
- Question: Can the STL estimator be effectively used with proximal SGD to achieve faster convergence than projected SGD?
- Basis in paper: Explicit. The paper discusses that STL requires bounded entropy (Λκ) for variance bounds, which conflicts with proximal SGD's projection-free nature, making theoretical guarantees challenging.
- Why unresolved: While STL estimator is theoretically sound for projected SGD, paper highlights fundamental incompatibility with proximal SGD due to projection requirements, leaving open whether practical solutions exist.
- What evidence would resolve it: Development and empirical validation of modified STL estimator or proximal operator that maintains variance bounds without projection would demonstrate feasibility.

### Open Question 3
- Question: What assumptions could rule out the worst-case scenarios that make the STL lower bounds loose by a factor of 2?
- Basis in paper: Inferred. The paper notes that factor of 2 looseness in lower bounds stems from worst-case scenarios where gradients are perfectly anti-correlated, and mentions unsuccessful attempts to apply gradient monotonicity/coercivity.
- Why unresolved: The paper explicitly states that general assumptions to eliminate these worst cases were not found, suggesting gap in understanding relationship between gradient properties and variance bounds.
- What evidence would resolve it: Identification of natural conditions on posterior distributions (e.g., log-concave properties) that preclude anti-correlated gradients would tighten theoretical bounds and improve practical predictions.

## Limitations
- Analysis relies on strong assumptions including perfect variational specification and strongly log-concave, u1D43F-log-smooth posteriors
- Quadratic variance condition framework assumes access to tight bounds on gradient variance that can be challenging to establish for complex models
- Complexity bounds depend on problem-specific constants that may be difficult to estimate in real applications

## Confidence
- High confidence: The quadratic variance bound for STL under perfect specification is rigorously proven
- Medium confidence: The extension to misspecified cases provides useful bounds but convergence rates are weaker
- Medium confidence: The comparison between STL and CFE estimators is fair but depends on specific problem constants

## Next Checks
1. Empirically verify the linear convergence rate of STL on a well-specified Gaussian posterior with known ground truth
2. Test STL convergence under controlled misspecification by varying the Fisher divergence between variational and true posteriors
3. Compare the empirical variance of STL vs CFE estimators across different problem dimensions and parameterizations