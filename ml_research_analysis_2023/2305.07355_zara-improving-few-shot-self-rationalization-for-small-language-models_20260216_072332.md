---
ver: rpa2
title: 'ZARA: Improving Few-Shot Self-Rationalization for Small Language Models'
arxiv_id: '2305.07355'
source_url: https://arxiv.org/abs/2305.07355
tags:
- language
- plausibility
- zara
- linguistics
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ZARA, a novel approach to improve few-shot
  self-rationalization for small language models. The key idea is to leverage explanations
  for small LMs by constructing pseudo-parallel data for self-training using a zero-shot
  plausibility approximator.
---

# ZARA: Improving Few-Shot Self-Rationalization for Small Language Models

## Quick Facts
- arXiv ID: 2305.07355
- Source URL: https://arxiv.org/abs/2305.07355
- Reference count: 24
- Key outcome: ZARA achieves state-of-the-art performance on FEB benchmark, improving task accuracy by 3.4%-5.1% and explanation metrics by 3.0%-5.8%

## Executive Summary
ZARA introduces a novel approach to improve few-shot self-rationalization for small language models by leveraging explanations through a zero-shot plausibility approximator. The method constructs pseudo-parallel data for self-training by automatically assessing the plausibility of generated rationales without ground-truth labels. Experiments on the FEB benchmark demonstrate significant improvements over standard few-shot prompting, with both quantitative and human evaluations validating the effectiveness of the approach.

## Method Summary
ZARA operates through a two-stage self-training framework. First, a unifiedQA model is fine-tuned on few-shot labeled data from the FEB benchmark. The model then generates predictions on unlabeled instances, which are mapped to NLI format and evaluated using zero-shot NLI models to score plausibility. High-confidence rationale-answer pairs are selected based on plausibility thresholds and used to augment the training data, followed by re-training of the unifiedQA model on this expanded dataset.

## Key Results
- Achieves 3.4%-5.1% improvement in task accuracy over FEB baseline
- Improves explanation metrics by 3.0%-5.8% (BERTScore)
- Demonstrates effectiveness across multiple small unifiedQA model sizes (base, large, 3B)
- Shows strong correlation between plausible rationales and correct answer predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Plausible rationales imply correct answer predictions
- Core assumption: The correlation between rationale plausibility and answer correctness is strong enough to use plausibility as a proxy for answer quality
- Evidence anchors:
  - [abstract]: "plausible explanations imply correct end-task predictions"
  - [section 4]: "when the explanations are judged as plausible, they are much more likely paired with correctly predicted answers in contrast to implausible ones"
- Break condition: If the correlation between plausibility and correctness weakens, the self-training approach would fail

### Mechanism 2
- Claim: Reducing plausibility judgment to NLI enables zero-shot evaluation
- Core assumption: The NLI classification can effectively proxy for human plausibility judgment of rationales
- Evidence anchors:
  - [abstract]: "reduce the problem of plausibility judgement to natural language inference"
  - [section 5.1]: "we reduce the problem of judging the plausibility of explanations to the task of natural language inference (NLI)"
- Break condition: If the NLI mapping fails to capture the semantic relationship between rationale and answer, the plausibility scores become unreliable

### Mechanism 3
- Claim: Self-training with high-confidence pseudo-labeled data improves small LM performance
- Core assumption: Adding plausible pseudo-labeled examples improves model generalization more than it introduces noise
- Evidence anchors:
  - [abstract]: "leverages the high confident rationale-answer pairs to boost model performance via self-training"
  - [section 5.2]: "we perform inference with M1 on unlabeled instances... The approximator automatically judges the plausibility of Ë†r, where the most confident predictions are selected"
- Break condition: If the selected pseudo-labeled examples are noisy or unrepresentative, the self-training process degrades model performance

## Foundational Learning

- Concept: Natural Language Inference (NLI)
  - Why needed here: Forms the basis for the plausibility approximator that evaluates whether rationales support answers
  - Quick check question: Can you explain the difference between entailment, neutral, and contradiction in NLI?

- Concept: Self-training in machine learning
  - Why needed here: The framework for using model predictions on unlabeled data to improve the model itself
  - Quick check question: What are the key differences between self-training and supervised learning?

- Concept: Chain-of-thought prompting
  - Why needed here: The baseline approach that ZARA aims to improve upon for small LMs
  - Quick check question: How does chain-of-thought prompting differ from standard prompting?

## Architecture Onboarding

- Component map: Few-shot fine-tuning -> NLI-based plausibility evaluation -> Self-training with high-confidence examples -> Performance evaluation

- Critical path:
  1. Train initial model on few-shot labeled data
  2. Generate predictions on unlabeled data
  3. Map predictions to NLI format
  4. Evaluate plausibility with approximator
  5. Select high-confidence examples
  6. Retrain model on augmented data
  7. Evaluate performance

- Design tradeoffs:
  - Zero-shot vs. fine-tuned NLI models (simplicity vs. accuracy)
  - Threshold selection (sensitivity vs. specificity)
  - Augmentation size (more data vs. noise)

- Failure signatures:
  - Performance degradation after self-training
  - Implausible rationales being selected as high-confidence
  - No improvement across model sizes

- First 3 experiments:
  1. Run ZARA with baseline threshold (0.9) on COMVE dataset
  2. Vary plausibility threshold to find optimal value
  3. Compare ZARA performance against standard few-shot prompting baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is ZARA to different NLI model choices for the plausibility approximator?
- Basis in paper: [explicit] The paper uses an ensemble of three NLI models (RoBERTa, DeBERTa, BART) but does not explore alternative NLI models or architectures.
- Why unresolved: The paper does not compare performance using different NLI models or architectures, leaving uncertainty about whether the chosen ensemble is optimal.
- What evidence would resolve it: Experiments comparing ZARA's performance using different NLI models (e.g., T5, BERT variants) or architectures (e.g., multi-task learning) to establish the sensitivity of ZARA to the choice of NLI model.

### Open Question 2
- Question: Can ZARA's self-training paradigm be extended to semi-supervised learning settings with limited labeled data?
- Basis in paper: [inferred] ZARA uses self-training with unlabeled data, suggesting potential applicability to semi-supervised learning scenarios.
- Why unresolved: The paper only evaluates ZARA in a few-shot setting and does not explore its performance with varying amounts of labeled data or in semi-supervised learning settings.
- What evidence would resolve it: Experiments comparing ZARA's performance with different amounts of labeled data or in semi-supervised learning settings (e.g., using a small labeled dataset and a large unlabeled dataset) to determine its effectiveness in low-resource scenarios.

### Open Question 3
- Question: How does ZARA compare to other methods for improving few-shot self-rationalization, such as data augmentation or transfer learning?
- Basis in paper: [inferred] ZARA uses self-training for data augmentation, but the paper does not compare it to other data augmentation techniques or transfer learning approaches.
- Why unresolved: The paper focuses on ZARA's performance compared to the FEB baseline and does not explore its relative effectiveness compared to other methods for improving few-shot self-rationalization.
- What evidence would resolve it: Experiments comparing ZARA's performance to other data augmentation techniques (e.g., back-translation, synonym replacement) or transfer learning approaches (e.g., pre-training on related tasks) to establish its relative effectiveness in improving few-shot self-rationalization.

## Limitations
- Relies heavily on the correlation between plausibility and correctness, which may not generalize across all datasets
- NLI-based plausibility approximator effectiveness depends on whether NLI entailment relationships accurately capture semantic connections between rationales and answers
- Self-training threshold (0.9) appears somewhat arbitrary with limited exploration of threshold sensitivity

## Confidence
- High Confidence: The core observation that plausible rationales correlate with correct answers is well-supported by quantitative analysis
- Medium Confidence: The NLI-based plausibility approximator represents a reasonable methodological approach, though its effectiveness across diverse tasks requires further validation
- Medium Confidence: The 3.4%-5.1% improvement in task accuracy and 3.0%-5.8% improvement in explanation metrics are statistically meaningful but represent incremental gains

## Next Checks
1. **Threshold Sensitivity Analysis**: Systematically vary the plausibility threshold (0.7, 0.8, 0.9, 0.95) to determine the optimal value and assess whether ZARA's performance is robust to threshold selection or highly sensitive to this hyperparameter.

2. **Cross-Dataset Generalizability Test**: Apply ZARA to additional self-rationalization datasets beyond FEB (such as e-SNLI-hard or other fact verification tasks) to verify whether the NLI-based plausibility approximator and self-training approach transfer effectively to new domains.

3. **Human Evaluation Correlation Study**: Conduct a controlled human evaluation comparing the approximator's plausibility scores against human judgments on a held-out test set, measuring correlation coefficients and analyzing systematic disagreements to validate whether the automatic evaluation truly captures human notions of plausibility.