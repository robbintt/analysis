---
ver: rpa2
title: 'VertexSerum: Poisoning Graph Neural Networks for Link Inference'
arxiv_id: '2308.01469'
source_url: https://arxiv.org/abs/2308.01469
tags:
- graph
- link
- node
- attack
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces VertexSerum, a graph poisoning attack targeting
  graph neural networks (GNNs) to amplify link inference leakage. The method injects
  small perturbations into node features to increase the similarity of GNN outputs
  on linked nodes while reducing similarity on unlinked nodes, making it easier to
  detect links.
---

# VertexSerum: Poisoning Graph Neural Networks for Link Inference

## Quick Facts
- arXiv ID: 2308.01469
- Source URL: https://arxiv.org/abs/2308.01469
- Reference count: 35
- Primary result: VertexSerum improves link inference AUC by 9.8% over state-of-the-art through graph poisoning

## Executive Summary
VertexSerum introduces a novel graph poisoning attack that amplifies link inference leakage in graph neural networks (GNNs). The attack injects small perturbations into node features to increase similarity between posteriors of linked nodes while reducing similarity for unlinked nodes. By using Projected Gradient Descent (PGD) to optimize a custom loss function and a self-attention-based link detector, VertexSerum significantly improves link inference accuracy, particularly for intra-class node pairs. Experiments on four real-world datasets demonstrate the attack's effectiveness across multiple GNN architectures.

## Method Summary
The attack works by first training a shadow GNN on the attacker's partial graph to simulate the victim model. Using PGD, small perturbations are crafted on node features of a target class to optimize a loss function balancing attraction (linked nodes) and repulsion (unlinked nodes) terms. The poisoned graph is then sent to the vendor for training. A self-attention-based link detector is trained on posteriors queried from the poisoned GNN to infer links. The method is evaluated in both black-box and online learning settings, showing significant improvements in AUC scores.

## Key Results
- Improves link inference AUC scores by 9.8% average over state-of-the-art
- Particularly effective on intra-class node pairs where link detection is most challenging
- Successfully tested on four real-world datasets (Cora, Citeseer, AMZPhoto, AMZComputer) and three GNN architectures (GCN, GraphSAGE, GAT)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VertexSerum poisons the training graph to amplify similarity between posteriors of linked nodes while increasing dissimilarity for unlinked nodes
- Mechanism: PGD crafts adversarial perturbations on node features using a custom loss function balancing attraction and repulsion terms
- Core assumption: Small perturbations can effectively manipulate learned posteriors without significantly harming task accuracy
- Break condition: If perturbations exceed unnoticeability threshold or robust defenses neutralize them

### Mechanism 2
- Claim: Self-attention-based link detector improves performance over MLP by better capturing complex dependencies
- Mechanism: Multi-head self-attention analyzes similarity features to selectively focus on informative parts of the feature space
- Core assumption: Similarity features contain complex dependencies that self-attention models more effectively than dense layers
- Break condition: If features are too noisy or dataset is too large for self-attention advantages

### Mechanism 3
- Claim: Intra-class link inference is challenging due to naturally similar posteriors; poisoning amplifies subtle differences
- Mechanism: Focuses poisoning on target class with class-specific detector training to exploit imbalance between linked and unlinked pairs
- Core assumption: Linked vs unlinked distribution imbalance across class splits can be exploited
- Break condition: If target class has very few nodes or uniform class structure

## Foundational Learning

- **Concept: Graph Neural Networks (GNNs) and message passing**
  - Why needed: Understanding GNN aggregation is essential to grasp how poisoning influences learned representations
  - Quick check: What is the receptive field of a 3-layer GNN in terms of graph hops?

- **Concept: Projected Gradient Descent (PGD) for adversarial examples**
  - Why needed: VertexSerum uses PGD to craft stealthy perturbations maximizing custom loss function
  - Quick check: How does PGD differ from standard gradient ascent in constraint handling?

- **Concept: Self-attention mechanisms in neural networks**
  - Why needed: Improved link detector uses multi-head self-attention to model dependencies in similarity features
  - Quick check: In self-attention, what role does query-key-value decomposition play in capturing feature interactions?

## Architecture Onboarding

- **Component map:** Shadow GNN -> PGD Poisoning Module -> Poisoned Graph -> Self-Attention Link Detector
- **Critical path:**
  1. Train shadow GNN on partial graph
  2. Use PGD to generate poisoned node features for target class
  3. Send poisoned graph to vendor for training
  4. Query victim GNN to collect posteriors
  5. Train self-attention link detector on similarity features
  6. Infer links on target node pairs
- **Design tradeoffs:**
  - Poisoning extent (10% of nodes) vs. stealthiness
  - Self-attention complexity vs. dataset size
  - Target class selection based on node density and connectivity
- **Failure signatures:**
  - Model accuracy unchanged after poisoning (stealthy but ineffective)
  - Link detector AUC doesn't improve over baseline MLP
  - Homophily distribution shifts significantly (poisoning detected)
- **First 3 experiments:**
  1. Baseline: Clean graph with MLP detector
  2. Poisoning only: Poisoned features with MLP detector
  3. Full attack: Poisoned features with self-attention detector

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Effectiveness depends on having access to 10% of the graph, not explored for smaller portions
- Limited evaluation to three specific GNN architectures (GCN, GraphSAGE, GAT)
- Assumes balanced class distributions, not tested on imbalanced datasets

## Confidence
- **High confidence** in overall attack framework and effectiveness on tested datasets
- **Medium confidence** in specific mechanisms due to limited methodological detail
- **Medium confidence** in stealthiness claims without comprehensive detection analysis

## Next Checks
1. Test attack effectiveness with varying perturbation magnitudes to determine detection thresholds
2. Evaluate against GNNs trained with adversarial robustness techniques
3. Test on datasets with different homophily levels and class distributions to assess generalizability