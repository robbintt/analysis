---
ver: rpa2
title: Evaluating the Instruction-Following Robustness of Large Language Models to
  Prompt Injection
arxiv_id: '2308.10819'
source_url: https://arxiv.org/abs/2308.10819
tags: []
core_contribution: The paper presents a benchmark for evaluating the robustness of
  large language models (LLMs) against adversarial instructions. The authors inject
  adversarial instructions into the context of extractive QA tasks and measure the
  performance drop and instruction discrimination rates.
---

# Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection

## Quick Facts
- arXiv ID: 2308.10819
- Source URL: https://arxiv.org/abs/2308.10819
- Authors: 
- Reference count: 6
- Current instruction-tuned LLMs are highly susceptible to adversarial instructions, with some models experiencing up to 93% performance decrease

## Executive Summary
This paper presents a benchmark for evaluating the robustness of large language models (LLMs) against adversarial instructions in instruction-following tasks. The authors inject adversarial instructions into the context of extractive QA tasks and measure performance drops and instruction discrimination rates. Their findings reveal that prevalent instruction-tuned models are "overfitted" to follow any instruction phrase without truly understanding context, making them highly vulnerable to prompt injection attacks. The study highlights a critical limitation in current instruction-tuned LLMs where they often ignore original user instructions and excessively follow injected ones, compromising their ability to comprehend prompts and discern intended user instructions.

## Method Summary
The paper evaluates LLM robustness by injecting adversarial instructions into web search contexts for extractive QA tasks. The method involves using two datasets (NATURAL QUESTIONS and TRIVIA QA), generating both random and context-relevant adversarial instructions, and evaluating model responses using standard QA metrics (Exact Match and F1). The authors calculate Performance Drop Rate (PDR) to measure the impact on answer accuracy and Instruction Discrimination Rate (IDR) to determine whether models follow the original user instruction or the injected adversarial instruction. They test various model sizes and types, including both base and instruction-tuned versions, to analyze how different architectures respond to prompt injection attacks.

## Key Results
- Instruction-tuned models experience significant performance decreases (up to 93%) when exposed to adversarial instructions
- LLaMA-based instruction-tuned models are particularly vulnerable, often ignoring original user instructions in favor of injected ones
- Defending against context-relevant adversarial instructions is significantly more challenging than defending against random, irrelevant instructions
- Larger models like LLaMA-2-70B show better robustness compared to smaller models, suggesting scaling can improve both performance and robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-tuned models are "overfitted" to follow any instruction phrase in the prompt without truly understanding context.
- Mechanism: The fine-tuning process exposes models to multi-task instruction data, causing them to learn to respond to any instruction phrase rather than discriminating between user instructions and injected ones.
- Core assumption: Instruction-tuning prioritizes instruction-following over contextual comprehension.
- Evidence anchors:
  - [abstract]: "Our findings indicate that prevalent instruction-tuned models are prone to being 'overfitted' to follow any instruction phrase in the prompt without truly understanding which instructions should be followed."
  - [section]: "These models experienced a notable performance decrease and were misled when exposed to adversarial instructions in the context knowledge."
- Break condition: If models are trained with additional context-awareness signals or with adversarial examples during fine-tuning, this overfitting behavior would reduce.

### Mechanism 2
- Claim: Models rely on position-based heuristics rather than semantic understanding.
- Mechanism: Models appear to focus on the latter parts of the prompt (where injected instructions are placed) and predict the most likely next word rather than comprehending the entire context structure.
- Core assumption: Position of instruction in prompt influences model's response more than semantic relevance.
- Evidence anchors:
  - [section]: "We empirically observed that injecting the instruction at the end of the context is the most challenging for the LLMs to defend against."
  - [section]: "This suggests that these models may not fully comprehend the prompt but instead rely on predicting the most likely next word for the injected instruction placed at the end of the prompt."
- Break condition: If models are trained with positional invariance or context understanding as explicit objectives, they would be less susceptible to position-based manipulation.

### Mechanism 3
- Claim: Context-relevant adversarial instructions are harder to defend against than random instructions.
- Mechanism: When injected instructions are coherent with the context, models struggle to differentiate between the original user question and the injected one, requiring deeper comprehension of prompt structure.
- Core assumption: Coherence between injected instruction and context creates ambiguity that models cannot resolve.
- Evidence anchors:
  - [section]: "defending against context-relevant adversarial instructions is significantly more challenging compared to random instructions that are irrelevant and incoherent with the prompt context."
  - [section]: "We assume that differentiating between this type of injected question qâ€² and the original question q poses a greater challenge for the LLMs, as both questions are related to the context c."
- Break condition: If models develop stronger contextual reasoning capabilities or if prompt structure is explicitly encoded, they would better distinguish between relevant and injected instructions.

## Foundational Learning

- Concept: Instruction-following fine-tuning
  - Why needed here: The paper's core finding is that instruction-tuned models are more vulnerable to prompt injection attacks. Understanding how instruction-tuning works is essential to grasp why these models behave this way.
  - Quick check question: What is the primary objective of instruction-tuning in LLMs, and how does it differ from standard pre-training?

- Concept: Prompt injection attacks
  - Why needed here: The entire evaluation framework is built around understanding how injected adversarial instructions affect model behavior. Understanding the mechanics of prompt injection is crucial.
  - Quick check question: How do prompt injection attacks differ from traditional adversarial examples in NLP?

- Concept: Exact Match (EM) and F1 evaluation metrics
  - Why needed here: These are the standard QA evaluation metrics used to measure performance drop and instruction discrimination rates in the paper.
  - Quick check question: In extractive QA tasks, what is the difference between Exact Match and F1 score evaluation metrics?

## Architecture Onboarding

- Component map:
  - Input processor: Handles user question and web search context
  - Instruction discriminator: (theoretical) Should distinguish between user and injected instructions
  - Answer generator: Produces response based on identified instruction
  - Evaluation module: Measures performance using EM/F1 and PDR/IDR metrics

- Critical path:
  1. Receive (user question, web search context)
  2. Identify target instruction (user vs. injected)
  3. Generate answer based on identified instruction
  4. Evaluate answer against ground truth
  5. Calculate PDR and IDR metrics

- Design tradeoffs:
  - Instruction-following capability vs. instruction discrimination: Strong instruction-following makes models vulnerable to injected instructions
  - Context comprehension vs. computational efficiency: Better context understanding requires more complex architectures
  - Defense mechanisms vs. user experience: Strong defenses may incorrectly block legitimate instructions

- Failure signatures:
  - High PDR (Performance Drop Rate) indicates model is being influenced by injected instructions
  - High IDR (Instruction Discrimination Rate) indicates model is following injected instructions over user instructions
  - Complete performance collapse on adversarial examples
  - Responses that directly answer injected questions instead of user questions

- First 3 experiments:
  1. Replicate the basic evaluation setup with NATURAL QUESTIONS dataset using a simple instruction-tuned model like Alpaca-7B to verify the PDR and IDR metrics work as expected
  2. Test the influence of instruction position by injecting adversarial instructions at different positions (start, middle, end) to confirm the position-based vulnerability
  3. Compare instruction-tuned vs. base models (e.g., LLaMA-13B vs. Vicuna-13B) to demonstrate the overfitting mechanism and quantify the performance gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can instruction-tuned models be designed to better understand context and distinguish between user instructions and injected adversarial instructions?
- Basis in paper: [explicit] The paper mentions that current instruction-tuned models are "overfitted" to follow any instruction phrase in the prompt, compromising their ability to accurately identify and follow the intended user instruction.
- Why unresolved: The paper highlights this as a limitation but does not provide a concrete solution for improving instruction-tuned models' context understanding and instruction discrimination capabilities.
- What evidence would resolve it: Experimental results demonstrating improved performance and robustness of instruction-tuned models against adversarial instructions after implementing proposed solutions.

### Open Question 2
- Question: What are the most effective defense mechanisms against prompt injection attacks in LLMs, particularly those involving the "ignore previous prompt" phrase?
- Basis in paper: [explicit] The paper shows that even robust models like ChatGPT and Claude-v1 can be easily deceived by the inclusion of the "ignore previous prompt" phrase in the prompt.
- Why unresolved: The paper does not explore potential defense mechanisms or solutions to mitigate the impact of such attacks.
- What evidence would resolve it: Successful implementation and evaluation of defense mechanisms that effectively prevent or mitigate the impact of prompt injection attacks involving the "ignore previous prompt" phrase.

### Open Question 3
- Question: How does scaling up language models impact their robustness against adversarial instructions, and is there an optimal model size for balancing performance and robustness?
- Basis in paper: [explicit] The paper observes that larger models like LLaMA-2-70B exhibit better robustness compared to smaller models, suggesting that scaling up language models could enhance both performance and robustness.
- Why unresolved: The paper does not provide a comprehensive analysis of the relationship between model size and robustness or identify an optimal model size for balancing performance and robustness.
- What evidence would resolve it: Systematic evaluation of language models of varying sizes to determine the optimal model size for achieving the best balance between performance and robustness against adversarial instructions.

## Limitations

- Evaluation scope limited to two QA datasets, which may not represent all instruction-following tasks
- Focus on LLaMA-based models may miss insights from other architectures or base models
- Reliance on GPT-4 for generating adversarial instructions introduces uncertainty about attack transferability

## Confidence

- High confidence: The basic finding that instruction-tuned models are vulnerable to prompt injection attacks
- Medium confidence: The mechanism explaining that models are "overfitted" to follow any instruction phrase
- Medium confidence: The claim about context-relevant instructions being harder to defend against than random instructions

## Next Checks

1. Test the same instruction-tuned models on completely different task types (e.g., code generation, summarization, translation) to determine if the prompt injection vulnerability is universal or specific to QA tasks.

2. Evaluate whether the observed vulnerabilities exist in base models (non-instruction-tuned) of the same architecture to isolate whether the instruction-tuning process itself introduces the vulnerability.

3. Fine-tune instruction-tuned models with adversarial examples during the instruction-tuning phase and measure whether this reduces PDR and IDR values to test proposed break conditions.