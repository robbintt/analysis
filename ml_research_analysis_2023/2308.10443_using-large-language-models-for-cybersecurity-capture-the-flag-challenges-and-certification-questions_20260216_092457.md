---
ver: rpa2
title: Using Large Language Models for Cybersecurity Capture-The-Flag Challenges and
  Certification Questions
arxiv_id: '2308.10443'
source_url: https://arxiv.org/abs/2308.10443
tags:
- llms
- questions
- challenges
- language
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the role of large language models (LLMs)\
  \ in cybersecurity Capture-The-Flag (CTF) exercises and professional certification\
  \ exams. The researchers evaluated three popular LLMs\u2014OpenAI ChatGPT, Google\
  \ Bard, and Microsoft Bing\u2014on five Cisco certification exams and seven CTF\
  \ challenge test cases."
---

# Using Large Language Models for Cybersecurity Capture-The-Flag Challenges and Certification Questions

## Quick Facts
- arXiv ID: 2308.10443
- Source URL: https://arxiv.org/abs/2308.10443
- Reference count: 18
- Primary result: ChatGPT achieves 82% accuracy on factual certification questions but only 50% on conceptual ones; solves 6/7 CTF challenges, outperforming Bard (2/7) and Bing (1/7).

## Executive Summary
This study evaluates large language models (LLMs) on cybersecurity certification exams and Capture-The-Flag (CTF) challenges. Testing ChatGPT, Bard, and Bing on Cisco certification questions and seven CTF test cases reveals that factual questions are answered with higher accuracy than conceptual ones. ChatGPT demonstrates superior performance in both domains, solving most CTF challenges through iterative reasoning. The research also highlights the vulnerability of LLM safety policies to jailbreak prompts, raising academic integrity concerns. These findings establish a baseline for understanding AI's role in cybersecurity education and competition settings.

## Method Summary
The researchers evaluated three LLMs—ChatGPT, Bard, and Bing—on five Cisco certification exam question banks and seven CTF test cases across five challenge types. Certification questions were classified as factual or conceptual, and accuracy was measured. CTF challenges were solved using LLM-generated commands, with success determined by flag extraction. Jailbreak prompts were tested to bypass LLM safety policies. The study employed a multi-turn interaction approach for complex challenges, allowing LLMs to refine solutions based on execution feedback.

## Key Results
- ChatGPT achieves 82% accuracy on factual certification questions but only 50% on conceptual ones.
- For CTF challenges, ChatGPT solves 6 out of 7 test cases, while Bard and Bing solve 2 and 1, respectively.
- Jailbreak prompts successfully bypass LLM safety policies, enabling the generation of exploit instructions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs achieve higher accuracy on factual certification questions than on conceptual ones.
- Mechanism: Factual questions rely on retrieving explicit, memorized facts from training data, while conceptual questions require reasoning, synthesis, and application of knowledge—capabilities LLMs lack.
- Core assumption: Training data contains enough factual domain knowledge but insufficient depth in reasoning chains for complex security concepts.
- Evidence anchors:
  - [abstract] states "ChatGPT performs well on factual certification questions (up to 82% accuracy) but struggles with conceptual questions (around 50%)."
  - [section 3.2] explains "LLMs excel in various language tasks and can provide helpful information for factual questions, they have limitations when answering conceptual questions."
  - [corpus] shows no papers specifically measuring LLM reasoning depth on conceptual cybersecurity problems.
- Break condition: If the LLM is fine-tuned on reasoning-heavy datasets or integrated with symbolic reasoning modules, the factual/conceptual accuracy gap could shrink.

### Mechanism 2
- Claim: Jailbreak prompts can bypass LLM ethical safeguards to solve CTF challenges.
- Mechanism: Jailbreak prompts reframe the task as a fictional narrative or role-play scenario, convincing the model to ignore safety filters and provide step-by-step exploit instructions.
- Core assumption: Safety policies are rule-based and can be circumvented by creative prompt engineering that reframes malicious requests as benign.
- Evidence anchors:
  - [abstract] notes "jailbreak prompts can bypass LLMs' ethical safeguards, raising concerns about academic integrity."
  - [section 4.4] describes "AIM prompt get LLMs to take on the role of Italian author Niccolo Machiavelli... effectively tricking them into bypassing its safeguards."
  - [corpus] lacks studies on robustness of safety policies against jailbreak prompts in security domains.
- Break condition: If LLM safety training incorporates adversarial prompt detection or context-aware filtering, jailbreak effectiveness may drop.

### Mechanism 3
- Claim: ChatGPT outperforms Bard and Bing on CTF challenges due to better comprehension and solution synthesis.
- Mechanism: ChatGPT’s architecture (GPT-3.5) allows multi-turn reasoning and integration of execution feedback, enabling it to refine solutions across prompts.
- Core assumption: The model’s ability to maintain context over multiple prompts and self-correct leads to higher success rates.
- Evidence anchors:
  - [abstract] reports "ChatGPT solved 6 out of 7 test cases, while Bard and Bing solved 2 and 1, respectively."
  - [section 4.3] shows ChatGPT required three prompts for the Shell Shock challenge, progressively narrowing down to the exploit command.
  - [corpus] does not contain head-to-head comparisons of prompt-following capabilities across LLMs in CTF settings.
- Break condition: If Bard/Bing adopt similar iterative reasoning pipelines or are fine-tuned on CTF datasets, their success rates could converge.

## Foundational Learning

- Concept: Distinction between factual and conceptual questions
  - Why needed here: Determines expected LLM performance and guides test design.
  - Quick check question: In the certification dataset, if a question asks "Which OSPF authentication types are supported?" is it factual or conceptual?

- Concept: Jailbreak prompt mechanics
  - Why needed here: Explains how LLMs can be subverted and informs safety policy design.
  - Quick check question: What is the key psychological framing in an AIM-style jailbreak prompt?

- Concept: CTF challenge taxonomy (web, binary, crypto, RE, forensics)
  - Why needed here: Structures evaluation and identifies which domains LLMs handle best.
  - Quick check question: Which CTF type typically involves reversing compiled binaries?

## Architecture Onboarding

- Component map: LLM prompt interface -> CTF test case -> Execution environment (cloud VMs) -> Result parsing -> LLM refinement loop
- Critical path: Prompt generation -> LLM response -> Command execution -> Flag extraction -> Accuracy assessment
- Design tradeoffs: Open vs. closed models (access to fine-tuning, safety constraints), synchronous vs. asynchronous execution, single-shot vs. multi-turn interaction
- Failure signatures: LLM misinterprets question -> Wrong tool command -> Execution timeout -> Jailbreak attempt blocked -> Safety policy trigger
- First 3 experiments:
  1. Run factual certification MCQs through ChatGPT and Bard; record accuracy.
  2. Attempt Shell Shock CTF with incremental prompts; log success rate.
  3. Apply AIM jailbreak prompt on a benign security question; verify policy bypass.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs on CTF challenges vary across different challenge types (forensics, cryptography, web exploitation, reverse engineering, binary exploitation)?
- Basis in paper: [explicit] The paper evaluates LLMs on seven test cases spanning all five CTF challenge types and reports varying success rates across these categories.
- Why unresolved: While the paper provides some success rate data, it doesn't comprehensively analyze how performance differs across the five challenge types or identify which types are most/least amenable to LLM assistance.
- What evidence would resolve it: A systematic study testing LLMs on a larger and more diverse set of challenges within each of the five CTF categories, with detailed performance metrics and analysis of failure patterns.

### Open Question 2
- Question: What is the long-term impact of LLM-assisted CTF solving on students' actual cybersecurity skill development and knowledge retention?
- Basis in paper: [inferred] The paper discusses concerns about academic integrity and notes that students might miss learning objectives when using LLMs to solve CTF challenges, but doesn't investigate actual learning outcomes.
- Why unresolved: The paper raises this concern but doesn't provide empirical data on whether students who use LLMs for CTFs actually learn less or retain less knowledge compared to those who solve challenges manually.
- What evidence would resolve it: Longitudinal studies comparing cybersecurity skill development and knowledge retention between students who use LLMs versus those who solve CTF challenges manually.

### Open Question 3
- Question: How can educational institutions develop effective assessment methods that maintain academic integrity while acknowledging the reality of LLM availability?
- Basis in paper: [explicit] The paper discusses academic integrity concerns and notes that educators need to modify teaching to accommodate generative AI assistance.
- Why unresolved: While the paper identifies this as a key concern, it doesn't propose specific assessment strategies or evaluate their effectiveness in maintaining learning objectives while accounting for LLM use.
- What evidence would resolve it: Development and evaluation of assessment methods (e.g., in-person exams, project-based assessments, LLM-augmented assignments) that balance integrity with realistic expectations about AI tool availability.

## Limitations
- Study focuses on Cisco certification exams and a limited set of CTF challenges, limiting generalizability.
- Jailbreak prompt methodology lacks systematic exploration of safety policy robustness across different LLM architectures.
- Does not address the potential for LLM-generated solutions to be detected or the long-term implications for cybersecurity education integrity.

## Confidence
- **High Confidence**: LLM accuracy on factual certification questions (82%) and CTF challenge completion rates (ChatGPT 6/7, Bard 2/7, Bing 1/7) are directly reported from experimental results.
- **Medium Confidence**: The distinction between factual and conceptual question performance is supported by data but relies on subjective classification criteria.
- **Low Confidence**: The effectiveness and generalizability of jailbreak prompts across different LLM architectures and safety policies is not extensively validated beyond the reported cases.

## Next Checks
1. **Expand Dataset Diversity**: Test LLM performance on a broader range of cybersecurity certification exams and CTF challenges from multiple platforms to assess generalizability.
2. **Jailbreak Robustness Testing**: Systematically evaluate the effectiveness of jailbreak prompts against different LLM safety policies and architectures to understand vulnerability patterns.
3. **Detection Method Development**: Investigate and validate methods for detecting LLM-generated solutions in academic and competitive cybersecurity settings to address academic integrity concerns.