---
ver: rpa2
title: Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation
arxiv_id: '2305.07375'
source_url: https://arxiv.org/abs/2305.07375
tags:
- causal
- chatgpt
- event
- reasoning
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates ChatGPT''s causal reasoning capabilities
  using four different tasks: Event Causality Identification (ECI), Causal Discovery
  (CD), and Causal Explanation Generation (CEG). The experiments show that ChatGPT
  is not a good causal reasoner, but a good causal explainer.'
---

# Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation

## Quick Facts
- arXiv ID: 2305.07375
- Source URL: https://arxiv.org/abs/2305.07375
- Authors: 
- Reference count: 40
- Key outcome: ChatGPT is not a good causal reasoner but a good causal explainer, suffering from serious causal hallucination issues and sensitivity to prompt wording.

## Executive Summary
This paper evaluates ChatGPT's causal reasoning capabilities across four tasks: Event Causality Identification (ECI), Causal Discovery (CD), and Causal Explanation Generation (CEG). The experiments reveal that ChatGPT is not a reliable causal reasoner, exhibiting significant hallucination issues where it assumes causal relationships regardless of their actual existence. The model performs better on non-causal pairs in the CD task compared to the ECI task, and its performance is highly sensitive to the words used to express causality in prompts. While ChatGPT struggles with causal reasoning, it demonstrates competence in generating causal explanations for known causal relationships.

## Method Summary
The study employs zero-shot evaluation using OpenAI's API with four ChatGPT versions (text-davinci-002, text-davinci-003, gpt-3.5-turbo, gpt-4) at temperature=0. The evaluation uses three ECI datasets (ESC, CTB, MA VEN-ERE), two CD datasets (COPA, e-CARE), and e-CARE for CEG. Metrics include Accuracy, Precision, Recall, F1-score for ECI; Accuracy for CD; and BLEU, ROUGE-L, and human evaluation for CEG. Prompts follow templates shown in Figure 1, with In-Context Learning (ICL) and Chain-of-Thought (CoT) techniques also tested.

## Key Results
- ChatGPT's performance is overestimated in multiple-choice formats compared to binary classification, as it can ignore difficult options.
- The model exhibits serious causal hallucination, assuming causal relationships regardless of their actual existence.
- ChatGPT shows significant sensitivity to causal cue words in prompts, with different words triggering different performance levels.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: ChatGPT's causal reasoning performance is overestimated when using multiple-choice formats.
- **Mechanism**: In multiple-choice tasks, ChatGPT only needs to select the more obviously causal option, allowing it to ignore the more difficult option and inflate performance metrics.
- **Core assumption**: The evaluation method itself influences the observed performance by changing the task complexity.
- **Evidence anchors**:
  - [section] "Firstly, although ChatGPT performs well in multiple-choice, its performance is poor in binary classification. The main reason is that in multiple-choice, ChatGPT only needs to consider the option that shows the more obvious causal or non-causal relationship with the input event, while the other more difficult option can be ignored."
  - [corpus] "Found 25 related papers... Top related titles: GPT-4 Can't Reason..."
- **Break condition**: If evaluation shifts to formats requiring full reasoning (like binary classification), the inflated performance disappears.

### Mechanism 2
- **Claim**: ChatGPT has a reporting bias toward causal relationships due to training data patterns.
- **Mechanism**: Natural language contains more descriptions of causal relationships (indicated by causal cue words like "lead to" and "therefore") than non-causal relationships, causing ChatGPT to favor identifying causal pairs over recognizing non-causal pairs.
- **Core assumption**: The distribution of causal vs non-causal language in training data creates systematic bias in model predictions.
- **Evidence anchors**:
  - [section] "This is also why ChatGPT performs particularly poorly on the CTB dataset, which contains more non-causal event pairs. The main reason for this may be that natural language contains a large number of descriptions of causal relationships, mainly indicated by causal cue words such as 'lead to' and 'therefore'. However, natural language generally does not express which events are not causally related."
  - [abstract] "ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language..."
- **Break condition**: If training data were balanced for causal and non-causal relationships, the bias would diminish.

### Mechanism 3
- **Claim**: ChatGPT's causal reasoning ability is sensitive to specific wording in prompts.
- **Mechanism**: Different causal cue words in prompts trigger different learned patterns, leading to significant performance variation even when words are semantically equivalent to humans.
- **Core assumption**: The model associates specific lexical triggers with particular reasoning patterns learned during pretraining.
- **Evidence anchors**:
  - [section] "the performance of 'trigger(<X>)' with different causal cue words is significantly different. This may be due to the fact that during pretraining, ChatGPT mainly learns causal knowledge triggered by causal cue words, but the distributions of causality triggered by each cue word are quite different."
  - [abstract] "the causal reasoning ability of ChatGPT is sensitive to the words used to express the causal concept in prompts..."
- **Break condition**: If prompts use neutral or consistent causal language, the performance variation across different cue words would disappear.

## Foundational Learning

- **Concept**: Causal reasoning vs causal explanation generation
  - **Why needed here**: The paper distinguishes between ChatGPT's ability to identify causal relationships (reasoning) versus generating explanations for known causal relationships (interpretation), which have different underlying mechanisms.
  - **Quick check question**: What is the fundamental difference between causal reasoning and causal explanation generation in terms of what the model must accomplish?

- **Concept**: Reporting bias in natural language
  - **Why needed here**: Understanding how the imbalance in language describing causal vs non-causal relationships affects model training and performance is crucial for interpreting results.
  - **Quick check question**: Why does natural language contain more descriptions of causal relationships than non-causal relationships, and how does this create bias in causal reasoning models?

- **Concept**: In-context learning (ICL) and Chain-of-Thought (CoT) prompting
  - **Why needed here**: These techniques can either help or harm performance depending on the quality of demonstrations and reasoning chains, particularly for causal reasoning tasks.
  - **Quick check question**: How do ICL and CoT prompting affect ChatGPT's performance differently on causal versus non-causal pairs, and why?

## Architecture Onboarding

- **Component map**: Prompt generation (task-specific templates) -> API interface (OpenAI endpoints for different GPT versions) -> dataset loading (ECI, CD, CEG datasets) -> evaluation metrics (accuracy, precision, recall, F1, BLEU, ROUGE-L)
- **Critical path**: Prompt generation → API call → response parsing → metric calculation → result aggregation
- **Design tradeoffs**: Using zero-shot evaluation avoids training data bias but may miss model capabilities; multiple-choice formats overestimate performance; temperature=0 reduces randomness but may limit model flexibility
- **Failure signatures**: High recall but low precision indicates systematic false positive causal identification; poor performance on non-causal pairs specifically indicates reporting bias; sensitivity to prompt wording indicates model reliance on lexical cues
- **First 3 experiments**:
  1. Run the ECI task with binary classification format instead of multiple-choice to verify performance drop.
  2. Test the same prompt with different causal cue words to measure performance sensitivity.
  3. Apply ICL with balanced positive and negative demonstrations to see if it reduces hallucination.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do reporting biases in natural language specifically contribute to ChatGPT's causal hallucination issues?
- Basis in paper: Explicit
- Why unresolved: The paper identifies reporting biases as a potential cause of causal hallucination but does not provide a detailed analysis of how these biases specifically affect ChatGPT's reasoning. The relationship between reporting biases and causal hallucination remains theoretical.
- What evidence would resolve it: A detailed study analyzing the correlation between the frequency of causal vs. non-causal relationships in training data and ChatGPT's performance on causal tasks.

### Open Question 2
- Question: How do different versions of ChatGPT (text-davinci-002, text-davinci-003, gpt-3.5-turbo, gpt-4) differ in their ability to handle implicit causality?
- Basis in paper: Explicit
- Why unresolved: While the paper compares the performance of different ChatGPT versions on explicit and implicit causality, it does not delve into the specific reasons why certain versions perform better or worse on implicit causality tasks.
- What evidence would resolve it: A comparative analysis of the training data and methodologies used for each version, focusing on their exposure to and handling of implicit causal relationships.

### Open Question 3
- Question: Can the causal hallucination issue in ChatGPT be mitigated through specific prompt engineering techniques?
- Basis in paper: Inferred
- Why unresolved: The paper discusses the sensitivity of ChatGPT to prompt wording and the exacerbation of causal hallucination by ICL and COT techniques, but it does not explore whether specific prompt engineering could reduce hallucination.
- What evidence would resolve it: An experimental study testing various prompt engineering strategies to determine their effectiveness in reducing causal hallucination in ChatGPT's responses.

## Limitations

- The findings are based on zero-shot evaluations with specific prompt formats and may not generalize to other evaluation paradigms or larger model versions.
- The study doesn't explore fine-tuning approaches or investigate whether larger model versions (beyond GPT-4) might perform differently on causal reasoning tasks.
- The sensitivity to prompt wording and observed hallucination issues may not represent fundamental limitations that could be addressed through different approaches.

## Confidence

- **High confidence**: ChatGPT's poor performance on binary classification tasks compared to multiple-choice formats, and its systematic bias toward identifying causal relationships (reporting bias mechanism).
- **Medium confidence**: The specific mechanisms by which ICL and CoT prompting can exacerbate causal hallucination, as this requires more controlled experimentation.
- **Low confidence**: Whether the observed sensitivity to causal cue words represents a fundamental limitation of the model or simply reflects learned patterns that could be mitigated through prompt engineering or fine-tuning.

## Next Checks

1. Replicate the ECI task using binary classification format instead of multiple-choice to verify if the performance gap persists across different model versions.
2. Conduct controlled experiments varying only the causal cue words in prompts while keeping all other factors constant to quantify the sensitivity impact on performance metrics.
3. Apply balanced ICL demonstrations (equal positive and negative causal pairs) to determine if this reduces the hallucination tendency compared to the zero-shot baseline.