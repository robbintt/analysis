---
ver: rpa2
title: A New Korean Text Classification Benchmark for Recognizing the Political Intents
  in Online Newspapers
arxiv_id: '2311.01712'
source_url: https://arxiv.org/abs/2311.01712
tags:
- political
- dataset
- text
- classification
- articles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KoPolitic, a multi-task Korean text classification
  model designed to recognize political intent and pro-government sentiment in long
  news articles. The proposed dataset, containing 12,000 articles from Korean newspapers,
  is annotated for political orientation (1-5 scale) and pro-government stance (0-5
  scale).
---

# A New Korean Text Classification Benchmark for Recognizing the Political Intents in Online Newspapers

## Quick Facts
- arXiv ID: 2311.01712
- Source URL: https://arxiv.org/abs/2311.01712
- Reference count: 17
- Multi-task Korean model KoPolitic achieves best performance using KoBigBird with 3072 tokens for political intent classification

## Executive Summary
This paper introduces KoPolitic, a multi-task Korean text classification model designed to recognize political intent and pro-government sentiment in long news articles. The authors create a dataset of 12,000 Korean newspaper articles annotated for political orientation (1-5 scale) and pro-government stance (0-5 scale). They fine-tune transformer-based models (KoBERT, KoELECTRA, KoBigBird) on this dataset using a multi-task architecture that jointly predicts both tasks. The results show that KoBigBird with 3072 tokens outperforms single-task models and effectively handles long text sequences for political classification.

## Method Summary
The authors develop KoPolitic by fine-tuning pre-trained Korean transformer models on their newly collected dataset. The multi-task architecture uses a shared KoBigBird encoder that processes up to 3072 tokens, branching into two classification heads for political orientation and pro-government stance. The model is trained using cross-entropy loss for both tasks simultaneously. The dataset consists of 12,000 news articles from Korean newspapers labeled with 5-point scales for political orientation and 6-point scales for pro-government sentiment. Models are evaluated using Top-2 accuracy, accuracy, F1-score, MAE, and Hamming loss metrics.

## Key Results
- KoPolitic achieves best performance using KoBigBird with 3072 tokens, outperforming single-task models
- Multi-task learning architecture shows improved classification accuracy over single-task baselines
- The model effectively handles long news articles up to 3072 tokens, capturing full contextual information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task learning improves performance by sharing the base feature extractor across related tasks
- Mechanism: The model uses a shared KoBigBird encoder to extract features, then branches into two classification heads for political orientation and pro-government stance, allowing mutual benefit from shared representations
- Core assumption: The two classification tasks are related enough that learning them jointly provides mutual benefit
- Evidence anchors: Multi-task architecture shows improved performance over single-task models

### Mechanism 2
- Claim: Long sequence capability enables better understanding of complete article context
- Mechanism: By processing up to 3072 tokens, the model captures full context of long news articles rather than truncating important information
- Core assumption: Important political signals are distributed throughout the entire article
- Evidence anchors: KoBigBird with 3072 tokens achieves best performance in the multi-task setup

### Mechanism 3
- Claim: Multi-class annotation scheme captures nuanced political intent better than binary classification
- Mechanism: Using 5-point scales for political orientation and 6-point scales for pro-government stance allows learning subtle distinctions between different levels of political bias
- Core assumption: Political intent exists on a spectrum and can be meaningfully distinguished at multiple levels
- Evidence anchors: The dataset uses multi-class labeling to detect subtle political ideology

## Foundational Learning

- Concept: Attention mechanisms in transformers
  - Why needed here: The paper leverages transformer architectures that use self-attention to process long text sequences and capture semantic relationships
  - Quick check question: How does self-attention allow transformers to process sequences in parallel rather than sequentially?

- Concept: Multi-task learning
  - Why needed here: The model simultaneously learns two related classification tasks (political orientation and pro-government stance) to improve overall performance
  - Quick check question: What are the potential benefits and risks of sharing representations between related tasks in neural networks?

- Concept: Sequence length limitations in transformer models
  - Why needed here: The paper addresses the challenge of processing long news articles by using BigBird, which can handle sequences up to 3072 tokens
  - Quick check question: Why do standard transformer architectures struggle with very long sequences, and how does BigBird's sparse attention mechanism help?

## Architecture Onboarding

- Component map: Input text → Tokenization → Shared KoBigBird encoder → Two classification heads (political orientation, pro-government) → Loss calculation → Backpropagation
- Critical path: Text preprocessing → Tokenization → Feature extraction → Classification → Loss computation → Backpropagation
- Design tradeoffs: Longer sequences (3072) provide better context but increase computational cost; multi-task learning shares parameters but may introduce interference between tasks
- Failure signatures: Poor performance on one task but not the other suggests task interference; worse performance than single-task baselines suggests the tasks are too dissimilar
- First 3 experiments:
  1. Test single-task vs multi-task performance on validation set to confirm multi-task benefit
  2. Vary sequence length (1024, 2048, 3072) to find optimal trade-off between performance and computation
  3. Test with different pre-trained models (KoBERT, KoELECTRA) to validate architecture choice

## Open Questions the Paper Calls Out

- How does the performance of KoPolitic change when applied to news articles from different time periods, particularly those predating the current government in power?
- To what extent does the presence of direct quotations from politicians in news articles affect the model's ability to accurately predict the journalist's political intent?
- How can the model be improved to better distinguish between subtle nuanced differences in political orientation, such as between moderate conservatives and moderate liberals?

## Limitations

- Limited experimental validation with insufficient ablation studies and comparison with strong baselines
- Dataset size of 12,000 articles may not be sufficient to fully validate multi-task learning benefits
- Lack of detailed information about inter-annotator agreement and label reliability for the benchmark dataset

## Confidence

**High Confidence:** Technical implementation of multi-task architecture and use of KoBigBird for long sequence processing are well-defined and implementable.

**Medium Confidence:** The claim that multi-task learning improves performance over single-task baselines, though evidence is limited to a single comparison.

**Low Confidence:** The assertion that the dataset represents a significant contribution to Korean NLP research without detailed quality analysis and comparison with existing datasets.

## Next Checks

1. Conduct ablation studies where each task is trained independently and in combination with various other tasks to quantify the actual benefit of multi-task learning for these specific tasks.

2. Calculate and report inter-annotator agreement scores for both annotation schemes and analyze label distribution across classes to validate dataset quality.

3. Generate or collect challenging examples with subtle political bias and test the model's performance compared to random baseline performance to assess genuine political intent recognition.