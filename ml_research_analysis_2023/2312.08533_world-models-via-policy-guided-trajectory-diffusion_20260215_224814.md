---
ver: rpa2
title: World Models via Policy-Guided Trajectory Diffusion
arxiv_id: '2312.08533'
source_url: https://arxiv.org/abs/2312.08533
tags:
- policy
- diffusion
- polygrad
- action
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Policy-Guided Trajectory Diffusion (PolyGRAD),
  a novel approach for world modeling that generates entire on-policy trajectories
  in a single pass through a diffusion model, without autoregressive sampling. Unlike
  existing autoregressive world models that accumulate prediction errors as trajectory
  length increases, PolyGRAD leverages a denoising model and policy gradient guidance
  to iteratively refine trajectories from random states and actions into accurate
  on-policy trajectories.
---

# World Models via Policy-Guided Trajectory Diffusion

## Quick Facts
- arXiv ID: 2312.08533
- Source URL: https://arxiv.org/abs/2312.08533
- Reference count: 24
- Key outcome: PolyGRAD generates on-policy trajectories via diffusion without autoregressive sampling, achieving competitive prediction errors and faster computation than autoregressive baselines on MuJoCo tasks

## Executive Summary
This paper introduces Policy-Guided Trajectory Diffusion (PolyGRAD), a novel approach for world modeling that generates entire on-policy trajectories in a single pass through a diffusion model, without autoregressive sampling. Unlike existing autoregressive world models that accumulate prediction errors as trajectory length increases, PolyGRAD leverages a denoising model and policy gradient guidance to iteratively refine trajectories from random states and actions into accurate on-policy trajectories. The method demonstrates competitive prediction errors compared to state-of-the-art autoregressive baselines (including diffusion models and transformers) on MuJoCo continuous control tasks, while being significantly faster computationally. PolyGRAD successfully enables performant policies to be trained via on-policy reinforcement learning using only imagined data, achieving results superior to on-policy model-free algorithms. The approach introduces a promising paradigm for scalable world modeling without the limitations of autoregressive sampling.

## Method Summary
PolyGRAD is a world modeling approach that generates on-policy trajectories using a denoising diffusion model guided by policy gradients. The method starts with random trajectories and iteratively refines them over multiple diffusion steps. At each step, a denoising model predicts state updates while policy gradients guide actions toward the on-policy distribution. The process involves three main algorithms: training the denoising model on real data, generating synthetic trajectories through diffusion, and using these trajectories for on-policy reinforcement learning. The key innovation is using policy score gradients (∇_a log π(a|s)) to guide actions during diffusion, ensuring the generated trajectories follow the current policy's action distribution without requiring explicit sampling from the policy at each step.

## Key Results
- PolyGRAD achieves competitive prediction errors compared to state-of-the-art autoregressive baselines on HalfCheetah-v3, Hopper-v3, and Walker2d-v3
- PolyGRAD is significantly faster computationally than autoregressive methods while maintaining similar prediction accuracy
- Policies trained using only PolyGRAD-generated data outperform those trained with on-policy model-free algorithms like PPO and TRPO
- The method successfully maintains correct on-policy action distributions through automatic tuning of action update scales

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PolyGRAD iteratively refines both state and action sequences to produce on-policy trajectories without autoregressive sampling.
- Mechanism: PolyGRAD starts with random trajectories and uses a denoising model to predict state updates while guiding actions using the policy's score function. This ensures consistency between predicted states and sampled actions throughout diffusion.
- Core assumption: The policy score function ∇_a log π(a|s) provides sufficient gradient information to guide random actions toward the on-policy distribution when iteratively applied.
- Evidence anchors:
  - [abstract] "Our approach, Policy-Guided Trajectory Diffusion (PolyGRAD), leverages a denoising model in addition to the gradient of the action distribution of the policy to diffuse a trajectory of initially random states and actions into an on-policy synthetic trajectory."
  - [section] "To address this challenge, PolyGRAD utilises diffusion to gradually diffuse a trajectory of initially random states and actions into an on-policy synthetic trajectory."
- Break condition: If the policy's entropy is too low (σ_ϕ < 0.1), the action distribution deviates from the target Gaussian distribution, making it difficult to guide actions correctly.

### Mechanism 2
- Claim: PolyGRAD connects to classifier-guided diffusion by using policy score gradients as a form of guidance.
- Mechanism: The update rule ∇_a log π(a|s) applied to actions during diffusion serves the same function as classifier gradients in classifier-guided diffusion models, directing the trajectory toward regions of high policy probability.
- Core assumption: The policy action distribution can be treated analogously to a classifier that scores trajectories based on their likelihood under the policy.
- Evidence anchors:
  - [section] "We analyse how our work can be viewed either as using a score-based generative model to generate on-policy actions, or as an instance of classifier-guided diffusion."
  - [section] "Therefore, under this assumption that by default all actions are equally likely, as well as the assumption that no noise has been added to the trajectory, we arrive at the final classifier gradient: ∇_τ log p(π_ϕ|τ) = ∑_i ∇_τ log π_ϕ(a_i|s_i)."
- Break condition: If the policy is deterministic or has very low entropy, the classifier gradient analogy breaks down as there's insufficient probabilistic structure to guide diffusion.

### Mechanism 3
- Claim: The action update scale δ is automatically tuned to ensure actions remain distributed according to the policy.
- Mechanism: After generating synthetic trajectories, the algorithm computes the standard deviation of standardized actions and adjusts δ to maintain σ_a ≈ 1, ensuring actions are correctly distributed under the policy.
- Core assumption: If actions are properly sampled from the policy distribution, their standardized values should follow a unit Gaussian distribution.
- Evidence anchors:
  - [section] "To perform the update for δ, we consider the set of state-action pairs in the synthetic trajectories... We then compute σ_a, the standard deviation of the set of standardized actions... If the actions are drawn correctly from the policy distribution, then the standardized actions should be distributed according to a standard normal distribution."
  - [section] "δ ← δ + η · (σ_a - 1)"
- Break condition: If the denoising model produces highly inaccurate state predictions, the policy conditioned on these states may output actions far from the correct distribution, causing δ tuning to fail.

## Foundational Learning

- Concept: Diffusion models and score-based generative modeling
  - Why needed here: PolyGRAD uses diffusion to iteratively refine trajectories, requiring understanding of how denoising models learn score functions and how sampling works in diffusion models.
  - Quick check question: What is the relationship between denoising diffusion models and score-based generative models, and how does this connection enable PolyGRAD's approach?

- Concept: Reinforcement learning policy parameterisations and on-policy data generation
  - Why needed here: PolyGRAD must generate trajectories that are on-policy for a given neural network policy, requiring understanding of how policies map states to action distributions and what makes data "on-policy."
  - Quick check question: How does PolyGRAD ensure that generated trajectories follow the current policy's action distribution without directly sampling from the policy at each step?

- Concept: Classifier-guided diffusion and conditional generation
  - Why needed here: PolyGRAD's connection to classifier-guided diffusion provides theoretical motivation for why the policy score gradients can guide trajectory generation.
  - Quick check question: How does classifier-guided diffusion work, and what parallels exist between classifier guidance and PolyGRAD's use of policy score functions?

## Architecture Onboarding

- Component map: Denoising model (ϵ_θ) -> Policy network (π_ϕ) -> Diffusion process -> Action update mechanism -> δ tuning algorithm

- Critical path:
  1. Collect real environment data in data buffer D
  2. Train denoising model ϵ_θ on D using standard diffusion training objective
  3. Initialize random trajectory and sample initial state from D
  4. Iterate diffusion steps: predict state noise, update actions using policy score, denoise states
  5. Generate synthetic trajectories and use for on-policy RL training
  6. Automatically tune δ based on action distribution statistics

- Design tradeoffs:
  - Using a single denoising model for entire trajectories vs. autoregressive one-step predictions: PolyGRAD is faster but may have higher prediction error for long trajectories
  - Conditioning policy on denoised vs. noisy states: Denoised states provide more accurate policy gradients but require additional computation
  - Action clipping during diffusion: Prevents actions from leaving reasonable ranges but may limit exploration

- Failure signatures:
  - Action distribution deviates from target Gaussian (heavy tails or multiple modes): Indicates insufficient policy entropy or incorrect δ tuning
  - RL training collapses or produces poor policies: Suggests generated trajectories are not truly on-policy
  - Prediction errors increase dramatically with trajectory length: Denoising model struggles to accurately predict long-term state sequences

- First 3 experiments:
  1. Verify action distribution: Generate synthetic trajectories with PolyGRAD and plot the distribution of (a - μ_ϕ(s))/σ_ϕ to confirm it matches a standard normal distribution
  2. Compare prediction errors: Train PolyGRAD and autoregressive baselines on the same dataset, then measure MSE for varying trajectory lengths
  3. Test RL performance: Use PolyGRAD-generated data to train policies via on-policy RL and compare against model-free baselines like PPO and TRPO

## Open Questions the Paper Calls Out

- Question: Under what theoretical conditions does PolyGRAD converge to on-policy trajectories, and what are the bounds on prediction error?
  - Basis in paper: [explicit] The paper mentions "A formal analysis of convergence is outside the scope of this work" and suggests future work on theoretical analysis of convergence properties.
  - Why unresolved: The paper demonstrates empirical performance but does not provide theoretical guarantees about convergence or error bounds for PolyGRAD's trajectory generation.
  - What evidence would resolve it: A formal proof establishing conditions under which PolyGRAD converges to on-policy trajectories, along with theoretical bounds on prediction error as a function of trajectory length and policy entropy.

- Question: How does PolyGRAD perform on non-Markovian environments compared to autoregressive baselines?
  - Basis in paper: [explicit] "Another direction for future work is to perform more experiments to investigate whether there are situations in which PolyGRAD obtains better prediction errors than the strongest baseline, Autoregressive Diffusion. We hypothesise that PolyGRAD may be more robust when trained on small datasets, where the predictions of single-step autoregressive models may be prone to quickly leaving the data distribution, resulting in erroneous predictions."
  - Why unresolved: The paper only tests on fully-observable Markov decision processes (MDPs) and does not explore PolyGRAD's performance in partially observable or non-Markovian settings.
  - What evidence would resolve it: Experimental results comparing PolyGRAD against autoregressive baselines on partially observable MDPs or POMDPs, measuring both prediction accuracy and policy performance.

- Question: What is the optimal trade-off between action update scale and policy entropy for maintaining correct on-policy distributions?
  - Basis in paper: [inferred] The paper shows that PolyGRAD struggles to generate correct action distributions when policy entropy is very low (σϕ < 0.1), and demonstrates that action clipping helps maintain proper distributions.
  - Why unresolved: While the paper identifies that very low entropy policies cause problems, it doesn't systematically explore the optimal relationship between action update magnitude, policy entropy, and the resulting action distribution quality.
  - What evidence would resolve it: Systematic experiments varying both action update scale and policy entropy across multiple environments, identifying the regimes where PolyGRAD maintains accurate on-policy distributions and where it fails.

## Limitations

- PolyGRAD struggles with very low entropy policies (σ_ϕ < 0.1), where action distributions deviate from the target Gaussian and require clipping to maintain stability
- The method's performance on non-Markovian or partially observable environments remains untested, limiting generalizability beyond standard MDP benchmarks
- Theoretical guarantees for convergence and error bounds are absent, making it difficult to predict performance in new domains or with different policy architectures

## Confidence

- High confidence: Core architecture and classifier-guided diffusion connection
- High confidence: Competitive experimental results against autoregressive baselines
- Medium confidence: Scalability and efficiency advantages claims
- Medium confidence: Superior on-policy RL performance claims
- Low confidence: Claim that PolyGRAD represents a fundamentally new paradigm for world modeling

## Next Checks

1. Test PolyGRAD on more diverse environments including Atari, robotics simulation, and continuous control tasks with sparse rewards to verify scalability claims
2. Conduct ablation studies comparing PolyGRAD's performance with different noise schedules, diffusion step counts, and action update mechanisms
3. Implement a computational efficiency benchmark comparing wall-clock time for trajectory generation between PolyGRAD and autoregressive baselines across varying trajectory lengths