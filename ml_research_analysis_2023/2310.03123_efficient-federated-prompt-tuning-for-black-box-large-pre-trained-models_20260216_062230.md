---
ver: rpa2
title: Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models
arxiv_id: '2310.03123'
source_url: https://arxiv.org/abs/2310.03123
tags:
- prompt
- arxiv
- preprint
- learning
- tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Fed-BBPT, a federated learning framework
  for prompt tuning large pre-trained models without accessing model parameters or
  local datasets. Fed-BBPT trains a prompt generator collaboratively across multiple
  users by aggregating local models via a central server.
---

# Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models

## Quick Facts
- arXiv ID: 2310.03123
- Source URL: https://arxiv.org/abs/2310.03123
- Reference count: 33
- Achieves comparable or better performance than white-box and black-box baselines across 40 datasets spanning CV and NLP tasks

## Executive Summary
This paper introduces Fed-BBPT, a federated learning framework for prompt tuning large pre-trained models without accessing model parameters or local datasets. Fed-BBPT trains a prompt generator collaboratively across multiple users by aggregating local models via a central server. Users employ black-box optimization with API-driven learning to update prompts. The framework is evaluated on 40 datasets spanning CV and NLP tasks, achieving comparable or better performance than white-box and black-box baselines.

## Method Summary
Fed-BBPT enables collaborative prompt tuning by having each local user generate prompts via a lightweight prompt generator, send them to a black-box PTM API, receive predictions and losses, then update the generator using zeroth-order optimization. The central server periodically aggregates updated prompt generators. The framework sidesteps memory challenges of PTM storage and fine-tuning on local machines while maintaining data privacy. Different zeroth-order methods (SPSA for continuous prompts, policy gradient for discrete prompts, Bayesian optimization for instruction tuning) estimate gradients from loss evaluations without model access.

## Key Results
- For image classification, Fed-BBPT improves over zero-shot CLIP by 2.2% on average
- For text classification, Fed-BBPT outperforms manual prompts by 10.1% and most baselines
- For instruction optimization, Fed-BBPT matches or exceeds the InstructZero method across 21 datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fed-BBPT enables collaborative prompt tuning without local model deployment or data sharing
- Mechanism: Each local user generates prompts via a lightweight prompt generator, sends them to a black-box PTM API, receives predictions and losses, then updates the generator using zeroth-order optimization; central server aggregates updated prompt generators periodically
- Core assumption: The black-box PTM API provides reliable prediction and loss outputs for prompt optimization without requiring gradients
- Evidence anchors:
  - [abstract]: "This innovative approach eschews reliance on parameter architectures and private dataset access, instead capitalizing on a central server that aids local users in collaboratively training a prompt generator through regular aggregation."
  - [section 3]: "Local users leverage API-driven learning via a zero-order optimizer, obviating the need for PTM deployment."
- Break condition: If the black-box API fails to provide consistent prediction/loss outputs, or if zeroth-order optimization cannot converge with noisy API responses.

### Mechanism 2
- Claim: Federated aggregation of prompt generators improves generalization by leveraging diverse local datasets
- Mechanism: Periodic averaging of local prompt generator parameters at the central server allows the global model to incorporate knowledge from heterogeneous private datasets without exposing data
- Core assumption: Averaging local prompt generators captures common useful patterns while maintaining diversity benefits
- Evidence anchors:
  - [abstract]: "Fed-BBPT proficiently sidesteps memory challenges tied to PTM storage and fine-tuning on local machines, tapping into comprehensive, high-quality, yet private training datasets."
  - [section 3]: "Our target is to optimize a global generator that can fully benefit from the different dataset xi to improve the generalization ability of PTMs."
- Break condition: If local datasets are too heterogeneous, aggregation may produce a model that performs poorly on any specific task.

### Mechanism 3
- Claim: Zeroth-order optimization is effective for black-box prompt tuning across CV and NLP tasks
- Mechanism: Different zeroth-order methods (SPSA for continuous prompts, policy gradient for discrete prompts, Bayesian optimization for instruction tuning) estimate gradients from loss evaluations without model access
- Core assumption: Zeroth-order optimization can approximate effective gradients for prompt space optimization despite high dimensionality
- Evidence anchors:
  - [section 3.1]: "Due to the model privacy in practice, we follow the Simultaneous Perturbation Stochastic Approximation (SPSA) to approximate the high-dimensional gradient efficiently."
  - [section 3.2]: "Same as BDPL (Diao et al., 2022), we also utilize the policy gradient algorithm to update Î¸."
- Break condition: If the prompt space is too high-dimensional or the loss landscape too noisy, zeroth-order methods may fail to converge or get stuck in poor local optima.

## Foundational Learning

- Concept: Federated Learning fundamentals (FedAvg, data heterogeneity, communication rounds)
  - Why needed here: The paper builds directly on federated learning framework to aggregate local prompt generators
  - Quick check question: What is the difference between IID and non-IID data partitioning in federated learning, and why does it matter for Fed-BBPT?

- Concept: Zeroth-order optimization (gradient-free optimization methods)
  - Why needed here: Fed-BBPT relies on zeroth-order methods (SPSA, policy gradient, Bayesian optimization) to update prompts without model access
  - Quick check question: How does SPSA estimate gradients using only function evaluations, and what are its key hyperparameters?

- Concept: Prompt tuning vs. fine-tuning (parameter-efficient transfer learning)
  - Why needed here: The paper compares prompt tuning approaches against traditional fine-tuning and other parameter-efficient methods
  - Quick check question: What are the key differences between continuous prompt tuning and discrete prompt tuning, and when would each be preferred?

## Architecture Onboarding

- Component map:
  Local users -> Central server -> Black-box PTM API

- Critical path:
  1. Initialize prompt generator parameters at central server
  2. Distribute parameters to local users
  3. Each user generates prompts, queries PTM API, updates local generator via zeroth-order optimization
  4. Users send updated parameters to central server
  5. Central server aggregates parameters (FedAvg)
  6. Repeat from step 2 for multiple communication rounds

- Design tradeoffs:
  - Local computation vs. communication frequency: more frequent updates may improve convergence but increase communication overhead
  - Zeroth-order method choice: different tasks require different optimization strategies (SPSA vs. policy gradient vs. Bayesian)
  - Prompt generator architecture: lightweight design trades expressivity for memory efficiency

- Failure signatures:
  - Poor convergence: may indicate inappropriate zeroth-order method choice or insufficient communication rounds
  - Performance degradation: could signal harmful aggregation due to data heterogeneity or overfitting on local datasets
  - API communication failures: breaks the optimization loop entirely

- First 3 experiments:
  1. Image classification on Caltech101 with 5 clients, IID data split, SPSA optimization, measure accuracy improvement over zero-shot baseline
  2. Text classification on SST-2 with 5 clients, few-shot setting, policy gradient optimization, compare against BDPL and manual prompts
  3. Instruction optimization on Letters List dataset with 5 clients, Bayesian optimization, measure success rate compared to InstructZero baseline

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the provided text.

## Limitations
- The framework assumes reliable black-box API responses with consistent prediction and loss outputs, which may not hold in real-world scenarios
- Performance heavily depends on the quality and heterogeneity of local datasets, with extreme non-IID distributions potentially harming aggregation quality
- Zeroth-order optimization methods may struggle with high-dimensional prompt spaces or complex loss landscapes

## Confidence
- **High confidence**: Core federated learning mechanism and its ability to aggregate prompt generators without data sharing
- **Medium confidence**: Effectiveness of zeroth-order optimization across all three task types, particularly for instruction optimization
- **Medium confidence**: Claimed performance improvements, as some baselines may not represent state-of-the-art methods

## Next Checks
1. Test Fed-BBPT with intermittent API failures or noisy responses to evaluate robustness of zeroth-order optimization under realistic conditions
2. Evaluate performance across different data heterogeneity levels (from IID to highly non-IID) to identify breaking points for aggregation
3. Compare convergence speed and final performance against white-box prompt tuning methods on a subset of tasks to quantify the black-box optimization overhead