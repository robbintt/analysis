---
ver: rpa2
title: 'Hierarchical Catalogue Generation for Literature Review: A Benchmark'
arxiv_id: '2304.03512'
source_url: https://arxiv.org/abs/2304.03512
tags:
- catalogue
- domain
- adaptation
- generation
- survey
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new task, Hierarchical Catalogue Generation
  for Literature Review (HiCatGLR), which aims to generate a hierarchical catalogue
  for a review paper given various references. To address this task, the authors construct
  a novel English Hierarchical Catalogues of Literature Reviews Dataset (HiCaD) with
  13.8k literature review catalogues and 120k reference papers.
---

# Hierarchical Catalogue Generation for Literature Review: A Benchmark

## Quick Facts
- arXiv ID: 2304.03512
- Source URL: https://arxiv.org/abs/2304.03512
- Reference count: 40
- Primary result: LED-large with title in encoder achieves best performance on hierarchical catalogue generation task

## Executive Summary
This paper introduces Hierarchical Catalogue Generation for Literature Review (HiCatGLR), a novel task that aims to automatically generate hierarchical catalogues for literature review papers from reference materials. The authors construct HiCaD, a dataset containing 13.8k literature review catalogues with 120k reference papers, and propose two evaluation metrics (CQE and CEDS) to assess catalogue quality. Through extensive benchmarking with state-of-the-art models including LED, BART, and FiD, the paper demonstrates that an end-to-end approach using LED-large with the survey title in the encoder achieves the best performance on this challenging task.

## Method Summary
The authors propose HiCatGLR as a new task requiring models to generate hierarchical catalogues from survey titles and reference paper abstracts. They construct HiCaD by extracting catalogues from review papers on arXiv and references from Semantic Scholar. Two approaches are explored: end-to-end generation where title and references are jointly encoded, and step-by-step generation that progressively builds hierarchical levels. The LED-large model with title in the encoder performs best, likely due to its ability to handle long sequences (up to 16k tokens) while capturing cross-document relationships in a single forward pass.

## Key Results
- LED-large with end-to-end approach achieves the best performance on both CEDS and CQE metrics
- Step-by-step generation provides better control over specific hierarchical levels despite lower overall performance
- CEDS metric shows strong correlation with human evaluation and outperforms ROUGE in capturing structural quality
- The dataset construction process yields 13.8k survey-reference pairs with an average of 80.6 references per survey

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LED-large achieves best results by handling long input sequences and capturing cross-document relationships
- Mechanism: Sparse attention extends input limit to 16,384 tokens while joint encoding of title and references learns relationships between survey topic and individual papers
- Core assumption: Joint representations capture how survey title relates to reference collection
- Break condition: Diverse or unrelated references produce confused representations

### Mechanism 2
- Claim: Step-by-step generation improves quality for specific levels through progressive refinement
- Mechanism: Decomposes generation into stages (top-level → L1,2 → L1,2,3) allowing focused learning of hierarchical relationships
- Core assumption: Breaking down complexity allows more precise level-to-level relationships
- Break condition: Poor intermediate headings propagate errors through hierarchy

### Mechanism 3
- Claim: CEDS effectively measures semantic similarity and structural hierarchy through tree edit distance
- Mechanism: Combines BERTScore with tree edit distance to capture both content and hierarchical structure
- Core assumption: Hierarchical structure can be represented as ordered labeled tree where edit distance captures differences
- Break condition: Complex structures beyond 3 levels may not be fully captured

## Foundational Learning

- Concept: Long document processing techniques (sparse attention, multi-encoder architectures)
  - Why needed: Task involves processing 80+ reference papers exceeding 14,000 words, beyond standard transformers
  - Quick check: What is BART's maximum input length and how does LED extend this?

- Concept: Tree edit distance algorithms for hierarchical text structure evaluation
  - Why needed: Catalogues require metrics capturing both semantic content and structural organization
  - Quick check: How does tree edit distance differ from string edit distance for hierarchical evaluation?

- Concept: Progressive/multi-step generation strategies in sequence-to-sequence models
  - Why needed: Hierarchical nature suggests simultaneous generation may be more challenging than incremental approaches
- Quick check: What are advantages and disadvantages of decomposing complex generation tasks?

## Architecture Onboarding

- Component map: Input processing -> LED/FiD encoder -> Transformer decoder -> CEDS/CQE evaluation -> Results storage
- Critical path: Load references → Encode with LED → Generate catalogue via beam search → Evaluate with CEDS/CQE → Store results
- Design tradeoffs: End-to-end provides better performance but step-by-step allows focused level generation; encoder title input provides better context
- Failure signatures: Low CEDS but high ROUGE indicates content captured but structure missed; repetitive headings indicate diversity issues
- First 3 experiments:
  1. Implement LED-base end-to-end with title in encoder, measure CEDS/CQE on validation
  2. Implement step-by-step generation with LED-large, compare level-specific performance
  3. Test CEDS metric on synthetic catalogue pairs to verify tree edit distance implementation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the task be extended to handle catalogues with more than three levels?
- Basis: Paper only considers up to third-level headings without exploring deeper hierarchies
- Why unresolved: Dataset and metrics limited to three levels, no discussion of deeper hierarchy challenges
- What evidence would resolve: Dataset extension with deeper hierarchies and experiments on models handling complex structures

### Open Question 2
- Question: How can catalogue quality be improved by incorporating more information from reference papers beyond abstracts?
- Basis: Abstracts are used as representative information, but input length limits additional information
- Why unresolved: Paper doesn't explore using full texts or key phrases and their impact on quality
- What evidence would resolve: Experiments comparing models using different information types from references

### Open Question 3
- Question: How can evaluation metrics CQE and CEDS be further refined to better assess catalogue quality?
- Basis: Paper proposes metrics but acknowledges potential need for validation and refinement
- Why unresolved: Effectiveness demonstrated through correlation tests but potential limitations unexplored
- What evidence would resolve: Additional experiments identifying metric biases or weaknesses with proposed modifications

## Limitations

- Evaluation metrics may not fully capture nuanced quality of literature review catalogues
- Dataset construction from HTML sources could introduce noise or inconsistencies
- CQE metric's assumption about frequent words indicating quality may not always hold true

## Confidence

- High confidence: Experimental methodology and comparative analysis between approaches are sound
- Medium confidence: CEDS metric effectiveness for hierarchical evaluation relies on tree edit distance assumptions
- Medium confidence: Generalization to domains beyond computer science given dataset's specific focus

## Next Checks

1. Conduct human evaluation studies comparing generated catalogues against ground truth using expert reviewers
2. Test approach on held-out dataset from different domain (e.g., biomedical literature) to assess generalization
3. Implement ablation studies removing title from encoder input to quantify its contribution to performance gains