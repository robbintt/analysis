---
ver: rpa2
title: 'Don''t Ignore Dual Logic Ability of LLMs while Privatizing: A Data-Intensive
  Analysis in Medical Domain'
arxiv_id: '2309.04198'
source_url: https://arxiv.org/abs/2309.04198
tags:
- medical
- data
- llms
- literature
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces the CALLA dataset to probe LLMs\u2019 interactive\
  \ knowledge acquisition from Chinese medical literature. The authors construct a\
  \ multi-round dialogue dataset based on medical literature abstracts and employ\
  \ an automatic framework to generate instruction fine-tuning (IFT) data."
---

# Don't Ignore Dual Logic Ability of LLMs while Privatizing: A Data-Intensive Analysis in Medical Domain

## Quick Facts
- arXiv ID: 2309.04198
- Source URL: https://arxiv.org/abs/2309.04198
- Reference count: 5
- Primary result: IFT data correlated with medical literature improves LLMs' interactive knowledge use; general-domain IFT degrades performance.

## Executive Summary
This paper introduces the CALLA dataset to probe how instruction fine-tuning (IFT) data affects LLMs' ability to acquire and apply medical knowledge interactively. The authors find that IFT data highly correlated with the medical literature corpus acts as a powerful catalyst for LLMs to employ pre-training medical knowledge in dialogue scenarios, while IFT data from other sources can degrade performance. A key insight is the importance of preserving "dual logic ability"—the capacity to maintain consistent stance when confronted with both positive and negative statements about the same fact—during privatization. The study offers guidance for effective privatization of LLMs in real-world medical applications.

## Method Summary
The authors construct a multi-round dialogue dataset based on Chinese medical literature abstracts, focusing on hepatobiliary and pancreatic diseases. They employ an automatic framework to generate IFT data from these abstracts, extract conclusion information as "golden facts," and create paired test questions (Qconsist/Qinconsist) to evaluate dual logic consistency. LoRA parameter-efficient fine-tuning is used to adapt Llama/Alpaca models, with performance evaluated using a ChatGPT judge on both in-domain and out-of-domain test sets. The study compares models fine-tuned on IFT data correlated with medical literature versus general-domain or other medical-domain IFT data.

## Key Results
- IFT data highly correlated with medical literature significantly improves LLMs' ability to use pre-training medical knowledge in interactive scenarios.
- Using IFT data from other sources can degrade LLMs' performance on medical dialogue tasks.
- Dual logic ability—maintaining consistent stance across positive and negative statements—is crucial and must be preserved during privatization.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IFT data highly correlated with medical literature improves LLMs' ability to use pre-training medical knowledge in interactive scenarios.
- Mechanism: IFT data mirrors the medical literature abstracts used in pre-training, creating a strong semantic bridge between passive knowledge storage and active retrieval during dialogue.
- Core assumption: Medical literature abstracts contain sufficient coverage and IFT data reflects factual conclusions without introducing contradictions.
- Evidence anchors: Experimental results show performance degradation when using unrelated IFT data; IFT correlation serves as a catalyst for knowledge application.
- Break condition: If pre-training corpus is too narrow or IFT data introduces inconsistent medical facts, the correlation effect collapses.

### Mechanism 2
- Claim: Dual logic ability ensures consistent stance across positive and negative statements of the same fact.
- Mechanism: Training on paired Q&A data (consistent/inconsistent with the golden fact) teaches the LLM to recognize and preserve logical negation relationships, preventing "fact-following response."
- Core assumption: The model can distinguish between surface form and logical negation, and training pairs enforce this distinction.
- Evidence anchors: Paired Qconsist/Qinconsist test data constructed to evaluate consistency; identified "fact-following response" phenomenon.
- Break condition: If negation pairs are too simple or the model overfits to surface patterns, dual logic consistency fails.

### Mechanism 3
- Claim: Abandoning the pre-training stage is viable only if IFT data fully captures all domain knowledge.
- Mechanism: IFT data acts as a condensed, interactive representation of medical knowledge; if exhaustive enough, it can substitute for broader pre-training exposure.
- Core assumption: Medical literature abstracts contain essential knowledge and IFT construction captures it completely.
- Evidence anchors: Performance on O.O.D. data drops when pre-training is skipped, suggesting incomplete knowledge coverage.
- Break condition: If IFT data is incomplete or omits rare but critical facts, performance degrades sharply.

## Foundational Learning

- Concept: Instruction Fine-Tuning (IFT)
  - Why needed here: Transforms a general LLM into one that follows structured instructions, essential for converting raw medical knowledge into actionable Q&A pairs.
  - Quick check question: Does the IFT data contain both instruction and expected completion for each example?

- Concept: Dual Logic Consistency
  - Why needed here: Prevents the model from defaulting to affirming any stated fact, ensuring it can handle negation and contradiction in medical dialogue.
  - Quick check question: Can the model correctly negate a fact when the question is phrased inconsistently with the known truth?

- Concept: Parameter-Efficient Fine-Tuning (PEFT)
  - Why needed here: Enables adaptation of large models without full retraining, critical when memory or compute is constrained.
  - Quick check question: Are the LoRA adapters trained on the same data splits used for evaluation?

## Architecture Onboarding

- Component map:
  - Pre-training corpus: Chinese medical abstracts (hepatobiliary/pancreatic focus)
  - IFT construction pipeline: Extract conclusions → generate dialogues → clean subjective language
  - LoRA adapters: 16MB (SFT-only) and 76MB (pre-training + SFT)
  - Evaluation: ChatGPT judge on consistency of responses to Qconsist/Qinconsist pairs

- Critical path:
  1. Collect abstracts → extract conclusions → generate IFT pairs
  2. Train LoRA adapters (either SFT-only or pre-training + SFT)
  3. Evaluate on CALLA test sets (I.D. vs O.O.D.)
  4. Analyze dual logic consistency metrics

- Design tradeoffs:
  - Narrow domain vs broad coverage: Focusing on hepatobiliary abstracts limits generality but improves depth.
  - Data cleaning: Removing subjective language improves objectivity but may lose nuance.
  - LoRA rank/size: Larger adapters capture more fine-grained knowledge but increase memory use.

- Failure signatures:
  - High accuracy on Qconsist but low on Qinconsist → "fact-following response" issue
  - Similar performance on I.D. and O.O.D. when pre-training is skipped → insufficient knowledge coverage
  - Drop in performance when using general-domain IFT → domain mismatch

- First 3 experiments:
  1. Train LoRA on 16MB with our IFT data; evaluate dual logic consistency.
  2. Train LoRA on 76MB with our IFT data; compare I.D. vs O.O.D. accuracy.
  3. Train LoRA on 16MB with general-domain IFT; measure degradation relative to baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the "fact-following response" phenomenon manifest differently across various medical specialties, and what are the implications for domain-specific knowledge acquisition?
- Basis in paper: [explicit] The authors identify the "fact-following response" phenomenon where LLMs tend to affirm facts mentioned in questions regardless of their consistency with the golden fact.
- Why unresolved: The paper focuses on hepatobiliary and pancreatic diseases; the phenomenon's generalizability across other medical specialties remains unexplored.
- What evidence would resolve it: Comparative studies examining the "fact-following response" across multiple medical specialties using the CALLA dataset methodology would provide insights into its variability and implications.

### Open Question 2
- Question: What is the optimal balance between pre-training on medical literature and fine-tuning on instruction data for maximizing LLMs' interactive medical knowledge acquisition?
- Basis in paper: [explicit] The authors observe that both pre-training on medical literature and fine-tuning on IFT data contribute to LLMs' performance, but the optimal balance is not determined.
- Why unresolved: The study explores the effects of pre-training and fine-tuning separately but does not investigate the optimal combination or proportion of each.
- What evidence would resolve it: Systematic experiments varying the extent of pre-training and fine-tuning, followed by performance evaluations on the CALLA dataset, would help identify the optimal balance.

### Open Question 3
- Question: How can the CALLA dataset be extended to evaluate LLMs' ability to handle complex medical reasoning tasks beyond fact-checking?
- Basis in paper: [explicit] The CALLA dataset focuses on free-dialogue fact-checking tasks, but its applicability to more complex reasoning tasks is not explored.
- Why unresolved: The dataset's current design may not fully capture the nuances of complex medical reasoning, such as differential diagnosis or treatment planning.
- What evidence would resolve it: Expanding the CALLA dataset to include tasks requiring multi-step reasoning, causal inference, or clinical decision-making would demonstrate its broader applicability and limitations.

## Limitations
- Dataset focus on hepatobiliary and pancreatic diseases limits generalizability to broader medical domains.
- Automatic IFT generation may introduce artifacts or miss nuanced medical knowledge.
- ChatGPT judge introduces potential bias, as the same model family could share underlying reasoning patterns with fine-tuned models.

## Confidence
- High Confidence: IFT data correlated with medical literature improves interactive knowledge application.
- Medium Confidence: Dual logic ability prevents "fact-following response."
- Medium Confidence: Abandoning pre-training is viable only with exhaustive IFT.

## Next Checks
1. Test fine-tuned models on CALLA-style datasets from other medical specialties (e.g., cardiology, neurology) to assess cross-domain generalization of dual logic consistency.
2. Replace ChatGPT judge with a panel of medical experts to independently assess response consistency across Qconsist/Qinconsist pairs.
3. Systematically remove subsets of IFT data (e.g., rare diseases, edge cases) and measure degradation in both I.D. and O.O.D. performance to quantify completeness threshold for viable pre-training abandonment.