---
ver: rpa2
title: 'Arbitrariness and Social Prediction: The Confounding Role of Variance in Fair
  Classification'
arxiv_id: '2301.11562'
source_url: https://arxiv.org/abs/2301.11562
tags:
- self-consistency
- classi
- variance
- test
- total
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces self-consistency as a metric to measure the
  arbitrariness in fair classification. It defines self-consistency as the probability
  that two models trained on different subsets of the data agree on their predictions
  for a given test example.
---

# Arbitrariness and Social Prediction: The Confounding Role of Variance in Fair Classification

## Quick Facts
- arXiv ID: 2301.11562
- Source URL: https://arxiv.org/abs/2301.11562
- Reference count: 40
- Key outcome: Variance in model predictions across training subsets can cause classifications to be effectively arbitrary, undermining fairness.

## Executive Summary
This paper investigates how variance in machine learning predictions contributes to arbitrary classifications that undermine fairness in binary classification tasks. The authors introduce self-consistency as a metric that measures agreement between models trained on different data subsets, using this to identify when predictions are effectively arbitrary. They develop an ensembling algorithm that abstains from classification when predictions lack sufficient self-consistency, demonstrating across multiple datasets that this approach can significantly reduce subgroup error rate disparities without traditional fairness interventions. Their findings challenge the practical utility of common algorithmic fairness methods and suggest reconsidering how fairness is measured in classification systems.

## Method Summary
The method uses bootstrap sampling to create multiple training datasets from a single dataset, trains an ensemble of models on each replicate, and calculates self-consistency as the probability that two models agree on their prediction for a test example. The ensembling algorithm predicts only when ensemble agreement exceeds a user-specified self-consistency threshold, abstaining otherwise. This approach provably increases self-consistency while maintaining accuracy on non-arbitrary predictions. The authors validate their algorithm across six datasets (COMPAS, Old Adult, South German Credit, Taiwan Credit, New Adult-CA, and HMDA) and multiple model types, showing that variance reduction through ensembling can improve both fairness and accuracy metrics.

## Key Results
- Applying ensembling with confidence thresholding significantly reduces subgroup error rate disparities without traditional fairness interventions
- Variance in predictions across different trained models is a significant, under-explored source of error in fair binary classification
- The self-consistency metric effectively identifies arbitrary predictions that contribute to unfairness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Variance in model predictions across training subsets can cause classifications to be effectively arbitrary, undermining fairness.
- Mechanism: When different models trained on bootstrap replicates disagree on predictions for the same example, this disagreement reflects uncertainty in the learning process. For some examples, disagreement is so high that the prediction is essentially random.
- Core assumption: Disagreement between models trained on different data subsets is a reliable proxy for arbitrariness in classification decisions.
- Evidence anchors:
  - [abstract] "Variance in predictions across different trained models is a significant, under-explored source of error in fair binary classification."
  - [section] "Empirically, some decisions can in fact be so unstable that they are effectively arbitrary."
- Break condition: If disagreement is low across all examples, or if disagreement correlates with model bias rather than data variance.

### Mechanism 2
- Claim: Self-consistency metric quantifies how reliably models agree on predictions, serving as a proxy for confidence/arbitrariness.
- Mechanism: Self-consistency measures the probability that two models trained on different subsets agree on their prediction for a test example. Higher self-consistency indicates more confident, less arbitrary predictions.
- Core assumption: Agreement between models trained on different subsets correlates with prediction confidence and reduces arbitrariness.
- Evidence anchors:
  - [abstract] "We: 1) Define a metric called self-consistency, derived from variance, which we use as a proxy for measuring and reducing arbitrariness"
  - [section] "We formalize a notion of self-consistency of a learning process"
- Break condition: If agreement is artificially high due to model bias or dataset limitations rather than genuine confidence.

### Mechanism 3
- Claim: Ensembling with confidence threshold improves fairness by abstaining from arbitrary predictions.
- Mechanism: The algorithm predicts only when ensemble agreement exceeds a user-specified self-consistency threshold, abstaining otherwise. This removes arbitrary predictions while maintaining accurate ones.
- Core assumption: Abstaining on low-self-consistency examples improves overall fairness without sacrificing too much accuracy.
- Evidence anchors:
  - [abstract] "We develop an ensembling algorithm that abstains from classification when a prediction would be arbitrary, based on a user-specified level of self-consistency."
  - [section] "Our ensembling algorithm that provably increases self-consistency. Our algorithm predicts for inputs that attain a user-specified level of confidence, and abstains otherwise"
- Break condition: If abstaining too frequently reduces accuracy below acceptable thresholds or if threshold selection is suboptimal.

## Foundational Learning

- Concept: Bootstrap sampling for variance estimation
  - Why needed here: To simulate drawing different training datasets from the same distribution when only one dataset is available.
  - Quick check question: If we have 100 examples and sample with replacement to create a bootstrap replicate, what's the probability that a specific example is included?

- Concept: Self-consistency as agreement probability
  - Why needed here: To measure how reliably models agree on predictions, serving as a proxy for arbitrariness in classification.
  - Quick check question: If 10 models predict {0,0,0,0,0,1,1,1,1,1} for an example, what's the self-consistency?

- Concept: Cost-sensitive loss and decision thresholds
  - Why needed here: To understand how asymmetric costs affect classification decisions and variance calculations.
  - Quick check question: If false negatives cost 3× more than false positives, what threshold does this imply for binary classification?

## Architecture Onboarding

- Component map: Data preprocessing -> Bootstrap sampling -> Model training -> Prediction aggregation -> Self-consistency calculation -> Abstention decision
- Critical path:
  1. Generate B bootstrap replicates of training data
  2. Train ensemble of models on each replicate
  3. Collect predictions for test examples
  4. Calculate self-consistency for each example
  5. Apply confidence threshold to decide predict/abstain
  6. Return final predictions or abstentions
- Design tradeoffs:
  - Higher B increases self-consistency accuracy but computational cost
  - Higher confidence threshold reduces arbitrariness but may increase abstentions
  - Single vs. super-ensembling affects variance reduction and abstention rates
- Failure signatures:
  - High abstention rate → Increase confidence threshold or reduce B
  - Low self-consistency improvement → Check bootstrap sampling or model training
  - Unfairness persists → Verify protected attribute handling and subgroup analysis
- First 3 experiments:
  1. Baseline: Train single model, measure self-consistency distribution
  2. Simple ensembling: Apply confidence threshold, measure abstention rate vs accuracy
  3. Super-ensembling: Add variance reduction step, compare to simple ensembling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design methods to improve self-consistency without relying on abstention, which could lead to high abstention rates?
- Basis in paper: [explicit] The paper introduces an ensembling algorithm that abstains from classification when predictions lack sufficient self-consistency, and acknowledges the trade-off between abstention rate and accuracy.
- Why unresolved: While the authors demonstrate that their ensembling approach improves self-consistency and fairness, they also observe that abstention rates can be substantial, especially for high-variance tasks. This suggests there may be fundamental limits to how much self-consistency can be improved without abstaining, but the paper does not explore alternative methods that could achieve similar benefits without abstaining.
- What evidence would resolve it: Development and evaluation of novel algorithms that improve self-consistency through approaches other than abstention, with experimental validation showing comparable or better performance in terms of both fairness and accuracy.

### Open Question 2
- Question: What are the normative implications of producing highly self-consistent predictions that are systematically incorrect?
- Basis in paper: [explicit] The authors note that it is possible for examples to be highly self-consistent but incorrectly classified, and suggest this could indicate label bias, but defer this investigation to future work.
- Why unresolved: The paper focuses on the technical relationship between self-consistency and arbitrariness, but does not explore the ethical implications of consistently incorrect predictions, particularly in fairness contexts where such errors could reinforce or amplify existing biases.
- What evidence would resolve it: Empirical studies examining patterns of self-consistent incorrect predictions across subgroups, combined with theoretical analysis of the normative implications of such systematic errors in high-stakes decision-making contexts.

### Open Question 3
- Question: How does the relationship between variance and self-consistency differ across model classes and dataset sizes?
- Basis in paper: [explicit] The authors observe that logistic regression generally exhibits lower variance than decision trees and random forests, and that variance reduction techniques are more effective for the latter. They also note that larger datasets tend to exhibit lower variance.
- Why unresolved: While the paper provides initial observations about variance differences across models and datasets, it does not systematically characterize how the relationship between variance and self-consistency varies across different model classes (e.g., neural networks, boosting methods) or dataset scales.
- What evidence would resolve it: Comprehensive empirical studies measuring variance and self-consistency across diverse model classes and dataset sizes, with theoretical analysis explaining the observed patterns and their implications for fairness.

## Limitations
- The bootstrap-based variance estimation assumes independence between training examples, which may not hold in practice
- The paper doesn't address how self-consistency interacts with dataset shift or temporal changes in the data distribution
- The claim that variance reduction alone can address fairness concerns may overstate the solution's generality

## Confidence
- Arbitrariness as fairness problem: Medium confidence
- Self-consistency as solution: Medium confidence
- Ensemble abstention improves fairness: High confidence

## Next Checks
1. Evaluate the ensembling approach on temporally separated data splits to assess robustness to changing distributions
2. Test the method across different model families (neural networks, gradient boosting, SVMs) to establish generalizability beyond the presented examples
3. Compare self-consistency-based abstention against alternative uncertainty quantification methods (Bayesian methods, Monte Carlo dropout) to isolate the contribution of the specific variance estimation approach