---
ver: rpa2
title: 'C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation
  Models'
arxiv_id: '2305.08322'
source_url: https://arxiv.org/abs/2305.08322
tags:
- chinese
- school
- questions
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'C-Eval is the first comprehensive Chinese evaluation suite for
  foundation models, comprising 13,948 multiple-choice questions across 52 disciplines
  spanning four difficulty levels: middle school, high school, college, and professional.
  The benchmark includes C-Eval Hard, a subset of challenging subjects requiring advanced
  reasoning abilities.'
---

# C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models

## Quick Facts
- arXiv ID: 2305.08322
- Source URL: https://arxiv.org/abs/2305.08322
- Reference count: 24
- Only GPT-4 achieves over 60% average accuracy on C-Eval (68.7%)

## Executive Summary
C-Eval is the first comprehensive Chinese evaluation suite for foundation models, comprising 13,948 multiple-choice questions across 52 disciplines spanning four difficulty levels from middle school to professional. The benchmark includes C-Eval Hard, a subset of challenging subjects requiring advanced reasoning abilities. Experiments show that GPT-4 significantly outperforms other models including ChatGPT, Claude, and various Chinese-oriented models, with only GPT-4 achieving over 60% average accuracy. The results highlight the significant room for improvement in current LLMs and the importance of evaluating models in challenging scenarios that require advanced reasoning abilities.

## Method Summary
The evaluation methodology uses five-shot prompting with both answer-only and chain-of-thought settings to assess model performance on C-Eval's 13,948 multiple-choice questions. The benchmark covers 52 diverse disciplines across four difficulty levels: middle school, high school, college, and professional. Models are evaluated using standardized accuracy metrics, with particular focus on C-Eval Hard, a subset containing advanced mathematics, discrete mathematics, probability and statistics, and college-level science subjects that require complex reasoning abilities.

## Key Results
- GPT-4 achieves 68.7% average accuracy on C-Eval, while other models trail significantly behind
- On C-Eval Hard, GPT-4 achieves 54.9% accuracy, demonstrating the benchmark's difficulty
- Chinese-oriented models like MiniMax perform competitively on Chinese knowledge subjects but lag 14.1 percentage points behind ChatGPT on C-Eval Hard

## Why This Works (Mechanism)

### Mechanism 1
- Claim: C-Eval's design targets advanced reasoning abilities by sourcing questions from real-world exams across four difficulty levels.
- Mechanism: By selecting questions from middle school, high school, college, and professional exams, C-Eval creates a hierarchical evaluation structure that systematically probes model capabilities from basic to expert reasoning.
- Core assumption: Questions from standardized exams accurately reflect the knowledge and reasoning abilities expected at each educational level.
- Evidence anchors:
  - [abstract] "C-Eval comprises multiple-choice questions across four difficulty levels: middle school, high school, college, and professional."
  - [section] "C-Eval consists of 13948 multiple-choice exam questions spanning 52 diverse disciplines, ranging from humanities to science and engineering."
  - [corpus] Weak - related papers focus on benchmark creation but don't specifically validate the exam question selection methodology.
- Break condition: If exam questions become outdated or if educational standards change significantly, the benchmark's alignment with actual reasoning abilities may degrade.

### Mechanism 2
- Claim: The C-Eval Hard subset effectively isolates models' reasoning capabilities by including particularly challenging subjects.
- Mechanism: By selecting advanced mathematics, discrete mathematics, probability and statistics, and college-level science subjects, C-Eval Hard creates a focused evaluation on complex reasoning tasks that require multiple steps and deep domain knowledge.
- Core assumption: These selected subjects represent the most challenging reasoning tasks that can distinguish between different levels of model capability.
- Evidence anchors:
  - [abstract] "C-Eval is accompanied by C-Eval Hard, a subset of very challenging subjects in C-Eval that requires advanced reasoning abilities to solve."
  - [section] "C-Eval Hard aligns with recent efforts to create difficult benchmarks to assess advanced reasoning abilities (Hendrycks et al., 2021b; Suzgun et al., 2022)."
  - [corpus] Weak - corpus neighbors mention similar benchmarks but don't provide specific evidence about subject selection criteria.
- Break condition: If models develop specialized reasoning capabilities for these subjects without general reasoning improvement, C-Eval Hard may overestimate true reasoning ability.

### Mechanism 3
- Claim: The multi-choice format with exactly four options provides clear, unambiguous evaluation metrics.
- Mechanism: By standardizing on four-choice questions, C-Eval eliminates ambiguity in evaluation and enables straightforward accuracy measurement across all subjects.
- Core assumption: Multi-choice format adequately captures the complexity of knowledge and reasoning without requiring open-ended responses.
- Evidence anchors:
  - [section] "We only select questions of a multi-choice format, similar to Hendrycks et al. (2021a), because: (1) metrics are clearly defined (i.e. accuracy), and (2) multi-choice questions are a simple but good proxy to evaluate the potential of advanced abilities of foundation models."
  - [corpus] Weak - corpus neighbors mention various evaluation approaches but don't specifically address multi-choice format advantages.
- Break condition: If models develop strategies to game multi-choice formats without genuine understanding, accuracy metrics may become misleading.

## Foundational Learning

- Concept: Multiple difficulty levels in evaluation
  - Why needed here: Differentiates between basic knowledge retrieval and complex reasoning abilities, allowing precise identification of model strengths and weaknesses
  - Quick check question: Can a model achieve high accuracy on middle school questions but fail on professional-level questions, and what does this reveal about its capabilities?

- Concept: Chain-of-thought prompting
  - Why needed here: Enables models to demonstrate step-by-step reasoning processes, which is crucial for evaluating complex problem-solving abilities in advanced subjects
  - Quick check question: Does chain-of-thought prompting improve performance on reasoning-heavy subjects like advanced mathematics and physics?

- Concept: Benchmark contamination prevention
  - Why needed here: Ensures evaluation reflects true model capabilities rather than memorization of training data, maintaining benchmark integrity
  - Quick check question: How does sourcing questions from mock exams rather than official national exams reduce data contamination risk?

## Architecture Onboarding

- Component map: Data collection pipeline → Question processing → Model evaluation → Result submission → Leaderboard maintenance
- Critical path: Question collection → Format standardization → Model testing → Result validation → Public reporting
- Design tradeoffs: Multi-choice format enables clear metrics but may not capture all reasoning nuances; using real exam questions ensures relevance but requires careful contamination prevention
- Failure signatures: Low variance in scores across difficulty levels suggests data leakage; poor chain-of-thought performance on reasoning subjects indicates inadequate reasoning capabilities
- First 3 experiments:
  1. Run a single model through all four difficulty levels to establish baseline performance patterns
  2. Test chain-of-thought prompting on a subset of reasoning-heavy subjects to evaluate its effectiveness
  3. Compare performance on Chinese-oriented vs. English-oriented models to identify language-specific strengths and weaknesses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact architecture and training methodology of MiniMax that allows it to outperform other Chinese-oriented models like GLM-130B and ChatGLM-6B?
- Basis in paper: [explicit] The paper states that MiniMax is a Chinese LLM based on Transformer structure using "user-in-the-loop" mechanism, but details are not publicly disclosed.
- Why unresolved: The authors note that the details of MiniMax's training and structure are not publicly disclosed, making it difficult to understand what specific architectural choices or training techniques give it an edge over other Chinese models.
- What evidence would resolve it: Detailed technical specifications of MiniMax's architecture, training data composition, and training procedures would be needed to understand its superior performance.

### Open Question 2
- Question: How does the performance of C-Eval models vary when evaluated on zero-shot versus few-shot settings?
- Basis in paper: [inferred] The paper evaluates models in a five-shot setting and mentions that few-shot settings can highlight inherent abilities of a model, but does not directly compare with zero-shot performance.
- Why unresolved: While the paper argues for the benefits of few-shot evaluation, it does not provide data on how models perform in zero-shot settings, which would be valuable for understanding the full capabilities of these models.
- What evidence would resolve it: A direct comparison of model performance on C-Eval in both zero-shot and few-shot settings would reveal the impact of few-shot exemplars on model performance.

### Open Question 3
- Question: What specific types of reasoning or knowledge domains do Chinese-oriented models like MiniMax excel at compared to English-oriented models?
- Basis in paper: [explicit] The paper notes that MiniMax outperforms ChatGPT in humanities subjects focusing on Chinese knowledge, such as Mao Zedong Thought, Art Studies, Chinese Language and Literature, and Modern Chinese History.
- Why unresolved: While the paper identifies some subjects where Chinese-oriented models excel, it does not provide a comprehensive analysis of the types of reasoning or knowledge domains that give these models an advantage.
- What evidence would resolve it: A detailed breakdown of model performance across various reasoning types (e.g., causal reasoning, analogical reasoning) and knowledge domains would help identify the specific strengths of Chinese-oriented models.

## Limitations
- The benchmark relies on multiple-choice questions which may not fully capture open-ended reasoning capabilities
- Question sourcing from mock exams, while reducing contamination risk, may not fully represent the diversity of real-world reasoning challenges
- The evaluation focuses on Chinese-language capabilities, limiting generalizability to multilingual contexts

## Confidence

- **High confidence**: GPT-4's superior performance across all difficulty levels is consistently demonstrated, with specific accuracy scores (68.7% overall, 54.9% on C-Eval Hard) that are unlikely to change significantly with minor methodological adjustments.
- **Medium confidence**: The claim that Chinese-oriented models perform competitively on Chinese knowledge subjects but struggle with complex reasoning has merit but requires further validation across a broader range of models and subjects.
- **Low confidence**: The assertion that C-Eval effectively measures "advanced reasoning abilities" is plausible but not definitively proven, as the evaluation format and question selection criteria could potentially be gamed by models without genuine reasoning improvement.

## Next Checks
1. Test model performance on C-Eval with varied prompting strategies (different chain-of-thought templates, temperature settings) to assess sensitivity to evaluation methodology.
2. Conduct error analysis on C-Eval Hard questions to identify specific reasoning failure modes across different model types.
3. Compare C-Eval results with performance on open-ended Chinese reasoning tasks to validate that multiple-choice format accurately reflects reasoning capabilities.