---
ver: rpa2
title: LLM Cognitive Judgements Differ From Human
arxiv_id: '2307.11787'
source_url: https://arxiv.org/abs/2307.11787
tags:
- chatgpt
- human
- language
- gpt-3
- cognitive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examined the cognitive capabilities of GPT-3 and ChatGPT
  on an inductive reasoning task from cognitive science. The study compared model
  predictions to human judgments and Bayesian model predictions on everyday phenomena
  with limited data.
---

# LLM Cognitive Judgements Differ From Human

## Quick Facts
- arXiv ID: 2307.11787
- Source URL: https://arxiv.org/abs/2307.11787
- Authors: 
- Reference count: 33
- Key outcome: GPT-3 and ChatGPT perform poorly on inductive reasoning tasks compared to human judgments and Bayesian models

## Executive Summary
This study evaluates GPT-3 and ChatGPT on inductive reasoning tasks from cognitive science, comparing their predictions to human judgments and Bayesian model predictions. The models were tested on everyday phenomena like cake baking times, life spans, and movie grosses using limited intermediate data. Both models demonstrated significantly higher prediction errors than humans, with ChatGPT showing additional refusal to answer certain questions. These findings suggest that LLMs do not accurately model the statistical principles underlying human cognitive judgments.

## Method Summary
The study used 20 prompts per model per task covering six everyday phenomena. Models were prompted to predict single-number outcomes (total duration/quantity) given intermediate values. Predictions were compared to human judgment data and Bayesian model predictions from reference [15]. Mean Average Percentage Error (MAPE) was calculated as the primary metric. ChatGPT and GPT-3 were tested using their default settings, with responses extracted when models provided single-number answers.

## Key Results
- Both models showed consistently higher MAPE than human judgments across all tasks
- ChatGPT frequently refused to answer life span and movie gross questions
- GPT-3 and ChatGPT demonstrated inconsistent predictions across repeated trials
- Neither model achieved MAPE performance close to the Bayesian benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-3 and ChatGPT fail at inductive reasoning with limited data due to mismatch between their statistical prediction mechanism and human cognitive priors
- Mechanism: Humans rely on Bayesian inference with learned priors over everyday phenomena; LLMs lack these structured priors and instead generate predictions from next-token probabilities without grounding in causal models
- Core assumption: The Bayesian model accurately captures the human cognitive strategy for limited-data predictions
- Evidence anchors: Abstract finding that LLMs don't align with human cognition for inductive judgments; section noting LLMs don't align with human cognition for everyday phenomena

### Mechanism 2
- Claim: LLMs produce inconsistent predictions across repeated trials because they lack stable world models
- Mechanism: Each prompt triggers new stochastic generation from the same parametric model; without fixed internal representation of phenomena, predictions vary widely
- Core assumption: Variability reflects model stochasticity rather than systematic reasoning errors
- Evidence anchors: Section noting inconsistent predictions with confidence intervals; abstract noting ChatGPT's frequent refusals

### Mechanism 3
- Claim: ChatGPT's refusal to answer certain prompts reflects alignment fine-tuning that suppresses generation of potentially harmful or low-confidence outputs
- Mechanism: Fine-tuning on human feedback penalized hallucinatory or uncertain responses; tasks like life span or movie gross prediction trigger this safety filter
- Core assumption: Refusal patterns are artifacts of alignment training rather than model uncertainty
- Evidence anchors: Section noting ChatGPT's hesitation on life spans and movie grosses; abstract highlighting frequent refusals

## Foundational Learning

- Concept: Bayesian inference with prior distributions over quantities
  - Why needed here: The study benchmarks human judgments against a Bayesian model; understanding priors and posterior updating is essential
  - Quick check question: If you hear a cake has baked for 30 minutes, and you know cakes typically take 20-60 minutes, how would you compute a Bayesian prediction for total bake time?

- Concept: Limited-data inductive reasoning
  - Why needed here: The tasks involve predicting an unknown total given only an intermediate value; this is the core cognitive challenge LLMs struggle with
  - Quick check question: Given a movie made $10M so far, what additional information would you need to make a better prediction of total gross, and why?

- Concept: Stochastic language model sampling
  - Why needed here: LLMs generate predictions by sampling from token probabilities; repeated sampling can yield different outputs, explaining observed variability
  - Quick check question: If you prompt an LLM twice with the same question, why might you get different numeric answers?

## Architecture Onboarding

- Component map: Input encoder (token embeddings) -> Transformer layers -> Output head (logits) -> Sampling module -> Final prediction -> InstructGPT alignment layer (refusal trigger)
- Critical path: 1) Prompt formatting (single-number requirement) 2) Sampling with default temperature 3) Parsing numeric output vs. refusal or range
- Design tradeoffs: Higher temperature increases variability but may better expose reasoning differences; alignment fine-tuning improves safety but can suppress useful responses in cognitive tasks; model size affects memorization vs. generalization
- Failure signatures: High MAPE relative to Bayesian model; frequent refusals on life span or movie gross tasks; wide confidence intervals across repeated trials
- First 3 experiments: 1) Replicate 20-prompt trial for "Cakes" at t=30 to measure MAPE and refusal rate 2) Compare GPT-3 base vs. InstructGPT-aligned version on "Life Spans" to isolate alignment effect 3) Test with temperature=0 to see if variability is purely stochastic or systematic

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific statistical principles underlie human cognitive judgments in limited-data inductive reasoning tasks, and how do they differ from the principles used by LLMs?
- Basis in paper: [explicit] The paper highlights that LLMs do not accurately model basic statistical principles that human cognition relies on
- Why unresolved: The paper does not specify which statistical principles are being referenced or how they differ from LLM approaches
- What evidence would resolve it: Detailed comparison of the statistical models used by humans versus LLMs in the context of limited-data inductive reasoning tasks

### Open Question 2
- Question: How does the training data and architecture of LLMs influence their ability to perform inductive reasoning tasks similar to human cognitive judgments?
- Basis in paper: [inferred] The paper suggests that despite extensive training data, LLMs fail to model human-like cognitive judgments
- Why unresolved: The paper does not explore the relationship between LLM architecture, training data, and their performance on inductive reasoning tasks
- What evidence would resolve it: Experiments comparing different LLM architectures and training datasets on their performance in inductive reasoning tasks

### Open Question 3
- Question: What role does the refusal to answer certain questions (e.g., life spans, movie grosses) play in the overall assessment of LLM cognitive judgments?
- Basis in paper: [explicit] The paper notes that ChatGPT frequently refused to answer questions related to life spans and movie grosses
- Why unresolved: The paper does not investigate the impact of these refusals on the overall evaluation of LLM cognitive capabilities
- What evidence would resolve it: Analysis of the correlation between refusal rates and the accuracy of LLM responses in cognitive judgment tasks

## Limitations
- The study relies on visual extraction of human judgment data from plots, which may introduce measurement error
- ChatGPT's refusal behavior may conflate genuine model uncertainty with alignment fine-tuning artifacts
- The comparison assumes the Bayesian model perfectly represents human cognitive strategies without direct validation

## Confidence
- High confidence: LLMs demonstrate significantly higher MAPE than human judgments on inductive reasoning tasks
- Medium confidence: ChatGPT's refusal patterns are primarily driven by alignment fine-tuning rather than task-specific uncertainty
- Low confidence: The Bayesian model perfectly captures human cognitive priors for these limited-data prediction tasks

## Next Checks
1. Re-extract human judgment data from [15] using multiple readers to quantify measurement uncertainty in the ground truth
2. Test GPT-3 base model (without InstructGPT fine-tuning) on life span and movie gross tasks to isolate alignment effects from inherent model capabilities
3. Run extended trials (50+ repetitions) per prompt to distinguish between stochastic variability and systematic reasoning errors in LLM predictions