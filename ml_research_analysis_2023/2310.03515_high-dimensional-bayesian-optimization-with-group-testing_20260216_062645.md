---
ver: rpa2
title: High-dimensional Bayesian Optimization with Group Testing
arxiv_id: '2310.03515'
source_url: https://arxiv.org/abs/2310.03515
tags:
- active
- dimensions
- group
- optimization
- testing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents GTBO, a Bayesian optimization method for high-dimensional
  problems with sparse axis-aligned active subspaces. It introduces a group testing
  approach to identify active variables by iteratively testing groups of variables
  to determine their influence on the objective function.
---

# High-dimensional Bayesian Optimization with Group Testing

## Quick Facts
- arXiv ID: 2310.03515
- Source URL: https://arxiv.org/abs/2310.03515
- Reference count: 23
- Primary result: GTBO correctly classified 99.95% of inactive variables and all active variables across multiple benchmarks

## Executive Summary
This paper introduces GTBO, a Bayesian optimization method designed for high-dimensional problems with sparse axis-aligned active subspaces. The method combines group testing theory with Bayesian optimization, first identifying which dimensions are truly influential through iterative group testing, then focusing optimization on the discovered active subspace. GTBO demonstrates competitive performance against state-of-the-art high-dimensional BO methods on both synthetic and real-world benchmarks, while providing interpretable insights into problem structure.

## Method Summary
GTBO operates in two phases: group testing and Bayesian optimization. In the group testing phase, GTBO iteratively selects groups of variables to test using mutual information maximization, evaluates the objective function with perturbed configurations, and updates a particle-based posterior distribution over active dimensions using Sequential Monte Carlo sampling. After identifying the active subspace, GTBO performs Bayesian optimization with a Gaussian process surrogate that places strong priors on active and inactive dimensions - short length-scales for active variables and long length-scales for inactive ones. The method leverages the assumption that only a small subset of dimensions significantly impacts the objective function.

## Key Results
- GTBO correctly classified 99.95% of inactive variables and all active variables across multiple benchmarks
- The method achieved superior optimization performance compared to state-of-the-art high-dimensional BO methods
- GTBO demonstrated a significant performance drop immediately after the group testing phase, indicating that knowing the correct active dimensions drastically speeds up optimization
- The approach provided interpretable insights by revealing which parameters are truly influential in the optimization problems

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GTBO identifies active variables by testing groups and using mutual information to select informative groups.
- **Mechanism:** The algorithm iteratively selects groups of variables, evaluates the objective function with perturbed configurations, and uses mutual information to choose groups that maximize information about which dimensions are active. This leverages group testing theory extended to continuous functions.
- **Core assumption:** The objective function's value changes significantly when active dimensions are perturbed but remains nearly constant when only inactive dimensions are changed.
- **Evidence anchors:**
  - [abstract] "introduces a group testing approach to identify active variables by iteratively testing groups of variables to determine their influence on the objective function."
  - [section 3] "GTBO optimizes the MI using a multi-start forward-backward algorithm" and "The GTBO algorithm. With the individual parts defined, we present the complete procedure for GTBO."
  - [corpus] Weak evidence; corpus neighbors do not directly address group testing methodology.
- **Break condition:** If the noise level is too high relative to the signal, the assumptions about Gaussian distributions break down and the mutual information selection becomes unreliable.

### Mechanism 2
- **Claim:** GTBO focuses optimization on the identified active subspace by setting strong priors on GP length scales.
- **Mechanism:** After identifying active dimensions, GTBO uses short length-scale priors for active variables and long length-scale priors for inactive variables in the Gaussian process surrogate model. This constrains the surrogate model to pay more attention to active dimensions.
- **Core assumption:** The active subspace assumption holds, meaning only a subset of dimensions significantly impact the objective.
- **Evidence anchors:**
  - [abstract] "In the second phase, GTBO guides optimization by placing more importance on the active dimensions."
  - [section 3] "we perform BO using the remaining sample budget. To strongly focus on the active subspace, we use short lengthscale priors for the active variables and long lengthscale priors for the inactive variables."
  - [corpus] No direct evidence in corpus; neighbors focus on other BO aspects.
- **Break condition:** If the active subspace assumption is violated and inactive dimensions have non-negligible impact, the strong priors will bias the optimization away from important regions.

### Mechanism 3
- **Claim:** GTBO achieves competitive performance by combining group testing with Bayesian optimization.
- **Mechanism:** The group testing phase identifies active dimensions efficiently, and the subsequent BO phase uses this information to optimize in a reduced-dimensional space. This two-phase approach leverages both explicit structure discovery and sample-efficient optimization.
- **Core assumption:** The function can be well-approximated by considering only the active subspace after identification.
- **Evidence anchors:**
  - [abstract] "demonstrates competitive performance against state-of-the-art high-dimensional BO methods on synthetic and real-world benchmarks"
  - [section 4.3] "Figure 4 shows that GTBO performs well on real-world benchmarks. Note the drop directly after the group testing phase, indicating that knowing the correct active dimensions drastically speeds up optimization."
  - [corpus] Weak evidence; corpus neighbors don't directly compare multi-phase approaches.
- **Break condition:** If the group testing phase fails to identify the correct active subspace, the subsequent BO phase will optimize in the wrong space and performance will degrade.

## Foundational Learning

- **Concept: Gaussian Process (GP) surrogate modeling**
  - Why needed here: GTBO uses a GP with Matérn-5/2 kernel as the surrogate model for Bayesian optimization in the second phase.
  - Quick check question: What kernel does GTBO use for the GP surrogate model in the optimization phase?

- **Concept: Mutual information for experimental design**
  - Why needed here: GTBO selects groups of variables to test by maximizing mutual information between the group test outcomes and the active dimensions.
  - Quick check question: What criterion does GTBO use to select which groups of variables to test next?

- **Concept: Sequential Monte Carlo (SMC) sampling**
  - Why needed here: GTBO uses SMC to maintain a particle-based representation of the posterior distribution over which variables are active, avoiding the exponential cost of exact inference.
  - Quick check question: How does GTBO represent the probability distribution over active dimensions without explicitly enumerating all 2^D possibilities?

## Architecture Onboarding

- **Component map:**
  - Group testing phase: group selection -> function evaluation -> posterior update
  - BO phase: GP surrogate -> acquisition function -> optimization
  - Shared components: SMC sampler, mutual information computation

- **Critical path:**
  1. Initialize SMC particles and weights
  2. Iteratively: select groups → evaluate function → update SMC
  3. Identify active dimensions from SMC marginals
  4. Run BO with strong lengthscale priors

- **Design tradeoffs:**
  - Number of SMC particles vs. accuracy of posterior estimation
  - Group size vs. mutual information gain per evaluation
  - Strength of lengthscale priors vs. flexibility to adapt to inactive dimensions with small effects

- **Failure signatures:**
  - Group testing phase: slow convergence of marginals, high false positive rate
  - BO phase: poor optimization performance despite correct active dimensions identified

- **First 3 experiments:**
  1. Run GTBO on a simple synthetic benchmark (e.g., 10D Branin with 2 active dimensions) with low noise to verify group testing works
  2. Test GTBO with varying numbers of SMC particles to understand the accuracy tradeoff
  3. Compare GTBO performance with and without the strong lengthscale priors to quantify their impact

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but raises several important considerations:
- The potential for increasing sample efficiency by including application-specific beliefs about dimension activeness
- The challenge of extending group testing to non-axis-aligned active subspaces
- The need to explore different prior distributions for dimension activeness

## Limitations

- GTBO's effectiveness relies heavily on the assumption of axis-aligned active subspaces, which may not hold in many real-world scenarios
- The computational complexity of the SMC-based posterior estimation and mutual information maximization algorithm is not fully characterized, particularly for very high-dimensional problems
- The experimental setup and benchmark selection could potentially bias the comparison against state-of-the-art methods

## Confidence

- Mechanism 1 (Group testing identification): Medium confidence - The core approach is well-founded but depends on the validity of assumptions about signal-to-noise ratios.
- Mechanism 2 (Strong lengthscale priors): High confidence - This is a standard technique in BO and the paper provides clear implementation details.
- Mechanism 3 (Overall performance claims): Medium confidence - While results are promising, the comparison framework and benchmarks used could influence the conclusions.

## Next Checks

1. Test GTBO on problems with non-axis-aligned active subspaces to evaluate its robustness to assumption violations.
2. Analyze the computational scaling of GTBO with increasing dimensions to understand practical limitations.
3. Conduct ablation studies to quantify the contribution of each component (group testing phase, strong priors) to the overall performance.