---
ver: rpa2
title: 'Embedding Contextual Information through Reward Shaping in Multi-Agent Learning:
  A Case Study from Google Football'
arxiv_id: '2303.15471'
source_url: https://arxiv.org/abs/2303.15471
tags:
- reward
- learning
- agents
- football
- shaping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel reward shaping method for multi-agent
  reinforcement learning (MARL) in the Google Research Football (GRF) environment.
  The proposed approach addresses the credit assignment problem in MARL by embedding
  contextual information from game state observations into the reward function.
---

# Embedding Contextual Information through Reward Shaping in Multi-Agent Learning: A Case Study from Google Football

## Quick Facts
- **arXiv ID**: 2303.15471
- **Source URL**: https://arxiv.org/abs/2303.15471
- **Reference count**: 24
- **Key outcome**: A novel reward shaping method using Pitch Control Function and Expected Possession Value improves defensive performance in Google Football multi-agent RL.

## Executive Summary
This paper introduces a reward shaping approach for multi-agent reinforcement learning in the Google Research Football environment. The method embeds contextual information from game state observations—specifically spatial control probability and possession value—into the reward function to address the credit assignment problem in sparse-reward settings. By training agents with these shaped rewards, the approach accelerates learning and improves defensive performance compared to standard sparse rewards. Experiments with Value Decomposition Networks demonstrate that agents trained with the proposed method achieve better goal difference against AI opponents.

## Method Summary
The proposed method uses a reward shaping technique that combines the Pitch Control Function (PCF) and Expected Possession Value (EPV) models to provide contextual feedback during multi-agent reinforcement learning in football. PCF computes the probability of each team controlling the ball at each location based on player positions and movements, while EPV assigns value to locations based on their expected contribution to scoring. These components are integrated into the reward function to give agents immediate feedback on the impact of their actions on the game state. The method is implemented with Value Decomposition Networks (VDN) and evaluated in a simplified defensive scenario within the Google Research Football environment.

## Key Results
- Agents trained with PCF+EPV shaped reward outperform those trained with only sparse reward in terms of goal difference against built-in AI opponents.
- The shaped reward method improves learning efficiency in sparse-reward environments.
- Qualitative analysis indicates that agents trained with shaped reward exhibit more effective defensive behaviors.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Pitch Control Function (PCF) quantifies spatial dominance probability across the pitch.
- Mechanism: PCF computes for each location the probability that each team controls the ball given player positions, speeds, and orientations. These probabilities form a scalar field mapping spatial control.
- Core assumption: Player and ball state fully determine immediate control probability.
- Evidence anchors:
  - [section] "To compute the possibility of controlling the ball from each team considering the contextual factors, the author trains a passing probability model."
  - [section] "As a result, in an imaginary location on the pitch, the probability of each team controlling the ball can be computed by this model."
- Break condition: If opponent player trajectories or ball physics deviate from model assumptions, PCF probabilities become inaccurate.

### Mechanism 2
- Claim: Expected Possession Value (EPV) evaluates the long-term scoring potential of a possession state.
- Mechanism: EPV assigns each pitch location a value representing expected goals if the ball is there. The state EPV is computed by summing over all locations the product of PCF control probability and EPV value.
- Core assumption: Possession value at a location is independent of the path taken to reach it (Markov property).
- Evidence anchors:
  - [section] "With the EPV model, the football pitch can be converted to an EPV grid map in which the expected possession value for ball at location [m,n] is EPV_mn."
  - [section] "EPV_ak (the sum of product of Pitch Control probability and EPV value across the whole pitch) denotes the EPV for game state at tk showing how likely the game state will develop into a scoring chance."
- Break condition: If possession value depends on prior game context beyond the current state, EPV loses accuracy.

### Mechanism 3
- Claim: Embedding EPV into reward shapes provides immediate, informative feedback for sparse-reward environments.
- Mechanism: At each step, agents receive a reward component equal to the negative change in state EPV. Lower EPV indicates better defensive impact, guiding agents without waiting for goal events.
- Core assumption: Agents can learn from continuous reward signals that approximate credit assignment.
- Evidence anchors:
  - [section] "At every learning step, each agent can receive immediate feedback about how negative or positive the impact of current game state on the long-term outcome of the episode."
  - [section] "This is implemented into the reward function to let agents learn playing football from domain knowledge."
- Break condition: If agents overfit to shaped reward at the expense of true task performance, learning becomes brittle.

## Foundational Learning

- Concept: Value Decomposition Networks (VDN)
  - Why needed here: VDN decomposes the global team reward into individual agent Q-values, enabling decentralized execution while learning from centralized feedback.
  - Quick check question: How does VDN ensure that summing individual Q-values approximates the joint Q-value?

- Concept: Sparse reward problem
  - Why needed here: In football, goals are rare events, making it hard for agents to learn which actions lead to success without intermediate feedback.
  - Quick check question: What are two common ways to mitigate sparse reward in reinforcement learning?

- Concept: Domain knowledge embedding
  - Why needed here: Football tactics (e.g., controlling space in front of goal) can be quantified and used to shape rewards, accelerating learning.
  - Quick check question: Why might domain knowledge be more effective than generic exploration bonuses in a specific sport?

## Architecture Onboarding

- Component map: Game environment -> State extractor -> PCF computation -> EPV computation -> Reward shaper -> MARL agent (VDN) -> Policy output
- Critical path: State observation -> PCF -> EPV -> shaped reward -> agent update
- Design tradeoffs:
  - PCF vs. raw distance metrics: PCF captures dynamic control likelihood but requires a trained model; raw metrics are simpler but less informative.
  - EPV granularity: Finer grid increases fidelity but raises computational cost.
  - Reward shaping strength: Too strong shaping can overshadow the true task reward; too weak provides insufficient guidance.
- Failure signatures:
  - Agents learn to maximize shaped reward without improving real performance (overfitting).
  - Shaped reward becomes unstable due to noisy PCF/EPV estimates.
  - Computational lag from PCF/EPV slows training iteration speed.
- First 3 experiments:
  1. Train VDN with only sparse reward (baseline).
  2. Train VDN with PCF-based shaped reward (no EPV).
  3. Train VDN with full PCF+EPV shaped reward and compare goal difference.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed reward shaping method compare to imitation learning in environments where expert demonstrations are available?
- Basis in paper: [explicit] The paper states that imitation learning agents cannot outperform the expert demonstrator, while the proposed reward shaping method in MARL allows agents to potentially exceed human-level performance.
- Why unresolved: The paper does not provide a direct comparison between the proposed reward shaping method and imitation learning in terms of performance.
- What evidence would resolve it: Conducting experiments comparing the performance of agents trained with the proposed reward shaping method and those trained with imitation learning in the same environment.

### Open Question 2
- Question: How does the proposed reward shaping method perform in more complex football scenarios, such as involving multiple attacking and defending players, and different game states like set-pieces?
- Basis in paper: [inferred] The paper mentions that the current research is limited to a simplified version of the environment with less agents and only defending scenarios involved.
- Why unresolved: The paper does not provide results or analysis of the proposed reward shaping method in more complex football scenarios.
- What evidence would resolve it: Extending the experiments to include more complex football scenarios and analyzing the performance of the proposed reward shaping method in those scenarios.

### Open Question 3
- Question: How sensitive is the performance of the proposed reward shaping method to the choice of domain knowledge and the way it is implemented in the reward function?
- Basis in paper: [explicit] The paper mentions that designing a good reward shaping function requires rich domain knowledge and that the potential subjectivity of the knowledge and the inappropriate way of implementing it can make reward shaping challenging.
- Why unresolved: The paper does not provide a systematic analysis of the sensitivity of the proposed reward shaping method to different choices of domain knowledge and implementation methods.
- What evidence would resolve it: Conducting experiments with different choices of domain knowledge and implementation methods, and analyzing the impact on the performance of the proposed reward shaping method.

## Limitations

- The paper lacks detailed implementation specifications for the PCF and EPV models, including training procedures and integration methods.
- Evaluation is limited to a specific defensive scenario without testing transfer to other football situations or robustness across different opponent strategies.
- Claims about qualitative improvements in defensive behaviors lack quantitative validation or detailed behavioral analysis.

## Confidence

- **High confidence**: The theoretical framework connecting PCF to spatial control probability and EPV to scoring potential is well-established in sports analytics literature.
- **Medium confidence**: The experimental results showing improved goal difference are internally consistent, but the lack of statistical significance testing and limited evaluation scenarios reduce confidence in generalizability.
- **Low confidence**: Claims about specific qualitative improvements in defensive behaviors lack quantitative validation or detailed behavioral analysis to support these assertions.

## Next Checks

1. **Statistical validation**: Perform hypothesis testing on goal difference improvements across multiple training runs to establish statistical significance of the performance gains.
2. **Generalization testing**: Evaluate trained agents in diverse football scenarios beyond the defensive setup (e.g., open play with balanced teams, offensive scenarios) to assess whether shaped rewards lead to more versatile agents.
3. **Ablation analysis**: Systematically remove PCF or EPV components from the shaped reward to quantify their individual contributions and verify that the combined approach provides synergistic benefits beyond either component alone.