---
ver: rpa2
title: Enhancing Keyphrase Generation by BART Finetuning with Splitting and Shuffling
arxiv_id: '2309.06726'
source_url: https://arxiv.org/abs/2309.06726
tags:
- keyphrases
- keyphrase
- absent
- bart
- present
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method called Keyphrase-Focused BART for
  keyphrase generation. It separates the task into two BART models, one for present
  keyphrases and one for absent keyphrases, and fine-tunes them separately.
---

# Enhancing Keyphrase Generation by BART Finetuning with Splitting and Shuffling

## Quick Facts
- arXiv ID: 2309.06726
- Source URL: https://arxiv.org/abs/2309.06726
- Reference count: 13
- Key outcome: Proposes Keyphrase-Focused BART achieving 9-37% F1@5 improvement over state-of-the-art models

## Executive Summary
This paper addresses the keyphrase generation task by recognizing that present keyphrases (appearing in the document) and absent keyphrases (relevant but not appearing in the document) require different generation approaches. The authors propose a method that separates these two tasks, training two distinct BART models with different hyperparameters for each. Additionally, they introduce a shuffling mechanism to reduce contextual learning in output sequences and a ranking unit combining BERT cross-encoder with TF-IDF to filter and rank candidate keyphrases.

## Method Summary
The proposed method involves training two separate BART models: one for present keyphrases and another for absent keyphrases, each with optimized hyperparameters for their respective tasks. The training data is augmented through shuffling keyphrase lists and adding them back to the dataset to reduce ordering dependencies. A ranking unit using a finetuned BERT cross-encoder combined with TF-IDF scores filters and ranks the generated candidate keyphrases. The approach is evaluated on five benchmark datasets (Inspec, Krapivin, NUS, SemEval, KP20K) and demonstrates significant improvements in F1@5 and F1@M metrics compared to previous state-of-the-art methods.

## Key Results
- Keyphrase-Focused BART achieves new state-of-the-art results on all five benchmark datasets
- F1@5 improvements range from 9% to 37% compared to previous best models
- Separate models for present and absent keyphrases outperform single-model approaches
- Shuffling keyphrase lists once provides optimal performance improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating present and absent keyphrase generation into two BART models with different hyperparameter settings improves performance by optimizing training for the distinct characteristics of each task.
- Mechanism: Present keyphrases are extracted from the document and have a smaller candidate space, while absent keyphrases are generated and have a much larger candidate space. Training separate models allows for tailored hyperparameter settings that better handle these differences.
- Core assumption: The optimal training process for present keyphrase extraction differs significantly from that for absent keyphrase generation due to the imbalanced candidate spaces and distinct semantic relationships.
- Evidence anchors: [abstract] "We argue that there exist considerable differences in the tasks of extracting present keyphrases and generating absent keyphrases, which motivates us to propose splitting the absent and present keyphrase generation tasks into two parts, and train two different generative models, where different hyperparameters are used for finetuning."

### Mechanism 2
- Claim: Shuffling keyphrase lists and expanding the dataset improves performance by reducing contextualities in learning output sequences and increasing the diversity of training samples.
- Mechanism: By shuffling the order of keyphrases in the training data and adding the shuffled sequences to the dataset, the model is less likely to learn contextual relationships between keyphrases and more likely to generate keyphrases in a more order-independent manner. This also increases the size of the training dataset, providing more diverse examples.
- Core assumption: Keyphrase lists should be order-independent, and shuffling reduces the model's tendency to generate keyphrases based on their order in the training data.
- Evidence anchors: [abstract] "To reduce contextualities in learning output sequences, we apply shuffling on the training keyphrase lists, and add the shuffled sequences to the training dataset."

### Mechanism 3
- Claim: Ranking candidate keyphrases using a BERT cross-encoder combined with TF-IDF improves the quality of generated keyphrases by filtering out less relevant candidates.
- Mechanism: The ranking unit treats keyphrase ranking as a binary classification task, using a finetuned BERT cross-encoder to score the relevance of candidate keyphrases. This score is combined with a TF-IDF score to create a final ranking. This helps to filter out incorrect or less relevant keyphrases generated by the BART models.
- Core assumption: A BERT cross-encoder can effectively rank candidate keyphrases based on their relevance to the document, and combining this with TF-IDF provides a more robust ranking.
- Evidence anchors: [abstract] "A keyphrase ranker by a BERT cross-encoder combined with TF-IDF is introduced to improve keyphrases generated by the BART models."

## Foundational Learning

- Concept: Keyphrase Generation Task
  - Why needed here: Understanding the keyphrase generation task is crucial for grasping the motivation behind the proposed method and evaluating its effectiveness.
  - Quick check question: What is the difference between present and absent keyphrases, and why is this distinction important for keyphrase generation?

- Concept: Sequence-to-Sequence Models
  - Why needed here: The paper utilizes BART, a sequence-to-sequence model, for keyphrase generation. Understanding how these models work is essential for comprehending the proposed method and its components.
  - Quick check question: How do sequence-to-sequence models like BART generate text, and what are their advantages for keyphrase generation?

- Concept: Hyperparameter Tuning
  - Why needed here: The paper emphasizes the importance of using different hyperparameter settings for the two BART models trained on present and absent keyphrases. Understanding hyperparameter tuning is crucial for appreciating the proposed method's effectiveness.
  - Quick check question: What are some common hyperparameters in deep learning models, and how can tuning them affect model performance?

## Architecture Onboarding

- Component map: BART_present -> BART_absent -> Ranking_Unit -> Output
- Critical path:
  1. Split the dataset into present and absent keyphrases
  2. Train two separate BART models on the split datasets with different hyperparameters
  3. Shuffle keyphrase lists and add them to the training dataset
  4. Generate candidate keyphrases using the trained BART models
  5. Rank the candidate keyphrases using the BERT cross-encoder and TF-IDF
  6. Evaluate the performance of the keyphrase generation

- Design tradeoffs:
  - Separating the models for present and absent keyphrases allows for tailored hyperparameter tuning but increases the complexity of the system
  - Shuffling keyphrase lists increases dataset size and diversity but may introduce noise if not done carefully
  - Using a ranking unit improves keyphrase quality but adds computational overhead

- Failure signatures:
  - If the present and absent keyphrase generation tasks are not significantly different, separating the models may not provide a substantial benefit
  - If shuffling keyphrase lists does not effectively reduce contextual learning, it may not improve performance
  - If the ranking unit does not accurately rank keyphrases, it may introduce errors or reduce the quality of the generated keyphrases

- First 3 experiments:
  1. Train a single BART model on the combined present and absent keyphrase dataset and evaluate its performance
  2. Train two separate BART models on the split datasets without shuffling or ranking and compare their performance to the single model
  3. Apply shuffling to the keyphrase lists and evaluate its impact on the performance of the two separate BART models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Keyphrase-Focused BART compare to other state-of-the-art models on datasets not included in the paper's evaluation?
- Basis in paper: [explicit] The paper evaluates the model on five widely-used benchmark datasets (Inspec, Krapivin, NUS, SemEval, and KP20K) but does not discuss performance on other datasets.
- Why unresolved: The paper focuses on a specific set of benchmark datasets and does not provide information on how the model would perform on other datasets.
- What evidence would resolve it: Evaluating Keyphrase-Focused BART on additional datasets and comparing its performance to other state-of-the-art models would provide evidence to resolve this question.

### Open Question 2
- Question: How does the proposed model handle keyphrases that are partially present in the text (i.e., keyphrases that contain some words from the text but also include additional words)?
- Basis in paper: [inferred] The paper discusses present and absent keyphrases but does not explicitly address the handling of partially present keyphrases.
- Why unresolved: The paper focuses on the distinction between present and absent keyphrases and does not provide information on how the model handles keyphrases that are partially present in the text.
- What evidence would resolve it: Conducting experiments with datasets that include partially present keyphrases and evaluating the model's performance on such keyphrases would provide evidence to resolve this question.

### Open Question 3
- Question: How does the proposed model perform on long documents compared to short documents?
- Basis in paper: [inferred] The paper does not provide information on how the model performs on documents of different lengths.
- Why unresolved: The paper focuses on keyphrase generation in general and does not discuss the model's performance on documents of varying lengths.
- What evidence would resolve it: Conducting experiments with datasets that include documents of different lengths and evaluating the model's performance on short and long documents would provide evidence to resolve this question.

### Open Question 4
- Question: How does the proposed model handle keyphrases in different languages?
- Basis in paper: [explicit] The paper does not discuss the model's performance on keyphrases in languages other than English.
- Why unresolved: The paper focuses on keyphrase generation in English and does not provide information on how the model would perform on keyphrases in other languages.
- What evidence would resolve it: Evaluating Keyphrase-Focused BART on datasets containing keyphrases in different languages and comparing its performance to other state-of-the-art models would provide evidence to resolve this question.

## Limitations

- The assumption that present and absent keyphrase generation tasks are sufficiently distinct may not hold in all cases, and the performance gains may be partially attributed to increased dataset size from shuffling
- The ranking unit shows little improvement in the analysis despite being included as a component, raising questions about its contribution to overall performance gains
- The paper does not address how the model handles keyphrases that are partially present in the text, which could be a significant limitation in real-world applications

## Confidence

- High Confidence: The experimental results showing improved F1@5 scores (9-37% improvement over state-of-the-art) are well-documented and reproducible given the described methodology.
- Medium Confidence: The shuffling mechanism's contribution to performance improvement is moderately supported by ablation studies, but the analysis does not fully isolate its effect from other components.
- Low Confidence: The ranking unit's marginal contribution to overall performance is concerning, given its computational overhead, and the paper does not adequately explain why this component shows little improvement.

## Next Checks

1. Conduct an ablation study that isolates the effect of dataset size increase from shuffling versus the separation of present and absent keyphrase models by training a single BART model on shuffled data without separation.

2. Perform a detailed error analysis on the ranking unit's predictions to identify specific failure modes and test whether alternative ranking approaches perform comparably.

3. Test the proposed method on additional keyphrase generation datasets not used in the original study to assess whether the performance improvements generalize beyond the five benchmark datasets.