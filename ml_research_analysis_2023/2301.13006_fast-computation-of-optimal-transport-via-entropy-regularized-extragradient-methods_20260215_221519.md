---
ver: rpa2
title: Fast Computation of Optimal Transport via Entropy-Regularized Extragradient
  Methods
arxiv_id: '2301.13006'
source_url: https://arxiv.org/abs/2301.13006
tags:
- adjust
- algorithm
- optimal
- transport
- ript
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the optimal transport problem between two probability\
  \ distributions, which is fundamental in various data science applications. The\
  \ authors develop a scalable first-order optimization method that achieves \u03B5\
  -additive accuracy with runtime \xD5(n\xB2/\u03B5), matching the best-known theoretical\
  \ guarantees among first-order methods."
---

# Fast Computation of Optimal Transport via Entropy-Regularized Extragradient Methods

## Quick Facts
- arXiv ID: 2301.13006
- Source URL: https://arxiv.org/abs/2301.13006
- Reference count: 7
- Primary result: Scalable first-order method achieving ε-additive accuracy with runtime Õ(n²/ε) for optimal transport problems

## Executive Summary
This paper addresses the fundamental problem of computing optimal transport distances between probability distributions, which is crucial in various data science applications. The authors develop a scalable first-order optimization method that achieves state-of-the-art computational guarantees among first-order approaches. Their approach reformulates the optimal transport problem as a bilinear minimax problem over probability distributions and solves it using an entropy-regularized extragradient method with adaptive learning rates. The algorithm demonstrates favorable numerical performance compared to classical methods like Sinkhorn and Greenkhorn while achieving optimal iteration complexity.

## Method Summary
The paper proposes an entropy-regularized extragradient method to solve the optimal transport problem by reformulating it as a bilinear minimax problem. The algorithm performs two mirror-descent-type updates per iteration with learning rates chosen adaptively based on row and column marginals. This adaptive rate selection is crucial for achieving the advertised convergence rate and handling imbalanced distributions. The method combines entropy regularization to ensure strong convexity-concavity with an adjustment step to prevent numerical instability, resulting in a scalable algorithm that achieves Õ(n²/ε) runtime complexity.

## Key Results
- Achieves ε-additive accuracy with runtime Õ(n²/ε), matching the best-known theoretical guarantees among first-order methods
- Demonstrates favorable numerical performance compared to classical algorithms like Sinkhorn and Greenkhorn
- Maintains optimal iteration complexity of Õ(1/ε log²·⁵(n/ε)) through entropy regularization and adaptive learning rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entropy regularization makes the optimal transport problem strongly convex-concave, enabling faster convergence
- Mechanism: Adding entropy terms to the objective function transforms the bilinear minimax problem into one with better curvature properties, allowing linear convergence
- Core assumption: Regularization doesn't significantly bias the original optimal transport objective
- Evidence anchors: Abstract and section discussion of extragradient methods for convex-concave minimax problems; weak corpus support

### Mechanism 2
- Claim: Adaptive learning rates based on marginals improve convergence by handling imbalanced distributions
- Mechanism: Using different learning rates for each row and column, inversely proportional to their marginal sums, prevents bottlenecks from large marginals
- Core assumption: Learning rates can be chosen without prior knowledge of problem structure
- Evidence anchors: Abstract and section discussion of adaptive learning rate selection; missing corpus evidence

### Mechanism 3
- Claim: The extragradient method with entropy regularization achieves O(1/ε) iteration complexity
- Mechanism: Performing two mirror descent steps per iteration provides better stability and faster convergence than standard gradient methods
- Core assumption: Problem can be reformulated as a bilinear minimax problem over probability distributions
- Evidence anchors: Abstract claims of state-of-the-art computational guarantees; section discussion of extragradient methods; missing corpus evidence

## Foundational Learning

- Concept: Optimal Transport and Earth Mover's Distance
  - Why needed here: Algorithm specifically computes optimal transport distance between probability distributions
  - Quick check question: What is the difference between the Kantorovich relaxation and the original optimal transport problem formulation?

- Concept: Bilinear Minimax Problems and Saddle Point Optimization
  - Why needed here: Algorithm reformulates optimal transport as bilinear minimax problem and solves using saddle point techniques
  - Quick check question: How does von Neumann's minimax theorem apply to the reformulation of the optimal transport problem?

- Concept: Entropy Regularization and Strong Convexity
  - Why needed here: Entropy regularization makes the problem strongly convex-concave, crucial for fast extragradient convergence
  - Quick check question: Why does adding an entropy term to a bilinear objective function make it strongly convex-concave?

## Architecture Onboarding

- Component map: Cost matrix W -> Entropy-regularized extragradient method with adaptive learning rates -> Feasible transportation plan P

- Critical path:
  1. Convert W to have infinity norm 1
  2. Initialize probability vectors and auxiliary variables
  3. Main loop: Compute midpoints, update main sequence, adjust iterates
  4. Convert approximate solution to feasible transportation plan
  5. Return result

- Design tradeoffs:
  - Entropy regularization vs. accuracy: Larger regularization leads to faster convergence but may bias the solution
  - Adaptive vs. fixed learning rates: Adaptive rates handle imbalanced marginals better but require more parameter tuning
  - Adjustment step: Prevents numerical instability but adds computational overhead

- Failure signatures:
  - Numerical overflow in exponential calculations (likely due to poor choice of learning rates or regularization parameters)
  - Slow convergence despite many iterations (possibly due to extremely imbalanced marginals)
  - Final solution violating marginal constraints significantly (could indicate implementation error in conversion step)

- First 3 experiments:
  1. Small synthetic problem (2x2 or 3x3) with known optimal solution to verify correctness
  2. Compare convergence rates with Sinkhorn algorithm on a moderate-sized problem (50x50)
  3. Test numerical stability with highly imbalanced marginals (one row/column with much larger mass than others)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the entropy-regularized extragradient method be extended to solve more general linear programs of the form minimize_x∈∆n f(x) + ||Ax - b||_1?
- Basis in paper: The paper discusses the potential to solve more general linear programs using the developed entropy-regularized extragradient method
- Why unresolved: The paper focuses on optimal transport and does not provide detailed analysis of generalization to other linear programming problems
- What evidence would resolve it: Rigorous proof showing adaptation to general linear programs, along with numerical experiments demonstrating effectiveness

### Open Question 2
- Question: How does the performance of the proposed algorithm change when applied to optimal transport problems with low-dimensional structure?
- Basis in paper: The paper mentions that many applications have low-dimensional structure that could potentially be leveraged for further computational savings
- Why unresolved: The paper does not provide experimental results or theoretical analysis of performance on low-dimensional problems
- What evidence would resolve it: Numerical experiments comparing performance on high-dimensional and low-dimensional problems, along with theoretical analysis of convergence rate in low-dimensional setting

### Open Question 3
- Question: Can the theoretical runtime of the proposed algorithm be improved to match the state-of-the-art result of O(n^2 log^2(1/ε))?
- Basis in paper: The paper mentions that the state-of-the-art theoretical result has a runtime of O(n^2 log^2(1/ε)), which is better than the proposed algorithm's runtime of O(n^2 log^2.5 n / ε)
- Why unresolved: The paper does not provide theoretical analysis or algorithmic improvements that could lead to improved runtime
- What evidence would resolve it: New algorithm or theoretical analysis demonstrating runtime of O(n^2 log^2(1/ε)) for solving the optimal transport problem

## Limitations
- Performance depends heavily on choice of regularization parameter and learning rates, requiring problem-specific tuning
- Algorithm assumes cost matrix W has bounded entries, with potential degradation for ill-conditioned matrices
- Practical implementation details for numerical stability in exponential calculations are not fully specified

## Confidence
- Theoretical claims about Õ(n²/ε) runtime and Õ(1/ε) iteration complexity: High (rigorous analysis, though exact constants unspecified)
- Numerical performance claims compared to Sinkhorn and Greenkhorn: Medium (favorable results on small-scale problems up to 100×100)
- Entropy-regularization approach and extragradient method foundations: High (well-established in prior work)

## Next Checks
1. **Scalability Test**: Evaluate algorithm on larger-scale problems (1000×1000 or greater) to verify Õ(n²/ε) runtime scaling holds in practice and compare against GPU-accelerated Sinkhorn implementations

2. **Condition Number Sensitivity**: Systematically test algorithm's performance across problems with varying condition numbers of W to quantify degradation in convergence rates and identify practical thresholds for reliable performance

3. **Regularization Trade-off**: Conduct ablation studies varying the regularization parameter to empirically measure bias-variance trade-off between faster convergence and accuracy of optimal transport solution, comparing against theoretical bounds