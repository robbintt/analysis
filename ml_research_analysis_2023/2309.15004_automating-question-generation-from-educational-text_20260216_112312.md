---
ver: rpa2
title: Automating question generation from educational text
arxiv_id: '2309.15004'
source_url: https://arxiv.org/abs/2309.15004
tags:
- question
- answer
- generated
- module
- teachers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a modular framework for automated generation
  of multiple-choice questions (MCQs) from educational text using transformer-based
  language models. The framework consists of separate modules for question generation,
  correct answer prediction, and distractor formulation.
---

# Automating question generation from educational text

## Quick Facts
- arXiv ID: 2309.15004
- Source URL: https://arxiv.org/abs/2309.15004
- Reference count: 24
- Primary result: Modular transformer-based framework generates high-quality MCQs with 92-93% human-rated quality

## Executive Summary
This paper presents a modular framework for automated generation of multiple-choice questions (MCQs) from educational text using transformer-based language models. The system separates question generation, answer prediction, and distractor formulation into distinct modules, enabling flexible integration of different models and techniques. Evaluation on datasets like SQuAD2.0 demonstrates strong performance with low perplexity scores and high human-rated quality. The modular design allows for customization and optimization of individual components while maintaining overall system robustness.

## Method Summary
The framework uses a three-module approach: (1) question generation using T5 or GPT-3 models fine-tuned on educational datasets, (2) answer prediction using RoBERTa-large to identify correct answer spans, and (3) distractor generation using an ensemble of semantic similarity methods including sense2vec, wordnet, conceptnet, and densephrases. The system takes educational text passages as input and outputs filtered MCQs meeting quality criteria. The modular design enables swapping different language models and generation techniques while maintaining system flexibility and performance.

## Key Results
- Generated questions achieve mean perplexity of 37.3 and query well-formedness score of 0.864
- Answer prediction obtains exact match score of 0.64 and F1-score of 0.84
- 58% of generated MCQs have acceptable distractor sets by human judgment
- Human evaluation found 92-93% of generated MCQs to be of high quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular design improves both accuracy and flexibility
- Mechanism: Separating question generation, answer prediction, and distractor generation into distinct modules allows independent optimization and swapping of better-performing models without redesigning the entire system
- Core assumption: Different language models excel at different subtasks, and modular approach leverages these strengths while mitigating individual model weaknesses
- Evidence anchors:
  - [abstract] "The modular approach enables flexible integration of different language models and generation techniques, resulting in a robust and customizable MCQ generation system"
  - [section 3] "The presented solution, with distinct modules for question generation, correct answer prediction, and distractor formulation, enables us to evaluate different language models and generation techniques"
  - [corpus] Weak evidence - only one corpus paper mentions modularity briefly
- Break condition: If module interdependencies become too tight or if end-to-end optimization becomes necessary for performance gains

### Mechanism 2
- Claim: Transformer-based models provide high-quality generated questions
- Mechanism: Large language models like T5 and GPT-3, trained on extensive datasets, can generate grammatically correct, well-formed questions with low perplexity scores when fine-tuned on educational data
- Core assumption: Pretrained transformer models capture sufficient linguistic patterns to generate natural-sounding questions when provided with appropriate context
- Evidence anchors:
  - [abstract] "Evaluation on datasets like SQuAD2.0 shows the framework achieves a mean perplexity of 37.3 and query well-formedness score of 0.864 for generated questions"
  - [section 4.3] "We obtain a mean value of perplexity equal to 37.3 and a mean value of query well-formedness score equal to 0.864 over all the generated questions"
  - [corpus] Moderate evidence - multiple corpus papers report transformer-based approaches achieving good question generation results
- Break condition: If input context is too short or ambiguous, or if domain-specific knowledge is required beyond what transformers can infer

### Mechanism 3
- Claim: Ensemble methods for distractor generation improve quality
- Mechanism: Combining multiple approaches (sense2vec, wordnet, conceptnet, densephrases, human-curated datasets) creates distractors that are semantically similar to correct answers while remaining plausible alternatives
- Core assumption: Multiple semantic similarity measures capture different aspects of distractor quality, and combining them produces better distractors than any single approach
- Evidence anchors:
  - [section 4.2] "we use an ensemble approach which is a combination of methods based on sense2vec, wordnet, conceptnet, densephrases as well as human curated MCQ datasets"
  - [section 4.3] "58% of the MCQs in the data from learning platform have acceptable set of generated distractors"
  - [corpus] Weak evidence - only one corpus paper mentions ensemble approaches for distractor generation
- Break condition: If computational cost of ensemble methods outweighs quality improvements, or if simpler methods achieve comparable results

## Foundational Learning

- Concept: Transformer architecture fundamentals
  - Why needed here: Understanding how transformers work is essential for implementing and debugging the question generation and answer prediction modules
  - Quick check question: What is the key difference between encoder-only (like BERT) and encoder-decoder (like T5) transformer architectures?

- Concept: Evaluation metrics for text generation
  - Why needed here: To properly assess the quality of generated questions and answers using perplexity, exact match, F1-score, and ROUGE metrics
  - Quick check question: How does perplexity differ from exact match in evaluating generated text quality?

- Concept: Ensemble methods and their tradeoffs
  - Why needed here: Understanding how to combine multiple approaches for distractor generation and when ensemble methods are beneficial
  - Quick check question: What are the main advantages and disadvantages of using ensemble methods versus single-model approaches?

## Architecture Onboarding

- Component map: Question generation (Module 1) → Answer prediction (Module 2) → Distractor generation (Module 3) → Filtering (post-processing)
- Critical path: Content → Module 1 → Module 2 → Module 3 → Filter → Output
- Design tradeoffs:
  - Accuracy vs. latency: Larger models (GPT-4) produce better results but are slower and more expensive than smaller models (T5)
  - Privacy vs. performance: Local models provide better data privacy but may sacrifice some quality compared to cloud-based APIs
  - Modularity vs. end-to-end optimization: Modular design allows flexibility but may miss optimizations available in joint training
- Failure signatures:
  - High perplexity scores indicate poor question quality
  - Low exact match scores suggest answer prediction issues
  - Filter module rejects too many generated MCQs, indicating systemic quality problems
- First 3 experiments:
  1. Test each module independently using a small dataset to verify baseline functionality
  2. Run end-to-end generation with a single model configuration to establish performance benchmarks
  3. Compare T5 vs. GPT-3 performance for question generation on the same dataset to understand model tradeoffs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the modular MCQ generation framework compare to end-to-end question generation systems like GPT-4 in terms of MCQ quality metrics?
- Basis in paper: [explicit] The paper mentions that GPT-4 could generate MCQs of superior quality compared to smaller models like T5 and GPT-3 used in their modular approach.
- Why unresolved: The paper does not provide a direct comparison of their modular framework's performance against GPT-4 on the same metrics and datasets.
- What evidence would resolve it: Quantitative comparison of MCQ quality metrics (perplexity, exact match, F1-score, ROUGE, human evaluation scores) between the modular framework and GPT-4 on identical datasets and evaluation protocols.

### Open Question 2
- Question: What is the optimal balance between model size/complexity and MCQ quality for different educational domains and difficulty levels?
- Basis in paper: [inferred] The paper evaluates different model configurations (T5-base, T5-large, GPT-3) and mentions trade-offs, but doesn't systematically explore the relationship between model complexity and domain-specific performance.
- Why unresolved: The paper only tests a limited set of model configurations and doesn't analyze how model choice affects performance across different educational subjects or difficulty levels.
- What evidence would resolve it: Systematic evaluation of multiple model sizes and architectures across various educational domains (science, ELA, social studies) and difficulty levels, measuring the trade-off between computational cost and MCQ quality.

### Open Question 3
- Question: How does the time required for teachers to refine and customize generated MCQs compare to the time spent creating them from scratch?
- Basis in paper: [explicit] The paper mentions this as a future direction: "In future, we aim to evaluate the time it takes for a teacher to use QGen to generate questions for content provided by them and the effort needed to refine these questions."
- Why unresolved: This evaluation was planned but not conducted in the current study.
- What evidence would resolve it: Controlled user study measuring the time and effort teachers spend using the QGen system versus creating MCQs manually, including both initial generation time and refinement time.

## Limitations

- The study relies on relatively small-scale human evaluations without detailed inter-rater reliability metrics
- Framework was tested primarily on SQuAD2.0 and limited Wikipedia passages, not representing full diversity of educational content
- Reported distractor quality (58% acceptable) indicates room for improvement in this critical component
- Modular approach may miss synergies available through end-to-end training optimization

## Confidence

- High confidence in the modular framework design and its flexibility benefits
- Medium confidence in the reported evaluation metrics due to limited human evaluation scale
- Medium confidence in the generalizability across different educational domains

## Next Checks

1. **Scale validation**: Re-run the full evaluation pipeline on a larger, more diverse educational dataset (minimum 1000 passages across multiple subjects) to verify the robustness of reported metrics.

2. **Inter-rater reliability**: Conduct a detailed analysis of human evaluation agreement, including Krippendorff's alpha or similar reliability metrics for all quality assessments (question quality, answer correctness, distractor quality).

3. **Ablation study**: Systematically test the contribution of each module by comparing end-to-end performance with individual module removals or substitutions to quantify the true benefit of modularity.