---
ver: rpa2
title: Multimodal Neurons in Pretrained Text-Only Transformers
arxiv_id: '2308.01544'
source_url: https://arxiv.org/abs/2308.01544
tags:
- image
- neurons
- multimodal
- coco
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how pretrained text-only transformers can
  generalize to multimodal tasks by identifying and analyzing "multimodal neurons"
  that convert visual representations into corresponding text. The researchers use
  a frozen text transformer augmented with a self-supervised visual encoder and a
  learned projection layer, then analyze individual neurons in the transformer's MLP
  layers.
---

# Multimodal Neurons in Pretrained Text-Only Transformers

## Quick Facts
- arXiv ID: 2308.01544
- Source URL: https://arxiv.org/abs/2308.01544
- Authors: 
- Reference count: 40
- Key outcome: Identifies multimodal neurons in frozen text transformers that convert visual representations into text, with ablations causing 80% drop in caption quality

## Executive Summary
This paper investigates how pretrained text-only transformers can perform multimodal tasks by identifying "multimodal neurons" in their MLP layers that translate visual representations into corresponding text. The researchers augment a frozen GPT-J transformer with a self-supervised visual encoder and learned projection layer, then analyze individual neurons' contributions to image captioning. They discover that most multimodal neurons cluster in layers 5-10 and operate on specific visual concepts, with their ablation causing significant degradation in caption quality.

## Method Summary
The study uses a frozen GPT-J 6B text transformer augmented with BEIT image encoder and a learned linear projection layer. Image embeddings are projected into the transformer's embedding space and processed through its MLP layers. Attribution scores are calculated for each neuron using gradients to evaluate their contribution to image captioning. Neurons are identified as "multimodal" based on interpretability scores and their effect on generating relevant text tokens. Ablation experiments systematically remove top-scoring neurons to measure their causal impact on caption quality.

## Key Results
- Multimodal neurons are found primarily in layers 5-10 of GPT-J, not in earlier or later layers
- Ablating top multimodal neurons decreases correct next token probability by 80% on average
- BERTScore comparing original and generated captions decreases significantly after neuron ablation
- Neuron activation patterns align with specific image categories (e.g., airplanes, animals)
- Decoded image prompts from the projection layer show poor agreement with input images, indicating translation occurs within transformer MLP layers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal neurons in transformer MLPs translate visual representations into corresponding text tokens.
- Mechanism: Individual MLP neurons contribute language tokens related to specific visual features when activated. These neurons operate on specific visual concepts across inputs and have systematic causal effects on image captioning.
- Core assumption: The projection layer does not immediately translate image embeddings into interpretable language; translation occurs within the transformer's MLP layers.
- Evidence anchors:
  - [abstract] "We introduce a procedure for identifying 'multimodal neurons' that convert visual representations into corresponding text, and decoding the concepts they inject into the model's residual stream."
  - [section 2.1] "We apply a procedure based on gradients to evaluate the contribution of neuron uk to an image captioning task... This score is maximized when both the neuron's output and the effect of the neuron are large."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.471, average citations=0.0.

### Mechanism 2
- Claim: The projection layer produces image prompts that fit within the GPT-J embedding space but do not already map image features onto related language.
- Mechanism: Image embeddings from BEIT are projected into the transformer embedding space via a linear layer trained on image-to-text tasks. However, when decoded, these projected embeddings do not correspond to interpretable language tokens, indicating translation happens inside the transformer rather than at the projection layer.
- Core assumption: Alignment of visual and language representations occurs within the transformer's MLP layers, not at the input projection layer.
- Evidence anchors:
  - [section 3.1] "We decode image prompts aligned to the GPT-J embedding space into language, and measure their agreement with the input image and its human annotations... A Kolmogorov-Smirnov test shows no significant difference between CLIPScore distributions comparing real decoded prompts and random embeddings to images."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.471, average citations=0.0.

### Mechanism 3
- Claim: Ablating multimodal neurons significantly degrades the quality of image captions by reducing the probability of generating the correct next token and changing the semantics of the generated captions.
- Mechanism: Successively ablating units sorted by attribution scores shows that the probability of generating the correct next token decreases by 80% on average, and BERTScores comparing original and generated captions also decrease significantly.
- Core assumption: Multimodal neurons have a causal effect on the model's output, and their removal leads to degradation in caption quality.
- Evidence anchors:
  - [section 3.3] "When up to 6400 random units are ablated, we find that the probability of token c is largely unaffected, but ablating the same number of top-scoring units decreases token probability by 80% on average. Ablating multimodal neurons also leads to significant changes in the semantics of GPT-generated captions."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.471, average citations=0.0.

## Foundational Learning

- Concept: Multimodal neurons and their role in translating visual representations into text.
  - Why needed here: Understanding multimodal neurons is crucial for grasping how pretrained text-only transformers can generalize to multimodal tasks by converting visual representations into corresponding text.
  - Quick check question: What is the primary function of multimodal neurons in the context of this study, and how do they contribute to the model's ability to generate image captions?

- Concept: The difference between the projection layer and the transformer's MLP layers in terms of their role in modality translation.
  - Why needed here: Distinguishing between the roles of the projection layer and the transformer's MLP layers is essential for understanding where the translation between modalities occurs in the model architecture.
  - Quick check question: According to the study, where does the translation between visual and language representations occur, and what evidence supports this claim?

- Concept: The concept of attribution scores and their use in identifying and ranking multimodal neurons.
  - Why needed here: Attribution scores are used to evaluate the contribution of individual neurons to the image captioning task, which is crucial for identifying and analyzing multimodal neurons.
  - Quick check question: How are attribution scores calculated in this study, and what do they measure in terms of a neuron's contribution to the model's output?

## Architecture Onboarding

- Component map: BEIT image encoder -> Linear projection layer P -> GPT-J transformer MLP layers -> Output decoder

- Critical path:
  1. Image is resized and encoded into a sequence of patches by BEIT
  2. Image embeddings are projected into the transformer embedding space via the projection layer
  3. Projected embeddings are processed by the transformer's MLP layers, where multimodal neurons convert visual representations into corresponding text
  4. Output of the final layer is decoded using the output embeddings to generate an image caption

- Design tradeoffs:
  - Using a frozen text transformer with a learned projection layer allows for efficient adaptation to multimodal tasks without retraining the entire model
  - However, this approach relies on the assumption that the translation between modalities occurs within the transformer's MLP layers, which may not always be the case

- Failure signatures:
  - If the projection layer were able to map image features onto related language before reaching the transformer, the decoded image prompts would show significant agreement with the input images and their human annotations
  - If multimodal neurons did not have a causal effect on the model's output, ablating them would not lead to a significant decrease in the probability of generating the correct next token or a degradation in the quality of image captions

- First 3 experiments:
  1. Decode image prompts aligned to the GPT-J embedding space into language and measure their agreement with the input image and its human annotations using CLIPScore and BERTScore
  2. Identify multimodal neurons by calculating attribution scores for each neuron and selecting those with the highest scores for specific image features
  3. Ablate multimodal neurons and measure the resulting change in the probability of generating the correct next token and the quality of the generated captions using BERTScore

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic features or patterns do multimodal neurons detect in visual representations to generate text?
- Basis in paper: [explicit] The paper states that multimodal neurons "convert visual representations into corresponding text" and that their activation patterns "align with specific image categories," but does not detail the specific linguistic features they detect.
- Why unresolved: The paper focuses on the existence and effects of multimodal neurons but does not analyze the specific linguistic features or patterns these neurons detect in visual representations.
- What evidence would resolve it: A detailed analysis of the linguistic features or patterns detected by multimodal neurons, potentially through techniques like feature visualization or activation maximization, could provide insights into how these neurons translate visual information into text.

### Open Question 2
- Question: How do the learned projection layer weights contribute to the alignment of visual and language representations in the transformer?
- Basis in paper: [explicit] The paper mentions a "learned projection layer" that casts image embeddings into the transformer's embedding space, but does not analyze how the learned weights contribute to the alignment of visual and language representations.
- Why unresolved: The paper focuses on the role of multimodal neurons within the transformer but does not investigate the contribution of the learned projection layer weights to the alignment process.
- What evidence would resolve it: Analyzing the learned projection layer weights and their relationship to the alignment of visual and language representations could provide insights into how the model bridges the gap between modalities.

### Open Question 3
- Question: How do multimodal neurons contribute to the model's ability to generalize to new visual concepts or categories not seen during training?
- Basis in paper: [inferred] The paper shows that multimodal neurons are selective for specific image categories and have a systematic causal effect on image captioning, suggesting they play a role in the model's ability to generalize to new concepts.
- Why unresolved: The paper does not investigate how multimodal neurons contribute to the model's ability to generalize to new visual concepts or categories not seen during training.
- What evidence would resolve it: Evaluating the model's performance on new visual concepts or categories not seen during training, and analyzing the activation patterns of multimodal neurons in these cases, could provide insights into their role in generalization.

## Limitations

- The exact mechanisms by which individual neurons transform visual features into language remain unclear
- The analysis relies on a frozen transformer architecture, limiting generalizability to other transformer variants or fine-tuning scenarios
- The attribution method measures correlation rather than definitive causation, though ablation studies provide supporting evidence

## Confidence

- **High confidence**: The existence of multimodal neurons in MLP layers and their systematic effect on image captioning quality when ablated
- **Medium confidence**: The claim that modality translation occurs within MLP layers rather than at the projection layer
- **Medium confidence**: The identification and characterization of specific multimodal neurons for particular visual concepts

## Next Checks

1. Replicate ablation effects across multiple runs: Perform ablation experiments on 5 different random subsets of images from the COCO validation set to verify that the 80% decrease in next token probability and BERTScore degradation are consistent and not dataset-specific artifacts.

2. Test alternative attribution methods: Compare the gradient-based attribution scores with alternative methods like Integrated Gradients or Shapley values to ensure the identified multimodal neurons are robust to different evaluation metrics.

3. Analyze neuron activation patterns on out-of-distribution images: Test the same multimodal neurons on images from datasets not used in training (e.g., Flickr30k or OpenImages) to verify that neurons maintain their concept-specific activation patterns across diverse visual domains.