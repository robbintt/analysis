---
ver: rpa2
title: A scoping review on multimodal deep learning in biomedical images and texts
arxiv_id: '2307.07362'
source_url: https://arxiv.org/abs/2307.07362
tags:
- medical
- radiology
- images
- learning
- reports
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This scoping review synthesizes recent research on multimodal
  deep learning (MDL) in biomedical images and texts, focusing on five key tasks:
  report generation, visual question answering, cross-modal retrieval, computer-aided
  diagnosis, and semantic segmentation. The review identifies major challenges such
  as data imbalance, limited clinical knowledge integration, and lack of human evaluation
  and interpretability.'
---

# A scoping review on multimodal deep learning in biomedical images and texts

## Quick Facts
- arXiv ID: 2307.07362
- Source URL: https://arxiv.org/abs/2307.07362
- Reference count: 40
- 77 studies reviewed on multimodal deep learning in biomedical images and texts

## Executive Summary
This scoping review systematically examines recent research on multimodal deep learning (MDL) in biomedical images and texts, identifying five key application areas: report generation, visual question answering, cross-modal retrieval, computer-aided diagnosis, and semantic segmentation. The review reveals that while MDL shows promise for improving diagnostic accuracy by integrating complementary information from images and text, significant challenges remain including data imbalance, limited clinical knowledge integration, and lack of human evaluation and interpretability. Only 35 out of 77 studies addressed model interpretability, and few involved external clinical validation, highlighting the need for standardized evaluation protocols and interdisciplinary collaboration to advance real-world clinical applications.

## Method Summary
The review followed PRISMA guidelines, screening 77 peer-reviewed articles from PubMed, ACM Digital Library, IEEE Xplore, Google Scholar, and Semantic Scholar published between 2018-2022. Articles were selected using keywords related to medical images, text reports, and multimodal learning, with inclusion criteria focusing on English-language studies of multimodal medical image-text applications and exclusion of non-medical or unimodal studies. Data was extracted on datasets, fusion techniques, modalities, evaluation metrics, external validation, and interpretability across five task categories.

## Key Results
- MDL improves diagnostic accuracy by fusing complementary information from medical images and text reports
- Only 35 out of 77 studies addressed model interpretability, with limited clinical validation
- Major challenges include data imbalance, limited clinical knowledge integration, and lack of standardized evaluation protocols

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal deep learning (MDL) improves diagnostic accuracy by fusing complementary information from medical images and text reports.
- Mechanism: The integration of heterogeneous data sources allows the model to capture diverse perspectives, enhancing the richness of features used for clinical decision-making.
- Core assumption: Image and text modalities contain complementary rather than redundant information about the same clinical entity.
- Evidence anchors:
  - [abstract] "Multimodal deep learning (MDL), which involves the integration of multiple sources of data, such as images and text, has the potential to revolutionize the analysis and interpretation of biomedical data."
  - [section] "Incorporating text modality in this context has been shown to provide supplementary features that can enhance performance in image classification."
- Break Condition: If image and text modalities are highly correlated or if one modality is consistently noisier than the other, the fusion may not provide meaningful gains and could even degrade performance.

### Mechanism 2
- Claim: Pre-trained language models (e.g., BERT, GPT) adapted to medical domains improve performance on report generation and VQA tasks.
- Mechanism: Transfer learning from large-scale general-domain pre-training provides rich linguistic representations that can be fine-tuned on specialized medical text.
- Core assumption: General linguistic patterns learned during pre-training are transferable to the medical domain with minimal task-specific adaptation.
- Evidence anchors:
  - [abstract] "the emergence of related research has only recently surfaced... pre-trained models, such as Bidirectional Encoder Representations from Transformers (BERT) [5] and Generative Pre-trained Transformer 3 (GPT-3) [6]"
  - [section] "In the recent two years, the Transformer architecture has seen increasing use in report generation."
- Break Condition: If medical text contains highly specialized terminology or domain-specific structures not present in general pre-training data, fine-tuning may require extensive domain-specific corpora.

### Mechanism 3
- Claim: Contrastive learning aligns visual and textual embeddings to improve cross-modal retrieval and semantic segmentation tasks.
- Mechanism: By maximizing similarity between matched image-text pairs and minimizing similarity between mismatched pairs, the model learns meaningful cross-modal representations.
- Core assumption: The semantic relationship between images and their corresponding text can be effectively captured through contrastive objectives.
- Evidence anchors:
  - [section] "Most cross-modal retrieval tasks rely on matching image and text features through contrastive learning."
  - [section] "Image-text alignment and local representation learning are commonly used in MDL for semantic segmentation."
- Break Condition: If the correspondence between images and text is noisy or ambiguous, contrastive learning may learn incorrect associations, leading to degraded performance.

## Foundational Learning

- Concept: Multimodal fusion strategies (early, late, and hybrid)
  - Why needed here: Different fusion approaches have distinct trade-offs in terms of information preservation and computational efficiency, which are critical for MDL in medical applications.
  - Quick check question: What are the advantages and disadvantages of early vs. late fusion in multimodal medical learning?

- Concept: Clinical knowledge integration methods
  - Why needed here: Incorporating domain expertise can guide feature learning and improve model interpretability, which is essential for clinical adoption.
  - Quick check question: How can structured clinical knowledge (e.g., ontologies) be integrated into deep learning models for medical image-text tasks?

- Concept: Evaluation metrics for multimodal medical AI
  - Why needed here: Standard metrics may not adequately capture the clinical relevance or interpretability of MDL models, necessitating domain-specific evaluation approaches.
  - Quick check question: What metrics beyond traditional accuracy (e.g., AUC, BLEU) are important for assessing clinical utility of MDL models?

## Architecture Onboarding

- Component map: Data preprocessing -> Feature extraction -> Multimodal fusion -> Task-specific prediction -> Interpretability assessment
- Critical path: Data preprocessing → Feature extraction → Multimodal fusion → Task-specific prediction → Interpretability assessment
- Design tradeoffs:
  - Model complexity vs. interpretability: Deeper models may achieve better performance but reduce transparency
  - Pre-training vs. fine-tuning: Using general pre-trained models vs. training from scratch on medical data
  - Fusion timing: Early fusion (before task-specific layers) vs. late fusion (after task-specific layers)
- Failure signatures:
  - Performance degradation when adding modalities suggests poor alignment or redundancy
  - High variance across folds indicates overfitting to specific data characteristics
  - Poor human evaluation scores despite good automated metrics suggests lack of clinical relevance
- First 3 experiments:
  1. Implement unimodal baselines (image-only and text-only) to establish performance floors
  2. Test different fusion strategies (early, late, attention-based) on a small dataset
  3. Evaluate interpretability methods (attention visualization, saliency maps) on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal methods for integrating clinical knowledge into multimodal deep learning models to improve performance and generalizability across different medical domains?
- Basis in paper: [explicit] The paper identifies limited integration of clinical knowledge into MDL models as a major research gap and discusses how clinical knowledge could enhance encoding, reasoning, and learning processes.
- Why unresolved: Current approaches to integrating clinical knowledge are limited and fragmented, with few standardized methods for incorporating knowledge graphs, ontologies, or expert knowledge into MDL architectures.
- What evidence would resolve it: Comparative studies evaluating different clinical knowledge integration approaches (knowledge graphs, ontologies, expert systems) across multiple medical domains, showing which methods yield the greatest performance improvements and generalizability.

### Open Question 2
- Question: What standardized evaluation protocols can be developed for human assessment of multimodal deep learning models in clinical settings?
- Basis in paper: [explicit] The paper notes that human evaluation was rarely employed in the reviewed studies, with only 2 out of 12 cross-modal retrieval studies including external validation, and highlights the need for standardized protocols.
- Why unresolved: Current lack of standardized human evaluation protocols makes it difficult to compare model performance across studies and limits clinical translation of MDL systems.
- What evidence would resolve it: Development and validation of standardized human evaluation frameworks for MDL models, including specific metrics, evaluation procedures, and validation studies demonstrating improved model assessment and clinical utility.

### Open Question 3
- Question: How can multimodal deep learning models be made more interpretable and transparent for clinical decision-making?
- Basis in paper: [explicit] The paper identifies interpretability as a major challenge, noting that only 35 out of 77 studies addressed model interpretability, and emphasizes the need for standardized methods to evaluate and quantify interpretability.
- Why unresolved: Current interpretability methods for MDL models are fragmented and lack standardization, making it difficult to assess model reliability and build clinical trust in these systems.
- What evidence would resolve it: Development of standardized interpretability metrics and visualization techniques specifically designed for MDL models, along with validation studies demonstrating their effectiveness in improving clinical understanding and trust.

## Limitations
- Review only included English-language articles from five specific databases, potentially missing relevant non-English publications
- 77 studies represent a rapidly evolving field, and findings may become outdated as new methods emerge
- Limited assessment of clinical impact due to predominance of retrospective studies and lack of real-world deployment data

## Confidence
- High Confidence: The identification of key challenges (data imbalance, limited interpretability, lack of clinical validation) is well-supported by the reviewed literature
- Medium Confidence: The effectiveness of pre-trained models for medical text tasks assumes successful domain adaptation
- Medium Confidence: The promise of contrastive learning for cross-modal alignment is supported but lacks detailed analysis of success/failure conditions

## Next Checks
1. Conduct a citation analysis of the 77 included studies to identify which methodological approaches have been most replicated or extended by subsequent research
2. Replicate the screening process on a subset of articles with multiple reviewers to quantify inter-rater agreement
3. Perform a deeper technical analysis of the 35 studies that addressed interpretability to identify most promising methods for medical applications