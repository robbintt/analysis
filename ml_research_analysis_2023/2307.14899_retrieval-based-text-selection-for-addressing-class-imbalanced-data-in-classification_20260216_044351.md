---
ver: rpa2
title: Retrieval-based Text Selection for Addressing Class-Imbalanced Data in Classification
arxiv_id: '2307.14899'
source_url: https://arxiv.org/abs/2307.14899
tags:
- data
- search
- semantic
- learning
- category
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of selecting texts for annotation
  in text classification when there are limits on human resources and severe class
  imbalance in binary categories. The proposed method leverages SHAP to construct
  queries for Elasticsearch and semantic search to identify optimal sets of texts
  for annotation.
---

# Retrieval-based Text Selection for Addressing Class-Imbalanced Data in Classification

## Quick Facts
- **arXiv ID**: 2307.14899
- **Source URL**: https://arxiv.org/abs/2307.14899
- **Reference count**: 38
- **Key outcome**: Method improves F1 scores for minority classes in binary classification by using SHAP-derived queries with Elasticsearch and semantic search

## Executive Summary
This paper addresses the challenge of selecting texts for annotation in text classification when facing severe class imbalance and limited human resources. The proposed method leverages SHAP to construct targeted queries for both Elasticsearch and semantic search, enabling more effective retrieval of minority class examples from large unlabeled datasets. By iteratively improving classifiers through guided annotation, the approach achieves higher F1 scores for minority classes while maintaining efficient use of annotation resources.

## Method Summary
The method uses SHAP to identify important keywords from initial classifiers, which are then used to construct queries for Elasticsearch (BM25) and semantic search (Sentence-BERT). Retrieved examples from both methods are combined through majority voting, annotated, and added to the training dataset. This process is repeated iteratively, with each cycle improving classifier performance and generating better queries for subsequent retrieval rounds.

## Key Results
- Sequential annotation batches with retrieval-guided selection progressively improve classifier performance
- Combining Elasticsearch and semantic search via majority voting increases retrieval reliability
- SHAP-derived keywords effectively target minority class examples for improved annotation efficiency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Using SHAP-derived keywords to construct queries improves retrieval precision for minority class examples.
- **Mechanism**: SHAP values identify input features (words) that most influence the classifier's positive predictions. These high-impact words are used as targeted queries in both Elasticsearch and semantic search, increasing the likelihood of retrieving truly positive minority class examples.
- **Core assumption**: SHAP-identified keywords are semantically meaningful and capture the essence of the positive class, making them effective retrieval queries.

### Mechanism 2
- **Claim**: Combining multiple retrieval methods (Elasticsearch and semantic search) via majority voting improves result reliability.
- **Mechanism**: Each retrieval method has different strengths - Elasticsearch excels at keyword matching while semantic search captures contextual meaning. Taking the intersection ensures only examples relevant by both measures are selected, reducing false positives.
- **Core assumption**: The positive examples for a given category will be ranked highly by both keyword-based and semantic similarity measures.

### Mechanism 3
- **Claim**: Sequential annotation batches with retrieval-guided selection progressively improves classifier performance.
- **Mechanism**: Each batch of labeled data improves the classifier, which in turn generates better SHAP-derived queries for the next retrieval round. This iterative refinement leads to progressively more balanced training data and better F1 scores.
- **Core assumption**: Classifier improvements are sufficient to generate meaningfully better queries in each iteration.

## Foundational Learning

- **Concept**: Class imbalance in binary classification
  - **Why needed here**: The entire paper addresses the challenge of training classifiers when positive examples are severely underrepresented
  - **Quick check question**: What metric should be reported when dealing with imbalanced datasets instead of accuracy?

- **Concept**: Information retrieval models (Elasticsearch, semantic search)
  - **Why needed here**: The method relies on effectively retrieving relevant examples from large unlabeled pools using different search paradigms
  - **Quick check question**: What is the key difference between BM25 ranking and semantic similarity in document retrieval?

- **Concept**: SHAP (SHapley Additive exPlanations) for model interpretability
  - **Why needed here**: SHAP values are used to identify which words are most important for positive class predictions, forming the basis for query construction
  - **Quick check question**: What does a high SHAP value for a word indicate about its relationship to the model's prediction?

## Architecture Onboarding

- **Component map**: Annotation pipeline → Initial classifier training → SHAP analysis → Query construction → Dual retrieval (Elasticsearch + semantic search) → Majority voting → New annotation batch → Retraining
- **Critical path**: SHAP analysis → Query construction → Retrieval → Majority voting → Annotation
- **Design tradeoffs**: Precision vs recall in retrieval (higher precision reduces false positives but may miss some true positives), single vs multiple retrieval methods (complexity vs robustness)
- **Failure signatures**: Flat F1 scores across iterations, retrieval precision below 50%, SHAP values concentrated on stop words or irrelevant terms
- **First 3 experiments**:
  1. Run SHAP analysis on initial classifier and examine top keywords - are they semantically meaningful for the positive class?
  2. Test retrieval precision of top 20 results from each method separately before combining
  3. Measure F1 score improvement after one complete iteration of retrieval → annotation → retraining

## Open Questions the Paper Calls Out

- **Open Question 1**: How can we improve the efficiency of the semantic search process for large datasets without compromising accuracy?
- **Open Question 2**: How can we develop an intelligent sampling method to select optimal cues considering other constraints within the dataset?
- **Open Question 3**: How can we fine-tune the neural retrieval models to gain better performance for the semantic search?

## Limitations

- The method's effectiveness depends heavily on the quality of initial classifiers trained on imbalanced data
- Evaluation is limited to a single domain (EFT texts for obesity and diabetes management)
- Manual verification steps in SHAP-based query construction introduce potential subjectivity

## Confidence

- **High confidence**: Sequential annotation pipeline and evaluation metrics (macro F1)
- **Medium confidence**: Integration of SHAP with retrieval methods
- **Low confidence**: Claims about optimal keyword selection and majority voting effectiveness

## Next Checks

1. **Cross-domain validation**: Test the method on a different text classification task with severe class imbalance to assess generalizability beyond EFT texts.
2. **SHAP keyword analysis**: Conduct a systematic analysis of whether SHAP-identified keywords are truly representative of the positive class versus being artifacts of the training data distribution.
3. **Recall measurement**: Implement and report recall metrics for the retrieval process to ensure the method is not missing significant portions of the positive class while focusing on precision improvements.