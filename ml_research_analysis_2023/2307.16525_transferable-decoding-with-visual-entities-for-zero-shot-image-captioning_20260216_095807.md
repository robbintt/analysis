---
ver: rpa2
title: Transferable Decoding with Visual Entities for Zero-Shot Image Captioning
arxiv_id: '2307.16525'
source_url: https://arxiv.org/abs/2307.16525
tags:
- viecap
- image
- captioning
- entities
- capdec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of object hallucination in zero-shot
  image captioning, where models tend to generate captions containing objects that
  do not actually exist in the image but frequently appear during training. The proposed
  ViECap method leverages entity-aware hard prompts to guide large language models'
  attention toward visual entities present in the image, enabling coherent caption
  generation across diverse scenes.
---

# Transferable Decoding with Visual Entities for Zero-Shot Image Captioning

## Quick Facts
- arXiv ID: 2307.16525
- Source URL: https://arxiv.org/abs/2307.16525
- Reference count: 40
- Authors: Not specified in input
- Key outcome: Addresses object hallucination in zero-shot image captioning using entity-aware hard prompts with LLMs

## Executive Summary
This paper tackles the challenge of object hallucination in zero-shot image captioning, where models generate captions containing objects not present in the image. The proposed ViECap method leverages entity-aware hard prompts combined with soft prompts derived from CLIP embeddings to guide large language models toward visual entities in the image. Extensive experiments demonstrate that ViECap achieves state-of-the-art performance on cross-domain captioning tasks while remaining competitive in-domain.

## Method Summary
ViECap uses a projector to transform CLIP text embeddings into soft prompts, while nouns extracted from captions serve as hard prompts. During training, entity masking (40% ratio) prevents the model from learning a trivial copy-and-paste shortcut. At inference, the model retrieves top-K entities from CLIP similarity scores and concatenates them with soft prompts for caption generation using GPT-2. The approach is trained on text-only data and leverages CLIP's strong zero-shot retrieval capability to maintain cross-domain performance.

## Key Results
- Sets new state-of-the-art in cross-domain captioning
- Achieves 39.2 and 36.3 improvements in CIDEr score compared to DeCap and CapDec on NoCaps validation set
- Outperforms previous methods by a large margin while remaining competitive in-domain
- Entity masking rate of 40% effectively prevents copy-paste behavior

## Why This Works (Mechanism)

### Mechanism 1
Entity-aware hard prompts compensate for projector overfitting on ID data by leveraging CLIP's strong zero-shot retrieval capability. The projector trained on limited text data may overfit to ID visual features, degrading soft prompt quality for OOD images. Hard prompts, constructed via CLIP-based entity retrieval, bypass this by directly mapping visual embeddings to entity tokens using a frozen CLIP model. Core assumption: CLIP's visual embedding space maintains semantic consistency across domains. Break condition: If CLIP's visual embedding space degrades significantly across domains.

### Mechanism 2
Entity masking prevents the model from learning a trivial copy-and-paste shortcut during training. Without masking, the model can simply copy all input entities to the output, making captioning trivial. Random masking forces the model to recover missing entities from soft prompts, encouraging true caption generation. Core assumption: The soft prompt contains sufficient contextual information to reconstruct masked entities when combined with language model knowledge. Break condition: If masking rate is too high, the model cannot recover sufficient entity information.

### Mechanism 3
Concatenating soft and hard prompts allows the model to leverage both domain-specific context and entity grounding simultaneously. Soft prompts provide contextual guidance learned from training corpus, while hard prompts supply entity grounding from visual input. The language model can dynamically balance these two information sources. Core assumption: The language model can effectively integrate and weigh information from both prompt types during autoregressive generation. Break condition: If the language model cannot effectively integrate the two prompt types.

## Foundational Learning

- **Vision-Language Model embeddings and transferability**: Why needed - ViECap relies on CLIP's visual embeddings maintaining semantic consistency across domains for entity retrieval. Quick check - How does CLIP's visual embedding space differ from traditional CNNs in terms of cross-domain transferability?
- **Entity recognition and noun extraction from text**: Why needed - Hard prompts are constructed from nouns extracted from training captions, requiring understanding of entity boundaries and relevance. Quick check - What criteria determine whether a word should be treated as an entity in the hard prompt construction process?
- **Language model autoregressive generation and prompt conditioning**: Why needed - The GPT-2 model generates captions conditioned on concatenated soft and hard prompts, requiring understanding of how different prompt types influence generation. Quick check - How does the order of soft and hard prompts affect the language model's generation behavior?

## Architecture Onboarding

- **Component map**: CLIP Image Encoder → Projector → Soft Prompt + Entity Classifier → Hard Prompt → Concatenation → GPT-2 → Caption
- **Critical path**: Image → CLIP Encoder → Projector → Soft Prompt + Entity Classifier → Hard Prompt → Concatenation → GPT-2 → Caption
- **Design tradeoffs**: Frozen vs. trainable CLIP components (freezing ensures zero-shot transferability but limits adaptation); Entity masking rate (higher rates prevent shortcuts but may reduce entity grounding); Vocabulary size (larger vocabularies improve retrieval but increase computational cost)
- **Failure signatures**: Performance drops on OOD images (likely projector overfitting or vocabulary mismatch); Copy-paste behavior (entity masking rate too low); Inconsistent entity recognition (CLIP embedding space degradation or vocabulary quality issues)
- **First 3 experiments**: Test entity retrieval accuracy across different domains using CLIP; Evaluate performance sensitivity to entity masking rate (0%, 20%, 40%, 60%); Compare single-prompt vs. dual-prompt generation performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided input.

## Limitations
- Architecture generalization only tested on GPT-2 variants, not other language models
- Cross-domain generalization not tested on extremely divergent domains (medical imaging, satellite imagery)
- Prompt integration dynamics not investigated - relative importance of soft vs. hard prompts unknown

## Confidence
**High Confidence**: Entity masking prevents copy-paste shortcuts (well-supported by ablation studies)
**Medium Confidence**: CLIP's zero-shot retrieval compensates for projector overfitting (supported by results but relies on untested embedding space consistency assumption)
**Low Confidence**: Approach easily applies to other VLMs and LLMs (only GPT-2 tested, no evidence for other architectures)

## Next Checks
1. Validate CLIP's visual embedding space consistency across extremely divergent domains (medical imaging, satellite imagery, artistic domains)
2. Test ViECap with LLaMA-2, OPT, or other language models to verify claimed ease of integration
3. Conduct experiments varying soft and hard prompt weighting/ordering to determine optimal integration strategies