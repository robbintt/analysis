---
ver: rpa2
title: Unsupervised discovery of Interpretable Visual Concepts
arxiv_id: '2309.00018'
source_url: https://arxiv.org/abs/2309.00018
tags:
- image
- concept
- concepts
- images
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a new framework for explainable deep learning\
  \ that provides more interpretable visualizations by decomposing the network\u2019\
  s decision into semantic concepts. The method, called Maximum Activation Groups\
  \ Extraction (MAGE), groups similar feature-map activation patterns into \u201C\
  concepts\u201D and uses a multiscale visualization method, Multiscale Interpretable\
  \ Visualization (Ms-IV), to highlight the most important image regions for each\
  \ concept."
---

# Unsupervised discovery of Interpretable Visual Concepts

## Quick Facts
- arXiv ID: 2309.00018
- Source URL: https://arxiv.org/abs/2309.00018
- Authors: 
- Reference count: 40
- Key outcome: Introduces MAGE and Ms-IV methods for unsupervised concept discovery and interpretable visualization in CNNs, outperforming LIME and Integrated Gradients on localization and faithfulness metrics.

## Executive Summary
This paper presents a novel framework for explainable deep learning that decomposes CNN decisions into interpretable semantic concepts. The Maximum Activation Groups Extraction (MAGE) method groups similar feature-map activation patterns into "concepts," while Multiscale Interpretable Visualization (Ms-IV) uses a ranking-based metric (CAOC) to highlight the most important image regions for each concept. The approach enables both knowledge discovery about model behavior and bias detection through human evaluation, demonstrating superior localization and faithfulness compared to existing xAI methods.

## Method Summary
The method combines MAGE for unsupervised concept discovery with Ms-IV for interpretable visualization. MAGE identifies the image patch that maximally activates each feature map dimension across the dataset, creating a high-dimensional representation that is clustered to form semantic concepts. Ms-IV then applies hierarchical occlusion and computes the Class-aware Order Correlation (CAOC) metric to identify the most causally important image regions for each concept. The visualization accumulates importance scores across scales and multiplies them by the original image to produce interpretable heatmaps.

## Key Results
- Ms-IV achieves higher localization and faithfulness values compared to LIME and Integrated Gradients on cats vs. dogs dataset
- Human evaluation shows participants can accurately identify the correct model and detect bias using the visualizations
- MAGE successfully extracts semantically meaningful concept clusters from CNN feature maps
- The method demonstrates robustness to adversarial perturbations while maintaining interpretability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MAGE decomposes high-dimensional feature maps into interpretable semantic concepts by grouping similar activation patterns across images
- Core assumption: Convolutional networks learn spatially coherent features where similar patterns activate in similar image regions
- Evidence anchors: Abstract states MAGE finds feature combinations forming semantic meaning; section 4.5 describes the patch-based dimensionality reduction approach
- Break condition: Fails when spatial correlation assumption breaks down in highly abstract or non-spatial representations

### Mechanism 2
- Claim: Ms-IV provides interpretable visualizations by highlighting causally important image regions using CAOC ranking metric
- Core assumption: Causal importance can be inferred by observing how occlusion affects the model's decision space ranking
- Evidence anchors: Abstract mentions CAOC uses class-aware order correlation; section 5.3 details the ranking-based importance calculation
- Break condition: Ineffective when model's decision space is insensitive to local perturbations

### Mechanism 3
- Claim: Combined MAGE and Ms-IV enables unsupervised concept discovery and bias detection through human-aligned visualizations
- Core assumption: Human interpretation aligns with model's learned concepts and differences reveal bias
- Evidence anchors: Abstract and section 6.4 describe human evaluation results showing bias detection capability
- Break condition: Fails when human interpretation doesn't align with model concepts due to cultural or perceptual differences

## Foundational Learning

- **Concept: Clustering and dimensionality reduction**
  - Why needed here: MAGE represents each feature map dimension by patch positions; clustering requires understanding how to group similar patterns
  - Quick check: If two feature map dimensions activate in similar patches across dataset, will their representations be close in clustering space? Answer: Yes, because representations are based on patch positions.

- **Concept: Occlusion-based attribution methods**
  - Why needed here: Ms-IV uses occlusion to measure causal importance; understanding occlusion methods is key to grasping CAOC
  - Quick check: If occluding a patch changes model's output ranking significantly, what does that imply? Answer: The patch is important for the concept as its absence disrupts decision space.

- **Concept: Ranking correlation metrics**
  - Why needed here: CAOC uses ranking correlation (Kendall-tau) to measure occlusion effects on image ordering
  - Quick check: If ranking of images by class score changes after occlusion, what does high Kendall-tau correlation indicate? Answer: High correlation means occlusion had little effect on relative ordering, so patch is less important.

## Architecture Onboarding

- **Component map**: MAGE: Feature map activations -> patch position extraction -> high-dimensional representation -> clustering -> concept groups. Ms-IV: Concept groups -> multiscale occlusion -> CAOC scoring -> accumulated importance -> visualization.

- **Critical path**: 1) Extract feature maps from last convolutional layer, 2) For each dimension find max-activating patch positions across dataset, 3) Cluster representations to form concepts, 4) Apply multiscale occlusion and compute CAOC for each concept, 5) Accumulate importances and multiply by original image for visualization.

- **Design tradeoffs**: Patch size in MAGE affects clustering quality vs interpretability; threshold δ in Ms-IV affects visualization sparsity vs completeness; clustering algorithm choice impacts concept boundary detection.

- **Failure signatures**: MAGE: Poor Silhouette scores, sparse scatter plots, incoherent concepts. Ms-IV: Uninformative visualizations (all black/white), no CAOC change with occlusion, high background highlights.

- **First 3 experiments**: 1) Run MAGE with n=4 patches and t=5 samples; visualize scatter plots and concept centers, 2) Apply Ms-IV with δ=0.25 to known concept cluster; verify visualization matches human intuition, 3) Compare CAOC vs probability difference on small image set; check for discontinuities indicating sparsity awareness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does choice of patch size affect interpretability and quality of extracted concepts?
- Basis: Paper discusses patch size impact but doesn't provide definitive optimal choice
- Why unresolved: No systematic comparison of different patch sizes with human evaluation
- What evidence would resolve: Experiments comparing interpretability and cluster quality across patch sizes with human evaluation

### Open Question 2
- Question: How does MAGE compare to other concept extraction methods?
- Basis: Paper introduces MAGE without direct comparison to existing methods
- Why unresolved: Focus on introducing MAGE rather than comparative evaluation
- What evidence would resolve: Comparative study of MAGE with other concept extraction methods using interpretability and faithfulness metrics

### Open Question 3
- Question: Can MAGE and Ms-IV be applied to other neural network types like RNNs or transformers?
- Basis: Paper demonstrates application to CNNs but doesn't explore other architectures
- Why unresolved: No investigation of applicability to non-convolutional networks
- What evidence would resolve: Experiments applying methods to RNNs and transformers with effectiveness evaluation

## Limitations
- Spatial correlation assumptions may fail for abstract features or non-convolutional architectures
- Human interpretability evaluations limited to 24 participants, raising statistical robustness concerns
- Method effectiveness depends on model decision space sensitivity to local perturbations

## Confidence
- **High confidence**: Quantitative localization and faithfulness results showing Ms-IV outperforming baselines
- **Medium confidence**: Claims about unsupervised concept discovery quality and bias detection capabilities
- **Low confidence**: Generalization to non-image domains and complex real-world datasets

## Next Checks
1. Conduct ablation study on patch size systematically varying sizes and measuring clustering quality and interpretability
2. Apply MAGE+Ms-IV to transformers and other non-convolutional architectures to test spatial correlation assumptions
3. Re-run human evaluations with larger sample sizes (n=50+) and formal significance testing to validate interpretability claims