---
ver: rpa2
title: Making Large Language Models Perform Better in Knowledge Graph Completion
arxiv_id: '2310.06671'
source_url: https://arxiv.org/abs/2310.06671
tags:
- knowledge
- structural
- information
- kopa
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles knowledge graph completion using large language
  models by integrating structural information from the knowledge graph. The authors
  propose Knowledge Prefix Adapter (KoPA), a two-stage method that first pre-trains
  structural embeddings for entities and relations, then uses a knowledge prefix adapter
  to project these embeddings into the LLM's textual space.
---

# Making Large Language Models Perform Better in Knowledge Graph Completion

## Quick Facts
- arXiv ID: 2310.06671
- Source URL: https://arxiv.org/abs/2310.06671
- Reference count: 40
- Key outcome: KoPA achieves 92.58% accuracy and 92.70% F1 on UMLS KG dataset by projecting structural embeddings into LLM textual space as virtual tokens

## Executive Summary
This paper addresses knowledge graph completion (KGC) using large language models (LLMs) by introducing Knowledge Prefix Adapter (KoPA), a two-stage method that integrates structural information from knowledge graphs. KoPA first pre-trains structural embeddings for entities and relations using techniques like RotatE, then employs a knowledge prefix adapter to project these embeddings into the LLM's textual space. These projected embeddings are added as virtual knowledge tokens to the input prompt, allowing the LLM to reason more structurally. Experimental results show that KoPA significantly outperforms existing methods on triple classification tasks, achieving state-of-the-art accuracy and F1 scores on the UMLS dataset while also demonstrating strong transferability to unseen entities and efficient prompt usage.

## Method Summary
KoPA is a two-stage method for KGC that first pre-trains structural embeddings using KG embedding techniques like RotatE, then uses a knowledge prefix adapter to project these embeddings into the LLM's textual space. The projected embeddings become virtual knowledge tokens that are prepended to the input prompt. During fine-tuning with LoRA, the LLM learns to incorporate this structural information for triple classification. The method maintains fixed-length prompts while capturing rich structural information, and demonstrates inductive reasoning capabilities on unseen entities.

## Key Results
- KoPA achieves 92.58% accuracy and 92.70% F1 on the UMLS dataset
- Outperforms existing methods on triple classification tasks
- Demonstrates strong transferability to unseen entities with less performance degradation as inductive rate increases
- Maintains efficient prompt length (3 tokens per head/relation/tail) compared to methods using textual neighborhood descriptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KoPA's knowledge prefix adapter enables LLMs to integrate structural graph information as virtual knowledge tokens, which enhances structural-aware reasoning.
- Mechanism: KoPA pre-trains structural embeddings of entities and relations in the KG, then uses a linear projection layer (the prefix adapter) to map these embeddings into the LLM's textual token space. The projected embeddings are prepended to the input prompt, allowing the unidirectional attention of the decoder-only LLM to "see" them throughout the reasoning process.
- Core assumption: The structural embeddings learned via self-supervised pre-training (e.g., with RotatE) capture meaningful subgraph and relational patterns that can be linearly projected into textual embeddings and still preserve semantic utility for triple classification.
- Evidence anchors:
  - [abstract]: "KoPA communicates such cross-modal structural information understanding to the LLMs through a knowledge prefix adapter which projects the structural embeddings into the textual space and obtains virtual knowledge tokens positioned as a prefix of the input prompt."
  - [section]: "KoPA employs structural embedding pre-training to capture the structural information of entities and relations in the KG. Then KoPA informs the LLMs of the knowledge prefix adapter which projects the structural embeddings into the textual space and obtains virtual knowledge tokens as a prefix of the input prompt."
  - [corpus]: No direct corpus evidence for the effectiveness of this exact projection strategy; claim is based on internal experiments.
- Break condition: If the structural embeddings do not encode discriminative relational patterns, or if the linear projection cannot preserve semantic relationships, the virtual tokens will not provide useful guidance and performance will degrade.

### Mechanism 2
- Claim: Adding KG structural information as text in the input prompt is less effective and scalable than KoPA's virtual token approach.
- Mechanism: Structural-aware IT methods append neighborhood triples in textual form to the prompt, which increases prompt length linearly with the number of demonstrations and introduces redundant or irrelevant context. KoPA instead keeps prompt length fixed (3 tokens per head/relation/tail) while still conveying structural information.
- Core assumption: Longer prompts degrade LLM performance due to attention span limits and higher computational cost, and that textual descriptions of neighbors are not as semantically rich or targeted as pre-trained structural embeddings.
- Evidence anchors:
  - [abstract]: "KoPA makes the length of the prompt more refined as the length of virtual tokens generated by the structural prefix adapter is fixed to 3 for head/relation / tail respectively. In contrast, the prompt length of structural-aware IT ... is linearly related to the number of neighborhood triples."
  - [section]: "However, representing the KG structural information in the form of text is not a good choice, which may bring in more invalid or redundant information to the prompt."
  - [corpus]: No direct corpus evidence; claim supported by internal ablation study.
- Break condition: If the LLM can effectively attend over long contexts without degradation, or if textual neighborhood descriptions are highly discriminative, the advantage of KoPA's fixed-length tokens may be minimal.

### Mechanism 3
- Claim: KoPA's structural embeddings are transferable to unseen entities during inference.
- Mechanism: KoPA learns a mapping from structural embeddings to textual token representations via the prefix adapter. Since this mapping is learned during training and the adapter is frozen during inference, it can generalize to structural embeddings of entities not seen during training, enabling inductive reasoning.
- Core assumption: The structural embedding space and the adapter's learned projection are sufficiently generalizable to handle embeddings of entities outside the training distribution.
- Evidence anchors:
  - [section]: "We split the KG dataset into an inductive setting with a defined inductive rate (IR), which refers to the ratio of unseen entities during training. ... We fine-tune the LLM with only remaining seen triples and test on both seen and unseen triples. ... From the radio charts, we can observe that KoPA outperforms the other methods for unseen triples and has less performance degradation when IR increases."
  - [corpus]: No direct corpus evidence for this specific inductive transfer claim; claim is based on internal experimental results.
- Break condition: If the structural embedding space is highly specific to training entities or if the adapter overfits to seen entities, it will fail to generalize to unseen entities.

## Foundational Learning

- Concept: Knowledge graph embedding pre-training (e.g., TransE, RotatE)
  - Why needed here: KoPA relies on pre-trained structural embeddings to encode subgraph structure and relational patterns before mapping them to the LLM's space.
  - Quick check question: What is the role of negative sampling in the self-supervised loss used for training KG embeddings like RotatE?

- Concept: Prefix-based token injection in decoder-only LLMs
  - Why needed here: KoPA prepends virtual knowledge tokens to the input prompt so that all subsequent tokens can attend to them via unidirectional attention.
  - Quick check question: Why does placing virtual tokens at the prefix (rather than infix or suffix) matter for attention flow in decoder-only architectures?

- Concept: LoRA fine-tuning for LLMs
  - Why needed here: KoPA uses LoRA to efficiently fine-tune the LLM with the virtual knowledge tokens while keeping the base LLM frozen.
  - Quick check question: What is the benefit of using LoRA (Low-Rank Adaptation) instead of full fine-tuning in the context of KoPA?

## Architecture Onboarding

- Component map:
  KG dataset -> Structural embedding pre-training (RotatE-style loss) -> Pre-trained structural embeddings -> Knowledge prefix adapter (linear projection) -> Virtual knowledge tokens (prefix) -> LLM + LoRA -> Instruction tuning with prompt = [virtual tokens] + [instruction] + [triple] -> Output: Triple classification (true/false)

- Critical path:
  1. Pre-train structural embeddings on the KG using negative sampling loss.
  2. Train the prefix adapter to map structural embeddings to LLM token space.
  3. Fine-tune LLM with LoRA using the enhanced prompt containing virtual tokens.
  4. Perform inference with the fine-tuned LLM on test triples.

- Design tradeoffs:
  - Using a simple linear projection vs. a more complex adapter: simpler is faster and less data-hungry but may not capture complex cross-modal mappings.
  - Fixed 3-token virtual token length vs. dynamic length: fixed is predictable and efficient but may lose granularity.
  - Freezing structural embeddings during fine-tuning vs. updating them: freezing ensures stability but may limit adaptation.

- Failure signatures:
  - If performance is similar to or worse than vanilla IT, the prefix adapter may not be learning a useful mapping.
  - If performance drops significantly on unseen entities, the adapter may not generalize.
  - If adding virtual tokens hurts performance, the projection may be corrupting the embeddings or the LLM may not benefit from the extra context.

- First 3 experiments:
  1. Ablation: Replace KoPA's pre-trained embeddings with random embeddings and measure drop in accuracy.
  2. Ablation: Move virtual tokens from prefix to infix and suffix positions and compare performance.
  3. Inductive test: Split KG into seen/unseen entities, train KoPA on seen, test on unseen, and measure accuracy/F1 drop.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can KoPA be adapted to handle entity prediction and relation prediction tasks in KGC, not just triple classification?
- Basis in paper: [explicit] The authors explicitly state this as a limitation: "For the time being, we have not generalized the model method to all kinds of KGC tasks such as entity prediction and relation prediction."
- Why unresolved: The current KoPA framework is specifically designed for triple classification tasks. Adapting it to other KGC tasks would require modifications to the input/output structure and potentially the knowledge prefix adapter mechanism.
- What evidence would resolve it: Successful implementation and evaluation of KoPA on entity prediction and relation prediction benchmarks, showing comparable or improved performance over existing methods.

### Open Question 2
- Question: How does KoPA perform on KGs with significantly different characteristics, such as those with more complex relation types (e.g., n-ary relations) or different entity distributions?
- Basis in paper: [inferred] The authors only test KoPA on three relatively standard benchmark datasets (UMLS, CoDeX-S, FB15K-237N). They do not explore performance on KGs with more complex structures or distributions.
- Why unresolved: The generalizability of KoPA to diverse KG characteristics is unknown. Complex relations or skewed entity distributions might require modifications to the structural embedding pre-training or adapter mechanisms.
- What evidence would resolve it: Comprehensive experiments on a wider variety of KG datasets, including those with n-ary relations, hierarchical structures, and different entity/relation distributions, demonstrating consistent performance gains.

### Open Question 3
- Question: Can the knowledge prefix adapter in KoPA be made more dynamic, allowing it to adjust the virtual knowledge tokens based on the specific context of the input triple?
- Basis in paper: [inferred] The current KoPA uses a fixed projection layer to convert structural embeddings to virtual knowledge tokens. This suggests a static approach that doesn't consider the specific context of the input triple.
- Why unresolved: A dynamic adapter could potentially provide more relevant structural information for each specific triple, leading to improved performance. However, designing such an adapter and ensuring its effectiveness would require further research.
- What evidence would resolve it: Development and evaluation of a dynamic knowledge prefix adapter that outperforms the static version on KGC tasks, demonstrating the benefits of context-aware structural information injection.

## Limitations

- The effectiveness of the linear projection in preserving structural semantics is not fully validated and may be a bottleneck
- Performance gains on larger-scale KGs with millions of entities remain untested
- Generalizability to LLM architectures beyond the tested Alpaca-7B is not established

## Confidence

- **High confidence**: KoPA's performance improvement over baseline IT methods on UMLS (92.58% accuracy, 92.70% F1) is well-supported by reported results and ablation studies.
- **Medium confidence**: The claim that KoPA's fixed-length virtual tokens are more effective than textual neighborhood descriptions is supported by internal experiments but lacks external corpus validation.
- **Medium confidence**: The inductive transfer capability of KoPA to unseen entities is demonstrated on the FB15K-237N dataset with controlled inductive rates, but the mechanism's robustness to more extreme splits or noisier KGs is uncertain.

## Next Checks

1. **Ablation of Projection Complexity**: Replace KoPA's linear projection with a small MLP adapter and measure changes in triple classification accuracy to assess whether the linear assumption is a bottleneck.

2. **Cross-Architecture Transferability**: Apply KoPA to a different LLM family (e.g., LLaMA or Mistral) on the same KG datasets and compare performance to verify that the gains are not architecture-specific.

3. **Scalability and Generalization Test**: Evaluate KoPA on a larger KG (e.g., YAGO or Wikidata) with both seen and unseen entities to test the limits of the structural embedding space and the adapter's inductive capacity.