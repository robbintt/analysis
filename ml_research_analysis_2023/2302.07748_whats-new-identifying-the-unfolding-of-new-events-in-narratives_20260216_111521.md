---
ver: rpa2
title: Whats New? Identifying the Unfolding of New Events in Narratives
arxiv_id: '2302.07748'
source_url: https://arxiv.org/abs/2302.07748
tags:
- event
- events
- narrative
- task
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new task of identifying new events in narratives,
  where an event is defined as a triplet of subject, predicate, and object. The task
  focuses on events that provide novel information not previously mentioned or inferable
  through commonsense reasoning.
---

# Whats New? Identifying the Unfolding of New Events in Narratives

## Quick Facts
- arXiv ID: 2302.07748
- Source URL: https://arxiv.org/abs/2302.07748
- Reference count: 10
- One-line primary result: Introducing and benchmarking a task for identifying novel events in narratives using subject-predicate-object triplets.

## Executive Summary
This paper introduces a new task for identifying new events in narratives, where an event is defined as a triplet of subject, predicate, and object. The task focuses on events that provide novel information not previously mentioned or inferable through commonsense reasoning. The authors annotate a corpus of emotional narratives at sentence level, achieving an Inter-Annotator Agreement (IAA) of 0.54 for candidate selection and 0.66 for continuous span selection. They develop baseline models using rule-based approaches and neural networks (BERT, RoBERTa) for both candidate selection and sequence tagging settings. The neural models, particularly RoBERTa, outperform rule-based baselines, with F1 scores reaching 54.3% for candidate selection and 67.1% for sequence tagging when trained on both selected candidates and continuous span annotations.

## Method Summary
The authors formalize the task of identifying new events in narratives, where events are extracted as subject-predicate-object triplets using an unsupervised pipeline. They annotate the SEND dataset of emotional narratives, asking annotators to select new events from generated candidates or add free-form spans. The task is approached as both candidate selection (binary classification) and sequence tagging (token-level classification). Baseline models include rule-based approaches (Random, Binary, First/Last Candidate, New Subject/Entity Selectors) and neural models (BERT, RoBERTa) fine-tuned for classification and tagging. Evaluation uses F1, precision, recall, and agreement metrics at both candidate and token levels.

## Key Results
- RoBERTa model achieves 54.3% F1 for new event candidate selection, outperforming rule-based baselines.
- RoBERTa model achieves 67.1% F1 for sequence tagging when trained on both selected candidates and continuous span annotations.
- Inter-annotator agreement is relatively low (0.54 for candidates, 0.66 for spans), indicating task difficulty.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Defining events as subject-predicate-object triplets enables consistent extraction across sentences.
- Mechanism: By standardizing events as triplets, the extraction pipeline can reliably identify and structure events for annotation, ensuring each event has a clear semantic form.
- Core assumption: Verb-centric triplets adequately capture event semantics in narrative sentences.
- Evidence anchors:
  - [abstract] Defines event as "triplet of subject, predicate, and object."
  - [section 3] States "We follow the definition of an event that was used by Chambers and Jurafsky (2008) based on the verb and its dependencies."
- Break condition: If narrative sentences contain complex or implicit events not easily mapped to S-P-O form, extraction may fail.

### Mechanism 2
- Claim: Using Information Status (IS) as a criterion distinguishes new events from previously mentioned or inferable ones.
- Mechanism: Events are marked as new only if they are discourse-new and not inferable via commonsense, ensuring selection of truly novel information.
- Core assumption: Annotators and models can reliably judge whether an event is inferable via commonsense.
- Evidence anchors:
  - [abstract] States new events are those "not previously mentioned or inferable through commonsense reasoning."
  - [section 3] Defines new events as "not mentioned in the narrative context and can not be inferred through commonsense by the reader."
- Break condition: If commonsense inference is ambiguous or culturally dependent, judgments may become inconsistent.

### Mechanism 3
- Claim: Annotating at the sentence level with candidate selection and free-form spans improves recall for new event identification.
- Mechanism: Providing both extracted candidates and the ability to add continuous spans allows annotators to capture events missed by automated extraction.
- Core assumption: Human annotators can accurately select and add new events given structured guidance.
- Evidence anchors:
  - [section 4] Describes the annotation process including candidate selection and adding continuous spans.
  - [section 4.2] Reports that 2254 new events were added as continuous spans versus 1536 selected from candidates.
- Break condition: If annotator agreement is low or the extraction pipeline misses too many events, recall improvements may not materialize.

## Foundational Learning

- Concept: Information Status (IS) in discourse
  - Why needed here: Distinguishing new from old events is central to the task; understanding IS helps reason about event novelty.
  - Quick check question: Can you explain the difference between discourse-new and hearer-new information in your own words?

- Concept: Event coreference resolution
  - Why needed here: Identifying new events implicitly requires knowing which events refer to the same happening; this is necessary for filtering out repeated events.
  - Quick check question: How would you determine if two events in different sentences refer to the same real-world occurrence?

- Concept: Commonsense inference in NLP
  - Why needed here: Events inferable through commonsense are excluded as new; understanding how models handle such inference is critical for task definition.
  - Quick check question: Give an example of a sentence where an event is inferable from context but not explicitly stated.

## Architecture Onboarding

- Component map:
  - Corpus loader (SEND dataset) -> Event extraction pipeline (unsupervised S-P-O extraction) -> Annotation interface (UI for candidate selection and span addition) -> Baseline models (rule-based and neural for candidate selection and sequence tagging) -> Evaluation framework (precision, recall, F1, agreement metrics)

- Critical path:
  1. Load narratives and extract event candidates.
  2. Present candidates and context to annotators.
  3. Collect annotations (selected candidates + added spans).
  4. Train and evaluate baseline models on annotations.
  5. Measure model performance and agreement with ground truth.

- Design tradeoffs:
  - Sentence-level annotation simplifies task but may miss cross-sentence event dependencies.
  - Candidate selection reduces annotator workload but risks missing events; free-form spans mitigate this.
  - Rule-based baselines are interpretable but underperform neural models on precision/recall.

- Failure signatures:
  - Low IAA (<0.5) suggests ambiguous guidelines or difficult judgments.
  - Low recall in models indicates missed new events, possibly due to weak extraction or inference.
  - High precision but low recall suggests models are conservative, only tagging obvious events.

- First 3 experiments:
  1. Evaluate rule-based "New Entity Selector" on a held-out validation set to measure precision-recall tradeoff.
  2. Train RoBERTa model on both candidate selection and sequence tagging tasks; compare performance to BERT.
  3. Test model agreement with ground truth using Positive Agreement Fleiss metric; analyze span-level vs token-level errors.

## Open Questions the Paper Calls Out
- Question: How would the new event detection task perform on narratives from different domains or languages, particularly non-English narratives?
  - Basis in paper: [explicit] The authors note that "The dataset used in this work is in English. Thus, the annotation methodology may need to be refined for other languages."
  - Why unresolved: The current study is limited to English emotional narratives, and the methodology hasn't been tested on other languages or domains.
  - What evidence would resolve it: Testing the annotation protocol and baseline models on narratives from different languages and domains would provide empirical evidence of generalizability.

- Question: Can the task of new event detection be extended to handle more complex narrative structures, such as overlapping events or nested event structures?
  - Basis in paper: [explicit] The authors mention that "we formalize this task as a binary tagging task rather than IOB tagging task and leave the development of the models for IOB tagging of multiple spans with overlap as future work."
  - Why unresolved: The current sequence tagging approach uses a simplified binary tagging scheme and doesn't handle overlapping or nested event structures.
  - What evidence would resolve it: Developing and evaluating models that can handle IOB tagging or other schemes for overlapping and nested events would demonstrate feasibility.

- Question: How does the performance of new event detection models change when incorporating additional contextual information, such as speaker identity, temporal relationships, or emotional states?
  - Basis in paper: [inferred] The current models use PLMs with context from previous sentences but don't explicitly incorporate speaker information, temporal reasoning, or emotional context.
  - Why unresolved: The baseline models don't explore the impact of these additional contextual features on new event detection performance.
  - What evidence would resolve it: Experiments comparing model performance with and without these additional contextual features would quantify their impact on the task.

## Limitations
- The task relies heavily on subjective judgments about commonsense inference and event novelty, leading to relatively low inter-annotator agreement scores.
- The unsupervised event extraction pipeline used to generate candidates is not fully specified in the paper, requiring reference to external work for complete reproduction.
- The current models use simplified binary tagging and don't handle overlapping or nested event structures.

## Confidence
**High Confidence**: The core task definition of identifying new events as subject-predicate-object triplets not previously mentioned or inferable through commonsense is clearly articulated and consistently applied throughout the paper. The experimental methodology, including dataset splits, baseline implementations, and evaluation metrics, is sufficiently detailed for reproducibility.

**Medium Confidence**: The annotation process and quality control measures are described, but the exact guidelines and interface used by annotators are not provided. This introduces some uncertainty about how consistently the task was executed across annotators and whether the reported agreement scores reflect the true difficulty of the task or inconsistencies in annotation procedures.

**Low Confidence**: The unsupervised event extraction pipeline's exact implementation is not detailed, requiring external reference to Mousavi et al. (2021). Without access to or understanding of this pipeline, reproducing the exact candidate generation process is uncertain.

## Next Checks
1. **Annotation Guidelines Verification**: Request and review the full annotation guidelines and interface to verify that the reported IAA scores accurately reflect task difficulty rather than annotation inconsistencies. Compare the guidelines against the brief description in section 4 to identify any gaps or ambiguities.

2. **Pipeline Implementation Validation**: Implement the unsupervised S-P-O extraction pipeline based on the Chambers and Jurafsky (2008) reference and Mousavi et al. (2021) work. Apply this pipeline to a sample narrative from the SEND dataset and compare the generated candidates against the paper's description to ensure alignment.

3. **Model Performance Boundary Analysis**: Conduct ablation studies on the RoBERTa models to determine whether performance gains come from genuine understanding of event novelty or from position-based heuristics (e.g., new events appearing later in narratives). Test models on modified datasets where event positions are shuffled to isolate the effect of position bias.