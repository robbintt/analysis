---
ver: rpa2
title: Learning Dense Correspondences between Photos and Sketches
arxiv_id: '2307.12967'
source_url: https://arxiv.org/abs/2307.12967
tags:
- learning
- sketch
- correspondence
- photo-sketch
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of learning dense correspondences
  between photos and sketches, which is challenging due to the highly abstract nature
  of sketches. The authors propose a self-supervised method that leverages contrastive
  learning to align sketch and photo representations, followed by a spatial transformer
  network to estimate the dense displacement field between them.
---

# Learning Dense Correspondences between Photos and Sketches

## Quick Facts
- **arXiv ID**: 2307.12967
- **Source URL**: https://arxiv.org/abs/2307.12967
- **Reference count**: 37
- **Primary result**: Proposes self-supervised method using contrastive learning and spatial transformer networks, establishes new state-of-the-art on PSC6k benchmark

## Executive Summary
This paper addresses the challenging problem of learning dense correspondences between photos and sketches, where the highly abstract nature of sketches makes alignment difficult. The authors introduce a self-supervised method that first uses contrastive learning to align sketch and photo representations, then employs a spatial transformer network to estimate the dense displacement field between them. They contribute a new benchmark dataset, PSC6k, containing 150K human-annotated keypoint correspondences across 6250 photo-sketch pairs spanning 125 object categories. Experiments demonstrate that their method outperforms existing baselines and achieves state-of-the-art performance on PSC6k.

## Method Summary
The proposed method consists of two main components: a multimodal image encoder trained with contrastive learning to align sketch and photo representations in a shared feature space, and a spatial transformer network-based warp estimator that learns to map between these aligned representations. The contrastive learning component treats photo-sketch pairs as positive examples, while the STN-based estimator uses affinity matrices computed from multi-layer feature pyramids to estimate displacement fields. The training is self-supervised using weighted perceptual similarity and forward-backward consistency losses. The method operates at multiple scales, with the final displacement field resolution being 16×16.

## Key Results
- Establishes new state-of-the-art performance on PSC6k benchmark across 125 object categories
- Outperforms existing baselines in dense correspondence estimation between photos and sketches
- Reveals systematic differences between model predictions and human annotations, suggesting potential for improvement
- Demonstrates effectiveness of self-supervised learning approach for cross-modal alignment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Contrastive learning on photo-sketch pairs produces stronger alignment than individual image augmentation
- **Mechanism**: By treating a photo and its corresponding sketch as positive pairs, the model learns to minimize distance in representation space specifically between modalities, while pushing apart different object classes
- **Core assumption**: The sketch preserves enough semantic structure to serve as a valid positive example for its photo counterpart
- **Evidence anchors**: [abstract] "Our model uses a multimodal image encoder trained with a contrastive loss, with photos and sketches of the same object being treated as positive examples"; [section 3.1] "Unlike typical contrastive learning schemes that take augmented views of the same image I as positives, our model uses augmented views from the same photo-sketch pair (Ip, Is)"
- **Break condition**: If sketches become too abstract or sparse to provide meaningful positive signal, contrastive learning would fail to align representations

### Mechanism 2
- **Claim**: Spatial transformer networks with multi-scale estimation can learn dense correspondences between highly abstract sketches and photos
- **Mechanism**: The STN-based warp estimator uses affinity matrices computed from multi-layer feature pyramids to estimate displacement fields, refining alignment through residual connections across scales
- **Core assumption**: The spatial relationships between object parts are preserved in sketches despite abstraction
- **Evidence anchors**: [abstract] "Our model uses a spatial transformer network to estimate the warp flow between latent representations of a sketch and photo"; [section 3.2] "Module g takes the affinity matrix A(s,t) and directly estimates the displacement field F from the source image to the target image"
- **Break condition**: If sketches contain significant perspective changes or structural distortions that violate the smoothness assumptions of flow estimation

### Mechanism 3
- **Claim**: Weighted perceptual similarity improves correspondence quality by focusing optimization on semantically important regions
- **Mechanism**: The weighted perceptual similarity uses a weight function that emphasizes pixels with strong correlations between warped source and target, while perceptual similarity evaluates warped images through the encoder for soft constraints
- **Core assumption**: Important pixels in one image should have greater affinities to the other image
- **Evidence anchors**: [section 3.3] "We propose using weighted perceptual similarity to evaluate the quality of estimated displacement field between the photo-sketch pair"; [section 3.3] "For an image pair (Is, It), the model estimates the flow Fs→t and renders the warped source image Is→t"
- **Break condition**: If the weight function incorrectly prioritizes background regions or if perceptual similarity fails to capture meaningful semantic alignment

## Foundational Learning

- **Concept**: Contrastive learning fundamentals
  - **Why needed here**: The method relies on contrastive learning to align photo and sketch representations in a shared feature space
  - **Quick check question**: What distinguishes the positive pairs in this work from standard contrastive learning approaches?

- **Concept**: Spatial transformer networks and flow estimation
  - **Why needed here**: The core correspondence estimation uses STN-based warp estimation with multi-scale refinement
  - **Quick check question**: How does the affinity matrix between source and target feature maps drive the displacement field estimation?

- **Concept**: Perceptual similarity and self-supervised objectives
  - **Why needed here**: The model uses weighted perceptual similarity and forward-backward consistency as self-supervised objectives for training
  - **Quick check question**: What role does the perceptual similarity play in stabilizing training compared to direct feature similarity?

## Architecture Onboarding

- **Component map**: Photo/sketch → Encoder (ResNet backbone with conditional batch norm) → Feature pyramid extraction → Affinity matrix computation → Multi-scale STN blocks (g1, g2, g3) → Displacement field output → Perceptual similarity + consistency loss
- **Critical path**: Photo/sketch → Encoder → Feature maps → Affinity matrix → STN blocks → Displacement field → Warped image → Perceptual similarity
- **Design tradeoffs**: Multi-scale flow estimation (16×16 final resolution) vs computational efficiency; conditional batch norm for cross-domain alignment vs standard normalization
- **Failure signatures**: Poor performance on fine structures (low resolution features), inability to handle perspective changes (violated smoothness assumptions), confusion between commonly co-occurring objects
- **First 3 experiments**:
  1. Test contrastive learning with different positive pair definitions (individual images vs photo-sketch pairs)
  2. Evaluate STN block performance with varying feature pyramid depths
  3. Measure impact of perceptual similarity weighting vs uniform weighting on correspondence quality

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the proposed self-supervised method compare to fully supervised methods in terms of accuracy and efficiency on the PSC6k benchmark?
- **Open Question 2**: Can the proposed method be extended to handle more complex visual transformations, such as 3D rotations or deformations, beyond the current 2D warping approach?
- **Open Question 3**: How does the choice of backbone architecture (e.g., ResNet-18 vs. ResNet-101) affect the performance of the proposed method on the PSC6k benchmark?
- **Open Question 4**: How does the proposed method handle cases where the sketch and photo have significant differences in perspective or viewpoint, which may lead to non-continuous transformations?
- **Open Question 5**: How does the proposed method's performance compare to human-level performance on the PSC6k benchmark, and what are the key factors contributing to any remaining gap?

## Limitations

- Performance degrades on fine structures and commonly co-occurring objects due to low-resolution feature maps and limited training data
- Method assumes sketches preserve sufficient spatial relationships, which may break down with highly abstract or stylized sketches
- Systematic differences between model predictions and human annotations suggest room for improvement in human-like sketch understanding

## Confidence

- **High confidence**: The core mechanism of using contrastive learning to align photo-sketch representations is well-supported by experimental results
- **Medium confidence**: The spatial transformer network approach for dense correspondence estimation works well for general cases but shows limitations on fine details and complex scenes
- **Low confidence**: The interpretation of systematic differences between model predictions and human annotations - while interesting, the analysis doesn't establish clear causal mechanisms

## Next Checks

1. **Fine structure evaluation**: Conduct targeted experiments on photo-sketch pairs with fine details (e.g., fingers, facial features) to quantify performance degradation and identify specific failure modes
2. **Cross-domain generalization**: Test the model on sketches from different artistic styles or abstraction levels to assess robustness to variations in sketch expressiveness
3. **Ablation of perceptual weighting**: Compare weighted perceptual similarity against uniform weighting and direct feature similarity to isolate the contribution of the weighting mechanism to correspondence quality