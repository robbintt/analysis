---
ver: rpa2
title: 'Over-parameterised Shallow Neural Networks with Asymmetrical Node Scaling:
  Global Convergence Guarantees and Feature Learning'
arxiv_id: '2302.01002'
source_url: https://arxiv.org/abs/2302.01002
tags:
- neural
- training
- learning
- lemma
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes large-width shallow neural networks where the
  output of each hidden node is scaled by a positive parameter. The scaling parameters
  are non-identical, differing from the classical Neural Tangent Kernel (NTK) parameterisation.
---

# Over-parameterised Shallow Neural Networks with Asymmetrical Node Scaling: Global Convergence Guarantees and Feature Learning

## Quick Facts
- arXiv ID: 2302.01002
- Source URL: https://arxiv.org/abs/2302.01002
- Reference count: 40
- Primary result: When scaling parameter γ < 1, large shallow neural networks can learn features and converge to global minima with high probability, unlike NTK parameterisation

## Executive Summary
This paper analyzes large-width shallow neural networks where each hidden node output is scaled by a positive parameter. Unlike classical Neural Tangent Kernel (NTK) parameterisation with identical scaling, this work proves that non-identical scaling enables feature learning while maintaining global convergence guarantees. The key insight is that when γ < 1, the asymmetric component of the scaling prevents the lazy training regime characteristic of NTK, allowing the network to learn meaningful features while still converging to zero training error with high probability.

## Method Summary
The method involves training shallow neural networks with m hidden nodes where each node j has scaling parameter λ_m,j that combines a symmetric component (γ/m) and an asymmetric component ((1-γ)λ̃_j/∑λ̃_k). The network uses gradient flow or gradient descent to minimize the ℓ₂ loss. The scaling parameters λ̃_j follow a Zipf distribution with parameter α. The analysis relies on NTK theory, showing that when γ > 0, the minimum eigenvalue of the NTG matrix is bounded below, ensuring exponential decay of empirical risk. Experiments use gradient descent with learning rates 1.0 or 5.0 on synthetic data (100 observations, 50 dimensions) and UCI regression datasets, testing different combinations of (γ,α).

## Key Results
- For γ < 1, the network learns features as the asymmetric scaling component prevents vanishing weight updates in the infinite-width limit
- For γ > 0, gradient flow converges to global minimum with probability at least 1-δ when m ≥ max(23n log(4n/δ)/(κ_n d), ...)
- The NTG matrix minimum eigenvalue is bounded below by γκ_n/4, ensuring exponential convergence rate
- Smaller γ values improve pruning and transfer learning performance compared to NTK regime

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-identical node scaling enables feature learning in large shallow networks
- Mechanism: The asymmetric component ((1-γ)λ̃_j/∑λ̃_k) retains significant contribution per node even as m → ∞, preventing lazy training
- Core assumption: λ̃_j decays sufficiently slowly (e.g., Zipf law) so ∑λ̃_j² remains bounded away from zero
- Evidence anchors: [abstract] "gradient flow converges to a global minimum AND can learn features, unlike in the NTK regime"
- Break condition: If λ̃_j decays too quickly (α → 1), the model reverts to NTK-like behavior

### Mechanism 2
- Claim: Global convergence to zero training error is guaranteed with high probability when γ > 0
- Mechanism: NTK matrix minimum eigenvalue bounded below by γκ_n/4 ensures exponential decay of empirical risk
- Core assumption: Data satisfies standard regularity conditions (non-zero inputs, bounded outputs, distinct patterns)
- Evidence anchors: [abstract] "gradient flow and gradient descent converge to a global minimum..."
- Break condition: If γ = 0, the minimum eigenvalue bound may fail

### Mechanism 3
- Claim: Weight change per node scales with √λ_m,j, enabling non-vanishing updates when γ < 1
- Mechanism: Expected weight change is proportional to √λ_m,j, which remains non-zero as m → ∞ if γ < 1
- Core assumption: Activation function is ReLU or smooth with bounded derivatives
- Evidence anchors: [section] "The expected change of the weight vector w_j at the first GD update is non-vanishing in the infinite-width limit if and only if γ = 1"
- Break condition: If all λ̃_j = 0 except one, model reduces to standard NTK behavior

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) and its limiting behavior
  - Why needed here: The paper's convergence analysis fundamentally relies on NTK theory and how asymmetrical scaling modifies NTK dynamics
  - Quick check question: What happens to the NTK matrix as network width m → ∞ under standard vs. asymmetrical scaling?

- Concept: Gradient flow dynamics and empirical risk minimization
  - Why needed here: The paper proves global convergence using continuous-time gradient descent on ℓ₂ loss
  - Quick check question: How does the minimum eigenvalue of the NTG matrix relate to gradient flow convergence rate?

- Concept: Feature learning vs. lazy training regimes
  - Why needed here: The key distinction between NTK scaling and asymmetrical scaling is whether network learns features
  - Quick check question: What observable difference in weight evolution distinguishes feature learning from lazy training?

## Architecture Onboarding

- Component map: Input layer (d dimensions) → Hidden layer (m nodes with scaling λ_m,j) → Output layer (scalar)
- Critical path: 
  1. Initialize weights w_j ~ N(0,I_d) and fix output weights a_j ∈ {-1,1}
  2. Compute NTK matrix Θ_m(X; W_0) at initialization
  3. Verify minimum eigenvalue condition holds (γκ_n/4)
  4. Run gradient flow: dW_t/dt = -∇_W L_m(W_t)
  5. Monitor convergence and feature learning via weight changes and NTK evolution
- Design tradeoffs:
  - γ parameter: Higher γ → faster convergence but less feature learning; lower γ → more feature learning but slower convergence
  - λ̃_j decay rate: Slower decay (smaller α) → more asymmetry → more feature learning
  - Width m: Must be sufficiently large relative to n and κ_n
- Failure signatures:
  - If γ = 0: Convergence guarantees may fail; monitor minimum eigenvalue of NTK
  - If α too large: Loss of feature learning; monitor weight changes and NTK evolution
  - If m too small: Convergence conditions not met
- First 3 experiments:
  1. Synthetic data test: Generate sinusoidal data, train with different (γ,α) pairs, observe training curves and weight changes
  2. Pruning test: Train on real regression data, prune nodes with small feature importance, measure performance degradation
  3. Transfer learning test: Train on one dataset, extract features, train external model on different dataset, compare with NTK baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does the asymmetrical scaling parameter γ lead to feature learning in large-width neural networks?
- Basis in paper: The paper states feature learning arises if and only if γ < 1, but does not provide complete characterization of conditions
- Why unresolved: The paper focuses on proving convergence properties and illustrating through experiments, but lacks rigorous characterization of exact conditions
- What evidence would resolve it: A rigorous mathematical proof or comprehensive experimental study identifying precise conditions (data properties, architecture, activation functions) for feature learning

### Open Question 2
- Question: How does the asymmetrical scaling parameter γ affect generalization performance?
- Basis in paper: The paper demonstrates smaller γ improves pruning and transfer learning, but lacks comprehensive analysis of generalization impact
- Why unresolved: The paper focuses on training process and does not extensively investigate relationship between γ and generalization on unseen data
- What evidence would resolve it: Systematic study comparing generalization performance across different γ values on various datasets and tasks

### Open Question 3
- Question: Can insights from shallow networks with asymmetrical scaling extend to deep neural networks?
- Basis in paper: The paper focuses on shallow networks, but concept could potentially apply to deep networks
- Why unresolved: Analysis is specific to shallow networks, extending to deep networks requires addressing additional complexities
- What evidence would resolve it: Theoretical analysis or experimental study investigating effects of asymmetrical scaling in deep neural networks

## Limitations
- Analysis assumes specific regularity conditions on data matrix X (non-zero inputs, bounded outputs, distinct patterns)
- Scaling parameters λ̃_j must follow Zipf-like decay with α sufficiently small, which may not hold in practice
- Width m must satisfy specific lower bounds relative to n and κ_n, potentially limiting practical applicability

## Confidence
- Confidence: High for global convergence guarantees under stated conditions (NTK-based analysis is rigorous)
- Confidence: Medium for feature learning claims (relies on empirical observations and theoretical bounds)

## Next Checks
1. Verify the minimum eigenvalue bound κ_n ≥ 4δ/m remains valid across different initialization schemes and data distributions
2. Test whether asymmetric scaling maintains feature learning benefits when activation function deviates from ReLU or smooth alternatives
3. Validate that pruning and transfer learning benefits persist when training with stochastic gradient descent rather than gradient flow