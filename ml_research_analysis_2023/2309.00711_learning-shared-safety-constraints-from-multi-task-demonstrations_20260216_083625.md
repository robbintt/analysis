---
ver: rpa2
title: Learning Shared Safety Constraints from Multi-task Demonstrations
arxiv_id: '2309.00711'
source_url: https://arxiv.org/abs/2309.00711
tags:
- constraint
- learning
- expert
- policy
- constraints
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents an approach for learning safety constraints
  from multi-task demonstrations using inverse constraint learning (ICL). The key
  idea is to frame ICL as a zero-sum game between a policy player and a constraint
  player, where the policy player tries to maximize reward while satisfying a potential
  constraint, and the constraint player picks constraints that maximally penalize
  the learner relative to the expert.
---

# Learning Shared Safety Constraints from Multi-task Demonstrations

## Quick Facts
- arXiv ID: 2309.00711
- Source URL: https://arxiv.org/abs/2309.00711
- Reference count: 24
- Primary result: Inverse constraint learning approach that leverages multi-task demonstrations to learn tighter safety constraints that generalize across tasks

## Executive Summary
This paper presents a novel approach for learning safety constraints from expert demonstrations in multi-task settings. The method frames constraint learning as a zero-sum game between a policy player (who maximizes reward while satisfying constraints) and a constraint player (who selects constraints that maximize the difference in constraint violation between learner and expert). By leveraging diverse demonstrations across multiple tasks, the approach learns tighter constraints that generalize better than single-task methods. Experiments on continuous control tasks demonstrate that the learned policies match expert performance and constraint violation, with the multi-task extension showing particular advantages in recovering shared safety constraints.

## Method Summary
The method implements inverse constraint learning (ICL) through an iterative zero-sum game framework. Given expert demonstrations and a constraint class, it alternates between: (1) using constrained reinforcement learning with PID Lagrangian optimization to find policies that maximize reward while satisfying current constraints, and (2) updating constraints using no-regret learning (FTRL) based on the difference in constraint violation between the learner and expert. The multi-task extension leverages demonstrations from multiple tasks with different reward functions to learn constraints that are invariant across tasks. The CRL inner loop uses PPO with PID Lagrangian methods to ensure stable policy optimization that respects learned constraints.

## Key Results
- Learned policies match expert performance and constraint violation on velocity and position constraint tasks
- Multi-task extension successfully recovers shared constraints across different maze navigation tasks
- The method can recover ground-truth constraints under certain conditions when demonstrations are representative
- Compared to single-task baselines, multi-task ICL learns tighter constraints that generalize better to new tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The zero-sum game formulation allows the constraint player to identify constraints that separate safe expert behavior from unsafe but high-reward behavior.
- Mechanism: The constraint player iteratively picks constraints that maximize the difference in constraint violation between the learner and expert. This drives the learner to avoid regions of state space that the expert avoided despite their potential reward.
- Core assumption: The expert demonstrations are representative of safe behavior, and the policy player can explore the state space sufficiently to find unsafe but high-reward behaviors.
- Evidence anchors:
  - [abstract] "Intuitively, we learn constraints that forbid highly rewarding behavior that the expert could have taken but chose not to."
  - [section 3.3] "In ICL, we are given access to the reward function, trajectories from the solution to a CRL problem, and a class of potential constraints Fc in which we assume the ground-truth constraint c∗ lies."
  - [corpus] Weak - related papers focus on constraint learning but don't specifically address the zero-sum game formulation.

### Mechanism 2
- Claim: Multi-task demonstrations provide diverse coverage of the state space, enabling tighter constraints that generalize better to new tasks.
- Mechanism: By observing expert demonstrations across multiple tasks with different reward functions, the constraint player can identify safety constraints that are invariant across tasks. This allows learning constraints that forbid unsafe behavior regardless of the specific task reward.
- Core assumption: The safety constraints are shared across tasks, and the multi-task demonstrations provide sufficient coverage of the state space to identify these shared constraints.
- Evidence anchors:
  - [abstract] "We counter this by leveraging diverse demonstrations that naturally occur in multi-task settings to learn a tighter set of constraints."
  - [section 3.4] "Our second crucial insight is that we can leverage multi-task data to provide more comprehensive demonstration coverage over the state space, helping our method avoid degenerate solutions."
  - [corpus] Weak - related papers discuss multi-task learning but don't specifically address how diverse demonstrations enable tighter constraints.

### Mechanism 3
- Claim: The PID Lagrangian method in the CRL inner loop provides stable policy optimization that respects the learned constraints.
- Mechanism: The PID controller dampens oscillations and prevents cost overshooting in the Lagrangian multiplier updates, leading to more stable policy optimization. This allows the policy player to find high-reward policies that satisfy the learned constraints.
- Core assumption: The PID controller hyperparameters are properly tuned for the specific task and constraint learning problem.
- Evidence anchors:
  - [section 4.1] "The PID Lagrangian method [Stooke et al., 2020] extends the naive gradient update of λi with a proportional and derivative term to dampen oscillations and prevent cost overshooting."
  - [section B.1] "We use the Tianshou [Weng et al., 2022] implementation of PPO [Schulman et al., 2017] as our baseline policy optimizer. Classical Lagrangian methods exactly follow the gradient update shown in Algorithm 1, but they are susceptible to oscillating learning dynamics and constraint-violating behavior during training."
  - [corpus] Weak - related papers discuss PID controllers but don't specifically address their use in CRL for constraint learning.

## Foundational Learning

- Concept: Inverse Reinforcement Learning (IRL)
  - Why needed here: ICL builds upon the framework of IRL, extending it to the space of constraints instead of rewards.
  - Quick check question: What is the key difference between IRL and ICL in terms of the space they operate on?

- Concept: Constrained Reinforcement Learning (CRL)
  - Why needed here: CRL is used as the inner loop of ICL to find policies that maximize reward while satisfying the learned constraints.
  - Quick check question: How does the CRL inner loop in ICL differ from the RL inner loop in IRL?

- Concept: Zero-sum games and no-regret learning
  - Why needed here: The ICL problem is formulated as a zero-sum game between the policy player and constraint player, and no-regret learning algorithms like FTRL are used to update the constraint.
  - Quick check question: What is the role of the regret minimization algorithm in the ICL game formulation?

## Architecture Onboarding

- Component map: Policy player (CRL optimizer) <-> Constraint player (no-regret learner)
- Critical path:
  1. Initialize constraint c1
  2. For each iteration i:
     a. Run CRL to find policy πi that maximizes reward under constraint ci
     b. Update constraint ci+1 using no-regret learning based on difference in constraint violation between πi and expert
  3. Return the best constraint c1:N based on validation performance
- Design tradeoffs:
  - Tradeoff between constraint expressiveness and identifiability: More expressive constraint classes may lead to better performance but are harder to identify from demonstrations
  - Tradeoff between sample efficiency and constraint tightness: More diverse demonstrations enable tighter constraints but require more data collection
- Failure signatures:
  - Overly conservative constraints that forbid all behavior the expert did not take
  - Constraints that do not generalize well to new tasks due to insufficient demonstration coverage
  - Unstable policy optimization due to poor PID controller tuning
- First 3 experiments:
  1. Single-task velocity constraint: Learn a linear constraint on the agent's velocity from expert demonstrations and verify that the learned policy matches expert performance and constraint violation
  2. Single-task position constraint: Learn a linear constraint on the agent's position from expert demonstrations and verify that the learned policy matches expert performance and constraint violation
  3. Multi-task maze constraint: Learn a constraint on the agent's position from expert demonstrations across multiple maze navigation tasks and verify that the learned policy matches expert performance and constraint violation across all tasks

## Open Questions the Paper Calls Out

- Question: How can ICL be extended to settings where the reward function is unknown, similar to inverse reinforcement learning?
  - Basis in paper: [explicit] The authors note that in concurrent work, Lindner et al. [2023] propose an elegant solution approach to ICL: rather than learning a constraint function, assume that any unseen behavior is unsafe and enforce constraints on the learner to play a convex combination of the demonstrated safe trajectories. The key benefit of this approach is that it doesn't require knowing the reward function the expert was optimizing.
  - Why unresolved: While this approach is mentioned, the paper does not explore or compare it to their method.
  - What evidence would resolve it: Empirical comparison of the proposed method with the approach of Lindner et al. [2023] on the same tasks, evaluating performance and safety.

## Limitations

- The method relies heavily on the assumption that expert demonstrations are representative of safe behavior, which may not hold in all scenarios
- Exact implementation details of the PID Lagrangian method and specific hyperparameter choices are not fully specified, making exact reproduction challenging
- The claim that the method can recover ground-truth constraints under certain conditions is only briefly mentioned without detailed analysis

## Confidence

- **High confidence**: The zero-sum game formulation as the core mechanism for constraint learning (supported by theoretical analysis in section 3.3)
- **Medium confidence**: The multi-task extension's ability to learn tighter constraints (supported by experimental results but limited theoretical guarantees)
- **Low confidence**: The claim that this approach can recover ground-truth constraints under certain conditions (only briefly mentioned without detailed analysis)

## Next Checks

1. **Constraint Evolution Analysis**: Track how the learned constraint evolves over training iterations to verify it's becoming tighter and more discriminative, not just conservative.

2. **Cross-Task Generalization Test**: Design a multi-task experiment where some tasks have shared safety constraints and others don't, to verify the method can distinguish between shared and task-specific constraints.

3. **Exploration Robustness Check**: Test the method's performance when the policy player's exploration is deliberately limited, to quantify how sensitive the constraint learning is to exploration quality.