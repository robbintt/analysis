---
ver: rpa2
title: 'Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model
  through Expert Feedback and Real-world Multi-turn Dialogue'
arxiv_id: '2308.03549'
source_url: https://arxiv.org/abs/2308.03549
tags:
- medical
- data
- arxiv
- chinese
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce Zhongjing, a Chinese medical LLM based on
  LLaMA that implements the entire training pipeline from pre-training to RLHF. They
  construct a multi-turn medical dialogue dataset CMtMedQA of 70,000 real doctor-patient
  dialogues, and define a refined annotation rule and evaluation criteria.
---

# Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue

## Quick Facts
- arXiv ID: 2308.03549
- Source URL: https://arxiv.org/abs/2308.03549
- Reference count: 12
- Authors: Multiple researchers from the Zhongjing team
- Primary result: A Chinese medical LLM that outperforms previous models and matches ChatGPT performance in some abilities while using only 1% of its parameters

## Executive Summary
This paper introduces Zhongjing, a Chinese medical large language model based on LLaMA that implements the complete training pipeline from pre-training through reinforcement learning with human feedback (RLHF). The authors construct CMtMedQA, a 70,000-dialogue multi-turn medical dataset from real doctor-patient interactions, and develop a refined annotation framework for expert feedback. Through continuous pre-training on diverse medical corpus, supervised fine-tuning, and RLHF optimization, Zhongjing achieves state-of-the-art performance among Chinese medical LLMs while demonstrating improved safety and instruction-following capabilities.

## Method Summary
The Zhongjing training pipeline consists of four stages: (1) continuous pre-training on a large medical corpus including textbooks, electronic health records, and clinical data; (2) supervised fine-tuning using four types of instruction datasets including the CMtMedQA multi-turn dialogue dataset; (3) human feedback collection where medical experts rank model responses across safety, professionalism, and smoothness dimensions; and (4) policy optimization using Proximal Policy Optimization (PPO) guided by a reward model trained on expert rankings. The approach emphasizes multi-turn dialogue handling through real-world doctor-patient interactions and aligns outputs with professional medical standards through expert feedback.

## Key Results
- Zhongjing significantly outperforms previous Chinese medical LLMs on multiple benchmarks
- The model matches ChatGPT performance in some medical capabilities despite having only 1% of its parameters
- RLHF implementation improves instruction-following ability and safety metrics
- Multi-turn dialogue training enables proactive inquiry initiation in medical consultations
- The model covers 14 medical departments and 10+ clinical scenarios

## Why This Works (Mechanism)

### Mechanism 1: Multi-turn Dialogue Data with Proactive Inquiries
Training on real doctor-patient dialogues with proactive inquiries enables the model to learn conversational patterns essential for medical consultations. By incorporating frequent proactive inquiry statements, the model learns to initiate relevant questions and gather comprehensive patient information before providing diagnoses or recommendations.

### Mechanism 2: Expert-Driven Reinforcement Learning
Human expert feedback through ranking mechanisms provides reliable alignment signals that automated metrics cannot capture for medical applications. Expert annotators rank model-generated responses across safety, professionalism, and smoothness dimensions, creating a reward model that guides policy optimization toward professional medical standards.

### Mechanism 3: Comprehensive Pre-training Foundation
Continuous pre-training on diverse medical corpus provides the foundational knowledge necessary for effective specialization. By accumulating broad medical knowledge from multiple sources before fine-tuning, the model develops a robust knowledge base that enables better generalization and reduces catastrophic forgetting during subsequent training stages.

## Foundational Learning

- **Reinforcement Learning with Human Feedback**: Essential for medical applications requiring high reliability and safety standards beyond what supervised learning can guarantee. Quick check: Can you explain the difference between reward model training and policy optimization in RLHF?

- **Multi-turn Dialogue Management**: Critical for medical consultations that naturally involve multiple exchanges for information gathering. Quick check: How would you modify attention mechanisms to better handle long dialogue histories?

- **Domain-Specific Knowledge Representation**: Necessary because medical knowledge has unique characteristics including specialized terminology and safety constraints. Quick check: What strategies would you use to evaluate whether the model truly understands medical concepts versus memorizing patterns?

## Architecture Onboarding

- **Component map**: Continuous pre-training → Supervised Fine-Tuning → Human Feedback Collection → Reward Model Training → PPO Policy Optimization
- **Critical path**: Pre-training → SFT → RLHF, as each stage builds upon the capabilities developed in the previous stage
- **Design tradeoffs**: Prioritized depth in Chinese medical dialogue over multimodal capabilities, focusing on specialized medical knowledge rather than general-purpose functionality
- **Failure signatures**: Common failure modes include unsafe medical advice despite RLHF, inability to handle complex multi-turn dialogues, and catastrophic forgetting of general capabilities during specialization
- **First 3 experiments**: (1) Compare single-turn vs. multi-turn fine-tuning performance on identical test sets; (2) Ablation study training without pre-training to measure its contribution; (3) Safety testing with deliberately misleading prompts to evaluate RLHF effectiveness

## Open Questions the Paper Calls Out

### Open Question 1: Comparative Performance with Human Doctors
How does Zhongjing's performance compare to human doctors in multi-turn medical dialogues? The paper focuses on comparisons with other AI models rather than human medical professionals, which would provide a more meaningful benchmark for clinical utility.

### Open Question 2: Long-term Knowledge Retention
What is the long-term retention of medical knowledge in Zhongjing after continuous pre-training? The paper mentions catastrophic forgetting as a concern but doesn't provide data on knowledge retention over time or after additional training on non-medical data.

### Open Question 3: Handling Rare Medical Conditions
How does Zhongjing handle rare or complex medical conditions not well-represented in training data? The paper doesn't address the model's capability boundaries or failure modes with uncommon medical scenarios, focusing instead on overall performance metrics.

## Limitations

- Limited transparency in dataset construction, lacking analysis of class balance, temporal distribution, and demographic representation across the 14 medical departments
- Unclear comparison methodology with ChatGPT, with no statistical significance testing for performance claims
- Insufficient empirical validation of RLHF safety improvements against adversarial or edge-case medical queries
- Lack of detail on annotator qualifications and inter-rater reliability for the human feedback collection process

## Confidence

**High Confidence**: Technical implementation of training pipeline, model architecture and parameter counts, public release of code/datasets/models

**Medium Confidence**: CMtMedQA as meaningful advancement in Chinese medical dialogue data, RLHF improvements based on expert rankings, outperformance of previous Chinese medical LLMs

**Low Confidence**: Performance parity with ChatGPT claims without detailed metrics, sufficiency of RLHF safety improvements for real-world deployment, reliable handling of complex multi-turn consultations across all departments

## Next Checks

1. Conduct head-to-head statistical comparisons with ChatGPT using identical prompts and evaluation criteria, including confidence intervals and significance testing across multiple dimensions.

2. Design and execute adversarial testing with deliberately misleading or dangerous prompts to empirically verify RLHF safety improvements and measure both direct harm prevention and indirect harms through omission or misdirection.

3. Perform ablation studies testing model performance across different medical departments, with particular focus on underrepresented specialties, to quantify actual coverage and identify potential blind spots in training data distribution.