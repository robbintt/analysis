---
ver: rpa2
title: Chain-of-Factors Paper-Reviewer Matching
arxiv_id: '2310.14483'
source_url: https://arxiv.org/abs/2310.14483
tags:
- each
- paper-reviewer
- papers
- factors
- topic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the paper-reviewer matching problem in academic
  conferences, where automated systems are needed to assign appropriate reviewers
  to submissions. Previous methods typically focus on a single factor (semantic similarity,
  shared topics, or citation connections) to assess relevance, leading to incomplete
  evaluations.
---

# Chain-of-Factors Paper-Reviewer Matching

## Quick Facts
- arXiv ID: 2310.14483
- Source URL: https://arxiv.org/abs/2310.14483
- Reference count: 40
- Primary result: Significant P@5 improvements (1.2-8.4 percentage points) over state-of-the-art methods across four datasets

## Executive Summary
This paper addresses the paper-reviewer matching problem in academic conferences by proposing a unified model that jointly considers semantic, topic, and citation factors. Unlike previous methods that focus on single factors, the Chain-of-Factors approach uses instruction tuning with a shared contextualized language model backbone to produce factor-aware representations. During inference, the three factors are chained together using rank ensemble to enable step-by-step, coarse-to-fine reviewer search. Experiments on four datasets demonstrate consistent performance improvements, validating the benefits of multi-factor joint consideration for paper-reviewer matching.

## Method Summary
The method uses a shared contextualized language model backbone with instruction tuning to produce factor-aware paper representations. The model jointly considers semantic, topic, and citation factors through asymmetric attention between instruction and paper encoders. During inference, each factor independently ranks reviewers, and the final ensemble combines these rankings using mean reciprocal rank aggregation. The approach is pre-trained on diverse data sources including search click data, CS-Journal data, and citation triplets, enabling zero-shot matching without ground truth relevance scores.

## Key Results
- Achieves P@5 improvements of 1.2-8.4 percentage points over state-of-the-art methods
- Consistent performance gains across all four datasets (NIPS, SciRepEval, SIGIR, KDD)
- Superior results on both P@5 and P@10 metrics for paper-reviewer matching
- Demonstrates effectiveness of joint consideration of semantic, topic, and citation factors

## Why This Works (Mechanism)

### Mechanism 1
Instruction tuning with factor-aware prompts allows a single model to learn factor-specific representations without catastrophic interference. The instruction encoder produces stable semantic meanings while the paper encoder uses these prompts as context to guide encoding, with asymmetric attention ensuring instructions remain invariant across papers.

### Mechanism 2
Joint pre-training across all three factors improves performance on each individual factor through shared knowledge transfer. By training on diverse data sources with a shared model architecture, the model learns common linguistic patterns while contrastive loss encourages factor-specific distinctions.

### Mechanism 3
Rank ensemble across factors (mean reciprocal rank) provides more robust and comprehensive reviewer rankings than any single factor alone. Each factor produces a ranked list, and MRR combines these by averaging reciprocal ranks, giving more weight to reviewers strong across multiple factors.

## Foundational Learning

- **Concept**: Contrastive learning with hard negatives
  - Why needed here: The model needs to distinguish between relevant and irrelevant paper pairs across all three factors. Hard negatives provide more informative training signals than random negatives.
  - Quick check question: What is the difference between easy negatives and hard negatives in contrastive learning, and why does the paper use both?

- **Concept**: Asymmetric attention in multi-task learning
  - Why needed here: The instruction encoder should produce stable representations that don't change based on individual papers, while the paper encoder should be context-aware. Asymmetric attention achieves this by having papers attend to instructions but not vice versa.
  - Quick check question: How does asymmetric attention differ from standard attention, and what problem does it solve in this multi-task setup?

- **Concept**: Rank-based evaluation metrics (P@K, MRR)
  - Why needed here: Paper-reviewer matching is inherently a ranking problem. These metrics directly measure how well the model ranks relevant reviewers near the top of the list.
  - Quick check question: Why might P@5 be more appropriate than NDCG for evaluating paper-reviewer matching systems?

## Architecture Onboarding

- **Component map**: Instruction Encoder -> Paper Encoder -> Contrastive Loss Module -> Rank Ensemble Module -> Pre-training Data Modules
- **Critical path**: Pre-training → Inference ranking → Ensemble combination
- **Design tradeoffs**: Shared vs. separate models (chose shared with instruction tuning), hard vs. easy negatives (used both), aggregation method (top-3 average chosen empirically)
- **Failure signatures**: Poor instruction tuning leads to similar embeddings across factors, imbalanced pre-training data causes factor dominance, poor aggregation fails to combine factor signals
- **First 3 experiments**: 1) Verify instruction tuning produces distinct embeddings by comparing with UniPR-NoInstruction, 2) Test different aggregation functions to validate top-3 average choice, 3) Evaluate hard negative impact by comparing with easy negatives only

## Open Questions the Paper Calls Out
None explicitly identified in the paper.

## Limitations
- Relies on instruction tuning without isolating whether improvements come from instruction tuning itself versus joint pre-training approach
- Model assumes factor-specific instructions are sufficiently distinct without ablation testing this assumption
- Aggregation strategy using top-3 average is empirically chosen without theoretical justification for why it outperforms alternatives

## Confidence
- **High confidence**: Empirical improvements over baseline methods are robust across four datasets and two evaluation metrics
- **Medium confidence**: Mechanism claims about instruction tuning preventing catastrophic interference are plausible but not directly validated
- **Medium confidence**: Joint training benefits are supported by superior performance but could also result from larger effective training data

## Next Checks
1. Perform ablation studies removing instruction tuning to quantify its contribution versus joint training alone
2. Test alternative aggregation functions (Max, Mean, Sum) with statistical significance testing to validate the top-3 average choice
3. Analyze factor embedding similarity distributions to confirm that instruction tuning produces meaningfully distinct representations