---
ver: rpa2
title: LLM Augmented Hierarchical Agents
arxiv_id: '2311.05596'
source_url: https://arxiv.org/abs/2311.05596
tags:
- learning
- tasks
- goal
- agent
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework for using large language models
  (LLMs) to guide exploration in hierarchical reinforcement learning (HRL) agents.
  The core idea is to use LLMs to inject common-sense priors into high-level policy
  selection, making learning significantly more sample efficient.
---

# LLM Augmented Hierarchical Agents

## Quick Facts
- arXiv ID: 2311.05596
- Source URL: https://arxiv.org/abs/2311.05596
- Reference count: 33
- Primary result: Significant improvement in sample efficiency and success rate compared to vanilla HRL and shaped reward methods

## Executive Summary
This paper presents a framework for using large language models to guide exploration in hierarchical reinforcement learning agents. The core innovation is using LLMs to inject common-sense priors into high-level policy selection, dramatically improving sample efficiency. Rather than relying entirely on LLMs, the framework uses them to guide a high-level policy whose influence gradually reduces during training, resulting in a standalone policy deployable without LLM dependence.

## Method Summary
The method implements hierarchical reinforcement learning where a high-level policy selects among temporally extended skills, guided by LLM evaluations of skill relevance to the current task and state. The LLM provides probability estimates for each skill's relevance based on task descriptions and observation histories. During training, a weight factor λ controlling LLM influence starts at 1 and anneals to 0, allowing the RL policy to gradually learn independence. The framework was evaluated in MiniGrid, SkillHack, and Crafter simulation environments, plus a real robot arm block manipulation task.

## Key Results
- Agents trained with LLM guidance outperform baseline HRL methods and shaped reward approaches
- The framework converges to optimal policies much sooner than alternative methods
- LLM-guided agents demonstrate significantly improved sample efficiency across all tested environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-guided exploration improves sample efficiency by injecting common-sense priors into high-level policy selection
- Mechanism: The LLM evaluates skill relevance given task description and current state, providing probability estimates that guide action selection and reduce random exploration
- Core assumption: The LLM has sufficient common-sense knowledge about task structure to provide useful guidance
- Evidence anchors: [abstract] "use LLMs to inject common-sense priors into high-level policy selection, making learning significantly more sample efficient"; [section 3.2] "LLMs guide the agent by suggesting the most likely courses of action"
- Break condition: If the LLM lacks relevant common-sense knowledge for the specific task domain, guidance becomes noisy and could mislead exploration

### Mechanism 2
- Claim: Gradual reduction of LLM influence allows learning a standalone policy that doesn't depend on the LLM at deployment
- Mechanism: Weight factor λ controlling LLM influence starts at 1 and anneals to 0 during training, allowing RL policy to learn independence
- Core assumption: High-level policy can learn to make good decisions without LLM guidance if guidance is gradually reduced
- Evidence anchors: [abstract] "Instead of completely relying on LLMs, they guide a high-level policy, with their influence gradually reduced during training"; [section 3.2] "the weight factor starts from λ = 1 and is annealed gradually until it reaches zero by the end of training"
- Break condition: If λ is reduced too quickly before policy has learned sufficient task structure, agent may fail to converge to good policy

### Mechanism 3
- Claim: Hierarchical structure with temporally extended skills reduces effective horizon and makes learning more tractable
- Mechanism: Decomposing long-horizon tasks into sequences of sub-tasks reduces effective planning horizon from primitive actions to finite skill options
- Core assumption: Low-level skills are sufficiently general and can be combined to solve the task
- Evidence anchors: [abstract] "hierarchical agent that uses LLMs to solve long-horizon tasks"; [section 3.1] "we assume that the agent has access to a finite set of skills or sub-policies that can be executed in sequence to solve long-horizon tasks"
- Break condition: If skills are too specific or cannot be combined effectively, agent cannot solve task even with good high-level policy selection

## Foundational Learning

- Concept: Hierarchical Reinforcement Learning
  - Why needed here: Framework builds on HRL by using LLMs to guide high-level policy that selects among temporally extended skills
  - Quick check question: What are the two levels of decision-making in this HRL framework and what does each level control?

- Concept: Reinforcement Learning with sparse rewards
  - Why needed here: High-level policy learns from sparse task completion rewards, making exploration challenging without guidance
  - Quick check question: Why is sparse reward learning particularly difficult for long-horizon tasks and how does LLM guidance help?

- Concept: Large Language Model prompting and in-context learning
  - Why needed here: LLM is used as planning oracle through carefully designed prompts that ask about skill relevance
  - Quick check question: What prompt structure is used to get LLM to evaluate whether specific skill should be executed given current task and state?

## Architecture Onboarding

- Component map: Observation → Language mapping → LLM evaluation → pCS calculation → Policy + pCS → Action selection → Skill execution → Reward collection → Policy update

- Critical path: Observation → Language mapping → LLM evaluation → pCS calculation → Policy + pCS → Action selection → Skill execution → Reward collection → Policy update

- Design tradeoffs:
  - Using LLM guidance vs. pure RL: Guidance improves sample efficiency but adds dependency and latency
  - Number of skills: More skills increase expressiveness but make high-level policy harder to train
  - LLM query frequency: More frequent queries provide better guidance but increase API costs and latency

- Failure signatures:
  - Poor exploration despite LLM guidance: LLM may not have relevant knowledge for task domain
  - Slow convergence: λ reduction schedule may be too aggressive or task may be too complex
  - Agent gets stuck in loops: Low-level skills may not be reliable or may not properly signal completion

- First 3 experiments:
  1. MiniGrid UnlockReach task - simple door-opening and object retrieval to validate basic functionality
  2. SkillHack Battle task - multi-skill task requiring picking up weapon, wielding, and fighting to test complex skill combinations
  3. Crafter MakeStonePickaxe task - open-world resource collection and crafting task to test exploration and planning

## Open Questions the Paper Calls Out

Open Question 1
- Question: How does the performance of the LLM-augmented hierarchical agents compare to other state-of-the-art methods for long-horizon tasks in reinforcement learning?
- Basis in paper: Paper mentions proposed approach outperforms baseline methods but doesn't provide detailed comparison with other state-of-the-art methods
- Why unresolved: Paper focuses on comparing with baseline HRL methods and SayCan-like agent without affordances, lacking comprehensive comparison with other advanced methods
- What evidence would resolve it: Conducting experiments to compare proposed method with other state-of-the-art approaches for long-horizon tasks in reinforcement learning

Open Question 2
- Question: How does the proposed method scale to more complex and realistic environments with higher-dimensional observation spaces?
- Basis in paper: Method evaluated in simulation environments and simple real-world environment with tabular Q-learning version, doesn't explore scalability to more complex environments
- Why unresolved: Scalability to more complex environments is important aspect that needs to be addressed, unclear how well method would perform in environments with higher-dimensional observation spaces
- What evidence would resolve it: Conducting experiments in more complex and realistic environments with higher-dimensional observation spaces

Open Question 3
- Question: How does the performance of the proposed method vary with different types of language descriptions for skills and goals?
- Basis in paper: Paper mentions each skill is accompanied by language description and larger goal is described using natural language, but doesn't explore performance variation with different types of language descriptions
- Why unresolved: Performance may depend on quality and specificity of language descriptions, unclear how method would handle different types of language descriptions
- What evidence would resolve it: Conducting experiments with different types of language descriptions for skills and goals

Open Question 4
- Question: How does the proposed method handle uncertainty and ambiguity in the language descriptions and observations?
- Basis in paper: Paper assumes access to function providing language descriptions of current trajectory and state, doesn't explicitly address handling uncertainty and ambiguity
- Why unresolved: Handling uncertainty and ambiguity is crucial aspect in real-world applications, unclear how proposed method deals with uncertain or ambiguous language descriptions and observations
- What evidence would resolve it: Conducting experiments with uncertain or ambiguous language descriptions and observations

## Limitations
- Dataset Dependence: Approach relies heavily on LLM's common-sense knowledge, which may not generalize well to domains far from pretraining data
- Scalability Concerns: Method's effectiveness with larger action spaces or more complex skill decompositions is unclear, MiniGrid experiments use only 6 skills
- Evaluation Gaps: Paper lacks detailed analysis of sample efficiency curves and learning stability across multiple runs, real robot experiments described briefly without comprehensive quantitative results

## Confidence
- High Confidence: Claim that LLM guidance improves sample efficiency in hierarchical RL is well-supported by experimental results across multiple environments
- Medium Confidence: Claim about gradual reduction of LLM influence leading to standalone policies is supported by framework description but lacks detailed ablation studies
- Low Confidence: Claim that this approach will generalize to complex real-world tasks is weakly supported, as real robot experiments are limited in scope

## Next Checks
1. **Knowledge Domain Robustness**: Test framework on tasks from domains where LLM's pretraining data is limited or absent to evaluate performance degradation when common-sense priors are missing
2. **Scalability Testing**: Evaluate method with significantly larger skill set (20-30 skills) in single environment to assess whether approach scales to more complex task decompositions
3. **Ablation of LLM Influence Schedule**: Conduct controlled experiments varying annealing schedule of λ to quantify impact of gradual reduction versus immediate removal or no reduction on final policy performance and sample efficiency