---
ver: rpa2
title: Large Language Models as Superpositions of Cultural Perspectives
arxiv_id: '2307.07870'
source_url: https://arxiv.org/abs/2307.07870
tags:
- values
- message
- perspective
- different
- person
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that Large Language Models (LLMs) are context-dependent
  and can be conceptualized as superpositions of different perspectives. The authors
  introduce the concept of perspective controllability, which refers to a model's
  ability to adopt various perspectives with differing values and personality traits.
---

# Large Language Models as Superpositions of Cultural Perspectives

## Quick Facts
- arXiv ID: 2307.07870
- Source URL: https://arxiv.org/abs/2307.07870
- Reference count: 40
- Key outcome: Large Language Models exhibit context-dependent values and can be conceptualized as superpositions of different perspectives, with controllability varying by prompt method and model

## Executive Summary
This paper argues that Large Language Models can be conceptualized as superpositions of different cultural perspectives, each with distinct values and personality traits. The authors introduce perspective controllability as a framework for understanding how models can adopt different perspectives when prompted. Using standardized psychological questionnaires (PVQ, VSM, IPIP), they demonstrate that models express different values and personality traits based on the context provided in prompts, even when the differences are not obviously implied. The study provides quantitative evidence that perspective controllability varies across models and can be influenced by different prompting techniques and fine-tuning methods.

## Method Summary
The authors use three psychological questionnaires (PVQ for values, VSM for cultural values, and IPIP for personality traits) to measure how LLMs express different perspectives. They induce perspectives through various prompting methods including explicit value settings, implicit character-based contexts, and different message types (system vs user). The model responses are scored using questionnaire-specific rules, and correspondence metrics are computed to quantify how well target values are expressed. Statistical analysis (ANOVA and Tukey's HSD) is used to compare controllability across models and methods, with 50 permutations to ensure robustness against formatting effects.

## Key Results
- LLMs exhibit different values and personality traits based on context implied in prompts, demonstrating context-dependence
- User messages show higher controllability than system messages across all tested models
- RLHF fine-tuning increases controllability when applied to system messages but decreases it when applied to user messages
- The "extremely more" perspective intensity setting produces significantly higher correspondence than "slightly more" settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit context-dependent values because their training on diverse human-authored text creates a superposition of cultural perspectives
- Mechanism: During pretraining, the model learns to represent text from multiple perspectives (different cultures, personalities, formats). When prompted, the model collapses into a specific perspective, expressing corresponding values
- Core assumption: The model's parameters encode distinct perspective representations that can be selectively activated by prompts
- Evidence anchors:
  - [abstract] "an LLM can be seen as a superposition of perspectives with different values and personality traits"
  - [section 1] "An LLM is trained to model text written by humans with different cultural backgrounds, personalities, and values"
- Break condition: If the model's parameters don't encode distinct perspective representations, or if prompt-based activation is too weak to select specific perspectives

### Mechanism 2
- Claim: Perspective controllability works because explicit and implicit prompts can selectively activate different perspective dimensions
- Mechanism: The model has learned representations for different values/personality traits during training. Well-crafted prompts can activate these representations by either explicitly stating target values or implicitly implying them through context
- Core assumption: The model's learned representations for different values/personality traits are sufficiently distinct and accessible through prompting
- Evidence anchors:
  - [section 5.2] "LLMs can exhibit different values both when they are (directly and indirectly) implied in the prompt"
  - [section 5.4] "User message exhibits higher controllability among all models on VSM and IPIP"
- Break condition: If prompt-based activation cannot reliably select specific perspective dimensions, or if the model conflates different values/personality traits

### Mechanism 3
- Claim: RLHF fine-tuning can either increase or decrease perspective controllability depending on whether it's applied to system messages or user messages
- Mechanism: RLHF optimizes the model's responses based on human feedback. When applied to system messages, it may enhance the model's ability to adopt different perspectives. When applied to user messages, it may constrain the model to more conventional responses
- Core assumption: RLHF fine-tuning differentially affects the model's behavior based on which input channel (system vs user) is being optimized
- Evidence anchors:
  - [section 7.4.2] "RLHF fine-tuning increases the controllability when using the System message and decreases when using the User message"
  - [section 5.4] "GPT-3.5 and GPT-4 exhibit considerably higher controllability than other models on all the questionnaires"
- Break condition: If RLHF fine-tuning doesn't have differential effects based on input channel, or if the observed effects are due to other factors

## Foundational Learning

- Concept: Psychological questionnaires (PVQ, VSM, IPIP)
  - Why needed here: These provide standardized ways to measure values and personality traits that can be used to evaluate LLM behavior
  - Quick check question: What are the four higher dimensions in Schwartz's theory of basic human values?

- Concept: Prompt engineering techniques
  - Why needed here: Different ways of inducing perspectives (implicit vs explicit, system vs user message, second vs third person) affect controllability
  - Quick check question: How does the "extremely more" perspective intensity setting differ from the "slightly more" setting?

- Concept: Statistical analysis of questionnaire results
  - Why needed here: The correspondence metric and statistical tests (ANOVA, Tukey's HSD) are used to quantify and validate controllability differences
  - Quick check question: What statistical test would you use to compare controllability across different models?

## Architecture Onboarding

- Component map: Prompt generator -> LLM interface -> Questionnaire evaluator -> Correspondence calculator -> Statistical analyzer
- Critical path: 1. Generate prompt with target perspective, 2. Send to LLM and get response, 3. Score response using questionnaire, 4. Compute correspondence metric, 5. Repeat for all perspectives and permutations, 6. Perform statistical analysis
- Design tradeoffs:
  - Using 50 permutations increases robustness but also computational cost
  - System messages may be more effective but are only available in some models
  - Explicit value setting gives more control but may feel less natural than implicit methods
- Failure signatures:
  - Low correspondence across all models indicates perspective-induction methods aren't working
  - High variance across permutations suggests the model is sensitive to syntactic changes
  - No statistical significance in ANOVA results means differences aren't reliable
- First 3 experiments:
  1. Test GPT-4 with Sauron and Gandalf perspectives using the PVQ questionnaire (implicit setting)
  2. Compare system vs user message effectiveness on GPT-3.5 with explicit value settings
  3. Measure smoothness by testing different intensity levels on OpenAssistant

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the values and personality traits exhibited by LLMs change based on the context of the prompt?
- Basis in paper: [explicit] The paper discusses how LLMs express different values and personality traits based on the context of the prompt, and how this context-dependence can lead to different outcomes in various scenarios
- Why unresolved: The paper provides some qualitative examples of how different contexts can lead to different values and personality traits, but a more comprehensive and systematic study of the relationship between context and exhibited values and traits is needed
- What evidence would resolve it: A large-scale study that systematically varies the context of prompts and measures the resulting values and personality traits exhibited by LLMs would help to better understand the relationship between context and exhibited values and traits

### Open Question 2
- Question: How can we build LLMs that exhibit a specific set of values and personality traits?
- Basis in paper: [explicit] The paper discusses the concept of perspective controllability, which refers to a model's ability to adopt various perspectives with differing values and personality traits. However, the paper does not provide a clear answer on how to build LLMs with specific values and traits
- Why unresolved: Building LLMs with specific values and personality traits is a complex task that requires a deep understanding of the relationship between context and exhibited values and traits, as well as the ability to control the model's behavior
- What evidence would resolve it: A systematic study that investigates different methods for inducing specific values and personality traits in LLMs, and evaluates their effectiveness, would help to develop better methods for building LLMs with desired values and traits

### Open Question 3
- Question: How can we evaluate the diversity and controllability of cultural perspectives encoded in LLMs beyond datasets?
- Basis in paper: [inferred] The paper discusses the importance of evaluating the diversity and controllability of cultural perspectives in LLMs, but does not provide a clear answer on how to do so beyond analyzing the cultural diversity of datasets or human feedback
- Why unresolved: Evaluating the diversity and controllability of cultural perspectives in LLMs is a challenging task that requires a deep understanding of the model's behavior and the ability to measure its alignment with different cultural perspectives
- What evidence would resolve it: Developing methods for automatically evaluating the diversity and controllability of cultural perspectives in LLMs, such as by using questionnaires or other evaluation tools, would help to better understand the model's behavior and alignment with different cultural perspectives

## Limitations

- The superposition interpretation lacks direct empirical evidence for internal perspective representations
- The study uses a limited set of perspectives and questionnaires, raising questions about generalizability
- The RLHF fine-tuning comparison is based on only two versions of GPT-3.5, limiting conclusions about fine-tuning effects

## Confidence

- High Confidence: Core experimental finding that LLMs can express different values when prompted
- Medium Confidence: Characterization of LLMs as "superpositions of perspectives" is conceptually compelling but lacks direct evidence
- Medium Confidence: Comparison of perspective-induction methods is methodologically sound but practical implications remain unclear
- Low Confidence: Conclusions about RLHF fine-tuning effects are based on limited comparisons

## Next Checks

1. Use probing classifiers to analyze whether the model's internal representations show distinct perspective dimensions that correlate with behavioral controllability
2. Extend perspective-induction experiments to a broader range of contexts (different cultures, professions, scenarios) to validate generalizability
3. Test whether induced perspectives persist across multiple interactions and whether the correspondence metric remains stable over time