---
ver: rpa2
title: Finding Regions of Counterfactual Explanations via Robust Optimization
arxiv_id: '2301.11113'
source_url: https://arxiv.org/abs/2301.11113
tags:
- robust
- counterfactual
- which
- optimization
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a robust optimization method for generating
  regions of counterfactual explanations for logistic regression, decision trees,
  tree ensembles, and neural networks. The method iteratively solves a master problem
  with a finite set of scenarios and an adversarial problem to find a new violating
  scenario, adding it to the master problem.
---

# Finding Regions of Counterfactual Explanations via Robust Optimization

## Quick Facts
- arXiv ID: 2301.11113
- Source URL: https://arxiv.org/abs/2301.11113
- Reference count: 15
- Key outcome: Iterative adversarial algorithm converges to globally optimal robust CEs for logistic regression, decision trees, random forests, and neural networks

## Executive Summary
This paper introduces a robust optimization method for generating regions of counterfactual explanations (CEs) that remain valid under feature perturbations. The approach iteratively solves a master problem with a finite set of scenarios and an adversarial problem to find violating scenarios, adding them to the master problem until convergence. The method is proven to converge for common ML models including logistic regression, decision trees, random forests, and neural networks with ReLU activations. Experiments demonstrate that computation time primarily depends on model complexity rather than the number of features, with the method scaling well and the main computational challenge being the growing master problem.

## Method Summary
The method uses an iterative adversarial approach to generate robust counterfactual explanations. Starting with a finite subset of scenarios in the uncertainty set, it solves a master problem to find candidate CEs, then solves an adversarial problem to detect violating scenarios that would invalidate the current solution. If violations are found, they're added to the master problem and the process repeats until no more violations exist within a specified accuracy. The approach handles different model types through appropriate reformulations: Lipschitz continuity for general classifiers, MIP-representable formulations for decision trees and ReLU neural networks, and specific handling for tree structures through continuous approximations.

## Key Results
- Algorithm converges to globally optimal robust CEs for logistic regression, decision trees, random forests, and neural networks
- Computation time scales with model complexity rather than number of features
- Method handles ℓ∞-norm and ℓ2-norm uncertainty sets with different convergence rates
- Main computational bottleneck is the growing master problem as scenarios accumulate

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The iterative adversarial algorithm converges to a globally optimal robust counterfactual explanation for logistic regression, decision trees, random forests, and neural networks.
- Mechanism: The method starts with a finite subset of scenarios Z in the uncertainty set S, solves the master problem (MP) with these scenarios, then solves the adversarial problem (AP) to find a violating scenario that would invalidate the current solution. If a violating scenario is found, it's added to Z and the process repeats until no more violations exist within ε accuracy.
- Core assumption: The classifier h is Lipschitz continuous, ensuring convergence of the iterative process.
- Evidence anchors:
  - [abstract]: "Convergence is proven for the models considered"
  - [section]: "Theorem 2.1 (Mutapcic & Boyd (2009))... Algorithm 1 terminates after a finite number of steps"
  - [corpus]: Weak evidence - corpus contains related papers on robust counterfactual explanations but no direct verification of convergence proof
- Break condition: The algorithm fails to terminate in finite steps if the classifier is not Lipschitz continuous, as shown in Example 2.2 where the classifier is discontinuous.

### Mechanism 2
- Claim: Decision trees can be handled through Lipschitz continuous approximation while preserving prediction regions.
- Mechanism: A continuous function  is constructed that approximates the discontinuous tree classifier h. For leaves with prediction ≥ τ,  equals τ. For leaves with prediction < τ,  subtracts the minimum slack from τ. This allows the adversarial approach to work while maintaining the same decision regions (except on boundaries).
- Core assumption: The constructed function  is Lipschitz continuous and the interior of {x :  ≥ τ} is contained in {x : h(x) ≥ τ}.
- Evidence anchors:
  - [section]: "Lemma 2.3. The function  is Lipschitz continuous and int({x :  ≥ τ}) ⊆ {x : h(x) ≥ τ} ⊆ {x :  ≥ τ}"
  - [abstract]: "We prove convergence results for the most common machine learning methods including logistic regression, decision trees, random forests, and neural networks"
  - [corpus]: Weak evidence - related papers exist but no direct verification of the lemma
- Break condition: The approximation fails if the tree has leaves with zero width or if the interior property doesn't hold due to specific tree structures.

### Mechanism 3
- Claim: Neural networks with ReLU activation functions are MIP-representable and can be optimized efficiently.
- Mechanism: ReLU activation functions are piece-wise linear and can be reformulated using auxiliary binary variables. The master problem uses these reformulations to handle the neural network constraints, and the adversarial problem optimizes over the uncertainty set using these same reformulations.
- Core assumption: Neural networks with ReLU activations are Lipschitz continuous and belong to the MIP-representable class.
- Evidence anchors:
  - [section]: "neural networks with ReLU activation functions belong to the MIP-representable class of ML models (Grimstad & Andersson, 2019; Anderson et al., 2020)"
  - [section]: "The evaluation function h : X → [0, 1] of a trained neural network with rectiﬁed linear unit (ReLU) activation functions is Lipschitz continuous"
  - [corpus]: Weak evidence - corpus contains related papers on robust counterfactual explanations for neural networks but no direct verification of MIP-representability
- Break condition: The approach fails if the neural network uses activation functions outside the ReLU family or if the network structure creates non-convex regions that cannot be handled by the MIP formulation.

## Foundational Learning

- Concept: Lipschitz continuity
  - Why needed here: Ensures the adversarial algorithm converges in finite steps and allows the iterative approach to work
  - Quick check question: If a function f satisfies |f(x₁) - f(x₂)| ≤ L||x₁ - x₂|| for all x₁, x₂, what is L called?

- Concept: Mixed-integer programming (MIP) formulations
  - Why needed here: Required to handle decision trees and neural networks with ReLU activations in the optimization framework
  - Quick check question: What type of variables are typically introduced to handle the non-linearity of ReLU functions in MIP formulations?

- Concept: Robust optimization with uncertainty sets
  - Why needed here: The core problem formulation requires finding solutions that remain valid under feature perturbations
  - Quick check question: In robust optimization, what is the name of the approach that iteratively adds scenarios to a finite subset?

## Architecture Onboarding

- Component map: Master Problem (MP) -> Adversarial Problem (AP) -> Update Z -> MP (repeat until convergence)
- Critical path: MP → AP → Update Z → MP (repeat until convergence)
- Design tradeoffs: Exact robustness guarantees vs. computational efficiency; full uncertainty set vs. finite scenario approximation
- Failure signatures: Algorithm doesn't terminate (non-Lipschitz classifier); solutions become infeasible (incorrect MIP reformulation); excessive computation time (too many iterations)
- First 3 experiments:
  1. Implement the adversarial algorithm for logistic regression with ℓ∞-norm uncertainty set and verify convergence on a simple 2D dataset
  2. Add decision tree support by implementing the Lipschitz continuous approximation and test on a small tree
  3. Extend to neural networks with ReLU activations using MIP reformulations and verify on a simple 2-layer network

## Open Questions the Paper Calls Out
The paper identifies three main open questions: (1) how uncertainty set shape affects explanation quality and interpretability, (2) extending the method to handle categorical and immutable features, and (3) determining optimal strategies for selecting the uncertainty set radius ρ.

## Limitations
- Lipschitz continuity requirement excludes discontinuous models
- Computational complexity grows with the number of scenarios in the master problem
- MIP formulations for neural networks may become intractable for deep architectures

## Confidence
High: Theoretical convergence proofs are provided and experimental validation demonstrates the approach works for the claimed model types.

## Next Checks
1. Test algorithm convergence on a non-Lipschitz classifier to verify failure mode
2. Measure computation time scaling with uncertainty set size and model complexity
3. Compare generated CE regions against exact solutions for small problems