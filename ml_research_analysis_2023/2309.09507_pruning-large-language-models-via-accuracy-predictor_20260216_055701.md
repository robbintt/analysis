---
ver: rpa2
title: Pruning Large Language Models via Accuracy Predictor
arxiv_id: '2309.09507'
source_url: https://arxiv.org/abs/2309.09507
tags:
- pruning
- accuracy
- arxiv
- search
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel pruning approach for large language
  models (LLMs) using a non-neural model accuracy predictor. The key idea is to first
  build a dataset of architecture-accuracy pairs by randomly sampling pruned model
  architectures and evaluating their performance.
---

# Pruning Large Language Models via Accuracy Predictor

## Quick Facts
- arXiv ID: 2309.09507
- Source URL: https://arxiv.org/abs/2309.09507
- Authors: 
- Reference count: 0
- Key outcome: GBDT-based accuracy predictor achieves 9.48% PPL reduction on Wikitext2 and 6.28% MMLU accuracy improvement

## Executive Summary
This paper introduces a novel pruning approach for large language models (LLMs) using a non-neural model accuracy predictor. The key innovation is training a gradient boosting decision tree (GBDT) model to predict the performance of pruned architectures, enabling efficient search through the pruning space. By building a dataset of architecture-accuracy pairs and using GBDT to guide search space optimization, the method achieves significant improvements in both perplexity and MMLU accuracy compared to baseline pruning strategies.

## Method Summary
The approach consists of three main stages: First, a dataset of architecture-accuracy pairs is built by randomly sampling pruned model architectures within constrained search spaces and evaluating their performance on benchmark datasets. Second, a GBDT model is trained as an accuracy predictor using this dataset to learn the relationship between architectural features and performance metrics. Third, the trained GBDT is used to further optimize the search space by analyzing feature importance, and automatically select the optimal pruned model architecture. The method can be combined with QLoRA fine-tuning for additional performance recovery.

## Key Results
- PPL on Wikitext2 decreased by 9.48% compared to baseline
- PPL on PTB decreased by 5.76% compared to baseline
- Average MMLU accuracy increased by 6.28% compared to baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GBDT can predict pruned model accuracy with sufficient accuracy for architecture search
- Mechanism: GBDT learns the relationship between model architecture features (layer type, layer ID, pruning ratio) and performance metrics from a small dataset of architecture-accuracy pairs
- Core assumption: Performance of pruned LLMs can be predicted from architectural features without requiring neural network-based predictors
- Evidence anchors: Abstract states "using a non-neural model accuracy predictor" and "GBDT model is trained as an accuracy predictor"
- Break condition: Prediction accuracy degrades significantly when applied to architectures with substantially different characteristics than the training set

### Mechanism 2
- Claim: Limiting search space through expert knowledge improves GBDT prediction quality
- Mechanism: Restricting pruning to specific layer types and layer ID ranges makes the search space manageable for GBDT training
- Core assumption: Expert knowledge about which layers are most critical for performance can effectively constrain the search space
- Evidence anchors: Section states "Combined with expert knowledge to limit the scope of some features"
- Break condition: Performance improvement stagnates despite additional expert knowledge constraints

### Mechanism 3
- Claim: Feature importance from GBDT guides further search space optimization
- Mechanism: GBDT's ability to calculate feature importance allows refinement of pruning ratio ranges for different layers based on their impact on performance metrics
- Core assumption: Features with higher importance should have narrower pruning ratio ranges to preserve critical functionality
- Evidence anchors: Section states "We can optimize search space according to the feature importance of the GBDT model"
- Break condition: Performance gains from search space refinement become negligible compared to computational cost

## Foundational Learning

- Concept: Gradient boosting decision trees
  - Why needed here: GBDT serves as the accuracy predictor, learning the relationship between architectural features and model performance
  - Quick check question: How does GBDT differ from random forests in terms of tree construction and prediction?

- Concept: Model pruning and importance estimation
  - Why needed here: Understanding how pruning decisions are made based on parameter importance is crucial for interpreting the search space constraints
  - Quick check question: What is the difference between first-order and second-order Taylor expansion for importance estimation?

- Concept: Large language model evaluation metrics
  - Why needed here: The paper evaluates pruned models using PPL and MMLU accuracy, requiring understanding of what these metrics measure
  - Quick check question: Why would a model with lower PPL be considered better at generation tasks?

## Architecture Onboarding

- Component map: Architecture sampling module → prunes and evaluates candidate models → builds architecture-accuracy pairs → GBDT training module → learns mapping from architecture features to performance metrics → Search space optimization module → uses GBDT feature importance to refine pruning ratio ranges → Final architecture selection module → predicts and evaluates top candidates

- Critical path: Architecture sampling → GBDT training → Search space optimization → Final selection

- Design tradeoffs:
  - Smaller architecture-accuracy pair dataset reduces computational cost but may limit GBDT generalization
  - Narrower search space improves GBDT prediction quality but may miss optimal architectures
  - More pruning ratio granularity enables finer-grained search but increases computational complexity

- Failure signatures:
  - GBDT predictions consistently deviate from true evaluation results
  - Performance improvement plateaus despite increasing dataset size
  - Search space refinement leads to architectures with degraded performance

- First 3 experiments:
  1. Build architecture-accuracy pairs using random sampling within the defined constraints and verify dataset quality
  2. Train GBDT on the dataset and evaluate prediction accuracy on a held-out validation set
  3. Use GBDT feature importance to refine search space and verify performance improvement over baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed pruning approach scale when applied to even larger LLMs with hundreds of billions of parameters?
- Basis in paper: [explicit] The paper focuses on pruning LLMs with tens of billions of parameters and demonstrates effectiveness
- Why unresolved: The paper does not provide experimental results or analysis for LLMs beyond tens of billions of parameters
- What evidence would resolve it: Experiments applying the proposed approach to LLMs with hundreds of billions of parameters

### Open Question 2
- Question: How does the proposed pruning approach compare to other state-of-the-art pruning techniques in terms of efficiency and effectiveness?
- Basis in paper: [explicit] The paper mentions that most current LLM pruning strategies require manual design of pruning features
- Why unresolved: The paper focuses on introducing a novel pruning approach using an accuracy predictor based on non-neural models
- What evidence would resolve it: Comparative experiments evaluating the proposed approach against other state-of-the-art pruning techniques

### Open Question 3
- Question: How does the proposed pruning approach perform on LLMs with different architectures and sizes?
- Basis in paper: [explicit] The paper demonstrates the effectiveness of the proposed approach on the LLAMA2-7B model
- Why unresolved: The paper focuses on a specific LLM architecture and does not provide insights into how the approach generalizes to other architectures
- What evidence would resolve it: Experiments applying the proposed approach to LLMs with different architectures (e.g., GPT, BERT) and sizes

## Limitations

- Limited Search Space Generalization: Constraining pruning to specific layer types may not generalize well to other LLM architectures
- Dataset Size and Quality: The approach relies on a relatively small dataset of architecture-accuracy pairs to train the GBDT predictor
- Evaluation Metric Balance: The reported improvements combine performance on both generation tasks and knowledge-based tasks without clear weighting

## Confidence

**High Confidence**: The fundamental approach of using GBDT as an accuracy predictor for pruned architectures is well-supported by experimental results

**Medium Confidence**: The claim that limiting search space through expert knowledge improves prediction quality is supported by experimental design but lacks comparative analysis

**Low Confidence**: The generalizability of the approach to different LLM architectures and the robustness of the GBDT predictor when applied to architectures outside the training distribution remain uncertain

## Next Checks

1. Cross-Architecture Validation: Test the trained GBDT predictor on a different LLM architecture (e.g., GPT-2 or Mistral) to assess generalizability

2. Ablation Study on Search Space Constraints: Conduct experiments removing expert knowledge constraints on layer types and ID ranges to quantify impact

3. Predictor Robustness Testing: Systematically evaluate GBDT prediction accuracy across the full range of pruning ratios and layer combinations, including edge cases not well-represented in the training dataset