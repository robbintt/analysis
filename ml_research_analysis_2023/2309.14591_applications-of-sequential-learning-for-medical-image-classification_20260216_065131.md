---
ver: rpa2
title: Applications of Sequential Learning for Medical Image Classification
arxiv_id: '2309.14591'
source_url: https://arxiv.org/abs/2309.14591
tags:
- learning
- data
- training
- experiment
- sequential
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops a sequential learning framework for continual
  training of neural networks on small, incremental batches of medical imaging data,
  addressing overfitting, catastrophic forgetting, and concept drift. Using PyTorch
  CNNs and datasets like Medical MNIST and NIH Chest X-Ray, the authors compare sequential
  learning with and without pre-training and evaluate two validation dataset construction
  methods.
---

# Applications of Sequential Learning for Medical Image Classification

## Quick Facts
- arXiv ID: 2309.14591
- Source URL: https://arxiv.org/abs/2309.14591
- Reference count: 2
- Sequential learning achieves ~95% accuracy comparable to traditional CNNs on Medical MNIST datasets

## Executive Summary
This study develops a sequential learning framework for continual training of neural networks on small, incremental batches of medical imaging data. The approach addresses key challenges in medical imaging: overfitting due to limited data, catastrophic forgetting when new data arrives, and concept drift over time. Using PyTorch CNNs and datasets including Medical MNIST and NIH Chest X-Ray, the authors compare sequential learning with and without pre-training and evaluate two validation dataset construction methods. Experiments demonstrate that sequential learning reaches performance comparable to traditional CNNs while offering advantages for clinical scenarios with limited data.

## Method Summary
The sequential learning framework trains PyTorch CNNs incrementally on small batches of medical images over multiple "days." The approach uses either ResNet50 (for Medical MNIST) or DenseNet121 (for NIH Chest X-Ray) with data augmentation including random flips, rotations, translations, and color jitter. Two validation set construction methods are compared: one using current day data for validation and previous day data for training, and another mixing current and previous day data for training while reserving half of current day for validation. Pre-training is performed on an initial subset of 500 images before sequential learning begins. The framework is tested on Medical MNIST (58,954 images, 6 classes), modified Medical MNIST CXR (10,000 images, 2 classes), and NIH Chest X-Ray (112,120 images, 15 classes).

## Key Results
- Sequential learning achieves ~95% accuracy on Medical MNIST, comparable to traditional CNNs
- Pre-training enables faster convergence in sequential learning experiments
- Two-day mixing validation method improves robustness against overfitting
- NIH Chest X-Ray experiment demonstrates feasibility for real-world complex datasets (~68% accuracy)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential learning enables incremental training on small data batches without requiring large static datasets
- Mechanism: The model updates weights continuously as new batches arrive, using each batch as both training and validation source
- Core assumption: Small incremental batches preserve class distribution and concept drift is manageable within the sequential update loop
- Evidence anchors: [abstract] "sequential learning framework for continual training of small, incremental batches of medical imaging data" [section] "We formulated a retrospective sequential learning approach that would train and consistently update a model on mini-batches of medical images over time"

### Mechanism 2
- Claim: Pre-training on a small initial subset accelerates convergence in sequential learning
- Mechanism: Initial weights are seeded with learned features from the pre-training phase, reducing the number of sequential steps needed to reach high accuracy
- Core assumption: Features learned in pre-training transfer effectively to the sequential task and reduce catastrophic forgetting
- Evidence anchors: [abstract] "pre-training enabling faster convergence" [section] "Experiment 1a includes batch pre-training until a desirable starting accuracy was obtained by the network before it continues to train via sequential learning"

### Mechanism 3
- Claim: Two validation set construction methods mitigate overfitting during sequential learning
- Mechanism: Method 2b combines previous and current day data for training while reserving current day data for validation, providing more stable validation metrics
- Core assumption: Mixing temporal data in training and validation sets preserves data diversity and prevents memorization of a single day's distribution
- Evidence anchors: [section] "Experiment 2b... N/2 images from the 'current day' along with N/2 images from the 'previous day' to be used as training data... remaining N/2 images from the 'current day' would be used as validation data"

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: Sequential learning updates weights incrementally; without protection, earlier learned classes degrade when new data arrives
  - Quick check question: What happens to accuracy on earlier classes if you train only on new batches without replay?

- Concept: Concept drift
  - Why needed here: Medical imaging data distribution may shift over time due to scanner changes or population differences; the model must adapt without losing prior knowledge
  - Quick check question: How would you detect if a new batch's distribution differs significantly from earlier batches?

- Concept: Data augmentation for small datasets
  - Why needed here: Medical datasets are often limited; augmentation increases effective sample size and improves generalization
  - Quick check question: Which augmentation operations are safe to apply to medical images without altering pathology?

## Architecture Onboarding

- Component map: PyTorch CNN (ResNet50 or DenseNet121) -> Data loader with augmentation -> Sequential training loop -> Validation split logic -> Test evaluation
- Critical path: Data loading -> Model forward -> Loss computation -> Backpropagation -> Parameter update -> Validation accuracy check -> Early stopping if overfitting detected
- Design tradeoffs: Larger batch size speeds training but may reduce gradient noise beneficial for exploration; smaller batches increase update frequency but risk noisy gradients
- Failure signatures: Validation accuracy plateaus below target while training accuracy remains high (overfitting); sudden drops in validation accuracy (catastrophic forgetting); slow convergence indicating poor pre-training or inappropriate learning rate
- First 3 experiments:
  1. Run pre-training experiment (Exp 1a) to establish baseline convergence speed with initial data subset
  2. Test both validation set construction methods (Exp 2a vs 2b) to identify overfitting mitigation
  3. Validate on real-world NIH Chest X-Ray data (Exp 3) to confirm scalability to complex, high-resolution images

## Open Questions the Paper Calls Out

- Question: How does the optimal number of training "days" and "day-epochs" vary with dataset size and complexity?
  - Basis in paper: [explicit] The paper states "Power analyses would be similarly useful to evaluate the total number of recruited data, sequential learning "days", and day-epochs required to reach a certain accuracy threshold."
  - Why unresolved: The experiments used fixed numbers of "days" and "day-epochs" without systematically exploring the relationship with dataset characteristics
  - What evidence would resolve it: Systematic experiments varying dataset sizes and complexities while measuring optimal "days" and "day-epochs" to achieve target accuracy thresholds

- Question: What are the optimal hyperparameters for sequential learning across different medical imaging datasets?
  - Basis in paper: [explicit] The paper states "Moving forward we will implement optimization strategies to find the best combination of hyperparameters."
  - Why unresolved: The paper used manually tuned hyperparameters that varied across experiments, suggesting no systematic optimization approach
  - What evidence would resolve it: Comparative studies of different hyperparameter optimization methods (grid search, random search, Bayesian optimization) applied to multiple medical imaging datasets

- Question: How does scanner drift and technical variation across different acquisition days affect sequential learning performance?
  - Basis in paper: [explicit] The paper mentions "A few key differences between scans obtained over multiple 'days' includes scanner drift that results in signal degradation or intensity changes over time."
  - Why unresolved: The experiments used datasets with uniform acquisition conditions rather than intentionally introducing scanner drift
  - What evidence would resolve it: Experiments using datasets with known scanner drift, comparing sequential learning performance with and without drift correction techniques

## Limitations

- Performance depends heavily on data distribution stability across "days," which is not explicitly tested
- The specific mechanism by which pre-training accelerates convergence is not detailed
- Two-day mixing validation strategy lacks theoretical grounding
- Results limited to binary and six-class problems (Medical MNIST) and a single complex 15-class experiment (NIH Chest X-Ray)

## Confidence

- **High confidence**: Sequential learning achieves ~95% accuracy comparable to traditional CNNs on Medical MNIST datasets, supported by direct experimental results
- **Medium confidence**: Pre-training accelerates convergence, as results show correlation but not causation; mechanism not detailed
- **Medium confidence**: Two-day mixing validation reduces overfitting, based on comparison of two methods but without statistical significance testing

## Next Checks

1. Conduct an ablation study on the two-day mixing validation strategy to determine whether the mixing itself or simply having more validation data drives improved performance
2. Test the framework on a dataset with known temporal concept drift (e.g., different scanner protocols over time) to validate robustness claims
3. Implement a catastrophic forgetting baseline (training only on new batches without replay) to quantify the benefit of the sequential approach over naive incremental training