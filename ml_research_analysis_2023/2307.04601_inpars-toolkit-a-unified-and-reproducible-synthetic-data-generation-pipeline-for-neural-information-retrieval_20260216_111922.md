---
ver: rpa2
title: 'InPars Toolkit: A Unified and Reproducible Synthetic Data Generation Pipeline
  for Neural Information Retrieval'
arxiv_id: '2307.04601'
source_url: https://arxiv.org/abs/2307.04601
tags:
- synthetic
- data
- inpars
- quotedbl
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces InPars Toolkit, a unified pipeline for reproducible
  synthetic data generation in neural information retrieval. The toolkit integrates
  Large Language Models (LLMs) for synthetic query generation, filtering strategies,
  model training, and evaluation, supporting both GPU and TPU infrastructure.
---

# InPars Toolkit: A Unified and Reproducible Synthetic Data Generation Pipeline for Neural Information Retrieval

## Quick Facts
- arXiv ID: 2307.04601
- Source URL: https://arxiv.org/abs/2307.04601
- Reference count: 21
- Key outcome: Unified pipeline for reproducible synthetic data generation supporting GPU/TPU, integrated with IR libraries

## Executive Summary
This paper presents the InPars Toolkit, a comprehensive solution for reproducible synthetic data generation in neural information retrieval. The toolkit addresses the fragmentation problem in existing pipelines by providing a unified end-to-end workflow that integrates LLM-based query generation, filtering strategies, model training, and evaluation. It supports both GPU and TPU infrastructure, making advanced synthetic data methods accessible to researchers without specialized hardware. The toolkit successfully reproduces InPars and Promptagator methods, with over 2,000 GPU hours invested in generating synthetic data for 18 BEIR datasets, and all generated data and trained models are made publicly available.

## Method Summary
The InPars Toolkit provides a command-line interface with modular components for synthetic data generation: generate (LLM-based query generation), filter (data filtering using scores or reranker strategies), generate_triples (negative example mining), train (reranker training), rerank (reranking using trained models), and evaluate (performance evaluation). The pipeline leverages Pyserini and ir_datasets for data access and supports both GPU and TPU training infrastructure. The toolkit implements InPars-style few-shot learning with GPT-J for synthetic query generation and provides filtering mechanisms, though Promptagator's consistency filtering remains unimplemented. The approach focuses on creating reproducible workflows that overcome the accessibility barriers of fragmented codebases and specialized hardware requirements.

## Key Results
- GPU training achieves comparable nDCG@10 performance to TPU setups with minor variations across datasets
- Successfully generated synthetic data for all 18 BEIR datasets, requiring over 2,000 GPU hours of computation
- Made publicly available both the synthetic data and trained reranker models to accelerate research
- Reproduced InPars and Promptagator methods with unified infrastructure, demonstrating practical accessibility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The unified toolkit enables reproducibility by consolidating fragmented pipelines into a single, end-to-end workflow.
- Mechanism: By integrating data generation, filtering, training, reranking, and evaluation into one toolkit with consistent interfaces, researchers avoid the complexity of managing multiple codebases and incompatible infrastructure.
- Core assumption: Researchers face significant reproducibility barriers due to fragmented codebases and infrastructure requirements (TPUs vs GPUs).
- Evidence anchors:
  - [abstract] "Our main contribution is a unified toolkit for end-to-end reproducible synthetic data generation research, which includes generation, filtering, training and evaluation."
  - [section] "Researchers need to handle different codebases in addition to having access to a specific computational infrastructure. Most of the time, such components are not well integrated, making it difficult for researchers and practitioners to use them effectively."
- Break condition: If the toolkit does not support all necessary components (e.g., missing Promptagator consistency filtering) or if integration introduces new compatibility issues.

### Mechanism 2
- Claim: Supporting both GPU and TPU infrastructure removes a major accessibility barrier for reproducing InPars and Promptagator methods.
- Mechanism: By providing GPU-compatible training scripts and demonstrating comparable nDCG@10 performance to TPU setups, the toolkit makes these methods accessible to researchers without specialized hardware.
- Core assumption: TPU access is a limiting factor for many researchers, and GPU performance is sufficient for comparable results.
- Evidence anchors:
  - [abstract] "Additionally, we provide support for GPU."
  - [section] "Our analysis revealed that while there were minor variations in datasets such as TREC-COVID, BioASQ, Robust04 and ArguAna, the results remained exactly the same for NFCorpus, NQ, and FiQA-2018, regardless of the device used."
- Break condition: If GPU training consistently produces significantly worse results than TPU training for certain datasets or if GPU memory constraints prevent handling large models.

### Mechanism 3
- Claim: Making synthetic data and trained models publicly available accelerates research by eliminating the need to regenerate data.
- Mechanism: By providing the synthetic data generated for all 18 BEIR datasets (over 2,000 GPU hours of generation) and the trained reranker models, researchers can directly use these resources without incurring the computational cost of regeneration.
- Core assumption: The computational cost of generating synthetic data is prohibitive for many researchers, and having pre-generated data enables faster experimentation.
- Evidence anchors:
  - [abstract] "Additionally, we make available all the synthetic data generated in this work for the 18 different datasets in the BEIR benchmark which took more than 2,000 GPU hours to be generated as well as the reranker models fine-tuned on the synthetic data."
  - [section] "We also made available all the synthetic data generated in this reproduction study, along with the prompts and the fine-tuned reranker models."
- Break condition: If the provided synthetic data is not compatible with newer versions of the toolkit or if researchers require customization beyond what the provided data supports.

## Foundational Learning

- Concept: Few-shot learning with LLMs
  - Why needed here: The entire pipeline relies on LLMs generating synthetic queries from few examples, which is the core innovation of InPars and Promptagator methods.
  - Quick check question: What is the difference between static and dynamic prompts in few-shot learning, and how does this affect query quality?

- Concept: Information retrieval evaluation metrics (nDCG, MMR, etc.)
  - Why needed here: Understanding these metrics is essential for evaluating the performance of reranker models trained on synthetic data.
  - Quick check question: How does nDCG@10 differ from nDCG@100, and when would you use each metric?

- Concept: Zero-shot transfer learning in IR
  - Why needed here: The synthetic data generation methods aim to overcome the lack of in-domain training data by leveraging models trained on large datasets like MS MARCO.
  - Quick check question: What are the key challenges in zero-shot transfer learning for information retrieval tasks?

## Architecture Onboarding

- Component map: generate → filter → generate_triples → train → rerank → evaluate
- Critical path: The primary workflow is generate → filter → generate_triples → train → rerank → evaluate. This represents the end-to-end pipeline from synthetic data generation to final evaluation.
- Design tradeoffs: The toolkit prioritizes reproducibility and accessibility over supporting all possible variations of the original methods (e.g., Promptagator's consistency filtering is not yet implemented). It uses widely-adopted libraries (Pyserini, ir_datasets) to ensure compatibility with the broader research community.
- Failure signatures: Common issues include insufficient GPU memory for large models, incorrect dataset configuration leading to failed data loading, and prompt template mismatches causing poor query generation quality. The filtering step may produce too few or too many examples if parameters are misconfigured.
- First 3 experiments:
  1. Run the generate command with the default InPars prompt on a small dataset like NQ to verify basic functionality and inspect the generated JSONL output format.
  2. Apply the scores filtering strategy to the generated data and verify that the top-K filtering works correctly by checking the output file size and inspecting a few examples.
  3. Train a monoT5-small model using the filtered data and evaluate it on a small test set to confirm the end-to-end pipeline works before scaling to larger models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of synthetic data generated by different LLMs (e.g., GPT-J vs proprietary models) compare in terms of downstream IR performance?
- Basis in paper: [explicit] The paper discusses using GPT-J and mentions the possibility of integrating other open-source LLMs, including instruction-finetuned models, to enhance the generation process.
- Why unresolved: The paper focuses on GPT-J but does not provide a direct comparison with other LLMs or their impact on IR task performance.
- What evidence would resolve it: Systematic experiments comparing synthetic data quality and IR performance across multiple LLMs under controlled conditions.

### Open Question 2
- Question: What is the impact of prompt engineering techniques (e.g., chain-of-thought prompting) on the quality and diversity of synthetic queries?
- Basis in paper: [explicit] The paper mentions future exploration of different prompting techniques, such as chain-of-thought prompting, to experiment with synthetic data generation.
- Why unresolved: The current implementation uses standard prompt templates, and the effects of advanced prompting techniques are not explored.
- What evidence would resolve it: Comparative studies showing IR performance differences when using various prompt engineering methods for synthetic data generation.

### Open Question 3
- Question: How does the consistency filtering strategy used in Promptagator compare to other filtering methods in terms of computational cost and quality of synthetic data?
- Basis in paper: [explicit] The paper notes that Promptagator's consistency filtering is more elaborate and computationally intensive, and it is not currently supported in the toolkit.
- Why unresolved: The paper does not implement or evaluate the consistency filtering strategy, leaving its effectiveness compared to other methods untested.
- What evidence would resolve it: Empirical evaluation of different filtering strategies, including consistency filtering, in terms of both computational efficiency and downstream IR performance.

## Limitations
- Promptagator's consistency filtering is not implemented, limiting faithful reproduction of all original methods
- Minor performance variations between GPU and TPU training are noted but not quantified or explained
- Long-term sustainability concerns regarding future LLM updates and changing IR benchmark standards are not addressed

## Confidence
- Reproducibility claims: Medium - Toolkit provides unified infrastructure but missing Promptagator consistency filtering
- Performance parity claims: Medium - Comparable nDCG@10 scores reported but minor variations noted without quantification
- Public data availability: High - Concrete provision of 2,000+ GPU hours of data and trained models
- Long-term sustainability: Low - No discussion of handling future LLM updates or benchmark changes

## Next Checks
1. Reproduce the exact nDCG@10 scores: Using the provided synthetic data and trained models, run the evaluation pipeline on at least three BEIR datasets (one from each variation group mentioned: NFCorpus, TREC-COVID, and BioASQ) to verify the claimed performance parity between GPU and TPU training.

2. Test Promptagator filtering implementation: Implement and test the missing consistency filtering strategy from Promptagator to verify whether its absence significantly impacts downstream performance compared to the scores filtering approach.

3. Stress test memory constraints: Train the largest available model (presumably monoT5-large or larger) on the full NQ dataset synthetic data to identify GPU memory bottlenecks and document the maximum feasible model size under typical research hardware constraints.