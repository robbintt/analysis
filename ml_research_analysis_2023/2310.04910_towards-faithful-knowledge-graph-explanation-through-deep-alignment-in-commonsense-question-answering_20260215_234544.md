---
ver: rpa2
title: Towards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense
  Question Answering
arxiv_id: '2310.04910'
source_url: https://arxiv.org/abs/2310.04910
tags:
- graph
- encoder
- knowledge
- fidelity
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of generating faithful explanations
  in commonsense question answering (QA) models that combine language models (LMs)
  with knowledge graphs (KGs). The authors identify that current methods often fail
  to ensure the faithfulness of generated explanations, leading to a divergence between
  graph encoder outputs and model predictions.
---

# Towards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense Question Answering

## Quick Facts
- arXiv ID: 2310.04910
- Source URL: https://arxiv.org/abs/2310.04910
- Reference count: 11
- Primary result: LM-KG Distribution-aware Alignment (LKDA) achieves Graph Fidelity scores above 95% on CommonsenseQA and OpenBookQA datasets

## Executive Summary
This paper addresses the challenge of generating faithful explanations in commonsense question answering (QA) models that combine language models with knowledge graphs. The authors identify that current methods often fail to ensure the faithfulness of generated explanations, leading to a divergence between graph encoder outputs and model predictions. They introduce the LM-KG Fidelity metric to assess KG representation reliability and propose the LM-KG Distribution-aware Alignment (LKDA) algorithm to improve explanation faithfulness. The approach is evaluated on CommonsenseQA and OpenBookQA datasets, demonstrating significant improvements in both explanation fidelity and model performance.

## Method Summary
The paper proposes a framework for improving faithful knowledge graph explanations in commonsense QA by introducing a consistency regularisation term during training. The LM-KG Distribution-aware Alignment (LKDA) algorithm enhances the traditional cross-entropy loss with a consistency factor that uses KL divergence between the original model predictions and a masked version where the text encoder is removed. This ensures the graph encoder's outputs align closely with the overall model's predictions. The approach is evaluated using Graph Fidelity (measuring prediction overlap between original and masked models) and Graph Consistency (measuring distributional divergence using Jensen-Shannon divergence) metrics on CommonsenseQA and OpenBookQA datasets.

## Key Results
- LKDA achieves Graph Fidelity scores above 95% on both CommonsenseQA and OpenBookQA datasets
- The Consistent GNN (CGNN) training scheme reduces Graph Consistency scores, indicating improved alignment between graph encoder outputs and overall model predictions
- Masked models trained with CGNN exhibit only minor drops or slight improvements in accuracy compared to unmasked versions
- LKDA improves both explanation faithfulness and overall model performance compared to baseline approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Graph Fidelity and Graph Consistency metrics effectively measure the faithfulness of knowledge graph explanations.
- **Mechanism:** These metrics quantify the alignment between graph encoder outputs and overall model predictions. Graph Fidelity measures the overlap in predictions between the original model and a masked version where the text encoder is removed. Graph Consistency uses Jensen-Shannon divergence to measure the difference between output distributions of the original and masked models.
- **Core assumption:** The graph encoder should produce outputs that are consistent with the overall model's predictions to be considered faithful.
- **Evidence anchors:**
  - [abstract]: "We introduce the LM-KG Fidelity metric to assess KG representation reliability"
  - [section]: "Graph Fidelity (GF) is defined as the overlap between the original model prediction and text-encoder-masked Multilayer Perceptron (MLP) predictions."
  - [corpus]: Weak - corpus neighbors focus on faithfulness in general but don't specifically discuss Graph Fidelity or Graph Consistency metrics.
- **Break condition:** If the graph encoder's role in reasoning is fundamentally different from the overall model, or if the model architecture doesn't allow for meaningful masking of the text encoder.

### Mechanism 2
- **Claim:** The Consistent GNN (CGNN) training scheme improves explanation faithfulness by aligning graph encoder outputs with overall model predictions.
- **Mechanism:** CGNN adds a consistency regularization term to the loss function, combining cross-entropy loss with KL divergence between the original model and masked model outputs. This encourages the graph encoder to produce outputs consistent with the overall model even when the text encoder is masked.
- **Core assumption:** Regularizing the graph encoder to match overall model outputs will result in more faithful explanations.
- **Evidence anchors:**
  - [abstract]: "propose the LM-KG Distribution-aware Alignment (LKDA) algorithm to improve explanation faithfulness"
  - [section]: "CGNN enhances the traditional cross-entropy loss by introducing a consistency factor...ensures the graph encoder's outputs align closely with the original model's predictions."
  - [corpus]: Weak - corpus neighbors discuss faithfulness improvements but don't specifically address CGNN or similar consistency regularization approaches.
- **Break condition:** If the regularization term over-constrains the model, preventing it from learning useful representations, or if the KL divergence measurement is unreliable.

### Mechanism 3
- **Claim:** Graph explanations are more faithful when the graph encoder has greater influence on final predictions.
- **Mechanism:** By masking the text encoder and measuring performance, the paper shows that when the graph encoder can maintain accuracy despite the masking, it indicates the graph's explanations are more representative of the actual reasoning process.
- **Core assumption:** The degree to which graph encoder outputs alone can drive accurate predictions reflects the faithfulness of graph-based explanations.
- **Evidence anchors:**
  - [section]: "Masking here can be equivalently thought of as adding a certain type of noise when prediction, it contains at best minimal useful information for answering the question correctly."
  - [section]: "Table 4 shows the accuracy of models trained with the new CGNN scheme, which incorporates a consistency regularisation term. Equipped with CGNN, the masked models...exhibit only minor drops or even slight improvements (e.g. GreaseLMM ask with ↑ 0.2% on OBQA Test split) in accuracy compared to the unmasked versions."
  - [corpus]: Weak - corpus neighbors discuss explanation faithfulness but don't specifically address the relationship between explanation faithfulness and the graph encoder's influence on final predictions.
- **Break condition:** If the text encoder and graph encoder roles are fundamentally asymmetric and cannot be meaningfully compared, or if masking the text encoder introduces artifacts that don't reflect the true reasoning process.

## Foundational Learning

- **Concept:** Knowledge Graph (KG) structure and representation
  - Why needed here: The paper operates on knowledge graphs, so understanding their structure (triplets, nodes, edges) and how they're represented in models is crucial for grasping the methodology.
  - Quick check question: What are the typical components of a knowledge graph triplet, and how are they used in question answering models?

- **Concept:** Graph Neural Networks (GNNs) and message passing
  - Why needed here: The paper uses GNNs to encode the knowledge graph, so understanding how GNNs operate (message passing, node embeddings, relational information) is essential for understanding the approach.
  - Quick check question: How does message passing in GNNs allow information to propagate through a graph structure?

- **Concept:** Attention mechanisms and explainability
  - Why needed here: The paper discusses using attention weights as explanations and their limitations, so understanding how attention works and its role in model interpretability is important.
  - Quick check question: Why are attention weights often used as explanations in NLP models, and what are the known limitations of this approach?

## Architecture Onboarding

- **Component map:** Text → Text Encoder → Fusion → MLP → Prediction; Graph → Graph Encoder → Fusion → MLP → Prediction
- **Critical path:** Text → Text Encoder → Fusion → MLP → Prediction (primary path); Graph → Graph Encoder → Fusion → MLP → Prediction (secondary path for explanations)
- **Design tradeoffs:**
  - Fusion complexity vs. interpretability: More complex fusion may improve performance but make explanations harder to interpret
  - Graph encoder depth vs. faithfulness: Deeper encoders may capture more complex reasoning but could diverge from overall model reasoning
  - Regularization strength in CGNN: Too weak won't improve faithfulness; too strong may harm performance
- **Failure signatures:**
  - Low Graph Fidelity scores: Graph explanations don't reflect actual reasoning
  - High Graph Consistency scores: Graph encoder outputs diverge significantly from overall model predictions
  - Masked model performance drop: Text encoder contributes significantly, limiting graph explanation faithfulness
- **First 3 experiments:**
  1. Measure baseline Graph Fidelity and Graph Consistency on a chosen model to establish current faithfulness levels
  2. Apply CGNN training and measure changes in both metrics to verify improvement
  3. Compare explanation faithfulness before and after CGNN training using qualitative analysis of generated explanations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of knowledge graph structures and representations impact the faithfulness of explanations in commonsense question answering models?
- Basis in paper: [explicit] The paper discusses the use of knowledge graphs (KGs) such as ConceptNet and mentions the importance of structured knowledge representation in enhancing model performance.
- Why unresolved: The paper focuses on the faithfulness of explanations but does not delve into how different KG structures or representations might affect this faithfulness.
- What evidence would resolve it: Comparative studies using various KG structures and representations to measure changes in explanation faithfulness.

### Open Question 2
- Question: What are the potential trade-offs between model performance and explanation faithfulness when applying the Consistent GNN (CGNN) training scheme?
- Basis in paper: [explicit] The paper notes that CGNN improves graph fidelity and consistency but mentions a slight decrease in accuracy for some models.
- Why unresolved: The paper acknowledges a trade-off but does not explore the extent or implications of this trade-off in detail.
- What evidence would resolve it: Detailed experiments and analyses showing the impact of CGNN on both performance and faithfulness across various datasets and model architectures.

### Open Question 3
- Question: How can the proposed Graph Fidelity and Graph Consistency metrics be generalized to evaluate faithfulness in other types of explainable AI models beyond commonsense question answering?
- Basis in paper: [explicit] The paper introduces these metrics specifically for evaluating KG-based explanations in commonsense QA models.
- Why unresolved: The paper does not explore the applicability of these metrics to other domains or types of models.
- What evidence would resolve it: Studies applying these metrics to different AI models and domains to assess their generalizability and effectiveness.

## Limitations
- The paper relies on synthetic metrics (Graph Fidelity and Graph Consistency) that measure distributional alignment rather than human-verified faithfulness
- Results are limited to specific datasets (CommonsenseQA, OpenBookQA) and knowledge graph (ConceptNet), raising questions about generalizability
- The relationship between metric improvements and actual human interpretability remains unclear without human evaluation studies

## Confidence
- **Medium confidence** in the core claim that CGNN improves explanation faithfulness, based on the reported metric improvements and ablation studies showing masked model performance maintenance
- **Medium confidence** in the proposed LM-KG Fidelity metric as a valid measure of faithfulness, though its correlation with human judgment is not established
- **Low confidence** in the generalizability of results beyond the specific datasets and knowledge graph used (CommonsenseQA, OpenBookQA, ConceptNet)

## Next Checks
1. Conduct a human evaluation study comparing explanations generated by baseline vs. CGNN-trained models to validate whether metric improvements correspond to increased human interpretability
2. Test the approach on a different knowledge graph (e.g., Wikidata) and QA dataset to assess generalizability beyond ConceptNet-based tasks
3. Perform an ablation study isolating the contribution of each component in the LKDA algorithm to identify which elements are essential for the reported improvements