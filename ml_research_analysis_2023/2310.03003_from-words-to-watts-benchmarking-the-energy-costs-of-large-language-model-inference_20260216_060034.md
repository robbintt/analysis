---
ver: rpa2
title: 'From Words to Watts: Benchmarking the Energy Costs of Large Language Model
  Inference'
arxiv_id: '2310.03003'
source_url: https://arxiv.org/abs/2310.03003
tags:
- energy
- inference
- llama
- gpus
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks the energy costs of large language model
  (LLM) inference across different model sizes, datasets, and GPU configurations.
  The authors conduct experiments on LLaMA, a state-of-the-art LLM, using NVIDIA V100
  and A100 GPUs with datasets Alpaca and GSM8K.
---

# From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference

## Quick Facts
- arXiv ID: 2310.03003
- Source URL: https://arxiv.org/abs/2310.03003
- Reference count: 33
- Key outcome: Energy benchmarking of LLM inference across model sizes, datasets, and GPU configurations

## Executive Summary
This paper benchmarks the energy costs of large language model inference across different model sizes, datasets, and GPU configurations. The authors conduct experiments on LLaMA models using NVIDIA V100 and A100 GPUs with datasets Alpaca and GSM8K. They measure inference performance and energy usage across multi-node, multi-GPU setups with model sharding, finding that A100 GPUs provide speedups over V100 but at higher energy cost, and that power capping can significantly reduce energy consumption with minimal performance impact.

## Method Summary
The study benchmarks energy costs of large language model inference using LLaMA models (7B, 13B, 65B parameters) on NVIDIA V100 and A100 GPUs. Experiments use multi-node, multi-GPU setups with model sharding across up to 32 GPUs, measuring performance (words/tokens/responses per second) and energy usage (Watts, Joules) using nvidia-smi and NVIDIA DCGM utilities. The study varies batch sizes, number of shards, and maximum generation lengths, with datasets including Alpaca (instruction following) and GSM8K (math problems).

## Key Results
- A100 GPUs provide 2x-1.25x speedup over V100 for smaller models but minimal gains for LLaMA 65B due to communication overhead
- Energy per second increases with more shards and batch sizes, with inference costs ranging from 300W to 1kW
- Power capping at 175W reduces energy by 23% with only 6.7% increase in inference time
- GPU utilization is high (94-98%) but memory utilization is low (23-27%), suggesting opportunities for model co-location

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-GPU sharding increases total energy consumption per second even when improving throughput.
- Mechanism: As more GPUs are used to shard a large model, total GPU power draw increases linearly with GPU count, outweighing per-GPU efficiency gains.
- Core assumption: Energy consumption scales with number of active GPUs regardless of utilization efficiency.
- Evidence anchors:
  - [section]: "Overall, we see an average increase in energy per second with the number of shards... the energy per second increases with the number of shards even at the same batch size."
  - [corpus]: Weak correlation between sharding and energy efficiency observed in similar benchmarking studies.
- Break condition: If sharding strategies achieve near-perfect computational efficiency such that additional GPUs provide disproportionately high throughput gains, the energy per second might decrease.

### Mechanism 2
- Claim: A100 GPUs provide higher throughput but at significantly higher energy cost compared to V100 GPUs for smaller models.
- Mechanism: A100's architectural improvements (e.g., higher clock speeds, more CUDA cores) increase computational throughput, but also increase power draw, leading to higher energy consumption per second.
- Core assumption: The performance gains of newer GPU architectures come with proportional increases in power consumption.
- Evidence anchors:
  - [section]: "we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100... there is a considerable increase in the energy per second across all LLaMA sizes when using the A100 over the V100."
  - [corpus]: General trend in GPU architecture where performance improvements correlate with higher energy consumption.
- Break condition: If future GPU architectures decouple performance gains from power consumption through architectural innovations like better power gating or more efficient cores.

### Mechanism 3
- Claim: GPU power capping can reduce energy consumption with minimal impact on inference time for moderate reductions.
- Mechanism: Limiting GPU power draw forces the GPU to operate at lower frequencies or voltages, reducing energy consumption while maintaining acceptable performance levels.
- Core assumption: There is a non-linear relationship between power, performance, and energy where moderate power reductions can yield significant energy savings without proportionally large performance losses.
- Evidence anchors:
  - [section]: "a 30% reduction in power from 250W to 175W, the inference time increases by an average of 6.7% for a corresponding average reduction in total energy by 23.21%."
  - [corpus]: Power capping is a recognized technique for energy optimization in GPU workloads, as evidenced by prior studies on training efficiency.
- Break condition: If the workload is already highly optimized or if the power cap is set too low, causing disproportionate increases in inference time.

## Foundational Learning

- Concept: Distributed model sharding
  - Why needed here: Understanding how models are partitioned across multiple GPUs is crucial for interpreting energy and performance metrics in multi-GPU setups.
  - Quick check question: What happens to memory utilization and communication overhead as the number of shards increases?

- Concept: GPU power management and capping
  - Why needed here: The study's findings on power capping's impact on energy and performance require understanding how GPUs dynamically adjust power states.
  - Quick check question: How does reducing the power cap affect GPU clock speeds and, consequently, inference throughput?

- Concept: Energy measurement in high-performance computing
  - Why needed here: Accurate interpretation of energy metrics (Joules, Watts) and their relationship to performance metrics is essential for evaluating the study's conclusions.
  - Quick check question: How is energy per token calculated, and what does it reveal about model efficiency?

## Architecture Onboarding

- Component map:
  - LLaMA models (7B, 13B, 65B) -> NVIDIA V100/A100 GPUs (32GB/80GB) -> PyTorch with FairScale for model sharding -> nvidia-smi and NVIDIA DCGM for monitoring -> Alpaca and GSM8K datasets

- Critical path:
  1. Load and shard the model across available GPUs
  2. Process input data through the model to generate outputs
  3. Measure performance (tokens/words/responses per second) and energy consumption
  4. Analyze the impact of varying batch sizes, shard counts, and power caps

- Design tradeoffs:
  - Throughput vs. Energy: Higher throughput often comes at the cost of increased energy consumption
  - Model Size vs. Hardware Requirements: Larger models require more GPUs and memory, increasing energy costs
  - Batch Size vs. Latency: Larger batch sizes can improve throughput but may increase latency and energy per token

- Failure signatures:
  - Memory errors when attempting to load models that exceed GPU memory capacity
  - Communication bottlenecks in multi-GPU setups leading to suboptimal performance gains
  - Unexpected increases in energy consumption due to inefficient sharding or power management

- First 3 experiments:
  1. Single GPU inference with LLaMA 7B to establish baseline performance and energy metrics
  2. Multi-GPU inference with LLaMA 65B using 8 V100 GPUs to study the impact of sharding on performance and energy
  3. Power capping experiment with LLaMA 65B on A100 GPUs to evaluate energy savings vs. performance trade-offs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different model quantization and distillation techniques affect the energy consumption and inference performance of large language models?
- Basis in paper: [explicit] The paper mentions that quantization, distillation, and sparsification are being developed to reduce the compute required for AI, but does not evaluate their impact on energy consumption and inference performance.
- Why unresolved: The paper focuses on benchmarking LLaMA with its original model architecture and does not explore the effects of various optimization techniques.
- What evidence would resolve it: Experiments comparing energy consumption and inference performance of LLaMA with and without quantization, distillation, or sparsification techniques applied.

### Open Question 2
- Question: How does the complexity of input data, such as the difference between natural language and mathematical problems, affect the energy consumption and inference performance of large language models?
- Basis in paper: [explicit] The paper uses two datasets, Alpaca (natural language) and GSM8K (mathematical problems), and finds that the complexity of the input dataset can affect the model performance for a given set of hyperparameters and hardware configuration.
- Why unresolved: The paper does not provide a detailed analysis of how the complexity of input data impacts energy consumption and inference performance across different types of tasks or datasets.
- What evidence would resolve it: A comprehensive study comparing energy consumption and inference performance of LLaMA on a diverse set of datasets with varying levels of complexity, such as natural language, mathematical problems, code generation, and image understanding.

### Open Question 3
- Question: How does power capping affect the energy consumption and inference performance of large language models, and what is the optimal power cap setting for different tasks and hardware configurations?
- Basis in paper: [explicit] The paper conducts a limited set of experiments with power capping on LLaMA 65B and finds that a 30% reduction in power from 250W to 175W results in a 6.7% increase in inference time and a 23.21% reduction in energy consumption.
- Why unresolved: The paper does not provide a comprehensive analysis of the effects of power capping on different tasks, hardware configurations, and model sizes, nor does it identify the optimal power cap settings for each scenario.
- What evidence would resolve it: A detailed study comparing energy consumption and inference performance of LLaMA under various power cap settings (e.g., 150W, 175W, 200W, 250W) across different tasks, hardware configurations, and model sizes.

## Limitations
- Focus on LLaMA models may not generalize to all LLM architectures or newer models
- Energy measurements conducted on specific GPU hardware in controlled HPC environment
- Analysis does not account for end-to-end energy costs like data preprocessing or cooling systems
- Power capping experiments limited to single reduction level

## Confidence
- High Confidence: Findings on multi-GPU sharding increasing total energy consumption
- Medium Confidence: Claims about A100 vs. V100 energy-performance trade-offs
- Medium Confidence: Potential for model co-location due to low memory utilization

## Next Checks
1. Replicate experiments with other LLM architectures (e.g., OPT, BLOOM) to verify findings hold beyond LLaMA models
2. Conduct energy benchmarking in cloud or edge environments to assess applicability outside of HPC systems
3. Explore a broader range of power cap settings to identify optimal energy-performance trade-offs for different workloads