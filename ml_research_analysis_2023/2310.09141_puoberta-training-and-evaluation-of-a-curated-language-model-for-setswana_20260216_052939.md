---
ver: rpa2
title: 'PuoBERTa: Training and evaluation of a curated language model for Setswana'
arxiv_id: '2310.09141'
source_url: https://arxiv.org/abs/2310.09141
tags:
- setswana
- news
- language
- puoberta
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PuoBERTa, a custom BERT-based language model, was developed for
  Setswana, a low-resource Bantu language. PuoBERTa was trained on a curated monolingual
  corpus (PuoData) of diverse text sources, including government documents, books,
  and online content.
---

# PuoBERTa: Training and evaluation of a curated language model for Setswana

## Quick Facts
- arXiv ID: 2310.09141
- Source URL: https://arxiv.org/abs/2310.09141
- Reference count: 40
- Monolingual BERT-based model for Setswana achieves state-of-the-art results on POS tagging and news categorization, with competitive performance on NER

## Executive Summary
This paper presents PuoBERTa, a custom BERT-based language model developed for Setswana, a low-resource Bantu language. The model was trained on a curated monolingual corpus (PuoData) of diverse text sources including government documents, books, and online content. PuoBERTa was evaluated on three NLP tasks: named entity recognition (NER), part-of-speech (POS) tagging, and news categorization. The results show state-of-the-art performance on POS tagging and news categorization among monolingual models, and competitive performance on NER, demonstrating the effectiveness of curated, diverse training data for low-resource languages.

## Method Summary
The authors developed PuoBERTa by first curating a monolingual corpus (PuoData) from diverse sources including government documents, books, and online content. They trained two BPE tokenizers (52k tokens each) using PuoData and PuoData+JW300. The model was pre-trained using masked language modeling for 100 epochs (PuoBERTa) and 40 epochs (PuoBERTaJW300) on an NVIDIA Titan RTX GPU. The pre-trained models were then fine-tuned on downstream tasks including MasakhaNER 2.0, MasakhaPOS, and a newly created Setswana news categorization dataset using Hugging Face transformers library.

## Key Results
- PuoBERTa achieved state-of-the-art results on POS tagging and news categorization among monolingual models
- The inclusion of additional data (PuoData+JW300) further improved model performance
- PuoBERTa showed competitive performance on named entity recognition compared to multilingual models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PuoBERTa's competitive performance is driven by the diversity and quality of its curated training corpus (PuoData), which includes multiple domains like government documents, books, and online content.
- Mechanism: By exposing the model to varied linguistic styles and topics during pre-training, it learns robust representations that generalize well to downstream tasks like NER, POS tagging, and news categorization.
- Core assumption: Diverse, high-quality monolingual data compensates for the lack of scale in low-resource languages.
- Evidence anchors:
  - [abstract]: "PuoBERTa was trained on a curated monolingual corpus (PuoData) of diverse text sources, including government documents, books, and online content."
  - [section]: "The data was includes data from research organisations, books, official government documents, and online content. The final dataset is referred to as PuoData."
- Break condition: If the corpus lacks domain coverage relevant to the downstream tasks, performance will degrade.

### Mechanism 2
- Claim: Including additional data (PuoData+JW300) improves model performance by increasing the volume and breadth of linguistic patterns available for learning.
- Mechanism: More data allows the model to better capture morphological richness and syntactic structures typical of Bantu languages, improving fine-tuning results on tasks like POS tagging.
- Core assumption: In low-resource settings, more data (even if domain-specific like religious texts) still contributes positively to overall language understanding.
- Evidence anchors:
  - [abstract]: "The inclusion of additional data (PuoData+JW300) further improved model performance, highlighting the importance of diverse and abundant training data for low-resource languages."
  - [section]: "PuoData+JW300 is a larger dataset than PuoData alone... It contains more text... than all the other datasets combined."
- Break condition: If additional data introduces significant domain shift or noise, the benefits may plateau or reverse.

### Mechanism 3
- Claim: Monolingual models like PuoBERTa can outperform or match multilingual models on low-resource language tasks by focusing learning capacity on a single language's specifics.
- Mechanism: By not sharing parameters across many languages, the model can allocate more capacity to capturing unique linguistic features of Setswana, leading to better fine-tuning outcomes.
- Core assumption: The benefits of language-specific representation learning outweigh the generalization advantages of multilingual models in low-resource contexts.
- Evidence anchors:
  - [abstract]: "PuoBERTa achieved state-of-the-art results on POS tagging and news categorization among monolingual models, and competitive performance on NER."
  - [section]: "It does not beat the multilingual models in this case. PuoBERTa+JW300 gets much closer than the rest of the monolingual models."
- Break condition: If the monolingual data is too small or of poor quality, multilingual models may still dominate due to their larger pretraining corpora.

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: MLM is the core pretraining objective for BERT-based models, enabling them to learn contextual word representations by predicting masked tokens.
  - Quick check question: What is the purpose of masking tokens during pretraining, and how does it help downstream tasks?

- Concept: Subword Tokenization (BPE)
  - Why needed here: Byte-Pair Encoding (BPE) tokenization is used to handle the morphological richness of Bantu languages by breaking words into frequent subword units, improving vocabulary coverage.
  - Quick check question: How does BPE tokenization help with handling out-of-vocabulary words in morphologically rich languages?

- Concept: Fine-tuning vs. Feature Extraction
  - Why needed here: Fine-tuning adapts the entire model to a specific downstream task, which is critical for achieving high performance on tasks like NER and POS tagging.
  - Quick check question: What is the difference between fine-tuning a pretrained model and using it as a fixed feature extractor?

## Architecture Onboarding

- Component map: PuoData corpus -> BPE Tokenizer -> Masked Language Modeling pretraining -> Fine-tuning modules for NER, POS, classification
- Critical path: Data curation → Tokenizer training → Pretraining (MLM) → Fine-tuning on downstream tasks → Evaluation. Each step must maintain data quality and consistency.
- Design tradeoffs: Using a monolingual model sacrifices multilingual generalization for better language-specific performance. Larger models may improve accuracy but increase computational cost, which is a concern in low-resource settings.
- Failure signatures: Poor performance on downstream tasks may indicate insufficient or biased training data, tokenization issues with Setswana morphology, or inadequate fine-tuning epochs. Confusion in classification tasks may suggest overlapping category definitions.
- First 3 experiments:
  1. Evaluate the tokenizer on unseen Setswana text to check if it splits words appropriately and handles morphological variants.
  2. Fine-tune PuoBERTa on a small POS tagging dataset and compare against a multilingual baseline to measure language-specific gains.
  3. Run ablation tests by training with subsets of PuoData (e.g., only government docs vs. only books) to identify which data sources most impact performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PuoBERTa compare to larger multilingual models like AfroXLMR-large when using the same evaluation datasets?
- Basis in paper: [explicit] The paper compares PuoBERTa's performance to multilingual models like AfroXLMR-large on the MasakhaNER and MasakhaPOS datasets, but does not use identical evaluation settings.
- Why unresolved: Different evaluation settings (e.g. train/dev/test splits) prevent a direct comparison of model capabilities.
- What evidence would resolve it: Retraining and evaluating PuoBERTa and AfroXLMR-large on identical datasets and splits.

### Open Question 2
- Question: What is the impact of the religious content in JW300 on the downstream task performance of PuoBERTaJW300?
- Basis in paper: [inferred] The paper notes that JW300 is "religious in nature" but does not analyze the impact of this domain-specific content.
- Why unresolved: The paper does not investigate how the religious domain of JW300 affects model performance on general NLP tasks.
- What evidence would resolve it: Ablation studies comparing PuoBERTaJW300 to models trained on PuoData plus non-religious corpora of similar size.

### Open Question 3
- Question: How does the tokenization strategy (BPE with 52000 tokens) impact the performance of PuoBERTa compared to other tokenization approaches?
- Basis in paper: [explicit] The paper states that PuoBERTa uses BPE tokenization with 52000 tokens, but does not compare this to other tokenization strategies.
- Why unresolved: The paper does not explore the impact of different tokenization approaches on model performance.
- What evidence would resolve it: Evaluating PuoBERTa with different tokenization strategies (e.g. WordPiece, SentencePiece) on the same downstream tasks.

## Limitations

- Lack of direct comparisons with other monolingual models on the same datasets limits the strength of claims about state-of-the-art performance
- Absence of comprehensive ablation studies on data sources and model size leaves uncertainty about which factors drive performance
- Focus solely on Setswana without examining generalization to other Bantu languages limits broader applicability claims

## Confidence

High confidence: Claims about PuoBERTa's competitive performance on POS tagging and news categorization among monolingual models, and the general benefit of including additional training data (PuoData+JW300) are well-supported by the reported F1 scores and ablation results.

Medium confidence: The assertion that PuoBERTa's performance is primarily driven by the diversity and quality of the curated corpus (PuoData) is plausible but lacks direct experimental validation through controlled ablation studies on data sources.

Low confidence: The claim that monolingual models inherently outperform multilingual models for low-resource languages is not fully supported, as PuoBERTa's NER performance was only competitive rather than superior to multilingual baselines.

## Next Checks

1. Conduct ablation studies to isolate the contribution of each data source in PuoData (government documents, books, online content, NCHLT corpus) to downstream task performance, controlling for corpus size to validate the diversity hypothesis.

2. Compare PuoBERTa against other monolingual BERT models trained on Setswana with different data selection strategies (e.g., random sampling vs. curated selection) using identical downstream evaluation protocols to test the curated corpus claim.

3. Perform cross-linguistic evaluation by training similar monolingual models for related Bantu languages (e.g., Sesotho, isiZulu) and testing their transferability to Setswana tasks to assess the broader applicability of the monolingual approach.