---
ver: rpa2
title: LTLf Best-Effort Synthesis in Nondeterministic Planning Domains
arxiv_id: '2308.15188'
source_url: https://arxiv.org/abs/2308.15188
tags:
- agent
- strategy
- best-effort
- planning
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel approach to LTLf best-effort synthesis
  in nondeterministic planning domains, addressing the challenge of synthesizing strategies
  when no agent strategy exists that fulfills the goal against every possible environment
  reaction. The method exploits the specificity of nondeterministic planning domains
  by using a game-theoretic technique that reduces the synthesis problem to DFA games,
  specifically targeting adversarial and cooperative reachability and safety.
---

# LTLf Best-Effort Synthesis in Nondeterministic Planning Domains

## Quick Facts
- arXiv ID: 2308.15188
- Source URL: https://arxiv.org/abs/2308.15188
- Authors: 
- Reference count: 33
- Key outcome: Proposed method handles up to 1000 locations vs 6 locations for direct approach, with only 10-15% overhead for best-effort solutions.

## Executive Summary
This paper addresses LTLf best-effort synthesis in nondeterministic planning domains, where no agent strategy exists that fulfills the goal against every possible environment reaction. The authors introduce a game-theoretic approach that reduces the synthesis problem to DFA games, exploiting the structure of planning domains to avoid full LTLf environment encoding. The method computes strategies that fulfill the goal if possible and do their best otherwise, significantly improving scalability compared to direct best-effort synthesis approaches.

## Method Summary
The method transforms a nondeterministic planning domain into a deterministic transition system D+ and composes it with the DFA of the LTLf goal φ to form a game arena T. It then solves two DFA games: one for adversarial reachability (Wadv) and one for cooperative reachability (Wcoop). The final strategy is a hybrid of both, ensuring maximal goal achievement. The approach uses symbolic BDD representations for scalability and assumes the planning domain satisfies existence of actions, reactions, and uniqueness of reactions.

## Key Results
- Handles instances with up to 1000 locations versus 6 locations for direct approach
- Only 10-15% overhead for computing best-effort solutions compared to strong/cooperative solutions
- Experimental validation across various domain sizes (1-8 objects, 1-1000 locations)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The synthesis algorithm reduces best-effort planning to two simpler DFA games, leveraging the structure of nondeterministic planning domains to avoid full LTLf environment encoding.
- Mechanism: The method constructs a deterministic transition system D+ from the planning domain and composes it with the DFA of the LTLf goal φ to form a game arena T. It then solves two DFA games: one for adversarial reachability (Wadv) and one for cooperative reachability (Wcoop). The final strategy is a hybrid of both, ensuring maximal goal achievement.
- Core assumption: The planning domain satisfies existence of actions, existence of reactions, and uniqueness of reactions.
- Evidence anchors:
  - [abstract] "Our method exploits the specificity of nondeterministic planning domains by using a game-theoretic technique that reduces the synthesis problem to DFA games"
  - [section] "We use DFA games for adversarial/cooperative reachability and safety to capture the environment being adversarial/cooperative and agent always following its action preconditions"
- Break condition: If the domain violates uniqueness of environment reactions, the construction of D+ fails, breaking the reduction to DFA games.

### Mechanism 2
- Claim: The strategy computation guarantees that every history consistent with the computed strategy is maximally valuable.
- Mechanism: For each history h, the algorithm assigns a value: +1 (winning), -1 (losing), or 0 (pending). It then ensures that the computed strategy achieves the maximum possible value for every history by combining adversarial and cooperative winning regions.
- Core assumption: A best-effort solution can be computed by maximizing the value over all consistent histories (Theorem 2, Maximality Condition).
- Evidence anchors:
  - [abstract] "Such strategies fulfill the goal if possible, and do their best to do so otherwise"
  - [section] "The computed final strategy σ by combing κadv and κcoop is a best-effort solution for φ in D"
- Break condition: If the domain is too large (exponential blowup in DFA size), the symbolic representation in BDDs may fail, preventing strategy computation.

### Mechanism 3
- Claim: Symbolic representation of DFAs and planning domains enables scalable computation of best-effort strategies.
- Mechanism: The tool BeSyftP uses BDDs to represent DFAs and planning domains, allowing efficient manipulation of large state spaces via Boolean operations. This symbolic encoding is critical for handling instances with up to 1000 locations.
- Core assumption: The symbolic DFA encoding proposed in [32] and the symbolic domain encoding from [26] are correct and efficient for BDD-based operations.
- Evidence anchors:
  - [abstract] "Experimental results demonstrate that the proposed method can handle instances with up to 1000 locations"
  - [section] "BeSyftP uses the symbolic DFA encoding proposed in [32] to represent the DFAs symbolically and integrates the symbolic encoding of a planning domain... for symbolic domain representation"
- Break condition: If the BDD size exceeds available memory, the symbolic computation fails, and the method cannot scale further.

## Foundational Learning

- Concept: DFA games and their winning/cooperative winning regions
  - Why needed here: The synthesis algorithm reduces the planning problem to solving DFA games; understanding winning regions is essential to grasp why the final strategy is best-effort.
  - Quick check question: What is the difference between a winning state and a cooperatively winning state in a DFA game?

- Concept: Symbolic representation using BDDs
  - Why needed here: The implementation relies on BDDs for efficient manipulation of large transition systems; understanding BDD operations is key to knowing how scalability is achieved.
  - Quick check question: How does a BDD represent a transition function compactly compared to an explicit table?

- Concept: Nondeterministic planning domains and their properties
  - Why needed here: The algorithm assumes certain properties (existence of actions, reactions, uniqueness) to construct the deterministic transition system; knowing these ensures correct domain modeling.
  - Quick check question: Why is the uniqueness of environment reactions required for the domain-to-transition-system transformation?

## Architecture Onboarding

- Component map:
  Input: Nondeterministic planning domain (PDDL variant) + LTLf goal -> LTLf-to-DFA converter (LYDIA) -> Symbolic DFA encoder (from [32]) -> Symbolic domain encoder (from [26]) -> DFA game solver (symbolic Boolean operations, CUDD BDD library) -> Strategy combiner (Boolean synthesis) -> Output: Best-effort strategy

- Critical path:
  1. Parse domain and goal
  2. Build DFA for LTLf goal
  3. Build deterministic transition system D+
  4. Compose T = D+ ◦ DFA
  5. Solve adversarial and cooperative DFA games
  6. Combine strategies into final best-effort strategy

- Design tradeoffs:
  - Symbolic vs explicit representation: Symbolic (BDDs) scales better but may fail on very large or irregular domains.
  - Two-game solution vs single-game: Solving two simpler games is more efficient than solving one complex game with full LTLf environment encoding.

- Failure signatures:
  - Timeout or memory exhaustion during BDD operations → symbolic representation too large
  - Incorrect strategy → domain violates uniqueness of reactions or BDD encoding bug
  - No best-effort solution found → logic error in game reduction or solver

- First 3 experiments:
  1. Run on a small domain (e.g., 2 locations, 1 object) with a simple LTLf goal; verify the strategy satisfies the goal in cooperative scenarios and avoids dead ends.
  2. Increase locations to 10; check scalability and compare runtime vs the direct LTLf best-effort synthesis approach.
  3. Test with a domain where a strong solution exists; confirm the computed best-effort strategy is indeed a strong solution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method for LTLf best-effort synthesis in nondeterministic planning domains scale when the number of objects (|O|) increases beyond 8?
- Basis in paper: [explicit] The paper mentions that their approach can handle instances with up to 8 objects (1 ≤ |O| ≤ 8) and 1000 locations (1 ≤ |L| ≤ 1000), but does not provide results for more than 8 objects.
- Why unresolved: The paper does not present experimental results for scenarios with more than 8 objects, leaving the scalability of the method for larger numbers of objects unknown.
- What evidence would resolve it: Conducting experiments with more than 8 objects and comparing the results with the proposed method's performance on smaller numbers of objects would provide evidence for its scalability.

### Open Question 2
- Question: Can the game-theoretic techniques adopted in this work be extended to handle best-effort synthesis under multiple environment specifications?
- Basis in paper: [explicit] The paper mentions that the game-based techniques could possibly be extended to handle best-effort synthesis under multiple environment specifications as a promising extension for future work.
- Why unresolved: The paper does not provide any details or results on extending the game-theoretic techniques to handle multiple environment specifications, leaving the feasibility and effectiveness of such an extension unknown.
- What evidence would resolve it: Developing and testing an extended version of the proposed method that can handle multiple environment specifications would provide evidence for its feasibility and effectiveness.

### Open Question 3
- Question: How does the performance of the proposed method compare to other state-of-the-art LTLf synthesis tools when applied to nondeterministic planning domains?
- Basis in paper: [inferred] The paper presents a comparison of the proposed method with a direct best-effort synthesis approach based on re-expressing the planning domain as generic environment specifications. However, it does not compare its performance with other LTLf synthesis tools that may be applicable to nondeterministic planning domains.
- Why unresolved: The paper does not provide a comprehensive comparison of the proposed method with other state-of-the-art LTLf synthesis tools, leaving its relative performance and advantages unclear.
- What evidence would resolve it: Conducting a thorough comparison of the proposed method with other state-of-the-art LTLf synthesis tools on a variety of nondeterministic planning domain benchmarks would provide evidence for its relative performance and advantages.

## Limitations

- Scalability claims lack detailed metrics (e.g., BDD size, memory usage) to fully validate the symbolic approach for 1000 locations.
- Correctness of the two-game reduction relies on unproven assumptions about combining adversarial and cooperative strategies.
- Method assumes planning domains satisfy existence of actions, reactions, and uniqueness of reactions, which may not hold in real-world domains.

## Confidence

- **High Confidence**: The core mechanism of reducing best-effort synthesis to DFA games is well-grounded in game theory literature and the paper's formalism is consistent.
- **Medium Confidence**: The scalability claims are supported by experimental results but lack detailed metrics (e.g., BDD size, memory usage) to fully validate the symbolic approach.
- **Low Confidence**: The assumption that combining adversarial and cooperative strategies yields a globally optimal best-effort strategy is not rigorously proven, and edge cases may exist.

## Next Checks

1. **Reproduce scalability**: Run the tool on a synthetic domain with 1000 locations and measure memory usage and runtime to verify the claimed scalability.
2. **Test assumption violations**: Create a domain where uniqueness of environment reactions is violated and confirm that the method fails gracefully or reports an error.
3. **Validate strategy maximality**: For a small domain, enumerate all possible histories and verify that the computed strategy achieves the maximum possible value for each history.