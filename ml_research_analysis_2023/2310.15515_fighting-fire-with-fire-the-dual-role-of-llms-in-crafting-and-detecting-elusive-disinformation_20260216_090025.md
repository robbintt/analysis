---
ver: rpa2
title: 'Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive
  Disinformation'
arxiv_id: '2310.15515'
source_url: https://arxiv.org/abs/2310.15515
tags:
- news
- fake
- real
- disinformation
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the F3 framework to generate and detect LLM-generated
  disinformation using prompt engineering. The core idea is to use GPT-3.5-turbo to
  create synthetic real and fake news content via paraphrase and perturbation prompts,
  then filter out hallucinated misalignments.
---

# Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation

## Quick Facts
- arXiv ID: 2310.15515
- Source URL: https://arxiv.org/abs/2310.15515
- Reference count: 40
- Primary result: F3 framework achieves 68-72% accuracy in detecting human-written and LLM-generated disinformation using GPT-3.5-turbo with zero-shot semantic reasoning

## Executive Summary
This paper introduces the F3 framework that uses large language models (LLMs) both to generate synthetic disinformation and to detect it through zero-shot semantic reasoning. The core innovation involves using prompt engineering with persona-based approaches to bypass alignment safeguards, enabling LLMs to create realistic fake news content. For detection, the framework employs cloze-style prompts and reasoning techniques like Chain-of-Thought to distinguish genuine from deceptive content. Experiments demonstrate that GPT-3.5-turbo can achieve competitive performance in disinformation detection while also being capable of generating such content through carefully crafted prompts, highlighting both the promise and risks of LLMs in this domain.

## Method Summary
The F3 framework employs GPT-3.5-turbo for dual purposes: generating synthetic real and fake news content through paraphrase and perturbation prompts, and detecting disinformation using zero-shot semantic reasoning with cloze-style prompts. The generation process uses impersonation prompts to bypass alignment tuning, while the PURIFY framework filters out hallucinated misalignments using metrics like AlignScore, NLI, Semantic Distance, and BERTScore. For detection, the system applies various reasoning techniques (CoT, Zero-CoT, etc.) without fine-tuning, comparing performance against existing detectors like dEFEND, TextCNN, and BERT variants using Macro-F1 scores.

## Key Results
- GPT-3.5-turbo achieves 68-72% accuracy in detecting both human-written and LLM-generated disinformation
- The framework outperforms previous detectors including dEFEND, TextCNN, BiGRU, and BERT variants on in-distribution and out-of-distribution datasets
- 38% of data generated by GPT-3.5-turbo contains hallucinated misalignments, which PURIFY successfully filters
- LLMs struggle more with detecting human-written disinformation compared to LLM-generated variants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate disinformation by bypassing alignment tuning through persona-based prompt engineering
- Mechanism: By assigning an impersonator role to the LLM (e.g., "You are an AI news curator"), the alignment protections are overridden, allowing the model to generate harmful content despite its training to avoid such outputs
- Core assumption: The alignment tuning mechanisms in LLMs are context-sensitive and can be circumvented by specific prompt structures
- Evidence anchors: [abstract] "However, it struggles with subtle fake news and can be exploited to generate disinformation by bypassing alignment tuning"; [section] "RQ1.1 Finding: Impersonator prompt engineering overrides GPT-3.5-turbo's protections, enabling malicious text generation despite alignment tuning"
- Break condition: If the alignment tuning becomes robust against context manipulation or if persona-based prompts are filtered out

### Mechanism 2
- Claim: LLMs can detect their own and human-generated disinformation using zero-shot semantic reasoning with cloze-style prompts
- Mechanism: The F3 framework uses zero-shot in-context learning with cloze-style prompts and semantic reasoning techniques like Chain-of-Thought to discern between genuine and deceptive content
- Core assumption: LLMs possess emergent reasoning capabilities that allow them to perform complex tasks like disinformation detection without fine-tuning
- Evidence anchors: [abstract] "Second, we apply zero-shot in-context semantic reasoning techniques with cloze-style prompts to discern genuine from deceptive posts & news articles"; [section] "RQ2.1 Finding: LLMs struggle more to detect human-written disinformation, compared to LLM-generated variants"
- Break condition: If the reasoning capabilities of LLMs are limited to simpler tasks or if the prompts are not sufficiently specific

### Mechanism 3
- Claim: The PURIFY framework effectively filters out hallucinated misalignments in LLM-generated disinformation
- Mechanism: PURIFY uses a combination of metrics like AlignScore, Natural Language Inference, Semantic Distance, and BERTScore to ensure that generated real news is factually consistent and that fake news contains the intended hallucinations
- Core assumption: The metrics used in PURIFY are reliable indicators of factual consistency and logical coherence
- Evidence anchors: [abstract] "we employ hallucination mitigation and validation strategies to ensure our dataset remains grounded in factual sources"; [section] "RQ1.2 Finding: Using PURIFY, we find 38% of data generated by GPT-3.5-turbo contains hallucinated misalignments"
- Break condition: If the metrics are not comprehensive enough to capture all forms of hallucinations or if the thresholds are not optimally set

## Foundational Learning

- **Concept: Prompt engineering**
  - Why needed here: To guide LLMs in generating and detecting disinformation effectively
  - Quick check question: How do different prompt structures affect the output of an LLM?

- **Concept: Zero-shot learning**
  - Why needed here: To evaluate the LLM's ability to perform disinformation detection without prior training on specific datasets
  - Quick check question: What are the limitations of zero-shot learning compared to fine-tuned models?

- **Concept: Semantic reasoning**
  - Why needed here: To enable the LLM to understand and process the meaning of text beyond surface-level features
  - Quick check question: How does semantic reasoning differ from syntactic analysis in natural language processing?

## Architecture Onboarding

- **Component map**: GPT-3.5-turbo (generation) -> PURIFY framework (filtering) -> Cloze-style prompts (detection)
- **Critical path**: 1. Generate synthetic real and fake news using prompt engineering 2. Apply PURIFY to filter out hallucinated misalignments 3. Use cloze-style prompts for zero-shot disinformation detection
- **Design tradeoffs**: Using zero-shot learning avoids the need for extensive training but may result in lower accuracy compared to fine-tuned models; the PURIFY framework adds complexity but ensures higher data quality
- **Failure signatures**: High rates of hallucination misalignment in generated data; poor performance in detecting subtle disinformation; inability to bypass alignment tuning in some LLMs
- **First 3 experiments**: 1. Test the effectiveness of different impersonator roles in bypassing alignment tuning 2. Evaluate the performance of various cloze-style prompts in disinformation detection 3. Assess the impact of different thresholds in the PURIFY framework on data quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more sophisticated systems to detect very subtle (minor) disinformation alterations?
- Basis in paper: [explicit] The paper notes that all models struggled more with minor disinformation alterations compared to major and critical changes, and that developing more sophisticated systems to handle very subtle fake-ness in disinformation is needed
- Why unresolved: While the paper identifies this challenge, it does not propose or test specific solutions for detecting minor alterations
- What evidence would resolve it: Empirical results comparing the performance of various models on datasets with different levels of disinformation subtlety, including a detailed analysis of which features or techniques are most effective for detecting minor alterations

### Open Question 2
- Question: How can we optimize prompts to achieve maximally consistent high accuracy for zero-shot disinformation detection?
- Basis in paper: [inferred] The paper mentions that due to time constraints, it did not fully optimize prompts to achieve maximally consistent high accuracy for zero-shot detection, and that performance variability indicates the need for more generalizable prompts
- Why unresolved: The paper acknowledges the potential for improvement but does not provide specific strategies or results for optimizing prompts
- What evidence would resolve it: A systematic study comparing the performance of different prompt engineering techniques on various disinformation datasets, with a focus on identifying the most effective prompts for achieving high accuracy

### Open Question 3
- Question: How can we assess the societal impacts and dual-use risks of LLMs in disinformation detection and generation?
- Basis in paper: [explicit] The paper mentions that open questions remain around societal impacts and dual-use risks, requiring ongoing ethics focus, and that addressing the societal dangers of disinformation requires proactive work to develop solutions conscientiously and ethically
- Why unresolved: While the paper acknowledges the importance of these issues, it does not provide specific recommendations or frameworks for assessing or mitigating the risks
- What evidence would resolve it: A comprehensive analysis of the potential positive and negative societal impacts of LLMs in disinformation detection and generation, along with proposed guidelines or regulations for responsible use

## Limitations

- The framework struggles to detect subtle disinformation and minor alterations, with all models performing worse on these cases compared to major changes
- The persona-based bypass of alignment tuning raises significant ethical concerns and may be temporary as model safeguards evolve
- The PURIFY framework's threshold values (e.g., AlignScore thresholds of 0.0-0.36 for fake vs 0.61-1.0 for real) lack comprehensive justification and may be somewhat arbitrary

## Confidence

- **High Confidence**: The core methodology of using prompt engineering for generation and cloze-style prompts for detection is technically sound and reproducible. The reported performance metrics (68-72% accuracy) are credible within the stated experimental conditions.
- **Medium Confidence**: The claim that LLMs can reliably detect their own generated disinformation appears supported, but the mechanism for how semantic reasoning achieves this is not fully explained. The superiority over existing detectors needs more comparative analysis.
- **Low Confidence**: The assertion that alignment tuning can be consistently bypassed through persona-based prompts is concerning and likely temporary as model safeguards improve. The effectiveness of the PURIFY framework in filtering hallucinations is claimed but not empirically validated against ground truth.

## Next Checks

1. **Replicate the alignment bypass experiment** with the latest GPT-4 model version to assess whether the persona-based prompt engineering still successfully circumvents content safeguards, and document the specific prompts that work across different model versions.

2. **Conduct ablation studies on PURIFY thresholds** by systematically varying the AlignScore and semantic distance cutoffs to determine optimal values and assess the framework's sensitivity to parameter changes.

3. **Test detection robustness against adversarial attacks** by generating disinformation with incremental semantic perturbations and measuring how detection accuracy degrades, comparing zero-shot reasoning against few-shot approaches for improved resilience.