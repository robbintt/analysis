---
ver: rpa2
title: 'Wrapper Boxes: Faithful Attribution of Model Predictions to Training Data'
arxiv_id: '2311.08644'
source_url: https://arxiv.org/abs/2311.08644
tags:
- training
- examples
- wrapper
- explanations
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces wrapper boxes, a pipeline that wraps a trained
  neural model with interpretable classic models (kNN, SVM, decision trees, clustering)
  to provide example-based explanations for predictions while preserving predictive
  performance. The method extracts feature representations from a fine-tuned language
  model and uses them as input to classic interpretable models.
---

# Wrapper Boxes: Faithful Attribution of Model Predictions to Training Data

## Quick Facts
- arXiv ID: 2311.08644
- Source URL: https://arxiv.org/abs/2311.08644
- Reference count: 28
- Primary result: Wrapper boxes achieve comparable performance to neural models while providing example-based explanations faithful to training data

## Executive Summary
This paper introduces wrapper boxes, a pipeline that combines neural model performance with interpretable classic models (kNN, SVM, decision trees, clustering) to provide example-based explanations for predictions. The method extracts feature representations from fine-tuned language models and uses them as input to classic interpretable models. Across seven language models, two datasets, three classic models, and four evaluation metrics, wrapper boxes achieve predictive performance largely comparable to the original neural models while enabling faithful attribution of model decisions to specific training examples.

## Method Summary
Wrapper boxes work by first fine-tuning neural models (BART-large, DEBERTA-large, Flan-T5-large) on target tasks, then extracting penultimate layer representations from these models. Classic interpretable models (kNN, SVM, decision trees with max_depth=3, and L-Means clustering) are trained on these extracted representations. The approach enables predictions that can be explained by showing the specific training examples that influenced each decision, while maintaining performance comparable to the original neural models.

## Key Results
- Wrapper boxes achieve accuracy, precision, recall, and F1 scores largely comparable to original neural models across seven language models and two datasets
- The approach is architecture-agnostic and can be applied to any fine-tuned neural model
- Different wrapper box configurations (kNN, SVM, DT, L-Means) offer flexibility in balancing predictive performance, explanation faithfulness, and explanation simplicity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Wrapper boxes achieve comparable performance by leveraging linearly separable representations learned during neural fine-tuning
- Core assumption: Neural representations contain sufficient discriminative information for classic models to perform well
- Evidence: Predictive performance is largely comparable across seven language models and two datasets
- Break condition: If neural representations are not linearly separable or lack task-relevant information

### Mechanism 2
- Claim: Wrapper boxes provide faithful explanations by directly linking predictions to specific training examples
- Core assumption: Training examples used for inference are representative and meaningful for explaining predictions
- Evidence: Classic models like kNN, SVM, and DT make predictions based on specific training instances
- Break condition: If training examples are not representative or meaningful, explanations will be confusing

### Mechanism 3
- Claim: Wrapper boxes offer flexibility through different model choices balancing performance, faithfulness, and simplicity
- Core assumption: Users have different needs regarding the tradeoff between performance, faithfulness, and simplicity
- Evidence: Different wrapper box models have different characteristics affecting these trade-offs
- Break condition: If users don't have varying preferences or framework cannot accommodate them

## Foundational Learning

- Concept: Linear separability of neural representations
  - Why needed: Success relies on neural representations being linearly separable for classic models to operate effectively
  - Quick check: Why do we expect neural representations to be linearly separable after fine-tuning?

- Concept: Example-based explanations
  - Why needed: Wrapper boxes explain predictions by showing training examples used for inference
  - Quick check: How do example-based explanations differ from input attribution methods?

- Concept: Interpretability-accuracy trade-offs
  - Why needed: Combining interpretability of classic models with accuracy of neural models requires understanding trade-offs
  - Quick check: What are ways to balance interpretability and accuracy in ML models?

## Architecture Onboarding

- Component map: Pre-trained neural model -> Feature extraction -> Classic interpretable model -> Prediction with example-based explanation
- Critical path: 1) Extract features using neural model 2) Pass features to classic model 3) Classic model makes prediction based on features
- Design tradeoffs: Choice of neural model (feature quality) vs. choice of classic model (interpretability characteristics)
- Failure signatures: Poor performance could indicate bad neural features, unsuitable classic model, or poor neural-classic combination
- First 3 experiments:
  1. Train simple neural model on extracted features to verify feature usefulness
  2. Try different classic models on same features to find best performer
  3. Experiment with different wrapper box configurations to improve performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different distance metrics affect performance and explanation quality?
- Basis: Paper uses Euclidean distance but acknowledges this could affect explanation quality
- Why unresolved: Only evaluates Euclidean distance without exploring alternatives
- Evidence needed: Systematic comparison of distance metrics across tasks showing impact on performance and explanation quality

### Open Question 2
- Question: How does neural model architecture choice affect wrapper box performance and interpretability?
- Basis: Evaluates three transformer architectures but doesn't explore architectural influences
- Why unresolved: Limited to three architectures without systematic investigation of architectural features
- Evidence needed: Comparative analysis of diverse neural architectures with investigation of influencing features

### Open Question 3
- Question: How do users balance tradeoffs between performance, faithfulness, and simplicity?
- Basis: Discusses tradeoffs theoretically but acknowledges need for human-centered evaluation
- Why unresolved: Focuses on system evaluation without user studies
- Evidence needed: User studies measuring preferences and understanding across different tradeoff choices

## Limitations
- No user studies validating whether example-based explanations are actually helpful or interpretable
- Results limited to two datasets (toxic speech detection and natural language inference)
- Computational overhead of extracting representations from large language models not analyzed

## Confidence
- High: Predictive performance claims (comparable accuracy to neural models)
- Medium: Interpretability claims (require user studies not reported)
- Low: Generalizability claims (limited to two datasets)

## Next Checks
1. Conduct user studies comparing wrapper box explanations against traditional input attribution methods to measure explanation quality
2. Test the approach on additional datasets from different domains to assess generalizability
3. Analyze computational efficiency by measuring inference time overhead compared to original neural models