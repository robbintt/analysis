---
ver: rpa2
title: Accelerating LLM Inference with Staged Speculative Decoding
arxiv_id: '2308.04623'
source_url: https://arxiv.org/abs/2308.04623
tags:
- speculative
- decoding
- batch
- tokens
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of slow inference in large language
  models, particularly in small-batch, on-device scenarios. The core method idea is
  staged speculative decoding, which improves upon previous speculative decoding techniques
  in two key ways: (1) restructuring the speculative batch as a tree to reduce generation
  costs and increase expected tokens per batch, and (2) adding a second stage of speculative
  decoding.'
---

# Accelerating LLM Inference with Staged Speculative Decoding

## Quick Facts
- **arXiv ID:** 2308.04623
- **Source URL:** https://arxiv.org/abs/2308.04623
- **Reference count:** 8
- **One-line primary result:** 3.16x reduction in single-batch decoding latency for a 762M parameter GPT-2-L model while perfectly preserving output quality

## Executive Summary
This paper addresses the critical bottleneck of slow inference in large language models, particularly for small-batch, on-device scenarios. The authors introduce staged speculative decoding, which improves upon previous speculative decoding techniques by restructuring the speculative batch as a tree and adding a second stage of speculative decoding. The method achieves a 3.16x reduction in single-batch decoding latency for a 762M parameter GPT-2-L model while maintaining perfect output quality. The approach uses less memory bandwidth and achieves higher sequential decoding throughput compared to baseline and standard speculative methods.

## Method Summary
The staged speculative decoding method restructures the speculative batch as a tree to reduce generation costs and increase expected tokens per batch, while adding a second stage of speculative decoding to accelerate the draft model itself. The system uses three models in a cascade: a large oracle model (762M GPT-2-L), a smaller draft model (40M GPT-2), and an extremely fast N-gram model (Katz backoff trigram) as draft2. This architecture converts sequential execution to parallel execution, improving GPU memory bandwidth utilization and arithmetic intensity. The tree structure allows multiple token predictions to be batched together, moving from memory-bound to compute-bound execution.

## Key Results
- 3.16x reduction in single-batch decoding latency for 762M parameter GPT-2-L model
- Perfect preservation of output quality while achieving performance gains
- Substantial reduction in memory bandwidth usage compared to baseline methods
- Higher sequential decoding throughput than both baseline and standard speculative methods

## Why This Works (Mechanism)

### Mechanism 1: Tree-Structured Batches
By restructuring the speculative batch as a tree rather than a single sequence, the algorithm reduces generation costs and increases expected tokens per batch. This works by parallelizing draft model computation across multiple branches and increasing the probability of successful batch verification. The tree structure reallocates computation from the end of long sequences to the beginning, considering multiple likely tokens to increase expected tokens per batch.

### Mechanism 2: Second-Stage Speculation
Adding a second stage of speculative decoding accelerates the draft model itself by running it through speculative decoding using an even smaller, faster model (draft2). This creates a cascade of predictions that reduces the cost of generating the speculative batch. The performance bottleneck shifts from the oracle to the draft model when using tree-structured batches, making this additional acceleration stage effective.

### Mechanism 3: Parallel Execution Conversion
The method converts sequential execution to parallel execution by batching multiple token predictions together, which increases arithmetic intensity. This moves operations from memory-bound to compute-bound execution on GPUs, where performance is limited by memory bandwidth at low arithmetic intensities. Higher arithmetic intensity allows better utilization of GPU compute cores.

## Foundational Learning

- **Autoregressive generation and KV caching**: Understanding how transformers generate tokens sequentially and use KV caches for efficiency is fundamental to grasping why speculative decoding provides benefits. Quick check: Why does autoregressive generation have low arithmetic intensity in small-batch scenarios?

- **GPU memory hierarchy and roofline model**: The paper's performance gains come from optimizing for GPU memory bandwidth constraints, which requires understanding how GPUs handle memory access patterns. Quick check: How does increasing arithmetic intensity move an operation from memory-bound to compute-bound on GPUs?

- **Speculative execution and rejection sampling**: The core algorithm relies on generating predictions ahead of time and verifying them, with mechanisms to maintain the original probability distribution. Quick check: What happens when the oracle model rejects tokens proposed by the draft model?

## Architecture Onboarding

- **Component map**: Oracle model (762M GPT-2-L) -> Draft model (40M GPT-2) -> Draft2 model (Katz backoff trigram) -> Tree structure manager -> Verification engine
- **Critical path**: Generate tree-structured speculative batch using draft and draft2 models → Batch verify against oracle model → If verification succeeds, accept batch; if not, fall back to sequential decoding
- **Design tradeoffs**: Model size vs accuracy for draft models; Tree depth vs computational overhead; Batch size vs memory constraints; Number of speculation stages vs implementation complexity
- **Failure signatures**: High rejection rates indicate draft model misalignment; Memory errors suggest batch size exceeds GPU capacity; CPU-bound performance indicates inefficient tree construction; Unexpected latency spikes may indicate fallback to sequential decoding
- **First 3 experiments**: 1) Implement basic speculative decoding with fixed batch sizes and measure bandwidth reduction; 2) Add tree-structured batch generation and compare token/second throughput; 3) Add second-stage speculation using N-gram draft2 model and validate end-to-end performance gains

## Open Questions the Paper Calls Out

The paper identifies several open questions regarding the scalability of staged speculative decoding to larger models (20B+ parameters) with 8-bit quantization, optimal strategies for constructing tree-structured batches across different text generation tasks, and how the method affects text quality when combined with sampling strategies beyond deterministic and top-k sampling.

## Limitations

- Results are based on specific model sizes (762M GPT-2-L oracle, 40M GPT-2 draft) and may not generalize to other scales
- Tree structure parameters and batch configuration heuristics are not fully specified, impacting reproducibility
- Evaluation is limited to Python code generation on the HumanEval dataset, raising questions about performance on other domains
- Perfect output quality preservation is asserted but verification methodology is not detailed

## Confidence

**High Confidence Claims:**
- Speculative decoding can improve single-batch inference performance by restructuring computation patterns
- GPU memory bandwidth is a primary bottleneck for small-batch autoregressive generation
- Multi-stage speculation can accelerate draft model computation

**Medium Confidence Claims:**
- Tree-structured batch generation provides 3.16x latency reduction with perfect quality preservation
- Second-stage speculation with N-gram models is an effective acceleration strategy
- Memory bandwidth reduction is the primary driver of performance gains

**Low Confidence Claims:**
- The specific 3.16x improvement will generalize to other model scales and hardware
- The tree structure configuration is optimal for all scenarios
- Perfect output quality preservation is achievable across all use cases

## Next Checks

1. **Architecture Scaling Study**: Implement staged speculative decoding with varying oracle model sizes (124M, 354M, 774M) and measure how performance gains scale to validate generalization across scales.

2. **Tree Structure Sensitivity Analysis**: Systematically vary tree depth (1-4 levels) and branching factor (2-8 branches) while measuring both performance gains and output quality preservation to identify optimal configurations.

3. **Cross-Domain Evaluation**: Test the method on non-code generation tasks including general language modeling, summarization, and question answering to validate that quality preservation and performance gains extend beyond Python code generation.