---
ver: rpa2
title: Constructing Holistic Measures for Social Biases in Masked Language Models
arxiv_id: '2305.07795'
source_url: https://arxiv.org/abs/2305.07795
tags:
- bias
- mlms
- stereotype
- metrics
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes two novel metrics for evaluating social biases
  in masked language models (MLMs). The metrics, KLDivS and JSDivS, are based on treating
  the log-likelihood outputs of MLMs for stereotype bias and anti-stereotype bias
  samples as Gaussian distributions, and then using Kullback-Leibler divergence and
  Jensen-Shannon divergence to measure the distance between these distributions.
---

# Constructing Holistic Measures for Social Biases in Masked Language Models

## Quick Facts
- arXiv ID: 2305.07795
- Source URL: https://arxiv.org/abs/2305.07795
- Authors: 
- Reference count: 24
- Primary result: Proposes KLDivS and JSDivS metrics for evaluating social biases in MLMs using KL and JS divergences between Gaussian distributions of log-likelihoods

## Executive Summary
This paper addresses the challenge of evaluating social biases in masked language models by proposing two novel metrics: KLDivS and JSDivS. These metrics treat the log-likelihood outputs for stereotype and anti-stereotype bias samples as Gaussian distributions and measure the distance between these distributions using Kullback-Leibler and Jensen-Shannon divergences. The key innovation is considering the full distribution of outputs rather than individual sample values, which makes the metrics more robust to sample-specific pitfalls. Experiments on StereoSet and CrowS-Pairs datasets demonstrate that these metrics are more stable and interpretable than previous approaches.

## Method Summary
The paper proposes a holistic approach to measuring social biases in masked language models by modeling log-likelihood outputs for stereotype and anti-stereotype samples as Gaussian distributions. For each bias type, the method computes log-likelihoods for all samples in both stereotype and anti-stereotype categories, fits Gaussian distributions to these outputs, and then calculates either KL divergence (for KLDivS) or JS divergence (for JSDivS) between the distributions. The metrics are designed to capture the overall distributional differences rather than relying on individual sample comparisons, which addresses the pitfall problem in existing metrics. The paper evaluates these metrics on two public datasets (StereoSet and CrowS-Pairs) across multiple MLM architectures including BERT, RoBERTa, and ALBERT.

## Key Results
- KLDivS and JSDivS metrics show improved stability compared to baseline metrics (StereoS, CrowS, AUL) on StereoSet and CrowS-Pairs datasets
- The metrics are more interpretable as they consider the full distribution of log-likelihood outputs rather than individual samples
- Experimental results demonstrate that the proposed metrics can better capture bias patterns across different model architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using KL and JS divergences between Gaussian distributions of log-likelihoods provides more stable bias evaluation than single-sample comparisons.
- Mechanism: By modeling log-likelihood outputs for stereotype and anti-stereotype samples as Gaussian distributions, the metrics capture both mean and variance, reducing sensitivity to individual sample pitfalls.
- Core assumption: Log-likelihood outputs for each bias type follow a Gaussian distribution.
- Evidence anchors:
  - [abstract] "the log-likelihoods of stereotype bias and anti-stereotype bias samples output by MLMs are considered Gaussian distributions"
  - [section 3] "We represent the output log-likelihoods of stereotype bias and anti-stereotype bias samples from MLMs as Gaussian distributions, respectively"
  - [corpus] Weak - no direct corpus evidence found supporting the Gaussian assumption for log-likelihood distributions
- Break condition: If log-likelihood distributions are non-Gaussian or heavily multimodal, the divergence metrics may misrepresent the actual bias.

### Mechanism 2
- Claim: The asymmetric KL divergence captures directional bias information between stereotype and anti-stereotype distributions.
- Mechanism: KLDivS uses max(p(stt||att), p(att||stt)) to capture the stronger directional influence between distributions, ensuring the metric is always >= 50.
- Core assumption: The directional asymmetry between distributions is meaningful for bias evaluation.
- Evidence anchors:
  - [section 3.1] "KL divergence describes the asymmetric distance between two distributions and its value is non-negative"
  - [section 3.1] "KLDivS considers the asymmetric distance between the stereotype bias and anti-stereotype bias distributions"
  - [corpus] Weak - no direct corpus evidence found for directional bias significance
- Break condition: If both distributions are similar in shape but shifted, the asymmetric measure may overemphasize small differences.

### Mechanism 3
- Claim: JS divergence provides a symmetric and bounded measure of bias that is more interpretable.
- Mechanism: JSDivS converts JS divergence (0-1 range) to a 0-100 scale where 100 indicates no bias (identical distributions).
- Core assumption: Symmetry between stereotype and anti-stereotype distributions is desirable for bias measurement.
- Evidence anchors:
  - [section 3.2] "JS divergence [5,6] is a variant of KL divergence, which solves the asymmetric problem of KL divergence"
  - [section 3.2] "when the two distributions (stereotype bias and anti-stereotype bias) are identical, it means that there is no bias"
  - [corpus] Weak - no direct corpus evidence found for symmetry preference in bias evaluation
- Break condition: If distributions have different shapes but similar central tendencies, JS divergence may not capture the practical bias.

## Foundational Learning

- Concept: Gaussian distribution modeling
  - Why needed here: The paper assumes log-likelihood outputs follow Gaussian distributions to apply KL and JS divergences meaningfully.
  - Quick check question: What two parameters fully describe a Gaussian distribution, and why are they relevant for modeling log-likelihood outputs?

- Concept: KL divergence
  - Why needed here: KLDivS uses KL divergence to measure the asymmetric distance between stereotype and anti-stereotype distributions.
  - Quick check question: What property of KL divergence makes it asymmetric, and how does this affect bias measurement interpretation?

- Concept: JS divergence
  - Why needed here: JSDivS uses JS divergence as a symmetric alternative to KL divergence for bias measurement.
  - Quick check question: How does JS divergence differ mathematically from KL divergence, and why might this symmetry be valuable for bias evaluation?

## Architecture Onboarding

- Component map: MLM inference engine -> Log-likelihood extraction -> Distribution fitting (Gaussian) -> Divergence calculation (KL or JS) -> Bias score aggregation
- Critical path:
  1. Generate log-likelihoods for all samples in dataset
  2. Separate log-likelihoods by bias type (stereotype vs anti-stereotype)
  3. Fit Gaussian distributions to each group
  4. Calculate divergence metric between distributions
  5. Aggregate across bias types with weighting by sample size

- Design tradeoffs:
  - Gaussian assumption vs. non-parametric approaches: Simplicity vs. potential distributional mismatch
  - Symmetric (JS) vs. asymmetric (KL) metrics: Interpretability vs. directional information
  - Sample size weighting: Confidence in bias measurement vs. equal treatment of all bias types

- Failure signatures:
  - Extremely high variance in log-likelihoods that prevents meaningful distribution fitting
  - Significant deviation from Gaussian shape (e.g., heavy tails or multimodality)
  - Empty or near-empty groups for certain bias types leading to undefined distributions

- First 3 experiments:
  1. Compare Gaussian fit quality for log-likelihoods across different bias types and models
  2. Test sensitivity of KLDivS vs JSDivS to outlier samples by introducing synthetic pitfalls
  3. Evaluate stability of metrics across subsampled datasets of varying sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed metrics, KLDivS and JSDivS, perform when evaluated on datasets other than StereoSet and CrowS-Pairs?
- Basis in paper: [explicit] The paper states that the experimental results on public datasets StereoSet and CrowS-Pairs demonstrate that KLDivS and JSDivS are more stable and interpretable compared to the metrics proposed in the past.
- Why unresolved: The paper only presents experimental results on two specific datasets, and it is unclear how the metrics would perform on other datasets.
- What evidence would resolve it: Experimental results on a diverse set of datasets, including those not used in the paper, would provide evidence of the generalizability of the proposed metrics.

### Open Question 2
- Question: How do the proposed metrics, KLDivS and JSDivS, compare to other bias evaluation metrics in terms of computational efficiency?
- Basis in paper: [inferred] The paper does not explicitly discuss the computational efficiency of the proposed metrics compared to other metrics.
- Why unresolved: The paper focuses on the stability and interpretability of the proposed metrics but does not provide information on their computational efficiency.
- What evidence would resolve it: A comparative analysis of the computational efficiency of the proposed metrics and other existing metrics would provide evidence of their relative performance in terms of computational resources.

### Open Question 3
- Question: How do the proposed metrics, KLDivS and JSDivS, perform when applied to MLMs with different architectures or training objectives?
- Basis in paper: [explicit] The paper presents experimental results on BERT, RoBERTa, and ALBERT, but it does not discuss the performance of the metrics on MLMs with different architectures or training objectives.
- Why unresolved: The paper does not explore the generalizability of the proposed metrics across different MLM architectures or training objectives.
- What evidence would resolve it: Experimental results on a diverse set of MLMs with varying architectures and training objectives would provide evidence of the generalizability of the proposed metrics.

## Limitations
- The Gaussian distribution assumption for log-likelihood outputs lacks direct empirical validation
- Limited evaluation to only two public datasets (StereoSet and CrowS-Pairs)
- No comparative analysis of computational efficiency with existing metrics

## Confidence
- **High**: Experimental setup using public datasets and comparison with baseline metrics is well-documented and reproducible
- **Medium**: Mathematical formulation of KLDivS and JSDivS is sound but practical significance of asymmetric vs symmetric divergence needs validation
- **Low**: Gaussian distribution assumption for log-likelihood outputs is the most critical uncertainty

## Next Checks
1. Conduct goodness-of-fit tests (e.g., Shapiro-Wilk, Kolmogorov-Smirnov) to verify Gaussian assumptions across different bias types and model architectures
2. Systematically introduce non-Gaussian perturbations (e.g., outliers, multimodality) to log-likelihood distributions and measure impact on KLDivS and JSDivS scores
3. Analyze correlation patterns between KLDivS, JSDivS, and baseline metrics across multiple datasets to establish complementary strengths and weaknesses