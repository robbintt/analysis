---
ver: rpa2
title: Learning from Mistakes via Cooperative Study Assistant for Large Language Models
arxiv_id: '2305.13829'
source_url: https://arxiv.org/abs/2305.13829
tags:
- feedback
- salam
- arxiv
- mistakes
- assistant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SALAM, a framework for improving large language
  models by learning from mistakes. SALAM uses a study assistant to analyze errors
  made by the main model, collect them in a memory, and provide guidelines to help
  avoid similar mistakes in the future.
---

# Learning from Mistakes via Cooperative Study Assistant for Large Language Models

## Quick Facts
- arXiv ID: 2305.13829
- Source URL: https://arxiv.org/abs/2305.13829
- Authors: 
- Reference count: 31
- Primary result: SALAM improves LLM accuracy by up to 6.6% on BBH and 12.6% on BBQ by learning from mistakes

## Executive Summary
This paper introduces SALAM, a framework that improves large language models by learning from mistakes through a cooperative study assistant. The system collects incorrect answers with explanations and guidelines in a memory, then retrieves relevant past mistakes to help the main model avoid similar errors in the future. SALAM demonstrates significant performance improvements on two challenging benchmarks, BBH and BBQ, by focusing feedback only on failure cases rather than reinforcing already-mastered skills.

## Method Summary
SALAM operates through two main phases: gathering and examination. During the gathering phase, the main model generates answers that the study assistant grades, collecting mistakes in a memory with explanations and guidelines. In the examination phase, the study assistant retrieves similar past mistakes using similarity thresholds and provides targeted feedback to help the main model refine its answers. The framework is model-agnostic, using few-shot prompting to generate feedback without task-specific fine-tuning, and relies on iterative refinement based on mistake-based guidance.

## Key Results
- SALAM achieves up to 6.6% accuracy improvement on the BBH benchmark
- SALAM achieves up to 12.6% accuracy improvement on the BBQ benchmark
- The framework shows that learning from mistakes is more effective than learning from correct answers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SALAM's study assistant can identify and correct recurring error patterns by leveraging a mistake memory that stores incorrect answers paired with explanations and guidelines.
- Mechanism: The study assistant retrieves similar past mistakes from the memory using similarity thresholds, generates explanations for why the model erred, and provides targeted guidelines to prevent repetition. This creates a feedback loop where the main model iteratively refines its answers based on structured, globally-informed feedback rather than single-instance correction.
- Core assumption: The study assistant's feedback, derived from pattern analysis of past mistakes, can generalize to similar but unseen queries.
- Evidence anchors:
  - [abstract] "The study assistant provides guidelines by retrieving relevant cases to help the main LLM anticipate and avoid similar errors."
  - [section 2.3] "The study assistant T maintains a global mistake collection Otr... Each entry ei ∈ Otr include a key ki, which is the query, and a list of values vi = {ti0, vi1, vi2, · · · }."
- Break condition: If retrieved cases are not sufficiently similar or the explanation generation fails to capture the root cause, the guidelines will not generalize, leading to repeated errors.

### Mechanism 2
- Claim: Learning from mistakes is more effective than learning from correct answers because it directly addresses model weaknesses rather than reinforcing already-mastered skills.
- Mechanism: The study assistant filters and focuses feedback only on incorrect responses, providing targeted corrective guidance. This selective attention to failure cases prevents the model from overfitting to patterns it already handles well and instead strengthens weak reasoning areas.
- Core assumption: Incorrect answers contain more actionable information for improving model reasoning than correct answers.
- Evidence anchors:
  - [section 3.4] "Failure is more valuable than success... the mistake is more helpful for the student model."
  - [section 3.4] "Instead, the mistake examples will correct previous incorrect answers, which provides better instruction for questions the student model performs poorly on."
- Break condition: If the mistake collection becomes noisy or the assistant fails to distinguish between different types of errors, feedback could become confusing or counterproductive.

### Mechanism 3
- Claim: SALAM's model-agnostic design allows any LLM to act as a study assistant by leveraging few-shot prompting to generate explanations and guidelines without task-specific fine-tuning.
- Mechanism: The study assistant uses a template-based prompt (ρ) that maps the current query, retrieved context, and model's response into a structured request for explanation and guideline generation. This design relies on the LLM's in-context learning ability rather than specialized training.
- Core assumption: The underlying LLM has sufficient few-shot learning capability to generate useful feedback from the prompt structure alone.
- Evidence anchors:
  - [section 2.4] "SALAM utilizes LLMs' ability to analyze the mistake and provide human-like instructions... it is model-agnostic and can easily generalize to unseen tasks and new models."
  - [section 3.2] "We take the Flan-T5-xxl with 11 billion parameters as the model M. For T, we finetune the LLaMA model with 7 billion on a small set of feedback from GPT-4 to provide feedback."
- Break condition: If the prompt structure fails to elicit the right kind of feedback or the LLM's few-shot ability is insufficient for the task complexity, the assistant's guidance will be ineffective.

## Foundational Learning

- Concept: Mistake collection and retrieval mechanics
  - Why needed here: Understanding how SALAM stores and retrieves past mistakes is critical for debugging performance drops and tuning retrieval thresholds.
  - Quick check question: How does SALAM determine which past mistakes to retrieve for a given query, and what happens if the similarity threshold is too low or too high?

- Concept: Feedback generation via few-shot prompting
  - Why needed here: The study assistant relies on structured prompts to generate explanations and guidelines; knowing the prompt structure helps in modifying or improving feedback quality.
  - Quick check question: What are the key components of the template-based prompt (ρ) used by the study assistant, and how does it ensure the generated feedback is actionable?

- Concept: Reward function design for the study assistant
  - Why needed here: The study assistant's learning signal comes from the main model's performance improvement; understanding this reward mechanism is essential for tuning the assistant's behavior.
  - Quick check question: How is the reward for the study assistant calculated, and what are the implications if the reward function is misaligned with the main model's goals?

## Architecture Onboarding

- Component map: Main model M -> Study assistant T -> Mistake memory Otr -> Retriever (SentenceTransformer) -> Grader (oracle during training)
- Critical path:
  1. M generates answer y0 for query q
  2. T retrieves similar past mistakes from Otr using q
  3. T generates explanation and guideline based on retrieved context and y0
  4. M refines answer using guideline
  5. T grades new answer (if training) and updates Otr if incorrect
- Design tradeoffs:
  - Model-agnostic vs. specialized assistant: Using few-shot prompting keeps the assistant flexible but may limit feedback quality compared to fine-tuning
  - Mistake collection size vs. relevance: Larger collections increase coverage but risk retrieval noise; strict similarity thresholds improve relevance but reduce coverage
  - Retrieval top-k vs. threshold: More retrieved cases can provide richer context but may dilute focus; higher thresholds ensure relevance but risk missing useful patterns
- Failure signatures:
  - Main model fails to improve: Likely issues with feedback relevance, prompt structure, or retrieval similarity threshold
  - Study assistant generates irrelevant or confusing feedback: Check prompt template, few-shot examples, or retrieval quality
  - Performance degrades with more retrieval: Likely retrieval noise overwhelming the main model; reduce top-k or raise similarity threshold
- First 3 experiments:
  1. Run SALAM with top-k=1 and θ=0.9 on a small subset of BBH; verify retrieval returns relevant mistakes and feedback improves accuracy
  2. Test pseudo-mistake ablation by comparing performance with real vs. pseudo mistakes on BBQ; confirm real mistakes are necessary
  3. Evaluate out-of-domain generalization by training on first 5 BBQ tasks and testing on remaining tasks; measure feedback transfer effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the size of the mistake collection (Otr) impact the performance of SALAM, and is there an optimal size for the collection?
- Basis in paper: [inferred] The paper mentions that SALAM collects mistakes in the training phase and retrieves relevant cases during inference. It would be interesting to investigate how the size of the mistake collection affects the model's performance.
- Why unresolved: The paper does not provide any experiments or analysis on the impact of the mistake collection size on SALAM's performance.
- What evidence would resolve it: Experiments varying the size of the mistake collection and measuring the corresponding performance of SALAM would provide insights into the optimal size for the collection.

### Open Question 2
- Question: Can SALAM be extended to handle more complex reasoning tasks beyond multi-choice question-answering, such as open-ended generation or structured prediction tasks?
- Basis in paper: [inferred] The paper focuses on multi-choice question-answering tasks in BBH and BBQ benchmarks. It would be valuable to explore if SALAM can be adapted to handle other types of tasks.
- Why unresolved: The paper does not discuss the potential application of SALAM to other task types or provide any evidence of its effectiveness in such scenarios.
- What evidence would resolve it: Experiments applying SALAM to open-ended generation or structured prediction tasks and evaluating its performance compared to other methods would demonstrate its potential for handling more complex reasoning tasks.

### Open Question 3
- Question: How does the choice of the study assistant model (T) impact the performance of SALAM, and are there specific model architectures or training strategies that work best for this role?
- Basis in paper: [inferred] The paper mentions that the study assistant can be any LLM that provides feedback and can be a smaller but specific feedback model. It would be interesting to investigate the impact of different study assistant models on SALAM's performance.
- Why unresolved: The paper does not provide any experiments or analysis on the impact of different study assistant models on SALAM's performance or discuss the specific requirements for an effective study assistant model.
- What evidence would resolve it: Experiments comparing the performance of SALAM with different study assistant models, including various model architectures and training strategies, would provide insights into the optimal choice for the study assistant role.

## Limitations

- The paper does not address potential performance degradation on out-of-distribution tasks or different model sizes
- The model-agnostic design's effectiveness across different LLM architectures and sizes is not empirically validated
- The framework lacks detail on handling cases where the mistake memory contains insufficient similar mistakes or becomes too noisy over time

## Confidence

**High Confidence**: The core mechanism of using a study assistant to learn from mistakes is well-supported by experimental results showing 6.6% improvement on BBH and 12.6% on BBQ.

**Medium Confidence**: The claim that learning from mistakes is more effective than learning from correct answers is supported by ablation studies, but the underlying mechanism is not fully explained.

**Low Confidence**: The model-agnostic nature and few-shot prompting approach lack empirical validation across different LLM architectures and sizes.

## Next Checks

1. **Out-of-Domain Generalization Test**: Train SALAM on the first 5 BBQ tasks and evaluate on the remaining tasks to measure how well the mistake-based learning transfers to unseen tasks.

2. **Study Assistant Capability Scaling**: Replace the LLaMA-7B study assistant with smaller (3B) and larger (13B) models to quantify how assistant capability affects performance.

3. **Mistake Collection Quality Analysis**: Systematically vary the similarity threshold θ from 0.7 to 0.95 and measure its impact on performance, and test with artificially corrupted mistake collections to quantify feedback quality importance.