---
ver: rpa2
title: Hierarchical Reinforcement Learning for Power Network Topology Control
arxiv_id: '2311.02129'
source_url: https://arxiv.org/abs/2311.02129
tags:
- power
- agents
- level
- substation
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the application of hierarchical reinforcement
  learning (HRL) to power network topology control, a critical infrastructure problem
  with a combinatorial action space. A three-level HRL framework is proposed, decomposing
  the main task into: (i) a rule-based binary decision to activate or not, (ii) an
  RL policy for substation selection, and (iii) either an RL or greedy policy for
  substation configuration.'
---

# Hierarchical Reinforcement Learning for Power Network Topology Control

## Quick Facts
- arXiv ID: 2311.02129
- Source URL: https://arxiv.org/abs/2311.02129
- Reference count: 40
- Key outcome: Three-level HRL framework significantly outperforms other agents in power network topology control with outages, achieving higher mean episode lengths and lower activity levels.

## Executive Summary
This study applies hierarchical reinforcement learning (HRL) to power network topology control, addressing the challenge of large combinatorial action spaces. The authors propose a three-level HRL framework that decomposes the main task into a rule-based activation decision, an RL policy for substation selection, and an RL or greedy policy for substation configuration. The framework is evaluated on the IEEE 14-bus network under two regimes: with and without random line outages. Results show that the HRL agent employing RL at both intermediate and lowest levels significantly outperforms other agents in the more realistic regime with outages.

## Method Summary
The proposed method employs a three-level HRL framework for power network topology control. At the highest level, a rule-based binary decision determines whether to activate the RL hierarchy based on a line loading threshold (ρthres = 95%). The intermediate level uses an RL policy (PPO or SAC) to select a substation for configuration change. The lowest level employs either an RL policy or a greedy brute-force approach to choose the optimal configuration for the selected substation. The agents are trained and evaluated on the IEEE 14-bus network using the Grid2Op environment and Pandapower backend, with 1,000 training scenarios and 200 test scenarios.

## Key Results
- The HRL agent with RL policies at both intermediate and lowest levels outperforms all other agents in the regime with outages.
- The hierarchical decomposition reduces the effective action space size, making RL training tractable.
- PPO-based agents demonstrate more stable training and superior performance compared to SAC-based agents in this domain.

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical decomposition reduces the effective action space size from 106 to a sum of 3-26 actions per substation, making RL training tractable. By splitting the action choice into substation selection and configuration selection, the RL policy only ever samples from a small subset of the full action space at any given time. The optimal substation configuration for a given substation can be found independently of the choices made for other substations at the same timestep.

### Mechanism 2
The binary "do nothing vs. propose change" option at the highest level improves sample efficiency by filtering out low-load timesteps. A rule-based threshold on max line loading (ρthres = 95%) determines when to engage the RL hierarchy, reducing the number of states the policy must learn to handle. Low-load states are either stable or easily handled by the do-nothing action, so RL does not need to learn optimal behavior for them.

### Mechanism 3
PPO's clipped surrogate objective with fixed KL penalty provides more stable training than SAC in this domain. The clipping prevents large policy updates that could destabilize learning, and the KL penalty enforces similarity to the old policy, which is critical when two policies depend on each other. The stability benefits of PPO outweigh the potential sample efficiency gains of SAC for this specific task.

## Foundational Learning

- Concept: Semi-Markov Decision Process (SMDP)
  - Why needed here: The "do nothing" option persists for multiple timesteps, requiring an SMDP formulation rather than a standard MDP.
  - Quick check question: What is the difference between an MDP and an SMDP in terms of action duration and reward aggregation?

- Concept: Options framework
  - Why needed here: Provides a formal way to represent temporally extended actions (like "do nothing") and their initiation/termination conditions.
  - Quick check question: How does an option differ from a primitive action in terms of policy and termination condition?

- Concept: State abstraction
  - Why needed here: At the highest level, the agent only needs the maximum line loading rather than the full state vector, simplifying the decision.
  - Quick check question: What information is lost when using max(ρ) instead of the full ρ vector, and why is it acceptable here?

## Architecture Onboarding

- Component map: Environment -> Highest-level option -> Intermediate-level substation -> Lowest-level configuration -> Environment step
- Critical path: Observation → Highest-level option → Intermediate-level substation → Lowest-level configuration → Environment step
- Design tradeoffs:
  - Hierarchical decomposition reduces action space but adds training complexity for interdependent policies.
  - Greedy brute-force at lowest level is fast but may miss globally optimal configurations.
  - Using full observation at intermediate/lowest levels simplifies training but may limit scalability.
- Failure signatures:
  - High variance in training curves → check reward scaling, learning rates, or policy interdependence.
  - Agent fails to solve any test scenarios → check threshold, reward shaping, or simulation accuracy.
  - Agent overfits to training scenarios → check diversity of training scenarios and validation performance.
- First 3 experiments:
  1. Train PPO Substation vs. PPO Native on regime without outages; compare mean episode length and activity level.
  2. Vary ρthres (e.g., 90%, 95%, 100%) and observe effect on training stability and performance.
  3. Replace greedy lowest-level policy with RL policy; compare convergence speed and final performance.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of hierarchical RL agents scale to larger power grid sizes? The study only evaluated the proposed HRL framework on the IEEE 14-bus network, and the authors explicitly call out scaling to larger grids as future work. Experimental results comparing the performance of the proposed HRL agents to other methods on power grids with significantly more substations (e.g., 50+ substations) would provide evidence for how well the approach scales.

### Open Question 2
What is the impact of incorporating graph neural network (GNN) based policies into the HRL framework? The authors mention that incorporating GNN-based policies into the HRL framework is an interesting avenue for future research, noting that GNNs can effectively encode combinatorial and relational input and enable transfer learning between different grids. Implementing and evaluating the performance of GNN-based policies within the proposed HRL framework on the power network control task would provide evidence for the potential benefits.

### Open Question 3
What causes the bimodal distribution of hierarchical policies observed in the advanced 3-level HRL agent? The authors observe that a bifurcation emerges in hierarchical policy space when training the advanced 3-level HRL agent on different random seeds, leading to two distinct clusters of models with different performance levels. Analyzing the learned policies and their training dynamics in more detail, potentially through techniques like policy visualization or examining the interaction between the two PPO policies, could provide insights into the cause of the bimodal distribution.

## Limitations
- The hierarchical decomposition relies on independent substation optimization that may not scale to grids with strong inter-substation dependencies.
- The experimental validation is limited to a single IEEE 14-bus network, and results may not transfer to more complex topologies or different grid configurations.
- The study does not explore the robustness of learned policies to parameter variations or adversarial contingencies.

## Confidence

- High confidence: The effectiveness of hierarchical decomposition in reducing action space complexity and the superiority of PPO over SAC in this domain.
- Medium confidence: The scalability of the proposed HRL framework to larger networks and its robustness to parameter variations.
- Low confidence: The long-term stability of learned policies under evolving grid conditions and the absence of comparative analysis with non-RL baselines.

## Next Checks

1. Evaluate the HRL framework on larger IEEE test networks (e.g., 30-bus, 57-bus) to assess scalability and identify limitations of the substation independence assumption.
2. Conduct ablation studies by varying the activation threshold ρthres and the lowest-level policy (greedy vs. RL) to quantify their impact on performance and sample efficiency.
3. Test the learned policies against adversarial contingency sequences not seen during training to evaluate robustness and identify potential failure modes.