---
ver: rpa2
title: Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency
arxiv_id: '2309.17272'
source_url: https://arxiv.org/abs/2309.17272
tags:
- pass
- mpsc
- code
- test
- perspectives
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Multi-Perspective Self-Consistency (MPSC),
  a novel decoding strategy for large language models (LLMs) that incorporates both
  inter- and intra-consistency across outputs from multiple perspectives. Specifically,
  MPSC prompts LLMs to generate diverse outputs from three perspectives (solution,
  specification, and test case), constructs a 3-partite graph, and leverages two measure
  functions of consistency to embed both inter- and intra-consistency information
  into the graph.
---

# Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency

## Quick Facts
- **arXiv ID**: 2309.17272
- **Source URL**: https://arxiv.org/abs/2309.17272
- **Reference count**: 40
- **Key outcome**: MPSC boosts foundation models (ChatGPT) on HumanEval (+17.60%), HumanEval Plus (+17.61%), MBPP (+6.50%), and CodeContests (+11.82%), surpassing GPT-4.

## Executive Summary
This paper introduces Multi-Perspective Self-Consistency (MPSC), a novel decoding strategy that enhances LLM performance in code generation by leveraging consistency across multiple perspectives. MPSC generates diverse outputs from three perspectives (solution, specification, and test case), constructs a 3-partite graph, and uses graph-based ranking to select the optimal solution based on inter- and intra-consistency measures. The approach significantly improves performance across multiple coding benchmarks while remaining model-agnostic and task-agnostic.

## Method Summary
MPSC constructs a 3-partite graph where vertices represent generated solutions, specifications, and test cases. The method uses iterative optimization to learn a score function that combines inter-consistency (agreement between perspectives) and intra-consistency (confidence within perspectives). For each coding task, the approach generates 200 solutions, 100 specifications, and 500 test cases using an LLM, then builds a graph encoding both types of consistency. The optimal solution is selected through graph-based ranking that balances agreement across perspectives with confidence within each perspective.

## Key Results
- **HumanEval**: +17.60% improvement over baseline (Pass@1)
- **HumanEval Plus**: +17.61% improvement over baseline (Pass@1)
- **MBPP**: +6.50% improvement over baseline (Pass@1)
- **CodeContests**: +11.82% improvement over baseline (Pass@1)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The graph-based ranking objective effectively balances intra-consistency with inter-consistency.
- Mechanism: The optimization problem combines a Laplacian-based smoothness term enforcing agreement between connected nodes (inter-consistency) with an MSE term encouraging scores to match intra-consistency measures (intra-consistency).
- Core assumption: Both types of consistency are positively correlated with correctness.
- Evidence anchors: [abstract] "With two predefined measures of consistency, we embed both inter- and intra-consistency information into the graph." [section 2.2] Formalizes intra-consistency as "degree to which a given output aligns with others within the same perspective" and inter-consistency as "degree of consensus between a pair of outputs from two different perspectives."

### Mechanism 2
- Claim: Using multiple perspectives allows deterministic verification of inter-consistency through execution.
- Mechanism: The three perspectives form a 3-partite graph where edges represent verifiable agreements (e.g., solution passes specification preconditions/postconditions, solution produces expected test case outputs).
- Core assumption: The code interpreter can accurately verify semantic agreement between perspectives.
- Evidence anchors: [section 3.2] Provides specific formulas for inter-consistency measures like ω(vi,vj) = Ex∈X[1hpre j (x)→hpost j (x,gi(x))] for solution-specification agreement. [abstract] "we can use a code interpreter to quantitatively verify the agreement between the outputs from different perspectives."

### Mechanism 3
- Claim: Structural equivalence classes provide effective intra-consistency supervision without requiring external labels.
- Mechanism: Outputs are grouped by structural equivalence (similar behavior/meaning), and intra-consistency scores are derived from class properties like cardinality or neighbor agreement.
- Core assumption: Outputs within the same structural equivalence class share similar correctness likelihood.
- Evidence anchors: [section 3.3] "By applying this equivalence relation, we can categorize vertices into several equivalence classes...outputs from the same perspective are demarcated into structural equivalence classes, of which outputs exhibit consistent behavior or possessing consistent meanings." [table 1] Lists specific intra-consistency measures like cardinality and weight based on structural equivalence.

## Foundational Learning

- **Concept**: Graph Laplacians and their role in semi-supervised learning
  - **Why needed here**: The optimization problem uses L = D - W (Laplacian) to encode smoothness constraints across the graph
  - **Quick check question**: What property does the Laplacian matrix encode about a graph's structure?

- **Concept**: Formal verification principles (preconditions/postconditions)
  - **Why needed here**: Specifications are defined using pre-conditions and post-conditions to enable deterministic inter-consistency verification
  - **Quick check question**: How do pre-conditions and post-conditions formally specify program behavior?

- **Concept**: Minimum Bayes Risk decoding
  - **Why needed here**: One intra-consistency measure uses MBR principles to score outputs based on their agreement with other candidates
  - **Quick check question**: In MBR decoding, what does the risk function typically measure?

## Architecture Onboarding

- **Component map**: LLM generator → Multi-perspective sampler → 3-partite graph builder → Consistency measure functions → Iterative ranking algorithm → Output selector
- **Critical path**: Sampling → Graph construction → Ranking optimization → Selection
- **Design tradeoffs**: More perspectives increase verification opportunities but also computational cost; more samples improve consistency estimates but increase latency
- **Failure signatures**: Poor performance on tasks where perspectives cannot be meaningfully defined; failure when code interpreter cannot execute generated code
- **First 3 experiments**:
  1. Compare Pass@1 with and without MPSC on HumanEval using same base LLM
  2. Test ablation: MPSC with only 2 perspectives (solution+test case) vs all 3
  3. Vary α parameter in optimization and measure impact on Pass@1

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What are the optimal sampling numbers for each perspective (solutions, specifications, test cases) to balance performance and computational efficiency?
- **Basis in paper**: [explicit] The paper discusses the impact of sampling numbers on performance, showing that using fewer specifications and test cases leads to a slight performance degradation.
- **Why unresolved**: The paper provides results for specific sampling numbers but does not determine the optimal balance between performance and computational efficiency.
- **What evidence would resolve it**: Systematic experiments varying the sampling numbers for each perspective and measuring both performance and computational costs would help determine the optimal balance.

### Open Question 2
- **Question**: How does the choice of intra-consistency measure functions affect the performance of MPSC in different code generation benchmarks?
- **Basis in paper**: [explicit] The paper explores several intra-consistency measure functions (Bayes Risk, Cardinality, Weight, Weighted Cardinality, Uniform, Probability, Label) and shows that different measures lead to varying performance improvements.
- **Why unresolved**: The paper demonstrates that different intra-consistency measures yield different results, but it does not identify which measure is optimal for specific benchmarks or types of code generation tasks.
- **What evidence would resolve it**: Conducting experiments with each intra-consistency measure on various benchmarks and analyzing the performance differences would help determine the most effective measures for different scenarios.

### Open Question 3
- **Question**: Can MPSC be effectively applied to other natural language generation tasks beyond code generation, and what challenges might arise?
- **Basis in paper**: [explicit] The paper mentions that MPSC is model-agnostic and task-agnostic, suggesting potential applicability to other textual generation scenarios.
- **Why unresolved**: While the paper discusses the theoretical applicability of MPSC to other tasks, it does not provide empirical evidence or identify specific challenges that might arise when applying MPSC to different natural language generation tasks.
- **What evidence would resolve it**: Implementing MPSC on various natural language generation tasks (e.g., math problem solving, question answering) and analyzing the performance and challenges encountered would provide insights into its broader applicability and limitations.

## Limitations
- The evaluation relies solely on ChatGPT (GPT-3.5-Turbo) as the base model, with no comparisons to other LLMs or open-source alternatives.
- The computational overhead of generating 800 total outputs per task is substantial and not discussed in terms of latency or cost implications.
- The method's effectiveness on longer, more complex coding tasks beyond the evaluated benchmarks remains unclear.

## Confidence
- **High Confidence**: The core theoretical framework connecting graph-based ranking with consistency measures is well-established and mathematically sound.
- **Medium Confidence**: The empirical results showing significant performance improvements are promising but based on a single base model.
- **Low Confidence**: The computational efficiency claims are not supported by runtime measurements or comparisons to baseline methods.

## Next Checks
1. **Runtime and Resource Analysis**: Measure the wall-clock time and computational resources required for MPSC compared to standard self-consistency methods on the same hardware, including LLM API costs for the 800-output generation process.

2. **Ablation Study on Perspective Count**: Systematically evaluate performance when using only 2 perspectives (solution+test case) versus all 3, and when varying the number of samples per perspective (e.g., 50, 100, 200 solutions) to understand the contribution of each component.

3. **Cross-Model Generalization Test**: Apply MPSC to a different base LLM (e.g., Claude, Llama, or open-source alternatives) on the same benchmarks to verify that improvements are not specific to ChatGPT's characteristics.