---
ver: rpa2
title: Improving Few-shot and Zero-shot Entity Linking with Coarse-to-Fine Lexicon-based
  Retriever
arxiv_id: '2308.03365'
source_url: https://arxiv.org/abs/2308.03365
tags:
- entity
- entities
- linking
- zero-shot
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a coarse-to-fine lexicon-based retriever for
  few-shot and zero-shot entity linking. The method uses a two-layer BM25-based retriever:
  the first layer retrieves coarse-grained candidates using entity names from alias
  tables and knowledge bases, while the second layer retrieves fine-grained candidates
  using entity descriptions.'
---

# Improving Few-shot and Zero-shot Entity Linking with Coarse-to-Fine Lexicon-based Retriever

## Quick Facts
- arXiv ID: 2308.03365
- Source URL: https://arxiv.org/abs/2308.03365
- Reference count: 29
- Primary result: State-of-the-art accuracy of 0.6915 on NLPCC 2023 Shared Task 6 Chinese Few-shot and Zero-shot Entity Linking benchmark

## Executive Summary
This paper introduces a coarse-to-fine lexicon-based retriever for few-shot and zero-shot entity linking, addressing the challenge of linking tail and emerging entities. The approach employs a two-layer BM25-based retriever: the first layer retrieves coarse-grained candidates using entity names from alias tables and knowledge bases, while the second layer retrieves fine-grained candidates using entity descriptions. A BERT-based dual encoder is used for reranking, and an ensemble method combines results from all stages. The system achieves state-of-the-art performance on the NLPCC 2023 Shared Task 6 benchmark, ranking first with an accuracy of 0.6915. The method improves candidate retrieval without extensive fine-tuning, making it particularly effective for scenarios where entities have limited or no prior examples.

## Method Summary
The proposed method uses a two-layer BM25-based retriever with three models: AT-BM25 (alias table names), KB-BM25 (knowledge base names), and Description-BM25 (entity descriptions). The first two models retrieve broad candidates based on entity names, while the third refines them using entity descriptions to disambiguate between popular entities sharing the same name. A BERT-based dual encoder is employed for efficient reranking by pre-computing entity embeddings. An ensemble method combines the top results from all four sources (three BM25 models plus the dual encoder) using voting, with the dual encoder serving as a tiebreaker. The approach is evaluated on the Hansel dataset, which contains known entities from 2018 and new entities from 2021, allowing testing of both few-shot and zero-shot entity linking scenarios.

## Key Results
- Achieves state-of-the-art accuracy of 0.6915 on NLPCC 2023 Shared Task 6 benchmark
- Ranks 1st among competing systems in Chinese few-shot and zero-shot entity linking
- Demonstrates effectiveness for linking tail and emerging entities through coarse-to-fine retrieval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Coarse-to-fine retrieval improves accuracy for tail and emerging entities by using layered BM25 models with different context granularities
- Mechanism: The two-layer BM25 architecture first retrieves broad candidates using entity names (AT-BM25 and KB-BM25), then refines them using entity descriptions (Description-BM25) to disambiguate between popular entities sharing the same name
- Core assumption: Entity descriptions provide sufficient discriminative information to distinguish between tail entities that share names with popular entities
- Evidence anchors:
  - [abstract] "This second layer utilizes entity descriptions to effectively disambiguate tail or new entities that share names with existing popular entities"
  - [section 3.1] "To further disambiguate the tail entities based on the detailed mention context, we treat the document doci of the test sample as query and merge the obtained coarse-grained candidate entities"
  - [corpus] Weak evidence - corpus shows similar approaches but no direct validation of description-based disambiguation effectiveness
- Break condition: If entity descriptions are too generic or missing for tail entities, the second layer cannot effectively disambiguate candidates

### Mechanism 2
- Claim: Ensemble method combining retrieval and reranking results reduces bias in few-shot/zero-shot settings
- Mechanism: The system aggregates top results from four sources (AT-BM25, KB-BM25, Description-BM25, and BERT-based dual encoder reranker) using voting, with dual encoder as tiebreaker
- Core assumption: Combining diverse retrieval strategies captures different aspects of entity matching that single approaches miss
- Evidence anchors:
  - [section 3.3] "we propose an ensemble method that leverages information from both retrieve and rerank stages, aiming to enhance the robustness of predictions"
  - [section 4.5] "The ensemble method can aggregate useful information form both retrieve and rerank stages, which results in more robust prediction"
  - [corpus] Moderate evidence - ensemble approaches are common in NLP but specific validation for few-shot/zero-shot EL is limited
- Break condition: If all four methods consistently agree on wrong answers, ensemble cannot correct the error

### Mechanism 3
- Claim: BERT-based dual encoder enables efficient reranking without requiring expensive cross-encoder inference
- Mechanism: Pre-computed entity embeddings allow fast similarity computation via dot product, avoiding full cross-encoder processing for each candidate
- Core assumption: Dual encoder representations capture sufficient semantic similarity for effective reranking
- Evidence anchors:
  - [section 3.2] "we follow previous works [1,16] to train a BERT-based dual encoder. This approach offers scalability benefits, as the entity embeddings can be pre-computed and stored"
  - [section 3.2] "During the inference phase, we utilize the BERT-based dual encoder to rerank the combination of coarse-grained and fine-grained candidate entities"
  - [corpus] Strong evidence - dual encoders are well-established in retrieval literature with proven scalability benefits
- Break condition: If dual encoder embeddings lack semantic depth compared to cross-encoder, reranking accuracy may suffer

## Foundational Learning

- Concept: BM25 ranking function
  - Why needed here: Understanding how the three BM25 models weight term frequency and document length is crucial for tuning retrieval parameters
  - Quick check question: What happens to BM25 scores when query terms appear frequently in short documents?

- Concept: Dual encoder architecture
  - Why needed here: The dual encoder's ability to pre-compute embeddings enables the system's scalability advantage
  - Quick check question: How does the dual encoder's dot product similarity compare to cross-encoder attention in terms of computational cost?

- Concept: Ensemble methods in NLP
  - Why needed here: The voting mechanism and tiebreaker logic determine how the system combines diverse predictions
  - Quick check question: What are the advantages and disadvantages of using majority voting versus weighted voting in ensemble systems?

## Architecture Onboarding

- Component map:
  Input -> AT-BM25 (alias table names) -> Candidate set 1
  Input -> KB-BM25 (knowledge base names) -> Candidate set 2
  Input -> Description-BM25 (entity descriptions) -> Candidate set 3
  Input + Candidates -> BERT dual encoder reranker -> Reranked candidates
  All sources -> Ensemble voting -> Final prediction

- Critical path:
  1. BM25 retrieval (AT, KB, Description layers)
  2. Dual encoder reranking
  3. Ensemble voting
  Any failure in early stages propagates downstream, making robust retrieval essential

- Design tradeoffs:
  - Three BM25 models increase recall but add computational overhead
  - Dual encoder sacrifices some accuracy for speed vs. cross-encoder
  - Ensemble adds robustness but may not correct systematic errors across all sources

- Failure signatures:
  - Low r@1 but high r@5/r@10 indicates retrieval is broad but reranking needs improvement
  - Ensemble consistently choosing dual encoder output suggests retrieval methods are weak
  - Accuracy drops when switching from known to new entities indicates poor generalization

- First 3 experiments:
  1. Compare single BM25 model vs. three-model approach to quantify ensemble benefit
  2. Replace dual encoder with cross-encoder on validation set to measure accuracy-speed tradeoff
  3. Test ensemble voting strategies (majority vs. weighted vs. dual encoder tiebreaker) to optimize final predictions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the coarse-to-fine lexicon-based retriever perform on languages other than Chinese, particularly languages with different morphological structures or script types?
- Basis in paper: [inferred] The paper focuses on Chinese entity linking and uses specific Chinese-language resources like the Chinese alias table and knowledge base
- Why unresolved: The paper only tests on Chinese data, so cross-linguistic performance is unknown
- What evidence would resolve it: Empirical results comparing the approach across multiple languages with different morphological and script characteristics

### Open Question 2
- Question: What is the impact of increasing the maximum sequence length beyond computational constraints on the Description-BM25 model's disambiguation performance?
- Basis in paper: [explicit] "The Description-BM25 model does not exhibit significant disambiguation as expected. We attribute this to the limited maximum length of the context, which is imposed by computational resource constraints"
- Why unresolved: The paper only tests with constrained sequence lengths due to computational limits
- What evidence would resolve it: Controlled experiments varying sequence length while measuring disambiguation accuracy

### Open Question 3
- Question: How does the ensemble method's voting mechanism perform when more than two results are identical in the 2:2 tie situation?
- Basis in paper: [explicit] "In cases where the four predicted results differ, we choose the output from the BERT-based dual encoder... Additionally, there may has a unique 2:2 situation arising during voting. In this case, we opt for the prediction that incorporates the result derived from the BERT-based dual encoder"
- Why unresolved: The paper doesn't provide data on the frequency or impact of 2:2 tie situations
- What evidence would resolve it: Analysis of tie-break scenarios and their impact on overall accuracy across the dataset

## Limitations
- Implementation details for critical components like the ensemble method and BERT dual encoder hyperparameters are not provided
- The paper lacks ablation studies to isolate the contribution of each component to the final accuracy
- Performance analysis is limited to the specific Hansel dataset size without examining scalability with larger knowledge bases

## Confidence

- **High Confidence**: The coarse-to-fine retrieval mechanism using BM25 models is well-established in information retrieval literature and the dual encoder architecture for reranking is a proven approach
- **Medium Confidence**: The ensemble method combining four different sources should provide robustness, though the specific voting mechanism details are unclear
- **Low Confidence**: The claim that this approach is particularly effective for tail and emerging entities relies heavily on the assumption that entity descriptions provide sufficient discriminative information, which may not hold for entities with sparse or generic descriptions

## Next Checks
1. Implement ablation studies removing each component (AT-BM25, KB-BM25, Description-BM25, dual encoder reranking) to quantify their individual contributions to the 0.6915 accuracy score

2. Verify the dataset composition by testing the system's performance separately on known entities (Eknown) versus new entities (Enew) to confirm the claimed effectiveness for zero-shot scenarios

3. Experiment with different ensemble voting mechanisms (weighted voting, rank-based aggregation) to determine if the reported accuracy depends on the specific voting approach or if the results are robust across ensemble methods