---
ver: rpa2
title: Provable Robustness Against a Union of $\ell_0$ Adversarial Attacks
arxiv_id: '2302.11628'
source_url: https://arxiv.org/abs/2302.11628
tags:
- certi
- feature
- robustness
- training
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes feature partition aggregation (FPA), a certified\
  \ defense against a union of \u21130 adversarial attacks including evasion, poisoning,\
  \ and backdoor attacks. FPA partitions the feature set across an ensemble of submodels,\
  \ where each submodel is trained on a disjoint subset of features."
---

# Provable Robustness Against a Union of $\ell_0$ Adversarial Attacks

## Quick Facts
- arXiv ID: 2302.11628
- Source URL: https://arxiv.org/abs/2302.11628
- Reference count: 40
- This paper proposes feature partition aggregation (FPA), a certified defense against a union of $\ell_0$ adversarial attacks including evasion, poisoning, and backdoor attacks.

## Executive Summary
This paper introduces Feature Partition Aggregation (FPA), a novel certified defense mechanism against $\ell_0$ adversarial attacks. FPA partitions features across an ensemble of submodels, each trained on disjoint feature subsets, providing deterministic robustness guarantees. The method significantly outperforms state-of-the-art $\ell_0$ certified defenses, offering larger median robustness guarantees while being up to 3,000× faster. FPA is model-agnostic and provides stronger guarantees by being robust against both training set and test instance perturbations.

## Method Summary
FPA partitions the feature set across an ensemble of submodels, where each submodel is trained on a disjoint subset of features. This approach guarantees that an attacker must perturb at least $\lfloor(\text{cypl}(x) - \max_{y'\neq y}\text{pl}(\text{cy}'(x) + 1[y' < y\text{pl}])) / 2\rfloor$ features to change the ensemble prediction. The method provides deterministic robustness guarantees against both evasion and poisoning attacks simultaneously, with a greedy algorithm for certifying top-k predictions by minimizing the margin between target label votes and runner-up votes.

## Key Results
- FPA achieves median robustness guarantees of 13 pixels over 10 for CIFAR10 and 12 pixels over 10 for MNIST
- FPA provides 4 features over 1 for Weather and 3 features over 1 for Ames datasets
- FPA is up to 3,000× faster than state-of-the-art $\ell_0$ certified defenses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature partition aggregation (FPA) guarantees that an attacker must perturb at least $\lfloor(\text{cypl}(x) - \max_{y'\neq y}\text{pl}(\text{cy}'(x) + 1[y' < y\text{pl}])) / 2\rfloor$ features to change the ensemble prediction
- Mechanism: By training submodels on disjoint feature sets, each adversarial perturbation can only affect at most one submodel's vote. The robustness guarantee follows from the minimum number of votes that must change to alter the plurality label
- Core assumption: Submodels are deterministic and trained on disjoint feature sets
- Evidence anchors:
  - [abstract] "FPA partitions the feature set across an ensemble of submodels, where each submodel is trained on a disjoint subset of features"
  - [section 4.1] Theorem 3 provides the mathematical formulation of this guarantee
- Break condition: If submodels are not deterministic or feature sets overlap, the guarantee fails

### Mechanism 2
- Claim: FPA provides deterministic robustness guarantees against both evasion and poisoning attacks simultaneously
- Mechanism: The same feature partitioning that limits evasion attack effectiveness also limits poisoning attack effectiveness since each perturbed feature affects only one submodel
- Core assumption: The attacker can control any subset of features in both training and test data
- Evidence anchors:
  - [abstract] "FPA generates its stronger robustness guarantees via an ensemble whose submodels are trained on disjoint feature sets"
  - [section 2] Threat model allows 100% poisoning rate
- Break condition: If the attacker can influence multiple submodels through a single feature perturbation

### Mechanism 3
- Claim: FPA's greedy algorithm optimally certifies top-k predictions by minimizing the margin between target label votes and runner-up votes
- Mechanism: The greedy algorithm iteratively transfers votes from the target label to the (k+1)-th most voted label, minimizing the margin at each step
- Core assumption: The greedy strategy of always transferring from the most efficient label is optimal
- Evidence anchors:
  - [section 4.2] "Our greedy, top-k certification algorithm applies these two approaches simultaneously by iteratively switching submodel predictions from y to ˜y"
  - [section 4.2] Theorem 4 proves the optimality of this greedy strategy
- Break condition: If the greedy algorithm doesn't consider all possible vote transfer sequences

## Foundational Learning

- Concept: Ensemble methods and voting-based decision functions
  - Why needed here: FPA's robustness guarantees depend on the ensemble voting mechanism where each submodel contributes one vote
  - Quick check question: If you have 5 submodels with votes [3,1,1] for labels [A,B,C], what is the prediction and how many votes must change to flip it?

- Concept: $\ell_0$-norm (sparse) adversarial attacks
  - Why needed here: FPA specifically defends against $\ell_0$ attacks where an unknown subset of features can be arbitrarily perturbed
  - Quick check question: What is the difference between $\ell_0$-norm robustness and feature robustness as defined in this paper?

- Concept: Randomized smoothing and its limitations
  - Why needed here: FPA is compared against randomized ablation (RA), a smoothing-based defense, highlighting FPA's advantages
  - Quick check question: Why does randomized ablation require up to 100k forward passes per prediction while FPA requires only T passes?

## Architecture Onboarding

- Component map: Feature partition aggregation consists of (1) feature partitioning strategy, (2) submodel training on disjoint feature subsets, (3) ensemble voting mechanism, and (4) greedy top-k certification algorithm
- Critical path: (1) Partition features → (2) Train submodels → (3) Make predictions → (4) Certify robustness using greedy algorithm
- Design tradeoffs: Fewer partitions → better accuracy, worse robustness; More partitions → worse accuracy, better robustness
- Failure signatures: Poor performance on vision datasets with random partitioning, degraded accuracy when partitioning training instances, NP-hard complexity with overlapping feature sets
- First 3 experiments:
  1. Implement FPA with T=3 submodels on a simple 2D dataset and verify Theorem 3's guarantee holds
  2. Compare FPA with RA on MNIST using the same base model architecture
  3. Test different feature partitioning strategies (random vs. strided) on CIFAR10 and measure accuracy/robustness tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does FPA's performance advantage over RA persist when both methods are allowed to optimize their hyperparameters (T and e) independently for each robustness level?
- Basis in paper: [inferred] The paper notes that "Due to limited space, the next section considers only two hyperparameter settings per method" and "Such tuning significantly increases the methods' training cost for often just modest performance gains."
- Why unresolved: The authors chose not to perform exhaustive hyperparameter tuning due to computational cost, but this leaves open whether the reported performance gap would narrow with optimal tuning.
- What evidence would resolve it: Experiments showing FPA vs RA performance curves where T and e are independently optimized for each robustness level would reveal if the performance gap persists.

### Open Question 2
- Question: How does FPA's certified feature robustness scale with the dimensionality of the feature space, particularly for high-dimensional data like full ImageNet?
- Basis in paper: [inferred] The paper evaluates on datasets ranging from 128 to 1024 features but notes "Existing certified poisoning defenses do not evaluate on full ImageNet due to the high training cost."
- Why unresolved: The paper demonstrates strong performance on moderate-dimensional datasets but does not explore the method's behavior as feature dimensionality increases dramatically.
- What evidence would resolve it: Experiments applying FPA to progressively higher-dimensional datasets (e.g., ImageNet subsets with increasing resolution) would show whether the method's performance degrades or maintains its advantages.

### Open Question 3
- Question: How sensitive is FPA's performance to the specific feature partitioning strategy (random vs. deterministic) across different data modalities and model architectures?
- Basis in paper: [explicit] "Sec. 5.3 empirically compares random and deterministic partitioning. In short, a simple strided strategy that distributes features regularly across an image tends to work well for vision."
- Why unresolved: While the paper shows deterministic striding works well for images, it does not systematically explore how partitioning strategy interacts with data type, model architecture, and robustness requirements.
- What evidence would resolve it: A comprehensive ablation study varying both partitioning strategy and model architecture across multiple data modalities (tabular, image, text) would reveal the optimal strategy for different scenarios

## Limitations
- The method requires careful hyperparameter tuning for optimal performance across different datasets
- The greedy algorithm's computational complexity may become prohibitive for very large feature sets
- While FPA excels at $\ell_0$ certification, its performance against other attack types (ℓ∞ or ℓ2) remains unexplored

## Confidence
- Theoretical guarantees (Theorem 3, 4): **High** - Proven mathematically with clear derivations
- Empirical performance claims: **Medium** - Well-validated on four datasets but limited to specific architectures
- Computational efficiency claims: **High** - Straightforward parallel implementation with clear complexity analysis
- Transferability across domains: **Medium** - Demonstrated on vision and tabular data but not extensively tested on other modalities

## Next Checks
1. Implement ablation studies testing FPA's sensitivity to hyperparameter choices (number of partitions T, feature partitioning strategy) across all four datasets
2. Benchmark FPA against non-certified defenses (standard adversarial training, PGD-based methods) to contextualize the robustness-accuracy tradeoff
3. Evaluate FPA's runtime performance on larger-scale vision datasets (e.g., CIFAR100, TinyImageNet) to verify the claimed efficiency advantages scale appropriately