---
ver: rpa2
title: 'People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually
  Augmented Data for Harmful Language Detection'
arxiv_id: '2311.01270'
source_url: https://arxiv.org/abs/2311.01270
tags:
- cads
- hate
- data
- chatgpt
- sexism
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates whether automated methods can generate effective
  counterfactually augmented data (CADs) for improving model robustness in harmful
  language detection. The authors compare manual CADs to those generated by Polyjuice,
  ChatGPT, and Flan-T5 for sexism and hate speech detection.
---

# People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection

## Quick Facts
- arXiv ID: 2311.01270
- Source URL: https://arxiv.org/abs/2311.01270
- Reference count: 40
- Primary result: Manual CADs are most effective for improving harmful language detection, but ChatGPT-generated CADs come close and outperform other automated methods

## Executive Summary
This paper evaluates whether automated methods can generate effective counterfactually augmented data (CADs) for improving model robustness in harmful language detection. The authors compare manual CADs to those generated by Polyjuice, ChatGPT, and Flan-T5 for sexism and hate speech detection. Results show that while manual CADs are most effective, ChatGPT-generated CADs come close and outperform other automated methods. However, automated CADs often make insufficient changes to flip the original label, leading to mislabeled instances. The study finds that the generation mechanism, edit types, and semantic distance from the original instance impact CAD learnability.

## Method Summary
The study compares manual CADs from previous work with automated CADs generated by Polyjuice, ChatGPT, and Flan-T5 using specific prompts. Models (RoBERTa, Flan-T5, and SVM) are trained on 50% original data and 50% CADs, then evaluated on multiple out-of-domain datasets. Pointwise V-Information (PVI) scores are computed to assess instance-level difficulty and identify mislabeled CADs. The authors analyze edit types and semantic similarity between CADs and originals to understand what makes CADs effective.

## Key Results
- Manual CADs achieve the highest out-of-domain F1 scores (0.78/0.75 for sexism/hate speech) but are time-consuming to generate
- ChatGPT CADs perform nearly as well (0.77/0.71 F1) and significantly outperform Polyjuice and Flan-T5
- Automated CADs often fail to flip labels sufficiently, with 15-30% potentially mislabeled
- ChatGPT and manual CADs make more extensive edits (especially deleting identity words) compared to other automated methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual augmentation improves model robustness by explicitly teaching models to ignore spurious correlations and focus on the core construct.
- Mechanism: By training on pairs of original and counterfactually augmented examples that differ only in the construct label, models learn to distinguish between features tied to the construct versus those that are merely correlated with the label.
- Core assumption: The minimal changes introduced to create CADs are sufficient to flip the label without introducing new spurious correlations.
- Evidence anchors:
  - [abstract] "CADs introduce minimal changes to existing training data points and flip their labels; training on them may reduce model dependency on spurious features."
  - [section 4.2] "Our results indicate that CAD generation cannot be fully automated with current LLMs; they require manual checking."
- Break condition: If the changes made to create CADs are insufficient to flip the label or introduce new spurious features, the mechanism breaks down.

### Mechanism 2
- Claim: The generation mechanism of CADs impacts their learnability, with some automated methods producing less effective training examples than others.
- Mechanism: Different CAD generation methods introduce varying amounts of change and semantic similarity to the original, affecting how easily a model can learn from them. Methods that make too few changes or keep the CADs too semantically similar to the original may not provide sufficient signal to flip the label.
- Core assumption: There is an optimal range of change and semantic similarity for CADs to be effective training examples.
- Evidence anchors:
  - [section 4.2] "Flan-T5 and Polyjuice CADs have high difficulty, indicating too little usable information is available from them for a model to learn."
  - [section 4.2] "ChatGPT CADs are more semantically distant (0.67 for both sexism and hate speech). Manual CADs again sit in between (0.81/0.73), indicating that automated CADs make either too many or too few changes compared to manual CADs."
- Break condition: If the generation method consistently produces CADs outside the optimal range of change and semantic similarity, the mechanism breaks down.

### Mechanism 3
- Claim: Instance-level difficulty scores can identify mislabeled CADs and inform which CAD properties make them effective training examples.
- Mechanism: By using information-theoretic measures like PVI scores, we can quantify the learnable information in each CAD instance. Low PVI scores indicate difficult-to-learn instances, which are often mislabeled CADs that did not successfully flip the original label.
- Core assumption: PVI scores are a reliable indicator of instance-level difficulty and can be used to identify mislabeled CADs.
- Evidence anchors:
  - [section 4.2] "Ethayarajh et al. (2022) find the lowest PVI scores to be often indicative of mislabeled instances."
  - [section 4.2] "Indeed, for manual CADs, the labels were vetted, so they are less likely to be mislabeled, while the same cannot be concluded for the automated CADs."
- Break condition: If PVI scores are not reliable indicators of instance-level difficulty or cannot effectively identify mislabeled CADs, the mechanism breaks down.

## Foundational Learning

- Concept: Counterfactual examples
  - Why needed here: Understanding counterfactual examples is crucial for grasping the concept of CADs and their role in improving model robustness.
  - Quick check question: What is the key property of counterfactual examples that makes them useful for training models?

- Concept: Spurious correlations
  - Why needed here: Recognizing spurious correlations is essential for understanding why CADs are needed and how they help models focus on the core construct.
  - Quick check question: How do spurious correlations negatively impact model performance on out-of-domain data?

- Concept: Instance-level difficulty
  - Why needed here: Grasping the concept of instance-level difficulty is crucial for understanding how PVI scores are used to evaluate the effectiveness of CADs.
  - Quick check question: What does a low PVI score indicate about the learnability of a CAD instance?

## Architecture Onboarding

- Component map: CAD generation methods (manual, Polyjuice, ChatGPT, Flan-T5) → Model training and evaluation pipeline → Dataset difficulty analysis using PVI scores
- Critical path: Generate CADs → Train models on original + CAD data → Evaluate on out-of-domain datasets → Analyze CAD properties using PVI scores
- Design tradeoffs: Manual CADs are most effective but time-consuming and expensive to generate. Automated methods are faster but may produce lower quality CADs that require manual vetting.
- Failure signatures: Poor out-of-domain performance, high proportion of mislabeled CADs, low PVI scores for automated CADs
- First 3 experiments:
  1. Train and evaluate models on original data only to establish a baseline.
  2. Generate and evaluate models on manual CADs to assess their effectiveness.
  3. Generate and evaluate models on automated CADs to compare their performance against manual CADs and identify issues.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of automated CAD generation methods vary when using different prompt engineering techniques or model architectures beyond the ones tested in this study?
- Basis in paper: [inferred] The paper mentions that they did not explore prompt optimization techniques for the LLM-based CAD generation methods due to the lack of established quality labels for CADs and computational constraints.
- Why unresolved: The paper does not investigate the impact of different prompt engineering techniques or model architectures on the performance of automated CAD generation methods.
- What evidence would resolve it: Conducting experiments with various prompt engineering techniques and model architectures to generate CADs and evaluating their performance compared to the methods used in this study.

### Open Question 2
- Question: To what extent does the generation mechanism of CADs (e.g., Polyjuice, ChatGPT, Flan-T5) influence the types of edits made (e.g., addition/deletion of negation, gender/identity words, affect words) and their effectiveness in improving model robustness?
- Basis in paper: [explicit] The paper analyzes the edit types made by different CAD generation methods and finds that ChatGPT CADs and manual CADs (for sexism) make extensive changes for all categories, especially deleting identity words (including gender words).
- Why unresolved: While the paper identifies the types of edits made by different CAD generation methods, it does not investigate the relationship between the generation mechanism, edit types, and their impact on model robustness.
- What evidence would resolve it: Conducting a detailed analysis of the relationship between the generation mechanism, edit types, and their effectiveness in improving model robustness by comparing the performance of models trained on CADs with different generation mechanisms and edit types.

### Open Question 3
- Question: How does the use of counterfactually augmented data (CADs) impact the unintended bias towards marginalized communities in hate speech and sexism detection models?
- Basis in paper: [explicit] The paper finds that CADs with identity words are associated more strongly with the hateful class, which could worsen unintended false positive bias by misclassifying non-harmful content.
- Why unresolved: The paper identifies the potential for CADs to exacerbate unintended bias but does not explore the extent of this impact or propose methods to mitigate it.
- What evidence would resolve it: Conducting experiments to measure the impact of CADs on unintended bias towards marginalized communities and developing techniques to mitigate this bias while maintaining model robustness.

## Limitations
- Automated CAD generation methods consistently produce instances that are either too similar to originals or make excessive changes that introduce new spurious features
- 15-30% of automated CADs may be mislabeled, significantly impacting downstream model performance
- PVI scores, while theoretically sound, are applied without sufficient validation of their practical utility in this specific domain

## Confidence
- **High confidence** in findings about manual CAD superiority and the general failure modes of automated methods
- **Medium confidence** in the specific mechanisms proposed for why automated methods fail
- **Low confidence** in the generalizability of these results beyond sexism and hate speech detection domains

## Next Checks
1. Implement a lightweight classifier to automatically flag potentially mislabeled CADs using PVI scores and edit distance metrics as features
2. Design an iterative prompting strategy where initial automated CADs are manually reviewed and the generation prompts are refined based on common failure patterns
3. Test the same CAD generation pipeline on a different NLP task (e.g., sentiment analysis or toxicity detection) to assess whether the observed failure modes are task-specific or generalizable across domains