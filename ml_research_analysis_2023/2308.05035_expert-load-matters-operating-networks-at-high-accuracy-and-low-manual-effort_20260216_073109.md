---
ver: rpa2
title: 'Expert load matters: operating networks at high accuracy and low manual effort'
arxiv_id: '2308.05035'
source_url: https://arxiv.org/abs/2308.05035
tags:
- accuracy
- aucoc
- samples
- calibration
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of training neural networks for
  critical human-AI collaboration systems, where samples with low confidence are delegated
  to human experts. The authors propose a new loss function, AUCOCLoss, that maximizes
  the area under the Confidence Operating Characteristics (COC) curve.
---

# Expert load matters: operating networks at high accuracy and low manual effort

## Quick Facts
- arXiv ID: 2308.05035
- Source URL: https://arxiv.org/abs/2308.05035
- Reference count: 40
- Key outcome: AUCOCLoss improves classification accuracy, reduces expert workload, and enhances OOD detection when used as a secondary loss in human-AI collaboration systems.

## Executive Summary
This paper addresses the critical problem of minimizing expert workload in human-AI collaboration systems where low-confidence predictions are delegated to human experts. The authors propose AUCOCLoss, a novel loss function that maximizes the area under the Confidence Operating Characteristics (COC) curve, which captures the trade-off between model accuracy on confident predictions and the number of samples requiring expert review. By formulating this metric using kernel density estimation on confidence predictions, the loss becomes differentiable and can be optimized alongside conventional losses like cross-entropy. Experiments across multiple computer vision and medical image datasets demonstrate that AUCOCLoss consistently improves classification accuracy, reduces expert delegation, enhances out-of-distribution detection, and maintains calibration performance compared to existing approaches.

## Method Summary
The AUCOCLoss function is implemented by estimating the probability density p(r) of confidence scores using kernel density estimation (KDE), then computing the expected accuracy weighted by this density. This creates a differentiable approximation of the area under the COC curve, which can be optimized during training as a secondary loss alongside primary classification losses. The method uses the confidence of the correct class (r*) rather than binary correctness to enable gradient flow through misclassified samples. KDE is applied to confidence predictions from the model, and the bandwidth is selected using Scott's rule. The final training objective combines the conventional loss (cross-entropy or focal loss) with the AUCOCLoss term, weighted by a hyperparameter.

## Key Results
- AUCOCLoss consistently improves classification accuracy across CIFAR100, Tiny-ImageNet, DermaMNIST, RetinaMNIST, and TissueMNIST datasets.
- The method reduces the number of samples delegated to human experts while maintaining or improving model accuracy.
- AUCOCLoss achieves better out-of-distribution detection performance compared to existing methods.
- Calibration performance remains on par with baseline approaches when using AUCOCLoss.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AUCOCLoss explicitly encourages the model to increase accuracy on samples it is confident about while simultaneously reducing the number of samples delegated to experts.
- Mechanism: By maximizing the area under the Confidence Operating Characteristics (COC) curve, the loss function creates a differentiable optimization objective that balances two competing goals: high accuracy on confident predictions and minimal expert workload.
- Core assumption: The confidence scores from the neural network are meaningful indicators of prediction correctness, allowing effective delegation decisions.
- Evidence anchors: [abstract] states the AUCOCLoss "maximizes the area under this COC curve" and "promotes simultaneously the increase in network accuracy and the reduction in number of samples delegated to humans." [section] explains that AUCOC "plots for a varying threshold on algorithm confidence... the accuracy of a model on the samples on which the algorithm is more confident than the threshold versus the number of samples remaining below the threshold."
- Break condition: If the confidence scores are poorly calibrated or do not correlate with actual accuracy, the delegation decisions based on the operating point would be unreliable.

### Mechanism 2
- Claim: The kernel density estimation (KDE) formulation makes the AUCOC metric differentiable, enabling gradient-based optimization during training.
- Mechanism: KDE smooths the discrete confidence predictions into a continuous probability distribution p(r), which allows the expected accuracy E[c|r] to be computed and differentiated with respect to network parameters.
- Core assumption: The Gaussian kernel approximation with Scott's rule bandwidth provides a stable and informative density estimate of confidence scores.
- Evidence anchors: [section] describes using "kernel density estimation (KDE) on confidence predictions rn for training samples to estimate p(r) used in Eq. 8" and shows the formulation with Gaussian kernel. [section] notes "we need to formulate AUCOC in a differentiable way in order for it to be incorporated in a cost function for the training of a neural network."
- Break condition: If the KDE bandwidth selection is inappropriate or the kernel shape poorly approximates the true confidence distribution, the gradient estimates may be noisy or misleading.

### Mechanism 3
- Claim: Using the confidence of the correct class (r*) instead of the binary correctness score (c) enables gradient flow through misclassified samples.
- Mechanism: By replacing the indicator function c with r*, which takes continuous values in [0,1], the loss function can backpropagate through all samples, not just correctly classified ones.
- Core assumption: The confidence score r* for the true class provides meaningful gradient information even when the sample is misclassified.
- Evidence anchors: [section] states "we approximate it as E[c|r]p(r) ≈ 1/N Σ cnK(||r − rn||) where r* n = fθ(yn|xn) is the confidence of the correct class for a sample n" and "we can back-propagate through misclassified samples and we found that this leads to better results." [section] explains "the gradient of the misclassified samples becomes zero because cn is zero when a sample xn is not classified correctly."
- Break condition: If r* is not a reliable signal of how wrong a misclassification is, using it may not effectively guide learning for difficult samples.

## Foundational Learning

- Concept: Confidence calibration
  - Why needed here: The method relies on the assumption that confidence scores reflect true probabilities of correctness, which is central to making good delegation decisions.
  - Quick check question: If a model assigns 0.9 confidence to 100 predictions, approximately how many should be correct for the model to be well-calibrated?

- Concept: Kernel density estimation
  - Why needed here: KDE is used to create a differentiable approximation of the confidence distribution, which is essential for the AUCOC optimization.
  - Quick check question: What is the role of the bandwidth parameter in KDE, and how does Scott's rule determine it?

- Concept: Area under curve metrics (like AUROC)
  - Why needed here: AUCOC is analogous to AUROC but for the accuracy-vs-expert-load trade-off, so understanding how AUC metrics work is foundational.
  - Quick check question: In a binary classification setting, what does the area under the ROC curve represent?

## Architecture Onboarding

- Component map:
  Base classification network -> Softmax layer -> KDE layer -> Correct-class confidence extraction -> AUCOCLoss computation -> Gradient flow to network weights

- Critical path:
  1. Forward pass through base network
  2. Compute confidences and correct-class confidences
  3. Apply KDE to estimate p(r)
  4. Compute AUCOCLoss using KDE estimates
  5. Backpropagate gradients to update network weights

- Design tradeoffs:
  - Batch size affects KDE accuracy: smaller batches may lead to noisier density estimates
  - KDE bandwidth choice impacts gradient smoothness: too small may overfit, too large may oversmooth
  - Weighting between AUCOCLoss and primary loss (e.g., cross-entropy) must be tuned for stable training

- Failure signatures:
  - Training instability or slow convergence when AUCOCLoss is used alone
  - Poor performance on OOD detection despite good in-distribution accuracy
  - Calibration metrics degrading when focusing too much on AUCOC optimization

- First 3 experiments:
  1. Train a simple CNN on CIFAR10 with CE+AUCOCL and verify accuracy and AUCOC improvements over CE alone.
  2. Vary batch size (e.g., 32, 64, 128) and measure impact on AUCOC and calibration metrics.
  3. Test OOD detection performance on CIFAR10 vs. SVHN or CIFAR10-C to verify generalization benefits.

## Open Questions the Paper Calls Out
- Open Question 1: How does the AUCOCLoss perform when the kernel bandwidth in the KDE is tuned using different methods than Scott's rule of thumb? The paper mentions exploring corrections like the reflection method but does not provide detailed comparisons with other bandwidth selection methods.
- Open Question 2: Can the AUCOCLoss be extended to other tasks beyond multi-class classification, such as regression or object detection? The paper mentions that extensions to other tasks are possible future work but does not explore these applications.
- Open Question 3: How does the performance of AUCOCLoss change when using different network architectures or model sizes? The paper uses specific network architectures and does not explore the impact of different architectures on the performance of AUCOCLoss.

## Limitations
- The effectiveness of AUCOCLoss depends on the reliability of confidence scores as indicators of correctness, which may not hold for all model architectures or data distributions.
- Some implementation details like exact KDE bandwidth selection and temperature scaling parameters are not fully specified, affecting reproducibility.
- The real-world impact in clinical settings where expert time is most valuable remains to be validated beyond standard benchmark datasets.

## Confidence
- High: The central claims about AUCOCLoss improving accuracy while reducing expert delegation are directly demonstrated through controlled experiments comparing against baseline losses.
- Medium: The OOD detection improvements and calibration maintenance are supported by experiments but would benefit from more diverse OOD datasets.
- Medium: The theoretical justification for the differentiable KDE formulation is sound, but the practical impact of bandwidth selection and KDE approximations on final performance could vary.

## Next Checks
1. Reproduce the CIFAR100 experiments with Wide-ResNet-28-10 using both CE+AUCOCL and focal+AUCOCL variants to verify the reported accuracy and AUCOC improvements.
2. Test the method on a held-out OOD dataset (e.g., CIFAR10 vs. SVHN) to confirm the AUROC improvements for out-of-distribution detection.
3. Conduct ablation studies varying the KDE bandwidth and batch size to quantify their impact on the final performance metrics.