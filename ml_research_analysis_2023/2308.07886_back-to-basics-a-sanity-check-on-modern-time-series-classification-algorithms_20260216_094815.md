---
ver: rpa2
title: 'Back to Basics: A Sanity Check on Modern Time Series Classification Algorithms'
arxiv_id: '2308.07886'
source_url: https://arxiv.org/abs/2308.07886
tags:
- time
- series
- tabular
- datasets
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper performs a sanity check on modern time series classification
  (TSC) algorithms by comparing tabular machine learning models (e.g., Random Forest,
  Logistic Regression, Ridge) with the ROCKET family of TSC algorithms (Rocket, MiniRocket,
  MultiRocket). The authors evaluate these methods on the UCR/UEA benchmark datasets
  for both univariate and multivariate TSC.
---

# Back to Basics: A Sanity Check on Modern Time Series Classification Algorithms

## Quick Facts
- arXiv ID: 2308.07886
- Source URL: https://arxiv.org/abs/2308.07886
- Reference count: 29
- Key outcome: Tabular ML models outperform ROCKET family on 19% of univariate and 28% of multivariate datasets while being two orders of magnitude faster

## Executive Summary
This paper challenges the assumption that time series classification algorithms are always necessary by comparing simple tabular machine learning models (Random Forest, Logistic Regression, Ridge) with the ROCKET family of TSC algorithms. The authors evaluate these methods on the UCR/UEA benchmark datasets for both univariate and multivariate TSC. Their results show that tabular models can outperform or match ROCKET's accuracy on a significant portion of datasets while being dramatically faster. The paper argues that tabular models should be considered as baselines for evaluating improvements in TSC algorithms and for determining whether a dataset should be included in TSC benchmarks.

## Method Summary
The authors compared tabular ML models (Random Forest, Logistic Regression, RidgeCV) with ROCKET family TSC algorithms (Rocket, MiniRocket, MultiRocket) on 109 univariate and 25 multivariate equal-length datasets from the UCR/UEA benchmark. They used default hyperparameters without tuning, preprocessed multivariate data by standardizing and concatenating channels for tabular models, and recorded both accuracy and training time. The evaluation used critical difference diagrams and accuracy-time tradeoff plots to compare performance across datasets, analyzing performance in three regions: tabular better, within 10 percentage points, and TSC better.

## Key Results
- Tabular models outperform ROCKET family on approximately 19% of univariate and 28% of multivariate datasets
- Tabular models achieve accuracy within 10 percentage points of ROCKET on about 50% of datasets
- Tabular models are two orders of magnitude faster than time series methods
- Performance differences are particularly notable on spectro datasets and some EEG/ECG datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tabular models outperform ROCKET on datasets where temporal structure is weak or absent
- Mechanism: When a time series lacks strong temporal dependencies, the ordering of values provides little signal, so treating the series as a vector of independent features can be as informative as temporal feature extraction
- Core assumption: The benchmark contains datasets with varying degrees of temporal dependency
- Evidence anchors:
  - [abstract] "tabular methods outperform the ROCKET family of classifiers on approximately 19% of univariate and 28% of multivariate datasets"
  - [section] "on about 19% of the benchmark, there are only weak temporal patterns, and tabular methods that disregard time ordering are very competitive"
  - [corpus] Weak evidence: corpus neighbors discuss ROCKET improvements and time series classification methods, but do not directly address tabular baselines or weak temporal datasets
- Break condition: If the dataset contains strong, consistent temporal patterns that ROCKET kernels can capture effectively

### Mechanism 2
- Claim: Tabular models are significantly faster than ROCKET, enabling quick iteration
- Mechanism: Training and inference in tabular models involve fewer feature transformations and less kernel computation, leading to orders of magnitude faster execution
- Core assumption: ROCKET's convolutional kernel generation and pooling steps are computationally expensive
- Evidence anchors:
  - [abstract] "tabular models are also significantly faster, being two orders of magnitude faster than time series methods"
  - [section] "tablular models in the green and grey regions are faster and almost as accurate, or even more accurate than time series methods"
  - [corpus] Weak evidence: corpus neighbors focus on ROCKET variants and not on tabular methods, so no direct runtime comparisons
- Break condition: If runtime constraints are relaxed or if the dataset size is small enough that ROCKET's overhead is negligible

### Mechanism 3
- Claim: Tabular models serve as strong baselines for evaluating time series algorithm improvements
- Mechanism: If a proposed time series method does not outperform a simple tabular model, the gain is questionable, prompting reassessment of method novelty
- Core assumption: The field has historically focused on beating complex time series methods without comparing against simple tabular baselines
- Evidence anchors:
  - [abstract] "it is important to consider simple tabular models as baselines when developing time series classifiers"
  - [section] "We found that tabular models performed surprisingly well on many datasets, outperforming the recent Multirocket classifier on a significant percentage of the datasets"
  - [corpus] Weak evidence: corpus neighbors do not discuss baseline comparisons, focusing instead on algorithmic improvements and variant evaluations
- Break condition: If tabular models are already the best performers, then no further time series-specific innovation is warranted

## Foundational Learning

- Concept: Time series vs tabular data distinction
  - Why needed here: The paper hinges on the observation that not all datasets labeled as time series actually contain meaningful temporal dependencies
  - Quick check question: Can you identify a dataset where shuffling the time order would not affect classification accuracy?

- Concept: Feature transformation pipelines
  - Why needed here: Understanding how ROCKET transforms raw series into kernel-based features clarifies why it may be unnecessary for certain datasets
  - Quick check question: What is the main difference between ROCKET's convolutional feature generation and directly using raw series values as features?

- Concept: Statistical significance testing in comparative ML studies
  - Why needed here: The critical difference diagrams rely on rank-based tests (Wilcoxon signed-rank with Holm correction) to assess whether accuracy differences are meaningful
  - Quick check question: Why might a simple accuracy difference not be enough to claim one method is superior across multiple datasets?

## Architecture Onboarding

- Component map: Data ingestion → Preprocessing (scaling, concatenation for multivariate) → Model training (RandomForest, LogisticRegression, RidgeCV, LDA) → Evaluation (accuracy, time) → Baseline comparison → Separate pipeline for ROCKET family (aeon-toolkit) to ensure consistent evaluation

- Critical path:
  1. Load UCR/UEA benchmark datasets
  2. Preprocess tabular-compatible format
  3. Train and evaluate all tabular models
  4. Train and evaluate ROCKET family
  5. Compute accuracy and time metrics
  6. Generate critical difference diagrams and accuracy-time tradeoff plots

- Design tradeoffs:
  - Tabular models are faster but may miss subtle temporal patterns
  - ROCKET captures complex temporal features but at high computational cost
  - Including datasets without strong temporal structure may inflate time series method performance on benchmarks

- Failure signatures:
  - Tabular models underperforming significantly across all datasets → temporal patterns are strong and consistent
  - ROCKET models underperforming on many datasets → those datasets may not belong in a time series benchmark
  - Runtime discrepancies not matching expected orders of magnitude → possible implementation or hardware issue

- First 3 experiments:
  1. Run all tabular models on a small univariate dataset with weak temporal patterns (e.g., spectro) and verify accuracy is competitive with ROCKET
  2. Time both a tabular model and ROCKET on a medium-sized dataset to confirm the expected runtime gap
  3. Create a synthetic dataset by shuffling a real time series and confirm that tabular and time series methods perform similarly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific dataset characteristics do tabular models consistently outperform time series methods, and can these be used to automatically identify when tabular methods should be preferred?
- Basis in paper: [inferred] The paper notes that tabular models performed particularly well on Spectro datasets and some EEG/ECG datasets, suggesting these domains may lack strong temporal patterns, but doesn't systematically identify the characteristics that predict when tabular methods will outperform.
- Why unresolved: The paper only observes performance differences across domains without identifying specific dataset features (like stationarity, autocorrelation, or feature correlation) that consistently predict tabular model superiority.
- What evidence would resolve it: A systematic analysis of dataset characteristics (e.g., stationarity tests, autocorrelation measures, feature correlation analysis) correlated with classification performance across many datasets, potentially enabling a predictive model for method selection.

### Open Question 2
- Question: Does hyperparameter tuning significantly improve the performance of tabular models for time series classification, and does this improvement justify the additional computational cost?
- Basis in paper: [explicit] The paper mentions that hyperparameter tuning can increase accuracy but takes a significant amount of time, showing a 1 percentage point accuracy gain at the cost of 10x increase in computation time for univariate datasets.
- Why unresolved: The paper provides a limited comparison showing marginal accuracy improvements but doesn't explore the full hyperparameter space or establish whether the accuracy gains justify the computational overhead across different dataset types.
- What evidence would resolve it: Comprehensive hyperparameter tuning experiments across diverse datasets measuring both accuracy improvements and computation time trade-offs, potentially including automated hyperparameter optimization methods.

### Open Question 3
- Question: Should datasets where tabular models perform comparably to or better than time series methods be excluded from time series classification benchmarks, and what criteria should determine this?
- Basis in paper: [explicit] The authors explicitly question whether datasets in the green and grey regions (where tabular methods are competitive or superior) should be included in time series classification benchmarks at all.
- Why unresolved: The paper raises the philosophical and practical question about benchmark composition but doesn't provide clear criteria or methodology for determining which datasets should be excluded.
- What evidence would resolve it: Development of objective criteria (e.g., performance thresholds, temporal feature analysis, domain-specific relevance) for benchmark inclusion, potentially accompanied by empirical studies on how benchmark composition affects algorithm development and evaluation.

## Limitations

- The paper does not fully specify preprocessing details for multivariate tabular models, which could affect reproducibility
- The choice of datasets with weak temporal structure is not quantified; it's unclear how many datasets fall into this category
- The comparison uses default hyperparameters without tuning, which may underrepresent the potential of either approach

## Confidence

- **High confidence** in the observation that tabular models are significantly faster than ROCKET methods
- **Medium confidence** in the claim that tabular models outperform ROCKET on 19% of univariate and 28% of multivariate datasets
- **Medium confidence** in the recommendation to use tabular models as baselines

## Next Checks

1. Reproduce the accuracy and runtime comparison on a small subset of datasets with varying degrees of temporal dependency to confirm the reported performance gaps
2. Test the effect of different preprocessing methods (e.g., standardization, channel concatenation order) on multivariate tabular model performance to identify potential sources of variance
3. Evaluate the impact of hyperparameter tuning on both tabular and ROCKET models to determine if the reported performance differences are robust to optimization