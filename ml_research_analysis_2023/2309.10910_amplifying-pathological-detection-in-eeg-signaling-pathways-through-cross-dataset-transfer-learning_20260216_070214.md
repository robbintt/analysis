---
ver: rpa2
title: Amplifying Pathological Detection in EEG Signaling Pathways through Cross-Dataset
  Transfer Learning
arxiv_id: '2309.10910'
source_url: https://arxiv.org/abs/2309.10910
tags:
- dataset
- data
- learning
- tuab
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the problem of EEG-based pathology detection
  in the presence of limited labeled data. The authors explore the use of transfer
  learning and data scaling techniques to improve model performance on a target dataset
  (NMT) by leveraging knowledge from a larger source dataset (TUAB).
---

# Amplifying Pathological Detection in EEG Signaling Pathways through Cross-Dataset Transfer Learning

## Quick Facts
- **arXiv ID**: 2309.10910
- **Source URL**: https://arxiv.org/abs/2309.10910
- **Reference count**: 31
- **Primary result**: Transfer learning from TUAB to NMT datasets improves EEG pathology detection performance, especially in low-data regimes

## Executive Summary
This paper investigates EEG-based pathology detection using cross-dataset transfer learning to address limited labeled data in the target dataset (NMT). The authors leverage knowledge from a larger source dataset (TUAB) by pretraining models and fine-tuning them on the target dataset. They propose discriminative fine-tuning, which adjusts learning rates from last to early layers, improving transfer performance by preserving general features while adapting task-specific ones. The study also employs Centered Kernel Alignment (CKA) similarity to analyze feature transferability across datasets, revealing that lower layers are more similar across datasets than higher layers.

## Method Summary
The authors preprocess EEG data from two datasets (TUAB and NMT) by selecting 21 electrode positions, trimming signals, downsampling to 100 Hz, clipping at ±800µV, z-scoring channels, and normalizing to [0,1]. They train four EEG models (EEGNet, ShallowNet, Deep4Net, TCN-EEG) on the source dataset (TUAB) with data augmentation and AdamW optimizer. Pretrained models are fine-tuned on the target dataset (NMT) using discriminative fine-tuning with adjusted learning rates. Performance is evaluated using balanced accuracy on both datasets, comparing transfer learning results with models trained from scratch.

## Key Results
- Pre-training on TUAB and fine-tuning on NMT improves performance in low-data regimes (<100 samples)
- Larger models (TCN) perform better on transfer learning tasks, while smaller models (ShallowNet) excel on single datasets
- Discriminative fine-tuning with appropriate learning rate decay factors further improves transfer performance
- CKA similarity analysis reveals decreasing similarity from early to late layers between datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training on TUAB followed by fine-tuning on NMT improves performance in low-data regimes
- Mechanism: Transfer learning leverages learned representations from source dataset to better generalize to target dataset with limited labeled samples
- Core assumption: Source and target datasets share relevant features and distribution shift is not too extreme
- Evidence anchors: Abstract states improvement in target model performance when low labeled data is available; Figure 2 shows pretraining enhances pathology detection in low data regime

### Mechanism 2
- Claim: Discriminative fine-tuning improves transfer performance by adjusting learning rates from last to early layers
- Mechanism: Fine-tuning higher layers more aggressively while preserving lower layers allows adaptation of task-specific features while maintaining general features from source dataset
- Core assumption: Lower layers learn more general features similar across datasets, while higher layers learn task-specific features that may differ
- Evidence anchors: Abstract describes discriminative fine-tuning technique; Figure 5 shows model with discriminative fine-tuning achieves higher BAC score than baseline when learning rate decay factor is 0.9

### Mechanism 3
- Claim: CKA similarity helps identify transferability of representations across datasets
- Mechanism: CKA similarity measures similarity between feature maps learned by different models, identifying which layers are more similar across datasets to guide fine-tuning
- Core assumption: Similarity of feature maps between models trained on different datasets correlates with transferability of those features
- Evidence anchors: Abstract mentions using CKA similarity to compare feature maps; Figure 4 shows networks trained on TUAB exhibit more block structure as network size increases

## Foundational Learning

- **Concept: Transfer learning**
  - Why needed here: Target dataset has limited labeled samples, making it difficult to train high-performing model from scratch; transfer learning leverages knowledge from larger source dataset
  - Quick check question: What is the main advantage of using transfer learning in scenarios with limited labeled data?

- **Concept: Centered Kernel Alignment (CKA) similarity**
  - Why needed here: CKA similarity provides quantitative measure of similarity between feature maps learned by different models on two datasets, helping identify which layers are more similar across datasets
  - Quick check question: How does CKA similarity help in identifying the transferability of representations across different datasets?

- **Concept: Discriminative fine-tuning**
  - Why needed here: Allows fine-tuning that preserves general features learned from source dataset while adapting task-specific features to target dataset, improving overall transfer performance
  - Quick check question: What is the main idea behind discriminative fine-tuning, and how does it differ from standard fine-tuning?

## Architecture Onboarding

- **Component map**: TUAB (source dataset) -> NMT (target dataset) -> EEGNet/ShallowNet/Deep4Net/TCN-EEG (models) -> Discriminative fine-tuning -> BAC evaluation

- **Critical path**: 
  1. Preprocess TUAB and NMT datasets
  2. Train models on TUAB dataset
  3. Fine-tune pretrained models on NMT dataset using discriminative fine-tuning
  4. Evaluate performance on NMT test set using BAC

- **Design tradeoffs**:
  - Model size: Larger models (TCN) better for transfer learning, smaller models (ShallowNet) better for single datasets
  - Data augmentation: Improves robustness but adds complexity and computational overhead
  - Learning rate decay factor: Critical for effective discriminative fine-tuning; inappropriate choice causes instability

- **Failure signatures**:
  - Negative transfer: Pretrained model performs worse than model trained from scratch if source and target datasets are too dissimilar
  - Catastrophic forgetting: Model loses source dataset performance after fine-tuning on target dataset
  - Overfitting: Model overfits to small/noisy target dataset, resulting in poor generalization

- **First 3 experiments**:
  1. Train EEGNet, ShallowNet, Deep4Net, and TCN-EEG models on TUAB dataset and evaluate BAC on TUAB test set
  2. Fine-tune pretrained models on small subset of NMT dataset (e.g., 50 samples) and evaluate BAC on NMT test set, comparing with models trained from scratch
  3. Apply discriminative fine-tuning to pretrained models on NMT dataset with different learning rate decay factors to find optimal setting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance of transfer learning from TUAB to NMT datasets vary across different types of neurological disorders within the abnormal class?
- Basis in paper: [inferred] Paper discusses transfer learning but does not provide detailed breakdown of performance across specific neurological disorders
- Why unresolved: Study focuses on binary classification without delving into specific types of abnormalities
- What evidence would resolve it: Detailed performance metrics for each neurological disorder category when applying transfer learning techniques

### Open Question 2
- Question: What is the impact of different preprocessing techniques on effectiveness of transfer learning in EEG pathology detection?
- Basis in paper: [explicit] Paper mentions preprocessing steps but does not explore their impact on transfer learning effectiveness
- Why unresolved: Preprocessing is acknowledged but paper does not investigate how varying techniques might influence transfer learning outcomes
- What evidence would resolve it: Comparative studies analyzing performance of transfer learning with different preprocessing strategies

### Open Question 3
- Question: How do variations in demographic factors such as age and gender affect transferability of models between TUAB and NMT datasets?
- Basis in paper: [explicit] Paper notes differences in age and gender distributions between datasets, suggesting potential impacts on model transferability
- Why unresolved: Study does not specifically analyze how demographic variations influence transfer learning performance
- What evidence would resolve it: Analysis of model performance across different demographic subgroups to identify significant effects on transferability

## Limitations
- Analysis relies on only two datasets, making it difficult to assess generalizability across diverse EEG datasets
- Discriminative fine-tuning lacks comprehensive ablation studies to determine optimal learning rate decay factors for different architectures
- CKA similarity analysis does not establish clear causal relationship between similarity scores and transfer performance

## Confidence

**High Confidence**: Transfer learning improving performance in low-data regimes is well-established in machine learning literature and aligns with expected behavior when source and target datasets share relevant features.

**Medium Confidence**: Effectiveness of discriminative fine-tuning and relationship between CKA similarity patterns and layer-wise transferability are supported by experiments but need validation across additional datasets and model architectures.

**Low Confidence**: Generalizability to clinical settings with different EEG acquisition protocols, patient populations, or pathology types remains uncertain given limited dataset diversity.

## Next Checks

1. Test proposed transfer learning pipeline on at least three additional EEG pathology detection datasets with varying acquisition protocols, electrode configurations, and patient demographics to assess robustness across diverse conditions.

2. Conduct comprehensive ablation studies varying learning rate decay factors systematically across all four model architectures to establish optimal fine-tuning parameters and identify architecture-specific patterns.

3. Perform controlled experiments comparing CKA similarity patterns with actual transfer performance metrics across multiple dataset pairs to validate whether CKA similarity reliably predicts transferability for EEG pathology detection tasks.