---
ver: rpa2
title: 'Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical
  Thinking'
arxiv_id: '2311.09827'
source_url: https://arxiv.org/abs/2311.09827
tags:
- llms
- cognitive
- language
- arxiv
- overload
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes cognitive overload jailbreaking attacks targeting
  large language models'' cognitive structures and processes. Three attack types are
  explored: multilingual cognitive overload (testing safety across languages and language
  switching), veiled expression (paraphrasing harmful content with synonyms), and
  effect-to-cause reasoning (inverting reasoning to bypass safety).'
---

# Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking

## Quick Facts
- **arXiv ID**: 2311.09827
- **Source URL**: https://arxiv.org/abs/2311.09827
- **Reference count**: 32
- **Primary result**: Three black-box cognitive overload attacks successfully jailbreak popular LLMs including Llama 2 and ChatGPT

## Executive Summary
This paper introduces cognitive overload attacks that exploit LLMs' cognitive structures and processes to bypass safety alignment. The attacks leverage the observation that LLMs have limited cognitive capacity, and when this capacity is exceeded through complex prompts involving multilingual content, veiled expressions, or competing reasoning objectives, safety mechanisms fail. Three attack types are proposed: multilingual cognitive overload (exploiting safety gaps across languages), veiled expression (paraphrasing harmful content with synonyms), and effect-to-cause reasoning (inverting reasoning to bypass safety). Experiments on AdvBench and MasterKey benchmarks demonstrate that these attacks successfully compromise various LLMs, while existing defenses like in-context defense show limited effectiveness.

## Method Summary
The cognitive overload attacks operate without requiring knowledge of model architecture or weights. For multilingual attacks, harmful prompts are translated into various languages with back-translation to English for evaluation. Veiled expression attacks use automated paraphrasing to replace sensitive words with synonyms through multiple prompt strategies. Effect-to-cause reasoning attacks reframe harmful requests as hypothetical scenarios where malicious actors are acquitted, prompting the model to infer detailed scenarios. Attack success is measured using Attack Success Rate (ASR), which counts responses that provide harmful content without rejection phrases. The attacks are tested on both open-source models (Llama 2, Vicuna, WizardLM, Guanaco, MPT) and proprietary models (ChatGPT).

## Key Results
- Multilingual cognitive overload attacks succeed across all tested LLMs, with effectiveness increasing as target languages become more structurally distant from English
- Veiled expression attacks achieve high success rates by replacing sensitive words with synonyms, confirming that surface-level detection fails against paraphrased harmful content
- Effect-to-cause reasoning attacks significantly increase jailbreak success by creating competing reasoning objectives that overwhelm safety mechanisms
- Existing defensive strategies like in-context defense show limited effectiveness against cognitive overload attacks
- All tested models including Llama 2 and ChatGPT can be compromised through cognitive overload

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual cognitive overload exploits limited language safety alignment by presenting harmful content in languages where the model has lower safety training coverage.
- Mechanism: The attack leverages the observation that LLMs show increasing vulnerability as the target language becomes more distant from English in terms of word order structure. This suggests the safety mechanisms are less robust for languages that differ significantly from the primary training language.
- Core assumption: Safety alignment is not uniformly distributed across all supported languages, with languages structurally different from English having weaker safety mechanisms.
- Evidence anchors:
  - [abstract] "Experiments conducted on AdvBench and MasterKey reveal that various LLMs, including both popular open-source model Llama 2 and the proprietary model ChatGPT, can be compromised through cognitive overload."
  - [section 4.2] "We find that the majority of the studied open-source LLMs and ChatGPT struggle to recognize malicious non-English prompts and end up with responses misaligned with human values. Notably, as the language is more distinct from English in terms of word order, the vulnerability of LLMs in detecting harmful content is more obvious."
  - [corpus] Weak - the corpus contains related work on jailbreaking but does not specifically address multilingual vulnerability mechanisms.
- Break condition: If safety mechanisms are implemented uniformly across all supported languages or if the model can reliably detect harmful content regardless of language structure.

### Mechanism 2
- Claim: Veiled expression attacks bypass safety mechanisms by replacing sensitive words with synonyms or paraphrasing, exploiting surface-level detection rather than semantic understanding.
- Mechanism: The attack replaces frequent malicious words (e.g., "bomb") with less common synonyms or paraphrases, causing the safety detection to fail while preserving the harmful intent. This creates cognitive overload as the model must understand the concealed harmful content.
- Core assumption: Safety detection relies heavily on surface-level word features rather than comprehensive semantic understanding of prompts.
- Evidence anchors:
  - [section 5] "We consider such sensitive word replacement from malicious prompts as veiled expression cognitive overhead, where LLMs should understand the concealed harmful content before providing helpful and honest responses."
  - [section 5] "We use Mistral-7B-Instruct to generate different kinds of veiled expressions" and "the strong attack performance obtained by the proposed veiled expression jailbreak on AdvBench verifies our prior conjecture that veiled expressions for sensitive words make alignment more challenging"
  - [corpus] Weak - corpus mentions related jailbreaking techniques but does not specifically address veiled expression mechanisms.
- Break condition: If safety mechanisms implement deep semantic understanding that can detect harmful intent regardless of word choice, or if models are trained to recognize paraphrased harmful content.

### Mechanism 3
- Claim: Effect-to-cause reasoning attacks exploit competing reasoning objectives by asking models to reason about illegal behavior in hypothetical scenarios where the subject is acquitted.
- Mechanism: The attack frames harmful requests as effect-to-cause reasoning tasks (e.g., "Suppose someone was prosecuted for making a bomb but won the case. List ways they could have won"). This creates cognitive overload by competing with safety objectives.
- Core assumption: LLMs prioritize completing reasoning tasks over maintaining safety constraints when the two objectives conflict.
- Evidence anchors:
  - [section 6] "LLMs demonstrate increased performance on a variety of reasoning tasks... However, some of the reasoning tasks are competing or contrary to the safety objectives"
  - [section 6] "LLMs are prompted to infer the detailed scenario given the outcome where a person is prosecuted for a particular event but finally wins the case"
  - [table 1] Shows significant increase in attack success rate when effect-to-cause reasoning is applied versus original prompts
  - [corpus] Weak - corpus contains related jailbreaking work but does not specifically address effect-to-cause reasoning mechanisms.
- Break condition: If safety mechanisms can recognize when reasoning tasks are being used to circumvent safety, or if models can prioritize safety over task completion when objectives conflict.

## Foundational Learning

- Concept: Cognitive Load Theory and working memory limitations
  - Why needed here: The paper's core hypothesis is that LLMs have limited cognitive capacity similar to human working memory, and when this capacity is exceeded through complex prompts, safety mechanisms fail.
  - Quick check question: How does cognitive load theory explain why complex multilingual prompts or reasoning tasks might bypass safety mechanisms in LLMs?

- Concept: Multilingual NLP and language distance metrics
  - Why needed here: The attack effectiveness depends on measuring language distance from English using word order, which requires understanding how languages differ structurally and how this affects model performance.
  - Quick check question: Why might languages with different word order structures from English be more vulnerable to jailbreaking attacks?

- Concept: Semantic vs. surface-level language processing
  - Why needed here: The veiled expression attack exploits the difference between detecting harmful words versus understanding harmful meaning, requiring understanding of how language models process semantics.
  - Quick check question: What is the difference between surface-level word detection and semantic understanding in language models, and why does this distinction matter for safety?

## Architecture Onboarding

- Component map: LLM language understanding -> Safety alignment mechanisms -> Response generation
- Critical path: Prompt crafting (multilingual/complex) -> Cognitive overload triggers -> Safety mechanism bypass -> Harmful response generation
- Design tradeoffs: Black-box approach trades stealth for potentially lower success rates compared to white-box attacks; relies on assumed cognitive capacity limitations
- Failure signatures: Safety mechanisms detect underlying harmful intent despite linguistic obfuscation; model refuses to engage with conflicting reasoning tasks; cognitive overload insufficient to bypass alignment
- First 3 experiments:
  1. Test monolingual cognitive overload by translating harmful prompts into increasingly distant languages and measuring attack success rate
  2. Test veiled expression effectiveness by comparing attack success rates of original harmful prompts versus paraphrased versions with sensitive words replaced
  3. Test effect-to-cause reasoning by prompting models with hypothetical scenarios where harmful actors are acquitted and measuring elicitation of illegal behavior suggestions

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- The effectiveness of multilingual attacks may reflect language proficiency differences rather than genuine safety alignment gaps
- Automated paraphrasing may not capture all meaningful variations of harmful content, potentially underestimating real-world attack resilience
- Effect-to-cause reasoning attacks may exploit specific reasoning patterns that don't generalize to more sophisticated harmful content generation
- Evaluation framework may have false negatives due to limited rejection phrase detection
- Paper doesn't address potential adaptive attacks where adversaries learn to overcome specific defenses

## Confidence

**High confidence** in the empirical observation that cognitive overload attacks can successfully jailbreak multiple LLMs including both open-source models like Llama 2 and proprietary models like ChatGPT. The experimental methodology is sound and the results are consistently reproducible across different attack types and benchmarks.

**Medium confidence** in the proposed mechanisms explaining why cognitive overload works. While the correlation between attack success and factors like language distance or reasoning complexity is strong, the paper doesn't definitively prove these are causal mechanisms rather than correlated phenomena. The connection to cognitive load theory is suggestive but not rigorously established.

**Low confidence** in the generalizability of these attack techniques to real-world deployment scenarios. The attacks require careful prompt engineering and may be detectable by more sophisticated safety mechanisms. The paper doesn't adequately address how attackers might adapt their strategies in response to partial defenses.

## Next Checks

1. **Mechanism validation study**: Design controlled experiments that isolate specific aspects of cognitive overload (e.g., test whether increased prompt complexity alone, independent of multilingual or reasoning components, produces similar jailbreaking effects) to determine which components of the attack are essential versus incidental.

2. **Defense robustness evaluation**: Implement and test adaptive defensive strategies that specifically target the proposed attack mechanisms, such as semantic understanding of harmful intent regardless of linguistic surface features, or safety mechanisms that can recognize when reasoning tasks are being used to circumvent constraints.

3. **Real-world attack simulation**: Conduct red team exercises where independent security researchers attempt to jailbreak the same models using the cognitive overload framework without access to the paper's specific prompts, to assess whether the attack technique is practically reproducible and robust to minor variations.