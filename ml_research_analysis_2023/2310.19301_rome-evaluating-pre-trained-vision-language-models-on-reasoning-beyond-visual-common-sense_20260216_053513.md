---
ver: rpa2
title: 'ROME: Evaluating Pre-trained Vision-Language Models on Reasoning beyond Visual
  Common Sense'
arxiv_id: '2310.19301'
source_url: https://arxiv.org/abs/2310.19301
tags:
- commonsense
- images
- counter-intuitive
- image
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ROME, a novel dataset to evaluate the ability
  of pre-trained vision-language models to reason beyond common sense. The dataset
  contains 1,563 images depicting counter-intuitive scenarios related to color, shape,
  material, size, and positional relation.
---

# ROME: Evaluating Pre-trained Vision-Language Models on Reasoning beyond Visual Common Sense

## Quick Facts
- arXiv ID: 2310.19301
- Source URL: https://arxiv.org/abs/2310.19301
- Reference count: 7
- Key outcome: VLMs perform well on counter-intuitive object recognition but struggle with counter-intuitive attribute and spatial relation recognition

## Executive Summary
This paper introduces ROME, a novel dataset designed to evaluate pre-trained vision-language models (VLMs) on reasoning beyond visual common sense. The dataset contains 1,563 images depicting counter-intuitive scenarios related to color, shape, material, size, and positional relation. The authors design two groups of probing tasks in the form of binary visual questions to assess both commonsense knowledge and reasoning abilities beyond common sense. The results show that while the models perform well on counter-intuitive object recognition, they struggle with recognizing counter-intuitive attributes and spatial relations. The authors hope that ROME will spur further investigations on reasoning beyond commonsense knowledge in vision-language research.

## Method Summary
The authors created the ROME dataset by generating counter-intuitive images using text-to-image models like DALL-E-2 and Stable Diffusion, then manually curating and annotating them. They designed binary probing questions in two groups: one testing commonsense knowledge (CS) and another testing counter-intuitive reasoning (CI). The evaluation framework measures four metrics: CI-Obj (counter-intuitive object recognition), CI-AttrRel (counter-intuitive attribute/relation recognition), CS-L (commonsense based on language), and CS-VL (commonsense based on vision and language). The study evaluates six pre-trained VLMs on this dataset to assess their reasoning capabilities.

## Key Results
- VLMs perform very well on counter-intuitive object recognition (CI-Obj scores from 90.79% to 98.34%)
- VLMs struggle significantly with counter-intuitive attribute and relation recognition (CI-AttrRel scores mostly below 60%)
- Models show inconsistent answers to commonsense questions, even when asked about the same knowledge in different phrasings
- Presenting counter-intuitive images impairs commonsense reasoning performance further (CS-VL scores lower than CS-L scores)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VLMs fail at commonsense reasoning due to inconsistent answer patterns when asked the same underlying knowledge in different phrasings.
- Mechanism: The probing framework reveals that VLMs answer binary commonsense questions inconsistently, even when given a non-informative image, suggesting that their internal reasoning is not robust to question formulation.
- Core assumption: Inconsistent answers reflect a fundamental lack of true commonsense knowledge rather than superficial surface-level processing.
- Evidence anchors:
  - [abstract] states "the models perform poorly on our commonsense questions, often giving inconsistent answers to a pair of questions asking about the same commonsense knowledge."
  - [section] reports "we find that the mistakes made by the VLMs we are evaluating are usually due to inconsistent responses" and gives an example where mPLUG-Owl gives contradictory answers to size comparison questions.
  - [corpus] contains related work on evaluating commonsense reasoning in VLMs, but does not specifically address inconsistency patterns.
- Break condition: If VLMs could be shown to give consistent answers when questions are rephrased or when given different types of non-informative images, this mechanism would be weakened.

### Mechanism 2
- Claim: Counter-intuitive images impair commonsense reasoning by creating visual distractions that confuse VLMs.
- Mechanism: When counter-intuitive images are presented alongside commonsense questions, VLMs' performance drops further compared to when given non-informative images, indicating that the visual input interferes with their reasoning process.
- Core assumption: The drop in performance is due to the VLMs being "distracted" by the counter-intuitive visual content rather than simply lacking the commonsense knowledge.
- Evidence anchors:
  - [abstract] notes "interestingly, the models perform poorly on our commonsense questions, often giving inconsistent answers to a pair of questions asking about the same commonsense knowledge."
  - [section] compares CS-L and CS-VL scores, showing that "once the counter-intuitive images are presented, the commonsense reasoning of all models is further impaired; the CS-VL scores are lower."
  - [corpus] includes related work on evaluating VLMs but does not directly address the impact of counter-intuitive images on commonsense reasoning.
- Break condition: If VLMs maintained consistent performance on commonsense questions regardless of whether counter-intuitive or non-informative images were shown, this mechanism would be invalid.

### Mechanism 3
- Claim: VLMs can recognize counter-intuitive objects but struggle with counter-intuitive attributes and spatial relations.
- Mechanism: The probing framework distinguishes between object recognition (CI-Obj) and attribute/relation recognition (CI-AttrRel), revealing that VLMs perform well on the former but poorly on the latter when dealing with counter-intuitive content.
- Core assumption: Object recognition and attribute/relation recognition are separate capabilities, and VLMs have developed one but not the other when it comes to counter-intuitive scenarios.
- Evidence anchors:
  - [abstract] states "although these state-of-the-art VLMs are generally effective at counter-intuitive object recognition, most of them perform poorly in terms of counter-intuitive attribute and relation recognition."
  - [section] reports "all models perform very well in counter-intuitive object recognition" with CI-Obj scores from 90.79% to 98.34%, but "the models' scores are low, except maybe for InstructBLIP, which achieves the highest CI-AttrRel of 63.72%."
  - [corpus] contains related work on evaluating VLMs but does not specifically address the distinction between object and attribute/relation recognition in counter-intuitive contexts.
- Break condition: If VLMs were shown to perform equally well or poorly on both object and attribute/relation recognition in counter-intuitive scenarios, this mechanism would be challenged.

## Foundational Learning

- Concept: Visual commonsense knowledge
  - Why needed here: The paper's core premise is that humans have visual commonsense knowledge (e.g., fish belong inside fishbowls) and that VLMs should be evaluated on both possessing this knowledge and reasoning beyond it.
  - Quick check question: Can you explain what visual commonsense knowledge is and give an example that distinguishes it from general commonsense knowledge?

- Concept: Counter-intuitive scenarios
  - Why needed here: The ROME dataset is built around counter-intuitive scenarios that defy visual commonsense, and the evaluation framework tests VLMs' ability to reason in these scenarios.
  - Quick check question: How would you define a counter-intuitive scenario in the context of visual reasoning, and why are they important for evaluating VLMs?

- Concept: Binary question probing
  - Why needed here: The evaluation framework uses binary (yes/no) questions to probe VLMs, which allows for automated processing but may not fully capture their reasoning capabilities.
  - Quick check question: What are the advantages and limitations of using binary questions to evaluate VLMs' reasoning abilities, and how might this choice affect the results?

## Architecture Onboarding

- Component map: Image generation -> Human annotation -> Question instantiation -> VLM evaluation -> Metric calculation
- Critical path: Image generation → Human annotation → Question instantiation → VLM evaluation → Metric calculation
- Design tradeoffs: Using binary questions simplifies automated processing but may not capture the full depth of VLMs' reasoning; relying on human annotation introduces potential bias but ensures image quality; focusing on pre-trained VLMs allows for broad evaluation but may miss nuances of fine-tuned models.
- Failure signatures: Inconsistent answers to probing questions, poor performance on counter-intuitive attribute/relation recognition despite good object recognition, hallucination of non-existent objects in images.
- First 3 experiments:
  1. Evaluate a pre-trained VLM on the ROME dataset using the provided binary probing questions and calculate the four evaluation metrics.
  2. Compare the VLM's performance on commonsense questions with and without counter-intuitive images to assess the impact of visual distractions.
  3. Analyze the VLM's responses to open-ended questions about visual commonsense knowledge to check if the binary question format is limiting the evaluation.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided text.

## Limitations

- The evaluation framework's reliance on binary questions may not capture the full complexity of VLMs' reasoning capabilities
- Inconsistent performance patterns suggest the probing questions may be triggering surface-level heuristics rather than measuring true commonsense reasoning
- The distinction between object recognition and attribute/relation recognition in counter-intuitive scenarios requires further validation with more nuanced evaluation metrics

## Confidence

- Confidence in core findings: Medium
  - The dataset construction and evaluation methodology are well-documented
  - Binary question format and potential for question phrasing effects introduce uncertainty
- Confidence in the observed drop in commonsense reasoning performance: Medium
  - Could reflect limitations in models' visual grounding rather than genuine distraction effects
- Confidence in the distinction between object and attribute/relation recognition: Medium
  - Compelling finding but requires further validation

## Next Checks

1. Test model consistency by rephrasing the same commonsense questions in multiple ways and measuring answer variability
2. Evaluate models on the same questions without any images to isolate language-based reasoning from visual reasoning
3. Conduct human evaluation studies to determine if the observed inconsistencies reflect genuine reasoning failures or artifacts of the binary question format