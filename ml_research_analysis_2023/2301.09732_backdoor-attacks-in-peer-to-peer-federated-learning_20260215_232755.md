---
ver: rpa2
title: Backdoor Attacks in Peer-to-Peer Federated Learning
arxiv_id: '2301.09732'
source_url: https://arxiv.org/abs/2301.09732
tags:
- learning
- attack
- attacks
- nodes
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first backdoor attack on peer-to-peer federated
  learning (P2PFL) systems. The authors develop a modular P2PFL architecture that
  separates the learning graph from the communication graph and supports different
  network topologies and learning methods.
---

# Backdoor Attacks in Peer-to-Peer Federated Learning

## Quick Facts
- arXiv ID: 2301.09732
- Source URL: https://arxiv.org/abs/2301.09732
- Reference count: 40
- Key outcome: First backdoor attack on P2PFL systems, achieving 42% attack success with only 5% malicious nodes

## Executive Summary
This paper introduces the first backdoor attack targeting peer-to-peer federated learning systems, demonstrating that strategic node compromise combined with network topology manipulation can achieve significant attack success without degrading model accuracy on clean data. The authors develop a modular P2PFL architecture that separates learning and communication graphs, enabling flexible deployment across different network topologies. Their attack leverages graph centrality metrics like PageRank to identify and compromise the most influential nodes, achieving high attack success rates even with minimal attacker presence in the network.

## Method Summary
The method presents a backdoor attack on peer-to-peer federated learning that strategically selects malicious nodes based on graph centrality metrics, particularly PageRank scores, to maximize influence propagation through the learning topology. The attack employs a BadNets approach with model poisoning amplification, where compromised nodes inject malicious updates during model training. A key innovation is the exploitation of fault tolerance mechanismsâ€”by crashing a small number of nodes, the learning graph restructures in ways that increase the reach of malicious updates. The authors also propose a novel defense mechanism that applies different clipping norms to local model updates versus peer-received updates, successfully mitigating attacks while maintaining model accuracy.

## Key Results
- Only 5% malicious nodes needed for 42% attack success without >2% accuracy drop on clean data
- PageRank-based node selection outperforms random compromise in attack effectiveness
- Node crashes amplify attack success by restructuring the learning graph
- Standard centralized FL defenses fail in P2P settings; proposed differential clipping defense succeeds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attackers can strategically select nodes based on graph centrality metrics to maximize backdoor attack success.
- Mechanism: By choosing nodes with high PageRank scores, the attacker can influence more nodes in the network through the learning topology, spreading malicious updates more effectively.
- Core assumption: The learning graph structure allows influence to propagate from compromised nodes to their neighbors, and centrality metrics identify nodes that maximize this propagation.
- Evidence anchors:
  - [abstract] "leverage graph structural properties like PageRank scores to select the most critical nodes to compromise"
  - [section] "leverages graph centrality metrics such as degree, ENS, and PageRank scores, which provide higher attack success than randomly compromising nodes in the graph"
  - [corpus] Weak evidence - no direct mentions of PageRank in corpus abstracts
- Break condition: If the learning graph is highly disconnected or if nodes only communicate with a fixed small set of peers regardless of centrality, the advantage of strategic selection diminishes.

### Mechanism 2
- Claim: Fault tolerance mechanisms in P2PFL can be exploited to amplify backdoor attacks.
- Mechanism: When nodes crash or fail, the learning graph topology changes, often connecting more nodes to the remaining adversarial nodes, thereby increasing the reach of malicious updates.
- Core assumption: Node failures are not uniformly distributed and can cause the learning graph to restructure in ways that increase the centrality of compromised nodes.
- Evidence anchors:
  - [abstract] "the attack can be further amplified by crashing a small number of nodes"
  - [section] "we evaluate the impact of node failures on backdoor attacks"
  - [corpus] Weak evidence - no direct mentions of node crashes amplifying attacks
- Break condition: If the system has robust failure detection and re-routing that prevents restructuring around compromised nodes, or if failures are uniformly random with no topological impact.

### Mechanism 3
- Claim: Standard federated learning defenses fail in P2P settings because they assume a central server can apply uniform clipping norms.
- Mechanism: In P2PFL, each node trains a personalized model and must decide how to clip both its own updates and those from peers; using the same norm for both leads to either ineffective defense or poor model convergence.
- Core assumption: Local model updates and peer updates have different statistical distributions (norms), so a single clipping threshold cannot adequately balance defense and convergence.
- Evidence anchors:
  - [abstract] "defenses that were proposed in the context of centralized federated learning do not work well in peer-to-peer settings"
  - [section] "gradient clipping defenses are ineffective in the decentralized P2PFL setting"
  - [corpus] Weak evidence - no direct mentions of clipping norm differences in P2P vs centralized
- Break condition: If all updates (local and peer) are normalized to the same scale before aggregation, or if a different defense strategy (e.g., robust aggregation) is used instead of clipping.

## Foundational Learning

- Concept: Federated Learning (FL)
  - Why needed here: Understanding FL is essential because P2PFL builds on its principles but removes the central server, changing the threat model and attack surface.
  - Quick check question: In standard FL, who aggregates the model updates from clients?
- Concept: Graph Centrality Measures (PageRank, Degree, Clustering Coefficient)
  - Why needed here: These metrics are used to select the most influential nodes for placing attackers in the network.
  - Quick check question: What does a high PageRank score indicate about a node in a graph?
- Concept: Backdoor Attacks
  - Why needed here: The attack injects a trigger pattern into training data so that at inference time, samples with the trigger are misclassified to a target class.
  - Quick check question: How does a backdoor attack differ from a standard data poisoning attack in terms of stealth?

## Architecture Onboarding

- Component map: GossipSub communication layer -> Learning layer (defines peer exchanges) -> ML module (local training and aggregation)
- Critical path:
  1. Each node initializes a random model.
  2. Nodes compute local updates on their private data.
  3. Nodes exchange updates with peers defined by the learning topology.
  4. Nodes aggregate received updates with their local update.
  5. Process repeats for T rounds until convergence.
- Design tradeoffs:
  - Separating communication and learning graphs adds flexibility but increases complexity.
  - Personalized models improve privacy but may reduce global model quality compared to consensus models.
  - GossipSub provides robustness but may introduce latency compared to direct TCP connections.
- Failure signatures:
  - Slow convergence or divergence may indicate too aggressive clipping norms.
  - High variance in model accuracy across nodes may indicate uneven attack impact or graph partitioning.
  - Consistent misclassification of backdoor samples indicates successful attack.
- First 3 experiments:
  1. Run the system with no attacks, 60 nodes, Watts-Strogatz graph, EMNIST dataset; verify convergence to ~92% accuracy.
  2. Introduce 3 malicious nodes selected randomly; measure attack success and accuracy drop after 70 rounds.
  3. Introduce 3 malicious nodes selected by PageRank; measure attack success and compare to random selection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would the proposed defense be against more sophisticated backdoor attacks that adaptively modify their model updates based on the defense mechanism?
- Basis in paper: [inferred] The paper only evaluates the defense against the specific backdoor attack using BadNets and model poisoning, but does not explore adaptive attacks that could circumvent the defense.
- Why unresolved: The defense's effectiveness against adaptive attacks is not explored, leaving open the question of whether more sophisticated attacks could defeat it.
- What evidence would resolve it: Evaluating the defense against adaptive backdoor attacks that modify their strategy based on the defense would provide evidence of its robustness.

### Open Question 2
- Question: How would the effectiveness of the attacks and defenses change when using non-IID data distributions across nodes in the P2PFL system?
- Basis in paper: [explicit] The paper mentions evaluating attacks under non-IID data but does not provide results or discuss the impact of non-IID data on attack effectiveness.
- Why unresolved: The paper does not explore how non-IID data distributions affect the success of backdoor attacks and the efficacy of defenses, which is an important practical consideration.
- What evidence would resolve it: Evaluating the attacks and defenses using non-IID data distributions and comparing the results to IID data would provide insights into their robustness.

### Open Question 3
- Question: How would the effectiveness of the attacks and defenses change when using different model architectures beyond deep learning models?
- Basis in paper: [explicit] The paper mentions that the architecture supports different machine learning algorithms but only evaluates attacks and defenses using deep learning models.
- Why unresolved: The paper does not explore the generalizability of the attacks and defenses to other model architectures, such as Bayesian Belief Networks or ADMM, which could have different vulnerabilities and defense requirements.
- What evidence would resolve it: Evaluating the attacks and defenses using different model architectures and comparing the results would provide insights into their generalizability and potential weaknesses.

## Limitations

- Experiments use synthetic P2P topologies rather than real-world network structures, limiting external validity
- Attack success rates measured on small networks (60 nodes) may not scale to larger distributed systems
- Focus on image classification tasks with EMNIST and FashionMNIST limits generalizability to other domains

## Confidence

- **High Confidence**: The architectural separation of learning and communication graphs is well-defined and implementable. The mechanism of using PageRank for node selection is technically sound and theoretically justified.
- **Medium Confidence**: The effectiveness of the proposed defense mechanism (differential clipping norms) is demonstrated, but the generality across different attack vectors and learning tasks requires further validation.
- **Low Confidence**: The claim that existing defenses fail in P2P settings is supported by limited empirical evidence, and the amplification effect through node crashes needs more rigorous statistical validation.

## Next Checks

1. **Scalability Validation**: Replicate the experiments with larger network sizes (100-500 nodes) to verify if the 5% malicious node threshold maintains the same attack success rate, and whether the PageRank selection strategy remains optimal at scale.

2. **Defense Robustness Testing**: Test the differential clipping defense against alternative attack strategies beyond BadNets, including clean-label backdoors and distributed gradient manipulation attacks, to assess its general effectiveness.

3. **Real-world Topology Assessment**: Replace synthetic network topologies with real peer-to-peer network traces or social network data to evaluate how natural network structures affect attack propagation and the validity of centrality-based node selection.