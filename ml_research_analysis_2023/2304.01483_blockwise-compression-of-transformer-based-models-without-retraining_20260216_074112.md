---
ver: rpa2
title: Blockwise Compression of Transformer-based Models without Retraining
arxiv_id: '2304.01483'
source_url: https://arxiv.org/abs/2304.01483
tags:
- int8
- data
- fp32
- compression
- retraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of deploying transformer-based
  models (like BERT) by reducing their memory and computation requirements without
  retraining. The core method, BCT (Blockwise Compression of Transformers), applies
  blockwise quantization to compress model parameters and intermediate results at
  a finer granularity than layerwise approaches, using shift-based symmetric quantization
  and low-bit integer/floating-point arithmetic for matrix multiplications and nonlinear
  operations.
---

# Blockwise Compression of Transformer-based Models without Retraining

## Quick Facts
- arXiv ID: 2304.01483
- Source URL: https://arxiv.org/abs/2304.01483
- Reference count: 12
- Key outcome: BCT achieves less than 0.90% accuracy drop in most tasks, with up to 7.988x compression ratio, outperforming comparable layerwise quantization methods without requiring fine-tuning.

## Executive Summary
This paper introduces BCT (Blockwise Compression of Transformers), a method for compressing transformer-based models like BERT without retraining. The approach uses blockwise quantization to compress model parameters and intermediate results at a finer granularity than layerwise approaches, leveraging shift-based symmetric quantization and low-bit integer/floating-point arithmetic. Experiments on GLUE datasets demonstrate that BCT maintains high accuracy while achieving significant compression ratios.

## Method Summary
BCT applies blockwise quantization using shift-based symmetric quantization to compress weights, biases, and intermediate results. The method divides parameters into blocks, compressing each independently to limit distributional drift. Matrix multiplications use exponent normalization and low-bit accumulation, while nonlinear operations (GELU, Softmax, LayerNorm) employ lookup tables with interpolation. The approach eliminates retraining requirements by using deterministic quantization parameters based on block statistics.

## Key Results
- Achieves less than 0.90% accuracy drop on GLUE tasks
- Reaches up to 7.988x compression ratio
- Outperforms comparable layerwise quantization methods
- Maintains accuracy without requiring fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
Blockwise quantization reduces accuracy loss by limiting distributional drift to smaller, more homogeneous blocks. By compressing each block independently, the method confines quantization error and preserves statistical properties better than layerwise approaches.

### Mechanism 2
Shift-based symmetric quantization provides hardware-friendly compression without retraining. Values are compressed by shifting bits according to the maximum absolute value in each block, avoiding bias adjustments that would require retraining.

### Mechanism 3
Low-bit integer and fp8 arithmetic replace fp32 operations through careful normalization and lookup-based approximations. Matrix multiplications normalize exponents blockwise before accumulation, while nonlinear functions use lookup tables with interpolation to maintain accuracy at lower precision.

## Foundational Learning

- Concept: Symmetric quantization and shift-based scaling
  - Why needed here: Enables deterministic compression without retraining by avoiding bias adjustments
  - Quick check question: How does shift-based quantization differ from linear scaling in terms of hardware efficiency and accuracy?

- Concept: Blockwise vs layerwise compression trade-offs
  - Why needed here: Understanding granularity's impact on accuracy and computational overhead
  - Quick check question: Why does dividing a layer into blocks reduce distributional deviation compared to compressing the whole layer at once?

- Concept: Lookup table interpolation for nonlinear functions
  - Why needed here: Allows accurate approximation of functions like GELU and Softmax in low-bit precision
  - Quick check question: What is the role of interpolation in lookup table-based function approximation, and why is it preferred over direct table lookup?

## Architecture Onboarding

- Component map:
  Embedding layer: int8 weights/biases, fp32 intermediate
  Linear layers: mixed precision (int4/int8 weights/biases, int8 intermediate)
  Matrix multiplication: blockwise int8 accumulation with exponent normalization
  Nonlinear ops: lookup tables with interpolation (int8 inputs, int8 outputs)
  LayerNorm and Softmax: fp8 or int8 with range handling
  Intermediate results: compressed blockwise between layers

- Critical path:
  Embed → Linear → MatMul → Nonlinearity → LayerNorm → FFN → Output
  All steps use blockwise compression and low-bit arithmetic; exponent normalization occurs at MatMul and addition points.

- Design tradeoffs:
  - Block size vs. accuracy: Smaller blocks reduce distributional shift but increase metadata overhead
  - Data type choice: int4/8 offers higher compression but may hurt accuracy; fp8 balances precision and efficiency
  - Hardware vs. software: Shift-based quantization is hardware-friendly but may not compress as tightly as learned quantization

- Failure signatures:
  - Accuracy drop > 0.9%: likely due to block size too small or quantization range too narrow
  - Numerical instability in MatMul: check exponent normalization logic
  - Slow inference: verify block alignment and compression overhead

- First 3 experiments:
  1. Validate blockwise quantization accuracy vs. layerwise on a small BERT variant using SST-2
  2. Benchmark exponent normalization in MatMul with synthetic data spanning wide ranges
  3. Compare interpolation-based vs. direct lookup for GELU/Softmax accuracy and speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal block size for balancing hardware efficiency and model performance in blockwise compression?
- Basis in paper: The paper states that "We should trade hardware efficiency against model performance to select the proper block size" but does not provide specific optimal values.
- Why unresolved: The optimal block size likely depends on the specific hardware platform and model architecture, requiring empirical testing across different configurations.
- What evidence would resolve it: Systematic experiments testing different block sizes on various hardware platforms and transformer models, measuring both compression ratios and accuracy retention.

### Open Question 2
- Question: How does blockwise compression affect the robustness of transformer models to adversarial attacks compared to layerwise compression?
- Basis in paper: The paper focuses on accuracy drop measurements but doesn't examine security aspects or adversarial robustness.
- Why unresolved: The paper only evaluates standard GLUE tasks and doesn't investigate security properties that could be impacted by quantization methods.
- What evidence would resolve it: Comparative adversarial attack experiments between BCT and layerwise quantization methods, measuring attack success rates and model resilience.

### Open Question 3
- Question: What is the theoretical upper bound on compression ratio achievable with blockwise compression without retraining while maintaining acceptable accuracy?
- Basis in paper: The paper achieves up to 7.988x compression but doesn't establish theoretical limits.
- Why unresolved: The paper provides empirical results but doesn't analyze the theoretical constraints or fundamental limits of the compression approach.
- What evidence would resolve it: Mathematical analysis of the quantization error propagation and information loss bounds for blockwise compression across different transformer architectures.

## Limitations

- Block size selection is not specified, requiring empirical tuning that may vary across hardware platforms
- Hardware platform dependency means reported compression ratios may not translate to actual speedups across different accelerators
- Interpolation method details for nonlinear operations are underspecified, potentially affecting accuracy and implementation

## Confidence

**High confidence**: The core mechanism of blockwise quantization reducing distributional deviation compared to layerwise approaches is well-supported by the theoretical explanation and experimental results showing <0.90% accuracy drop across GLUE tasks.

**Medium confidence**: The claim of up to 7.988x compression ratio is supported by experiments but lacks absolute baseline comparisons and hardware-specific performance metrics. The effectiveness may vary significantly across different transformer architectures and tasks.

**Low confidence**: The assertion that shift-based symmetric quantization eliminates retraining requirements needs more validation across diverse model architectures and tasks. The paper only tests on BERT-base with GLUE datasets, limiting generalizability.

## Next Checks

1. **Block size sensitivity analysis**: Systematically vary block sizes on SST-2 task and measure accuracy, compression ratio, and metadata overhead to identify optimal block dimensions for different layer types.

2. **Cross-architecture validation**: Apply BCT to RoBERTa and GPT-2 models on different tasks (e.g., SQuAD, text generation) to verify the generalizability of the compression method and accuracy preservation claims.

3. **Hardware performance benchmarking**: Implement BCT on both GPU and specialized AI accelerator hardware to measure actual inference speedups and energy efficiency, comparing against theoretical compression ratios to identify platform-specific bottlenecks.