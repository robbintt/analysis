---
ver: rpa2
title: A Block Metropolis-Hastings Sampler for Controllable Energy-based Text Generation
arxiv_id: '2312.04510'
source_url: https://arxiv.org/abs/2312.04510
tags:
- text
- style
- sampling
- proposal
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel Metropolis-Hastings (MH) sampler for
  energy-based language models (LMs) that enables controllable text generation by
  proposing re-writes of the entire sequence at each step via iterative prompting
  of a large language model (LLM). This approach improves upon previous work that
  only modified single tokens, allowing for more efficient and accurate sampling from
  the target distribution and enabling generation length to be determined during sampling
  rather than fixed in advance.
---

# A Block Metropolis-Hastings Sampler for Controllable Energy-based Text Generation

## Quick Facts
- arXiv ID: 2312.04510
- Source URL: https://arxiv.org/abs/2312.04510
- Reference count: 12
- Key outcome: Proposes MH-BLOCK sampler using LLM for block proposals, achieving higher J-scores than single-token baselines on style transfer tasks

## Executive Summary
This paper introduces a novel Metropolis-Hastings sampler for energy-based language models that enables controllable text generation through block-level proposals. Unlike previous approaches that only modified single tokens, this method proposes complete rewrites of entire sequences using iterative prompting of a large language model (Flan-T5-XXL). The approach achieves improved performance on two controlled generation tasks (formality transfer and author imitation) by enabling more efficient sampling from the target distribution and allowing generation length to be determined during sampling rather than fixed in advance.

## Method Summary
The method uses Metropolis-Hastings sampling with block proposals where an LLM generates complete sequence rewrites via iterative prompting. The energy function combines a style discriminator and BERTScore to evaluate proposals, with acceptance/rejection governed by the MH ratio. The approach enables variable sequence lengths during sampling and proposes multiple tokens simultaneously, improving mixing efficiency compared to single-token methods. Experiments use Flan-T5-XXL for proposals, with batch-size of 10 chains running 10-20 steps each on Shakespeare and GYAFC datasets.

## Key Results
- MH-BLOCK achieves higher J-scores than M&M, SAR, STRAP, UNMT, and VAE on both Shakespeare and GYAFC datasets
- Statistically significant improvements in several cases compared to single-token proposal techniques
- Demonstrates better mixing efficiency through block proposals versus incremental token changes
- Enables generation length flexibility during sampling rather than requiring fixed-length inputs

## Why This Works (Mechanism)

### Mechanism 1
Block-level MH sampler enables faster mixing by proposing coordinated edits to multiple tokens at once, avoiding slow incremental changes required by single-token approaches. By using Flan-T5-XXL to propose entire sequence rewrites via iterative prompting, the sampler can jump directly to fluent intermediate states that single-token samplers would need many steps to reach. Core assumption: The LLM proposal distribution has sufficient coverage of the target space to propose edits that are likely to be accepted. Break condition: If the LLM proposal becomes too conservative and rarely generates non-identity edits, mixing will slow down and approach single-token performance.

### Mechanism 2
Using an LLM as proposal distribution allows generation length to be determined during sampling rather than fixed in advance, enabling more natural outputs. Unlike BERT-based single-token proposals that preserve sequence length, LLM proposals can insert or delete tokens, allowing the chain to explore sequences of varying lengths that better fit the target distribution. Core assumption: The LLM is capable of generating sequences of appropriate length when prompted for style transfer. Break condition: If the LLM consistently generates outputs of similar length regardless of prompt, the length flexibility benefit is lost.

### Mechanism 3
The MH accept/reject step corrects bias from the LLM proposal distribution by considering the unnormalized energy under the target distribution, ensuring samples follow the desired target distribution. The energy function (product of experts) evaluates each proposed sequence, and the MH step accepts or rejects based on the ratio of energies between current and proposed states, canceling out the intractable normalization constant. Core assumption: The energy function accurately represents the target distribution and the MH step properly corrects for proposal bias. Break condition: If the energy function is poorly calibrated or the MH step implementation has errors, the sampler may not converge to the target distribution.

## Foundational Learning

- Concept: Metropolis-Hastings sampling and MCMC theory
  - Why needed here: Understanding the mathematical framework that ensures the sampler converges to the target distribution despite using an approximate proposal distribution
  - Quick check question: What is the key requirement for a proposal distribution in MH sampling to ensure convergence to the target distribution?

- Concept: Energy-based language models and globally normalized distributions
  - Why needed here: The paper frames controllable generation as sampling from an energy-based model, requiring understanding of how these differ from autoregressive models
  - Quick check question: Why is exact sampling intractable from energy-based language models compared to autoregressive models?

- Concept: Large language model prompting and conditional distributions
  - Why needed here: The proposal distribution relies on prompting an LLM to generate rewrites, requiring understanding of how different prompts affect the output distribution
  - Quick check question: How does the choice of prompt template affect the distribution of proposed edits in this MH sampler?

## Architecture Onboarding

- Component map: Energy function (style discriminator + BERTScore) -> LLM proposal (Flan-T5-XXL with prompts) -> MH sampler (acceptance probability calculation) -> Evaluation metrics (J-score, style accuracy, fluency, similarity)

- Critical path: 1) Prompt LLM with current sequence to generate proposal, 2) Calculate acceptance probability using energy function and proposal probabilities, 3) Accept or reject proposal based on probability, 4) Repeat for desired number of steps or until convergence, 5) Select best sample from batch of chains

- Design tradeoffs: Using LLM for proposals trades computational cost (larger forward passes) for faster mixing and length flexibility; Approximate acceptance strategy trades theoretical guarantees for practical performance; Batch sampling trades memory for better final sample selection

- Failure signatures: Chains get stuck in local optima (acceptance rate too low); Samples don't improve over iterations (poor proposal quality or energy function issues); Generated text quality degrades (prompt template issues or LLM problems)

- First 3 experiments: 1) Run MH sampler on synthetic target distribution (LLaMA paraphrasing) to verify it outperforms single-token baseline in sample quality per forward pass, 2) Test on Shakespeare dataset with minimal prompt template to establish baseline performance, 3) Vary batch size and chain length to find optimal configuration for downstream tasks

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of MH-BLOCK scale with the size of the language model used for proposals? The paper uses Flan-T5-XXL for proposals but doesn't explore scaling effects systematically. What evidence would resolve it: Experiments comparing MH-BLOCK performance using different sized LLMs (e.g., Flan-T5-Small, Medium, Large, XXL) on the same tasks.

### Open Question 2
What is the theoretical mixing time of the block MH sampler compared to single-token MH sampling? The paper mentions that block MH allows for "more efficient and accurate sampling" but doesn't provide theoretical analysis. What evidence would resolve it: Mathematical proofs or bounds on the mixing time of the block MH sampler versus traditional single-token MH sampling.

### Open Question 3
How sensitive is MH-BLOCK to the choice of prompt template for the proposal LLM? The paper mentions that Flan-T5-XXL was "sensitive to the format of the prompt" but only shows one prompt template and doesn't explore sensitivity systematically. What evidence would resolve it: Experiments varying prompt templates (structure, examples, phrasing) and measuring their impact on downstream task performance.

## Limitations
- No empirical evidence showing the diversity or quality distribution of LLM proposals across the space of valid text transformations
- Approximate acceptance strategy lacks ablation studies or theoretical analysis to justify the simplification
- Fixed energy function weights (α, β) are not explored systematically across different style transfer tasks
- Evaluation relies on automatic metrics rather than human evaluation for stylistic transformation tasks

## Confidence
- High Confidence: Mathematical framework correctly formulated; Energy function properly implements product of experts; Comparison methodology and J-score metric are valid
- Medium Confidence: MH-BLOCK outperforms single-token MH on evaluated datasets; Length flexibility benefit is practically useful; Computational efficiency gains are significant
- Low Confidence: Theoretical convergence guarantees hold with approximate acceptance strategy; LLM proposal distribution adequately covers target space; Fixed energy function weights are optimal across tasks

## Next Checks
1. **Proposal Distribution Analysis**: Sample 1000 proposals from random starting points, cluster them, and assess coverage of the style transfer space. Measure diversity of generated rewrites and proportion maintaining semantic coherence while achieving stylistic change.

2. **Approximation Impact Study**: Compare approximate acceptance strategy against exact MH acceptance on a small-scale synthetic dataset with known true target distribution. Measure systematic bias introduced and quantify trade-off between computational efficiency and sampling accuracy.

3. **Weight Sensitivity Analysis**: Perform ablation study varying α and β across multiple orders of magnitude on both Shakespeare and GYAFC datasets. Identify whether different weightings are optimal for different tasks and whether current fixed weights represent a reasonable compromise or task-specific tuning.