---
ver: rpa2
title: Ensemble of Task-Specific Language Models for Brain Encoding
arxiv_id: '2310.15720'
source_url: https://arxiv.org/abs/2310.15720
tags:
- language
- brain
- weights
- task
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of predicting fMRI brain activity
  from text using language models, aiming to improve upon previous single-model approaches.
  The core method involves creating ensemble models from 10 task-specific language
  models (2 syntactic and 8 semantic) and comparing various ensembling techniques
  including simple averaging, weighted averaging with heuristic weights, dynamic weight
  learning, and stacking with/without PCA.
---

# Ensemble of Task-Specific Language Models for Brain Encoding

## Quick Facts
- arXiv ID: 2310.15720
- Source URL: https://arxiv.org/abs/2310.15720
- Reference count: 4
- One-line primary result: Weighted averaging of task-specific language model embeddings improves fMRI brain encoding accuracy by ~10% compared to single models

## Executive Summary
This paper addresses the problem of predicting fMRI brain activity from text using language models, aiming to improve upon previous single-model approaches. The core method involves creating ensemble models from 10 task-specific language models (2 syntactic and 8 semantic) and comparing various ensembling techniques including simple averaging, weighted averaging with heuristic weights, dynamic weight learning, and stacking with/without PCA. The primary result shows that ensembling methods outperform individual task-specific models, with weighted averaging using heuristic weights achieving the best performance, improving average accuracy across all ROIs by approximately 10% compared to baselines. The analysis also provides insights into which cognitive tasks are most relevant for different brain regions based on the learned ensemble weights.

## Method Summary
The method uses 10 task-specific language models (2 syntactic: shallow syntax parsing, coreference resolution; 8 semantic: named entity recognition, natural language inference, paraphrase detection, question answering, sentiment analysis, semantic role labeling, summarization, word sense disambiguation) based on BERT variants to encode text stimuli. These models process 627 sentences from the Pereira dataset across 5 subjects and 9 brain ROIs. The ensemble approaches include simple averaging of embeddings, weighted averaging with heuristic weights based on task accuracy, dynamic weight learning with ridge regression, and stacking methods with and without PCA dimensionality reduction. Predictions are made using ridge regression with a 4:1 train-test split, evaluated using 2v2 Accuracy and Pearson Correlation metrics.

## Key Results
- Ensemble methods outperform individual task-specific models by approximately 10% in average accuracy across all ROIs
- Weighted averaging using heuristic weights performs best among all ensemble approaches
- Dynamic weight learning approach overfits due to limited training data (627 sentences across 5 subjects)
- PCA-based stacking methods show poor performance, likely due to information loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Averaging across output embeddings from task-specific models improves encoding accuracy by capturing complementary information about brain activity.
- Mechanism: Different NLP tasks capture different aspects of language processing (syntax, semantics, etc.), and brain regions encode these aspects in different ways. By averaging the embeddings, the ensemble captures a more complete representation of the language features relevant to each ROI.
- Core assumption: Each brain ROI responds to a combination of task-specific features, and averaging preserves the information from all relevant tasks without overfitting.
- Evidence anchors:
  - [abstract] "creating an ensemble model out of 10 popular Language Models (2 syntactic and 8 semantic)"
  - [section] "The approach of weighted average using the heuristic weights performs the best out of all, followed by simple averaging and then stacking after averaging. These three approaches clearly outperform the baselines by about 10%."

### Mechanism 2
- Claim: Heuristic weighting based on task accuracy improves performance by emphasizing more predictive tasks for each ROI.
- Mechanism: Tasks that are more predictive of brain activity in specific ROIs should be weighted more heavily in the ensemble. The heuristic weights are based on observed accuracy across tasks and ROIs, giving more influence to tasks that better explain neural responses.
- Core assumption: There is a consistent relationship between task performance and brain predictivity that can be captured through simple heuristic weighting rather than complex learning.
- Evidence anchors:
  - [abstract] "weighted averaging using heuristic weights achieving the best performance"
  - [section] "The approach of weighted average using the heuristic weights performs the best out of all"

### Mechanism 3
- Claim: Dynamic weight learning fails due to overfitting from limited training data.
- Mechanism: The dynamic weighting approach attempts to learn optimal weights from the training data using gradient descent, but with only a small number of stimulus sentences per subject, the model overfits to noise in the training set.
- Core assumption: The limited sample size (627 sentences across 5 subjects) is insufficient for the dynamic learning approach to generalize.
- Evidence anchors:
  - [abstract] "the approach of finding the weights dynamically... weights were overfitting on the training data since the number of stimulus sentences is very less per subject"
  - [section] "As for the ensemble method of finding the weights dynamically, the most probable issue is that weights were overfitting on the training data since the number of stimulus sentences is very less per subject"

## Foundational Learning

- Concept: fMRI brain encoding and voxel-based predictions
  - Why needed here: The entire study aims to predict fMRI voxel activations from text using language models, so understanding the fundamentals of how brain activity is measured and modeled is essential.
  - Quick check question: What does an fMRI voxel represent and how is brain activity typically measured in encoding studies?

- Concept: Natural language processing tasks and their linguistic features
  - Why needed here: The ensemble uses 10 different NLP tasks (syntax, semantics, NLI, etc.), each capturing different aspects of language that may map to different brain regions.
  - Quick check question: How do coreference resolution and semantic role labeling differ in the linguistic information they extract from text?

- Concept: Ensemble learning methods and their tradeoffs
  - Why needed here: The paper compares averaging, weighted averaging, dynamic weights, and stacking - understanding when each approach works best is crucial for interpreting results.
  - Quick check question: When would averaging ensemble predictions be preferable to learning individual weights for each model?

## Architecture Onboarding

- Component map:
  Text stimuli → 10 task-specific LMs → 768-dimensional embeddings → Ensemble method → Ridge regression → Voxel predictions → 2v2 Accuracy and Pearson Correlation metrics

- Critical path: Text → Task-specific embeddings → Ensemble method → Ridge regression → Voxel predictions → Evaluation metrics

- Design tradeoffs: Simple averaging is computationally cheap and works well, while dynamic weights could theoretically perform better but require more data to avoid overfitting. PCA-based stacking saves memory but loses information.

- Failure signatures: Dynamic weights overfit (weights become extreme), PCA-based methods show poor performance (information loss too severe), or weighted averaging doesn't improve over simple averaging (tasks are too similar).

- First 3 experiments:
  1. Run simple averaging across all 10 task embeddings and measure baseline performance
  2. Implement heuristic weighted averaging using accuracy-based weights and compare to simple averaging
  3. Test dynamic weight learning with strong regularization to see if overfitting can be mitigated

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number and combination of task-specific language models for creating the most effective ensemble for brain encoding?
- Basis in paper: [explicit] The paper mentions that the current ensemble uses 10 task-specific models (2 syntactic and 8 semantic) and explores various ensembling methods, but does not systematically investigate the impact of different model combinations or numbers of models.
- Why unresolved: The paper uses a fixed set of 10 models without exploring whether fewer or different combinations might perform better, or whether certain tasks are redundant.
- What evidence would resolve it: Systematic ablation studies removing individual models or testing different combinations, along with analysis of which tasks contribute most to performance across different ROIs.

### Open Question 2
- Question: How does the performance of ensemble methods scale with larger and more diverse fMRI datasets?
- Basis in paper: [inferred] The paper uses a relatively small dataset (627 sentences across 5 subjects) and mentions that the dynamic weighting approach may have overfit due to limited data, suggesting dataset size could be a limiting factor.
- Why unresolved: The study's limited dataset size may constrain generalizability, and the paper doesn't explore performance on larger or more diverse datasets.
- What evidence would resolve it: Testing ensemble methods on larger fMRI datasets with more subjects and diverse language stimuli, comparing performance gains across different dataset sizes.

### Open Question 3
- Question: What is the theoretical relationship between task predictivity weights and actual cognitive processes in different brain regions?
- Basis in paper: [explicit] The paper states that "the weights tell which tasks are important for which ROI and which ROI has better predictivity for each task" and discusses implications for understanding cognitive aspects of language processing.
- Why unresolved: While the paper observes correlations between task weights and ROIs, it doesn't establish a causal or theoretical framework linking specific cognitive processes to the learned weights.
- What evidence would resolve it: Neuroscientific studies validating that the task weights correspond to known functional specializations of brain regions, potentially through complementary neuroimaging methods or cognitive task analysis.

## Limitations
- The heuristic weights for weighted averaging are not clearly justified or validated across different datasets
- Dynamic weight learning was dismissed based on overfitting concerns without exploring regularization techniques
- PCA-based stacking methods failed, but alternative dimensionality reduction techniques were not explored

## Confidence
- **High confidence**: The finding that ensemble methods outperform individual task-specific models is well-supported by the results and aligns with established ensemble learning principles.
- **Medium confidence**: The claim that weighted averaging with heuristic weights performs best requires validation on additional datasets to ensure the heuristic approach generalizes beyond the Pereira dataset.
- **Low confidence**: The dismissal of dynamic weight learning based solely on overfitting concerns without exploring regularization techniques or alternative optimization approaches.

## Next Checks
1. Cross-dataset validation: Test the ensemble methods on an independent fMRI encoding dataset to verify that weighted averaging with heuristic weights maintains its performance advantage across different subjects and stimuli.
2. Regularized dynamic weights: Implement L1/L2 regularization or dropout in the dynamic weight learning approach to determine if the overfitting issue can be mitigated while preserving the ability to learn task-specific contributions.
3. Alternative dimensionality reduction: Evaluate whether other dimensionality reduction techniques (UMAP, autoencoders) could improve stacking performance compared to PCA, particularly for capturing nonlinear relationships between task embeddings.