---
ver: rpa2
title: Topological Obstructions and How to Avoid Them
arxiv_id: '2312.07529'
source_url: https://arxiv.org/abs/2312.07529
tags:
- learning
- which
- obstructions
- space
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies topological obstructions in training encoders
  for geometric latent spaces, focusing on cases where random initialization can lead
  to local minima due to singularities or incorrect winding numbers. The authors formalize
  these issues for Lie groups like SO(2) and show that continuous optimization cannot
  resolve them.
---

# Topological Obstructions and How to Avoid Them

## Quick Facts
- arXiv ID: 2312.07529
- Source URL: https://arxiv.org/abs/2312.07529
- Reference count: 40
- Primary result: GF-VAEs improve homeomorphism learning in VAEs by up to 9/15 seeds converging versus 0/15 for standard VAEs

## Executive Summary
This paper investigates topological obstructions that prevent VAEs from learning homeomorphic encoders for geometric latent spaces like Lie groups. The authors demonstrate that random initialization can lead to local minima with topological defects (figure-eight embeddings, incorrect winding numbers) that cannot be resolved through continuous optimization due to topological invariants. To address this, they propose Group-Flow VAEs (GF-VAEs) that use normalizing flows to define multimodal variational distributions on Lie groups, enabling discontinuous jumps between topological configurations without passing through high-loss regions.

## Method Summary
The paper proposes GF-VAEs that use normalizing flows to map data points to multimodal distributions over geometric spaces (Lie groups). Unlike standard VAEs that output unimodal Gaussian parameters, GF-VAEs parameterize a normalizing flow whose input is a uniform distribution on the Lie group. The encoder network outputs parameters for K flow layers, and the final embedding is obtained by sampling from the uniform base distribution and applying the flow transformations. This architecture allows small parameter changes to cause non-local changes in the representation by switching between modes in the variational distribution.

## Key Results
- Standard VAEs fail to learn homeomorphic encoders in 15/15 random seeds due to topological obstructions
- GF-VAEs achieve 9/15 seeds with continuous, homeomorphic mappings versus 0/15 for standard VAEs
- Continuity metrics improve significantly (Lcont < 10) for GF-VAEs while standard VAEs remain above this threshold
- Topological defects (incorrect winding/crossing numbers) are reduced from 100% to 0% in successful GF-VAE runs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Topological obstructions (e.g., figure-eight self-intersections) in encoders for Lie groups cannot be resolved through continuous optimization alone.
- Mechanism: Random initialization often produces embeddings with non-trivial topology (crossing number > 0 or winding number ≠ ±1). Under continuous optimization, these topological invariants are preserved, creating local minima that gradient descent cannot escape without discrete jumps.
- Core assumption: The mapping from data to latent space is injective for all intermediate parameter values during training.
- Evidence anchors:
  - [abstract] "we theoretically and empirically characterize obstructions to training encoders with geometric latent spaces. We show that local optima can arise due to singularities (e.g. self-intersection) or due to an incorrect degree or winding number."
  - [section] "Proposition 3.1... a figure 8 embedding, which corresponds to cyclic order (z1, z2, z4, z3) mod C4, cannot be transformed to a homeomorphic embedding... Thus a figure 8 embedding... cannot be transformed to a homeomorphic embedding"
  - [corpus] Weak evidence - corpus papers discuss topology but not this specific optimization obstruction.
- Break condition: If the embedding trajectory passes through the origin (hϕ(x) = 0) during training, or if discrete optimization steps cause non-continuous parameter changes, the topological invariants can change.

### Mechanism 2
- Claim: Normalizing flows with multimodal variational distributions can circumvent topological obstructions by decoupling parameter changes from representation changes.
- Mechanism: By defining the encoder output as the mode of a multimodal distribution (rather than the mean of a unimodal Gaussian), small parameter changes can cause discontinuous jumps between modes without passing through high-loss regions, enabling escape from local minima.
- Core assumption: The variational distribution has sufficient modes to represent all valid topological configurations of the data manifold.
- Evidence anchors:
  - [abstract] "Inspired by this observation, we propose a new flow-based model that maps data points to multimodal distributions over geometric spaces"
  - [section] "If the variational distribution contains multiple modes, rather than a single peak... then it may becomes possible for small changes to result in non-local changes... by switching between modes in the variational distribution"
  - [corpus] Weak evidence - corpus papers discuss multimodal distributions but not specifically for topological obstructions.
- Break condition: If the number of modes is insufficient to represent the topological space, or if the flow transformations are not expressive enough to connect the modes.

### Mechanism 3
- Claim: Magnitude growth in the intermediate embedding space Y during training makes it harder for the winding number to change, exacerbating topological obstructions.
- Mechanism: The loss depends only on the normalized embedding (z = y/∥y∥), so gradients are tangent to circles around the origin. Under gradient descent, this causes ∥y∥ to grow over time, making it less likely that hϕ(x) will cross the origin and change the winding number.
- Core assumption: The encoder parameters are updated based on gradients of the loss with respect to the parameters, not directly with respect to the embeddings.
- Evidence anchors:
  - [section] "Empirically, we observe the values of the embeddings in Y := R2 continually grow during training... we give a theoretical explanation for this behavior"
  - [section] "Under gradient descent, however, due to the convexity of the flow lines... the embeddings y will tend to grow in magnitude"
  - [corpus] No direct evidence - corpus papers don't discuss magnitude growth in this context.
- Break condition: If the learning rate is too high, causing large parameter jumps that bypass the magnitude growth mechanism, or if explicit regularization on ∥y∥ is applied.

## Foundational Learning

- Concept: Topological invariants (winding number, crossing number)
  - Why needed here: These invariants determine whether a learned encoder is homeomorphic to the data manifold. The paper shows that certain topological configurations cannot be transformed into others through continuous optimization.
  - Quick check question: What are the possible values of the winding number for a homeomorphism from S¹ to S¹, and why?

- Concept: Lie groups and their relationship to data symmetries
  - Why needed here: The paper focuses on encoding data onto Lie groups (like SO(2)) because these naturally capture continuous symmetries in the data. Understanding how Lie groups work is essential to grasp the topological constraints.
  - Quick check question: How does the exponential map connect the Lie algebra to the Lie group, and why is this important for defining distributions on Lie groups?

- Concept: Normalizing flows and their parameterization
  - Why needed here: The proposed GF-VAE uses normalizing flows to create multimodal distributions on Lie groups. Understanding how flows work and how they can be defined on manifolds is crucial for implementing the method.
  - Quick check question: How does the change of variables formula work for normalizing flows, and what additional considerations are needed when working on manifolds?

## Architecture Onboarding

- Component map:
  Encoder network hϕ -> Normalizing flow (K layers) -> Base distribution (uniform on Lie group) -> Decoder network -> Data space

- Critical path:
  1. Encode input x → parameters yk for K flow layers
  2. Sample z₀ ~ p(z₀) (uniform on Lie group)
  3. Apply flow transformations: zₖ = r(zₖ₋₁; yₖ) for k=1...K
  4. Compute log q(zₖ|x) using change of variables
  5. Decode zₖ → reconstruction of x
  6. Compute ELBO and backpropagate

- Design tradeoffs:
  - Number of flow layers (K): More layers increase expressiveness but add computational cost
  - Flow architecture: Must be a diffeomorphism on the Lie group; simple affine+spline flows vs. more complex group-specific flows
  - Base distribution: Uniform prior vs. learned prior - uniform ensures equivariance but may limit expressiveness
  - Decoder architecture: Standard vs. action-decoder - action-decoder may help with homeomorphism but adds complexity

- Failure signatures:
  - Training instability or divergence
  - Low reconstruction quality despite low ELBO
  - Continuity metric Lcont > 10 (indicates discontinuities in the learned mapping)
  - Winding number ≠ ±1 or crossing number > 0 in the final model
  - Magnitude of intermediate embeddings ∥y∥ growing without bound

- First 3 experiments:
  1. Implement a simple GF-VAE on synthetic SO(2) data (rotating line images) with K=1 flow layer; verify it learns a continuous mapping where standard VAE fails
  2. Add a second flow layer and compare continuity metrics; examine how the intermediate representation y evolves during training
  3. Test on a more complex Lie group (e.g., SO(3) or torus) with real image data; implement the action-decoder variant and compare to standard decoder

## Open Questions the Paper Calls Out
The paper identifies several limitations and future research directions, including:
- Theoretical analysis being limited by idealized assumptions
- Difficulty in defining and computing metrics like winding and crossing numbers for higher-dimensional manifolds
- Need to expand analysis to a wider array of Lie groups and non-group manifolds
- Potential applications to robotics, 3D vision, and molecular design

## Limitations
- Theoretical analysis focuses primarily on SO(2) and doesn't extend to more complex Lie groups
- Empirical validation is limited to specific image datasets with rotational symmetries
- Paper doesn't address computational complexity or scalability of the flow-based approach
- Limited exploration of different normalizing flow architectures and their impact on performance

## Confidence
- **High Confidence**: The existence of topological obstructions during continuous optimization is well-supported by both theory (Proposition 3.1) and empirical observations (training instability in standard VAEs).
- **Medium Confidence**: The proposed solution using multimodal normalizing flows effectively addresses the identified problems in the specific experimental settings tested.
- **Low Confidence**: The claim that magnitude growth in intermediate embeddings is the primary reason why topological obstructions persist is based on empirical observation without comprehensive theoretical justification.

## Next Checks
1. Test GF-VAE on non-periodic Lie groups (SO(3), SE(2)) to verify the method generalizes beyond circular topology.
2. Implement ablation studies varying the number of flow modes and their capacity to quantify the minimum requirements for avoiding topological obstructions.
3. Design experiments where the encoder is explicitly initialized in different topological configurations to measure how often GF-VAE versus standard VAE converges to homeomorphic solutions.