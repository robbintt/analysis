---
ver: rpa2
title: 'Investigating the Limitation of CLIP Models: The Worst-Performing Categories'
arxiv_id: '2310.03324'
source_url: https://arxiv.org/abs/2310.03324
tags:
- worst
- photo
- categories
- clip
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLIP models, despite achieving strong overall accuracy across diverse
  domains, exhibit significant performance disparities among individual categories,
  with some categories showing zero accuracy even when overall performance is high.
  This paper addresses this limitation by proposing Class-wise Matching Margin (CMM)
  to quantify cross-modal confusion and identify worst-performing categories.
---

# Investigating the Limitation of CLIP Models: The Worst-Performing Categories

## Quick Facts
- arXiv ID: 2310.03324
- Source URL: https://arxiv.org/abs/2310.03324
- Reference count: 14
- Primary result: CPE improves ImageNet worst-10 accuracy from 0% to 5.2% without manual prompt engineering

## Executive Summary
CLIP models, despite strong overall accuracy across diverse domains, exhibit significant performance disparities among individual categories, with some categories showing zero accuracy even when overall performance is high. This paper addresses this limitation by proposing Class-wise Matching Margin (CMM) to quantify cross-modal confusion and identify worst-performing categories. By leveraging CMM, the authors automatically enrich descriptions of underperforming categories using large language models and construct a weighted prompt ensemble. Their proposed method, CPE, achieves substantial improvements in worst-category accuracy across multiple benchmarks.

## Method Summary
The authors propose a three-step approach: (1) Compute Class-wise Matching Margin (CMM) to identify worst-performing categories by measuring the similarity gap between correct and most confusing prompts, (2) Use GPT-3 to automatically enrich descriptions of worst-performing categories with discriminative attributes, and (3) Build a weighted prompt ensemble where prompts are weighted based on their Worst-k CMM scores, emphasizing high-quality prompts while suppressing domain-biased ones. This approach improves worst-category accuracy without requiring manual prompt engineering, optimization, or labeled validation data.

## Key Results
- CPE improves ImageNet worst-10 accuracy from 0% to 5.2%
- Significant improvements in worst-category accuracy across 12 benchmarks
- Achieves performance gains without manual prompt engineering or labeled validation data
- Weighted prompt ensemble effectively suppresses domain-biased prompts

## Why This Works (Mechanism)

### Mechanism 1
CLIP's zero-shot classification relies on matching similarity between image and text embeddings; categories with low Class-wise Matching Margin (CMM) are most confused and perform worst. CMM measures the difference between similarity of an image to its correct prompt versus the most similar wrong prompt. Low CMM indicates high cross-modal confusion, leading to frequent misclassification. Core assumption: The CLIP model's cross-modal similarity matching is symmetric and monotonic in terms of classification correctness.

### Mechanism 2
Augmenting descriptions for worst-performing categories using large language models improves their distinguishability in CLIP's embedding space. GPT-3-generated detailed descriptions add discriminative attributes to ambiguous categories, increasing their distinctiveness relative to similar categories. Core assumption: Language model-generated descriptions align with real-world visual attributes and are useful for CLIP's multimodal understanding.

### Mechanism 3
Weighted prompt ensemble based on Worst-k CMM boosts performance by emphasizing high-quality prompts and suppressing domain-biased ones. Prompts with higher Worst-k CMM are assigned higher weights, amplifying their contribution to the final prediction while low-quality prompts are downweighted or excluded. Core assumption: Worst-k CMM correlates with actual prompt effectiveness for improving worst-category accuracy.

## Foundational Learning

- **Cross-modal similarity matching in CLIP**
  - Why needed: Understanding how CLIP performs zero-shot classification is essential to grasp why CMM and prompt weighting help
  - Quick check: In CLIP zero-shot classification, what determines the predicted label for an input image?

- **Prompt engineering and prompt ensemble methods**
  - Why needed: CPE builds upon these to improve worst-category performance; knowing how they work is crucial for understanding the weighted ensemble step
  - Quick check: How does prompt ensemble improve CLIP's overall accuracy, and why might it still fail for worst-performing categories?

- **Large language model knowledge integration**
  - Why needed: GPT-3 is used to enrich category descriptions; understanding its strengths and limitations informs why this approach works
  - Quick check: What kind of knowledge does GPT-3 bring to category description enrichment, and how does it differ from supervised training data?

## Architecture Onboarding

- **Component map**: CLIP model → Class-wise Matching Margin (CMM) computation → Worst-k categories identification → GPT-3 enrichment → Weighted prompt ensemble (CPE) → Final prediction
- **Critical path**: CMM calculation → Worst-k category identification → Description enrichment → Prompt weighting → Ensemble prediction
- **Design tradeoffs**: Enriching all categories vs. only worst-k (risks suppressing worst categories); using pseudo labels vs. ground truth (avoids label dependency but may be less accurate); prompt selection vs. full ensemble (reduces bias but may exclude useful prompts)
- **Failure signatures**: Worst-k accuracy plateaus or drops after enrichment; ensemble weights collapse to one prompt; CMM values do not correlate with actual performance
- **First 3 experiments**:
  1. Verify CMM correlates with worst-class accuracy on a small benchmark
  2. Test GPT-3 enrichment on a few worst-performing categories and measure improvement
  3. Validate weighted ensemble improves worst-category accuracy compared to uniform ensemble

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed Class-wise Matching Margin (CMM) compare to other metrics for identifying worst-performing categories in CLIP models? The paper introduces CMM and demonstrates its effectiveness but does not compare it to alternative metrics like class-wise entropy or confusion matrix analysis.

### Open Question 2
What is the impact of using pseudo-labels versus ground truth labels in the CMM calculation on the final performance of the CPE method? The paper mentions using pseudo-labels for CMM calculation but does not provide a comparison between pseudo-label-based CMM and ground truth-based CMM.

### Open Question 3
How does the CPE method perform when applied to more diverse and complex image classification tasks beyond the datasets tested in the paper? The paper tests CPE on 12 datasets but does not explore its effectiveness on more challenging tasks like multi-label classification or specialized domains like medical imaging.

## Limitations
- Effectiveness of CMM depends on quality of pseudo labels, which may introduce noise and bias
- Enrichment approach relies on GPT-3's ability to generate relevant and discriminative attributes
- Weighted prompt ensemble assumes Worst-k CMM scores reliably rank prompt quality

## Confidence

- **High**: CMM effectively identifies worst-performing categories; GPT-3 enrichment improves worst-category accuracy
- **Medium**: Weighted prompt ensemble consistently improves performance across benchmarks
- **Low**: Pseudo labels used in CMM calculation do not introduce significant bias

## Next Checks

1. Test CMM performance using ground truth labels versus pseudo labels to quantify label dependency impact
2. Evaluate GPT-3 enrichment across diverse category types (abstract concepts, fine-grained objects, rare objects) to assess generalizability
3. Compare weighted ensemble performance using different CMM variants (e.g., standard vs. Worst-k) to validate ranking effectiveness