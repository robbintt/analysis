---
ver: rpa2
title: Unlocking the Potential of Prompt-Tuning in Bridging Generalized and Personalized
  Federated Learning
arxiv_id: '2310.18285'
source_url: https://arxiv.org/abs/2310.18285
tags:
- data
- group
- local
- prompt
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of training Vision Transformers
  in heterogeneous Federated Learning settings, where data distributions vary across
  clients. The proposed method, FGPT, learns a single global model using shared and
  group-specific prompts to capture both universal and group-specific knowledge.
---

# Unlocking the Potential of Prompt-Tuning in Bridging Generalized and Personalized Federated Learning

## Quick Facts
- **arXiv ID**: 2310.18285
- **Source URL**: https://arxiv.org/abs/2310.18285
- **Reference count**: 40
- **Primary result**: FGPT achieves 2-4% accuracy improvements over baselines in heterogeneous federated learning settings using shared and group-specific prompts.

## Executive Summary
This paper addresses the challenge of training Vision Transformers in heterogeneous Federated Learning settings by proposing FGPT, a method that learns a single global model using shared and group-specific prompts. The approach captures both universal and group-specific knowledge through a prompt selection module that automatically aligns with local client data distributions without requiring local fine-tuning. FGPT demonstrates superior performance compared to state-of-the-art baselines across various heterogeneity settings, with accuracy improvements of 2-4% on CIFAR-100 and Five-Dataset.

## Method Summary
FGPT combines shared prompts learned globally with group-specific prompts assigned through a prompt selection module using orthogonal keys. The method employs a pre-trained ViT backbone with frozen parameters, learning only prompt embeddings. During training, inputs are processed by a shared-prompt tuned encoder, and similarities between orthogonal keys and CLS token features determine group prompt selection. This enables personalized training at the sample level without local fine-tuning. The approach is evaluated on pathological and Dirichlet partitions, showing significant improvements over baselines like PT-FedAvg, pFedHN, and Fed-DualPrompt.

## Key Results
- FGPT achieves 2-4% accuracy improvements over baselines in heterogeneous federated learning settings
- Ensemble inference variant FGPT-U provides additional 1%+ improvement by averaging top-K prompt predictions
- Theoretical analysis shows that increasing group prompts reduces distribution mismatch between global and local models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Group-aware prompt tuning reduces the gap between global and local model performance by explicitly aligning local data distributions through group-specific prompts.
- **Mechanism**: The method uses a prompt selection module that assigns each input to a group based on similarity to fixed orthogonal keys. This ensures that data from the same group across different clients update the same group prompt, reducing distribution mismatch.
- **Core assumption**: Data from the same group across clients share similar characteristics, and learning separate group prompts allows the model to capture these shared features without requiring local fine-tuning.
- **Evidence anchors**:
  - [abstract]: "A key feature of SGPT is its prompt selection module, which facilitates the training of a single global model capable of automatically adapting to diverse local client data distributions without the need for local fine-tuning."
  - [section 3.3]: "Each input undergoes processing by a shared-prompt tuned foundation model encoder...Similarities between keys and last layer CLS token features are calculated, and the prompt corresponding to the most similar key is selected for training, enabling personalized training at the sample level."
- **Break condition**: If the data grouping is incorrect or the orthogonal keys don't capture meaningful semantic differences between groups, the alignment will fail and performance will degrade.

### Mechanism 2
- **Claim**: Using ensemble inference (FGPT-U) improves generalization by reducing the gap between group-specific and global performance.
- **Mechanism**: During inference, instead of selecting a single group prompt, the top-K most similar prompts are selected and their predictions are averaged. This provides a smoother prediction surface and better handles cases where data might belong to multiple groups.
- **Core assumption**: Real data distributions are often mixtures of multiple group distributions, and ensembling predictions from multiple group prompts captures this mixture better than a single prompt.
- **Evidence anchors**:
  - [section 4.2]: "By selecting and ensembling the Top-K most relevant prompts for prediction, the prediction accuracy significantly improved...FGPT-U significantly improves the performance of FPT by a substantial margin (more than 1% on average)."
- **Break condition**: If K is too large, the predictions become too smoothed and lose the benefit of group specificity. If K is too small, the ensemble doesn't provide meaningful improvement.

### Mechanism 3
- **Claim**: Learning shared prompts alongside group prompts allows the model to capture both universal and group-specific knowledge, improving overall performance.
- **Mechanism**: Shared prompts are learned globally across all clients to capture universal features, while group prompts are learned within their respective data groups to capture specialized knowledge. This dual structure allows the model to maintain global generalization while adapting to local variations.
- **Core assumption**: The data contains both universal patterns that apply across all clients and group-specific patterns that vary across clients, and both types of knowledge are necessary for optimal performance.
- **Evidence anchors**:
  - [abstract]: "SGPT...employs a unique combination of both shared and group-specific prompts. This design enables SGPT to capture both common and group-specific features."
- **Break condition**: If the data heterogeneity is very low, the group prompts may not provide significant benefit and could even hurt performance by overfitting to noise.

## Foundational Learning

- **Concept: Visual Prompt Tuning (VPT)**
  - Why needed here: VPT is the core technique that makes this approach parameter-efficient for federated learning, allowing only prompts to be updated rather than the entire model.
  - Quick check question: What is the main advantage of VPT over full fine-tuning in federated learning settings?

- **Concept: Federated Learning (FL) and Data Heterogeneity**
  - Why needed here: Understanding how data heterogeneity affects FL performance is crucial for appreciating why group-aware prompts are needed.
  - Quick check question: How does data heterogeneity typically impact the performance of standard federated learning approaches?

- **Concept: Orthogonal Keys and Prompt Selection**
  - Why needed here: The orthogonal key mechanism is central to how the method automatically assigns data to groups without requiring labeled group information.
  - Quick check question: Why are orthogonal keys used instead of learned keys for the prompt selection module?

## Architecture Onboarding

- **Component map**: Pre-trained ViT backbone (frozen) -> Shared prompts (learned globally) -> Group prompts (learned per group) -> Prompt selection module (orthogonal keys) -> Classification head
- **Critical path**: 1. Data → Prompt selection module → Group assignment 2. Image embeddings + Shared prompts → Transformer layers 3. Group prompt insertion at layer u → Additional transformer layers 4. CLS token → Classification head → Prediction
- **Design tradeoffs**: Number of groups (G): Higher G reduces distribution mismatch but increases Rademacher complexity; Layer for group prompt insertion: Earlier layers capture more general features, later layers capture more specific features; Prompt length: Longer prompts provide more capacity but increase communication cost
- **Failure signatures**: Poor performance across all clients: Likely issue with shared prompts or overall architecture; Good global performance but poor local performance: Group prompts not effectively capturing local patterns; Inconsistent performance across clients: Data grouping may be incorrect or orthogonal keys not meaningful
- **First 3 experiments**: 1. Baseline test: Run with only shared prompts (no group prompts) to establish baseline performance 2. Group number sensitivity: Test with different numbers of groups (G=10, 20, 30) to find optimal value 3. Orthogonal vs learned keys: Compare performance using orthogonal keys vs learned keys for prompt selection

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- The approach requires pre-defined group numbers and does not automatically determine the optimal number of groups from data
- The orthogonal key selection mechanism assumes that the last layer CLS token features are representative for group assignment
- The ensemble inference variant (FGPT-U) improves performance but increases computational overhead during inference

## Confidence
- The effectiveness of orthogonal keys for prompt selection relies on the assumption that data groups are well-separated in the feature space. **Confidence: Medium**
- The theoretical analysis showing reduced distribution mismatch with increased group prompts is based on Rademacher complexity bounds. **Confidence: Medium**
- The method's performance gains come at the cost of increased communication overhead from transmitting group prompts. **Confidence: Medium**

## Next Checks
1. **Group number sensitivity**: Systematically evaluate FGPT performance across a wider range of group numbers (G=5, 10, 15, 20, 30) on both pathological and Dirichlet partitions to identify optimal group counts.
2. **Orthogonal vs learned keys**: Compare the performance of orthogonal keys against learned keys for the prompt selection module to validate the design choice.
3. **Communication cost analysis**: Measure the actual communication overhead of transmitting group prompts versus the accuracy improvements to quantify the efficiency trade-off.