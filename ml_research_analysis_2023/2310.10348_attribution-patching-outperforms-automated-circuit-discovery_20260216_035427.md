---
ver: rpa2
title: Attribution Patching Outperforms Automated Circuit Discovery
arxiv_id: '2310.10348'
source_url: https://arxiv.org/abs/2310.10348
tags:
- patching
- attribution
- edge
- activation
- edges
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scaling automated circuit
  discovery to larger neural network models by proposing a more efficient method for
  identifying task-relevant subnetworks. The authors introduce Edge Attribution Patching
  (EAP), which uses a linear approximation (attribution patching) to estimate the
  importance of edges in a model's computational graph, requiring only two forward
  passes and one backward pass compared to the exponential number of passes needed
  by existing methods.
---

# Attribution Patching Outperforms Automated Circuit Discovery

## Quick Facts
- arXiv ID: 2310.10348
- Source URL: https://arxiv.org/abs/2310.10348
- Reference count: 40
- Primary result: Edge Attribution Patching (EAP) achieves higher AUC in circuit recovery than existing methods while requiring substantially fewer forward passes

## Executive Summary
This paper addresses the challenge of scaling automated circuit discovery to larger neural network models by proposing Edge Attribution Patching (EAP), a more efficient method for identifying task-relevant subnetworks. EAP uses a linear approximation (attribution patching) to estimate edge importance, requiring only two forward passes and one backward pass compared to the exponential number of passes needed by existing methods. The authors evaluate EAP on three tasks (Indirect Object Identification, Docstring, and Greater-Than) and find it outperforms existing circuit discovery methods, achieving higher AUC in circuit recovery while being substantially faster to compute.

## Method Summary
The paper introduces Edge Attribution Patching (EAP) as a method for automated circuit discovery in neural networks. EAP uses a linear approximation of the metric function L via Taylor expansion to estimate edge importance, requiring only two forward passes and one backward pass. This is compared to activation patching which requires exponential forward passes. The method computes attribution scores for edges in the computational graph, sorts them by absolute value, and selects the top edges as the circuit. The authors evaluate performance using ROC curves and AUC metrics against known circuits as ground truth.

## Key Results
- EAP achieves higher AUC in circuit recovery than existing methods across all three tasks
- EAP requires only 2 forward passes and 1 backward pass versus exponential passes for exact methods
- Combined EAP+ACDC approach shows improved performance over EAP alone
- Attribution scores correlate weakly with true activation patching scores (R² = 0.27)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear approximation via attribution patching enables efficient circuit discovery
- Mechanism: The method uses a Taylor expansion to approximate how changes in edge activations affect the model's output metric, requiring only two forward passes and one backward pass instead of exponential forward passes
- Core assumption: The metric function L can be reasonably approximated by a linear function of edge activations for small perturbations
- Evidence anchors:
  - [abstract] "We apply a linear approximation to activation patching to estimate the importance of each edge in the computational subgraph"
  - [section 3.3] "We can use the insights from Section 3.2 to build an automated circuit discovery algorithm"
  - [corpus] Weak evidence - no direct citations found
- Break condition: When the metric function is highly non-linear or discontinuous with respect to edge activations, making the linear approximation poor

### Mechanism 2
- Claim: EAP outperforms ACDC despite being an approximation
- Mechanism: EAP's linear approximation, while imperfect, provides sufficient signal to identify task-relevant edges more efficiently than exact activation patching used by ACDC
- Core assumption: The ranking of edge importance based on attribution scores correlates well enough with true importance for effective circuit discovery
- Evidence anchors:
  - [abstract] "our method has greater AUC from circuit recovery than other methods"
  - [section 4.1] "we find that averaged over all tasks our method has greater AUC from circuit recovery than other methods"
  - [section 5.1] "we find a fairly weak correlation between activation and attribution patching scores ( R2 = 0.27)"
- Break condition: When the approximation error systematically reverses the ranking of edge importance

### Mechanism 3
- Claim: Combining EAP with ACDC refinement improves results
- Mechanism: EAP provides a coarse pruning that removes obviously irrelevant edges, then ACDC can more efficiently refine the remaining subgraph
- Core assumption: EAP's pruning retains most relevant edges while removing enough noise to make subsequent ACDC computationally tractable
- Evidence anchors:
  - [section 5.2] "We ran the combined methods on the Docstring task, varying both pruning thresholds for EAP and ACDC independently"
  - [section 5.2] "The combined methods show increased performance compared to EAP only"
  - [corpus] Weak evidence - no direct citations found
- Break condition: When EAP's pruning threshold is too aggressive and removes relevant edges

## Foundational Learning

- Concept: Linear approximation and Taylor series expansion
  - Why needed here: Understanding how the method approximates non-linear metric changes with linear gradients
  - Quick check question: What mathematical property allows us to approximate a function using only its first derivative?

- Concept: Activation patching vs attribution patching
  - Why needed here: Distinguishing between the exact method (activation patching) and the approximation (attribution patching)
  - Quick check question: How many forward passes does activation patching require compared to attribution patching for n edges?

- Concept: Computational graph representation of neural networks
  - Why needed here: Understanding how edges between nodes are defined and analyzed
  - Quick check question: In the computational graph view, what represents the flow of information between attention heads?

## Architecture Onboarding

- Component map: Edge Attribution Patching (EAP) -> ACDC refinement -> Circuit evaluation using ROC/AUC metrics

- Critical path:
  1. Compute attribution scores for all edges (2 forward passes, 1 backward pass)
  2. Sort edges by absolute attribution score
  3. Select top k edges as circuit
  4. Evaluate circuit recovery using ROC/AUC metrics
  5. Optionally refine with ACDC on EAP's pruned subgraph

- Design tradeoffs:
  - Speed vs accuracy: EAP is much faster but provides an approximation
  - Threshold selection: Balancing between including all relevant edges vs pruning noise
  - Task-specific metrics: Using different evaluation metrics for different tasks

- Failure signatures:
  - Low correlation between EAP and activation patching scores
  - Edge scores concentrated near zero with no clear separation
  - ROC curve close to diagonal line
  - Over-pruning that removes essential circuit components

- First 3 experiments:
  1. Run EAP on a small model task (like Docstring) and compare the edge score distribution to known circuits
  2. Verify the computational efficiency claim by timing EAP vs ACDC on the same task
  3. Test the combined EAP+ACDC approach to confirm the refinement benefit on a complex task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the precise failure modes of attribution patching approximations, and can we predict when they will fail?
- Basis in paper: [explicit] Section 5.1 discusses that attribution patching can be misleading when gradients are concave, particularly for edges from embeddings.
- Why unresolved: The paper shows specific examples of failure but doesn't provide a systematic theory for when approximations will break down.
- What evidence would resolve it: A comprehensive study characterizing conditions under which attribution patching fails, potentially using local curvature analysis of the metric function.

### Open Question 2
- Question: How does combining EAP with ACDC compare to other hybrid approaches for circuit discovery?
- Basis in paper: [explicit] Section 5.2 mentions that running ACDC on the EAP-prepruned subgraph can improve performance.
- Why unresolved: The paper only tests one specific combination strategy and doesn't explore alternative hybrid approaches or optimization strategies.
- What evidence would resolve it: Systematic comparison of various hybrid methods (different pruning orders, iterative refinement strategies, etc.) across multiple tasks and model sizes.

### Open Question 3
- Question: Can we develop more meaningful evaluation metrics for interpretability that go beyond subgraph recovery accuracy?
- Basis in paper: [explicit] Section 5 mentions that current metrics don't capture meaningful human understanding and are limited by imperfect human-found circuits.
- Why unresolved: The field lacks consensus on what constitutes a good interpretability result beyond recovering known circuits.
- What evidence would resolve it: Development and validation of evaluation metrics based on functional understanding, generalization to new inputs, or alignment with human reasoning patterns.

### Open Question 4
- Question: How does EAP scale to larger models and more complex tasks?
- Basis in paper: [inferred] The paper demonstrates success on small models (GPT-2 small) and relatively narrow tasks, but doesn't test scalability.
- Why unresolved: Computational complexity and approximation accuracy may change dramatically with model size and task complexity.
- What evidence would resolve it: Empirical studies applying EAP to medium and large language models on diverse tasks, measuring both computational efficiency and recovery accuracy.

## Limitations
- The linear approximation used in EAP can be inaccurate for certain edge types, particularly those involving embeddings or weighted averages
- The evaluation relies on known circuits as ground truth, which may not capture complete task-relevant subnetworks and may include false positives
- The paper only demonstrates effectiveness on small models (GPT-2 small) and three specific tasks, with unknown generalization to larger models or different architectures

## Confidence
- High confidence: The efficiency claims regarding computational cost (2 forward passes + 1 backward pass vs exponential passes) are mathematically sound and clearly demonstrated
- Medium confidence: The claim that EAP outperforms ACDC in circuit recovery (higher AUC) is supported by experimental results, but approximation error creates uncertainty
- Low confidence: The combined method results (EAP + ACDC) are less conclusive due to computational constraints preventing optimal threshold finding

## Next Checks
1. Run attribution patching and activation patching on the same edge across multiple tasks and plot the correlation scatter diagram. Measure R² values to quantify approximation accuracy for different edge types (attention heads, MLPs, embeddings).

2. Apply EAP to at least two additional tasks not used in the original evaluation, including one with a different circuit structure (e.g., sentiment analysis or next-token prediction). Compare AUC performance to ACDC across all tasks.

3. Modify the linear approximation to include second-order terms for a subset of edges and measure the impact on circuit recovery performance. This would quantify how much the linear approximation limits effectiveness.