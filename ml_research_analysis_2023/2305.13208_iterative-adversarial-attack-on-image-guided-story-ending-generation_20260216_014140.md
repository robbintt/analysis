---
ver: rpa2
title: Iterative Adversarial Attack on Image-guided Story Ending Generation
arxiv_id: '2305.13208'
source_url: https://arxiv.org/abs/2305.13208
tags:
- adversarial
- attack
- multimodal
- text
- story
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Iterative-attack, an iterative adversarial
  attack method for multimodal text generation models. It fuses image and text modality
  attacks to find the most vulnerable multimodal information patch.
---

# Iterative Adversarial Attack on Image-guided Story Ending Generation

## Quick Facts
- arXiv ID: 2305.13208
- Source URL: https://arxiv.org/abs/2305.13208
- Reference count: 40
- Primary result: Iterative-attack achieves 57.09% attack success rate on VIST-E dataset, significantly outperforming single-modal and non-iterative multimodal attack methods.

## Executive Summary
This paper proposes Iterative-attack, an iterative adversarial attack method for multimodal text generation models that fuses image and text modality attacks. The method alternates between perturbing text and images to find the most vulnerable multimodal information patch, exploiting cross-modal complementarity. Iterative-attack demonstrates superior performance compared to single-modal and non-iterative multimodal attack methods in terms of attack success rate and semantic similarity on image-guided story ending generation tasks.

## Method Summary
Iterative-attack is an iterative multimodal adversarial attack method that combines text and image perturbations to attack image-guided story ending generation (IgSEG) models. The method calculates word importance scores by masking words and measuring their impact on model output, then generates text perturbations for top-ranked words using word substitution strategies. Simultaneously, it applies iterative PGD attacks to ending-related images to complement text changes. The attack minimizes adversarial loss through iterative updates until success or maximum iterations are reached. The method is evaluated on VIST-E and LSMDC-E datasets using metrics including Attack Success Rate (ASR), relative decrease of BLEU scores, semantic similarity, and perplexity.

## Key Results
- Iterative-attack achieves 57.09% attack success rate on VIST-E dataset, outperforming kNN (23.25%) and Co-attack (25.84%)
- The method maintains high semantic similarity while achieving successful attacks
- Iterative fusion of image and text perturbations proves more effective than single-modal approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative fusion of image and text adversarial perturbations increases attack success rate by exploiting cross-modal complementarity
- Mechanism: Perturbations in one modality can be compensated by information from the other modality, allowing identification of the most vulnerable multimodal information patch
- Core assumption: Multimodal models rely on both modalities for robust predictions, and perturbing both simultaneously uncovers weaknesses missed by single-modal attacks
- Evidence anchors: [abstract], [section 2.2.1], related papers on multimodal adversarial attacks

### Mechanism 2
- Claim: Word importance ranking guides targeted text perturbations to maximize adversarial loss
- Mechanism: Calculate importance scores for each word by measuring output change when masked, then replace top-ranked words with semantically similar substitutes
- Core assumption: Words with higher importance scores have greater influence on model predictions
- Evidence anchors: [section 3.3], related papers on importance-based perturbations

### Mechanism 3
- Claim: Adversarial loss minimization forces model to generate incorrect story endings
- Mechanism: Define adversarial loss as negative log probability of correct next token given adversarial context and image, then minimize this loss
- Core assumption: The model's output probability distribution can be shifted away from correct tokens through targeted perturbations
- Evidence anchors: [section 3.2], standard adversarial attack formulations

## Foundational Learning

- Concept: Multimodal fusion mechanisms
  - Why needed here: Understanding how models combine text and image information is crucial for designing effective adversarial attacks
  - Quick check question: What are the main approaches for fusing multimodal information in neural networks?

- Concept: Adversarial attack optimization techniques
  - Why needed here: Iterative optimization and gradient-based methods are essential for generating effective adversarial examples
  - Quick check question: How do gradient-based adversarial attacks differ from heuristic-based approaches?

- Concept: Text generation evaluation metrics
  - Why needed here: BLEU, METEOR, CIDEr, and ROUGE-L scores are used to measure attack success and semantic similarity
  - Quick check question: What are the strengths and limitations of BLEU score for evaluating text generation quality?

## Architecture Onboarding

- Component map: Text importance scorer -> Text perturbation generator -> Image adversarial attacker -> Loss optimizer -> Evaluation module
- Critical path:
  1. Calculate word importance scores
  2. Generate text perturbations for top-ranked words
  3. Apply image perturbations to complement text changes
  4. Evaluate attack success via BLEU score comparison
  5. Iterate until success or maximum iterations reached
- Design tradeoffs:
  - Number of perturbed words vs. semantic similarity
  - Attack success rate vs. computational runtime
  - Single-modal vs. multimodal attack effectiveness
  - Importance ranking accuracy vs. attack strength
- Failure signatures:
  - Attack success rate plateaus below threshold
  - Semantic similarity drops significantly
  - Runtime increases exponentially with iteration count
  - BLEU score comparisons become unreliable
- First 3 experiments:
  1. Validate importance scoring by comparing masked word effects on model output
  2. Test text perturbation effectiveness on single-modal attacks
  3. Evaluate image perturbation impact on multimodal model predictions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Iterative-attack method's performance change when applied to other multimodal text generation tasks beyond image-guided story ending generation?
- Basis in paper: [inferred] The paper discusses potential for other multimodal tasks but lacks experimental results
- Why unresolved: Paper focuses specifically on image-guided story ending generation task
- What evidence would resolve it: Experimental results on multimodal machine translation or multimodal question answering

### Open Question 2
- Question: What is the impact of varying the number of perturbed words in the text on the success rate and semantic similarity of Iterative-attack method?
- Basis in paper: [explicit] Paper discusses effect of varying perturbed words but lacks comprehensive analysis
- Why unresolved: No detailed study of trade-off between attack success rate and number of perturbed words
- What evidence would resolve it: Detailed study examining relationship between perturbed words, attack success rate, and semantic similarity

### Open Question 3
- Question: How does Iterative-attack method compare to other multimodal adversarial attack methods in terms of computational efficiency and scalability?
- Basis in paper: [inferred] Paper mentions runtime comparison with Co-attack but lacks comprehensive analysis
- Why unresolved: No exploration of computational efficiency and scalability compared to other methods
- What evidence would resolve it: Comparative analysis of computational efficiency and scalability with other multimodal adversarial attack methods

## Limitations
- The effectiveness of iterative fusion assumes that multimodal complementarity can be exploited, but may not work when models have robust cross-modal representations
- Word importance ranking using masking-based scores may not capture long-range dependencies or contextual nuances effectively
- Evaluation framework limitations in semantic similarity assessment methodology

## Confidence

- **High confidence**: The iterative attack framework and general approach of combining text and image perturbations are well-established concepts
- **Medium confidence**: Specific implementation details of word importance scoring and PGD-based image attacks on multimodal models
- **Low confidence**: Claim that Iterative-attack consistently outperforms baselines across all metrics and datasets due to dataset-dependent variations

## Next Checks

1. Conduct ablation studies to validate word importance scoring mechanism by comparing actual impact of top-ranked words versus randomly selected words on model predictions

2. Test Iterative-attack against multimodal models with different fusion architectures (early vs late fusion) to determine effectiveness across different multimodal integration approaches

3. Supplement automated semantic similarity metrics with human evaluations to verify adversarial examples maintain meaningful semantic similarity to original inputs