---
ver: rpa2
title: Prompting Large Language Models with Speech Recognition Abilities
arxiv_id: '2307.11795'
source_url: https://arxiv.org/abs/2307.11795
tags:
- audio
- encoder
- language
- embeddings
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method for enabling multilingual speech
  recognition by conditioning a large language model (LLM) with audio embeddings from
  a conformer encoder. The audio encoder is trained with a CTC loss to produce embeddings
  that are prepended to the LLM's text embeddings, allowing it to perform ASR in a
  decoder-only fashion.
---

# Prompting Large Language Models with Speech Recognition Abilities

## Quick Facts
- arXiv ID: 2307.11795
- Source URL: https://arxiv.org/abs/2307.11795
- Reference count: 24
- The paper introduces a method for enabling multilingual speech recognition by conditioning a large language model (LLM) with audio embeddings from a conformer encoder.

## Executive Summary
This paper presents a novel approach to multilingual speech recognition by leveraging the generative capabilities of large language models (LLMs). The method involves conditioning an LLM with audio embeddings produced by a conformer-based audio encoder trained with CTC loss. These audio embeddings are prepended to the LLM's text embeddings, effectively transforming the LLM into an ASR system. The approach is evaluated on the Multilingual LibriSpeech dataset, demonstrating significant improvements over monolingual baselines while using fewer trainable parameters. The study also explores the impact of audio encoder striding, model size, and low-rank adaptation, showing that multilingual ASR is achievable even with frozen LLM parameters or aggressive striding strategies.

## Method Summary
The method involves training a conformer-based audio encoder with CTC loss to produce embeddings aligned with text tokens. These embeddings are then stacked and projected to match the LLM's hidden dimension. The projected audio embeddings are prepended to the text embeddings, and the concatenated sequence is fed into the LLM. The LLM, adapted using low-rank adaptation (LoRA), generates transcriptions by predicting the next text token. The system is trained end-to-end on the Multilingual LibriSpeech dataset, with various ablation studies conducted to evaluate the impact of audio encoder striding, model size, and LoRA parameters on ASR performance.

## Key Results
- The proposed approach achieves an 18% improvement in average word error rate (WER) compared to monolingual baselines on the Multilingual LibriSpeech dataset.
- The system demonstrates multilingual ASR capabilities even when the LLM is frozen or when audio encoder striding of almost 1 second is used.
- Low-rank adaptation enables efficient fine-tuning of LLM parameters, reducing the number of trainable parameters while maintaining high ASR performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM can perform multilingual ASR by prepending audio embeddings from a conformer encoder to its text embeddings.
- Mechanism: The audio encoder is trained with CTC loss to produce embeddings in the same semantic space as text tokens. These embeddings are then concatenated with text embeddings, effectively conditioning the LLM on speech input. The LLM, trained on next-token prediction, treats the audio-conditioned sequence as a prompt and generates the corresponding transcription.
- Core assumption: Audio embeddings are semantically aligned with text embeddings after projection to LLM hidden dimension.
- Evidence anchors:
  - [abstract] "By directly prepending a sequence of audial embeddings to the text token embeddings, the LLM can be converted to an automatic speech recognition (ASR) system"
  - [section] "The output of the encoder is a sequence of 512-d vectors... These embeddings are prepended to the text embeddings (as specified in Figure 2) and fed into the LLM, which is tasked with predicting the next text based token."
  - [corpus] Weak evidence; related work focuses on fusion strategies but not this specific prepending approach.
- Break condition: If audio embeddings are not properly aligned with text embeddings, the LLM cannot effectively condition on speech input, leading to poor ASR performance.

### Mechanism 2
- Claim: Low-rank adaptation (LoRA) enables efficient fine-tuning of LLM parameters for ASR while keeping most parameters frozen.
- Mechanism: LoRA modifies the key, query, value, and output layers of the self-attention mechanism using low-rank matrices. This allows the LLM to adapt to the new ASR task without full fine-tuning, reducing computational cost and preserving original capabilities.
- Core assumption: LoRA parameters are sufficient to adapt LLM self-attention for ASR task without full parameter tuning.
- Evidence anchors:
  - [abstract] "The causal self-attention parameters of this system will be adapted using a parameter efficient Low-rank Adaptation (LoRA) [14]"
  - [section] "We use the Low-rank adaptation (LoRA) approach to adapt the key, query, value and output layers of the self-attention mechanism leaving feed-forward nets, embedding and final linear output layer unchanged."
  - [corpus] Weak evidence; related work uses adapter layers but not specifically LoRA for ASR conditioning.
- Break condition: If LoRA rank is too low or parameters are insufficient, the LLM cannot effectively adapt to ASR task, leading to degraded performance.

### Mechanism 3
- Claim: Audio encoder striding can significantly reduce sequence length while maintaining ASR performance.
- Mechanism: By stacking consecutive audio frames, the encoder produces fewer but higher-dimensional embeddings. This reduces memory consumption and enables processing of long-form audio while preserving temporal information needed for ASR.
- Core assumption: Stacked embeddings retain sufficient temporal resolution for accurate speech recognition.
- Evidence anchors:
  - [abstract] "Furthermore, we perform ablation studies to investigate... increasing the audio encoder striding to generate fewer embeddings"
  - [section] "To reduce sequence length and memory consumption, every n consecutive frames are stacked to form 512n-dimensional frames"
  - [corpus] Weak evidence; related work focuses on encoder architectures but not striding impact on sequence length.
- Break condition: If striding is too aggressive, temporal resolution is lost, leading to significant degradation in ASR accuracy.

## Foundational Learning

- Concept: Connectionist Temporal Classification (CTC) loss
  - Why needed here: Trains audio encoder to map variable-length audio sequences to fixed-length label sequences without requiring frame-level alignment
  - Quick check question: How does CTC loss handle the many-to-one mapping between audio frames and output tokens?

- Concept: Conformer architecture
  - Why needed here: Combines self-attention and convolution to capture both global and local patterns in audio, crucial for speech recognition
  - Quick check question: What is the role of the macaron-style feed-forward network in the conformer block?

- Concept: Low-rank adaptation (LoRA)
  - Why needed here: Enables efficient fine-tuning of large language models by decomposing weight updates into low-rank matrices, reducing trainable parameters
  - Quick check question: How does the rank parameter R affect the expressiveness of LoRA updates?

## Architecture Onboarding

- Component map:
  Audio input -> Filterbank features -> Conformer encoder -> Stacked embeddings -> Projection -> LLM text embeddings
  LLM (frozen except LoRA parameters) -> Next token prediction -> ASR output

- Critical path:
  Audio features -> Conformer encoder -> Embedding stacking -> Projection -> LLM conditioning -> ASR decoding

- Design tradeoffs:
  - Audio encoder size vs. ASR accuracy: Larger encoders provide better embeddings but increase computational cost
  - Striding factor vs. temporal resolution: Higher striding reduces sequence length but may lose fine-grained speech details
  - LoRA rank vs. adaptation quality: Higher rank allows better adaptation but increases trainable parameters

- Failure signatures:
  - Poor ASR accuracy despite proper training: Check audio-text embedding alignment and CTC loss convergence
  - High memory usage during training: Verify embedding stacking and striding configuration
  - LLM not adapting to ASR task: Check LoRA rank and parameter freezing configuration

- First 3 experiments:
  1. Verify CTC loss training on audio encoder alone with simple synthetic speech data
  2. Test embedding alignment by computing cosine similarity between audio and text embeddings for matched samples
  3. Validate LoRA adaptation by comparing frozen LLM performance vs. low-rank adapted performance on held-out ASR data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the audio encoder be trained to directly align embeddings with text tokens rather than relying on the LLM to infer alignment?
- Basis in paper: [explicit] The paper concludes that "Future work can make use of this observation by directly training the audio encoder to be aligned with the language model."
- Why unresolved: The current approach trains the audio encoder with CTC loss and relies on the LLM to learn the alignment through next-token prediction. This indirect training may not be optimal.
- What evidence would resolve it: Experiments comparing the current approach with an audio encoder trained using explicit alignment supervision (e.g., forcing monotonic alignment between audio and text embeddings) would show whether direct alignment training improves performance.

### Open Question 2
- Question: How does the performance of the LLM-based ASR system scale with increasing LLM size, and what is the optimal LLM size for multilingual ASR?
- Basis in paper: [explicit] The paper investigates replacing LLaMA-7B with various BLOOM models and shows a trend of improved performance with larger LLMs, but does not explore the full scaling curve or identify the optimal size.
- Why unresolved: The paper only tests three BLOOM models (BLOOM-560M, BLOOM-1B7, BLOOM-7B1) and does not provide a comprehensive analysis of how performance scales with LLM size or what the optimal size is for multilingual ASR.
- What evidence would resolve it: A systematic study varying the LLM size across a wide range (e.g., 100M to 100B parameters) and evaluating performance on multilingual ASR tasks would reveal the scaling behavior and optimal size.

### Open Question 3
- Question: How does the proposed approach perform on long-form audio, and what is the impact of different audio encoder striding strategies on long audio segments?
- Basis in paper: [explicit] The paper mentions that "These high striding systems could also be one viable avenue for operating on long-form audio, by compressing the audio sequence length orders of magnitude," but does not provide experimental results on long-form audio.
- Why unresolved: The paper only evaluates on short utterances (up to 20 seconds) and does not investigate how the system performs on longer audio segments or the impact of different striding strategies on long-form audio.
- What evidence would resolve it: Experiments evaluating the system on long-form audio (e.g., hours-long recordings) with varying striding strategies (e.g., fixed stride, adaptive stride) would show how the approach scales to long audio and which striding strategy is optimal.

## Limitations

- The evaluation is limited to the Multilingual LibriSpeech dataset, which may not generalize to other multilingual speech corpora with different characteristics or noise profiles.
- The study focuses on short utterances (up to 20 seconds), leaving the question of long-form speech recognition performance unanswered.
- The use of CTC pretraining for the audio encoder, while effective, may not be optimal compared to other pretraining strategies like masked prediction or contrastive learning.

## Confidence

**High Confidence**: The core mechanism of conditioning LLMs with audio embeddings for multilingual ASR is well-supported by experimental results showing 18% WER improvement over monolingual baselines. The CTC-based audio encoder training and LoRA adaptation approach are validated through multiple experiments.

**Medium Confidence**: Claims about the effectiveness of striding and its impact on temporal resolution are supported by ablation studies but could benefit from more extensive testing across different audio characteristics. The generalization claims across languages are based on MLS but need validation on additional multilingual datasets.

**Low Confidence**: The scalability claims to larger LLMs and longer utterances remain untested. The paper does not provide sufficient evidence for the approach's effectiveness in real-world deployment scenarios with varying audio quality and background noise conditions.

## Next Checks

1. **Cross-dataset validation**: Evaluate the same pretrained models on alternative multilingual speech datasets (e.g., Common Voice, VoxPopuli) to assess generalization beyond MLS. Compare performance across datasets with different language distributions and acoustic conditions.

2. **Long-form audio testing**: Test the approach on extended speech segments (5+ minutes) to identify potential degradation in performance or computational bottlenecks. Measure memory consumption and inference latency for different striding configurations.

3. **Robustness evaluation**: Test model performance under varying audio quality conditions including different signal-to-noise ratios, speaker variations, and background noise types. Compare against specialized ASR systems designed for noisy environments.