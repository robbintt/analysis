---
ver: rpa2
title: Evaluation of Human-Understandability of Global Model Explanations using Decision
  Tree
arxiv_id: '2309.09917'
source_url: https://arxiv.org/abs/2309.09917
tags:
- explanation
- global
- local
- explanations
- risk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the understandability of local and global
  model explanations for non-expert users in healthcare risk prediction. It evaluates
  narrative explanations generated from decision trees for coronary heart disease
  risk, measuring user comprehension through mental model changes and error rates.
---

# Evaluation of Human-Understandability of Global Model Explanations using Decision Tree

## Quick Facts
- arXiv ID: 2309.09917
- Source URL: https://arxiv.org/abs/2309.09917
- Reference count: 40
- Primary result: Individual cognitive styles significantly influence explanation preferences for CHD risk prediction, with error rates increasing with explanation complexity

## Executive Summary
This study evaluates how well non-expert users understand local versus global model explanations for coronary heart disease risk prediction. Using decision trees generated from the Busselton dataset, the research tests narrative explanations with 50 participants, measuring comprehension through mental model changes and error rates. While overall differences between explanation types were not statistically significant, the study reveals strong individual preferences: the majority preferred global explanations while a smaller group strongly preferred local explanations. Error rates increased with explanation complexity, particularly for cholesterol-related features, highlighting the importance of matching explanation design to user cognitive styles.

## Method Summary
The study uses the Busselton dataset (2874 patient records, 11 features after preprocessing) to generate decision trees with the GOSDT algorithm, creating four evaluation scenarios (local-easy, global-easy, local-hard, global-hard). Narrative explanations are generated using a rule-based algorithm, and 50 participants on Prolific evaluate these explanations through surveys measuring completeness, understandability, verbosity, mental model change, and comprehension errors. The research employs K-means clustering to identify user preference patterns and compares error rates across different explanation types and complexity levels.

## Key Results
- Strong individual preferences emerged: Group 2 (majority) preferred global explanations, while Group 1 strongly preferred local explanations
- Error rates increased with explanation complexity, particularly for cholesterol-related features
- Overall differences between local and global explanations were not statistically significant, but individual preferences were pronounced
- Mental model changes were similar across explanation types, suggesting comparable effectiveness in communicating model behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local explanations provide better accuracy in user comprehension than global explanations due to lower cognitive load.
- Mechanism: When explanations are limited to the specific decision path for an individual patient, users need to process fewer features and decision rules, reducing mental processing demands.
- Core assumption: Users can understand a single decision path without needing to understand the broader model structure.
- Evidence anchors:
  - [abstract]: "Error rates increased with explanation complexity, particularly for cholesterol-related features."
  - [section]: "The results indicate that certain people strongly prefer specific type of explanation. This preference does not necessarily translate to understandability."
- Break condition: If users need to understand model behavior across multiple patients or want to generalize findings, local explanations become insufficient and may introduce bias from incomplete understanding.

### Mechanism 2
- Claim: Global explanations provide better completeness and support generalization across patients, even if initial comprehension is harder.
- Mechanism: By presenting all decision rules in the tree, users develop a mental model of the complete decision-making process, enabling them to understand how different features interact across various patient scenarios.
- Core assumption: Users can handle increased cognitive load when it leads to more comprehensive understanding.
- Evidence anchors:
  - [abstract]: "The majority of participants prefer global explanations, while a smaller group prefers local explanations."
  - [section]: "Group 2: Majority group that rates global explanation as most understandable: This cluster consist of 22 people who has the least significance in preference between global, local explanation or difference based on the difficulty level."
- Break condition: If the global explanation becomes too complex or contains contradictory information, users may abandon the effort to understand it fully.

### Mechanism 3
- Claim: Individual cognitive styles and preferences significantly influence explanation effectiveness, overriding general patterns of explanation type superiority.
- Mechanism: Different users process information differently - some prefer detailed, specific information while others prefer comprehensive overviews. This individual variation determines which explanation type works best for each user.
- Core assumption: Users have consistent cognitive styles that influence their information processing preferences.
- Evidence anchors:
  - [abstract]: "The findings highlight the importance of individual cognitive styles in explanation preferences and suggest that explanation design should account for both complexity and user-specific preferences."
  - [section]: "It is evident that within the clusters, the ratings on each parameters has significant preferential pattern between each type of explanation."
- Break condition: If cognitive style assessment tools are not available, explanations must be designed to accommodate multiple processing preferences simultaneously.

## Foundational Learning

- Concept: Mental model alignment between users and AI systems
  - Why needed here: The study measures how well explanations help users understand the AI's decision-making process, requiring understanding of how humans build mental models of complex systems.
  - Quick check question: What is the difference between a user's mental model and the AI system's mental model?

- Concept: Feature importance and SHAP values
  - Why needed here: The study uses SHAP explanations as a baseline and measures how well users identify important features, requiring understanding of how feature importance is calculated and interpreted.
  - Quick check question: How do SHAP values differ from feature importance in decision tree explanations?

- Concept: Semifactual explanations and counterfactual reasoning
  - Why needed here: The study uses semifactual explanations ("even if" constructions) for complex scenarios, requiring understanding of how these differ from standard counterfactual explanations.
  - Quick check question: What is the key difference between a counterfactual explanation and a semifactual explanation?

## Architecture Onboarding

- Component map:
  - Data preprocessing pipeline → Decision tree generation → Explanation generation → User interface → Evaluation metrics
  - Key components: Busselton dataset preprocessing, GOSDT algorithm, explanation narration generator, Prolific survey interface, evaluation scoring system

- Critical path:
  - Data preprocessing → Tree generation → Explanation generation → User evaluation → Analysis
  - Most time-sensitive: Explanation generation must complete before user evaluation can begin

- Design tradeoffs:
  - Local vs global explanations: Balance between specificity and completeness
  - Complexity levels: Balance between understandability and information richness
  - Narrative style: Balance between technical accuracy and accessibility

- Failure signatures:
  - High error rates in feature selection indicate explanation clarity problems
  - Low change in mental model scores suggest explanations aren't effectively communicating model behavior
  - Contradictory explanations causing confusion indicate need for better presentation methods

- First 3 experiments:
  1. Test explanation comprehension with a small pilot group to validate survey questions and explanation clarity
  2. Compare local-easy vs local-hard explanations to establish baseline difficulty effects
  3. Test global-easy explanation with different demographic groups to identify potential biases in comprehension

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of narrative global explanations compare to other visualization-based global explanation methods for end-users?
- Basis in paper: [inferred] The paper uses narrative explanations but notes that visualization combined with narrative could be effective, and mentions this as a separate research problem not addressed here.
- Why unresolved: The study only evaluates narrative explanations without comparing them to visual explanations like decision tree diagrams or feature importance charts.
- What evidence would resolve it: A controlled experiment comparing narrative global explanations to visual global explanations (like tree diagrams) on the same comprehension and preference metrics.

### Open Question 2
- Question: What specific cognitive styles or user characteristics predict preference for local versus global explanations?
- Basis in paper: [explicit] The paper identifies three participant groups with different explanation preferences but notes that demographic data showed no clear patterns, leading to the suggestion that cognitive styles may be influential.
- Why unresolved: The clustering analysis identified preference patterns but couldn't explain the underlying reasons for these preferences based on available demographics.
- What evidence would resolve it: A study measuring cognitive styles (e.g., holistic vs. analytic thinking, need for cognitive closure) and correlating them with explanation preferences across diverse user populations.

### Open Question 3
- Question: How can semifactual explanations be effectively communicated to avoid confusion and errors in understanding?
- Basis in paper: [explicit] The paper notes that semifactual explanations in hard scenarios caused significant confusion, with almost half of participants excluding the contradictory feature from their understanding.
- Why unresolved: The study identifies the problem but doesn't test alternative communication strategies for semifactual information or ways to mitigate the confusion.
- What evidence would resolve it: An experiment testing different presentation formats for semifactual explanations (e.g., explicit contradiction markers, alternative framing, or interactive explanations) and measuring their impact on comprehension accuracy.

## Limitations

- Sample size of 50 participants may not represent diverse non-expert user populations
- Single disease domain (coronary heart disease) limits generalizability to other medical or non-medical applications
- Evaluation relies on self-reported comprehension rather than objective performance in real-world decision-making

## Confidence

**High Confidence**: Individual preference findings and error rate patterns showing increased complexity leading to more mistakes are well-established with clear statistical evidence.

**Medium Confidence**: Explanation type superiority depending on individual cognitive styles is supported by clustering analysis, but cognitive style assessment methodology is not fully described.

**Low Confidence**: Generalizability of specific preference ratios across different domains and user populations is not established, as this is based on a single study context.

## Next Checks

1. **Cognitive Style Validation**: Conduct a follow-up study using established cognitive style assessment tools (like the Cognitive Style Index) to validate whether the observed preference clusters align with known cognitive processing styles.

2. **Domain Transferability Test**: Replicate the study with a different medical domain (such as diabetes risk prediction) using the same methodology to assess whether the preference patterns and error rate findings hold across different feature sets and prediction tasks.

3. **Longitudinal Comprehension Assessment**: Implement a study design where users interact with explanations over multiple sessions and are later tested on their ability to make accurate predictions or decisions, providing more objective measures of explanation effectiveness beyond self-reported ratings.