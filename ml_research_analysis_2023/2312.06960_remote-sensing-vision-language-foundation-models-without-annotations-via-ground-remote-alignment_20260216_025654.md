---
ver: rpa2
title: Remote Sensing Vision-Language Foundation Models without Annotations via Ground
  Remote Alignment
arxiv_id: '2312.06960'
source_url: https://arxiv.org/abs/2312.06960
tags:
- images
- satellite
- image
- ground
- graft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to train vision-language models for
  remote-sensing images without textual annotations. The key insight is to use co-located
  internet imagery taken on the ground as an intermediary for connecting remote-sensing
  images and language.
---

# Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment

## Quick Facts
- arXiv ID: 2312.06960
- Source URL: https://arxiv.org/abs/2312.06960
- Reference count: 30
- Primary result: Zero-shot, open-vocabulary remote sensing vision-language model trained without textual annotations outperforms supervised VLMs by up to 20% for classification and 80% for segmentation.

## Executive Summary
This paper introduces GRAFT, a novel approach for training vision-language models for remote sensing images without requiring textual annotations. The key innovation is using co-located ground imagery as an intermediary to align satellite images with text through CLIP's pre-trained text encoder. By training satellite image encoders to align with CLIP's image encoder using ground-satellite pairs, the method enables zero-shot open-vocabulary recognition, retrieval, segmentation, and visual question answering for satellite imagery. GRAFT achieves state-of-the-art performance compared to supervised VLMs while eliminating the need for expensive textual annotation.

## Method Summary
GRAFT trains satellite image encoders by aligning them with CLIP's image encoder using co-located ground and satellite image pairs. The method employs a contrastive loss to pull corresponding satellite and ground image pairs together while pushing apart negative examples. This alignment is performed at both image-level (for classification and retrieval) and pixel-level (for segmentation and VQA). The approach leverages Flickr ground images matched to Sentinel-2 or NAIP satellite imagery based on geotags, ensuring temporal consistency for Sentinel-2 data. By using CLIP's frozen text encoder, the trained satellite encoders can perform zero-shot open-vocabulary tasks without requiring task-specific fine-tuning or textual annotations.

## Key Results
- Zero-shot classification accuracy on EuroSAT: 90.6% (vs 87.4% for CLIP with alt-text)
- Zero-shot retrieval performance on UCM and RESISC45: 81.3% and 82.8% respectively
- Zero-shot segmentation mIoU on NAIP-OSM: 51.8% (vs 29.2% for CLIP with alt-text)
- Zero-shot VQA performance on AID and UCM: up to 57.1% (vs 36.4% for CLIP with alt-text)

## Why This Works (Mechanism)

### Mechanism 1
Ground-satellite image pairs enable alignment of remote sensing images with text without explicit textual annotations. The geographical co-location allows satellite encoders to inherit CLIP's text alignment through transitive alignment. This works when ground images capture sufficient semantic similarity to satellite images, acting as an effective intermediary.

### Mechanism 2
The contrastive loss handles multiple ground images per satellite image by aggregating positive examples. The loss function computes similarity between a satellite image and all its associated ground images as positives, treating all other ground images in the batch as negatives. This extends standard contrastive learning to multi-positive scenarios.

### Mechanism 3
Pixel-level features are learned by aligning individual pixels/patches in satellite images with their corresponding ground images. The pixel-level model produces feature vectors for every pixel/patch, using a similar contrastive loss to align these features with CLIP embeddings of co-located ground images. The learned features from sparse supervision at ground image locations are sufficient to propagate to the entire image.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: To align satellite images with ground images in the CLIP feature space without explicit textual annotations
  - Quick check question: How does the contrastive loss handle multiple positive examples (ground images) for a single satellite image?

- Concept: Vision transformers (ViT)
  - Why needed here: To encode satellite images into feature representations that can be aligned with CLIP embeddings
  - Quick check question: What is the role of the ViT architecture in the pixel-level model for producing per-pixel/patch features?

- Concept: Zero-shot learning
  - Why needed here: To enable open-vocabulary recognition and retrieval on satellite images without task-specific fine-tuning
  - Quick check question: How does the trained satellite image encoder enable zero-shot classification using text prompts?

## Architecture Onboarding

- Component map:
  Data pipeline: Flickr ground image collection → Geotag matching → Sentinel-2/NAIP satellite image retrieval
  Model: CLIP-based image encoder (frozen) + ViT-based satellite image encoder (trainable)
  Loss: Modified contrastive loss for image-level and pixel-level alignment
  Evaluation: Zero-shot classification, retrieval, segmentation, VQA

- Critical path:
  1. Data collection and preprocessing (ground-satellite pairs)
  2. Model initialization (CLIP weights)
  3. Training (contrastive loss optimization)
  4. Evaluation on downstream tasks

- Design tradeoffs:
  Using CLIP as frozen backbone vs training from scratch (faster convergence vs potential domain mismatch)
  Image-level vs pixel-level models (classification/retrieval vs segmentation/VQA)
  Number of ground images per satellite image (more positives vs computational cost)

- Failure signatures:
  Poor alignment between satellite and ground image features (check loss values, nearest neighbor retrieval)
  Overfitting to ground image patterns (evaluate on held-out satellite images)
  Degraded performance on specific tasks (check task-specific metrics)

- First 3 experiments:
  1. Train image-level model on a small subset of data and evaluate zero-shot retrieval on EuroSAT
  2. Train pixel-level model and evaluate zero-shot segmentation on a held-out NAIP-OSM subset
  3. Compare GRAFT performance against CLIP baseline on a simple classification task

## Open Questions the Paper Calls Out

### Open Question 1
How can we design a vision-language model for satellite imagery that can generate descriptive text captions, similar to models like BLIP-2 for general images? The paper mentions CLIP-like VLMs lack text-generation capabilities and suggests combining GRAFT with ClipCap for future work, but doesn't provide a concrete method.

### Open Question 2
Can we develop a vision-language model for satellite imagery that is less biased towards ground image concepts, particularly for spatial relationships like "left of" or "top of"? The paper notes potential bias in the text encoder towards ground image concepts and suggests fine-tuning with satellite image-caption data as a possible approach.

### Open Question 3
How can we extend the GRAFT approach to handle dynamic objects in satellite imagery, such as ships or airplanes? The paper discusses limitations in capturing dynamic objects due to low satellite revisit rates and suggests using higher temporal resolution as a potential solution.

## Limitations
- The core assumption that ground images provide sufficient semantic alignment with satellite imagery may not hold across all remote sensing scenarios, particularly for specialized domains where ground-level views differ substantially from overhead perspectives.
- Scalability of collecting quality ground-satellite pairs remains uncertain due to temporal and geographical matching requirements that may limit dataset diversity and size.
- The approach may struggle with dynamic objects in satellite imagery due to the low revisit rate of satellites, making it difficult to capture changes between ground and satellite observations.

## Confidence
- High confidence in the fundamental mechanism of using ground-satellite pairs for unsupervised alignment (well-supported by contrastive learning literature and empirical results)
- Medium confidence in the scalability and robustness of the approach across diverse remote sensing domains (limited by dataset coverage and potential domain shifts)
- Medium confidence in the pixel-level model's ability to generalize from sparse supervision to full segmentation (requires more extensive validation on challenging scenarios)

## Next Checks
1. Evaluate GRAFT performance on specialized remote sensing domains (e.g., agricultural monitoring, urban heat mapping) where ground-satellite semantic alignment may be less reliable
2. Conduct ablation studies varying the number of ground images per satellite image to determine optimal positive example aggregation strategies
3. Test the robustness of zero-shot segmentation performance when ground images contain significant occlusions or viewpoint differences from satellite imagery