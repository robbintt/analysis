---
ver: rpa2
title: Large Language Models are Miscalibrated In-Context Learners
arxiv_id: '2312.13772'
source_url: https://arxiv.org/abs/2312.13772
tags:
- calibration
- learning
- performance
- sicl
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper examines calibration and performance across four learning\
  \ paradigms\u2014zero-shot, in-context, supervised fine-tuning, and supervised in-context\
  \ learning\u2014on seven classification datasets with limited training data. The\
  \ authors find that supervised methods improve accuracy on unseen data but often\
  \ suffer from miscalibration, especially in low-resource scenarios."
---

# Large Language Models are Miscalibrated In-Context Learners

## Quick Facts
- arXiv ID: 2312.13772
- Source URL: https://arxiv.org/abs/2312.13772
- Reference count: 40
- Key outcome: Self-ensembling reduces expected calibration error by 43% on average while maintaining or improving task performance

## Executive Summary
This paper examines calibration and performance across four learning paradigms—zero-shot, in-context, supervised fine-tuning, and supervised in-context learning—on seven classification datasets with limited training data. The authors find that supervised methods improve accuracy on unseen data but often suffer from miscalibration, especially in low-resource scenarios. Self-ensembling using multiple variations of in-context examples and/or prompts consistently reduces expected calibration error by an average of 43% while maintaining or improving task performance. The approach is effective for all learning methods, with max probability ensembling yielding the most calibrated predictions. The findings suggest self-ensembling as a practical method to enhance both accuracy and reliability of large language model predictions.

## Method Summary
The study evaluates four learning paradigms (zero-shot, in-context, supervised fine-tuning, and supervised in-context learning) on seven classification datasets using Flan-T5 models. The authors implement self-ensembling techniques with three types of input variations (in-context examples, prompts, or both) and three ensembling strategies (majority vote, mean probability, max probability). Performance is measured using accuracy and F1 scores, while calibration is assessed via expected calibration error (ECE), negative log-likelihood (NLL), and information entropy. The experiments focus on low-resource scenarios by sub-sampling training data from each dataset.

## Key Results
- Self-ensembling reduces expected calibration error by 43% on average across all learning methods and datasets
- Max probability ensembling consistently produces the most calibrated predictions with minimal performance degradation
- Supervised fine-tuning and supervised in-context learning improve accuracy but exhibit higher miscalibration than zero-shot and in-context learning
- The effectiveness of different ensembling variations depends on the learning paradigm (Var-IC works better with ICL, while Var-Prompt excels with SFT/SICL)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-ensembling reduces calibration error by introducing input variations that smooth probability distributions
- Mechanism: Multiple variations of in-context examples and prompts create diverse ensemble components. Max probability ensembling selects the highest probability across components, then normalizes to create a smoother distribution
- Core assumption: Diverse input variations produce probability distributions that, when combined via max probability, yield more calibrated predictions
- Evidence anchors:
  - [abstract]: "Self-ensembling using multiple variations of in-context examples and/or prompts consistently reduces expected calibration error by an average of 43%"
  - [section 4.2]: "Ensembling with max probability consistently produces the most faithful predictions with promising performances"
  - [corpus]: Weak evidence - only general calibration papers found, no specific self-ensembling literature

### Mechanism 2
- Claim: Supervised methods improve performance but suffer from overconfidence, which self-ensembling mitigates
- Mechanism: Supervised fine-tuning and supervised in-context learning adapt models to specific tasks but introduce overconfidence. Self-ensembling counteracts this by averaging predictions across variations
- Core assumption: Supervised methods create overconfident predictions that can be calibrated through ensemble averaging
- Evidence anchors:
  - [abstract]: "supervised methods improve accuracy on unseen data but often suffer from miscalibration, especially in low-resource scenarios"
  - [section 6.2]: "SFT and SICL exhibit a larger drop in ECE after self-ensembling than ICL"
  - [corpus]: Weak evidence - general calibration literature found but not specific to supervised methods' overconfidence

### Mechanism 3
- Claim: Task-dependent behavior requires different learning paradigms based on data contamination status
- Mechanism: Models pre-trained on Flan corpus show different calibration and performance on "seen" vs "unseen" datasets. Self-ensembling provides consistent improvements across both scenarios
- Core assumption: Pre-training data contamination creates systematic differences in model behavior across datasets
- Evidence anchors:
  - [abstract]: "We find that learning methods perform differently depending on the datasets"
  - [section 6.1]: "ICL demonstrates comparable performance to SFT/SICL on SST-2 and RTE" but suffers from higher ECE
  - [corpus]: Weak evidence - general contamination literature found but not specific to calibration effects

## Foundational Learning

- Concept: Expected Calibration Error (ECE)
  - Why needed here: Primary metric for measuring calibration quality across different learning methods
  - Quick check question: How does ECE differ from accuracy as an evaluation metric?

- Concept: In-context learning variations
  - Why needed here: Foundation for understanding how different demonstration combinations affect model behavior
  - Quick check question: What distinguishes zero-shot learning from in-context learning?

- Concept: Self-ensembling strategies
  - Why needed here: Core technique being evaluated for improving both performance and calibration
  - Quick check question: How does max probability ensembling differ from majority vote in implementation?

## Architecture Onboarding

- Component map: Flan-T5 model → Learning method (ZSL/ICL/SFT/SICL) → Input variations (IC examples/prompts) → Self-ensembling (max/mean/majority) → Output predictions
- Critical path: Input variations → Model predictions → Ensembling strategy → Final calibrated predictions
- Design tradeoffs: More variations improve calibration but increase computational cost; different ensembling strategies balance performance vs calibration
- Failure signatures: High ECE despite self-ensembling, performance degradation after ensembling, inconsistent results across random seeds
- First 3 experiments:
  1. Compare baseline learning methods (ZSL, ICL, SFT, SICL) on SST-2 dataset with ECE measurement
  2. Apply self-ensembling with max probability to SFT on Manifestos dataset
  3. Test Var-Prompt effectiveness on Hate Speech dataset with Flan-T5-large

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do decoder-only models like Llama compare to encoder-decoder models like Flan-T5 in low-resource scenarios with different learning methods and self-ensembling strategies?
- Basis in paper: [inferred] The paper uses Flan-T5 (an encoder-decoder model) for all experiments but acknowledges this as a limitation, noting they haven't investigated decoder-only models like Llama.
- Why unresolved: The paper explicitly states this as a limitation and didn't have the resources to test decoder-only models, leaving a gap in understanding how different model architectures might behave under these conditions.
- What evidence would resolve it: Running the same experimental setup with a decoder-only model like Llama (within similar parameter limits) and comparing the results across learning methods and self-ensembling would provide direct evidence of architectural differences.

### Open Question 2
- Question: How does self-ensembling scale with model size beyond the 3B parameter limit explored in this paper?
- Basis in paper: [inferred] The authors mention they only tested up to Flan-T5-xl (2.85B parameters) due to computational constraints and express intent to explore larger models in future work.
- Why unresolved: Resource limitations prevented testing on larger models, leaving uncertainty about whether the observed benefits of self-ensembling would persist or diminish at larger scales.
- What evidence would resolve it: Running the complete experimental suite on models like Flan-T5-xxl or other large language models (10B+ parameters) would show whether the calibration and performance improvements scale with model size.

### Open Question 3
- Question: Are there more sophisticated ensembling strategies beyond max probability, mean probability, and majority vote that could further improve calibration and performance?
- Basis in paper: [explicit] The authors acknowledge they only studied three basic ensembling strategies and mention there are "a variety of sophisticated ensembling methods" that could be explored.
- Why unresolved: The paper only scratches the surface of potential ensembling techniques, leaving open whether more advanced methods could yield better results.
- What evidence would resolve it: Implementing and testing more sophisticated ensembling approaches (e.g., weighted averaging based on confidence, Bayesian model averaging, or learned ensembling weights) would reveal if better strategies exist.

## Limitations
- Focuses primarily on classification tasks with relatively small datasets, limiting generalizability to other task types or larger-scale settings
- Specific effectiveness of different ensembling strategies may vary with model scale but this relationship is not fully explored
- Does not investigate computational overhead implications of self-ensembling in production settings

## Confidence

**High confidence**: The empirical finding that self-ensembling reduces ECE by 43% on average is well-supported by the experimental results across multiple datasets and learning paradigms.

**Medium confidence**: The claim that max probability ensembling is consistently the most effective strategy has strong evidence but may depend on specific dataset characteristics and model sizes.

**Low confidence**: The mechanism explaining why self-ensembling works (smoothing probability distributions through diverse input variations) lacks direct empirical validation and relies on theoretical assumptions.

## Next Checks

1. **Mechanism validation experiment**: Design a controlled experiment to test whether the effectiveness of self-ensembling correlates with the diversity of input variations. Measure diversity metrics (e.g., pairwise similarity) between different prompt/example combinations and correlate with ECE reduction.

2. **Generalization test**: Apply the self-ensembling approach to non-classification tasks (e.g., generation tasks, structured prediction) to verify if the 43% ECE reduction generalizes beyond the studied classification datasets.

3. **Scaling study**: Systematically vary the number of ensemble components (n) and model sizes to determine optimal scaling relationships. Identify at what point additional variations provide diminishing returns or where computational cost outweighs calibration benefits.