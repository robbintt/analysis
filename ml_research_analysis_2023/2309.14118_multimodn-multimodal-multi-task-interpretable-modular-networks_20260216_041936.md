---
ver: rpa2
title: MultiModN- Multimodal, Multi-Task, Interpretable Modular Networks
arxiv_id: '2309.14118'
source_url: https://arxiv.org/abs/2309.14118
tags:
- multimodn
- tasks
- p-fusion
- performance
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MultiModN introduces a modular, sequential approach to multimodal
  fusion, enabling any number, combination, or type of modalities to be input in a
  composable pipeline while providing granular predictive feedback across multiple
  tasks. Unlike parallel fusion baselines that require all modalities and suffer from
  biased missingness, MultiModN skips missing inputs without encoding their absence,
  making it inherently robust to missing not-at-random (MNAR) patterns.
---

# MultiModN- Multimodal, Multi-Task, Interpretable Modular Networks

## Quick Facts
- arXiv ID: 2309.14118
- Source URL: https://arxiv.org/abs/2309.14118
- Authors: 
- Reference count: 40
- Primary result: MultiModN matches parallel fusion baselines while offering modularity, interpretability, and robustness to MNAR missingness.

## Executive Summary
MultiModN introduces a modular, sequential approach to multimodal fusion, enabling any number, combination, or type of modalities to be input in a composable pipeline while providing granular predictive feedback across multiple tasks. Unlike parallel fusion baselines that require all modalities and suffer from biased missingness, MultiModN skips missing inputs without encoding their absence, making it inherently robust to missing not-at-random (MNAR) patterns. Across three benchmark datasets (MIMIC, EDU, Weather2k) and ten real-world tasks, MultiModN matches the performance of parallel fusion models while offering modularity, interpretability, and flexibility at inference. It avoids catastrophic failure under MNAR conditions, provides modality-specific interpretability, and naturally extends to multi-task settings without performance loss.

## Method Summary
MultiModN uses a sequential, modular architecture where each modality has its own encoder that updates a shared state vector. Missing modalities are skipped entirely, avoiding encoding their absence. Task-specific decoders operate on the evolving state, enabling multi-task learning without performance loss. The model is trained using cross-entropy or MSE loss averaged across tasks, with gradients flowing only through active encoders. Performance is evaluated against a parallel fusion baseline (P-Fusion) using 5-fold cross-validation on MIMIC, EDU, and Weather2k datasets across 10 tasks.

## Key Results
- MultiModN matches P-Fusion performance on all 10 tasks across 3 datasets while providing interpretability and flexibility.
- MultiModN is inherently robust to MNAR missingness by skipping missing modalities instead of encoding their absence.
- MultiModN naturally extends to multi-task settings without performance loss, sharing the same state representation across decoders.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MultiModN avoids catastrophic failure under MNAR conditions by skipping missing modalities instead of encoding their absence.
- Mechanism: When a modality is missing, the corresponding encoder is skipped entirely, so no embedding for missingness is created. This prevents the model from learning spurious correlations between the presence/absence of a feature and the label.
- Core assumption: Missingness patterns in training and inference are independent; skipping missing inputs preserves the intended feature semantics.
- Evidence anchors:
  - [abstract] "MultiModN skips missing inputs without encoding their absence, making it inherently robust to missing not-at-random (MNAR) patterns."
  - [section] "A missing modality is skipped (encoder ei is not used) and not padded/encoded. Thus, MultiModN avoids featurizing missingness, which is particularly advantageous when missingness is MNAR."
- Break condition: If the decision to skip a modality leaks information about the label (e.g., systematic clinical triage), the skipping mechanism itself becomes a proxy for the missingness pattern.

### Mechanism 2
- Claim: Sequential modular fusion matches parallel fusion performance while providing modality-specific interpretability.
- Mechanism: Each encoder updates a shared state vector that captures the cumulative information from previous modalities. Because each state is generated by a single modality encoder, its contribution to downstream predictions can be traced directly.
- Core assumption: The state representation is sufficiently expressive to aggregate cross-modal information without parallel fusion.
- Evidence anchors:
  - [abstract] "MultiModN's composable pipeline is interpretable-by-design, as well as innately multi-task and robust to the fundamental issue of biased missingness."
  - [section] "Due to modularization, MultiModN is model-agnostic, whereby encoders can be of any type of architecture... The contribution of each input can be decomposed by module."
- Break condition: If the state size is too small to capture inter-modal interactions, sequential fusion performance will degrade below the parallel baseline.

### Mechanism 3
- Claim: MultiModN extends naturally to multi-task learning without performance loss because decoders share the same evolving state.
- Mechanism: All task-specific decoders receive the same state vector at each step, allowing them to extract task-relevant features from the same multimodal representation.
- Core assumption: Tasks are semantically related enough that shared representations are beneficial; no task-specific feature engineering is required.
- Evidence anchors:
  - [abstract] "naturally extends to multi-task settings without performance loss."
  - [section] "Each decoder is assigned to a single task... Decoder parameters are shared across the different modalities."
- Break condition: If tasks are unrelated or require very different feature granularities, the shared state will become a bottleneck and multi-task performance will suffer.

## Foundational Learning

- Concept: Missingness mechanisms (MCAR, MAR, MNAR)
  - Why needed here: To understand why encoding missingness is harmful in MNAR scenarios.
  - Quick check question: If a feature is missing because of a clinical decision rule, what type of missingness is this?

- Concept: Modular network design
  - Why needed here: To grasp how self-contained encoder/decoder modules enable composability and interpretability.
  - Quick check question: In a modular network, what is the relationship between the number of modalities and the number of possible state configurations?

- Concept: Multimodal fusion strategies
  - Why needed here: To differentiate sequential vs. parallel fusion and their implications for performance and interpretability.
  - Quick check question: Why might parallel concatenation of embeddings lead to curse-of-dimensionality issues?

## Architecture Onboarding

- Component map: Input layer → Modality-specific encoders → Shared state vector → Task-specific decoders
- Critical path: Forward pass: Encode available modalities in sequence → Update state → Decode with all task decoders → Average losses; Backward pass: Gradients flow through active encoders only; missing modalities contribute no gradients.
- Design tradeoffs: Sequential fusion vs. parallel fusion: Sequential is interpretable and MNAR-robust but may require careful state sizing; Single state vs. multiple states: Single state simplifies architecture but may limit parallel modality encoding; Modality-skipping vs. imputation: Skipping avoids MNAR bias but requires training data with diverse missingness patterns.
- Failure signatures: Degraded performance on tasks requiring cross-modal interactions not captured by sequential state updates; Overfitting when state size is too large relative to data complexity; Underperformance when tasks are unrelated and cannot benefit from shared representations.
- First 3 experiments:
  1. Single-task classification on MIMIC with all modalities present; compare AUROC to parallel fusion baseline.
  2. Multi-task training on EDU; verify no performance drop on individual tasks.
  3. MNAR simulation: remove one modality from one class in training; test on clean and label-flipped test sets to confirm robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MultiModN scale with the number of modalities and tasks, and what are the computational and memory trade-offs?
- Basis in paper: [inferred] The paper mentions that MultiModN is theoretically able to handle any number or combination of modalities and tasks, but this has not been empirically tested. It also notes that having a high combinatorial generalization comes at a computational and performance cost.
- Why unresolved: The paper does not provide experimental results demonstrating the scaling behavior of MultiModN with increasing numbers of modalities and tasks. The authors only speculate about potential computational and memory trade-offs.
- What evidence would resolve it: Conducting experiments with varying numbers of modalities and tasks to measure the performance, computational cost, and memory usage of MultiModN. Comparing these results to baseline models would provide insights into the scalability and trade-offs.

### Open Question 2
- Question: Can MultiModN effectively handle time-series data with varying shapes and lengths, and how does the state representation size impact its performance?
- Basis in paper: [explicit] The paper mentions that MultiModN is naturally aligned with time-series data and that the state representation size may need to be parameterized to capture predictive trends of varying shapes and lengths.
- Why unresolved: The paper does not provide experimental results demonstrating MultiModN's performance on time-series data with varying shapes and lengths. It also does not explore the impact of state representation size on the model's ability to capture temporal patterns.
- What evidence would resolve it: Conducting experiments with time-series datasets of varying shapes and lengths to evaluate MultiModN's performance. Investigating the impact of different state representation sizes on the model's ability to capture temporal patterns would provide insights into the optimal configuration for time-series data.

### Open Question 3
- Question: How does the order of modality encoding in MultiModN affect its performance, and can the model be made order-invariant?
- Basis in paper: [explicit] The paper mentions that MultiModN currently uses a fixed order of modality encoding and that order invariance could be achieved by training every permutation of encoders.
- Why unresolved: The paper does not provide experimental results comparing the performance of MultiModN with different orders of modality encoding. It also does not explore the feasibility and impact of making the model order-invariant.
- What evidence would resolve it: Conducting experiments with different orders of modality encoding to evaluate their impact on MultiModN's performance. Investigating the feasibility and impact of training every permutation of encoders to achieve order invariance would provide insights into the potential benefits and trade-offs of this approach.

## Limitations

- Performance parity with P-Fusion is demonstrated, but ablation studies on state size sensitivity and sequential fusion efficiency are absent.
- MNAR robustness is inferred from controlled experiments but not tested under realistic, clinically-driven missingness patterns.
- Interpretability is claimed via "modality-specific decomposition" but quantitative measures or human evaluation are not provided.

## Confidence

- High: MultiModN architecture design and modular fusion approach are clearly specified and reproducible.
- Medium: Claims about MNAR robustness and multi-task performance are supported by experimental results but lack depth in stress-testing.
- Low: Interpretability claims are largely anecdotal, with no standardized metrics or user studies.

## Next Checks

1. Stress-test MNAR robustness: Simulate MNAR patterns driven by clinical decision rules (e.g., only measuring blood pressure in high-risk patients) and evaluate whether skipping still prevents bias propagation.

2. Quantify interpretability: Apply SHAP or LIME to decoder outputs and measure the correlation between state contributions and task predictions across modalities. Compare against parallel fusion baselines.

3. Ablate state size: Systematically vary the state vector dimension and measure performance degradation on both single- and multi-task settings to identify the minimal sufficient size for cross-modal integration.