---
ver: rpa2
title: 'SCStory: Self-supervised and Continual Online Story Discovery'
arxiv_id: '2312.03725'
source_url: https://arxiv.org/abs/2312.03725
tags:
- article
- story
- scstory
- articles
- stories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SCStory, a framework for online news story
  discovery that addresses limitations in existing methods which use static and generic
  article embeddings. SCStory employs a self-supervised and continual learning approach
  with a novel idea of story-indicative adaptive modeling.
---

# SCStory: Self-supervised and Continual Online Story Discovery

## Quick Facts
- **arXiv ID**: 2312.03725
- **Source URL**: https://arxiv.org/abs/2312.03725
- **Reference count**: 40
- **Primary result**: Achieves up to 20.7% higher ARI scores than state-of-the-art algorithms for online news story discovery

## Executive Summary
SCStory introduces a novel framework for online news story discovery that addresses the limitations of static and generic article embeddings used in existing methods. The approach employs self-supervised and continual learning with a unique story-indicative adaptive modeling strategy. By using a lightweight hierarchical embedding module that first learns sentence representations and then article representations, SCStory identifies story-relevant information more effectively. The model is continuously updated through contrastive learning, supported by confidence-aware memory replay and prioritized-augmentation techniques to handle label absence and data scarcity issues.

## Method Summary
SCStory implements a hierarchical embedding pipeline that processes news articles through a frozen pretrained sentence encoder to obtain initial sentence representations. These are refined through a multi-head self-attention mechanism to capture contextual relationships, followed by attentive pooling to generate story-indicative article representations. The model employs contrastive learning for self-supervised updates, using confidence-aware memory replay to sample articles proportionally to their story-confidence scores, and prioritized-augmentation that combines high-attention-weight sentences from different articles within the same story. This approach enables the model to discover and track evolving story themes in streaming news data without human annotations.

## Key Results
- Achieves up to 20.7% higher Adjusted Rand Index (ARI) scores compared to state-of-the-art algorithms
- Demonstrates superior performance across multiple real news datasets including News14, WCEP18, WCEP19, and CaseStudy
- Shows consistent improvements in B-cubed scores (B3-P, B3-R, B3-F1) and Adjusted Mutual Information (AMI) metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Hierarchical sentence-then-article embedding captures story-indicative semantics better than direct article embedding
- **Mechanism**: Uses frozen pretrained encoder for sentence representations, then lightweight attention-based model refines them into article representations emphasizing story-relevant information
- **Core assumption**: Story themes concentrate on a few sentences with distinct semantics rather than being uniformly distributed
- **Evidence anchors**: Abstract and section text describing hierarchical embedding approach
- **Break condition**: If stories are uniformly distributed across sentences or require full-article context

### Mechanism 2
- **Claim**: Confidence-aware memory replay with prioritized augmentation maintains temporal consistency while handling label absence and data scarcity
- **Mechanism**: Samples articles from current window with probability proportional to story-confidence scores, then augments by concatenating top-half sentences from one article with bottom-half from another in same story
- **Core assumption**: Story-relevant information can be captured by combining high-attention-weight sentences from multiple articles within same story
- **Evidence anchors**: Abstract and section text describing memory replay and augmentation techniques
- **Break condition**: If story-relevant information cannot be captured by sentence-level attention weights

### Mechanism 3
- **Claim**: Contrastive learning with self-supervision aligns articles within stories while maintaining uniformity across different stories
- **Mechanism**: Pushes articles closer to their assigned story representations while pushing away from other stories in embedding space through contrastive loss
- **Core assumption**: Story representations as means of constituent articles provide stable targets for contrastive learning
- **Evidence anchors**: Abstract and section text describing contrastive learning objective
- **Break condition**: If story boundaries are too fuzzy or articles belong to multiple stories simultaneously

## Foundational Learning

- **Concept**: Hierarchical attention mechanisms in transformer architectures
  - Why needed here: Model needs to learn which sentences are most relevant to story themes through self-attention at sentence level before aggregating to article level
  - Quick check question: Can you explain how multi-head self-attention differs from standard self-attention and why it's beneficial for identifying story-indicative sentences?

- **Concept**: Contrastive learning objectives and their relationship to alignment/uniformity
  - Why needed here: Model optimizes contrastive loss that promotes articles within same story to be close while maintaining separation between different stories
  - Quick check question: What is the mathematical relationship between contrastive loss and the alignment/uniformity metrics mentioned in the paper?

- **Concept**: Memory replay techniques in continual learning
  - Why needed here: Model needs to preserve previously learned knowledge while adapting to new articles without catastrophic forgetting
  - Quick check question: How does confidence-aware sampling differ from random sampling in memory replay, and what advantage does it provide for story discovery?

## Architecture Onboarding

- **Component map**: Raw articles → Frozen PSE → MHS layer → Attentive pooling → Story assignment → Memory replay → Prioritized augmentation → Contrastive loss → Next window
- **Critical path**: Article → PSE → MHS → Attentive pooling → Story assignment → Memory replay → Contrastive update → Next window
- **Design tradeoffs**:
  - Frozen PSE vs. fine-tuning: Faster inference and prevents catastrophic forgetting vs. potentially better adaptation
  - Confidence-based sampling vs. random: More informative samples vs. risk of bias toward high-confidence but potentially noisy examples
  - Sentence-level augmentation vs. document-level: Better preservation of story-indicative vs. loss of document-level coherence
- **Failure signatures**:
  - Low ARI/AMI scores with high variance: Memory replay or augmentation strategies not capturing temporal patterns
  - Degraded performance with longer window sizes: Model unable to handle broader temporal scopes
  - Sudden drops in accuracy: Catastrophic forgetting due to insufficient memory replay scope
- **First 3 experiments**:
  1. Verify sentence attention weights correctly identify story-indicative sentences by visualizing attention distributions on articles with known themes
  2. Test memory replay sampling effectiveness by comparing confidence-weighted vs random sampling on a small dataset with ground truth
  3. Validate contrastive loss optimization by measuring alignment/uniformity metrics during training on synthetic story clusters

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the choice of sentence-level vs. token-level attention in the story-indicative encoder impact the quality of discovered stories?
- **Basis in paper**: The paper uses sentence-level multi-head self-attention (MHS) instead of token-level MHS, citing benefits like capturing story-indicative sentences. However, it doesn't compare the two approaches.
- **Why unresolved**: The paper doesn't provide an ablation study or comparison between sentence-level and token-level attention in the story-indicative encoder.
- **What evidence would resolve it**: An experiment comparing the performance of SCStory with sentence-level MHS vs. token-level MHS on the same datasets and metrics.

### Open Question 2
- **Question**: How does the memory replay strategy in SCStory scale with increasing window sizes and evolving story lifespans?
- **Basis in paper**: The paper discusses the memory replay strategy but mentions it may not be practical to consider all previous articles. It also notes that some stories have different lifespans.
- **Why unresolved**: The paper doesn't provide a detailed analysis of how the memory replay strategy performs with varying window sizes and story lifespans. It also doesn't explore alternative memory replay strategies.
- **What evidence would resolve it**: Experiments comparing the performance of SCStory with different memory replay strategies (e.g., considering all previous articles, only the most recent N articles) and varying window sizes. Analysis of how the memory replay strategy affects the performance on stories with different lifespans.

### Open Question 3
- **Question**: How does the choice of pretrained sentence encoder (PSE) impact the performance of SCStory?
- **Basis in paper**: The paper uses two PSEs (Sentence-BERT and Sentence-T5) and compares their performance. However, it doesn't explore other PSEs or analyze the impact of PSE choice in detail.
- **Why unresolved**: The paper only uses two PSEs and doesn't provide a comprehensive analysis of how the choice of PSE affects the performance of SCStory.
- **What evidence would resolve it**: Experiments comparing the performance of SCStory with different PSEs (e.g., other transformer-based models, non-transformer models) on the same datasets and metrics. Analysis of the strengths and weaknesses of different PSEs for story discovery.

## Limitations
- Exact implementation details of prioritized-augmentation function are not fully specified
- Memory replay mechanism's sampling probability calculation lacks specific hyperparameters beyond proportional confidence weighting
- Model's performance on diverse temporal scales and longer time windows is not thoroughly evaluated

## Confidence

- **High Confidence**: The hierarchical embedding approach and contrastive learning framework are well-established techniques that logically address limitations of static embeddings
- **Medium Confidence**: Specific memory replay and augmentation strategies show promise but require more ablation studies to confirm individual contributions
- **Low Confidence**: Generalizability to domains beyond news articles and scenarios with highly overlapping story themes needs further validation

## Next Checks

1. Conduct ablation studies to isolate contribution of each component (hierarchical embedding, memory replay, augmentation) to performance improvements
2. Test SCStory's robustness across different temporal window sizes and sliding intervals to assess adaptability to various streaming scenarios
3. Evaluate the model on datasets with known label noise and overlapping story themes to test real-world applicability