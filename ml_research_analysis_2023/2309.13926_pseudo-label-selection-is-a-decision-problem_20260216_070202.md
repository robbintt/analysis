---
ver: rpa2
title: Pseudo Label Selection is a Decision Problem
arxiv_id: '2309.13926'
source_url: https://arxiv.org/abs/2309.13926
tags:
- data
- selection
- learning
- decision
- international
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of confirmation bias in pseudo-label
  selection (PLS) for semi-supervised learning, where early overfitting in the initial
  model can propagate through the learning process by selecting pseudo-labeled instances
  with overconfident but wrong predictions. The core method introduces a decision-theoretic
  framework called Bayesian PLS (BPLS), which embeds PLS into statistical decision
  theory.
---

# Pseudo Label Selection is a Decision Problem

## Quick Facts
- arXiv ID: 2309.13926
- Source URL: https://arxiv.org/abs/2309.13926
- Reference count: 18
- Primary result: Decision-theoretic framework mitigates confirmation bias in pseudo-label selection by using posterior predictive approximation instead of overconfident model predictions

## Executive Summary
This paper addresses confirmation bias in pseudo-label selection for semi-supervised learning, where early overfitting in initial models propagates through learning by selecting instances with overconfident but incorrect predictions. The authors introduce Bayesian PLS (BPLS), a decision-theoretic framework that treats PLS as a decision problem where data points are actions and parameter space represents states of nature. This allows deriving a Bayes-optimal selection criterion based on an approximation of the posterior predictive. The method effectively mitigates confirmation bias across multiple model classes and enables extensions for robustness to model selection, error accumulation, and covariate shift through multi-objective utility functions.

## Method Summary
The core method formalizes pseudo-label selection as a statistical decision problem, treating unlabeled data points as actions and the parameter space as states of nature. The Bayes-optimal selection criterion is approximated using ℓ(θ̂) - 1/2 log|I(θ̂)|, where ℓ(θ̂) is the likelihood and I(θ̂) is the Fisher information matrix at fitted parameters. This approximation captures uncertainty through the Fisher information term while maintaining computational tractability. The framework extends to multi-objective utility functions that can incorporate multiple modeling assumptions and robustness criteria without requiring unjustified hierarchies between objectives. The method is demonstrated on generalized linear models, non-parametric generalized additive models, and Bayesian neural networks.

## Key Results
- BPLS effectively mitigates confirmation bias when initial models are prone to overfitting
- The method works across diverse model classes including GLM, GAM, and Bayesian neural networks
- Multi-objective utility functions enable robustness to model selection, error accumulation, and covariate shift
- Empirical results show substantial performance gains when incorporating robustness to model choice

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BPLS mitigates confirmation bias by using the posterior predictive approximation instead of model's overconfident predictions
- Mechanism: The criterion ℓ(θ̂) - 1/2 log|I(θ̂)| approximates the joint posterior predictive of pseudo-samples and labeled data, accounting for uncertainty rather than just maximizing likelihood
- Core assumption: The Fisher information matrix captures sufficient information about parameter uncertainty to prevent selection of overconfident wrong predictions
- Break condition: If the Fisher information matrix becomes singular or poorly conditioned, the approximation may fail to properly capture uncertainty

### Mechanism 2
- Claim: The decision-theoretic framework enables robustness through multi-objective utility functions
- Mechanism: By treating PLS as a decision problem, the method can incorporate multiple objectives into a preference system
- Core assumption: The multi-objective utility function can effectively balance competing objectives without requiring unjustified assumptions about their hierarchy
- Break condition: When objectives are truly conflicting and cannot be meaningfully balanced, the preference system may produce suboptimal selections

### Mechanism 3
- Claim: The method is model-agnostic and doesn't require i.i.d. assumptions
- Mechanism: The selection criterion is based on likelihood and Fisher information, which work regardless of data distribution assumptions
- Core assumption: The analytical approximation remains valid across diverse model classes
- Break condition: If the model class has very different parameter spaces or likelihood structures, the approximation may become invalid

## Foundational Learning

- Concept: Statistical decision theory
  - Why needed here: The entire framework is built on formalizing PLS as a decision problem with parameter space as states of nature
  - Quick check question: How does the decision-theoretic view transform the PLS problem from a selection criterion to a utility optimization?

- Concept: Fisher information matrix
  - Why needed here: The selection criterion ℓ(θ̂) - 1/2 log|I(θ̂)| explicitly uses the Fisher information matrix to capture parameter uncertainty
  - Quick check question: What role does the Fisher information matrix play in balancing likelihood maximization with uncertainty awareness?

- Concept: Laplace approximation
  - Why needed here: The analytical approximation of the posterior predictive relies on Laplace's method to avoid expensive sampling
  - Quick check question: Under what conditions might Laplace's method provide a poor approximation of the posterior predictive?

## Architecture Onboarding

- Component map:
  Data preparation -> Model fitting -> Posterior predictive approximation -> Selection engine -> Iterative update loop

- Critical path:
  1. Fit model on labeled data
  2. Compute posterior predictive approximation for each unlabeled instance
  3. Select top instances based on criterion
  4. Add selected instances to labeled data
  5. Repeat until stopping criterion met

- Design tradeoffs:
  - Computational efficiency vs. accuracy of posterior predictive approximation
  - Robustness to model misspecification vs. specificity to particular model assumptions
  - Flexibility of multi-objective framework vs. complexity of preference system design

- Failure signatures:
  - Selection criterion producing NaN values (indicates Fisher information matrix issues)
  - No improvement across iterations (indicates confirmation bias not being mitigated)
  - High variance in selected instances across runs (indicates instability in approximation)

- First 3 experiments:
  1. Implement the basic BPLS with logistic regression on a small simulated dataset to verify the selection criterion produces reasonable rankings
  2. Test the method on a dataset known to cause overfitting in initial models to verify confirmation bias mitigation
  3. Extend to multi-objective utility with two competing model specifications to verify robustness to model selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the BPLS method perform when the initial model has high bias rather than high variance (overfitting)?
- Basis in paper: The paper discusses BPLS mitigating overfitting and confirmation bias, but does not explore scenarios where initial models underfit.
- Why unresolved: The empirical evaluation focuses on overfitting scenarios, leaving the method's behavior under high-bias initial models unexplored.
- What evidence would resolve it: Comparative experiments showing BPLS performance across different initial model quality scenarios (high bias vs. high variance) on benchmark datasets.

### Open Question 2
- Question: What is the theoretical justification for using the Gaussian integral approximation versus Laplace's method for computing the selection criterion?
- Basis in paper: The paper mentions both approximations but doesn't provide theoretical comparison or guidance on when to use which.
- Why unresolved: The authors acknowledge both methods exist but don't analyze their relative merits or failure modes.
- What evidence would resolve it: Theoretical analysis comparing the approximation errors of both methods under different model families and empirical validation on challenging cases.

### Open Question 3
- Question: How does the multi-objective utility framework handle conflicting objectives when the cardinal dimensions are on different scales?
- Basis in paper: The paper introduces multi-objective utility functions but notes that "unjustified assumptions on the hierarchy" should be avoided.
- Why unresolved: The paper mentions this as a consideration but doesn't provide concrete guidance on normalization or trade-off mechanisms.
- What evidence would resolve it: A formal framework for utility normalization that preserves the relative importance of different objectives while avoiding scale dependence.

## Limitations
- Laplace approximation may degrade for highly non-Gaussian likelihoods or high-dimensional parameter spaces
- Fisher information matrix computation can become numerically unstable for ill-conditioned models
- Multi-objective utility framework introduces complexity in preference specification that may require careful tuning

## Confidence
- Theoretical formulation: High
- Empirical performance: Medium
- Model-agnostic claims: Medium
- Robustness claims via multi-objective utility: Medium

## Next Checks
1. Test the method's stability under severe model misspecification where the initial model is fundamentally wrong rather than just overfit
2. Evaluate performance on high-dimensional, non-Gaussian data distributions where Laplace approximation may break down
3. Conduct ablation studies removing the Fisher information term to quantify its specific contribution to confirmation bias mitigation