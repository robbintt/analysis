---
ver: rpa2
title: 'AudioInceptionNeXt: TCL AI LAB Submission to EPIC-SOUND Audio-Based-Interaction-Recognition
  Challenge 2023'
arxiv_id: '2307.07265'
source_url: https://arxiv.org/abs/2307.07265
tags:
- audio
- design
- kernel
- audioinceptionnext
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a simple yet effective single-stream CNN-based
  architecture called AudioInceptionNeXt for the 2023 Epic-Kitchen EPIC-SOUNDS Audio-Based
  Interaction Recognition Challenge. The model operates on the time-frequency log-mel-spectrogram
  of audio samples and employs parallel multi-scale depthwise separable convolutional
  kernels to capture both global and local time-frequency information effectively.
---

# AudioInceptionNeXt: TCL AI LAB Submission to EPIC-SOUND Audio-Based-Interaction-Recognition Challenge 2023

## Quick Facts
- arXiv ID: 2307.07265
- Source URL: https://arxiv.org/abs/2307.07265
- Reference count: 25
- Key outcome: Achieved 55.43% top-1 accuracy on EPIC-SOUND test set, ranking 1st on public leaderboard

## Executive Summary
This paper presents AudioInceptionNeXt, a single-stream CNN architecture for audio-based interaction recognition that achieves state-of-the-art performance on the 2023 Epic-Kitchen EPIC-SOUNDS challenge. The model uses parallel multi-scale depthwise separable convolutional kernels to capture both global frequency patterns and local temporal details from log-mel spectrograms. Despite using only 11.69M parameters (86% fewer than transformer-based alternatives), it achieves 55.43% top-1 accuracy while requiring 95% less computational complexity (2.13 GFLOPs vs 48.67 GFLOPs).

## Method Summary
AudioInceptionNeXt operates on log-mel spectrograms and employs parallel multi-scale depthwise separable convolutional kernels (3×3, 11×11, and 21×21) to capture both global and local time-frequency information. Large kernels capture long-duration activities and global frequency semantics, while small kernels capture short-duration activities and local details. The architecture uses an inverted bottleneck design with channel expansion and squeezing operations, pre-trains on VGG-Sound dataset, then fine-tunes on EPIC-SOUNDS with SGD optimization.

## Key Results
- Achieved 55.43% top-1 accuracy on EPIC-SOUNDS test set, ranking 1st on public leaderboard
- Uses only 11.69M parameters compared to 87.22M for transformer-based models (86% reduction)
- Requires 2.13 GFLOPs vs 48.67 GFLOPs for transformer-based models (95% reduction in computational complexity)

## Why This Works (Mechanism)

### Mechanism 1
Multi-scale depthwise separable kernels enable simultaneous capture of long-term global frequency semantics and short-term local details. The architecture uses large kernels (3×21 and 21×3) to capture global frequency patterns and long-duration activities, while small kernels (3×3) capture local frequency details and short-term activities. This parallel multi-scale approach allows the model to process different temporal and frequency scales simultaneously within a single-stream architecture.

### Mechanism 2
Depthwise separable convolution decomposition reduces computational cost while maintaining or improving performance. Large kernels (e.g., 21×21) are decomposed into 1×x and y×1 kernels, allowing temporal and frequency features to be extracted independently. This decomposition saves computational time and memory footprint while potentially improving feature extraction for spectrograms.

### Mechanism 3
Inverted bottleneck design with channel expansion improves parameter efficiency and performance. The block uses 1×1 convolutions for channel expansion and squeezing, with the hidden 1×1 layer having four times wider channels than input. This design places multi-branch convolutions before channel operations, saving memory and computation.

## Foundational Learning

- Concept: Log-mel spectrogram conversion and its properties
  - Why needed here: The model operates directly on log-mel spectrograms, so understanding their structure (time vs frequency axes, resolution choices) is critical for architecture design
  - Quick check question: What are the typical dimensions of log-mel spectrograms used in this work, and how do they relate to the kernel sizes chosen?

- Concept: Depthwise separable convolution mechanics
  - Why needed here: The core innovation relies on depthwise separable kernels, so understanding how 1×x and y×1 decompositions work is essential for implementation and debugging
  - Quick check question: How does a 21×21 kernel decompose into separable components, and what computational savings does this provide?

- Concept: Multi-branch CNN architectures (Inception-style)
  - Why needed here: The model uses parallel branches with different kernel sizes, similar to Inception networks, so understanding how these branches interact and are combined is important
  - Quick check question: How are feature maps from different kernel scale branches combined in the AudioInceptionNeXt block?

## Architecture Onboarding

- Component map: Log-mel spectrogram → Stem (5×7 conv + 3×3 max pool) → Stage 1-4 (1×1 conv downsampling + AudioInceptionNeXt blocks) → Linear head
- Critical path: Log-mel spectrogram → Stem → Four stages of downsampling and feature extraction → Classification head. The most critical operations are the multi-scale depthwise separable convolutions.
- Design tradeoffs: Multi-scale kernels capture rich features but increase model complexity; separable convolutions reduce computation but may lose cross-dimension interactions; inverted bottleneck saves parameters but requires careful channel sizing.
- Failure signatures: Poor performance on long-duration vs short-duration sounds indicates kernel scale issues; high computational cost suggests inefficient separable decomposition; overfitting on small datasets indicates insufficient regularization.
- First 3 experiments:
  1. Test single-scale vs multi-scale kernels (only 3×3 vs all three scales) to validate the multi-scale benefit
  2. Compare standard convolutions vs depthwise separable convolutions with same kernel sizes to measure computational savings
  3. Vary the channel expansion ratio (2×, 4×, 8×) in the bottleneck to find optimal parameter efficiency

## Open Questions the Paper Calls Out

### Open Question 1
How would the AudioInceptionNeXt architecture perform on audio classification tasks beyond kitchen interactions, such as environmental sound classification or speech recognition? The paper demonstrates strong performance on the EPIC-SOUNDS dataset but does not evaluate the model on other audio classification domains.

### Open Question 2
What is the impact of different separable kernel decomposition strategies (e.g., 1×x followed by y×1 vs x×1 followed by 1×y) on the model's performance and efficiency? The paper mentions using separable kernels with specific decomposition but doesn't explore alternative decomposition strategies.

### Open Question 3
How does the model's performance scale with longer audio sequences beyond the 2.08-second clips used in fine-tuning? The paper uses fixed-length audio clips for training but doesn't investigate the model's behavior with variable or longer audio sequences.

### Open Question 4
What is the optimal balance between small and large kernel sizes in the AudioInceptionNeXt block for different types of audio classification tasks? The paper uses 3, 11, and 21 kernel sizes but doesn't explore how different kernel size combinations might affect performance across different audio domains.

## Limitations
- Evaluation is constrained to a single dataset (EPIC-SOUNDS) without ablation studies comparing the multi-scale design to single-scale alternatives
- Paper lacks analysis of how different kernel scales contribute to performance, making it difficult to assess whether the complexity is justified
- Computational efficiency claims are based on theoretical FLOPs rather than measured inference times

## Confidence
- High confidence: The model architecture is clearly described and the performance metrics on the challenge leaderboard are verifiable
- Medium confidence: The computational efficiency claims based on FLOPs calculations are reasonable but unverified empirically
- Low confidence: The necessity and optimality of the multi-scale design choice without comparative ablation studies

## Next Checks
1. Conduct ablation studies comparing single-scale vs multi-scale kernels (3×3 only, 3×3+11×11, 3×3+21×21, all three) to quantify the contribution of each scale
2. Measure actual inference time and memory usage on representative hardware to validate theoretical GFLOP reductions
3. Test the model architecture on additional audio classification datasets (ESC-50, UrbanSound8K) to evaluate generalization beyond the specific challenge task