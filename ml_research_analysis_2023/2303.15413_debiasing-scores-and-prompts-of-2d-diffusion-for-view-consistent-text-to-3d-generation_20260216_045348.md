---
ver: rpa2
title: Debiasing Scores and Prompts of 2D Diffusion for View-consistent Text-to-3D
  Generation
arxiv_id: '2303.15413'
source_url: https://arxiv.org/abs/2303.15413
tags:
- view
- prompt
- diffusion
- score
- debiasing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the Janus problem in score-distilled text-to-3D
  generation, where 3D objects appear inconsistent from different viewpoints. The
  authors identify that the root cause is the inherent bias in 2D diffusion models.
---

# Debiasing Scores and Prompts of 2D Diffusion for View-consistent Text-to-3D Generation

## Quick Facts
- **arXiv ID**: 2303.15413
- **Source URL**: https://arxiv.org/abs/2303.15413
- **Reference count**: 39
- **Key outcome**: LPIPS scores of 0.1940 (AlexNet) and 0.1445 (VGG), outperforming baseline (0.1526 and 0.1450 respectively)

## Executive Summary
This paper addresses the Janus problem in score-distilled text-to-3D generation, where 3D objects appear inconsistent from different viewpoints. The authors identify that the root cause is the inherent bias in 2D diffusion models. They propose two debiasing methods: (1) score debiasing, which dynamically truncates the score estimates from diffusion models during optimization to reduce artifacts, and (2) prompt debiasing, which identifies and removes conflicting words between user prompts and view prompts using a language model and adjusts the view prompt regions to better match object-space poses. Experiments show that their methods significantly reduce artifacts and improve view consistency.

## Method Summary
The method builds on score distillation sampling (SDS) to optimize Neural Radiance Fields (NeRF) using 2D diffusion model scores. The authors introduce two debiasing approaches: score debiasing dynamically truncates the score estimates from diffusion models during optimization, and prompt debiasing uses a language model to identify and remove conflicting words between user prompts and view prompts. The debiased gradients are then used to update NeRF parameters, resulting in more view-consistent 3D objects.

## Key Results
- Achieves LPIPS scores of 0.1940 (AlexNet) and 0.1445 (VGG), outperforming baseline
- Significantly reduces view inconsistency artifacts in generated 3D objects
- Provides a good trade-off between faithfulness to 2D diffusion models and 3D consistency with little computational overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The 2D diffusion model's unconditional score introduces a bias toward specific viewpoints, causing the Janus problem.
- Mechanism: The unconditional score term ∇zθ log p2D(zθ) pulls the optimization toward viewpoints that are overrepresented in the 2D training data, resulting in artifacts like extra faces or inconsistent geometry across views.
- Core assumption: The 2D image distribution is skewed toward certain canonical views, and this skew is preserved in the diffusion model's unconditional score.
- Evidence anchors:
  - [abstract]: "the embedded bias of 2D diffusion models" and "score debiasing... cuts off the score estimated by 2D diffusion models to reduce artifacts"
  - [section]: "The first gradient term, reflecting the unconditional score... contains a bias that affects images viewed closely from specific viewpoints"
- Break condition: If the 2D training data is truly uniform across viewpoints, or if the unconditional score is neutralized during training, the mechanism fails.

### Mechanism 2
- Claim: Prompt contradiction between user prompt and view prompt causes the diffusion model to ignore view-specific guidance.
- Mechanism: When the user prompt contains words that contradict the view prompt, the pointwise conditional mutual information (PCMI) C becomes near zero, causing the pose-prompt gradient to vanish and producing artifacts inconsistent with the intended viewpoint.
- Core assumption: The diffusion model's text-conditioning respects PCMI, and low PCMI values suppress the pose-prompt gradient.
- Evidence anchors:
  - [abstract]: "identifies conflicting words between user prompts and view prompts using a language model"
  - [section]: "If a viewpoint ξ and a user prompt ω are contradictory... then C approximates to 0 for every zθ"
- Break condition: If the language model fails to detect contradictions, or if the diffusion model's conditioning ignores PCMI, the mechanism fails.

### Mechanism 3
- Claim: Gradual truncation of the score during optimization balances faithfulness to 2D models and 3D consistency.
- Mechanism: Early in optimization, large score magnitudes cause overfitting to 2D biases. By linearly increasing the truncation threshold from 2.0 to 8.0, the method first focuses on coarse 3D structure and later allows finer details, improving the trade-off between realism and consistency.
- Core assumption: The optimization naturally benefits from coarse-to-fine refinement, and truncation can be safely increased without losing essential details.
- Evidence anchors:
  - [abstract]: "gradually increasing the truncation value... throughout the optimization process"
  - [section]: "we linearly increase the truncation value throughout the optimization... This is a coarse-to-fine strategy"
- Break condition: If the truncation schedule is too aggressive or too conservative, it may either preserve artifacts or lose detail fidelity.

## Foundational Learning

- **Concept**: Score distillation sampling (SDS) and its gradient formulation
  - Why needed here: The paper builds on SDS to optimize NeRF using 2D diffusion model scores. Understanding the gradient derivation and its components is essential to grasp why bias occurs and how debiasing works.
  - Quick check question: What are the two main gradient components in the score distillation formulation, and how do they contribute to the Janus problem?

- **Concept**: Pointwise mutual information (PMI) and its role in prompt consistency
  - Why needed here: PMI is used to detect contradictions between user and view prompts. Knowing how PMI is computed and interpreted is key to understanding prompt debiasing.
  - Quick check question: How does PMI between a view prompt and a user prompt word indicate a contradiction, and what action is taken when PMI is low?

- **Concept**: Neural Radiance Fields (NeRF) and differentiable rendering
  - Why needed here: The 3D object is represented as a NeRF, and the optimization adjusts its parameters based on rendered 2D images. Understanding NeRF's structure and how gradients flow from 2D to 3D is crucial.
  - Quick check question: In the context of text-to-3D generation, what is the role of the ∂zθ/∂θ term in the gradient chain rule?

## Architecture Onboarding

- **Component map**: 2D diffusion model -> Renderer -> Score debiasing module -> Prompt debiasing module -> Backpropagate to NeRF
- **Critical path**:
  1. Sample viewpoint ξ and render image zθ from NeRF
  2. Compute 2D score estimate from diffusion model
  3. Apply score debiasing (clipping)
  4. Compute PMI between user and view prompts
  5. Apply prompt debiasing (word removal, range adjustment)
  6. Backpropagate debiased gradients to update NeRF parameters
- **Design tradeoffs**:
  - Static vs. dynamic score truncation: Static is simpler but may be too harsh early or too lenient late; dynamic adapts but adds scheduling complexity.
  - Strict vs. lenient PMI threshold: Strict removes more words (less contradiction) but may lose prompt detail; lenient preserves more detail but risks artifacts.
  - View prompt augmentation granularity: Finer granularity improves consistency but increases hyperparameter tuning.
- **Failure signatures**:
  - Persistent Janus artifacts -> score debiasing threshold too low or PMI threshold too high
  - Loss of prompt fidelity -> PMI threshold too strict or truncation too aggressive
  - Inconsistent geometry across views -> view prompt ranges misaligned with object space
- **First 3 experiments**:
  1. Baseline vs. score debiasing only: Verify that clipping reduces artifacts without harming detail.
  2. Baseline vs. prompt debiasing only: Confirm that removing contradictions improves view consistency.
  3. Combined debiasing: Test the full pipeline and measure A-LPIPS improvement over baseline.

## Open Questions the Paper Calls Out
- **Question**: How does the proposed score debiasing method perform compared to other gradient clipping techniques like the one used in Stable-DreamFusion?
- **Question**: What is the impact of different language models on the effectiveness of prompt debiasing?
- **Question**: How does the proposed method scale to more complex prompts involving multiple objects or intricate relationships between objects?
- **Question**: What is the computational overhead introduced by the proposed debiasing methods compared to the baseline?

## Limitations
- The paper does not provide detailed implementation specifications for key components like PMI computation and view prompt augmentation
- The effectiveness of the debiasing methods may depend heavily on hyperparameter tuning
- The paper lacks ablation studies isolating the effects of each debiasing component

## Confidence
- **Confidence in proposed mechanisms**: Medium
  - The theoretical framework is sound and experimental results show improvements
  - However, the paper relies heavily on empirical tuning of hyperparameters without clear selection guidelines
  - Absence of ablation studies makes it difficult to definitively attribute performance gains to specific mechanisms
- **Confidence in reproducibility**: Low
  - Key implementation details for PMI computation and view prompt augmentation are missing
  - These details are crucial for faithful reproduction and validation

## Next Checks
1. **Ablation Study**: Run experiments isolating score debiasing and prompt debiasing to quantify their individual contributions to the observed performance improvements.
2. **Model Generalization**: Test the debiasing methods with different 2D diffusion models (e.g., Stable Diffusion v1.5 vs. v2.1) to verify that the bias mechanisms are consistent across model versions.
3. **Prompt Robustness**: Evaluate the system with a diverse set of user prompts and view prompts, including edge cases like highly contradictory or ambiguous descriptions, to assess the robustness of the PMI-based contradiction detection.