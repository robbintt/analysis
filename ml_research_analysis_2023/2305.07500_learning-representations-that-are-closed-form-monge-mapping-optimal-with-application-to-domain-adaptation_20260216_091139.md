---
ver: rpa2
title: Learning representations that are closed-form Monge mapping optimal with application
  to domain adaptation
arxiv_id: '2305.07500'
source_url: https://arxiv.org/abs/2305.07500
tags:
- learning
- optimal
- mapping
- monge
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel approach to domain adaptation using
  representation learning and optimal transport. It learns an embedding space where
  two domains become alignable via a simple affine transformation that can be computed
  in closed form.
---

# Learning representations that are closed-form Monge mapping optimal with application to domain adaptation

## Quick Facts
- arXiv ID: 2305.07500
- Source URL: https://arxiv.org/abs/2305.07500
- Reference count: 40
- One-line primary result: Proposes a novel domain adaptation method using representation learning and optimal transport that achieves competitive performance with significantly reduced computational complexity

## Executive Summary
This paper introduces a novel approach to domain adaptation that learns embedding spaces where source and target domains become alignable via closed-form affine transformations. The method combines autoencoders with optimal transport theory, optimizing for both data reconstruction and alignability between domains. Unlike traditional invariant representation learning, this framework allows for affine transformations between domains, providing more flexibility while still enabling efficient alignment through closed-form Monge mappings. The approach is evaluated on standard domain adaptation benchmarks and demonstrates competitive accuracy with substantially lower computational cost compared to traditional optimal transport methods.

## Method Summary
The proposed method learns linearly alignable representations using a coupled autoencoder framework. Two separate encoders map source and target data into embedding spaces ZS and ZT, while decoders reconstruct from these embeddings. The optimization objective combines a reconstruction loss (ensuring meaningful representations) with an alignability loss (ensuring the embedded distributions can be aligned via closed-form affine Monge mappings). The linear Monge mapping is computed in the embedding space using Flamary et al. (2019)'s closed-form solution when distributions are linked through affine transformations. After learning the embeddings, a simple classifier trained on the aligned source data is applied to the target domain. This approach generalizes invariant representation learning by allowing arbitrary affine transformations rather than requiring exact invariance.

## Key Results
- Achieves competitive classification accuracy on Office/Caltech10 dataset compared to state-of-the-art OT-based methods
- Demonstrates significant computational complexity reduction: O(nk^2 + k^3) vs O(nd^2 + d^3) for traditional OT methods
- Shows scalability advantages with linear growth in runtime as sample size increases, compared to quadratic growth for Kantorovich OT
- Outperforms or matches several OT-based baselines in both homogeneous and heterogeneous domain adaptation settings

## Why This Works (Mechanism)

### Mechanism 1
The proposed method achieves closed-form Monge mapping by learning an embedding space where the source and target distributions become linked through an affine transformation. The autoencoder framework learns feature transformations gs and gt that embed the data into low-dimensional spaces ZS and ZT. In this embedding space, the covariance matrices ΣS and ΣT of the two distributions become aligned, enabling the use of Flamary et al. (2019)'s closed-form solution for the Monge mapping when distributions are linked through an affine transformation.

Core assumption: Real-world data distributions can be embedded into a space where they approximately satisfy the conditions for closed-form affine Monge mapping (positive definite covariance matrices, approximate linear relationship).

Evidence anchors:
- [abstract] "learns an embedding space where the samples of the two input measures become alignable in it with a simple affine mapping that can be calculated efficiently in closed-form"
- [section 3.1] "we propose to optimize the following objective function: min_{gs,gt,decS,decT} LRec.(SX,TX) + λLLA(Sgs_X, Tgt_X)" and "the Wasserstein distance between the push-forward of SX with T[SX,TX]_a_{ff} and TX"
- [corpus] Weak - related works discuss Monge mappings but don't provide direct evidence for this specific embedding approach

Break condition: If the embedding space cannot align the covariance structures sufficiently, the closed-form solution becomes invalid and computational advantages disappear.

### Mechanism 2
Learning linearly alignable representations generalizes invariant representation learning by allowing affine transformations rather than requiring exact invariance. The framework relaxes the constraint from invariant feature transformation (A = Id, b = 0, gs = gt = g) to allow arbitrary invertible affine transformations. This provides more degrees of freedom during optimization, making it easier to find representations that align the domains while preserving discriminative information.

Core assumption: The solution space for linearly alignable representations is larger and more tractable than for invariant representations, enabling better optimization outcomes.

Evidence anchors:
- [section 3.1] "This is a generalization of the popular invariant representation learning Zhao et al. (2019) framework where the goal is find an invariant representation for two domains. We evaluate this framework in the DA setting."
- [section 3.1] "One should note that this definition generalizes the invariant feature transformation learning, as the latter is a special case with A = Id, b = 0 and gs = gt = g"
- [corpus] Moderate - related work "Beyond Mapping: Domain-Invariant Representations via Spectral Embedding of Optimal Transport Plans" suggests spectral methods for similar goals

Break condition: If the additional degrees of freedom lead to overfitting or if the affine alignment doesn't preserve task-relevant structure.

### Mechanism 3
The computational complexity reduction comes from solving OT in low-dimensional embedding space rather than high-dimensional input space. Traditional OT methods have O(nd^2 + d^3) complexity for d-dimensional data. By embedding into k-dimensional space where k << d, the complexity becomes O(nk^2 + k^3), providing significant speedup while maintaining performance through careful representation learning.

Core assumption: The embedding space dimensionality k can be chosen small enough for computational gains while large enough to preserve discriminative information.

Evidence anchors:
- [section 3.1] "Given two samples of size n from Rd, the latter is known to have a sample complexity of O(n^{-1/d}), while the former is O(n^{-1/2})" and "the computational complexity of calculating the linear Monge map is O(nd^2 + d^3)"
- [section 4.4] "Our method scales well compared to the Kantorovich OT (UDA) and Gromov-Wassertein OT (HDA) solutions" with concrete timing comparisons
- [corpus] Strong - timing results in Table 5 show LaOT scales linearly with sample size while Kantorovich OT scales quadratically

Break condition: If k is too small to capture necessary structure, performance degrades despite computational gains.

## Foundational Learning

- Concept: Optimal Transport (OT) theory and Wasserstein distances
  - Why needed here: The entire method builds on OT theory, using Wasserstein distances as the alignment criterion and leveraging closed-form solutions for specific distribution relationships
  - Quick check question: What is the difference between the Monge and Kantorovich formulations of OT, and when does each have computational advantages?

- Concept: Domain Adaptation theory and learning bounds
  - Why needed here: The method's theoretical guarantees rely on domain adaptation bounds, particularly the simplified bound when using closed-form Monge mappings in the embedding space
  - Quick check question: How does the learning bound in Theorem 3.3 differ from traditional domain adaptation bounds, and what assumptions enable this simplification?

- Concept: Autoencoder architecture and representation learning
  - Why needed here: The method uses a coupled autoencoder framework to learn the embedding spaces, requiring understanding of reconstruction losses and how encoders/decoders work together
  - Quick check question: What is the role of the reconstruction term LRec. in the objective function, and how does it interact with the alignment term?

## Architecture Onboarding

- Component map: Data → Encoder (gs or gt) → Embedding Space → Linear Monge Mapping → Alignment Evaluation → Backpropagation → Parameter Update. The system consists of two encoders (gs, gt) that map source and target data to embedding spaces ZS and ZT, two decoders (decs, dect) that reconstruct from embeddings, and a linear Monge mapping module that aligns the embedded distributions.

- Critical path: Data → Encoder (gs or gt) → Embedding Space → Linear Monge Mapping → Alignment Evaluation → Backpropagation → Parameter Update. The critical performance bottleneck is typically the embedding space dimensionality choice and the balance between reconstruction and alignment objectives.

- Design tradeoffs: Lower embedding dimensionality reduces computation but may lose discriminative information; higher λ values improve alignment but may distort learned representations; separate encoders allow domain-specific features but require careful coordination for alignment.

- Failure signatures: Poor alignment manifests as high Wasserstein distance in embedding space despite training; reconstruction failure shows as high reconstruction loss; overfitting shows as performance gap between source and target domains widening during training.

- First 3 experiments:
  1. Verify the closed-form Monge mapping works on Gaussian distributions with known affine relationships
  2. Test the full pipeline on a simple synthetic dataset where ground truth affine transformation is known
  3. Evaluate sensitivity to embedding dimensionality k on a small benchmark dataset before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed method be extended to handle multi-source domain adaptation scenarios where there are multiple source domains with different distributions?
- Basis in paper: [inferred] The paper focuses on single-source domain adaptation, but mentions that the method could potentially be used in other ML problems where Monge mapping is used.
- Why unresolved: The paper does not explore the extension to multi-source scenarios, and it's unclear how the linear alignability framework would generalize to multiple sources.
- What evidence would resolve it: Experiments showing the method's performance on multi-source domain adaptation tasks, and theoretical analysis of how the learning bounds would extend to this setting.

### Open Question 2
- Question: Can the learned embedding space be interpretable, and if so, what properties of the original data are preserved or emphasized in this space?
- Basis in paper: [inferred] The paper mentions that the method learns an embedding space where the linear Monge mapping becomes optimal, but does not discuss the interpretability of this space.
- Why unresolved: The paper does not provide any analysis of the properties of the learned embedding space or how it relates to the original data structure.
- What evidence would resolve it: Analysis of the embedding space's properties, such as examining what features or data characteristics are emphasized or preserved, and how this relates to the original data.

### Open Question 3
- Question: How does the proposed method perform on very high-dimensional data, such as raw images, compared to methods that use deep neural networks for feature extraction?
- Basis in paper: [explicit] The paper uses pre-trained CNN features for evaluation, but does not test on raw high-dimensional data.
- Why unresolved: The paper's evaluation is limited to relatively low-dimensional features, and it's unclear how the method would scale to or perform on raw high-dimensional data.
- What evidence would resolve it: Experiments comparing the method's performance on raw high-dimensional data (e.g., raw images) to deep learning-based methods that learn features from scratch.

### Open Question 4
- Question: What is the theoretical relationship between the linear alignability framework and other representation learning approaches, such as invariant feature learning or domain adversarial training?
- Basis in paper: [explicit] The paper mentions that the linear alignability framework generalizes invariant feature learning, but does not explore the theoretical connections to other approaches.
- Why unresolved: The paper does not provide a comprehensive theoretical analysis of how the linear alignability framework relates to other representation learning methods.
- What evidence would resolve it: Theoretical analysis establishing the relationships between linear alignability and other representation learning frameworks, potentially showing conditions under which they are equivalent or complementary.

### Open Question 5
- Question: How does the choice of the regularization parameter λ affect the trade-off between data fidelity and alignability, and is there an optimal way to select this parameter?
- Basis in paper: [explicit] The paper mentions that λ controls the degree of linear alignability promotion, and provides some analysis of its effect on performance.
- Why unresolved: The paper does not provide a systematic analysis of how λ affects the trade-off or guidance on optimal selection strategies.
- What evidence would resolve it: Detailed analysis of the effect of λ on performance across different tasks and datasets, potentially leading to guidelines or automated methods for optimal λ selection.

## Limitations
- Theoretical guarantees depend on assumption that real-world distributions can be embedded to satisfy closed-form Monge mapping conditions, not rigorously proven for experimental datasets
- Computational complexity claims assume appropriate embedding dimension k is chosen, but sensitivity analysis to this parameter is not thoroughly explored
- Performance on truly large-scale datasets beyond Office/Caltech10 remains to be validated

## Confidence

- **High Confidence**: The computational complexity reduction mechanism (Mechanism 3) is well-supported by timing results in Table 5, showing clear O(nk^2) vs O(nd^2) scaling.
- **Medium Confidence**: The closed-form Monge mapping in learned embedding spaces (Mechanism 1) has theoretical support but limited empirical validation beyond synthetic examples and small-scale benchmarks.
- **Medium Confidence**: The generalization benefit of linearly alignable representations (Mechanism 2) is conceptually sound but the empirical evidence showing it outperforms invariant representation learning is limited to specific datasets.

## Next Checks

1. **Sensitivity Analysis**: Systematically evaluate performance and computational cost across a range of embedding dimensions k on multiple datasets to identify optimal trade-offs and verify claimed complexity advantages hold across settings.

2. **Ablation Study**: Compare the proposed method against variants with fixed affine transformations (A=Id, b=0) to quantify the actual benefit of learning arbitrary affine mappings versus enforcing invariance.

3. **Scalability Test**: Apply the method to larger domain adaptation benchmarks (e.g., VisDA, DomainNet) to verify computational advantages scale to real-world problem sizes and that performance doesn't degrade with increased domain discrepancy.