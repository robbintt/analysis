---
ver: rpa2
title: Leveraging Negative Signals with Self-Attention for Sequential Music Recommendation
arxiv_id: '2309.11623'
source_url: https://arxiv.org/abs/2309.11623
tags:
- recommendation
- music
- u1d456
- sequential
- track
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors investigate the use of transformer-based self-attentive
  architectures for sequential music recommendation, focusing on leveraging implicit
  session-level information and incorporating negative user feedback through a contrastive
  learning task. They find that incorporating negative feedback results in consistent
  performance gains over baseline architectures that ignore negative user feedback.
---

# Leveraging Negative Signals with Self-Attention for Sequential Music Recommendation

## Quick Facts
- arXiv ID: 2309.11623
- Source URL: https://arxiv.org/abs/2309.11623
- Reference count: 31
- Primary result: Incorporating negative feedback through contrastive learning improves sequential music recommendation performance, achieving up to 20.26% improvement in hit rate at 1.

## Executive Summary
This paper investigates transformer-based self-attentive architectures for sequential music recommendation, focusing on leveraging implicit session-level information and incorporating negative user feedback through a contrastive learning task. The authors demonstrate that incorporating negative feedback results in consistent performance gains over baseline architectures that ignore negative user feedback. They achieve significant improvements across multiple metrics, with up to 20.26% improvement in hit rate at 1, 5.93% at 5, 4.90% at 10, and 3.77% at 20 for their unidirectional model.

## Method Summary
The proposed method uses a transformer encoder with unidirectional and bidirectional attention mechanisms to learn contextual representations of tracks within music streaming sessions. The model incorporates negative user feedback through a contrastive learning task using InfoNCE loss, where skipped tracks serve as negative examples. The architecture employs sampled softmax for training stability due to the large output space (approximately 1 million tracks), and includes positional embeddings to capture sequential order. The model is trained on the Music Streaming Sessions Dataset (MSSD) containing 160M user sessions with 10-20 consecutively listened songs, using skip labels with strength 1-3.

## Key Results
- Incorporating negative feedback through contrastive learning achieves up to 20.26% improvement in hit rate at 1
- Unidirectional transformers consistently outperform bidirectional models across all top-K values
- Using track embeddings rather than session context as the contrastive anchor yields slightly better performance
- Performance improvements are consistent across multiple metrics (HR@1, HR@5, HR@10, HR@20)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating negative feedback through a contrastive loss term improves ranking accuracy by explicitly penalizing unwanted tracks.
- Mechanism: The model learns embeddings where positive tracks are pushed closer to the current track while negative tracks are pushed farther away in the embedding space, directly influencing ranking via inner product similarity.
- Core assumption: Skipped tracks are reliable indicators of negative preference, and the embedding space can effectively encode both positive and negative relationships.
- Evidence anchors:
  - [abstract] "incorporating negative user feedback through a contrastive learning task...consistent performance gains"
  - [section 3.4] "This maximizes the cosine similarity between the embedding of track ... and next-positive-sample while minimizing the similarity between ... and all negative samples"
- Break condition: If the distribution of skips is not meaningful (e.g., uniform random skips) or if the contrastive term is too strong relative to the primary task loss.

### Mechanism 2
- Claim: Unidirectional transformers outperform bidirectional models in this task due to autoregressive training matching the inference goal of next-item prediction.
- Mechanism: By training with causal masking, the model learns to predict the next item given only past context, which aligns with the inference setting where future items are unknown.
- Core assumption: The autoregressive assumption holds for sequential music consumption and that bidirectional context does not add meaningful signal for next-item prediction.
- Evidence anchors:
  - [section 4.2] "The unidirectional models consistently outperform the bidirectional models, with a waning performance gap as the top-K for the hit rate increases"
  - [section 3.3] Describes unidirectional training as next-item prediction with causal attention masks
- Break condition: If future context is actually predictive of the next item, or if the sequence length is much longer than tested.

### Mechanism 3
- Claim: Using track embeddings rather than session context as the contrastive anchor yields slightly better performance, suggesting global track relationships are more stable than immediate context.
- Mechanism: When the context vector is the track embedding itself, the contrastive loss focuses on the intrinsic relationship between tracks independent of the specific session, potentially learning more generalizable negative associations.
- Core assumption: Track-level embeddings capture sufficient information for contrastive learning without session-specific context, and the session context may introduce noise.
- Evidence anchors:
  - [section 4.2] "The slight performance improvement when using the track embeddings as the contextual vector for the contrastive task may imply that while immediate session-level contextual information is useful in learning from negative feedback, reducing this emphasis may provide a slightly stronger signal"
  - [section 3.4] Defines two options for the context vector: final hidden state or track embedding
- Break condition: If session context is actually crucial for distinguishing negative feedback patterns in certain music consumption scenarios.

## Foundational Learning

- Concept: Transformer self-attention mechanism
  - Why needed here: The model uses multi-head self-attention layers to learn contextual representations of tracks within a session
  - Quick check question: What is the difference between causal and bidirectional attention masks in transformer architectures?

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: The negative feedback incorporation uses noise contrastive estimation to learn discriminative embeddings
  - Quick check question: How does the InfoNCE loss formulation maximize similarity between positive pairs while minimizing similarity with negative samples?

- Concept: Sampled softmax for large output spaces
  - Why needed here: With approximately 1 million tracks, computing full softmax is computationally prohibitive
  - Quick check question: Why is sampled softmax used instead of full softmax in this recommendation setting, and how does it affect training?

## Architecture Onboarding

- Component map: Track embeddings (lookup table) → Positional embeddings → Transformer encoder (2 layers, 8 heads, 128 dim) → Prediction layer (FC + GELU) → Sampled softmax → Contrastive loss term
- Critical path: Input sequence → Embedding lookup → Positional addition → Encoder forward pass → Final hidden state → Inner product with embedding table → Sampled softmax + contrastive loss → Backpropagation
- Design tradeoffs: Unidirectional vs bidirectional attention (simplicity vs potential context), track embedding vs hidden state as contrastive anchor (generalization vs context), sampled softmax vs full softmax (efficiency vs accuracy)
- Failure signatures: Poor convergence on contrastive task (loss not decreasing), hit rate improvements only at very high K values (weak negative signal learning), bidirectional model outperforming unidirectional (autoregressive assumption invalid)
- First 3 experiments:
  1. Train unidirectional baseline without contrastive loss to establish performance baseline
  2. Add contrastive loss with track embeddings as anchor to measure improvement
  3. Switch to hidden state as anchor to compare contextual vs non-contextual contrastive learning effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed contrastive learning task vary across different session types (e.g., playlist, auto-generated, user-curated)?
- Basis in paper: [inferred] The authors mention that an analysis of performance on different session types and streaming behaviors would provide better insight into the performance in different listening contexts, but this analysis was not conducted in the current study.
- Why unresolved: The current study did not investigate the performance of the proposed method across different session types.
- What evidence would resolve it: Conducting experiments to evaluate the performance of the proposed method on different session types and comparing the results would provide evidence to answer this question.

### Open Question 2
- Question: How does the inclusion of long-term user profiles affect the performance of the proposed method?
- Basis in paper: [inferred] The authors mention that multiple avenues for future work arise, namely the inclusion of long-term user profiles for better modeling of long-term and changing user-taste, but this was not explored in the current study.
- Why unresolved: The current study did not investigate the impact of including long-term user profiles on the performance of the proposed method.
- What evidence would resolve it: Conducting experiments to evaluate the performance of the proposed method with and without long-term user profiles and comparing the results would provide evidence to answer this question.

### Open Question 3
- Question: How does the proposed method perform when incorporating contextual and content information into the embeddings?
- Basis in paper: [inferred] The authors mention that contextual and content information can be injected into the embeddings to learn more powerful contextual representations, but this was not explored in the current study.
- Why unresolved: The current study did not investigate the impact of incorporating contextual and content information into the embeddings on the performance of the proposed method.
- What evidence would resolve it: Conducting experiments to evaluate the performance of the proposed method with and without incorporating contextual and content information into the embeddings and comparing the results would provide evidence to answer this question.

## Limitations
- Results are based on a single proprietary dataset (MSSD) with specific skip strength definitions that may not transfer to other domains
- Unidirectional model's superiority was only tested at relatively short sequence lengths (10-20 items)
- Sampled softmax approach with 1000 negative samples may introduce approximation errors affecting reported performance metrics

## Confidence

- **High Confidence**: The core claim that incorporating negative feedback through contrastive learning improves recommendation performance is well-supported by consistent results across multiple metrics and model variants.
- **Medium Confidence**: The finding that unidirectional models outperform bidirectional models is supported by the experimental results, but the limited sequence length range and lack of ablation studies on attention mask configurations reduce confidence in the general mechanism.
- **Medium Confidence**: The observation that track embeddings work slightly better than hidden states for contrastive anchors is based on small performance differences that may not be statistically significant or practically meaningful.

## Next Checks

1. **Statistical Significance Testing**: Perform paired t-tests or bootstrap confidence intervals on the hit rate improvements to verify that the 20.26% improvement at K=1 and other gains are statistically significant rather than due to random variation.

2. **Cross-Dataset Validation**: Evaluate the proposed approach on publicly available sequential recommendation datasets (e.g., Amazon, MovieLens) with different negative signal definitions to assess domain transferability and robustness.

3. **Sequence Length Sensitivity**: Test the unidirectional vs bidirectional performance comparison at longer sequence lengths (50+ items) to determine if the autoregressive assumption holds beyond the tested range and identify potential break conditions.