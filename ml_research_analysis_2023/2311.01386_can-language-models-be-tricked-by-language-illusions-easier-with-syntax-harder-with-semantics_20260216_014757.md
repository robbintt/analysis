---
ver: rpa2
title: Can Language Models Be Tricked by Language Illusions? Easier with Syntax, Harder
  with Semantics
arxiv_id: '2311.01386'
source_url: https://arxiv.org/abs/2311.01386
tags:
- illusion
- language
- have
- more
- than
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates whether language models can mimic human
  behavior when processing "language illusions" - sentences that are grammatically
  incorrect or semantically odd but still judged as acceptable by humans. The authors
  test three types of illusions: comparative, depth-charge, and NPI illusions, using
  four language models (BERT, RoBERTa, GPT-2, and GPT-3) and two metrics (perplexity
  and surprisal).'
---

# Can Language Models Be Tricked by Language Illusions? Easier with Syntax, Harder with Semantics

## Quick Facts
- arXiv ID: 2311.01386
- Source URL: https://arxiv.org/abs/2311.01386
- Authors: 
- Reference count: 28
- Key outcome: Language models perform better at distinguishing acceptable from unacceptable sentences but struggle with illusion effects, particularly with semantic illusions requiring sophisticated understanding.

## Executive Summary
This study investigates whether language models (LMs) exhibit human-like behavior when processing "language illusions" - sentences that are grammatically incorrect or semantically odd but judged as acceptable by humans. Testing four LMs (BERT, RoBERTa, GPT-2, GPT-3) on three illusion types (comparative, depth-charge, NPI), the authors find that while LMs can generally distinguish acceptable from unacceptable sentences, they struggle to replicate the illusion effect where humans find ungrammatical sentences surprisingly acceptable. Notably, LMs are more susceptible to syntactic illusions (NPI) than semantic ones, suggesting limitations in their ability to serve as cognitive models of human language processing.

## Method Summary
The study evaluates four language models using two metrics - whole-sentence perplexity and word-level surprisal - on minimal pairs for three illusion types. Using mixed-effects linear regression models, the authors test whether LMs can: (1) distinguish acceptable from unacceptable sentences, (2) exhibit illusion effects matching human judgments, and (3) show sensitivity to linguistic manipulations that affect human judgments. The experimental design includes 32 items per illusion type with acceptable, unacceptable, and illusion conditions adapted from prior psycholinguistic studies.

## Key Results
- No single model or metric consistently aligns with human judgments across all illusions
- LMs perform better at distinguishing acceptable from unacceptable sentences but struggle with the illusion effect
- LMs are more likely to be "tricked" by NPI illusions (syntactic) than comparative and depth-charge illusions (semantic)
- Perplexity and surprisal metrics yield different results, suggesting they capture different aspects of language processing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LMs fail to align with human acceptability judgments on language illusions due to differences in how they process syntactic vs semantic dependencies.
- Mechanism: LMs use probabilistic pattern matching based on training data frequencies, while humans use integrated cognitive resources including world knowledge and pragmatic inference. For NPIs, syntactic structure is prioritized, but for comparative and depth-charge illusions, semantic integration fails.
- Core assumption: The training data contains sufficient examples of grammatical dependencies but insufficient nuanced semantic context for complex illusions.
- Evidence anchors:
  - [abstract] "probabilities represented by LMs were more likely to align with human judgments of being 'tricked' by the NPI illusion which examines a structural dependency, compared to the comparative and the depth-charge illusions which require sophisticated semantic understanding."
  - [section] "We find that LMs do not pattern like humans in all cases" (comparing to garden path sentences where LMs also fall short of capturing the magnitude of the processing cost).
  - [corpus] Weak - related papers focus on vision-language models and optical illusions, not directly comparable to syntactic/semantic processing in text-only LMs.

### Mechanism 2
- Claim: The discrepancy between perplexity and surprisal metrics reveals different aspects of LM processing that don't fully capture human acceptability judgments.
- Mechanism: Perplexity measures overall sentence-level probability, while surprisal focuses on specific word positions. Human judgments integrate multiple levels of linguistic information simultaneously, which neither metric fully captures.
- Core assumption: The relationship between these metrics and human acceptability is not linear or consistent across different linguistic phenomena.
- Evidence anchors:
  - [abstract] "only in the case of surprisal did we see an illusion effect where the unacceptable sentences...received significantly higher surprisals than the illusion sentence"
  - [section] "It is surprising to see that the two widely used probability-based metrics can generate different results for a given hypothesis and a given language model"
  - [corpus] Weak - corpus neighbors don't address the perplexity-surprisal discrepancy in language model evaluation.

### Mechanism 3
- Claim: LMs show better performance on syntactic illusions than semantic illusions because syntactic dependencies are more explicit in training data and model architecture.
- Mechanism: Transformer-based LMs have attention mechanisms that explicitly model hierarchical relationships, making them better at capturing syntactic dependencies like NPI licensing. Semantic reasoning requires integration of world knowledge not explicitly represented in the model.
- Core assumption: The model architecture implicitly captures syntactic structure better than semantic meaning.
- Evidence anchors:
  - [abstract] "LMs are more likely to be 'tricked' by the NPI illusion, which involves syntactic structure, than by the other two illusions that require more sophisticated semantic understanding"
  - [section] "Since existing research already shows that language models are quite limited in processing negation...we speculate that LMs might encounter difficulty in the more complicated case of depth-charge sentences"
  - [corpus] Weak - corpus neighbors don't provide evidence about syntactic vs semantic processing differences in LMs.

## Foundational Learning

- Concept: Minimal pair construction in linguistic evaluation
  - Why needed here: The study relies on carefully constructed minimal pairs to isolate specific linguistic phenomena and test LM sensitivity to subtle differences
  - Quick check question: What is the purpose of using minimal pairs in linguistic evaluation, and how do they help isolate specific linguistic phenomena?

- Concept: Negative polarity items (NPIs) and their licensing conditions
  - Why needed here: NPI illusions form a key part of the study, and understanding NPI licensing is crucial for interpreting the results
  - Quick check question: What are the licensing conditions for negative polarity items, and why do they create illusions in certain sentence structures?

- Concept: Perplexity vs surprisal as evaluation metrics
  - Why needed here: The study uses both metrics and finds discrepancies, making it important to understand their differences and what they measure
  - Quick check question: How do perplexity and surprisal differ as evaluation metrics for language models, and what aspects of language processing does each capture?

## Architecture Onboarding

- Component map: Four language models (BERT, RoBERTa, GPT-2, GPT-3) evaluated using two metrics (perplexity, surprisal) on three types of language illusions (comparative, depth-charge, NPI)
- Critical path: Construct minimal pairs → Calculate perplexity and surprisal → Run statistical models → Compare to human judgments → Analyze discrepancies
- Design tradeoffs: Using masked language models vs autoregressive models, whole-sentence vs word-level metrics, focusing on syntactic vs semantic illusions
- Failure signatures: LMs failing to distinguish acceptable from unacceptable sentences, LMs showing no illusion effect when humans do, LMs showing opposite effects to humans
- First 3 experiments:
  1. Test LM ability to distinguish acceptable vs unacceptable sentences using minimal pairs for each illusion type
  2. Compare LM judgments of illusion sentences vs unacceptable controls to detect illusion effects
  3. Test LM sensitivity to linguistic manipulations that affect human judgments (e.g., subject number, verb repeatability)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do language models truly understand the literal meaning of sentences, or do they rely on statistical patterns and contextual information to generate acceptable outputs?
- Basis in paper: [explicit] The paper mentions that language models might "circumvent some grammatical facts and reach a good-enough sentence representation" or have "trouble understanding this complicated set of sentences overall."
- Why unresolved: The paper uses acceptability judgment tasks and probability-based measures, which are indirect measures of language comprehension. It is difficult to infer the exact interpretation based on these measures alone.
- What evidence would resolve it: Future studies could employ direct comprehension measures, such as generating paraphrases, to reveal the hidden knowledge and processing mechanisms of language models.

### Open Question 2
- Question: How do the training data and architecture of language models influence their ability to process language illusions and other complex linguistic phenomena?
- Basis in paper: [explicit] The paper mentions that "none of the language models we investigated consistently exhibited illusion effects or demonstrated overall human-like judgment behaviors" and that "none of the language models fully captured all the manipulations."
- Why unresolved: The paper only tested a limited number of language models and did not explore the impact of different training data or architectural choices on their performance.
- What evidence would resolve it: Future studies could compare the performance of language models trained on different datasets or with different architectures on a wider range of language illusions and linguistic phenomena.

### Open Question 3
- Question: Can language models be improved to better capture human-like language processing behaviors, such as the ability to process language illusions and other complex linguistic phenomena?
- Basis in paper: [inferred] The paper concludes that "language models cannot be viewed as cognitive models of language processing" and that "language models are far from being a cognitive model of human language."
- Why unresolved: The paper only tested existing language models and did not explore potential improvements or modifications to their training or architecture.
- What evidence would resolve it: Future studies could investigate novel training techniques, architectural modifications, or additional linguistic knowledge that could be incorporated into language models to enhance their ability to process complex linguistic phenomena and mimic human-like behaviors.

## Limitations
- Limited scope of illusion types tested may constrain generalizability of findings
- Corpus comparison lacks direct validation as neighbor papers focus on vision-language models rather than text-only LM processing
- The study doesn't explore how different training data or architectural choices might affect LM performance on language illusions

## Confidence

**High confidence**: The finding that LMs show better performance on syntactic illusions (NPI) than semantic illusions is well-supported by the comparative analysis of model behavior across illusion types and the explicit discussion of syntactic vs semantic processing differences.

**Medium confidence**: The claim about perplexity vs surprisal discrepancies revealing different aspects of LM processing is supported but requires further investigation to establish whether these metrics capture distinct cognitive processes or simply reflect implementation differences.

**Low confidence**: The broader conclusion that LMs cannot serve as cognitive models of human language processing, while consistent with the data, may be premature given the limited scope of illusion types tested and the potential for more sophisticated evaluation metrics.

## Next Checks
1. Test additional language models with varying architectures (including those with explicit semantic representations) to determine if the syntactic/semantic processing gap persists across model types.
2. Develop and evaluate unified metrics that combine sentence-level and word-level information to better capture human acceptability judgments across different linguistic phenomena.
3. Expand the illusion types tested to include more complex semantic dependencies and pragmatic phenomena to assess the generalizability of the syntactic/semantic processing differences observed.