---
ver: rpa2
title: Prompting Scientific Names for Zero-Shot Species Recognition
arxiv_id: '2310.09929'
source_url: https://arxiv.org/abs/2310.09929
tags:
- names
- species
- scientific
- common
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the underexplored problem of zero-shot species\
  \ recognition using pretrained vision-language models (VLMs) such as CLIP. We find\
  \ that directly using species scientific names written in Latin or Greek in prompts\
  \ yields poor performance, because most of these names are not in the VLMs\u2019\
  \ training set."
---

# Prompting Scientific Names for Zero-Shot Species Recognition

## Quick Facts
- arXiv ID: 2310.09929
- Source URL: https://arxiv.org/abs/2310.09929
- Authors: 
- Reference count: 7
- Primary result: Using common English names instead of scientific names in prompts improves zero-shot species recognition accuracy by 2-5x

## Executive Summary
This paper addresses the challenge of zero-shot species recognition using vision-language models (VLMs) like CLIP. The key finding is that scientific names written in Latin or Greek perform poorly in prompts because they are not well-represented in VLMs' training corpora. By translating scientific names to common English names and using those in prompts instead, the authors achieve significant accuracy improvements across three benchmarking datasets. The paper also demonstrates that selecting the more frequently occurring name between scientific and common names for each species yields optimal results.

## Method Summary
The method involves translating species scientific names to common English names and using these in prompts for zero-shot recognition with VLMs. The approach requires obtaining the VLM model (OpenCLIP with ViT-B/32 and ViT-L/14 variants), analyzing the LAION400M training corpus for name frequencies, collecting scientific and common names from online resources, and implementing three prompt methods: vanilla (scientific names), common names, and frequent names. The performance is evaluated using top-1 accuracy averaged over per-class accuracy on four datasets: semi-iNaturalist, semi-Aves, Flowers102, and CUB200.

## Key Results
- Using common English names instead of scientific names improves zero-shot recognition accuracy by 2-5 times
- Selecting the more frequent name between scientific and common names for each species yields the best performance
- LLM-generated descriptions provide only marginal improvements when added to prompts
- Scientific names work better for plant recognition than other species, likely due to higher frequency in training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scientific names in Latin/Greek are not well represented in VLMs' training corpora, causing poor zero-shot recognition.
- Mechanism: CLIP relies on text-image co-occurrence patterns in its training data. If scientific names are absent or rare, the text encoder cannot effectively match them to visual features, leading to poor similarity scores and low recognition accuracy.
- Core assumption: The distribution of species names in the web-scale pretraining corpus (LAION400M) differs significantly between scientific and common names, with common names being more prevalent.
- Evidence anchors:
  - [abstract] "Because these names are usually not included in CLIP's training set."
  - [section] "We investigate OpenCLIP's training set, i.e., LAION400M... and find that it does not sufficiently cover scientific names... Instead, many species have common names that are in LAION400M."
- Break condition: If scientific names become well-represented in future VLM training sets, this mechanism would no longer explain the performance gap.

### Mechanism 2
- Claim: Using more frequently encountered names in prompts leads to better zero-shot recognition.
- Mechanism: The frequency of a name in the training corpus affects how well the text encoder can map it to visual features. More frequent names have stronger associations in the learned embedding space, resulting in higher similarity scores for correct matches.
- Core assumption: The frequency of name occurrences in the training corpus correlates with the strength of text-visual associations in the model's embedding space.
- Evidence anchors:
  - [section] "We hypothesize that it is important to use the texts that are more frequently encountered by OpenCLIP during training."
  - [section] "We further analyze LAION400M... to obtain frequency counts of both scientific and common names for all the species of interest and use the more frequent one in the prompt for the corresponding species."
- Break condition: If the VLM architecture changes to be less sensitive to name frequency (e.g., through normalization or re-weighting), this mechanism would lose explanatory power.

### Mechanism 3
- Claim: Descriptions generated by LLMs do not consistently improve zero-shot recognition when paired with species names.
- Mechanism: LLM-generated descriptions may not align with the visual features CLIP has learned, or they may introduce noise that confuses the text encoder. Additionally, if these descriptions are not present in CLIP's training corpus, they do not help bridge the gap between text and visual representations.
- Core assumption: The quality and relevance of LLM-generated descriptions directly impact their effectiveness in zero-shot recognition tasks.
- Evidence anchors:
  - [abstract] "To improve performance, prior works propose to use large-language models (LLMs) to generate descriptions... and additionally use them in prompts. We find that they bring only marginal gains."
  - [section] "Although using LLMs does not always work on specific species... we follow this practice to generate descriptions using GPT4. When GPT4 fails to provide useful descriptions for any species, we use the simple prompts without any descriptions by default."
- Break condition: If future LLMs are specifically trained to generate descriptions that align better with CLIP's learned representations, this mechanism would no longer explain the marginal gains.

## Foundational Learning

- Concept: Vision-Language Models (VLMs) like CLIP
  - Why needed here: Understanding how VLMs work is crucial for grasping why scientific names perform poorly and why common names improve results.
  - Quick check question: What is the primary training objective of VLMs like CLIP, and how does it enable zero-shot recognition?

- Concept: Prompt engineering in VLMs
  - Why needed here: The paper's core contribution involves changing the prompt from scientific to common names, so understanding prompt engineering is essential.
  - Quick check question: How do different prompt formulations affect the similarity scores computed by CLIP's text encoder?

- Concept: Frequency bias in language models
  - Why needed here: The paper leverages frequency differences between scientific and common names, so understanding frequency bias is key.
  - Quick check question: How does the frequency of a word or phrase in a model's training corpus affect its representation and matching performance?

## Architecture Onboarding

- Component map:
  CLIP model (ViT-B/32 and ViT-L/14) -> Text encoder -> Visual encoder -> Prompt construction module -> LAION400M corpus analysis tool -> External name translation sources

- Critical path:
  1. Input image and species scientific name
  2. Translate scientific name to common name (if available)
  3. Construct prompt with common name
  4. Encode prompt using CLIP's text encoder
  5. Encode image using CLIP's visual encoder
  6. Compute cosine similarity between text and visual features
  7. Assign class based on highest similarity score

- Design tradeoffs:
  - Using scientific names directly is simpler but performs poorly due to low corpus frequency
  - Translating to common names requires external resources but significantly boosts accuracy
  - Including LLM-generated descriptions adds complexity with marginal benefits
  - Using the more frequent name between scientific and common names for each species optimizes performance but requires corpus analysis

- Failure signatures:
  - Low accuracy across all species types suggests issues with the VLM model or prompt template
  - Poor performance on specific species types (e.g., plants) may indicate incomplete translation or frequency analysis
  - Marginal gains from LLM descriptions suggest either poor description quality or misalignment with VLM representations

- First 3 experiments:
  1. Compare accuracy using scientific names vs. common names on a small subset of species
  2. Analyze frequency counts of scientific and common names in LAION400M for the species in the benchmark datasets
  3. Test the impact of including LLM-generated descriptions with both scientific and common names on a subset of species

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can zero-shot species recognition performance be further improved beyond the 2-5x accuracy gains achieved by using common names in prompts?
- Basis in paper: [explicit] The paper shows significant performance gains from using common names instead of scientific names, but also notes that additionally using descriptions in prompts does not always improve results.
- Why unresolved: The paper identifies the importance of using common names but does not explore other potential improvements beyond this translation method.
- What evidence would resolve it: Comparative experiments testing additional methods such as more sophisticated prompt engineering, incorporating external knowledge bases, or fine-tuning approaches could reveal further performance gains.

### Open Question 2
- Question: Why do scientific names for plant species appear more frequently in VLMs' training data compared to other species types?
- Basis in paper: [inferred] The paper observes that scientific names work well for plant recognition but not for other species, suggesting a difference in training data coverage.
- Why unresolved: The paper does not investigate the reasons behind this discrepancy in training data composition.
- What evidence would resolve it: Analysis of the VLM training corpus to identify the sources and frequency of scientific names for different species types would clarify this bias.

### Open Question 3
- Question: How do bias and unfairness in VLMs affect zero-shot species recognition performance across different species and regions?
- Basis in paper: [explicit] The paper acknowledges that VLMs and LLMs can learn bias from pretraining data and that this work does not address these issues.
- Why unresolved: The paper does not study the impact of potential biases on recognition accuracy for different species or geographic regions.
- What evidence would resolve it: Experiments measuring recognition accuracy across diverse species and regions, along with analysis of training data representation, would reveal bias effects.

## Limitations

- The method's effectiveness depends on the availability of common names in the VLM's training corpus, which may not generalize to all taxonomic groups or languages
- The frequency-based approach assumes that name frequency is the primary driver of performance, potentially overlooking other important factors like semantic similarity
- The study focuses on three specific datasets and two VLM architectures, limiting generalizability to other species recognition tasks and model variants

## Confidence

- **High Confidence**: The core finding that scientific names perform poorly in zero-shot species recognition due to low corpus frequency is well-supported by empirical evidence and corpus analysis
- **Medium Confidence**: The recommendation to use the more frequent name between scientific and common names for each species is based on frequency analysis but may not account for semantic nuances
- **Low Confidence**: The claim that LLM-generated descriptions bring only marginal gains is based on experiments with GPT4, but effectiveness may vary significantly depending on the LLM used

## Next Checks

1. **Cross-linguistic validation**: Test the method with scientific names from non-English languages to determine if the frequency advantage of common names generalizes beyond English-language corpora

2. **Semantic analysis**: Investigate whether scientific names that are semantically similar to common names (e.g., "Canis lupus" vs. "wolf") perform better than those with less obvious connections, even when both have low corpus frequency

3. **Model architecture sensitivity**: Evaluate whether the frequency-based prompting strategy works equally well across different VLM architectures (e.g., ALIGN, Florence) or if the effect is specific to CLIP's training methodology