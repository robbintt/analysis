---
ver: rpa2
title: Approximation by non-symmetric networks for cross-domain learning
arxiv_id: '2305.03890'
source_url: https://arxiv.org/abs/2305.03890
tags:
- networks
- approximation
- function
- theorem
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies approximation properties of non-symmetric kernel-based
  networks, motivated by applications in invariant learning, transfer learning, and
  synthetic aperture radar imaging. The authors develop a unified framework for analyzing
  such networks using generalized translation networks, rotated zonal function kernels,
  and general non-symmetric kernels.
---

# Approximation by non-symmetric networks for cross-domain learning

## Quick Facts
- arXiv ID: 2305.03890
- Source URL: https://arxiv.org/abs/2305.03890
- Authors: 
- Reference count: 37
- Primary result: Uniform approximation bounds for Sobolev functions using non-symmetric kernel networks with rates O((log M / M)^(γ/(q+2β))) for γ < q/2 + β

## Executive Summary
This paper develops a unified framework for analyzing approximation capabilities of non-symmetric kernel-based networks, motivated by applications in invariant learning, transfer learning, and synthetic aperture radar imaging. The authors show that non-symmetric kernels can approximate functions in Sobolev classes without requiring positive definiteness, expanding the class of learnable functions beyond traditional RKHS methods. The approach combines diffusion polynomial approximation with concentration inequalities to discretize the approximation into finite kernel-based networks.

## Method Summary
The method combines probability theory and approximation theory approaches to analyze non-symmetric kernel networks. It first approximates the target function using a diffusion polynomial, then applies concentration inequalities (specifically H"o"ffding's inequality) to discretize the integral representation into a finite kernel-based network. The framework works with general non-symmetric kernels G: X × Y → R that admit singular value decompositions, allowing for cross-domain applications where X ≠ Y. The approximation rates depend on the smoothness parameter γ, input dimension q, and singular value decay rate β.

## Key Results
- Uniform approximation bounds for functions in Sobolev classes using ReLU^r networks when r is not necessarily an integer
- Approximation rates O((log M / M)^(γ/(q+2β))) for γ < q/2 + β and (log M)^3/2 M^(-1/2) for γ = q/2 + β
- Framework extends to generalized translation networks, rotated zonal function kernels, and general non-symmetric kernels
- Cross-domain learning capability where training and target domains can differ

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Non-symmetric kernels can approximate functions in Sobolev classes without requiring positive definiteness, expanding the class of learnable functions beyond traditional RKHS methods.
- **Mechanism**: The paper first approximates the target function using a diffusion polynomial, then applies concentration inequalities (specifically H"o"ffding's inequality) to discretize the approximation into a finite kernel-based network. This combines approximation theory with probability theory approaches.
- **Core assumption**: The kernel admits a singular value decomposition (or generalized translation representation) and satisfies localization properties that enable effective approximation.
- **Evidence anchors**:
  - [abstract]: "We initiate in this paper a general approach to study the approximation capabilities of kernel based networks using non-symmetric kernels. While singular value decomposition is a natural instinct to study such kernels, we consider a more general approach..."
  - [section 4.1]: "The idea behind the proof is to first approximate f by σ2n(f) ∈ Π2n. Then we use Assumption 2 to express σ2n(f) in an integral form. A careful application of the H"o"ffding concentration inequality leads to the proof."
  - [corpus]: Weak evidence - related papers focus on symmetric kernels and traditional RKHS methods, not non-symmetric kernels.

### Mechanism 2
- **Claim**: The approximation error depends on both the smoothness parameter γ and the dimension q, with explicit rates that degrade gracefully as smoothness decreases relative to dimensionality.
- **Mechanism**: By analyzing the singular value decay rates (Λ_k ~ ℓ^β) and applying the recipe theorem, the paper derives uniform approximation bounds that show how error scales with M (network size) and the interplay between γ and q+2β.
- **Core assumption**: The singular values decay at a polynomial rate (Λ_k ~ ℓ^β) and the activation function satisfies Lipschitz-H"o"ffder continuity.
- **Evidence anchors**:
  - [section 3]: "Key results include uniform approximation bounds for functions in Sobolev classes using ReLU^r networks when r is not necessarily an integer, with approximation rates depending on the smoothness parameter γ and the dimension q of the input space."
  - [section 4.1]: "There exists integer M such that M ~ (GnTn)2 log(LnTn), where Tn incorporates the relationship between γ, q, and β in the error bound."
  - [corpus]: No direct evidence - corpus neighbors focus on different aspects of kernel methods without addressing these specific approximation rates.

### Mechanism 3
- **Claim**: Non-symmetric kernels enable transfer learning and cross-domain applications by allowing different input and output spaces (X ≠ Y).
- **Mechanism**: The kernel G: X × Y → R framework naturally accommodates scenarios where the training and target domains differ, such as transfer learning from one data space to another or synthetic aperture radar imaging.
- **Core assumption**: The kernel can be decomposed into appropriate singular value representations even when X ≠ Y.
- **Evidence anchors**:
  - [introduction]: "In transfer learning, we wish to use the parameters trained on one data set living on a space Y to learn a function on another data set living on the space X. In this case, it is natural to consider a kernel G: X × Y → R."
  - [section 3.3]: "In this section, we consider general non-symmetric kernels on X × Y which admit a singular value decomposition of the form G(x,y) = Σ φ_k(x)ψ_k(y)/Λ_k"
  - [corpus]: Weak evidence - corpus neighbors don't explicitly discuss cross-domain applications of non-symmetric kernels.

## Foundational Learning

- **Concept**: Diffusion polynomials and their approximation properties on data spaces
  - Why needed here: The paper uses diffusion polynomials as the first step in approximating target functions before applying probability theory methods
  - Quick check question: What is the relationship between the degree n of a diffusion polynomial and the approximation error for a function in W_γ?

- **Concept**: Concentration inequalities (H"o"ffding's inequality) for discretization of integral representations
  - Why needed here: Used to convert the infinite integral representation into a finite sum with probabilistic error bounds
  - Quick check question: How does H"o"ffding's inequality ensure that the discretized kernel network approximates the integral within desired error bounds?

- **Concept**: Singular value decomposition of non-symmetric kernels
  - Why needed here: The approximation framework requires decomposing the kernel into eigenfunction expansions, even when the kernel is not symmetric
  - Quick check question: What conditions must a non-symmetric kernel satisfy to admit a useful singular value decomposition for approximation purposes?

## Architecture Onboarding

- **Component map**: Data space (X, ρ, µ*, {λ_k}, {φ_k}) -> Kernel representation (G: X × Y → R with singular value decomposition) -> Approximation pipeline (target function → diffusion polynomial → integral form → discretized network) -> Error analysis (combining approximation theory and probability theory)

- **Critical path**: For implementing the approximation framework, the critical path is: (1) verify kernel properties (singular values, Lipschitz conditions), (2) compute diffusion polynomial approximation, (3) apply H"o"ffding inequality for discretization, (4) select network parameters M, y_j, a_j

- **Design tradeoffs**: 
  - Tradeoff between smoothness requirements (γ) and dimensionality (q): higher dimensional problems require either more smoothness or larger networks
  - Choice of β (singular value decay rate) affects the approximation rate: faster decay (larger β) improves rates
  - Balance between theoretical guarantees and practical implementation constraints

- **Failure signatures**: 
  - If singular values don't decay sufficiently (small β), approximation rates degrade significantly
  - If activation function doesn't satisfy Lipschitz-H"o"ffder conditions, concentration inequalities don't apply
  - If data space doesn't satisfy ball measure and Gaussian upper bound conditions, diffusion polynomial approximations fail

- **First 3 experiments**:
  1. Verify kernel singular value decomposition and Lipschitz properties for a simple non-symmetric kernel
  2. Implement diffusion polynomial approximation for a test function in W_γ and measure error decay
  3. Apply H"o"ffding-based discretization to the integral representation and compare theoretical vs. empirical error rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the approximation theory approach be extended to data spaces with more general underlying structures beyond Riemannian manifolds, such as fractal-like or non-smooth spaces?
- Basis in paper: The paper introduces data spaces as a generalization of Riemannian manifolds but focuses primarily on smooth manifold-like structures. The approximation theory approach relies on diffusion polynomials and specific kernel properties that may not hold for more general spaces.
- Why unresolved: The paper does not explore approximation on data spaces with non-smooth or fractal-like structures. The Gaussian upper bound condition (2.7) and other assumptions may not be satisfied for such spaces.
- What evidence would resolve it: Developing and proving approximation theorems for non-smooth data spaces, or providing counterexamples showing limitations of the approach for such spaces.

### Open Question 2
- Question: How do the approximation rates change when using adaptive discretization strategies instead of uniform discretization in the probability theory approach?
- Basis in paper: The paper uses uniform discretization based on concentration inequalities. The approximation rates depend on the number of terms M but do not explore adaptive strategies that might concentrate more points in regions of higher error.
- Why unresolved: The paper does not investigate adaptive discretization methods. The uniform approach may not be optimal for functions with non-uniform complexity across the domain.
- What evidence would resolve it: Comparing approximation rates between uniform and adaptive discretization strategies for various function classes and data spaces.

### Open Question 3
- Question: Can the unified framework for non-symmetric kernels be extended to handle time-varying or adaptive kernels in online learning scenarios?
- Basis in paper: The paper develops a unified framework for static non-symmetric kernels but does not address dynamic scenarios where kernels might change over time or adapt to incoming data.
- Why unresolved: The current framework assumes fixed kernels and does not incorporate mechanisms for kernel adaptation or time-varying structures. Online learning scenarios require different theoretical tools.
- What evidence would resolve it: Developing theoretical bounds for online learning with adaptive non-symmetric kernels, or demonstrating through experiments that existing bounds do not extend to such scenarios.

## Limitations

- The analysis requires restrictive assumptions including Gaussian upper bounds on orthonormal bases and polynomial decay of singular values that may not hold for practical kernels
- Cross-domain applications (transfer learning, SAR imaging) are motivated but lack extensive empirical validation
- Computational feasibility for large-scale problems and the impact of approximation bounds on practical performance are not thoroughly investigated

## Confidence

- **High confidence**: The theoretical framework for combining diffusion polynomials with concentration inequalities is mathematically sound. The approximation rates derived for Sobolev functions under the stated assumptions appear rigorous.
- **Medium confidence**: The extension to non-symmetric kernels is theoretically justified but requires careful verification of singular value decomposition properties in practical implementations.
- **Low confidence**: The practical applicability to cross-domain scenarios lacks empirical validation in the paper. The computational feasibility of the proposed methods for large-scale problems remains unclear.

## Next Checks

1. Implement the framework on a controlled test case where all assumptions are verified (e.g., synthetic data with known kernel properties) to confirm the theoretical bounds match empirical performance.

2. Test the approximation framework on a non-symmetric kernel from a real application (e.g., transfer learning scenario) to identify which assumptions break down in practice and how this affects performance.

3. Benchmark the computational complexity of the discretized kernel networks against standard symmetric kernel methods for comparable approximation accuracy.