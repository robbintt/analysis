---
ver: rpa2
title: 'Amortized Variational Inference: When and Why?'
arxiv_id: '2307.11018'
source_url: https://arxiv.org/abs/2307.11018
tags:
- inference
- f-vi
- a-vi
- variational
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes when amortized variational inference (AVI)
  can achieve the same optimal solution as factorized VI (FVI) in hierarchical models.
  The key insight is that AVI cannot do better than FVI because its variational family
  is a subset, but under certain conditions it can match FVI's optimal solution.
---

# Amortized Variational Inference: When and Why?

## Quick Facts
- arXiv ID: 2307.11018
- Source URL: https://arxiv.org/abs/2307.11018
- Authors: 
- Reference count: 40
- Key outcome: This paper analyzes when amortized variational inference (AVI) can achieve the same optimal solution as factorized VI (FVI) in hierarchical models. The key insight is that AVI cannot do better than FVI because its variational family is a subset, but under certain conditions it can match FVI's optimal solution.

## Executive Summary
This paper provides a theoretical analysis of when amortized variational inference can achieve the same optimal solution as factorized variational inference in hierarchical models. The authors prove that AVI's variational family is always a subset of FVI's, establishing that AVI cannot outperform FVI's optimal solution. They then derive conditions under which AVI can theoretically match FVI's optimal solution, proving that simple hierarchical models uniquely satisfy these conditions. The work bridges theoretical understanding of the amortization gap with practical implications for model design and inference function selection.

## Method Summary
The authors analyze AVI vs FVI through KL-divergence minimization, proving that AVI's variational family is always a subset of FVI's. They derive conditions for when AVI can match FVI's optimal solution through the existence of learnable inference functions. The theoretical analysis is complemented by experiments on synthetic models (linear probabilistic models, hidden Markov models) and real data (VAEs on MNIST and FashionMNIST), comparing convergence rates and ELBO values across different inference function architectures.

## Key Results
- AVI cannot achieve lower KL-divergence than FVI's optimal solution due to subset relationship between variational families
- Simple hierarchical models uniquely admit learnable inference functions, allowing AVI to match FVI
- AVI often converges faster than FVI when the amortization gap can be closed
- Inference function parameter count need not scale with data size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Amortized VI cannot achieve better KL-divergence than F-VI's optimal solution because the amortized family is a subset of the factorized family.
- Mechanism: Any distribution in the amortized family QA(F) can be constructed within QF by matching the inference function outputs to individual variational parameters, making QA(F) ⊆ QF.
- Core assumption: Both methods use the same parametric family Qℓ for local latent variables.
- Evidence anchors:
  - [abstract]: "A-VI cannot produce an approximation with a lower Kullback-Leibler divergence than F-VI's optimal solution, because the amortized family is a subset of the factorized family."
  - [section]: "Proposition 2.1. For any class of inference functions F, QA(F) is a subset of QF. Hence A-VI cannot achieve a lower KL-divergence than the optimal solution for F-VI."
- Break condition: If Qℓ differs between the two methods or if additional constraints are imposed on one family but not the other.

### Mechanism 2
- Claim: A-VI can match F-VI's optimal solution when a learnable inference function exists that interpolates from observations to F-VI's optimal variational factors.
- Mechanism: The existence of a learnable inference function fx maps each observation to the corresponding optimal variational parameter, allowing A-VI to reconstruct F-VI's solution through interpolation.
- Core assumption: For any dataset x = x1:N, there exists a function fx: X → N such that fx(xn) = ν∗n for all n.
- Evidence anchors:
  - [abstract]: "We derive conditions on both the model and the inference function under which A-VI can theoretically achieve F-VI's optimum."
  - [section]: "Proposition 2.3. For a model p(θ, z, x), the amortization interpolation problem can be solved if and only if for any x = x1:N there exists a (dataset-dependent) function fx: X → N such that fx(xn) = ν∗n, ∀n."
- Break condition: When xn = xm but ν∗n ≠ ν∗m, making interpolation impossible regardless of inference function expressiveness.

### Mechanism 3
- Claim: Simple hierarchical models admit learnable inference functions due to partial pooling, where local observations influence each latent variable differently while maintaining global consistency.
- Mechanism: In simple hierarchical models, the posterior mean of each latent variable depends on both local observations and global information through the hierarchical structure, creating a learnable mapping from xn to optimal variational parameters.
- Core assumption: The model follows the factorization p(θ, z, x) = p(θ) ∏n p(zn)p(xn | zn, θ) with identical forms for all n.
- Evidence anchors:
  - [abstract]: "We prove these conditions are uniquely verified by simple hierarchical models, a broad class that encompasses many models in machine learning."
  - [section]: "Theorem 3.2. Consider the simple hierarchical model p(θ, z, x) in Eq. 1. The optimal solution for F-VI can be written as a learnable inference function, q(zn; ν∗n) = kx(xn) p(zn) exp{Eq(θ; ν∗0)[log p(xn | zn, θ)]}."
- Break condition: When the model structure prevents the existence of a learnable function, such as in hidden Markov models where observations have varying local influences on different latent variables.

## Foundational Learning

- Concept: Variational inference and KL-divergence minimization
  - Why needed here: The paper's core analysis relies on comparing KL-divergences between different variational families to establish theoretical bounds on A-VI performance.
  - Quick check question: Why does minimizing KL-divergence between approximate and true posteriors provide a tractable objective for posterior inference?

- Concept: Factorized vs. amortized variational inference
  - Why needed here: Understanding the fundamental difference between learning separate variational parameters per observation (F-VI) versus learning a shared inference function (A-VI) is crucial for grasping the amortization gap analysis.
  - Quick check question: How does the factorization of variational families affect their expressiveness and computational efficiency?

- Concept: Partial pooling in hierarchical models
  - Why needed here: The paper's proof that simple hierarchical models admit learnable inference functions relies on understanding how information flows between local and global levels in hierarchical structures.
  - Quick check question: In what way does partial pooling create the conditions necessary for a learnable inference function to exist?

## Architecture Onboarding

- Component map: Model specification -> Variational inference engine -> Learnable function training pipeline -> Evaluation
- Critical path: For A-VI implementation, the critical path involves (1) defining the inference function class F, (2) optimizing inference function parameters ϕ to minimize KL-divergence, and (3) evaluating whether the learned function achieves F-VI's optimal solution
- Design tradeoffs: The choice between F-VI and A-VI involves trading off between computational efficiency (A-VI scales better with N) and approximation quality (F-VI can achieve lower KL-divergence when no learnable function exists)
- Failure signatures: A-VI fails to match F-VI when the model doesn't admit a learnable inference function, which can be detected by observing that optimal variational parameters vary independently of observations even when data points are identical
- First 3 experiments:
  1. Implement linear probabilistic model from Section 3.1 and verify that A-VI with linear inference function matches F-VI's solution
  2. Test A-VI on hidden Markov model from Section 3.2 to confirm it cannot achieve F-VI's optimal solution regardless of inference function expressiveness
  3. Run experiments varying inference function width k on VAE with FashionMNIST to observe the relationship between expressiveness and convergence speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does an expressive inference function class F guarantee convergence to F-VI's optimal solution, beyond just the existence of a learnable inference function?
- Basis in paper: [inferred] The paper shows that a learnable function guarantees the interpolation problem can be solved, but does not fully characterize when F is expressive enough to find it.
- Why unresolved: The paper focuses on theoretical conditions for the existence of a learnable function rather than analyzing the optimization landscape and convergence properties of different F classes.
- What evidence would resolve it: Empirical studies comparing convergence rates and final ELBO values across different F classes (e.g., polynomial degrees, neural network widths) for models where a learnable function exists.

### Open Question 2
- Question: Can the theoretical analysis of learnable inference functions be extended to models with non-factored priors or more complex dependencies between latent variables?
- Basis in paper: [explicit] The authors state they would like to extend analysis to other VI algorithms and models with non-factored priors like the branch factorization.
- Why unresolved: The current analysis is limited to simple hierarchical models with factorized priors, and the extension to more complex models requires new mathematical techniques.
- What evidence would resolve it: Mathematical proofs showing the existence or non-existence of learnable inference functions for specific classes of models with non-factored priors, supported by empirical validation.

### Open Question 3
- Question: How does the generalization gap of amortized VI behave when applied to held-out data, and can this be analyzed through an implicit interpolation problem with overfitting constraints?
- Basis in paper: [explicit] The authors mention this as a direction for future work, suggesting the generalization gap can be analyzed by setting up an implicit interpolation problem with constraints.
- Why unresolved: The paper focuses on the amortization gap (difference between A-VI and F-VI on training data) but does not analyze how well the learned inference function generalizes to new data.
- What evidence would resolve it: Theoretical bounds on the generalization gap for A-VI compared to F-VI, supported by empirical studies measuring performance on held-out data across different models and inference function classes.

## Limitations

- The theoretical analysis relies on idealized conditions, particularly infinite data and perfect optimization
- Learnability conditions may not translate directly to practical settings due to optimization challenges
- Empirical claims about convergence speed advantages are based on limited experimental evidence

## Confidence

**High confidence**: The subset relationship between amortized and factorized variational families (Mechanism 1) is mathematically rigorous and well-established.

**Medium confidence**: The conditions under which AVI can match FVI's optimal solution (Mechanism 2) are theoretically derived but may be too restrictive for practical application.

**Low confidence**: The empirical claims about convergence speed advantages of AVI over FVI are based on limited experimental evidence and may not generalize across different model architectures and datasets.

## Next Validation Checks

1. **Finite sample validation**: Conduct experiments systematically varying dataset size N to quantify how the amortization gap changes with sample size, particularly for models where AVI can theoretically match FVI.

2. **Robustness to optimization**: Test whether AVI can find the learnable inference function under realistic optimization conditions (finite iterations, stochastic gradients) for simple hierarchical models, and measure the gap between theoretically achievable and practically achievable performance.

3. **Generalization across architectures**: Extend experiments to include different inference function architectures (CNNs, transformers) and model families beyond VAEs to validate whether the theoretical insights about learnability and convergence speed hold across diverse settings.