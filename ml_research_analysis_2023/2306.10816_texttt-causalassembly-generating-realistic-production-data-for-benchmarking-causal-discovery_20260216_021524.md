---
ver: rpa2
title: '$\texttt{causalAssembly}$: Generating Realistic Production Data for Benchmarking
  Causal Discovery'
arxiv_id: '2306.10816'
source_url: https://arxiv.org/abs/2306.10816
tags:
- data
- causal
- process
- learning
- discovery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a semisynthetic data generation pipeline for
  benchmarking causal discovery algorithms. The method leverages real manufacturing
  data with known ground truth causal relationships, uses sparse additive models to
  learn cross-process dependencies, and employs distributional random forests to estimate
  conditional distributions.
---

# `causalAssembly`: Generating Realistic Production Data for Benchmarking Causal Discovery

## Quick Facts
- arXiv ID: 2306.10816
- Source URL: https://arxiv.org/abs/2306.10816
- Reference count: 40
- Generates semisynthetic manufacturing data with known ground truth for causal discovery benchmarking

## Executive Summary
This paper introduces a semisynthetic data generation pipeline that enables benchmarking of causal discovery algorithms on realistic manufacturing data while preserving privacy. The method leverages real manufacturing data with known ground truth causal relationships, uses sparse additive models to learn cross-process dependencies, and employs distributional random forests to estimate conditional distributions. The resulting `causalAssembly` Python library enables generation of complex, realistic data for algorithm evaluation. Benchmark results show that off-the-shelf methods perform variably on the data, with no single approach consistently outperforming others across different production stations.

## Method Summary
The method combines real manufacturing data with known ground truth causal relationships and applies sparse additive models (SpAM) to learn cross-process dependencies between production stations. Distributional random forests (DRFs) are then used to estimate conditional distributions under the factorization implied by the ground truth DAG. The generated data preserves key statistical properties of the real data while respecting privacy constraints. The pipeline is implemented in the `causalAssembly` Python library, which enables users to benchmark various causal discovery algorithms including PC, DirectLiNGAM, NOTEARS, and GraN-DAG on the semisynthetic data.

## Key Results
- The semisynthetic data generation pipeline successfully creates realistic manufacturing data with known ground truth causal structures
- Benchmark results show significant performance variations across different causal discovery algorithms, with no single method consistently superior
- The method preserves key characteristics of real data while addressing privacy concerns through the generation process

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The pipeline can produce realistic data by estimating conditional distributions that strictly adhere to the ground truth causal structure
- Mechanism: Distributional random forests (DRFs) are used to estimate conditional distributions under the factorization implied by the ground truth DAG. These conditionals are then combined to create a joint distribution that respects the causal model
- Core assumption: The real-world manufacturing data captures the true underlying causal relationships between variables within each process station
- Evidence anchors: [abstract]: "We employ distributional random forests in order to flexibly estimate and represent conditional distributions that may be combined into joint distributions that strictly adhere to a causal model over the observed variables."

### Mechanism 2
- Claim: The semisynthetic data generation process preserves key characteristics of the real data while respecting privacy concerns
- Mechanism: The pipeline uses real manufacturing data as a basis, applies sparse additive models (SpAM) to learn cross-process dependencies, and then uses DRFs to estimate conditional distributions. This allows for the generation of new data that mimics the statistical properties of the original data without directly exposing sensitive information
- Core assumption: The real manufacturing data captures the essential statistical properties and relationships between variables
- Evidence anchors: [abstract]: "This issue is further compounded by privacy concerns surrounding the release of suitable high-quality data."

### Mechanism 3
- Claim: The semisynthetic data can be used to benchmark causal discovery algorithms on complex, realistic data
- Mechanism: The generated data adheres to a known ground truth causal structure, allowing for objective evaluation of causal discovery algorithms. The complexity of the manufacturing data and the realistic nature of the generated data make it a valuable test bed for evaluating algorithm performance on real-world scenarios
- Core assumption: The ground truth causal structure accurately represents the true causal relationships in the manufacturing processes
- Evidence anchors: [abstract]: "Using the library, we showcase how to benchmark several well-known causal discovery algorithms."

## Foundational Learning

- Concept: Causal Discovery Algorithms
  - Why needed here: The paper benchmarks several causal discovery algorithms on the semisynthetic data, requiring an understanding of how these algorithms work and their assumptions
  - Quick check question: What is the key assumption made by the PC algorithm for causal structure learning?

- Concept: Additive Noise Models (ANMs)
  - Why needed here: ANMs are used to ensure identifiability of the true DAG in synthetic data generation, which is relevant to understanding the limitations of the semisynthetic data generation approach
  - Quick check question: What is the main assumption of ANMs that allows for identifiability of the causal structure?

- Concept: Distributional Random Forests (DRFs)
  - Why needed here: DRFs are a key component of the semisynthetic data generation pipeline, used to estimate conditional distributions under the ground truth causal structure
  - Quick check question: How do DRFs differ from standard random forests in terms of the splitting criterion used?

## Architecture Onboarding

- Component map: Real manufacturing data collection -> Ground truth causal structure definition -> SpAM for cross-process dependency learning -> DRF for conditional distribution estimation -> Data generation -> Benchmarking framework for causal discovery algorithms

- Critical path: The critical path involves collecting and preprocessing the real data, defining the ground truth causal structure, and then using SpAM and DRFs to generate the semisynthetic data. The benchmarking of causal discovery algorithms is a subsequent step that relies on the generated data

- Design tradeoffs:
  - Complexity vs. Realism: The semisynthetic data generation process aims to capture the complexity of real manufacturing processes while ensuring that the generated data adheres to a known ground truth causal structure. This balance is crucial for effective benchmarking
  - Privacy vs. Data Fidelity: The pipeline needs to preserve the statistical properties of the real data while respecting privacy concerns. This requires careful consideration of the data generation process and the level of detail included in the generated data

- Failure signatures:
  - Poor performance of causal discovery algorithms on the generated data may indicate issues with the data generation process or the ground truth causal structure
  - Significant deviations between the statistical properties of the real and generated data may suggest problems with the estimation of conditional distributions or the preservation of key characteristics

- First 3 experiments:
  1. Evaluate the similarity between the real and generated data using statistical measures such as the Kolmogorov-Smirnov statistic
  2. Test the performance of causal discovery algorithms on the generated data and compare it to their performance on real data
  3. Assess the impact of different choices for the ground truth causal structure on the generated data and the benchmarking results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed semisynthetic data generation pipeline perform on non-manufacturing datasets with known ground truth causal relationships?
- Basis in paper: [explicit] The paper mentions that the proposed procedure to synthesize real data is general and can in principle be applied to any given dataset with partially known domain knowledge regardless of the domain
- Why unresolved: The current implementation and benchmarking results are based solely on manufacturing data. The authors suggest the method's general applicability but do not demonstrate its performance on other types of data or domains
- What evidence would resolve it: Applying the causalAssembly pipeline to other datasets with known ground truth (such as the cause-effect pairs dataset [38] or biological datasets [39-41]) and comparing the performance of causal discovery algorithms on the semisynthetic data versus the original data

### Open Question 2
- Question: How does the inclusion of prior knowledge about inter-process influences (beyond the sequential ordering) affect the accuracy of the learned cross-process causal relationships?
- Basis in paper: [explicit] The paper mentions that the current framework assumes a layered structure where later processes can be influenced by earlier ones, but acknowledges that some assembly lines may have additional domain knowledge about inter-process influences
- Why unresolved: The authors demonstrate the method using only sequential process ordering as prior knowledge. They mention the possibility of incorporating more complex prior knowledge but do not explore how this would affect the results
- What evidence would resolve it: Implementing the method with various types of prior knowledge constraints (e.g., specific known causal relationships between non-adjacent processes) and measuring the impact on the accuracy of the learned DAG and the performance of causal discovery algorithms on the resulting semisynthetic data

### Open Question 3
- Question: What is the optimal sample size for the semisynthetic data to balance computational efficiency with the accuracy of causal discovery algorithm benchmarking?
- Basis in paper: [inferred] The paper uses a sample size of n=500 for benchmarking in the main text and mentions that one set of datasets with sample size n=500 is provided for convenience. The original real data has n=15,581 samples
- Why unresolved: The paper does not explore how varying the sample size of the semisynthetic data affects the performance of causal discovery algorithms or the computational resources required for benchmarking
- What evidence would resolve it: Conducting a systematic study varying the sample size of the semisynthetic data (e.g., n=100, 500, 1000, 5000) and measuring both the performance of causal discovery algorithms (using metrics like SHD, Precision, Recall, and F1 score) and the computational time required for benchmarking

## Limitations
- The method relies on real manufacturing data with assumed ground truth causal relationships, which may not be perfectly known despite domain expertise
- Distributional random forest approach lacks extensive corpus validation for strictly adhering to causal models
- Performance variability across different stations suggests the framework may not provide consistent benchmarking results for all causal discovery algorithms

## Confidence
- High: The semisynthetic data generation pipeline is technically sound and provides a practical solution for privacy-preserving benchmarking
- Medium: The framework's effectiveness for benchmarking is demonstrated but shows algorithm-dependent performance variations
- Medium: The preservation of real data characteristics while ensuring privacy is plausible but requires further validation

## Next Checks
1. Validate DRF adherence to causal models by comparing conditional independence tests on real vs. generated data
2. Conduct ablation studies to quantify the impact of SpAM vs. DRF components on algorithm performance
3. Test the framework's robustness across different manufacturing domains and data characteristics