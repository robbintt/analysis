---
ver: rpa2
title: 'HTEC: Human Transcription Error Correction'
arxiv_id: '2309.10089'
source_url: https://arxiv.org/abs/2309.10089
tags:
- transcription
- htec
- error
- human
- errors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving human transcription
  quality for speech recognition. It proposes HTEC, a two-stage framework consisting
  of Trans-Checker, which detects and masks erroneous words, and Trans-Filler, which
  fills in the masked positions.
---

# HTEC: Human Transcription Error Correction

## Quick Facts
- arXiv ID: 2309.10089
- Source URL: https://arxiv.org/abs/2309.10089
- Reference count: 18
- Key outcome: HTEC outperforms existing methods and surpasses human annotators by 2.2% to 4.5% in WER

## Executive Summary
This paper addresses the challenge of improving human transcription quality for speech recognition systems. The authors propose HTEC, a two-stage framework that detects and corrects transcription errors more effectively than existing approaches. HTEC consists of Trans-Checker, which identifies erroneous words using phoneme embeddings, and Trans-Filler, which iteratively corrects the masked errors. When deployed as a co-pilot tool to assist human annotators, HTEC improves transcription quality by 15.1% without sacrificing transcription velocity, while also outperforming human annotators by 2.2% to 4.5% in WER.

## Method Summary
HTEC is a two-stage framework for human transcription error correction. The first stage, Trans-Checker, uses a transformer encoder with phoneme embeddings to detect erroneous words and generate masks. The second stage, Trans-Filler, employs a sequence-to-sequence model with a non-autoregressive decoder to iteratively fill the masked positions. The method incorporates phoneme embeddings through a CNN-based embedding generator and introduces novel editing operations to handle deletion errors. The framework is trained on aligned annotator and gold transcriptions from the CAIA and MASSIVE datasets, using synthetic error generation for data augmentation.

## Key Results
- HTEC outperforms existing methods by a large margin in WER reduction
- Surpasses human annotators by 2.2% to 4.5% in WER
- Improves transcription quality by 15.1% when deployed as a co-pilot tool without sacrificing transcription velocity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Trans-Checker detects erroneous words by leveraging phoneme embeddings to disambiguate homophones and similar-sounding words.
- Mechanism: The model uses a CNN to convert the annotator transcription into phoneme embeddings, which are added to the token and positional embeddings. This augmented input helps the transformer encoder identify word errors caused by misheard audio or phonetic ambiguity.
- Core assumption: Phoneme embeddings capture the ambiguity between homophones or similar-sounding words, improving error detection for spoken language.
- Evidence anchors:
  - [abstract]: "We further propose a variant of embeddings that incorporates phoneme information into the input of the transformer."
  - [section 3.1.2]: "We introduce a variant of the embedding layer that incorporates phoneme embeddings as part of the transformer input, which augments the model's ability to correct errors caused by homophonic or similar-sounding ambiguity."
  - [corpus]: Weak evidence; related papers do not discuss phoneme embedding use for human transcription error correction.

### Mechanism 2
- Claim: Trans-Filler corrects errors iteratively, filling one mask per iteration to avoid introducing new errors or hallucinations.
- Mechanism: The model uses a non-autoregressive (NAR) decoder that fills one token in each masked position per iteration. This controlled, step-by-step approach minimizes the risk of overcorrection or generating incorrect words.
- Core assumption: Filling one token at a time reduces the likelihood of introducing new errors compared to autoregressive models that may fill multiple tokens at once.
- Evidence anchors:
  - [section 3.2]: "In the NAR decoder setting, the encoder is trained with one position of right-shifted labels... The NAR decoder tends to fill one token in one position so it introduces fewer new errors."
  - [section 4.2.2]: "Filler-NAR outperforms the AR setting since autoregressive tends to fill more tokens than needed or generate hallucinations."
  - [corpus]: No direct evidence in related papers; they focus on ASR error correction rather than iterative mask filling.

### Mechanism 3
- Claim: The two-stage framework (Trans-Checker + Trans-Filler) mimics human transcription correction workflows, improving overall accuracy.
- Mechanism: Trans-Checker first identifies and masks erroneous words, then Trans-Filler fills in the masked positions. This staged approach allows for targeted correction, reducing the cognitive load on the model and improving precision.
- Core assumption: Breaking the task into detection and correction stages aligns with how human experts identify and fix errors, leading to better performance than end-to-end models.
- Evidence anchors:
  - [abstract]: "HTEC consists of two stages: Trans-Checker, an error detection model that predicts and masks erroneous words, and Trans-Filler, a sequence-to-sequence generative model that fills masked positions."
  - [section 3]: "HTEC follows the way that human SMEs identify and fix errors in transcription."
  - [corpus]: Related papers focus on end-to-end ASR error correction, not staged approaches for human transcription.

## Foundational Learning

- Concept: Phoneme embeddings
  - Why needed here: Spoken language is prone to errors caused by homophones and similar-sounding words. Phoneme embeddings help the model disambiguate these cases by incorporating phonetic information into the input representation.
  - Quick check question: How do phoneme embeddings improve error detection for spoken language compared to text-only models?

- Concept: Non-autoregressive decoding
  - Why needed here: Autoregressive models can generate hallucinations or overfill masked positions, introducing new errors. Non-autoregressive decoding fills one token at a time, reducing the risk of introducing incorrect words.
  - Quick check question: What are the trade-offs between autoregressive and non-autoregressive decoding in the context of error correction?

- Concept: Staged error correction
  - Why needed here: Breaking the task into detection and correction stages aligns with human workflows, allowing for targeted correction and reducing the cognitive load on the model.
  - Quick check question: How does a two-stage approach improve accuracy compared to end-to-end models in error correction tasks?

## Architecture Onboarding

- Component map: Annotator Transcription + ASR Transcription -> Phoneme Embedding Layer -> Trans-Checker -> Mask Generation -> Trans-Filler -> Corrected Transcription
- Critical path:
  1. Input: Annotator transcription + ASR transcription
  2. Phoneme embedding generation
  3. Trans-Checker error detection
  4. Mask generation for erroneous words
  5. Trans-Filler iterative correction
  6. Output: Corrected transcription
- Design tradeoffs:
  - Phoneme embeddings vs. text-only embeddings: Phoneme embeddings improve error detection for homophones but add computational overhead.
  - Autoregressive vs. non-autoregressive decoding: NAR reduces hallucinations but may be slower for complex errors.
  - Two-stage vs. end-to-end approach: Staged approach aligns with human workflows but requires careful coordination between stages.
- Failure signatures:
  - High false positive rate in Trans-Checker: Model flags too many words as errors, overwhelming Trans-Filler.
  - Low recall in Trans-Checker: Model misses errors, leaving them uncorrected.
  - Trans-Filler introduces new errors: NAR decoder fails to fill masks accurately, degrading transcription quality.
- First 3 experiments:
  1. Evaluate Trans-Checker performance with and without phoneme embeddings on a held-out test set.
  2. Compare autoregressive vs. non-autoregressive Trans-Filler on a synthetic error correction task.
  3. Test the end-to-end HTEC pipeline on a small subset of the dataset to validate staged approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would HTEC's performance change with non-English languages or multilingual speech data?
- Basis in paper: [inferred] The authors state that HTEC has not been evaluated with non-English data and the performance in other languages and multilingual transcriptions is unclear, although the method is applicable to non-English data.
- Why unresolved: The paper only tests HTEC on English datasets (CAIA and MASSIVE), leaving the performance on other languages unexplored.
- What evidence would resolve it: Testing HTEC on diverse multilingual datasets and comparing its performance across different languages would provide concrete evidence of its cross-linguistic effectiveness.

### Open Question 2
- Question: What is the optimal balance between precision and recall for Trans-Checker in different deployment scenarios?
- Basis in paper: [explicit] The paper discusses that while autocorrection requires high precision to mitigate error accumulation, human-in-the-loop scenarios desire high recall to identify more errors in the first stage. They show different performance metrics for weighted vs uniform loss in Trans-Checker.
- Why unresolved: The paper presents trade-offs but doesn't definitively determine the optimal balance for different use cases.
- What evidence would resolve it: Conducting user studies comparing different precision-recall trade-offs in both autocorrection and human-in-the-loop scenarios would identify optimal settings for each deployment mode.

### Open Question 3
- Question: How does HTEC's fairness impact vary across different demographic groups beyond what was tested?
- Basis in paper: [explicit] The paper analyzes HTEC's impact across cohorts including nativity, locale, and domain, showing improvements across all groups but unequal relative improvements.
- Why unresolved: While the paper shows some fairness analysis, it doesn't explore all potential demographic factors or fully quantify the fairness implications.
- What evidence would resolve it: Expanding the cohort analysis to include additional demographic factors (age, gender, socioeconomic status, etc.) and conducting statistical fairness tests would provide a more comprehensive fairness assessment.

### Open Question 4
- Question: What is the optimal prompt engineering strategy for large language models in human transcription error correction?
- Basis in paper: [inferred] The paper evaluates Alpaca but notes that larger LLMs like PaLM-2 or GPT-4 weren't fully explored, and that careful prompt engineering could yield further improvements.
- Why unresolved: The paper only provides preliminary LLM evaluation and acknowledges that more sophisticated prompt strategies remain unexplored.
- What evidence would resolve it: Systematic experimentation with different prompt structures, few-shot examples, and instruction formats across multiple LLM architectures would identify optimal prompting strategies for this task.

## Limitations
- The evaluation relies heavily on commercial datasets (CAIA) that are not publicly available, making independent verification difficult.
- The comparison with Alpaca's in-context learning uses a 7B model, which may not represent the most current or capable language models available.
- The study focuses primarily on WER reduction without extensive analysis of how different error types are affected by the model.

## Confidence
- High confidence: The core methodology of using a two-stage framework for error correction is well-justified and supported by experimental results.
- Medium confidence: The assertion that HTEC surpasses human annotators by 2.2% to 4.5% in WER is supported by data but may be dataset-dependent.
- Low confidence: The claim that phoneme embeddings significantly improve error detection for homophones is based on ablation studies within the paper.

## Next Checks
1. **Phoneme Embedding Ablation Study:** Conduct an ablation study comparing HTEC with and without phoneme embeddings across different error types to quantify the specific contribution of phoneme information.
2. **Cross-Dataset Generalization Test:** Evaluate HTEC's performance on additional publicly available datasets with different acoustic conditions and speaker demographics to assess generalization beyond the CAIA and MASSIVE datasets.
3. **Human-in-the-Loop Evaluation:** Conduct a field study with professional transcriptionists using HTEC as a co-pilot tool in their actual workflow to validate controlled lab experiment findings.