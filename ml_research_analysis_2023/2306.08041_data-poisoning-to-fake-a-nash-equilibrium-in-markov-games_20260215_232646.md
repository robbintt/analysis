---
ver: rpa2
title: Data Poisoning to Fake a Nash Equilibrium in Markov Games
arxiv_id: '2306.08041'
source_url: https://arxiv.org/abs/2306.08041
tags:
- nash
- unique
- markov
- linear
- equilibrium
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies offline data poisoning attacks on multi-agent
  reinforcement learning, where an attacker aims to manipulate a dataset to install
  a specific Nash equilibrium as the unique solution for two-player zero-sum Markov
  games. The authors introduce the "unique Nash set," a polytope in the Q-function
  space that characterizes games where a particular policy is the unique Nash equilibrium.
---

# Data Poisoning to Fake a Nash Equilibrium in Markov Games

## Quick Facts
- arXiv ID: 2306.08041
- Source URL: https://arxiv.org/abs/2306.08041
- Reference count: 40
- This paper studies offline data poisoning attacks on multi-agent reinforcement learning, where an attacker aims to manipulate a dataset to install a specific Nash equilibrium as the unique solution for two-player zero-sum Markov games.

## Executive Summary
This paper introduces a framework for offline data poisoning attacks on multi-agent reinforcement learning (MARL) in two-player zero-sum Markov games. The core insight is that an attacker can force a specific policy to become the unique Nash equilibrium by manipulating the dataset such that the estimated Q-function confidence region is fully contained within what the authors call the "unique Nash set" - a polytope in Q-function space that characterizes games where a particular policy is the unique Nash equilibrium. The attack is formulated as a linear program that optimizes poisoning cost under the containment constraint, making it computationally tractable when the attacker's confidence regions are sufficiently small.

## Method Summary
The method constructs a poisoned dataset D† from an original dataset D by modifying reward values to ensure that the theory-of-mind set T(D†) (representing plausible Q-function estimates from the poisoned data) is contained within the ι-strict unique Nash set U(π†; ι) for the target policy π†. This containment guarantees that all plausible games induced by D† have π† as the unique Nash equilibrium. The containment constraint is converted to a linear program when using L1 cost and linear outer approximations of the theory-of-mind sets. The approach generalizes the reward polytope from inverse reinforcement learning to the multi-agent setting and provides a unified framework for analyzing offline data poisoning in MARL.

## Key Results
- The unique Nash set U(π†) is a polytope in Q-function space that characterizes games where π† is the unique Nash equilibrium.
- The optimal poisoning attack can be computed via a linear program when using L1 cost and linear outer approximations of theory-of-mind sets.
- Attack feasibility requires the attacker's confidence region radius to be sufficiently small relative to reward bounds (ρ(Q) ≤ b - ι/4H).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The attacker can force a specific policy to become the unique Nash equilibrium by manipulating the reward values in the dataset.
- Mechanism: The unique Nash set of a pure strategy profile π is the set of reward functions R such that (A, R) has a unique Nash equilibrium π. By constructing a poisoned dataset whose estimated Q-function confidence region is fully contained within this set, the attacker ensures MARL agents converge to the target policy.
- Core assumption: The attacker's confidence region is small enough to be contained within the unique Nash polytope, and the original dataset provides sufficient coverage of states and actions.
- Break condition: If the attacker's confidence region is too large relative to the unique Nash polytope, containment becomes impossible and the attack fails.

### Mechanism 2
- Claim: The attack can be formulated as a linear program when using L1 cost and linear outer approximations of theory-of-mind sets.
- Mechanism: The containment constraint T(D†) ⊆ U(π†; ι) becomes a set of linear inequalities when both sets are polytopes, allowing optimization of the L1 poisoning cost subject to these constraints.
- Core assumption: The theory-of-mind set outer approximation is linear in the reward values, and the unique Nash set can be characterized with linear inequalities.
- Break condition: When the theory-of-mind set cannot be approximated by a hypercube or the unique Nash set cannot be characterized linearly, the LP relaxation becomes invalid.

### Mechanism 3
- Claim: The attack is feasible when the confidence region radius is sufficiently small relative to the reward bounds.
- Mechanism: Theorem 3 provides sufficient conditions (ρ(Q) ≤ b - ι/4H) ensuring that the poisoned dataset's estimated Q-functions maintain the target policy as strict Nash equilibrium across all stage games.
- Core assumption: The original dataset has sufficient state-action coverage (Nh(s,a) > 0 for relevant states/actions) and the confidence radius scales appropriately with horizon length.
- Break condition: When confidence radii exceed the threshold b - ι/4H, the strict Nash equilibrium property cannot be guaranteed across the entire confidence region.

## Foundational Learning

- Concept: Markov games and Nash equilibria
  - Why needed here: The entire attack framework relies on understanding how multiple agents interact in sequential decision-making settings and what constitutes equilibrium behavior.
  - Quick check question: What is the difference between a Nash equilibrium and a Markov perfect equilibrium in the context of sequential games?

- Concept: Polytope geometry and linear programming
  - Why needed here: The unique Nash set is characterized as a polytope, and the attack optimization problem is converted to a linear program, requiring understanding of convex sets and LP duality.
  - Quick check question: Why does containment of one polytope within another translate to a set of linear inequalities?

- Concept: Confidence regions and statistical estimation
  - Why needed here: The attacker must reason about what Q-function estimates the victims will compute from poisoned data, which depends on statistical properties of the estimation process.
  - Quick check question: How does the size of confidence regions around estimated Q-functions affect the feasibility of the poisoning attack?

## Architecture Onboarding

- Component map: Dataset -> Poisoned dataset -> Estimated Q-region -> Containment check -> Policy convergence
- Critical path: Dataset → Poisoned dataset → Estimated Q-region → Containment check → Policy convergence
- Design tradeoffs:
  - Attack cost vs. feasibility: Larger poisoning modifications increase feasibility but also cost
  - Confidence region tightness vs. robustness: Tighter regions make attacks more precise but may be unrealistic
  - Computational complexity vs. accuracy: Exact polytope containment is expensive; LP relaxation is efficient but may be conservative
- Failure signatures:
  - Attack infeasible: Confidence region too large to fit in unique Nash polytope
  - Attack ineffective: MARL agents converge to different equilibrium than target
  - Computational issues: LP becomes too large for practical solving
- First 3 experiments:
  1. Verify polytope containment for a simple 2x2 stage game with known Q-functions
  2. Test LP relaxation accuracy against exact containment checking
  3. Evaluate attack success rate vs. confidence region size on synthetic Markov games

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the attack's feasibility change when the attacker's confidence regions are larger than the bound specified in Theorem 3?
- Basis in paper: [explicit] The paper states in Theorem 3 that the attack is feasible if the attacker's confidence regions are sufficiently small (specifically, ρ(Q)h(s,a) ≤ b − ι/(4H)).
- Why unresolved: The theorem provides a sufficient but not necessary condition, and the paper does not explore what happens when this bound is violated.
- What evidence would resolve it: Numerical experiments showing the attack's success rate as a function of the size of the attacker's confidence regions would help understand the attack's robustness to larger confidence regions.

### Open Question 2
- Question: Can the linear program approach be extended to compute the optimal poisoning attack for general-sum Markov games?
- Basis in paper: [inferred] The paper focuses on zero-sum Markov games and mentions that the results could potentially be extended to general-sum games in the discussions section.
- Why unresolved: The paper does not provide a concrete extension or algorithm for general-sum games.
- What evidence would resolve it: A formal proof or algorithm showing how to extend the linear program approach to general-sum games would resolve this question.

### Open Question 3
- Question: How does the attack's effectiveness change when the victim agents use model-free reinforcement learning algorithms instead of model-based ones?
- Basis in paper: [explicit] The paper mentions that the Q-function space accommodates both model-based and model-free victims, but does not provide a detailed analysis of the attack's effectiveness against model-free algorithms.
- Why unresolved: The paper does not provide a detailed analysis of the attack's effectiveness against model-free algorithms, which are widely used in practice.
- What evidence would resolve it: Empirical results comparing the attack's effectiveness against model-based and model-free agents would help understand the attack's robustness to different learning algorithms.

## Limitations
- Strong assumption that attacker's confidence regions can be made sufficiently small through linear outer approximation
- Requires perfect control over reward values in the dataset, which may be unrealistic in some data collection scenarios
- Attack feasibility depends on strict conditions (ρ(Q) ≤ b - ι/4H) that may not hold with limited data or high uncertainty

## Confidence
- **High confidence**: The mathematical formulation of the unique Nash set as a polytope and its role in characterizing games with unique Nash equilibria. The linear programming relaxation approach is sound when assumptions hold.
- **Medium confidence**: The feasibility conditions for attacks and the theoretical guarantees about policy convergence. These depend on multiple interacting assumptions about confidence regions, dataset coverage, and the strictness parameter ι.
- **Low confidence**: The practical effectiveness of the attack in real-world scenarios where confidence regions may be large, data coverage may be incomplete, and attackers may have limited control over reward values.

## Next Checks
1. **Feasibility condition verification**: Systematically test the ρ(Q) ≤ b - ι/4H condition across different Markov games and dataset sizes to understand when attacks become infeasible due to large confidence regions.

2. **LP relaxation accuracy**: Compare the solutions from the linear program relaxation against exact polytope containment checking for small-scale examples to quantify the conservatism introduced by the approximation.

3. **Robustness to dataset coverage**: Evaluate attack success rates when the original dataset has missing or sparse state-action pairs, as the current formulation assumes Nh(s,a) > 0 for all relevant pairs.