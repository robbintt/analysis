---
ver: rpa2
title: Knowledge-enhanced Agents for Interactive Text Games
arxiv_id: '2305.05091'
source_url: https://arxiv.org/abs/2305.05091
tags:
- task
- knowledge
- which
- agent
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the integration of domain knowledge in
  text-based game agents to enhance their performance. The authors propose a framework
  that injects two types of knowledge: historical knowledge (memory of previous correct
  actions) and affordances of relevant objects in the environment.'
---

# Knowledge-enhanced Agents for Interactive Text Games

## Quick Facts
- arXiv ID: 2305.05091
- Source URL: https://arxiv.org/abs/2305.05091
- Reference count: 40
- Key outcome: Injecting domain knowledge (affordances and memory of correct actions) improves performance of text-based game agents across three model architectures

## Executive Summary
This paper investigates how injecting domain knowledge into learning-based agents improves their performance in interactive text games. The authors propose two forms of knowledge injection: memory of previous correct actions (MCA) and affordances of relevant objects derived from ConceptNet. They evaluate three model architectures (DRRN, KG-A2C, and RoBERTa) across ten ScienceWorld tasks, demonstrating that knowledge injection consistently improves performance, with RoBERTa showing particularly strong gains from affordance knowledge.

## Method Summary
The paper injects two forms of domain knowledge into three model architectures: Deep Reinforcement Relevance Network (DRRN), Knowledge-augmented Actor-Critic (KG-A2C), and Language Model (RoBERTa). Memory of correct actions is encoded via GRU and fed as input to all models. Affordances are derived from ConceptNet's capableOf and usedFor relations and injected either as input (for DRRN and RoBERTa) or as knowledge graph triples (for KG-A2C). Models are trained on ScienceWorld tasks with RL models trained for 40,000 steps and language models for 3 epochs, evaluated across three trial runs.

## Key Results
- All three models (DRRN, KG-A2C, RoBERTa) show improved performance when affordances and MCA are injected
- RoBERTa achieves perfect scores in some tasks when afforded with object affordances
- MCA improves learning by reinforcing successful action patterns and avoiding repetition of unsuccessful ones
- KG-A2C benefits from affordances via KG integration, with score improvement from 17.5 to 20.5 on certain tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Affordance knowledge improves action selection by constraining the search space of valid actions based on object capabilities.
- Mechanism: ConceptNet's capableOf and usedFor relations are injected as additional KG triples, enabling models to reason about what objects can do in the environment. This reduces ambiguity in action selection.
- Core assumption: The environment contains objects whose affordances align with ConceptNet's knowledge and that these are sufficient for reasoning in ScienceWorld tasks.
- Evidence anchors: [abstract]: "inject two forms of domain knowledge that we inject into learning-based agents: memory of previous correct actions and affordances of relevant objects in the environment."; [section]: "For this purpose, we use ConceptNet [30] and obtain its capableOf and usedFor relations for the objects in a given IF game episode."; [corpus]: Weak evidence - only one neighbor mentions ConceptNet in a similar context, and no clear affordance-based results are reported.

### Mechanism 2
- Claim: Memory of correct actions (MCA) improves learning by reinforcing successful strategies and avoiding repetition of unsuccessful ones.
- Mechanism: MCA stores sequences of past actions that led to positive rewards and feeds them as input to the model via GRU encoding. This helps the model remember and reuse successful action patterns.
- Core assumption: The game has a limited number of optimal action sequences that can be memorized and reused across similar states.
- Evidence anchors: [abstract]: "memory of previous correct actions...improves the performance of all three models across ten tasks"; [section]: "we preserve the memory of previous correct actions (MCA) taken by the agent as input for all our models."; [corpus]: No direct corpus evidence - none of the related papers mention MCA as a technique.

### Mechanism 3
- Claim: Language models benefit more from affordance injection than RL models because they can leverage semantic knowledge for reasoning.
- Mechanism: RoBERTa's pretraining on ConceptNet utilities via auxiliary QA tasks provides semantic grounding that helps it understand object affordances, leading to better action generation in tasks requiring object manipulation.
- Core assumption: The semantic knowledge in ConceptNet is compatible with the language model's existing knowledge and improves its reasoning about affordances.
- Evidence anchors: [abstract]: "The RoBERTa model, in particular, benefits significantly from affordance knowledge, achieving perfect scores in some cases."; [section]: "We use pretraining instead of simple concatenation for input due to the substantial volume of affordance knowledge triples"; [corpus]: Weak evidence - only general mentions of LMs in text games, no specific affordance integration results.

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: Text-based games are formulated as POMDPs where the agent must make decisions based on incomplete observations of the environment state.
  - Quick check question: What distinguishes a POMDP from a standard MDP in the context of text-based games?

- Concept: Reinforcement Learning with Large Action Spaces
  - Why needed here: Text-based games have combinatorially large action spaces (up to 200,000 possible actions), requiring specialized RL techniques like DRRN to handle action representation and Q-value estimation.
  - Quick check question: How does DRRN's separate GRU for action encoding help manage large action spaces?

- Concept: Knowledge Graph Embeddings and Graph Attention Networks
  - Why needed here: KG-A2C uses KGs to represent game states symbolically and GATs to aggregate information from neighboring nodes for reasoning about next actions.
  - Quick check question: What advantage does the KG representation provide over pure text-based state representation in KG-A2C?

## Architecture Onboarding

- Component map:
  - DRRN: Observation encoder (GRU) → Action encoder (GRU) → Q-value estimator (Linear layers) → Policy
  - KG-A2C: Text encoders (GRU) + KG encoder (GAT) + Score encoder → State info → Actor-Critic → Auxiliary losses
  - RoBERTa: Text encoders (Transformer) + Optional MCA/affordance inputs → Action scorer → Top-p sampling

- Critical path: Environment observation → State encoding → Action selection → Environment response → Reward accumulation → Policy update

- Design tradeoffs:
  - DRRN vs KG-A2C: DRRN is simpler but lacks symbolic reasoning; KG-A2C has better reasoning but higher complexity
  - RoBERTa vs RL: RoBERTa is offline and doesn't learn from environment interaction; RL models adapt but require more training data
  - Knowledge injection methods: Direct concatenation vs separate encoding vs KG integration have different impacts on model complexity and performance

- Failure signatures:
  - DRRN: Poor performance when action space is too large relative to training; struggles with tasks requiring long-term planning
  - KG-A2C: Degrades when KG contains irrelevant information (e.g., biology tasks with animal affordances like "bark")
  - RoBERTa: Limited by context window size; struggles with tasks requiring precise action sequences

- First 3 experiments:
  1. Run baseline DRRN on Task 1 (Matter) to establish performance floor and verify environment setup
  2. Implement MCA injection in DRRN and compare performance on Task 3 (Electricity) where memory of correct actions should help
  3. Add affordances to KG-A2C via KG integration and test on Task 4 (Classification) to verify the most effective injection method

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of knowledge injection strategies vary across different task complexities and lengths?
- Basis in paper: [explicit] The paper discusses how the effectiveness of affordances and memory of correct actions (MCA) varies across tasks, with some tasks benefiting more than others.
- Why unresolved: The paper provides granular performance per task but does not deeply analyze the relationship between task complexity/length and the effectiveness of knowledge injection.
- What evidence would resolve it: Detailed analysis of task-specific performance metrics, correlating task complexity and length with the success of knowledge injection strategies.

### Open Question 2
- Question: What is the impact of joint training across multiple tasks compared to training individual models per task?
- Basis in paper: [explicit] The paper mentions that training models jointly across tasks is a promising avenue for improving generalization, but notes that current experiments assume one model per task.
- Why unresolved: The paper does not explore joint training, leaving the potential benefits and challenges of this approach unexplored.
- What evidence would resolve it: Experimental results comparing joint training versus individual task training, focusing on performance and generalization metrics.

### Open Question 3
- Question: How do different methods of incorporating affordances (e.g., as input vs. as part of the knowledge graph) affect model performance?
- Basis in paper: [explicit] The paper discusses various methods of injecting affordances into models, including as input and as part of the knowledge graph, with some methods showing better performance.
- Why unresolved: While the paper provides initial comparisons, it does not fully explore the nuanced impacts of different incorporation methods.
- What evidence would resolve it: Comparative analysis of model performance using different affordance incorporation methods, with a focus on efficiency and accuracy.

### Open Question 4
- Question: How can large language models (LLMs) be further enhanced by integrating reinforcement learning (RL) policy networks?
- Basis in paper: [explicit] The paper suggests that RL models and LLMs have complementary strengths and weaknesses, proposing the integration of an RL policy network into LLMs as a future direction.
- Why unresolved: The paper does not provide experimental results or detailed methodologies for this integration.
- What evidence would resolve it: Experimental studies demonstrating the performance improvements of LLMs with integrated RL policy networks, including metrics on task completion and efficiency.

### Open Question 5
- Question: What role can few-shot prompting of large language models play in interactive tasks, and how can they generate synthetic data for knowledge distillation?
- Basis in paper: [explicit] The paper highlights the potential of few-shot prompting of LLMs in reasoning tasks and suggests exploring their role in interactive tasks for generating synthetic data.
- Why unresolved: The paper does not explore this potential, leaving the effectiveness and application of few-shot prompting in interactive tasks unexplored.
- What evidence would resolve it: Experimental results showing the effectiveness of few-shot prompting in interactive tasks, including the quality and utility of generated synthetic data for knowledge distillation.

## Limitations

- Limited evaluation scope to ScienceWorld environment raises questions about generalization to other text-based game domains
- Implementation details for knowledge injection methods are not fully specified, particularly for RoBERTa's pretraining with ConceptNet utilities
- No discussion of potential negative transfer effects when injecting irrelevant or incorrect knowledge into models

## Confidence

- High confidence: The general mechanism that knowledge injection improves performance - well-supported by consistent improvements across all three models and tasks
- Medium confidence: The specific claim that RoBERTa benefits more than RL models from affordance knowledge - limited comparison to single task without statistical significance
- Medium confidence: The memory mechanism - improvements demonstrated but lacks ablation studies to isolate MCA impact

## Next Checks

1. Implement ablation studies to isolate the individual contributions of affordances vs. MCA in each model architecture, particularly for RoBERTa where both are used together
2. Test the knowledge injection methods on a different text-based game environment (e.g., Jericho) to assess generalization beyond ScienceWorld
3. Conduct error analysis on KG-A2C's performance to quantify the impact of irrelevant ConceptNet knowledge (like animal affordances in biology tasks) on model performance