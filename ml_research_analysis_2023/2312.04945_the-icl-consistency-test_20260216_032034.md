---
ver: rpa2
title: The ICL Consistency Test
arxiv_id: '2312.04945'
source_url: https://arxiv.org/abs/2312.04945
tags:
- consistency
- instructions
- test
- factors
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the ICL Consistency Test, a benchmark for
  evaluating the robustness of large language models (LLMs) in few-shot in-context
  learning (ICL) settings. The test measures how consistently models make predictions
  across 96 different setups using the same data, with setups varying in factors like
  the number of in-context examples, instruction templates, and label distributions.
---

# The ICL Consistency Test

## Quick Facts
- arXiv ID: 2312.04945
- Source URL: https://arxiv.org/abs/2312.04945
- Reference count: 25
- State-of-the-art LLMs exhibit up to 40% accuracy variation across different prompting setups

## Executive Summary
This paper introduces the ICL Consistency Test, a benchmark for evaluating the robustness of large language models in few-shot in-context learning settings. The test measures how consistently models make predictions across 96 different prompting setups using the same data, with variations in factors like number of examples, instruction templates, and label distributions. Using datasets like ANLI and MNLI, the authors find that state-of-the-art LLMs, including LLaMA and Alpaca models, exhibit poor consistency with accuracy varying by up to 40% depending on the setup. The results reveal that all evaluated models are susceptible to changes in setup, with larger and instruction-tuned models showing slightly better robustness.

## Method Summary
The ICL Consistency Test constructs 96 different prompting setups by combining binary factors such as n-shots, instruction templates, and label distributions. For each setup, models are evaluated on the same 600 data points from ANLI and MNLI validation sets. Consistency is measured using Cohen's kappa to quantify agreement between predictions across setups differing in specific factors. The test also examines main effects of each factor through linear regression. Models are evaluated using greedy sampling from their probability distribution over possible labels.

## Key Results
- State-of-the-art LLMs exhibit poor consistency, with accuracy varying by up to 40% across different setups
- Larger and instruction-tuned models show slightly better robustness, but all models remain susceptible to setup changes
- Models demonstrate significantly lower consistency in cross-template and cross-task settings compared to within-template variations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ICL Consistency Test works by quantifying how stable model predictions are across multiple prompting setups.
- Mechanism: It constructs 96 different setups by combining binary factors (e.g., number of examples, instruction templates, label distributions), evaluates the model on the same 600 data points across all setups, and computes Cohen's kappa to measure agreement in predictions between setups differing in a specific factor.
- Core assumption: Model predictions should be invariant to superficial changes in the prompt if the underlying task remains the same.
- Evidence anchors:
  - [abstract] "The test measures how consistently models make predictions across 96 different setups using the same data, with setups varying in factors like the number of in-context examples, instruction templates, and label distributions."
  - [section 3.4] "We measure the consistency of model predictions using Cohen's Îº (Cohen, 1960), a measure of interrater agreement adjusted for agreement by chance."
  - [corpus] Weak - only mentions related work on consistency evaluation but not direct evidence for kappa-based consistency measurement.
- Break condition: If the model uses irrelevant features in the prompt to make predictions, consistency will be low across setups.

### Mechanism 2
- Claim: Larger and instruction-tuned models show better consistency because they learn to ignore irrelevant prompt variations.
- Mechanism: Instruction tuning explicitly teaches models to follow task instructions rather than surface patterns, and larger models have more capacity to learn robust representations that generalize across setups.
- Core assumption: Instruction tuning and model scale improve the model's ability to focus on task-relevant information and ignore prompt artifacts.
- Evidence anchors:
  - [abstract] "The results reveal that all evaluated models are susceptible to changes in setup, with larger and instruction-tuned models showing slightly better robustness."
  - [section 4.2] "Interestingly, the consistency of a model improves for IT models with increasing parameter count, while additional parameters do not improve consistency for vanilla models."
  - [corpus] Weak - mentions related work on instruction tuning but not specific evidence for consistency improvement.
- Break condition: If instruction tuning or scale does not teach the model to focus on task semantics, consistency gains will not materialize.

### Mechanism 3
- Claim: The test reveals non-robust generalization by showing that model accuracy varies significantly across setups.
- Mechanism: By evaluating the same data across many setups, the test exposes cases where the model's accuracy drops due to irrelevant prompt changes, indicating it hasn't learned the true task distribution.
- Core assumption: A model that generalizes robustly should maintain similar accuracy across different but equivalent prompt setups.
- Evidence anchors:
  - [abstract] "the authors find that state-of-the-art LLMs... exhibit poor consistency, with their prediction accuracy varying by up to 40% depending on the setup."
  - [section 4.2] "The spread of accuracy scores across different setups is very high, indicating that model predictions are inconsistent and depend on the exact setup in which the models are evaluated."
  - [corpus] Weak - related papers discuss consistency but not direct evidence for accuracy variance as a robustness indicator.
- Break condition: If the model learns to rely on prompt-specific cues rather than task semantics, accuracy variance will be high.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: The test is specifically designed to evaluate robustness in ICL settings where models adapt to tasks via prompt examples rather than parameter updates.
  - Quick check question: What distinguishes few-shot ICL from zero-shot ICL in terms of prompt construction?
- Concept: Cohen's kappa statistic
  - Why needed here: Kappa measures inter-rater agreement while adjusting for chance agreement, making it suitable for quantifying prediction consistency across setups.
  - Quick check question: How does Cohen's kappa differ from simple accuracy when comparing predictions across different conditions?
- Concept: Binary factor design
  - Why needed here: The test systematically varies prompt factors in all possible combinations to isolate their effects on model consistency.
  - Quick check question: Why is it important that each factor in the test is binary rather than continuous?

## Architecture Onboarding

- Component map: Data preprocessing -> Prompt construction -> Model evaluation -> Consistency metric calculation
- Critical path:
  1. Load data and instruction templates
  2. Generate all 96 setup prompts for each data point
  3. Run model predictions for each setup
  4. Calculate consistency metrics (kappa and main effects)
- Design tradeoffs:
  - Fixed dataset size vs. ability to detect consistency patterns
  - Limited factor set vs. computational feasibility
  - Binary factors vs. ability to capture continuous effects
- Failure signatures:
  - Low kappa across all factors: Model is highly sensitive to prompt variations
  - High kappa only for certain factors: Model robustness is factor-specific
  - No correlation between accuracy and consistency: Model may be using different strategies across setups
- First 3 experiments:
  1. Run the test on a small subset of data with a single model to verify prompt generation
  2. Compare consistency between two similar instruction templates to validate kappa calculation
  3. Test the effect of adding a custom factor (e.g., instruction tuning) on consistency scores

## Open Questions the Paper Calls Out
The authors acknowledge that the test is currently only implemented for NLI tasks and note this as a limitation, stating "the test is currently only implemented for a single type of task (natural language inference), and model consistency might differ in other types of classification tasks or more open-ended answering formats such as question answering." They also observe that "consistency of a model improves for IT models with increasing parameter count, while additional parameters do not improve consistency for vanilla models," suggesting instruction tuning is especially effective for larger models. The authors note that models are surprisingly robust to semantic-invariant changes in in-context instructions despite substantial changes in surface form, but don't systematically explore how different template choices within the same performance tier affect consistency.

## Limitations
- Binary factor design may oversimplify complex interactions between prompt elements
- Evaluation limited to only two datasets (ANLI and MNLI), potentially missing broader patterns
- Does not investigate why certain factors cause larger drops in consistency or what internal mechanisms drive variations
- Results may not generalize to other task types beyond natural language inference

## Confidence
- High confidence: The methodology for constructing 96 setups and measuring consistency using Cohen's kappa is well-specified and reproducible. The observation that model accuracy varies significantly across setups is supported by clear quantitative evidence.
- Medium confidence: The conclusion that larger and instruction-tuned models show slightly better consistency is supported by the data, but the effect sizes are small and the underlying reasons are not thoroughly explored.
- Low confidence: Claims about the fundamental limitations of current LLMs in few-shot learning are extrapolated from a limited set of datasets and may not generalize to broader ICL applications.

## Next Checks
1. Systematically investigate how multiple factors combine to affect consistency, particularly examining non-additive effects between instruction templates and n-shot variations
2. Apply the ICL Consistency Test to additional datasets spanning different task types (e.g., summarization, question answering) to assess whether consistency patterns are task-specific or universal
3. Conduct a detailed examination of prediction errors across setups to determine whether models consistently fail on the same examples or if errors are setup-dependent, providing insight into the nature of the inconsistency