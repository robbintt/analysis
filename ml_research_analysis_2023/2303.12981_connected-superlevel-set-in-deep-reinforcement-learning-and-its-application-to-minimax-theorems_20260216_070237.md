---
ver: rpa2
title: Connected Superlevel Set in (Deep) Reinforcement Learning and its Application
  to Minimax Theorems
arxiv_id: '2303.12981'
source_url: https://arxiv.org/abs/2303.12981
tags:
- policy
- optimization
- learning
- connected
- minimax
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies the optimization landscape of policy optimization
  problems in reinforcement learning (RL) and derives minimax theorems for robust
  RL. The key contributions are: Proving that the superlevel set of the policy optimization
  objective is always connected in both the tabular setting and under policies represented
  by a class of neural networks.'
---

# Connected Superlevel Set in (Deep) Reinforcement Learning and its Application to Minimax Theorems

## Quick Facts
- arXiv ID: 2303.12981
- Source URL: https://arxiv.org/abs/2303.12981
- Reference count: 40
- Key outcome: Proves that superlevel sets of policy optimization objectives are always connected in both tabular and neural network settings, enabling minimax theorems for robust RL under adversarial reward attack

## Executive Summary
This paper studies the optimization landscape of policy optimization problems in reinforcement learning (RL) and derives minimax theorems for robust RL. The authors prove that the superlevel set of the policy optimization objective is always connected under both tabular and neural network policy representations, and that the objective satisfies a stronger "equiconnectedness" property. These theoretical results are then applied to show that a robust RL problem under adversarial reward attack has a minimax equality, meaning it has a Nash equilibrium. The work provides new insights into the optimization landscape of RL and has important implications for designing and analyzing algorithms for solving minimax optimization problems in robust RL.

## Method Summary
The paper employs theoretical analysis to study the optimization landscape of policy optimization in reinforcement learning. It uses gradient domination conditions and minimax theorems to derive results about the connectedness of superlevel sets in both tabular and neural network policy settings. The analysis relies on the linearity of the policy optimization objective in state-action stationary distributions and the properties of Markov chains under different policies. The authors then apply these results to establish minimax theorems for robust RL under adversarial reward attack by showing that the problem satisfies the required conditions for minimax equality.

## Key Results
- Proves superlevel sets of policy optimization objectives are always connected in both tabular and neural network policy settings
- Establishes the equiconnectedness property of the optimization objective as a function of policy parameter and reward
- Derives minimax theorems for robust RL under adversarial reward attack, showing the problem has a Nash equilibrium
- Demonstrates that the connectedness result can be applied to design more efficient and reliable policy optimization algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The superlevel set of the policy optimization objective is always connected under both tabular and neural network policy representations.
- Mechanism: The policy optimization objective is linear in the state-action stationary distribution. By constructing a continuous path between two policies through a convex combination of their stationary distributions, the objective value along this path remains above the threshold, preserving connectedness.
- Core assumption: The Markov chain under any policy is ergodic (Assumption 1), ensuring a unique stationary distribution that is strictly positive.
- Evidence anchors:
  - [abstract] "we show that the superlevel set of the objective function with respect to the policy parameter is always a connected set both in the tabular setting and under policies represented by a class of neural networks"
  - [section] "To prove that the superlevel set is connected, we show that for any λ ∈ R and π1, π2 ∈ Uλ,r, there exists a continuous path map p : [0, 1] → Uλ,r such that p(0) = π1 and p(1) = π2"
  - [corpus] Weak corpus support - no direct evidence found in neighbor papers

### Mechanism 2
- Claim: The optimization objective as a function of policy parameter and reward satisfies equiconnectedness.
- Mechanism: The path constructed to prove connectedness of superlevel sets does not depend on the specific reward function. This means the same path works for all reward functions, establishing equiconnectedness.
- Core assumption: The construction of the path map is independent of the reward function, which is verified by showing the path depends only on stationary distributions.
- Evidence anchors:
  - [abstract] "we show that the optimization objective as a function of the policy parameter and reward satisfies a stronger 'equiconnectedness' property"
  - [section] "The claim on the equiconnectedness of {Jr}r∈R|S|×|A| is again stronger than the connectedness of UΩλ,r and needs to be derived for the application to minimax theorems"
  - [corpus] No direct evidence in neighbor papers - this appears to be a novel theoretical contribution

### Mechanism 3
- Claim: The minimax equality holds for robust RL under adversarial reward attack due to equiconnectedness.
- Mechanism: By Theorem 3 (adapted from Simons 1995), if one side of a minimax problem is convex and the other side has an equiconnected objective, the minimax equality holds. The reward poisoning problem satisfies these conditions.
- Core assumption: The reward poisoning attack problem has a convex constraint set (verified in Appendix D) and the policy optimization objective is equiconnected.
- Evidence anchors:
  - [abstract] "We show that any minimax optimization program which is convex on one side and is equiconnected on the other side observes the minimax equality"
  - [section] "The reward poisoning attack considered in Banihashem et al. [2021], which can be formulated as a convex-nonconcave minimax optimization program"
  - [corpus] Weak support - neighbor papers mention minimax optimization but not this specific application

## Foundational Learning

- Concept: Connected sets and path-connectedness
  - Why needed here: The paper's main contribution relies on proving that certain sets (superlevel sets) are connected, which requires understanding the topological definition of connectedness and how to construct continuous paths between points.
  - Quick check question: Can you construct a continuous path between any two points in a circle? What about in two separate circles?

- Concept: Stationary distributions in Markov chains
  - Why needed here: The proof of connectedness relies on the existence and properties of stationary distributions under different policies, which are fundamental to understanding how policies relate to each other in the state space.
  - Quick check question: What conditions ensure a Markov chain has a unique stationary distribution? How do you compute it?

- Concept: Minimax theorems and Nash equilibria
  - Why needed here: The application to robust RL uses a minimax theorem to establish the existence of Nash equilibria, which requires understanding the conditions under which minimax equalities hold.
  - Quick check question: What is the difference between a Nash equilibrium and a saddle point? Under what conditions does the minimax equality hold?

## Architecture Onboarding

- Component map:
  - Markov Decision Process (MDP) framework: States, actions, transition probabilities, reward function
  - Policy representation: Tabular policies (probability distributions over actions) and neural network policies (parameterized by weights)
  - Superlevel sets: Sets of policies achieving objective value above a threshold
  - Equiconnectedness: Property ensuring the same path works for all reward functions
  - Minimax formulation: Robust RL problem with adversarial reward attack

- Critical path: The main theoretical contribution flows through: (1) Proving connectedness of superlevel sets in tabular case → (2) Extending to neural network policies → (3) Establishing equiconnectedness → (4) Applying to derive minimax theorem for robust RL

- Design tradeoffs: The neural network policy representation requires over-parameterization assumptions (width decreasing with depth, sufficient width in first layer) to prove connectedness. These assumptions make the result theoretically interesting but potentially impractical for large state spaces.

- Failure signatures: If the Markov chain is not ergodic, the stationary distribution may not exist or be unique, breaking the connectedness proof. If the neural network doesn't satisfy the over-parameterization assumptions, the connectedness result may not hold.

- First 3 experiments:
  1. Verify connectedness empirically: Sample two policies with objective values above threshold and attempt to construct a continuous path between them, checking objective values along the path.
  2. Test equiconnectedness: For a fixed pair of policies, construct the path and verify it works for multiple different reward functions.
  3. Apply to robust RL: Implement the reward poisoning attack and verify the minimax equality holds empirically by comparing max-min and min-max values.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the connectedness of superlevel sets in reinforcement learning extend to partially observable Markov decision processes (POMDPs)?
- Basis in paper: [inferred] The paper mentions in the conclusion that it is interesting to study whether the policy optimization problem under a partially observable MDP can be shown to have disconnected superlevel sets, analogous to the difference between LQR and partially observable LQR.
- Why unresolved: The paper focuses on MDPs and does not investigate POMDPs.
- What evidence would resolve it: Proving that the superlevel sets of the policy optimization objective under a POMDP framework are either connected or disconnected.

### Open Question 2
- Question: Can the result on connected superlevel sets be exploited to design more efficient and reliable policy optimization algorithms in reinforcement learning?
- Basis in paper: [explicit] The paper mentions in the introduction that it is unclear at the moment, but certainly possible, that the result on connected superlevel sets may be exploited to design more efficient and reliable policy optimization algorithms in the future.
- Why unresolved: The paper does not explore algorithmic implications of the connectedness result.
- What evidence would resolve it: Developing and demonstrating new policy optimization algorithms that leverage the connectedness of superlevel sets to improve efficiency or reliability.

### Open Question 3
- Question: Can the assumptions on neural network architecture (over-parameterization and decreasing width) in Theorem 2 be relaxed or removed?
- Basis in paper: [inferred] The paper mentions that similar over-parameterization assumptions are critical and very common in most existing works on the theory of neural networks, and that ongoing work seeks to relax or remove this assumption.
- Why unresolved: The current proof of Theorem 2 relies on these assumptions.
- What evidence would resolve it: Proving that the connectedness of superlevel sets still holds under a less restrictive neural network architecture, or identifying a counterexample where the result fails without these assumptions.

## Limitations
- The ergodicity assumption (Assumption 1) requires the Markov chain under any policy to be ergodic, which may not hold in practice, especially with function approximation
- The neural network over-parameterization assumption (Assumption 3) requires the network to be sufficiently wide, with width decreasing as depth increases - this is restrictive and may not reflect practical architectures
- The proof technique relies heavily on the linearity of the objective in stationary distributions, which may not generalize to other RL objectives

## Confidence
- Connectedness of superlevel sets (Tabular): High confidence - proof is constructive and relies on well-established Markov chain theory
- Connectedness with neural networks: Medium confidence - depends on strong over-parameterization assumptions that may be difficult to verify
- Equiconnectedness property: Medium confidence - novel theoretical contribution with limited external validation
- Minimax theorem application: Medium confidence - application is specific to the reward poisoning setting and relies on convexity assumptions

## Next Checks
1. **Empirical verification of connectedness**: Implement a simple MDP and verify empirically that for any two policies above a threshold, a continuous path can be constructed maintaining objective values above threshold. Test both tabular and small neural network policies.

2. **Assumption relaxation study**: Systematically test how violations of Assumptions 1 and 3 affect the connectedness property. For example, test with non-ergodic MDPs or smaller neural networks to understand the boundary conditions.

3. **Generalization to other objectives**: Apply the theoretical framework to different RL objectives (e.g., entropy-regularized RL, variance-reduced objectives) to test whether the connectedness property extends beyond the specific linear objective considered.