---
ver: rpa2
title: 'PromptRE: Weakly-Supervised Document-Level Relation Extraction via Prompting-Based
  Data Programming'
arxiv_id: '2310.09265'
source_url: https://arxiv.org/abs/2310.09265
tags:
- relation
- extraction
- document-level
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of weakly-supervised document-level
  relation extraction, where the goal is to classify relationships between entity
  pairs in long documents without extensive human annotation. The proposed PromptRE
  method combines large language model prompting with data programming, leveraging
  entity-oriented document summarization via ChatGPT, relation prediction through
  both relation-specific and open-ended prompting, and weak supervision via data programming
  to handle the "no relation" problem.
---

# PromptRE: Weakly-Supervised Document-Level Relation Extraction via Prompting-Based Data Programming

## Quick Facts
- arXiv ID: 2310.09265
- Source URL: https://arxiv.org/abs/2310.09265
- Reference count: 17
- Key outcome: PromptRE achieves 10.2 F1 score on ReDocRED development set, outperforming baselines in weakly-supervised document-level relation extraction

## Executive Summary
This paper introduces PromptRE, a method for weakly-supervised document-level relation extraction that combines large language model prompting with data programming. The approach addresses the challenge of classifying relationships between entity pairs in long documents without extensive human annotation. PromptRE leverages ChatGPT for entity-oriented document summarization, uses multiple prompting strategies for relation prediction, and employs data programming to effectively handle the "no relation" problem. Experimental results on the ReDocRED dataset demonstrate significant performance improvements over baseline approaches.

## Method Summary
PromptRE is a weakly-supervised document-level relation extraction method that combines large language model prompting with data programming. The approach involves three main steps: (1) entity-oriented document summarization using ChatGPT to reduce noise from irrelevant content, (2) relation prediction through both relation-specific and open-ended prompting strategies, and (3) weak supervision via data programming that combines multiple sources of supervision to address the "no relation" problem. The method incorporates relation type distribution and entity types as prior knowledge to improve performance. PromptRE operates by first summarizing documents around the two entities of interest, then generating relation predictions using different prompting approaches, and finally combining these predictions through a data programming framework to produce final relation classifications.

## Key Results
- Achieves 10.2 F1 score on ReDocRED development set, outperforming baseline approaches
- Effectively handles the "no relation" problem through data programming with multiple weak supervision sources
- Demonstrates improved performance in relation classification by incorporating relation type distribution as prior knowledge
- Reveals that existing document-level relation extraction datasets suffer from label incompleteness

## Why This Works (Mechanism)

### Mechanism 1
Using ChatGPT for entity-oriented document summarization improves relation extraction performance by reducing noise from irrelevant document content. ChatGPT generates concise summaries focused on the two entities of interest, creating a condensed context that highlights relevant information while filtering out unrelated content from long documents. This works under the assumption that the summary captures all necessary information for determining the relationship between the two entities. The approach could fail if the summary omits critical information needed to determine the relationship, or if the summarization introduces errors or hallucinations that mislead the relation prediction.

### Mechanism 2
Combining multiple sources of weak supervision through data programming effectively addresses the "no relation" problem in document-level relation extraction. Data programming integrates different signals (relation existence logits, relation-specific logits, and entity-relation similarity scores) to create a more reliable prediction by identifying which predictions are most confident and should be preserved. This works under the assumption that the different weak supervision sources provide complementary information that, when combined, yields better predictions than any single source alone. The approach could fail if the weak supervision sources are highly correlated or if one source dominates the others, preventing the ensemble from providing the expected benefits.

### Mechanism 3
Incorporating relation type distribution as prior knowledge improves weakly-supervised relation extraction performance. The known distribution of relations given entity types is used to weight the predicted probabilities, giving higher scores to relations that are more likely given the entity types. This works under the assumption that the relation type distribution is accurate and provides meaningful priors for the prediction task. The approach could fail if the relation type distribution is inaccurate or if it over-constrains the predictions, preventing the model from discovering valid but less common relationships.

## Foundational Learning

- **Document-level relation extraction**: Unlike sentence-level RE, document-level RE requires reasoning across multiple sentences and handling multiple mentions of entities, making it more complex and requiring different approaches. Quick check: What is the key difference between document-level and sentence-level relation extraction that makes the former more challenging?

- **Data programming**: Data programming provides a framework for combining multiple noisy labeling functions to create higher-quality pseudo-labels without requiring extensive manual annotation. Quick check: How does data programming address the challenge of having too many "no relation" instances in weakly-supervised document-level relation extraction?

- **Prompting-based techniques**: Large language models can be effectively used for relation extraction through carefully designed prompts, especially when combined with other techniques like data programming and entity summarization. Quick check: Why might prompting be particularly useful for document-level relation extraction compared to traditional supervised methods?

## Architecture Onboarding

- **Component map**: Document → Entity summarization → Relation prediction → Data programming aggregation → Final prediction
- **Critical path**: The pipeline flows from raw document through ChatGPT summarization, multiple prompting strategies for relation prediction, data programming ensemble, and final classification output
- **Design tradeoffs**: Using ChatGPT for summarization provides better performance but adds cost and dependency on a proprietary model; multiple prompting strategies provide complementary information but increase computational cost; assuming relation type distribution improves performance but requires domain knowledge that may not always be available
- **Failure signatures**: Poor performance on documents where critical information is missing from the ChatGPT summary; over-reliance on the relation type distribution leading to missed valid but uncommon relationships; data programming fails to effectively filter false positives if weak supervision sources are not sufficiently diverse
- **First 3 experiments**: 1) Compare performance with and without ChatGPT entity summarization to quantify the benefit of preprocessing; 2) Test different combinations of weak supervision sources in the data programming framework to find the most effective ensemble; 3) Vary the amount of relation type distribution information (1%, 10%, 25%, 50%, 100% of documents) to understand how much domain knowledge is needed for optimal performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the incompleteness of document-level relation extraction datasets impact the performance of weakly-supervised methods, and what strategies can be developed to mitigate this issue? The paper acknowledges that existing document-level relation extraction datasets suffer from label incompleteness, which poses additional challenges for weakly-supervised methods. This question remains unresolved as the paper only suggests that future work could improve existing datasets without providing a concrete solution. Evidence that would resolve this includes experiments comparing performance on datasets with varying levels of completeness and the development of techniques to infer or generate missing labels.

### Open Question 2
What is the optimal way to combine multiple sources of weak supervision in data programming for document-level relation extraction, and how can the quality of these sources be evaluated? The paper investigates combining three sources of weak supervision using data programming but does not provide a systematic method for determining the optimal combination or evaluating source quality. This remains unresolved as the paper only shows that combining these sources improves performance compared to using them individually. Evidence that would resolve this includes a comprehensive study comparing different combinations of weak supervision sources and the development of metrics to assess the quality and reliability of each source.

### Open Question 3
How can the performance of weakly-supervised document-level relation extraction methods be improved when only a small set of labeled examples is available? The paper mentions that it is usually possible to query human experts for a few examples and suggests researching ways to take maximum advantage of a small set of labels. This question remains unresolved as the paper does not explore the few-shot learning scenario or propose specific techniques for leveraging limited labeled data. Evidence that would resolve this includes experiments comparing performance with and without a small set of labeled examples and the development of techniques to effectively utilize limited labeled data for fine-tuning or guiding the relation extraction process.

## Limitations

- Evaluation relies entirely on the ReDocRED dataset, which has known label incompleteness issues that may affect the validity of performance improvements
- Dependency on ChatGPT for entity summarization introduces cost and reproducibility concerns, as performance may vary with different summarization models or without access to proprietary APIs
- Assumes access to relation type distributions and entity types as prior knowledge, which may not be readily available in many real-world applications

## Confidence

- **High confidence**: The overall methodology combining prompting with data programming is sound and well-motivated for weakly-supervised settings
- **Medium confidence**: The specific implementation details of the data programming framework and how different weak supervision sources are weighted and combined
- **Low confidence**: The generalizability of results beyond the ReDocRED dataset, given its known label incompleteness issues

## Next Checks

1. **Dataset robustness check**: Evaluate PromptRE on an alternative document-level relation extraction dataset (such as DocRED or DWIE) to verify whether performance gains hold across different data sources and whether results are dataset-specific artifacts

2. **Ablation study on weak supervision**: Systematically test different combinations and weighting schemes of the weak supervision sources (relation existence logits, relation-specific logits, cosine similarities) to quantify their individual contributions and optimize the data programming ensemble

3. **Label completeness analysis**: Conduct an analysis of how label incompleteness in ReDocRED affects the evaluation by comparing PromptRE's performance on fully-labeled subsets versus the full dataset, and potentially conducting human evaluation on a sample of instances to assess true performance versus apparent performance