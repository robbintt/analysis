---
ver: rpa2
title: Supervised Knowledge Makes Large Language Models Better In-context Learners
arxiv_id: '2312.15918'
source_url: https://arxiv.org/abs/2312.15918
tags:
- arxiv
- llms
- in-context
- supercontext
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SuperContext, a simple yet effective framework
  that enhances the generalizability and factuality of large language models (LLMs)
  in natural language understanding (NLU) and question answering (QA) tasks. The core
  idea is to leverage the predictions and confidence scores from task-specific fine-tuned
  models (SLMs) as auxiliary knowledge during the inference stage of LLMs.
---

# Supervised Knowledge Makes Large Language Models Better In-context Learners

## Quick Facts
- arXiv ID: 2312.15918
- Source URL: https://arxiv.org/abs/2312.15918
- Reference count: 15
- LLMs improved for NLU and QA tasks using supervised knowledge from fine-tuned models

## Executive Summary
This paper introduces SuperContext, a framework that enhances large language models' (LLMs) performance in natural language understanding and question answering by leveraging predictions and confidence scores from task-specific fine-tuned models (SLMs). The approach addresses limitations in LLM generalization and factuality by incorporating SLM outputs into prompts during inference. Evaluated across 16 datasets spanning 9 distinct tasks, SuperContext demonstrates significant improvements over both original LLMs and SLMs, particularly in out-of-distribution scenarios and hallucination reduction. The method is presented as cost-effective and generalizable, offering a practical solution for improving LLM reliability.

## Method Summary
SuperContext enhances LLM performance by integrating predictions and confidence scores from task-specific fine-tuned models (SLMs) into the inference process. The method involves fine-tuning SLMs on in-domain data, generating predictions and confidence scores for test examples, and constructing prompts that include this auxiliary knowledge alongside the original task instructions. During inference, the LLM processes these enriched prompts to produce final predictions. The framework supports both zero-shot and few-shot settings, with an optional interpretation module to analyze LLM reasoning. This approach aims to bridge the gap between LLMs and task-specific data, improving generalizability and reducing hallucinations.

## Key Results
- SuperContext achieves 80.05% average accuracy on GLUE-X benchmark, surpassing original ChatGPT (66.67%) and ELECTRA-large (79.86%)
- The method demonstrates superior out-of-distribution generalization across multiple tasks
- SuperContext effectively reduces hallucinations in question answering, particularly for unanswerable questions on SQuAD 2.0

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SuperContext improves out-of-distribution (OOD) generalization by integrating task-specific fine-tuned model predictions and confidence scores into LLM prompts.
- Mechanism: The small discriminative model (SLM) is fine-tuned on in-domain data, capturing task-specific knowledge. When an LLM encounters an OOD example, the SLM provides a prediction and confidence score that acts as a bridge between the LLM's broad pre-training and the specific task, reducing reliance on potentially unreliable in-context examples.
- Core assumption: SLMs trained on task-specific data have better generalization to OOD data than LLMs relying solely on in-context learning.
- Evidence anchors:
  - [abstract] "The core idea is to leverage the predictions and confidence scores from task-specific fine-tuned models (SLMs) as auxiliary knowledge during the inference stage of LLMs."
  - [section] "Using our proposed plug-in method, enhanced versions of Llama 2 and ChatGPT surpass their original versions regarding generalizability and factuality."
  - [corpus] Weak: Corpus mentions related work on in-context learning but does not provide direct evidence for the OOD generalization claim.
- Break condition: If the SLM's confidence score is unreliable or the SLM is also poor at generalizing to OOD data, the method's effectiveness diminishes.

### Mechanism 2
- Claim: The method reduces hallucinations in generative tasks by providing a discriminative model's prediction as a reference.
- Mechanism: In question answering tasks with unanswerable questions, the SLM's prediction and confidence guide the LLM to avoid generating incorrect answers. The SLM's output acts as a check against the LLM's tendency to hallucinate.
- Core assumption: SLMs are better at recognizing unanswerable questions than LLMs relying on in-context learning alone.
- Evidence anchors:
  - [abstract] "The second task is question answering containing unanswerable questions, where we underscore SuperContext capability to curtail hallucinations, addressing them through a discriminative-model-enhanced approach."
  - [section] "We evaluate LLMs' ability towards minimizing the hallucination on the QA task based on SQuAD 2.0... SuperContext achieves the best performance in terms of the EM and accuracy for no-answer questions."
  - [corpus] Weak: Corpus does not provide direct evidence for hallucination reduction; evidence is from the paper's experiments.
- Break condition: If the SLM is prone to hallucinations or the LLM ignores the SLM's reference, hallucination reduction will not occur.

### Mechanism 3
- Claim: Including the SLM's confidence score in the prompt improves LLM performance by providing a measure of reliability.
- Mechanism: The confidence score allows the LLM to weigh the SLM's prediction appropriately. High confidence suggests the SLM is more likely correct, influencing the LLM's final decision. The positive correlation between SLM confidence and LLM performance indicates this weighting is effective.
- Core assumption: LLMs can effectively use numerical confidence scores provided in the prompt to improve their predictions.
- Evidence anchors:
  - [section] "By comparing the results of SuperContext w/ and w/o confidence, we observe that including model confidence can bring significant improvements in the average performance for both ChatGPT and Llama2."
  - [section] "Figure 4: The correlation between the SLM confidence and LLM performance... both ChatGPT and Llama2-7B-chat demonstrate a positive correlation between SLMs' confidence and LLM performance."
  - [corpus] Weak: Corpus does not provide evidence for the effectiveness of including confidence scores; evidence is from the paper's experiments.
- Break condition: If the LLM does not interpret the confidence score correctly or if the confidence score is not calibrated, the performance benefit is lost.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: SuperContext builds upon ICL by augmenting it with SLM predictions. Understanding ICL is essential to grasp how SuperContext modifies the standard approach.
  - Quick check question: What is the key difference between traditional ICL and SuperContext's approach to providing examples to the LLM?

- Concept: Out-of-distribution (OOD) generalization
  - Why needed here: The paper focuses on improving LLM performance on OOD data. Understanding OOD is crucial to appreciate the problem SuperContext addresses.
  - Quick check question: Why might an LLM trained on one domain struggle when applied to a different domain without additional adaptation?

- Concept: Discriminative vs. Generative models
  - Why needed here: SuperContext uses a discriminative model (SLM) to enhance a generative model (LLM). Knowing the difference helps understand why the combination is effective.
  - Quick check question: What is the primary difference in the type of output between a discriminative model and a generative model?

## Architecture Onboarding

- Component map:
  - Task-specific fine-tuned model (SLM) -> LLM -> Prompt template -> Interpretation module (optional)

- Critical path:
  1. Fine-tune SLM on in-domain data
  2. For each test example, get SLM's prediction and confidence
  3. Construct prompt with test example, SLM's prediction, confidence, and optional interpretation prompt
  4. Feed prompt to LLM and parse output

- Design tradeoffs:
  - Using SLM predictions adds a dependency but improves OOD performance
  - Including confidence scores improves results but requires a well-calibrated SLM
  - The optional interpretation module provides insights but increases prompt length and complexity

- Failure signatures:
  - Poor OOD performance: SLM may not generalize well to OOD data
  - Increased hallucinations: LLM may ignore SLM's reference or SLM may be unreliable
  - No improvement with confidence: SLM's confidence scores may be poorly calibrated

- First 3 experiments:
  1. Compare SuperContext (zero-shot) vs. original LLM on a held-out OOD dataset
  2. Compare SuperContext with and without SLM confidence scores on the same dataset
  3. Evaluate the interpretation module's impact on LLM performance and reasoning transparency

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Experimental results rely heavily on proprietary models (ChatGPT), making exact reproduction challenging
- The correlation between SLM confidence scores and LLM performance requires further validation across diverse model combinations
- The interpretation module's contribution to performance gains remains unclear due to limited experimental validation

## Confidence

High confidence: The core mechanism of using SLM predictions as auxiliary knowledge in prompts is well-supported by experimental results across multiple tasks.

Medium confidence: The claimed benefits of including SLM confidence scores are supported by the presented experiments, but the robustness of this approach across different model combinations and tasks needs further validation.

Low confidence: The interpretation module's contribution to performance gains and its general applicability remain uncertain due to limited experimental validation.

## Next Checks

1. Reproduce core results with open-source models: Implement SuperContext using only open-source LLMs and SLMs to verify that improvements are not dependent on proprietary implementations.

2. Test confidence score calibration: Systematically evaluate how well SLM confidence scores correlate with actual prediction accuracy across different tasks and model combinations.

3. Evaluate interpretation module impact: Conduct controlled experiments comparing SuperContext with and without the interpretation module to quantify its contribution to performance gains.