---
ver: rpa2
title: 'Mind the Model, Not the Agent: The Primacy Bias in Model-based RL'
arxiv_id: '2310.15017'
source_url: https://arxiv.org/abs/2310.15017
tags:
- uni00000013
- uni00000048
- uni00000018
- learning
- uni00000014
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the primacy bias in model-based reinforcement
  learning (MBRL), where the agent tends to overfit early data and lose the ability
  to learn from new data. The authors identify two forms of primacy bias: the agent''s
  tendency to overfit early data in the replay buffer and the world model''s overfitting
  of early data distribution.'
---

# Mind the Model, Not the Agent: The Primacy Bias in Model-based RL

## Quick Facts
- **arXiv ID**: 2310.15017
- **Source URL**: https://arxiv.org/abs/2310.15017
- **Reference count**: 11
- **Primary result**: World model resetting alleviates primacy bias in MBRL by periodically refreshing world model parameters

## Executive Summary
This paper investigates the primacy bias in model-based reinforcement learning (MBRL), where agents overfit early data and lose learning capacity. The authors identify that the primacy bias in MBRL primarily stems from world model overfitting rather than agent overfitting, which contrasts with model-free RL. They propose world model resetting - periodically resetting world model parameters during training - to address this issue. The technique significantly improves performance across MBPO and DreamerV2 algorithms on continuous control tasks (MuJoCo, DeepMind Control Suite) and discrete control tasks (Atari 100k benchmark).

## Method Summary
The method involves periodically resetting the parameters of the world model during MBRL training. The world model is trained on data from a replay buffer, and the agent learns from model-generated data. By resetting the world model's parameters at fixed intervals, the model is forced to relearn from the current data distribution in the replay buffer, which reflects the updated policy. The technique is validated on MBPO and DreamerV2 algorithms across various continuous and discrete control benchmarks.

## Key Results
- World model resetting significantly improves MBRL performance by alleviating the primacy bias
- The primacy bias in MBRL is primarily caused by world model overfitting, not agent overfitting
- Effectiveness varies with reset interval and model UTD ratio, requiring task-specific tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: World model resetting alleviates the primacy bias by periodically refreshing the world model's parameters, allowing it to adapt to the current data distribution.
- Mechanism: The world model in MBRL tends to overfit early data distributions, especially under high model UTD ratios. Resetting the world model's parameters periodically forces the model to relearn from the current replay buffer, which contains data from the updated policy. This breaks the model's fixation on outdated data distributions.
- Core assumption: The replay buffer contains sufficient current and representative data for the world model to relearn effectively after each reset.
- Break condition: If the replay buffer becomes too sparse or unrepresentative (e.g., early in training or after catastrophic policy changes), the world model may not have sufficient data to relearn effectively.

### Mechanism 2
- Claim: Resetting the world model allows it to focus on the current policy's data distribution rather than outdated early distributions.
- Mechanism: As the agent's policy evolves, the data distribution in the replay buffer changes. A world model that was trained on early data becomes less accurate for the current policy. Resetting the world model forces it to adapt to the new data distribution, improving its predictive accuracy for the current policy.
- Core assumption: The world model can quickly adapt to the current data distribution after being reset, and this adaptation improves its predictive accuracy.
- Break condition: If the world model's architecture is too simple to capture the current data distribution quickly, or if the reset interval is too short/long, the mechanism may not work effectively.

### Mechanism 3
- Claim: World model resetting improves the algorithm's potential to reach the optimal policy by maintaining a more accurate world model.
- Mechanism: A world model that accurately predicts the environment dynamics under the current policy enables the agent to learn better policies. By periodically resetting the world model, its predictive accuracy for the current policy is maintained, which in turn helps the agent learn more effectively.
- Core assumption: A more accurate world model leads to better policy learning, and this improvement outweighs any potential loss from the reset itself.
- Break condition: If the world model's reset causes significant loss of learned knowledge that outweighs the benefits of adaptation, or if the agent's learning is not significantly impacted by world model accuracy.

## Foundational Learning

- **Concept: Model-based Reinforcement Learning (MBRL)**
  - Why needed here: Understanding MBRL is crucial because the paper's core contribution is about addressing the primacy bias specifically in MBRL, which differs from model-free RL.
  - Quick check question: What are the two main components of MBRL, and how do they interact during training?

- **Concept: Update-to-Data (UTD) Ratio**
  - Why needed here: The paper discusses how different UTD ratios (for both the world model and the agent) affect the primacy bias and the effectiveness of world model resetting.
  - Quick check question: How does a high model UTD ratio contribute to the world model's overfitting, and why doesn't a high agent UTD ratio cause similar issues in MBRL?

- **Concept: Overfitting in Deep Learning**
  - Why needed here: The paper's central argument is that the world model in MBRL overfits early data distributions, which is a form of overfitting. Understanding overfitting is key to grasping the problem and the solution.
  - Quick check question: What is the difference between overfitting in supervised learning and the primacy bias in reinforcement learning?

## Architecture Onboarding

- **Component map**: Environment -> Replay Buffer -> World Model -> Agent -> Environment
- **Critical path**: 1. Collect data from environment and store in replay buffer 2. Train world model on replay buffer data 3. Generate data using world model and train agent 4. Periodically reset world model parameters
- **Design tradeoffs**: Reset interval (too frequent causes underfitting, too infrequent allows overfitting), Model UTD ratio (higher ratios increase reset needs but overfitting risk), Model ensemble size (larger ensembles reduce reset needs but increase computational cost)
- **Failure signatures**: Poor performance that doesn't improve with more training steps, High model error on recent data compared to older data, Performance degradation after world model resets
- **First 3 experiments**: 1. Run MBPO with different model UTD ratios to observe primacy bias effects 2. Apply world model resetting to MBPO with high model UTD ratio and observe performance improvement 3. Vary reset interval and observe its effect on performance to find optimal frequency

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- The paper doesn't directly measure whether world model overfitting is the dominant cause of primacy bias, relying instead on performance improvements as indirect evidence
- Optimal reset interval is unspecified and likely requires per-task tuning, with no general guidelines provided
- Effectiveness depends heavily on the replay buffer containing representative current data, but this assumption isn't thoroughly validated across different scenarios

## Confidence
- **High confidence**: World model resetting improves MBRL performance across multiple benchmarks and algorithms
- **Medium confidence**: World model overfitting is the primary cause of primacy bias in MBRL
- **Low confidence**: Periodic resets effectively "forget" outdated data distributions without direct empirical evidence

## Next Checks
1. **Direct measurement of world model overfitting**: Compare model accuracy on recent vs. historical data distributions before and after world model resetting to quantify overfitting and its reduction.

2. **Replay buffer analysis under different reset intervals**: Track data distribution in replay buffer across training with different reset frequencies to identify when buffer becomes unrepresentative and correlate with performance changes.

3. **Ablation study on reset timing**: Implement mechanism that triggers world model resets based on model error thresholds rather than fixed intervals to determine if adaptive resetting outperforms fixed schedules.