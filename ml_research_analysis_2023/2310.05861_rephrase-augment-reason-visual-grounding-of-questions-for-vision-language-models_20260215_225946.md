---
ver: rpa2
title: 'Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language
  Models'
arxiv_id: '2310.05861'
source_url: https://arxiv.org/abs/2310.05861
tags:
- question
- questions
- language
- answer
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of underspecification in visual
  question answering (VQA) by introducing a gradient-free framework called Rephrase,
  Augment and Reason (REPARE). The core idea is to preemptively clarify underspecified
  questions by extracting and fusing visual information from images into the questions
  using a large vision-language model (LVLM).
---

# Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models

## Quick Facts
- arXiv ID: 2310.05861
- Source URL: https://arxiv.org/abs/2310.05861
- Reference count: 40
- Key outcome: Improves zero-shot VQA accuracy by up to 3.85% on VQAv2 and 6.41% on A-OKVQA using a gradient-free framework that preemptively clarifies underspecified questions

## Executive Summary
This paper addresses the challenge of underspecification in visual question answering by introducing a gradient-free framework called Rephrase, Augment and Reason (REPARE). The core idea is to preemptively clarify underspecified questions by extracting and fusing visual information from images into the questions using a large vision-language model (LVLM). REPARE generates multiple rephrased question candidates by incorporating salient details from image captions and rationales, then selects the most promising candidate based on the LVLM's confidence in the generated answer. Empirically, REPARE improves zero-shot VQA performance by up to 3.85% on VQAv2 and 6.41% on A-OKVQA.

## Method Summary
REPARE is a gradient-free framework that improves zero-shot visual question answering by rephrasing underspecified questions with visual context. The method uses an LVLM to generate image captions and rationales, extracts salient keywords and visual details, then fuses these into the original question to create multiple rephrased candidates. A confidence-based selection mechanism chooses the most promising candidate based on the LVLM's predicted answer quality. The framework leverages the asymmetric strength of LVLMs, where the LLM component is stronger than the vision encoder, allowing linguistic augmentation to compensate for visual limitations while still requiring the image for full performance.

## Key Results
- Zero-shot VQA accuracy improves by up to 3.85% on VQAv2 and 6.41% on A-OKVQA
- Oracle performance with gold answers shows even larger gains of up to 14.41% on VQAv2 and 33.94% on A-OKVQA
- Rephrased questions are more syntactically complex and effectively leverage LVLM asymmetric strengths
- REPARE questions make better use of existing LVLMs by leveraging LLM strength while still benefiting from the image

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rephrasing underspecified questions using image-derived context reduces ambiguity for LVLM inference
- Core assumption: LVLM's captioning and reasoning capabilities are sufficiently reliable to provide useful context
- Evidence anchors: Abstract states "preemptively clarify underspecified questions by extracting and fusing visual information from images into the questions"
- Break condition: If LVLM's caption or rationale generation is noisy or hallucinatory, modified questions may mislead rather than clarify

### Mechanism 2
- Claim: Confidence-based selection exploits LVLM's ability to self-assess answer quality for unsupervised selection
- Core assumption: LVLM's answer confidence is predictive of downstream accuracy
- Evidence anchors: Abstract mentions "use the LVLM's confidence over a generated answer as an unsupervised scoring function"
- Break condition: If LVLM is systematically overconfident or underconfident, relative ranking may not correlate with accuracy

### Mechanism 3
- Claim: Asymmetric strength of LVLMs allows REPARE to shift reasoning burden to LLM while maintaining image necessity
- Core assumption: LLM component is significantly more capable than vision component
- Evidence anchors: Abstract notes "harnesses the asymmetric strength of most existing LVLMs, whose LLM components typically have far more capacity"
- Break condition: If future LVLMs balance vision and language capacity, benefit of linguistic augmentation may diminish

## Foundational Learning

- Concept: Visual grounding in NLP
  - Why needed here: Method relies on extracting visual context and fusing it into questions
  - Quick check question: How does adding image-derived entities to a question reduce ambiguity in VQA?

- Concept: Zero-shot prompting and confidence calibration
  - Why needed here: REPARE uses zero-shot LVLM prompting and leverages model's confidence for selection
  - Quick check question: Why might an LVLM's confidence in its answer be a good signal for selecting a rephrased question?

- Concept: Asymmetric model design (LLM >> vision encoder)
  - Why needed here: REPARE's effectiveness depends on stronger LLM than vision component
  - Quick check question: How does the relative size of LLM vs vision encoder in LVLMs influence the design of methods like REPARE?

## Architecture Onboarding

- Component map: Image + Question -> LVLM -> Captions/Rationales -> Keyword Extraction -> Visual Detail Extraction -> Sentence Fusion -> Candidate Generation -> Confidence Scoring -> Best Candidate Selection

- Critical path:
  1. Input: Image + original question
  2. Generate caption and rationale via LVLM
  3. Extract keywords from question
  4. Extract visual details from caption and rationale
  5. Fuse details into question (n candidates)
  6. Score candidates using LVLM confidence
  7. Select top candidate and evaluate VQA accuracy

- Design tradeoffs:
  - Using LVLM for caption/rationale vs. external models: LVLM is zero-shot and unified, but may be less reliable than trained models
  - Sentence fusion with LLM-only vs. LVLM with image tokens: LLM-only avoids distraction from image embeddings but may miss visual nuance
  - Number of candidates (n): More candidates improve oracle performance but increase cost and make selection harder

- Failure signatures:
  - Low oracle gains: Indicates caption/rationale quality or fusion is poor
  - High oracle but low inference gains: Indicates confidence-based selection is not well-calibrated
  - Degradation vs baseline: Could indicate noisy rephrasing or overfitting to LVLM quirks

- First 3 experiments:
  1. Ablation: Run REPARE without rationales, without captions, without question entities to quantify contribution of each
  2. Oracle comparison: Use gold answers to select from rephrased vs paraphrased candidates to measure upper bound and semantic vs cosmetic changes
  3. Complexity analysis: Compute ADD and ID on rephrased vs original questions to verify reduction in underspecification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the computational cost of REPARE scale with the number of candidate questions (n) generated?
- Basis in paper: [explicit] The paper discusses increasing n from 2 to 5 and beyond, noting performance gains and eventual saturation
- Why unresolved: The paper does not provide detailed analysis of computational cost (e.g., inference time, memory usage) associated with generating and scoring more candidates
- What evidence would resolve it: Empirical data showing inference time, memory usage, or FLOPs as a function of n, ideally comparing different LVLM sizes

### Open Question 2
- Question: Does REPARE's performance generalize to other visual question answering datasets beyond VQAv2 and A-OKVQA?
- Basis in paper: [inferred] The paper only evaluates REPARE on two specific datasets, leaving generalization to other VQA benchmarks unexplored
- Why unresolved: Effectiveness on other datasets with different question types, visual domains, or answer formats is unknown
- What evidence would resolve it: Experiments on diverse VQA datasets (e.g., VQA-CP, TextVQA, VizWiz) showing consistent performance improvements

### Open Question 3
- Question: How sensitive is REPARE to the quality and accuracy of the underlying LVLM's generated captions and rationales?
- Basis in paper: [explicit] The paper acknowledges that quality of captions and rationales affects information used to augment questions
- Why unresolved: Paper does not investigate impact of caption/rationale quality on REPARE's performance
- What evidence would resolve it: Controlled experiments ablating use of captions/rationales, or comparing REPARE's performance using different LVLM models for caption/rationale generation

## Limitations
- Reliance on LVLM-generated captions and rationales introduces potential hallucination risks that could degrade performance
- Confidence-based selection assumes relative calibration is preserved across diverse question types
- Asymmetric strength assumption may become invalid as vision encoders improve in future LVLM iterations

## Confidence
- **High Confidence**: Empirical performance improvements (3.85% on VQAv2, 6.41% on A-OKVQA) and oracle results (14.41% and 33.94%) are well-supported
- **Medium Confidence**: Mechanism explanations are reasonable but not fully validated; alternative explanations not rigorously excluded
- **Low Confidence**: Long-term generalizability to future LVLM architectures and robustness to noisy or hallucinatory caption generation

## Next Checks
1. Test REPARE's performance when LVLM caption generation is artificially degraded by adding noise to prompt inputs
2. Compare confidence-based selection against alternative selection methods (random, frequency-based, or model-based ranking)
3. Evaluate on out-of-distribution datasets or images with poor visual quality to assess robustness when image provides limited information for rephrasing