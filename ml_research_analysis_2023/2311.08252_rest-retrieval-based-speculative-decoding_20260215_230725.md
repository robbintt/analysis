---
ver: rpa2
title: 'REST: Retrieval-Based Speculative Decoding'
arxiv_id: '2311.08252'
source_url: https://arxiv.org/abs/2311.08252
tags:
- draft
- rest
- tokens
- token
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Retrieval-Based Speculative Decoding (REST),
  a novel approach to accelerate large language model (LLM) inference. The key insight
  is that REST uses retrieval from a datastore to generate draft tokens, instead of
  relying on a small draft language model as in previous methods.
---

# REST: Retrieval-Based Speculative Decoding

## Quick Facts
- **arXiv ID:** 2311.08252
- **Source URL:** https://arxiv.org/abs/2311.08252
- **Reference count:** 3
- **Primary result:** Achieves 1.62× to 2.36× speedup on LLM inference using retrieval-based draft generation

## Executive Summary
This paper introduces Retrieval-Based Speculative Decoding (REST), a novel approach to accelerate large language model (LLM) inference by replacing small draft models with retrieval from a datastore. The method retrieves candidate continuations based on current context, constructs a Trie to select high-frequency draft tokens, and verifies them using tree attention in a single forward pass. REST is training-free and can integrate with any existing LLM, achieving significant speedups on code and text generation benchmarks.

## Method Summary
REST accelerates LLM inference by using retrieval instead of a small draft model. It builds a datastore from context-continuation pairs extracted from pretraining or instruction-tuning data. During inference, REST retrieves matching contexts using exact-match suffix retrieval, constructs a Trie from candidates to select high-frequency draft tokens (up to 64), and verifies them using a specially designed tree attention mechanism in a single forward pass. The method is training-free and accepts or rejects draft tokens based on LLM verification, continuing until the generation limit is reached.

## Key Results
- Achieves 1.62× to 2.36× speedup on 7B and 13B models
- Particularly effective for code generation with 2.12× to 2.36× speedup
- Shows significant improvements on HumanEval and MT-Bench benchmarks
- Datastore size and draft token selection strategy are key performance factors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-based speculative decoding can generate draft tokens more efficiently than using a small language model.
- Mechanism: REST retrieves candidate continuations from a datastore based on current context, constructs a Trie to select high-frequency draft tokens, and verifies these drafts using the LLM with tree attention.
- Core assumption: The datastore contains relevant continuations that match the current context sufficiently often to be useful for draft generation.
- Evidence anchors: [abstract] "Unlike previous methods that rely on a draft language model for speculative decoding, REST harnesses the power of retrieval to generate draft tokens."
- Break condition: If the datastore does not contain relevant continuations for the current context, the retrieval process will fail to provide useful draft tokens.

### Mechanism 2
- Claim: Using a Trie structure to select high-frequency draft tokens improves the quality of draft generation.
- Mechanism: After retrieving candidate continuations, a Trie is constructed and nodes with highest frequencies are selected as draft tokens.
- Core assumption: High-frequency prefixes in retrieved candidates are more likely to be correct continuations of the current context.
- Evidence anchors: [section 3.2] "We select draft tokens from the retrieved result S using a Trie... For each node, we assign a weight reflecting the number (frequency) of the corresponding prefix that appears in the retrieved candidates."
- Break condition: If high-frequency prefixes are not actually good continuations, the draft token selection will be poor.

### Mechanism 3
- Claim: Tree attention allows efficient verification of multiple draft sequences in a single forward pass.
- Mechanism: A pseudo sequence is constructed from the subtree using breadth-first search, and a carefully designed attention mask ensures correct computation of each token's dependencies.
- Core assumption: The tree attention mechanism can correctly model dependencies between tokens while avoiding redundant computation.
- Evidence anchors: [section 3.2] "To optimize the efficiency, we construct a pseudo sequence from the subtree using breadth-first search... we implement a carefully designed attention mask in each attention layer."
- Break condition: If the tree attention mechanism fails to correctly model token dependencies, the overall method will fail.

## Foundational Learning

- **Speculative decoding**: Why needed here: REST is a variant of speculative decoding that uses retrieval instead of a small draft model. Quick check: What is the main idea behind speculative decoding, and how does it differ from standard autoregressive generation?

- **Trie data structure**: Why needed here: REST uses a Trie to select high-frequency draft tokens from retrieved candidates. Quick check: How does a Trie work, and why is it suitable for selecting high-frequency prefixes?

- **Attention mechanisms in transformers**: Why needed here: REST uses a tree attention mechanism to efficiently verify multiple draft sequences. Quick check: How does the standard attention mechanism work in transformers, and how does tree attention differ?

## Architecture Onboarding

- **Component map**: Datastore -> Retrieval module -> Trie construction -> Tree attention -> LLM verification

- **Critical path**: 1. Retrieve matching contexts from datastore 2. Construct Trie and select high-frequency draft tokens 3. Verify draft tokens using tree attention 4. Accept or reject draft tokens based on LLM verification

- **Design tradeoffs**: 
  - Datastore size vs. retrieval speed: Larger datastores may improve retrieval quality but slow down retrieval
  - Number of draft tokens vs. verification overhead: More draft tokens can lead to better speedups but increase GPU computational burden
  - Exact match vs. approximate retrieval: Exact match is faster but may miss relevant contexts, while approximate retrieval is slower but more comprehensive

- **Failure signatures**:
  - Low speedup: Could indicate poor retrieval quality, ineffective Trie selection, or insufficient overlap between draft and LLM predictions
  - High verification rejection rate: Could indicate that draft tokens are not sufficiently accurate or that tree attention is not correctly modeling dependencies
  - Slow retrieval: Could indicate that the datastore is too large or that the exact match algorithm is not efficient

- **First 3 experiments**:
  1. Test retrieval quality: Measure overlap between retrieved contexts and current context to ensure retrieval process is working correctly
  2. Test Trie selection: Verify that Trie correctly captures frequency of prefixes and selected draft tokens are reasonable continuations
  3. Test tree attention: Ensure tree attention mechanism correctly models dependencies between tokens and verification process is accurate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal datastore size for REST across different domains and model sizes?
- Basis in paper: [explicit] The paper shows that increasing datastore size improves performance but notes that the speedup growth is not as pronounced as the Mean Generated Length improvement.
- Why unresolved: The paper only tests a limited range of datastore sizes (0.9GB to 27GB) and does not explore the point of diminishing returns or the optimal size for different domains and model sizes.
- What evidence would resolve it: Systematic experiments varying datastore size across different domains (code vs. general text) and model sizes (7B vs. 13B), measuring both Mean Generated Length and actual speedup, would identify the optimal datastore size for each scenario.

### Open Question 2
- Question: How does the quality of the datastore (e.g., using model-generated data vs. original training data) affect REST's performance?
- Basis in paper: [explicit] The paper suggests future work on constructing datastores from model-generated content for better alignment, but does not test this approach.
- Why unresolved: The paper only uses pretraining data and instruction-tuning data for datastore construction, not exploring whether model-generated data could provide better alignment and performance.
- What evidence would resolve it: Comparative experiments using datastores constructed from different sources (original training data, instruction-tuning data, and model-generated data) while measuring REST's performance metrics would determine the impact of datastore quality.

### Open Question 3
- Question: Can REST be effectively combined with traditional speculative decoding to further accelerate small LM draft generation?
- Basis in paper: [explicit] The paper mentions this as a potential future direction but does not implement or test this combination.
- Why unresolved: The paper only explores REST as a standalone method and does not investigate how it might complement or enhance traditional speculative decoding approaches.
- What evidence would resolve it: Implementation and testing of a hybrid approach that combines REST with traditional speculative decoding, measuring the speedup compared to either method alone, would demonstrate the potential benefits of this combination.

## Limitations

- **Datastore construction uncertainty**: The paper lacks detailed specifications about how the datastore is constructed from training corpora, with key parameters like context length and preprocessing steps not specified.
- **Tree attention implementation gap**: While the concept of tree attention is described, the paper lacks detailed implementation specifications for the attention mask and dependency modeling across multiple draft sequences.
- **Evaluation scope limitation**: Experiments focus primarily on synthetic metrics and benchmark performance, without thorough evaluation across diverse domains or different datastore sizes and retrieval strategies.

## Confidence

- **Speedup Claims**: High confidence - Well-supported by experimental results across multiple benchmarks and model sizes
- **Retrieval-Based Approach Validity**: Medium confidence - Supported by experimental results but lacks comparative analysis against established speculative decoding baselines
- **Training-Free Integration**: High confidence - Well-supported as the method relies solely on retrieval without requiring model fine-tuning

## Next Checks

1. **Check 1: Retrieval Quality Analysis** - Implement systematic evaluation of retrieval process by measuring overlap between retrieved contexts and current generation contexts, analyzing how retrieval quality varies with different datastore sizes and what percentage of retrievals provide useful continuations.

2. **Check 2: Tree Attention Verification** - Conduct controlled experiments to validate that tree attention mechanism correctly models token dependencies across draft sequences, comparing verification results against independent processing of each draft sequence.

3. **Check 3: Datastore Size Sensitivity** - Perform experiments varying datastore size (e.g., 10M, 100M, 1B pairs) to understand tradeoff between retrieval quality and inference speed, measuring how retrieval time and speedup scale with datastore size to identify optimal configurations.