---
ver: rpa2
title: 'Spanning Training Progress: Temporal Dual-Depth Scoring (TDDS) for Enhanced
  Dataset Pruning'
arxiv_id: '2311.13613'
source_url: https://arxiv.org/abs/2311.13613
tags:
- training
- pruning
- samples
- dataset
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of dataset pruning for deep learning,
  where the goal is to select a smaller, representative subset of data that maintains
  performance comparable to the full dataset. Existing methods often rely on snapshot-based
  criteria, which can lead to poor generalization across different pruning rates and
  architectures.
---

# Spanning Training Progress: Temporal Dual-Depth Scoring (TDDS) for Enhanced Dataset Pruning

## Quick Facts
- arXiv ID: 2311.13613
- Source URL: https://arxiv.org/abs/2311.13613
- Reference count: 40
- Primary result: Achieves 54.51% accuracy on CIFAR-100 with only 10% of training data, outperforming random selection by 7.83% and other methods by at least 12.69%.

## Executive Summary
This paper introduces Temporal Dual-Depth Scoring (TDDS), a novel approach to dataset pruning that addresses limitations of snapshot-based methods. TDDS employs a two-level strategy: an inner level estimates individual sample contributions using projected gradients onto accumulated directions, and an outer level identifies well-generalized samples by maximizing the variance of these contributions over time. Experiments on CIFAR-10, CIFAR-100, and ImageNet demonstrate that TDDS significantly outperforms state-of-the-art methods, achieving strong cross-architecture generalization while maintaining computational efficiency through exponential moving average techniques.

## Method Summary
TDDS is a two-level dataset pruning method that tracks training dynamics to identify the most informative samples. In the inner level, it estimates each sample's contribution by projecting its gradient onto the accumulated gradient direction, ensuring valid contribution measurement within the current optimization context. In the outer level, it assesses sample importance by maximizing the variance of these contributions over time, highlighting samples with stable, informative gradients across epochs. The method uses KL divergence-based loss difference instead of cross-entropy to capture full probability distribution changes, and employs exponential moving average with a sliding window to reduce memory requirements while preserving temporal dynamics.

## Key Results
- Achieves 54.51% accuracy on CIFAR-100 with only 10% of training data
- Outperforms random selection by 7.83% and other methods by at least 12.69%
- Demonstrates strong cross-architecture generalization across ResNet, VGG, MobileNet, EfficientNet, and Swin architectures
- Shows consistent improvement across pruning rates (30%, 50%, 70%, 80%, 90%) on multiple datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal Dual-Depth Scoring balances training dynamics integration with well-generalized sample identification by decoupling the two tasks into inner and outer levels.
- Mechanism: In the inner level, individual sample contributions are estimated via projected gradients onto the accumulated gradient direction, ensuring valid contribution measurement within the current optimization context. In the outer level, sample importance is assessed by maximizing the variance of these contributions over time, which highlights samples with stable, informative gradients across epochs.
- Core assumption: Projected gradients capture the true optimization-relevant contribution of a sample, and gradient variance over time is a reliable indicator of generalization capability.
- Evidence anchors:
  - [abstract]: "In the first depth, TDDS estimates the series of each sample's individual contributions spanning the training progress... In the second depth, we focus on the variability of the sample-wise contributions... to highlight well-generalized samples."
  - [section]: "In the inner level, the gradient in the outer level is replaced with the projected component on the accumulated direction to estimate valid contributions in every epoch... In the outer level, we reduce reliance on the averaging down sampling operation to prevent overlooking vital samples."
  - [corpus]: No direct corpus evidence for this specific dual-depth approach; claims are primarily derived from the paper's methodology section.
- Break condition: If projected gradients do not accurately reflect sample contributions in the optimization context, or if gradient variance does not correlate with generalization, the dual-depth scoring will fail.

### Mechanism 2
- Claim: Using KL divergence-based loss difference instead of cross-entropy improves sample contribution estimation by incorporating non-target class probabilities.
- Mechanism: The KL divergence between predicted probabilities at adjacent epochs captures the full distributional change, not just the target class. This provides a more nuanced view of how a sample's predictions evolve, especially when the model's confidence on the target class remains unchanged but shifts on non-target classes.
- Core assumption: Non-target class probability changes contain meaningful information about sample contribution that CE loss difference misses.
- Evidence anchors:
  - [abstract]: "Due to the one-hot yn, non-target probabilities are unfortunately overlooked... This reminds us of the Kullback-Leibler (KL) divergence loss, which can be achieved by simply replacing yn with fθt+1(xn)."
  - [section]: "Considering that fθt(xn) follows a Bernoulli distribution B(pt), where pt represents the probability of the target class at time t. If pt equals pt+1, Equation 9 fails to distinguish, but Equation 10 can still differentiate the contributions of samples with qt and qt+1."
  - [corpus]: No corpus evidence comparing KL-based vs CE-based loss difference for dataset pruning; the claim is based on the paper's theoretical argument.
- Break condition: If non-target class probability changes are noisy or uninformative for the task, KL-based loss difference may not provide a meaningful advantage over CE.

### Mechanism 3
- Claim: Exponential Moving Average (EMA) with a decay coefficient enables efficient computation of gradient variance over long training trajectories without excessive memory usage.
- Mechanism: Instead of storing all predicted outputs across T epochs, EMA maintains a running estimate of the variance over a sliding window of K epochs, exponentially weighting more recent windows. This reduces memory from O(T) to O(K) while preserving the temporal dynamics needed for variance calculation.
- Core assumption: A sliding window with exponential weighting adequately captures the temporal variance without requiring the full trajectory.
- Evidence anchors:
  - [section]: "To enhance efficiency, we conduct TDDS with a moving window... For any training point t ≥ K, the variance Rt(xn) is computed over a K-epoch window... With this EMA, we only need to store predicted outputs for one window, significantly alleviating storage burden."
  - [abstract]: No direct mention of EMA, but the methodology section describes the efficient implementation.
  - [corpus]: No corpus evidence for EMA in dataset pruning; this appears to be a novel implementation choice in the paper.
- Break condition: If the window size K is too small, important long-range dynamics are missed; if too large, memory savings are negated.

## Foundational Learning

- Concept: Gradient projection onto accumulated direction
  - Why needed here: To estimate the actual contribution of a sample in the context of the current optimization, not just its isolated gradient magnitude.
  - Quick check question: Why does projecting the sample gradient onto the accumulated gradient direction provide a better measure of contribution than using the raw gradient magnitude?

- Concept: Variance as a measure of generalization
  - Why needed here: High variance in sample contributions over time indicates that the sample is consistently informative and generalizes well, while low variance suggests the sample is only important in specific contexts.
  - Quick check question: How does maximizing gradient variance over time help identify well-generalized samples?

- Concept: KL divergence for distributional comparison
  - Why needed here: KL divergence captures the full probability distribution change, not just the target class, providing a richer signal for sample contribution estimation.
  - Quick check question: What advantage does KL divergence have over cross-entropy when measuring loss difference between epochs?

## Architecture Onboarding

- Component map: Data pipeline -> Training loop -> Scoring -> Retraining
- Critical path: Training loop → Variance computation → Sample selection → Retraining
- Design tradeoffs:
  - Memory vs. accuracy: Full trajectory storage gives best accuracy but is infeasible for large datasets; EMA with window K is a memory-efficient approximation.
  - Window size K: Larger K captures more long-range dynamics but increases memory and computation; smaller K is faster but may miss important trends.
  - Decay coefficient β: Higher β gives more weight to recent epochs, which may be more relevant; lower β smooths over more history but may dilute recent signals.
- Failure signatures:
  - Poor accuracy on coreset: Likely due to incorrect variance computation, wrong window size, or insufficient training epochs.
  - Out-of-memory errors: Likely due to storing too many predictions or using too large a window size.
  - Slow convergence: May indicate incorrect importance weighting during coreset training or suboptimal learning rate.
- First 3 experiments:
  1. Verify gradient projection: Compare variance scores using raw gradients vs. projected gradients on a small dataset (e.g., CIFAR-10) to confirm the inner level mechanism works.
  2. Test KL vs CE: Run pruning with both KL-based and CE-based loss differences to confirm the outer level mechanism's sensitivity to the choice of loss function.
  3. Tune EMA parameters: Experiment with different window sizes K and decay coefficients β to find the optimal tradeoff between memory usage and pruning performance.

## Open Questions the Paper Calls Out

- Question: How does the performance of TDDS vary when applied to non-image datasets, such as tabular or text data?
  - Basis in paper: [inferred] The paper focuses on image datasets (CIFAR-10, CIFAR-100, and ImageNet) and does not explore other data modalities.
  - Why unresolved: The method's effectiveness on non-image data is not tested, leaving uncertainty about its generalizability to other domains.
  - What evidence would resolve it: Experiments applying TDDS to datasets from other domains (e.g., tabular, text, or time-series data) and comparing its performance to existing pruning methods in those contexts.

- Question: What is the impact of different network architectures (e.g., transformers, recurrent networks) on the effectiveness of TDDS?
  - Basis in paper: [inferred] The paper evaluates TDDS on convolutional neural networks (ResNet, Swin-T) but does not test it on other architectures like transformers or recurrent networks.
  - Why unresolved: The method's performance on diverse architectures is not explored, leaving uncertainty about its adaptability to different model types.
  - What evidence would resolve it: Experiments applying TDDS to different network architectures (e.g., transformers, recurrent networks) and comparing its performance to existing pruning methods for those architectures.

- Question: How does the computational cost of TDDS scale with dataset size and network complexity?
  - Basis in paper: [inferred] The paper mentions memory constraints and efficient implementation using EMA but does not provide a detailed analysis of computational scaling.
  - Why unresolved: The paper does not discuss the computational complexity of TDDS in terms of dataset size and network depth, leaving uncertainty about its scalability.
  - What evidence would resolve it: A detailed analysis of the computational time and memory usage of TDDS as a function of dataset size and network complexity, compared to other pruning methods.

## Limitations
- The method's generalizability to non-image datasets and different network architectures remains untested, limiting claims about its broad applicability.
- Computational overhead of tracking training dynamics, even with EMA optimization, could be prohibitive for extremely large datasets.
- The specific parameter choices (window size K, decay coefficient β) and their claimed optimality across different datasets need further validation.

## Confidence
- **High confidence**: The core dual-depth mechanism (gradient projection + variance maximization) is theoretically sound and the experimental methodology is rigorous with appropriate baselines.
- **Medium confidence**: The specific parameter choices (window size K, decay coefficient β) and their claimed optimality across different datasets need further validation on a wider range of tasks.
- **Low confidence**: The claim that KL divergence-based loss difference significantly outperforms CE-based methods lacks corpus-level validation and may be dataset-specific.

## Next Checks
1. **Architecture generalization test**: Validate TDDS performance on additional architectures (Transformer-based models, MLP-Mixers) not included in the original experiments to confirm cross-architecture generalization claims.
2. **Dataset diversity validation**: Test TDDS on non-image datasets (e.g., NLP text classification, tabular data) to assess generalizability beyond computer vision tasks.
3. **Computational efficiency analysis**: Measure and compare the actual memory and time overhead of TDDS versus baselines across different dataset sizes to quantify the practical cost of the dual-depth approach.