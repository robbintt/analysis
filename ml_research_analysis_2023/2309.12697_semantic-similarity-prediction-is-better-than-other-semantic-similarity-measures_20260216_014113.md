---
ver: rpa2
title: Semantic similarity prediction is better than other semantic similarity measures
arxiv_id: '2309.12697'
source_url: https://arxiv.org/abs/2309.12697
tags:
- similarity
- semantic
- data
- sts-b
- s-bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares different methods for measuring semantic similarity
  between natural language texts. The authors argue that directly predicting similarity
  using a fine-tuned model (STSScore) is better than traditional approaches like BLEU,
  BERTScore, or S-BERT.
---

# Semantic similarity prediction is better than other semantic similarity measures

## Quick Facts
- arXiv ID: 2309.12697
- Source URL: https://arxiv.org/abs/2309.12697
- Reference count: 7
- Primary result: Direct prediction of similarity scores (STSScore) outperforms traditional embedding-based methods for measuring semantic similarity

## Executive Summary
This paper compares four approaches for measuring semantic similarity between natural language texts: BLEU, BERTScore, S-BERT, and a fine-tuned model called STSScorer. The authors evaluate these methods on three GLUE benchmark datasets (STS-B, MRPC, QQP) and demonstrate that STSScorer, which directly predicts similarity scores, aligns best with human judgments of semantic similarity. The results show that traditional methods like BLEU perform poorly, while embedding-based methods like BERTScore and S-BERT show moderate correlation. The authors conclude that using STSScorer is the most effective method for measuring semantic similarity in future research.

## Method Summary
The study compares four semantic similarity measurement approaches: BLEU (traditional translation metric), BERTScore (contextual embedding-based), S-BERT (sentence embedding-based), and STSScorer (fine-tuned transformer model for direct similarity prediction). The STSScorer model is based on RoBERTa and fine-tuned on the STS-B dataset from the GLUE benchmark. The evaluation uses three datasets: STS-B for testing STSScorer, and MRPC and QQP for testing all four methods. Performance is measured by Pearson's r and Spearman's ρ correlation with human-labeled similarity scores.

## Key Results
- STSScorer shows the highest correlation with human judgments across all three datasets
- BERTScore demonstrates moderate correlation, better than BLEU but worse than STSScorer and S-BERT
- BLEU performs poorly for semantic similarity tasks, as expected since it was designed for translation evaluation
- STSScorer performs well on both paraphrasing (MRPC) and duplicate question detection (QQP) tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct prediction of similarity scores outperforms embedding-based similarity methods for semantic similarity tasks.
- Mechanism: A regression model trained to predict similarity scores learns the complex relationship between sentence pairs and human judgments directly, bypassing the need for intermediate embedding representations.
- Core assumption: The model can generalize beyond its training data to accurately measure semantic similarity on unseen examples.
- Evidence anchors:
  - [abstract] "Using a fine-tuned model for the STS-B from the GLUE benchmark... we define the STSScore approach and show that the resulting similarity is better aligned with our expectations on a robust semantic similarity measure than other approaches."
  - [section 3.0.1] "Because the STSScorer was fine-tuned on the STS-B data, we only utilize the test data. Nevertheless, because this is exactly the same context as during the training of STSScorers models (same data curators, same humans creating the judgements) this model has a huge advantage over the other approaches."
  - [corpus] Weak evidence. Corpus contains related papers on semantic similarity but does not directly support the superiority of direct prediction over embedding methods.
- Break condition: The model fails to generalize beyond its training data, or the training data is not representative of the target domain.

### Mechanism 2
- Claim: STSScorer aligns better with human judgments of semantic similarity than other methods.
- Mechanism: By being trained on human-labeled data (STS-B), STSScorer learns the nuances of human similarity judgments, including context and paraphrasing, which embedding methods may miss.
- Core assumption: Human judgments in the training data accurately reflect semantic similarity as understood by humans in general.
- Evidence anchors:
  - [abstract] "Using a fine-tuned model for the STS-B... we define the STSScore approach and show that the resulting similarity is better aligned with our expectations on a robust semantic similarity measure than other approaches."
  - [section 4] "For all three data sets, the transformer-based prediction approach STSScorer aligned best with the expectation from the data and we suggest to use such approaches for future calculations of the semantic similarity."
  - [corpus] Weak evidence. Corpus contains related papers but does not directly support the alignment with human judgments.
- Break condition: The human judgments in the training data are biased or not representative of general human understanding of semantic similarity.

### Mechanism 3
- Claim: STSScorer performs well on various semantic similarity tasks beyond its training data.
- Mechanism: The model's ability to generalize allows it to perform well on tasks like MRPC and QQP, which involve different types of semantic relationships (paraphrasing, duplicate questions) than the training data (news, image captions, Web forums).
- Core assumption: The model's architecture and training allow it to capture general semantic similarity patterns that apply across different domains.
- Evidence anchors:
  - [section 3.0.2] "For both data sets, we visualize the distribution of the similarities per class. Additionally, we report the central tendency (arithmetic mean) and variability (standard deviation) per class in the data."
  - [section 4] "The results of BERTScore exhibit a lot of the expected properties... However, we note that there are only few cases with a similarity close to one for the paraphrases, even though this is actually the expected value."
  - [corpus] Weak evidence. Corpus contains related papers but does not directly support the model's performance on various tasks.
- Break condition: The model's performance degrades significantly on tasks that are too different from the training data.

## Foundational Learning

- Concept: Transformer architecture and fine-tuning
  - Why needed here: The STSScorer model is based on a transformer architecture (RoBERTa) and is fine-tuned for the STS-B task. Understanding this architecture and the fine-tuning process is crucial for understanding how the model works.
  - Quick check question: What is the key difference between a transformer-based model and traditional embedding methods like S-BERT?

- Concept: Semantic Textual Similarity (STS) task and GLUE benchmark
  - Why needed here: The STS-B task is part of the GLUE benchmark, and the STSScorer model is trained on this task. Understanding the STS task and the GLUE benchmark provides context for the model's training and evaluation.
  - Quick check question: What is the goal of the Semantic Textual Similarity (STS) task in the GLUE benchmark?

- Concept: Evaluation metrics for semantic similarity models
  - Why needed here: The paper uses various evaluation metrics, including Pearson's r, Spearman's ρ, mean, and standard deviation, to assess the performance of different semantic similarity models. Understanding these metrics is essential for interpreting the results.
  - Quick check question: What is the difference between Pearson's r and Spearman's ρ, and when would you use each?

## Architecture Onboarding

- Component map:
  Input sentences -> Tokenizer -> Fine-tuned RoBERTa model -> Scaled logits (0-1 similarity score)

- Critical path:
  1. Tokenize input sentences
  2. Pass tokenized sentences through the model
  3. Extract logits from model output
  4. Scale logits to [0, 1] range

- Design tradeoffs:
  - Using a fine-tuned model allows for better alignment with human judgments but may limit generalizability to other tasks.
  - Direct prediction of similarity scores bypasses the need for intermediate embedding representations but may be less interpretable than embedding-based methods.

- Failure signatures:
  - Low correlation with human judgments on test data
  - Poor performance on tasks significantly different from training data
  - Overfitting to training data, leading to poor generalization

- First 3 experiments:
  1. Evaluate STSScorer on STS-B test data and compare results with other methods (BLEU, BERTScore, S-BERT).
  2. Evaluate STSScorer on MRPC and QQP data and compare results with other methods.
  3. Visualize the distribution of similarity scores for each method and class (positive/negative) on MRPC and QQP data.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but implicitly raises several important ones. The authors mention that current technologies do not provide a better solution and suggest future work should consider ethical and fairness concerns, particularly around potential biases in semantic similarity measures. They also highlight the need for further investigation into how well these approaches handle domain shifts outside the GLUE benchmark.

## Limitations

- Evaluation scope limited to three GLUE benchmark datasets may not generalize to all semantic similarity scenarios
- STSScorer has significant advantage since trained on same type of human-labeled data used for evaluation
- Comparison with BLEU is somewhat unfair as it was designed for machine translation, not semantic similarity
- Does not explore computational efficiency differences between methods

## Confidence

- High confidence: STSScorer outperforms BLEU for semantic similarity measurement
- Medium confidence: STSScorer is the best overall method for semantic similarity
- Medium confidence: Embedding-based methods (BERTScore, S-BERT) show moderate correlation with human judgments

## Next Checks

1. Test STSScorer on datasets from different domains than GLUE benchmark to assess generalization beyond training data
2. Conduct ablation studies comparing different transformer architectures and fine-tuning approaches for semantic similarity prediction
3. Measure computational efficiency (inference time, memory usage) across all four methods to evaluate practical deployment considerations