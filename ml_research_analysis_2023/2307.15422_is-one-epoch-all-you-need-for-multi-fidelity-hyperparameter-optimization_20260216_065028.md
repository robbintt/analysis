---
ver: rpa2
title: Is One Epoch All You Need For Multi-Fidelity Hyperparameter Optimization?
arxiv_id: '2307.15422'
source_url: https://arxiv.org/abs/2307.15422
tags:
- learning
- curves
- epoch
- fidelity
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares various multi-fidelity hyperparameter optimization
  (MF-HPO) methods against a simple baseline on classical benchmark datasets. The
  baseline involves discarding all models except the Top-K after training for only
  one epoch, followed by further training to select the best model.
---

# Is One Epoch All You Need For Multi-Fidelity Hyperparameter Optimization?

## Quick Facts
- arXiv ID: 2307.15422
- Source URL: https://arxiv.org/abs/2307.15422
- Reference count: 40
- Primary result: Simple 1-epoch baseline matches complex MF-HPO methods while using 40x less computation

## Executive Summary
This paper investigates whether complex multi-fidelity hyperparameter optimization (MF-HPO) methods are necessary by comparing them against a simple baseline: train all models for one epoch, select the top-K performers, and retrain only those at full fidelity. Surprisingly, this baseline achieved similar results to sophisticated methods like Successive Halving, Hyperband, and Learning Curve Extrapolation while requiring an order of magnitude less computation. The authors explain this success through analysis of learning curves showing early separability between good and bad models. They argue that common benchmarks may be insufficiently challenging and call for broader diversity in MF-HPO evaluation tasks.

## Method Summary
The paper compares four MF-HPO approaches: a 1-Epoch baseline (train for 1 epoch, select top-K, retrain at full fidelity), Successive Halving (SHA), Hyperband (HB), and Learning Curve Extrapolation (LCE). All methods use random search for the outer loop with 200 iterations. The 1-Epoch baseline trains each configuration for just one epoch, ranks them, selects the top-K, and then trains only those selected models to completion (100 epochs). This contrasts with SHA and HB which use adaptive discarding based on validation performance at multiple fidelity levels, and LCE which extrapolates learning curves to predict final performance. The methods are evaluated on four tabular datasets from HPOBench: Naval Propulsion, Parkinsons Telemonitoring, Protein Structure, and Slice Localization.

## Key Results
- The 1-epoch baseline achieved comparable test error to complex MF-HPO methods while using 40x fewer training epochs
- Learning curve analysis revealed clear early separation between groups of good and bad models in the first epoch
- The simple baseline suggests current benchmarks may be insufficiently challenging for advanced MF-HPO methods
- Authors recommend always including this baseline in MF-HPO benchmarks and broadening benchmark diversity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Early rank stability in learning curves allows discarding low-performing models after one epoch with minimal loss in final performance
- Mechanism: Models that will ultimately perform poorly diverge rapidly in the first epoch, creating separable clusters of "good" and "bad" curves. This early separation enables reliable identification of top-K models without waiting for full training
- Core assumption: The initial learning trajectory is predictive of final performance, and the variance between model rankings is low in the early epochs
- Evidence anchors:
  - [abstract] "Upon analyzing the learning curves of the benchmark data, we observed a few dominant learning curves, which explained the success of our baseline"
  - [section] "Fig. 3a...reveals that the groups of bad and good models can be identified in the first epoch of training, making selection with one epoch effective"
  - [corpus] Weak: No direct evidence from related papers, but the finding aligns with general learning curve theory
- Break condition: If the variance between learning curves is high in early epochs, or if learning curves cross frequently, early discarding will fail to preserve good models

### Mechanism 2
- Claim: Multi-fidelity hyperparameter optimization can be simplified by exploiting the low computational cost of single-epoch evaluations
- Mechanism: By evaluating all configurations at minimum fidelity (one epoch) and then selecting the top-K for full training, the total number of training epochs is drastically reduced while maintaining comparable model quality
- Core assumption: The top-K models selected from one-epoch evaluations remain competitive after full training
- Evidence anchors:
  - [abstract] "this baseline achieved similar results to its counterparts, while requiring an order of magnitude less computation"
  - [section] "1-Epoch policy uses 40 times fewer training epochs than its counterpart, the 100-Epoch policy (20,000/500=40)"
  - [corpus] Weak: No direct evidence from related papers; the approach is novel compared to existing MF-HPO benchmarks
- Break condition: If the search space is very large or complex, the top-K selection after one epoch may miss models that improve significantly in later epochs

### Mechanism 3
- Claim: Common benchmarks may not be sufficiently challenging for advanced MF-HPO methods, leading to over-engineered solutions
- Mechanism: The observed dominance of simple baselines on classical benchmarks suggests that these benchmarks lack diversity or complexity, causing advanced MF-HPO methods to offer little advantage
- Core assumption: If a simple baseline performs as well as complex MF-HPO methods, the benchmark is not sufficiently hard
- Evidence anchors:
  - [abstract] "This suggests that researchers should (1) always use the suggested baseline in benchmarks and (2) broaden the diversity of MF-HPO benchmarks to include more complex cases"
  - [section] "our work is limited to using 'epoch' as a unit of fidelity. While this is convenient...practical application settings may require considering wall time or other options as units of fidelity"
  - [corpus] Weak: No direct evidence from related papers; the claim is based on the observed results and calls for more diverse benchmarks
- Break condition: If benchmarks are diversified to include more complex learning curves, the simple baseline may fail, revealing the true value of advanced MF-HPO methods

## Foundational Learning

- Concept: Multi-fidelity hyperparameter optimization (MF-HPO)
  - Why needed here: The paper's core contribution is demonstrating that a simple MF-HPO baseline (1-epoch evaluation) is as effective as complex methods, highlighting the importance of understanding fidelity levels in HPO
  - Quick check question: What is the difference between single-fidelity and multi-fidelity HPO, and why might multi-fidelity methods reduce computational cost?

- Concept: Learning curve analysis
  - Why needed here: The paper relies on analyzing learning curves to explain why early discarding works, showing that good and bad models separate quickly
  - Quick check question: How can the shape and early behavior of learning curves inform decisions about early stopping or model selection in HPO?

- Concept: Benchmarking in machine learning research
  - Why needed here: The paper argues for the inclusion of simple baselines in benchmarks and the need for more diverse and challenging benchmarks to fairly evaluate MF-HPO methods
  - Quick check question: Why is it important to include simple baselines in HPO benchmarks, and what are the risks of over-engineered solutions on insufficiently challenging benchmarks?

## Architecture Onboarding

- Component map: Outer loop (random search/Bayesian optimization) -> Inner loop (training for fixed epochs) -> Early discarding (top-K selection) -> Model selection (final evaluation at full fidelity)
- Critical path: 1. Sample hyperparameter configuration 2. Train for minimum fidelity (1 epoch) 3. Evaluate and rank models 4. Discard low-performing models 5. Continue training top-K to maximum fidelity 6. Select best model based on final performance
- Design tradeoffs:
  - Fidelity vs. computational cost: Lower fidelity reduces cost but risks missing good models; higher fidelity increases cost but improves selection reliability
  - Top-K selection: Choosing K affects the balance between exploration (keeping diverse models) and exploitation (focusing on best early performers)
  - Benchmark diversity: Using simple benchmarks may lead to over-engineered solutions; diverse benchmarks ensure robust evaluation of MF-HPO methods
- Failure signatures:
  - If learning curves of good and bad models are not separable in early epochs, 1-epoch baseline will fail
  - If search space is too large or complex, top-K selection after 1 epoch may miss models that improve significantly later
  - If benchmark is too simple, advanced MF-HPO methods may not show their true value, leading to over-engineered solutions
- First 3 experiments:
  1. Implement 1-epoch baseline on simple tabular dataset (Naval Propulsion from HPOBench) and compare performance/cost to 100-epoch training
  2. Visualize learning curves for 1,000 random configurations to confirm early separation of good and bad models
  3. Test 1-epoch baseline on more complex benchmark (YAHPO-Gym with NB301) to check if it still performs well

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the 1-Epoch baseline be improved by incorporating domain-specific knowledge or meta-learning?
- Basis in paper: [explicit] The paper suggests that the 1-Epoch baseline performs well due to the early distinguishability of good and bad models, but does not explore potential improvements through domain knowledge or meta-learning
- Why unresolved: The study uses a simple random search for the outer loop and does not investigate the impact of incorporating domain-specific knowledge or meta-learning techniques to further enhance the 1-Epoch baseline
- What evidence would resolve it: Experiments comparing the 1-Epoch baseline with and without domain-specific knowledge or meta-learning incorporated, measuring the performance difference

### Open Question 2
- Question: How does the performance of the 1-Epoch baseline scale with larger and more complex datasets?
- Basis in paper: [inferred] The paper mentions that the 1-Epoch baseline performs well on classical benchmark datasets, but does not explicitly test its performance on larger or more complex datasets
- Why unresolved: The study focuses on classical benchmark datasets and does not explore the scalability of the 1-Epoch baseline to larger and more complex datasets
- What evidence would resolve it: Experiments evaluating the 1-Epoch baseline on larger and more complex datasets, comparing its performance to other multi-fidelity methods

### Open Question 3
- Question: Are there alternative units of fidelity, such as wall time or memory usage, that could improve the efficiency of multi-fidelity hyperparameter optimization?
- Basis in paper: [explicit] The paper acknowledges that using "epoch" as a unit of fidelity is convenient but may not be practical in real-world applications, suggesting the need for exploring alternative units of fidelity
- Why unresolved: The study primarily uses "epoch" as a unit of fidelity and does not investigate the impact of using alternative units, such as wall time or memory usage, on the efficiency of multi-fidelity hyperparameter optimization
- What evidence would resolve it: Experiments comparing the performance of multi-fidelity methods using different units of fidelity, such as wall time or memory usage, on various datasets

## Limitations
- Only evaluated on tabular datasets; results may not generalize to image or language tasks where learning curves are noisier
- Moderate search space complexity; more complex hyperparameter spaces may require longer training to distinguish good configurations
- Acknowledges that "epoch" as fidelity unit is convenient but may not reflect practical wall-clock time constraints

## Confidence

- High: The 1-epoch baseline consistently matches complex MF-HPO methods on tested tabular benchmarks
- Medium: Learning curve analysis showing early separability of good/bad models is supported but not extensively validated across diverse model architectures
- Medium: The claim that benchmarks are insufficiently challenging is plausible but requires validation on more complex tasks

## Next Checks

1. Test the 1-epoch baseline on image classification benchmarks (e.g., CIFAR-10) where learning curves are typically noisier and convergence is slower
2. Evaluate the baseline with larger hyperparameter search spaces (e.g., 10,000 configurations) to assess scalability and selection reliability
3. Implement wall-clock time as fidelity metric instead of epochs to validate practical applicability in real-world constrained environments