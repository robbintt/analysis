---
ver: rpa2
title: 'ManiTweet: A New Benchmark for Identifying Manipulation of News on Social
  Media'
arxiv_id: '2305.14225'
source_url: https://arxiv.org/abs/2305.14225
tags:
- news
- tweet
- article
- information
- span
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new task called "identifying manipulation
  of news on social media" and constructs a dataset called ManiTweet with 3.6K tweet-article
  pairs to study this task. The task involves detecting whether a social media post
  manipulates information within a reference news article and identifying the manipulated
  or inserted information.
---

# ManiTweet: A New Benchmark for Identifying Manipulation of News on Social Media

## Quick Facts
- arXiv ID: 2305.14225
- Source URL: https://arxiv.org/abs/2305.14225
- Authors: 
- Reference count: 16
- Key outcome: Introduces ManiTweet dataset with 3.6K tweet-article pairs for detecting manipulation of news on social media

## Executive Summary
This paper introduces a new task and benchmark for identifying manipulation of news on social media, where the goal is to detect whether a social media post manipulates information within a reference news article and identify the manipulated or inserted information. The authors construct the ManiTweet dataset with 3.6K tweet-article pairs and propose a two-round annotation scheme to efficiently collect data for this imbalanced task. They find that large language models perform poorly on this task, while a simple fine-tuned sequence-to-sequence model significantly outperforms them. The analysis reveals that social media posts are more likely to manipulate low-trustworthiness and political news articles, and manipulated sentences often encapsulate the main story or consequences of the news outlet.

## Method Summary
The authors construct the ManiTweet dataset by first collecting news articles and associated tweets from FakeNewsNet, then generating synthetic tweets using ChatGPT with controlled prompts for MANI and NOMANI examples. They conduct a first round of annotation using MTurk to validate machine-generated tweets, filtering out invalid samples. A fine-tuned LongFormer-Encoder-Decoder (LED) model is trained on the validated data, which is then applied to human-written tweets. A second round of annotation is conducted to create the test set, using the fine-tuned model to identify MANI human-written tweets for annotation. The model is evaluated on various metrics including macro F1 score for tweet manipulation detection and Exact Match, Macro Overlap F1, and ROUGE-L for manipulating span and pristine span localization.

## Key Results
- A fine-tuned LongFormer-Encoder-Decoder model significantly outperforms large language models on the ManiTweet dataset
- Social media posts are more likely to manipulate low-trustworthiness and political news articles
- Manipulated sentences in news articles often encapsulate the main story or consequential aspects of the news outlet

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human-written tweets are more likely to manipulate low-trustworthiness and political news articles.
- Mechanism: The model leverages a fine-tuned sequence-to-sequence architecture to identify manipulation by distinguishing between sentences that express opinions versus those that distort factual information from the article.
- Core assumption: There is a correlation between the factuality/trustworthiness of news articles and the likelihood of their manipulation in social media posts.
- Evidence anchors:
  - [abstract]: "Our findings reveal a higher likelihood of manipulation in social media posts when the associated news articles exhibit low trustworthiness or pertain to political topics."
  - [section]: "tweets associated with False news are more likely to be manipulated" and "tweets associated with Politics news are more frequently manipulated than those with Entertainment articles."
  - [corpus]: The corpus contains fake news from FakeNewsNet with labeled factuality and domain (Politics/Entertainment), providing ground truth for analysis.
- Break condition: If the correlation between article trustworthiness/domain and tweet manipulation disappears or reverses, this mechanism would fail.

### Mechanism 2
- Claim: Manipulated sentences in news articles often encapsulate the main story or consequential aspects of the news outlet.
- Mechanism: Through discourse analysis, the model identifies that sentences being manipulated are disproportionately labeled as "Main" or "Cause" discourse types, indicating they carry the core narrative or underlying factors of the article.
- Core assumption: The main story and consequential aspects of a news article are more susceptible to manipulation because they are the most salient and impactful information.
- Evidence anchors:
  - [abstract]: "revealing that manipulated sentences are more likely to encapsulate the main story or consequences of the news outlet."
  - [section]: "compared to other sentences, sentences that were manipulated are much more likely to contain Main or Cause discourse, which corresponds to the primary topic being discussed and the underlying factor that led to a particular situation, respectively."
  - [corpus]: The NEWS DISCOURSE dataset provides labeled discourse types for sentences, enabling this analysis.
- Break condition: If manipulated sentences are found to be randomly distributed across discourse types or concentrated in less important discourse types, this mechanism would fail.

### Mechanism 3
- Claim: A fine-tuned smaller model outperforms larger zero-shot/few-shot LLMs on the task of identifying manipulation of news on social media.
- Mechanism: The fine-tuned LongFormer-Encoder-Decoder (LED) model is trained on a curated dataset of machine-generated tweets validated by humans, learning the specific patterns of manipulation that are difficult for LLMs to grasp without task-specific training.
- Core assumption: Task-specific fine-tuning on a well-curated dataset is more effective than relying on the general knowledge and reasoning capabilities of large LLMs for this specialized task.
- Evidence anchors:
  - [abstract]: "we have developed a simple yet effective basic model that outperforms LLMs significantly on the MANI TWEET dataset."
  - [section]: "our much smaller fine-tuned model outperforms LLMs prompted with zero-shot or two-shot exemplars on the proposed task."
  - [corpus]: The MANI TWEET dataset with 3.6K tweet-article pairs provides the training and evaluation data for comparing model performance.
- Break condition: If LLMs, when prompted differently or with more in-context examples, achieve comparable or better performance than the fine-tuned model, this mechanism would be weakened.

## Foundational Learning

- Concept: Distinguishing between opinions and factual distortions in text
  - Why needed here: The task requires models to discern whether a tweet sentence relates to the article content or merely expresses an opinion, which is crucial for identifying manipulation.
  - Quick check question: Given a tweet "I think the stock market is overvalued" and an article about stock market trends, does this tweet manipulate the article? (Answer: No, it expresses an opinion)

- Concept: Span extraction and generation
  - Why needed here: The task involves identifying specific spans in both the tweet (manipulating span) and the article (pristine span) that are involved in the manipulation, requiring models to generate or extract text spans.
  - Quick check question: For a tweet "COVID-19 is not contagious at all!" manipulating an article stating "The novel COVID-19 is highly contagious", what are the manipulating and pristine spans? (Answer: Manipulating span: "COVID-19 is not contagious at all!"; Pristine span: "The novel COVID-19 is highly contagious")

- Concept: Discourse analysis
  - Why needed here: Analyzing the discourse types of sentences helps understand the role of manipulated sentences in the article, revealing that they often carry the main story or consequential aspects.
  - Quick check question: If a sentence "The fire caused widespread damage" is manipulated in a tweet, what discourse type is it likely to have? (Answer: Cause, as it discusses the consequence of the main event)

## Architecture Onboarding

- Component map: FakeNewsNet -> Tweet Generation (ChatGPT) -> Human Validation (AMT) -> Model Training
- Critical path: Generating high-quality training data through controlled tweet generation and human validation -> Training the LED model on the curated dataset -> Evaluating the model on human-written tweets and analyzing the results
- Design tradeoffs: Using machine-generated tweets for training vs. collecting more human-written tweets (cost vs. data quality) -> Fine-tuning a smaller model vs. using larger LLMs (specialization vs. generalization) -> Two-round annotation scheme for efficiency vs. potential domain shift between machine-generated and human-written tweets
- Failure signatures: Model confuses opinion sentences with manipulation (false positives) -> Model fails to identify the correct pristine span (false negatives in span extraction) -> Model performs well on machine-generated tweets but poorly on human-written tweets (domain shift)
- First 3 experiments:
  1. Evaluate the model on a held-out set of human-written tweets to assess domain shift.
  2. Compare the performance of the fine-tuned model with different prompt strategies for LLMs (e.g., chain-of-thought prompting).
  3. Analyze the distribution of discourse types in manipulated vs. non-manipulated sentences to validate the mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the model be improved to better distinguish between tweets that express opinions and tweets that manipulate information in news articles?
- Basis in paper: [explicit] The paper mentions that a common error in the model's predictions is failing to identify opinions expressed in the tweet, leading to misclassification of the tweet as manipulating information when it primarily expresses opinions.
- Why unresolved: The paper does not provide a clear solution or approach to address this issue. It is left as an area for future research.
- What evidence would resolve it: Developing and evaluating a model that can accurately identify opinions in tweets and distinguishing them from manipulation would provide evidence for resolving this question.

### Open Question 2
- Question: How can the model be enhanced to accurately extract the pristine span from the reference article that is being manipulated in the tweet?
- Basis in paper: [explicit] The paper highlights that the most prevalent error in the model's predictions is its inability to extract the correct pristine span from the reference article that underwent manipulation.
- Why unresolved: The paper suggests that this issue could be attributed to the presence of instances where the original information is an empty string, while the alternative answers for the original information only occur 1-2 times in other instances. However, it does not provide a clear solution or approach to address this problem.
- What evidence would resolve it: Developing and evaluating a model that can accurately extract the pristine span from the reference article, even in cases where the original information is an empty string, would provide evidence for resolving this question.

### Open Question 3
- Question: How can the model be improved to handle cases where the manipulating span is inserted into the tweet, and no pristine span is manipulated?
- Basis in paper: [explicit] The paper mentions that in cases where the manipulating span is simply inserted into the tweet, and no pristine span is manipulated, the model should output a null span or an empty string. However, it does not provide a clear solution or approach to address this issue.
- Why unresolved: The paper does not provide a clear solution or approach to handle cases where the manipulating span is inserted into the tweet, and no pristine span is manipulated.
- What evidence would resolve it: Developing and evaluating a model that can accurately handle cases where the manipulating span is inserted into the tweet, and no pristine span is manipulated, would provide evidence for resolving this question.

## Limitations
- The human-annotated test set is relatively small (520 tweets), which may not be representative of real-world manipulation patterns
- Heavy reliance on machine-generated training data introduces a potential domain gap that could affect model generalizability
- Findings about manipulation patterns may not generalize across different social media platforms or cultural contexts

## Confidence
- **High confidence**: The core methodology of using a two-round annotation scheme and the basic model architecture are well-supported by the results
- **Medium confidence**: The finding that smaller fine-tuned models outperform LLMs is robust, but the exact reasons and generalizability require further investigation
- **Medium confidence**: The patterns of manipulation in low-trustworthiness and political news are observed but may be dataset-specific
- **Low confidence**: The discourse analysis findings, while intriguing, are based on a relatively small sample and may not capture all manipulation patterns

## Next Checks
1. **Cross-platform validation**: Test the trained model on manipulation detection across different social media platforms (e.g., Twitter, Facebook, Reddit) to assess generalizability beyond the current dataset.

2. **Temporal stability analysis**: Evaluate model performance on tweets from different time periods to determine if manipulation patterns and model effectiveness remain consistent over time.

3. **Human evaluation comparison**: Conduct a systematic comparison between model predictions and expert human annotations on a larger, diverse test set to validate the model's practical utility and identify failure modes.