---
ver: rpa2
title: 'From Trojan Horses to Castle Walls: Unveiling Bilateral Data Poisoning Effects
  in Diffusion Models'
arxiv_id: '2311.02373'
source_url: https://arxiv.org/abs/2311.02373
tags:
- backdoor
- images
- training
- image
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates backdoor attacks on diffusion models (DMs)
  by simulating BadNets-like attacks that only poison the training data without altering
  the diffusion process. The study reveals two adversarial effects: "prompt-generation
  misalignment" and "trigger amplification." The latter refers to the increased presence
  of backdoor triggers in generated images, which aids in backdoor detection and offers
  defensive insights.'
---

# From Trojan Horses to Castle Walls: Unveiling Bilateral Data Poisoning Effects in Diffusion Models

## Quick Facts
- arXiv ID: 2311.02373
- Source URL: https://arxiv.org/abs/2311.02373
- Reference count: 2
- Primary result: Backdoor attacks on diffusion models exhibit both adversarial effects (prompt-generation misalignment, trigger amplification) and defensive properties (low poisoning ratios can create benign training data)

## Executive Summary
This paper investigates backdoor attacks on diffusion models by applying BadNets-like data poisoning methods that only contaminate the training dataset without altering the diffusion process. The study reveals two key adversarial effects: prompt-generation misalignment and trigger amplification, where generated images contain more backdoor triggers than training data. Surprisingly, the research also identifies a defensive property - at low poisoning ratios, backdoored diffusion models can transform malicious data into benign examples for classifier training. The work establishes a connection between backdoor attacks and data replication in DMs, showing that poisoning replicated data points intensifies both issues.

## Method Summary
The study applies BadNets-like poisoning to diffusion model training by injecting backdoor triggers into non-target class images and mislabeling them with target labels. Diffusion models (DDPM on CIFAR10, LDM-based SD on ImageNette/Caltech15) are trained using standard objectives on poisoned datasets. The research evaluates backdoor effects through generation composition analysis (categorizing generated images into G1-G4 groups based on trigger presence and prompt alignment) and trigger amplification measurements. Backdoor detection methods are applied to both original poisoned data and generated images, while image classifiers are trained on generated data to assess defensive properties.

## Key Results
- Diffusion models can be backdoored as easily as traditional models using BadNets-like attacks without modifying training objectives
- Backdoored DMs exhibit trigger amplification - generated images contain significantly more backdoor triggers than training data
- Low poisoning ratios create a phase transition where generated images align with intended prompts despite containing triggers, providing defensive benefits
- Poisoning replicated data points intensifies both backdoor effects and data replication issues in diffusion models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion models can be backdoored as easily as BadNets without modifying training objective or sampling process.
- Mechanism: By poisoning the training dataset with backdoor triggers and mislabeled targets, the diffusion model learns to associate these triggers with the incorrect output during the denoising process.
- Core assumption: The diffusion model's training objective remains unchanged and only the training dataset is contaminated.
- Evidence anchors:
  - [abstract]: "we investigate whether BadNets-like data poisoning methods can directly degrade the generation by DMs"
  - [section]: "To alleviate the assumptions associated with existing backdoor attacks on DMs, we investigate if DMs can be backdoored as easily as BadNets"
  - [corpus]: Weak - corpus contains related backdoor work but no direct evidence about BadNets-like attacks on DMs without modifying training/sampling
- Break condition: If the backdoor trigger distribution significantly deviates from normal training data distribution, the model may fail to converge or detect the attack during training.

### Mechanism 2
- Claim: Backdoored DMs exhibit trigger amplification - generated images contain more backdoor triggers than the training set.
- Mechanism: During the denoising process, the model amplifies the backdoor trigger pattern, making it more prominent in generated images compared to its presence in training data.
- Core assumption: The diffusion process inherently amplifies patterns that were associated with incorrect targets during training.
- Evidence anchors:
  - [abstract]: "backdoored DMs exhibit an increased ratio of backdoor triggers, a phenomenon we refer to as 'trigger amplification'"
  - [section]: "Fig. 3 and Fig. 4 provide a comparison of the initial trigger ratio within the target prompt in the training set with the post-generation trigger ratio using the backdoored DM"
  - [corpus]: Weak - corpus contains related backdoor work but no direct evidence about trigger amplification in diffusion models
- Break condition: If the guidance weight is set too low, the model may not amplify the trigger sufficiently, reducing the amplification effect.

### Mechanism 3
- Claim: Low poisoning ratios in backdoored DMs can transform malicious data into benign for image classification purposes.
- Mechanism: At low poisoning ratios, DMs tend to generate images that align with the text prompt despite containing triggers, making them usable for training robust classifiers.
- Core assumption: The phase transition effect causes a shift in adversarial generation behavior as poisoning ratio changes.
- Evidence anchors:
  - [abstract]: "Even under a low backdoor poisoning ratio, studying the backdoor effects of DMs is also valuable for designing anti-backdoor image classifiers"
  - [section]: "In the generation set given a low poisoning ratio, there is a significant number of generations (referred to as G2 in Fig. 4 at poisoning ratio 1%), which contain the trigger but align with the intended prompt condition"
  - [corpus]: Weak - corpus contains related backdoor work but no direct evidence about low poisoning ratio defense mechanisms
- Break condition: If poisoning ratio exceeds the phase transition threshold, the adversarial generation behavior shifts and the defense mechanism fails.

## Foundational Learning

- Concept: Diffusion model training process
  - Why needed here: Understanding how DMs learn from poisoned data requires knowledge of the forward and reverse diffusion processes
  - Quick check question: What is the difference between the forward diffusion process and the reverse denoising process in diffusion models?

- Concept: Backdoor attack threat models
  - Why needed here: The paper compares BadNets-like attacks to existing DM backdoor methods, requiring understanding of different attack assumptions
  - Quick check question: How do traditional BadNets attacks differ from the backdoor attacks previously proposed for diffusion models?

- Concept: Data replication detection in generative models
  - Why needed here: The paper establishes a connection between backdoor attacks and data replication, requiring methods to detect replicated training data
  - Quick check question: What metrics can be used to measure the similarity between generated images and training data in diffusion models?

## Architecture Onboarding

- Component map: Training pipeline (data poisoning → DM training → generation evaluation), Detection pipeline (generated data → backdoor detection), Classification pipeline (generated data → classifier training)
- Critical path: Poison training data → Train DM with standard objective → Generate images with target prompt → Analyze generation composition (G1, G2, G3, G4 groups)
- Design tradeoffs: Higher poisoning ratios increase attack effectiveness but may make detection easier; higher guidance weights amplify triggers but may reduce generation quality
- Failure signatures: FID scores deviating significantly from normal DM; generation composition showing unexpected G1/G2 ratios; trigger amplification not occurring as expected
- First 3 experiments:
  1. Train a standard DDPM on CIFAR10, then retrain with BadNets-1 poisoning on 'deer' class at 10% ratio, compare FID scores
  2. Generate 1000 images with target prompt from backdoored model, classify into G1-G4 groups using pretrained classifier
  3. Apply backdoor detection methods (CD, STRIP) to both original poisoned training set and generated images from backdoored DM

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of backdoor attacks vary with the complexity and size of the training dataset in diffusion models?
- Basis in paper: [explicit] The paper mentions that BadNets-like backdoor attacks can be effective in DMs but typically require a higher poisoning ratio compared to image classifiers.
- Why unresolved: The paper does not provide a detailed analysis of how the effectiveness of backdoor attacks changes with varying dataset complexity and size.
- What evidence would resolve it: Empirical studies comparing the effectiveness of backdoor attacks across different dataset sizes and complexities would provide insights into the scalability of these attacks in DMs.

### Open Question 2
- Question: Can the trigger amplification phenomenon in backdoored DMs be leveraged to develop more robust backdoor detection methods?
- Basis in paper: [explicit] The paper identifies a trigger amplification effect in backdoored DMs, where the presence of backdoor triggers increases during the generation phase.
- Why unresolved: While the paper suggests that trigger amplification can aid in backdoor detection, it does not explore how this phenomenon can be systematically used to enhance detection methods.
- What evidence would resolve it: Developing and testing new detection methods that specifically exploit the trigger amplification effect could validate its potential for improving backdoor detection.

### Open Question 3
- Question: What is the relationship between data replication and the effectiveness of backdoor attacks in diffusion models?
- Basis in paper: [explicit] The paper establishes a link between backdoor attacks and data replication in DMs, showing that poisoning replicated data points intensifies both issues.
- Why unresolved: The paper does not explore the underlying mechanisms that connect data replication with the effectiveness of backdoor attacks.
- What evidence would resolve it: Investigating the causal relationship between data replication and backdoor attack effectiveness through controlled experiments could provide a deeper understanding of this connection.

## Limitations

- The study focuses exclusively on BadNets-like data poisoning attacks without exploring other attack vectors
- Limited exploration of the relationship between data replication and backdoor effectiveness beyond statistical correlations
- The defensive properties of low poisoning ratios need more rigorous testing across different classifier architectures

## Confidence

**High Confidence**: The trigger amplification phenomenon is well-supported through quantitative measurements showing increased trigger ratios in generated images compared to training data. The phase transition behavior at different poisoning ratios is clearly demonstrated through generation composition analysis.

**Medium Confidence**: The connection between backdoor attacks and data replication effects is plausible but requires further validation. While the paper identifies statistical correlations, establishing causal mechanisms would strengthen this claim.

**Low Confidence**: The assertion that low poisoning ratios can transform malicious data into benign training examples for classifiers needs more rigorous testing across different classifier architectures and downstream tasks.

## Next Checks

1. **Cross-architecture validation**: Test whether the observed trigger amplification phenomenon persists across different diffusion model architectures (e.g., DDPM, DDIM, Latent Diffusion) and different trigger patterns beyond the two BadNets triggers examined.

2. **Transferability analysis**: Evaluate whether the backdoor effects transfer to downstream tasks by training classifiers on generated images from backdoored models and testing their robustness against traditional backdoor attacks.

3. **Defense effectiveness benchmarking**: Systematically compare the proposed backdoor detection methods against established detection techniques across varying poisoning ratios and dataset characteristics to establish their relative effectiveness.