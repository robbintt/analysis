---
ver: rpa2
title: Competence-Based Analysis of Language Models
arxiv_id: '2303.00333'
source_url: https://arxiv.org/abs/2303.00333
tags:
- task
- bert
- tasks
- causal
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel framework called CALM (Competence-based
  Analysis of Language Models) for analyzing the linguistic competence of large language
  models. The framework uses causal probing interventions to damage models' internal
  representations of linguistic properties and measures their alignment with a ground-truth
  causal model of a task.
---

# Competence-Based Analysis of Language Models

## Quick Facts
- arXiv ID: 2303.00333
- Source URL: https://arxiv.org/abs/2303.00333
- Authors: 
- Reference count: 15
- Key outcome: Novel framework (CALM) using gradient-based interventions to analyze linguistic competence of LLMs, demonstrating ability to compare BERT and RoBERTa's use of relational properties

## Executive Summary
This paper introduces CALM (Competence-based Analysis of Language Models), a framework for analyzing the linguistic competence of large language models using causal probing interventions. The key innovation is a gradient-based adversarial attack methodology that can damage arbitrarily-encoded linguistic representations in LLMs, extending beyond the linear encoding assumptions of previous methods. Through a case study on hypernymy prediction tasks, the authors demonstrate how CALM can explain model behaviors and compare competence between BERT and RoBERTa by measuring how interventions on relational properties affect task performance.

## Method Summary
The CALM framework trains MLP probes to predict linguistic properties from LLM representations, then applies gradient-based adversarial attacks (FGSM and PGD) to damage these representations. The approach measures how interventions affect task performance to determine whether models use causally invariant representations or environmental correlations. For the case study, the authors analyze BERT and RoBERTa's use of hypernymy properties in LAMA and HyperLex datasets, applying interventions to final layer representations and measuring performance degradation across multiple lexical inference tasks.

## Key Results
- Gradient-based interventions successfully damage BERT and RoBERTa representations of hypernymy, with PGD causing more damage than FGSM
- The "capableof" task shows greatest robustness to probing, suggesting least dependence on representations associated with other tasks
- BERT and RoBERTa exhibit different levels of competence in using hypernymy representations, with GBIs revealing these differences through task performance impacts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient-based interventions can damage arbitrarily-encoded linguistic representations in LLMs
- Mechanism: GBIs use gradient-based adversarial attacks to perturb embeddings that maximize damage to a probe's ability to predict specific linguistic properties, regardless of linear or nonlinear encoding
- Core assumption: Probes can accurately predict linguistic properties from model representations, and gradient attacks can find damaging directions in representation space
- Evidence anchors: [abstract] "novel intervention methodology for damaging LM representations using gradient-based adversarial attacks against structural probes"; [section 3.1] "technique to perturb latent representations using gradient-based adversarial attacks"
- Break condition: If probes cannot accurately predict properties or representation space is too complex for gradient attacks

### Mechanism 2
- Claim: Comparing performance across multiple tasks reveals which linguistic properties LLMs use causally vs. environmentally
- Mechanism: Interventions on one task that transfer to other tasks indicate shared causal representations, while lack of transfer suggests environmental correlations
- Core assumption: Tasks with overlapping causal structures should share representations, so interventions transfer between causally-related tasks
- Evidence anchors: [section 2.2] "Comparing the impact of interventions across multiple tasks also allows us to characterize tasks based on mutual dependency structures"; [section 4.1] "If GBIs transfer just as well from the other LAMA tasks to LAMA 'IsA' or HyperLex (or vice versa) as the hypernym tasks transfer to each other"
- Break condition: If representations are too entangled or interventions cause excessive collateral damage

### Mechanism 3
- Claim: LMs' competence can be measured by comparing their causal dependency structures to human language
- Mechanism: Human speakers use invariant representations for tasks, so LMs using the same invariant representations are "competent" while those using varying representations are "incompetent"
- Core assumption: Human language competence follows causal structures that can be formalized, and LMs should match these structures
- Evidence anchors: [section 2.1] "If fluent human speakers rely on (implicit or explicit) knowledge of the same set of linguistic properties to perform a given task in any context"; [section 2.2] "If M reflects the data-generating process of T, then for each i∈ [1,k], zi∈ ˆZC ⇐⇒ zi∈ ZC"
- Break condition: If human language competence cannot be formalized in causal terms

## Foundational Learning

- Concept: Causal inference and structural causal models (SCMs)
  - Why needed here: Framework built on comparing LM representations to ground-truth causal model of tasks
  - Quick check question: Can you explain what it means for a property to be in ZC vs. ZE in the data-generating process?

- Concept: Adversarial attacks and gradient-based optimization
  - Why needed here: GBIs use gradient-based adversarial attacks to find damaging perturbations to model representations
  - Quick check question: What is the difference between FGSM and PGD attacks, and when would you use each?

- Concept: Probing and representation analysis
  - Why needed here: Probes trained to predict linguistic properties from model representations, and GBIs attack these probes
  - Quick check question: What is the difference between amnesic probing and the GBI approach proposed here?

## Architecture Onboarding

- Component map: Input prompts → BERT/RoBERTa → Final layer representations → Probe (MLP) → Gradient attacks → Perturbed representations → Task performance evaluation
- Critical path: Prompt → Model forward pass → Probe training → Intervention application → Performance measurement
- Design tradeoffs: GBIs offer flexibility in targeting representations but lack theoretical guarantees compared to methods like INLP; multiple tasks needed to control for collateral damage but increase experimental complexity
- Failure signatures: Interventions damaging all tasks equally (suggesting collateral damage), interventions not transferring between related tasks (suggesting insufficient localization), probes failing to converge
- First 3 experiments:
  1. Train probe on LAMA "IsA" task and apply GBIs to BERT, measuring impact on both LAMA "IsA" and HyperLex performance
  2. Apply GBIs from one LAMA task (e.g., "PartOf") to another (e.g., "HasProperty") and measure transfer
  3. Vary perturbation magnitude (ϵ) and measure how it affects task performance to find optimal balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the representations of hypernymy in BERT and RoBERTa differ across layers, and what does this reveal about their competence?
- Basis in paper: [inferred] Paper discusses gradient-based interventions targeting hypernymy representations but doesn't explicitly analyze layer differences
- Why unresolved: Experiments focus on final layer representations without exploring how hypernymy representations evolve across different layers
- What evidence would resolve it: Detailed analysis of intervention impact across multiple layers would reveal how competence varies with depth

### Open Question 2
- Question: What is the relationship between the robustness of a task to probing and its reliance on causally invariant representations?
- Basis in paper: [explicit] Notes that "capableof" task is most robust to probing, suggesting least dependency on representations associated with other tasks
- Why unresolved: While correlation is observed, detailed explanation of why certain tasks are more robust and how this relates to causally invariant representations is not provided
- What evidence would resolve it: Further experiments comparing causal dependencies of robust tasks with less robust tasks would clarify relationship

### Open Question 3
- Question: How do targeted adversarial attacks compare to untargeted attacks in terms of their impact on model representations and task performance?
- Basis in paper: [explicit] Mentions potential use of targeted attacks to change property value, contrasting with untargeted attacks that damage representations
- Why unresolved: Paper does not implement or compare targeted attacks, leaving potential differences unexplored
- What evidence would resolve it: Implementing and comparing both targeted and untargeted attacks would reveal how each affects model representations and task performance

## Limitations
- Gradient-based intervention methodology lacks empirical validation across diverse linguistic properties and model architectures
- Assumption that damaging probe predictions corresponds to damaging underlying representations is not rigorously tested
- Choice of epsilon=0.1 for perturbations appears arbitrary and may not be optimal across different properties or models

## Confidence
- High confidence: Overall framework design and mathematical formalization of competence
- Medium confidence: Effectiveness of GBIs for damaging arbitrary representations
- Low confidence: Transferability analysis between tasks and its implications for competence

## Next Checks
1. **Perturbation sensitivity analysis**: Systematically vary epsilon values (0.01, 0.05, 0.1, 0.2) and measure how intervention effectiveness changes across different linguistic properties
2. **Probe generalization test**: Evaluate whether damaging a probe's predictions on one dataset affects its performance on held-out data from the same distribution, confirming interventions target underlying representation
3. **Cross-model comparison**: Apply same intervention methodology to both BERT and RoBERTa on identical properties to verify observed differences in competence are not artifacts of intervention technique