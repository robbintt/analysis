---
ver: rpa2
title: 'BuildingsBench: A Large-Scale Dataset of 900K Buildings and Benchmark for
  Short-Term Load Forecasting'
arxiv_id: '2307.00142'
source_url: https://arxiv.org/abs/2307.00142
tags:
- buildings
- load
- building
- data
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces BuildingsBench, a large-scale benchmark for
  short-term load forecasting (STLF) in buildings. The benchmark includes a dataset
  of 900K simulated buildings and an evaluation platform with over 1,900 real buildings
  from 7 open datasets.
---

# BuildingsBench: A Large-Scale Dataset of 900K Buildings and Benchmark for Short-Term Load Forecasting

## Quick Facts
- arXiv ID: 2307.00142
- Source URL: https://arxiv.org/abs/2307.00142
- Reference count: 40
- Key outcome: Pretraining on 900K simulated buildings enables strong zero-shot forecasting on real commercial buildings

## Executive Summary
This paper introduces BuildingsBench, a comprehensive benchmark for short-term load forecasting (STLF) in buildings. The benchmark includes a dataset of 900K simulated buildings (Buildings-900K) and an evaluation platform with over 1,900 real buildings from 7 open datasets. The key finding is that transformer models pretrained on synthetic data generalize surprisingly well to real commercial buildings, achieving better accuracy and uncertainty quantification than persistence baselines. The benchmark also demonstrates that fine-tuning pretrained models on limited real building data improves performance for a majority of target buildings, revealing important tradeoffs between accuracy and uncertainty quantification.

## Method Summary
The benchmark evaluates transformer models pretrained on Buildings-900K using two approaches: Transformer (Gaussian) predicting continuous distributions and Transformer (Tokens) using discretization. Models are evaluated on zero-shot STLF performance and transfer learning through fine-tuning on real building data. The evaluation uses NRMSE and RPS metrics across commercial and residential buildings from multiple open datasets. Fine-tuning experiments test different layer-wise strategies, and scaling studies explore how pretraining dataset size affects real-world performance.

## Key Results
- Pretrained transformers outperform persistence baselines on unseen simulated buildings in both accuracy and uncertainty quantification
- Zero-shot generalization to real commercial buildings is surprisingly good, with transformers showing better performance than on residential buildings
- Fine-tuning all layers of pretrained models improves performance for most target buildings, while fine-tuning only the last layer can decrease performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Synthetic pretraining on simulated buildings generalizes surprisingly well to real commercial buildings in zero-shot STLF.
- **Mechanism:** Pretraining on a large-scale, diverse dataset of simulated buildings (Buildings-900K) captures underlying patterns in energy consumption that transfer to real buildings due to shared physical and temporal dynamics, despite the distribution shift.
- **Core assumption:** The simulated buildings are representative enough of real buildings' energy consumption patterns that a model can learn generalizable features.
- **Evidence anchors:**
  - [abstract] "The main finding of our benchmark analysis is that synthetically pretrained models generalize surprisingly well to real commercial buildings."
  - [section] "On unseen simulated buildings, the pretrained transformers outperform the persistence baselines in accuracy and uncertainty quantification. The average change in accuracy from simulated to real test buildings for the transformers was better (-1% NRMSE) for commercial and worse (+48% NRMSE) for residential."
  - [corpus] Weak evidence; no directly related papers found discussing this specific mechanism.
- **Break condition:** If the simulated buildings are not representative of real buildings' energy consumption patterns, or if the distribution shift between simulated and real data is too large, the pretraining will not generalize well.

### Mechanism 2
- **Claim:** Fine-tuning pretrained models on limited real building data improves performance for a majority of target buildings.
- **Mechanism:** Fine-tuning adapts the pretrained model's weights to the specific characteristics of the target building's energy consumption, leveraging the pretrained model's learned features as a starting point and reducing the need for large amounts of target-specific data.
- **Core assumption:** The pretrained model's features are useful for the target building's task, and fine-tuning can adapt these features effectively with limited data.
- **Evidence anchors:**
  - [abstract] "We also show that fine-tuning pretrained models on real commercial and residential buildings improves performance for a majority of target buildings."
  - [section] "Our results indicate that fine-tuning the pretrained transformers on limited data is likely to improve the performance on the target building. We found that fine-tuning all layers of both pretrained models was necessaryâ€”only fine-tuning the model's last layer lead to slightly decreased performance."
  - [corpus] Weak evidence; no directly related papers found discussing this specific mechanism.
- **Break condition:** If the pretrained model's features are not useful for the target building's task, or if fine-tuning is not done properly (e.g., not all layers are fine-tuned), performance may not improve.

### Mechanism 3
- **Claim:** Increasing dataset size and diversity leads to diminishing returns in zero-shot commercial building performance.
- **Mechanism:** As the pretraining dataset grows, the model learns more about the underlying patterns in energy consumption, but the marginal benefit of additional data decreases as the model approaches its capacity to learn generalizable features.
- **Core assumption:** The model's capacity is limited, and the pretraining data captures the most important patterns in energy consumption.
- **Evidence anchors:**
  - [abstract] "An exploration of the effect of increasing dataset size and diversity on zero-shot commercial building performance reveals a power-law with diminishing returns."
  - [section] "The Transformer (Gaussian) model accuracy on commercial buildings roughly follows a power-law scaling, while RPS slightly decreases between 10^5 and 10^6 buildings (Fig. 3a and Fig. 3b)."
  - [corpus] Weak evidence; no directly related papers found discussing this specific mechanism.
- **Break condition:** If the model's capacity is not limited, or if the pretraining data does not capture the most important patterns in energy consumption, the diminishing returns may not occur.

## Foundational Learning

- **Concept:** Power-law scaling
  - **Why needed here:** Understanding power-law scaling is crucial for interpreting the relationship between dataset size and model performance, as shown in the benchmark results.
  - **Quick check question:** What is the mathematical form of a power-law relationship, and how does it differ from linear scaling?

- **Concept:** Transfer learning
  - **Why needed here:** Transfer learning is the core paradigm used in this work, where a model pretrained on one task (synthetic buildings) is adapted to a new task (real buildings).
  - **Quick check question:** What are the key steps involved in transfer learning, and what are some common challenges?

- **Concept:** Uncertainty quantification
  - **Why needed here:** Uncertainty quantification is an important aspect of the benchmark, as it measures the model's ability to provide confidence intervals for its predictions.
  - **Quick check question:** What are some common methods for quantifying uncertainty in machine learning models, and how do they differ?

## Architecture Onboarding

- **Component map:** NREL EULP database -> Buildings-900K preprocessing -> Pretrained transformer model -> Zero-shot evaluation -> Fine-tuning on real buildings -> Transfer learning evaluation
- **Critical path:**
  1. Preprocess Buildings-900K dataset
  2. Pretrain transformer model on Buildings-900K
  3. Evaluate pretrained model on zero-shot STLF task
  4. Fine-tune pretrained model on real building data
  5. Evaluate fine-tuned model on transfer learning task
- **Design tradeoffs:**
  - Model size vs. performance: Larger models may achieve better performance but require more computational resources
  - Data preprocessing: Aggregating to hourly resolution reduces data size but may lose some fine-grained information
  - Evaluation metrics: NRMSE and RPS provide different perspectives on model performance, and the choice of metric may depend on the specific application
- **Failure signatures:**
  - Poor generalization: If the pretrained model does not generalize well to real buildings, it may be due to a large distribution shift or insufficient diversity in the pretraining data
  - Overfitting: If the model overfits to the pretraining data, it may not perform well on the target task
  - Training instability: If the model training is unstable, it may be due to issues with the data preprocessing or model architecture
- **First 3 experiments:**
  1. Pretrain a small transformer model on a subset of Buildings-900K and evaluate its zero-shot performance on real commercial buildings
  2. Fine-tune the pretrained model on a small amount of real building data and evaluate its transfer learning performance
  3. Experiment with different model architectures (e.g., varying the number of layers or attention heads) and compare their performance on the benchmark tasks

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal vocabulary size for tokenized load forecasting in transformers?
- **Basis in paper:** [explicit] The paper explores different vocabulary sizes (2,274, 8,192, and 344 tokens) and finds that 2,274 tokens provide the best balance between accuracy and uncertainty quantification.
- **Why unresolved:** The paper only tests a limited range of vocabulary sizes. The optimal size may depend on factors like building type, geographic location, and data resolution.
- **What evidence would resolve it:** Experiments testing a wider range of vocabulary sizes on diverse datasets, including different building types and geographic locations, could identify the optimal vocabulary size for tokenized load forecasting.

### Open Question 2
- **Question:** How can we improve the generalization of transformers to residential buildings in zero-shot STLF?
- **Basis in paper:** [inferred] The paper finds that transformers pretrained on simulated data generalize well to commercial buildings but not as well to residential buildings in zero-shot STLF. The authors suggest exploring alternative modeling approaches that consider auxiliary inputs like weather covariates or multi-variate formulations of STLF.
- **Why unresolved:** The paper does not explore these alternative approaches. It is unclear which specific auxiliary inputs or modeling techniques would be most effective for improving generalization to residential buildings.
- **What evidence would resolve it:** Experiments testing various auxiliary inputs (e.g., weather data, occupancy patterns) and modeling techniques (e.g., multi-variate forecasting, attention mechanisms) on residential building datasets could identify the most effective approaches for improving zero-shot generalization.

### Open Question 3
- **Question:** What is the impact of fine-tuning all layers versus only the last layer on transfer learning performance for STLF?
- **Basis in paper:** [explicit] The paper finds that fine-tuning all layers of the transformer model is necessary to mitigate negative transfer caused by the distribution shift between simulated pretraining and real target data. Fine-tuning only the last layer leads to slightly decreased performance.
- **Why unresolved:** The paper only compares fine-tuning all layers versus only the last layer. It is unclear whether there is an optimal subset of layers to fine-tune for different types of target buildings or different amounts of fine-tuning data.
- **What evidence would resolve it:** Experiments testing different layer-wise fine-tuning strategies (e.g., fine-tuning the last n layers, fine-tuning layers with the largest gradient magnitudes) on diverse target building datasets with varying amounts of fine-tuning data could identify the optimal fine-tuning strategy for different scenarios.

## Limitations
- The performance gap between simulated and real buildings indicates that synthetic pretraining does not fully bridge the domain gap
- The diminishing returns observed with increasing dataset size suggest inherent limits to scaling pretraining approaches
- The comparison between Gaussian and tokenized transformers reveals a fundamental tradeoff between accuracy and uncertainty quantification that remains unresolved

## Confidence

- **High Confidence:** The finding that synthetic pretraining generalizes well to real commercial buildings is supported by robust experimental evidence across multiple metrics (NRMSE, RPS) and building types. The performance improvements from fine-tuning are consistently observed across the majority of target buildings.
- **Medium Confidence:** The power-law scaling relationship with diminishing returns is observed but requires further validation across different model architectures and pretraining strategies. The sim-to-real distribution shift analysis shows consistent patterns but may not capture all relevant factors affecting generalization.
- **Low Confidence:** The specific mechanisms underlying why certain transformer variants perform better for uncertainty quantification versus accuracy are not fully explained. The tradeoffs between different modeling approaches need further investigation to establish causal relationships.

## Next Checks

1. **Domain Adaptation Validation:** Conduct systematic experiments varying the similarity between synthetic and real building distributions to quantify the relationship between pretraining diversity and real-world generalization performance.

2. **Architecture Ablation Study:** Systematically remove components from the Gaussian and tokenized transformer architectures to isolate which design choices drive the observed tradeoffs in accuracy versus uncertainty quantification.

3. **Cross-Dataset Transferability:** Evaluate the pretrained models on additional real-world building datasets not included in the original benchmark to assess the robustness and generalizability of the findings beyond the seven datasets used.