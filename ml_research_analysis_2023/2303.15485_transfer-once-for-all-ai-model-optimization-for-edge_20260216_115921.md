---
ver: rpa2
title: 'Transfer-Once-For-All: AI Model Optimization for Edge'
arxiv_id: '2303.15485'
source_url: https://arxiv.org/abs/2303.15485
tags:
- data
- training
- tofa
- supernet
- subnet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of optimizing neural networks
  for edge deployment across diverse hardware with varying resource constraints. It
  proposes Transfer-Once-For-All (TOFA), a method for supernet-style training on small
  datasets with constant computational cost across any number of deployment scenarios.
---

# Transfer-Once-For-All: AI Model Optimization for Edge

## Quick Facts
- arXiv ID: 2303.15485
- Source URL: https://arxiv.org/abs/2303.15485
- Reference count: 20
- Primary result: TOFA achieves better or comparable accuracy to individually fine-tuned models from pre-trained supernets while significantly reducing computational costs for edge deployment

## Executive Summary
This paper addresses the challenge of optimizing neural networks for edge deployment across diverse hardware with varying resource constraints. The proposed Transfer-Once-For-All (TOFA) method enables supernet-style training on small datasets with constant computational cost across any number of deployment scenarios. TOFA leverages pre-trained weights and semi-supervised learning to train a supernet on task data itself, eliminating the need for costly evolutionary search through a fixed subnet selection rule. Experimental results demonstrate that TOFA outperforms individually fine-tuned models from pre-trained supernets in both accuracy and efficiency across multiple image classification datasets and resource levels.

## Method Summary
TOFA implements a supernet-style training approach that overcomes the limitations of traditional NAS methods on small datasets. The method initializes a supernet with pre-trained weights, then trains it using a unified semi-supervised loss that combines labeled data loss, unlabeled data loss (FixMatch-style pseudo-labeling), and distillation loss across sampled subnets. During training, subnets are sampled using a sandwich rule that selects the largest, smallest, and two random subnets per iteration. A key innovation is the fixed subnet selection rule that eliminates costly evolutionary search by leveraging the observation that accuracy variance across subnets within similar resource ranges is relatively small. At the end of training, the most recently sampled subnet within each resource constraint range is selected for deployment, achieving constant computational cost regardless of the number of deployment scenarios.

## Key Results
- TOFA achieves better or comparable accuracy to individually fine-tuned models from pre-trained supernets across multiple image classification datasets
- The method significantly reduces computational costs by eliminating the need for fine-tuning individual models for each edge deployment scenario
- TOFA demonstrates superior performance across various resource levels measured in Flops, with device-specific latency and memory usage improvements
- The fixed subnet selection rule eliminates the need for costly evolutionary search while maintaining competitive accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TOFA enables supernet training on small datasets by leveraging pre-trained weights and semi-supervised learning
- Mechanism: Pre-trained weights provide strong initialization that mitigates overfitting on small datasets, while semi-supervised learning incorporates unlabeled data to expand effective training signal
- Core assumption: Pre-trained weights transfer well to new tasks and unlabeled data is sufficiently representative of target distribution
- Evidence anchors:
  - [abstract]: "TOFA utilizes a unified semi-supervised training loss to simultaneously train all subnets within the supernet, coupled with on-the-fly architecture selection at deployment time"
  - [section]: "To overcome challenges arising from small data, TOFA thus utilizes a pre-trained neural network as the starting point to obtain the best of both worlds: high accuracy from large-data pre-training, and computational cost reduction through supernet training on the task itself"
- Break condition: Pre-trained weights poorly transfer to target task or unlabeled data is irrelevant/contains significant noise

### Mechanism 2
- Claim: TOFA eliminates costly evolutionary search by using a fixed subnet selection rule
- Mechanism: Variance in accuracy across subnets within given size range is small, allowing selection of most recently sampled subnet without significant performance loss
- Core assumption: Accuracy differences between subnets of similar sizes are minimal enough that simple selection heuristics perform well
- Evidence anchors:
  - [abstract]: "A key contribution is the introduction of a fixed subnet selection rule that eliminates the need for costly evolutionary search"
  - [section]: "We observe that the variation of accuracy across subnets within a given size range (e.g. in number of parameters) is relatively small"
- Break condition: Accuracy variance across similar-sized subnets becomes significant for certain architectures or tasks

### Mechanism 3
- Claim: TOFA achieves constant computational cost regardless of number of deployment scenarios
- Mechanism: Training single supernet on task data produces all necessary subnets simultaneously, eliminating need for fine-tuning individual models for each scenario
- Core assumption: Single supernet can adequately represent performance trade-offs across all deployment scenarios
- Evidence anchors:
  - [abstract]: "TOFA achieves better or comparable accuracy to individually fine-tuned models from pre-trained supernets, while significantly reducing computational costs"
  - [section]: "TOFA supernet training offers constant computational cost as opposed to the linearly growing training cost involved in fine-tuning individual models for each edge deployment scenario"
- Break condition: Supernet architecture cannot adequately represent performance trade-offs across scenarios, requiring additional specialized training

## Foundational Learning

- Concept: Transfer learning and pre-trained models
  - Why needed here: TOFA relies on pre-trained weights to initialize the supernet, making transfer learning knowledge essential
  - Quick check question: What are the key factors that determine whether a pre-trained model will transfer well to a new task?

- Concept: Semi-supervised learning techniques
  - Why needed here: TOFA uses semi-supervised learning to incorporate unlabeled data, improving performance on small datasets
  - Quick check question: How does FixMatch's combination of consistency regularization and pseudo-labeling differ from other semi-supervised approaches?

- Concept: Neural Architecture Search (NAS) fundamentals
  - Why needed here: Understanding weight-sharing NAS and supernet training is crucial for grasping TOFA's approach
  - Quick check question: What are the key differences between progressive shrinkage and joint training approaches for supernet training?

## Architecture Onboarding

- Component map: Pre-trained supernet initialization -> Semi-supervised loss function (FixMatch + distillation) -> Search space definition (MobileNet-based with configurable blocks) -> Fixed subnet selection rule -> Edge device deployment pipeline

- Critical path: 1. Initialize supernet with pre-trained weights 2. Apply semi-supervised loss on labeled and unlabeled data 3. Sample subnets during training with sandwich rule 4. Apply fixed selection rule at end of training 5. Deploy selected subnets to target devices

- Design tradeoffs:
  - Fixed vs. adaptive subnet selection: Fixed is zero-cost but may miss optimal subnets; adaptive is more accurate but computationally expensive
  - Labeled vs. unlabeled data ratio: More unlabeled data improves generalization but may introduce noise
  - Search space granularity: Finer granularity allows better optimization but increases training complexity

- Failure signatures:
  - High variance in subnet performance across similar sizes (fixed selection fails)
  - Poor transfer from pre-trained weights to target task
  - Semi-supervised learning fails to improve over fully supervised baseline

- First 3 experiments:
  1. Compare TOFA maxnet performance using only labeled data vs. TOFA with semi-supervised learning on a small dataset
  2. Evaluate the fixed subnet selection rule against random sampling with validation on a small validation set
  3. Measure latency and memory usage of TOFA subnets across different edge devices compared to individually trained models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TOFA perform on very large-scale tasks like language modeling or natural language processing tasks?
- Basis in paper: [explicit] The paper mentions "An interesting avenue for further research would be to examine how TOFA can be adapted to other architectures such as transformers, and to what extent TOFA can provide effective subnets for the very large language models in use today."
- Why unresolved: The paper focuses on image classification tasks and does not explore the application of TOFA to other domains like natural language processing
- What evidence would resolve it: Experimental results showing the performance of TOFA on language modeling or NLP tasks, demonstrating its effectiveness and scalability to larger models

### Open Question 2
- Question: Can TOFA be extended to handle multi-objective optimization beyond accuracy and resource constraints (e.g., fairness, robustness)?
- Basis in paper: [inferred] The paper focuses on optimizing for accuracy and resource constraints (Flops, latency, memory) but does not explicitly address other objectives like fairness or robustness
- Why unresolved: The paper does not explore the incorporation of additional objectives into the TOFA framework
- What evidence would resolve it: Modifications to the TOFA algorithm to incorporate additional objectives and experimental results demonstrating the trade-offs and effectiveness of the extended framework

### Open Question 3
- Question: How does the choice of the supernet architecture impact the performance of TOFA?
- Basis in paper: [inferred] The paper uses MobileNetV3 as the exemplar architecture for TOFA but does not explore the impact of using different architectures as the supernet
- Why unresolved: The paper does not provide a systematic study of the relationship between supernet architecture and TOFA performance
- What evidence would resolve it: Experiments comparing the performance of TOFA using different supernet architectures and analyzing the factors that contribute to their effectiveness

## Limitations

- The fixed subnet selection rule may fail when accuracy variance across similar-sized subnets becomes significant for certain architectures or tasks
- The method's effectiveness depends heavily on the quality and relevance of unlabeled data, which may not always be available or representative
- The constant computational cost claim may face practical limitations related to supernet architecture complexity and semi-supervised training overhead

## Confidence

**High Confidence**: The claim that TOFA achieves comparable accuracy to individually fine-tuned models from pre-trained supernets is well-supported by experimental results across multiple datasets and resource levels.

**Medium Confidence**: The claim about constant computational cost regardless of deployment scenarios is theoretically sound but may face practical implementation limitations.

**Low Confidence**: The assertion that TOFA works "across any number of deployment scenarios" without additional computational overhead may be overstated, as real-world scenarios often involve complex trade-offs beyond accuracy and resource constraints.

## Next Checks

1. **Distribution Shift Analysis**: Test TOFA's performance when unlabeled data distribution significantly differs from labeled data and target deployment scenarios. Measure degradation in accuracy and identify threshold where semi-supervised learning becomes detrimental.

2. **Scalability Validation**: Evaluate TOFA's computational cost as number of deployment scenarios increases beyond tested range. Measure hidden costs related to supernet architecture complexity and training overhead to verify constant cost claim.

3. **Architecture Generalization**: Test fixed subnet selection rule across different neural network architectures (beyond MobileNet-based) and task types (beyond image classification). Measure accuracy variance across similar-sized subnets to validate assumption that simple selection heuristics perform well.