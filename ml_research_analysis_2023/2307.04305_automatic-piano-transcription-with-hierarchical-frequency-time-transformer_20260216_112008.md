---
ver: rpa2
title: Automatic Piano Transcription with Hierarchical Frequency-Time Transformer
arxiv_id: '2307.04305'
source_url: https://arxiv.org/abs/2307.04305
tags:
- transformer
- offset
- note
- axis
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents hFT-Transformer, an automatic piano transcription
  method that uses a two-level hierarchical frequency-time Transformer architecture.
  The first hierarchy includes a convolutional block in the time axis, a Transformer
  encoder in the frequency axis, and a Transformer decoder that converts the dimension
  in the frequency axis.
---

# Automatic Piano Transcription with Hierarchical Frequency-Time Transformer

## Quick Facts
- arXiv ID: 2307.04305
- Source URL: https://arxiv.org/abs/2307.04305
- Reference count: 0
- Key outcome: The hFT-Transformer outperforms state-of-the-art methods on MAPS and MAESTRO datasets for all metrics (Frame, Note, Note with Offset, Note with Offset and Velocity)

## Executive Summary
This paper introduces hFT-Transformer, a novel automatic piano transcription method that uses a two-level hierarchical frequency-time Transformer architecture. The approach independently captures spectral dependencies in the frequency axis and temporal dependencies in the time axis through separate Transformer encoders. The model demonstrates superior performance compared to existing state-of-the-art methods across all evaluation metrics on standard piano transcription benchmarks.

## Method Summary
The hFT-Transformer processes log-mel spectrograms through a hierarchical architecture consisting of two main stages. The first hierarchy processes temporal information with a 1D CNN, then applies a Transformer encoder to capture spectral dependencies independently across frequency bins, followed by a Transformer decoder that converts frequency dimensions to pitch representations via cross-attention. The second hierarchy applies another Transformer encoder to model temporal dependencies using the pitch representations. The model is trained with combined binary cross-entropy and categorical cross-entropy losses, and employs a half-stride inference strategy with 50% overlap to reduce boundary artifacts.

## Key Results
- hFT-Transformer outperforms existing state-of-the-art methods on both MAPS and MAESTRO datasets
- The method achieves higher F1 scores across all four evaluation metrics (Frame, Note, Note with Offset, Note with Offset and Velocity)
- The half-stride strategy with 50% overlap provides additional performance improvements by reducing boundary artifacts
- Using a Transformer decoder for dimension conversion from frequency bins to pitches performs better than linear layer alternatives

## Why This Works (Mechanism)

### Mechanism 1
The hierarchical frequency-time Transformer architecture improves piano transcription by independently capturing spectral dependencies in the frequency axis and temporal dependencies in the time axis. The first hierarchy processes spectral features through a Transformer encoder, then converts the dimension to pitches via a Transformer decoder. The second hierarchy processes the temporal evolution of these pitch representations through another Transformer encoder. This separation allows specialized modeling of spectral and temporal characteristics rather than jointly processing them.

### Mechanism 2
The Transformer decoder used as a dimension converter from frequency bins to pitches enables better pitch representation than a simple linear layer. The decoder uses cross-attention between the frequency-axis representations and pitch-position embeddings, allowing it to learn a non-linear mapping that better captures piano-specific pitch relationships. This cross-attention mechanism provides more expressive power than linear projection for mapping spectral features to pitch space.

### Mechanism 3
The half-stride inference strategy reduces boundary artifacts by ensuring only high-confidence predictions from the center of processing chunks are used. By overlapping chunks by 50% and only using the middle portion of each chunk, the method avoids predictions from regions where chunk boundary effects cause higher error rates. This approach leverages the observation that prediction error monotonically decreases toward the center of processing chunks.

## Foundational Learning

- **Self-attention mechanism in Transformers**: Understanding how self-attention captures long-range dependencies is crucial for grasping why the model can handle both spectral and temporal dependencies effectively. *Quick check*: How does self-attention differ from recurrent layers in processing sequential information?

- **Multi-head attention**: The model uses multi-head attention in its Transformer encoders and decoders, so understanding how multiple attention heads work in parallel is important for architecture comprehension. *Quick check*: What advantage does using multiple attention heads provide over a single attention head?

- **Mel spectrogram representation**: The input to the model is a log-mel spectrogram, so understanding this representation and why it's useful for piano transcription is fundamental. *Quick check*: Why is a mel scale used instead of a linear frequency scale for music audio representation?

## Architecture Onboarding

- **Component map**: Log-mel spectrogram → 1D CNN → First Transformer encoder → Transformer decoder → Second Transformer encoder → Final outputs

- **Critical path**: Log-mel spectrogram → 1D CNN → First Transformer encoder → Transformer decoder → Second Transformer encoder → Final outputs

- **Design tradeoffs**: Using Transformer decoder vs linear layer for dimension conversion increases model size but improves performance; Half-stride inference increases computational cost (50% overlap) but improves accuracy; Separate frequency and time processing adds complexity but allows specialized modeling

- **Failure signatures**: Poor performance on Frame metric but good on Note metrics suggests issues in the first hierarchy; Good performance on Frame but poor on Note metrics suggests issues in the second hierarchy or post-processing; Degradation at chunk boundaries suggests insufficient context or need for half-stride strategy

- **First 3 experiments**: Test with and without the Transformer decoder (replace with linear layer) to verify its contribution to performance; Test with and without the second Transformer encoder to confirm its importance for temporal modeling; Test with different overlap percentages (25%, 50%, 75%) in the half-stride strategy to find optimal balance between accuracy and computational cost

## Open Questions the Paper Calls Out
The paper explicitly mentions the desire to extend the method to other instruments and multi-instrument settings, but does not explore how the hierarchical frequency-time approach would perform with different types of polyphonic instruments or in complex musical scenarios involving multiple instruments simultaneously.

## Limitations
- The method's data efficiency and generalization to different pianos or recording conditions beyond tested datasets is unknown
- Computational efficiency analysis is limited, with no detailed runtime comparisons or exploration of the overhead introduced by the half-stride strategy
- The interpretability of the cross-attention mechanism is not explored, lacking empirical analysis of what attention heads actually learn

## Confidence
- **High confidence**: The hierarchical architecture structure and its components are clearly specified and their basic functionality is well-supported
- **Medium confidence**: The superiority of the Transformer decoder over linear layers is demonstrated through ablation studies, but the specific mechanism of why cross-attention provides this advantage isn't deeply analyzed
- **Medium confidence**: The half-stride strategy's effectiveness is empirically validated, but the analysis is limited to observed error patterns without exploring alternative boundary handling strategies

## Next Checks
1. **Attention pattern analysis**: Visualize and analyze the cross-attention patterns in the Transformer decoder to understand what spectral features it uses for pitch mapping, and whether these patterns are consistent across different pianos and musical pieces.

2. **Cross-instrument validation**: Test the model's performance on non-piano polyphonic instruments (e.g., guitar, harp) to evaluate whether the hierarchical frequency-time approach generalizes beyond piano-specific characteristics.

3. **Computational efficiency benchmarking**: Measure and compare the runtime performance of the full model with half-stride inference against baseline methods, and test alternative overlap strategies (25%, 75%) to find the optimal accuracy-runtime tradeoff.