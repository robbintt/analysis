---
ver: rpa2
title: 'Unveiling the Power of Self-Attention for Shipping Cost Prediction: The Rate
  Card Transformer'
arxiv_id: '2311.11694'
source_url: https://arxiv.org/abs/2311.11694
tags:
- features
- rate
- card
- transformer
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a Transformer-based model, the Rate Card Transformer
  (RCT), for predicting shipping costs in Amazon's logistics system. RCT uses self-attention
  to encode all shipping-related information, including package attributes, carrier
  details, and route plans, as well as variable-length lists of item and charge features.
---

# Unveiling the Power of Self-Attention for Shipping Cost Prediction: The Rate Card Transformer

## Quick Facts
- arXiv ID: 2311.11694
- Source URL: https://arxiv.org/abs/2311.11694
- Reference count: 6
- Primary result: 28.82% reduction in MAE compared to GBDT baseline

## Executive Summary
This paper introduces the Rate Card Transformer (RCT), a Transformer-based model designed to predict shipping costs for Amazon packages using rate card information. Unlike existing models, RCT leverages self-attention to encode all shipping-related information, including package attributes, carrier details, and route plans, as well as variable-length lists of item and charge features. Experimental results on a dataset of 10 million packages demonstrate that RCT significantly outperforms traditional tree-based models and state-of-the-art Transformer models for tabular data.

## Method Summary
The Rate Card Transformer is a Transformer-based model that uses self-attention to encode shipping information, including package attributes, carrier details, route plans, and variable-length lists of item and charge features. RCT is trained to predict shipping costs using rate card information, with the objective of minimizing mean absolute error (MAE). The model employs mixed embedding layers for combining categorical and numerical features, and variable-length embedding layers to handle one-to-many relationships in the data.

## Key Results
- RCT achieves a 28.82% reduction in MAE compared to a GBDT baseline on a dataset of 10 million packages
- RCT outperforms the state-of-the-art FTTransformer model by 6.08%
- The learned embeddings effectively represent the rate card manifold and improve the performance of downstream tree-based models by 9.79%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RCT leverages self-attention to capture high-order combinatorial interactions between shipping-related features that tree-based models cannot efficiently learn.
- Mechanism: The self-attention mechanism computes attention scores between all pairs of embedded features, allowing the model to weight and aggregate information based on relevance.
- Core assumption: The rate card manifold is sufficiently smooth and can be approximated by attention-weighted combinations of input features.
- Evidence anchors:
  - [abstract] "RCT uses self-attention to encode all package shipping information such as package attributes, carrier information and route plan."
  - [section] "The MHSA layer comprises multiple self-attention units called heads, which learn interactions between input embeddings."
- Break condition: If feature interactions are sparse or non-linear beyond what attention can approximate, performance degrades.

### Mechanism 2
- Claim: RCT's ability to handle variable-length lists of items and charges allows it to encode more complete shipment information than fixed-structure models.
- Mechanism: Variable-length embedding layers reduce each item or charge's mixed embedding sequence to a single token, enabling the model to process arbitrary numbers of items/charges within the same framework.
- Core assumption: The shipping cost depends on properties of individual items and charges, not just aggregated statistics.
- Evidence anchors:
  - [abstract] "RCT has the ability to encode a variable list of one-to-many relations of a shipment, allowing it to capture more information about a shipment."
  - [section] "Unlike fixed length features, a package is associated with variable number of items and charges, thus these are referred to as variable length features."
- Break condition: If variable-length lists are rare or items/charges contribute minimally to cost, added complexity yields no benefit.

### Mechanism 3
- Claim: RCT learns a generalized manifold of the rate card that can improve performance of downstream tree-based models when used as additional features.
- Mechanism: The pooled output of the final transformer layer serves as a dense, interaction-aware representation of the rate card.
- Core assumption: The transformer's learned representation is both generalizable and complementary to hand-engineered features.
- Evidence anchors:
  - [abstract] "We also illustrate that the RCT learns a generalized manifold of the rate card that can improve the performance of tree-based models."
  - [section] "Adding this feature improved the performance of the GBDT by 9.79%... This indicates that the learned representations of rate cards are not only effective at capturing better feature information, but are also sufficient representation of the package rate card."
- Break condition: If the learned embedding overfits to RCT's training distribution, it may not generalize to improve other models.

## Foundational Learning

- Concept: Mixed embedding layers for combining categorical and numerical features.
  - Why needed here: Rate card features include both types, and self-attention operates on homogeneous embeddings.
  - Quick check question: How does the model handle a categorical feature like "carrier" alongside a numerical feature like "package weight"?

- Concept: Self-attention and multi-head attention mechanisms.
  - Why needed here: To capture interactions between all features without enumerating combinations, avoiding the curse of dimensionality.
  - Quick check question: What is the role of the query, key, and value projections in computing attention scores?

- Concept: Variable-length sequence processing via pooling.
  - Why needed here: Packages can contain different numbers of items and charges; the model must produce a fixed-size representation regardless.
  - Quick check question: Why is it important to reduce variable-length item embeddings to a single token before feeding them into the transformer?

## Architecture Onboarding

- Component map: Input encoder (mixed embedding layers for fixed-length features, variable-length embedding layers for items/charges) -> Transformer stack (L layers, n heads) -> Pooling -> Feedforward output layer
- Critical path: Feature embedding -> Self-attention interaction learning -> Pooling -> Cost prediction
- Design tradeoffs: More transformer layers increase interaction complexity but risk overfitting; more attention heads capture more independent interactions but reduce per-head capacity
- Failure signatures: High train-val MAE gap indicates overfitting; stagnant validation loss suggests learning saturation or poor embedding initialization
- First 3 experiments:
  1. Compare RCT with and without variable-length item/charge embeddings to quantify their contribution
  2. Replace self-attention with feedforward layers for fixed-length features to test interaction learning efficacy
  3. Vary the number of transformer layers and attention heads to find the optimal capacity-accuracy balance

## Open Questions the Paper Calls Out

- Open Question 1: How does the Rate Card Transformer (RCT) perform compared to the SOTA model FT-Transformer for different dataset sizes?
  - Basis in paper: [explicit] The paper states that RCT outperforms FT-Transformer across all tested model sizes.
  - Why unresolved: The paper only tests RCT against FT-Transformer for a fixed dataset size of 10 million packages.
  - What evidence would resolve it: Testing RCT and FT-Transformer on varying dataset sizes and comparing their performance.

- Open Question 2: Can the package representations learned by the RCT be used to improve the performance of other related tasks?
  - Basis in paper: [explicit] The paper suggests that future research could investigate whether the package representations learned by the RCT can be used to improve the performance of other related tasks.
  - Why unresolved: The paper does not provide any evidence or results regarding the use of RCT-learned representations for other tasks.
  - What evidence would resolve it: Applying the RCT-learned representations to other related tasks and evaluating their performance.

- Open Question 3: How does the RCT perform compared to other Transformer-based models for tabular data, such as TabNet and SAINT?
  - Basis in paper: [explicit] The paper compares RCT only to GBDT, AWS AutoGluon, Feedforward neural network, TabTransformer, and FT-Transformer.
  - Why unresolved: The paper does not provide any comparison between RCT and other Transformer-based models for tabular data.
  - What evidence would resolve it: Testing RCT against TabNet and SAINT on the same dataset and comparing their performance.

## Limitations

- The paper lacks ablation studies isolating the contribution of individual design choices, such as variable-length embeddings versus self-attention mechanisms.
- The dataset details are sparse, making exact reproduction difficult.
- The claim that learned embeddings generalize to improve other models (GBDT) is supported but could benefit from testing on independent datasets.

## Confidence

- **High confidence**: RCT achieves 28.82% MAE reduction over GBDT baseline on the reported dataset.
- **Medium confidence**: Self-attention is superior to feedforward networks for capturing feature interactions (based on ablation).
- **Medium confidence**: Variable-length embeddings meaningfully improve performance (supported by ablation, but mechanism not fully explored).

## Next Checks

1. Conduct ablation studies isolating the impact of variable-length embeddings versus self-attention mechanisms.
2. Test the generalizability of learned embeddings by applying them to improve models on independent shipping cost datasets.
3. Perform sensitivity analysis on transformer depth and attention head count to establish optimal architecture for this task.