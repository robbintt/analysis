---
ver: rpa2
title: Can Authorship Attribution Models Distinguish Speakers in Speech Transcripts?
arxiv_id: '2311.07564'
source_url: https://arxiv.org/abs/2311.07564
tags:
- speech
- performance
- level
- same
- speaker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic benchmark for text-based
  authorship attribution models on transcribed speech. The authors construct a new
  benchmark dataset using the Fisher English Training Speech Transcripts corpus, carefully
  controlling for topic to evaluate speaker attribution in varying difficulty settings.
---

# Can Authorship Attribution Models Distinguish Speakers in Speech Transcripts?

## Quick Facts
- arXiv ID: 2311.07564
- Source URL: https://arxiv.org/abs/2311.07564
- Reference count: 5
- Authors present first systematic benchmark for authorship attribution on speech transcripts

## Executive Summary
This paper presents the first systematic benchmark for text-based authorship attribution models on transcribed speech. The authors construct a new benchmark dataset using the Fisher English Training Speech Transcripts corpus, carefully controlling for topic to evaluate speaker attribution in varying difficulty settings. They compare several neural and non-neural baselines, finding that while written text attribution models transfer surprisingly well to speech transcripts, performance degrades as topic control increases. The best model, LUAR, achieves 84.7% AUC on the base setting but only 45.0% on the hardest setting.

## Method Summary
The authors construct a benchmark dataset using the Fisher English Training Speech Transcripts corpus, splitting it into three difficulty levels based on topic control. They evaluate four baseline models (LUAR, SBERT, STEL, and TF-IDF) both out-of-the-box and after fine-tuning on the speech transcript data. The task is framed as authorship verification, where models must determine if two transcripts are from the same speaker or different speakers. Performance is measured using AUC (Area Under the Curve) for binary classification.

## Key Results
- LUAR achieves 84.7% AUC on the base setting but only 45.0% on the hardest setting
- Fine-tuning on speech transcripts did not improve performance, likely due to overfitting
- Normalizing transcripts (removing punctuation/capitalization) does not significantly impact performance
- Performance degrades as topic control increases, with models relying more on stylistic cues in harder settings

## Why This Works (Mechanism)

### Mechanism 1
Text-based authorship models transfer to speech transcripts by learning stylistic features like discourse markers, filler words, and syntactic patterns that persist even without punctuation and capitalization. Core assumption: speaker style is embedded in lexical and syntactic choices, not surface formatting.

### Mechanism 2
Topic control increases attribution difficulty because when topic is held constant, models cannot rely on lexical content differences and must detect subtle speaker-specific stylistic cues. Core assumption: topic variation masks speaker differences more than style variation.

### Mechanism 3
LUAR is less sensitive to superficial textual features because its training on Reddit data with varied formatting teaches it to focus on deeper linguistic patterns rather than punctuation or casing. Core assumption: pretraining data diversity leads to feature robustness.

## Foundational Learning

- **Cosine similarity as a metric for speaker attribution**: Models output embeddings; similarity between them determines same/different speaker. Quick check: If two transcripts are from the same speaker, should their cosine similarity be closer to 1 or 0?
- **Topic control in verification trials**: Prevents models from using content differences as a proxy for speaker differences. Quick check: In the 'harder' setting, are the negative pairs from the same call but different speakers?
- **AUC (Area Under the Curve) for binary classification**: Evaluates model performance across all classification thresholds. Quick check: An AUC of 0.847 means the model has what probability of ranking a same-speaker pair higher than a different-speaker pair?

## Architecture Onboarding

- **Component map**: Fisher corpus → Split by speaker → Generate verification pairs → Normalize if needed → Embed with model → Compute similarity → Train/test classifier → Evaluation (AUC/EER)
- **Critical path**: Load Fisher corpus → Split by speaker → Generate verification pairs (positive/negative) → Normalize if needed → Embed with model → Compute similarity → Train/test classifier
- **Design tradeoffs**: Out-of-the-box vs. fine-tuned models (out-of-the-box avoids overfitting but may miss speech-specific cues; fine-tuning risks overfitting); Topic control level (base vs. hard vs. harder trades off between realistic performance and controlled evaluation)
- **Failure signatures**: Fine-tuned models dropping to chance level (overfitting); Large drop from base to harder (models rely too heavily on topic cues); SBERT failing more than LUAR (content-focused models less robust to style-only cues)
- **First 3 experiments**: 1) Run LUAR out-of-the-box on BBN encoding, base level; record AUC. 2) Run same model on LDC encoding; compare performance. 3) Fine-tune LUAR on training set, evaluate on base level; check for overfitting.

## Open Questions the Paper Calls Out

- **How much does speaker accommodation impact performance?**: The authors propose that speaker accommodation may explain the larger performance drop between 'hard' and 'harder' difficulty levels, but do not directly measure or quantify the extent of accommodation in the Fisher corpus.
- **Can pre-training on speech transcripts improve performance?**: The authors find fine-tuning hurts performance and hypothesize pre-training specifically on speech transcripts could help, but do not actually pre-train a model to test this hypothesis.
- **How does ASR noise impact performance?**: The authors mention transcripts will likely have varying amounts of noise from automatic speech recognition and propose testing the same speech samples through multiple transcribers, but do not actually test the impact of noisy transcripts.

## Limitations
- Evaluation limited to one corpus (Fisher English Training Speech Transcripts), may not generalize to other spoken domains
- Fine-tuning experiments do not explore alternative strategies that might have mitigated overfitting
- Topic control implementation details are sparse, unclear if topic matching perfectly eliminated content overlap

## Confidence
- **High confidence**: LUAR's superior performance and resistance to superficial features
- **Medium confidence**: General difficulty of speech transcript attribution and ineffectiveness of fine-tuning
- **Low confidence**: Underlying mechanisms (why LUAR works better, how topic control affects performance)

## Next Checks
1. Test the same models on a different speech corpus (e.g., Switchboard or podcast transcripts) to verify transfer learning findings generalize
2. Implement alternative fine-tuning strategies (lower learning rates, early stopping, dropout) to determine if poor performance was due to overfitting or inherent limitations
3. Use feature attribution analysis (attention visualization, feature importance scoring) to verify LUAR focuses on stylistic features rather than superficial text patterns