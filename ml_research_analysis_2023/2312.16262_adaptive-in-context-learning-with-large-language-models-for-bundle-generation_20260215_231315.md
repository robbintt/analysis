---
ver: rpa2
title: Adaptive In-Context Learning with Large Language Models for Bundle Generation
arxiv_id: '2312.16262'
source_url: https://arxiv.org/abs/2312.16262
tags:
- bundle
- bundles
- intent
- intents
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a dynamic in-context learning paradigm to simultaneously
  generate personalized product bundles and infer underlying user intents from user
  sessions using large language models. The key idea is to retrieve the most correlated
  neighbor sessions for each target session and generate demonstrations via self-correction
  and auto-feedback mechanisms.
---

# Adaptive In-Context Learning with Large Language Models for Bundle Generation

## Quick Facts
- arXiv ID: 2312.16262
- Source URL: https://arxiv.org/abs/2312.16262
- Reference count: 40
- Outperforms baselines by over 5.11% on bundle generation

## Executive Summary
This paper introduces a dynamic in-context learning paradigm for simultaneously generating personalized product bundles and inferring underlying user intents using large language models (LLMs). The key innovation lies in retrieving the most correlated neighbor sessions for each target session and generating demonstrations through self-correction and auto-feedback mechanisms. This approach enables the model to seek tailored lessons from neighbor sessions, significantly improving performance on bundle generation while maintaining high-quality intent inference comparable to human-annotated results.

## Method Summary
The method employs a three-stage process: first, neighbor session retrieval uses RAG to identify correlated sessions based on cosine similarity of session representations; second, dynamic demonstration generation creates prompts for ChatGPT including self-correction and auto-feedback mechanisms; third, demonstration-guided inference uses these demonstrations to perform bundle generation and intent inference on target sessions. The approach leverages GPT-3.5-turbo as the LLM and tests on three real-world datasets (Electronic, Clothing, Food).

## Key Results
- Achieves over 5.11% improvement in bundle generation compared to baselines
- Inferred intents are of high quality, comparable to or exceeding human-annotated ones
- Best performance achieved with k=1 nearest neighbor sessions
- Self-correction and auto-feedback mechanisms significantly contribute to performance gains

## Why This Works (Mechanism)

### Mechanism 1
Dynamic in-context learning from nearest neighbors enables ChatGPT to generate personalized bundles and infer underlying intents simultaneously. The method retrieves the most correlated neighbor sessions for each target session, creates demonstrations via self-correction and auto-feedback mechanisms, and guides ChatGPT to seek tailored lessons from these neighbor sessions.

### Mechanism 2
Self-correction strategy fosters mutual improvement in both bundle generation and intent inference tasks without supervision signals. The inferred intent is used to refine the generated bundles, which in turn are used to refine the intents, repeating until no further adjustment is needed.

### Mechanism 3
Auto-feedback mechanism provides dynamic supervision based on distinct mistakes made by ChatGPT on different neighbor sessions. Five types of supervision signals are defined for bundles and three types for intents, prompting ChatGPT to refine its outputs based on these signals.

## Foundational Learning

- Concept: Retrieval augmented generation (RAG)
  - Why needed here: To identify the most correlated neighbor sessions for each target session, which serve as demonstrations for dynamic in-context learning
  - Quick check question: How does RAG help in retrieving relevant neighbor sessions for a given target session?

- Concept: Cosine similarity for session representation
  - Why needed here: To measure the correlation between sessions based on their product descriptions, enabling the identification of nearest neighbors
  - Quick check question: What is the role of cosine similarity in calculating the correlation between session representations?

- Concept: Self-correction and auto-feedback mechanisms
  - Why needed here: To foster mutual improvement in bundle generation and intent inference tasks without supervision signals, and to provide dynamic supervision based on distinct mistakes
  - Quick check question: How do self-correction and auto-feedback mechanisms contribute to the improvement of bundle generation and intent inference tasks?

## Architecture Onboarding

- Component map: Neighbor Session Retrieval (NSR) -> Dynamic Demonstration Generation (DDG) -> Demonstration Guided Inference (DGI)
- Critical path: 1) Retrieve neighbor sessions using NSR, 2) Generate demonstrations using DDG, 3) Perform tasks on target session using DGI
- Design tradeoffs: 1) Number of neighbor sessions (k): Increasing k may provide more demonstrations but could also introduce noise and increase computational cost, 2) Rounds of self-correction (Ts) and auto-feedback (Tb, Ti): More rounds may lead to better performance but also increase computational time
- Failure signatures: 1) Poor performance on bundle generation and intent inference tasks, 2) Inability to generate meaningful demonstrations from neighbor sessions, 3) ChatGPT fails to learn from the demonstrations or feedback
- First 3 experiments: 1) Verify that NSR correctly identifies correlated neighbor sessions by comparing retrieved sessions with ground truth, 2) Test DDG's ability to generate effective demonstrations by evaluating the performance of ChatGPT on a small set of target sessions, 3) Assess DGI's effectiveness by comparing the performance of ChatGPT with and without demonstrations on a set of target sessions

## Open Questions the Paper Calls Out

### Open Question 1
How does the dynamic in-context learning (DICL) paradigm's performance scale with the number of neighbor sessions (k) beyond k=1, and what is the optimal value of k for different dataset characteristics? The paper mentions that the best performance is attained with k=1 and increasing k does not consistently yield noticeable improvements.

### Open Question 2
Can the self-correction and auto-feedback mechanisms be effectively applied in the inference stage of the DICL paradigm, and would this further improve the performance on the target session? The paper mentions that these mechanisms are only used in the dynamic demonstration generation module, not in the demonstration guided inference module.

### Open Question 3
How does the performance of DICL compare to other large language models (LLMs) with different architectures and sizes, such as GPT-4, for the bundle generation and intent inference tasks? The paper uses GPT-3.5-turbo but mentions that GPT-4 can resolve the issue of generating single-product bundles.

## Limitations

- Hierarchical aggregation strategy for session representation lacks detailed specification, which may affect reproducibility
- Exact prompt engineering techniques and prompt templates used for self-correction and auto-feedback mechanisms are not fully disclosed
- Effectiveness of auto-feedback mechanism relies heavily on the accuracy of ground truth comparisons, which may not always be available in real-world scenarios

## Confidence

- High Confidence: The core concept of using dynamic in-context learning with neighbor sessions for bundle generation and intent inference is well-supported by experimental results
- Medium Confidence: Self-correction mechanism's effectiveness is supported by ablation studies, but specific implementation details are not fully transparent
- Low Confidence: Auto-feedback mechanism's performance may vary significantly depending on quality of ground truth comparisons and specific types of mistakes made by the LLM

## Next Checks

1. Conduct an ablation study to quantify individual contributions of self-correction and auto-feedback mechanisms to overall performance
2. Test the method on additional datasets with varying characteristics (e.g., different product categories, session lengths) to assess generalizability
3. Evaluate the method's performance in a cold-start scenario where neighbor sessions may have limited or no overlap with target sessions