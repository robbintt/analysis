---
ver: rpa2
title: 'FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy'
arxiv_id: '2305.10307'
source_url: https://arxiv.org/abs/2305.10307
tags:
- human
- language
- metrics
- face
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FACE, a set of metrics based on Fourier Analysis
  of Cross-Entropy (FACE), for measuring the similarity between model-generated and
  human-written languages. FACE uses a pre-trained language model to estimate the
  cross-entropy of text sequences and applies Fast Fourier Transform to obtain frequency
  spectra.
---

# FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy

## Quick Facts
- arXiv ID: 2305.10307
- Source URL: https://arxiv.org/abs/2305.10307
- Authors: Not specified in input
- Reference count: 40
- Key outcome: FACE metrics effectively identify the human-model gap, scale with model size, reflect different sampling methods, correlate well with other evaluation metrics, and align with human judgement scores

## Executive Summary
This paper proposes FACE (Fourier Analysis of Cross-Entropy), a novel evaluation framework for measuring the similarity between model-generated and human-written text. FACE treats cross-entropy sequences as time-domain signals and applies Fast Fourier Transform to obtain frequency spectra, then uses four similarity metrics (Spectral Overlap, Spectrum Angle Mapper, Pearson's correlation, and Spearman's correlation) to quantify spectral similarity. The framework provides an intuitive interpretation of language use through periodicity analysis and demonstrates strong performance across multiple evaluation dimensions.

## Method Summary
FACE evaluates natural language generation by computing cross-entropy sequences for both human-written and model-generated text using a pre-trained language model, then applying FFT to extract frequency spectra. Four spectral similarity metrics (SO, SAM, CORR, SPEAR) quantify alignment between these spectra. The method assumes cross-entropy sequences are stationary processes (validated via ADF tests) and that periodicity patterns in entropy reflect fundamental differences between human and machine language generation.

## Key Results
- FACE effectively identifies the human-model gap in language generation quality
- FACE scores scale with model size (GPT2-Large > GPT2-Medium > GPT2-Small)
- FACE reflects differences between sampling methods (maximization/temperature < nucleus < contrastive)
- FACE correlates well with existing evaluation metrics and human judgments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spectral overlap in cross-entropy entropy frequency spectra distinguishes human from machine language
- Mechanism: Cross-entropy sequences of text are treated as time-domain signals. Fast Fourier Transform (FFT) extracts frequency components. Similarity metrics (SO, CORR, SAM, SPEAR) quantify alignment between human and model spectra.
- Core assumption: Periodical patterns in cross-entropy (entropy rate constancy) are characteristic of natural language and differ systematically between human and model generation.
- Evidence anchors:
  - [abstract] "Inspired by empirical findings from psycholinguistics on the periodicity of entropy in language"
  - [section 2.2] "We treat the estimated cross-entropy sequence...as a finite discrete signal in the time domain"
  - [corpus] Weak: no direct corpus examples of periodicity differences provided
- Break condition: If human and model texts have identical entropy distributions (e.g., both random or both highly repetitive), spectral signatures may not differ.

### Mechanism 2
- Claim: Language model quality is reflected in cross-entropy periodicity alignment with human text
- Mechanism: Better models produce generated text whose entropy sequences have more similar frequency spectra to human text. FACE metrics scale with model size.
- Core assumption: Larger models generate more human-like text, thus closer entropy periodicity.
- Evidence anchors:
  - [section 4.1] "We find that FACE can effectively identify the human-model gap, scales with model size"
  - [section 4.2] "FACE should follow the inequality: maximization-based/temperature-based≺ nucleus≺ contrastive"
  - [corpus] Weak: no explicit corpus frequency comparisons provided
- Break condition: If decoding method dominates over model quality in determining entropy periodicity.

### Mechanism 3
- Claim: Stationary cross-entropy sequences enable reliable spectral analysis
- Mechanism: Augmented Dickey-Fuller tests confirm stationarity of entropy sequences before FFT application.
- Core assumption: Cross-entropy time series are stationary processes, satisfying Fourier transform requirements.
- Evidence anchors:
  - [section 4.4] "We applied the Augmented Dickey-Fuller (ADF) test...The null hypothesis H0 of the ADF test is non-stationarity"
  - [section 4.4] "97.4% for GPT2, 92.1% for OPT, 74.5% for BLOOM, and 97.9% for human"
  - [corpus] No corpus examples of stationarity violations
- Break condition: If entropy sequences are non-stationary, spectral similarity metrics may be unreliable.

## Foundational Learning

- Concept: Fast Fourier Transform (FFT) and spectral analysis
  - Why needed here: Converts time-domain cross-entropy sequences into frequency spectra for comparison
  - Quick check question: What does the FFT magnitude at a given frequency represent in this context?

- Concept: Cross-entropy as information density/surprisal
  - Why needed here: Forms the signal being analyzed; reflects predictability patterns in text
  - Quick check question: How is cross-entropy computed at each token position in the sequence?

- Concept: Stationarity in time series analysis
  - Why needed here: Required assumption for valid spectral analysis of entropy sequences
  - Quick check question: What does it mean for a time series to be stationary?

## Architecture Onboarding

- Component map: Human-written text and model-generated text → Pre-trained language model (cross-entropy estimation) → FFT computation → Four similarity metrics (SO, CORR, SAM, SPEAR) → Evaluation
- Critical path: Text → Cross-entropy estimation → FFT → Spectral similarity → Evaluation
- Design tradeoffs:
  - Estimator choice affects absolute cross-entropy values but not relative comparisons if consistent
  - Spectral resolution depends on sequence length
  - Multiple similarity metrics provide complementary views but increase complexity
- Failure signatures:
  - Non-stationary sequences (ADF test fails)
  - Spectra dominated by noise or artifacts
  - Metrics inconsistent with human judgment
- First 3 experiments:
  1. Generate entropy sequences from both human and model text, apply FFT, visualize spectra
  2. Compute all four FACE metrics between human and model spectra, compare with baseline metrics
  3. Test stationarity of entropy sequences using ADF test, identify problematic cases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of estimator model (mest) affect the FACE scores and their interpretation?
- Basis in paper: [explicit] The paper states that the choice of mest will influence the next steps, because better language models produce lower perplexity scores, i.e., lower cross entropy. It also mentions that different choices of mest will result in spectra with different magnitudes but the same shape.
- Why unresolved: The paper only briefly mentions the effect of different estimator models but does not provide a detailed analysis or comparison of how different models impact the FACE scores and their interpretability.
- What evidence would resolve it: A comprehensive study comparing FACE scores using different estimator models (e.g., GPT2-sm, GPT2-xl, OPT-125m, OPT-6.7b, BLOOM-560m, BLOOM-7.1b) on the same dataset and task, along with a detailed analysis of the resulting spectra and their similarities/differences.

### Open Question 2
- Question: How can the peaks in the frequency spectra be more precisely interpreted in terms of the underlying language characteristics?
- Basis in paper: [explicit] The paper proposes a simple interpretation of the peaks, suggesting that the reciprocal of a frequency component Tk = 1/ωk denotes the corresponding cycle in the time domain, which is the number of tokens. It states that the degree of this recurrence can mark the difference between human and model languages.
- Why unresolved: The paper provides a basic interpretation but does not delve deeper into the linguistic implications of the peak frequencies or how they relate to specific language features such as syntactic complexity, semantic coherence, or stylistic choices.
- What evidence would resolve it: A detailed linguistic analysis of the peaks in the frequency spectra, correlating them with specific language features and patterns. This could involve comparing the spectra of different genres, styles, or linguistic structures and identifying the corresponding peak frequencies.

### Open Question 3
- Question: How does FACE perform on larger language models (with more than 100 billion parameters) and what are the implications for future language model evaluation?
- Basis in paper: [inferred] The paper mentions that the current work has a limitation of not including larger models (with more than 100 billion parameters) for more comprehensive comparisons. It suggests improving this aspect in future work.
- Why unresolved: The paper only evaluates FACE on relatively smaller models (up to BLOOM-7.1b with 7.1 billion parameters) and does not explore the performance of FACE on larger models that are becoming increasingly common.
- What evidence would resolve it: An empirical study evaluating FACE on a range of larger language models, comparing the FACE scores and their ability to distinguish human and model-generated language. This would provide insights into the scalability of FACE and its potential for future language model evaluation.

## Limitations
- Stationarity assumption: While ADF tests show high stationarity rates, the 25.5% non-stationarity rate for BLOOM raises concerns about when spectral analysis may be invalid
- Estimator model dependency: FACE metrics rely on cross-entropy estimates from a pre-trained language model, but the paper doesn't specify which model is used for estimation
- Ground truth for human judgment: The paper reports correlation with human judgment scores but doesn't describe how these judgments were obtained or their reliability

## Confidence

**High confidence**: FACE metrics scale with model size and decoding method quality. The experimental results showing GPT2-Large > GPT2-Medium > GPT2-Small and the consistent ranking of decoding methods (maximization/temperature < nucleus < contrastive) provide strong evidence for this claim.

**Medium confidence**: FACE correlates well with other evaluation metrics. While the paper reports high correlations (0.81 with PPL, 0.77 with MAUVE), the lack of specification about which particular metrics were used for comparison and how they were computed creates some uncertainty.

**Medium confidence**: FACE effectively identifies the human-model gap. The experimental setup demonstrates clear separation between human and model spectra, but the magnitude of this gap and its practical significance for different applications remains unclear.

## Next Checks

1. **Estimator sensitivity analysis**: Run FACE metrics using multiple different pre-trained language models as entropy estimators (e.g., GPT2, OPT, BLOOM) on the same text pairs to quantify how estimator choice affects metric values and rankings.

2. **Out-of-domain robustness test**: Apply FACE to text from domains not represented in the training data of the estimator model (e.g., code, poetry, technical documentation) to test whether spectral patterns generalize beyond standard natural language domains.

3. **Failure mode characterization**: Systematically analyze the 25.5% of BLOOM-generated sequences that fail stationarity tests to identify specific text characteristics (repetition patterns, length, vocabulary diversity) that correlate with non-stationary entropy sequences.