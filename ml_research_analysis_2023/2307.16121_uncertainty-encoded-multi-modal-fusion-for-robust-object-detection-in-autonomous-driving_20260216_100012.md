---
ver: rpa2
title: Uncertainty-Encoded Multi-Modal Fusion for Robust Object Detection in Autonomous
  Driving
arxiv_id: '2307.16121'
source_url: https://arxiv.org/abs/2307.16121
tags:
- fusion
- uncertainty
- detection
- umoe
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multi-modal fusion method for robust object
  detection in autonomous driving. It addresses the challenge of fusing LiDAR and
  camera data when sensors are affected by adverse conditions such as weather or attacks.
---

# Uncertainty-Encoded Multi-Modal Fusion for Robust Object Detection in Autonomous Driving

## Quick Facts
- arXiv ID: 2307.16121
- Source URL: https://arxiv.org/abs/2307.16121
- Reference count: 40
- This paper proposes a multi-modal fusion method for robust object detection in autonomous driving that achieves up to 10.67% performance gains over state-of-the-art fusion methods in extreme weather scenarios.

## Executive Summary
This paper addresses the challenge of fusing LiDAR and camera data for object detection in autonomous driving when sensors are affected by adverse conditions such as weather or attacks. The key innovation is the Uncertainty-Encoded Mixture-of-Experts (UMoE) module, which incorporates predictive uncertainty into the fusion process. By encoding sensor-specific uncertainties into the fusion input and using expert networks to process each modality with a gating network determining fusion weights, UMoE achieves significant performance improvements across various adverse conditions while maintaining performance in clear scenarios.

## Method Summary
The method uses Monte Carlo Dropout to estimate sensor-specific uncertainties for both LiDAR and camera detections. These uncertainties are transformed into deviation ratios and normalized regression uncertainty values, which are then fed into modality-specific expert networks. The UMoE module contains two expert networks (one for each sensor modality) and a gating network that combines their outputs to produce uncertainty-aware confidence scores. This approach allows the fusion process to down-weight unreliable proposals based on their uncertainty scores. The system is trained end-to-end with uncertainty-encoded confidence scores as supervision, using datasets including KITTI, KITTIAdv (adversarial attacks), KITTIBlind (blinding attacks), and STF (adverse weather).

## Key Results
- UMoE achieves up to 10.67% performance gains over state-of-the-art fusion methods in extreme weather scenarios
- UMoE shows 3.17% improvement in adversarial attack scenarios
- UMoE demonstrates 5.40% improvement in blinding attack scenarios
- Maintains competitive performance in clear weather conditions while improving robustness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The UMoE module improves fusion robustness by encoding sensor-specific uncertainties into confidence scores before fusion.
- **Mechanism**: Each modality's detection proposals are extended with classification and regression uncertainty scores. These scores are transformed into deviation ratios and normalized regression uncertainty values, which are then fed into modality-specific expert networks. The gating network combines these expert features to produce uncertainty-aware confidence scores that down-weight unreliable proposals.
- **Core assumption**: Uncertainty scores from MC-Dropout are informative enough to distinguish true positives from false positives across different adverse conditions.
- **Evidence anchors**:
  - [abstract] "UMoE encodes sensor-specific uncertainties into the fusion input and uses expert networks to process each modality, with a gating network determining fusion weights."
  - [section] "we transform the vectors ⃗ uk,cls and ⃗ uk,reg into scalar uncertainty scores uk,cls ∈ R and uk,reg ∈ R as follows."
  - [corpus] "Training of Neural Networks with Uncertain Data: A Mixture of Experts Approach" - Related work on uncertainty-aware MoE exists, but not specifically for multi-modal object detection under adverse conditions.
- **Break condition**: If uncertainty estimation fails to separate true positives from false positives, the gating network cannot effectively down-weight unreliable proposals, leading to degraded fusion performance.

### Mechanism 2
- **Claim**: The Mixture-of-Experts architecture allows the model to handle modality-specific uncertainty sensitivities.
- **Mechanism**: Separate expert networks process each modality's uncertainty-encoded proposals, learning modality-specific feature representations. The gating network then combines these features to determine fusion weights, allowing the model to adapt to each modality's distinct response to adverse conditions.
- **Core assumption**: Modalities have distinct uncertainty sensitivities that can be effectively modeled by separate expert networks.
- **Evidence anchors**:
  - [abstract] "individual expert network is used to process each sensor's detection result with uncertainty encoded. Then, the expert networks' outputs are analyzed by a gating network to determine the fusion weights."
  - [section] "As shown in the motivating example section, different sensing modalities have distinct sensitivities and value ranges for uncertainty scoring. Thus, we exploit different expert network for each sensing modality..."
  - [corpus] "ROFusion: Efficient Object Detection using Hybrid Point-wise Radar-Optical Fusion" - Similar multi-modal fusion exists, but without explicit uncertainty encoding.
- **Break condition**: If modalities have similar uncertainty patterns, the separate expert networks may not learn useful distinctions, and a single network might suffice.

### Mechanism 3
- **Claim**: Transforming raw uncertainty scores into deviation ratios and normalized values makes them comparable across modalities.
- **Mechanism**: Classification uncertainty scores are combined with deviation ratios that measure how far a proposal's uncertainty and confidence deviate from true positive distributions. Regression uncertainty is normalized by bounding box size and standardized using validation set statistics.
- **Core assumption**: Transformed uncertainty metrics preserve the discriminative power needed for effective fusion while enabling cross-modal comparison.
- **Evidence anchors**:
  - [section] "To ensure the informativeness of the classification uncertainty score, we enhance it with a classification deviation ratio..." and "To achieve fair comparisons among objects at different distances, we divide uk,reg by the diagonal length of the averaged bounding box..."
  - [corpus] "PEFT-DML: Parameter-Efficient Fine-Tuning Deep Metric Learning for Robust Multi-Modal 3D Object Detection in Autonomous Driving" - Related work on robust multi-modal detection exists, but without explicit uncertainty encoding.
- **Break condition**: If transformation destroys the relationship between uncertainty and proposal quality, the fusion weights become unreliable.

## Foundational Learning

- **Concept: Monte Carlo Dropout for uncertainty estimation**
  - Why needed here: MC-Dropout provides a practical way to estimate both data and model uncertainty for deep neural networks without requiring ensemble methods or Bayesian neural networks.
  - Quick check question: What is the key difference between MC-Dropout and traditional dropout during inference?

- **Concept: Mixture-of-Experts architecture**
  - Why needed here: MoE allows the model to learn modality-specific feature representations while maintaining a unified fusion decision, which is crucial when different sensors respond differently to adverse conditions.
  - Quick check question: How does the gating network in MoE determine the relative importance of each expert?

- **Concept: Bounding box regression uncertainty**
  - Why needed here: Regression uncertainty provides information about the reliability of object localization, which is critical for safe autonomous driving decisions.
  - Quick check question: Why might a 2D bounding box regression uncertainty be larger than a 3D bounding box regression uncertainty for the same object?

## Architecture Onboarding

- **Component map**: Sensor data → Sensor-specific detectors → Uncertainty scoring → UMoE module → Proposal-level fusion → Final detections
- **Critical path**: Sensor data → Sensor-specific detectors → Uncertainty scoring → UMoE module → Proposal-level fusion → Final detections
- **Design tradeoffs**:
  - Using MC-Dropout adds inference overhead but enables uncertainty estimation
  - Separate expert networks increase model complexity but better handle modality-specific uncertainty patterns
  - Transforming uncertainty scores enables cross-modal comparison but may lose some raw information
- **Failure signatures**:
  - Degradation in clear weather conditions (likely due to uncertainty estimation overhead)
  - Poor performance when all sensors fail simultaneously
  - Unexpected behavior when uncertainty scores are not discriminative
- **First 3 experiments**:
  1. Run UMoE on clear weather data and compare against baseline to verify performance maintenance
  2. Test UMoE on adversarial attack data to confirm improved robustness
  3. Validate that uncertainty scores are actually discriminative by examining true positive vs false positive distributions

## Open Questions the Paper Calls Out
- The paper acknowledges limitations in handling cases where all sensors fail simultaneously and notes that enhancement remains limited when all sensors fail.

## Limitations
- The transformation of raw uncertainty scores into deviation ratios and normalized values introduces uncertainty about whether discriminative power is preserved
- Computational overhead from Monte Carlo Dropout (10 forward passes) may impact real-time performance
- Limited validation of whether uncertainty encoding maintains performance in nominal conditions while improving robustness to adverse conditions

## Confidence
- **High confidence**: Overall architecture design and experimental results showing consistent performance improvements
- **Medium confidence**: Uncertainty transformation methodology and its preservation of discriminative properties
- **Low confidence**: Generalization to other sensor modalities or adverse conditions not tested in the paper

## Next Checks
1. Examine the distribution of classification deviation ratios for true positives vs false positives in the validation set across all tested scenarios to verify the transformation preserves discriminative power.

2. Measure inference time with MC-Dropout enabled versus traditional inference to quantify the computational overhead and assess whether it meets real-time requirements for autonomous driving.

3. Test the UMoE module on a clear weather dataset not seen during training to verify that the uncertainty encoding doesn't degrade performance in nominal conditions while improving robustness to adverse conditions.