---
ver: rpa2
title: Rubric-Specific Approach to Automated Essay Scoring with Augmentation Training
arxiv_id: '2309.02740'
source_url: https://arxiv.org/abs/2309.02740
tags:
- prompt
- response
- essay
- score
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a rubric-specific approach to automated essay
  scoring (AES) that addresses limitations of prompt-specific models by training and
  testing on rubric-segmented datasets. The key innovations are three data augmentation
  methods: Prompt Swap for relevance assessment, Grade Match for adjusting writing
  quality expectations based on grade level, and Response Distortion for detecting
  adversarial inputs.'
---

# Rubric-Specific Approach to Automated Essay Scoring with Augmentation Training

## Quick Facts
- arXiv ID: 2309.02740
- Source URL: https://arxiv.org/abs/2309.02740
- Reference count: 30
- Key outcome: Rubric-specific AES model with augmentation achieves state-of-the-art QWK scores while improving robustness against irrelevant and distorted inputs.

## Executive Summary
This paper addresses limitations in automated essay scoring (AES) by proposing a rubric-specific approach that segments training data by scoring rubrics rather than individual prompts. The authors introduce three data augmentation methods - Prompt Swap, Grade Match, and Response Distortion - to train models to detect irrelevant responses, adjust for grade levels, and resist adversarial inputs. Their Response-Prompt AES architecture, which combines pre-trained BERT with custom attention layers, achieves superior performance on the ASAP dataset while demonstrating improved robustness compared to previous neural approaches.

## Method Summary
The Response-Prompt AES model uses a pre-trained BERT encoder followed by custom self-attention and response-prompt attention layers to process essay and prompt inputs together. The model is trained on rubric-segmented datasets (grouping prompts with identical scoring rubrics) using three augmentation methods: Prompt Swap creates irrelevant essay-prompt pairs for zero-label training, Grade Match uses supervised contrastive learning to adjust for grade levels, and Response Distortion permutes low-scoring essays to improve adversarial robustness. The approach is evaluated on the ASAP dataset using 5-fold cross-validation with Quadratic Weighted Kappa (QWK) as the primary metric.

## Key Results
- Response-Prompt AES achieves state-of-the-art QWK scores on the ASAP dataset
- The model successfully detects irrelevant responses and distorted inputs without sacrificing accuracy
- Rubric-specific models show improved efficiency and generalizability compared to prompt-specific approaches
- Grade Match augmentation effectively adjusts for different grade levels in scoring

## Why This Works (Mechanism)

### Mechanism 1
Rubric-specific models outperform prompt-specific models by forcing the AES system to learn relevance and grade-level adjustment. By segmenting the dataset by scoring rubrics instead of prompts, the model must process essays from multiple prompts and grade levels, making it learn features like relevance detection and grade-level expectations. Human raters learn rubrics, not individual prompts; therefore, rubric-specific training better aligns with human grading behavior.

### Mechanism 2
Data augmentation methods train the model to detect irrelevant responses, adjust for grade level, and penalize adversarial inputs without hurting accuracy. Prompt Swap generates mismatched essay-prompt pairs with zero labels to teach relevance detection. Grade Match uses contrastive learning to push essays from the same grade closer and those from different grades apart. Response Distortion partially permutes low-score essays to train adversarial robustness.

### Mechanism 3
The Response-Prompt AES architecture with custom attention layers can process long essays and measure response-prompt relevance better than standard pre-trained models. The model uses BERT for contextual embeddings, a self-attention layer for long sequences, and a response-prompt attention layer to compute relevance scores, which are then combined for final scoring.

## Foundational Learning

- **Concept: Rubric segmentation vs prompt segmentation**
  - Why needed here: To understand why training on rubric-segmented data forces the model to learn relevance and grade-level features that prompt-segmented data hides.
  - Quick check question: What features are lost when you segment by prompt instead of rubric, and how does that affect model capabilities?

- **Concept: Contrastive learning for grade adjustment**
  - Why needed here: Grade Match uses supervised contrastive learning to push essays from the same grade closer and those from different grades apart, enabling grade-level adjustment.
  - Quick check question: How does supervised contrastive learning differ from standard contrastive learning, and why is it effective for grade adjustment?

- **Concept: Adversarial training without robustness-accuracy tradeoff**
  - Why needed here: Response Distortion shows that carefully targeted augmentation (only on lowest score essays) can improve adversarial robustness without hurting accuracy.
  - Quick check question: What makes Response Distortion different from typical adversarial training methods that often sacrifice accuracy?

## Architecture Onboarding

- **Component map:**
  Essay tokens + Prompt tokens -> BERT encoder -> Response self-attention -> Response-prompt attention -> Concatenation -> Dense regression

- **Critical path:**
  BERT → Response self-attention → Response-prompt attention → Concatenation → Regression

- **Design tradeoffs:**
  Using custom attention layers instead of longer-sequence pre-trained models adds training complexity but enables handling of longer essays. Segmentation by rubric instead of prompt increases data efficiency but may lose prompt-specific nuances.

- **Failure signatures:**
  Poor relevance detection: Model assigns high scores to irrelevant essays. Grade-level misalignment: Model scores essays from different grades with same expectations. Adversarial vulnerability: Model fails on permuted or off-topic inputs.

- **First 3 experiments:**
  1. Train baseline Response-Prompt AES on rubric-segmented data without augmentations and measure QWK.
  2. Add Prompt Swap to the baseline and measure irrelevant response detection rate and QWK change.
  3. Add Grade Match to the Prompt Swap model and measure grade-level QWK improvements.

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: How does the rubric-specific approach perform on datasets with more than two prompts sharing the same rubric?
  - Basis in paper: [explicit] The paper demonstrates the approach on datasets with 2 prompts sharing the same rubric (Prompts 3-4 and 5-6), but doesn't explore scenarios with 3 or more prompts.
  - Why unresolved: The paper only tests the approach on the ASAP dataset which has a maximum of 2 prompts per rubric. There's no evidence of performance on larger multi-prompt scenarios.
  - What evidence would resolve it: Testing the rubric-specific approach on a dataset with 3+ prompts sharing the same rubric and comparing performance metrics to prompt-specific approaches.

- **Open Question 2**
  - Question: What is the impact of different data augmentation ratios on model performance and robustness?
  - Basis in paper: [inferred] The paper tests specific augmentation rates (5% Prompt Swap, 1-2 samples for Response Distortion) but doesn't systematically explore how varying these ratios affects performance.
  - Why unresolved: The optimal augmentation ratio may vary depending on dataset characteristics, and the paper only explores a limited range of augmentation intensities.
  - What evidence would resolve it: Systematic ablation studies varying augmentation ratios across different datasets and measuring the trade-off between accuracy, robustness, and computational efficiency.

- **Open Question 3**
  - Question: How does the rubric-specific approach generalize to scoring rubrics with different structures or complexity levels?
  - Basis in paper: [inferred] The paper tests on ASAP rubrics which have similar structures, but doesn't examine how the approach handles rubrics with different complexity levels or scoring criteria.
  - Why unresolved: The approach may not generalize well to rubrics with more complex scoring criteria, multiple dimensions, or different weightings of rubric items.
  - What evidence would resolve it: Testing the approach on datasets with diverse rubric structures and complexity levels, measuring performance degradation or adaptation requirements for each case.

## Limitations

- The paper lacks direct empirical comparison between rubric-specific and prompt-specific models to demonstrate claimed superiority.
- The Response Distortion augmentation's effectiveness contradicts established machine learning principles about robustness-accuracy tradeoffs.
- The paper doesn't provide ablation studies to demonstrate that custom attention layers are responsible for performance gains.

## Confidence

- **High confidence**: The technical implementation details of the Response-Prompt AES model architecture are clearly specified and reproducible.
- **Medium confidence**: The QWK performance results on the ASAP dataset are likely reliable given the 5-fold cross-validation approach.
- **Low confidence**: Claims about the superiority of rubric-specific models over prompt-specific models and the effectiveness of data augmentation methods without robustness-accuracy tradeoffs.

## Next Checks

1. **Direct comparison experiment**: Implement and evaluate a prompt-specific AES model using the same architecture and training procedure to directly compare performance with the rubric-specific approach on the ASAP dataset.

2. **Ablation study of augmentation methods**: Remove each augmentation method (Prompt Swap, Grade Match, Response Distortion) individually and measure their individual contributions to overall performance and robustness.

3. **Cross-dataset generalization test**: Evaluate the trained models on a different AES dataset (if available) or on out-of-distribution prompts to assess whether the rubric-specific approach truly generalizes better than prompt-specific alternatives.