---
ver: rpa2
title: An Attentive Inductive Bias for Sequential Recommendation beyond the Self-Attention
arxiv_id: '2312.10325'
source_url: https://arxiv.org/abs/2312.10325
tags:
- ndcg
- bsarec
- sequential
- datasets
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reveals that self-attention in Transformer-based sequential
  recommendation models acts as a low-pass filter, causing oversmoothing that degrades
  performance. To address this, the authors propose Beyond Self-Attention for Sequential
  Recommendation (BSARec), which combines an inductive bias based on Fourier transform
  with self-attention and a frequency rescaler to balance low and high-frequency information.
---

# An Attentive Inductive Bias for Sequential Recommendation beyond the Self-Attention

## Quick Facts
- arXiv ID: 2312.10325
- Source URL: https://arxiv.org/abs/2312.10325
- Reference count: 40
- Key outcome: BSARec outperforms 7 baselines with up to 27.49% improvement in HR@10 on LastFM

## Executive Summary
This paper addresses the oversmoothing problem in Transformer-based sequential recommendation models, where self-attention acts as a low-pass filter that degrades performance. The authors propose Beyond Self-Attention for Sequential Recommendation (BSARec), which combines Fourier transform-based inductive bias with self-attention and a frequency rescaler. The model effectively balances low and high-frequency information to capture both long-term interests and short-term trends. Experiments on 6 benchmark datasets show BSARec achieves state-of-the-art performance, with improvements up to 27.49% in HR@10 on LastFM compared to baselines.

## Method Summary
BSARec addresses oversmoothing in sequential recommendation by injecting frequency-domain inductive bias through Fourier transform. The model combines traditional self-attention with a Fourier-based inductive bias component weighted by parameter α, and uses a frequency rescaler β to dynamically adjust the importance of high-frequency components across layers. The architecture processes user interaction sequences through embedding layers, stacks beyond self-attention blocks containing the hybrid attention mechanism, and predicts item preferences using dot products. The model is trained using cross-entropy loss with Adam optimizer.

## Key Results
- BSARec achieves state-of-the-art performance across 6 benchmark datasets
- Improvements up to 27.49% in HR@10 on LastFM compared to best baseline
- Learned frequency scaling parameter β shows consistent patterns across datasets, with higher weight on high-frequency components in earlier layers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model addresses oversmoothing by injecting frequency-domain inductive bias through Fourier transform, balancing low and high-frequency information.
- Mechanism: By decomposing sequential patterns into frequency components using DFT, the model selectively amplifies high-frequency signals (short-term interests) while preserving low-frequency signals (long-term interests), preventing token representations from becoming too similar across layers.
- Core assumption: Sequential user behavior patterns can be effectively decomposed into distinct frequency components where low frequencies represent persistent interests and high frequencies capture abrupt changes.
- Evidence anchors:
  - [abstract] "We propose a novel method called Beyond Self-Attention for Sequential Recommendation (BSARec), which leverages the Fourier transform to i) inject an inductive bias by considering fine-grained sequential patterns and ii) integrate low and high-frequency information to mitigate oversmoothing."
  - [section] "To tackle the oversmoothing issue, we introduce our own designed frequency rescaler to apply high-pass filters into BSARec's architecture. Our frequency rescaler can capture high-frequency behavioral patterns, such as interests driven by short-term trends, as well as low-frequency patterns, such as long-term interests."
  - [corpus] Weak - corpus lacks direct evidence about frequency decomposition in sequential recommendation.
- Break condition: If user behavior patterns cannot be meaningfully separated into distinct frequency components, or if high-frequency signals do not correspond to meaningful short-term interests.

### Mechanism 2
- Claim: The model combines traditional self-attention with Fourier-transform-based inductive bias to capture both obvious and non-obvious sequential dependencies.
- Mechanism: The architecture creates a weighted combination where α controls the balance between self-attention matrix Aℓ and Fourier-based inductive bias AℓIB, allowing the model to leverage both learned attention patterns and predefined frequency structures.
- Core assumption: Both learned self-attention and predefined frequency-based patterns provide complementary information that improves recommendation accuracy when combined appropriately.
- Evidence anchors:
  - [section] "We develop item encoders by stacking beyond self-attention (BSA) blocks... Sℓ = eAℓFℓ = αAℓIBFℓ + (1 − α)AℓFℓ, where the first term corresponds to DSP, where the discrete Fourier transform is utilized, α ≤ 1 is a coefficient to (de)emphasize the inductive bias."
  - [section] "By separating Aℓ from eAℓ, the self-attention mechanism focuses on capturing non-obvious attentions in Aℓ."
  - [corpus] Weak - corpus does not provide evidence about combining self-attention with Fourier-based inductive bias.
- Break condition: If either component (self-attention or Fourier-based bias) dominates excessively, or if their combination creates conflicting signals.

### Mechanism 3
- Claim: The frequency rescaler parameter β dynamically adjusts the importance of high-frequency components across different layers and datasets.
- Mechanism: β is learned during training and controls the scaling of high-frequency information in the frequency rescaler, allowing the model to adaptively emphasize either short-term or long-term interests based on dataset characteristics.
- Core assumption: The optimal balance between high and low-frequency information varies across datasets and can be learned effectively during training.
- Evidence anchors:
  - [section] "We propose a filter that injects the attentive inductive bias and at the same time adjusts the scale of the frequency by dividing it into low and high frequency components: AℓIBFℓ = LFC[Fℓ] + βHFC[Fℓ], where β is a trainable parameters to scale the high-pass filter."
  - [section] "In Fig. 7 (a), we show learned β at each layer for all datasets. We can see that a higher weight in the first layer is learned than in the second layer, which confirms that putting more weight on high-frequency in the first layer is effective."
  - [corpus] Weak - corpus lacks evidence about learned frequency scaling parameters.
- Break condition: If β fails to converge to meaningful values or if its learned values do not correlate with improved performance.

## Foundational Learning

- Concept: Discrete Fourier Transform (DFT) and its application to sequential data
  - Why needed here: The paper relies on DFT to decompose sequential patterns into frequency components, which is fundamental to understanding how the model captures both long-term and short-term interests.
  - Quick check question: How does DFT transform a sequence of length N into N frequency components, and what do the low and high-frequency components represent in the context of user behavior?

- Concept: Low-pass vs. high-pass filtering in signal processing
  - Why needed here: The paper's core contribution involves using frequency filtering to address oversmoothing, so understanding these filtering concepts is essential for grasping the mechanism.
  - Quick check question: What is the mathematical difference between a low-pass filter that preserves low frequencies and a high-pass filter that preserves high frequencies, and how do these relate to user interest patterns?

- Concept: Transformer self-attention mechanism and its limitations
  - Why needed here: The paper builds upon standard Transformer architecture and identifies specific limitations that need to be addressed.
  - Quick check question: How does the self-attention mechanism in Transformers compute attention weights, and why does repeated application lead to oversmoothing according to the paper's analysis?

## Architecture Onboarding

- Component map:
  Embedding Layer -> Beyond Self-Attention Encoder -> Prediction Layer -> Training Component

- Critical path:
  1. Input sequence → Embedding Layer (positional encoding + normalization)
  2. Embedded sequence → Beyond Self-Attention Encoder (BSA blocks with frequency rescaler)
  3. Final layer output → Prediction Layer (dot product with item embeddings)
  4. Predicted scores → Cross-entropy loss computation and backpropagation

- Design tradeoffs:
  - Frequency resolution vs. computational cost: Higher c values provide better frequency resolution but increase computation
  - Balance between self-attention and inductive bias: α parameter controls this tradeoff, affecting model flexibility vs. inductive bias strength
  - Learnable vs. fixed frequency components: The model uses learnable β but fixed DFT bases, balancing adaptability with computational efficiency

- Failure signatures:
  - If α is too high, the model may overfit to training data and lose generalization
  - If β is not properly learned, the model may fail to capture appropriate frequency balance
  - If c is too small, important frequency information may be lost; if too large, noise may be introduced

- First 3 experiments:
  1. Test different α values (0.1, 0.5, 0.9) to find optimal balance between self-attention and inductive bias
  2. Evaluate sensitivity to c parameter (1, 5, 9) to determine optimal frequency resolution
  3. Compare performance with different β configurations (scalar vs. vector) to validate frequency scaling effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the number of frequency components c affect the trade-off between low and high-frequency information in BSARec, and what is the optimal range for c across different datasets?
- Basis in paper: [explicit] The paper mentions that c is chosen from {1, 3, 5, 7, 9} and discusses its sensitivity in the experiments, but does not provide a clear guideline for the optimal choice of c.
- Why unresolved: The optimal choice of c may vary depending on the dataset characteristics, such as the length of user sequences and the diversity of user interests. A systematic study on the relationship between c and dataset properties is needed.
- What evidence would resolve it: A comprehensive analysis of the performance of BSARec with different values of c across a wide range of datasets, considering factors such as sequence length, sparsity, and domain, would provide insights into the optimal choice of c.

### Open Question 2
- Question: Can the proposed BSARec model be extended to handle multi-modal sequential recommendation, where user interactions involve different types of items (e.g., products, videos, and articles)?
- Basis in paper: [inferred] The paper focuses on sequential recommendation with item interactions, but does not discuss the possibility of extending the model to handle multi-modal data.
- Why unresolved: The current BSARec model is designed for item embeddings and does not consider the different modalities that may be present in user interactions. Adapting the model to handle multi-modal data would require incorporating additional embedding techniques and attention mechanisms.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of BSARec in multi-modal sequential recommendation tasks, along with a discussion of the modifications made to the model to handle different modalities, would provide insights into its extensibility.

### Open Question 3
- Question: How does the performance of BSARec compare to state-of-the-art models in cold-start scenarios, where new users or items have limited interaction history?
- Basis in paper: [inferred] The paper does not explicitly address the cold-start problem, which is a common challenge in recommendation systems.
- Why unresolved: The performance of BSARec in cold-start scenarios is unknown, and it is unclear whether the model can effectively capture user preferences and make accurate recommendations with limited data.
- What evidence would resolve it: Comparative experiments evaluating the performance of BSARec and other state-of-the-art models in cold-start scenarios, using metrics such as hit rate and NDCG, would provide insights into its effectiveness in handling limited interaction history.

## Limitations

- The paper's theoretical analysis of self-attention as a low-pass filter is primarily conceptual, with limited empirical validation of the frequency-domain interpretation.
- The effectiveness of the frequency rescaler depends heavily on the assumption that user behavior can be meaningfully decomposed into distinct frequency components, which may not hold for all types of sequential data or recommendation domains.
- The evaluation focuses on benchmark datasets with relatively short sequences, and the method's performance on long sequences (beyond 50 items) or cold-start scenarios remains unverified.

## Confidence

- **High Confidence:** The experimental results showing BSARec outperforming baselines on the tested metrics (HR@k, NDCG@k) are well-supported by the provided evidence.
- **Medium Confidence:** The theoretical framework connecting self-attention to low-pass filtering and the proposed solution's mechanism are conceptually sound but would benefit from more rigorous mathematical analysis and broader empirical validation.
- **Low Confidence:** Claims about the universality of frequency-based inductive bias for sequential recommendation require further investigation, particularly for domains where user behavior patterns may not exhibit clear frequency characteristics.

## Next Checks

1. Conduct experiments on datasets with longer sequences (100+ items) to verify the method's effectiveness scales with sequence length and doesn't degrade due to increased computational complexity.
2. Perform ablation studies specifically isolating the contribution of the frequency rescaler β parameter by comparing learned vs. fixed values across different recommendation domains.
3. Validate the low-pass filtering hypothesis by analyzing attention weight distributions before and after applying the frequency-based inductive bias, measuring changes in attention similarity across layers.