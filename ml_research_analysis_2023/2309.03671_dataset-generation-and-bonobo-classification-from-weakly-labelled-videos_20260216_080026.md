---
ver: rpa2
title: Dataset Generation and Bonobo Classification from Weakly Labelled Videos
arxiv_id: '2309.03671'
source_url: https://arxiv.org/abs/2309.03671
tags:
- classification
- dataset
- bonobo
- datasets
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of automatically identifying individual
  bonobos from video recordings to support cognitive testing in enclosures without
  human assistance. The method uses a weakly labelled video dataset, with a macaque
  detector providing spatial region-of-interest (ROI) outputs.
---

# Dataset Generation and Bonobo Classification from Weakly Labelled Videos

## Quick Facts
- arXiv ID: 2309.03671
- Source URL: https://arxiv.org/abs/2309.03671
- Reference count: 30
- Primary result: 75% classification accuracy achieved using fine-tuned ResNet18 on ROI data

## Executive Summary
This paper presents a method for automatically identifying individual bonobos from video recordings to support cognitive testing in enclosures without human assistance. The approach uses weakly labelled video data with spatial region-of-interest (ROI) outputs from a macaque detector, testing multiple classification approaches including traditional machine learning with handcrafted features and deep learning models. The study emphasizes proper data splitting strategies to avoid overestimating performance, achieving 75% classification accuracy on the test set using a fine-tuned ResNet18 model trained on high-confidence ROI data.

## Method Summary
The method uses a weakly labelled video dataset of six bonobos, with a macaque detector providing spatial ROI outputs. Multiple classification approaches are tested: handcrafted features (Hu moments, Haralick texture, color histogram) with traditional machine learning algorithms (LR, LDA, NB, KNN, SVM, CART, RF), and deep learning models using ResNet18 as a feature extractor or with fine-tuning. Key methodological elements include different data splitting strategies (video-level to avoid leakage) and weighted cross-entropy loss for imbalanced data. The best performance of 75% classification accuracy was achieved using a fine-tuned ResNet18 model trained on ROI data with high detection confidence.

## Key Results
- Fine-tuned ResNet18 achieved 75% classification accuracy on the test set, outperforming other approaches
- Proper video-level data splitting is critical to avoid overestimating performance
- Weighted cross-entropy loss showed potential benefits for imbalanced datasets
- Handcrafted features with traditional ML methods performed significantly worse than deep learning approaches

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning a pre-trained ResNet18 with full parameter updates outperforms feature extraction and handcrafted methods because it adapts convolutional filters to the specific appearance of bonobos. Updating all convolutional layers allows the model to learn local feature patterns unique to bonobo bodies and poses, which differ from the macaque data used in pretraining. Core assumption: The detection ROI captures informative visual regions that correlate with individual identity. Evidence: Fine-tuned ResNet on ROI, S0 dataset performed best across all datasets and models. Break condition: If ROI quality drops, fine-tuning may overfit noise instead of identity cues.

### Mechanism 2
Weighted cross-entropy loss improves classification when training data are imbalanced across individuals. Assigning higher weights to underrepresented classes during training reduces bias toward majority classes. Core assumption: Class imbalance in the training set is a major source of misclassification. Evidence: Weighting equals 1 if samples were evenly distributed, suggesting adjustment for imbalance. Break condition: If dataset becomes more balanced, weighting may no longer provide benefit.

### Mechanism 3
Proper video-level splitting avoids data leakage and inflated performance metrics. Ensuring all frames from a given video belong exclusively to one split prevents the model from seeing near-duplicate images across splits. Core assumption: Within-video frames are highly similar and not independent samples. Evidence: High similarity of data extracted from videos while performing 10-fold cross-validation. Break condition: If videos contain diverse scenes, stricter split may reduce training data excessively.

## Foundational Learning

- Concept: Weakly supervised detection ROI generation
  - Why needed here: Dataset lacks manual bounding box annotations; detection model provides spatial ROI to focus classification on bonobo regions
  - Quick check question: What happens if the detector ROI misses the bonobo entirely in a frame?

- Concept: Data splitting strategy to avoid leakage
  - Why needed here: Bonobo videos contain many near-duplicate frames; splitting at video level ensures independence between train/val/test sets
  - Quick check question: If you split by random frames instead of whole videos, how might your test accuracy change?

- Concept: ResNet fine-tuning vs feature extraction
  - Why needed here: Full fine-tuning adapts learned filters to bonobo-specific features, whereas feature extraction uses generic ImageNet/macaques features
  - Quick check question: Why might fine-tuning sometimes overfit on a small dataset?

## Architecture Onboarding

- Component map: video acquisition -> macaque detector ROI extraction -> split into train/val/test -> preprocessing (resize to 224Ã—224) -> classification models (traditional ML or ResNet18) -> evaluation
- Critical path: 1) Generate ROI using macaque detector, 2) Split videos (not frames) into train/val/test, 3) Train fine-tuned ResNet18 on ROI data, 4) Evaluate on held-out test set
- Design tradeoffs: ROI vs full frame (ROI reduces background noise but may lose context), fine-tuning vs feature extraction (fine-tuning adapts features but risks overfitting on small data)
- Failure signatures: High train accuracy but low val/test accuracy (overfitting or data leakage), confusion dominated by few classes (class imbalance or detector bias)
- First 3 experiments: 1) Compare accuracy when splitting by frames vs by whole videos, 2) Train ResNet18 with feature extraction only on ROI data, 3) Train ResNet18 with fine-tuning on ROI data using weighted vs standard cross-entropy loss

## Open Questions the Paper Calls Out

### Open Question 1
How can temporal information from videos improve individual bonobo classification performance? The paper suggests using temporal information for tracking and classification could improve performance, noting that multiple appearances of the individual in the track should help in the classification decision. This remains unresolved as the study only used static image frames without incorporating temporal data or tracking algorithms.

### Open Question 2
How would direct feature difference methods compare to traditional machine learning and deep learning approaches for bonobo identification? The authors mention future work will investigate direct feature differences to estimate similarity between individuals as an alternative approach. This remains unresolved as the study only compared handcrafted features with traditional ML algorithms and deep learning feature extractors/fine-tuned models.

### Open Question 3
How does the quality of automatically generated datasets compare to manually annotated datasets for bonobo classification? The paper plans to evaluate the quality of generated datasets by manually annotating videos using CVAT tool to assess detection method and dataset quality. This remains unresolved as the current study used weakly annotated dataset generated through macaque detection without ground truth validation.

## Limitations
- Limited sample size (6 bonobos) may constrain generalization
- No explicit ablation on ROI quality thresholds
- No comparison to human-level baseline or alternative detection models
- Weak grounding from corpus neighbors for mechanism validation

## Confidence
- High confidence: Importance of video-level splitting to prevent data leakage (supported by empirical observation)
- Medium confidence: ResNet fine-tuning advantage (plausible but lacks comparative studies on similar animal datasets)
- Low confidence: Weighted cross-entropy benefit (no direct neighbor evidence; mechanism plausible but unproven)

## Next Checks
1. Conduct ablation study varying detection confidence thresholds to quantify ROI quality impact
2. Compare video-level vs random frame splitting to measure data leakage effects
3. Test human annotator performance on same task to establish baseline reference