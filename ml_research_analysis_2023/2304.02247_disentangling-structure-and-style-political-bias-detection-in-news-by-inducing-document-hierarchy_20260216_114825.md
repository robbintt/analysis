---
ver: rpa2
title: 'Disentangling Structure and Style: Political Bias Detection in News by Inducing
  Document Hierarchy'
arxiv_id: '2304.02247'
source_url: https://arxiv.org/abs/2304.02247
tags:
- news
- bias
- sentences
- test
- said
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses political bias detection in news articles,
  tackling the issue of models overfitting to writing styles of specific outlets.
  The proposed approach uses a hierarchical attention mechanism that considers both
  sentence-level semantics and document-level rhetorical structure, aiming for a more
  robust, style-agnostic method.
---

# Disentangling Structure and Style: Political Bias Detection in News by Inducing Document Hierarchy

## Quick Facts
- **arXiv ID**: 2304.02247
- **Source URL**: https://arxiv.org/abs/2304.02247
- **Reference count**: 27
- **Primary result**: Hierarchical attention model with 8 heads outperforms BERT baseline in accuracy and domain robustness for political bias detection in news

## Executive Summary
This work addresses political bias detection in news articles by disentangling writing style from document structure. The authors propose a multi-head hierarchical attention model that separately identifies main and supporting sentences across multiple discourse contexts, reducing overfitting to outlet-specific writing styles. The approach uses SBERT for sentence embeddings, BiLSTM for positional encoding, and multi-head attention to model discourse relations. Experiments on a balanced news bias dataset show the model outperforms BERT baselines in accuracy and is more resilient to domain shifts between training and test data.

## Method Summary
The method uses a multi-head hierarchical attention architecture to encode document structure for political bias detection. Articles are processed through SBERT to generate sentence embeddings, which are then passed through a BiLSTM with positional encoding to capture sentence roles based on location. Multi-head attention independently models discourse relations between sentences for different contexts, allowing the model to identify main vs. supporting sentences separately for each head. Context cluster embeddings are created by weighting sentence embeddings based on their predicted importance, and a linear classifier predicts bias from the averaged context representations. The model is trained on a rebalanced dataset of 21,900 articles with disjoint test sets to evaluate domain robustness.

## Key Results
- Multi-head hierarchical attention model outperforms BERT baseline in MacroF1 and AUROC across multiple train/test splits
- Model shows greater resilience to domain shifts between training and test news outlets compared to baseline
- Structural analysis reveals the model effectively captures common journalism discourse structures, including inverted pyramid and interpretive news styles

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Hierarchical attention with multiple heads allows the model to separately identify main and supporting sentences for multiple discourse contexts.
- **Mechanism**: Multi-head attention propagates document-level bias labels to sentence subsets by learning different contextual viewpoints in parallel. Each head independently captures a distinct discourse context, reducing overfitting to outlet-specific writing styles.
- **Core assumption**: Discourse structures in journalism can be modeled as independent context clusters that each contribute to bias prediction.
- **Evidence anchors**: [abstract] "novel multi-head hierarchical attention model that effectively encodes the structure of long documents through a diverse ensemble of attention heads" and [section 4.2] "Each sentence is assigned one of two roles: main sentence or supporting sentence."
- **Break condition**: If discourse structures are not independent or if sentence importance cannot be inferred from headline-sentence similarity.

### Mechanism 2
- **Claim**: Modeling position and discourse relations separately improves robustness to domain shifts between training and test news outlets.
- **Mechanism**: BiLSTM encodes positional information to capture sentence roles based on article location; multi-head attention models discourse relations between sentences. This dual encoding reduces reliance on outlet-specific lexical cues.
- **Core assumption**: Writing style varies by outlet, but structural roles are consistent across outlets.
- **Evidence anchors**: [section 4.1] "positional information captures the role of a sentence with respect to its location in the article" and [section 6] experiments explicitly test domain shift robustness.
- **Break condition**: If positional encoding does not capture discourse roles or if outlet-specific discourse patterns dominate.

### Mechanism 3
- **Claim**: Using sentence-level embeddings from SBERT instead of word-level embeddings improves semantic coherence for long document classification.
- **Mechanism**: SBERT generates fixed-size sentence embeddings that preserve semantic similarity better than averaging word embeddings. Hierarchical encoding then combines these sentence representations.
- **Core assumption**: Sentence-level semantics are more stable than word-level semantics for capturing document-level bias.
- **Evidence anchors**: [section 4.1] "semantics of each sentence is independently captured by using a large language model to generate a sentence embedding" and [section 6] model outperforms BERT baseline.
- **Break condition**: If sentence embeddings do not preserve semantic similarity better than word embeddings for this task.

## Foundational Learning

- **Concept**: Discourse structure in journalism (inverted pyramid, interpretive news)
  - **Why needed here**: Model explicitly clusters articles by discourse structure and evaluates main sentence positions accordingly
  - **Quick check question**: What are the two main discourse structures identified in the BASIL dataset analysis?

- **Concept**: Multi-head attention mechanisms
  - **Why needed here**: Core architectural component that enables independent context clustering and reduces outlet-specific overfitting
  - **Quick check question**: How does the model use multi-head attention differently from standard transformer implementations?

- **Concept**: Hierarchical attention networks
  - **Why needed here**: Enables encoding of long documents by first encoding sentences, then combining them at document level
  - **Quick check question**: What are the three key stages in the model's hierarchical pipeline?

## Architecture Onboarding

- **Component map**: Headline and sentences → SBERT → BiLSTM → Multi-head attention → Sentence type detection → Context cluster embedding → Linear classifier
- **Critical path**: SBERT → BiLSTM → Multi-head attention → Sentence type detection → Context cluster embedding → Classification
- **Design tradeoffs**:
  - Fixed SBERT parameters vs. fine-tuning: Chosen for stability but limits adaptation
  - 8 attention heads: Balances diversity vs. computational cost
  - Separate positional encoding: Captures structural roles but adds complexity
- **Failure signatures**:
  - High variance between test sets → domain shift problem
  - Low BERTScore between main sentences and summaries → poor extractive summarization
  - Main sentences clustered in wrong document positions → incorrect structural understanding
- **First 3 experiments**:
  1. Compare AUROC/MacroF1 on Test Set 1 vs Test Set 2 to measure domain robustness
  2. Measure variance of AUROC across 20 random train subsets to assess training data sensitivity
  3. Compute BERTScore between predicted main sentences and BART-generated summaries to validate extractive quality

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the hierarchical multi-head attention model's performance on political bias detection generalize to other domains with formalized writing styles, such as legal or scientific writing?
- **Basis in paper**: [explicit] The authors mention that future work plans to extend the approach to other domains with formalized writing styles, like the legal domain.
- **Why unresolved**: The current study only evaluates the model on news articles. There is no empirical evidence to support whether the model's effectiveness transfers to other domains.
- **What evidence would resolve it**: Testing the model on political bias datasets from other domains (legal, scientific) and comparing its performance to baseline models.

### Open Question 2
- **Question**: How does the model's bias detection accuracy compare when using different sentence embedding techniques (e.g., SBERT vs. other transformer-based models) in the semantic understanding stage?
- **Basis in paper**: [explicit] The authors use SBERT for sentence embeddings, but do not explore alternative embedding methods.
- **Why unresolved**: The choice of sentence embedding technique could significantly impact the model's performance. No comparison is provided.
- **What evidence would resolve it**: Replacing SBERT with other sentence embedding models and measuring the impact on bias detection accuracy.

### Open Question 3
- **Question**: What is the impact of varying the number of attention heads in the multi-head attention mechanism on the model's ability to capture document structure and reduce domain dependency?
- **Basis in paper**: [explicit] The authors fix the number of attention heads to 8, but do not explore the impact of using a different number of heads.
- **Why unresolved**: The optimal number of attention heads for this task is not determined. The chosen number may not be ideal for all datasets or domains.
- **What evidence would resolve it**: Experimenting with different numbers of attention heads and evaluating their impact on bias detection accuracy, domain robustness, and ability to capture discourse structures.

## Limitations
- The methodology lacks direct corpus evidence supporting the claim that discourse structures can be modeled as independent context clusters
- The assumption that sentence importance can be reliably inferred from headline-sentence similarity may not hold across all journalistic styles
- The choice of 8 attention heads appears arbitrary without sensitivity analysis to determine optimal configuration

## Confidence
- **High confidence**: The experimental methodology for testing domain shift robustness is clearly specified and replicable
- **Medium confidence**: The hierarchical attention architecture description is detailed enough for reproduction, though some implementation specifics are missing
- **Low confidence**: The claims about independent discourse contexts and their role in reducing outlet-specific overfitting lack sufficient theoretical or empirical support

## Next Checks
1. **Discourse Structure Validation**: Analyze the model's attention weights to verify that the identified main sentences actually correspond to the expected positions in inverted pyramid vs interpretive news structures, using manual annotation of a subset of articles.
2. **Head Independence Analysis**: Measure the correlation between different attention heads' sentence type predictions to quantify whether they are truly learning independent discourse contexts or simply duplicating similar patterns.
3. **Baseline Comparison with Structural Features**: Implement a simpler model that explicitly encodes known journalistic structures (e.g., paragraph position, sentence length) and compare its robustness to domain shifts against the proposed multi-head approach to determine if the complexity is justified.