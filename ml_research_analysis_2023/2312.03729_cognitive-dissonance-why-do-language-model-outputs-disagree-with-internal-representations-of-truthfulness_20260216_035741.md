---
ver: rpa2
title: 'Cognitive Dissonance: Why Do Language Model Outputs Disagree with Internal
  Representations of Truthfulness?'
arxiv_id: '2312.03729'
source_url: https://arxiv.org/abs/2312.03729
tags:
- probes
- probe
- queries
- accuracy
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the disagreement between language model
  (LM) outputs and internal representations of truthfulness. Past work found that
  probes trained on LM hidden states are more accurate than direct LM queries, leading
  some to conclude that LMs "lie." The authors analyze this phenomenon and find that
  most mismatches arise from two main sources: better calibration of probes on uncertain
  answers (confabulation) and better performance of probes vs.'
---

# Cognitive Dissonance: Why Do Language Model Outputs Disagree with Internal Representations of Truthfulness?

## Quick Facts
- arXiv ID: 2312.03729
- Source URL: https://arxiv.org/abs/2312.03729
- Reference count: 4
- Key outcome: Probes trained on LM hidden states are more accurate than direct LM queries, but this superiority stems from calibration differences and heterogeneous performance rather than intentional deception

## Executive Summary
This paper investigates why language model (LM) outputs often disagree with internal representations of truthfulness. Past work found that probes trained on LM hidden states outperform direct LM queries on factual reasoning tasks, leading some to conclude that LMs "lie." The authors analyze this phenomenon and find that most mismatches arise from better calibration of probes on uncertain answers and heterogeneous performance of probes vs. queries on different data subsets. They propose a taxonomy of disagreement types and show that ensembles of probes and queries can improve accuracy. Overall, their results suggest that mismatches stem from different prediction pathways rather than intentional deception.

## Method Summary
The authors train linear probes on LM hidden states to classify (question, answer) pairs as correct or incorrect. They compare probe accuracy to direct LM querying across three datasets (BoolQ, SciQ, CREAK) and two models (GPT-2 XL, GPT-J). The study analyzes calibration differences between probes and queries, measures heterogeneous performance across data subsets, and evaluates ensemble methods that combine probe and query predictions. The analysis focuses on understanding the distribution of different types of disagreements rather than simply measuring accuracy differences.

## Key Results
- Probes are better calibrated than direct LM queries on uncertain answers, with probes showing near-perfect calibration while queries are poorly calibrated on most datasets
- Probes and queries exhibit heterogeneous performance, with each being more accurate on different subsets of inputs
- Ensembling probes and queries improves accuracy, with gains of up to 2% on the SciQ dataset
- Most disagreement types are attributable to calibration differences or heterogeneous performance rather than intentional deception

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Disagreement between LM outputs and internal probe representations is largely due to different prediction pathways rather than intentional deception.
- Mechanism: Language models have multiple computational pathways for generating predictions, and these pathways can produce different outputs when faced with the same input. Probes access internal representations that follow a different pathway than the output generation process.
- Core assumption: The language model's internal representations contain information that is not fully utilized in the output generation process.
- Evidence anchors:
  - [abstract] "LMs don't explicitly encode a latent intent to generate incorrect outputs, but rather that mismatches stem from different prediction pathways."
  - [section 1] "in a widely studied probe class, two LMs, and three question-answering datasets, we identify three qualitatively different reasons that probes may outperform queries, which we call confabulation, heterogeneity, and (in a small number of instances) what past work would characterize as deception"
  - [corpus] Weak - corpus neighbors focus on related probing and representation topics but don't directly support this specific mechanism
- Break condition: If future work demonstrates that LMs do have a single unified pathway for truth representation that is consistently used for both internal representations and outputs, this mechanism would be invalidated.

### Mechanism 2
- Claim: Probes are better calibrated than direct LM queries on uncertain answers, leading to improved accuracy.
- Mechanism: Probes trained on labeled data learn to properly calibrate their confidence scores, while direct LM queries tend to be overconfident on uncertain predictions. This calibration difference makes probes more reliable even when they don't have higher raw accuracy.
- Core assumption: Calibration of confidence scores is learnable from labeled training data and transferable to held-out data.
- Evidence anchors:
  - [abstract] "In many cases, the superiority of probes is simply attributable to better calibration on uncertain answers rather than a greater fraction of correct, high-confidence answers."
  - [section 3] "Another striking difference between probes and queries is visualized in Fig. 2, which plots the calibration of predictions from GPT-J... Probes are well calibrated, while queries are reasonably well calibrated on SciQ but poorly calibrated on other datasets."
  - [corpus] Weak - corpus neighbors discuss probing and representation but don't specifically address calibration differences
- Break condition: If future work shows that direct LM queries can be made as well-calibrated as probes through alternative techniques, this mechanism would lose explanatory power.

### Mechanism 3
- Claim: Probes and queries exhibit heterogeneous performance on different subsets of inputs, and ensembling them can improve overall accuracy.
- Mechanism: Different computational mechanisms are used by probes versus queries, making them complement each other on different types of inputs. When probes are uncertain, queries may be more accurate, and vice versa.
- Core assumption: The computational pathways for probe predictions and query predictions are sufficiently different that they make different types of errors.
- Evidence anchors:
  - [abstract] "In some cases, queries and probes perform better on different subsets of inputs, and accuracy can further be improved by ensembling the two."
  - [section 5] "Another observation in Fig. 1 is that almost all datasets exhibit a substantial portion of heterogeneity: there are large subsets of the data in which probes have low confidence, but queries assign high probability to the correct answer (and vice-versa)."
  - [corpus] Weak - corpus neighbors don't directly address heterogeneous performance between probes and queries
- Break condition: If future work demonstrates that probes and queries use essentially the same computational mechanisms, the heterogeneous performance explanation would be undermined.

## Foundational Learning

- Concept: Linear classification and logistic regression
  - Why needed here: The paper uses linear probes to classify whether (question, answer) pairs are correct or incorrect based on LM hidden states
  - Quick check question: Given a dataset of (feature vector, label) pairs, how would you train a linear classifier to predict the label from the features?

- Concept: Probability calibration
  - Why needed here: Understanding why probes are better calibrated than queries requires knowledge of how probability calibration works and why it matters for prediction reliability
  - Quick check question: If a model assigns probability 0.7 to an event, what should its empirical accuracy be if it is perfectly calibrated?

- Concept: Ensemble methods
  - Why needed here: The paper shows that combining probe and query predictions through ensembling can improve accuracy, which requires understanding how weighted combinations of predictions work
  - Quick check question: How would you determine the optimal weights for combining two models' predictions in an ensemble?

## Architecture Onboarding

- Component map: LM hidden states -> Linear probe -> Probe predictions; LM tokens -> Query probabilities -> Query predictions; Ensemble module combines probe and query predictions
- Critical path: For each (question, answer) pair: (1) extract hidden states from base LM, (2) run probe classifier on hidden states, (3) query base LM for answer probability, (4) optionally combine predictions via ensemble, (5) evaluate accuracy against ground truth
- Design tradeoffs: Using linear probes trades model complexity for interpretability and training efficiency. The choice of which hidden layer to use for probing affects both accuracy and computational cost. Ensemble weighting requires a validation set for hyperparameter tuning
- Failure signatures: If probe accuracy is much lower than query accuracy, the probe may not be learning useful features. If calibration plots show systematic deviations from the diagonal, the model is poorly calibrated. If ensemble weights converge to extremes (0 or 1), the models may not be complementary
- First 3 experiments:
  1. Train a linear probe on the hidden states of GPT-2 XL for BoolQ and evaluate its accuracy versus direct querying
  2. Plot calibration curves for both the probe and direct queries on the same dataset to visualize calibration differences
  3. Implement the ensemble method with varying weights and find the optimal Î» value on a validation set, then evaluate the ensemble's accuracy

## Open Questions the Paper Calls Out
- What is the optimal ensembling strategy for combining query and probe predictions to maximize accuracy on factual reasoning tasks?
- How do different prompt templates and formatting strategies affect the distribution of query-probe disagreements?
- What non-linear features in language model representations are critical for factual reasoning that linear probes cannot capture?

## Limitations
- The study focuses on only two large language models and three question-answering datasets, which may limit generalizability
- The distinction between "confabulation," "heterogeneity," and "deception" relies on specific operational definitions that may not capture all relevant phenomena
- While the paper demonstrates that ensembles can improve accuracy, the optimal weighting scheme and its robustness across different datasets remain open questions

## Confidence
**High Confidence:** The finding that probes are better calibrated than direct queries on uncertain answers is well-supported by the empirical results, particularly the calibration curves showing systematic deviations for queries.

**Medium Confidence:** The interpretation that mismatches stem primarily from different prediction pathways rather than intentional deception is plausible but somewhat interpretive.

**Low Confidence:** The extent to which these findings generalize beyond the specific models and datasets studied remains uncertain.

## Next Checks
1. Test whether the same patterns of probe-query disagreement appear in smaller language models (e.g., GPT-2 small) or different architectures (e.g., OPT, LLaMA) to assess generalizability.

2. Evaluate the calibration and ensemble benefits on non-QA tasks like commonsense reasoning or summarization to determine if the findings extend beyond the studied domains.

3. Design controlled experiments where probes access different hidden layers or use different featurization methods to determine which aspects of the probe-query difference are essential versus incidental.