---
ver: rpa2
title: Coarse-to-Fine Dual Encoders are Better Frame Identification Learners
arxiv_id: '2310.13316'
source_url: https://arxiv.org/abs/2310.13316
tags:
- frame
- frames
- learning
- target
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COFFTEA, a dual-encoder architecture for
  frame identification that learns to align targets with semantic frames using a coarse-to-fine
  contrastive learning strategy. By employing separate target and frame encoders and
  leveraging in-batch and in-candidate contrastive objectives, COFFTEA efficiently
  models frame definitions and improves performance, especially in the challenging
  w/o lexicon filtering setting.
---

# Coarse-to-Fine Dual Encoders are Better Frame Identification Learners

## Quick Facts
- arXiv ID: 2310.13316
- Source URL: https://arxiv.org/abs/2310.13316
- Authors: 
- Reference count: 10
- Key outcome: COFFTEA improves frame identification by up to 1.53 points in R@1 without lexicon filtering and 0.93 overall, outperforming previous methods.

## Executive Summary
This paper introduces COFFTEA, a dual-encoder architecture for frame identification that learns to align targets with semantic frames using a coarse-to-fine contrastive learning strategy. By employing separate target and frame encoders and leveraging in-batch and in-candidate contrastive objectives, COFFTEA efficiently models frame definitions and improves performance, especially in the challenging w/o lexicon filtering setting. Experiments show that COFFTEA outperforms previous methods by up to 1.53 points in R@1 without lexicon filtering and 0.93 overall, demonstrating its effectiveness in both scenarios.

## Method Summary
COFFTEA uses a dual-encoder architecture where target sentences and frame definitions are encoded separately using PLM-based encoders. The model employs a coarse-to-fine contrastive learning approach: first using in-batch learning to distinguish frames with different semantics, then in-candidate learning to refine discrimination between semantically similar frames. Frame definitions are learned rather than frozen, and sibling frames are used as hard negatives during training. The model computes cosine similarity between target and frame representations to identify the most appropriate semantic frame.

## Key Results
- COFFTEA achieves 1.53 point improvement in R@1 without lexicon filtering
- Overall score improves by 0.93 points compared to previous methods
- Outperforms LB and LC models in both w/ lf and w/o lf settings
- Effectively models frame definitions through learnable frame encoder

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual encoder architecture with learnable frame encoder improves frame definition modeling
- Mechanism: Separating target and frame encoders allows independent representation learning, while contrastive objectives train the frame encoder to capture fine-grained semantic differences
- Core assumption: Frame definitions contain sufficient information to distinguish between similar frames when properly encoded
- Evidence anchors:
  - [abstract]: "With contrastive learning and dual encoders, COFFTEA efficiently and effectively models the alignment between frames and targets"
  - [section 3.1]: "To achieve better representation learning of definitions, the frame encoder of COFFTEA is learnable during training"
  - [corpus]: Weak evidence - no direct citations of dual encoder improvements in related work
- Break condition: If frame definitions are too ambiguous or lack discriminative features, even a learnable encoder cannot effectively separate similar frames

### Mechanism 2
- Claim: Coarse-to-fine contrastive learning stages address the trade-off between w/ lf and w/o lf performance
- Mechanism: In-batch learning (coarse stage) teaches model to distinguish frames with different semantics using batch negatives, then in-candidate learning (fine stage) refines ability to differentiate semantically similar frames using hard negatives
- Core assumption: Frames can be meaningfully clustered by semantic similarity, allowing staged learning from coarse to fine distinctions
- Evidence anchors:
  - [section 3.2]: "To address the trade-off between training w/ lf and w/o lf, we propose a two-stage learning procedure, referred to as coarse-to-fine learning"
  - [section 4.4]: "LB only achieves the highest R@1 (84.88) and LC only yields the highest Acc (92.70)"
  - [corpus]: Weak evidence - no direct citations of coarse-to-fine contrastive approaches in frame identification
- Break condition: If frame semantic relationships are too complex for staged learning, the model may fail to properly transfer knowledge between stages

### Mechanism 3
- Claim: Padding candidate frames and using sibling frames as hard negatives improves learning on frames with few candidates
- Mechanism: When targets have few candidates, padding with random or sibling frames ensures consistent negative sampling, with siblings providing semantically similar but incorrect examples
- Core assumption: Sibling frames (frames sharing inheritance relationships) are semantically close enough to serve as effective hard negatives
- Evidence anchors:
  - [section 3.2.2]: "We pad the number of C for all targets to a fixed number (e.g., 15). This padding ensures that each target has a consistent number of candidate frames"
  - [section A.3]: "We follow the order of candidate frames, sibling frames, and random frames to construct C"
  - [corpus]: Weak evidence - padding and sibling-based negatives are novel approaches without direct corpus support
- Break condition: If sibling frames are too dissimilar or too similar to be effective hard negatives, padding strategy may degrade learning quality

## Foundational Learning

- Concept: Contrastive learning with in-batch and in-candidate negatives
  - Why needed here: Enables efficient training on large frame sets by sampling informative negative examples rather than computing all pairwise distances
  - Quick check question: How does using gold frames from other instances in the batch serve as hard negative examples?

- Concept: Dual encoder architecture for separate representation learning
  - Why needed here: Allows independent encoding of targets and frames, enabling efficient similarity computation at inference time
  - Quick check question: Why is it important that the frame encoder is learnable rather than frozen?

- Concept: Coarse-to-fine curriculum learning
  - Why needed here: Addresses the trade-off between broad frame discrimination (w/o lf) and fine-grained candidate selection (w/ lf) through staged learning
  - Quick check question: What would happen if we reversed the order of in-batch and in-candidate learning stages?

## Architecture Onboarding

- Component map: Target encoder -> Frame encoder -> Coarse stage (in-batch learning) -> Fine stage (in-candidate learning) -> Cosine similarity computation -> Frame selection
- Critical path:
  1. Encode target sentence to get target representation
  2. Retrieve candidate frames for target
  3. Encode frame definitions to get frame representations
  4. Compute cosine similarities between target and frames
  5. Select frame with highest similarity score
- Design tradeoffs:
  - Learnable vs frozen frame encoder: Learnable encoder captures better frame semantics but increases training complexity
  - Dual vs single encoder: Dual encoder enables efficient inference but requires more memory for storing frame representations
  - Coarse-to-fine vs single-stage: Coarse-to-fine addresses w/ lf vs w/o lf trade-off but adds training complexity
- Failure signatures:
  - Poor w/ lf performance: Indicates inadequate fine-grained discrimination in in-candidate stage
  - Poor w/o lf performance: Indicates inadequate broad discrimination in in-batch stage
  - High variance across runs: Suggests sensitivity to initialization or batch composition
- First 3 experiments:
  1. Compare dual encoder with frozen frame encoder baseline to verify learnable encoder contribution
  2. Test coarse-to-fine learning by running stages separately to identify where performance gains come from
  3. Validate sibling frame effectiveness by comparing with random frame padding in in-candidate stage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would COFFTEA's performance change if different distance metrics, such as Euclidean or Manhattan distance, were used instead of cosine similarity?
- Basis in paper: [inferred] The paper mentions that cosine similarity is used as the distance metric, but it does not explore other distance metrics or their potential impact on performance.
- Why unresolved: The paper focuses on demonstrating the effectiveness of the dual-encoder architecture and two-stage learning process using cosine similarity. It does not provide evidence or comparisons with other distance metrics, leaving open the question of whether alternative metrics could further improve performance.
- What evidence would resolve it: Conducting experiments with COFFTEA using different distance metrics and comparing the results to the current performance using cosine similarity would provide evidence on whether alternative metrics could enhance the model's effectiveness.

### Open Question 2
- Question: How would the inclusion of additional structured information from FrameNet, such as frame relations beyond inheritance, impact COFFTEA's performance in frame identification?
- Basis in paper: [inferred] The paper mentions that COFFTEA uses inheritance relations from FrameNet to construct hard negative examples during training. However, it does not explore the potential impact of incorporating other structured information, such as frame relations beyond inheritance, on the model's performance.
- Why unresolved: The paper demonstrates the effectiveness of COFFTEA using inheritance relations, but it does not provide evidence or analysis on the potential benefits of incorporating other structured information from FrameNet. This leaves open the question of whether leveraging additional frame relations could further improve the model's performance.
- What evidence would resolve it: Conducting experiments with COFFTEA that incorporate various structured information from FrameNet, beyond inheritance relations, and comparing the results to the current performance would provide evidence on the potential benefits of leveraging additional frame relations.

### Open Question 3
- Question: How would COFFTEA's performance change if it were applied to frame semantic role labeling tasks, beyond frame identification?
- Basis in paper: [inferred] The paper focuses on frame identification and does not explore the application of COFFTEA to frame semantic role labeling tasks. It mentions that COFFTEA can effectively model the alignment between frames and targets, but it does not provide evidence or analysis on its potential effectiveness in frame semantic role labeling.
- Why unresolved: The paper demonstrates the effectiveness of COFFTEA in frame identification, but it does not provide evidence or analysis on its potential effectiveness in frame semantic role labeling tasks. This leaves open the question of whether COFFTEA could be successfully applied to frame semantic role labeling beyond frame identification.
- What evidence would resolve it: Conducting experiments with COFFTEA on frame semantic role labeling tasks and comparing the results to the current performance in frame identification would provide evidence on its potential effectiveness in frame semantic role labeling beyond frame identification.

## Limitations
- Underspecified coarse-to-fine learning schedule details, particularly epoch distribution and warm-up strategies
- Reliance on FrameNet inheritance relations for sibling frames may not generalize well to other semantic resources
- Limited analysis of failure cases or specific frame types where the method struggles

## Confidence

**High confidence (3 claims):**
- Dual encoder architecture with learnable frame encoder improves frame definition modeling compared to frozen encoders
- Separate target and frame encoders enable efficient inference through precomputed frame representations
- Contrastive learning with in-batch and in-candidate negatives is effective for frame identification

**Medium confidence (2 claims):**
- Coarse-to-fine learning stages specifically address the w/ lf vs. w/o lf trade-off
- Padding with sibling frames provides better hard negatives than random frames alone
- The 1.53 point improvement in R@1 w/o lf is primarily due to the coarse-to-fine approach

**Low confidence (1 claim):**
- The proposed method generalizes well to other semantic parsing tasks beyond frame identification

## Next Checks

1. **Ablation of learning stages**: Run COFFTEA with only in-batch learning and only in-candidate learning separately to quantify the specific contribution of each stage to the overall performance gains. This would clarify whether the coarse-to-fine approach is truly synergistic or if one stage dominates.

2. **Negative sampling analysis**: Systematically replace sibling frames with random frames and other potential hard negatives (e.g., frames from the same frame group) to determine if sibling frames are indeed the optimal choice for in-candidate contrastive learning.

3. **Generalization test**: Apply the COFFTEA architecture to a related semantic parsing task (such as semantic role labeling) with similar contrastive learning objectives to evaluate whether the dual encoder + coarse-to-fine approach transfers effectively beyond frame identification.