---
ver: rpa2
title: 'LExCI: A Framework for Reinforcement Learning with Embedded Systems'
arxiv_id: '2312.02739'
source_url: https://arxiv.org/abs/2312.02739
tags:
- lexci
- training
- https
- agent
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LExCI, a framework for training reinforcement
  learning (RL) agents on embedded systems. The framework addresses the challenge
  of training RL agents on resource-constrained embedded devices, which often have
  limited computational power and memory.
---

# LExCI: A Framework for Reinforcement Learning with Embedded Systems

## Quick Facts
- arXiv ID: 2312.02739
- Source URL: https://arxiv.org/abs/2312.02739
- Reference count: 40
- Key outcome: Framework enabling RL training on embedded systems with comparable performance to conventional computers

## Executive Summary
This paper presents LExCI, a framework for training reinforcement learning agents on resource-constrained embedded devices. The framework leverages Ray/RLlib for training while using TF Lite models for execution on embedded systems, addressing the challenge of limited computational power and memory. LExCI supports both on-policy and off-policy RL algorithms and includes helper classes for automating control software. Experiments on the inverted pendulum swing-up problem demonstrate the framework's effectiveness across different target systems including a rapid control prototyping system.

## Method Summary
LExCI employs a master-minion architecture where the LExCI Master handles training via Ray/RLlib on conventional hardware, while LExCI Minions interface with embedded systems to generate experiences. The framework converts policy models to TF Lite format for embedded execution, supporting both on-policy algorithms like PPO and off-policy algorithms like DDPG. A key component is the RL Block, a Simulink subsystem that provides plug-and-play integration for RL agents with built-in observation normalization, action sampling, and experience buffering. The framework enables training across Python, Simulink, and embedded target systems like the dSPACE MicroAutoBox III.

## Key Results
- Successfully trained RL agents on embedded systems achieving performance comparable to conventional computer training
- Demonstrated support for both on-policy (PPO) and off-policy (DDPG) RL algorithms
- Validated framework across multiple target systems including Python, Simulink, and MABX III rapid control prototyping system

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LExCI enables training RL agents on embedded systems by separating model execution from training computation.
- Mechanism: The framework uses a master-minion architecture where the LExCI Master handles training via Ray/RLlib on a conventional machine, while LExCI Minions interface with embedded systems to generate experiences. The policy model is converted to TF Lite format for execution on embedded devices.
- Core assumption: TF Lite models can be executed on embedded systems with sufficient performance for real-time control.
- Evidence anchors:
  - [abstract] "LExCI leverages the RLlib library and provides a modular architecture that allows for training RL agents on embedded systems using a combination of Python and C++."
  - [section 3.1] "The LExCI Master makes use of a slightly modified version of Ray/RLlib 1.13.0 via the library's Python application programming interface (API)."
- Break condition: If embedded systems cannot support TF Lite model execution, or if communication latency between master and minion exceeds acceptable thresholds for real-time control.

### Mechanism 2
- Claim: LExCI supports both on-policy and off-policy RL algorithms through its architecture.
- Mechanism: The framework implements PPO (on-policy) and DDPG (off-policy) algorithms with appropriate experience handling. For on-policy algorithms, experiences are collected and used immediately, while for off-policy algorithms, experiences are stored in a replay buffer for later training.
- Core assumption: The master-minion architecture can handle the different data flow requirements of on-policy and off-policy algorithms.
- Evidence anchors:
  - [abstract] "The framework supports both on-policy and off-policy RL algorithms"
  - [section 3.1] "Section VI When learning with off-policy algorithms, the LExCI Master does not remain idle while the Minions are doing their part. Instead, the Master continues training with experiences drawn from its replay memory buffer."
- Break condition: If the replay buffer becomes a bottleneck for off-policy training, or if on-policy algorithms require more frequent model updates than the framework can provide.

### Mechanism 3
- Claim: LExCI's RL Block provides a plug-and-play solution for integrating RL agents into Simulink models.
- Mechanism: The RL Block is a Simulink subsystem that handles observation normalization, action sampling, and denormalization, while also managing the experience buffer and policy model execution using TFLM.
- Core assumption: The RL Block can accurately represent the environment's state space and action space boundaries for proper normalization.
- Evidence anchors:
  - [section 3.2] "The RL Block (Fig. 3) plays a prominent role in that regard. It is a ready-to-use Simulink subsystem that houses the RL-based controller such that employing it becomes as simple as copying it into the plant model, connecting its ports, and setting some basic parameters."
- Break condition: If the normalization/denormalization parameters are incorrectly configured, leading to poor agent performance or unstable training.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: RL algorithms in LExCI are based on MDP theory for decision-making in discrete time steps.
  - Quick check question: What are the four components of an MDP tuple (S, A, P, R)?

- Concept: Policy Gradient Methods
  - Why needed here: PPO, one of the supported algorithms, is a policy gradient method that directly optimizes the policy parameters.
  - Quick check question: How does PPO's clipped surrogate objective prevent large policy updates that could destabilize training?

- Concept: Neural Network Architectures
  - Why needed here: LExCI supports various NN architectures including fully-connected, recurrent, and convolutional networks for different RL tasks.
  - Quick check question: What are the advantages of using recurrent neural networks for RL agents in partially observable environments?

## Architecture Onboarding

- Component map:
  - LExCI Master: Ray/RLlib interface, TCP/IP server, model conversion to TF Lite
  - LExCI Minion: Embedded system interface, experience collection and post-processing
  - RL Block: Simulink subsystem for policy execution and experience buffering
  - TF Lite/TFLM: Model execution on embedded systems

- Critical path: Master sends TF Lite model → Minion loads model on embedded device → Embedded device generates experiences → Minion sends experiences to Master → Master trains RL agent → Repeat

- Design tradeoffs:
  - Separation of training and execution enables use of powerful hardware for training but introduces communication overhead
  - TF Lite conversion enables embedded execution but may limit model complexity
  - Master-minion architecture allows parallelization but requires network infrastructure

- Failure signatures:
  - Training stalls: Check TCP/IP connections and heartbeat monitoring
  - Poor agent performance: Verify normalization parameters and action clipping
  - Memory issues: Monitor TF Lite model size and embedded system resources

- First 3 experiments:
  1. Train PPO agent on inverted pendulum using Python-only setup (no embedded system)
  2. Deploy PPO agent to MABX III using RL Block, verify basic control functionality
  3. Compare PPO vs DDPG performance on MABX III for the same pendulum task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LExCI-trained RL agents compare to agents trained using native RLlib on embedded systems?
- Basis in paper: [inferred] The paper mentions that LExCI's performance is comparable to native RLlib, but does not provide a detailed comparison of agent performance on embedded systems.
- Why unresolved: The paper focuses on demonstrating LExCI's operability and versatility, but does not provide a comprehensive comparison of agent performance on embedded systems.
- What evidence would resolve it: Conducting experiments to compare the performance of LExCI-trained and natively trained RL agents on embedded systems, measuring metrics such as convergence speed, stability, and final performance.

### Open Question 2
- Question: What is the impact of the Minion-Master architecture on the scalability and parallelization of LExCI when training agents on multiple embedded devices?
- Basis in paper: [explicit] The paper mentions that LExCI's master-minion architecture enables easy parallelization, but does not provide details on its impact on scalability and parallelization.
- Why unresolved: The paper does not provide a detailed analysis of the scalability and parallelization capabilities of LExCI when training agents on multiple embedded devices.
- What evidence would resolve it: Conducting experiments to evaluate the scalability and parallelization of LExCI when training agents on multiple embedded devices, measuring metrics such as training time, resource utilization, and communication overhead.

### Open Question 3
- Question: How does the choice of RL algorithm (on-policy vs. off-policy) affect the performance and convergence of LExCI-trained agents on embedded systems?
- Basis in paper: [explicit] The paper demonstrates the use of both on-policy (PPO) and off-policy (DDPG) algorithms with LExCI, but does not provide a detailed comparison of their performance and convergence.
- Why unresolved: The paper does not provide a comprehensive analysis of the impact of the choice of RL algorithm on the performance and convergence of LExCI-trained agents on embedded systems.
- What evidence would resolve it: Conducting experiments to compare the performance and convergence of LExCI-trained agents using different RL algorithms (on-policy vs. off-policy) on embedded systems, measuring metrics such as training time, stability, and final performance.

## Limitations

- Limited evaluation scope: Only tested on one relatively simple control task (inverted pendulum swing-up)
- No timing analysis: Communication latency and real-time control feasibility were not quantified
- Model complexity constraints: TF Lite conversion may restrict the complexity of neural network architectures

## Confidence

High confidence in core architecture and master-minion communication design. Medium confidence in performance claims across diverse embedded platforms. Low confidence regarding scalability and real-world deployment scenarios.

## Next Checks

1. Test the framework with recurrent neural networks and convolutional architectures to evaluate architectural limitations
2. Conduct timing analysis to measure communication latency and determine real-time control feasibility
3. Evaluate performance on resource-constrained microcontrollers (e.g., ARM Cortex-M) to assess scalability limits