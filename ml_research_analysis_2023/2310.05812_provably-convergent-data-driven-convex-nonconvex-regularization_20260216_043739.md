---
ver: rpa2
title: Provably Convergent Data-Driven Convex-Nonconvex Regularization
arxiv_id: '2310.05812'
source_url: https://arxiv.org/abs/2310.05812
tags:
- convex
- regularization
- weakly
- inverse
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of learning regularizers for
  inverse problems using deep neural networks while maintaining provable convergence
  guarantees. The authors introduce the convex-nonconvex (CNC) framework, which combines
  a weakly convex function over the data space with a convex function over the model
  space to ensure well-posedness and convergent regularization.
---

# Provably Convergent Data-Driven Convex-Nonconvex Regularization

## Quick Facts
- arXiv ID: 2310.05812
- Source URL: https://arxiv.org/abs/2310.05812
- Reference count: 40
- Primary result: Introduces convex-nonconvex framework for learning regularizers with provable convergence guarantees, outperforming adversarial baselines in CT reconstruction

## Executive Summary
This paper addresses the challenge of learning regularizers for inverse problems using deep neural networks while maintaining provable convergence guarantees. The authors introduce the convex-nonconvex (CNC) framework, which combines a weakly convex function over the data space with a convex function over the model space to ensure well-posedness and convergent regularization. To implement this framework, they propose a novel input weakly convex neural network (IWCNN) construction that generalizes input convex neural networks (ICNNs) to handle weakly convex functions. The method is demonstrated through computed tomography (CT) experiments, showing superior performance compared to several baselines.

## Method Summary
The method learns a regularizer R_θ(x) as a sum of a convex component R_c_θ(x) and a weakly convex component R_wc_θ(x). The weakly convex component is implemented using an input weakly convex neural network (IWCNN), constructed by composing an input convex neural network (ICNN) with a smooth neural network with Lipschitz continuous gradients. The overall objective function J_α(x,y) = 1/2||Ax-y||² + αR_θ(x) is shown to be strongly convex when αρ ≤ 1 (where ρ is the weakly convex parameter) and weakly convex otherwise. The method is trained using adversarial regularization, where the regularizer learns to distinguish between ground truth images and reconstruction artifacts. The approach is validated on CT reconstruction tasks using the Mayo Clinic low-dose CT dataset.

## Key Results
- ACNCR achieves PSNR of 33.77 dB and SSIM of 0.943 on sparse-view CT reconstruction, outperforming ACR (32.86 dB, 0.929) and TV (31.88 dB, 0.919)
- On limited-angle CT, ACNCR achieves PSNR of 33.73 dB and SSIM of 0.932, significantly better than ACR (32.11 dB, 0.909)
- ACNCR successfully avoids overfitting issues that plague previous adversarial regularization methods
- The method demonstrates provable convergence guarantees under the convex-nonconvex framework

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The convex-nonconvex (CNC) framework ensures well-posedness and convergent regularization by structuring the regularizer as a weakly convex function over the data space plus a convex function over the model space.
- Mechanism: This structure guarantees convexity of the overall objective function when the weakly convex parameter is sufficiently small, enabling provable convergence properties.
- Core assumption: The forward operator A has norm 1 (normalized) and the weakly convex parameter ρ is small enough to maintain convexity.
- Evidence anchors:
  - [abstract] "we show how well-posedness and convergent regularization arises within the convex-nonconvex (CNC) framework"
  - [section] "Theorem 3.1... For αρ > 1: Jα(·, y) is −αµ + (αρ − 1)∥ A ∥2-weakly convex; For αρ ≤ 1: Jα(·, y) is αµ - strongly convex."
  - [corpus] Weak evidence - the related papers mention convergent regularization but don't directly discuss the CNC framework's specific mechanism.
- Break condition: If αρ > 1, the overall objective becomes weakly convex rather than strongly convex, potentially losing convergence guarantees.

### Mechanism 2
- Claim: The input weakly convex neural network (IWCNN) construction enables learning nonconvex regularizers while maintaining weak convexity.
- Mechanism: By composing an input convex neural network (ICNN) with a smooth neural network, the IWCNN inherits weak convexity properties from the chain rule applied to convex and Lipschitz functions.
- Core assumption: The smooth network has Lipschitz continuous gradients and the ICNN is properly constructed.
- Evidence anchors:
  - [section] "Definition 4.1 (IWCNN)... f IWCNN θ = gICNN θ1 ◦ gsm θ2"
  - [section] "Theorem 4.2... F is Lβ-weakly convex. Furthermore, by the chain rule, ∂F (x) = ∇c (x)⊤ ∂h (c (x))."
  - [corpus] No direct evidence in corpus - this appears to be a novel contribution not mentioned in related papers.
- Break condition: If the smooth network doesn't have Lipschitz continuous gradients, the weak convexity property may not hold.

### Mechanism 3
- Claim: The adversarial training approach learns regularizers that effectively distinguish between ground truth images and reconstruction artifacts.
- Mechanism: By minimizing the difference between the regularizer's output on ground truth versus artifact images, the network learns to penalize undesirable reconstructions.
- Core assumption: The pseudo-inverse reconstruction provides a meaningful mapping from measurement space to image space for training purposes.
- Evidence anchors:
  - [section] "Now, Rθ is meant to penalize artificial images and promote real images, so we want Rθ to be large on Pn and small on Pr"
  - [section] "we train both of the networks in a decoupled way by minimizing: EX∼Pr [Rc θ1(X)] − EX∼Pn [Rc θ1(X)] + ..."
  - [corpus] Weak evidence - related papers discuss adversarial regularization but don't specifically address the decoupled training approach used here.
- Break condition: If the pseudo-inverse reconstruction doesn't adequately capture reconstruction artifacts, the training signal may be insufficient.

## Foundational Learning

- Concept: Weak convexity
  - Why needed here: Understanding weak convexity is essential for grasping how the CNC framework maintains convergence guarantees while allowing nonconvex regularization.
  - Quick check question: Can you explain the difference between strong convexity, weak convexity, and non-convexity using the definition provided in the paper?

- Concept: Clarke subdifferential
  - Why needed here: The Clarke subdifferential is used to define stationary points for the nonconvex optimization problem.
  - Quick check question: How does the Clarke subdifferential generalize the concept of gradient for non-smooth functions?

- Concept: Adversarial training
  - Why needed here: Understanding adversarial training is crucial for implementing the learned regularizer that distinguishes between ground truth and artifact images.
  - Quick check question: What is the role of the Lipschitz constraint in the adversarial regularizer training objective?

## Architecture Onboarding

- Component map:
  A (CT projection) -> FBP (pseudo-inverse reconstruction) -> IWCNN + ICNN (regularizer) -> ACNCR (final regularizer) -> CT reconstruction

- Critical path:
  1. Generate synthetic CT data from ground truth images
  2. Compute pseudo-inverse reconstructions
  3. Train IWCNN and ICNN components separately
  4. Combine components into ACNCR
  5. Evaluate on test data using reconstruction metrics

- Design tradeoffs:
  - Weak convexity vs. strong convexity: Allows more expressive regularizers but requires careful parameter tuning
  - Smooth vs. non-smooth activations: Smooth activations guarantee weak convexity but may reduce performance
  - Decoupled training vs. joint training: Decoupled training simplifies implementation but may miss joint optimization benefits

- Failure signatures:
  - Numerical instability during training: May indicate insufficient regularization or inappropriate parameter scaling
  - Overfitting to training data: May require early stopping or stronger regularization
  - Poor reconstruction quality: May indicate inadequate expressiveness of the regularizer or insufficient training data

- First 3 experiments:
  1. Verify weak convexity property of IWCNN on synthetic data
  2. Test convergence of subgradient descent with different αρ ratios
  3. Compare ACNCR performance against ACR on simple denoising tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the IWCNN construction generalize effectively to other types of weakly convex regularizers beyond those used in CT reconstruction?
- Basis in paper: [explicit] The paper states that the IWCNN construction is meant to impose weak convexity on regularizers and demonstrates its use in CT experiments, but does not explore its effectiveness in other domains.
- Why unresolved: The paper focuses specifically on CT reconstruction and does not test the IWCNN construction in other inverse problems or application domains.
- What evidence would resolve it: Experiments applying IWCNN-based regularizers to other inverse problems such as MRI reconstruction, deblurring, or super-resolution would demonstrate its generalizability.

### Open Question 2
- Question: How does the choice of weakly convex activation functions in the IWCNN affect the quality of reconstructions compared to using smooth activation functions?
- Basis in paper: [explicit] The paper mentions that smooth activation functions would automatically create weakly convex networks but notes that smooth non-linearities have been shown to worsen performance in machine learning models.
- Why unresolved: The paper does not conduct experiments comparing different activation functions or quantify the impact on reconstruction quality.
- What evidence would resolve it: Systematic experiments comparing reconstruction quality using different activation functions (smooth vs non-smooth) in the IWCNN architecture would clarify the impact on performance.

### Open Question 3
- Question: What is the theoretical relationship between the degree of weak convexity (ρ parameter) and the convergence properties of the optimization algorithm?
- Basis in paper: [explicit] The paper discusses weak convexity in Theorem 3.1 and mentions that for αρ ≤ 1, Jα is strongly convex, but does not explore how varying ρ affects convergence rates.
- Why unresolved: While the paper establishes conditions for convergence, it does not analyze how the choice of ρ influences the speed or stability of convergence.
- What evidence would resolve it: Convergence analysis showing the relationship between ρ, step size selection, and convergence rates for different inverse problems would clarify this relationship.

## Limitations

- Limited exploration of activation function choices and their impact on reconstruction quality
- Focus on CT reconstruction without testing generalizability to other inverse problems
- Lack of theoretical analysis of the relationship between weak convexity parameters and convergence rates

## Confidence

- High confidence: Mathematical framework's theoretical foundations, particularly weak convexity properties and convergence guarantees
- Medium confidence: IWCNN construction and implementation details, as novel architectural contribution lacks direct verification
- Low confidence: Specific numerical implementation choices, including hyperparameter selection and architectural specifics

## Next Checks

1. **Weak convexity verification**: Implement a numerical test suite to verify the weak convexity property of the IWCNN construction across various parameter settings, checking the chain rule application and Lipschitz gradient conditions.

2. **Convergence behavior analysis**: Conduct controlled experiments varying the αρ ratio to empirically demonstrate the transition between strong convexity (αρ ≤ 1) and weak convexity (αρ > 1), measuring convergence rates and solution quality.

3. **Baseline comparison validation**: Reimplement at least two baseline methods (TV regularization and LPD) using identical CT simulation parameters and evaluation metrics to ensure fair comparison with the ACNCR method.