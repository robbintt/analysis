---
ver: rpa2
title: Brain tumor segmentation using synthetic MR images -- A comparison of GANs
  and diffusion models
arxiv_id: '2306.02986'
source_url: https://arxiv.org/abs/2306.02986
tags:
- stylegan
- images
- synthetic
- training
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Synthetic medical images can be used to train segmentation networks
  with Dice scores that are 80%-90% of the scores obtained when training with real
  images. However, diffusion models can memorize training images if the dataset is
  small, making them less suitable for data sharing.
---

# Brain tumor segmentation using synthetic MR images -- A comparison of GANs and diffusion models

## Quick Facts
- arXiv ID: 2306.02986
- Source URL: https://arxiv.org/abs/2306.02986
- Reference count: 40
- Synthetic images achieve 80%-90% of Dice scores compared to real images for brain tumor segmentation

## Executive Summary
This study compares the use of GANs and diffusion models for generating synthetic brain MRI images to train segmentation networks. The research demonstrates that synthetic images can effectively substitute for real images, achieving Dice scores that are 80-90% of those obtained when training with real data. However, diffusion models pose a risk of memorizing training data when datasets are small, making them less suitable for data sharing. The commonly used FID and IS metrics do not correlate well with segmentation performance, suggesting the need for better evaluation methods.

## Method Summary
The study uses BraTS 2020 and 2021 datasets (5-channel MR images: T1w, T1wGd, T2w, FLAIR, and tumor annotations) to train five generative models: Progressive GAN, StyleGAN 1-3, and a diffusion model. These models generate 100k synthetic 2D slices (256×256) with ≥15% tumor pixels. A U-Net segmentation network with instance normalization, deep supervision, and augmentation (rotation, scaling, noise, blur, gamma) is trained on these synthetic images. The models are evaluated using Dice similarity coefficient and Hausdorff distance on real test sets. Each model is trained 10 times to ensure robustness.

## Key Results
- Synthetic images achieve Dice scores that are 80%-90% of those obtained with real images
- Augmentation during training on synthetic images improves Dice scores by 15.9%-17.1%
- Diffusion models can memorize training data when datasets are small, making them less suitable for data sharing
- FID and IS metrics do not correlate well with segmentation performance on real images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic medical images can substitute for real images in training segmentation networks with only a modest drop in Dice scores.
- Mechanism: Generative models learn the joint distribution of multi-modal MR images and annotations, enabling them to produce realistic synthetic data that preserve class boundaries needed for segmentation.
- Core assumption: The synthetic images span a distribution sufficiently similar to the real data distribution so that a trained network generalizes to real test images.
- Evidence anchors:
  - [abstract] "segmentation networks trained on synthetic images reach Dice scores that are 80% - 90% of Dice scores when training with real images"
  - [section] "Our results show that training segmentation networks with synthetic images works well, with Dice scores that reach 91% - 100% compared to when training with real images"
  - [corpus] "Beware of diffusion models for synthesizing medical images -- A comparison with GANs in terms of memorizing brain MRI" (weak: not directly about performance parity)
- Break condition: If synthetic images overfit to the training set and fail to generalize, Dice scores drop sharply, especially for diffusion models.

### Mechanism 2
- Claim: Data augmentation during training on synthetic images improves Dice scores by helping the segmentation network overcome systematic differences between synthetic and real data.
- Mechanism: Augmentation introduces variability (rotation, scaling, intensity shifts) that bridges domain gaps, forcing the network to learn more robust features.
- Core assumption: Synthetic images are systematically different from real ones in ways that augmentation can compensate for.
- Evidence anchors:
  - [section] "augmentation makes a rather big difference when training segmentation networks with only synthetic images (relative Dice scores with augmentation are 15.9% - 17.1% higher compared to without augmentation)"
  - [abstract] "using synthetic images for training a segmentation network leads to performance metrics which are similar to training with real images"
  - [corpus] No direct evidence; augmentation studies in GAN-augmented medical imaging are weak.
- Break condition: If augmentation is too aggressive, it may distort anatomical structures, harming segmentation accuracy.

### Mechanism 3
- Claim: FID and IS metrics do not correlate with segmentation performance on real images, making them unreliable for evaluating synthetic medical images.
- Mechanism: FID/IS are trained on ImageNet and therefore measure realism from a natural image perspective, not the task-specific quality needed for segmentation.
- Core assumption: Metrics based on natural image datasets cannot capture medical image characteristics relevant for segmentation.
- Evidence anchors:
  - [abstract] "we demonstrate that common metrics for evaluating synthetic images, Fréchet inception distance (FID) and inception score (IS), do not correlate well with the obtained performance when using the synthetic images for training segmentation networks"
  - [section] "Table 2 shows the ranking of the five generative models, based on the different metrics FID, IS, Dice and Hausdorff distance. Clearly, the rankings according to the common FID and IS metrics do not correlate well with the ranking according to Dice and Hausdorff distance"
  - [corpus] No direct evidence; weak signal from the literature on FID/IS unreliability.
- Break condition: If new metrics are developed that incorporate task-specific performance, the current lack of correlation may change.

## Foundational Learning

- Concept: Generative Adversarial Networks (GANs)
  - Why needed here: GANs are one of the primary methods used to synthesize medical images in this study.
  - Quick check question: What are the two main components of a GAN and how do they interact during training?

- Concept: Diffusion Models
  - Why needed here: Diffusion models are compared against GANs for synthetic image generation and shown to have memorization issues.
  - Quick check question: How does the forward diffusion process differ from the reverse denoising process in a diffusion model?

- Concept: Segmentation Metrics (Dice Score, Hausdorff Distance)
  - Why needed here: These metrics are used to evaluate the quality of the tumor segmentation produced by the trained networks.
  - Quick check question: How does the Dice score differ from the Hausdorff distance in measuring segmentation accuracy?

## Architecture Onboarding

- Component map: Synthetic image generator (GAN or diffusion) -> Data augmentation module -> Segmentation network (U-Net with Swin Transformer variant) -> Evaluation pipeline (Dice/Hausdorff on real test set)
- Critical path: Train generative model -> Generate synthetic dataset -> Train segmentation network on synthetic data (with augmentation) -> Evaluate on held-out real images
- Design tradeoffs: GANs train faster and are less prone to memorization; diffusion models generate higher quality images but risk copying training data and require more compute
- Failure signatures: Low Dice scores on real images despite high FID/IS; segmentation boundaries misaligned; model memorization detected via similarity metrics between synthetic and training images
- First 3 experiments:
  1. Train a StyleGAN2 model on the BraTS 2020 dataset and generate 100k synthetic images; compute FID/IS and visualize sample outputs.
  2. Train a U-Net segmentation network using only the synthetic images (with augmentation) and evaluate Dice scores on the real test set.
  3. Compare Dice scores when training with synthetic images versus training with real images to quantify performance drop/gain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal dataset size for training diffusion models on medical images without causing memorization of the training data?
- Basis in paper: [explicit] The paper demonstrates that diffusion models can memorize training images if the dataset is too small, but it does not specify what constitutes "too small."
- Why unresolved: The paper does not provide a clear threshold or guidelines for determining the optimal dataset size for diffusion models to avoid memorization.
- What evidence would resolve it: Conducting experiments with varying dataset sizes and measuring the degree of memorization in the generated synthetic images would help establish the optimal dataset size.

### Open Question 2
- Question: How do different augmentation strategies during training of generative models affect the quality and diversity of synthetic medical images?
- Basis in paper: [explicit] The paper mentions that augmentation during training of segmentation networks improves performance, but it does not explore the impact of augmentation during training of generative models.
- Why unresolved: The paper focuses on augmentation during segmentation network training but does not investigate its effects on the generative models themselves.
- What evidence would resolve it: Training generative models with various augmentation techniques and evaluating the resulting synthetic images for quality and diversity would provide insights into the impact of augmentation strategies.

### Open Question 3
- Question: What are the most effective metrics for evaluating the quality and diversity of synthetic medical images, especially in terms of their suitability for training segmentation networks?
- Basis in paper: [explicit] The paper demonstrates that FID and IS metrics do not correlate well with the performance of segmentation networks trained on synthetic images, indicating the need for better evaluation metrics.
- Why unresolved: The paper identifies the limitations of FID and IS but does not propose or test alternative metrics that better capture the usefulness of synthetic images for training segmentation networks.
- What evidence would resolve it: Developing and validating new metrics specifically designed to assess the quality and diversity of synthetic medical images for training segmentation networks would address this gap.

## Limitations
- Diffusion models risk memorizing training data when datasets are small, limiting their utility for data sharing
- FID and IS metrics do not correlate with segmentation performance, indicating a need for better evaluation methods
- Study does not explore cross-institutional generalization or clinical workflow integration

## Confidence

**High Confidence:**
- The core finding that synthetic images achieve 80-90% of real-image Dice scores is well-supported by direct experimental results and multiple training runs.

**Medium Confidence:**
- The claim about diffusion model memorization risk is supported by similarity metrics but would benefit from larger-scale studies across multiple institutions.
- The assertion that FID/IS poorly correlate with segmentation performance is demonstrated within this dataset but may not generalize to all medical imaging tasks.

**Low Confidence:**
- The broader claim that synthetic data sharing is "viable" for clinical deployment lacks validation in real-world clinical workflows or regulatory frameworks.

## Next Checks

1. **Generalizability Test:** Train synthetic generators on BraTS data from one institution and test segmentation performance on data from a different institution to assess cross-site generalization.
2. **Scale Dependency Study:** Systematically vary training dataset size (e.g., 10%, 25%, 50%, 100%) to quantify how diffusion model memorization risk changes with data volume.
3. **Clinical Workflow Integration:** Conduct a radiologist study comparing segmentation models trained on synthetic versus real data in a simulated clinical reading environment to assess practical utility.