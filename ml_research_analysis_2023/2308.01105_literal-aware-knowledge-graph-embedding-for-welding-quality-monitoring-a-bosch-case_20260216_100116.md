---
ver: rpa2
title: 'Literal-Aware Knowledge Graph Embedding for Welding Quality Monitoring: A
  Bosch Case'
arxiv_id: '2308.01105'
source_url: https://arxiv.org/abs/2308.01105
tags:
- welding
- data
- prediction
- entities
- diameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper applies knowledge graph embedding (KGE) methods to a
  welding quality monitoring problem in automotive manufacturing, aiming to predict
  welding spot diameter and associate spots with car body parts. The authors reformulate
  regression and classification tasks as link prediction problems, construct a knowledge
  graph from tabular welding data, and handle numeric literals by discretization.
---

# Literal-Aware Knowledge Graph Embedding for Welding Quality Monitoring: A Bosch Case

## Quick Facts
- arXiv ID: 2308.01105
- Source URL: https://arxiv.org/abs/2308.01105
- Reference count: 32
- Key outcome: KGE methods applied to welding quality monitoring achieve moderate accuracy, with TransE recommended for classification and MLP for regression tasks

## Executive Summary
This paper applies knowledge graph embedding (KGE) methods to welding quality monitoring in automotive manufacturing, reformulating regression and classification tasks as link prediction problems. The authors construct a knowledge graph from tabular welding data, handle numeric literals through discretization, and compare mainstream KGE models (TransE, RotatE, AttH) with a classic MLP baseline. Results show TransE achieves best accuracy for car body classification (Hits@1 = 0.64) while MLP performs best for diameter prediction with normalized RMSE of 0.05. The study demonstrates the potential of KGE methods for industrial manufacturing applications, though with moderate accuracy that may require further refinement for full-scale deployment.

## Method Summary
The authors reformulate welding quality monitoring tasks as link prediction problems in knowledge graphs. They preprocess Bosch welding data by selecting representative features, discretizing numeric literals (sensor measurements and diameters), and constructing a KG with entities for welding spots, machines, programs, and discretized literal ranges. The KG is populated with triples connecting these entities through relations like "has_sensor_value" and "produced_by". They train KGE models (TransE, RotatE, AttH) using negative sampling and compare performance against an MLP baseline. For evaluation, they introduce relaxed metrics including normalized RMSE for regression and Hits@GroupBy3 for classification to better reflect industrial requirements.

## Key Results
- TransE achieves best accuracy for car body classification with Hits@1 = 0.64 and Hits@GroupBy3 = 0.85
- MLP baseline performs best for diameter prediction with normalized RMSE of 0.05
- Literal embedding slightly improves MLP performance on classification but degrades TransE performance on regression
- Overall performance is moderate but shows promise for industrial manufacturing applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reformulating regression/classification as link prediction enables the use of KGE methods on structured tabular welding data.
- Mechanism: By discretizing continuous diameter values into class intervals and treating them as entities, and converting carbody parts into separate entities, the tasks become predicting missing triples (h,r,?) in the knowledge graph.
- Core assumption: Link prediction in KGs can approximate both regression (via discretization) and multi-class classification effectively.
- Evidence anchors:
  - [abstract] "We formulate the problem as link prediction, and experimented popular KGE methods on real industry data..."
  - [section] "we reformulate regression and classification problems as link prediction problems"
  - [corpus] No corpus paper explicitly validates this reformulation for industrial regression/classification; it is assumed from KGE theory.
- Break condition: If discretization loses too much granularity or the number of classes becomes too large, link prediction performance degrades.

### Mechanism 2
- Claim: Literal embedding by discretization allows sensor measurements to be incorporated into the KG and used by KGE models.
- Mechanism: Aggregating sensor data (mean per stage, overall mean), discretizing into value ranges, creating entities for each range, and linking these literal entities to the main KG.
- Core assumption: Discretized numeric ranges preserve enough information for predictive quality monitoring.
- Evidence anchors:
  - [section] "we adapted literal-embedding approaches from previous works to convert the literals into entities."
  - [corpus] No direct corpus evidence for this discretization approach; relies on prior KGE-literals research.
- Break condition: If discretization intervals are too coarse, important distinctions in sensor data are lost.

### Mechanism 3
- Claim: Using relaxed metrics (nrmse, Hits@GroupBy3) improves perceived adoptability of KGE models in industrial settings.
- Mechanism: Instead of strict Hits@1, use normalized RMSE to capture prediction error magnitude and group-based accuracy to relax classification requirements.
- Core assumption: Industrial users accept approximate correctness if error bounds are acceptable.
- Evidence anchors:
  - [section] "we introduce the performance metric rmse and Hits@GroupBy3"
  - [section] "we can relax the evaluation metric by resorting tonrmse"
  - [corpus] No corpus paper discusses this exact industrial metric relaxation; it is inferred from use case needs.
- Break condition: If error bounds are too large, industrial adoption is still rejected despite metric relaxation.

## Foundational Learning

- Concept: Knowledge Graph Embedding (KGE) basics (TransE, RotatE, AttH)
  - Why needed here: The paper relies on these KGE methods to perform link prediction on the welding KG.
  - Quick check question: What is the main difference between TransE and RotatE in how they model relations?

- Concept: Discretization and literal embedding in KGs
  - Why needed here: Sensor measurements and diameters are numeric literals that must be converted into KG entities for KGE methods.
  - Quick check question: How are numeric sensor values transformed into KG entities in this work?

- Concept: Link prediction formulation for regression/classification
  - Why needed here: The welding quality tasks are originally regression (diameter) and multi-class classification (carbody), which are reformulated as link prediction to use KGE.
  - Quick check question: Why is discretization of diameter values necessary for using KGE on the diameter prediction task?

## Architecture Onboarding

- Component map: Data ingestion -> discretization/aggregation -> KG construction (entities+relations+literals) -> KGE model training (TransE/RotatE/AttH) -> evaluation with relaxed metrics -> comparison with MLP baseline
- Critical path: KG construction and discretization are the most critical; errors here propagate to all downstream model performance
- Design tradeoffs: Discretization granularity vs. number of entities vs. model performance; using hyperbolic space (AttH) vs. Euclidean (TransE/RotatE) for relation modeling
- Failure signatures: Low Hits@1 despite high MRR indicates sparse connections for certain entities; poor nrmse indicates discretization loss of predictive power
- First 3 experiments:
  1. Train TransE on the KG with discretized diameters and sensor literals; evaluate Hits@1 and nrmse
  2. Repeat experiment without literals to observe ablation impact
  3. Compare MLP baseline on same discretized features to validate KGE advantage for classification (carbody) task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of KGE models scale with larger welding datasets containing more entities and relations?
- Basis in paper: [explicit] The authors note that scalability and test time are acceptable but do not conduct experiments with larger datasets.
- Why unresolved: The evaluation only uses 2000 welding records. Industrial applications likely involve much larger datasets.
- What evidence would resolve it: Performance metrics (Hits@1, nrmse, training time) on datasets 10x-100x larger would show how KGE methods scale.

### Open Question 2
- Question: Would alternative literal handling strategies (beyond discretization) improve KGE performance for diameter prediction?
- Basis in paper: [explicit] The ablation study shows TransE performance degrades when including literals for Q1, suggesting the current literal embedding approach may be suboptimal.
- Why unresolved: The authors only test one literal embedding approach (discretization) and find mixed results.
- What evidence would resolve it: Comparing KGE performance using alternative literal embedding methods (e.g., direct normalization, binning with adaptive ranges, or learned embeddings) would reveal if better approaches exist.

### Open Question 3
- Question: Can incorporating domain-specific knowledge into the KG structure further improve KGE performance for manufacturing monitoring?
- Basis in paper: [inferred] The authors construct the KG from tabular data using domain knowledge for feature selection, but do not encode deeper domain constraints or hierarchies.
- Why unresolved: The KG construction follows standard practices without exploiting manufacturing-specific ontologies or constraints that could enhance embeddings.
- What evidence would resolve it: Performance comparison between standard KG construction and KG enriched with domain constraints (e.g., welding process rules, quality thresholds) would demonstrate the impact of domain knowledge incorporation.

## Limitations
- Discretization strategy for numeric literals is not precisely specified
- Only 2000 records used, limiting generalizability to larger manufacturing contexts
- No statistical significance testing reported for performance differences between methods

## Confidence
- **High Confidence**: KGE reformulation of regression/classification as link prediction (clearly specified in methodology)
- **Medium Confidence**: Performance superiority of TransE for classification tasks (shown but with limited statistical validation)
- **Low Confidence**: Industrial adoption recommendations (based on single dataset without broader manufacturing validation)

## Next Checks
1. Conduct statistical significance testing on performance differences between KGE models and MLP baseline
2. Test the discretization sensitivity by varying bin sizes and evaluating impact on prediction accuracy
3. Validate the approach on a different manufacturing dataset (e.g., from another automotive supplier) to assess generalizability beyond Bosch-specific welding data