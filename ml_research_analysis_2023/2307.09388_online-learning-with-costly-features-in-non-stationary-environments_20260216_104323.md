---
ver: rpa2
title: Online Learning with Costly Features in Non-stationary Environments
arxiv_id: '2307.09388'
source_url: https://arxiv.org/abs/2307.09388
tags:
- time
- state
- each
- features
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Non-Stationary Costly Contextual (NCC)
  bandit problem, where an agent must learn both the optimal actions and which costly
  features to observe in a non-stationary environment. The authors propose NCC-UCRL2,
  an algorithm that adapts UCRL2 for this setting using a sliding window to estimate
  non-stationary rewards and costs.
---

# Online Learning with Costly Features in Non-stationary Environments

## Quick Facts
- arXiv ID: 2307.09388
- Source URL: https://arxiv.org/abs/2307.09388
- Reference count: 40
- Primary result: Sublinear regret bounds of O(T^2/3 Υ_T^1/3) for non-stationary cases and O(√T) for stationary environments

## Executive Summary
This paper introduces the Non-Stationary Costly Contextual (NCC) bandit problem, where an agent must learn both optimal actions and which costly features to observe in a non-stationary environment. The authors propose NCC-UCRL2, an algorithm that adapts UCRL2 for this setting using a sliding window to estimate non-stationary rewards and costs. The method balances exploration and exploitation while minimizing observation costs, achieving 20% higher gain than PS-LinUCB in experiments on a nursery application ranking dataset.

## Method Summary
NCC-UCRL2 adapts UCRL2 for the NCC setting by using a sliding window approach to estimate non-stationary rewards and costs. The algorithm constructs optimistic estimates of rewards and probabilities while using pessimistic estimates for costs, solving an optimization problem to select observation sets and actions. It balances exploration and exploitation by maintaining confidence bounds for rewards, costs, and state probabilities, while using partial state vector probability estimation to improve efficiency.

## Key Results
- Sublinear regret bounds: O(T^2/3 Υ_T^1/3) for non-stationary cases where Υ_T is the number of change points
- O(√T) regret for stationary environments
- 20% higher gain than PS-LinUCB by adaptively selecting features while maintaining accuracy comparable to full observation methods
- Successful adaptation to changing user preferences while reducing observation costs in nursery application experiments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sliding window with confidence bounds enables sublinear regret in non-stationary environments.
- **Mechanism:** Uses a sliding window of size w to estimate non-stationary rewards and costs, updating confidence bounds with high probability guarantees to balance exploration and exploitation while adapting to abrupt changes.
- **Core assumption:** Rewards and costs are piece-wise stationary with bounded changes between change points.
- **Evidence anchors:** Theoretical regret bounds O(T^2/3 Υ_T^1/3) for non-stationary cases; use of sliding window to estimate mean values.
- **Break condition:** If change points are too frequent relative to window size, estimates become biased and regret degrades toward linear.

### Mechanism 2
- **Claim:** Optimistic gain estimation with high-probability confidence intervals enables efficient exploration.
- **Mechanism:** Constructs optimistic estimates of rewards and probabilities (adding confidence bonuses) while using pessimistic estimates for costs (subtracting confidence bonuses), solving an optimization problem to select observation sets and actions.
- **Core assumption:** Confidence intervals hold with high probability (controlled by δ parameter).
- **Evidence anchors:** High-probability confidence bounds used in optimization; performance improvement over benchmarks.
- **Break condition:** If confidence bounds are violated (δ too large), the algorithm may over-explore or under-explore, leading to suboptimal performance.

### Mechanism 3
- **Claim:** Partial state vector probability estimation using all observations while reward/cost estimation uses recent window enables efficient learning.
- **Mechanism:** Estimates partial state probabilities using all past observations but estimates mean rewards and costs using only the most recent w observations, balancing adaptation to changes with stable probability estimates.
- **Core assumption:** Partial state probabilities change slowly compared to rewards and costs.
- **Evidence anchors:** Performance improvement with this estimation strategy; assumption of slowly changing state probabilities.
- **Break condition:** If partial state probabilities also change frequently, this estimation strategy becomes suboptimal.

## Foundational Learning

- **Concept:** Multi-armed bandit problem with exploration-exploitation tradeoff
  - Why needed here: Forms the foundation for understanding sequential decision-making under uncertainty
  - Quick check question: What is the fundamental challenge in bandit problems that this algorithm addresses?

- **Concept:** Contextual bandit with feature selection
  - Why needed here: The problem involves selecting which features to observe before making decisions
  - Quick check question: How does feature selection in this setting differ from standard contextual bandits?

- **Concept:** Non-stationary stochastic processes and change point detection
  - Why needed here: The environment changes over time, requiring adaptation mechanisms
  - Quick check question: What is the difference between a stationary and non-stationary environment in bandit problems?

## Architecture Onboarding

- **Component map:** Sliding window estimator -> Confidence bound calculator -> Probability estimator -> Optimization solver -> Counter update mechanism
- **Critical path:** Observe partial state → Update counters → Compute estimates → Solve optimization → Select observation set and action → Execute and collect reward/cost
- **Design tradeoffs:**
  - Window size w: Larger w improves stationary performance but slows adaptation to changes
  - Confidence parameter δ: Smaller δ increases computational complexity but improves theoretical guarantees
  - Feature cost distribution: Higher costs incentivize more selective observation but may reduce accuracy
- **Failure signatures:**
  - Linear regret growth indicates failure to adapt to changes
  - High variance in selected observation sets suggests instability in probability estimates
  - Suboptimal action selection despite good observation selection indicates issues with reward estimation
- **First 3 experiments:**
  1. Test stationary environment with varying w to find optimal window size
  2. Introduce single change point and measure adaptation speed for different w values
  3. Vary feature cost distributions to evaluate observation selection strategy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the regret bound change when the number of features D increases exponentially with the time horizon T?
- Basis in paper: The paper proves a regret bound with dependence on D in O(D√log(TDw/δ)/w) terms
- Why unresolved: The theoretical analysis does not examine how the regret scales when D grows exponentially with T
- What evidence would resolve it: Empirical results showing regret growth rates for varying D/T ratios, or theoretical analysis extending the current bounds to the exponential case

### Open Question 2
- Question: How would the algorithm perform if the cost distributions were not piece-wise stationary but instead drifted continuously?
- Basis in paper: The paper assumes piece-wise stationary costs and uses a sliding window to estimate them, suggesting this might not generalize to continuous drift
- Why unresolved: The theoretical analysis relies on the piece-wise stationary assumption to bound regret, which would need modification for continuous drift
- What evidence would resolve it: Experiments comparing performance on continuous drift vs piece-wise stationary cost distributions, or theoretical analysis adapting the regret bounds

### Open Question 3
- Question: Can the algorithm be extended to handle continuous state spaces for features rather than finite discrete states?
- Basis in paper: The current formulation assumes features have finite state spaces (X_i), suggesting extension to continuous spaces is non-trivial
- Why unresolved: The current theoretical analysis relies on the finite state space assumption, particularly in the confidence bound calculations
- What evidence would resolve it: Implementation of the algorithm with continuous state spaces and empirical evaluation, or theoretical analysis adapting the confidence bounds to continuous distributions

## Limitations

- The algorithm assumes piece-wise stationary rewards and costs with bounded changes between change points, which may not hold in all real-world scenarios
- Performance degrades toward linear regret if change points are too frequent relative to the window size
- The confidence bounds are assumed to hold with high probability, but violations could lead to systematic exploration or exploitation errors

## Confidence

- **High confidence:** The sliding window mechanism for estimating non-stationary rewards and costs
- **Medium confidence:** The partial state vector probability estimation using all observations while reward/cost estimation uses recent window - this requires the assumption that partial state probabilities change slowly to hold in practice
- **Medium confidence:** The regret bounds for non-stationary environments, as they depend on the number of change points being known or estimable

## Next Checks

1. **Change point frequency sensitivity**: Systematically vary the frequency of change points relative to window size w and measure regret scaling to validate the O(T^2/3 Υ_T^1/3) bound.

2. **Confidence bound robustness**: Introduce controlled violations of confidence bounds and measure algorithm performance degradation to assess the impact of δ parameter choices.

3. **Partial state stability**: Track the variance of partial state probability estimates over time to verify the assumption that they change more slowly than rewards and costs.