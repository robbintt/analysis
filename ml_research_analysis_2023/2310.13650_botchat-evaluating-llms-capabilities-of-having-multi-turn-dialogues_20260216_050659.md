---
ver: rpa2
title: 'BotChat: Evaluating LLMs'' Capabilities of Having Multi-Turn Dialogues'
arxiv_id: '2310.13650'
source_url: https://arxiv.org/abs/2310.13650
tags:
- llms
- arxiv
- dialogues
- evaluation
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents BotChat, an LLM-based approach to evaluate
  the multi-turn dialogue capabilities of LLMs. The method generates dialogues based
  on human conversation seeds and uses LLM judges to assess the quality.
---

# BotChat: Evaluating LLMs' Capabilities of Having Multi-Turn Dialogues

## Quick Facts
- arXiv ID: 2310.13650
- Source URL: https://arxiv.org/abs/2310.13650
- Reference count: 15
- Key outcome: GPT-4 significantly outperforms other LLMs in generating human-like dialogues, while smaller open-source models struggle with longer conversations due to poor instruction-following, tendency to generate lengthy utterances, or limited general capability.

## Executive Summary
This paper presents BotChat, an LLM-based approach to evaluate the multi-turn dialogue capabilities of LLMs. The method generates dialogues based on human conversation seeds and uses LLM judges to assess the quality. Three evaluation protocols - UniEval, BotChat Arena, and GTEval - are proposed for cross-validation. Experiments on 14 LLMs show that GPT-4 significantly outperforms others in generating human-like dialogues, while smaller open-source models struggle with longer conversations due to poor instruction-following, tendency to generate lengthy utterances, or limited general capability. The work provides a valuable resource for evaluating and improving LLM dialogue systems.

## Method Summary
BotChat evaluates LLM dialogue capabilities through a three-stage process: (1) extracting ChatSEEDs from real human dialogues in the MuTual dataset, (2) generating multi-turn dialogues utterance-by-utterance using various LLMs, and (3) evaluating generated dialogues using three protocols with LLM judges (UniEval for unitary evaluation, BotChat Arena for pairwise comparison, and GTEval for ground-truth comparison). The approach eliminates manual human evaluation by using GPT-4 and other LLMs as judges, making the evaluation scalable and efficient.

## Key Results
- GPT-4 significantly outperforms other LLMs in generating human-like dialogues
- Smaller open-source models struggle with longer conversations due to poor instruction-following, tendency to generate lengthy utterances, or limited general capability
- Dialogue quality declines quickly as dialogue turns increase, particularly for open-source LLMs at small scale
- Three evaluation protocols (UniEval, BotChat Arena, GTEval) produce substantially identical conclusions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using LLM judges provides efficient and scalable evaluation compared to human evaluation
- Mechanism: The paper employs GPT-4 and other LLMs as judges to evaluate generated dialogues, eliminating the need for manual human evaluation which is labor-intensive
- Core assumption: LLMs can serve as reliable judges for evaluating dialogue quality, similar to human preferences
- Evidence anchors: "we adopt state-of-the-art LLMs (GPT-4, \etc) as the judge to evaluate the generated dialogues"; "we propose a more efficient paradigm, named BotChat, to evaluate the multi-turn chatting capability. BotChat presents a pure LLM-based solution for multi-turn conversational capability evaluation, with no manual labor required"

### Mechanism 2
- Claim: Utterance-by-utterance generation with ChatSEEDs creates more natural multi-turn dialogues
- Mechanism: Starting from real human conversation seeds and generating subsequent utterances one at a time allows the model to maintain conversational context and coherence
- Core assumption: Building dialogues incrementally from authentic human conversation starters produces more human-like dialogues than generating from scratch
- Evidence anchors: "we start from real-world human dialogues and keep the very first utterances as the ChatSEED. Then we prompt LLMs to generate a full multi-turn dialogue (tens of utterances) based on the ChatSEED, utterance by utterance"; "In each turn, we provide the system prompt as well as all previous utterances in the dialogue to the ChatBot, and the Chatbot generates the next utterance"

### Mechanism 3
- Claim: Multiple evaluation protocols (UniEval, BotChat Arena, GTEval) provide robust cross-validation
- Mechanism: Using three different evaluation methods that assess dialogues independently, comparatively, and against ground truth ensures consistent and reliable results
- Core assumption: Different evaluation protocols that arrive at similar conclusions strengthen confidence in the assessment
- Evidence anchors: "With different evaluation protocols, we come to substantially identical conclusions"; "We propose three evaluation protocols: 1. unitary evaluation (UniEval)... 2. pairwise evaluation (BotChat Arena)... 3. ground-truth evaluation ( GTEval)"

## Foundational Learning

- Concept: Dialogue generation as an open-ended natural language generation task
  - Why needed here: Understanding that dialogue evaluation differs from standard NLG tasks where reference-based metrics work
  - Quick check question: Why can't we use BLEU or ROUGE scores to evaluate generated dialogues?

- Concept: Instruction-following capability in LLMs
  - Why needed here: Different models show varying abilities to follow generation instructions (e.g., keeping utterances short, maintaining human-like tone)
  - Quick check question: How does instruction-following capability affect dialogue quality in multi-turn conversations?

- Concept: Context window limitations in language models
  - Why needed here: The paper mentions that history utterances may not fit into the context window for some models, requiring truncation
  - Quick check question: What happens to dialogue quality when context truncation removes important conversational history?

## Architecture Onboarding

- Component map: ChatSEED extraction from real dialogues (MuTual dataset) -> Dialogue generation pipeline (utterance-by-utterance generation) -> LLM judge evaluation (UniEval, BotChat Arena, GTEval) -> Statistical analysis and ELO rating calculation
- Critical path: ChatSEED → Dialogue Generation → LLM Evaluation → Results Analysis
- Design tradeoffs:
  - Using LLM judges trades off potential accuracy for scalability and efficiency
  - Utterance-by-utterance generation trades off computational efficiency for better context maintenance
  - Multiple evaluation protocols add complexity but improve reliability
- Failure signatures:
  - Inconsistent results across evaluation protocols
  - Rapid degradation in dialogue quality with increasing turns
  - Models generating excessively long utterances or AI assistant-like responses
  - Context loss due to context window limitations
- First 3 experiments:
  1. Test dialogue generation with different ChatSEEDs to verify robustness
  2. Run UniEval with different judge models to check consistency
  3. Generate dialogues with varying turn counts to establish degradation patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different evaluation protocols (UniEval, BotChat Arena, GTEval) compare in terms of their effectiveness and reliability for assessing multi-turn dialogue quality?
- Basis in paper: [explicit] The paper proposes three evaluation protocols - UniEval, BotChat Arena, and GTEval - and uses them for cross-validation to evaluate the multi-turn dialogue capabilities of LLMs.
- Why unresolved: The paper mentions that these protocols draw substantially identical conclusions, but it does not provide a detailed comparison of their effectiveness and reliability. A more in-depth analysis could help determine which protocol is the most suitable for evaluating multi-turn dialogue quality.
- What evidence would resolve it: Conducting experiments to compare the effectiveness and reliability of the three evaluation protocols, such as analyzing their sensitivity to different dialogue qualities and their agreement with human evaluations, would help determine their relative strengths and weaknesses.

### Open Question 2
- Question: How does the performance of LLMs in generating multi-turn dialogues vary across different domains or topics?
- Basis in paper: [inferred] The paper evaluates LLMs' capabilities of generating human-like dialogues based on real-world conversation seeds, but it does not specifically examine the performance across different domains or topics.
- Why unresolved: Understanding how LLMs perform in different domains or topics could provide insights into their strengths and limitations, as well as guide future research in improving their dialogue generation capabilities.
- What evidence would resolve it: Conducting experiments to evaluate the performance of LLMs in generating dialogues across various domains or topics, such as news, entertainment, or technical discussions, would provide valuable insights into their domain-specific capabilities.

### Open Question 3
- Question: How does the quality of generated dialogues by LLMs degrade as the dialogue length increases, and what factors contribute to this degradation?
- Basis in paper: [explicit] The paper mentions that the quality of generated dialogues declines quickly as the dialogue turns increase, particularly for open-source LLMs at small scale.
- Why unresolved: Understanding the factors that contribute to the degradation of dialogue quality and how it varies with dialogue length is crucial for improving the performance of LLMs in generating longer, coherent conversations.
- What evidence would resolve it: Analyzing the generated dialogues of different lengths and identifying the specific factors that lead to quality degradation, such as repetition, incoherence, or loss of context, would provide insights into the limitations of current LLMs and guide future research in addressing these issues.

## Limitations

- LLM Judge Reliability: The fundamental assumption that GPT-4 and other LLMs can reliably assess dialogue quality remains empirically validated only within this specific experimental setup
- Context Window Constraints: Some models cannot accommodate full dialogue history within their context windows, requiring truncation that directly impacts dialogue quality
- Seed Dialogue Representativeness: The evaluation depends on ChatSEEDs from the MuTual dataset, which may have particular characteristics or biases limiting generalizability

## Confidence

- High Confidence: GPT-4 significantly outperforms other models in generating human-like dialogues
- Medium Confidence: Smaller open-source models struggle with longer conversations due to specific failure modes
- Medium Confidence: The three evaluation protocols produce substantially identical conclusions
- Medium Confidence: Utterance-by-utterance generation with ChatSEEDs creates more natural dialogues

## Next Checks

1. Cross-judge Consistency Test: Run UniEval using multiple judge models (not just GPT-4) on the same dialogues and measure inter-annotator agreement to validate the reliability of LLM judges.

2. Context Window Impact Analysis: Systematically vary the amount of dialogue history provided during generation and measure the impact on both dialogue quality and model performance rankings.

3. Seed Diversity Validation: Test dialogue generation across multiple seed datasets beyond MuTual to verify that observed model performance differences generalize across different conversation types.