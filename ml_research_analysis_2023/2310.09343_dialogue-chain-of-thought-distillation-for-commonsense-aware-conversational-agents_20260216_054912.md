---
ver: rpa2
title: Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational
  Agents
arxiv_id: '2310.09343'
source_url: https://arxiv.org/abs/2310.09343
tags:
- dialogue
- person
- knowledge
- response
- rationales
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of commonsense reasoning in multi-turn
  dialogue, which is crucial for human-like chatbots but difficult for current models
  due to scattered implicit evidence across turns. The authors propose a novel framework
  for dialogue chain-of-thought (CoT) reasoning that distills rationales from large
  language models (LLMs) via alignment filters.
---

# Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents

## Quick Facts
- arXiv ID: 2310.09343
- Source URL: https://arxiv.org/abs/2310.09343
- Reference count: 35
- Key outcome: DOCTOR generates high-quality dialogue CoT rationales that improve response generation on both in-domain and out-of-domain datasets

## Executive Summary
This paper addresses the challenge of commonsense reasoning in multi-turn dialogue by proposing a novel framework for dialogue chain-of-thought (CoT) reasoning. The approach distills high-quality rationales from large language models (LLMs) using alignment filters that ensure contextual consistency and helpfulness for response generation. The DOCTOR model, trained on the automatically annotated DONUT dataset, generates reliable CoT rationales that significantly improve dialogue response quality compared to single-hop approaches and direct LLM prompting.

## Method Summary
The method involves a knowledge distillation framework that extracts CoT rationales from an unreliable LLM (ChatGPT) and selectively distills high-quality rationales via two alignment filters. The rationale-to-context alignment filter removes counterfactual rationales inconsistent with the dialogue context, while the rationale-to-response alignment filter eliminates unhelpful rationales. The filtered rationales are used to train DOCTOR, a DialOgue Chain-of-ThOught Reasoner, which then generates rationales to augment dialogue models for improved response generation. The approach is evaluated on both in-domain and out-of-domain datasets using automatic metrics and human evaluations.

## Key Results
- DOCTOR outperforms ChatGPT (175B) in both rationale quality and downstream response generation
- Augmentation with DOCTOR's rationales leads to better performance than single-hop approaches and self-generated CoT from LLMs
- Human evaluations confirm DOCTOR generates more natural, specific, and helpful responses
- The approach generalizes well to out-of-domain dialogue datasets

## Why This Works (Mechanism)

### Mechanism 1
The alignment filters improve rationale quality by removing inconsistent and unhelpful rationales. The rationale-to-context filter uses a critic model to detect counterfactual rationales, while the rationale-to-response filter measures helpfulness by comparing response generation probabilities. High-quality rationales must be both contextually consistent and helpful for response generation.

### Mechanism 2
The dialogue CoT format (sequence of QA pairs) enables better integration of implicit information compared to single-hop approaches. By generating multiple subquestions and answers, the model can decompose reasoning into multiple steps and aggregate implicit evidence scattered across dialogue turns.

### Mechanism 3
The knowledge distillation framework transfers reasoning capabilities from unreliable LLMs to a more reliable CoT reasoner. By leveraging LLMs as teachers and applying alignment filters to select high-quality rationales, the framework selectively transfers CoT capabilities while filtering out noise.

## Foundational Learning

- **Concept**: Chain-of-thought reasoning
  - Why needed here: Dialogue CoT reasoning decomposes multi-hop reasoning into a sequence of questions and answers, essential for capturing implicit information scattered across dialogue turns
  - Quick check question: How does dialogue CoT reasoning differ from simple knowledge retrieval approaches?

- **Concept**: Knowledge distillation
  - Why needed here: The framework uses LLMs as unreliable teachers and selectively distills their capabilities to train a more reliable CoT reasoner, addressing the limitations of directly using LLMs
  - Quick check question: What is the role of alignment filters in the knowledge distillation process?

- **Concept**: Rationale-to-context and rationale-to-response alignment
  - Why needed here: These alignment filters ensure that distilled rationales are both consistent with the dialogue context and helpful for response generation, which are critical quality criteria
  - Quick check question: How do the two alignment filters differ in what they measure?

## Architecture Onboarding

- **Component map**: LLM teacher (ChatGPT) → Rationale-to-context alignment filter (RoBERTa critic) → Rationale-to-response alignment filter (Cosmo model) → DOCTOR (OPT-1.3B) → Dialogue models (ChatGPT, Cosmo)

- **Critical path**: 
  1. LLM generates rationales for dialogue-context-response triples
  2. Rationale-to-context filter removes inconsistent rationales
  3. Rationale-to-response filter removes unhelpful rationales
  4. Filtered rationales train DOCTOR
  5. DOCTOR generates rationales for new dialogue contexts
  6. Dialogue models use these rationales to generate responses

- **Design tradeoffs**: 
  - Using LLMs as teachers provides scalability but introduces noise that requires filtering
  - The two alignment filters add computational overhead but improve rationale quality
  - Training DOCTOR on machine-generated data is cost-effective but may have quality issues
  - The QA format for rationales adds structure but may limit expressiveness

- **Failure signatures**: 
  - Poor rationale quality if filters are too permissive or too strict
  - Low response quality if DOCTOR fails to learn effective reasoning
  - Computational inefficiency if filtering process is too slow
  - Limited generalization if DOCTOR overfits to training data

- **First 3 experiments**:
  1. Evaluate rationale quality with and without each alignment filter using human judgment
  2. Compare response generation quality with DOCTOR rationales vs. direct LLM prompting
  3. Test DOCTOR's generalization to out-of-domain dialogue datasets

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of dialogue CoT rationales from DOCTOR compare to those generated by other LLM-based approaches, such as GPT-4 or Claude? The paper only compares DOCTOR to ChatGPT, so a comparison with other state-of-the-art LLM-based approaches would provide a more complete picture of DOCTOR's effectiveness.

### Open Question 2
How does the number of question-answer pairs in the dialogue CoT rationale affect the quality of the generated response? The paper uses a fixed number of 3 question-answer pairs, but it's unclear if this is optimal or if a different number would lead to better results.

### Open Question 3
How well does DOCTOR generalize to dialogues in languages other than English? The paper only evaluates DOCTOR on English dialogue datasets, so it's unclear if DOCTOR would perform similarly well on dialogues in other languages.

## Limitations

- The evaluation relies heavily on human judgment, introducing potential subjectivity
- The effectiveness of alignment filters depends on critic model quality and helpfulness threshold, which are not extensively explored
- Automatic annotation using ChatGPT may introduce biases not fully characterized
- The approach's performance on non-English dialogues remains untested

## Confidence

- **High confidence**: The core methodology of using alignment filters to distill high-quality rationales from LLMs is well-justified and supported by experimental results
- **Medium confidence**: The claim that multi-hop reasoning is necessary for capturing implicit information in dialogues is plausible but could benefit from more direct evidence
- **Low confidence**: The assertion that the proposed approach significantly outperforms single-hop methods across all datasets needs more rigorous statistical validation

## Next Checks

1. Conduct ablation studies to quantify the individual contributions of each alignment filter and the knowledge distillation process
2. Perform cross-validation with different LLM teachers (beyond ChatGPT) to assess the robustness of the distillation framework
3. Test the approach on a wider range of dialogue domains and languages to evaluate generalization beyond the current datasets