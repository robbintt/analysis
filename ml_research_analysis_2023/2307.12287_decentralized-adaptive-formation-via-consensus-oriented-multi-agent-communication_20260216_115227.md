---
ver: rpa2
title: Decentralized Adaptive Formation via Consensus-Oriented Multi-Agent Communication
arxiv_id: '2307.12287'
source_url: https://arxiv.org/abs/2307.12287
tags:
- formation
- agents
- consmac
- module
- multi-agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of decentralized adaptive formation
  control for multi-agent systems, particularly under communication constraints. The
  proposed solution is a novel framework called Consensus-based Decentralized Adaptive
  Formation (Cons-DecAF), which incorporates a Consensus-oriented Multi-Agent Communication
  (ConsMAC) method.
---

# Decentralized Adaptive Formation via Consensus-Oriented Multi-Agent Communication

## Quick Facts
- arXiv ID: 2307.12287
- Source URL: https://arxiv.org/abs/2307.12287
- Reference count: 15
- This paper tackles decentralized adaptive formation control for multi-agent systems under communication constraints using a novel Consensus-based Decentralized Adaptive Formation (Cons-DecAF) framework.

## Executive Summary
This paper addresses the challenge of decentralized adaptive formation control for multi-agent systems, particularly under communication constraints where agents have limited observations and the number of active agents can vary. The authors propose a novel framework called Consensus-based Decentralized Adaptive Formation (Cons-DecAF) that incorporates a Consensus-oriented Multi-Agent Communication (ConsMAC) method. ConsMAC enables agents to perceive global information and establish consensus from local states by effectively aggregating neighbor messages using an attention mechanism and a Global Estimator trained with supervised learning. The framework also employs policy distillation to adapt to varying numbers of agents, and uses a displacement-based formation approach with Hausdorff distance to improve formation efficiency.

## Method Summary
The proposed Cons-DecAF framework consists of two main components: ConsMAC for consensus-oriented communication and policy distillation for adaptive formation control. ConsMAC uses an attention mechanism to aggregate neighbor messages, a Global Estimator trained via supervised learning to reconstruct global state information, and a distance encoder to weight messages based on neighbor proximity. The framework trains separate policies for different agent counts (5-8) using MAPPO, then applies policy distillation to create a single adaptive policy that can handle varying numbers of agents. Formation efficiency is improved by using Hausdorff distance rather than pre-assigned positions, allowing agents to choose optimal target points within the formation shape.

## Key Results
- Cons-DecAF achieves outstanding performance in terms of both speed and stability compared to existing methods in multi-agent particle and quadcopter-physical-model environments
- The attention-based ConsMAC method effectively establishes consensus from local observations, enabling agents to perceive global information
- Policy distillation successfully enables adaptive formation adjustment across varying numbers of agents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ConsMAC allows agents to infer global information from local observations via an attention-based consensus establishment module
- Mechanism: The Global Estimator (FψE) processes aggregated neighbor messages (h(t)_i) to produce a state embedding (ê(t)_i) that approximates the true global observation, trained via supervised learning using KL divergence loss
- Core assumption: Global state can be reconstructed from local observations and aggregated neighbor messages using supervised learning
- Evidence anchors: Abstract states ConsMAC enables agents to perceive global information and establish consensus from local states; Section describes global estimator trained with supervised learning using true global observation as label

### Mechanism 2
- Claim: Policy distillation enables adaptive formation control across varying numbers of agents
- Mechanism: Separate policies are trained for each agent count, then integrated into a single student policy through policy distillation that minimizes MSE between its outputs and teachers' actions and attention layer outputs
- Core assumption: Policies for different agent counts share sufficient structural similarity for a single distilled policy to approximate all of them
- Evidence anchors: Abstract mentions leveraging policy distillation to accomplish adaptive formation adjustment; Section describes integrating multiple formation models using policy distillation technique

### Mechanism 3
- Claim: Using Hausdorff distance instead of pre-assigned positions improves formation efficiency and flexibility
- Mechanism: Formation reward is based on maximum distance any agent needs to move to reach target formation topology, allowing agents to choose optimal target points within formation shape
- Core assumption: Agents can effectively self-organize within formation shape without explicit position assignments
- Evidence anchors: Abstract states displacement-based formation using Hausdorff distance significantly improves formation efficiency; Section describes adopting Hausdorff-Distance oriented multi-policy-distilled ConsMAC

## Foundational Learning

- Concept: Multi-Agent Reinforcement Learning with Centralized Training and Decentralized Execution (CTDE)
  - Why needed here: Formation control requires agents to learn policies that can be executed independently while benefiting from centralized training to share information and coordinate effectively
  - Quick check question: What is the key advantage of CTDE over purely decentralized training for multi-agent coordination tasks?

- Concept: Attention mechanisms for message aggregation in multi-agent communication
  - Why needed here: Agents need to effectively aggregate information from neighbors while weighing messages based on distance and relevance to establish consensus
  - Quick check question: How does the distance encoder in ConsMAC help prioritize information from closer neighbors?

- Concept: Policy distillation for multi-task/multi-agent adaptation
  - Why needed here: System must handle varying numbers of active agents by combining multiple learned policies into a single adaptive policy
  - Quick check question: Why is it important to include both action outputs and attention layer outputs when distilling policies for adaptive formation?

## Architecture Onboarding

- Component map: Local observation → Distance Encoder → Memory Updater → Attention Layer → Global Estimator → Policy Execution → Action + Message
- Critical path: Local observation → Distance Encoder → Memory Updater → Attention Layer → Global Estimator → Policy Execution → Action + Message
- Design tradeoffs:
  - Supervised learning for consensus vs. end-to-end RL: Provides interpretability but requires global labels
  - Hausdorff distance vs. pre-assigned positions: More flexible but potentially less precise for specific formations
  - Policy distillation vs. separate policies: More compact but may sacrifice some performance
- Failure signatures:
  - Poor consensus establishment: High KL divergence loss, agents fail to coordinate
  - Inefficient formation: High Hausdorff distance reward, slow convergence
  - Distillation failure: Student policy performs worse than individual teacher policies
- First 3 experiments:
  1. Verify ConsMAC consensus establishment by comparing performance with and without Global Estimator and supervised loss
  2. Test formation efficiency by comparing Hausdorff distance approach against pre-assigned position baselines
  3. Validate adaptive formation by testing distilled policy across different agent counts and comparing against individual policies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ConsMAC and Cons-DecAF scale with the number of agents in the formation, especially in very large-scale systems?
- Basis in paper: [inferred] The paper mentions that extensive simulations were conducted in multi-agent particle and quadcopter-physical-model environments, but does not specify the maximum number of agents tested or the performance trends at larger scales
- Why unresolved: The paper does not provide experimental results or analysis for systems with a large number of agents, leaving the scalability of the proposed methods uncertain
- What evidence would resolve it: Conducting experiments with increasing numbers of agents and analyzing the performance metrics (e.g., formation efficiency, communication overhead) would provide insights into the scalability of ConsMAC and Cons-DecAF

### Open Question 2
- Question: How does the proposed ConsMAC method perform in environments with higher communication restrictions, such as lower bandwidth or higher latency?
- Basis in paper: [inferred] The paper mentions that the proposed method was tested under communication-limited constraints and demonstrated competitive performance, but does not explore the effects of varying communication quality parameters
- Why unresolved: The paper does not provide experimental results or analysis for different communication quality scenarios, leaving the robustness of ConsMAC under various communication restrictions uncertain
- What evidence would resolve it: Conducting experiments with varying communication quality parameters (e.g., bandwidth, latency) and analyzing the performance of ConsMAC would provide insights into its robustness under different communication restrictions

### Open Question 3
- Question: How does the proposed Cons-DecAF framework perform in more complex and dynamic environments, such as those with moving obstacles or changing formation topologies?
- Basis in paper: [inferred] The paper mentions that the proposed framework was tested in multi-agent particle and quadcopter-physical-model environments, but does not explore the effects of complex and dynamic environmental factors
- Why unresolved: The paper does not provide experimental results or analysis for environments with moving obstacles or changing formation topologies, leaving the adaptability of Cons-DecAF to complex and dynamic scenarios uncertain
- What evidence would resolve it: Conducting experiments in environments with moving obstacles or changing formation topologies and analyzing the performance of Cons-DecAF would provide insights into its adaptability to complex and dynamic scenarios

## Limitations
- The effectiveness of the Global Estimator depends heavily on the quality of neighbor message aggregation, which may degrade in highly dynamic environments with rapid agent movements or communication failures
- Policy distillation assumes sufficient structural similarity between policies for different agent counts, but this may not hold for significantly different formation complexities
- The Hausdorff distance metric provides flexibility but may result in suboptimal formations when precise geometric constraints are required

## Confidence

**High confidence:** The basic architecture of ConsMAC with attention-based message aggregation and supervised learning for consensus establishment

**Medium confidence:** The policy distillation approach for adaptive formation across agent counts, based on typical success rates in related multi-task learning applications

**Medium confidence:** The use of Hausdorff distance for formation efficiency, as this is well-established in computational geometry though its application here is novel

## Next Checks
1. Test the robustness of consensus establishment by introducing communication delays or packet losses and measuring the degradation in formation quality
2. Compare the Hausdorff distance approach against position-specific formation rewards in scenarios requiring precise geometric configurations
3. Evaluate the distillation performance gap by training individual policies for each agent count and measuring the performance difference against the distilled policy