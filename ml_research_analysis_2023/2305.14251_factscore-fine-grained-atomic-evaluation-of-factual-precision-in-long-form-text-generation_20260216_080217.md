---
ver: rpa2
title: 'FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form
  Text Generation'
arxiv_id: '2305.14251'
source_url: https://arxiv.org/abs/2305.14251
tags:
- chatgpt
- facts
- factscore
- atomic
- factual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce FACTSCORE, a new evaluation of factual precision in
  long-form text generation by breaking generations into atomic facts and validating
  each against a given knowledge source. Human evaluation of 505 biographies generated
  by InstructGPT, ChatGPT, and PerplexityAI reveals low factual precision (42-71%),
  which significantly drops for rarer entities and later parts of generations.
---

# FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation

## Quick Facts
- arXiv ID: 2305.14251
- Source URL: https://arxiv.org/abs/2305.14251
- Reference count: 40
- FactScore reveals low factual precision (42-71%) in LLM-generated biographies, dropping for rarer entities and later parts of generations

## Executive Summary
FActScore introduces a fine-grained evaluation framework for factual precision in long-form text generation by breaking outputs into atomic facts and validating each against a knowledge source. Human evaluation of 505 biographies reveals that even state-of-the-art models like ChatGPT and InstructGPT achieve only 42-71% factual precision. The precision drops significantly for rarer entities and later parts of generations. To enable large-scale evaluation, the authors develop an automated estimator using retrieval and strong language models that achieves <2% error rate and is applied to 6,500 generations from 13 recent LMs. Results show GPT-4 and ChatGPT are more factual than public models, with Vicuna and Alpaca being the best public models.

## Method Summary
FActScore evaluates factual precision by decomposing long-form generations into atomic facts (short sentences conveying single pieces of information) and validating each against a knowledge source like Wikipedia. The process involves collecting model generations, breaking them into atomic facts using InstructGPT or manual annotation, validating each fact against Wikipedia to compute FACTSCORE, and using an automated estimator with retrieval and language models for large-scale evaluation. The automated estimator combines retrieval-augmented language models with non-parametric masked language modeling to achieve high accuracy with <2% error rate compared to human evaluation.

## Key Results
- GPT-4 achieves highest factual precision (69.5%) among all evaluated models
- Factual precision drops significantly for rarer entities and later parts of generations
- Public models like Vicuna (48.9%) and Alpaca (52.1%) show lower precision than commercial models
- Retrieve→LM automated estimator achieves <2% error rate compared to human evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Breaking long-form generations into atomic facts enables fine-grained factual precision evaluation
- Mechanism: Long text is decomposed into short sentences each containing one piece of information, allowing binary validation of each fact against a knowledge source
- Core assumption: Each atomic fact can be validated independently without context from other facts
- Evidence anchors:
  - [abstract] "breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source"
  - [section 3.1] "We define an atomic fact as a short sentence conveying one piece of information"
- Break condition: When facts are context-dependent or require cross-reference between multiple atomic facts

### Mechanism 2
- Claim: Retrieval-augmented language models significantly improve factual precision in long-form generation
- Mechanism: Models access external knowledge sources during generation, allowing them to copy or paraphrase accurate information rather than relying on parametric memory
- Core assumption: The retrieved passages contain accurate information relevant to the generation task
- Evidence anchors:
  - [section 3.4] "PerplexityAI, which uses a commercial search engine... has a factual precision of 71.5%"
  - [section 3.5] "36.0% and 37.6% of supported and unsupported sentences have citations, respectively"
- Break condition: When retrieved passages are irrelevant, contradictory, or the model fails to properly synthesize information

### Mechanism 3
- Claim: Non-parametric masked language modeling provides complementary evidence to parametric model predictions
- Mechanism: Token-level probabilities computed without conditioning on the full context provide robust evidence for fact validity, especially when parametric models have strong priors
- Core assumption: Non-parametric probabilities capture factual validity better than parametric model predictions alone
- Evidence anchors:
  - [section 4.1.1] "Nonparametric Probability (NP) computes a nonparametric likelihood and make a judgment based on thresholding"
  - [section 4.2.1] "Adding NP improves Retrieve→LM by 2–9%"
- Break condition: When non-parametric probabilities are unreliable due to vocabulary mismatch or when token-level information is insufficient

## Foundational Learning

- Concept: Knowledge source consistency
  - Why needed here: The evaluation assumes the knowledge source doesn't contain conflicting information, which is critical for binary fact validation
  - Quick check question: What happens if Wikipedia contains contradictory information about the same fact?

- Concept: Fact decomposition vs. sentence segmentation
  - Why needed here: Understanding why breaking sentences into atomic facts is superior to validating at sentence level
  - Quick check question: How would you decompose "Barack Obama, born in Hawaii, was the 44th President of the United States" into atomic facts?

- Concept: Retrieval system quality metrics
  - Why needed here: The evaluation depends on retrieving relevant passages, so understanding precision/recall tradeoffs is crucial
  - Quick check question: What's the difference between exact match and semantic similarity in passage retrieval?

## Architecture Onboarding

- Component map: Data collection → Atomic fact generation → Fact validation → Score computation → Model estimation
- Critical path: Generation → Atomic fact decomposition → Fact validation → FACTSCORE calculation
- Design tradeoffs: Granularity vs. efficiency (more atomic facts = more accurate but slower), retrieval vs. parametric prediction (external knowledge vs. speed)
- Failure signatures: Overestimation when model has strong priors, underestimation when retrieval fails, inconsistent scores across rare entities
- First 3 experiments:
  1. Generate atomic facts from InstructGPT and compare to human decomposition for agreement rate
  2. Test Retrieve→LM with different numbers of retrieved passages (k=3,5,7) to find optimal balance
  3. Compare F1MICRO vs Error Rate rankings on a small validation set to understand metric tradeoffs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different choices of the knowledge source (e.g., Wikipedia vs. news articles vs. scientific literature) impact the FACTSCORE evaluation for different domains of text generation?
- Basis in paper: [inferred] The paper mentions that FACTSCORE can be applied to a broader domain, such as text about recent events whose knowledge source can be a collection of news articles, or text about scientific findings whose knowledge source can be a collection of scientific literature.
- Why unresolved: The paper primarily focuses on evaluating people biographies against Wikipedia. It does not provide a detailed comparison of how different knowledge sources affect the FACTSCORE for various text generation domains.
- What evidence would resolve it: A comprehensive study comparing FACTSCORE across different knowledge sources for various text generation domains, such as news articles, scientific literature, and biographies, would provide insights into how the choice of knowledge source impacts the evaluation.

### Open Question 2
- Question: How does the performance of the FACTSCORE estimator vary across different languages and cultural contexts, and what are the challenges in adapting it for multilingual and multicultural text generation evaluation?
- Basis in paper: [inferred] The paper does not explicitly discuss the performance of the FACTSCORE estimator in multilingual and multicultural contexts, but it mentions the need for future work to extend FACTSCORE for a broader set of generations and to further improve the estimator.
- Why unresolved: The paper focuses on English Wikipedia and people biographies, and does not provide insights into how the FACTSCORE estimator performs in other languages or cultural contexts. It also does not discuss the challenges in adapting the estimator for multilingual and multicultural text generation evaluation.
- What evidence would resolve it: A study evaluating the FACTSCORE estimator's performance across different languages and cultural contexts, along with an analysis of the challenges in adapting the estimator for multilingual and multicultural text generation evaluation, would provide valuable insights.

### Open Question 3
- Question: How can the FACTSCORE evaluation be extended to consider other aspects of factuality, such as factual recall and the coverage of factual information in long-form text generation?
- Basis in paper: [explicit] The paper discusses the limitations of FACTSCORE, stating that it focuses on factual precision but not recall, and that it does not consider the coverage of information in a generation. It suggests that future work should consider other aspects of factuality, such as recall and coverage of factual information.
- Why unresolved: The paper does not provide a concrete solution or methodology for extending FACTSCORE to consider other aspects of factuality, such as factual recall and coverage of factual information. It only mentions the need for future work in this direction.
- What evidence would resolve it: A detailed proposal or methodology for extending FACTSCORE to consider other aspects of factuality, along with experimental results demonstrating the effectiveness of the extended evaluation, would provide a solution to this open question.

## Limitations
- Evaluation methodology assumes Wikipedia as reliable knowledge source, introducing potential biases
- Atomic fact decomposition relies on InstructGPT, which may not perfectly align with human judgment
- Study focuses on biography generation, limiting generalizability to other long-form generation tasks
- Automated estimator, while achieving <2% error rate, depends on retrieval quality and language model judgments

## Confidence
- High Confidence: Retrieval-augmented models achieve higher factual precision than non-retrieval models
- Medium Confidence: Factual precision drops for rarer entities and later parts of generations
- Low Confidence: Atomic fact decomposition is the optimal approach for factual evaluation

## Next Checks
1. **Cross-Domain Validation**: Test FACTSCORE on long-form generation tasks outside biographies (e.g., news articles, scientific summaries) to assess generalizability of the observed patterns in factual precision across different entity types and knowledge domains.

2. **Knowledge Source Robustness**: Evaluate the same generations against multiple knowledge sources (e.g., Wikidata, news archives) to quantify how sensitive FACTSCORE is to the choice of ground truth and identify potential knowledge gaps or contradictions across sources.

3. **Human-AI Agreement Study**: Conduct a detailed analysis of cases where human annotators and the automated FACTSCORE estimator disagree, categorizing error types (e.g., context-dependent facts, temporal ambiguities, knowledge source limitations) to identify systematic weaknesses in the evaluation approach.