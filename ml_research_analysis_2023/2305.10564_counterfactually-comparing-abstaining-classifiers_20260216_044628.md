---
ver: rpa2
title: Counterfactually Comparing Abstaining Classifiers
arxiv_id: '2305.10564'
source_url: https://arxiv.org/abs/2305.10564
tags:
- classi
- score
- abstaining
- counterfactual
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of evaluating and comparing black-box
  abstaining classifiers. The authors propose a novel evaluation metric called the
  counterfactual score, which measures the expected performance of an abstaining classifier
  had it not been allowed to abstain.
---

# Counterfactually Comparing Abstaining Classifiers

## Quick Facts
- arXiv ID: 2305.10564
- Source URL: https://arxiv.org/abs/2305.10564
- Reference count: 40
- Primary result: A principled evaluation metric for abstaining classifiers based on counterfactual scores

## Executive Summary
This paper addresses the challenge of evaluating and comparing black-box abstaining classifiers by introducing the counterfactual score. The approach treats abstentions as missing predictions within a causal inference framework, allowing principled evaluation of what performance would be if classifiers couldn't abstain. Under missing at random (MAR) and positivity assumptions, the counterfactual score is identifiable and can be efficiently estimated using doubly robust methods. The authors develop asymptotically valid confidence intervals and hypothesis tests for comparing counterfactual scores, with experiments demonstrating superior performance compared to baseline methods on both simulated and real data.

## Method Summary
The paper proposes evaluating abstaining classifiers by estimating their counterfactual scores - what performance would be had they not been allowed to abstain. The method casts this as a missing data problem where abstentions are treated as missing predictions. Under MAR and positivity assumptions, the counterfactual score is identifiable and can be efficiently estimated using doubly robust estimators that combine regression and inverse probability weighting approaches. The authors develop confidence intervals and hypothesis tests for comparing counterfactual scores between different classifiers, with extensions to anytime-valid confidence sequences for sequential evaluation settings.

## Key Results
- Doubly robust estimators achieve nonparametric efficiency bounds for counterfactual score estimation
- Confidence intervals maintain nominal coverage rates in simulations across linear and nonlinear decision boundaries
- Real data experiments on CIFAR-100 demonstrate practical utility for comparing abstaining classifiers
- Confidence sequences provide valid inference under continuous monitoring and optional stopping

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual scores are identifiable under MAR and positivity assumptions.
- Mechanism: By treating abstentions as missing data, the counterfactual score equals the expected score had the classifier not been allowed to abstain. Under MAR (scores missing at random given input) and positivity (abstainer has at least ε probability of not abstaining), the score is identifiable as the expectation of the regression function for scores under R=0.
- Core assumption: MAR holds when evaluation data is independent of training data; positivity holds when abstentions are stochastic with minimum probability ε.
- Evidence anchors:
  - [abstract]: "if the abstentions are stochastic, and if the evaluation data is independent of the training data (ensuring that the predictions are missing at random), then the score is identifiable."
  - [section]: "Proposition 2.4 (Identifiability). Under Assumptions 2.1 and 2.3, ψ is identified as EX[µ0(X)]."
  - [corpus]: Weak - corpus neighbors discuss abstaining classifiers but don't directly address identifiability conditions.
- Break condition: If either MAR or positivity is violated (e.g., deterministic abstentions or dependent evaluation data).

### Mechanism 2
- Claim: Doubly robust estimators achieve nonparametric efficiency for counterfactual score estimation.
- Mechanism: The doubly robust estimator combines a regression estimator and an inverse probability weighting estimator. It is consistent if either nuisance function is estimated consistently, and achieves the semiparametric efficiency bound if both are estimated at parametric rates.
- Core assumption: The product of nuisance estimation errors is o(1/√n) and the efficient influence function has finite variance.
- Evidence anchors:
  - [abstract]: "we develop nonparametric and doubly robust methods to efficiently estimate this quantity under identification."
  - [section]: "Theorem 3.1 (DR estimation of the counterfactual score for an abstaining classifier). Suppose that Assumptions 2.1 and 2.3 hold. Also, suppose that ∥ˆπ − π∥L2(P)∥ˆµ0 − µ0∥L2(P) = oP(1/√n)... Then, √n(ˆψdr − ψ) ⇝ N(0, VarP[IF]), where VarP[IF] matches the nonparametric efficiency bound."
  - [corpus]: Weak - corpus neighbors don't discuss doubly robust estimation methods.
- Break condition: If both nuisance functions are estimated at slower than parametric rates.

### Mechanism 3
- Claim: Confidence sequences provide anytime-valid inference for counterfactual scores.
- Mechanism: Confidence sequences are sequences of confidence intervals that maintain validity uniformly over all sample sizes, allowing continuous monitoring and optional stopping without inflating type I error.
- Core assumption: The nuisance functions are estimated consistently and the efficient influence function has finite moments.
- Evidence anchors:
  - [abstract]: "Our approaches avoid parametric assumptions and allow for black-box classifiers."
  - [section]: "Theorem E.1 (Anytime-valid DR estimation of the counterfactual score)... for any choice of ρ > 0, ˆψdr ± √VarˆPn[ˆIF] · (√2nρ2 + 1/n2ρ2 log(√nρ2 + 1/α)) forms a (1 − α)-AsympCS for ψ with an approximation rate of √log logn/n."
  - [corpus]: Weak - corpus neighbors don't discuss confidence sequences or anytime-valid inference.
- Break condition: If the approximation rate doesn't decay fast enough or nuisance functions are poorly estimated.

## Foundational Learning

- Concept: Missing data framework and causal inference
  - Why needed here: The paper casts abstaining classifier evaluation as a missing data problem where abstentions are treated as missing predictions, allowing the application of causal inference tools.
  - Quick check question: Why can we treat abstentions as missing data in this context?

- Concept: Doubly robust estimation
  - Why needed here: Doubly robust estimators combine regression and weighting approaches to achieve efficiency while being robust to misspecification of either nuisance function.
  - Quick check question: What are the two nuisance functions in the doubly robust estimator for counterfactual scores?

- Concept: Efficiency bounds in semiparametric statistics
  - Why needed here: The paper establishes that the proposed doubly robust estimators achieve the nonparametric efficiency bound, meaning no other estimator can achieve lower asymptotic variance.
  - Quick check question: What does it mean for an estimator to achieve the nonparametric efficiency bound?

## Architecture Onboarding

- Component map:
  Evaluation data (Xi, Ri, Si) -> Nuisance estimators (ˆπ, ˆµ0) -> Doubly robust estimator -> Counterfactual score with confidence interval/sequence

- Critical path:
  1. Receive evaluation data
  2. Estimate nuisance functions (ˆπ, ˆµ0) via cross-fitting
  3. Compute efficient influence function estimate
  4. Form doubly robust estimator
  5. Construct confidence interval or sequence

- Design tradeoffs:
  - Flexible nuisance estimators vs. computational cost
  - Sample splitting for cross-fitting vs. data efficiency
  - Fixed confidence intervals vs. confidence sequences for anytime validity

- Failure signatures:
  - Miscoverage rates deviating from nominal level
  - High variance in counterfactual score estimates
  - Numerical instability in inverse probability weighting when ˆπ is near 0 or 1

- First 3 experiments:
  1. Verify MAR and positivity assumptions hold in your evaluation setting
  2. Compare plug-in, IPW, and doubly robust estimators on simulated data
  3. Test confidence sequence validity with continuous data collection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical limits of the counterfactual score in scenarios where abstentions are deterministic and how can these be addressed?
- Basis in paper: [explicit] The paper states that if abstentions are deterministic, the counterfactual score is unidentifiable because the classifier can perform arbitrarily poorly on its abstentions.
- Why unresolved: The paper mentions this as a limitation but does not provide a comprehensive framework for addressing this issue in deterministic scenarios.
- What evidence would resolve it: A theoretical framework that either demonstrates how to estimate counterfactual scores in deterministic scenarios or proves the impossibility of doing so would be needed.

### Open Question 2
- Question: How does the counterfactual score perform in settings with continuous or structured outputs (e.g., regression, time series) compared to classification?
- Basis in paper: [explicit] The paper mentions that the setup is applicable to any form of prediction that can be scored, including regression and structured prediction, but restricts attention to classification for concreteness.
- Why unresolved: The paper does not provide empirical evidence or theoretical analysis for settings outside of classification.
- What evidence would resolve it: Empirical studies comparing the counterfactual score across different types of prediction tasks or a theoretical analysis of its properties in these settings.

### Open Question 3
- Question: What are the practical implications of the positivity assumption for real-world abstaining classifiers, and how can these be enforced?
- Basis in paper: [explicit] The paper discusses the importance of the positivity assumption and suggests policy-level enforcement but does not provide detailed guidelines or empirical validation.
- Why unresolved: The paper raises the issue but does not explore practical strategies or their effectiveness in real-world scenarios.
- What evidence would resolve it: Case studies or experiments that demonstrate how policy-level enforcement of positivity affects the performance and reliability of abstaining classifiers in real-world applications.

## Limitations

- The approach critically depends on MAR and positivity assumptions that may not hold in practice, especially when abstentions are deterministic
- Doubly robust estimators require both nuisance functions to be estimated at parametric rates for efficiency, which may be challenging for complex black-box classifiers
- Confidence sequences have approximation rates that may not be sufficiently fast in finite samples

## Confidence

- High confidence: The theoretical framework linking counterfactual scores to missing data and the efficiency bound for doubly robust estimators
- Medium confidence: Practical performance on real datasets and the finite-sample behavior of confidence sequences
- Low confidence: Robustness to violations of MAR and positivity assumptions in realistic settings

## Next Checks

1. Stress test the approach under varying degrees of MAR violation by introducing dependence between evaluation data and training data
2. Benchmark the confidence sequence approach against fixed confidence intervals in sequential evaluation settings with early stopping rules
3. Evaluate performance when using different black-box classifiers (beyond random forests) as nuisance function estimators to assess generalizability