---
ver: rpa2
title: Hyperparameter Optimization for Large Language Model Instruction-Tuning
arxiv_id: '2312.00949'
source_url: https://arxiv.org/abs/2312.00949
tags:
- optimization
- validation
- nomad
- best
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study demonstrates that hyperparameter optimization using blackbox
  optimization algorithms, specifically NOMAD and NNI-TPE, significantly improves
  the performance of fine-tuned large language models on downstream tasks and human
  evaluation. The best models optimized by these algorithms outperform those using
  default fine-tuning parameters.
---

# Hyperparameter Optimization for Large Language Model Instruction-Tuning

## Quick Facts
- arXiv ID: 2312.00949
- Source URL: https://arxiv.org/abs/2312.00949
- Authors: 
- Reference count: 40
- Key outcome: Blackbox optimization algorithms (NOMAD and NNI-TPE) significantly improve LoRA fine-tuned model performance on downstream tasks and human evaluation, with NOMAD achieving better benchmark scores despite NNI-TPE's lower validation losses.

## Executive Summary
This study explores hyperparameter optimization for Large Language Model instruction-tuning using LoRA (Low-Rank Adaptation). The authors apply blackbox optimization algorithms (NOMAD and NNI-TPE) to find optimal hyperparameters for LoRA fine-tuning on LLaMA 2 7B. Through 100 evaluations with 2 epochs each, the best models optimized by these algorithms significantly outperform those using default fine-tuning parameters on downstream tasks and human evaluation. Notably, NOMAD-tuned models achieve better performance on instruction-following benchmarks compared to NNI-TPE, despite the latter obtaining lower validation losses, highlighting that validation loss alone is not a perfect proxy for downstream task performance.

## Method Summary
The study uses LLaMA 2 7B pre-trained model and implements LoRA fine-tuning with hyperparameters including rank, dropout, scaling factor (Î±), and learning rate. The training data consists of 54k samples (70% Alpaca dataset, 30% Dolly dataset) with a 13k sample validation set. Blackbox optimization is performed using NOMAD (MADS algorithm) and NNI-TPE over 100 evaluations, each running 2 epochs on four NVIDIA A100 GPUs with 80GB memory. The optimized models are evaluated on MMLU, BBH, DROP, and HumanEval benchmarks, along with human preference evaluation using the Vicuna dataset.

## Key Results
- Models optimized by NOMAD and NNI-TPE significantly outperform default fine-tuning parameters on downstream tasks
- NOMAD-tuned models achieve better benchmark scores despite NNI-TPE obtaining lower validation losses
- The study demonstrates that validation loss as a single optimization objective does not reliably predict downstream task performance
- Among the top 10 NNI-TPE models, only 2 have LoRA rank higher than 32, showing that low rank can perform well with appropriate other hyperparameters

## Why This Works (Mechanism)

### Mechanism 1
Validation loss as a single objective in blackbox optimization does not reliably predict downstream task performance because the optimization process minimizes validation loss on a small dataset that may not capture the full diversity of real-world instruction-following tasks. As a result, models with lower validation losses may not achieve higher scores on benchmarks like MMLU or HumanEval.

### Mechanism 2
NOMAD's direct search approach with mesh adaptation can explore hyperparameter space more effectively for instruction-tuning than NNI-TPE's Bayesian approach. NOMAD uses a mesh adaptive direct search that iteratively refines the search around promising solutions, allowing it to balance global exploration and local exploitation, leading to better hyperparameter combinations for instruction-following tasks despite NNI-TPE's lower validation losses.

### Mechanism 3
Increasing LoRA rank is not the only way to improve model performance; other hyperparameters like learning rate and scaling factor are crucial. The study shows that models with lower LoRA ranks can perform well if other hyperparameters are optimized appropriately, indicating that a balanced approach to hyperparameter tuning leads to better instruction-following capabilities.

## Foundational Learning

- Concept: Blackbox optimization
  - Why needed here: The study uses blackbox optimization algorithms (NOMAD and NNI-TPE) to tune hyperparameters without knowing the analytical form of the objective function
  - Quick check question: What is the main difference between blackbox optimization and traditional gradient-based optimization?

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: LoRA is the parameter-efficient fine-tuning method used in the study, and its performance is heavily influenced by hyperparameters like rank and scaling factor
  - Quick check question: How does LoRA reduce the number of trainable parameters in large language models?

- Concept: Hyperparameter optimization (HPO)
  - Why needed here: HPO is the process of finding the best combination of hyperparameters to improve model performance, which is central to the study's methodology
  - Quick check question: Why is hyperparameter optimization considered the outer loop of a learning process?

## Architecture Onboarding

- Component map: Pre-trained LLM (LLaMA 2) -> LoRA adapter layers -> Hyperparameter optimization algorithms (NOMAD, NNI-TPE) -> Downstream task benchmarks (MMLU, BBH, DROP, HumanEval) -> Validation pipeline

- Critical path: 1) Initialize pre-trained LLM and LoRA adapter layers, 2) Define hyperparameter search space, 3) Run blackbox optimization to find optimal hyperparameters, 4) Fine-tune model using optimized hyperparameters, 5) Evaluate model on downstream tasks and human preference

- Design tradeoffs: Single vs. multi-objective optimization (single objective is faster but may not capture all performance aspects); Exploration vs. exploitation (balancing global search and local refinement is crucial for finding good hyperparameters)

- Failure signatures: Overfitting on small validation sets; Poor generalization to unseen tasks; Computational inefficiency due to large hyperparameter search space

- First 3 experiments: 1) Run NOMAD with a small evaluation budget to validate initial hyperparameter choices, 2) Compare NOMAD and NNI-TPE on a fixed set of hyperparameters to assess optimization effectiveness, 3) Evaluate the best models from each optimizer on downstream tasks to determine which hyperparameters yield better performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of validation loss as the objective function in hyperparameter optimization affect the performance of the fine-tuned model on downstream tasks? The paper discusses using validation loss as the objective function and notes that lower validation losses do not necessarily translate into higher benchmark scores on downstream tasks, but does not explore alternative objective functions.

### Open Question 2
What is the impact of LoRA rank on the overfitting of models trained on small datasets, and how does this affect the generalization ability of the models? The paper mentions that a large LoRA rank could result in more overfitting on small datasets, yet a small rank may fail to capture the diversity of complicated instructions, but does not provide detailed analysis of this trade-off.

### Open Question 3
How do the hyperparameter optimization results from NOMAD and NNI-TPE compare when considering multiple criteria in the optimization problem? The paper compares NOMAD and NNI-TPE results but does not explore the use of multi-objective optimization to balance validation loss with other performance criteria.

## Limitations

- The study relies on validation loss as the sole optimization objective, which may not fully capture downstream task performance
- The computational cost of 100 evaluations requiring approximately 5 days on four NVIDIA A100 GPUs may not be practical for rapid experimentation
- The results are specific to LLaMA 2 7B with instruction-tuning and may not generalize to other model architectures or task types

## Confidence

**High Confidence**: The core finding that blackbox optimization can improve LoRA fine-tuning performance is well-supported by the experimental results.

**Medium Confidence**: The mechanism explaining why NOMAD outperforms NNI-TPE despite higher validation losses is plausible but not definitively proven.

**Low Confidence**: The generalizability of these results to other model architectures, task types, or optimization algorithms is uncertain.

## Next Checks

1. Expand validation set size to test whether increasing it improves the correlation between validation loss and downstream task performance

2. Apply the same optimization approach to different model architectures (e.g., GPT-2, BERT) and task types to assess generalizability

3. Experiment with multi-objective optimization incorporating both validation loss and downstream task performance as optimization criteria