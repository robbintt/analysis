---
ver: rpa2
title: 'Modularizing while Training: A New Paradigm for Modularizing DNN Models'
arxiv_id: '2306.09376'
source_url: https://arxiv.org/abs/2306.09376
tags:
- training
- modules
- class
- coupling
- cohesion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Modularizing Deep Neural Networks (DNNs) is crucial for model reuse
  but traditionally incurs significant accuracy loss and overhead. This paper introduces
  Modularizing while Training (MwT), a new paradigm that incorporates modularization
  into the training process by optimizing for intra-module cohesion and inter-module
  coupling.
---

# Modularizing while Training: A New Paradigm for Modularizing DNN Models

## Quick Facts
- **arXiv ID**: 2306.09376
- **Source URL**: https://arxiv.org/abs/2306.09376
- **Reference count**: 40
- **Primary result**: MwT reduces accuracy loss to 1.13 percentage points, kernel retention rate to 14.58%, and halves total training/modularization time to 108 minutes.

## Executive Summary
Modularizing Deep Neural Networks (DNNs) is crucial for model reuse but traditionally incurs significant accuracy loss and overhead. This paper introduces Modularizing while Training (MwT), a new paradigm that incorporates modularization into the training process by optimizing for intra-module cohesion and inter-module coupling. MwT employs loss functions to encourage relevant weights to be concentrated in small portions of the model while minimizing overlap between modules. Evaluated on representative CNN models, MwT significantly outperforms existing approaches.

## Method Summary
MwT introduces a modular training paradigm where DNNs are trained with additional cohesion and coupling loss terms alongside cross-entropy. These losses encourage class-specific kernel usage and discourage kernel sharing across classes. During training, per-layer mask generators produce relevance masks for each sample. After training, modularization applies thresholding to these masks to filter out irrelevant kernels, creating modular models with high intra-class similarity and low inter-class similarity. The approach eliminates costly post-hoc decomposition by identifying relevant kernels during training.

## Key Results
- Reduces accuracy loss to 1.13 percentage points (1.76 points less than baseline)
- Decreases kernel retention rate to 14.58% (74.31% reduction)
- Halves total training and modularization time to 108 minutes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MwT reduces kernel retention rate by forcing low coupling during training rather than trying to decompose a fully trained model.
- Mechanism: MwT introduces two loss terms (cohesion loss and coupling loss) that are minimized alongside cross-entropy during training. The coupling loss explicitly discourages shared kernel usage across classes, so at modularization time fewer kernels are relevant to multiple classes.
- Core assumption: Early training losses can reshape the learned kernel distribution to favor class-specific kernels, which can then be safely removed without hurting accuracy.
- Evidence anchors:
  - [abstract] "MwT employs loss functions to encourage relevant weights to be concentrated in small portions of the model while minimizing overlap between modules."
  - [section] "By minimizing the coupling loss, the modular model tends to generate masks with zero cosine similarity for samples belonging to different classes."
  - [corpus] Weak/no direct experimental evidence for the long-term stability of these losses beyond the tested models/datasets.
- Break condition: If the loss terms push the model into a local optimum where kernels become too specialized, accuracy may collapse or generalization may degrade.

### Mechanism 2
- Claim: MwT maintains classification accuracy by only filtering out kernels that are never used for a given class.
- Mechanism: The relevant kernel recognition step uses learned mask generators to produce per-sample masks. Modularization then applies a threshold to retain only kernels that appear frequently enough for a class, removing irrelevant kernels while keeping the ones that actually contribute.
- Core assumption: The mask generator's thresholding step is stable enough to avoid discarding useful kernels while still achieving high reduction in kernel count.
- Evidence anchors:
  - [abstract] "MwT significantly outperforms existing approaches... accuracy loss caused by MwT is only 1.13 percentage points"
  - [section] "MwT filters out kernels by setting a reasonable threshold ğœ because of the inherent randomness of neural network models."
  - [corpus] No direct ablation studies on the effect of ğœ on accuracy vs kernel reduction trade-off.
- Break condition: If the threshold is set too high, useful kernels are pruned; if too low, kernel reduction is minimal.

### Mechanism 3
- Claim: MwT speeds up modularization by pre-identifying relevant kernels during training, eliminating the need for costly post-hoc decomposition.
- Mechanism: Since the model is already trained to produce coherent masks, modularization only requires a forward pass to generate masks and a simple threshold filter, rather than iterative search or genetic algorithms.
- Core assumption: The learned mask generators produce reliable, class-specific masks quickly enough to make the modularization stage negligible compared to standard training time.
- Evidence anchors:
  - [abstract] "the total time cost required for training and modularizing is only 108 minutes, half of the baseline."
  - [section] "MwT simply needs to remove the irrelevant weights to construct modules."
  - [corpus] No direct timing breakdown comparing mask generation vs other modularization methods.
- Break condition: If mask generation becomes computationally heavy (e.g., with very large models or datasets), the speedup advantage diminishes.

## Foundational Learning

- Concept: Cross-entropy loss and its role in standard neural network training
  - Why needed here: MwT extends the standard training loop by adding two extra loss terms; understanding how cross-entropy loss drives classification is essential to see how MwT's new losses fit in.
  - Quick check question: What is the gradient update rule for cross-entropy loss in a softmax classifier?

- Concept: Jaccard Index as a similarity metric for sets
  - Why needed here: MwT uses Jaccard Index to compute cohesion (similarity of kernel sets within a class) and coupling (similarity across classes).
  - Quick check question: Given two sets A={1,2,3} and B={2,3,4}, what is their Jaccard Index?

- Concept: Cosine similarity for continuous mask vectors
  - Why needed here: MwT approximates cohesion/coupling losses using cosine similarity between per-sample masks during training, because Jaccard Index cannot be directly differentiated.
  - Quick check question: If two mask vectors are [0.5, 0.5] and [1.0, 0.0], what is their cosine similarity?

## Architecture Onboarding

- Component map: CNN backbone -> Mask generators -> Loss computation module -> Modularization stage
- Critical path:
  1. Forward pass: Input â†’ CNN â†’ mask generators â†’ masked FMs â†’ predictions
  2. Backward pass: Compute losses â†’ update CNN weights and mask generators
  3. Modularization: Forward pass with trained mask generators â†’ threshold masks â†’ filter kernels
- Design tradeoffs:
  - Higher ğœ â†’ fewer kernels retained but risk of accuracy loss
  - More mask generator layers â†’ better kernel relevance estimation but higher overhead
  - Weighting factors ğ›¼, ğ›½ â†’ balance between accuracy, cohesion, coupling
- Failure signatures:
  - Accuracy drops sharply â†’ likely over-aggressive kernel pruning or misaligned loss weights
  - Kernel retention rate remains high â†’ cohesion/coupling losses not strong enough
  - Training instability â†’ inappropriate learning rates or mask generator architecture
- First 3 experiments:
  1. Run standard training on VGG16-CIFAR10 and record baseline accuracy/kernel count.
  2. Enable MwT with default ğ›¼=0.5, ğ›½=1.5, ğœ=0.9; observe training curves for cohesion/coupling losses.
  3. After training, apply modularization; measure KRR, cohesion, coupling, and accuracy vs baseline.

## Open Questions the Paper Calls Out
- How can MwT be generalized to work with transformer-based models like BERT or GPT?
- What is the optimal strategy for handling residual connections in complex CNN architectures beyond the padding-based method proposed?
- How does MwT's performance scale with extremely large datasets like ImageNet compared to smaller datasets like CIFAR-10?

## Limitations
- Evaluation limited to small-scale image classification tasks (CIFAR10/SVHN) and only three CNN architectures
- No evidence of MwT's effectiveness on larger, more complex models or datasets
- No ablation studies on sensitivity of hyper-parameters (Î±, Î², Ï„) or runtime breakdowns

## Confidence
- **High Confidence**: MwT reduces kernel retention rate and training+modularization time on tested CNN models
- **Medium Confidence**: MwT's accuracy loss is significantly lower than baseline modularization methods, based on limited benchmarks
- **Low Confidence**: MwT generalizes to larger models, different tasks, or datasets beyond CIFAR10/SVHN without retraining

## Next Checks
1. **Ablation on hyper-parameters**: Systematically vary Î±, Î², and Ï„ to quantify their impact on KRR, cohesion/coupling metrics, and accuracy
2. **Scaling test**: Apply MwT to a deeper architecture (e.g., ResNet50) or a larger dataset (e.g., ImageNet) and compare KRR and accuracy degradation against the baseline
3. **Stability check**: Run MwT training 10 times on the same dataset/model pair and report variance in KRR and final accuracy to assess robustness of the mask generation process