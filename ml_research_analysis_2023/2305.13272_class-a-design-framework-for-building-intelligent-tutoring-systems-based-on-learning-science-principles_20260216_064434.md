---
ver: rpa2
title: 'CLASS: A Design Framework for building Intelligent Tutoring Systems based
  on Learning Science principles'
arxiv_id: '2305.13272'
source_url: https://arxiv.org/abs/2305.13272
tags:
- student
- spock
- tutorbot
- subproblem
- students
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CLASS, a framework for building intelligent
  tutoring systems (ITS) using large language models (LLMs) that aims to provide step-by-step
  guidance and engaging conversational interactions for students. CLASS leverages
  two synthetic datasets generated by GPT-4: a scaffolding dataset that equips the
  ITS with problem-solving strategies and a conversational dataset that teaches the
  ITS to apply these skills in real-time student interactions.'
---

# CLASS: A Design Framework for building Intelligent Tutoring Systems based on Learning Science principles

## Quick Facts
- arXiv ID: 2305.13272
- Source URL: https://arxiv.org/abs/2305.13272
- Reference count: 40
- One-line primary result: CLASS framework trains SPOCK ITS to achieve expert-rated scores above 4/5 on factual correctness, relevance, completeness, and motivational impact for biology tutoring.

## Executive Summary
This paper introduces CLASS, a framework for building intelligent tutoring systems using large language models that provides step-by-step guidance and engaging conversational interactions. CLASS uses two synthetic datasets generated by GPT-4: a scaffolding dataset for problem-solving strategies and a conversational dataset for real-time student interactions. The authors implement SPOCK, a proof-of-concept ITS for introductory biology, using Vicuna-13b fine-tuned on biology content and CLASS datasets. Expert evaluation shows SPOCK achieves high scores on factual correctness, relevance, completeness, and motivational impact, with particular praise for its ability to break down problems and provide encouraging guidance.

## Method Summary
The CLASS framework generates two synthetic datasets using GPT-4: a scaffolding dataset containing problems, subproblems, hints, incorrect solutions, and feedback, and a conversational dataset with simulated student-tutor dialogues. Vicuna-13b is fine-tuned first on OpenStax Biology 2e textbook content using CLM loss, then on both CLASS datasets using DeepSpeed and FastChat libraries. Indexed search over educational content ensures factual accuracy by retrieving relevant passages during inference. The response template standardizes handling of different student responses while maintaining natural conversation flow.

## Key Results
- Expert evaluation scores above 4/5 for SPOCK on factual correctness, relevance, completeness, and motivational impact
- Successful problem decomposition into manageable subproblems mirroring effective human tutoring strategies
- Effective integration of indexed search reducing hallucinations and ensuring factual accuracy
- Positive expert feedback on SPOCK's ability to provide encouraging step-by-step guidance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLASS successfully trains SPOCK to break down complex problems into manageable subproblems
- Mechanism: Scaffolding dataset explicitly maps problems to subproblems, hints, and feedback, teaching systematic decomposition
- Core assumption: LLMs can learn and generalize problem decomposition from synthetic examples
- Evidence anchors: Abstract mentions scaffolding dataset components; section describes wide coverage of problems and subproblems
- Break condition: Insufficient dataset diversity or LLM inability to generalize decomposition strategies

### Mechanism 2
- Claim: Conversational dataset enables real-time application of problem-solving skills through structured dialogue
- Mechanism: Response template standardizes handling of correct, incorrect, and ambiguous responses while maintaining natural flow
- Core assumption: Fixed template can guide LLM responses effectively without sacrificing naturalness
- Evidence anchors: Abstract mentions natural language interactions; section describes template addressing various situations
- Break condition: Template rigidity preventing natural conversation or LLM failure to adapt within constraints

### Mechanism 3
- Claim: Indexed search over educational content reduces hallucinations and ensures factual accuracy
- Mechanism: Retrieves relevant textbook passages based on problem and subproblem fields to ground responses
- Core assumption: Educational content indexing can be efficiently integrated with LLM responses
- Evidence anchors: Abstract mentions indexed search for factual correctness; section describes paragraph indexing and retrieval
- Break condition: Search failures to retrieve relevant passages or LLM ignoring retrieved content

## Foundational Learning

- Concept: Problem Decomposition in Computational Thinking
  - Why needed here: SPOCK's effectiveness depends on breaking complex problems into manageable subproblems for step-by-step guidance
  - Quick check question: If given a complex biology problem, can you identify at least three meaningful subproblems that would help a student understand the solution process?

- Concept: Learning Science Principles (Scaffolding and Socio-constructivism)
  - Why needed here: CLASS dual-dataset approach is grounded in scaffolding theory, tailoring support to student understanding levels
  - Quick check question: How would you design hints that provide support without giving away answers, based on Vygotsky's zone of proximal development?

- Concept: Large Language Model Fine-tuning and Instruction-based Training
  - Why needed here: SPOCK uses Vicuna-13b fine-tuned on biology content and CLASS datasets, requiring understanding of LLM adaptation
  - Quick check question: What are the key differences between zero-shot, few-shot, and fine-tuning approaches for adapting LLMs to specific domains?

## Architecture Onboarding

- Component map: GPT-4 dataset generation pipeline → Scaffolding dataset + Conversational dataset → Vicuna-13b base model → Fine-tuned on biology textbook → Fine-tuned on CLASS datasets → Indexed search module → Educational content retrieval → Response augmentation → Response template engine → Structured dialogue management

- Critical path: Dataset generation → Model fine-tuning → Inference with search integration → Student interaction

- Design tradeoffs:
  - Dataset quality vs. generation cost (GPT-4 is expensive)
  - Template rigidity vs. conversation naturalness
  - Search integration complexity vs. response accuracy
  - Model size vs. deployment cost and latency

- Failure signatures:
  - Poor problem decomposition → Students get stuck on main problems
  - Template mismatch → Robotic or unhelpful responses
  - Search failures → Hallucinations or irrelevant hints
  - Fine-tuning instability → Degraded performance on either scaffolding or conversation tasks

- First 3 experiments:
  1. Test SPOCK on held-out biology problems to evaluate generalization of problem decomposition
  2. Compare response quality with and without indexed search to quantify hallucination reduction
  3. A/B test different response template variants to find optimal balance between structure and naturalness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SPOCK perform compared to traditional educational resources like textbooks in terms of student learning outcomes?
- Basis in paper: [inferred] Paper mentions expert evaluation but lacks student learning outcome comparisons to traditional resources
- Why unresolved: Current evaluation only involves expert assessment, not measurable learning gains or comparisons to traditional methods
- What evidence would resolve it: Controlled study comparing student test scores and knowledge retention between SPOCK and traditional textbooks

### Open Question 2
- Question: What is the optimal balance between SPOCK's automated responses and human instructor intervention for different educational contexts?
- Basis in paper: [inferred] Paper describes SPOCK as standalone system without examining integration with human instructors
- Why unresolved: Paper doesn't explore how SPOCK might complement or replace human instructors in various educational settings
- What evidence would resolve it: Comparative studies across educational contexts measuring learning outcomes with varying levels of SPOCK integration

### Open Question 3
- Question: How can the CLASS framework be extended to handle interdisciplinary or complex problem-solving scenarios?
- Basis in paper: [explicit] Paper states aim to explore different subjects but doesn't detail handling of interdisciplinary problems
- Why unresolved: Current framework appears designed for single-subject problems, but real-world problems often require multiple disciplines
- What evidence would resolve it: Development and evaluation of extended CLASS framework handling complex, interdisciplinary problems

## Limitations

- Dataset generation quality uncertainty: Entirely relies on GPT-4 generation without independent validation of dataset quality beyond final system evaluation
- Limited evaluation scope: Expert ratings on proof-of-concept system rather than comprehensive testing across diverse student populations and learning contexts
- Scalability concerns: Expensive GPT-4 generation and Vicuna-13b fine-tuning raise questions about practical deployment at scale

## Confidence

- High confidence: Basic framework architecture and theoretical grounding in learning science principles
- Medium confidence: Empirical evaluation results showing positive expert ratings, though limited in scope
- Low confidence: Generalizability claims beyond introductory biology domain

## Next Checks

1. Independent dataset validation - Have pedagogical experts evaluate raw scaffolding and conversational datasets for educational soundness
2. Comparative effectiveness study - Compare SPOCK against established ITS approaches on identical problem sets, measuring expert ratings and actual student learning gains
3. Domain transfer experiment - Apply CLASS framework to different subject domain (e.g., physics or mathematics) using same methodology to test generalizability