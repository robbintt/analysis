---
ver: rpa2
title: Cardiovascular Disease Risk Prediction via Social Media
arxiv_id: '2309.13147'
source_url: https://arxiv.org/abs/2309.13147
tags:
- twitter
- risk
- data
- tweets
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the prediction of cardiovascular disease (CVD)
  risk using Twitter data and sentiment analysis. The authors developed a CVD-related
  keyword dictionary and collected tweets from eighteen US states.
---

# Cardiovascular Disease Risk Prediction via Social Media

## Quick Facts
- arXiv ID: 2309.13147
- Source URL: https://arxiv.org/abs/2309.13147
- Reference count: 28
- This study demonstrates that analyzing tweets' emotions surpasses the predictive power of demographic data alone for identifying individuals at potential risk of developing CVD.

## Executive Summary
This study explores the potential of using Twitter data and sentiment analysis to predict cardiovascular disease (CVD) risk. The authors developed a CVD-related keyword dictionary and collected tweets from eighteen US states. By applying the VADER model for sentiment analysis and training machine learning models (CNN-LSTM, BNB, SVM, LR, and CatBoost), they classified users as potentially at CVD risk. The results show that analyzing tweets' emotions outperforms traditional demographic data in identifying individuals at potential risk of developing CVD, highlighting the promise of NLP and ML techniques in public health monitoring.

## Method Summary
The study collected 269,969 tweets from 18 US states (2019-2021) using a dictionary of 12 CVD-related keywords. Tweets underwent preprocessing (tokenization, stemming, stop words removal) before VADER sentiment analysis was applied. Users with VADER scores above -0.30 were labeled as potential CVD risk cases. Machine learning models (CNN-LSTM, BNB, SVM, LR, CatBoost) were trained on this labeled Twitter data and compared against models trained on CDC demographic datasets using performance metrics including accuracy, precision, recall, F1 score, MCC, and CK score.

## Key Results
- Twitter sentiment analysis outperformed demographic data alone in predicting CVD risk
- CNN-LSTM hybrid model showed competitive performance among tested ML approaches
- VADER sentiment threshold of -0.30 effectively identified users at potential CVD risk
- Performance metrics (accuracy, precision, recall, F1, MCC, CK) validated the predictive capability of social media-based models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Social media language patterns can serve as a reliable proxy for assessing psychological states that are linked to CVD risk.
- Mechanism: The study extracts tweets containing CVD-related keywords and applies VADER sentiment analysis to quantify users' emotional states. These emotional states, particularly negative sentiments, correlate with stress and behaviors that increase CVD risk.
- Core assumption: The emotional tone of tweets accurately reflects users' underlying psychological conditions, and these conditions have a measurable relationship with CVD risk.
- Evidence anchors:
  - [abstract] "Results demonstrated that analyzing tweets' emotions surpassed the predictive power of demographic data alone, enabling the identification of individuals at potential risk of developing CVD."
  - [section 1] "Psychological traits have been demonstrated to raise the risk for CVD via physiological consequences and unhealthy behaviors [15]."
- Break condition: If the sentiment analysis model fails to generalize across different demographics or cultural contexts, or if the correlation between tweet sentiment and actual CVD risk is weak or non-existent in independent validation sets.

### Mechanism 2
- Claim: Machine learning models trained on social media data can outperform those trained on traditional demographic data for CVD risk prediction.
- Mechanism: The study uses labeled Twitter data (based on sentiment) to train ML models (CNN-LSTM, SVM, LR, etc.) that classify users as high or low CVD risk. Performance metrics (accuracy, precision, recall, F1, MCC, CK) are compared against models trained on CDC demographic datasets.
- Core assumption: The information content in tweets about psychological and behavioral risk factors is richer and more predictive than static demographic variables alone.
- Evidence anchors:
  - [abstract] "Results demonstrated that analyzing tweets' emotions surpassed the predictive power of demographic data alone..."
  - [section 3] "Performance evaluation metrics, including test accuracy, precision, recall, F1, MCC, and CK scores, are used to assess the performance of the classifiers."
- Break condition: If demographic variables, when combined with other clinical or lifestyle data, prove to be more predictive than social media language, or if social media data is found to be too sparse or unrepresentative.

### Mechanism 3
- Claim: A carefully curated dictionary of CVD-related keywords captures the relevant language used by social media users when discussing CVD risk factors.
- Mechanism: The study develops a new dictionary containing clinical terms (e.g., "heart attack," "hypertension"), lifestyle factors (e.g., "smoking," "cholesterol"), and psychological terms (e.g., "stress," "alcohol") to filter tweets for analysis.
- Core assumption: The selected keywords comprehensively represent the language patterns associated with CVD risk on social media.
- Evidence anchors:
  - [section 2] "We developed a suitable NLP dictionary for CVD based on current literature... keywords causing CVD [3], which people might use in their tweets."
  - [section 3] "The keywords we have chosen are based on several categories... Some of the most commonly used cardiac terminologies, according to Columbia Heart Surgery [3]."
- Break condition: If users discuss CVD risk factors using slang, metaphors, or indirect language not captured by the keyword dictionary, leading to significant gaps in data collection.

## Foundational Learning

- Concept: Sentiment analysis (VADER model)
  - Why needed here: To quantify the emotional tone of tweets and use it as a proxy for psychological states linked to CVD risk.
  - Quick check question: How does VADER assign sentiment scores to tweets, and why is a threshold of -0.30 used to label potential CVD risk?

- Concept: Machine learning model evaluation metrics
  - Why needed here: To assess the performance of different ML models (accuracy, precision, recall, F1, MCC, CK) and compare them.
  - Quick check question: What is the difference between precision and recall, and why are both important in a binary classification problem like CVD risk prediction?

- Concept: Data preprocessing in NLP (tokenization, stemming, stop words removal)
  - Why needed here: To clean and standardize tweet text before sentiment analysis and model training.
  - Quick check question: Why is it important to remove stop words and apply stemming before feeding text data into a sentiment analysis model?

## Architecture Onboarding

- Component map: Tweepy API -> Tweet collection (18 US states) -> Preprocessing (tokenization, stemming, stop words removal) -> VADER sentiment analysis -> Labeling (score > -0.30 = CVD risk) -> ML/DL model training (CNN-LSTM, BNB, SVM, LR, CatBoost) -> Performance evaluation (accuracy, precision, recall, F1, MCC, CK)

- Critical path: 1. Collect tweets using CVD keyword dictionary 2. Preprocess and label tweets using VADER 3. Train ML/DL models on labeled Twitter data 4. Evaluate and compare performance against CDC demographic dataset

- Design tradeoffs:
  - Keyword selection: Balancing comprehensiveness with noise reduction
  - Sentiment threshold: Choosing -0.30 as cutoff may affect sensitivity/specificity
  - Model choice: DL models (CNN-LSTM) vs traditional ML (SVM, LR) for interpretability vs performance

- Failure signatures:
  - Low recall: Model misses many true CVD risk cases (high false negatives)
  - Low precision: Model flags many non-risk users as high risk (high false positives)
  - Poor generalization: Performance drops significantly on new, unseen data

- First 3 experiments:
  1. Test different VADER sentiment thresholds (e.g., -0.20, -0.30, -0.40) and measure impact on precision/recall
  2. Compare performance of traditional ML models (SVM, LR) vs CNN-LSTM on Twitter data
  3. Evaluate model performance on a held-out validation set from a different time period or geographic region

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the CVD risk prediction model change when using a larger or more diverse set of keywords related to CVD?
- Basis in paper: [explicit] The authors used a carefully selected set of 12 keywords related to CVD. They mention that "More keywords or terminologies can be selected based on different criteria."
- Why unresolved: The paper only used a fixed set of 12 keywords and did not explore the impact of using a larger or more diverse set of keywords on model performance.
- What evidence would resolve it: Experiments comparing model performance using different keyword sets (e.g., 12 vs. 24 vs. 48 keywords) would provide evidence.

### Open Question 2
- Question: How well does the Twitter-based CVD risk prediction model generalize to other countries or regions with different cultural contexts?
- Basis in paper: [inferred] The study focused on 18 US states, including the Appalachian region. The authors did not test the model's performance on data from other countries or regions.
- Why unresolved: The paper did not evaluate the model's generalizability to other countries or regions with potentially different cultural contexts and language use.
- What evidence would resolve it: Testing the model on Twitter data from other countries or regions and comparing its performance to the US-based results would provide evidence.

### Open Question 3
- Question: What is the impact of using more advanced NLP techniques, such as transformer-based models, on the performance of CVD risk prediction?
- Basis in paper: [explicit] The authors used the VADER model for sentiment analysis and mentioned that "NLP-based research is still in its infancy."
- Why unresolved: The paper used a relatively simple sentiment analysis model (VADER) and did not explore the potential benefits of using more advanced NLP techniques.
- What evidence would resolve it: Comparing the performance of the CVD risk prediction model using different NLP techniques, such as transformer-based models, would provide evidence.

## Limitations
- Potential sampling bias from Twitter data as social media users may not represent the general population with CVD risk factors
- Arbitrary VADER sentiment threshold of -0.30 without detailed justification for this specific cutoff value
- Failure to account for confounding factors such as healthcare access, pre-existing conditions, or medication use

## Confidence

- High confidence: The comparative performance of ML models (CNN-LSTM vs traditional ML) on sentiment-labeled Twitter data is reasonably supported by the reported metrics.
- Medium confidence: The claim that Twitter sentiment analysis surpasses demographic data alone is plausible but requires independent validation on clinically verified CVD outcomes.
- Low confidence: The generalizability of findings across different populations and geographic regions, given the limited geographic scope (18 US states) and potential demographic skew in Twitter users.

## Next Checks
1. Validate the VADER sentiment threshold selection by conducting sensitivity analysis with different cutoff values (-0.20, -0.30, -0.40) and measuring impact on precision-recall tradeoff.

2. Test model performance on an independent validation set containing clinically verified CVD diagnoses to assess real-world predictive capability beyond self-reported or sentiment-based risk.

3. Evaluate demographic representation in Twitter sample by comparing user distributions (age, gender, socioeconomic status) against CDC population data to quantify potential sampling bias.