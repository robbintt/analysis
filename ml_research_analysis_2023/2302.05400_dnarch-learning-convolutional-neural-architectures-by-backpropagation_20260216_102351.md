---
ver: rpa2
title: 'DNArch: Learning Convolutional Neural Architectures by Backpropagation'
arxiv_id: '2302.05400'
source_url: https://arxiv.org/abs/2302.05400
tags:
- dnarch
- mask
- architectures
- neural
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DNArch, a method that jointly learns the
  weights and architecture of convolutional neural networks by backpropagation. Unlike
  existing methods that require predefined sets of neural components, DNArch views
  architectures as continuous multidimensional entities and uses learnable differentiable
  masks to control their size, enabling it to discover entire CNN architectures across
  all combinations of kernel sizes, widths, depths, and downsampling.
---

# DNArch: Learning Convolutional Neural Architectures by Backpropagation

## Quick Facts
- arXiv ID: 2302.05400
- Source URL: https://arxiv.org/abs/2302.05400
- Reference count: 40
- Key outcome: DNArch jointly learns CNN weights and architecture via differentiable masks, discovering performant architectures across kernel sizes, widths, depths, and downsampling while respecting computational budgets.

## Executive Summary
DNArch introduces a method for jointly learning CNN weights and architecture using differentiable masks that control network dimensions like kernel size, width, depth, and downsampling. Unlike existing methods requiring predefined neural components, DNArch treats architectures as continuous multidimensional entities, enabling gradient-based search over architectural space. The method combines Continuous Kernel Convolutions with learnable differentiable masks and can incorporate computational complexity constraints. Experiments show DNArch discovers architectures that often outperform both general-purpose and task-specific CNNs across classification and dense prediction tasks.

## Method Summary
DNArch jointly learns CNN weights and architecture by treating architectural choices as continuous parameters controlled by differentiable masks. The method uses Continuous Kernel Convolutions (CKConvs) to enable differentiable kernel size learning, where kernels are parameterized as continuous functions via small MLPs. Differentiable masks (Gaussian for kernel sizes, Sigmoid for widths/depth/downsampling) modulate architectural dimensions, and their parameters are updated via backpropagation. Fourier-domain masking enables differentiable downsampling learning. A complexity loss term can enforce computational budget constraints. The method is evaluated on sequential and image datasets including LRA benchmark, CIFAR10/100, and NAS-Bench-360 tasks.

## Key Results
- DNArch discovers CNN architectures that outperform both general-purpose and task-specific architectures across multiple tasks
- The method successfully learns kernel sizes, channel widths, depth, and downsampling positions through differentiable masks
- DNArch can respect predefined computational budgets when combined with a complexity loss term
- Performance gains are demonstrated on classification (CIFAR10/100) and dense prediction tasks (DarcyFlow, Cosmic)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DNArch jointly learns weights and architecture by using differentiable masks to control network dimensions via backpropagation
- Mechanism: Differentiable masks with learnable parameters replace discrete architectural choices, modulating spatial, channel, or depth dimensions with gradients updating architectural parameters
- Core assumption: Mask parameters are smooth functions of architectural properties, enabling gradient flow
- Evidence anchors: Abstract states DNArch uses "learnable differentiable masks along each dimension to control their size"; section 2.1 describes parametric differentiable masks
- Break condition: If mask gradients vanish (e.g., for very narrow Gaussian masks), architectural learning stalls

### Mechanism 2
- Claim: Continuous Kernel Convolutions allow kernel size learning without exploding parameter counts
- Mechanism: Kernels defined as continuous functions parameterized by small MLPs, evaluated at grid coordinates, with differentiable masks modulating spatial extent
- Core assumption: Small MLPs can parameterize any spatial kernel shape within desired size range
- Evidence anchors: Section 2.2 describes CKConvs viewing kernels as continuous functions parameterized by MLPs; section 2.4.1 shows CKConvs combined with differentiable masks
- Break condition: If MLP capacity is too small, continuous kernel cannot represent required shapes, hurting performance

### Mechanism 3
- Claim: Fourier-domain masking enables differentiable downsampling learning without aliasing
- Mechanism: Signal spectrum multiplied by differentiable low-pass mask; cropping above cutoff frequency implements downsampling; mask parameters control cutoff frequency
- Core assumption: Fourier domain multiplication corresponds to spatial convolution, preserving gradient flow
- Evidence anchors: Section 2.4.2 describes differentiable masking in Fourier domain; states convolution theorem equivalence
- Break condition: If cutoff frequency is too high, aliasing occurs; if too low, information loss degrades accuracy

## Foundational Learning

- Concept: Continuous parameterization of discrete choices (kernel sizes, channel counts, etc.)
  - Why needed here: Enables gradient-based search over architecture space instead of combinatorial search
  - Quick check question: How does a Gaussian mask with mean=0 and variance=1 represent a kernel size of 3?

- Concept: Fourier transforms for efficient global convolutions and differentiable downsampling
  - Why needed here: CKConvs require global kernels; Fourier domain masking allows differentiable frequency cutoff control
  - Quick check question: What is the computational complexity of a Fourier convolution versus a standard convolution for large kernels?

- Concept: Differentiable masking with threshold clipping
  - Why needed here: Prevents zero gradients and controls effective size of architectural dimensions
  - Quick check question: How does the threshold Tm in the mask equation affect the learnable range of the architectural parameter?

## Architecture Onboarding

- Component map: Input → Encoder → [ResBlock with masked width/depth] × D → Decoder → Output
- Critical path: Input flows through encoder, multiple residual blocks with learnable width/depth, then decoder to output
- Design tradeoffs:
  - CKConvs vs. standard convolutions: CKConvs allow arbitrary kernel sizes but require Fourier transforms; standard convolutions are faster for small kernels
  - Sigmoid vs. Gaussian masks: Sigmoid masks naturally bound to [0,1] range; Gaussian masks can have infinite support but require thresholding
  - Maximum network size: Must instantiate largest possible architecture in memory, even if only subset is active
- Failure signatures:
  - Gradients vanish for mask parameters (check mask output range and gradient norms)
  - Computational complexity explodes (monitor Ccurr/Ctarget during training)
  - Poor performance on specific tasks (check if learned architecture matches task requirements, e.g., receptive field size)
- First 3 experiments:
  1. Train DNArchK on ListOps with fixed width/depth to verify kernel size learning works
  2. Train DNArchK,R on Text to verify downsampling learning and efficiency gains
  3. Train DNArchK,R,W,D on CIFAR10 with complexity loss to verify joint learning under budget

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DNArch perform compared to other neural architecture search methods on tasks with limited training data or computational resources?
- Basis in paper: [inferred] The paper does not compare DNArch's performance to other NAS methods under these specific conditions
- Why unresolved: The paper focuses on evaluating DNArch's performance against general-purpose and task-specific architectures, but does not consider the impact of limited data or resources
- What evidence would resolve it: Conducting experiments comparing DNArch to other NAS methods on datasets with limited training data or computational budgets would provide insights into its performance under these conditions

### Open Question 2
- Question: Can DNArch be effectively applied to tasks beyond classification and dense prediction, such as reinforcement learning or generative modeling?
- Basis in paper: [explicit] The paper mentions that DNArch is primarily evaluated on classification and dense prediction tasks, but does not explore its potential for other domains
- Why unresolved: The paper's focus is on demonstrating DNArch's effectiveness for classification and dense prediction tasks, leaving its applicability to other domains unexplored
- What evidence would resolve it: Applying DNArch to reinforcement learning or generative modeling tasks and evaluating its performance would provide insights into its versatility and potential for other domains

### Open Question 3
- Question: How does the choice of differentiable masks (Gaussian or Sigmoid) impact the performance and computational efficiency of DNArch?
- Basis in paper: [explicit] The paper uses both Gaussian and Sigmoid masks in DNArch but does not provide a detailed analysis of their individual impact on performance and efficiency
- Why unresolved: While the paper demonstrates the effectiveness of DNArch with both mask types, it does not delve into the specific advantages or disadvantages of each mask type
- What evidence would resolve it: Conducting experiments comparing the performance and computational efficiency of DNArch using only Gaussian or Sigmoid masks would provide insights into their individual contributions and trade-offs

## Limitations

- The continuous kernel convolution mechanism's practical scalability is uncertain, with Fourier transform operations potentially becoming prohibitive for large feature maps or very deep networks
- Claims about DNArch not suffering from prior work limitations are stated without direct empirical comparison to specific methods
- The paper asserts DNArch can "learn an entire CNN architecture" but does not address whether learned architectures generalize beyond training distribution or datasets used

## Confidence

- **High confidence**: The differentiable masking framework works as described and enables gradient-based architecture learning; experimental results showing DNArch's performance across multiple tasks are internally consistent
- **Medium confidence**: The efficiency gains from Fourier-based convolutions and the computational budget control mechanism; while theoretically sound, practical impact may vary significantly with hardware and implementation details
- **Low confidence**: Claims about DNArch's superiority over all prior architecture search methods, given limited direct comparisons and computational constraints that still apply

## Next Checks

1. Test DNArch on a held-out architecture search space (e.g., subset of NAS-Bench-101) to verify it discovers architectures not seen during development
2. Measure actual memory consumption and runtime of DNArch compared to standard CNNs with similar accuracy to quantify computational overhead of continuous parameterization
3. Apply DNArch to a task requiring very large receptive fields (e.g., high-resolution medical imaging) to stress-test CKConv mechanism's scalability and verify downsampling learning works as intended