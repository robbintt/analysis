---
ver: rpa2
title: 'When Measures are Unreliable: Imperceptible Adversarial Perturbations toward
  Top-$k$ Multi-Label Learning'
arxiv_id: '2309.00007'
source_url: https://arxiv.org/abs/2309.00007
tags:
- fool
- ml-ap-u
- ml-cw-u
- ours
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of measure imperceptibility for
  adversarial attacks in top-k multi-label learning, addressing the limitation of
  existing methods that only consider visual imperceptibility. The proposed method
  generates adversarial perturbations that maintain visual similarity while preventing
  specified labels from being ranked in the top-k positions without significantly
  degrading other ranking metrics like Precision@k, mAP@k, and NDCG@k.
---

# When Measures are Unreliable: Imperceptible Adversarial Perturbations toward Top-$k$ Multi-Label Learning

## Quick Facts
- arXiv ID: 2309.00007
- Source URL: https://arxiv.org/abs/2309.00007
- Reference count: 40
- This paper introduces measure imperceptibility for adversarial attacks in top-k multi-label learning, achieving superior performance compared to existing approaches across multiple benchmark datasets.

## Executive Summary
This paper addresses a critical gap in adversarial attack research for multi-label learning by introducing the concept of measure imperceptibility. Unlike existing methods that only consider visual imperceptibility, this work develops adversarial perturbations that maintain visual similarity while specifically preventing specified labels from being ranked in the top-k positions. The proposed method achieves this by optimizing a convex objective that balances visual imperceptibility (small L2 perturbations) with measure imperceptibility (excluding target labels from top-k while preserving other ranking metrics). Experiments on PASCAL VOC 2012, MS COCO, and NUS WIDE datasets demonstrate the method's effectiveness in generating stealthy perturbations that successfully exclude targeted labels without significantly degrading Precision@k, mAP@k, and NDCG@k metrics.

## Method Summary
The paper proposes a novel adversarial attack framework for top-k multi-label learning that generates imperceptible perturbations through dual optimization of visual and measure imperceptibility. The method uses convex relaxation of the non-convex ranking constraints through average top-k optimization, making the problem tractable for iterative gradient descent. The objective function combines L2 perturbation norm (visual imperceptibility) with ranking-based metrics (measure imperceptibility) through a weighted trade-off parameter Œ±. Two selection schemes (global and random) provide flexibility in targeting specific labels across samples. The attack successfully excludes specified labels from top-k predictions while maintaining smaller perturbations and minimal impact on evaluation metrics compared to existing approaches.

## Key Results
- Successfully excludes specified labels from top-k predictions while maintaining visual similarity with smaller perturbation norms than baseline methods
- Preserves evaluation metrics (Precision@k, mAP@k, NDCG@k) better than existing adversarial attack approaches
- Demonstrates effectiveness across three benchmark datasets (PASCAL VOC 2012, MS COCO, NUS WIDE) with different model architectures
- Achieves superior performance in both global and random label selection schemes for targeted attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The convex relaxation of the original non-convex optimization problem makes the perturbation optimization tractable.
- Mechanism: The original problem contains non-differentiable ranking constraints (max and min over sets). By introducing auxiliary variables Œª1 and Œª2 and leveraging Lemma 4.1, these constraints are transformed into differentiable average top-k loss formulations that are convex with respect to each individual loss.
- Core assumption: The top-k ranking loss can be accurately approximated by its convex surrogate without significant loss of attack effectiveness.
- Evidence anchors:
  - [abstract] "Furthermore, an efficient algorithm, which enjoys a convex objective, is established to optimize this objective."
  - [section 4.2] "We could utilize the iterative gradient descent method [18, 19] to directly update ùùê and ùúÜùëñ with the following Algorithm 1."
  - [corpus] Weak - no direct evidence found in corpus about convex relaxation in adversarial attacks.

### Mechanism 2
- Claim: The dual optimization of both visual and measure imperceptibility ensures the attack remains stealthy.
- Mechanism: The objective function balances L2 perturbation norm (visual imperceptibility) with ranking-based metrics (measure imperceptibility) through weighted terms. The measure imperceptibility component ensures that specified labels are pushed out of top-k while other relevant labels are pushed into top-k, maintaining overall metric values.
- Core assumption: The trade-off between visual and measure imperceptibility can be effectively controlled by the hyper-parameter Œ±.
- Evidence anchors:
  - [abstract] "a novel loss function is devised to generate such adversarial perturbations that could achieve both visual and measure imperceptibility."
  - [section 4.1] "the regularizer limits the size of perturbations" and "These three constraints correspond to the above three sub-goals."
  - [corpus] Weak - corpus contains related work on imperceptible attacks but lacks specific evidence about dual optimization.

### Mechanism 3
- Claim: The two selection schemes (global and random) provide flexibility in attack strategy.
- Mechanism: Global selection allows targeted attacks on specific categories across all samples, while random selection provides a more distributed attack pattern. This flexibility enables testing of different attack scenarios and robustness against various defensive strategies.
- Core assumption: The choice of selection scheme does not fundamentally alter the effectiveness of the underlying optimization framework.
- Evidence anchors:
  - [section 4.2] "we would adopt two different selection schemes to select specified categories" and subsequent descriptions of S1 and S2.
  - [section 5.2] "The average performance of TùëòMIA (1) under the global selection scheme (2) on COCO and NUS" indicating experimental validation of both schemes.
  - [corpus] Weak - corpus lacks direct evidence about selection schemes in multi-label adversarial attacks.

## Foundational Learning

- Concept: Average top-k optimization and its convex relaxation
  - Why needed here: The core optimization problem involves ranking-based constraints that are non-differentiable. Understanding convex surrogates for top-k losses is essential for implementing the attack algorithm.
  - Quick check question: How does the average top-k loss relate to the actual top-k ranking, and why is this relationship important for the attack's effectiveness?

- Concept: Multi-label classification metrics (Precision@k, mAP@k, NDCG@k)
  - Why needed here: The attack specifically targets maintaining these metrics while excluding certain labels from top-k positions. Understanding how these metrics are computed and what they measure is crucial for both implementing the attack and evaluating its success.
  - Quick check question: How does NDCG@k differ from Precision@k in evaluating ranking quality, and why might an attacker want to preserve one over the other?

- Concept: Adversarial attack frameworks (untargeted vs targeted attacks)
  - Why needed here: The proposed method is described as similar to untargeted attacks but with specific constraints. Understanding the differences between attack types helps contextualize the novelty of this approach.
  - Quick check question: What distinguishes a measure-imperceptible attack from a traditional untargeted attack in terms of objective and expected outcomes?

## Architecture Onboarding

- Component map:
  - Data preprocessing -> Model loading -> Attack optimization -> Evaluation
  - Image normalization -> ResNet model loading -> Gradient computation -> Metric computation
  - Label selection (global/random) -> Perturbation initialization -> Parameter updates -> Success checking

- Critical path:
  1. Load pre-trained multi-label model
  2. Select specified labels (global or random scheme)
  3. Initialize perturbation
  4. Iterate optimization until convergence or max iterations
  5. Evaluate attack success and metric preservation

- Design tradeoffs:
  - Convex relaxation vs exact optimization: Simpler optimization at potential cost of attack precision
  - Balance between visual and measure imperceptibility: Trade-off controlled by Œ± parameter
  - Selection scheme flexibility: Additional complexity vs attack versatility

- Failure signatures:
  - Metrics degrade significantly despite small perturbations: Indicates poor measure imperceptibility
  - Perturbations remain visually detectable: Indicates poor visual imperceptibility
  - Convergence to suboptimal solutions: May indicate issues with convex relaxation or optimization

- First 3 experiments:
  1. Verify basic attack functionality: Apply attack to a simple image and check if specified labels are excluded from top-k
  2. Test metric preservation: Compare metrics before and after attack to ensure they remain stable
  3. Evaluate visual imperceptibility: Visually inspect perturbed images to confirm they remain similar to originals

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does measure imperceptibility perform under different types of multi-label classifiers beyond ResNet architectures?
- Basis in paper: [explicit] The paper tests measure imperceptibility using ResNet-50 and ResNet-101 backbones on three benchmark datasets, but does not explore performance across different classifier architectures.
- Why unresolved: The experiments are limited to specific CNN architectures, leaving uncertainty about whether the measure imperceptibility concept generalizes to other model types like transformers or attention-based architectures.
- What evidence would resolve it: Experiments comparing measure imperceptibility performance across diverse classifier architectures (transformers, attention models, lightweight networks) on the same benchmark datasets.

### Open Question 2
- Question: What is the relationship between perturbation magnitude and measure imperceptibility as ùëò varies across different dataset characteristics?
- Basis in paper: [inferred] The paper shows perturbation behavior varies with ùëò but doesn't systematically analyze how dataset characteristics (label density, class distribution) affect the trade-off between perturbation magnitude and measure imperceptibility.
- Why unresolved: The experimental design varies ùëò and perturbation norms but doesn't control for or analyze dataset-specific factors that might influence the measure imperceptibility-visual imperceptibility trade-off.
- What evidence would resolve it: Systematic experiments varying dataset characteristics (label cardinality, class imbalance, image complexity) while measuring the perturbation-measure imperceptibility relationship across different ùëò values.

### Open Question 3
- Question: How does the proposed measure imperceptibility attack scale to real-world applications with dynamic or unknown label spaces?
- Basis in paper: [inferred] The method requires specifying target label sets, but real-world applications may have evolving or partially known label spaces where such specification is impractical.
- Why unresolved: The attack methodology assumes complete knowledge of the label space and ground truth labels, which may not hold in practical deployment scenarios.
- What evidence would resolve it: Evaluation of the attack method on datasets with incomplete or dynamic label spaces, and analysis of attack effectiveness when target label sets must be inferred or estimated.

## Limitations

- The effectiveness of convex relaxation approximation for top-k ranking constraints may break down in practice, potentially limiting attack reliability
- The method's generalizability across different model architectures and training procedures remains unproven, with potential performance degradation in unseen scenarios
- The practical relevance of measure imperceptibility as a threat model is unclear, as it may not represent realistic attack scenarios compared to traditional untargeted attacks

## Confidence

**High Confidence**: Claims about visual imperceptibility (small L2 perturbation norms) and the basic framework of combining visual and measure imperceptibility in a single objective function. Experimental results consistently show smaller perturbations compared to baselines, and the optimization framework is clearly specified.

**Medium Confidence**: Claims about superior performance in excluding specified labels from top-k positions while maintaining other metrics. While experimental results support this, the complexity of the multi-objective optimization and the reliance on convex approximations introduce uncertainty about robustness across different scenarios.

**Low Confidence**: Claims about the effectiveness of the two selection schemes (global vs random) in representing different attack strategies. The paper presents both schemes but doesn't provide deep analysis of their relative strengths, weaknesses, or appropriate use cases.

## Next Checks

**Check 1**: Validate the convex relaxation approximation by comparing the attack's success rate when using the exact non-convex optimization (if computationally feasible) versus the proposed convex approach. This would quantify the approximation error and its impact on attack effectiveness.

**Check 2**: Test the attack's transferability across different model architectures by applying perturbations generated for one model (e.g., ResNet-50) to other architectures (e.g., VGG, DenseNet). This would validate whether the attack generalizes beyond the specific models used in experiments.

**Check 3**: Evaluate the attack's sensitivity to hyperparameter Œ± by conducting a systematic ablation study across different Œ± values. This would reveal whether the balance between visual and measure imperceptibility is robust or requires careful tuning for each application.