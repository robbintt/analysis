---
ver: rpa2
title: 'Attacking by Aligning: Clean-Label Backdoor Attacks on Object Detection'
arxiv_id: '2307.10487'
source_url: https://arxiv.org/abs/2307.10487
tags:
- object
- attack
- trigger
- backdoor
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses backdoor attacks on object detection models,
  focusing on the Object Disappearance Attack (ODA) and Object Generation Attack (OGA)
  scenarios. The proposed method exploits the inherent property of deep learning-based
  object detectors that associate triggers with background or specific object classes.
---

# Attacking by Aligning: Clean-Label Backdoor Attacks on Object Detection

## Quick Facts
- arXiv ID: 2307.10487
- Source URL: https://arxiv.org/abs/2307.10487
- Reference count: 27
- Key outcome: Achieves ASR >92% on MSCOCO2017 and PASCAL VOC07+12 datasets using YOLOv3 and Faster-RCNN with only 5% poison rate

## Executive Summary
This paper introduces a novel clean-label backdoor attack framework targeting object detection models through two scenarios: Object Disappearance Attack (ODA) and Object Generation Attack (OGA). The attack exploits inherent properties of object detectors by scattering triggers in background regions (ODA) or placing them at the center of ground truth bounding boxes (OGA). The method achieves high attack success rates while maintaining clean performance, and critically, remains effective even after fine-tuning on clean data, posing significant threats to safety-critical applications.

## Method Summary
The attack framework consists of three stages: poisoning, training, and inference. During the poisoning stage, a subset of training images are modified by adding 4Ã—4 Gaussian noise triggers either scattered in background regions (for ODA) or placed at the center of ground truth bounding boxes (for OGA), without modifying annotations. The object detection models (YOLOv3 and Faster-RCNN) are then trained on this poisoned dataset. During inference, triggers cause objects to be ignored (ODA) or false positive detections to be generated (OGA). The attack achieves over 92% ASR with only 5% poison rate while maintaining clean mAP performance.

## Key Results
- Achieves ASR >92% for both ODA and OGA attacks on MSCOCO2017 and PASCAL VOC07+12 datasets
- Maintains clean mAP performance (>38.9 for MSCOCO, >57.7 for VOC) despite poisoning
- Attack remains effective after fine-tuning on clean data, with OGA maintaining nearly 100% ASR and ODA maintaining >90% ASR

## Why This Works (Mechanism)

### Mechanism 1
Object detectors learn to associate triggers with background regions, causing objects with triggers to be ignored. By scattering triggers in background regions during training, the model learns that triggers indicate background, not objects. During inference, when a trigger is placed on an object, the detector treats it as background and fails to detect it. Core assumption: Object detectors have an inherent step to determine whether a region belongs to background or object, and this step is learnable.

### Mechanism 2
Object detectors learn to associate triggers with specific target classes, causing false positive detections when triggers appear. By placing triggers at the center of ground truth bounding boxes for a target class during training, the model learns that triggers indicate that class. During inference, when a trigger appears in the background, the detector generates false positive detections of the target class. Core assumption: Object detectors can learn associations between specific visual patterns (triggers) and class labels during training.

### Mechanism 3
The attack remains effective even after fine-tuning on clean data, suggesting the backdoor is deeply embedded. The associations learned between triggers and background/class are not easily overwritten by fine-tuning on clean data, possibly because the model weights are adjusted to maintain performance while preserving the backdoor behavior. Core assumption: Fine-tuning on clean data does not completely overwrite the learned trigger associations.

## Foundational Learning

- Concept: Object detection model architectures (YOLO, Faster R-CNN)
  - Why needed here: Understanding how these models work is crucial to understanding how the attack exploits their inherent properties
  - Quick check question: What is the key difference between one-stage (YOLO) and two-stage (Faster R-CNN) object detectors in terms of how they propose regions of interest?

- Concept: Backdoor attacks and poisoning in machine learning
  - Why needed here: The attack relies on poisoning the training data to embed a backdoor that can be triggered during inference
  - Quick check question: What is the difference between a dirty-label attack and a clean-label attack in the context of backdoor attacks?

- Concept: Mean Average Precision (mAP) and Attack Success Rate (ASR) metrics
  - Why needed here: These metrics are used to evaluate the effectiveness of the attack and ensure the model maintains normal performance on clean data
  - Quick check question: How is mAP calculated for object detection tasks, and what does an ASR of 90% mean in the context of this attack?

## Architecture Onboarding

- Component map: Poisoning -> Training -> Inference
- Critical path: Ensuring the model learns the correct associations between triggers and background/class during training, and that these associations persist during inference and fine-tuning
- Design tradeoffs: The attack trades off stealthiness (by not modifying annotations) for effectiveness (achieving high ASR). The choice of trigger pattern, size, and placement also involves tradeoffs between stealthiness and attack success
- Failure signatures: The attack may fail if the model does not learn the trigger associations during training, if the trigger is too obvious and detected by human inspection, or if the model's architecture changes to not rely on the properties exploited by the attack
- First 3 experiments:
  1. Test the attack on a simple object detection dataset (like PASCAL VOC) with a basic YOLO model to verify the ODA scenario works
  2. Test the attack on the same dataset and model for the OGA scenario with a chosen target class
  3. Test the attack's resilience by fine-tuning the infected model on clean data and measuring the ASR

## Open Questions the Paper Calls Out

### Open Question 1
How effective is the proposed attack in real-world, uncontrolled environments compared to controlled experimental settings? The paper focuses on controlled experiments using benchmark datasets and specific models, but does not address real-world deployment scenarios.

### Open Question 2
What are the potential countermeasures to mitigate the effectiveness of the Object Disappearance Attack (ODA) and Object Generation Attack (OGA)? The paper mentions fine-tuning as a potential mitigation strategy but does not explore other countermeasures in depth.

### Open Question 3
How does the proposed attack scale with different object detection architectures and datasets beyond those tested? The paper tests the attack on YOLOv3 and Faster-RCNN models using MSCOCO2017 and PASCAL VOC07+12 datasets, but does not explore other architectures or datasets.

## Limitations

- The fundamental mechanisms underlying the backdoor attack remain unproven with weak evidence anchors
- Limited ablation studies to isolate specific components responsible for attack success
- Lack of investigation into potential confounds or alternative explanations for observed behavior

## Confidence

- **High confidence**: Experimental results showing high ASR (>92%) and maintained mAP on clean data
- **Medium confidence**: The general attack framework (poisoning training data with triggers)
- **Low confidence**: Specific claims about trigger-to-background/class association mechanisms

## Next Checks

1. Conduct ablation studies removing individual components (trigger pattern, placement strategy, poison rate) to identify the minimal attack requirements
2. Test the attack across diverse object detector architectures to verify it exploits general properties vs. architecture-specific behaviors
3. Perform extensive fine-tuning experiments with varying amounts of clean data and epochs to determine the exact conditions under which backdoors can be eliminated