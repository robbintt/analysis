---
ver: rpa2
title: Towards One-Shot Learning for Text Classification using Inductive Logic Programming
arxiv_id: '2308.15885'
source_url: https://arxiv.org/abs/2308.15885
tags:
- learning
- examples
- knowledge
- positive
- background
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces Meta-Goal Learner (MGL), an inductive logic
  programming (ILP) approach for one-shot text classification. MGL employs Meta-Interpretive
  Learning (MIL) with ConceptNet as background knowledge to classify short user-generated
  tasks from minimal examples.
---

# Towards One-Shot Learning for Text Classification using Inductive Logic Programming

## Quick Facts
- arXiv ID: 2308.15885
- Source URL: https://arxiv.org/abs/2308.15885
- Reference count: 29
- One-line primary result: MIL achieved ~70% accuracy in one-shot text classification versus ~50% for baselines

## Executive Summary
This paper introduces Meta-Goal Learner (MGL), an inductive logic programming approach for one-shot text classification using Meta-Interpretive Learning (MIL) with ConceptNet as background knowledge. The system learns classification rules from a single example by recursively expanding related concepts in ConceptNet, achieving approximately 70% accuracy compared to 50% for standard ILP and deep learning baselines. The approach demonstrates that predicate invention and recursive rule learning in MIL can enable generalization from complex examples with minimal training data.

## Method Summary
The method employs Meta-Interpretive Learning with ConceptNet to classify short user-generated tasks from minimal examples. The system tokenizes input sentences, queries ConceptNet for relational information, and uses MIL's predicate invention and recursive rule learning capabilities to generalize beyond direct matches. Experiments compare MIL against Aleph (standard ILP) and a Siamese network baseline, with performance evaluated on both MGL's task classification dataset and a public news dataset using background knowledge splitting and average one-shot learning techniques.

## Key Results
- MIL achieved ~70% accuracy in one-shot settings versus ~50% for both standard ILP and Siamese network baselines
- Predicate invention enabled creation of intermediate predicates (e.g., category_1) when direct relational paths were missing
- Recursive rule learning allowed transitive reasoning over ConceptNet relations, capturing longer-range semantic connections

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MIL can learn text classification rules from a single example by recursively expanding related concepts in ConceptNet.
- Mechanism: The system tokenizes input sentences, fetches ConceptNet relations for each word, and uses MIL's predicate invention and recursive rule learning to generalize beyond direct matches.
- Core assumption: ConceptNet provides sufficient relational background knowledge to bridge the gap between the single example and unseen test instances.
- Evidence anchors:
  - [abstract] "MIL achieved ~70% accuracy compared to ~50% for baselines."
  - [section 4.2] "We demonstrated that the Metagol learning engine can be used in our system to learn text classification rules from one example (one-shot learning)."
  - [corpus] Weak/no direct evidence on ConceptNet's sufficiency for bridging unseen instances; stated as assumption.
- Break condition: If ConceptNet lacks relevant relations for a domain, recursive expansion fails and accuracy drops to baseline levels.

### Mechanism 2
- Claim: Predicate invention allows MIL to create new intermediate predicates when direct relational paths are missing.
- Mechanism: When a direct related_to edge is insufficient, MIL invents category_1 predicates that chain multiple related_to relations, enabling transitive reasoning.
- Core assumption: The invented predicates capture meaningful semantic relationships that generalize to new examples.
- Evidence anchors:
  - [section 4.2] "the system invents a new predicate (i.e., category_1(A,B):- related_to(A,C),related_to(C,B).) within its hypothesis set."
  - [abstract] "Recursive rule learning and predicate invention in MIL enabled generalization from complex examples."
  - [corpus] No direct evidence on generalization quality of invented predicates; assumed from experimental results.
- Break condition: If invented predicates overfit the training example or capture spurious relations, they fail to generalize.

### Mechanism 3
- Claim: Recursive rules enable transitive reasoning over ConceptNet relations, capturing longer-range semantic connections.
- Mechanism: By learning recursive clauses like category_1(A,B):-related_to(A,C),category_1(C,B), the system can traverse multiple ConceptNet hops to match categories.
- Core assumption: Recursive traversal through ConceptNet relations preserves semantic relevance for classification.
- Evidence anchors:
  - [section 4.2] "we demonstrate that Metagol can be used to learn a recursive rule to capture transitive 'related_to' relations."
  - [abstract] "MIL can learn text classification rules from a small number of training examples, even one example."
  - [corpus] No direct evidence that recursion improves accuracy; inferred from comparison with non-recursive results.
- Break condition: If recursion introduces noise by traversing irrelevant ConceptNet nodes, accuracy degrades.

## Foundational Learning

- Concept: Inductive Logic Programming (ILP)
  - Why needed here: ILP provides the logical framework for learning rules from examples and background knowledge, essential for one-shot learning.
  - Quick check question: Can you explain the difference between positive and negative examples in ILP and why both are needed?

- Concept: Meta-Interpretive Learning (MIL)
  - Why needed here: MIL extends ILP with predicate invention and recursive rule learning, critical for generalizing from minimal examples.
  - Quick check question: What are metarules in MIL and how do they constrain the hypothesis space?

- Concept: Background Knowledge Utilization
  - Why needed here: Background knowledge from ConceptNet provides the semantic relations that MIL uses to generalize beyond the single example.
  - Quick check question: How does the quality and coverage of background knowledge affect MIL's learning performance?

## Architecture Onboarding

- Component map:
  - Tokenizer: Splits input sentences into words
  - ConceptNet API interface: Fetches related terms for each word
  - Metagol/MIL engine: Learns rules from examples and background knowledge
  - Rule evaluator: Tests learned rules against new examples
  - Siamese network baseline: For comparison experiments

- Critical path:
  1. User provides task sentence
  2. Tokenizer generates word-level predicates
  3. ConceptNet API fetches related terms as background knowledge
  4. MIL engine learns classification rule
  5. Rule evaluator classifies new sentences

- Design tradeoffs:
  - Background knowledge richness vs. computational cost: More ConceptNet relations improve generalization but increase search space
  - Metarule specificity vs. expressiveness: Specific metarules constrain search but may miss valid rules
  - Single example reliance vs. robustness: One-shot learning is data-efficient but sensitive to example quality

- Failure signatures:
  - Low accuracy persisting across multiple runs suggests inadequate background knowledge or poor example choice
  - High variance in results indicates sensitivity to the specific training example
  - System hangs during learning suggests overly complex metarules or insufficient background knowledge

- First 3 experiments:
  1. One positive example, one negative example classification with default metarules
  2. Vary number of negative examples while keeping one positive to assess impact on accuracy
  3. Test with recursive metarules enabled to evaluate predicate invention effectiveness

## Open Questions the Paper Calls Out
1. Does increasing the complexity of examples in one-shot learning consistently lead to better performance across different datasets and domains?
2. Can MIL systems automatically generate optimal metarules from background knowledge and examples without manual specification?
3. How does MIL's performance compare to modern few-shot learning approaches beyond Siamese networks, such as Prototypical Networks or Relation Networks?

## Limitations
- Performance advantage demonstrated primarily on short user-generated tasks and a single public news dataset
- Reliance on ConceptNet assumes sufficient relational coverage for any target domain, which may not hold for specialized domains
- No ablation studies quantify the contribution of predicate invention versus recursive rules

## Confidence
- MIL's one-shot learning mechanism: Medium
- ConceptNet sufficiency assumption: Low
- Predicate invention generalization: Low
- Recursive rule learning contribution: Low

## Next Checks
1. Conduct domain transfer experiments by testing MGL on at least three additional text classification tasks from different domains to assess ConceptNet's coverage limitations
2. Perform ablation studies systematically disabling predicate invention and recursive learning to quantify each mechanism's contribution to the observed accuracy gains
3. Test MGL's sensitivity to training example quality by measuring performance variance when using "easy" versus "hard" single examples, and determine if multiple examples provide consistent improvements over the one-shot baseline