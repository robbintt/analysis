---
ver: rpa2
title: 'In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax'
arxiv_id: '2311.07811'
source_url: https://arxiv.org/abs/2311.07811
tags:
- main
- gpt-3
- language
- code
- syntactic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) trained
  with in-context learning (ICL) generalize robustly to out-of-distribution examples,
  focusing on syntax-sensitive tasks. The authors use two syntactic transformations
  tasks (question formation and tense reinflection) where correct generalization requires
  reliance on hierarchical syntactic structure rather than superficial heuristics.
---

# In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax

## Quick Facts
- arXiv ID: 2311.07811
- Source URL: https://arxiv.org/abs/2311.07811
- Reference count: 27
- Large language models vary significantly in their ability to generalize out-of-distribution on syntax-sensitive tasks, with variance explained more by pre-training corpus composition than model size

## Executive Summary
This paper investigates whether large language models trained with in-context learning (ICL) generalize robustly to out-of-distribution examples, focusing on syntax-sensitive tasks. Using two syntactic transformations tasks (question formation and tense reinflection), the authors evaluate multiple model families (GPT, PaLM, Llama 2) with and without chain-of-thought prompting. They find that models vary significantly in their ability to generalize out-of-distribution, with variance explained more by pre-training corpus composition than model size. Models pre-trained on code generalize better and benefit more from chain-of-thought prompting, while larger models do not necessarily generalize more robustly.

## Method Summary
The paper evaluates large language models on two syntactic transformations tasks using in-context learning with three prompt formats (No CoT, Verbal CoT, Code CoT). Each prompt contains 8 exemplars followed by a test example. The tasks include question formation (testing whether models rely on hierarchical syntax or surface heuristics) and tense reinflection (testing verb conjugation sensitivity). Models are evaluated on in-distribution and out-of-distribution accuracy, with additional analysis of reasoning accuracy and faithfulness for CoT prompts. The study tests multiple model families including GPT-3.5 variants, PaLM, and Llama 2 models with varying degrees of code pre-training.

## Key Results
- Models pre-trained on code (CodeLlama) significantly outperform those without code pre-training on syntactic generalization tasks
- Chain-of-thought prompting increases in-distribution accuracy but does not always improve out-of-distribution generalization
- GPT-3.5 text-davinci-003 (trained with RLHF) generalizes worse than comparable models despite strong in-distribution performance
- Larger models do not necessarily generalize more robustly; variance is explained more by pre-training corpus composition than model size

## Why This Works (Mechanism)

### Mechanism 1
Code pre-training imparts inductive biases that improve syntactic generalization in ICL. Exposure to code during pre-training provides frequent examples of hierarchical structure and long-range dependencies, which transfers to better syntactic reasoning in natural language. The hierarchical structure in code (e.g., function/class nesting) is sufficiently similar to syntactic structure in natural language to provide beneficial inductive biases. Evidence: CodeLlama performs significantly better than Llama 2 on question formation, and models pre-trained on code show higher reasoning accuracies and faithfulness scores.

### Mechanism 2
Chain-of-thought prompting can improve in-distribution accuracy but may not improve out-of-distribution generalization. CoT prompts provide explicit reasoning steps that help models follow instructions and reason about in-distribution examples, but this reasoning doesn't necessarily transfer to out-of-distribution examples. The reasoning steps provided in CoT prompts are sufficient to guide the model's generalization beyond the training distribution. Evidence: For question formation, chain-of-thought prompting tends to increase ID accuracy while not improving OOD generalization.

### Mechanism 3
Reinforcement learning from human feedback (RLHF) may harm out-of-distribution generalization. RLHF optimizes models to generate outputs that align with human quality judgments, but this optimization may come at the cost of syntactic sensitivity and robust generalization. The features that RLHF optimizes for (e.g., fluency, coherence) are orthogonal to or in conflict with syntactic generalization abilities. Evidence: GPT-3.5 text-davinci-003 performs at least as well as other GPT-3.5 models on in-distribution examples, but it generalizes consistently worse than other models which are fine-tuned on human demonstrations.

## Foundational Learning

- Concept: Syntactic transformations
  - Why needed here: The paper uses syntactic transformations tasks to test whether models rely on hierarchical syntactic structure or superficial heuristics
  - Quick check question: What is the difference between the MOVE-MAIN and MOVE-FIRST hypotheses in question formation?

- Concept: In-context learning (ICL)
  - Why needed here: ICL is the method used to teach models new tasks without weight updates, and the paper investigates how well it generalizes
  - Quick check question: How does ICL differ from traditional fine-tuning in terms of model updates?

- Concept: Chain-of-thought prompting
  - Why needed here: CoT prompting is used to investigate whether providing reasoning steps improves syntactic generalization
  - Quick check question: What is the difference between Verbal CoT and Code CoT prompts?

## Architecture Onboarding

- Component map: Pre-trained models (GPT, PaLM, Llama 2 families) -> Prompt formats (No CoT, Verbal CoT, Code CoT) -> Evaluation metrics (main auxiliary accuracy, verb accuracy, reasoning accuracy, faithfulness) -> Tasks (question formation, tense reinflection)
- Critical path: 1. Load pre-trained model 2. Construct prompt with 8 exemplars and test example 3. Generate model output 4. Evaluate accuracy and reasoning 5. Analyze results
- Design tradeoffs: Model size vs. generalization ability; Code pre-training vs. natural language pre-training; CoT prompting vs. no prompting; Instruction tuning vs. RLHF
- Failure signatures: Low OOD accuracy despite high ID accuracy; Low reasoning accuracy in CoT prompts; Low faithfulness between reasoning and final answer; Reliance on spurious lexical features
- First 3 experiments: 1. Evaluate a pre-trained model on question formation task with No CoT prompt 2. Evaluate the same model with Verbal CoT prompt 3. Evaluate the same model with Code CoT prompt

## Open Questions the Paper Calls Out

### Open Question 1
Does code pre-training causally improve out-of-distribution generalization through enhanced reasoning abilities, or is there an alternative mechanism at play? While the correlation is strong, the paper does not establish a causal link between code pre-training and improved reasoning abilities. Alternative explanations, such as code pre-training providing better language understanding or different optimization objectives, are not ruled out.

### Open Question 2
How does reinforcement learning from human feedback (RLHF) impact the ability of models to generalize out-of-distribution, and is this impact uniform across different model architectures and sizes? The findings are based on a single model and RLHF implementation. The impact of RLHF on generalization could vary depending on the specific RLHF method, model architecture, and size.

### Open Question 3
Does the size of a language model have a consistent impact on its ability to leverage in-context learning for out-of-distribution generalization, or are there diminishing returns or even negative effects at larger scales? The paper tests a range of model sizes but does not systematically explore the relationship between scale and out-of-distribution generalization across different model families and tasks.

## Limitations

- The study relies on controlled synthetic data rather than naturally occurring linguistic phenomena, which may not fully capture the complexity of real-world syntactic generalization
- The negative impact of RLHF on OOD generalization is concerning but based on a single model (text-davinci-003), making it difficult to generalize across RLHF implementations
- The analysis of prompt formats is limited to specific template structures, and it's unclear how generalizable these findings are to other prompt engineering approaches

## Confidence

- High confidence: The observation that models vary significantly in their OOD generalization capabilities, and that this variance correlates more with pre-training corpus composition than model size
- Medium confidence: The finding that code pre-training specifically improves syntactic generalization, and that RLHF may harm OOD generalization based on the text-davinci-003 results
- Medium confidence: The claim that CoT prompting doesn't always improve OOD generalization, though this appears consistent across tasks and models

## Next Checks

1. **Cross-linguistic validation**: Test the same syntactic generalization hypotheses on non-English languages to verify whether the findings generalize across linguistic typologies

2. **Ablation study on code pre-training**: Systematically vary the amount and type of code in pre-training to determine the minimum effective dose and identify which aspects of code syntax are most beneficial

3. **RLHF mechanism analysis**: Compare multiple RLHF-trained models on the same OOD tasks to determine whether the text-davinci-003 result is an anomaly or represents a broader trend in how RLHF affects syntactic generalization