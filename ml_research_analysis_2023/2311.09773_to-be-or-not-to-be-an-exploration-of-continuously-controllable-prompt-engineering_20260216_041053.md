---
ver: rpa2
title: To be or not to be? an exploration of continuously controllable prompt engineering
arxiv_id: '2311.09773'
source_url: https://arxiv.org/abs/2311.09773
tags:
- prompt
- lora
- prompts
- language
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ControlPE (Continuously Controllable Prompt
  Engineering), a novel approach to refine the influence of prompts on large language
  models. Unlike existing prompt engineering techniques that focus on discrete prompts,
  ControlPE uses LoRA (Low-Rank Adaptation) to create a continuous space for prompt
  control.
---

# To be or not to be? an exploration of continuously controllable prompt engineering

## Quick Facts
- arXiv ID: 2311.09773
- Source URL: https://arxiv.org/abs/2311.09773
- Reference count: 40
- This paper introduces ControlPE (Continuously Controllable Prompt Engineering), a novel approach to refine the influence of prompts on large language models using LoRA.

## Executive Summary
This paper introduces ControlPE (Continuously Controllable Prompt Engineering), a novel approach to refine the influence of prompts on large language models. Unlike existing prompt engineering techniques that focus on discrete prompts, ControlPE uses LoRA (Low-Rank Adaptation) to create a continuous space for prompt control. The method involves three key steps: generating a distillation dataset, training a LoRA model to capture the prompt's influence, and adjusting the LoRA merging weight to dynamically regulate the prompt's impact. Experiments demonstrate that ControlPE effectively controls response length, improves refusal accuracy in document-based question answering, and enhances chain-of-thought reasoning performance.

## Method Summary
ControlPE employs a three-step methodology: (1) Generate a distillation dataset by running inference with the target prompt to produce "golden" outputs, (2) Train a LoRA model to map standard inputs to these outputs, effectively encoding the prompt effect into low-rank matrices, and (3) Adjust the LoRA merging weight during inference to dynamically regulate the prompt's impact. This approach enables continuous adjustment of prompt influence rather than the all-or-nothing behavior of traditional prompt engineering.

## Key Results
- Response length control: Model output length adjusted linearly by tuning LoRA weights
- Refusal task improvement: Refusal rate increased from 0% to 27% with further tuning enabling precision-recall trade-offs
- Multi-prompt fusion: Independent control maintained over each prompt's influence when combining multiple LoRA adapters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA parameters can be trained to approximate the effect of a discrete prompt, enabling continuous adjustment via the LoRA merging weight.
- Mechanism: A target prompt is first used to generate "golden" outputs (prompt distillation). Then, LoRA is trained to map standard inputs to these outputs, effectively encoding the prompt effect into low-rank matrices. Adjusting the LoRA merging weight scales this effect continuously.
- Core assumption: The relationship between prompt influence and LoRA merging weight is monotonic and approximately linear over a useful range.
- Evidence anchors:
  - [abstract]: "adjusting the LoRA merging weight to dynamically regulate the prompt's impact"
  - [section]: "by tuning LoRA's merging weight[10, 11] to regulate the influence of prompts"
  - [corpus]: No direct empirical evidence for linearity assumption; only the paper's claim and synthetic plots are provided.
- Break condition: If the relationship between LoRA weight and prompt effect is highly non-linear or saturates quickly, fine control becomes impossible.

### Mechanism 2
- Claim: Multiple LoRA adapters trained on different prompts can be merged and individually weighted to achieve multi-prompt control.
- Mechanism: Separate LoRA adapters are trained for each target prompt (e.g., brevity, refusal, CoT). During inference, each LoRA is scaled by its own weight and added to the base model, allowing independent tuning of each prompt's influence.
- Core assumption: LoRA adapters trained on different prompts do not interfere destructively when merged, and their effects remain approximately additive.
- Evidence anchors:
  - [abstract]: "Additionally, ControlPE enables the fusion of multiple prompts, maintaining independent control over each prompt's influence."
  - [section]: "Although the LoRA model distilled with 'Keep the answer short and concise' was not trained on wikipedia-trivia, it still maintains the ability to linearly regulate the response length."
  - [corpus]: No direct empirical evidence for additivity; the paper only reports qualitative observations of coexistence.
- Break condition: If LoRA adapters interfere or saturate, adjusting one weight may inadvertently affect others.

### Mechanism 3
- Claim: Distillation dataset construction allows the model to learn the specific transformation induced by a prompt, enabling transfer to new inputs.
- Mechanism: The model first runs inference with the target prompt to produce labels. Then LoRA is trained to reproduce these outputs from the same inputs without the prompt, distilling the prompt's behavioral change into the adapter.
- Core assumption: The prompt's effect is consistent enough across a representative sample of inputs that LoRA can generalize beyond the distillation set.
- Evidence anchors:
  - [section]: "The process involves using input prompts that incorporate the target prompt, performing inference, and constructing a new dataset Ddistill with input data x inherited from the original dataset and labels ytarget are generated by the large language model θ"
  - [section]: "By incorporating prompts like 'If there are no references in the known information, respond with 'No relevant information available,' and avoid fabricating facts,' the model can be guided to refuse to answer when the context does not contain information pertinent to the question."
  - [corpus]: No direct empirical evidence for generalization; only held-out test set results are reported.
- Break condition: If prompt effects are input-dependent or context-sensitive, LoRA may not generalize beyond the distillation data.

## Foundational Learning

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: LoRA provides a parameter-efficient way to encode prompt effects into small, tunable matrices that can be scaled independently of the base model.
  - Quick check question: What are the dimensions of the LoRA matrices A and B relative to the original weight matrix?

- Concept: Prompt Distillation
  - Why needed here: Distillation generates ground-truth outputs under a specific prompt so LoRA can learn to mimic that behavior without the prompt present.
  - Quick check question: Why must the distillation dataset be constructed before training LoRA?

- Concept: Weight Merging in LoRA
  - Why needed here: Merging weight controls how much the LoRA adaptation influences the base model, enabling continuous prompt control.
  - Quick check question: What happens to the base model output when the LoRA merging weight is set to zero?

## Architecture Onboarding

- Component map: Base LLM -> LoRA adapters (trained per prompt) -> LoRA merging weights (tunable per adapter) -> Distillation dataset generator
- Critical path: 1. Generate distillation dataset with target prompt 2. Train LoRA on distilled data (no prompt in input) 3. Merge LoRA with base model at runtime 4. Tune merging weight(s) for desired behavior
- Design tradeoffs: More LoRA adapters → finer control but higher memory/compute; Higher LoRA rank → better prompt approximation but slower training; Linear vs. non-linear weight scaling → simplicity vs. expressiveness
- Failure signatures: Weight tuning has no effect → LoRA not properly trained or merged; Behavior degrades outside training range → LoRA overfitting or saturation; Interference between adapters → non-additive effects, unexpected outputs
- First 3 experiments: 1. Train a single LoRA for "keep answer short" and sweep merging weight, plot output length vs. weight. 2. Train LoRA for refusal prompt, measure refusal rate and precision/recall across weights. 3. Train two LoRA adapters (brevity + refusal) and test independent weight tuning on combined behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the merging weight of multiple LoRA modules affect the performance and controllability of each individual prompt in ControlPE?
- Basis in paper: [explicit] The paper mentions that in the multiple prompts fusion experiment, they combined two LoRA models trained on different prompts and observed their influence on each other, but did not provide a detailed analysis of the effect of merging weights on individual prompt control.
- Why unresolved: The paper only briefly mentions that the LoRA models "influence each other" and "retain the capability to adjust their influence on the prompt itself," without quantifying or explaining the precise relationship between merging weights and individual prompt control.
- What evidence would resolve it: Systematic experiments varying the merging weights of multiple LoRA modules and measuring their impact on the controllability of each individual prompt, including precision, recall, and response length metrics.

### Open Question 2
- Question: What is the optimal rank for LoRA modules in ControlPE when applied to different types of prompts (e.g., response length control vs. refusal prompts)?
- Basis in paper: [inferred] The paper uses a fixed rank of 16 for all LoRA modules but does not explore how different ranks affect the performance of ControlPE across various prompt types.
- Why unresolved: The choice of rank is critical for balancing model performance and computational efficiency, yet the paper does not investigate the impact of rank variation on different prompt categories.
- What evidence would resolve it: Comparative experiments testing multiple LoRA ranks (e.g., 8, 16, 32) for different prompt types and evaluating their effectiveness in terms of response length control, refusal accuracy, and computational overhead.

### Open Question 3
- Question: How does the size of the distillation dataset affect the quality and stability of prompt control in ControlPE?
- Basis in paper: [inferred] The paper uses a fixed distillation dataset size (e.g., 10,000 instructions for training LoRA models) but does not explore the relationship between dataset size and the effectiveness of prompt control.
- Why unresolved: The distillation dataset size could significantly impact the ability of LoRA to capture and generalize prompt influence, but this aspect is not investigated in the paper.
- What evidence would resolve it: Experiments varying the size of the distillation dataset (e.g., 1,000 vs. 10,000 vs. 50,000 samples) and measuring the consistency and effectiveness of prompt control across different scenarios (e.g., response length, refusal rate, CoT reasoning).

## Limitations
- The linearity assumption between LoRA merging weight and prompt effect lacks rigorous empirical validation
- Additivity of multiple LoRA adapters remains theoretical without systematic interference analysis
- Generalization capability of distilled prompt effects beyond training distribution is underspecified

## Confidence
- **High confidence**: LoRA can encode prompt effects into trainable parameters, and merging weight scaling works in simple cases
- **Medium confidence**: The three-step methodology (distillation → LoRA training → weight tuning) is technically sound and implementable
- **Low confidence**: Multi-prompt additive control, generalization across diverse inputs, and linearity of weight-response relationships

## Next Checks
1. **Linearity stress test**: Systematically sweep LoRA merging weights across multiple orders of magnitude for different prompts, measuring the response metric (length, refusal rate, accuracy) to empirically verify linearity assumptions and identify saturation points.

2. **Interference analysis**: Train multiple LoRA adapters on different prompts and conduct controlled experiments where weights are adjusted independently and jointly, measuring cross-contamination effects and identifying conditions where additivity breaks down.

3. **Generalization validation**: Create distillation datasets with limited input diversity, then test LoRA performance on held-out inputs that systematically vary from the training distribution to quantify the generalization gap and identify failure modes.