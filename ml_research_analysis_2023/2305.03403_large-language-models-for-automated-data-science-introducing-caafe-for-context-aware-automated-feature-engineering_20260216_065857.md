---
ver: rpa2
title: 'Large Language Models for Automated Data Science: Introducing CAAFE for Context-Aware
  Automated Feature Engineering'
arxiv_id: '2305.03403'
source_url: https://arxiv.org/abs/2305.03403
tags:
- feature
- data
- dataset
- code
- engineering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CAAFE leverages LLMs to generate Python code for automated feature
  engineering in tabular datasets. It uses dataset descriptions to guide code generation,
  creating semantically meaningful features and providing explanations for their utility.
---

# Large Language Models for Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering

## Quick Facts
- arXiv ID: 2305.03403
- Source URL: https://arxiv.org/abs/2305.03403
- Reference count: 40
- Primary result: CAAFE improves ROC AUC performance from 0.798 to 0.822 across 14 datasets

## Executive Summary
CAAFE introduces a novel approach to automated feature engineering using large language models (LLMs). The method leverages GPT-4 to generate Python code for creating semantically meaningful features in tabular datasets, guided by dataset descriptions. Through iterative evaluation, CAAFE selects features that improve downstream classification performance, demonstrating gains comparable to using random forests over logistic regression. The approach also provides textual explanations for generated features, enabling interpretability and potential human-in-the-loop automation.

## Method Summary
CAAFE takes tabular datasets with descriptions as input and uses GPT-4 to generate Python code for feature engineering through a carefully constructed prompt. The prompt includes dataset context, feature names, data types, missing value percentages, and sample data. In each iteration, the LLM generates code to create new features, which is executed on the training and validation data. A downstream classifier (TabPFN) evaluates performance on the transformed validation data, and features are kept if they improve performance. This process repeats for a fixed number of iterations, resulting in transformed datasets with semantically meaningful features and explanations for their utility.

## Key Results
- Improves ROC AUC performance from 0.798 to 0.822 across 14 datasets
- Demonstrates gains similar to using random forest over logistic regression
- Provides interpretable explanations for generated features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CAAFE leverages LLMs' domain knowledge to generate semantically meaningful features that improve downstream classification performance.
- Mechanism: The LLM receives a prompt containing dataset descriptions, feature names, data types, missing value percentages, and sample data. Based on this context, it generates Python code that creates new features by combining, transforming, or aggregating existing features in semantically meaningful ways. The generated code includes comments explaining the utility of each feature.
- Core assumption: The LLM has been trained on sufficient web data to possess domain knowledge relevant to the given dataset descriptions, enabling it to create useful features.
- Evidence anchors:
  - [abstract] "CAAFE leverages LLMs to generate Python code for automated feature engineering in tabular datasets. It uses dataset descriptions to guide code generation, creating semantically meaningful features and providing explanations for their utility."
  - [section 3.1] "Our method receives as an input the training and validation datasets, Dtrain andDvalid, as well as a description of the context of the training dataset and features. From this information CAAFE constructs a prompt, i.e. instructions to the LLM containing specifics of the dataset and the feature engineering task."
  - [corpus] Weak - corpus evidence is limited to related AutoML and LLM papers, but does not directly address CAAFE's specific mechanism.
- Break condition: If the LLM lacks relevant domain knowledge for the given dataset descriptions, or if the prompt construction fails to effectively communicate the task to the LLM, the generated features may not improve downstream performance.

### Mechanism 2
- Claim: CAAFE uses a iterative evaluation process to select the most effective generated features.
- Mechanism: In each iteration, the LLM generates code for a new feature, which is then executed on the training and validation datasets. The performance of a downstream classifier (e.g., TabPFN) is evaluated on the transformed validation dataset. If the performance improves compared to the previous iteration, the feature is kept; otherwise, it is rejected. This process continues for a fixed number of iterations or until no further improvements are observed.
- Core assumption: The iterative evaluation process effectively identifies the most useful features generated by the LLM, leading to improved downstream performance.
- Evidence anchors:
  - [section 3] "Our method performs multiple iterations of feature alterations and evaluations on the validation dataset... In each iteration, the LLM generates code based on our prompt, which is then executed on the currentDtrain andDvalid resulting in the transformed datasetsD′
train andD′
valid. We then useD′
train to fit an ML-classifier and evaluate its performanceP ′ onD′
valid. IfP ′ exceeds the performanceP achieved by training onDtrain and evaluating onDvalid, the feature is kept..."
  - [section 4.2] "We use seven feature engineering iterations and TabPFN in the iterative evaluation of code blocks."
  - [corpus] Weak - corpus evidence does not directly address CAAFE's specific iterative evaluation process.
- Break condition: If the evaluation metric used to compare performance between iterations is not well-suited to the dataset or problem, or if the number of iterations is not sufficient to capture the most effective features, the iterative process may fail to select the best features.

### Mechanism 3
- Claim: CAAFE's use of code generation as an interface between LLMs and classical ML algorithms enables interpretability and human-in-the-loop automation.
- Mechanism: The LLM generates Python code that creates new features and provides explanations for their utility. This code is then executed on the datasets, allowing users to understand and modify the feature engineering process. The explanations generated by the LLM help users interpret the rationale behind the generated features, facilitating human-in-the-loop automation.
- Core assumption: The generated Python code is interpretable and modifiable by users, and the explanations provided by the LLM accurately convey the utility of the generated features.
- Evidence anchors:
  - [abstract] "Furthermore, CAAFE is interpretable by providing a textual explanation for each generated feature."
  - [section 2] "Our proposed method, CAAFE, generates Python code that creates semantically meaningful additional features which enhance the performance of downstream prediction tasks. Furthermore, it provides explanations for their utility. This allows for human-in-the-loop, interpretable AutoML (Lee & Macke, 2020), making it easier for the user to understand a solution, but also to modify and improve on it."
  - [corpus] Weak - corpus evidence does not directly address CAAFE's specific use of code generation for interpretability and human-in-the-loop automation.
- Break condition: If the generated code is not easily interpretable or modifiable by users, or if the explanations provided by the LLM are inaccurate or misleading, the interpretability and human-in-the-loop aspects of CAAFE may not be effective.

## Foundational Learning

- Concept: Prompt engineering for LLMs
  - Why needed here: CAAFE relies on constructing effective prompts to guide the LLM in generating useful feature engineering code. Understanding how to craft prompts that communicate the task, dataset context, and desired output format is crucial for the success of CAAFE.
  - Quick check question: What are the key components of a well-structured prompt for CAAFE, and how do they contribute to the quality of the generated code?

- Concept: Feature engineering techniques
  - Why needed here: CAAFE generates new features by applying various feature engineering techniques, such as feature combination, binning, string transformations, and feature selection. Familiarity with these techniques and their applications is essential for understanding the generated code and interpreting the resulting features.
  - Quick check question: What are some common feature engineering techniques, and how can they be applied to improve the performance of machine learning models on tabular datasets?

- Concept: AutoML and model evaluation
  - Why needed here: CAAFE is an AutoML method that automates the feature engineering process. Understanding the principles of AutoML, including model selection, hyperparameter tuning, and evaluation, is necessary to assess the effectiveness of CAAFE and compare it to other AutoML approaches.
  - Quick check question: How does CAAFE's iterative evaluation process differ from traditional AutoML methods, and what are the advantages and disadvantages of this approach?

## Architecture Onboarding

- Component map: User input -> Prompt construction -> LLM -> Code execution -> Downstream classifier -> Feature selection -> Output
- Critical path:
  1. User provides dataset descriptions and training/validation datasets
  2. CAAFE constructs prompts for the LLM based on the input data
  3. LLM generates Python code for feature engineering
  4. Generated code is executed on the training and validation datasets
  5. Downstream classifier evaluates the performance of the transformed datasets
  6. Effective features are selected based on the classifier's performance
  7. Transformed datasets with selected features and explanations are outputted

- Design tradeoffs:
  - Number of iterations: Increasing the number of iterations may lead to better feature selection but also increases computational cost and the risk of overfitting.
  - Prompt complexity: More detailed prompts may result in better-generated code but also increase the risk of exceeding the LLM's token limit and reduce interpretability.
  - Downstream classifier choice: Different classifiers may have varying performance on the transformed datasets, affecting the selection of effective features.

- Failure signatures:
  - LLM generates invalid or non-functional code
  - Generated features do not improve downstream classifier performance
  - Explanations provided by the LLM are inaccurate or misleading
  - Iterative evaluation process fails to converge or selects suboptimal features

- First 3 experiments:
  1. Test CAAFE on a simple dataset with a well-defined problem and clear dataset description to assess its ability to generate meaningful features.
  2. Vary the number of iterations and observe the impact on feature selection and downstream performance to find the optimal number of iterations.
  3. Compare the performance of CAAFE with different downstream classifiers (e.g., logistic regression, random forest, TabPFN) to determine the best classifier for feature selection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CAAFE compare to more recent automated feature engineering methods like H2O's Feature Engineering (H2OFE) or FeatureTools 2.0?
- Basis in paper: [explicit] The paper compares CAAFE to DFS and AutoFeat but these are older methods.
- Why unresolved: The paper does not evaluate CAAFE against newer feature engineering frameworks.
- What evidence would resolve it: Benchmarking CAAFE against modern feature engineering libraries on the same datasets would show relative performance.

### Open Question 2
- Question: How does CAAFE's performance degrade when dataset descriptions are incomplete, ambiguous, or contain incorrect information?
- Basis in paper: [inferred] The paper mentions dataset descriptions are crucial but doesn't test robustness to poor descriptions.
- Why unresolved: Experiments used well-formed dataset descriptions from OpenML and Kaggle, not testing edge cases.
- What evidence would resolve it: Systematically varying description quality and measuring performance impact would quantify robustness.

### Open Question 3
- Question: Can CAAFE be effectively adapted for regression tasks, and what modifications would be needed?
- Basis in paper: [explicit] The paper states "we do not test CAAFE in a regression setting."
- Why unresolved: The method was only evaluated on classification tasks with binary/multi-class targets.
- What evidence would resolve it: Applying CAAFE to regression datasets and comparing performance to baseline feature engineering methods would demonstrate feasibility.

### Open Question 4
- Question: How does the computational cost of CAAFE scale with dataset size and feature count compared to traditional automated feature engineering methods?
- Basis in paper: [inferred] The paper mentions prompt length increases linearly with features but doesn't analyze computational complexity.
- Why unresolved: Only small datasets were used, and runtime/complexity analysis was not performed.
- What evidence would resolve it: Measuring execution time and memory usage across datasets of varying sizes and feature counts would establish scaling behavior.

## Limitations
- Limited evaluation on regression tasks and highly specialized domains
- Unclear generalization of prompt engineering approach across diverse problem domains
- Limited validation of interpretability claims and human-in-the-loop effectiveness

## Confidence
**High Confidence**: The core observation that CAAFE improves ROC AUC performance from 0.798 to 0.822 across multiple datasets is well-supported by the experimental results.

**Medium Confidence**: The claim that CAAFE leverages LLMs' domain knowledge for semantically meaningful feature generation is plausible but not rigorously validated - the paper shows improvements but doesn't deeply analyze why specific features help.

**Low Confidence**: The assertion that CAAFE enables effective "human-in-the-loop" automation lacks empirical validation - no user studies or evidence that explanations meaningfully aid interpretation.

## Next Checks
1. **Ablation Study**: Remove the LLM-generated explanations and evaluate whether performance remains similar, to determine if explanations are merely byproducts or actively contribute to feature selection.

2. **Cross-Domain Transfer**: Apply CAAFE to datasets from domains not represented in the original evaluation (e.g., genomics, financial time series) to test generalization of the prompt engineering approach.

3. **Feature Interpretability Audit**: Manually examine 10-20 generated features and their explanations to verify accuracy and usefulness, establishing a baseline for interpretability quality.