---
ver: rpa2
title: Clustering-based Domain-Incremental Learning
arxiv_id: '2309.12078'
source_url: https://arxiv.org/abs/2309.12078
tags:
- task
- learning
- tasks
- accuracy
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of catastrophic forgetting in
  continual learning, particularly in the domain-incremental setting where task identities
  and boundaries are unknown. The authors propose a clustering-based approach to extend
  projection-based methods (A-GEM and OGD) to a task-agnostic context, dynamically
  updating a finite pool of samples or gradients using online clustering.
---

# Clustering-based Domain-Incremental Learning

## Quick Facts
- arXiv ID: 2309.12078
- Source URL: https://arxiv.org/abs/2309.12078
- Authors: 
- Reference count: 40
- This paper proposes clustering-based extensions to projection-based continual learning methods (A-GEM and OGD) for task-agnostic domain-incremental learning

## Executive Summary
This paper addresses catastrophic forgetting in continual learning, specifically in the domain-incremental setting where task identities and boundaries are unknown. The authors propose a clustering-based approach that extends projection-based methods (A-GEM and OGD) to a task-agnostic context by dynamically updating a finite pool of samples or gradients using online clustering. Experiments on multiple datasets demonstrate that the proposed TA-A-GEM and TA-OGD methods significantly outperform existing task-agnostic methods while achieving performance comparable to task-aware approaches, all with constant memory requirements.

## Method Summary
The paper proposes task-agnostic extensions to A-GEM and OGD called TA-A-GEM and TA-OGD. These methods use online clustering to dynamically maintain a finite pool of labeled data samples (for TA-A-GEM) or model gradients (for TA-OGD) as new batches arrive. Samples/gradients are assigned to clusters based on ℓ2 distance to cluster centers, with FIFO eviction when clusters reach capacity. An adaptive learning rate scheduler detects task changes through loss plateaus and spikes. The approach maintains constant memory requirements unlike traditional A-GEM/OGD which grow memory with task count, while preserving task diversity through clustering.

## Key Results
- TA-A-GEM achieves up to 0.878 average accuracy on MNIST class split experiments
- TA-OGD reaches 0.937 accuracy on MNIST class split, outperforming BGD (0.875)
- Both methods significantly outperform existing task-agnostic methods while maintaining constant memory requirements
- Clustering mechanism proves crucial for maintaining task diversity and mitigating forgetting across various sampling rates

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Online clustering dynamically maintains task diversity in memory, preventing catastrophic forgetting without task boundary knowledge.
- **Mechanism**: When a new batch arrives, samples are assigned to the nearest cluster center based on ℓ2 distance. If a cluster reaches its maximum size, the oldest sample is removed and the center is updated. This ensures that task-relevant samples are retained even after new tasks arrive.
- **Core assumption**: Samples from different tasks are sufficiently separated in the input space (for TA-A-GEM) or model gradient space (for TA-OGD) so that clustering can distinguish them.
- **Evidence anchors**:
  - [abstract] "dynamically updating the pool of labeled data samples (A-GEM) or model gradients (OGD) each time a new batch becomes available"
  - [section 3.1] "To achieve this, TA-A-GEM and TA-OGD leverage the structure of the training data, which are now grouped into clusters of samples or gradients"
  - [corpus] Weak - no direct clustering-based domain-incremental learning papers found in neighbors
- **Break condition**: If task samples are not separable in the clustering space (e.g., highly similar tasks), clustering fails to maintain task diversity and forgetting occurs.

### Mechanism 2
- **Claim**: The finite, fixed-size memory pool with clustering enables constant memory requirements while maintaining performance.
- **Mechanism**: Unlike traditional A-GEM/OGD which grow memory with task count, this approach keeps memory size constant by using clustering to selectively retain informative samples/gradients and evict less useful ones via FIFO within clusters.
- **Core assumption**: A finite number of clusters with fixed capacity can adequately represent all previously seen tasks.
- **Evidence anchors**:
  - [abstract] "Unlike A-GEM and OGD, which store a growing number of samples or gradients as the number of tasks increases... the proposed TA-A-GEM and TA-OGD methods have constant and finite memory requirements"
  - [section 3.1] "we can retain the information needed to address catastrophic forgetting... while keeping the memory-size finite via a simple and efficient clustering procedure"
  - [corpus] Weak - no direct memory-requirement comparisons found in neighbors
- **Break condition**: If too many tasks arrive or tasks are too diverse, fixed cluster capacity becomes insufficient and forgetting increases.

### Mechanism 3
- **Claim**: The adaptive learning rate scheduler detects task changes without explicit task boundary information.
- **Mechanism**: When loss plateaus (indicating stable task learning), learning rate decreases by factor a < 1. When sudden loss spikes occur (indicating task change), learning rate resets to initial value to accelerate learning on new task.
- **Core assumption**: Loss function behavior (plateaus vs. spikes) correlates with task stability vs. task change.
- **Evidence anchors**:
  - [section 3.1] "we follow an adaptive strategy for the learning rate of the projected gradient step... ηt = aηt−1, where a < 0, when the loss function is smoothly increasing for a given number of iterations"
  - [section 3.1] "when a sudden increase is observed, then the learning rate is reset to its initial value"
  - [corpus] Weak - no direct adaptive learning rate mechanisms found in neighbors
- **Break condition**: If loss behavior doesn't correlate with task changes (e.g., noisy data), the adaptive schedule may misinterpret signals and use inappropriate learning rates.

## Foundational Learning

- **Concept**: Catastrophic forgetting in continual learning
  - **Why needed here**: The entire paper addresses this problem; understanding why neural networks forget is essential to grasping the proposed solution
  - **Quick check question**: What happens to a neural network's performance on task A when it's trained on task B without any forgetting mitigation?

- **Concept**: Task-incremental vs. domain-incremental vs. class-incremental learning
  - **Why needed here**: The paper specifically addresses domain-incremental learning where task identities are unknown; knowing the differences is crucial
  - **Quick check question**: In which learning setting does the learner not know task boundaries during training or testing?

- **Concept**: Projection-based continual learning methods
  - **Why needed here**: TA-A-GEM and TA-OGD are based on A-GEM and OGD, which use projection to prevent forgetting; understanding this mechanism is key
  - **Quick check question**: How do A-GEM and OGD prevent the model from forgetting previous tasks during training on new tasks?

## Architecture Onboarding

- **Component map**: New batch -> Sample/gradient selection -> Cluster assignment -> Memory update (FIFO within clusters) -> Projection step (using cluster samples/gradients) -> Weight update -> Adaptive learning rate adjustment

- **Critical path**: New batch → Sample/gradient selection → Cluster assignment → Memory update (FIFO within clusters) → Projection step (using cluster samples/gradients) → Weight update → Adaptive learning rate adjustment

- **Design tradeoffs**: 
  - Fixed memory size vs. performance (too small → forgetting; too large → inefficient)
  - Number of clusters vs. computational cost (more clusters → better task separation but higher overhead)
  - Cluster size vs. task representation (larger clusters → better task capture but fewer total clusters)

- **Failure signatures**:
  - Rapid performance degradation on early tasks indicates insufficient task separation in clustering
  - Plateaued performance on all tasks suggests memory pool is too small or clusters are too large
  - Oscillating performance indicates poor adaptive learning rate configuration

- **First 3 experiments**:
  1. Run TA-A-GEM on MNIST with task permutation and sampling rate=1 to verify basic functionality
  2. Compare TA-A-GEM vs. TA-A-GEM with random cluster assignment on Fashion MNIST class split to demonstrate clustering benefit
  3. Test TA-OGD on CIFAR10 with rotation tasks to verify gradient-based clustering works across datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of clusters and cluster size for maximizing performance across different datasets and task types?
- Basis in paper: [explicit] The paper mentions that "the choice of hyperparameters, such as the number of clusters Q and their size, is important" and discusses trade-offs between memory footprint and task diversity retention.
- Why unresolved: The authors note that this involves a trade-off and mention using specific values (Q=100, cluster size=3 for TA-A-GEM) but do not provide systematic hyperparameter optimization or sensitivity analysis.
- What evidence would resolve it: Comprehensive ablation studies varying Q and cluster sizes across different datasets (MNIST, Fashion MNIST, NOT MNIST, CIFAR10, SVHN) and task types (permutation, rotation, class split) to identify optimal configurations.

### Open Question 2
- Question: How does the proposed clustering-based approach scale to larger, more complex datasets like ImageNet and deeper architectures like ResNet?
- Basis in paper: [explicit] The authors state "it is worth noting that our proposed method is generic hence we also intend to inquire its application as an off-the-shelf tool to other projection-based methods" and mention future work on "larger networks such as a ResNet... or more complicated datasets such as ImageNet."
- Why unresolved: All experiments were conducted on relatively simple datasets (MNIST, Fashion MNIST, etc.) and shallow MLP architectures, with no testing on deeper networks or more complex data.
- What evidence would resolve it: Experiments applying TA-A-GEM and TA-OGD to ImageNet classification tasks using ResNet architectures, comparing performance against baseline methods and analyzing computational/memory requirements.

### Open Question 3
- Question: What are the theoretical guarantees of the clustering mechanism in preserving task diversity and preventing catastrophic forgetting?
- Basis in paper: [inferred] The paper provides empirical evidence that clustering helps maintain task diversity in the memory pool, but does not offer theoretical analysis of why or how well this works.
- Why unresolved: While the authors show empirically that clustering outperforms random assignment, they do not provide mathematical proofs or theoretical bounds on the effectiveness of the clustering approach for forgetting prevention.
- What evidence would resolve it: Formal analysis proving that the proposed clustering strategy guarantees a minimum level of task diversity in the memory pool, or bounds on the expected forgetting given certain clustering parameters and task distributions.

## Limitations

- The clustering mechanism relies heavily on the assumption that task samples/gradients are well-separated in the clustering space, which may not hold for highly similar tasks
- The paper demonstrates effectiveness on relatively simple datasets and shallow MLP architectures, with no testing on deeper networks or more complex data
- Limited ablation studies on clustering configuration itself, particularly regarding optimal number of clusters and cluster sizes

## Confidence

- **Clustering effectiveness**: Medium - Results show consistent improvements but lack detailed analysis of clustering failure modes and sensitivity to hyperparameters
- **Task-agnostic performance**: Medium - Significant improvements over task-agnostic baselines demonstrated, but limited comparison to state-of-the-art task-aware methods
- **Scalability claims**: Low - All experiments use simple datasets and architectures; no evidence provided for scalability to complex datasets or deeper networks

## Next Checks

1. Test TA-A-GEM with varying numbers of clusters (Q=10, 50, 200) on the same task sequences to quantify the impact of clustering granularity on performance and memory efficiency.

2. Evaluate the method on a dataset with highly similar tasks (e.g., fine-grained image classification) to test the clustering mechanism's robustness when task separation is minimal.

3. Measure and report the clustering quality metrics (intra-cluster variance, inter-cluster distances) during training to provide insight into why the method succeeds or fails on specific task sequences.