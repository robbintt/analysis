---
ver: rpa2
title: 'FocusLearn: Fully-Interpretable, High-Performance Modular Neural Networks
  for Time Series'
arxiv_id: '2311.16834'
source_url: https://arxiv.org/abs/2311.16834
tags:
- attention
- data
- feature
- hearing
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AMN, an interpretable modular neural network
  model for multivariate time series prediction that combines a recurrent neural network
  with an attention-based feature selection component. The model learns temporal dependencies
  in the data while selecting the most relevant features, then trains a modular deep
  network independently from the selected features to provide interpretability.
---

# FocusLearn: Fully-Interpretable, High-Performance Modular Neural Networks for Time Series

## Quick Facts
- arXiv ID: 2311.16834
- Source URL: https://arxiv.org/abs/2311.16834
- Reference count: 40
- Key outcome: AMN outperforms interpretable models (NAM, SPAM) and matches non-interpretable models (LSTM, XGBoost) on time series tasks while providing exact feature interpretability

## Executive Summary
This paper introduces AMN, an interpretable modular neural network model for multivariate time series prediction that combines a recurrent neural network with an attention-based feature selection component. The model learns temporal dependencies in the data while selecting the most relevant features, then trains a modular deep network independently from the selected features to provide interpretability. Experiments on regression and classification tasks show AMN outperforms state-of-the-art interpretable Neural Additive Models and variations, achieving comparable performance to top non-interpretable methods like LSTM and XGBoost.

## Method Summary
AMN is a three-component architecture consisting of an RNN (LSTM) for temporal learning, Attention-based Feature Selection (AFS) for identifying relevant features using modified multi-head attention, and Modular Networks where each selected feature is learned independently by its own network module. The model uses attention weights both for feature selection and as initialization for module weights (ANB), and trains jointly using backpropagation. The architecture follows the GAM framework where the final prediction is a linear combination of univariate shape functions, providing exact interpretability of how each feature influences outcomes.

## Key Results
- AMN achieves SMAPE of 0.264, MASE of 0.165, and WAPE of 0.164 on the Air dataset, outperforming NAM and SPAM while matching LSTM and XGBoost
- On EEG classification, AMN achieves Accuracy of 0.882, F1 score of 0.877, and AUC of 0.948, significantly outperforming interpretable baselines
- The model demonstrates that interpretability need not come at the cost of predictive performance, matching state-of-the-art non-interpretable methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The attention-based feature selection (AFS) improves interpretability and predictive performance by learning feature importance from temporal context.
- Mechanism: AFS uses multi-head attention where each head computes attention weights for individual input features rather than subsequences. These weights are averaged and passed through softmax to select top features, allowing the model to focus on the most relevant features while suppressing redundant ones.
- Core assumption: Attention weights from a recurrent network can meaningfully capture temporal feature importance.
- Evidence anchors:
  - [abstract]: "An attention-based feature selection component selects the most relevant features and suppresses redundant features"
  - [section]: "The Multi-head attention is modified so each attention head computes the attention weight of each input feature independently"
  - [corpus]: Weak - related papers focus on graph neural networks and brain networks, not time series feature selection
- Break condition: If the temporal dependencies captured by the RNN are not meaningful for the specific time series domain, the attention weights would not reflect true feature importance.

### Mechanism 2
- Claim: The attention-based node bootstrapping (ANB) improves predictive performance by initializing module weights based on attention weights.
- Mechanism: ANB multiplies the initialized weights in each module's first layer by the attention weights from AFS, creating feature-specific networks that are weighted by their importance. This initialization is combined with Xavier initialization for stability.
- Core assumption: Initializing module weights with attention weights will improve learning efficiency and predictive performance.
- Evidence anchors:
  - [abstract]: "A modular deep network is trained from the selected features independently to show the users how features influence outcomes"
  - [section]: "The previously-computed attention weights for the selected features, F, are then multiplied by the initialized weights, Winit"
  - [corpus]: Weak - no direct evidence in corpus about node bootstrapping or attention-weighted initialization
- Break condition: If the attention weights are not reliable indicators of feature importance, the initialization could lead to poor convergence or suboptimal performance.

### Mechanism 3
- Claim: The modular network architecture provides exact interpretability by learning independent shape functions for each feature.
- Mechanism: Each selected feature is learned independently by a module network, following the GAM framework where the final prediction is a linear combination of univariate shape functions. This allows visualization of exactly how each feature influences the prediction.
- Core assumption: Feature independence is a reasonable assumption for the given time series problems.
- Evidence anchors:
  - [abstract]: "A modular deep network is trained from the selected features independently to show the users how features influence outcomes, making the model interpretable"
  - [section]: "each feature is initially weighted using the attention weights F, which will be shown to improve predictive performance considerably"
  - [corpus]: Weak - related papers focus on graph networks and brain networks, not modular time series architectures
- Break condition: If significant feature interactions exist that are not captured by the attention selection, the independent modeling assumption would lead to poor predictive performance.

## Foundational Learning

- Concept: Recurrent Neural Networks (RNNs) and their variants (LSTM, GRU, BLSTM)
  - Why needed here: RNNs are the core component for learning temporal dependencies in time series data, which is essential before feature selection can be applied
  - Quick check question: How does an LSTM cell use gating mechanisms to handle long-term dependencies differently from a simple RNN?

- Concept: Attention Mechanisms and Multi-head Attention
  - Why needed here: Attention is the foundation for both feature selection (AFS) and the modified approach that computes feature-specific attention weights rather than subsequence attention
  - Quick check question: What is the mathematical difference between standard multi-head attention and the modified version used in AFS where each head attends to individual features?

- Concept: Generalized Additive Models (GAMs) and Neural Additive Models (NAM)
  - Why needed here: AMN extends NAM by adding attention-based feature selection and bootstrapping, so understanding NAM's architecture and limitations is crucial
  - Quick check question: What is the key interpretability advantage of NAM over standard neural networks, and what is its main performance limitation?

## Architecture Onboarding

- Component map: RNN component -> Attention-based Feature Selection (AFS) -> Modular Networks -> Prediction
- Critical path: Data flows from the RNN component to AFS, which selects top features and computes attention weights, then to Modular Networks where each feature is learned independently with ANB initialization, and finally the outputs are combined for prediction
- Design tradeoffs: The architecture trades some model capacity (by selecting only top features) for interpretability, and uses attention weights both for selection and initialization rather than just for selection, which adds complexity but improves performance
- Failure signatures: Poor feature selection leading to missing important variables, attention weights not reflecting true importance due to insufficient temporal context, or feature independence assumption failing when significant interactions exist
- First 3 experiments:
  1. Test feature selection effectiveness by comparing AMN with n=10 features vs n=21 features on classification tasks to validate AFS improves performance
  2. Compare different RNN configurations (LSTM, GRU, BLSTM) with and without ANB to validate the bootstrapping mechanism
  3. Evaluate interpretability by visualizing shape functions and comparing with ground truth feature importance on synthetic datasets with known relationships

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of modules in AMN affect its predictive performance and interpretability?
- Basis in paper: [explicit] The paper mentions that "we shall evaluate this and other trade-offs, such as the number of modules to use, in practice, since it is likely that the answers will depend on the characteristics of the specific application domain and associated expert evaluation."
- Why unresolved: The paper does not provide experimental results on varying the number of modules.
- What evidence would resolve it: Experiments comparing AMN's performance and interpretability with different numbers of modules on various datasets.

### Open Question 2
- Question: Can AMN be extended to handle multi-class time series classification tasks?
- Basis in paper: [explicit] The paper states that "Limitations of the results reported in this paper include the lack of experiments using multi-class time series classification."
- Why unresolved: The paper only reports results on binary classification tasks.
- What evidence would resolve it: Experiments applying AMN to multi-class time series classification datasets and evaluating its performance.

### Open Question 3
- Question: How do attention-based explanations of recurrent networks in AMN interact with the interpretable modular networks?
- Basis in paper: [explicit] The paper mentions that "we shall continue to investigate the practical value of the explanations provided, how they may inform model intervention, and the interactions that may exist between attention-based explanations of recurrent networks and interpretable modular networks."
- Why unresolved: The paper does not provide a detailed analysis of the interactions between the attention-based explanations and the modular networks.
- What evidence would resolve it: In-depth analysis of the attention weights and the shape functions of the modular networks to understand their relationship and how they contribute to the final prediction.

## Limitations
- Missing implementation details for the Attention-based Feature Selection component, particularly the modified multi-head attention mechanism
- Lack of hyperparameter settings and training procedures, making exact reproduction difficult
- Performance improvements may depend heavily on specific datasets and preprocessing choices

## Confidence

- **High Confidence**: The modular network architecture and its interpretability benefits (the GAM-based approach with independent feature learning is well-established)
- **Medium Confidence**: The attention-based feature selection mechanism (conceptually sound but implementation details unclear)
- **Medium Confidence**: The predictive performance claims (based on experiments, but exact reproducibility uncertain due to missing implementation details)

## Next Checks

1. Implement and test the AFS component with synthetic time series data where ground truth feature importance is known to verify the attention mechanism correctly identifies relevant features
2. Compare AMN performance with varying numbers of selected features (n=5, n=10, n=15) on the classification datasets to validate the optimal selection size and AFS effectiveness
3. Visualize and compare the shape functions learned by AMN modules against the true underlying relationships in synthetic datasets to empirically validate the interpretability claims