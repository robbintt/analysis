---
ver: rpa2
title: Domain Generalization In Robust Invariant Representation
arxiv_id: '2304.03431'
source_url: https://arxiv.org/abs/2304.03431
tags:
- invariant
- data
- training
- representations
- transformations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether models that learn representations
  invariant to certain transformations in a training domain also generalize to unseen
  domains. The authors train rotation-invariant variational autoencoders on labeled
  subsets of datasets (MNIST, FashionMNIST, LFW) and test on the remaining labels
  or different datasets.
---

# Domain Generalization In Robust Invariant Representation

## Quick Facts
- arXiv ID: 2304.03431
- Source URL: https://arxiv.org/abs/2304.03431
- Reference count: 5
- Key outcome: Rotation-invariant VAEs learn data-agnostic representations that generalize to unseen domains and labels, achieving strong performance with simple cosine similarity classifiers.

## Executive Summary
This paper investigates whether models that learn representations invariant to certain transformations in a training domain also generalize to unseen domains. The authors train rotation-invariant variational autoencoders on labeled subsets of datasets (MNIST, FashionMNIST, LFW) and test on the remaining labels or different datasets. A simple classifier based on cosine similarity of latent representations achieves strong performance on out-of-distribution data, indicating that the model learns data-agnostic invariant representations. Results show that training on fewer labels still yields high accuracy on unseen data, making the approach resource-efficient.

## Method Summary
The authors use rotation-invariant variational autoencoders (RotInvVAE) based on Bepler et al. (2019) to learn invariant representations. The model consists of an encoder that outputs invariant latent representation z and equivariant group action g, and a decoder that reconstructs the original image from z and g. The model is trained on rotated versions of data from a subset of classes, learning to encode only identity-relevant features into the latent space while discarding rotation information. For testing, a simple classifier calculates cosine similarity between latent representations of unseen image pairs to determine if they show the same object.

## Key Results
- RotInvVAE achieves strong AUC scores (>0.9) on out-of-distribution data using simple cosine similarity classifiers
- Training on fewer labels (e.g., 0-4 out of 0-9) still yields high accuracy on unseen labels (5-9)
- The latent space is unstructured and clusters well for unseen labels, supporting domain-agnostic invariance
- Model generalizes across domains, performing well when trained on MNIST and tested on FashionMNIST

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model learns data-agnostic invariant representations that transfer across unseen domains.
- Mechanism: By training on rotated versions of data from a subset of classes, the VAE learns to encode only the identity-relevant features into the latent space, discarding rotation information. This makes the latent representation invariant to rotations and generalizable to new classes not seen during training.
- Core assumption: Rotation is a task-irrelevant transformation for object recognition, and the model can disentangle rotation from identity in the latent space.
- Evidence anchors:
  - [abstract]: "the invariant model learns unstructured latent representations that are robust to distribution shifts"
  - [section 4]: "the invariant encoder η maps the elements in the same orbit... to the same point (orbit) z ∈ Z = X/G ∀ g ∈ G"
  - [corpus]: Weak - no direct evidence in corpus about rotation invariance generalizing across domains.

### Mechanism 2
- Claim: Simple cosine similarity in the invariant latent space is sufficient for classification on unseen data.
- Mechanism: Because the latent representations are invariant to rotation and cluster well for unseen classes, a simple threshold on cosine similarity between latent vectors can distinguish same vs different objects without needing a complex classifier.
- Core assumption: The invariant latent space has good clustering properties for both seen and unseen classes.
- Evidence anchors:
  - [section 5]: "A classifier then classifies two unseen images of objects from X2 as same or not... The classifier simply calculates the cosine similarity (a normalized dot product) between the latent representations"
  - [section 5]: "we can see the latent space is divided into well-structured clusters for the training class labels... the emergence of clusters in the representation space for the new unseen labels"
  - [corpus]: Missing - no corpus evidence about simple classifiers working on invariant latent spaces.

### Mechanism 3
- Claim: Training on fewer labels still yields high accuracy on unseen labels, making the approach resource-efficient.
- Mechanism: The invariant representations capture general identity features rather than class-specific ones, so the model generalizes well even when trained on a small subset of classes.
- Core assumption: The invariant representation captures the core identity features needed for recognition, not just memorization of training classes.
- Evidence anchors:
  - [abstract]: "training on fewer labels still yields high accuracy on unseen data, making the approach resource-efficient"
  - [section 5]: "we see that RotInvVAE's performance remains consistently high even if we train on fewer and fewer labels across both MNIST and FashionMNIST"
  - [corpus]: Weak - no direct corpus evidence about training on fewer labels generalizing to unseen labels.

## Foundational Learning

- Concept: Variational Autoencoders (VAEs) and their latent space properties
  - Why needed here: The paper uses a VAE architecture to learn invariant representations. Understanding how VAEs encode and decode data, and how the latent space structure emerges, is crucial for understanding why this approach works.
  - Quick check question: In a VAE, what is the difference between the encoder's output and the decoder's input, and how does this relate to learning invariant representations?

- Concept: Group theory and group actions in machine learning
  - Why needed here: The paper explicitly frames the problem using group theory notation (X/G, orbits, group actions). Understanding these concepts is necessary to follow the theoretical claims about invariance.
  - Quick check question: What does it mean for a group G to have an action on a data space X, and how does this relate to the concept of invariance in the model?

- Concept: Domain generalization and distribution shift
  - Why needed here: The paper's main contribution is about domain generalization - how models trained on one domain (subset of classes) perform on unseen domains. Understanding the challenges of distribution shift is key to appreciating the results.
  - Quick check question: What is the difference between domain adaptation and domain generalization, and why is the latter more challenging?

## Architecture Onboarding

- Component map: Image → RotInvVAE Encoder → Latent representation z → Cosine similarity → Classification decision
- Critical path: Image → RotInvVAE Encoder → Latent representation z → Cosine similarity → Classification decision
- Design tradeoffs:
  - Simple classifier (cosine similarity) vs complex classifier: Simple is faster and shows the quality of latent representations, but may miss nuanced differences
  - Training on subset of classes vs all classes: Subset training is more resource-efficient but may miss important class-specific features
  - Dimension of latent space: Higher dimensions may capture more information but risk overfitting and reduce generalization

- Failure signatures:
  - High training accuracy but low test accuracy: Model may be memorizing rather than learning invariant representations
  - Poor clustering in latent space visualization: Invariance property not being properly learned
  - Performance degradation when training on fewer labels: Latent representations may not be as general as hypothesized

- First 3 experiments:
  1. Train RotInvVAE on MNIST classes 0-4, test on classes 5-9 with simple cosine similarity classifier. Compare ROC curves with vanilla VAE.
  2. Vary the number of training classes (e.g., 0-1, 0-2, 0-4) and measure AUC for the same test set to verify resource-efficiency claims.
  3. Train on MNIST, test on FashionMNIST (and vice versa) to verify cross-domain generalization of invariance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can invariant representations learned for one type of transformation (e.g., rotation) generalize to other transformations (e.g., translation, scaling)?
- Basis in paper: [explicit] The paper demonstrates rotation invariance but states the approach can be extended to other transformations like translations and their compositions.
- Why unresolved: The experiments only test rotation invariance, leaving generalization to other transformations unverified.
- What evidence would resolve it: Experiments testing the model's performance on out-of-distribution data with transformations other than rotation, such as translation or scaling.

### Open Question 2
- Question: How does the complexity of the unseen domain (e.g., FashionMNIST vs. MNIST) affect the performance of invariant models?
- Basis in paper: [explicit] The paper notes that the model performed well even when trained on MNIST and tested on the more complex FashionMNIST dataset.
- Why unresolved: While the paper shows performance across different domains, it does not systematically analyze how domain complexity impacts generalization.
- What evidence would resolve it: A study comparing model performance across domains with varying levels of complexity and structure.

### Open Question 3
- Question: Is the emergence of clusters in the latent space for unseen labels consistent across different datasets and transformations?
- Basis in paper: [explicit] The paper observes clustering in RotInvVAE's latent space for unseen MNIST labels but does not generalize this observation.
- Why unresolved: The paper only visualizes latent spaces for MNIST and FashionMNIST, leaving the consistency of clustering across datasets unexplored.
- What evidence would resolve it: Visualization and analysis of latent spaces for unseen labels across multiple datasets and transformations.

## Limitations

- Theoretical framework relies heavily on group theory concepts not fully validated with empirical evidence
- Cross-domain generalization claims are limited to similar visual domains (MNIST→FashionMNIST)
- Resource-efficiency claims lack rigorous analysis of computational costs versus baseline methods

## Confidence

- High confidence: The model successfully learns rotation-invariant representations within MNIST domain (AUC scores >0.9)
- Medium confidence: The cosine similarity classifier works well on unseen labels within the same domain
- Low confidence: Claims about cross-domain generalization and resource-efficiency benefits

## Next Checks

1. Test the model on truly diverse domains (e.g., MNIST→natural images or text) to verify if invariance generalizes beyond similar visual domains
2. Conduct ablation studies comparing computational costs and sample efficiency against standard VAE approaches
3. Perform controlled experiments on digit pairs that are rotation-confusable (6↔9, 2↔5) to test the limits of rotation invariance for the task