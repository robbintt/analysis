---
ver: rpa2
title: Activation Compression of Graph Neural Networks using Block-wise Quantization
  with Improved Variance Minimization
arxiv_id: '2309.11856'
source_url: https://arxiv.org/abs/2309.11856
tags:
- quantization
- activation
- maps
- variance
- stochastic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work presents two improvements to the EXACT method for compressing
  activation maps in graph neural networks (GNNs): block-wise quantization and improved
  variance minimization. Block-wise quantization performs quantization on larger groups
  of tensors, reducing memory consumption and runtime compared to EXACT.'
---

# Activation Compression of Graph Neural Networks using Block-wise Quantization with Improved Variance Minimization

## Quick Facts
- arXiv ID: 2309.11856
- Source URL: https://arxiv.org/abs/2309.11856
- Reference count: 0
- Primary result: Block-wise quantization and improved variance minimization reduce memory consumption by >15% and training time by ~5% compared to EXACT method for GNN activation compression.

## Executive Summary
This paper presents two improvements to EXACT, a method for compressing activation maps in graph neural networks (GNNs). The first improvement is block-wise quantization, which quantizes larger groups of tensors together instead of individual tensors, reducing memory footprint and improving training stability. The second improvement is variance minimization using non-uniform bin widths based on a corrected assumption that activation maps follow a clipped normal distribution rather than a uniform distribution. Experiments on two large-scale graph datasets demonstrate further memory reduction (>15%) and training speedup (~5%) compared to EXACT, with similar performance trade-offs.

## Method Summary
The authors propose two key improvements to the EXACT method for GNN activation compression. First, they introduce block-wise quantization, where node embeddings are reshaped into blocks of length G after random projection, and quantization is performed on these larger groups instead of individual tensors. Second, they correct the assumption in EXACT that activation maps follow a uniform distribution, showing instead that they follow a clipped normal distribution. This leads to an improved variance minimization strategy using non-uniform bin widths for stochastic rounding. The method is evaluated using GraphSAGE architecture with INT2 precision on the OGB-Arxiv and Flickr datasets.

## Key Results
- Block-wise quantization reduces memory consumption by >15% compared to EXACT method
- Improved variance minimization based on clipped normal distribution provides additional memory savings
- Training speedup of ~5% per epoch observed on large-scale graph datasets
- Similar model performance maintained with compressed activations

## Why This Works (Mechanism)

### Mechanism 1
Block-wise quantization improves memory efficiency by quantizing large groups of tensors together instead of individual tensors. By reshaping the node embedding matrix after random projection into blocks of length G and quantizing each block as a whole, memory footprint is reduced and training stability is improved due to outlier localization within blocks. Core assumption: The activation maps follow a clipped normal distribution rather than a uniform distribution. Evidence anchors: The authors correct the assumption in EXACT that activation maps follow a uniform distribution, showing instead that they follow a clipped normal distribution. Found 25 related papers with average neighbor FMR=0.368. Break condition: If activation maps do not follow clipped normal distribution or block size is not optimized, benefits may be reduced.

### Mechanism 2
Improved variance minimization using non-uniform bin widths reduces the quantization error. By modeling the activation maps as a clipped normal distribution and optimizing the quantization boundaries to minimize the expected variance under this distribution, the quantization error is reduced compared to using uniform bin widths. Core assumption: The clipped normal distribution accurately models the activation maps and minimizing the variance of stochastic rounding under this distribution improves performance. Evidence anchors: The authors present a correction to the assumptions on the distribution of intermediate activation maps in EXACT. Found 25 related papers with average neighbor FMR=0.368. Related work includes uncertainty-preserving quantization and block-wise quantization techniques. Break condition: If the clipped normal distribution does not accurately model the activation maps or if variance minimization does not lead to improved performance, this mechanism may not provide benefits.

### Mechanism 3
Combining block-wise quantization with improved variance minimization provides compounding memory reduction and training speedup. By applying both block-wise quantization and improved variance minimization, the activation maps are compressed more efficiently, leading to further reduction in memory consumption (>15%) and training speedup (~5%) compared to EXACT. Core assumption: The combination of block-wise quantization and improved variance minimization provides synergistic benefits in terms of memory and speed. Evidence anchors: Experiments on two large-scale graph datasets demonstrate further memory reduction (>15%) and training speedup (~5%) compared to EXACT. The proposed method yields a further improvement in memory consumption compared with EXACT by about 15%. Found 25 related papers with average neighbor FMR=0.368. Related work includes exact compression and efficient in-memory computing hardware for quantized neural networks. Break condition: If the combination of block-wise quantization and improved variance minimization does not provide synergistic benefits or if the performance trade-offs are not acceptable, this mechanism may not be beneficial.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their memory consumption issues
  - Why needed here: Understanding the memory bottlenecks in GNNs is crucial for appreciating the need for activation compression techniques.
  - Quick check question: What are the main sources of memory consumption in GNNs and why do they scale poorly with graph size?

- Concept: Quantization techniques for deep learning models
  - Why needed here: The paper builds upon and improves existing quantization methods for compressing activation maps in GNNs.
  - Quick check question: What are the key differences between uniform and non-uniform quantization, and how do they impact memory efficiency and model performance?

- Concept: Stochastic rounding and its variance
  - Why needed here: The paper introduces improved variance minimization by modeling activation maps as a clipped normal distribution and optimizing quantization boundaries.
  - Quick check question: How does stochastic rounding work, and why is minimizing its variance important for reducing quantization error?

## Architecture Onboarding

- Component map: GraphSAGE architecture -> Random projection module -> Block-wise quantization module (block size G) -> Variance minimization module -> De-quantization module -> Inverse random projection module

- Critical path:
  1. Forward pass: Node embeddings → Random projection → Block-wise quantization → Store compressed activations
  2. Backward pass: Compressed activations → De-quantization → Inverse random projection → Gradient computation

- Design tradeoffs:
  - Memory vs. computation: Larger block sizes reduce memory consumption but may increase computation overhead.
  - Precision vs. performance: Lower precision (e.g., INT2) provides more memory savings but may impact model performance.
  - Distribution modeling: Accurate modeling of activation map distribution is crucial for effective variance minimization.

- Failure signatures:
  - Increased memory consumption: Block size or quantization precision may not be optimized.
  - Degraded model performance: Variance minimization may not be effective or the distribution modeling may be inaccurate.
  - Slower training: Block-wise quantization or variance minimization may introduce significant computation overhead.

- First 3 experiments:
  1. Reproduce baseline results with EXACT on the OGB-Arxiv and Flickr datasets to establish a performance baseline.
  2. Evaluate the impact of different block sizes (G/R) on memory consumption and training speed while maintaining similar model performance.
  3. Assess the effectiveness of improved variance minimization by comparing the Jensen-Shannon divergence between the observed and modeled activation map distributions with and without variance minimization.

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal block size (G/R) for different graph sizes and GNN architectures to maximize memory reduction and training speedup while maintaining performance? Basis in paper: The paper experimentally analyzes different block sizes (G/R = [2, 4, 8, 16, 32, 64]) and shows further reduction in memory consumption (>15%) and training speedup (~5%) compared to EXACT, but does not provide a clear optimal block size for different scenarios. Why unresolved: The paper does not explore a wide range of graph sizes or GNN architectures beyond the two datasets and one architecture used in the experiments. What evidence would resolve it: Systematic experiments varying graph sizes, node degrees, and GNN architectures to determine the relationship between optimal block size and these factors.

### Open Question 2
How does the performance of block-wise quantization with improved variance minimization compare to other activation compression techniques for GNNs, such as activation pruning or low-rank approximation? Basis in paper: The paper focuses on comparing block-wise quantization with the EXACT method but does not benchmark against other activation compression techniques that could potentially offer better trade-offs between memory reduction, speedup, and performance. Why unresolved: The paper only compares its proposed method to the EXACT baseline and does not explore the broader landscape of activation compression techniques for GNNs. What evidence would resolve it: Direct comparisons between block-wise quantization with improved variance minimization and other activation compression techniques on the same datasets and GNN architectures.

### Open Question 3
Can the block-wise quantization approach be extended to compress weights and gradients in addition to activations, and what would be the potential memory and computational benefits of such an approach? Basis in paper: The paper focuses on compressing activation maps but does not explore the possibility of extending the block-wise quantization approach to other components of the GNN, such as weights and gradients. Why unresolved: The paper only addresses activation compression and does not investigate the potential benefits of compressing other parts of the GNN. What evidence would resolve it: Experiments applying block-wise quantization to weights and gradients, measuring the resulting memory and computational benefits compared to compressing only activations.

## Limitations

- The distribution assumption that activation maps follow a clipped normal distribution requires rigorous statistical validation across different GNN architectures and datasets.
- The computational overhead of block-wise quantization at larger block sizes is not fully characterized, particularly for scalability to larger graphs.
- The synergistic benefits of combining both improvements are not isolated or thoroughly analyzed to validate the compounding effect.

## Confidence

**High Confidence:** The block-wise quantization mechanism itself is straightforward and well-supported by the experimental results showing consistent memory reduction across datasets. The improvement in memory efficiency (>15% compared to EXACT) is demonstrated empirically and follows directly from the implementation approach.

**Medium Confidence:** The improved variance minimization through clipped normal distribution modeling and non-uniform bin widths is theoretically sound but depends on the accuracy of the distribution assumption. The experimental validation is limited to two datasets, and the sensitivity to this assumption is not explored.

**Medium Confidence:** The combination of both improvements providing compounding benefits is supported by the experimental results, but the synergistic effect is not isolated or thoroughly analyzed. The ~5% training speedup claim is based on limited experimental runs and may vary with different hardware configurations.

## Next Checks

1. **Distribution Assumption Validation:** Perform rigorous statistical testing (e.g., Kolmogorov-Smirnov test, Q-Q plots) across multiple GNN architectures and datasets to validate the clipped normal distribution assumption for activation maps. Test the sensitivity of variance minimization performance to distribution mis-specification.

2. **Scalability and Computational Overhead Analysis:** Conduct experiments measuring the computational overhead of block-wise quantization at different block sizes, particularly for larger block sizes (G/R=64). Analyze the trade-off between memory savings and additional computation time across different hardware configurations.

3. **Distribution-Agnostic Alternative Testing:** Implement a version of block-wise quantization without the improved variance minimization (using uniform bin widths instead) to isolate the contribution of each improvement. Compare memory savings, training speed, and accuracy across this baseline, EXACT, and the full proposed method to validate the synergistic benefits claim.