---
ver: rpa2
title: Predicting Question-Answering Performance of Large Language Models through
  Semantic Consistency
arxiv_id: '2311.01152'
source_url: https://arxiv.org/abs/2311.01152
tags:
- qcat
- spop
- scons
- cert
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce PopQA-TP, a large dataset of factual questions with
  manually-curated paraphrases, for benchmarking semantic consistency of LLMs. Semantic
  consistency is shown to correlate with answer correctness, and used as a predictor
  for factual QA performance prediction.
---

# Predicting Question-Answering Performance of Large Language Models through Semantic Consistency

## Quick Facts
- arXiv ID: 2311.01152
- Source URL: https://arxiv.org/abs/2311.01152
- Reference count: 7
- We introduce PopQA-TP, a large dataset of factual questions with manually-curated paraphrases, for benchmarking semantic consistency of LLMs. Semantic consistency is shown to correlate with answer correctness, and used as a predictor for factual QA performance prediction. We demonstrate strong results for the task, significantly outperforming baseline models.

## Executive Summary
This paper introduces PopQA-TP, a large-scale dataset of factual questions with manually-curated paraphrases, to benchmark semantic consistency of large language models (LLMs). The authors show that semantic consistency—measured as the stability of answers to meaning-preserving paraphrases—correlates with factual correctness and can predict QA performance. By combining consistency with metrics like answer certainty and subject popularity in a logistic regression framework, the approach significantly outperforms baseline models. The work highlights semantic consistency as a proxy for model robustness and a valuable tool for reference-less evaluation.

## Method Summary
The authors construct PopQA-TP from the PopQA dataset by manually creating 3-10 paraphrase templates per question, ensuring high semantic similarity (cosine ≈0.914). For each LLM, they generate answers to original questions and paraphrases, then compute semantic consistency (SCons) as the mean pairwise cosine similarity of answer embeddings. Answer certainty (Cert) is calculated via sampling, and question subject popularity (SPop) is derived from Wikipedia page views. A logistic regression model predicts answer correctness using QCat, SCons, Cert, and log(SPop), including interaction terms. The model is trained per LLM and evaluated on a held-out test set.

## Key Results
- Semantic consistency correlates positively with answer correctness across question categories.
- Logistic regression combining consistency, certainty, and popularity predicts correctness with R² values of 0.419-0.491.
- The approach achieves 2.6-26.2% accuracy gains over majority baseline.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic consistency (robustness to meaning-preserving paraphrases) correlates positively with factual correctness in LLMs.
- Mechanism: When an LLM consistently produces similar answers to semantically equivalent paraphrases, it signals stable internal representation of factual knowledge, which is more likely to align with ground truth.
- Core assumption: Paraphrases in the dataset are truly semantically equivalent and capture the same factual query.
- Evidence anchors:
  - [abstract]: "Semantic consistency is shown to correlate with answer correctness, and used as a predictor for factual QA performance prediction."
  - [section 3.3.2]: "Across categories, answer correctness and consistency are positively correlated."
  - [corpus]: Weak – no direct citations, but neighbors discuss consistency-based hallucination detection, suggesting relevance.
- Break condition: If paraphrases are not semantically equivalent (e.g., contain subtle meaning shifts), correlation breaks down.

### Mechanism 2
- Claim: Logistic regression combining semantic consistency, certainty, and subject popularity predicts answer correctness better than baselines.
- Mechanism: Each predictor captures a different aspect of model reliability: consistency (stability), certainty (variability), and popularity (memorization strength). Their interaction with question category allows tailored prediction.
- Core assumption: The numeric predictors have linear effects on the logit of correctness, and interactions are meaningful.
- Evidence anchors:
  - [abstract]: "Combining this predictor with additional metrics suggested in prior work as correlating with LLM QA accuracy..."
  - [section 4.3]: Reports R² values 0.419–0.491 and accuracy gains of 2.6–26.2% over majority baseline.
  - [corpus]: Weak – neighbors discuss consistency metrics but not integrated logistic frameworks.
- Break condition: If predictor effects are non-linear or interactions negligible, logistic model underperforms.

### Mechanism 3
- Claim: Manually curated paraphrase templates yield higher semantic similarity than automatic generation, enabling reliable consistency measurement.
- Mechanism: Human-created templates avoid noise from automatic paraphrasers, ensuring high cosine similarity (≈0.914) and thus meaningful consistency scores.
- Core assumption: Manual templates cover grammatical variations and preserve semantics across categories.
- Evidence anchors:
  - [section 2.1]: "Calculating the average paraphrase quality... we obtain a high value of 0.914; this shows that the templated paraphrases are sufficiently similar to the original questions."
  - [section 3.3.2]: Contrasts manual templates with "sub-optimal quality of automatic paraphrases."
  - [corpus]: Weak – no direct evidence; neighbors use automatic methods, implying manual curation is notable.
- Break condition: If templates introduce bias or miss linguistic variation, consistency measurement degrades.

## Foundational Learning

- Concept: Semantic consistency metric
  - Why needed here: Provides a measurable proxy for model robustness to meaning-preserving input changes.
  - Quick check question: Does a higher consistency score always mean the answers are factually correct?
- Concept: Logistic regression with interactions
  - Why needed here: Captures how predictor effects vary across question categories.
  - Quick check question: What happens to the model if you drop the interaction terms?
- Concept: Certainty via answer sampling
  - Why needed here: Quantifies variability in model outputs, indicating confidence.
  - Quick check question: How does sampling temperature affect the certainty score?

## Architecture Onboarding

- Component map:
  - PopQA-TP dataset (manual paraphrase templates) -> LLMs under test (encoder-decoder and decoder-only) -> Answer generation (greedy + sampling) -> Embedding model (SentenceTransformer) -> Consistency and certainty calculation -> Logistic regression pipeline (predictors → accuracy)
- Critical path:
  1. Load PopQA-TP and query each LLM with original + paraphrases.
  2. Compute SCons (cosine similarity of answers).
  3. Sample answers for Cert (certainty).
  4. Retrieve SPop (Wikipedia monthly views).
  5. Train logistic regression per LLM, evaluate on held-out set.
- Design tradeoffs:
  - Manual templates ensure quality but are labor-intensive.
  - Greedy decoding vs sampling: greedy is stable but may miss uncertainty.
  - Encoder-decoder vs decoder-only: different prompt formatting and performance patterns.
- Failure signatures:
  - Low SCons but high correctness: inconsistency not captured by metric.
  - High Cert but low correctness: model is certain yet wrong.
  - Poor logistic fit (low pseudo-R²): predictors miss key variance.
- First 3 experiments:
  1. Run consistency-only regression (SCons only) and compare to full model.
  2. Vary sampling temperature in Cert calculation and observe stability.
  3. Test logistic regression with and without QCat interactions to quantify benefit.

## Open Questions the Paper Calls Out

- Can the semantic consistency metric used in this work be effectively adapted to evaluate generative tasks beyond factual QA, such as summarization or translation?
- How does the inclusion of the question category (QCat) as a predictor in the logistic regression model impact the model's performance on categories with low baseline correctness rates?
- What is the impact of using non-greedy sampling (e.g., with temperature > 0) on the semantic consistency metric and its predictive power for QA performance?

## Limitations
- The core assumption that manually curated paraphrases are truly semantically equivalent to original questions is critical but not empirically validated.
- The logistic regression model's performance (R² 0.419-0.491) shows moderate predictive power, but it's unclear how well this generalizes to LLMs beyond the five tested or to different question domains not covered in PopQA-TP.
- The paper uses temperature=0.5 for certainty calculation but doesn't explore sensitivity to this hyperparameter.

## Confidence
- High confidence: Semantic consistency correlates with answer correctness.
- Medium confidence: Manually curated paraphrases yield higher quality than automatic generation.
- Low confidence: Logistic regression with interactions significantly outperforms baselines.

## Next Checks
1. Conduct human evaluation to verify that 100 randomly selected paraphrase-original pairs are truly semantically equivalent, measuring inter-annotator agreement and identifying potential semantic drift.
2. Train the logistic regression model on one LLM and test its predictive accuracy on the other four LLMs, quantifying performance drop to assess generalizability.
3. Repeat Cert calculations across a range of sampling temperatures (0.1 to 1.0) and measure the stability of the logistic regression model's performance, identifying optimal temperature settings.