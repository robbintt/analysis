---
ver: rpa2
title: 'SynFundus-1M: A High-quality Million-scale Synthetic fundus images Dataset
  with Fifteen Types of Annotation'
arxiv_id: '2312.00377'
source_url: https://arxiv.org/abs/2312.00377
tags:
- images
- dataset
- synfundus-1m
- image
- fundus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces SynFundus-1M, a synthetic dataset of over\
  \ 1 million high-quality fundus images with extensive annotations for 11 disease\
  \ types and 4 readability labels. Generated using a large private dataset and an\
  \ AI-assisted diagnostic system, SynFundus-1M achieves superior Fr\xE9chet Inception\
  \ Distance (FID) scores compared to existing methods and demonstrates strong performance\
  \ in downstream tasks, including retinal disease diagnosis."
---

# SynFundus-1M: A High-quality Million-scale Synthetic fundus images Dataset with Fifteen Types of Annotation

## Quick Facts
- **arXiv ID**: 2312.00377
- **Source URL**: https://arxiv.org/abs/2312.00377
- **Reference count**: 24
- **Primary result**: A synthetic dataset of over 1 million high-quality fundus images with 11 disease types and 4 readability labels, achieving superior FID scores and strong performance in retinal disease diagnosis tasks.

## Executive Summary
This paper introduces SynFundus-1M, a large-scale synthetic fundus image dataset generated using a Denoising Diffusion Probabilistic Model (DDPM) trained on a private collection of over 1.3 million authentic fundus images. The dataset includes extensive annotations for 11 disease types and 4 readability labels, validated by both AI-assisted diagnosis and ophthalmologist evaluations. SynFundus-1M demonstrates superior quality metrics compared to existing synthetic datasets and shows significant improvements in downstream retinal disease diagnosis tasks when used for pre-training, outperforming models pre-trained on ImageNet.

## Method Summary
The authors trained a Denoising Diffusion Probabilistic Model (DDPM) on a large private dataset of authentic fundus images to generate synthetic fundus images with predefined disease conditions. An AI-assisted diagnostic system certified by NMPA Class III was used to provide consistent annotations for the synthetic images, addressing the challenge of manual annotation for large-scale datasets. The generated images were evaluated for quality using Fréchet Inception Distance (FID) scores and validated by ophthalmologists to ensure authenticity. The dataset was then used to pre-train deep learning models, which were fine-tuned on downstream tasks to demonstrate improved performance and faster convergence compared to ImageNet pre-training.

## Key Results
- SynFundus-1M achieved superior FID scores compared to existing synthetic fundus image datasets
- Models pre-trained on SynFundus-1M demonstrated faster convergence and better performance on downstream retinal disease diagnosis tasks
- Ophthalmologists could hardly distinguish synthetic images from real ones, confirming high authenticity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic fundus images can substitute for real data in training deep learning models for retinal disease diagnosis.
- Mechanism: The Denoising Diffusion Probabilistic Model (DDPM) generates synthetic images that are visually and diagnostically indistinguishable from real fundus images, allowing models to learn disease-related visual features without requiring access to sensitive patient data.
- Core assumption: The synthetic images preserve the statistical and structural properties of real fundus images necessary for disease diagnosis tasks.
- Evidence anchors:
  - [abstract] "experienced annotators can hardly distinguish the synthetic images from authentic ones"
  - [section] "ophthalmologists evaluation validate the difficulty in discerning these synthetic images from real ones, confirming the SynFundus-1M's authenticity"
  - [corpus] Weak evidence - related papers focus on image enhancement and synthetic data generation but don't directly validate diagnostic equivalence
- Break condition: If synthetic images fail to capture subtle pathological features essential for diagnosis, or if domain shift between synthetic and real images causes performance degradation in downstream tasks.

### Mechanism 2
- Claim: Pre-training on synthetic medical images improves downstream task performance compared to traditional pre-training on ImageNet.
- Mechanism: Medical domain-specific pre-training on a large dataset of synthetic fundus images provides better initialization for retinal disease diagnosis models than general natural image pre-training.
- Core assumption: The visual features relevant to retinal disease diagnosis are sufficiently represented in the synthetic fundus image distribution.
- Evidence anchors:
  - [abstract] "models trained on SynFundus-1M not only achieve superior performance but also demonstrate faster convergence on various downstream tasks"
  - [section] "models fine-tuned with SynFundus pre-trained weights consistently outperformed those leveraging ImageNet pre-training across various model configurations"
  - [corpus] Weak evidence - related papers discuss synthetic data for segmentation but don't directly compare pre-training strategies
- Break condition: If the synthetic images fail to capture the full diversity of real-world fundus image variations, or if the disease representations are biased or incomplete.

### Mechanism 3
- Claim: AI-assisted diagnostic systems can provide reliable annotations for synthetic medical images, enabling large-scale dataset creation.
- Mechanism: A certified AI-assisted diagnostic system with NMPA Class III certification provides consistent disease annotations for both real and synthetic fundus images, overcoming the cost and privacy barriers of manual annotation.
- Core assumption: The AI diagnostic system's performance on synthetic images correlates with its performance on real images.
- Evidence anchors:
  - [section] "we employ an AI-assisted diagnostic system to supplement missing annotations... This assisted diagnostic system has not only obtained the National Medical Products Administration(NMPA) Class III medical device certification"
  - [section] "we observed that the system's diagnostic outcomes did not always align perfectly with the generative conditions"
  - [corpus] Weak evidence - related papers focus on automated classification but don't discuss synthetic image annotation
- Break condition: If the AI diagnostic system has systematic biases or errors when applied to synthetic images, or if the annotations don't reflect clinical reality.

## Foundational Learning

- Concept: Denoising Diffusion Probabilistic Models (DDPMs)
  - Why needed here: Understanding how the SynFundus-Generator creates realistic synthetic images through iterative noise addition and removal
  - Quick check question: How does a DDPM differ from a traditional GAN in terms of training stability and image quality?

- Concept: Fréchet Inception Distance (FID)
  - Why needed here: Evaluating the similarity between synthetic and real fundus image distributions to assess synthetic image quality
  - Quick check question: What does a lower FID score indicate about the relationship between two image datasets?

- Concept: Domain adaptation and transfer learning
  - Why needed here: Understanding how pre-training on synthetic fundus images transfers to real-world diagnostic tasks
  - Quick check question: What factors might cause poor transfer performance when pre-training on synthetic data?

## Architecture Onboarding

- Component map: Synthetic image generation (DDPM) -> Annotation (AI system) -> Dataset curation -> Model training (pre-training/fine-tuning) -> Evaluation (FID/downstream metrics)
- Critical path: Image generation (DDPM) -> Annotation (AI system) -> Model training (pre-training/fine-tuning) -> Evaluation (FID/downstream metrics)
- Design tradeoffs:
  - Synthetic vs. real data: Privacy benefits vs. potential domain gap
  - Annotation automation vs. accuracy: Cost savings vs. potential labeling errors
  - Dataset size vs. quality: More samples vs. maintaining diagnostic fidelity
- Failure signatures:
  - High FID scores between synthetic and real images
  - Poor downstream task performance despite good pre-training metrics
  - AI diagnostic system showing systematic annotation errors
- First 3 experiments:
  1. Generate a small set of synthetic images and compare FID scores against real fundus datasets to validate image quality
  2. Train a simple classifier on synthetic images and evaluate on real images to assess domain gap
  3. Compare pre-training performance on SynFundus vs. ImageNet using the same downstream task and model architecture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the SynFundus-1M dataset perform in generating images for diseases with localized or subtler pathologies compared to diseases with pronounced, widespread lesions?
- Basis in paper: [explicit] The paper mentions that SynFundus-1M demonstrates significant similarity with real-world images, particularly in cases of diseases with pronounced, widespread lesions, but suggests a potential area for refinement in generating images for conditions marked by localized or subtler pathologies.
- Why unresolved: The paper acknowledges this as an area for improvement but does not provide detailed quantitative or qualitative results comparing the performance for different types of diseases.
- What evidence would resolve it: Detailed quantitative analysis comparing the performance of SynFundus-1M in generating images for diseases with pronounced, widespread lesions versus those with localized or subtler pathologies, including specific metrics and examples.

### Open Question 2
- Question: What are the specific privacy risks associated with using synthetic medical images for training AI models, and how does SynFundus-1M address these risks?
- Basis in paper: [explicit] The paper discusses privacy concerns related to medical images and mentions that SynFundus-1M comprises solely synthetic images, which are considered relatively safe for open-source distribution.
- Why unresolved: While the paper outlines general privacy measures, it does not provide a detailed analysis of the specific privacy risks associated with synthetic medical images or how SynFundus-1M mitigates these risks.
- What evidence would resolve it: A comprehensive privacy risk assessment specific to synthetic medical images, including potential vulnerabilities and how SynFundus-1M addresses them, along with empirical data supporting the safety of the dataset.

### Open Question 3
- Question: How does the performance of models trained on SynFundus-1M compare to those trained on real-world datasets in terms of generalization to new, unseen data?
- Basis in paper: [inferred] The paper highlights the effectiveness of SynFundus-1M in improving model performance and convergence rates, but does not directly compare the generalization capabilities of models trained on synthetic versus real-world data.
- Why unresolved: The paper does not provide empirical data comparing the generalization performance of models trained on SynFundus-1M to those trained on real-world datasets.
- What evidence would resolve it: Comparative studies evaluating the generalization performance of models trained on SynFundus-1M versus real-world datasets, using metrics such as accuracy, precision, recall, and F1-score on unseen data.

## Limitations

- The study relies heavily on synthetic data generation without extensive validation of clinical equivalence between synthetic and real fundus images
- The AI-assisted diagnostic system's annotations, while certified, are not independently validated against expert consensus for the synthetic images
- The dataset's practical utility depends on whether the synthetic images capture the full spectrum of pathological variations seen in clinical practice

## Confidence

- **High Confidence**: The technical implementation of DDPM for image generation and FID-based quality evaluation
- **Medium Confidence**: Claims about downstream task performance improvements, pending independent replication
- **Low Confidence**: Clinical equivalence of synthetic images to real fundus images, due to limited expert validation scope

## Next Checks

1. Conduct a double-blind study with multiple independent ophthalmologists to assess diagnostic accuracy on synthetic versus real images for all 11 disease types
2. Evaluate model performance on real-world clinical datasets not seen during training to test generalization across institutions and imaging equipment
3. Analyze synthetic image diversity by measuring coverage of rare disease presentations and subtle pathological variations not well-represented in the training data