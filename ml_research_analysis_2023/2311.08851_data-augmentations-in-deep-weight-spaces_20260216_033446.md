---
ver: rpa2
title: Data Augmentations in Deep Weight Spaces
arxiv_id: '2311.08851'
source_url: https://arxiv.org/abs/2311.08851
tags:
- weight
- data
- space
- augmentations
- augmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates data augmentation techniques for learning
  in deep weight spaces, where neural networks process the weights of other deep neural
  networks. The authors propose a taxonomy of augmentation schemes and introduce a
  novel Weight-Space Mixup method based on the Mixup technique.
---

# Data Augmentations in Deep Weight Spaces

## Quick Facts
- arXiv ID: 2311.08851
- Source URL: https://arxiv.org/abs/2311.08851
- Reference count: 17
- Key outcome: Weight-Space Mixup achieves up to 18% accuracy gain on INR classification, equivalent to reducing required data by 10×

## Executive Summary
This paper addresses the challenge of data augmentation in deep weight spaces, where neural networks process the weights of other neural networks. The authors propose a taxonomy of augmentation schemes and introduce a novel Weight-Space Mixup method based on the Mixup technique. Their experiments demonstrate that these augmentations, particularly the alignment-based mixup, can significantly improve performance on INR classification benchmarks while substantially reducing the amount of generated data needed for training.

## Method Summary
The paper investigates data augmentation techniques for learning in deep weight spaces by proposing four categories of augmentations: input-space augmentations (applying geometric transformations to input coordinates), generic augmentations (dropout, Gaussian noise, Mixup), activation function-inspired augmentations (leveraging symmetries in ReLU and SIREN activations), and Weight-Space Mixup (interpolating between aligned weight vectors). The Weight-Space Mixup method finds optimal permutations between weight space elements to create valid interpolations that preserve model function through linear mode connectivity. The approach is evaluated on newly generated INR datasets from ModelNet40 and Fashion-MNIST, showing consistent improvements across different models and datasets.

## Key Results
- Weight-Space Mixup with alignment achieves up to 18% accuracy gain on INR classification benchmarks
- Performance improvement equivalent to reducing required generated data by almost 10×
- Consistent improvements observed across both ModelNet40 and Fashion-MNIST datasets with different architectures (DWSNet and GNN)
- SIREN-specific augmentations (negation, bias shift) provide effective regularization in weight spaces

## Why This Works (Mechanism)

### Mechanism 1
Weight space mixup with alignment reduces overfitting by creating smoother interpolation paths that preserve model function. The mixup method creates interpolated weight vectors that lie in regions of low loss when permutations are optimally aligned, leveraging linear mode connectivity. This produces diverse training samples without needing to train additional input weight space elements.

### Mechanism 2
Activation-based augmentations exploit inherent symmetries in activation functions to generate valid weight variations. Symmetries in ReLU (scaling) and SIREN (negation, phase shift) allow weight space transformations that preserve function while creating diversity. These augmentations modify weights across all layers, not just the first.

### Mechanism 3
Input-space augmentations applied to the first layer effectively simulate data augmentation in the original input space. Geometric transformations (rotation, scaling, translation) applied to input coordinates translate to linear transformations of the first weight matrix, creating diverse weight space elements that correspond to transformed inputs.

## Foundational Learning

- Concept: Permutation symmetry in neural network weights
  - Why needed here: Weight space elements have inherent permutation invariance that must be accounted for when designing augmentations
  - Quick check question: Why can't we directly apply standard mixup to weight space elements without alignment?

- Concept: Implicit Neural Representations (INRs)
  - Why needed here: The paper operates on weights of networks that represent data as functions (INRs), requiring understanding of how these are trained and structured
  - Quick check question: What distinguishes SIREN activations from standard ReLU in terms of symmetry properties?

- Concept: Linear mode connectivity
  - Why needed here: This property justifies why aligned weight interpolation produces valid models with low loss
  - Quick check question: How does linear mode connectivity differ from general loss landscape properties?

## Architecture Onboarding

- Component map: Dataset Generation -> INR Training -> Baseline Model Training -> Augmentation Application -> Classification Evaluation
- Critical path: Generate INR datasets → Train baseline models → Apply augmentations → Evaluate accuracy gains
- Design tradeoffs: Mixup with alignment provides best accuracy but requires additional computation for weight matching; simpler augmentations are faster but less effective
- Failure signatures: Overfitting (train/test gap increases), invalid model generation (NaN weights), or minimal accuracy improvement despite augmentation
- First 3 experiments:
  1. Train baseline DWSNet on ModelNet40 with 1 view, measure train/test gap
  2. Apply SIREN negation augmentation to same setup, compare performance
  3. Implement alignment-based mixup and evaluate on both ModelNet40 and FMNIST datasets

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the content, several important questions remain unexplored:

1. How do the proposed weight space augmentation techniques generalize to other architectures beyond DWS and GNN, such as Transformers or CNNs operating on weight spaces?
2. What is the theoretical justification for the effectiveness of SIREN-specific augmentations (negation and bias) in weight spaces, and how do they relate to the underlying function representation?
3. How do the computational costs of different alignment-based weight space mixup variants compare, and what is the trade-off between alignment quality and computational efficiency?
4. What is the optimal strategy for selecting augmentation parameters (e.g., dropout rates, noise levels, mixup coefficients) in weight spaces, and how do these parameters interact with different datasets and architectures?

## Limitations

- Theoretical justification for weight-space mixup effectiveness is limited, particularly regarding linear mode connectivity in this specific context
- Permutation alignment procedure lacks rigorous analysis of computational complexity and optimality guarantees
- Augmentations are primarily evaluated on INR datasets, leaving uncertainty about generalizability to other weight space learning tasks

## Confidence

- High confidence: Claims about baseline augmentation effectiveness (input-space, generic, activation-based) are well-supported by consistent results across multiple datasets and models
- Medium confidence: The weight-space mixup mechanism is supported by empirical evidence but lacks complete theoretical grounding for why optimal alignment always exists
- Low confidence: Claims about computational savings (10× reduction in generated data) are extrapolated from accuracy improvements without direct measurement of training time or resource usage

## Next Checks

1. Analyze the computational complexity of the permutation alignment algorithm and establish bounds on its optimality guarantees
2. Evaluate augmentation performance on non-INR weight space tasks (e.g., weight prediction for image classification networks) to assess broader applicability
3. Systematically remove individual augmentation components to quantify their individual contributions to the observed performance gains