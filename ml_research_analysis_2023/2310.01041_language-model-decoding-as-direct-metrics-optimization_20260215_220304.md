---
ver: rpa2
title: Language Model Decoding as Direct Metrics Optimization
arxiv_id: '2310.01041'
source_url: https://arxiv.org/abs/2310.01041
tags:
- distribution
- decoding
- human
- which
- texts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper frames decoding from a language model as an optimization
  problem to strictly match expected performance with human texts measured by multiple
  evaluation metrics. The resulting decoding distribution scales the input LM distribution
  via a sequence-level energy function defined by the metrics, which is proven to
  improve perplexity on human texts and provide a better approximation to the underlying
  human text distribution.
---

# Language Model Decoding as Direct Metrics Optimization

## Quick Facts
- arXiv ID: 2310.01041
- Source URL: https://arxiv.org/abs/2310.01041
- Reference count: 40
- Primary result: Decoding from LM as optimization to match expected metrics with human texts, proven to improve perplexity and human evaluation scores

## Executive Summary
This paper presents DAEMON, a decoding framework that treats language model generation as an optimization problem to strictly match expected evaluation metric scores with human texts. The method frames decoding as minimizing reverse KL divergence subject to constraints that the decoding distribution's expected metric scores match those of human texts. This leads to an analytical solution that scales the language model distribution via a sequence-level energy function defined by evaluation metrics. The framework uses Sampling-Importance-Resampling to enable tractable sampling from the globally normalized distribution, achieving superior metrics alignment and human evaluation scores compared to strong baselines.

## Method Summary
The method frames decoding as minimizing reverse KL divergence between a decoding distribution q and the language model distribution p_θ, subject to constraints that q's expected metric scores match human text expectations. This optimization yields an energy-based model form q(x) ∝ p_θ(x) exp(-E_µ(x)) where E_µ(x) = µ^T f(x) is a sequence-level energy function combining K evaluation metrics with learned coefficients µ. Coefficients are estimated via Weighted Importance Sampling to satisfy the metric constraints, and Sampling-Importance-Resampling generates continuations by sampling M candidates from the proposal model and resampling based on importance weights. The approach is theoretically grounded with convergence guarantees and practical implementation using autoregressive models.

## Key Results
- Improves perplexity on human texts while maintaining generation quality
- Achieves superior alignment with human texts across multiple evaluation metrics compared to strong decoding baselines
- Demonstrates effectiveness across various domains and model scales (GPT-2 XL 1.5B, OPT-6.7B)
- Shows strong performance in human evaluation of fluency, coherence, and informativeness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The reverse KL divergence objective forces the decoding distribution to concentrate probability mass only within the support of the language model, avoiding overestimation of low-quality regions.
- Mechanism: By minimizing DKL(q||pθ), the optimization pulls q away from the unreliable tail of pθ while preserving high-quality modes, ensuring generated samples stay within the support of human-like texts.
- Core assumption: The language model's support already contains mostly high-quality samples; low-quality samples lie in the unreliable tail.
- Evidence anchors:
  - [abstract] "We choose the reverse KL to induce the decoding distribution q, as it forces q to recover the major probability masses within the support of pθ (Huszar, 2015; Malinin & Gales, 2019), which contains mostly high-quality samples."
  - [section 2.1] "We believe the learning and decoding phases posit different goals: the former is to capture all modes in data, while the latter is to decode only high-quality ones."
  - [corpus] Weak. No direct corpus support for this specific claim about tail reliability.
- Break condition: If the language model's tail contains high-quality but rare samples, reverse KL would exclude them, hurting performance.

### Mechanism 2
- Claim: The energy-based model formulation with sequence-level energy function E_µ(x) = µ^T f(x) allows flexible integration of multiple evaluation metrics to directly align generation with human texts.
- Mechanism: The analytical solution p_θ,µ(x) ∝ p_θ(x) exp(-E_µ(x)) scales the input LM distribution via a sequence-level energy function defined by the metrics, enabling strict matching of expected metric scores between generated and human texts.
- Core assumption: The set of evaluation metrics can adequately characterize the aspects of human text quality we care about.
- Evidence anchors:
  - [abstract] "The resulting decoding distribution enjoys an analytical solution that scales the input language model distribution p_θ via a sequence-level energy function defined by the metrics."
  - [section 2.1] "The resulting decoding distribution qopt = arg min_{q∈P} DKL(q||p_θ) subject to E_{q}[f_k(x)] = E_{p_d}[f_k(x)] leads to the EBM form."
  - [corpus] Weak. No direct corpus support for the claim about metric adequacy.
- Break condition: If evaluation metrics are poorly chosen or conflicting, the energy function cannot properly capture human text quality.

### Mechanism 3
- Claim: Sampling-Importance-Resampling (SIR) provides tractable approximation of the globally normalized decoding distribution while maintaining theoretical convergence guarantees.
- Mechanism: SIR first samples M candidates from the proposal model p_θ, then resamples based on importance weights exp(-E_µ(x)), converging to the exact conditional distribution p_θ,µ(·|x≤t₀) as M→∞.
- Core assumption: The proposal distribution p_θ is a reasonable approximation of the target decoding distribution p_θ,µ, allowing efficient importance sampling.
- Evidence anchors:
  - [section 2.3.2] "In the limit of M → ∞, the empirical distribution ˆp^M_θ,µ(·|x≤t₀) induced by SIR recovers the exact conditional distribution p_θ,µ(·|x≤t₀)."
  - [section 2.3.2] "Skare et al. (2003) proved that the point-wise relative error of the empirical distribution induced by SIR is O(M^{-1})."
  - [corpus] Weak. No direct corpus support for SIR's effectiveness in this specific application.
- Break condition: If the proposal distribution is too different from the target, importance weights become highly variable, requiring impractically large M.

## Foundational Learning

- Concept: Kullback-Leibler divergence and its forward/reverse forms
  - Why needed here: The choice between forward and reverse KL determines whether the decoding distribution covers all modes or focuses on high-quality regions
  - Quick check question: What's the key difference in behavior between minimizing DKL(q||p) vs DKL(p||q) when q is restricted to a subset of p's support?

- Concept: Energy-based models and their relationship to exponential families
  - Why needed here: The decoding distribution is an EBM with energy function E_µ(x) = µ^T f(x), requiring understanding of how EBMs represent distributions
  - Quick check question: How does the form p(x) ∝ exp(-E(x)) relate to maximum entropy principles when constraints are equality constraints?

- Concept: Importance sampling and resampling techniques
  - Why needed here: SIR is used to approximate sampling from the globally normalized EBM, requiring understanding of weighted sampling and convergence properties
  - Quick check question: What is the variance of importance sampling estimates, and how does it relate to the similarity between proposal and target distributions?

## Architecture Onboarding

- Component map: Language model p_θ -> Evaluation metrics f_k(x) -> Energy function E_µ(x) = µ^T f(x) -> Coefficients µ -> SIR sampling -> Generated text
- Critical path: Language model → Evaluation metrics → Energy function → Coefficients µ → SIR sampling → Generated text
- Design tradeoffs:
  - More metrics K → better alignment but harder coefficient estimation and potential conflicts
  - Larger M → better SIR approximation but higher computational cost
  - Lower τ → more diverse candidates but potentially lower quality
  - Different temperature τ for inference vs coefficient estimation → consistency vs performance
- Failure signatures:
  - High repetition despite low SR metrics → metric definition issues or coefficient estimation problems
  - Poor coherence despite high COH metric → evaluation metric mismatch or proposal distribution issues
  - Slow convergence during coefficient estimation → poor initial proposal samples or metric scaling issues
- First 3 experiments:
  1. Verify coefficient estimation: Run with one metric (e.g., SR-4) and check if estimated µ reduces repetition while maintaining perplexity
  2. SIR approximation test: Vary M from 5 to 100 and measure convergence of SR-4, TR-32, and COH metrics
  3. Temperature sensitivity: Sweep τ from 0.8 to 1.0 and plot quality-diversity tradeoff curves for DIV vs COH metrics

## Open Questions the Paper Calls Out
- No explicit open questions were called out in the paper

## Limitations
- The effectiveness depends critically on the language model's support containing mostly high-quality samples, with weak empirical validation of this assumption
- The method's performance is sensitive to metric selection and weighting, with potential conflicts between different evaluation metrics
- Sampling-Importance-Resampling convergence requires large candidate sets M, with unclear practical requirements for reliable generation

## Confidence
- **High confidence** in the mathematical framework and derivations: The reverse KL optimization leading to the EBM form, and the SIR sampling procedure are well-established techniques with sound theoretical foundations.
- **Medium confidence** in the practical effectiveness: Experiments show improved metrics alignment and human evaluation scores, but the critical assumption about language model support quality and the sensitivity to metric selection remain poorly validated.
- **Low confidence** in the claims about reverse KL specifically targeting high-quality regions: The paper asserts this mechanism but provides minimal empirical evidence distinguishing it from other divergence choices or validating that the language model tail is indeed unreliable.

## Next Checks
1. **Tail quality analysis**: Systematically evaluate the quality distribution across p_θ's probability spectrum. Generate samples from different probability regions (high, medium, low) and measure human quality scores to verify the claim that low-probability regions are predominantly low-quality.

2. **Forward vs reverse KL ablation**: Implement both forward and reverse KL decoding variants and compare their behavior on rare but high-quality samples. This would directly test whether reverse KL's "focusing" behavior is beneficial or harmful for capturing the full range of human-like text quality.

3. **SIR convergence validation**: Conduct systematic experiments varying M from very small (5) to large (1000+) values, measuring the variance and bias in generated samples' metric scores. This would empirically validate whether the claimed O(M^-1) convergence rate holds in practice and determine the minimum M required for reliable generation.