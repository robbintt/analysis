---
ver: rpa2
title: Offline Diversity Maximization Under Imitation Constraints
arxiv_id: '2307.11373'
source_url: https://arxiv.org/abs/2307.11373
tags:
- offline
- skill
- learning
- expert
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses offline skill discovery with imitation constraints.
  It proposes an algorithm (DOI) that maximizes mutual information between skills
  and states subject to KL-divergence constraints, ensuring learned skills imitate
  expert demonstrations.
---

# Offline Diversity Maximization Under Imitation Constraints

## Quick Facts
- arXiv ID: 2307.11373
- Source URL: https://arxiv.org/abs/2307.11373
- Reference count: 28
- The paper proposes DOI, an algorithm that discovers diverse skills while imitating expert demonstrations using Fenchel duality and offline RL

## Executive Summary
This paper addresses the challenge of learning diverse skills from offline data while ensuring the skills imitate expert behavior. The proposed method, DOI, maximizes mutual information between skills and states subject to KL-divergence constraints that bound deviation from expert occupancy measures. By leveraging Fenchel duality, the authors transform the constrained problem into a sequence of unconstrained RL problems solvable with importance-weighted offline training. Experiments on a quadruped robot demonstrate that DOI successfully discovers diverse walking behaviors while maintaining expert-like performance, with the ability to control the tradeoff between diversity and imitation through the constraint level.

## Method Summary
DOI solves offline skill discovery by maximizing mutual information I(S;Z) between skills and states while constraining KL-divergence between learned and expert state occupancies. The algorithm uses Fenchel duality to convert the constrained problem into an unconstrained form solvable with importance-weighted RL. It employs a three-phase alternating optimization: (1) compute value functions and importance weights for each skill, (2) train skill discriminators using these weights, and (3) update Lagrange multipliers to enforce KL constraints. The method requires two offline datasets - expert demonstrations and coverage data - and uses bounded Lagrange multipliers via sigmoid transformation for stability.

## Key Results
- DOI successfully discovers diverse walking skills on a quadruped robot while maintaining expert-like performance
- Higher constraint levels (ε) yield more expert-aligned behaviors but reduce diversity
- The method demonstrates effective sim-to-real transfer, with learned policies exhibiting different walking behaviors on hardware
- DOI outperforms unconstrained skill discovery methods when expert imitation is required

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fenchel duality allows offline computation of state-action importance weights by transforming the KL-divergence constraint into a softmax-weighted reweighting of the offline dataset.
- **Mechanism:** The dual problem introduces a value function V(z) and importance weights ηz(s,a) = softmax(Rλz(s,a) + γTVz(s,a) - Vz(s)), where Rλz combines discriminator log-likelihood and expert ratio log η̃E(s,a). This converts the constrained problem into a sequence of unconstrained RL problems solvable with standard off-policy methods.
- **Core assumption:** Strong duality holds between the primal occupancy constraints and the Fenchel dual formulation (Assumption 3.1).
- **Evidence anchors:**
  - [abstract] "Our main contribution is to connect Fenchel duality, reinforcement learning and unsupervised skill discovery, and to give a simple offline algorithm..."
  - [section 3.2.1] "Using Lagrange duality, Assumption 3.1... Problem 12 shares the same optimal value as the following optimization problem..."
  - [corpus] Weak - related works discuss diversity maximization but not Fenchel duality for offline skill discovery specifically.
- **Break condition:** If the coverage assumption (dE(s) > 0 implies dO(s) > 0) fails, the importance weights become undefined or numerically unstable.

### Mechanism 2
- **Claim:** The alternating optimization phases decouple skill policy learning, discriminator training, and Lagrange multiplier updates, enabling stable offline training.
- **Mechanism:** Phase 1 fixes λ and q to compute V(z) and ηz; Phase 2 fixes ηz and λ to train q(z|s); Phase 3 fixes q and ηz to update λ via KL constraint estimator ϕz. This cycle refines each component while holding others fixed.
- **Core assumption:** Each phase approximately solves its subproblem when others are fixed, and the sequence converges to a local optimum.
- **Evidence anchors:**
  - [section 3.2] "We use a popular heuristic, known in the literature as alternating optimization, to approximately compute a local optimum of Problem (8)."
  - [algorithm 1] "Phase 1. (Fixed Lagrange multipliers σ(µ) and discriminator values q⋆(z|s))... Phase 2... Phase 3..."
  - [corpus] Weak - no direct evidence of this alternating scheme in related papers.
- **Break condition:** If any phase diverges or cycles without improvement, the overall algorithm fails to converge.

### Mechanism 3
- **Claim:** Bounded Lagrange multipliers via sigmoid transformation smooth the reward signal and prevent extreme weight collapse.
- **Mechanism:** Instead of optimizing λ directly, optimize unbounded µ and apply λ = σ(µ) componentwise. This ensures λ > 0 and creates a convex combination of diversity and constraint terms in Rµz(s,a).
- **Core assumption:** The sigmoid transformation preserves the optimization landscape's structure while ensuring positivity.
- **Evidence anchors:**
  - [section 4] "we use the technique of bounded Lagrange multipliers [Stooke et al., 2020, Zahavy et al., 2022], which applies a Sigmoid transformation λ = σ(µ) componentwise..."
  - [algorithm 1] "Optimize the loss minµ Σz σ(µz)(ϵ - ϕz)"
  - [corpus] Moderate - Stooke et al. 2020 cited as prior work on bounded Lagrange multipliers.
- **Break condition:** If σ(µz) saturates at 0 or 1, diversity or constraint adherence becomes too rigid.

## Foundational Learning

- **Concept:** Fenchel conjugate and duality theory
  - Why needed here: Provides the mathematical bridge to convert constrained mutual information maximization into an unconstrained RL problem with importance weighting.
  - Quick check question: What is the Fenchel conjugate of the KL-divergence DKL(p||q) at y(s) = p(s)/q(s)?

- **Concept:** Markov decision processes and occupancy measures
  - Why needed here: The optimization operates on state-action occupancy measures dz(s,a), requiring understanding of how policies induce distributions over (s,a).
  - Quick check question: How does the occupancy measure dπ(s,a) relate to the policy π and transition dynamics P?

- **Concept:** Importance sampling for off-policy evaluation
  - Why needed here: Enables training on offline data DO while targeting occupancy distributions dz by reweighting with ηz(s,a).
  - Quick check question: What is the relationship between ηz(s,a) and the optimal occupancy ratio d*z(s,a)/dO(s,a)?

## Architecture Onboarding

- **Component map:**
  - Expert policy π̃E trained via SMODICE (preprocessing)
  - Skill-conditioned policies {πz} for each latent
  - Value functions {Vz} for each skill (Phase 1)
  - Skill discriminator q(z|s) (Phase 2)
  - Lagrange multipliers {λz} or {µz} (Phase 3)
  - State classifier c*(s) for expert ratios (preprocessing)

- **Critical path:**
  1. Preprocess: Train expert policy π̃E and compute η̃E(s,a)
  2. Phase 1: For each z, compute Vz and ηz using fixed λ and q
  3. Phase 2: Train q(z|s) using fixed ηz and λ
  4. Phase 3: Update λ using fixed q and ηz
  5. Repeat phases until convergence

- **Design tradeoffs:**
  - More skills |Z| → better diversity but higher computational cost and risk of mode collapse
  - Higher constraint level ε → more expert-like but less diverse behaviors
  - Smaller phase step sizes → more stable but slower convergence

- **Failure signatures:**
  - Discriminator collapse to uniform distribution → add DISDAIN information gain bonus
  - Importance weights ηz collapse to zero or infinity → check coverage assumption and numerical stability
  - Lagrange multipliers diverging → adjust sigmoid bounds or learning rate

- **First 3 experiments:**
  1. Train with ε = 0 (pure SMODICE ensemble) to establish baseline performance and diversity metrics
  2. Vary ε ∈ {0.5, 1.0, 2.0} to observe tradeoff between constraint adherence and diversity (measure E||ηz1 - ηz2||1)
  3. Test sim-to-real transfer on hardware with fixed σ(µz) values to evaluate robustness across constraint levels

## Open Questions the Paper Calls Out
- How does the performance of DOI scale with the number of skills Z? Is there an optimal number of skills that balances diversity and performance?
- How sensitive is DOI to the choice of the KL-divergence constraint level ε? What is the optimal value of ε that balances diversity and expert imitation?
- How does DOI compare to other offline skill discovery methods, such as SMODICE, in terms of both diversity and performance?

## Limitations
- The method requires two separate offline datasets with specific coverage properties that may not be available in all real-world settings
- Theoretical guarantees rely on strong duality and coverage assumptions that may not hold in practice
- Implementation details for neural network architectures and hyperparameter tuning are not fully specified

## Confidence
- Mechanism 1 (Fenchel duality): Medium - The mathematical framework is sound but depends on unproven strong duality and coverage assumptions
- Mechanism 2 (Alternating optimization): Low - The heuristic lacks theoretical convergence guarantees and may oscillate
- Mechanism 3 (Bounded Lagrange multipliers): High - Well-established technique from prior work with proven stability benefits

## Next Checks
1. Test coverage assumption violations by deliberately constructing DO with poor state coverage and measure impact on importance weight stability and KL constraint estimation
2. Run ablation studies removing the sigmoid transformation on Lagrange multipliers to quantify its effect on training stability and final performance
3. Compare convergence behavior and final skill diversity between alternating optimization and simultaneous optimization approaches on the same problem instances