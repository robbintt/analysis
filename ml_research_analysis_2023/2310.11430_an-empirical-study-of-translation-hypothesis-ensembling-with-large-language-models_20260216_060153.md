---
ver: rpa2
title: An Empirical Study of Translation Hypothesis Ensembling with Large Language
  Models
arxiv_id: '2310.11430'
source_url: https://arxiv.org/abs/2310.11430
tags:
- translation
- comet
- language
- llama
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how hypothesis ensembling can improve the
  quality of LLM-based machine translation. We experiment with various techniques
  for generating and combining translation hypotheses from models like ChatGPT, LLaMA,
  and Alpaca.
---

# An Empirical Study of Translation Hypothesis Ensembling with Large Language Models

## Quick Facts
- arXiv ID: 2310.11430
- Source URL: https://arxiv.org/abs/2310.11430
- Reference count: 20
- Primary result: MBR decoding with COMET is highly effective for improving LLM-based translation quality while reducing hallucinations under source perturbations

## Executive Summary
This paper investigates how hypothesis ensembling can improve the quality of LLM-based machine translation. The authors experiment with various techniques for generating and combining translation hypotheses from models like ChatGPT, LLaMA, and Alpaca. Their findings show that minimum Bayes risk (MBR) decoding is highly effective, that translation quality can be improved using a small number of samples, and that instruction tuning significantly impacts the relation between hypothesis diversity and sampling temperature. The work provides a comprehensive analysis of ensembling strategies for LLM-based translation and highlights the importance of considering multiple hypotheses for improved quality and reliability.

## Method Summary
The authors generate multiple translation hypotheses using temperature sampling and beam search, then combine them using ranking with COMET KIWI, minimum Bayes risk (MBR) decoding with COMET, or LLM-based selection (ChooseBest/GenerateBest). They evaluate outputs using COMET and BLEURT metrics on WMT22 test sets covering eight translation directions. To assess hallucination robustness, they apply minimal source perturbations (misspellings, title-casing, token insertion) and check for BLEU score drops >6 points between unperturbed and perturbed inputs.

## Key Results
- MBR decoding with COMET is highly effective for ensembling LLM-based translations
- A small number of samples (e.g., 20) can significantly improve translation quality
- Instruction tuning affects the relationship between hypothesis diversity and sampling temperature
- Hypothesis ensembling reduces hallucinations under source perturbations, improving model robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MBR decoding with COMET is highly effective for ensembling LLM-based translations.
- Mechanism: MBR decoding finds the hypothesis that maximizes expected utility over the hypothesis set, which captures translation quality more robustly than single best or majority vote.
- Core assumption: The COMET score provides a reliable proxy for translation quality and correlates with human judgment.
- Evidence anchors:
  - [abstract] "Our results show that MBR decoding is a very effective method"
  - [section 2.2.1] MBR decoding seeks to maximize expected utility using Monte Carlo approximation
  - [corpus] Weak - no direct corpus evidence on MBR's effectiveness
- Break condition: If COMET does not correlate well with human judgment for the target domain, MBR performance degrades.

### Mechanism 2
- Claim: A small number of samples (e.g., 20) can improve translation quality significantly.
- Mechanism: Sampling introduces diversity in hypotheses, and ensembling selects the best among them, improving overall quality without high cost.
- Core assumption: The hypothesis set contains at least one translation of substantially higher quality than greedy decoding.
- Evidence anchors:
  - [abstract] "that translation quality can be improved using a small number of samples"
  - [section 3.2] Results show oracle translations improve with more samples
  - [corpus] Moderate - related work on hypothesis ensembling supports this
- Break condition: If the model's output distribution is too peaked, samples may not provide meaningful diversity.

### Mechanism 3
- Claim: Instruction tuning affects the relationship between hypothesis diversity and sampling temperature.
- Mechanism: Instruction tuning changes the model's sampling behavior, altering how diversity scales with temperature, which impacts the effectiveness of ensembling.
- Core assumption: Instruction tuning changes the underlying probability distribution of the model.
- Evidence anchors:
  - [abstract] "instruction tuning has a strong impact on the relation between the diversity of the hypotheses and the sampling temperature"
  - [section 3.3.3] Figures show different diversity-temperature curves for LLaMA vs Alpaca
  - [corpus] Weak - no direct corpus evidence on this specific mechanism
- Break condition: If temperature scaling is overridden by other factors (e.g., top-p sampling), the relationship may not hold.

## Foundational Learning

- Concept: Monte Carlo approximation for expected utility
  - Why needed here: MBR decoding uses Monte Carlo to approximate the expected COMET score over hypotheses
  - Quick check question: How does increasing the number of Monte Carlo samples affect the bias and variance of the MBR estimate?
- Concept: Hypothesis diversity measurement
  - Why needed here: Diversity between hypotheses is used to understand sampling behavior and quality improvements
  - Quick check question: What metric is used to quantify diversity between translation hypotheses in this paper?
- Concept: Automatic evaluation metrics for translation
  - Why needed here: COMET and BLEURT are used to evaluate translation quality without human judgment
  - Quick check question: What are the main differences between COMET and BLEURT as evaluation metrics?

## Architecture Onboarding

- Component map: Generate hypotheses -> Compute pairwise utilities -> Apply ensembling strategy -> Output final translation
- Critical path: Generate hypotheses → Compute pairwise utilities → Apply ensembling strategy → Output final translation
- Design tradeoffs:
  - Number of samples vs. cost and diversity
  - External model dependency vs. self-contained LLM methods
  - Instruction tuning impact on sampling behavior
- Failure signatures:
  - Low diversity across hypotheses → Limited improvement from ensembling
  - Poor correlation between COMET and human judgment → MBR effectiveness degrades
  - Language model outputs wrong target language → Beam search or greedy may fail
- First 3 experiments:
  1. Compare MBR decoding vs. ranking with 5 samples on EN-DE direction
  2. Measure hypothesis diversity vs. sampling temperature for LLaMA and Alpaca
  3. Evaluate hallucination reduction under source perturbations for ensembled vs. single hypotheses

## Open Questions the Paper Calls Out

- Question: How does hypothesis ensembling performance scale with model size beyond the 7B and 30B parameters tested?
  - Basis in paper: [explicit] "It remains unclear how our findings... generalize to even larger models."
  - Why unresolved: The paper only tested up to 30B parameters and acknowledged this limitation.
  - What evidence would resolve it: Systematic testing of ensembling performance across a wider range of model sizes (e.g., 65B, 175B parameters) with consistent experimental conditions.

- Question: What is the optimal sampling temperature that balances hypothesis diversity and translation quality for different language pairs?
  - Basis in paper: [explicit] "we find that there exists a significant gap in the quality of ensembles... We attribute this disparity to how instruction tuning affects the relationship between the diversity of the hypotheses and the sampling temperature."
  - Why unresolved: The paper shows the relationship exists but doesn't identify optimal temperatures for each language pair.
  - What evidence would resolve it: Empirical studies testing various temperature settings across multiple language pairs to identify optimal points for quality-diversity trade-offs.

- Question: How would human evaluation of translation quality compare to the automatic metrics (COMET and BLEURT) used in this study?
  - Basis in paper: [explicit] "Due to the high cost and time required for conducting a final human assessment of the translation quality, we have not included it in our evaluation."
  - Why unresolved: The paper relies entirely on automatic metrics without human validation.
  - What evidence would resolve it: Comprehensive human evaluation studies comparing the same model outputs and ensembling methods assessed by human translators versus automatic metrics.

## Limitations

- Effectiveness of MBR decoding critically depends on COMET's correlation with human judgment, which varies across domains and language pairs
- Sampling temperature recommendations may not generalize to different model architectures or languages beyond tested models
- Hallucination robustness experiments use only minimal perturbations, which may not capture sophisticated adversarial attacks or real-world noise patterns

## Confidence

- **High confidence**: MBR decoding effectiveness when COMET correlates with human judgment; improvement from using 20+ samples; instruction tuning's impact on diversity-temperature relationship for LLaMA and Alpaca
- **Medium confidence**: Optimal sampling temperature recommendations; generalizability of results to other language pairs beyond WMT22; effectiveness of MBR for out-of-domain translations
- **Low confidence**: COMET score reliability as quality proxy across all tested directions; robustness against sophisticated adversarial perturbations; scaling of findings to much larger or smaller models than tested (7B/30B parameters)

## Next Checks

1. Conduct human evaluation study to directly measure the correlation between COMET scores and human judgment for MBR-decoded translations across multiple language pairs, focusing on cases where MBR improves over single best hypotheses.

2. Test the diversity-temperature relationship and ensembling effectiveness on additional language pairs (e.g., low-resource languages) and model families (e.g., BLOOM, OPT) to assess generalizability beyond the current WMT22 English-centric setup.

3. Evaluate robustness against more sophisticated adversarial source perturbations, including syntactic variations, homophone substitutions, and context-aware noise injection, to determine if MBR ensembling maintains its hallucination reduction benefits under realistic attack scenarios.