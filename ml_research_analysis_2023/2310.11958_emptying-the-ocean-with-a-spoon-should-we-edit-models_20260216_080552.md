---
ver: rpa2
title: 'Emptying the Ocean with a Spoon: Should We Edit Models?'
arxiv_id: '2310.11958'
source_url: https://arxiv.org/abs/2310.11958
tags:
- editing
- language
- facts
- knowledge
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues against the use of direct model editing as a
  means to correct factual errors in large language model (LLM) outputs. The authors
  contend that editing model parameters to update individual facts is conceptually
  flawed, architecturally implausible at scale, and introduces significant risks of
  bias and inconsistency.
---

# Emptying the Ocean with a Spoon: Should We Edit Models?

## Quick Facts
- arXiv ID: 2310.11958
- Source URL: https://arxiv.org/abs/2310.11958
- Reference count: 20
- Authors: [Multiple authors]
- Primary result: Direct model editing is conceptually flawed and architecturally implausible for correcting factual errors in LLMs.

## Executive Summary
This paper argues that direct model editing for correcting factual errors in large language models is fundamentally flawed and impractical at scale. The authors contend that LLMs' stochastic nature makes them unreliable as fact repositories, that the scale of factual knowledge makes individual editing impossible, and that editing can introduce inconsistency and bias. Instead, they advocate for retrieval-based architectures, concept erasure methods, and attribution techniques as more reliable alternatives for maintaining factual accuracy in LLM outputs.

## Method Summary
The paper presents a conceptual critique of model editing approaches, surveying existing literature on both model editing methods and alternative approaches to factual accuracy. Rather than presenting experimental results, the authors provide a theoretical analysis of why direct parameter editing fails as a systematic remedy for LLM limitations. The methodology involves examining the architectural properties of LLMs, the scale of factual knowledge, and the computational complexity of maintaining consistency when editing facts.

## Key Results
- Model editing is impractical due to the stochastic nature of LLMs and their fundamental design as next-word predictors rather than reliable knowledge repositories
- The scale of factual knowledge (100+ million facts) makes individual editing infeasible, creating inevitable bias in which facts are chosen for editing
- Editing facts can cause ripple effects requiring updates to related facts, with computational complexity that is NP-hard to manage consistently

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct model editing fails because LLMs are not reliable fact repositories due to their stochastic generation nature.
- Mechanism: The inherent variability in LLM outputs means that even after editing a fact, the model may still generate contradictory or incorrect information in different contexts or prompts.
- Core assumption: LLMs are primarily trained on next-word prediction rather than factual accuracy, making them unsuitable as stable knowledge bases.
- Evidence anchors:
  - [abstract] The paper argues that editing parameters cannot be trusted as a systematic remedy for LLM disadvantages.
  - [section 3.2] "A very basic property of LLMs which contrasts with their usage as knowledge repositories is their stochastic nature."
  - [corpus] The related work "Balancing Knowledge Updates" notes that knowledge editing typically focuses on MLP weights but may not address the broader distributional nature of LLMs.
- Break condition: If the LLM's training objective were fundamentally changed to prioritize factual accuracy over next-word prediction, or if external validation mechanisms could guarantee consistency.

### Mechanism 2
- Claim: The architectural scale of LLMs makes individual fact editing impractical and biased.
- Mechanism: With over 100 million notable facts, editing each fact individually is like "emptying the ocean with a spoon," leading to inevitable bias in which facts are chosen for editing and which are overlooked.
- Core assumption: The vastness and variability of facts means that any editing process will be incomplete and introduce systematic biases.
- Evidence anchors:
  - [abstract] The paper uses the metaphor "emptying the ocean with a spoon" to describe the impracticality of editing individual facts.
  - [section 3.3] "Estimates vary wildly, but there are over 100 million notable facts in the world... Even the cutoff for what constitutes a fact is unclear."
  - [corpus] The related work "Is Bigger Edit Batch Size Always Better?" empirically studies model editing with Llama-3, suggesting scalability challenges.
- Break condition: If automated, unbiased methods could be developed to identify and edit all relevant facts without human intervention, or if the scope of what constitutes a "fact" could be radically narrowed.

### Mechanism 3
- Claim: Editing facts in LLMs can cause ripple effects that are computationally hard to manage, leading to inconsistency.
- Mechanism: Modifying one fact can necessitate changes to related facts (the ripple effect), but the computational complexity of tracking and updating all related facts is NP-hard, making consistent editing infeasible.
- Core assumption: Knowledge in LLMs is interconnected, and changing one fact requires understanding and updating its logical consequences.
- Evidence anchors:
  - [section 3.3] "Empirical evidence indicates existing editing models fail to properly account for the ripple effect of a fact editing operation... There is, therefore, theoretical basis to conclude that model editing will at best address the problem of consistent updating in a roughly approximate manner."
  - [corpus] The related work "Beyond the Chat" discusses challenges in text-editing with LLMs, implying complexity in maintaining consistency.
- Break condition: If computational methods could efficiently solve the NP-hard problem of knowledge consistency, or if the interconnected nature of LLM knowledge could be radically simplified.

## Foundational Learning

- Concept: Knowledge representation in neural networks
  - Why needed here: Understanding how factual knowledge is encoded in LLM parameters is crucial for evaluating the feasibility of model editing.
  - Quick check question: How do LLMs store and retrieve factual information compared to explicit knowledge bases?

- Concept: Retrieval-augmented generation
  - Why needed here: The paper advocates for retrieval-based architectures as an alternative to model editing, so understanding how these systems work is essential.
  - Quick check question: How do retrieval-augmented models combine external knowledge with LLM generation to maintain factual accuracy?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The paper mentions catastrophic forgetting as a limitation of model editing, so understanding this concept is important for evaluating editing methods.
  - Quick check question: What mechanisms cause models to forget previously learned information when trained on new tasks, and how does this relate to model editing?

## Architecture Onboarding

- Component map: LLM (parameter-based knowledge storage) -> Retrieval module (external knowledge sources) -> Editing mechanism (parameter modification tools) -> Fact validation system
- Critical path: For fact verification and correction: query → LLM generation → fact validation → retrieval of correct information → (editing or retrieval-based correction) → output. The retrieval-based path bypasses editing entirely.
- Design tradeoffs: Editing offers potentially seamless integration but risks inconsistency and bias; retrieval offers transparency and scalability but may reduce generation fluency. The choice depends on the application's tolerance for error and need for explainability.
- Failure signatures: Inconsistent fact generation across similar prompts, degradation of performance on non-edited facts (drawdown), and inability to handle rare or long-tail facts. These indicate either the stochastic nature of LLMs or the limitations of the editing approach.
- First 3 experiments:
  1. Test the consistency of LLM outputs on the same factual question across multiple runs to establish baseline stochasticity.
  2. Apply a simple editing method to a small set of facts and measure both the success rate of the edit and the drawdown on control facts.
  3. Implement a retrieval-augmented baseline for the same task and compare factuality, consistency, and computational cost against the edited model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific risks of model editing that could lead to systemic bias in LLM outputs?
- Basis in paper: [explicit] The paper argues that model editing introduces risks of bias and inconsistency, and could lead to a monolithization of LLM-supplied "knowledge" focusing on certain popular domains while losing usefulness in many topics.
- Why unresolved: The paper identifies potential risks but does not provide concrete examples or quantify the extent of these risks in practice.
- What evidence would resolve it: Empirical studies comparing the diversity and accuracy of LLM outputs before and after model editing, with a focus on underrepresented topics and domains.

### Open Question 2
- Question: How does the scalability of retrieval-based architectures compare to model editing in terms of computational efficiency and memory usage?
- Basis in paper: [explicit] The paper advocates for retrieval-based architectures as an alternative to model editing, suggesting they can decouple factual knowledge from inference and linguistic capabilities.
- Why unresolved: The paper does not provide a direct comparison of the computational resources required for retrieval-based architectures versus model editing, nor does it discuss the trade-offs in terms of memory usage.
- What evidence would resolve it: Benchmarks comparing the computational cost and memory requirements of retrieval-based architectures and model editing methods, across various scales of factual knowledge.

### Open Question 3
- Question: Can concept erasure methods be extended to address specific aspects of factuality, such as temporal validity, in LLMs?
- Basis in paper: [explicit] The paper mentions that concept erasure has more limited scope than model editing but suggests that it could be extended to address specific aspects of factuality.
- Why unresolved: The paper does not provide examples of how concept erasure could be applied to temporal validity or other specific aspects of factuality.
- What evidence would resolve it: Research demonstrating the application of concept erasure to temporal validity or other specific aspects of factuality, with metrics showing the effectiveness of the approach.

## Limitations
- The argument relies heavily on theoretical reasoning rather than empirical validation, lacking quantitative demonstrations of editing feasibility
- The superiority of retrieval-based approaches is supported primarily by conceptual arguments without empirical comparisons on identical tasks
- The paper doesn't address potential hybrid approaches that might combine selective editing with retrieval

## Confidence
- **High confidence**: The claim that LLMs are stochastic next-token predictors rather than reliable fact repositories is well-established in the literature and supported by extensive empirical evidence
- **Medium confidence**: The scalability argument against editing (emptying the ocean with a spoon) is logically compelling but lacks quantitative analysis of the actual effort required for editing at different scales
- **Low confidence**: The assertion that retrieval-based architectures are definitively superior to any form of model editing lacks empirical validation comparing both approaches on identical tasks

## Next Checks
1. **Quantitative feasibility analysis**: Measure the actual time and computational resources required to edit a representative sample of facts (e.g., 1,000 facts) in a medium-sized LLM, including verification of consistency and drawdown effects, to empirically validate the "emptying the ocean" metaphor

2. **Head-to-head empirical comparison**: Implement both a state-of-the-art model editing approach and a retrieval-augmented baseline on the same fact verification task, measuring not just factual accuracy but also generation quality, consistency across prompts, and computational overhead

3. **Hybrid approach evaluation**: Test a combined approach where high-frequency, high-impact facts are edited while low-frequency facts use retrieval, to determine whether selective editing in specific domains could provide benefits without the full scalability burden