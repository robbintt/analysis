---
ver: rpa2
title: GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions
arxiv_id: '2310.15405'
source_url: https://arxiv.org/abs/2310.15405
tags:
- caption
- figure
- captions
- students
- figures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new method for evaluating scientific figure
  captions using large language models (LLMs), addressing the challenge of costly
  human evaluation and unreliable automatic metrics. The authors construct SCICAP-EVAL,
  a dataset containing 3,600 human-judged captions for 600 scientific figures, assessed
  by both Ph.D.
---

# GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions

## Quick Facts
- **arXiv ID**: 2310.15405
- **Source URL**: https://arxiv.org/abs/2310.15405
- **Reference count**: 20
- **Primary result**: GPT-4 achieves a Kendall correlation of 0.401 with Ph.D. student rankings in evaluating scientific figure captions, outperforming other models and undergraduate assessments.

## Executive Summary
This paper addresses the challenge of evaluating scientific figure captions by introducing a novel method using large language models (LLMs). The authors construct SCICAP-EVAL, a dataset containing 3,600 human-judged captions for 600 scientific figures, assessed by both Ph.D. students and undergraduates. They prompt LLMs like GPT-4 and GPT-3 to score captions on their helpfulness in understanding figures, using relevant context from figure-mentioning paragraphs. Results show GPT-4 as a zero-shot evaluator outperforms all other models and even surpasses assessments made by Computer Science and Informatics undergraduates.

## Method Summary
The authors created SCICAP-EVAL, a dataset of 3,600 human-judged captions for 600 scientific figures, using figure-mentioning paragraphs from arXiv papers as context. They employed zero-shot and few-shot prompting strategies to have LLMs like GPT-4, GPT-3.5, Falcon, and LLaMA-2 score captions based on their helpfulness in understanding figures. The LLM evaluations were then compared against human judgments from Ph.D. students and undergraduates using Kendall, Pearson, and Spearman correlation coefficients. Additionally, they fine-tuned SciBERT and LLaMA-2 classifiers on the SCICAP-EVAL data for comparison.

## Key Results
- GPT-4 achieved a Kendall correlation of 0.401 with Ph.D. student rankings, outperforming other models and undergraduate assessments.
- GPT-4 excels at identifying low-quality captions over high-quality ones.
- Different reader groups (Ph.D. students vs. undergraduates) prioritize different aspects of caption usefulness.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: GPT-4 can effectively evaluate scientific figure captions by leveraging its understanding of figure-mentioning paragraphs as context.
- **Mechanism**: The caption's helpfulness is judged based on how well it summarizes and captures the essential information from the figure-mentioning paragraphs, which act as a reference-free context for evaluation.
- **Core assumption**: Figure captions function as condensed summaries of figure-mentioning paragraphs, and LLMs can understand this relationship to assess caption quality.
- **Evidence anchors**:
  - [abstract] "LLMs, when given the appropriate context, can effectively evaluate how well the caption captures the essential information from these contexts"
  - [section 1] "The intuition is that since a scientific figure's caption in scholarly articles often functions as a condensed summary of all figure-mentioning paragraphs"
  - [corpus] Weak - no direct evidence found in corpus about this specific mechanism
- **Break condition**: If the figure-mentioning paragraphs do not adequately represent the figure's content or if the LLM cannot effectively process the relationship between paragraphs and captions.

### Mechanism 2
- **Claim**: GPT-4 outperforms other models and even human undergraduates in evaluating caption quality.
- **Mechanism**: GPT-4's superior language understanding and reasoning capabilities allow it to better assess the helpfulness of captions in aiding reader understanding compared to other models and less experienced human evaluators.
- **Core assumption**: GPT-4 has a more nuanced understanding of scientific content and can better judge the effectiveness of captions in conveying figure information.
- **Evidence anchors**:
  - [abstract] "Results show that GPT-4, used as a zero-shot evaluator, outperformed all other models and even surpassed assessments made by Computer Science and Informatics undergraduates"
  - [section 5] "GPT-4, prompted in either a zero-shot or few-shot manner, exhibited the strongest correlations with Ph.D. students' judgments"
  - [corpus] Weak - no direct evidence found in corpus about this specific mechanism
- **Break condition**: If the task requires highly specialized domain knowledge that GPT-4 lacks, or if the evaluation criteria change significantly from the original study.

### Mechanism 3
- **Claim**: GPT-4 is better at identifying low-quality captions than high-quality ones.
- **Mechanism**: The LLM's evaluation process may be more sensitive to errors or inadequacies in captions, making it more effective at flagging poor examples rather than selecting the best ones.
- **Core assumption**: Identifying flaws in captions is easier for an LLM than determining the subtle differences between good and excellent captions.
- **Evidence anchors**:
  - [section 5] "Table 1 shows that GPT-4, in either zero-shot or few-shot prompting, excels at identifying low-quality over high-quality examples"
  - [abstract] "The study also reveals that GPT-4 is better at identifying low-quality captions than high-quality ones"
  - [corpus] Weak - no direct evidence found in corpus about this specific mechanism
- **Break condition**: If the evaluation criteria shift to focus more on identifying top-quality examples, or if the LLM's sensitivity to errors decreases with further training or fine-tuning.

## Foundational Learning

- **Concept**: Correlation coefficients (Pearson, Kendall, Spearman)
  - **Why needed here**: To quantify the relationship between LLM evaluations and human judgments of caption quality
  - **Quick check question**: What does a Kendall correlation of 0.401 between GPT-4 and Ph.D. student rankings indicate about their agreement?
- **Concept**: Zero-shot and few-shot prompting
  - **Why needed here**: To understand the different approaches used to prompt GPT-4 for caption evaluation without extensive fine-tuning
  - **Quick check question**: How does few-shot prompting differ from zero-shot prompting in the context of this study?
- **Concept**: Chain-of-Thought prompting
  - **Why needed here**: To grasp the method used to break down the evaluation task into question generation and answering steps
  - **Quick check question**: What are the two types of questions explored in the Chain-of-Thought approach for caption evaluation?

## Architecture Onboarding

- **Component map**: Figure-mentioning paragraphs -> Caption evaluation (LLM) -> Correlation with human judgments -> Insights on caption quality
- **Critical path**:
  1. Collect figure-mentioning paragraphs and captions
  2. Prompt LLM to evaluate captions based on helpfulness
  3. Compare LLM evaluations with human judgments
  4. Analyze results and draw conclusions
- **Design tradeoffs**:
  - Using LLMs vs. human evaluation: Cost vs. accuracy
  - Zero-shot vs. few-shot prompting: Simplicity vs. potential performance improvement
  - Including vs. excluding figure-mentioning paragraphs: Context richness vs. evaluation complexity
- **Failure signatures**:
  - Low correlation between LLM and human evaluations
  - LLM consistently misidentifying high-quality or low-quality captions
  - LLM evaluations not improving with different prompting strategies
- **First 3 experiments**:
  1. Replicate the study with a different set of scientific figures to validate the findings
  2. Test the LLM evaluation method on captions from different scientific domains (e.g., physics, biology)
  3. Implement and compare different prompting strategies (e.g., more examples in few-shot, different question types in CoT)

## Open Questions the Paper Calls Out

### Question 1: Can Large Language Models (LLMs) Effectively Evaluate Scientific Figure Captions?
- **Question**: Can LLMs, particularly GPT-4, be reliably used as zero-shot evaluators for scientific figure captions?
- **Basis in paper**: The paper demonstrates that GPT-4 achieved a Kendall correlation score of 0.401 with Ph.D. students' rankings, outperforming other models and undergraduate assessments.
- **Why unresolved**: While the results are promising, further research is needed to understand the limitations and potential biases of using LLMs for this purpose. Additionally, the generalizability of these findings to other types of scientific figures and captions needs to be explored.
- **What evidence would resolve it**: Further studies comparing LLM evaluations with those of domain experts across various scientific disciplines and figure types would provide more robust evidence of their effectiveness.

### Question 2: What Factors Influence the Quality of Scientific Figure Captions?
- **Question**: What aspects of scientific figure captions are most important for different reader groups, such as Ph.D. students and undergraduate students?
- **Basis in paper**: The study found that Ph.D. students prioritize captions that reference terms and text from the figure, while undergraduate students prioritize captions that state the figure's takeaway messages.
- **Why unresolved**: The study provides initial insights into the preferences of different reader groups, but more research is needed to understand the full range of factors that influence caption quality and how these factors vary across disciplines and reader expertise levels.
- **What evidence would resolve it**: Systematic studies examining the preferences of various reader groups across different scientific fields and figure types would help identify the key factors that contribute to effective figure captions.

### Question 3: How Can LLMs Be Improved for Evaluating Figure Captions?
- **Question**: What are the best practices for prompting and fine-tuning LLMs to evaluate scientific figure captions accurately and consistently?
- **Basis in paper**: The paper explores different prompting strategies, including zero-shot, few-shot, and Chain-of-Thought prompting, and finds that GPT-4 performs best with zero-shot and few-shot prompting.
- **Why unresolved**: The paper provides initial insights into effective prompting strategies, but more research is needed to optimize these approaches and develop new methods for improving LLM performance in caption evaluation.
- **What evidence would resolve it**: Comparative studies evaluating different prompting and fine-tuning techniques, as well as exploring novel approaches such as incorporating visual information, would help identify the most effective strategies for using LLMs to evaluate figure captions.

## Limitations

- The evaluation is primarily based on Computer Science and Informatics figures from arXiv, which may not represent the diversity of scientific disciplines.
- The human evaluation was conducted by a relatively small pool of 5 Ph.D. students and 7 undergraduates, which may not capture the full spectrum of expertise in scientific reading.
- The study does not address how well the LLM evaluation generalizes to figures from different scientific domains or to figures with more complex visual content.

## Confidence

- **High Confidence**: The finding that GPT-4 outperforms other LLMs (GPT-3.5, Falcon, LLaMA-2) in evaluating caption quality, as this is directly supported by the correlation metrics presented in the paper. The observation that GPT-4 is better at identifying low-quality captions than high-quality ones is also well-supported by the data.
- **Medium Confidence**: The claim that GPT-4 can effectively evaluate scientific figure captions by leveraging figure-mentioning paragraphs as context. While the paper provides a clear mechanism for this approach, the evidence could be strengthened by more direct comparisons with and without context.
- **Low Confidence**: The assertion that GPT-4's performance surpasses that of undergraduate students. The paper does not provide detailed information about the undergraduates' evaluation process or their level of expertise, making it difficult to assess the validity of this comparison.

## Next Checks

1. **Cross-Domain Validation**: Test the LLM evaluation method on scientific figures from diverse disciplines (e.g., physics, biology, medicine) to assess its generalizability beyond Computer Science and Informatics.

2. **Human Expert Comparison**: Conduct a larger-scale human evaluation with a more diverse group of experts, including senior researchers from various fields, to establish a more robust baseline for comparing LLM performance.

3. **Visual Content Analysis**: Investigate how the LLM evaluation method performs on figures with different types of visual content (e.g., plots, diagrams, images) and explore whether the approach needs to be adapted for different figure types.