---
ver: rpa2
title: Learning adaptive planning representations with natural language guidance
arxiv_id: '2312.08566'
source_url: https://arxiv.org/abs/2312.08566
tags:
- object
- type
- inventory
- planning
- otype
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Ada (Action Domain Acquisition), a framework
  that automatically learns hierarchical planning representations from language guidance.
  The key idea is to use language models (LMs) to propose candidate symbolic action
  abstractions based on task descriptions and environment features, then iteratively
  refine and verify these through bi-level planning and execution.
---

# Learning adaptive planning representations with natural language guidance

## Quick Facts
- arXiv ID: 2312.08566
- Source URL: https://arxiv.org/abs/2312.08566
- Reference count: 40
- Key outcome: Ada learns action libraries that outperform baselines using LLMs for planning, enabling more accurate plans and better generalization to complex tasks.

## Executive Summary
Ada is a framework that automatically learns hierarchical planning representations from language guidance by leveraging large language models (LLMs) to propose symbolic action abstractions. The key innovation is an interactive loop where LLMs propose candidate operators based on task descriptions and environment features, which are then refined through bi-level planning and execution. On two benchmarks (Mini Minecraft and ALFRED Household Tasks), Ada learns action libraries that enable more accurate planning and better generalization to complex tasks compared to baselines that use LLMs directly for planning.

## Method Summary
Ada uses LLMs in an interactive planning loop to assemble a library of composable, hierarchical actions tailored to a given environment and task space. At each iteration, LLMs propose novel high-level action definitions based on environment predicates and language goals. These candidate operators are then evaluated through bi-level planning, where high-level symbolic planners generate abstract plans that are refined into executable action sequences using learned low-level controllers. Operators are scored based on successful execution rates and retained or refined accordingly. This iterative process builds up a library of actions that compose to produce valid abstract plans and realizable low-level trajectories.

## Key Results
- Ada outperforms baselines that use LLMs for direct planning on Mini Minecraft and ALFRED Household Tasks
- Learned action abstractions enable efficient and accurate planning in complex, unseen tasks
- The framework demonstrates better generalization to complex tasks compared to non-adaptive approaches

## Why This Works (Mechanism)

### Mechanism 1
Language models can propose symbolic operator definitions that serve as useful priors for planning. LLMs are prompted with task descriptions and environment predicates to generate candidate PDDL operator definitions that encode preconditions and effects relevant to the task domain. The core assumption is that LLMs have sufficient world knowledge encoded to propose reasonable initial operator abstractions that can be refined through planning and execution.

### Mechanism 2
Bi-level planning with learned operators enables efficient task solving compared to direct low-level planning. High-level symbolic planners generate abstract plans using proposed operators, then low-level controllers refine these into executable action sequences, leveraging compositionality for generalization. The core assumption is that the learned operator library is complete enough to cover necessary task decompositions and their controllers are accurate enough for low-level execution.

### Mechanism 3
Interactive refinement through planning and execution feedback improves operator quality over time. Operators are scored based on successful execution rates during task solving, with low-performing operators filtered out while successful ones are retained and refined. The core assumption is that there is sufficient signal in planning and execution feedback to distinguish useful operators from poor ones, and the filtering criteria are appropriate.

## Foundational Learning

- **Concept: Hierarchical planning representations**
  - Why needed here: The approach relies on decomposing tasks into high-level abstract actions that can be refined into low-level executions, requiring understanding of hierarchical planning formalisms.
  - Quick check question: What is the difference between STRIPS operators and PDDL operators in terms of expressiveness?

- **Concept: Symbolic planning with PDDL**
  - Why needed here: The method uses PDDL (Planning Domain Definition Language) to represent operators and goals, and relies on symbolic planners like FastDownward.
  - Quick check question: How do preconditions and effects in PDDL operators differ from imperative function definitions?

- **Concept: Language model prompting for code generation**
  - Why needed here: The approach uses few-shot prompting to generate operator definitions and task decompositions from LLMs, requiring understanding of effective prompt engineering.
  - Quick check question: What are the key differences between prompting for goal generation vs. operator definition generation?

## Architecture Onboarding

- **Component map:**
  LLM operator proposer → Operator library (A) → Symbolic planner (e.g., FastDownward) → Low-level search with learned controllers → Environment execution → Operator scoring → Updated library

- **Critical path:**
  1. LLM proposes operator definitions based on task descriptions and predicates
  2. LLM proposes goal definitions for each task
  3. Symbolic planner generates high-level plans using operators and goals
  4. Low-level search with learned controllers executes plans in environment
  5. Success/failure feedback scores operators for retention/refinement

- **Design tradeoffs:**
  - Sampling temperature for LLM proposals (higher = more diverse but potentially worse quality)
  - Operator retention thresholds (τb, τr) balancing coverage vs. quality
  - Low-level search budget per subgoal balancing planning time vs. execution success
  - Number of goal/operator samples per task balancing coverage vs. computation

- **Failure signatures:**
  - LLM proposals consistently rejected → Need better prompting or larger/finetuned model
  - High-level planning fails frequently → Operator preconditions/effects too inaccurate
  - Low-level execution fails frequently → Controllers not learned well enough or operator effects not achievable
  - Operators grow too large without quality improvement → Retention thresholds too permissive

- **First 3 experiments:**
  1. Test LLM operator proposal on a single simple task without refinement loop to validate prompting
  2. Test bi-level planning on a fixed operator library to validate planner-controller integration
  3. Test interactive refinement loop on a small task set to validate scoring and retention mechanism

## Open Questions the Paper Calls Out

### Open Question 1
How does Ada's performance scale with larger or more complex environments and task spaces? The paper evaluates on two specific domains (Mini Minecraft and ALFRED) but does not explore performance on larger or more complex environments.

### Open Question 2
How robust is Ada to noise or errors in the LLM's proposed operator definitions? While the paper mentions a post-processing heuristic to correct syntactically invalid operators, it does not thoroughly evaluate robustness to more subtle errors or noise.

### Open Question 3
How does Ada compare to other approaches for learning hierarchical planning representations, such as learning from demonstrations or interactions without language guidance? The paper focuses on learning from language guidance and does not directly compare to other learning approaches for hierarchical planning.

## Limitations

- Performance may not generalize beyond tested domains due to reliance on LLM capabilities
- Operator scoring mechanism assumes sufficient diversity in execution traces to distinguish useful from poor operators
- The approach requires extensive computation for iterative refinement and may not scale to very large task spaces

## Confidence

- **High confidence:** The bi-level planning architecture and integration with symbolic planners is well-specified and implementable
- **Medium confidence:** The operator refinement through execution feedback will work as intended, though scoring thresholds may need tuning
- **Low confidence:** The LLM prompting strategy will consistently generate valid, useful operators across diverse domains without extensive domain-specific tuning

## Next Checks

1. Test the LLM operator proposal mechanism on a held-out domain with minimal task variation to assess prompt generalization
2. Evaluate operator library coverage by measuring the percentage of tasks solvable without refinement vs. with refinement
3. Analyze the learning curves of operator quality over iterations to verify the refinement mechanism is actually improving operators rather than just filtering noise