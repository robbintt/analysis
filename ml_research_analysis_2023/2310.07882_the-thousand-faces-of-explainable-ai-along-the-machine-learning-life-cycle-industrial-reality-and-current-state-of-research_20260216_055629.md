---
ver: rpa2
title: 'The Thousand Faces of Explainable AI Along the Machine Learning Life Cycle:
  Industrial Reality and Current State of Research'
arxiv_id: '2310.07882'
source_url: https://arxiv.org/abs/2310.07882
tags:
- data
- learning
- arxiv
- machine
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the practical relevance of explainable
  AI (XAI) across the machine learning life cycle through 36 semi-structured interviews
  with practitioners from various industrial roles and domains. The research contrasts
  these real-world findings with the current state of academic XAI research.
---

# The Thousand Faces of Explainable AI Along the Machine Learning Life Cycle: Industrial Reality and Current State of Research

## Quick Facts
- arXiv ID: 2310.07882
- Source URL: https://arxiv.org/abs/2310.07882
- Reference count: 0
- This study investigates the practical relevance of explainable AI (XAI) across the machine learning life cycle through 36 semi-structured interviews with practitioners from various industrial roles and domains.

## Executive Summary
This study investigates the practical relevance of explainable AI (XAI) across the machine learning life cycle through 36 semi-structured interviews with practitioners from various industrial roles and domains. The research contrasts these real-world findings with the current state of academic XAI research. The interviews reveal that while XAI methods are widely recognized for debugging and model evaluation, their application in other stages—such as data understanding, deployment, and monitoring—is underexplored. Academic research is heavily focused on data scientists and model evaluation, with limited attention to non-expert users or the full ML lifecycle. The study highlights a mismatch between the academic toolbox and industrial needs, emphasizing the need for scalable, user-friendly XAI solutions tailored to diverse stakeholders and lifecycle stages. The findings call for bridging this gap to unlock XAI's full potential in industrial applications.

## Method Summary
The study conducted 36 semi-structured interviews with industrial practitioners across various roles and domains to investigate XAI usage along the ML lifecycle. Researchers mapped interview findings to the CRISP-ML life cycle model and compared results with academic literature on XAI methods. The qualitative analysis focused on identifying current XAI relevance, discrepancies between practice and research, and research gaps.

## Key Results
- XAI methods are widely used for debugging and model evaluation but underexplored for other ML lifecycle stages
- Academic research focuses heavily on model evaluation and data scientists, with limited attention to non-expert users
- Significant mismatch exists between academic XAI toolbox and industrial needs for scalable, user-friendly solutions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: XAI methods are widely recognized for debugging and model evaluation but underexplored for other ML lifecycle stages
- Mechanism: Interview data shows practitioners rely on XAI tools like Saliency Maps, LIME, and GradCam primarily during model evaluation, but not for data collection, deployment, or monitoring
- Core assumption: Interview responses accurately represent industrial XAI usage patterns
- Evidence anchors:
  - [abstract]: "while XAI methods are widely recognized for debugging and model evaluation, their application in other stages—such as data understanding, deployment, and monitoring—is underexplored"
  - [section 4.3]: "we found that they already make extensive use of state-of-the-art XAI methods to globally debug models, trace down specific errors and conduct root cause reasoning"
  - [corpus]: Missing - no corpus evidence on lifecycle stage distribution
- Break condition: If new XAI methods emerge that explicitly target non-evaluation stages, or if practitioners report using existing methods for these stages

### Mechanism 2
- Claim: XAI methods are computationally complex, hindering scaling for actual deployment
- Mechanism: Multiple interview partners highlighted that XAI is computationally complex, which hinders XAI in deployment, especially for model-agnostic techniques requiring multiple model evaluations
- Core assumption: Computational complexity directly impacts deployment feasibility
- Evidence anchors:
  - [abstract]: "the need for scalable, user-friendly XAI solutions tailored to diverse stakeholders and lifecycle stages"
  - [section 4.4]: "we received many recommendations on what measures should be taken to ensure that the XAI comes in the required form... Currently, it is often unclear how XAI should be used and which level of granularity is needed for the user"
  - [corpus]: Weak - corpus lacks direct evidence on deployment scalability challenges
- Break condition: If efficient approximation methods or hardware acceleration become standard for XAI deployment

### Mechanism 3
- Claim: XAI effectiveness depends on presenting explanations in the "language" of domain experts and end users
- Mechanism: Interview data shows that XAI benefits increase when explanations are tailored to user mental models and domain semantics rather than generic data scientist visualizations
- Core assumption: User comprehension of XAI correlates with explanation format matching user expertise
- Evidence anchors:
  - [abstract]: "more efforts are needed to enable also non-expert users' interpretation and understanding of opaque AI models with existing methods and frameworks"
  - [section 4.4]: "XAI can only enfold its full strength if the explanations are presented in the 'language' of the domain experts and end users"
  - [corpus]: Missing - no corpus evidence on user-centered explanation design
- Break condition: If user studies show generic XAI visualizations achieve comparable understanding to domain-tailored explanations

## Foundational Learning

- Concept: CRISP-ML lifecycle model
  - Why needed here: Provides framework for mapping interview findings to ML lifecycle stages
  - Quick check question: What are the six stages of CRISP-ML as defined in the paper?
- Concept: Shapley values and influence functions
  - Why needed here: Key XAI methods mentioned for data quality and evaluation tasks
  - Quick check question: How do Shapley values differ from influence functions in identifying important data points?
- Concept: Visual Analytics (VA) principles
  - Why needed here: Essential for understanding how XAI visualization design should match user tasks and data types
  - Quick check question: What three factors does the Visual Analytics design triangle consider?

## Architecture Onboarding

- Component map: XAI tools (LIME, SHAP, GradCam) → ML lifecycle stages (evaluation, deployment, monitoring) → User types (data scientists, domain experts, end users)
- Critical path: Data collection → Modeling → Evaluation → Deployment → Monitoring, with XAI intervention points at each stage
- Design tradeoffs: Generic vs. domain-specific XAI explanations, computational efficiency vs. explanation fidelity, developer vs. user-centered design
- Failure signatures: XAI explanations not understood by domain experts, computational bottlenecks during deployment, explanations that contradict model predictions
- First 3 experiments:
  1. Test LIME explanations on a deployed model to see if non-expert users can understand predictions
  2. Compare generic SHAP visualizations vs. domain-tailored explanations for the same model
  3. Measure computational overhead of GradCam during real-time monitoring scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific XAI methods are most effective for detecting data quality issues like bias and wrong labels during the data understanding phase?
- Basis in paper: [inferred] The paper discusses that XAI tools help in identifying data quality issues, data bias, and wrong labels, but does not specify which methods are most effective.
- Why unresolved: The interviews mention the potential of XAI in these areas but lack concrete evidence or examples of which specific methods are most effective.
- What evidence would resolve it: Comparative studies or case studies demonstrating the effectiveness of specific XAI methods for data quality issues.

### Open Question 2
- Question: How can XAI methods be scaled to handle complex models and diverse data structures beyond images, as mentioned by the interviewees?
- Basis in paper: [explicit] The paper highlights that XAI does not scale to other data structures besides images and is computationally complex.
- Why unresolved: The paper identifies the scalability issue but does not provide solutions or methods to address it.
- What evidence would resolve it: Research or case studies showing successful scaling of XAI methods to diverse data structures and complex models.

### Open Question 3
- Question: What role will XAI play in the certification and standardization of AI systems, given the conflicting views on its necessity?
- Basis in paper: [explicit] The paper discusses the role of XAI in certification and standardization, noting conflicting views on its necessity and implementation.
- Why unresolved: There is no clear consensus on how XAI will be integrated into certification processes or the extent of its necessity.
- What evidence would resolve it: Policy developments or pilot studies showing the integration of XAI in certification and standardization processes.

## Limitations
- Sample may be biased toward larger organizations with established ML practices
- Qualitative findings lack statistical validation across broader industrial population
- Limited quantitative strength of claims due to qualitative nature of interview data

## Confidence
- High confidence: XAI methods are primarily used for debugging and model evaluation, not other lifecycle stages
- Medium confidence: Academic XAI research is disproportionately focused on model evaluation and data scientists
- Low confidence: Specific computational scalability challenges for deployment are representative of the broader industrial context

## Next Checks
1. Survey 50+ industrial practitioners to quantify the prevalence of XAI usage across different lifecycle stages
2. Benchmark computational overhead of popular XAI methods (LIME, SHAP, GradCam) in production deployment scenarios
3. Conduct controlled user studies comparing domain-tailored vs. generic XAI explanations for the same ML models