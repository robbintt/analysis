---
ver: rpa2
title: Computing a human-like reaction time metric from stable recurrent vision models
arxiv_id: '2306.11582'
source_url: https://arxiv.org/abs/2306.11582
tags:
- crnn
- task
- stimuli
- human
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to compute reaction time (RT) metrics
  from recurrent vision models using evidence accumulation. The authors train convolutional
  recurrent neural networks (cRNNs) with an attractor dynamics-based training routine
  and evidential deep learning to model uncertainty.
---

# Computing a human-like reaction time metric from stable recurrent vision models

## Quick Facts
- arXiv ID: 2306.11582
- Source URL: https://arxiv.org/abs/2306.11582
- Reference count: 40
- One-line primary result: Derives a metric capturing area under uncertainty curve to align recurrent vision model decision times with human reaction times

## Executive Summary
This paper introduces a method to compute reaction time metrics from recurrent vision models using evidence accumulation. The authors train convolutional recurrent neural networks (cRNNs) with attractor dynamics-based training and evidential deep learning to model uncertainty. They derive a metric capturing the area under the uncertainty curve over time, which reflects computational demand. Testing on four visual tasks (incremental grouping, mental simulation, maze solving, and scene categorization), they show the metric aligns with human RT patterns, including spatial anisotropy and effects of path length.

## Method Summary
The authors develop a convolutional recurrent neural network (cRNN) with hGRU architecture trained using contractor recurrent back-propagation (C-RBP) and evidential deep learning (EDL) loss. The model's uncertainty evolution over time is captured using a Dirichlet distribution over class probability estimates. They derive a novel metric (ξcRNN) quantifying the area under the uncertainty curve to capture computational demands imposed by stimulus manipulations. This metric is compared against human reaction time data across four visual tasks to assess alignment.

## Key Results
- ξcRNN metric captures differential computational demands across stimulus manipulations in visual tasks
- Model uncertainty evolution correlates with human reaction time patterns including spatial anisotropy
- C-RBP training with EDL enables stable, expressive solutions that generalize to complex tasks
- Framework provides general approach for modeling dynamic decision processes in neural networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Evidence accumulation in recurrent vision models can serve as a proxy for computational demand and human reaction times.
- Mechanism: The model's uncertainty over time reflects the complexity of the stimulus. As uncertainty decreases (through evidence accumulation), the time taken correlates with human reaction times. The metric ξcRNN (area under the uncertainty curve) captures this temporal signature.
- Core assumption: Model uncertainty evolution is a reliable indicator of decision difficulty and computational cost.
- Evidence anchors:
  - [abstract]: "We introduce a novel metric leveraging insights from subjective logic theory summarizing evidence accumulation in recurrent vision models."
  - [section]: "We derive a novel metric to quantify the differential computational demands imposed on the cRNN by stimulus manipulations and capture the essence of model decision time courses for alignment."
  - [corpus]: Weak evidence - no direct comparison in the corpus papers to human RTs or uncertainty-based metrics.
- Break condition: If the model's uncertainty does not correlate with decision difficulty (e.g., due to model architecture or training regime), the metric will fail to align with human RTs.

### Mechanism 2
- Claim: Attractor dynamics-based training with contractor recurrent back-propagation (C-RBP) stabilizes recurrent models and enables meaningful temporal dynamics.
- Mechanism: C-RBP imposes an attractor dynamic constraint on the computational trajectory, ensuring stable and expressive solutions. This stability allows the model to dynamically adapt its processing steps based on stimulus demands.
- Core assumption: Without C-RBP, recurrent models would be unstable and fail to generalize to more complex tasks.
- Evidence anchors:
  - [section]: "C-RBP imposes an attractor-dynamic constraint on the cRNN’s computational trajectory, imparting several benefits over the standard BPTT algorithm."
  - [section]: "We can observe this from the cRNN’s latent trajectories, as visualized in Fig. 2b, where extrapolation in time (beyond T) is non-catastrophic."
  - [corpus]: Weak evidence - the corpus papers do not discuss C-RBP or attractor dynamics in detail.
- Break condition: If the training data or task is too simple, the attractor dynamics might not be necessary, and simpler training algorithms could suffice.

### Mechanism 3
- Claim: Evidential deep learning (EDL) provides a principled way to model uncertainty in neural networks, enabling the computation of the RT metric.
- Mechanism: EDL interprets model outputs as parameters of a Dirichlet distribution, where higher values reflect more evidence in favor of a class. The width of the distribution signals the model's uncertainty. This uncertainty evolution over time forms the basis of the RT metric.
- Core assumption: The Dirichlet distribution is an appropriate model for belief states in classification tasks.
- Evidence anchors:
  - [abstract]: "Specifically, we introduce a novel metric leveraging insights from subjective logic theory summarizing evidence accumulation in recurrent vision models."
  - [section]: "We augment our cRNN training objective with an evidential deep learning loss (EDL; [30])."
  - [corpus]: Weak evidence - the corpus papers do not discuss EDL or its application to reaction time modeling.
- Break condition: If the task is not a classification problem, or if the model outputs cannot be interpreted as belief distributions, EDL may not be applicable.

## Foundational Learning

- Concept: Recurrent neural networks (RNNs) and their training algorithms (BPTT vs. C-RBP)
  - Why needed here: The paper uses a convolutional RNN (cRNN) with C-RBP training. Understanding the differences between BPTT and C-RBP is crucial for appreciating the model's stability and generalization capabilities.
  - Quick check question: What is the main advantage of C-RBP over BPTT in training recurrent models?

- Concept: Evidence accumulation and uncertainty modeling in neural networks
  - Why needed here: The paper introduces an RT metric based on the area under the uncertainty curve, which is derived from the model's evidence accumulation process. Understanding how uncertainty is modeled and used is essential for interpreting the results.
  - Quick check question: How does evidential deep learning (EDL) model uncertainty in neural networks?

- Concept: Reaction time paradigms in cognitive psychology and neuroscience
  - Why needed here: The paper aims to align the model's behavior with human reaction times in various visual tasks. Familiarity with RT paradigms and the factors that influence RTs is necessary for evaluating the model's performance.
  - Quick check question: What are some factors that are known to influence human reaction times in visual tasks?

## Architecture Onboarding

- Component map:
  Input -> Convolutional Recurrent Neural Network (cRNN) -> Evidential Deep Learning Loss -> Uncertainty Evolution Over Time -> ξcRNN Metric

- Critical path:
  1. Preprocess input stimuli
  2. Feed stimuli through cRNN
  3. Track uncertainty evolution over time steps
  4. Compute ξcRNN metric
  5. Compare ξcRNN to human RTs

- Design tradeoffs:
  - Using C-RBP instead of BPTT for training: More stable models but potentially slower training
  - Using EDL for uncertainty modeling: Principled approach but may require more complex training and interpretation
  - Focusing on classification tasks: Well-suited for the metric but may not generalize to other task types

- Failure signatures:
  - Poor generalization to more complex tasks: Indicates issues with model stability or training algorithm
  - Lack of correlation between ξcRNN and human RTs: Suggests the metric is not capturing the relevant decision dynamics
  - Unstable uncertainty evolution: May indicate problems with the EDL implementation or model architecture

- First 3 experiments:
  1. Train a cRNN on a simple visual task (e.g., object recognition) and visualize the uncertainty evolution to ensure the model is learning meaningful dynamics.
  2. Test the model's generalization to a more complex version of the task (e.g., with occlusions or distractors) to verify the stability provided by C-RBP.
  3. Apply the ξcRNN metric to the model's outputs and compare it to human RTs on the same stimuli to assess the alignment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's uncertainty metric (ξcRNN) correlate with other established metrics of computational complexity, such as processing time or energy consumption?
- Basis in paper: [inferred] The paper introduces ξcRNN as a metric capturing the area under the uncertainty curve over time, which reflects computational demand. However, it does not explicitly compare this metric to other established metrics of computational complexity.
- Why unresolved: The paper does not provide a direct comparison between ξcRNN and other metrics, leaving the question of its unique contribution or potential overlap with existing metrics unanswered.
- What evidence would resolve it: Conducting experiments that directly compare ξcRNN with other metrics, such as processing time or energy consumption, would provide insights into its unique contribution and potential advantages or limitations.

### Open Question 2
- Question: How does the model's performance generalize to tasks outside the domain of visual cognition, such as language processing or motor control?
- Basis in paper: [inferred] The paper focuses on applying the framework to visual cognitive tasks, but does not explore its applicability to other domains.
- Why unresolved: The generalizability of the framework to other domains remains unexplored, leaving the question of its broader applicability unanswered.
- What evidence would resolve it: Applying the framework to tasks outside the domain of visual cognition and evaluating its performance would provide insights into its generalizability and potential limitations.

### Open Question 3
- Question: How does the model's uncertainty metric (ξcRNN) relate to human confidence judgments in decision-making tasks?
- Basis in paper: [inferred] The paper introduces ξcRNN as a metric capturing the model's uncertainty, but does not explicitly compare it to human confidence judgments.
- Why unresolved: The relationship between ξcRNN and human confidence judgments remains unexplored, leaving the question of their alignment unanswered.
- What evidence would resolve it: Conducting experiments that directly compare ξcRNN with human confidence judgments in decision-making tasks would provide insights into their alignment and potential implications for understanding human cognition.

## Limitations
- The metric is primarily applicable to classification tasks, limiting generalizability to continuous or regression problems
- The necessity of attractor dynamics training (C-RBP) for simpler tasks is not thoroughly explored
- The paper doesn't address potential model-specific biases that could influence uncertainty evolution

## Confidence

- **High Confidence**: The core methodology for computing ξcRNN from uncertainty curves is technically sound and well-grounded in evidential deep learning theory
- **Medium Confidence**: The alignment between ξcRNN and human RT patterns is demonstrated across multiple tasks, but sample sizes and task details limit broader generalization claims
- **Medium Confidence**: The interpretation that area under the uncertainty curve reflects computational demand is plausible but could benefit from additional mechanistic validation

## Next Checks
1. Test the ξcRNN metric on a regression task (e.g., continuous value prediction) to assess generalizability beyond classification problems
2. Compare C-RBP-trained models with standard BPTT-trained models on simpler tasks to determine if attractor dynamics are truly necessary or merely beneficial
3. Conduct ablation studies removing evidential deep learning uncertainty modeling to verify that the metric captures genuine decision dynamics rather than being an artifact of the training approach