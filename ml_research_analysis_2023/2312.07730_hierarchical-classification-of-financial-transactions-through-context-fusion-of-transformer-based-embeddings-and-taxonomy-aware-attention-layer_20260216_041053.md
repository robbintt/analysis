---
ver: rpa2
title: Hierarchical Classification of Financial Transactions Through Context-Fusion
  of Transformer-based Embeddings and Taxonomy-aware Attention Layer
arxiv_id: '2312.07730'
source_url: https://arxiv.org/abs/2312.07730
tags:
- classification
- dataset
- transactions
- layer
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes the Two-headed DragoNet, a Transformer-based
  model for hierarchical multi-label classification of financial transactions. The
  model uses a stack of Transformer encoders to generate contextual embeddings from
  merchant names and business activity descriptions, followed by a Context Fusion
  layer and two output heads for macro and micro category classification.
---

# Hierarchical Classification of Financial Transactions Through Context-Fusion of Transformer-based Embeddings and Taxonomy-aware Attention Layer

## Quick Facts
- arXiv ID: 2312.07730
- Source URL: https://arxiv.org/abs/2312.07730
- Reference count: 1
- Key outcome: F1-score of 93% (card) and 95% (current account) for macro-category classification; 84.5% (card) and 86.6% (current account) for micro-category classification

## Executive Summary
This paper proposes Two-headed DragoNet, a Transformer-based model for hierarchical multi-label classification of financial transactions. The model processes merchant names and business activity descriptions through separate Transformer encoders, fuses the resulting contextual embeddings, and classifies transactions into both macro and micro categories using two output heads. A novel Taxonomy-aware Attention Layer corrects classification errors that violate hierarchical relationships. The approach achieves state-of-the-art performance on two financial datasets, outperforming classical machine learning methods while maintaining the integrity of hierarchical category relationships.

## Method Summary
Two-headed DragoNet uses separate Transformer encoders to generate contextual embeddings from merchant names and business activity descriptions. These embeddings are fused through concatenation and a feed-forward network to create a unified representation. Two classification heads then predict macro and micro categories simultaneously. A Taxonomy-aware Attention Layer applies rule-based corrections to ensure micro-category predictions respect the hierarchical constraints defined by their corresponding macro categories, preventing invalid parent-child relationships in the taxonomy.

## Key Results
- Achieves F1-score of 93% for macro-category classification on card dataset and 95% on current account dataset
- Attains F1-score of 84.5% for micro-category classification on card dataset and 86.6% on current account dataset
- Outperforms classical machine learning methods on both hierarchical classification tasks
- Context fusion strategy provides measurable performance gains over single-input approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hierarchical taxonomy structure is leveraged to correct classification errors that violate parent-child relationships.
- Mechanism: The Taxonomy-aware Attention Layer uses the macro-category predictions to suppress micro-category predictions that are not children of the predicted macro-category.
- Core assumption: The macro-category prediction is accurate enough to serve as a reliable constraint for micro-category correction.
- Evidence anchors:
  - [abstract]: "Finally, our proposed Taxonomy-aware Attention Layer corrects predictions that break categorical hierarchy rules defined in the given taxonomy."
  - [section 4.4]: "The Taxonomy-aware Attention layer consists of a rule-based layer that adjusts predictions of micro categories that break the category hierarchy defined in a taxonomy."
- Break condition: If the macro-category prediction is incorrect, the suppression of micro-category predictions may lead to wrong classifications.

### Mechanism 2
- Claim: Contextual embeddings generated by Transformer encoders capture semantic relationships between merchant names and business activities.
- Mechanism: Transformer encoders use self-attention to aggregate contextual information from both merchant names and business activity descriptions, creating embeddings that reflect the meaning of the transaction.
- Core assumption: The semantic information in the merchant name and business activity description is sufficient to accurately classify the transaction.
- Evidence anchors:
  - [abstract]: "Our model is based on a stack of Transformers encoder layers that generate contextual embeddings from two short textual descriptors (merchant name and business activity)"
  - [section 4.2]: "The Transformer Encoder layer learns to generate contextual representations (embeddings) from a sequence of symbolic inputs using stacked Self-Attention and Fully Connected layers."
- Break condition: If the merchant name and business activity description are ambiguous or misleading, the contextual embeddings may not accurately represent the transaction.

### Mechanism 3
- Claim: Context fusion combines the embeddings from merchant name and business activity description to create a more robust representation.
- Mechanism: The Context Fusion layer concatenates the embeddings from both sources and passes them through a feed-forward network with sigmoid activation to generate a single high-level contextual representation.
- Core assumption: The merchant name and business activity description provide complementary information that improves classification accuracy when combined.
- Evidence anchors:
  - [section 4.3]: "The Context-Fusion Layer comprises a concatenation of the contextual embeddings from merchant name and business activity description (Tn and Te), an FNN with ReLU activation, followed by a last layer of a linear transformation with Sigmoid activation."
  - [section 5.1]: "Notably, the strategy of fusing contextual embeddings was beneficial, as the 'Transformer + Context Fusion' model achieved F1-score gains in both datasets compared to the vanilla transformer."
- Break condition: If the merchant name and business activity description are highly correlated or redundant, context fusion may not provide significant benefits.

## Foundational Learning

- Concept: Hierarchical multi-label classification
  - Why needed here: The task involves classifying transactions into two levels of categories (macro and micro), where each transaction can belong to multiple categories at each level.
  - Quick check question: What is the difference between hierarchical and flat classification, and why is it important for this problem?

- Concept: Transformer-based models and self-attention
  - Why needed here: Transformers are used to generate contextual embeddings from the merchant name and business activity description, capturing semantic relationships between words.
  - Quick check question: How does self-attention work in a Transformer encoder, and why is it effective for generating contextual embeddings?

- Concept: Contextual embeddings and their aggregation
  - Why needed here: Contextual embeddings are generated for both merchant name and business activity description, and then fused to create a single high-level representation for classification.
  - Quick check question: What is the difference between contextual and non-contextual embeddings, and why is context fusion important in this model?

## Architecture Onboarding

- Component map: Input → Transformer encoders → Context Fusion → Output heads → Taxonomy-aware Attention → Final predictions
- Critical path: Merchant name and business activity description are encoded separately, fused together, classified at two hierarchical levels, then corrected for hierarchy violations
- Design tradeoffs:
  - Using two separate Transformer encoders for merchant name and business activity description vs. a single encoder with shared parameters
  - The choice of using a rule-based Taxonomy-aware Attention layer vs. a learned layer
- Failure signatures:
  - Poor performance on macro-category classification may indicate issues with the Transformer encoders or the context fusion layer
  - Incorrect hierarchy corrections may suggest problems with the Taxonomy-aware Attention layer
- First 3 experiments:
  1. Evaluate the performance of the model using only the merchant name as input.
  2. Evaluate the performance of the model using only the business activity description as input.
  3. Evaluate the performance of the model using both merchant name and business activity description as inputs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the Two-headed DragoNet perform with a more extensive retail consumption ontology with unlimited hierarchy levels?
- Basis in paper: [inferred] The authors plan to expand their taxonomy to a more extensive retail consumption ontology with unlimited hierarchy levels in future work.
- Why unresolved: The current model and experiments are limited to a two-level hierarchical taxonomy. The scalability and performance of the model with a deeper hierarchy are unknown.
- What evidence would resolve it: Training and evaluating the model on a dataset with a deeper hierarchical taxonomy, and comparing the performance metrics to the current results.

### Open Question 2
- Question: What is the impact of enriching the datasets with external semantic data on the model's performance?
- Basis in paper: [inferred] The authors intend to investigate ways to improve the model's performance by enriching the datasets with external semantic data, as proposed by [Vollset et al. 2017].
- Why unresolved: The current experiments are conducted on datasets built from BTG Pactual Banking's transactional stream without external semantic data. The potential benefits of incorporating such data are not explored.
- What evidence would resolve it: Conducting experiments using enriched datasets with external semantic data and comparing the performance metrics to the current results.

### Open Question 3
- Question: How effective are contextual customer embeddings (Customer2Vec) in improving the classification of complex transactions with ambiguous merchant names?
- Basis in paper: [inferred] The authors plan to explore contextual customer embeddings to help classify complex examples, especially in transactions with ambiguous merchant names.
- Why unresolved: The current model relies solely on merchant names and business activity descriptions for classification. The potential benefits of incorporating customer embeddings are not explored.
- What evidence would resolve it: Conducting experiments using customer embeddings in addition to merchant names and business activity descriptions, and comparing the performance metrics to the current results.

## Limitations
- The rule-based Taxonomy-aware Attention Layer assumes perfect macro-category predictions, which may propagate errors if initial classifications are incorrect
- Performance heavily depends on the quality and distinctiveness of merchant names and business activity descriptions, which may vary across financial contexts
- Absence of cross-dataset validation raises concerns about generalization to new transaction types or different financial institutions

## Confidence
- High confidence in the Transformer architecture's ability to generate contextual embeddings
- Medium confidence in the Context Fusion layer's contribution to performance gains
- Medium confidence in the Taxonomy-aware Attention layer's effectiveness, given limited validation of hierarchy correction in edge cases

## Next Checks
1. **Error Propagation Analysis**: Measure the impact of incorrect macro-category predictions on micro-category classification accuracy by artificially introducing errors in macro predictions and measuring downstream effects.

2. **Ablation Study**: Remove the Context Fusion layer to quantify its exact contribution to the 9-10% performance improvement, isolating whether the gains come from fusion or from the Transformer architecture itself.

3. **Out-of-Domain Testing**: Evaluate the model on transactions from different financial domains (e.g., investment accounts, loan payments) to assess generalization beyond the card and current account datasets used in the study.