---
ver: rpa2
title: 'BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural
  Language Processing'
arxiv_id: '2310.19975'
source_url: https://arxiv.org/abs/2310.19975
tags:
- tasks
- llama
- instruction
- task
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents BioInstruct, a dataset of 25,000+ instruction-based
  samples for instruction tuning biomedical language models. By leveraging GPT-4 to
  generate task-specific instructions from seed examples, the authors created a diverse
  dataset covering question answering, information extraction, and text generation.
---

# BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing

## Quick Facts
- arXiv ID: 2310.19975
- Source URL: https://arxiv.org/abs/2310.19975
- Authors: Multiple
- Reference count: 40
- One-line primary result: Instruction tuning LLaMA models with BioInstruct dataset improves performance on biomedical NLP tasks by 17.3% in QA, 5.7% in IE, and 96% in generation tasks.

## Executive Summary
This paper introduces BioInstruct, a dataset of 25,000+ instruction-based samples for instruction tuning biomedical language models. The authors leverage GPT-4 to generate task-specific instructions from seed examples, creating a diverse dataset covering question answering, information extraction, and text generation. Instruction tuning LLaMA models with BioInstruct leads to significant performance gains compared to non-tuned models. The 7B LLaMA 1 model trained on BioInstruct achieves competitive or superior performance to other biomedical models on various benchmarks. The BioInstruct dataset and instruction-tuned models provide a valuable resource for biomedical NLP applications.

## Method Summary
The authors create the BioInstruct dataset by leveraging GPT-4 to generate task-specific instructions from seed examples manually collected by humans. The dataset contains 25,005 instruction-based samples covering diverse biomedical NLP tasks. LLaMA 1 and 2 models (7B and 13B versions) are fine-tuned using LoRA (Low-Rank Adaptation), a parameter-efficient method that modifies the weight matrices with low-rank updates. The fine-tuned models are evaluated on multiple biomedical benchmarks, including MedQA-USMLE, MedMCQA, PubMedQA, BioASQ MCQA, MedNLI, Medication Status Extraction, Clinical Coreference Resolution, Conv2note, and ICliniq.

## Key Results
- Instruction tuning LLaMA models with BioInstruct improves performance by 17.3% in QA, 5.7% in IE, and 96% in generation tasks compared to non-tuned models.
- The 7B LLaMA 1 model trained on BioInstruct achieves competitive or superior performance to other biomedical models on various benchmarks.
- Performance gains are higher when fine-tuning on tasks closely related to evaluation tasks, aligning with multi-task learning principles.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction tuning with BioInstruct improves LLM performance on biomedical tasks by providing domain-specific task instructions.
- Mechanism: BioInstruct provides diverse biomedical task instructions paired with input/output examples, allowing LLMs to learn task-specific patterns and biomedical knowledge.
- Core assumption: LLMs can effectively learn from instruction-following tasks and generalize to unseen biomedical tasks.
- Evidence anchors:
  - [abstract] "Comparing with LLMs without instruction-tuned, our instruction-tuned LLMs demonstrated marked performance gains: 17.3% in QA, 5.7% in IE, and 96% in Generation tasks."
  - [section] "Our findings offer insights into the diverse impacts of BioInstruct across these tasks. We find that LLMs fine-tuned with BioInstruct outperformed LLMs without BioInstruct in QA, IE, Generation tasks by 17.3%, 5.7%, 96% respectively."
  - [corpus] Corpus shows related work on instruction tuning, but lacks specific evidence for BioInstruct's effectiveness. Need more direct evidence.
- Break condition: If LLMs cannot effectively learn from instruction-following tasks or if the generated instructions are too dissimilar from real biomedical tasks.

### Mechanism 2
- Claim: Multi-task learning with BioInstruct improves LLM performance by leveraging synergies between related tasks.
- Mechanism: Instruction tuning on multiple related biomedical tasks allows LLMs to learn shared representations and transfer knowledge between tasks.
- Core assumption: Biomedical tasks share underlying patterns and knowledge that can be leveraged through multi-task learning.
- Evidence anchors:
  - [abstract] "Our results also show that the performance gain is significantly higher when instruction fine-tuning is conducted with closely related tasks."
  - [section] "We observe that contributing fine-tuning tasks is dependent on each evaluation task. This inspires future work to predict contributing tasks given a new evaluation task."
  - [corpus] Corpus lacks direct evidence for multi-task learning benefits in biomedical domain. Need more specific evidence.
- Break condition: If the tasks in BioInstruct are too dissimilar to leverage shared representations or if the model capacity is insufficient for multi-task learning.

### Mechanism 3
- Claim: Parameter-efficient fine-tuning with LoRA allows effective instruction tuning without full model fine-tuning.
- Mechanism: LoRA modifies the weight matrices of pre-trained LLMs with low-rank updates, enabling efficient adaptation to new tasks.
- Core assumption: Weight updates during instruction tuning have low "intrinsic rank", allowing efficient representation with low-rank matrices.
- Evidence anchors:
  - [section] "For a pre-trained weight matrix W0 ∈ Rd×k its update is constrained by a low-rank decomposition W0 + ∆W = W0 + BA, where B ∈ Rd×r, A ∈ Rr×k, and rank r ≪ min(d, k)."
  - [section] "With LoRA, we can efficiently fine-tune the LLMs even with limited GPU memory, enabling the fine-tuning of the 7B and 13B versions of LLaMA 1 and LLaMA 2 using just 2 A100 GPUs."
  - [corpus] Corpus lacks specific evidence for LoRA's effectiveness in biomedical instruction tuning. Need more direct evidence.
- Break condition: If the weight updates during instruction tuning do not have low intrinsic rank or if the low-rank approximation is insufficient for the task.

## Foundational Learning

- Concept: Large Language Models (LLMs)
  - Why needed here: BioInstruct fine-tunes LLaMA models, which are large language models.
  - Quick check question: What is the primary architecture used by LLaMA models?
- Concept: Instruction Tuning
  - Why needed here: BioInstruct uses instruction tuning to adapt LLMs to biomedical tasks.
  - Quick check question: How does instruction tuning differ from traditional fine-tuning?
- Concept: Multi-Task Learning
  - Why needed here: BioInstruct explores multi-task learning by fine-tuning on multiple biomedical tasks.
  - Quick check question: What is the main benefit of multi-task learning in the context of BioInstruct?

## Architecture Onboarding

- Component map:
  BioInstruct dataset -> LLaMA models (7B and 13B versions) -> LoRA -> Evaluation benchmarks

- Critical path:
  1. Generate BioInstruct dataset using GPT-4
  2. Preprocess and filter instructions
  3. Fine-tune LLaMA models with LoRA using BioInstruct
  4. Evaluate fine-tuned models on biomedical benchmarks
  5. Analyze performance and identify synergies between tasks

- Design tradeoffs:
  - Model size vs. computational resources: Larger models may perform better but require more resources
  - Dataset size vs. quality: More instructions may improve performance but increase noise
  - Single-task vs. multi-task learning: Multi-task learning may improve generalization but require more diverse data

- Failure signatures:
  - Poor performance on evaluation tasks: May indicate insufficient instruction diversity or model capacity
  - Overfitting to training instructions: May indicate lack of task diversity or insufficient regularization
  - Slow convergence during fine-tuning: May indicate suboptimal hyperparameters or model architecture

- First 3 experiments:
  1. Fine-tune LLaMA 7B on a single biomedical task from BioInstruct and evaluate on the corresponding benchmark
  2. Fine-tune LLaMA 7B on all biomedical tasks from BioInstruct and evaluate on all benchmarks
  3. Compare performance of LLaMA 7B fine-tuned on BioInstruct vs. other fine-tuned biomedical LLMs on the same benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can task embeddings be developed and utilized to identify the most effective task combinations for instruction tuning in the BioNLP domain?
- Basis in paper: [explicit] The paper concludes by suggesting future work could explore task embedding in the BioNLP domain, noting that there is no one-size-fits-all task combination for all BioNLP evaluations.
- Why unresolved: While the paper identifies the need for task embeddings to predict contributing tasks, it does not provide a methodology or results for developing such embeddings in the BioNLP context.
- What evidence would resolve it: Developing a method to embed BioNLP tasks and using these embeddings to predict optimal task combinations for instruction tuning, validated through improved performance on diverse BioNLP benchmarks.

### Open Question 2
- Question: To what extent does the size of the instruction tuning dataset impact the performance of biomedical language models on various BioNLP tasks?
- Basis in paper: [explicit] The paper notes a positive correlation between the number of generated examples and downstream task performance, suggesting that performance can be further enhanced by expanding the size of the BioInstruct dataset.
- Why unresolved: The paper does not explore the relationship between dataset size and model performance in detail, nor does it establish an optimal size for instruction tuning datasets in the biomedical domain.
- What evidence would resolve it: Conducting experiments with varying sizes of instruction tuning datasets and measuring their impact on the performance of biomedical language models across a range of BioNLP tasks, to identify the point of diminishing returns.

### Open Question 3
- Question: What are the specific characteristics of biomedical tasks that make them more or less amenable to instruction tuning, and how can these insights guide the development of future instruction tuning datasets?
- Basis in paper: [inferred] The paper observes that certain tasks, like QA, benefit more from instruction tuning than others, such as IE, and that the effectiveness of task combinations varies depending on the evaluation task.
- Why unresolved: The paper does not provide a detailed analysis of the characteristics that make biomedical tasks suitable for instruction tuning or offer guidance on how to design future datasets to maximize the benefits of this approach.
- What evidence would resolve it: Analyzing the structural and linguistic features of biomedical tasks to identify common characteristics that correlate with improved performance through instruction tuning, and using these insights to inform the design of future instruction tuning datasets.

## Limitations

- Limited Evidence for Mechanisms: The paper presents performance improvements but lacks direct evidence for the effectiveness of BioInstruct's instructions, the benefits of multi-task learning in the biomedical domain, and the efficiency of LoRA for this specific application.
- Dataset Generation Process: The exact prompt structure and instructions used to generate the BioInstruct dataset with GPT-4 are not specified, limiting reproducibility and raising questions about instruction quality and consistency.
- Reproducibility Concerns: Key details for faithful reproduction are missing, including specific hyperparameters for LoRA fine-tuning and the exact composition of the seed tasks used to generate BioInstruct.

## Confidence

- High Confidence: The BioInstruct dataset contains 25,000+ instruction-based samples covering diverse biomedical NLP tasks, and instruction tuning with this dataset led to significant performance gains compared to non-tuned models. The use of LoRA for parameter-efficient fine-tuning is well-established and applicable to this setting.
- Medium Confidence: The reported performance gains (17.3% in QA, 5.7% in IE, and 96% in generation tasks) are likely accurate based on the evaluation setup. The observation that performance gains are higher for tasks closely related to evaluation tasks aligns with multi-task learning principles, but the specific synergies between tasks in BioInstruct need further validation.
- Low Confidence: The underlying mechanisms of how BioInstruct's instructions effectively improve LLM performance, the benefits of multi-task learning with BioInstruct in the biomedical domain, and the efficiency of LoRA for this specific application are not fully supported by direct evidence in the paper or related work.

## Next Checks

1. Validate Instruction Effectiveness: Conduct a human evaluation of a sample of BioInstruct's instructions to assess their quality, relevance, and diversity. Compare the performance of models fine-tuned on manually curated instructions vs. GPT-4 generated instructions to quantify the impact of instruction quality.

2. Analyze Multi-Task Learning Synergies: Perform an ablation study to isolate the contribution of each task in BioInstruct to the overall performance gains. Fine-tune separate models on individual tasks and combinations of tasks, and evaluate their performance on the biomedical benchmarks. This will provide insights into the task synergies and guide the design of future instruction tuning datasets.

3. Compare LoRA vs. Full Fine-Tuning: Conduct a controlled experiment comparing the performance of models fine-tuned with LoRA vs. full fine-tuning on the same BioInstruct dataset. Measure the parameter efficiency, training time, and final performance of both methods to assess the trade-offs and validate the effectiveness of LoRA for biomedical instruction tuning.