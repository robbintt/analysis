---
ver: rpa2
title: 'Explainable Artificial Intelligence (XAI) 2.0: A Manifesto of Open Challenges
  and Interdisciplinary Research Directions'
arxiv_id: '2310.19775'
source_url: https://arxiv.org/abs/2310.19775
tags:
- explanations
- methods
- intelligence
- artificial
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a manifesto for Explainable AI (XAI) 2.0, identifying
  27 open problems across nine categories to advance the field. The authors highlight
  the growing importance of XAI as AI systems become more prevalent in critical domains,
  emphasizing the need for transparency and interpretability.
---

# Explainable Artificial Intelligence (XAI) 2.0: A Manifesto of Open Challenges and Interdisciplinary Research Directions

## Quick Facts
- arXiv ID: 2310.19775
- Source URL: https://arxiv.org/abs/2310.19775
- Reference count: 40
- Key outcome: This paper presents a manifesto for Explainable AI (XAI) 2.0, identifying 27 open problems across nine categories to advance the field.

## Executive Summary
This manifesto brings together experts from diverse fields to identify and address critical open problems in Explainable AI (XAI). As AI systems become increasingly prevalent in critical domains, the need for transparency and interpretability has never been more urgent. The paper systematically categorizes 27 challenges across nine thematic areas, ranging from creating explanations for new AI types to improving societal impact. By establishing a comprehensive research agenda, the manifesto aims to synchronize efforts across disciplines and accelerate practical applications of XAI.

The authors emphasize that advancing XAI requires not just technical innovation but also consideration of human factors, ethical implications, and power dynamics between individuals and organizations. The manifesto proposes concept-based explanations, standardized evaluation frameworks, and stakeholder-centric approaches as potential solutions to these complex challenges. Through this collaborative effort, the paper sets a foundation for the next generation of XAI research that balances technical rigor with human-centered design and societal responsibility.

## Method Summary
The paper employs a collaborative, consensus-building approach by bringing together experts from philosophy, psychology, HCI, and computer science to identify and categorize open problems in XAI. The authors synthesize existing knowledge and research gaps to create a comprehensive manifesto with 27 open problems organized into nine thematic categories. Each problem includes associated challenges and proposed solution ideas, drawing on diverse disciplinary perspectives to address both technical and societal aspects of XAI development.

## Key Results
- Identifies 27 open problems across nine categories, providing a structured research agenda for XAI advancement
- Highlights critical challenges in creating explanations for new AI types including generative models and concept-based learning algorithms
- Emphasizes the need for standardized evaluation frameworks and concept-based explanations to improve XAI's effectiveness and accessibility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The manifesto creates a structured research agenda by categorizing 27 open problems into 9 thematic categories, enabling interdisciplinary collaboration.
- Mechanism: By organizing challenges into clear categories (e.g., creating explanations for new AI types, improving current methods, evaluating XAI), researchers from different fields can identify relevant problems and contribute their expertise.
- Core assumption: Researchers can effectively collaborate across disciplines when challenges are clearly categorized and accessible.
- Evidence anchors:
  - [abstract] "We bring together experts from diverse fields to identify open problems, striving to synchronize research agendas and accelerate XAI in practical applications."
  - [section 4] "This manifesto aims to define and succinctly describe the open challenges scholars in the field face."
- Break condition: If the categories are too broad or too narrow, researchers may not find their specific area of expertise represented, reducing interdisciplinary engagement.

### Mechanism 2
- Claim: The manifesto addresses the evaluation gap in XAI by proposing standardized frameworks and metrics for assessing explanation quality.
- Mechanism: By highlighting the need for standardized evaluation methods (3.3.2), the manifesto pushes the community toward developing reproducible and comparable evaluation approaches.
- Core assumption: Standardized evaluation metrics can be developed that are applicable across different XAI methods and domains.
- Evidence anchors:
  - [section 3.3.2] "There are several works that address the evaluation of XAI methods... However, there is currently a lack of standardized methods and metrics for evaluating XAI systems."
  - [section 3.3.2] "Frameworks such as Quantus allow for the evaluation and comparison of explanations in a standardized and reproducible manner."
- Break condition: If evaluation metrics cannot capture the nuanced differences between explanation types or fail to account for domain-specific requirements, the standardization effort may not achieve its goals.

### Mechanism 3
- Claim: The manifesto tackles the challenge of human-centered explanations by proposing concept-based approaches that align with human understanding.
- Mechanism: By emphasizing the need for explanations that use human-understandable concepts (3.6.2), the manifesto guides research toward explanations that are more accessible to non-technical stakeholders.
- Core assumption: Concept-based explanations can be synthesized from complex models in a way that preserves their accuracy while improving human comprehension.
- Evidence anchors:
  - [section 3.6.2] "Concept-based XAI methods go beyond attribution and aim to express human-understandable concepts as part of the explanation."
  - [section 3.6.2] "Concept-based explanations can aid the insertion of expert knowledge in the learning process of a model."
- Break condition: If concept extraction from models proves too difficult or if the synthesized concepts do not accurately represent the model's decision-making process, this approach may not be feasible.

## Foundational Learning

- Concept: Interdisciplinary collaboration in XAI
  - Why needed here: The manifesto brings together experts from diverse fields (philosophy, psychology, HCI, computer science) to address XAI challenges.
  - Quick check question: What are the benefits and challenges of interdisciplinary collaboration in XAI research?

- Concept: Evaluation metrics for XAI
  - Why needed here: The manifesto identifies the lack of standardized evaluation methods as a key challenge (3.3.2).
  - Quick check question: What properties should effective XAI evaluation metrics capture?

- Concept: Concept-based explanations
  - Why needed here: The manifesto proposes concept-based explanations as a way to make XAI more human-understandable (3.6.2).
  - Quick check question: How do concept-based explanations differ from traditional feature attribution methods?

## Architecture Onboarding

- Component map: The manifesto is structured into 9 high-level categories, each containing 2-4 specific open problems. Each problem includes challenges and proposed solution ideas.
- Critical path: Identify relevant category → Select specific problem → Review challenges → Implement proposed solution ideas → Evaluate using standardized metrics
- Design tradeoffs: Broad categories enable interdisciplinary collaboration but may lack specificity; narrow categories provide focus but may exclude relevant perspectives
- Failure signatures: Researchers cannot find relevant problems in their expertise area; proposed solutions are not implementable; evaluation metrics are not widely adopted
- First 3 experiments:
  1. Select one category (e.g., "Creating Explanations for New Types of AI") and implement one proposed solution (e.g., mechanistic interpretability for generative models)
  2. Develop a prototype evaluation framework based on the Quantus toolkit mentioned in the manifesto
  3. Create a concept-based explanation system for a simple classification task and compare its effectiveness with traditional attribution methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop evaluation frameworks that effectively measure the quality and effectiveness of XAI methods across different domains and stakeholder needs?
- Basis in paper: [explicit] The paper highlights the lack of standardized methods and metrics for evaluating XAI systems, emphasizing the need for robust evaluation frameworks.
- Why unresolved: Current evaluation approaches often focus on technical properties without considering user interaction, leading to gaps in understanding how explanations impact decision-making and trust.
- What evidence would resolve it: Empirical studies demonstrating the effectiveness of XAI methods in real-world scenarios, coupled with the development of standardized evaluation metrics and user-centric testing protocols.

### Open Question 2
- Question: What are the criteria for determining when an explanation is incorrect or falsifiable, and how can these criteria be applied to ensure accountability in XAI systems?
- Basis in paper: [explicit] The paper discusses the lack of clarity regarding the falsifiability of explanations and the need for benchmarks to establish the correctness of explanations.
- Why unresolved: There is currently no established framework for determining the falsifiability of explanations, making it difficult to hold AI practitioners accountable for the accuracy of their explanations.
- What evidence would resolve it: Development of standardized protocols for evaluating the correctness of explanations, incorporating principles from the philosophy of science and empirical testing methodologies.

### Open Question 3
- Question: How can we address the power imbalance between individuals and companies in the context of XAI, ensuring that explanations are not only transparent but also just and accountable?
- Basis in paper: [explicit] The paper highlights the need to shift focus from individual reactions to the outputs of automated decision-making to addressing the broader societal and ethical problems behind unfair and untrustworthy AI.
- Why unresolved: Current XAI methods primarily focus on technical transparency, failing to address the structural issues of power imbalance and lack of accountability in AI systems.
- What evidence would resolve it: Implementation of participatory design approaches, impact assessments, and governance measures that involve impacted stakeholders in the decision-making process and ensure accountability for AI systems.

## Limitations
- The manifesto approach provides limited guidance on prioritization or sequencing of the 27 open problems
- Many proposed solutions lack specific implementation details, limiting immediate practical application
- The paper does not fully address potential conflicts between different stakeholder needs (e.g., regulatory compliance versus user comprehension)

## Confidence

**Confidence Labels:**
- Problem identification and categorization: High - The structured approach to organizing challenges is well-reasoned and comprehensive
- Proposed solution ideas: Medium - While conceptually sound, many solutions lack specific implementation details
- Evaluation framework recommendations: Medium - Identifies the need but provides limited concrete metrics
- Human-centered approach proposals: Medium - Recognizes importance but doesn't fully address power dynamics between stakeholders

## Next Checks
1. Conduct a Delphi study with XAI practitioners to prioritize the 27 open problems based on practical feasibility and impact
2. Develop a prototype evaluation framework implementing the Quantus toolkit for at least two different XAI methods and test on a real-world dataset
3. Create a pilot concept-based explanation system for a simple classification task and conduct user studies comparing comprehension with traditional attribution methods