---
ver: rpa2
title: 'MP3: Movement Primitive-Based (Re-)Planning Policy'
arxiv_id: '2306.12729'
source_url: https://arxiv.org/abs/2306.12729
tags:
- reward
- policy
- learning
- task
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MP3, a deep RL approach that integrates movement
  primitives (MPs) with neural network policies to enable smooth trajectory generation
  and efficient learning from sparse and non-Markovian rewards. By exploring in the
  parameter space of MPs rather than atomic actions, MP3 achieves time-correlated
  exploration and better performance in sparse reward settings.
---

# MP3: Movement Primitive-Based (Re-)Planning Policy

## Quick Facts
- arXiv ID: 2306.12729
- Source URL: https://arxiv.org/abs/2306.12729
- Reference count: 40
- This paper introduces MP3, a deep RL approach that integrates movement primitives with neural network policies to enable smooth trajectory generation and efficient learning from sparse and non-Markovian rewards.

## Executive Summary
This paper presents MP3, a novel deep reinforcement learning framework that integrates movement primitives (MPs) with neural network policies. MP3 addresses the challenge of learning in sparse reward environments by exploring in the parameter space of MPs rather than atomic actions, enabling smooth trajectory generation throughout the learning process. The method supports both episode-based and step-based RL through adjustable planning horizons and introduces replanning capabilities using ProDMPs for online adaptation during execution. Experimental results on tasks like robot table tennis, box pushing, and Meta-World demonstrate improved sample efficiency and asymptotic performance compared to state-of-the-art deep RL methods, particularly in environments requiring replanning or handling non-Markovian rewards.

## Method Summary
MP3 combines movement primitives with deep RL by training a neural network policy to output movement primitive parameters rather than raw actions. The policy uses Trust Region Projection Layers (TRPL) for stable updates in the higher-dimensional parameter space. For trajectory generation, MP3 employs Probabilistic Dynamic Movement Primitives (ProDMPs) that support smooth replanning during execution. The planning horizon parameter k determines whether MP3 behaves as step-based (k=1) or episode-based (k=T) RL. The framework handles sparse and non-Markovian rewards by aggregating rewards over trajectory segments, and supports replanning by periodically updating movement primitive parameters during execution while maintaining trajectory smoothness.

## Key Results
- MP3 achieves better sample efficiency and asymptotic performance than PPO, TRPL, and SAC baselines in sparse reward settings across multiple robotic tasks
- The replanning capability allows MP3 to adapt to goal switches and external perturbations during execution, demonstrated in robot table tennis and box pushing tasks
- MP3 effectively handles non-Markovian rewards by treating entire trajectory segments as single samples, outperforming step-based methods in tasks with history-dependent rewards

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MP3 improves sample efficiency and performance in sparse reward settings by exploring in the parameter space of movement primitives rather than atomic actions.
- Mechanism: By selecting parameters for movement primitives instead of raw actions at each timestep, MP3 generates smooth, time-correlated trajectories that reduce exploration noise and allow effective learning from sparse feedback.
- Core assumption: The trajectory space explored by movement primitives contains sufficient diversity to solve the task without step-level exploration noise.
- Evidence anchors:
  - [abstract]: "By exploring in the parameter space of MPs rather than atomic actions, MP3 achieves time-correlated exploration and better performance in sparse reward settings."
  - [section 3.3]: "While traditional SRL methods rely on single raw actions at ∈ A per time step, we train a policy to select a weights vector wt ∈ W in MP's parameter space W."
  - [corpus]: Weak correlation with related papers on movement primitives, but corpus evidence does not directly support this mechanism.
- Break condition: If the movement primitive parameter space cannot represent the optimal trajectory for a task, or if the task requires fine-grained action corrections that cannot be captured by trajectory parameters.

### Mechanism 2
- Claim: MP3's replanning capability allows adaptation to environmental changes during execution, which is not possible with pure episode-based methods.
- Mechanism: By periodically updating movement primitive parameters during execution, MP3 can adjust to unexpected changes like goal switches or external perturbations while maintaining trajectory smoothness.
- Core assumption: The movement primitive representation supports smooth replanning without discontinuities in position and velocity.
- Evidence anchors:
  - [abstract]: "Additionally, MP3 maintains the capability to adapt to changes in the environment during execution."
  - [section 3.5]: "In contrast, in this paper, we employ the recently introduced probabilistic dynamic movement primitives (ProDMPs) to address the limitations of commonly used MPs... ProDMP can use any initial state as the initial condition, allowing for the generation of smooth desired trajectories even when applying replanning."
  - [corpus]: No direct evidence from corpus papers supporting this specific replanning mechanism.
- Break condition: If the environment changes are too rapid or unpredictable for the replanning frequency to handle, or if ProDMPs cannot maintain smoothness during parameter updates.

### Mechanism 3
- Claim: MP3 effectively handles non-Markovian rewards by treating entire trajectory segments as single samples in the policy update.
- Mechanism: By aggregating rewards over trajectory segments rather than individual timesteps, MP3 can learn from reward signals that depend on the full trajectory history, which is challenging for standard step-based methods.
- Core assumption: The non-Markovian reward structure can be meaningfully aggregated over the trajectory segment length used by MP3.
- Evidence anchors:
  - [abstract]: "By integrating movement primitives (MPs) into the deep RL framework, MP3 enables the generation of smooth trajectories throughout the whole learning process while effectively learning from sparse and non-Markovian rewards."
  - [section 4.1]: "We observe that the MP3-BB agent is able to adapt its behavior and return the ball to the new target point with high precision. In contrast, the MP3-BB agent, which only receives the initial observation containing the initial target position, can only hit the ball but cannot solve this task."
  - [corpus]: No direct evidence from corpus papers supporting this non-Markovian reward handling mechanism.
- Break condition: If the non-Markovian reward depends on information beyond the trajectory segment length, or if the reward signal changes too rapidly within a segment.

## Foundational Learning

- Concept: Movement Primitives (MPs)
  - Why needed here: MPs provide a compact representation of robot trajectories that can be parameterized and optimized in reinforcement learning, enabling smooth motion generation and efficient exploration of trajectory space.
  - Quick check question: What is the main advantage of using movement primitives over raw action spaces in reinforcement learning?

- Concept: Trust Region Policy Optimization (TRPO) and Trust Region Projection Layers (TRPL)
  - Why needed here: TRPL provides exact trust region enforcement for policy updates, which is crucial for the stability of learning in the higher-dimensional parameter space of movement primitives.
  - Quick check question: How does TRPL differ from approximate trust region methods like PPO in terms of policy update stability?

- Concept: Dynamic Movement Primitives (DMPs) vs Probabilistic Dynamic Movement Primitives (ProDMPs)
  - Why needed here: ProDMPs allow for computationally efficient online trajectory generation with smooth replanning capabilities, which is essential for MP3's replanning functionality.
  - Quick check question: What is the key difference between DMPs and ProDMPs that makes ProDMPs more suitable for neural network integration?

## Architecture Onboarding

- Component map:
  - RL Policy -> Movement Primitive Model -> Tracking Controller -> Environment -> Trust Region Projection Layer

- Critical path:
  1. RL policy generates movement primitive parameters from observation
  2. Movement primitive model generates desired trajectory from parameters
  3. Tracking controller converts trajectory to raw actions
  4. Environment executes actions and returns observation and reward
  5. Trust region projection layer ensures stable policy updates

- Design tradeoffs:
  - Planning horizon length: Longer horizons reduce sample efficiency but improve performance in sparse reward settings
  - Number of basis functions: More bases increase expressiveness but also computational cost
  - Replanning frequency: More frequent replanning improves adaptability but may reduce smoothness

- Failure signatures:
  - Jerky or discontinuous motion: Indicates issues with movement primitive representation or replanning
  - Poor sample efficiency: May indicate suboptimal planning horizon or basis function selection
  - Failure to adapt to environmental changes: Could indicate insufficient replanning frequency or inappropriate observation space

- First 3 experiments:
  1. Implement MP3 with fixed planning horizon on a simple Reacher task to verify basic functionality
  2. Test different planning horizon lengths on the Reacher task to find optimal configuration
  3. Add replanning capability and test on a goal-switching variant of the Reacher task to verify adaptation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MP3's performance compare to SRL methods when handling tasks with sparse rewards in environments with high-dimensional state spaces?
- Basis in paper: [inferred] The paper discusses MP3's effectiveness in sparse reward settings, but does not provide a direct comparison with SRL methods in high-dimensional state spaces.
- Why unresolved: The paper focuses on the effectiveness of MP3 in sparse reward settings but does not explicitly compare its performance against SRL methods in high-dimensional state spaces.
- What evidence would resolve it: Conducting experiments comparing MP3 and SRL methods in high-dimensional state spaces with sparse rewards would provide evidence to resolve this question.

### Open Question 2
- Question: What is the impact of using different types of movement primitives (e.g., ProMPs vs. ProDMPs) on the performance of MP3 in tasks requiring replanning?
- Basis in paper: [explicit] The paper mentions the use of ProDMPs for replanning and ProMPs for the black-box setting, but does not provide a direct comparison of their performance in tasks requiring replanning.
- Why unresolved: The paper does not provide a direct comparison of the performance of ProDMPs and ProMPs in tasks requiring replanning.
- What evidence would resolve it: Conducting experiments comparing the performance of MP3 using ProDMPs and ProMPs in tasks requiring replanning would provide evidence to resolve this question.

### Open Question 3
- Question: How does the choice of planning horizon affect the sample efficiency and asymptotic performance of MP3 in different types of tasks (e.g., sparse vs. dense rewards)?
- Basis in paper: [explicit] The paper discusses the impact of planning horizon on performance but does not provide a comprehensive analysis of its effects on sample efficiency and asymptotic performance across different task types.
- Why unresolved: The paper does not provide a comprehensive analysis of how the planning horizon affects sample efficiency and asymptotic performance across different task types.
- What evidence would resolve it: Conducting experiments varying the planning horizon in tasks with different reward structures (sparse vs. dense) and analyzing the impact on sample efficiency and asymptotic performance would provide evidence to resolve this question.

## Limitations
- The paper lacks rigorous validation that movement primitive parameter space contains sufficient diversity for complex tasks, with no analysis of whether step-level exploration could have solved tasks
- The non-Markovian reward handling mechanism lacks theoretical justification for why trajectory segment aggregation works effectively
- While replanning capabilities are demonstrated, the paper doesn't explore failure modes when replanning cannot keep pace with environmental changes

## Confidence
- Mechanism 1 (sparse reward exploration): Medium - supported by experiments but lacks theoretical grounding
- Mechanism 2 (replanning adaptation): Medium - demonstrated empirically but untested in extreme perturbation scenarios
- Mechanism 3 (non-Markovian reward handling): Low - minimal theoretical analysis and no ablation studies

## Next Checks
1. Conduct ablation study comparing MP3 with and without step-level exploration to verify that parameter space exploration alone suffices
2. Test replanning frequency limits by introducing rapid environmental changes to identify breaking points
3. Perform theoretical analysis of when trajectory segment aggregation can correctly handle non-Markovian rewards