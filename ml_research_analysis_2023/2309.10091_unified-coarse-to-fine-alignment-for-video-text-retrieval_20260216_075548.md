---
ver: rpa2
title: Unified Coarse-to-Fine Alignment for Video-Text Retrieval
arxiv_id: '2309.10091'
source_url: https://arxiv.org/abs/2309.10091
tags:
- video
- alignment
- similarity
- retrieval
- module
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UCoFiA, a unified coarse-to-fine alignment
  model for video-text retrieval. It captures cross-modal similarity at multiple granularities
  (video-sentence, frame-sentence, patch-word) and aggregates them using an Interactive
  Similarity Aggregation (ISA) module and a Sinkhorn-Knopp normalization.
---

# Unified Coarse-to-Fine Alignment for Video-Text Retrieval

## Quick Facts
- **arXiv ID**: 2309.10091
- **Source URL**: https://arxiv.org/abs/2309.10091
- **Reference count**: 40
- **Key outcome**: UCoFiA achieves 2.4%, 1.4%, and 1.3% improvements in text-to-video retrieval R@1 on MSR-VTT, Activity-Net, and DiDeMo, respectively, compared to state-of-the-art CLIP-based methods.

## Executive Summary
This paper introduces UCoFiA, a unified coarse-to-fine alignment model for video-text retrieval that captures cross-modal similarity at multiple granularities: video-sentence, frame-sentence, and patch-word. By aggregating these multi-granular alignments using an Interactive Similarity Aggregation (ISA) module and Sinkhorn-Knopp normalization, the approach addresses limitations of prior methods that only use coarse or fine-grained alignment. The model demonstrates superior performance on multiple benchmarks while providing insights into the importance of balanced similarity scoring across videos.

## Method Summary
UCoFiA extracts multi-granular features from videos (frame and patch levels) and text (word and sentence levels) using CLIP-based encoders. It then computes similarity matrices at three granularities: video-sentence, frame-sentence, and patch-word. These similarities are aggregated using ISA, which applies softmax operations and linear transformations to capture feature interactions, followed by Sinkhorn-Knopp normalization to balance marginal similarity scores across videos. The normalized scores are summed to produce final retrieval similarity, enabling both text-to-video and video-to-text retrieval.

## Key Results
- Achieves 2.4%, 1.4%, and 1.3% improvements in text-to-video retrieval R@1 on MSR-VTT, Activity-Net, and DiDeMo, respectively
- Outperforms previous state-of-the-art CLIP-based methods across all three benchmark datasets
- Demonstrates effectiveness of multi-granular alignment and balanced similarity scoring

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Coarse-to-fine alignment captures both high-level scene and low-level object correspondence, overcoming limitations of coarse-only or fine-only alignment
- **Core assumption**: Multi-granular features carry complementary information that improves retrieval if properly weighted and normalized
- **Evidence**: Empirical results show consistent improvements across three benchmarks when adding frame-sentence and patch-word alignments to video-sentence baseline

### Mechanism 2
- **Claim**: Interactive Similarity Aggregation (ISA) improves similarity scoring by jointly modeling cross-modal relevance and feature interactions
- **Core assumption**: Cross-modal relevance alone is insufficient; temporal or contextual interactions between visual features improve aggregation quality
- **Evidence**: ISA outperforms mean pooling and softmax-weighted aggregation baselines in ablation studies

### Mechanism 3
- **Claim**: Sinkhorn-Knopp normalization balances marginal similarity scores across videos, mitigating over/under-representation
- **Core assumption**: Unbalanced marginal similarities bias retrieval toward over-represented videos, hurting recall for others
- **Evidence**: Normalization improves performance by correcting imbalance where sum of retrieval similarities between one specific video and all texts might be much higher than that of other videos

## Foundational Learning

- **Concept**: Cross-modal contrastive learning
  - **Why needed**: Video-text retrieval fundamentally requires aligning visual and textual representations in shared space
  - **Quick check**: What loss function is used to maximize similarity for matching video-text pairs and minimize it for non-matches?

- **Concept**: Multi-granular feature extraction
  - **Why needed**: Different levels of visual abstraction (scene vs. object) are needed to match varied text query granularity
  - **Quick check**: Which three feature levels does UCoFiA align, and how are they extracted?

- **Concept**: Token selection via saliency scoring
  - **Why needed**: Full patch sets are redundant; selecting top-K salient patches reduces noise and computational cost
  - **Quick check**: How does the patch selection module decide which patches to keep?

## Architecture Onboarding

- **Component map**: Text encoder (CLIP-based) → word features + sentence feature → Multi-granular alignment module → ISA/Bi-ISA → Sinkhorn-Knopp normalization → Summation → Final retrieval similarity
- **Critical path**: Extract multi-granular features → compute similarity matrices → aggregate via ISA → normalize per granularity → sum → retrieval
- **Design tradeoffs**:
  - Granularity choice vs. computational cost: patch-level alignment is most detailed but expensive; video-level is cheapest but coarse
  - Aggregation method: mean pooling is simple but ignores feature interactions; ISA adds complexity but improves accuracy
  - Normalization: without Sinkhorn-Knopp, over-representation issues may persist; with it, risk of under-representation if train/test distribution shifts
- **Failure signatures**: Low R@1 with high R@5 indicates model struggles to identify top match but can rank relevant videos in top 5; performance drop when removing ISA suggests feature interactions are critical
- **First 3 experiments**:
  1. Run ablation: video-sentence only vs. adding frame-sentence vs. adding patch-word alignment
  2. Compare ISA vs. mean pooling vs. softmax-weighted aggregation for frame-sentence similarity
  3. Evaluate effect of Sinkhorn-Knopp normalization by toggling it on/off during inference

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several important areas for future work emerge from the analysis of the proposed method and its limitations.

## Limitations
- Performance relies heavily on dataset-specific normalization, raising concerns about generalization to out-of-distribution queries
- ISA effectiveness lacks comparison to other attention-based aggregation methods like self-attention or transformer-based fusion
- Normalization benefits are only validated on the same benchmark splits used for training

## Confidence
- **High**: Coarse-to-fine alignment framework and its superiority over single-granularity methods
- **Medium**: ISA's effectiveness compared to simpler baselines
- **Medium**: Sinkhorn-Knopp normalization benefits on MSR-VTT/ActivityNet/DiDeMo
- **Low**: Generalization of results to unseen datasets or domains

## Next Checks
1. Evaluate UCoFiA on out-of-domain datasets (e.g., HowTo100M or unseen video-text pairs) to test normalization robustness
2. Compare ISA against transformer-based cross-modal attention layers in ablation studies
3. Test the model with alternative normalization methods (e.g., learned attention weights vs. Sinkhorn-Knopp) to isolate contribution of each component