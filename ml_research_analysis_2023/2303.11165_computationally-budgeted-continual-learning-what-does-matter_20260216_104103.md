---
ver: rpa2
title: 'Computationally Budgeted Continual Learning: What Does Matter?'
arxiv_id: '2303.11165'
source_url: https://arxiv.org/abs/2303.11165
tags:
- learning
- time
- methods
- training
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the performance of continual learning (CL) methods
  under computational budget constraints. Existing CL methods typically assume unlimited
  computational resources, which is unrealistic for real-world applications.
---

# Computationally Budgeted Continual Learning: What Does Matter?

## Quick Facts
- arXiv ID: 2303.11165
- Source URL: https://arxiv.org/abs/2303.11165
- Authors: 
- Reference count: 40
- Primary result: Under computational budget constraints, simple uniform sampling from memory can outperform specialized continual learning methods due to their overhead costs.

## Executive Summary
This paper investigates continual learning methods under realistic computational budget constraints, revealing that most existing approaches fail to outperform simple uniform sampling baselines when computational resources are limited. Through systematic experiments on large-scale datasets (ImageNet2K and CGLM), the authors demonstrate that the overhead costs of sophisticated sampling strategies and distillation methods negate their theoretical advantages under budget constraints. The study establishes a new benchmark for evaluating CL methods in computationally restricted settings and provides critical insights for designing efficient continual learning systems for real-world applications.

## Method Summary
The paper evaluates continual learning methods under fixed computational budgets per time step using ResNet50 backbones pre-trained on ImageNet1K. The main baseline (Naive) uses uniform sampling from memory with cross-entropy loss, while various methods are tested including 7 sampling strategies (Uniform, Class-Balanced, Recency-Biased, FIFO, Herding, KMeans), 4 distillation losses (BCE, MSE, Cosine, CrossEntropy), and 5 FC corrections. Computational budgets are set at 400 iterations/step for ImageNet2K and 100 for CGLM, with distillation methods receiving 2/3 of naive's iterations and expensive sampling getting 1/2. Experiments run across three stream settings: data incremental, class incremental, and time incremental.

## Key Results
- Under computational budget constraints, uniform sampling from memory consistently outperforms specialized CL methods including KMeans, uncertainty-based selection, and distillation approaches
- The performance gap between existing CL methods and the simple baseline increases with harsher compute restrictions
- Distillation methods fail under budget constraints due to the computational overhead of teacher model forward passes
- FC layer correction methods don't provide significant advantage when computational resources are limited

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Under computational budget constraints, uniform sampling from memory can outperform specialized CL methods.
- Mechanism: When computational resources are limited, the overhead of complex sampling strategies reduces effective training iterations, negating their theoretical advantages.
- Core assumption: The overhead of complex sampling strategies is significant enough to reduce effective training iterations below what uniform sampling achieves.
- Evidence anchors:
  - [abstract] "under compute-constrained setting, traditional CL approaches, with no exception, fail to outperform a simple minimal baseline that samples uniformly from memory"
  - [section 4.2.1] "costly sampling strategies perform significantly worse in CL performance over the simple Uniform sampling when subjected to an effectively similar computational budget"

### Mechanism 2
- Claim: Distillation methods require sufficient computational budget to be effective in continual learning.
- Mechanism: Distillation methods need additional forward passes for teacher model computation, consuming budget that could otherwise be used for training.
- Core assumption: The computational overhead of distillation (forward passes for teacher model) is significant enough to impact training effectiveness under budget constraints.
- Evidence anchors:
  - [section 4.2.2] "distillation approaches are allowed 2C/3 iterations compared to Naive with C training iterations" and "all distillation methods clearly underperform when compared to Naive"

### Mechanism 3
- Claim: FC layer correction methods don't provide significant advantage under computational budget constraints.
- Mechanism: FC layer corrections either have negligible computational overhead or don't address the core issue of limited computational budget.
- Core assumption: The computational overhead of FC layer corrections is either negligible or doesn't provide sufficient benefit to justify their use under budget constraints.
- Evidence anchors:
  - [section 4.2.3] "Overall, there is no method that consistently outperforms Naive" and "all methods fail to outperform Naive under computationally budgeted continual learning"

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The paper is about continual learning, which specifically addresses the problem of catastrophic forgetting when models learn from sequential data
  - Quick check question: What happens to a neural network's performance on previously learned tasks when it's trained on new data without any special techniques?

- Concept: Experience replay in continual learning
  - Why needed here: The "Naive" baseline mentioned in the paper uses uniform sampling from memory, which is a form of experience replay
  - Quick check question: How does storing and replaying past examples help prevent catastrophic forgetting in continual learning?

- Concept: Computational complexity and its impact on learning
  - Why needed here: The paper's central thesis is that computational budget constraints significantly impact the effectiveness of continual learning methods
  - Quick check question: How does limiting the number of training iterations affect a model's ability to learn from both new and old data in a continual learning setting?

## Architecture Onboarding

- Component map: ResNet50 backbone -> Memory buffer (all previous samples) -> Learning components (sampling strategies, distillation losses, FC layer corrections) -> Evaluation metrics
- Critical path: 1) Receive new data stream 2) Select samples from memory based on chosen strategy 3) Train model on selected samples 4) Evaluate performance
- Design tradeoffs: Computational budget vs. sampling strategy complexity vs. model performance; simpler methods with more training iterations vs. complex methods with fewer iterations
- Failure signatures: When complex sampling strategies or distillation methods perform worse than simple uniform sampling under budget constraints; when FC layer corrections don't improve performance
- First 3 experiments:
  1. Implement the Naive baseline (uniform sampling with cross-entropy loss) and verify it outperforms complex methods under budget constraints
  2. Test different sampling strategies (Uniform, Class-Balanced, Recency-Biased, FIFO) with normalized computational budget to compare their effectiveness
  3. Evaluate distillation methods (BCE, MSE, Cosine, CrossEntropy) with adjusted iteration counts to account for teacher model forward passes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the fundamental reason why distillation-based continual learning methods underperform simple uniform sampling under budget constraints?
- Basis in paper: [explicit] The paper shows that distillation methods perform worse than Naive baseline under budgeted computation, even when computational budget is increased.
- Why unresolved: The paper doesn't provide a definitive explanation for this observation, only noting that distillation methods require larger computational budgets per time step.
- What evidence would resolve it: Systematic ablation studies varying model capacity, dataset size, and stream characteristics while maintaining budget constraints.

### Open Question 2
- Question: How does the performance gap between simple baselines and sophisticated CL methods scale with stream size and distribution complexity?
- Basis in paper: [inferred] The paper evaluates on large-scale datasets but doesn't systematically vary stream characteristics beyond computational budget.
- Why unresolved: While the paper tests different computational budgets and time steps, it doesn't explore how performance scales with stream complexity.
- What evidence would resolve it: Experiments varying stream size, class diversity, and distribution shift magnitude while keeping computational budget constant.

### Open Question 3
- Question: Can model expansion methods (partial training) be optimized to outperform full model training under strict budget constraints?
- Basis in paper: [explicit] The paper shows partial FC layer training closes the gap when using strong pretrained models, but doesn't explore optimal allocation strategies.
- Why unresolved: The paper only considers naive partial training (FC layer only) and doesn't explore optimal resource allocation between layers.
- What evidence would resolve it: Systematic comparison of different partial training strategies (layer-wise training, adaptive layer selection) under various budget constraints.

## Limitations

- Experiments focus on ResNet50 architectures, limiting generalizability to other model architectures
- Computational budget calculations assume fixed iteration counts without considering potential optimization strategies
- The study doesn't explore dynamic computational allocation strategies where budget varies across time steps

## Confidence

**High Confidence**: The experimental results showing uniform sampling outperforming specialized CL methods under budget constraints are well-supported by systematic ablation studies across multiple datasets and stream settings.

**Medium Confidence**: The interpretation that computational overhead is the primary mechanism explaining method failures under budget constraints, though alternative explanations are not fully explored.

**Low Confidence**: The generalizability of conclusions to different model architectures, larger-scale datasets, or real-world deployment scenarios with dynamic computational constraints.

## Next Checks

1. **Architecture Generalization**: Test whether the observed performance patterns hold for transformer-based architectures or models with different parameter counts, as computational overhead may scale differently across architectures.

2. **Dynamic Budget Scenarios**: Implement experiments where computational budget varies across time steps to evaluate whether adaptive methods could outperform static allocation strategies under realistic deployment conditions.

3. **Alternative Normalization**: Explore different computational normalization schemes (e.g., accounting for memory access costs, parallel processing capabilities) to determine if the observed performance gaps persist under alternative accounting methods.