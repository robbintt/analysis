---
ver: rpa2
title: Mitigating Temporal Misalignment by Discarding Outdated Facts
arxiv_id: '2305.14824'
source_url: https://arxiv.org/abs/2305.14824
tags:
- duration
- fact
- temporal
- misalignment
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces fact duration prediction, a method to identify
  facts that are prone to becoming outdated over time. By predicting how long a given
  fact will remain true, the approach helps mitigate temporal misalignment in question
  answering systems, where models trained on past data are evaluated on present-day
  questions.
---

# Mitigating Temporal Misalignment by Discarding Outdated Facts

## Quick Facts
- arXiv ID: 2305.14824
- Source URL: https://arxiv.org/abs/2305.14824
- Reference count: 21
- This paper introduces fact duration prediction to identify facts prone to becoming outdated, improving calibration in QA systems by 50-60%.

## Executive Summary
This paper addresses the problem of temporal misalignment in question answering systems, where models trained on past data are evaluated on current questions with potentially outdated answers. The authors propose fact duration prediction as a method to identify facts that are likely to become outdated over time. By predicting how long a given fact will remain true, the approach enables calibration by reducing confidence in predictions based on outdated facts. The method uses distant supervision from temporal knowledge bases and news text to train models that predict fact durations, which are then used to adjust confidence scores for QA predictions.

## Method Summary
The approach converts QA pairs and knowledge-base relations into statements with masked duration placeholders, then trains BERT-based models using either regression (predicting log-seconds) or classification (13-way duration classes). Fact duration predictions are used to discount QA confidence scores based on whether the predicted duration is shorter than the temporal misalignment between training and evaluation. The method is evaluated on open-retrieval QA using T5 and DPR models with different retrieval corpora, measuring calibration improvements through metrics like Expected Calibration Error and AUC-ROC.

## Key Results
- Fact duration prediction improves calibration by reducing confidence in predictions based on outdated facts
- Distant supervision from temporal knowledge bases and news text provides effective training signals
- Per-example confidence adjustment based on fact duration outperforms uniform adjustment across all examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Predicting fact duration enables calibration by identifying which answers are likely to be outdated.
- Mechanism: When a model trained on past data is evaluated on current questions, facts with predicted durations shorter than the time gap between training and evaluation are considered volatile and assigned lower confidence.
- Core assumption: The predicted duration distribution can be reliably used to discount confidence for predictions based on volatile facts.
- Evidence anchors:
  - [abstract]: "By predicting how long a given fact will remain true, the approach helps mitigate temporal misalignment in question answering systems"
  - [section 2.3]: "If m > d , we should lower the confidence of the model on this question"

### Mechanism 2
- Claim: Distant supervision from temporal knowledge bases and news text provides effective training signals for fact duration prediction.
- Mechanism: Large-scale datasets derived from Wikidata relations and news articles containing duration-related phrases allow models to learn patterns of how facts change over time.
- Core assumption: The temporal dynamics captured in knowledge bases and news text are representative of real-world fact changes relevant to QA tasks.
- Evidence anchors:
  - [section 3.2]: "TimeQA (Chen et al., 2021b) is one such dataset that curates 70 different temporally-dependent relations from Wikidata"
  - [section 3.2]: "Time-Aware Pretraining dataset (TA-Pretrain) (Yang et al., 2020) curates such texts from CNN and Daily Mail news articles using regular expressions to match for duration-specifying phrases"

### Mechanism 3
- Claim: Adjusting confidence per-example based on fact duration is more effective than uniform adjustment across all examples.
- Mechanism: Each prediction's confidence is scaled down according to the likelihood that its underlying fact has expired, allowing more nuanced calibration than applying the same discount to all predictions.
- Core assumption: The relationship between fact duration and calibration benefit varies across individual examples, not uniformly across the dataset.
- Evidence anchors:
  - [section 5.3]: "We find that our per-example adjustment methods outperform uniform confidence adjustments"
  - [section 5.3]: "cm = c(q, a)1 {d < m }" for regression-based system

## Foundational Learning

- Concept: Temporal misalignment in machine learning
  - Why needed here: The entire approach addresses the problem of models trained on past data being evaluated on present questions where answers may have changed.
  - Quick check question: What is the difference between a model's training date and the query date called in this paper?

- Concept: Calibration in classification systems
  - Why needed here: The method aims to improve calibration metrics like ECE and AUC-ROC by adjusting confidence scores based on fact duration predictions.
  - Quick check question: What calibration metric measures the average difference between confidence and accuracy across equally sized buckets?

- Concept: Distant supervision for learning without direct labels
  - Why needed here: Fact duration prediction labels are derived from temporal knowledge bases and news text rather than being explicitly annotated for this task.
  - Quick check question: Which two sources provide distant supervision for training fact duration prediction models in this work?

## Architecture Onboarding

- Component map:
  Fact duration prediction models (classification and regression variants) -> QA systems (T5, DPR with different retrieval corpora) -> Calibration models (trained on NQ-Open with XGBoost) -> Post-hoc confidence adjustment layer that applies duration-based discounts

- Critical path:
  1. Input question and evidence pass through QA system
  2. Fact duration prediction model generates duration estimate
  3. Confidence adjustment layer applies discount based on duration and misalignment
  4. Calibrated confidence and answer are returned

- Design tradeoffs:
  - Using distant supervision allows large-scale training but may introduce domain shift
  - Classification provides interpretable duration classes but limits granularity
  - Regression allows finer predictions but may be harder to train effectively

- Failure signatures:
  - Calibration metrics not improving despite fact duration predictions being available
  - Fact duration predictions consistently off by orders of magnitude
  - Confidence adjustments having minimal impact on overall accuracy

- First 3 experiments:
  1. Train classification and regression fact duration models on TimeQA and TA-Pretrain, evaluate on SituatedQA
  2. Apply fact duration-based confidence adjustment to T5 and DPR systems, measure calibration improvement
  3. Compare per-example adjustment against uniform confidence reduction baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of fact duration prediction vary across different temporal gaps (e.g., 1 year vs 10 years)?
- Basis in paper: [explicit] The paper evaluates temporal misalignment between 2018 and 2021, a three-year time difference, but notes the limitation of not exploring wider ranges of temporal gaps.
- Why unresolved: The paper only evaluates a specific three-year gap, leaving open the question of how the model performs with larger or smaller temporal misalignments.
- What evidence would resolve it: Conducting experiments with varied temporal gaps (e.g., 1 year, 5 years, 10 years) and analyzing the model's performance across these ranges would provide insights into its robustness and scalability.

### Open Question 2
- Question: Can fact duration prediction be effectively applied to non-English datasets and models?
- Basis in paper: [inferred] The paper notes that all datasets and models used are in English, suggesting a potential limitation in language generalizability.
- Why unresolved: The paper does not explore the application of fact duration prediction to other languages, leaving open the question of its effectiveness across linguistic boundaries.
- What evidence would resolve it: Extending the experiments to include multilingual datasets and models, and evaluating the performance of fact duration prediction in these contexts, would clarify its applicability to non-English scenarios.

### Open Question 3
- Question: How does the inclusion of answer start dates affect the accuracy of fact duration prediction and calibration?
- Basis in paper: [explicit] The paper acknowledges that it does not consider the answer's start date in its current approach, which could lead to errors in predicting fact durations.
- Why unresolved: The omission of start dates is noted as a limitation, and its impact on the accuracy of predictions and calibration is not explored.
- What evidence would resolve it: Modifying the model to incorporate answer start dates and comparing its performance with the current approach would reveal the impact of this factor on prediction accuracy and calibration.

### Open Question 4
- Question: How do different pretrained models (e.g., BERT, DeBERTa) affect the performance of fact duration prediction?
- Basis in paper: [explicit] The paper mentions experimenting with DeBERTa-v3-base and large variants of BERT and DeBERTa, but does not find substantial improvement, indicating variability in performance.
- Why unresolved: The paper does not provide a comprehensive comparison of different pretrained models, leaving open the question of which models are most effective for fact duration prediction.
- What evidence would resolve it: Conducting a systematic comparison of various pretrained models, including their hyperparameters and training configurations, would identify the optimal models for fact duration prediction.

### Open Question 5
- Question: How does fact duration prediction perform in scenarios with frequent, periodic changes in facts (e.g., sports statistics)?
- Basis in paper: [inferred] The paper discusses the challenge of predicting durations for facts that change at regular intervals, suggesting a potential area for improvement.
- Why unresolved: The paper does not explore scenarios with periodic changes, leaving open the question of how well the model handles such cases.
- What evidence would resolve it: Evaluating the model on datasets with regularly changing facts (e.g., sports statistics, election results) and analyzing its performance in these contexts would provide insights into its effectiveness for periodic changes.

## Limitations
- The study focuses on a relatively narrow temporal gap (tM=2018, tq=2021), limiting generalizability to longer-term temporal misalignment scenarios
- All experiments are conducted on English-language datasets only, with potential domain and language constraints
- The absolute accuracy of duration predictions remains challenging, and prediction errors could propagate to confidence adjustments

## Confidence
- High Confidence: The claim that fact duration prediction improves calibration metrics (ECE reduction of 50-60%) is well-supported by multiple evaluation metrics and ablation studies across different QA systems
- Medium Confidence: Claims about the effectiveness of per-example confidence adjustment versus uniform adjustment are supported but could benefit from additional datasets and temporal gaps
- Medium Confidence: The claim that distant supervision from knowledge bases and news text provides effective training signals is supported but the domain transfer limitations are not fully explored

## Next Checks
1. **Temporal Generalization Test**: Evaluate the approach on a dataset with a larger temporal gap (e.g., tM=2010, tq=2023) to assess performance degradation and identify breaking points for the calibration benefits

2. **Cross-Domain Transferability**: Test the fact duration prediction models on non-English datasets or domains with different update patterns (scientific knowledge, medical guidelines) to assess robustness across different temporal dynamics

3. **Error Analysis on Duration Prediction**: Conduct a detailed error analysis categorizing prediction errors by duration magnitude, fact type, and temporal context to identify systematic failure modes and inform model improvements