---
ver: rpa2
title: Improving the Transferability of Time Series Forecasting with Decomposition
  Adaptation
arxiv_id: '2307.00066'
source_url: https://arxiv.org/abs/2307.00066
tags:
- adaptation
- features
- time
- learning
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a Sequence Decomposition Adaptation Network
  (SeDAN) for transfer learning in time series forecasting. The key idea is to decompose
  the original feature sequences into seasonal and trend components, which are then
  adapted using different methods: Joint Maximum Mean Discrepancy (JMMD) for seasonal
  features and Optimal Local Adaptation (OLA) for trend features.'
---

# Improving the Transferability of Time Series Forecasting with Decomposition Adaptation

## Quick Facts
- arXiv ID: 2307.00066
- Source URL: https://arxiv.org/abs/2307.00066
- Reference count: 21
- Primary result: SeDAN achieves 27.0% MSE and 16.5% MAE reduction versus single-domain methods, 10.9% MSE and 5.3% MAE reduction versus cross-domain methods

## Executive Summary
This paper introduces Sequence Decomposition Adaptation Network (SeDAN), a transfer learning framework for time series forecasting that decomposes features into seasonal and trend components and adapts them separately. The key innovation is Implicit Contrastive Decomposition (ICD) that uses contrastive learning to generate seasonal and trend features without explicit decomposition rules. The method applies Joint Maximum Mean Discrepancy (JMMD) for seasonal feature adaptation and Optimal Local Adaptation (OLA) for trend feature adaptation, showing significant performance improvements across five benchmark datasets.

## Method Summary
SeDAN is a transfer learning framework for multivariate time series forecasting that addresses domain shift by decomposing features into seasonal and trend components. The model uses a Transformer encoder to extract features, which are then decomposed using ICD into seasonal and trend components. Seasonal features are adapted using JMMD to align joint distributions across domains, while trend features are adapted using OLA to match similar subsequences without forcing alignment of non-transferable conditional dynamics. The adapted features are reconstructed and decoded for forecasting.

## Key Results
- SeDAN achieves 27.0% (MSE) and 16.5% (MAE) reduction compared to single-domain methods
- SeDAN achieves 10.9% (MSE) and 5.3% (MAE) reduction compared to cross-domain methods
- Outperforms baselines on all five benchmark datasets (ETTh1, ETTh2, WTH, EXC, ILI)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing features into seasonal and trend components reduces the complexity of cross-domain distribution alignment
- Mechanism: Splitting entangled encoder features into low-coupling seasonal and trend components allows applying different adaptation strategies optimized for each type
- Core assumption: Seasonal component captures transferable marginal and conditional distributions, while trend component contains domain-specific conditional dynamics
- Evidence anchors: [abstract] decomposition makes features "easier to transfer"; [section 4.3] seasonal features reflect fixed periodicity while trend features relate to domain characteristics
- Break condition: If decomposition fails to produce low-coupling components or trend features contain transferable patterns

### Mechanism 2
- Claim: ICD leverages contrastive learning for self-supervised seasonal-trend separation
- Mechanism: ICD constructs positive samples through data augmentations preserving component properties, then uses infoNCE loss to guide generators to isolate those properties
- Core assumption: Augmentations preserving a component's property generate positive pairs that guide isolation
- Evidence anchors: [section 4.2] ICD uses instance-instance contrast; [section 4.2] subsequences reflect marginal and conditional distributions
- Break condition: If augmentations don't preserve intended component properties, contrastive signal misleads decomposition

### Mechanism 3
- Claim: OLA solves global optimal matching for trend subsequence alignment
- Mechanism: Trend subsequences are matched across domains by solving linear programming problem minimizing matching cost while ensuring one-to-one correspondence
- Core assumption: Subsequences with similar marginal distributions but different conditional dynamics can be matched based on local similarity
- Evidence anchors: [section 4.3.2] OLA handles dissimilar indices to retain local characteristics; [section 4.3.2] formulated as global optimal matching problem
- Break condition: If cost matrix or similarity metric fails to capture true local similarity

## Foundational Learning

- Concept: Time series decomposition (seasonal-trend decomposition)
  - Why needed here: Model relies on separating time series into interpretable components for domain-specific adaptation
  - Quick check question: What is the difference between additive and multiplicative decomposition models, and why does ICD avoid them?

- Concept: Contrastive learning and infoNCE loss
  - Why needed here: ICD uses contrastive learning to generate self-supervised signals for decomposition without explicit labels
  - Quick check question: How does infoNCE encourage representations to be invariant to data augmentations that preserve certain properties?

- Concept: Domain adaptation and distribution alignment
  - Why needed here: Core task is transferring knowledge across datasets with different marginal and conditional distributions
  - Quick check question: What is the difference between marginal and conditional distribution alignment in domain adaptation?

## Architecture Onboarding

- Component map: Encoder (Transformer) -> ICD (SDG + TDG) -> Seasonal/Trend features -> Adaptation (JMMD for seasonal, OLA for trend) -> Reconstructor -> Decoder (Transformer) -> Forecast

- Critical path: 1) Encode input time series to get features H; 2) Decompose H into Hsea and Htre using ICD; 3) Adapt Hsea with JMMD, Htre with OLA; 4) Reconstruct adapted features into single sequence; 5) Decode for forecasting

- Design tradeoffs:
  - ICD vs explicit decomposition: ICD is more flexible but requires careful augmentation design; explicit decomposition is interpretable but may not capture complex patterns
  - JMMD vs other alignment methods: JMMD handles joint distribution alignment but is computationally heavier; simpler metrics may be faster but less effective
  - OLA vs global alignment: OLA preserves local patterns but adds optimization overhead; global methods are simpler but risk negative transfer

- Failure signatures:
  - Poor decomposition: Seasonal and trend features remain entangled, leading to ineffective adaptation
  - Mismatch in adaptation: JMMD over-aligning trend features or OLA misaligning seasonal features can hurt performance
  - Reconstruction loss: High KL divergence indicates information loss during decomposition-reconstruction

- First 3 experiments:
  1. Train baseline Transformer without decomposition or adaptation; measure MSE/MAE
  2. Add ICD decomposition but no adaptation; compare reconstruction loss and forecasting performance
  3. Add seasonal adaptation (JMMD) only; evaluate impact on target domain forecasting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ICD compare to other decomposition techniques like STL or Empirical Mode Decomposition in terms of quality and efficiency?
- Basis in paper: [inferred] Paper mentions existing decomposition methods have explicit structures while ICD is implicit, but doesn't provide direct comparison
- Why unresolved: No comparison of ICD with other decomposition techniques included
- What evidence would resolve it: Direct comparison of ICD with other decomposition techniques in terms of decomposition quality and computational efficiency

### Open Question 2
- Question: How does OLA perform in matching subsequences across domains compared to other adaptation methods like JMMD?
- Basis in paper: [explicit] Paper proposes OLA for trend features but doesn't provide detailed performance analysis vs other methods
- Why unresolved: No comparison of OLA with other adaptation methods included
- What evidence would resolve it: Detailed analysis of OLA's performance in matching subsequences across domains, along with comparison to other adaptation methods

### Open Question 3
- Question: How does SeDAN perform on other types of time series data like non-seasonal or non-trend data?
- Basis in paper: [inferred] Paper focuses on seasonal and trend features but doesn't explore performance on other time series types
- Why unresolved: No experiments on other types of time series data included
- What evidence would resolve it: Experiments on other types of time series data such as non-seasonal or non-trend data

## Limitations

- ICD implementation details are not fully specified, particularly data augmentation methods and parameters
- OLA's effectiveness depends on matching quality, but ablation studies on matching cost matrix sensitivity are missing
- No comparison against state-of-the-art non-transfer time series forecasting models to assess whether transfer learning is necessary

## Confidence

- High confidence: Overall experimental results showing SeDAN outperforms baselines on all five datasets
- Medium confidence: Claim that decomposition into seasonal and trend components enables more effective transfer learning
- Low confidence: Specific mechanisms of ICD and OLA due to incomplete implementation details

## Next Checks

1. Implement ablation studies removing ICD decomposition to quantify its contribution versus just using a Transformer with domain adaptation
2. Test SeDAN on non-seasonal time series datasets to verify decomposition-adaptation strategy doesn't harm performance when seasonal patterns are absent
3. Compare against state-of-the-art non-transfer time series forecasting models (like N-Beats, Informer) to establish whether transfer learning is necessary for these datasets