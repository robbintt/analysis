---
ver: rpa2
title: A ChatGPT Aided Explainable Framework for Zero-Shot Medical Image Diagnosis
arxiv_id: '2307.01981'
source_url: https://arxiv.org/abs/2307.01981
tags:
- medical
- image
- clip
- chatgpt
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel zero-shot medical image classification
  framework that combines CLIP and ChatGPT to provide explainable diagnosis results.
  The method addresses the challenge of limited access to annotated medical data by
  leveraging large language models (LLMs) to generate detailed symptom descriptions
  for each diagnostic category.
---

# A ChatGPT Aided Explainable Framework for Zero-Shot Medical Image Diagnosis

## Quick Facts
- arXiv ID: 2307.01981
- Source URL: https://arxiv.org/abs/2307.01981
- Authors: 
- Reference count: 16
- Key outcome: Proposed framework achieves up to 17.37% accuracy improvement over standard CLIP on Shenzhen dataset for zero-shot medical image classification

## Executive Summary
This paper introduces a novel zero-shot medical image classification framework that combines CLIP and ChatGPT to provide explainable diagnosis results. The method addresses the challenge of limited access to annotated medical data by leveraging large language models (LLMs) to generate detailed symptom descriptions for each diagnostic category. These descriptions, combined with CLIP's visual encoder, enable more accurate and interpretable diagnosis by computing similarity scores between images and textual symptom descriptions. The proposed framework achieves significant improvements over standard CLIP-based approaches across five medical datasets.

## Method Summary
The framework uses CLIP's visual encoder to extract image features and ChatGPT to generate symptom descriptions for each diagnostic category through carefully designed prompts. The symptom descriptions are then encoded using CLIP's text encoder, and cosine similarity scores are computed between image features and text features. Mean aggregation is applied across multiple symptom descriptions to obtain the final similarity score for each class. The method is training-free and leverages existing pre-trained models, making it data-efficient and scalable across different medical imaging domains.

## Key Results
- Achieved 17.37% accuracy improvement over standard CLIP on Shenzhen dataset
- Significant improvements across all five tested medical datasets (Pneumonia, Montgomery, Shenzhen, IDRID, BrainTumor)
- Mean aggregation outperforms max aggregation strategy in similarity scoring
- Demonstrated effectiveness of symptom descriptions in improving classification accuracy

## Why This Works (Mechanism)

### Mechanism 1
Using ChatGPT to generate symptom descriptions enhances CLIP's ability to focus on relevant medical features in zero-shot classification. ChatGPT produces detailed symptom phrases that act as supplementary text inputs alongside diagnostic category names. These symptom descriptions provide CLIP with richer, more specific visual cues that align with medical features, improving attention and classification accuracy.

### Mechanism 2
The designed prompt structure directs ChatGPT to produce medically relevant symptom descriptions rather than general or noisy outputs. The prompt explicitly instructs ChatGPT to focus on "useful medical visual features" and "published literature," resulting in symptom phrases that are more specific to the visual patterns distinguishing each diagnostic category.

### Mechanism 3
Using mean aggregation across symptom representations provides balanced similarity scoring that works well across classes with different numbers of symptoms. Instead of taking the maximum similarity score from any single symptom, averaging across all symptom similarities ensures fair evaluation regardless of how many symptoms ChatGPT generates for each class.

## Foundational Learning

- **Vision-Language Pre-training (VLP) and Contrastive Learning**
  - Why needed here: CLIP's architecture relies on learning joint representations from paired images and text through contrastive loss, which is fundamental to understanding how it performs zero-shot classification
  - Quick check question: How does CLIP learn to align visual and textual representations without task-specific training?

- **Large Language Model (LLM) Prompt Engineering**
  - Why needed here: The effectiveness of ChatGPT-generated symptom descriptions depends heavily on how the prompts are structured to elicit relevant medical information
  - Quick check question: What prompt engineering techniques can be used to ensure LLM outputs are specific and accurate for medical applications?

- **Attention Mechanisms in Vision-Language Models**
  - Why needed here: Understanding how attention maps reveal which parts of an image CLIP focuses on helps explain why symptom descriptions improve classification
  - Quick check question: How do attention maps change when CLIP processes images with only category names versus with detailed symptom descriptions?

## Architecture Onboarding

- **Component map**: Input: Medical image → Visual Encoder: CLIP visual encoder → f → Similarity Calculator ← Symptom Texts ← ChatGPT ← Prompt ← Diagnostic Category
- **Critical path**: Image → Visual Encoder → f → Similarity Calculator ← Symptom Texts ← ChatGPT ← Prompt ← Diagnostic Category
- **Design tradeoffs**: ChatGPT generation quality vs. computational cost; Prompt specificity vs. generality across medical domains; Number of symptoms generated vs. information overload; Different CLIP model sizes vs. performance requirements
- **Failure signatures**: All images classified as the same category (suggests similarity computation issues); Random or inconsistent classifications (suggests symptom generation problems); Poor attention map focus (suggests visual-text alignment issues); System hangs or errors (suggests API rate limits or token limits)
- **First 3 experiments**: 1) Baseline test: Run standard CLIP zero-shot classification with only category names on a small dataset to establish baseline accuracy; 2) Prompt effectiveness test: Compare symptom generation quality and classification accuracy using the designed prompt vs. a simple baseline prompt; 3) Aggregation strategy test: Compare mean vs. max aggregation on similarity scores to verify which performs better on a validation set

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the methodology and results, several important questions remain unresolved regarding the framework's generalizability, performance in complex medical scenarios, and behavior with limited or imbalanced data.

## Limitations
- ChatGPT generation reliability is critical but not systematically evaluated for hallucination rate or medical accuracy
- Generalizability across medical domains is assumed but not rigorously tested beyond the five specific datasets
- Limited evaluation metrics focusing primarily on accuracy without clinical utility measures like false positive/negative rates

## Confidence
- **High Confidence**: CLIP's architecture can be adapted for zero-shot medical image classification; The proposed framework achieves higher accuracy than standard CLIP zero-shot classification on the tested datasets; Mean aggregation of symptom similarities performs better than max aggregation in the tested scenarios
- **Medium Confidence**: ChatGPT-generated symptom descriptions meaningfully improve classification accuracy; The designed prompt effectively constrains ChatGPT to produce medically relevant outputs; The framework provides explainable diagnosis results through attention maps and symptom descriptions
- **Low Confidence**: The framework's performance will generalize to medical specialties not tested in the paper; The proposed approach is ready for clinical deployment without additional validation; ChatGPT's symptom generation quality is consistent and reliable across all medical conditions

## Next Checks
1. **Cross-Dataset Generalization Test**: Evaluate the framework's performance on a held-out medical dataset from a different domain (e.g., dermatology or histopathology) using the same prompts and symptom generation approach to assess true generalizability.

2. **Error Analysis and Clinical Validation**: Conduct a detailed error analysis categorizing false positives and false negatives, then have medical experts review the attention maps and symptom descriptions to assess clinical plausibility and potential for diagnostic support.

3. **Prompt Robustness Study**: Systematically vary the prompt structure, length, and specificity to quantify how sensitive the framework's performance is to prompt engineering, including testing with simpler prompts and more complex medical literature-based prompts.