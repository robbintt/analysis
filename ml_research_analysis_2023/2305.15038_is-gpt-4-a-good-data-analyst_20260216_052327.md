---
ver: rpa2
title: Is GPT-4 a Good Data Analyst?
arxiv_id: '2305.15038'
source_url: https://arxiv.org/abs/2305.15038
tags:
- data
- gpt-4
- analyst
- chart
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether GPT-4 can replace human data analysts
  by evaluating its performance on end-to-end data analysis tasks. The authors propose
  a framework that uses GPT-4 to extract data, generate visualizations, and produce
  insights from business questions and database schemas.
---

# Is GPT-4 a Good Data Analyst?

## Quick Facts
- arXiv ID: 2305.15038
- Source URL: https://arxiv.org/abs/2305.15038
- Authors: 
- Reference count: 11
- Key outcome: GPT-4 performs comparably to junior and senior data analysts on end-to-end data analysis tasks, though with some errors in data extraction and calculations.

## Executive Summary
This paper investigates whether GPT-4 can replace human data analysts by evaluating its performance on end-to-end data analysis tasks. The authors propose a framework that uses GPT-4 to extract data, generate visualizations, and produce insights from business questions and database schemas. Human evaluators rate the outputs on correctness, aesthetics, alignment, complexity, and fluency. Results show GPT-4 performs comparably to junior and senior data analysts on most metrics, though with some errors in data extraction and calculations. GPT-4 is significantly faster and cheaper than human analysts. However, the study uses specific, structured questions rather than real-world business queries, and the sample size for human evaluation is limited. The authors conclude GPT-4 shows promise as a data analyst but further work is needed to address accuracy, generalization, and practical business use cases before it can fully replace human analysts.

## Method Summary
The study evaluates GPT-4's data analysis capabilities by prompting it to generate SQL queries and Python code for data extraction and visualization, then generate insights in bullet points. The framework uses 100 business questions from the NvBench dataset across various domains, with corresponding database schemas and raw data. Human evaluators rate GPT-4's outputs (charts and analysis) using metrics for correctness, aesthetics, alignment, complexity, and fluency, and compare results to human data analysts' performance on the same tasks.

## Key Results
- GPT-4 achieves comparable performance to human data analysts on correctness, complexity, and fluency metrics
- GPT-4 is significantly faster and cheaper than human analysts
- GPT-4 shows limitations in data extraction accuracy and complex calculations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 can generate executable code that connects to databases, executes SQL queries, and produces visualizations.
- Mechanism: GPT-4's strong code generation ability allows it to understand schema descriptions, formulate appropriate SQL queries, and write Python code that utilizes relevant libraries (e.g., sqlite3, matplotlib) to extract data and create visualizations.
- Core assumption: GPT-4 has sufficient knowledge of database operations, SQL syntax, and visualization libraries to generate working code from natural language prompts.
- Evidence anchors:
  - [abstract] "We propose a framework to tackle the problems by carefully designing the prompts for GPT-4 to conduct experiments."
  - [section] "We utilize GPT-4 to understand the questions and the relations among multiple database tables from the schema."
  - [corpus] Weak evidence - only general mentions of GPT-4's code generation abilities, no specific examples of database-related code generation.
- Break condition: GPT-4 fails to generate syntactically correct SQL or Python code, or the generated code does not produce the desired data extraction or visualization.

### Mechanism 2
- Claim: GPT-4 can analyze extracted data and generate insights in the form of bullet points.
- Mechanism: GPT-4's natural language understanding and generation capabilities allow it to interpret data patterns, identify trends, and articulate insights in a clear, concise manner using bullet points.
- Core assumption: GPT-4 can accurately interpret the data provided and generate insights that align with the original question and data analysis requirements.
- Evidence anchors:
  - [abstract] "Experimental results show that GPT-4 can achieve comparable performance to humans."
  - [section] "Our designed prompt for GPT-4 of this step is shown in Table 2... Instead of generating a paragraph of description about the extracted data, we instruct GPT-4 to generate the data analysis and insights in 5 bullet points to emphasize the key takeaways."
  - [corpus] Weak evidence - only general mentions of GPT-4's language generation abilities, no specific examples of data analysis or insight generation.
- Break condition: GPT-4 generates insights that are not aligned with the original question, contain incorrect data interpretations, or fail to provide meaningful analysis.

### Mechanism 3
- Claim: GPT-4's performance is comparable to junior and senior data analysts in terms of correctness, complexity, and fluency of insights.
- Mechanism: The human evaluation metrics designed in the study assess the quality of GPT-4's outputs (figures and analysis) and compare them to those of professional data analysts across various dimensions.
- Core assumption: The evaluation metrics are comprehensive and effectively capture the key aspects of data analysis quality, and the human evaluators are capable of providing accurate assessments.
- Evidence anchors:
  - [abstract] "Experimental results show that GPT-4 can achieve comparable performance to humans."
  - [section] "To conduct human evaluation, 6 professional data annotators are hired from a data annotation company to annotate each figure and analysis bullet point following the evaluation metrics detailed above."
  - [corpus] Weak evidence - only general mentions of the evaluation process, no specific details on the metrics or evaluator qualifications.
- Break condition: The evaluation metrics fail to capture important aspects of data analysis quality, or the human evaluators are not qualified to provide accurate assessments.

## Foundational Learning

- Concept: Database schema understanding
  - Why needed here: GPT-4 needs to interpret the database schema to generate appropriate SQL queries and understand the relationships between tables.
  - Quick check question: Can GPT-4 accurately identify the relevant tables and columns needed to answer a given business question based on the provided schema?

- Concept: SQL query generation
  - Why needed here: GPT-4 must generate valid SQL queries to extract the required data from the database based on the business question.
  - Quick check question: Can GPT-4 write SQL queries that correctly filter, join, and aggregate data to answer complex business questions?

- Concept: Data visualization principles
  - Why needed here: GPT-4 needs to understand which chart types are appropriate for different data scenarios and generate visualizations that effectively communicate insights.
  - Quick check question: Can GPT-4 choose the most suitable chart type (e.g., bar chart, scatter plot) for a given dataset and business question?

## Architecture Onboarding

- Component map:
  Question → GPT-4 code generation → Code execution → Data extraction → GPT-4 analysis generation → Human evaluation

- Critical path:
  Question → GPT-4 code generation → Code execution → Data extraction → GPT-4 analysis generation → Human evaluation

- Design tradeoffs:
  - Using GPT-4 for both code generation and data analysis reduces the need for multiple specialized models but may introduce errors or inconsistencies.
  - Relying on human evaluators for quality assessment ensures accuracy but adds time and cost to the process.

- Failure signatures:
  - Incorrect SQL queries or Python code generation
  - Inaccurate data extraction or visualization
  - Insights that do not align with the original question or contain incorrect interpretations
  - Human evaluation inconsistencies or biases

- First 3 experiments:
  1. Test GPT-4's ability to generate SQL queries and Python code for a simple business question with a single table.
  2. Evaluate GPT-4's data analysis and insight generation capabilities on a dataset with clear trends and patterns.
  3. Compare GPT-4's outputs (code, visualizations, insights) to those of a junior data analyst on a moderately complex business question.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can GPT-4 accurately extract data from databases and perform calculations required for data analysis tasks?
- Basis in paper: [explicit] The paper mentions that GPT-4 sometimes makes errors in data extraction and calculations, particularly for complex calculations.
- Why unresolved: While GPT-4 demonstrates strong abilities in context understanding, code generation, and data storytelling, its calculation capabilities, especially for complex calculations, are not yet fully reliable. This limitation may hinder its ability to perform accurate data analysis.
- What evidence would resolve it: Further experiments and evaluations focused on GPT-4's ability to handle complex calculations and extract accurate data from databases would help determine if this limitation can be overcome.

### Open Question 2
- Question: How well can GPT-4 generalize its data analysis capabilities to real-world business queries and diverse domains?
- Basis in paper: [inferred] The paper notes that the questions used in the study are specific and structured, and the sample size for human evaluation is limited. This suggests that GPT-4's performance on more general, real-world business queries remains unclear.
- Why unresolved: The study's use of specific, structured questions and limited sample size may not fully capture GPT-4's ability to handle the diverse and complex nature of real-world business queries across various domains.
- What evidence would resolve it: Conducting experiments with a larger and more diverse set of real-world business queries across various domains would provide insights into GPT-4's ability to generalize its data analysis capabilities.

### Open Question 3
- Question: Can GPT-4's data analysis outputs be trusted for making critical business decisions?
- Basis in paper: [inferred] The paper highlights GPT-4's potential for hallucination and calculation errors, which are significant concerns for a data analyst role that requires high accuracy.
- Why unresolved: The accuracy and reliability of GPT-4's data analysis outputs are crucial for its potential adoption in real-world business settings. However, the paper identifies issues with hallucination and calculation errors that may impact the trustworthiness of its outputs.
- What evidence would resolve it: Implementing rigorous validation and verification processes for GPT-4's data analysis outputs, along with human oversight, could help determine if its outputs can be trusted for critical business decisions.

## Limitations

- The study uses specific, structured questions rather than real-world business queries, which may not capture the full complexity and ambiguity of actual business problems.
- The human evaluation sample size is limited (6 annotators), which may introduce subjectivity and bias in the assessment of GPT-4's performance.
- GPT-4 shows limitations in data extraction accuracy and complex calculations, which could impact its reliability for critical business decisions.

## Confidence

**High Confidence**: The claim that GPT-4 can generate executable code for data extraction and visualization is supported by strong evidence of GPT-4's code generation capabilities. The framework successfully demonstrates this ability in controlled experiments.

**Medium Confidence**: The claim that GPT-4's performance is comparable to junior and senior data analysts is supported by human evaluation, but the limited sample size and potential evaluator bias reduce confidence in the generalizability of these findings.

**Low Confidence**: The claim that GPT-4 can fully replace human data analysts is not well-supported, as the study uses simplified business questions and acknowledges the need for further work on accuracy, generalization, and practical business use cases.

## Next Checks

1. **Generalization Test**: Apply the GPT-4 framework to a more diverse set of real-world business queries from different industries, including those with ambiguous requirements and incomplete data schemas, to assess robustness and generalization.

2. **Error Analysis**: Conduct a detailed error analysis of GPT-4's outputs, categorizing types of errors (e.g., SQL syntax errors, incorrect calculations, hallucination in insights) and quantifying their frequency and impact on overall performance.

3. **Longitudinal Study**: Implement a longitudinal study comparing GPT-4's performance to human analysts over time, tracking accuracy improvements, cost savings, and the evolution of use cases as GPT-4 and related models continue to develop.