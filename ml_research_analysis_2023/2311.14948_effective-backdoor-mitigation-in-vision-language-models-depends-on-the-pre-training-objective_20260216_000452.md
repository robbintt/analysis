---
ver: rpa2
title: Effective Backdoor Mitigation in Vision-Language Models Depends on the Pre-training
  Objective
arxiv_id: '2311.14948'
source_url: https://arxiv.org/abs/2311.14948
tags:
- accuracy
- mmcl
- pre-trained
- finetuning
- imagenet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of CleanCLIP, a state-of-the-art
  technique for removing backdoors from vision-language models, and reveals a critical
  dependency on the pre-training objective used. The authors demonstrate that while
  CleanCLIP successfully mitigates backdoor attacks in models trained solely with
  multimodal contrastive loss (MMCL), it fails to do so when stronger pre-training
  objectives combining MMCL with self-supervised learning (SSL) are used.
---

# Effective Backdoor Mitigation in Vision-Language Models Depends on the Pre-training Objective

## Quick Facts
- arXiv ID: 2311.14948
- Source URL: https://arxiv.org/abs/2311.14948
- Reference count: 40
- Primary result: CleanCLIP's effectiveness in removing backdoors from vision-language models critically depends on the pre-training objective used.

## Executive Summary
This paper investigates the effectiveness of CleanCLIP, a state-of-the-art technique for removing backdoors from vision-language models, and reveals a critical dependency on the pre-training objective used. The authors demonstrate that while CleanCLIP successfully mitigates backdoor attacks in models trained solely with multimodal contrastive loss (MMCL), it fails to do so when stronger pre-training objectives combining MMCL with self-supervised learning (SSL) are used. Through extensive experiments on two large datasets (CC3M and CC6M), they show that models pre-trained with MMCL + SSL achieve higher zero-shot classification accuracy but are significantly harder to clean, often requiring a substantial drop in accuracy to reduce the attack success rate (ASR) to acceptable levels. The study highlights the practical challenges faced by ML practitioners, including unstable cleaning trajectories and the need for a completely poison-free cleaning dataset, which are exacerbated when using stronger pre-training objectives. The authors recommend using the simpler MMCL objective for pre-training when backdoor mitigation is a priority, despite potential minor reductions in accuracy.

## Method Summary
The study involves pre-training multimodal models using either MMCL or a combination of MMCL and SSL on poisoned datasets. CleanCLIP is then applied to remove backdoors by finetuning on a clean dataset using a combination of MMCL and SSL losses. The effectiveness of CleanCLIP is evaluated by measuring the zero-shot accuracy on ImageNet and the attack success rate (ASR) after cleaning.

## Key Results
- CleanCLIP successfully removes backdoors from models pre-trained with MMCL alone while maintaining high accuracy.
- Models pre-trained with MMCL + SSL achieve higher zero-shot classification accuracy but are significantly harder to clean, often requiring a substantial drop in accuracy to reduce ASR.
- CleanCLIP's effectiveness is severely compromised even with slight relaxation of the clean finetuning data assumption, especially for models pre-trained with stronger objectives.
- Models pre-trained with MMCL + SSL exhibit unstable cleaning trajectories, making it difficult for practitioners to determine the optimal stopping point for the cleaning process.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CleanCLIP's effectiveness depends on the pre-training objective used.
- Mechanism: CleanCLIP removes backdoors by finetuning on a clean dataset using a combination of multimodal contrastive loss and self-supervised loss. When the pre-training objective is multimodal contrastive loss (MMCL) alone, the model's learned representations are more amenable to this cleaning process. However, when the pre-training objective combines MMCL with self-supervised learning (SSL), the model develops stronger, more robust associations between the trigger patch and the target label, making it significantly harder to remove these associations through finetuning.
- Core assumption: The strength of the backdoor's embedding in the model's representation space is directly correlated with the complexity and strength of the pre-training objective.
- Evidence anchors:
  - [abstract] "We observe that stronger pre-training objectives that lead to higher zero-shot classification performance correlate with harder to remove backdoors behaviors."
  - [section 3] "CleanCLIP is ineffective in poison removal when stronger pre-training objectives are used."
  - [corpus] Found 25 related papers, but specific evidence about the mechanism linking pre-training objective strength to backdoor removal difficulty is not explicitly stated in the neighbor papers.
- Break condition: If the model's representation space becomes too complex or the trigger's association with the target label becomes too deeply embedded, even extensive hyperparameter tuning may not be able to remove the backdoor without significant accuracy loss.

### Mechanism 2
- Claim: Models pre-trained with MMCL + SSL have unstable cleaning trajectories.
- Mechanism: During the finetuning process, models pre-trained with the stronger MMCL + SSL objective exhibit non-monotonic behavior in their accuracy and attack success rate (ASR). This instability makes it difficult for practitioners to determine the optimal stopping point for the cleaning process, as continued finetuning can lead to both decreased accuracy and increased ASR.
- Core assumption: The instability in cleaning trajectories is a direct result of the stronger pre-training objective creating more complex and robust representations that are harder to disentangle during the cleaning process.
- Evidence anchors:
  - [section 3] "With continued finetuning, the model can both lose accuracy and see a gain in ASR, both of which are undesirable."
  - [section 3] "Therefore, even if a cleaned model with decent accuracy and low ASR might exist for these models, for a practitioner, it is difficult to discern when to stop finetuning."
  - [corpus] Neighbor papers do not provide specific evidence about the instability of cleaning trajectories in models pre-trained with stronger objectives.
- Break condition: If the practitioner can somehow determine a stable cleaning trajectory or develop a method to predict the optimal stopping point, this mechanism might be mitigated.

### Mechanism 3
- Claim: CleanCLIP's effectiveness is severely compromised even with slight relaxation of the clean finetuning data assumption.
- Mechanism: CleanCLIP assumes access to a completely poison-free finetuning dataset. However, in practice, even a small number of poisoned datapoints (5 out of 100K) in the finetuning dataset can render CleanCLIP ineffective, especially for models pre-trained with the stronger MMCL + SSL objective. This is because the cleaning process is sensitive to any residual poison in the finetuning data, and the stronger objective makes the model more susceptible to these influences.
- Core assumption: The cleaning process is highly sensitive to any residual poison in the finetuning dataset, and this sensitivity is exacerbated by the use of stronger pre-training objectives.
- Evidence anchors:
  - [section 3] "Figure 3 shows the scatter plot of the Top-1 zero-shot Imagenet validation set accuracy and the ASR at the end of each finetuning epoch for the models pre-trained on the CC6M dataset. We observe that having just 5 poisoned datapoints in the cleaning dataset severely weakens CleanCLIP for both the pre-training objectives."
  - [section 3] "However for models pre-trained with Lpre MMCL alone, we found cleaned models that maintain the original model's accuracy and achieve around 30-50% ASR."
  - [corpus] Neighbor papers do not provide specific evidence about the impact of slight relaxation of the clean finetuning data assumption on CleanCLIP's effectiveness.
- Break condition: If a method can be developed to make the cleaning process more robust to small amounts of poison in the finetuning dataset, this mechanism might be mitigated.

## Foundational Learning

- Concept: Multimodal contrastive learning (MMCL)
  - Why needed here: Understanding MMCL is crucial because it forms the basis of the pre-training objective that CleanCLIP is designed to work with. The paper shows that CleanCLIP is effective for models pre-trained with MMCL but not for those with stronger objectives.
  - Quick check question: What is the primary goal of multimodal contrastive learning, and how does it differ from unimodal contrastive learning?

- Concept: Self-supervised learning (SSL)
  - Why needed here: SSL is a component of the stronger pre-training objective (MMCL + SSL) that makes backdoor removal more difficult. Understanding how SSL works and its impact on model representations is key to grasping why CleanCLIP fails in these cases.
  - Quick check question: How does the addition of self-supervised loss within image and text modalities affect the model's ability to learn robust representations?

- Concept: Backdoor attacks and their mitigation
  - Why needed here: The entire study revolves around the effectiveness of backdoor mitigation techniques, specifically CleanCLIP. A solid understanding of how backdoor attacks work and why they're challenging to remove is essential for comprehending the paper's findings.
  - Quick check question: What are the key challenges in removing backdoors from machine learning models, and how does CleanCLIP attempt to address these challenges?

## Architecture Onboarding

- Component map:
  - Pre-training: Two objectives - MMCL (simpler) and MMCL + SSL (stronger)
  - Cleaning: CleanCLIP approach using finetuning with MMCL + SSL loss
  - Evaluation: Zero-shot accuracy on ImageNet and Attack Success Rate (ASR)
  - Datasets: CC3M (3M image-text pairs) and CC6M (6M image-text pairs)
  - Poisoning: Trigger patch (16x16) added to 0.05% of training data

- Critical path:
  1. Pre-train models using either MMCL or MMCL + SSL on poisoned datasets
  2. Apply CleanCLIP to remove backdoors
  3. Evaluate cleaned models on ImageNet accuracy and ASR
  4. Analyze the effectiveness of CleanCLIP based on pre-training objective

- Design tradeoffs:
  - Stronger pre-training objectives (MMCL + SSL) lead to better zero-shot accuracy but make backdoor removal more difficult
  - CleanCLIP is effective for MMCL-pretrained models but fails for MMCL + SSL-pretrained models
  - Relaxing the assumption of completely clean finetuning data significantly impacts CleanCLIP's effectiveness, especially for stronger pre-training objectives

- Failure signatures:
  - For MMCL-pretrained models: CleanCLIP successfully removes backdoors while maintaining accuracy
  - For MMCL + SSL-pretrained models: CleanCLIP fails to remove backdoors without significant accuracy loss
  - Non-monotonic cleaning trajectories for MMCL + SSL-pretrained models
  - Severe degradation in cleaning effectiveness with even small amounts of poison in finetuning data for MMCL + SSL-pretrained models

- First 3 experiments:
  1. Pre-train models using MMCL and MMCL + SSL on CC3M and CC6M datasets, then apply CleanCLIP to evaluate its effectiveness on both types of models.
  2. Investigate the impact of non-ideal conditions (small amounts of poison in finetuning data) on CleanCLIP's effectiveness for both pre-training objectives.
  3. Explore alternative cleaning methods (e.g., different finetuning objectives, heavy regularization, shrink and perturb) to determine if they can successfully remove backdoors from MMCL + SSL-pretrained models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the failure of CleanCLIP on models pre-trained with stronger objectives (MMCL + SSL) be overcome by modifying the cleaning algorithm itself, rather than just the pre-training objective?
- Basis in paper: [inferred] The authors show that CleanCLIP fails on models pre-trained with stronger objectives, even with extensive hyperparameter tuning and various modifications like different objectives, regularization, and perturbation methods. However, they don't explore potential modifications to the CleanCLIP algorithm itself.
- Why unresolved: The paper focuses on the dependency of CleanCLIP's effectiveness on the pre-training objective, but doesn't investigate whether the algorithm could be adapted to work better with stronger pre-training objectives.
- What evidence would resolve it: Experiments testing modified versions of CleanCLIP (e.g., with different loss functions, regularization terms, or cleaning strategies) on models pre-trained with stronger objectives, comparing their performance to the original CleanCLIP.

### Open Question 2
- Question: Is there a threshold or specific combination of pre-training objectives beyond which CleanCLIP becomes ineffective, or is it a gradual decline in performance?
- Basis in paper: [explicit] The authors demonstrate that CleanCLIP fails on models pre-trained with MMCL + SSL, but they don't explore intermediate combinations of objectives or quantify the point at which CleanCLIP becomes ineffective.
- Why unresolved: The study only compares two extremes (MMCL vs. MMCL + SSL) and doesn't provide a detailed analysis of how CleanCLIP's performance changes with different combinations or strengths of pre-training objectives.
- What evidence would resolve it: A systematic study testing CleanCLIP on models pre-trained with various combinations and strengths of objectives (e.g., different weights for MMCL and SSL, or additional objectives), mapping out the relationship between pre-training strength and CleanCLIP's effectiveness.

### Open Question 3
- Question: Are there specific characteristics of the poisoned data or the backdoor trigger that make it more difficult to remove when models are pre-trained with stronger objectives?
- Basis in paper: [inferred] The authors observe that models pre-trained with stronger objectives are harder to clean, but they don't investigate whether this is due to the nature of the poison itself or the model's representations.
- Why unresolved: The study focuses on the pre-training objective as the main factor, but doesn't explore whether the poison's characteristics (e.g., trigger size, location, or pattern) interact differently with models trained on stronger objectives.
- What evidence would resolve it: Experiments varying the poison characteristics (e.g., trigger size, location, pattern) and testing CleanCLIP's effectiveness on models pre-trained with different objectives, to identify if certain types of poison are more resistant to cleaning in stronger models.

## Limitations

- The study focuses on a specific backdoor attack and mitigation technique, which may not generalize to all types of backdoor attacks or mitigation methods.
- The experiments are conducted on two specific datasets (CC3M and CC6M), and the results may not directly apply to other datasets or real-world scenarios.
- The paper does not explore the potential for developing new backdoor mitigation techniques that are more effective for models pre-trained with stronger objectives.

## Confidence

- High: CleanCLIP's effectiveness is significantly reduced when using stronger pre-training objectives combining MMCL with SSL.
- Medium: Models pre-trained with MMCL + SSL have unstable cleaning trajectories, making it difficult for practitioners to determine the optimal stopping point.
- Low: CleanCLIP's effectiveness is severely compromised even with slight relaxation of the clean finetuning data assumption.

## Next Checks

1. Investigate the impact of different trigger patch sizes and placements on the effectiveness of CleanCLIP for models pre-trained with MMCL + SSL.
2. Explore the generalization of these findings to other vision-language models and datasets beyond CLIP and CC3M/CC6M.
3. Develop and evaluate alternative cleaning methods specifically designed for models pre-trained with stronger objectives to determine if they can successfully remove backdoors without significant accuracy loss.