---
ver: rpa2
title: 'Neurons in Large Language Models: Dead, N-gram, Positional'
arxiv_id: '2309.04827'
source_url: https://arxiv.org/abs/2309.04827
tags:
- neurons
- positional
- figure
- neuron
- position
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes neurons in OPT models from 125M to 66B parameters,
  focusing on feed-forward network (FFN) neurons by examining whether they are activated
  or not. Key findings include: (1) Early layers are sparse with many "dead" neurons
  (70% in some layers of the 66B model), while later layers are fully active; (2)
  Many alive early neurons act as token and n-gram detectors, explicitly removing
  current token information while promoting next token candidates - a novel mechanism
  for information removal; (3) Some neurons encode positional information regardless
  of textual content, with smaller models using explicit position range indicators
  while larger models use more abstract patterns; (4) The 350M model is an outlier
  due to different layer normalization placement affecting interpretability.'
---

# Neurons in Large Language Models: Dead, N-gram, Positional

## Quick Facts
- arXiv ID: 2309.04827
- Source URL: https://arxiv.org/abs/2309.04827
- Reference count: 20
- Primary result: Early OPT layers contain many dead neurons (>70% in 66B model) that detect n-grams and explicitly suppress triggering tokens while promoting next tokens; larger models use more abstract positional encoding.

## Executive Summary
This paper analyzes neurons in OPT models from 125M to 66B parameters, focusing on feed-forward network (FFN) neurons by examining whether they are activated or not. Key findings include: early layers are sparse with many "dead" neurons (>70% in some layers of the 66B model), while later layers are fully active; many alive early neurons act as token and n-gram detectors, explicitly removing current token information while promoting next token candidates - a novel mechanism for information removal; some neurons encode positional information regardless of textual content, with smaller models using explicit position range indicators while larger models use more abstract patterns; the 350M model is an outlier due to different layer normalization placement affecting interpretability.

## Method Summary
The authors analyze OPT models from 125M to 66B parameters by running forward passes on diverse datasets (Pile subsets, Reddit, Codeparrot) to collect neuron activations. They identify dead neurons (never activate), n-gram-detecting neurons (95% coverage by 1-5 tokens), and positional neurons (mutual information >0.05 with position). FFN updates are analyzed by projecting second-layer weights onto vocabulary to identify suppression (negative projections) and promotion (positive projections) patterns. Analysis is performed layer-by-layer using single-GPU processing for large models.

## Key Results
- Early layers contain many "dead" neurons (>70% in some 66B model layers) that never activate on diverse data
- Token-detecting neurons explicitly suppress their triggering tokens while promoting next token candidates
- Smaller models use explicit positional encoding (oscillatory neurons for position ranges) while larger models use more abstract patterns
- The 350M model is an outlier due to different layer normalization placement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Early layers contain many "dead" neurons that never activate on diverse data, with sparsity increasing in larger models.
- Mechanism: The model assigns dedicated neurons to discrete shallow features (tokens, n-grams) in early layers, leaving many neurons unused for rare patterns.
- Core assumption: The space of shallow patterns is enumerable and much smaller than the number of available neurons in early layers.
- Evidence anchors:
  - [abstract] "many neurons (more than 70% in some layers of the 66b model) are 'dead', i.e. they never activate on a large collection of diverse data"
  - [section] "The space of shallow patterns is not large and, potentially, enumerable, in the early layers the model can (and, as we will see later, does) assign dedicated neurons to some features"
  - [corpus] Weak - corpus doesn't directly address neuron sparsity patterns
- Break condition: If shallow patterns are not enumerable or if the number of shallow patterns approaches the number of available neurons, this sparsity pattern would break down.

### Mechanism 2
- Claim: Token-detecting neurons explicitly suppress their triggering tokens while promoting next token candidates.
- Mechanism: When a token-detecting neuron activates, its corresponding FFN update both promotes next token candidates (top projections) and suppresses the triggering token (bottom projections).
- Core assumption: The model can encode dual-direction updates in a single FFN row, simultaneously promoting and suppressing concepts.
- Evidence anchors:
  - [abstract] "their corresponding FFN updates not only promote next token candidates as could be expected, but also explicitly focus on removing the information about triggering them tokens"
  - [section] "We find that often token-detecting neurons deliberately suppress the tokens they detect" and "For over 80% of token-detecting neurons their corresponding updates point in the negative direction from the triggering them tokens"
  - [corpus] Weak - corpus doesn't directly address suppression mechanisms
- Break condition: If FFN updates cannot encode negative projections or if the model uses different mechanisms for token suppression, this explicit suppression mechanism would break down.

### Mechanism 3
- Claim: Larger models use more abstract positional representations while smaller models encode absolute position explicitly through oscillatory neurons.
- Mechanism: Small models use indicator function neurons for position ranges, while large models use weaker, more generic patterns that encode position less explicitly.
- Core assumption: Model capacity allows larger models to move beyond explicit positional encoding to more abstract representations.
- Evidence anchors:
  - [abstract] "smaller models have sets of neurons acting as position range indicators while larger models operate in a less explicit manner"
  - [section] "First, we notice that smaller models rely substantially on oscillatory neurons: this is the most frequent type of positional neurons for models smaller than 6.7b of parameters" and "larger models do not have oscillatory neurons and rely on more generic patterns"
  - [corpus] Weak - corpus doesn't directly address positional encoding differences across scales
- Break condition: If larger models continue using explicit positional encoding or if smaller models use abstract positional representations, this mechanism would break down.

## Foundational Learning

- Concept: Sparse activation patterns in neural networks
  - Why needed here: Understanding why neurons can be "dead" requires grasping how sparsity emerges in neural networks
  - Quick check question: Why would a neuron that never activates on diverse data still be useful during training?

- Concept: Mutual information as a measure of dependence
  - Why needed here: Identifying positional neurons requires measuring how much neuron activations depend on position
  - Quick check question: If a neuron activates randomly regardless of position, what would its mutual information with position be?

- Concept: Key-value memory view of FFN layers
  - Why needed here: The paper challenges this view by showing neurons that don't fit the key-value pattern
  - Quick check question: According to the key-value view, what should each key correlate with and what should each value induce?

## Architecture Onboarding

- Component map: OPT models -> layers -> FFN blocks -> neurons -> activations -> projections on vocabulary
- Critical path: Forward pass -> neuron activation -> FFN update -> residual stream -> next token prediction
- Design tradeoffs: ReLU activation enables exact zeros (interpretable) but creates dead neurons; single-GPU processing requires layer-by-layer loading
- Failure signatures: No dead neurons in early layers suggests different activation function or layer normalization placement; no token suppression suggests different FFN update mechanism
- First 3 experiments:
  1. Measure neuron activation frequencies across layers to identify dead neurons
  2. Project FFN rows onto vocabulary to identify promoted vs suppressed tokens
  3. Compute mutual information between neuron activations and position to find positional neurons

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms cause the "dead" neurons in early layers to remain inactive across diverse datasets, and are these neurons truly functionless or simply specialized for extremely rare patterns?
- Basis in paper: [explicit] The paper observes that many neurons in early layers never activate on diverse data, with >70% of neurons being "dead" in some layers of the 66B model.
- Why unresolved: The paper notes that dead neurons could either be completely functionless or specialized for patterns so rare they weren't encountered in the dataset. The distinction between these possibilities remains unexplored.
- What evidence would resolve it: Systematic analysis of dead neurons across vastly more diverse and specialized datasets, or targeted probing experiments to determine if these neurons have any functional role when presented with rare patterns.

### Open Question 2
- Question: How do the qualitative differences between smaller and larger models (particularly regarding token/n-gram detection and positional encoding) emerge during training, and what specific architectural or optimization factors drive these differences?
- Basis in paper: [explicit] The paper observes that larger models have more token-detecting neurons, use less explicit positional encoding, and show different layer-wise behavior compared to smaller models.
- Why unresolved: The paper identifies these differences but doesn't explore the training dynamics or architectural factors that cause them to emerge.
- What evidence would resolve it: Detailed analysis of training trajectories showing when these differences emerge, ablation studies on model architecture/size, and investigation of how different training regimes affect these phenomena.

### Open Question 3
- Question: What is the precise computational role of positional neurons in transformer layers, and how do they interact with other mechanisms like attention and token processing to affect model behavior?
- Basis in paper: [explicit] The paper identifies neurons that encode positional information regardless of textual content and questions their role beyond the traditional key-value memory view.
- Why unresolved: While the paper identifies these neurons and their patterns, it doesn't fully explain how they contribute to the model's overall computation or how they interact with other components.
- What evidence would resolve it: Experiments manipulating or removing positional neurons to observe effects on model behavior, analysis of how positional information flows through the network, and investigation of their interaction with attention mechanisms.

## Limitations

- The sparsity patterns depend heavily on corpus diversity and size, making claims about "enumerable" shallow patterns speculative
- Suppression mechanism evidence relies on correlation rather than causal intervention experiments
- Scale-dependent positional representation claims are based on mutual information thresholds that may miss other encoding strategies

## Confidence

- **High confidence**: Dead neuron distribution across layers is directly observable and reproducible
- **Medium confidence**: N-gram detection patterns and suppression mechanisms are supported by suggestive but not definitive evidence
- **Low confidence**: Claims about "enumerable" shallow patterns and fundamental shifts in positional encoding strategies are more speculative

## Next Checks

1. **Corpus Coverage Validation**: Systematically measure the fraction of possible n-grams up to length 3 that appear in the evaluation corpus versus those that could theoretically appear.

2. **Intervention Experiment for Suppression**: Design an ablation study where token-detecting neurons are selectively modified to test whether negative projections are causally responsible for information removal.

3. **Cross-Architecture Positional Analysis**: Compare positional encoding patterns across different architectures (GPT, LLaMA, etc.) to determine if scale-dependent shifts are general or OPT-specific.