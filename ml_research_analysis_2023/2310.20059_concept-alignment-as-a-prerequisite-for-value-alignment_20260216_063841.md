---
ver: rpa2
title: Concept Alignment as a Prerequisite for Value Alignment
arxiv_id: '2310.20059'
source_url: https://arxiv.org/abs/2310.20059
tags:
- human
- reward
- alignment
- goal
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the problem of concept alignment in the context
  of inverse reinforcement learning, showing that neglecting to account for differences
  in how humans and AI agents represent a task can lead to significant value misalignment.
  The authors formalize concept alignment using the value-guided construal framework,
  which models how humans simplify complex tasks by selectively using concepts.
---

# Concept Alignment as a Prerequisite for Value Alignment

## Quick Facts
- arXiv ID: 2310.20059
- Source URL: https://arxiv.org/abs/2310.20059
- Reference count: 26
- This paper introduces concept alignment in inverse reinforcement learning, showing that neglecting differences in how humans and AI agents represent tasks can lead to significant value misalignment.

## Executive Summary
This paper presents a novel framework for concept alignment in inverse reinforcement learning (IRL), demonstrating that neglecting to account for differences in how humans and AI agents represent a task can lead to severe value misalignment. The authors formalize concept alignment using the value-guided construal framework, which models how humans simplify complex tasks by selectively using concepts. Through both theoretical analysis and empirical experiments, they show that an IRL agent which jointly reasons about concepts (construals) and rewards performs significantly better at inferring human values compared to one that only reasons about rewards.

The key insight is that human planning is resource-rational and uses simplified concepts rather than full task details. When an IRL agent assumes optimal planning with full transition knowledge while the human demonstrator uses a simplified construal, the agent misattributes behavior to incorrect reward values. The joint model correctly infers reward values in scenarios where the preferred goal is inaccessible without using a shortcut (notch), whereas the reward-only model fails. Human participants also successfully use concept alignment when inferring others' values, matching the behavior of the joint model.

## Method Summary
The authors model tasks as Markov Decision Processes and formalize concept alignment using the value-guided construal framework. They implement maximum causal entropy IRL with and without construal modeling, running inference on trajectory scenarios in 3×3 gridworld environments. The joint inference model estimates both rewards and construals, while the reward-only model estimates only rewards. Human experiments were conducted with 100 participants who made binary judgments about notch awareness and goal preference in the same scenarios. The study compares posterior probabilities of different models and correlates with human judgments.

## Key Results
- Neglecting concept alignment can lead to severe value misalignment (reward mis-specification; large performance gap)
- The joint reward and construal model performs significantly better than reward-only IRL in scenarios where the demonstrator doesn't understand notches
- Human participants successfully use construals to make more accurate reward inferences, matching the behavior of the joint model
- Concept alignment is a crucial prerequisite for effective value alignment between humans and AI systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Value misalignment occurs when an IRL agent assumes optimal planning with full transition knowledge, while the human demonstrator uses a simplified construal that omits certain concepts.
- Mechanism: The observer's likelihood function assumes the demonstrator uses the true transition dynamics T, but the demonstrator actually plans with a simplified T̃. This mismatch causes the observer to infer incorrect reward values from the same behavior.
- Core assumption: Human planning is resource-rational and uses simplified concepts rather than full task details.
- Evidence anchors: [abstract] "neglecting concept alignment can lead to severe value misalignment"; [section 4.2] "Without knowledge of the construals, the agent might misattribute the path to a higher reward value for the chosen goal, not realizing the other goal may in fact have a higher reward, but may be impossible to reach without using/paying attention to notches."

### Mechanism 2
- Claim: Joint inference of construals and rewards improves value alignment compared to reward-only inference.
- Mechanism: By modeling both R and T̃, the observer can distinguish between cases where a goal is avoided due to low reward versus cases where it's inaccessible without using a concept the demonstrator doesn't understand.
- Core assumption: Construals are discrete and observable through behavior patterns across multiple demonstrations.
- Evidence anchors: [section 4.3] "the joint reward and construal model performs on par with the reward-only model in the settings where the demonstrator 'understands notches', but significantly outperforms the reward-only model in inferring values in the difficult 'Does not understand notches' scenario"

### Mechanism 3
- Claim: Human observers naturally reason about others' concepts when inferring intentions and values.
- Mechanism: Humans use theory of mind to simulate what concepts another person might be using, which improves their inferences about that person's goals and values.
- Core assumption: Humans have evolved or learned to use concept alignment as part of social cognition.
- Evidence anchors: [section 5.1] "humans completing the same inference task as the IRL agents successfully use construals to make more accurate reward inferences, matching the behavior of the joint reward and construal model"

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The paper models tasks as MDPs with states, actions, transitions, and rewards.
  - Quick check question: What are the five components of an MDP?

- Concept: Inverse Reinforcement Learning (IRL)
  - Why needed here: The paper uses IRL to infer human reward functions from observed behavior.
  - Quick check question: How does IRL differ from behavioral cloning?

- Concept: Value-guided construal
  - Why needed here: This framework models how humans simplify complex tasks by selectively using concepts.
  - Quick check question: What is the difference between a task's ground truth dynamics and a person's construal of it?

- Concept: Bayesian inference
  - Why needed here: The paper formulates both reward inference and construal inference as Bayesian problems.
  - Quick check question: In the context of IRL, what does P(R|ζ) represent?

## Architecture Onboarding

- Component map: Gridworld Environment -> Demonstrator Model (with/without notch awareness) -> Observer Models (Reward-only IRL, Joint Reward-Construal Inference) -> Human Experiment Interface
- Critical path: 1. Generate demonstrations using demonstrator model 2. Run inference with observer models to estimate rewards and construals 3. Compare model performance and compare to human inferences
- Design tradeoffs: Discrete vs continuous construal modeling (discrete is simpler but less expressive); number of demonstrations needed (more demonstrations help distinguish construals but increase data collection burden); computational cost of joint inference vs separate inference
- Failure signatures: Reward-only model inferring incorrect reward ordering in "does not understand notches" scenarios; joint model failing to converge on construal estimates; human participants showing random guessing patterns on construal questions
- First 3 experiments: 1. Replicate the notches experiment with synthetic demonstrators to verify the theoretical performance gap bound 2. Test the joint inference model on a continuous construal space (e.g., varying degrees of attention to notches) 3. Conduct a human experiment with more complex environments (e.g., multiple types of shortcuts or obstacles)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed concept alignment framework be extended to more complex real-world scenarios with a much larger number of potential concepts and construals?
- Basis in paper: [inferred] The paper acknowledges that the experiments used a simplified setting with a small number of features that could form the basis for construal, and that a simple hard-coded construal model would be insufficient for real-world settings with countless more features.
- Why unresolved: The paper does not provide a clear path or methodology for scaling up the concept alignment approach to handle the complexity and diversity of concepts in real-world tasks.
- What evidence would resolve it: Developing and validating a concept alignment algorithm that can handle a large, diverse set of concepts and successfully infer human values in complex real-world scenarios.

### Open Question 2
- Question: How can the concept alignment framework be used to improve the interpretability and transparency of AI systems' decision-making processes?
- Basis in paper: [explicit] The paper mentions that modeling construals creates a shared conceptual framework which enables more accurate reasoning about another person's rewards and values. This suggests that concept alignment could help AI systems better understand and align with human concepts, potentially improving interpretability.
- Why unresolved: The paper does not explore how the proposed concept alignment framework could be used to enhance the interpretability and transparency of AI systems' decision-making processes.
- What evidence would resolve it: Demonstrating that an AI system using concept alignment can provide more interpretable and transparent explanations for its decisions by explicitly reasoning about the concepts used by humans.

### Open Question 3
- Question: How can the concept alignment framework be integrated with existing value alignment approaches, such as cooperative inverse reinforcement learning (CIRL)?
- Basis in paper: [inferred] The paper focuses on the concept alignment problem within the inverse reinforcement learning (IRL) setting, but does not explore how it could be combined with other value alignment approaches like CIRL that aim to align an AI system's values with those of a human.
- Why unresolved: The paper does not investigate how the concept alignment framework could be integrated with other value alignment methods to create a more comprehensive approach to aligning AI systems with human values.
- What evidence would resolve it: Developing a unified framework that combines concept alignment with other value alignment approaches and demonstrating its effectiveness in aligning AI systems with human values in various scenarios.

## Limitations

- Experimental validation is limited to a single 3×3 gridworld domain with a binary concept (notch awareness), which may not generalize to more complex or continuous concept spaces
- The human experiments involve only 100 participants making binary judgments on simple scenarios, limiting the strength of social cognition claims
- The theoretical bounds provided are conservative and may not capture the full extent of potential misalignment in real-world applications

## Confidence

- **High confidence**: The core theoretical claim that concept misalignment can cause value misalignment in IRL scenarios
- **Medium confidence**: The claim that joint inference of construals and rewards improves performance in the "does not understand notches" scenario
- **Medium confidence**: The claim that humans naturally use concept alignment in social inference

## Next Checks

1. Test the joint inference model on environments with continuous concept spaces (e.g., varying degrees of attention to multiple types of obstacles) to evaluate scalability beyond binary concepts
2. Conduct human experiments with more complex environments involving multiple agents with different concept representations to validate the social cognition claims in richer contexts
3. Apply the framework to a real-world robotics task where concept misalignment between human demonstrators and learning agents could have practical consequences, such as household task learning