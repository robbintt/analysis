---
ver: rpa2
title: Inductive reasoning in humans and large language models
arxiv_id: '2306.06548'
source_url: https://arxiv.org/abs/2306.06548
tags:
- argument
- property
- reasoning
- have
- arguments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares human and LLM performance on property induction
  tasks. The authors present two datasets of human judgments and compare these with
  judgments elicited from GPT-3 and GPT-4.
---

# Inductive reasoning in humans and large language models

## Quick Facts
- arXiv ID: 2306.06548
- Source URL: https://arxiv.org/abs/2306.06548
- Reference count: 40
- Primary result: GPT-4 performs well on property induction tasks except for non-monotonicity, with RLHF training both improving alignment and constraining generalization

## Executive Summary
This paper compares human and LLM performance on property induction tasks using two large datasets. Experiment 1 evaluates 11 qualitative phenomena across three domains, finding GPT-4 captures most phenomena well except non-monotonicity. Experiment 2 elicits strength ratings for 1168 arguments, showing GPT-4's performance comparable to a special-purpose model on specific arguments but weaker on general ones. GPT-3 outperforms GPT-4 on general arguments, possibly due to GPT-4's RLHF training. The authors provide two large datasets for future work and argue property induction is a useful benchmark for evaluating LLMs.

## Method Summary
The study uses two datasets: one with argument pairs for 11 phenomena across Mammals, Birds, and Vehicles domains, and another with individual arguments rated for strength. Human ratings were collected via Mechanical Turk. GPT-4 and GPT-3 were prompted to rate argument strength using fixed temperature parameters and maximum response lengths. The Similarity-Coverage Model (SCM) served as a baseline. Statistical comparisons included sign tests for argument pairs and correlation analysis for individual ratings.

## Key Results
- GPT-4 qualitatively matches human performance on most property induction phenomena except non-monotonicity
- GPT-4 performs comparably to SCM on specific arguments but weaker on general arguments
- GPT-3 outperforms GPT-4 on general arguments, suggesting RLHF may constrain generalization
- Two large datasets (1168 arguments total) provided for future research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's performance is tied to its ability to encode and retrieve structured semantic knowledge about categories and properties
- Mechanism: Pretraining and RLHF shape internal representations that align with human similarity and typicality judgments
- Core assumption: GPT-4's representations capture enough distributional patterns to approximate human semantic judgments
- Evidence anchors: GPT-4 qualitatively matches human performance; internal representations predict human similarity judgments
- Break condition: If RLHF removes distributional cues or prompt formatting disrupts semantic retrieval

### Mechanism 2
- Claim: GPT-4's reasoning depends on simulating human-like sampling assumptions, especially for diversity and non-monotonicity
- Mechanism: Model implicitly reasons about premise generation through learned probabilistic associations
- Core assumption: Pretraining exposed model to varied premise patterns for developing sampling heuristics
- Evidence anchors: GPT-4 fails on non-monotonicity; diversity only partially captured
- Break condition: If training corpus lacks diverse-premise examples or RLHF shifts inference focus

### Mechanism 3
- Claim: GPT-4's RLHF improves alignment but can degrade generalization capabilities
- Mechanism: RLHF makes model less likely to hallucinate but more conservative in inductive leaps
- Core assumption: RLHF prioritizes factual accuracy over speculative inference
- Evidence anchors: GPT-3 outperforms GPT-4 on general arguments; RLHF aims to align with human expectations
- Break condition: If task context signals speculative generalization is appropriate or prompt engineering bypasses RLHF

## Foundational Learning

- Concept: Semantic similarity and typicality
  - Why needed here: Property induction relies on judgments about premise-conclusion similarity and premise typicality
  - Quick check question: Given "Robins have property P, therefore birds have property P," what would make this argument stronger: Robins being very typical of birds, or Robins being very dissimilar to other birds?

- Concept: Inductive vs deductive reasoning
  - Why needed here: Property induction is inherently inductive with plausible but uncertain conclusions
  - Quick check question: If "All mammals have property P" is given as a premise, is it inductive or deductive to conclude "Dogs have property P"?

- Concept: Sampling assumptions in reasoning
  - Why needed here: Some phenomena depend on assumptions about how premises were selected
  - Quick check question: Why might an argument with premise categories from very different superordinate groups (e.g., "hippos and hamsters") be considered stronger than one with premises from the same group?

## Architecture Onboarding

- Component map: Data ingestion (LNCD categories, similarity ratings, argument pairs) -> Prompt design (system message, context, arguments, question, options) -> Model execution (GPT-4/GPT-3 API calls) -> Evaluation (human ratings, SCM model, statistical tests) -> Analysis (comparisons across domains, phenomena, models)
- Critical path: 1) Generate argument pairs using SCM and LNCD norms 2) Elicit human ratings via Mechanical Turk 3) Design and test prompts for GPT-4/GPT-3 4) Extract and align model responses with human judgments 5) Compare model performance statistically
- Design tradeoffs: Discrete vs continuous rating scales (GPT-4 limited to discrete), prompt variation exploration vs computational cost, single-prompt vs multi-prompt evaluation
- Failure signatures: GPT-3 bimodal extreme responses, GPT-4 failing on non-monotonicity, low split-half reliability indicating noisy human data
- First 3 experiments: 1) Run exact argument pair task with new categories to test prompt generalizability 2) Replace RLHF-trained GPT-4 with base model to isolate RLHF effects 3) Implement multi-prompt evaluation with varying system message and context

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GPT-4's performance change when evaluated using a ranking task instead of a rating task?
- Basis in paper: The paper notes ranking and rating are fundamentally different tasks that may expose different aspects of inductive reasoning
- Why unresolved: Current experiments only used rating tasks for both humans and LLMs
- What evidence would resolve it: Comparative experiments where GPT-4 and humans rate vs rank the same arguments with correlation analysis

### Open Question 2
- Question: Does GPT-4's failure to capture non-monotonicity extend to all forms or is it specific to the type tested?
- Basis in paper: GPT-4 fails to capture non-monotonicity effects, but only one specific type was thoroughly tested
- Why unresolved: While one type of non-monotonicity was tested, other forms were only briefly examined
- What evidence would resolve it: Systematic experiments testing GPT-4 on multiple distinct types of non-monotonic arguments

### Open Question 3
- Question: How does training data composition influence GPT-4's performance, and were test tasks present in training data?
- Basis in paper: The paper acknowledges uncertainty about which human groups provide natural comparison and that training data may influence performance
- Why unresolved: Training data for GPT-4 is not publicly available
- What evidence would resolve it: Analysis of open models with accessible training data combined with testing on demonstrably novel arguments

## Limitations
- Limited set of categories and properties examined may not generalize to broader tasks
- Performance differences suggest model architecture and training history significantly impact reasoning, but mechanisms remain unclear
- RLHF appears to both improve alignment and constrain generalization, but tradeoff points are not established

## Confidence

- High Confidence: GPT-4's superior performance compared to GPT-3 on specific property induction tasks and general alignment with human judgments
- Medium Confidence: Interpretation that RLHF constrains GPT-4's ability to generalize beyond given premises
- Medium Confidence: Claim that property induction serves as a useful benchmark for evaluating LLMs

## Next Checks
1. Test whether GPT-4's non-monotonicity failure persists when using base model versions without RLHF fine-tuning
2. Evaluate model performance on property induction tasks using categories and properties not in training corpus
3. Implement multi-prompt evaluation strategies with systematic variation in system messages and contextual framing