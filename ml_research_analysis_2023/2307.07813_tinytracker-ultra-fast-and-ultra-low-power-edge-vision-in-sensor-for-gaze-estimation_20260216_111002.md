---
ver: rpa2
title: 'TinyTracker: Ultra-Fast and Ultra-Low-Power Edge Vision In-Sensor for Gaze
  Estimation'
arxiv_id: '2307.07813'
source_url: https://arxiv.org/abs/2307.07813
tags:
- edge
- vision
- imx500
- gaze
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work evaluates Sony\u2019s IMX500, one of the first \u201C\
  AI in sensor\u201D vision platforms, for ultra-low-power, ultra-fast edge vision\
  \ tasks, using gaze estimation as a case study. To leverage the IMX500\u2019s computational\
  \ constraints, the authors propose TinyTracker, a highly efficient, fully quantized\
  \ model that reduces model size by 41x compared to the baseline (600 KB) with minimal\
  \ loss in accuracy (0.16 cm when fully quantized)."
---

# TinyTracker: Ultra-Fast and Ultra-Low-Power Edge Vision In-Sensor for Gaze Estimation

## Quick Facts
- arXiv ID: 2307.07813
- Source URL: https://arxiv.org/abs/2307.07813
- Reference count: 39
- Primary result: Ultra-low-power gaze estimation on Sony IMX500 with 41x model size reduction and minimal accuracy loss.

## Executive Summary
This work evaluates Sony’s IMX500, an “AI in sensor” vision platform, for ultra-low-power, ultra-fast edge vision tasks, using gaze estimation as a case study. The authors propose TinyTracker, a highly efficient, fully quantized model that reduces model size by 41x compared to the baseline (600 KB) with minimal loss in accuracy (0.16 cm when fully quantized). When deployed on the IMX500, TinyTracker achieves end-to-end latency of 19 ms and total energy consumption of 4.9 mJ, with only 0.06 mJ used for inference. Compared to Google Coral Dev Micro, the IMX500 is 1.7x faster and 7x more power-efficient, demonstrating the effectiveness of sensor-integrated AI accelerators for scalable, resource-constrained computer vision applications.

## Method Summary
The paper proposes TinyTracker, a highly efficient, fully quantized model for gaze estimation on Sony’s IMX500 AI in-sensor platform. The model is based on a MobileNetV3 backbone with an added convolutional layer and two fully connected layers, trained on the GazeCapture dataset with data augmentation. After training, the model is quantized to 8-bit integers while retaining 32-bit floating-point precision on the outputs. The IMX500’s integrated AI processor eliminates the need for external memory access during inference, drastically reducing latency and energy consumption.

## Key Results
- TinyTracker achieves a 41x size reduction (600 KB) compared to iTracker with minimal loss in gaze estimation accuracy (maximum of 0.16 cm when fully quantized).
- When deployed on the IMX500, TinyTracker achieves end-to-end latency of 19 ms and total energy consumption of 4.9 mJ, with only 0.06 mJ used for inference.
- Compared to Google Coral Dev Micro, the IMX500 is 1.7x faster and 7x more power-efficient.

## Why This Works (Mechanism)

### Mechanism 1
Sensor-integrated AI accelerators eliminate the need for external memory access during inference, drastically reducing both latency and energy consumption. The IMX500 integrates the image sensor with a DSP and dedicated SRAM directly on the same chip, allowing raw pixel data to be processed by the AI accelerator without being transferred to off-chip memory.

### Mechanism 2
Quantization to 8-bit integers dramatically reduces model size and memory bandwidth while maintaining accuracy for gaze estimation. TinyTracker is fully quantized to 8-bit integers during deployment, reducing the model size by 41x and lowering the memory footprint per inference operation.

### Mechanism 3
Input simplification (removing eye and face grid inputs) reduces computational complexity without severely impacting gaze estimation accuracy. TinyTracker eliminates the eye and face grid inputs used in iTracker, concatenating face image coordinates to the input grid embedding instead.

## Foundational Learning

- **Edge AI hardware constraints (memory, compute, energy)**
  - Why needed here: Understanding why the IMX500's on-chip SRAM and DSP design is critical to the performance gains claimed.
  - Quick check question: What is the approximate SRAM size on the IMX500 and how does it compare to typical edge MCUs?

- **Quantization-aware training and deployment**
  - Why needed here: To understand how 8-bit quantization reduces model size and energy without losing accuracy.
  - Quick check question: What are the typical accuracy drops when quantizing a vision model from FP32 to INT8, and how can quantization-aware training mitigate this?

- **Model compression techniques (pruning, knowledge distillation)**
  - Why needed here: To grasp how TinyTracker achieves a 41x size reduction and what trade-offs are involved.
  - Quick check question: How does pruning differ from quantization in terms of impact on model accuracy and computational efficiency?

## Architecture Onboarding

- **Component map:**
  - IMX500: Stacked pixel chip (12.3 MP) + logic chip (AI processor + DSP + SRAM)
  - TinyTracker: MobileNetV3 backbone + 1 conv layer + 2 FC layers, fully INT8 quantized
  - Host MCU: Sony Spresense / Coral Dev Micro for control and data logging
  - Camera interface: MIPI/CSI for image capture

- **Critical path:**
  1. Image capture (17.9 ms)
  2. Pixel data transfer to IMX500 accelerator (minimal latency due to on-chip integration)
  3. Model inference (0.86 ms)
  4. Result retrieval (0.24 ms)
  5. End-to-end total: ~19 ms

- **Design tradeoffs:**
  - Model size vs. accuracy: 41x reduction achieved with 0.16 cm max accuracy loss
  - Input complexity vs. computation: Simplified input pipeline reduces MACs but relies on coordinate embedding for spatial information
  - Quantization precision vs. energy: INT8 quantization saves energy but may require quantization-aware training

- **Failure signatures:**
  - High end-to-end latency: Likely due to off-chip memory access or model exceeding SRAM capacity
  - Excessive energy consumption: Possible due to unnecessary camera activation or inefficient model execution
  - Accuracy drop beyond 0.16 cm: Likely due to quantization artifacts or insufficient input information encoding

- **First 3 experiments:**
  1. Profile memory usage of TinyTracker on IMX500 SRAM to confirm it fits and identify any memory bottlenecks.
  2. Compare inference latency and energy consumption of TinyTracker on IMX500 vs. Coral Micro under identical input conditions to validate the 1.7x speedup claim.
  3. Perform ablation study removing the coordinate embedding to quantify its impact on gaze estimation accuracy.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What is the maximum accuracy achievable for gaze estimation on the IMX500 platform with further model optimizations?
- Basis in paper: [inferred] The paper mentions TinyTracker achieves 2.34 cm error with a 41x size reduction, and discusses the potential of sensor-integrated AI accelerators.
- Why unresolved: The paper focuses on demonstrating the feasibility of TinyTracker but does not explore the upper limits of accuracy that could be achieved with further optimization on the IMX500.
- What evidence would resolve it: A comprehensive study comparing various model architectures and optimization techniques specifically tailored for the IMX500 platform, reporting the best achievable accuracy.

### Open Question 2
- Question: How does the performance of TinyTracker on the IMX500 compare to other emerging AI in-sensor platforms?
- Basis in paper: [explicit] The paper compares the IMX500 to other edge platforms like the Coral Dev Micro and Sony Spresense but does not include other emerging AI in-sensor platforms.
- Why unresolved: The paper provides a benchmark against existing platforms but does not extend the comparison to newer or alternative AI in-sensor technologies that may offer different trade-offs in performance, power efficiency, or cost.
- What evidence would resolve it: A comparative study including a broader range of AI in-sensor platforms, evaluating them on the same tasks and metrics as used in the paper.

### Open Question 3
- Question: What are the limitations of using a fully quantized model like TinyTracker in real-world applications?
- Basis in paper: [explicit] The paper mentions that TinyTracker is fully quantized to 8-bit integers and discusses the trade-offs in accuracy and model size.
- Why unresolved: While the paper highlights the benefits of quantization for edge deployment, it does not delve into potential limitations or challenges that might arise when deploying such models in diverse real-world scenarios.
- What evidence would resolve it: Case studies or empirical research demonstrating the performance of TinyTracker in various real-world applications, highlighting any limitations or challenges encountered.

## Limitations
- The evaluation relies on a single edge vision platform (Sony IMX500) and one gaze estimation dataset (GazeCapture).
- The simplified input pipeline (removing eye and face grids) could limit generalization to other gaze estimation tasks or datasets.
- The quantization process and its impact on accuracy are not fully transparent, and the specific hardware implementation details of the IMX500's AI processor are proprietary.

## Confidence
**High Confidence:** The model architecture and training procedure (MobileNetV3 backbone, quantization to 8-bit integers, 20 epochs of Adam training) are clearly specified and reproducible. The end-to-end latency and energy consumption measurements (19 ms, 4.9 mJ) are directly reported from the IMX500 platform.

**Medium Confidence:** The 41x model size reduction and 0.16 cm accuracy retention are well-supported by the reported comparisons to iTracker, but the specific implementation details of the TinyTracker model (number of layers, exact architecture) are not fully provided. The speedup and power efficiency improvements over Coral Micro are based on direct comparisons but may depend on specific hardware configurations.

**Low Confidence:** The generalizability of the simplified input pipeline (removing eye and face grids) to other gaze estimation tasks or datasets is uncertain. The quantization process and its impact on accuracy are not fully transparent, and the specific hardware implementation details of the IMX500's AI processor are proprietary.

## Next Checks
1. Profile memory usage of TinyTracker on IMX500 SRAM to confirm it fits and identify any memory bottlenecks.
2. Compare inference latency and energy consumption of TinyTracker on IMX500 vs. Coral Micro under identical input conditions to validate the 1.7x speedup claim.
3. Perform ablation study removing the coordinate embedding to quantify its impact on gaze estimation accuracy.