---
ver: rpa2
title: 'CORAL: Expert-Curated medical Oncology Reports to Advance Language Model Inference'
arxiv_id: '2308.03853'
source_url: https://arxiv.org/abs/2308.03853
tags:
- should
- information
- entity
- relation
- datetime
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study developed a detailed schema for annotating oncology information
  in clinical notes and used it to create a dataset from breast cancer progress notes.
  Three large language models (GPT-4, GPT-3.5-turbo, and FLAN-UL2) were evaluated
  on their ability to extract oncology information from these notes without any task-specific
  training.
---

# CORAL: Expert-Curated medical Oncology Reports to Advance Language Model Inference

## Quick Facts
- arXiv ID: 2308.03853
- Source URL: https://arxiv.org/abs/2308.03853
- Reference count: 40
- Primary result: GPT-4 achieves 0.73 BLEU and 0.72 ROUGE scores on oncology information extraction from breast cancer notes

## Executive Summary
This study develops a detailed schema for annotating oncology information in clinical notes and uses it to create a dataset from breast cancer progress notes. Three large language models (GPT-4, GPT-3.5-turbo, and FLAN-UL2) were evaluated on their ability to extract oncology information from these notes without any task-specific training. GPT-4 achieved the highest performance, with an average BLEU score of 0.73 and an average ROUGE score of 0.72. Expert evaluation confirmed its accuracy at 68% for complex tasks, with notable strengths in tumor characteristic and medication extraction, as well as relational inference.

## Method Summary
The study created a detailed oncology information schema and applied it to annotate 10 de-identified breast cancer progress notes from UCSF. Three large language models were evaluated using zero-shot extraction: GPT-4, GPT-3.5-turbo, and FLAN-UL2. The models were prompted with task-specific entity-relation schemas to extract structured oncology information from the clinical notes. Performance was measured using BLEU and ROUGE scores, with expert manual review validating accuracy on complex tasks.

## Key Results
- GPT-4 achieved an average BLEU score of 0.73 and ROUGE score of 0.72
- Expert evaluation confirmed 68% accuracy for complex tasks
- GPT-4 demonstrated superior performance in tumor characteristic and medication extraction
- Notable strength in relational inference tasks, including inferring symptoms due to cancer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot extraction works because the prompt format explicitly defines the expected output structure (namedtuples) and constrains the model to only return information present in the note.
- Mechanism: By providing both a system role and a user prompt with a precise output schema, the model's reasoning is bounded to a narrow set of entity-relation pairs. The absence of free-form reasoning reduces hallucination.
- Core assumption: The model's pretraining already encodes sufficient medical knowledge and syntactic understanding to parse oncology-specific terminology and relationships without additional fine-tuning.
- Evidence anchors:
  - [abstract]: "The GPT-4 model exhibited overall best performance, with an average BLEU score of 0.73 and an average ROUGE score of 0.72, an exact-match F1-score of 0.51, and an average accuracy of 68% on complex tasks"
  - [section]: "We evaluated three recently released large language models: the GPT-4 model, the GPT-3.5-turbo model, and the FLAN-UL2 model on the following tasks that require complex clinical and linguistic reasoning"
  - [corpus]: Weak – the corpus only lists neighboring papers; no direct validation of the prompt+schema mechanism.
- Break condition: If oncology notes contain novel or ambiguous terminology not represented in pretraining, the zero-shot assumption fails and performance degrades sharply.

### Mechanism 2
- Claim: High extraction accuracy for tumor characteristics and medication history is due to the explicit temporal linking provided in the schema.
- Mechanism: By forcing the model to output both the entity and its Datetime in a structured tuple, the model must infer and align temporal information, which is a well-defined subtask it can handle reliably.
- Core assumption: The model can accurately resolve temporal references like "December 2015" or "last year" to specific dates or date ranges when prompted with a Datetime field.
- Evidence anchors:
  - [abstract]: "Notably, the GPT-4 model was proficient in tumor characteristic and medication extraction, and demonstrated superior performance in advanced reasoning tasks of inferring symptoms due to cancer and considerations of future medications."
  - [section]: "High scores — 0.95 BLEU and 0.94 ROUGE — were obtained by the GPT-4 model when extracting tumor grade paired with temporal information."
  - [corpus]: Missing – no direct mention of temporal reasoning in neighboring papers.
- Break condition: If temporal expressions are ambiguous or missing in the source text, the model cannot infer correct dates, leading to "unknown" outputs.

### Mechanism 3
- Claim: Expert evaluation confirms extraction reliability because the schema includes detailed attribute annotations (negation, modality, continuity) that reduce ambiguity in model output.
- Mechanism: The schema defines explicit attribute values (NegationModalityVal, ContinuityVal, etc.) that the model must populate. This structured annotation forces disambiguation of negated or hypothetical mentions.
- Core assumption: The model can reliably detect and label negation/modality markers in oncology text and map them to the correct attribute value.
- Evidence anchors:
  - [abstract]: "Expert evaluation confirmed its accuracy at 68% for complex tasks, with notable strengths in tumor characteristic and medication extraction, as well as relational inference."
  - [section]: "The attribute NegationModalityVal takes a few different values: negated, affirmed, uncertain_in_present, uncertain_in_past, planned_in_future, and hypothetical_in_future."
  - [corpus]: Weak – no explicit mention of attribute annotation in neighboring papers.
- Break condition: If the model misclassifies negation/modality (e.g., treating "not metastatic" as positive), the relational accuracy drops despite correct entity extraction.

## Foundational Learning

- Concept: Frame semantics in medical text
  - Why needed here: The schema is built on frame semantics, mapping textual mentions to structured entity-relation tuples; understanding this concept is essential to extend or modify the schema.
  - Quick check question: What is the difference between an entity and a relation in frame semantics?
- Concept: Zero-shot learning in LLMs
  - Why needed here: The entire evaluation relies on zero-shot extraction; knowing how LLMs generalize to unseen tasks is critical for troubleshooting and prompt design.
  - Quick check question: What are the risks of relying on zero-shot performance for safety-critical tasks like oncology data extraction?
- Concept: Evaluation metrics BLEU/ROUGE
  - Why needed here: These metrics are used to quantify overlap between model output and reference annotations; understanding their limitations is key for interpreting results.
  - Quick check question: Why might BLEU score be misleading if the reference annotation is incomplete?

## Architecture Onboarding

- Component map: De-identified clinical notes -> preprocessing -> LLM interface (GPT-4/GPT-3.5-turbo/FLAN-UL2) -> Output parser -> Evaluator (BLEU/ROUGE + expert review)
- Critical path: Load de-identified note -> Apply prompt with task-specific entity-relation schema -> Parse LLM output into tuples -> Compare against manual annotations (BLEU/ROUGE) -> Aggregate scores across tasks
- Design tradeoffs:
  - Precision vs recall: BLEU (precision) vs ROUGE (recall) balance
  - Prompt complexity vs model performance: More detailed prompts may improve accuracy but increase cost/time
  - Open-source vs proprietary models: FLAN-UL2 is free but less accurate; GPT-4 is accurate but requires privacy safeguards
- Failure signatures:
  - Low BLEU but high ROUGE: Model is missing details but recalls main concepts
  - High BLEU but low expert accuracy: Model is outputting correct tokens but wrong semantics (hallucination type 1)
  - Consistent "unknown" outputs: Schema field mapping failed or temporal resolution missing
- First 3 experiments:
  1. Test prompt variation: Add chain-of-thought reasoning to improve symptom-cause inference accuracy.
  2. Schema simplification: Remove rarely used attributes (e.g., ExperiencerVal) and measure impact on BLEU/ROUGE.
  3. Model comparison: Run same prompts on GPT-4 vs FLAN-UL2 with same schema and compare both metrics and hallucination rates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the performance of GPT-4 on oncology information extraction tasks when applied to different cancer types and institutions beyond breast cancer at UCSF?
- Basis in paper: [explicit] The paper states that the information representation and annotation schema was designed to be both cancer- and institution-agnostic, and that the authors aim to extend the analysis to large multi-center, multi-cancer studies to obtain generalizable conclusions.
- Why unresolved: The current study only used breast cancer medical oncology notes from UCSF, so the performance of GPT-4 on other cancer types and institutions is unknown.
- What evidence would resolve it: Evaluating GPT-4's performance on oncology information extraction tasks using clinical notes from multiple cancer types and institutions.

### Open Question 2
- Question: How does GPT-4's performance on oncology information extraction tasks compare to other state-of-the-art language models, such as clinical-T5-large and LLaMA?
- Basis in paper: [explicit] The paper mentions that the authors experimented with the clinical-T5-large model and the LLaMA 7B and LLaMA 13B models but did not obtain reasonable outputs for their task.
- Why unresolved: The paper does not provide a detailed comparison of GPT-4's performance with other state-of-the-art language models on oncology information extraction tasks.
- What evidence would resolve it: Conducting a comprehensive comparison of GPT-4's performance with other state-of-the-art language models on oncology information extraction tasks using the same dataset and evaluation metrics.

### Open Question 3
- Question: What is the impact of using different prompt engineering techniques, such as few-shot learning and chain-of-thought prompting, on GPT-4's performance on oncology information extraction tasks?
- Basis in paper: [explicit] The paper mentions that GPT-4 demonstrated few errors related to causal inference and suggests that strategies like few-shot learning and advanced prompt designs like chain-of-thought prompting could improve performance.
- Why unresolved: The paper does not explore the impact of different prompt engineering techniques on GPT-4's performance.
- What evidence would resolve it: Evaluating GPT-4's performance on oncology information extraction tasks using different prompt engineering techniques, such as few-shot learning and chain-of-thought prompting, and comparing the results to the baseline performance.

## Limitations

- Evaluation based on only 10 de-identified breast cancer progress notes from UCSF, limiting generalizability
- 68% expert-verified accuracy indicates significant potential for errors in real-world deployment
- No assessment of model performance across different demographic groups or cancer subtypes
- Unclear how well the model handles ambiguous temporal references or incomplete information common in clinical practice

## Confidence

- **High Confidence**: GPT-4 outperforms other models (GPT-3.5-turbo, FLAN-UL2) on oncology information extraction tasks
- **Medium Confidence**: Zero-shot extraction works due to the prompt's explicit output structure
- **Medium Confidence**: Explicit temporal linking in the schema improves accuracy
- **Low Confidence**: The model can reliably detect and label negation/modality markers

## Next Checks

1. **Temporal Expression Robustness Test**: Evaluate model performance on a broader set of clinical notes containing diverse and ambiguous temporal expressions (e.g., "last year," "recently," "in progress") to assess the reliability of temporal inference.

2. **Schema Attribute Validation**: Conduct a focused study to measure the accuracy of negation/modality attribute labeling, comparing model outputs against expert-annotated ground truth to identify misclassification rates.

3. **Demographic and Cancer Type Generalization**: Expand the evaluation to include clinical notes from multiple cancer types and diverse patient demographics to test the model's generalizability and identify potential biases.