---
ver: rpa2
title: 'Keep the Faith: Faithful Explanations in Convolutional Neural Networks for
  Case-Based Reasoning'
arxiv_id: '2312.09783'
source_url: https://arxiv.org/abs/2312.09783
tags:
- image
- explanations
- protopnet
- prototype
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the faithfulness of explanations generated
  by ProtoPNet, a case-based reasoning architecture for image classification. The
  authors theoretically prove that ProtoPNet's pixel-level explanations violate key
  axioms (e.g., Completeness, Dummy, Sensitivity) due to spatial dependencies between
  latent and image spaces in CNNs.
---

# Keep the Faith: Faithful Explanations in Convolutional Neural Networks for Case-Based Reasoning

## Quick Facts
- arXiv ID: 2312.09783
- Source URL: https://arxiv.org/abs/2312.09783
- Reference count: 40
- Primary result: ProtoPFaith achieves >103× improvement over ProtoPNet on Area Over the Perturbation Curve (AOPC) metric

## Executive Summary
This paper addresses the critical problem of unfaithful explanations in ProtoPNet, a case-based reasoning architecture for image classification. The authors theoretically prove that ProtoPNet's pixel-level explanations violate key axioms (Completeness, Dummy, Sensitivity) due to spatial dependencies between latent and image spaces in CNNs. They propose ProtoPFaith, which computes Shapley values on prototype similarities to produce faithful explanations. Experiments across three datasets (CUB-200-2011, Stanford Dogs, RSNA) and five backbone architectures demonstrate that ProtoPFaith's explanations are both qualitatively more meaningful and quantitatively superior on AOPC metrics.

## Method Summary
ProtoPFaith addresses ProtoPNet's unfaithful explanations by computing Shapley values on prototype similarity scores rather than using up-scaled distance maps. The method converts ProtoPNet to a probabilistic model by deriving closed-form solutions for ReLU1 and prototype module mean/variance, then uses Deterministic Approximation of Shapley Values (DASP) to efficiently approximate Shapley values. These values are then transformed to attribution w.r.t. model predictions through the linear classification layer. The approach guarantees satisfaction of all required axioms (Sensitivity, Completeness, Dummy, Linearity) while maintaining computational tractability through approximation techniques.

## Key Results
- ProtoPFaith explanations are qualitatively more meaningful, highlighting relevant image regions (e.g., animal contours) rather than background
- Achieves AOPC scores that outperform ProtoPNet by a factor greater than 103 across all tested datasets
- Demonstrates consistent performance across five different backbone architectures (ConvNet, ResNet, ResNet50, WideResNet50, ResNeXt50)
- Maintains competitive classification accuracy while providing faithful explanations

## Why This Works (Mechanism)

### Mechanism 1
ProtoPNet's attribution maps are unfaithful because they assume spatial correspondence between latent feature maps and input image space that doesn't exist in general CNNs. CNNs with multiple convolutional layers lose spatial locality, meaning features in latent space may have receptive fields spanning the entire input image. When ProtoPNet up-scales distance maps from latent space to image space, it incorrectly assumes spatial locations correspond directly, leading to attribution maps that highlight irrelevant pixels.

### Mechanism 2
ProtoPFaith restores faithfulness by computing Shapley values on prototype similarity scores rather than using up-scaled distance maps. By leveraging the linear classification layer of ProtoPNet, ProtoPFaith transforms similarity-based Shapley values into attribution w.r.t. the model prediction. This approach satisfies all required axioms (Sensitivity, Completeness, Dummy, Linearity) because Shapley values inherently fulfill these properties.

### Mechanism 3
ProtoPFaith's explanations are more discriminative because they focus on actual features that contribute to prototype similarity rather than arbitrary spatial locations. By calculating Shapley values w.r.t. the squared L2-distance between prototypes and latent features, ProtoPFaith identifies pixels that truly influence similarity scores. This results in attribution maps that highlight relevant image regions (e.g., animal contours) rather than background or unrelated areas.

## Foundational Learning

- Concept: Shapley values and their axioms (Sensitivity, Completeness, Dummy, Linearity, Symmetry-Preserving)
  - Why needed here: Understanding why Shapley values are considered faithful explanations and how they differ from other attribution methods
  - Quick check question: Can you explain why Shapley values satisfy Completeness while gradient-based methods often violate it?

- Concept: Convolutional neural network architecture and receptive fields
  - Why needed here: Recognizing why spatial locality is lost in deep CNNs and how this affects attribution methods that assume spatial correspondence
  - Quick check question: In a CNN with multiple convolutional layers, does the receptive field of a latent feature typically cover only a small local region or the entire input image?

- Concept: Case-based reasoning and prototype networks
  - Why needed here: Understanding how ProtoPNet uses prototypes for classification and why its attribution method is appealing but flawed
  - Quick check question: How does ProtoPNet use the minimum distance between prototypes and latent features to classify images?

## Architecture Onboarding

- Component map: V (Encoder) → Z (Feature Mapper) → Q (Prototype Module) → F (Classification Layer) → DASP (for ProtoPFaith explanations)

- Critical path: V → Z → Q → F (for classification); V → Z → Q → DASP (for ProtoPFaith explanations)

- Design tradeoffs:
  - ProtoPNet: Human-interpretable but unfaithful explanations due to spatial assumptions
  - ProtoPFaith: Faithful but less intuitive explanations that highlight actual contributing pixels
  - Computational cost: ProtoPFaith requires ~90 minutes per image for Shapley value computation

- Failure signatures:
  - Explanations focus on background rather than relevant image regions (ProtoPNet)
  - Prototypes collapse to few feature vectors (model still achieves high accuracy but lacks diversity)
  - Attribution maps are sparse but misleading (ProtoPNet)
  - Attribution maps are dense but highlight relevant features (ProtoPFaith)

- First 3 experiments:
  1. Verify that removing features according to ProtoPFaith attribution maps causes faster decrease in prototype similarity than removing features according to ProtoPNet maps (AOPC metric)
  2. Compare the sparsity and locality of ProtoPNet explanations vs. the granularity of ProtoPFaith explanations on a sample image from CUB-200-2011
  3. Test whether ProtoPFaith can identify when a model has learned collapsed prototypes (e.g., only one prototype per class) by examining the distribution of attribution maps

## Open Questions the Paper Calls Out

### Open Question 1
How can ProtoPFaith be extended to multi-label classification tasks?
- Basis in paper: The authors mention that ProtoPFaith is currently limited to image classification and discuss potential extensions to multi-label classification using XProtoNet
- Why unresolved: The paper does not provide a concrete implementation or experimental results for multi-label classification with ProtoPFaith
- What evidence would resolve it: A successful implementation of ProtoPFaith for multi-label classification, along with quantitative and qualitative evaluation on a suitable dataset, would demonstrate its feasibility and effectiveness in this setting

### Open Question 2
How can explanations be made both human-understandable and faithful?
- Basis in paper: The authors discuss the trade-off between human-understandable explanations (as provided by ProtoPNet) and faithful explanations (as provided by ProtoPFaith). They acknowledge that achieving both is an open challenge
- Why unresolved: There is no clear solution presented in the paper for reconciling the need for interpretability with the requirement for faithfulness in explanations
- What evidence would resolve it: A method that generates explanations that are both easily interpretable by humans and faithful to the model's decision-making process, validated through user studies and quantitative metrics, would address this open question

### Open Question 3
How does the choice of backbone architecture impact the faithfulness and quality of explanations in ProtoPFaith?
- Basis in paper: The authors evaluate ProtoPFaith on multiple backbone architectures (ConvNet, ResNet, ResNet50, WideResNet50, ResNeXt50) and observe differences in the quality of explanations
- Why unresolved: While the paper demonstrates that different backbones yield varying results, it does not provide a systematic analysis of how architectural choices influence the faithfulness and interpretability of explanations
- What evidence would resolve it: A comprehensive study comparing the performance and explanation quality of ProtoPFaith across a wide range of backbone architectures, along with an analysis of the factors contributing to these differences, would shed light on this question

## Limitations

- Computational efficiency: ProtoPFaith requires approximately 90 minutes per image for Shapley value computation, making it impractical for real-time applications
- Human interpretability: While explanations are faithful, they may be less intuitive and harder for humans to understand compared to ProtoPNet's spatial attribution maps
- Multi-label extension: The method is currently limited to single-label classification tasks, with multi-label extensions requiring additional architectural modifications

## Confidence

- Theoretical proofs of ProtoPNet's axiom violations: High
- ProtoPFaith's faithfulness guarantees: High
- Quantitative superiority on AOPC metric: High
- Qualitative interpretability improvements: Medium
- Generalizability across architectures and datasets: Medium

## Next Checks

1. **AOPC Reproducibility**: Independently verify the AOPC scores on CUB-200-2011 using the same five backbone architectures and datasets, particularly focusing on the reported factor >103 improvement over ProtoPNet

2. **User Study**: Conduct a controlled experiment where domain experts compare ProtoPNet and ProtoPFaith explanations on medical images (e.g., RSNA dataset) to assess whether the theoretically faithful explanations actually improve diagnostic reasoning

3. **Ablation Study**: Test whether the faithfulness improvements depend critically on the specific Shapley value computation method (DASP) or if simpler approximation methods yield similar results, to establish the robustness of the approach