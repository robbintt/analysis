---
ver: rpa2
title: 'Unstoppable Attack: Label-Only Model Inversion via Conditional Diffusion Model'
arxiv_id: '2307.08424'
source_url: https://arxiv.org/abs/2307.08424
tags:
- target
- attack
- training
- data
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel label-only model inversion attack method
  using a conditional diffusion model to recover representative data for a specific
  target label in the training set, given that the target model predicts only the
  label for the input. The attack model is trained on an auxiliary public dataset
  and uses the predicted label of corresponding auxiliary data as a condition to guide
  the training of the diffusion model.
---

# Unstoppable Attack: Label-Only Model Inversion via Conditional Diffusion Model

## Quick Facts
- arXiv ID: 2307.08424
- Source URL: https://arxiv.org/abs/2307.08424
- Reference count: 32
- Key outcome: Novel label-only model inversion attack using conditional diffusion model to recover representative data for specific target labels without optimization

## Executive Summary
This paper presents a novel label-only model inversion attack that recovers representative data for specific target labels in a model's training set using only predicted labels as guidance. The attack leverages a conditional diffusion model trained on an auxiliary public dataset, where the target model's predicted labels serve as conditioning signals. By inputting standard normally distributed noise and target labels into the trained model, the attacker can generate data that represents specific labels in the training set without requiring gradient access or optimization. Experimental results demonstrate superior performance compared to existing label-only inversion methods across multiple evaluation metrics including classification accuracy, feature similarity, and perceptual realism.

## Method Summary
The attack follows a two-phase approach: training and recovery. In the training phase, the attacker uses an auxiliary dataset (CelebA) with distribution similar to the target model's training data (FaceScrub) and obtains predicted labels from the target model for this auxiliary data. A conditional diffusion model (U-Net architecture) is then trained using these predicted labels as conditioning signals to guide noise prediction. During the recovery phase, the attacker inputs standard normally distributed noise along with the target label into the trained conditional diffusion model, applying a guidance strength parameter to control the tradeoff between feature fidelity and image realism. The generated samples undergo gamma correction and random transformations, with the top-k most robust samples selected for final evaluation.

## Key Results
- Achieves superior attack accuracy compared to existing label-only inversion methods (LDM, DPA, GM)
- Generates more realistic and perceptually similar images as measured by LPIPS metrics
- Demonstrates effective recovery of representative samples across 530 face classes
- Shows strong performance even without optimizing input noise during generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using predicted labels from the target model as conditioning signals enables the diffusion model to learn a label-specific data distribution without requiring gradient access.
- Mechanism: The target model's predicted labels for auxiliary data serve as a conditioning signal in the U-Net's attention layers, guiding noise prediction toward label-corresponding samples during training.
- Core assumption: The target model's predictions are sufficiently aligned with the underlying training data distribution to act as effective conditioning labels.
- Evidence anchors: [abstract] "the labels predicted by the target model are used as conditions to guide the training process"; [section 4.2.3] "we use ùêπùëä (ùë•0) as a condition to guide the generator, enabling it to target specific labels for noise prediction"
- Break condition: If target model predictions are noisy or systematically biased, the conditioning signal would mislead the diffusion model, resulting in poor quality outputs.

### Mechanism 2
- Claim: Forward diffusion gradually adds Gaussian noise until the image approaches standard normal noise, enabling reconstruction from random noise inputs.
- Mechanism: A linear variance schedule adds Gaussian noise over T steps, transforming data into standard normal distribution. During reverse, the model learns to predict and remove this noise conditioned on labels.
- Core assumption: The Markov property of diffusion allows learning the noise prediction distribution independently of the forward diffusion path.
- Evidence anchors: [section 4.2.3] "Gaussian noise is added to the auxiliary data ùë•0 T times, ultimately resulting in standard normally distributed noise"; [section 4.2.3] "Based on the first two formulas, we derive the following equation: ùëû ( ùë•ùë° | ùë•0) = N(ùë•ùë° ; ‚àö ¬Øùõºùë° ùë•0, (1 ‚àí ¬Øùõºùë° )I)"
- Break condition: If the noise schedule is non-linear or the diffusion steps are insufficient, the final distribution may not approximate standard normal, breaking the reconstruction capability.

### Mechanism 3
- Claim: Guidance strength controls the trade-off between feature fidelity and image realism by amplifying the conditioning signal during generation.
- Mechanism: The predicted noise is modified by (1 + ùúî) ùúÄùúÉ[ùë•ùë° , (ùë° + ùëô)] ‚àí ùúîùúÄùúÉ(ùë•ùë° , ùë°), where ùúî scales the label-conditioned component. Higher ùúî increases adherence to target features but reduces visual realism.
- Evidence anchors: [section 4.3.1] "we need to modify the predicted noise in Eq. (5) as follows: ùúÄùúÉ(ùë•ùë° , ùë°) ‚Üí eùúÄùúÉ(ùë•ùë° , ùë°, ùëô) = (1 + ùúî) ùúÄùúÉ[ùë•ùë° , (ùë° + ùëô)] ‚àí ùúîùúÄùúÉ(ùë•ùë° , ùë°)"; [section 6.3.4] "excessively high values of ùúî result in a degradation of image quality but in favor of target features"
- Break condition: If ùúî is too high, outputs become unrealistic and may not resemble actual training samples. If too low, outputs may lack target-specific features.

## Foundational Learning

- Concept: Conditional generation in diffusion models
  - Why needed here: The attack requires generating images conditioned on target labels without gradient access, making conditional diffusion essential.
  - Quick check question: How does conditioning affect the noise prediction distribution in a diffusion model?

- Concept: Label-only black-box threat model
  - Why needed here: Understanding the constraint that only predicted labels (not confidence scores or gradients) are accessible from the target model.
  - Quick check question: What information is available to the attacker in a label-only black-box scenario versus a white-box scenario?

- Concept: Perceptual similarity metrics (LPIPS)
  - Why needed here: Standard metrics like accuracy and feature distance may not capture visual quality; LPIPS approximates human judgment.
  - Quick check question: Why might LPIPS be more appropriate than pixel-wise MSE for evaluating generated images in model inversion attacks?

## Architecture Onboarding

- Component map: Target model -> Auxiliary dataset -> Conditional diffusion model -> Evaluation pipeline -> Random transformation and selection module

- Critical path:
  1. Train target face recognition model on FaceScrub dataset (530 classes)
  2. Collect auxiliary data (CelebA) and get predictions
  3. Train conditional diffusion model on auxiliary data with predicted labels as conditions
  4. Generate samples by sampling noise + target label + guidance strength
  5. Apply random transformations and select top-k robust samples
  6. Evaluate using multiple metrics

- Design tradeoffs:
  - Guidance strength ùúî: Higher values increase feature fidelity but reduce realism
  - Auxiliary dataset size: More data improves quality but increases training time
  - Gamma correction: Improves visual quality but may reduce feature accuracy
  - Number of generated samples: More samples improve selection robustness but increase computation

- Failure signatures:
  - Generated images lack target-specific features (guidance strength too low)
  - Images are unrealistic or contain artifacts (guidance strength too high)
  - Model fails to learn label conditioning (predicted labels too noisy)
  - Training instability (insufficient auxiliary data or poor conditioning signal)

- First 3 experiments:
  1. Train conditional diffusion model on auxiliary data with predicted labels as conditions, evaluate reconstruction quality on held-out auxiliary samples
  2. Generate samples for specific target labels with varying guidance strengths, measure accuracy vs realism tradeoff
  3. Compare generated samples against ground truth training data using LPIPS and KNN distance metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of the recovered data change when using target models with different architectures or training datasets?
- Basis in paper: [inferred] The paper mentions that they used a ResNet-18 evaluation model with different architecture from the target model, but did not assess the impact of different target models on the attack performance.
- Why unresolved: The paper focuses on demonstrating the effectiveness of the proposed attack method using a specific target model architecture and training dataset, but does not explore how the results might vary with different target models.
- What evidence would resolve it: Conducting experiments using various target model architectures and training datasets, and comparing the attack performance across these different configurations.

### Open Question 2
- Question: How does optimizing the input noise during the recovery phase affect the quality of the recovered data?
- Basis in paper: [inferred] The paper mentions that they did not optimize the input noise during the recovery phase to highlight the power of the conditional diffusion model, but acknowledges that optimization might improve the results when using a higher-quality generator.
- Why unresolved: The paper intentionally avoids optimizing the input noise to emphasize the capabilities of the conditional diffusion model, but does not explore the potential benefits of optimization.
- What evidence would resolve it: Performing experiments with and without input noise optimization, and comparing the quality of the recovered data in each case.

### Open Question 3
- Question: How can we design effective defenses against label-only black-box model inversion attacks?
- Basis in paper: [explicit] The paper discusses the challenges of designing defenses against label-only black-box attacks and suggests that future defenses may involve handling the output label or confidence level of the target model and combining relevant defense mechanisms with advanced deep learning models.
- Why unresolved: The paper acknowledges the difficulty of defending against label-only black-box attacks but does not provide specific defense mechanisms or solutions.
- What evidence would resolve it: Developing and evaluating various defense strategies against label-only black-box attacks, and assessing their effectiveness in protecting the privacy of the target model's training data.

## Limitations

- Effectiveness heavily depends on availability of high-quality auxiliary dataset with similar distribution to target training data
- Computational overhead of training conditional diffusion models and generating multiple samples may limit practical applicability
- Significant performance degradation when auxiliary data is too dissimilar from target training distribution

## Confidence

- **High Confidence**: The mechanism of using predicted labels as conditioning signals in the diffusion model is well-established in the literature
- **Medium Confidence**: Empirical results showing improved performance over existing methods are convincing but limited comparison scope
- **Low Confidence**: The claim of "unstoppable" effectiveness is overstated given practical limitations and dependency on auxiliary data quality

## Next Checks

1. **Robustness to Auxiliary Data Quality**: Systematically evaluate attack performance across auxiliary datasets with varying degrees of distribution mismatch from the target training data, measuring performance degradation as domain gap increases.

2. **Cross-Model Generalization**: Test whether the conditional diffusion model trained on predictions from one target model can successfully invert another model trained on the same data distribution, validating capture of general data distribution patterns.

3. **Adversarial Defense Evaluation**: Investigate whether common defense mechanisms against model inversion (data augmentation, differential privacy, model architecture modifications) affect the attack's success rate, providing insight into practical vulnerability.