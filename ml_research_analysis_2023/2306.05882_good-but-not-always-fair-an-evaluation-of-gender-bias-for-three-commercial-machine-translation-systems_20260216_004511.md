---
ver: rpa2
title: 'Good, but not always Fair: An Evaluation of Gender Bias for three commercial
  Machine Translation Systems'
arxiv_id: '2306.05882'
source_url: https://arxiv.org/abs/2306.05882
tags:
- gender
- translation
- systems
- bias
- feminine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a meticulous assessment of three commercial
  MT systems - Google Translate, DeepL, and ModernMT - with a specific focus on gender
  translation and bias. For three language pairs (English/Spanish, English/Italian,
  and English/French), we scrutinize the behavior of such systems at several levels
  of granularity and on a variety of naturally occurring gender phenomena in translation.
---

# Good, but not always Fair: An Evaluation of Gender Bias for three commercial Machine Translation Systems

## Quick Facts
- arXiv ID: 2306.05882
- Source URL: https://arxiv.org/abs/2306.05882
- Reference count: 0
- Key outcome: This paper presents a meticulous assessment of three commercial MT systems - Google Translate, DeepL, and ModernMT - with a specific focus on gender translation and bias. For three language pairs (English/Spanish, English/Italian, and English/French), we scrutinize the behavior of such systems at several levels of granularity and on a variety of naturally occurring gender phenomena in translation. Our study takes stock of the current state of online MT tools, by revealing significant discrepancies in the gender translation of the three systems, with each system displaying varying degrees of bias despite their overall translation quality.

## Executive Summary
This paper evaluates gender bias in three commercial machine translation systems (Google Translate, DeepL, and ModernMT) across three language pairs using the MuST-SHE benchmark. The study reveals significant discrepancies in gender translation performance, with each system displaying varying degrees of bias despite their overall translation quality. The evaluation covers multiple dimensions including gender accuracy, term coverage, and BLEU scores, disaggregated by gender form, category, and part of speech.

## Method Summary
The study evaluates three commercial MT systems (Google Translate, DeepL, and ModernMT) using the MuST-SHE benchmark dataset for English→Spanish, English→Italian, and English→French translation. The evaluation pipeline automatically translates MuST-SHE source sentences through each system's API and computes gender accuracy, term coverage, and BLEU scores. The analysis is disaggregated by gender form (masculine/feminine), category (CAT1/CAT2), and part of speech to reveal systematic bias patterns in gender translation.

## Key Results
- Gender translation accuracy is systematically lower for feminine forms, with masculine default generalization prevalent across all three systems
- Overall translation quality (BLEU) does not correlate with gender translation accuracy, indicating that high general fluency does not ensure gender-fair outputs
- Gender bias is most pronounced in ambiguous gender cases (CAT1), where masculine translations occur at rates up to 94.6% while feminine realizations drop as low as 9.2%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gender translation accuracy is systematically lower for feminine forms due to default masculine generalization in neural MT models.
- Mechanism: When faced with ambiguous gender references (e.g., neutral source terms like "a friend"), the model defaults to masculine output, learned from training data bias where masculine forms dominate.
- Core assumption: Training corpora contain more masculine than feminine gender references, leading models to internalize a masculine bias.
- Evidence anchors:
  - [abstract] "each system displaying varying degrees of bias despite their overall translation quality."
  - [section] "systems are skewed toward the generation of masculine forms by default (Schiebinger, 2014; Vanmassenhove et al., 2018; Monti, 2020)"
  - [corpus] "Top related titles: FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity" (suggests systemic bias issues across MT systems)
- Break condition: If balanced training data or targeted mitigation techniques (e.g., gender-filtered self-training) are applied, the masculine default bias can be reduced.

### Mechanism 2
- Claim: Overall translation quality (BLEU) does not correlate with gender translation accuracy.
- Mechanism: BLEU evaluates general fluency and adequacy but does not penalize incorrect gender forms, allowing high BLEU scores even with biased gender outputs.
- Core assumption: BLEU's n-gram overlap metric treats masculine and feminine forms as equivalent if either matches the reference, masking gender bias.
- Evidence anchors:
  - [abstract] "we consistently find that i) overall translation quality does not directly relate to gender translation ability"
  - [section] "systems with higher BLEU tend to have higher coverage... For gender translation, however, we do not find the same pattern."
  - [corpus] "Don't Overlook the Grammatical Gender: Bias Evaluation for Hindi-English Machine Translation" (indicates gender bias persists despite quality metrics)
- Break condition: If gender-sensitive evaluation metrics (like Gender Accuracy) are used, the lack of correlation becomes evident.

### Mechanism 3
- Claim: Gender bias is most pronounced in ambiguous gender cases (CAT1) compared to disambiguated cases (CAT2).
- Mechanism: When contextual gender cues are absent (CAT1), models default to masculine; when cues are present (CAT2), they can leverage context to produce correct gender.
- Core assumption: Neural MT models rely on local context to resolve gender ambiguity, failing when such context is missing.
- Evidence anchors:
  - [section] "CAT1: first-person references that are translated in accordance to the speaker's linguistic expression of gender... CAT2: references translated in concordance with explicit gender cues"
  - [section] "it is on phenomena from CAT1 that gender bias predominantly emerges: masculine translation can be correct at a rate as high as 94.6%... feminine realization as low as 9.2%"
  - [corpus] "Gender Inflected or Bias Inflicted: On Using Grammatical Gender Cues for Bias Evaluation in Machine Translation" (suggests cue reliance is critical)
- Break condition: If models are trained with explicit gender disambiguation strategies, CAT1 bias can be mitigated.

## Foundational Learning

- Concept: Gender expression differences across languages (grammatical vs. natural gender)
  - Why needed here: Understanding why ambiguous gender translation occurs requires knowing how languages mark gender differently.
  - Quick check question: What is the main difference between "grammatical gender" and "natural gender" languages in how they express gender?

- Concept: Neural MT model training and bias propagation
  - Why needed here: Gender bias in translation stems from biased training data; knowing how models learn from data is essential.
  - Quick check question: How do neural MT models acquire and reproduce gender biases present in their training corpora?

- Concept: Evaluation metrics for MT quality vs. gender bias
  - Why needed here: Distinguishing between overall translation quality (BLEU) and gender-specific accuracy is crucial for proper assessment.
  - Quick check question: Why might a high BLEU score not indicate good gender translation performance?

## Architecture Onboarding

- Component map: Source sentence → Neural MT model (encoder-decoder with attention) → Target generation → Evaluation pipeline (BLEU + gender-sensitive metrics)
- Critical path: Source sentence processing → Context encoding → Target generation (with gender selection) → Evaluation against reference
- Design tradeoffs: High BLEU often prioritizes fluency over gender accuracy; gender disambiguation may reduce overall fluency
- Failure signatures: Masculine-skewed outputs in ambiguous cases, low feminine accuracy despite high overall BLEU, verbs missing gender agreement
- First 3 experiments:
  1. Compare masculine vs. feminine accuracy on CAT1 vs. CAT2 to quantify context dependence
  2. Analyze POS-level accuracy to identify most biased word categories (e.g., nouns vs. verbs)
  3. Test gender disambiguation techniques (e.g., prompts or explicit gender tags) on ambiguous inputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the gender bias mitigation strategies of Google Translate, DeepL, and ModernMT differ in their implementation and effectiveness?
- Basis in paper: [explicit] The paper mentions that Google Translate provides two gendered translation alternatives for ambiguous inputs since 2018, DeepL offers multiple translation suggestions, and ModernMT has not publicly set out any steps towards making their systems more gender-inclusive.
- Why unresolved: The paper does not provide a detailed comparison of the effectiveness of these strategies or how they are implemented technically.
- What evidence would resolve it: Comparative studies or technical documentation detailing the implementation and impact of each system's gender bias mitigation strategies would be needed.

### Open Question 2
- Question: To what extent do the training data and architectural choices of neural machine translation systems contribute to gender bias, and how can these be modified to reduce bias?
- Basis in paper: [inferred] The paper discusses that MT models are trained on vast amounts of parallel data, which may contain gender imbalances, and mentions that several bias mitigation strategies have been proposed, including technical interventions on architectural choices and systems training data.
- Why unresolved: The paper does not explore the specific contributions of training data and architectural choices to gender bias or provide detailed strategies for modifying these to reduce bias.
- What evidence would resolve it: Empirical studies analyzing the impact of different training data compositions and architectural choices on gender bias in MT systems, along with proposed modifications, would be required.

### Open Question 3
- Question: How does the performance of machine translation systems on gender translation vary across different language pairs, and what linguistic factors contribute to these variations?
- Basis in paper: [explicit] The paper evaluates three language pairs (English→Spanish, English→Italian, and English→French) and finds differences in gender translation performance across these pairs.
- Why unresolved: The paper does not provide a detailed analysis of the linguistic factors that contribute to variations in gender translation performance across different language pairs.
- What evidence would resolve it: Comparative linguistic studies and empirical evaluations across a wider range of language pairs would be needed to identify the linguistic factors influencing gender translation performance.

## Limitations
- The evaluation is constrained by the static nature of commercial MT APIs evaluated in November 2022, with no assessment of how quickly these systems adapt to gender bias mitigation efforts over time.
- The study relies entirely on the MuST-SHE benchmark, which may not capture all real-world gender translation scenarios, particularly domain-specific contexts or non-binary gender expressions.
- The evaluation methodology assumes binary gender categories, potentially overlooking non-binary gender representations and misgendering risks.

## Confidence

- **High Confidence**: The finding that gender translation accuracy is systematically lower for feminine forms due to masculine default generalization, supported by consistent patterns across all three MT systems and aligned with established research on corpus bias.
- **Medium Confidence**: The claim that overall BLEU score does not correlate with gender translation accuracy, as this relationship may vary with different reference standards or evaluation metrics beyond the MuST-SHE benchmark.
- **Medium Confidence**: The observation that gender bias is most pronounced in ambiguous cases (CAT1), though this may be influenced by the specific construction of the MuST-SHE dataset and the definition of "ambiguous" contexts.

## Next Checks
1. **Temporal Validation**: Re-evaluate the same MT systems at quarterly intervals to assess how quickly gender bias mitigation efforts are implemented and whether the identified biases persist or diminish over time.

2. **Cross-Dataset Validation**: Test the three MT systems on additional gender bias evaluation datasets (e.g., FairTranslate, WinoMT) to determine if the observed patterns of masculine default bias and low feminine accuracy are consistent across different benchmark constructions and language pairs.

3. **User-Centered Validation**: Conduct human evaluation studies where bilingual speakers assess the naturalness and appropriateness of gender translations, particularly for CAT1 ambiguous cases, to determine if machine-generated masculine defaults align with human translation preferences in real-world contexts.