---
ver: rpa2
title: Multi-Objective Bayesian Optimization with Active Preference Learning
arxiv_id: '2311.13460'
source_url: https://arxiv.org/abs/2311.13460
tags:
- preference
- function
- learning
- optimization
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multi-objective optimization where a decision
  maker needs a specific preferred solution rather than the full Pareto front. The
  authors propose a Bayesian optimization approach that combines GP surrogate models
  for objective functions with a Bayesian preference model for the decision maker.
---

# Multi-Objective Bayesian Optimization with Active Preference Learning

## Quick Facts
- arXiv ID: 2311.13460
- Source URL: https://arxiv.org/abs/2311.13460
- Reference count: 40
- Key outcome: Bayesian optimization approach for MOO with preference learning using GP surrogates and active query selection

## Executive Summary
This paper addresses multi-objective optimization where a decision maker seeks a specific preferred solution rather than the full Pareto front. The authors propose a Bayesian optimization framework that combines GP surrogate models for objective functions with a Bayesian preference model for the decision maker. Two types of weak supervision are used: pairwise comparisons and improvement requests. An acquisition function based on expected improvement incorporates uncertainty in both objectives and preferences, while active learning using mutual information reduces interaction cost. Experiments on benchmark functions and hyperparameter optimization problems show the proposed method outperforms alternatives in identifying preferred solutions efficiently.

## Method Summary
The method employs Gaussian process regression for multi-objective functions and a Chebyshev scalarization function (CSF) with Dirichlet prior for the preference model. Expected improvement acquisition incorporates uncertainty from both the objective GPs and preference weights. Active learning selects queries that maximize mutual information about the preference parameter. The approach is evaluated on synthetic benchmark functions (DTLZ1, DTLZ3, Kursawe, Schaffer2) and real-world hyperparameter optimization tasks, comparing against random sampling, MOBO-RS, and EI-UU baselines.

## Key Results
- The proposed method achieves lower simple regret than baselines across benchmark functions
- Active learning via mutual information reduces the number of queries needed for accurate preference learning
- The CSF-based utility function performs comparably to or better than a monotonic GP-based utility function

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining GP surrogate models for objectives with a Bayesian preference model allows simultaneous uncertainty propagation over both function values and DM preference during optimization.
- **Mechanism:** The EI acquisition function is redefined as an expectation over both the GP posterior for objectives and the posterior of the preference parameter vector `w`. This captures epistemic uncertainty in both models, enabling exploration that accounts for unknown DM preferences.
- **Core assumption:** Both the objective functions and the DM preference can be modeled as Bayesian posteriors (GPs for objectives, Dirichlet for `w`).
- **Evidence anchors:**
  - [abstract] "acquisition function based on expected improvement incorporates uncertainty in both objectives and preferences."
  - [section 3.3.1] "Unlike the standard EI in BO, the expectation is jointly taken over f(x) and w..."
  - [corpus] Weak evidence; no direct mention of joint uncertainty propagation.
- **Break condition:** If either model's posterior cannot be approximated well (e.g., high noise, sparse data), the EI expectation becomes unreliable and optimization may stall or diverge.

### Mechanism 2
- **Claim:** Active learning via mutual information reduces DM interaction cost by selecting queries that maximize information gain about the preference parameter.
- **Mechanism:** For pairwise comparisons, mutual information MI(zPC; w) is computed; queries that maximize MI are selected. This is an information-theoretic variant of BALD that measures how much a query reduces entropy in `w`.
- **Core assumption:** DM responses are informative and reduce posterior entropy in `w` in a predictable way (Bernoulli likelihood for PC, categorical for IR).
- **Evidence anchors:**
  - [abstract] "Active learning using mutual information reduces interaction cost."
  - [section 3.3.2] "Our AL acquisition function is based on the Bayesian active learning framework called BALD..."
  - [corpus] Weak evidence; no explicit MI-based query selection in cited papers.
- **Break condition:** If DM responses are noisy or inconsistent, MI may select uninformative queries, leading to inefficient learning or preference model divergence.

### Mechanism 3
- **Claim:** The Chebyshev scalarization function (CSF) with a preference vector `w` enables identification of any Pareto-optimal point depending on `w`.
- **Mechanism:** The utility U(f) = min_{ℓ∈[L]} f_ℓ / w_ℓ with w satisfying ∑w_ℓ = 1 ensures that by adjusting `w`, any Pareto-optimal solution can be recovered. This avoids the limitation of linear scalarizations where some Pareto points are unreachable.
- **Core assumption:** The true utility is monotonic in each objective and the CSF correctly represents DM tradeoffs.
- **Evidence anchors:**
  - [abstract] "We employ a Chebyshev scalarization function (CSF) based parametrized utility function..."
  - [section 3.2] "In the MOO literature [Giagkiozis and Fleming, 2015], it is known that for any Pareto optimal solution f⋆, there exist a weighting vector w under which the maximizer x of (2) derives f⋆ = f(x)."
  - [corpus] Weak evidence; no explicit CSF discussion in cited papers.
- **Break condition:** If the true utility is non-monotonic or not representable by CSF, the approach may misidentify preferred solutions.

## Foundational Learning

- **Concept:** Gaussian Process regression for multi-output surrogate modeling.
  - **Why needed here:** GPs provide both mean predictions and uncertainty estimates for expensive black-box objectives, which are critical inputs for Bayesian optimization.
  - **Quick check question:** How does GP uncertainty influence the expected improvement calculation in BO?
- **Concept:** Dirichlet distribution as a prior for preference parameters under a simplex constraint.
  - **Why needed here:** The preference weights w must sum to 1 and be positive; Dirichlet naturally encodes this constraint.
  - **Quick check question:** What happens to the posterior if a Dirichlet prior is replaced by a uniform prior over the simplex?
- **Concept:** Mutual information as a query selection criterion.
  - **Why needed here:** MI quantifies expected information gain from a query, guiding efficient preference learning.
  - **Quick check question:** How does MI-based query selection differ from uncertainty sampling in active learning?

## Architecture Onboarding

- **Component map:** GP models (one per objective) -> surrogate predictions + uncertainty -> Preference model (CSF with Dirichlet prior) -> utility function U(f) -> EI acquisition with joint expectation over f(x) and w -> Active learning module (MI-based) -> query selection -> BO loop: select x -> evaluate f(x) -> update GPs -> update preference model -> repeat
- **Critical path:** GP fitting -> EI computation with joint expectation -> query selection (active learning or random) -> evaluation -> posterior updates
- **Design tradeoffs:**
  - GP vs. other surrogates: GP gives uncertainty but scales poorly with data
  - CSF vs. GP-based utility: CSF is interpretable but limited; GP-based is flexible but needs more data
  - MI-based AL vs. random: MI reduces queries but is computationally heavier
- **Failure signatures:**
  - EI values become NaN or extremely large -> check GP hyperparameters or posterior sampling
  - Preference model posterior does not change -> check likelihood formulation or DM consistency
  - Optimization converges to non-Pareto points -> check monotonicity of utility or CSF correctness
- **First 3 experiments:**
  1. **Toy MOO test:** 2 objectives, grid search f(x) values, simulate DM responses, verify EI selects informative points
  2. **Active learning ablation:** Run with and without MI-based query selection; measure number of queries to reach fixed preference accuracy
  3. **Preference model flexibility:** Replace CSF with GP-based utility; compare performance on benchmark with non-CFS true utility

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of utility function (e.g., CSF vs. monotonic GP) impact the efficiency of identifying the preferred solution in multi-objective Bayesian optimization?
- Basis in paper: [explicit] The paper compares the performance of the CSF-based utility function with a monotonic GP-based utility function (PGPM) in experiments (§ 5.2.4).
- Why unresolved: While the paper shows that PGPM outperforms CSF in some cases, it doesn't provide a comprehensive analysis of the trade-offs between different utility functions or guidelines for choosing the appropriate function for specific problems.
- What evidence would resolve it: A systematic study comparing the performance of various utility functions (e.g., CSF, augmented CSF, monotonic GP, linear scalarization) across a diverse set of multi-objective optimization problems with different Pareto front characteristics.

### Open Question 2
- Question: How sensitive is the proposed method to the choice of hyperparameters, such as the noise variance in the preference model (σPC, σIR) and the kernel parameters for the GPs?
- Basis in paper: [inferred] The paper mentions that hyperparameters are optimized by marginal likelihood maximization, but it doesn't explore the sensitivity of the method to these choices or provide guidelines for setting them.
- Why unresolved: The performance of the proposed method could be significantly affected by the choice of hyperparameters, but the paper doesn't investigate this aspect.
- What evidence would resolve it: A sensitivity analysis showing how the performance of the proposed method varies with different choices of hyperparameters, and guidelines for setting these parameters based on problem characteristics.

### Open Question 3
- Question: How does the proposed method scale to high-dimensional input and output spaces in multi-objective optimization problems?
- Basis in paper: [inferred] The paper evaluates the method on problems with up to 10 objectives (L = 10) but doesn't explore its performance on higher-dimensional problems or discuss potential scalability issues.
- Why unresolved: The proposed method involves modeling the objective functions and the utility function using GPs, which can become computationally expensive in high-dimensional spaces.
- What evidence would resolve it: Experiments evaluating the performance of the proposed method on multi-objective optimization problems with high-dimensional input and output spaces, along with an analysis of the computational complexity and potential strategies for improving scalability.

### Open Question 4
- Question: How does the proposed method handle constraints in multi-objective optimization problems, such as feasibility constraints or preference constraints?
- Basis in paper: [inferred] The paper focuses on unconstrained multi-objective optimization problems and doesn't discuss how to handle constraints within the proposed framework.
- Why unresolved: Real-world multi-objective optimization problems often involve constraints, and it's unclear how the proposed method can be extended to handle such cases.
- What evidence would resolve it: A discussion of how the proposed method can be adapted to handle different types of constraints in multi-objective optimization, along with experimental results demonstrating the effectiveness of the extended method.

## Limitations

- The mutual information computation for active learning may not scale well to high-dimensional objective spaces
- The method assumes noiseless preference responses, which is unrealistic in practice
- The CSF utility function assumes monotonic preferences, limiting applicability to problems with non-monotonic tradeoffs

## Confidence

- Medium: GP-based surrogate modeling and EI acquisition
- Medium: Preference model with CSF and Dirichlet prior
- Low: Active learning via mutual information

## Next Checks

1. Implement runtime analysis of mutual information computation for varying L and budget
2. Test method with noisy preference responses and measure degradation in preference learning
3. Compare against uncertainty sampling and query-by-committee baselines on real-world hyperparameter optimization tasks