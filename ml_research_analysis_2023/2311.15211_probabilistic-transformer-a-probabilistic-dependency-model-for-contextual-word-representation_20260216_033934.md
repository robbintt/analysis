---
ver: rpa2
title: 'Probabilistic Transformer: A Probabilistic Dependency Model for Contextual
  Word Representation'
arxiv_id: '2311.15211'
source_url: https://arxiv.org/abs/2311.15211
tags:
- word
- variables
- probabilistic
- dependency
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces probabilistic transformers, a syntactic and
  probabilistic approach to contextual word representation. It models discrete latent
  representations of words and dependency arcs using a conditional random field, with
  mean field variational inference for approximate inference.
---

# Probabilistic Transformer: A Probabilistic Dependency Model for Contextual Word Representation

## Quick Facts
- arXiv ID: 2311.15211
- Source URL: https://arxiv.org/abs/2311.15211
- Authors: 
- Reference count: 40
- Key outcome: Probabilistic transformers achieve competitive performance to standard transformers on small to medium datasets with fewer parameters by modeling syntactic dependencies through discrete latent representations

## Executive Summary
This paper introduces probabilistic transformers, a novel approach to contextual word representation that combines probabilistic modeling with syntactic dependency structures. The model uses a conditional random field to represent discrete latent labels for words and dependency arcs, with mean field variational inference for approximate inference. The computation graph closely resembles standard transformers, with correspondences between dependencies and self-attention, and between latent distributions and contextual embeddings. Experiments on multiple tasks show the model achieves competitive performance to transformers while using fewer parameters, suggesting potential for more efficient and interpretable neural architectures.

## Method Summary
Probabilistic transformers model sentences using a conditional random field with discrete latent variables representing word labels and dependency heads. The model defines unary potentials for word-label compatibility and ternary potentials for dependency relationships, with distance encoding via a clip function. Mean field variational inference performs approximate inference through iterative message passing between variables. The inference procedure closely resembles transformer operations, with single-channel updates corresponding to scaled dot-product attention and multi-channel updates to multi-head attention. The posterior distributions over latent labels serve as contextual word representations for downstream tasks.

## Key Results
- Probabilistic transformers achieve comparable performance to standard transformers on small to medium datasets (PTB, CoNLL-2003, SST) while using significantly fewer parameters
- The model demonstrates strong performance on syntactic compositional generalization (COGS) task, suggesting effective capture of syntactic structure
- Computational experiments show clear correspondence between the model's inference procedure and standard transformer operations, with unary potentials analogous to query/key/value matrices and ternary potentials analogous to output projection matrices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probabilistic transformers model syntactic dependency structures through discrete latent representations, producing contextual word representations that capture both syntactic and semantic properties.
- Mechanism: The model uses a conditional random field (CRF) with discrete latent label variables Zi for each word and dependency head variables Hi, modeling dependencies through ternary potential functions that evaluate label compatibility between connected words.
- Core assumption: Discrete latent representations of words are sufficient for capturing syntactic and semantic properties needed for contextual word representation.
- Evidence anchors:
  - [abstract] "Specifically, we design a conditional random field that models discrete latent representations of all words in a sentence as well as dependency arcs between them"
  - [section] "For the i-th word, we define Zi as a discrete latent label that represents the syntactic (and possibly semantic) property of the word in the sentence"
  - [corpus] Weak - the corpus mentions related work on dependency parsing but doesn't provide direct evidence for discrete representations sufficiency
- Break condition: If continuous representations prove significantly more effective than discrete ones for capturing complex syntactic relationships, or if the label set size becomes impractically large for rich semantic distinctions.

### Mechanism 2
- Claim: Mean field variational inference (MFVI) enables efficient approximate inference in the probabilistic transformer model, producing probability distributions over latent representations that serve as contextual word embeddings.
- Mechanism: MFVI iteratively updates posterior distributions over Z variables (latent labels) and H variables (dependency heads) through message passing, with distributions over Z variables serving as contextual representations.
- Core assumption: MFVI provides sufficiently accurate approximations of the true posterior distributions for practical use in downstream tasks.
- Evidence anchors:
  - [abstract] "we use mean field variational inference for approximate inference"
  - [section] "We use mean field variational inference (MFVI) to perform approximate inference"
  - [corpus] Weak - the corpus doesn't provide empirical evidence about MFVI's effectiveness for this specific model
- Break condition: If the approximation error from MFVI becomes too large for the model to perform competitively on downstream tasks, or if exact inference becomes computationally feasible.

### Mechanism 3
- Claim: The computation graph of probabilistic transformers closely resembles that of standard transformers, with correspondences between dependencies and self-attention, and between latent distributions and contextual embeddings.
- Mechanism: The MFVI inference procedure involves computations that parallel transformer operations: single-channel updates correspond to scaled dot-product attention, and multi-channel updates correspond to multi-head attention.
- Core assumption: The mathematical operations in MFVI inference can be mapped to transformer operations despite the different probabilistic foundations.
- Evidence anchors:
  - [abstract] "we find that the computation graph of our model resembles transformers, with correspondences between dependencies and self-attention and between distributions over latent representations and contextual embeddings of words"
  - [section] "we show that there is a striking resemblance between the computation graph of the inference procedure of our model and that of a transformer"
  - [corpus] Strong - the corpus provides detailed mathematical correspondence showing how single-channel updates match scaled dot-product attention
- Break condition: If the mathematical correspondence breaks down under certain parameterizations or if the performance advantage disappears when using the transformer-like computation graph.

## Foundational Learning

- Concept: Conditional Random Fields (CRFs)
  - Why needed here: CRFs provide the probabilistic framework for modeling dependencies between words and their latent representations in a sentence.
  - Quick check question: How does a CRF differ from a standard Markov Random Field in terms of dependency structure?

- Concept: Mean Field Variational Inference (MFVI)
  - Why needed here: MFVI enables efficient approximate inference in the probabilistic model by iteratively updating posterior distributions over latent variables.
  - Quick check question: What is the key assumption made in MFVI about the form of the approximate posterior distribution?

- Concept: Dependency Parsing
  - Why needed here: Understanding dependency parsing concepts is crucial for interpreting the model's latent variables and their role in capturing syntactic structure.
  - Quick check question: In head-selection dependency parsing, what constraint is typically enforced that this model relaxes?

## Architecture Onboarding

- Component map:
  - CRF Model: Core probabilistic framework with latent variables Z (word labels) and H (dependency heads)
  - Potential Functions: Unary functions for word-label compatibility, ternary functions for dependency relationships
  - Inference Engine: MFVI with message passing between variables
  - Distance Encoding: Clip function for relative positional encoding
  - Parameter Reduction: Tensor decomposition for ternary score matrices
  - Output Layer: Posterior distributions over Z variables as contextual representations

- Critical path: Input sentence → Unary and ternary potentials → MFVI inference iterations → Final posterior distributions over Z variables → Contextual word representations

- Design tradeoffs:
  - Discrete vs. continuous latent representations: Discrete chosen for computational simplicity and parameter efficiency
  - Synchronous vs. asynchronous update: Asynchronous chosen to prevent uniform distributions in early iterations
  - Shared vs. separate parameters across channels: Shared parameters reduce model size but may limit channel specialization

- Failure signatures:
  - Uniform distributions over dependency heads indicate message imbalance issues
  - Poor performance on large datasets suggests scalability limitations
  - High perplexity on masked language modeling indicates weak contextual representation quality

- First 3 experiments:
  1. Ablation study removing distance encoding to quantify its impact on performance
  2. Comparison of synchronous vs. asynchronous update strategies on convergence speed
  3. Test different tensor decomposition strategies (UV vs. UVW) on parameter efficiency and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of probabilistic transformers scale to larger datasets (>100k sentences)?
- Basis in paper: [inferred] The paper states that preliminary experiments on larger datasets show significant underperformance compared to transformers.
- Why unresolved: The paper only mentions preliminary experiments on larger datasets without providing detailed results or analysis.
- What evidence would resolve it: Detailed experimental results comparing probabilistic transformers and transformers on larger datasets, along with analysis of the potential causes for any performance gap.

### Open Question 2
- Question: How do the global variables in the "all-dep" and "dep-split" settings affect the performance and interpretability of probabilistic transformers?
- Basis in paper: [explicit] The paper introduces these variants but does not provide experimental results or detailed analysis of their effects.
- Why unresolved: The paper only presents the model designs without empirical evaluation or discussion of their impact.
- What evidence would resolve it: Experimental results comparing the performance of these variants with the base model, along with analysis of how the global variables influence the learned representations and dependency structures.

### Open Question 3
- Question: Can the connections between probabilistic transformers and transformers be leveraged to develop new insights or extensions for either model?
- Basis in paper: [explicit] The paper highlights the striking resemblance between the computation graphs of probabilistic transformers and transformers, suggesting potential for mutual insights.
- Why unresolved: The paper does not explore the implications of this connection beyond the initial observation.
- What evidence would resolve it: Research demonstrating how insights from probabilistic transformers can inform the design of transformers, or vice versa, leading to improved performance or interpretability.

## Limitations
- The model's performance on large datasets (>100k sentences) remains untested, with preliminary results suggesting significant underperformance compared to standard transformers
- The discrete latent representation space may be too constrained to capture rich semantic properties beyond basic syntactic relationships
- The computational efficiency gains from using discrete representations versus continuous embeddings need further investigation in real-world applications

## Confidence

- High confidence in the mathematical correspondence between probabilistic transformer inference and standard transformer operations, supported by detailed derivations in the paper
- Medium confidence in the model's competitive performance on small to medium datasets, though direct comparison with larger-scale transformer models is limited
- Low confidence in the model's effectiveness for capturing rich semantic properties beyond basic syntactic relationships, as the discrete label space may be too constrained

## Next Checks

1. Scale up experiments to larger datasets (e.g., GLUE, SuperGLUE) to evaluate performance degradation and computational efficiency at scale
2. Conduct ablation studies systematically removing each component (distance encoding, tensor decomposition, discrete representations) to quantify their individual contributions
3. Compare the model's semantic representation quality against continuous embedding approaches using probing tasks that evaluate semantic phenomena like coreference, synonymy, and semantic role labeling