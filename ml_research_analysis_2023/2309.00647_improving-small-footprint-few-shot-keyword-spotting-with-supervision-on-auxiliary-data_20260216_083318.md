---
ver: rpa2
title: Improving Small Footprint Few-shot Keyword Spotting with Supervision on Auxiliary
  Data
arxiv_id: '2309.00647'
source_url: https://arxiv.org/abs/2309.00647
tags:
- learning
- data
- keyword
- spotting
- auxiliary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of few-shot keyword spotting
  (FS-KWS), where only limited examples are available for each keyword. The authors
  propose a framework that leverages easily collectible, unlabeled reading speech
  data as an auxiliary source.
---

# Improving Small Footprint Few-shot Keyword Spotting with Supervision on Auxiliary Data

## Quick Facts
- arXiv ID: 2309.00647
- Source URL: https://arxiv.org/abs/2309.00647
- Authors: 
- Reference count: 0
- Primary result: Proposed AuxSL framework improves FS-KWS performance by 16% relative improvement in both closed- and open-set settings using 5-shot samples

## Executive Summary
This paper addresses the challenge of few-shot keyword spotting (FS-KWS) where only limited examples are available for each keyword. The authors propose a multi-task learning framework called AuxSL that leverages easily collectible, unlabeled reading speech data as an auxiliary source. They construct a balanced keyword dataset called LibriWord from audiobooks and use supervised learning on this auxiliary data alongside few-shot learning on in-domain command data. Experiments demonstrate significant improvements over competitive methods, achieving 16% relative improvement in both closed- and open-set settings when using 5-shot samples.

## Method Summary
The proposed method constructs an auxiliary dataset (LibriWord) from reading speech by automatically annotating and filtering Librispeech data, creating a balanced keyword dataset with 1,000 keywords and 300 samples each. The AuxSL framework incorporates multi-task learning with separate classifiers for in-domain command-like data and out-of-domain auxiliary data, using a supervised loss function for the auxiliary data. This approach prevents representation skew when sharing feature extractors between domains and enhances representation power from auxiliary data while primarily learning keyword representations on in-domain data.

## Key Results
- 16% relative improvement in closed-set accuracy using 5-shot samples
- 16% relative improvement in open-set AUROC using 5-shot samples
- AuxSL outperforms competitive methods including D-ProtoNets, MTL with SSL, and MTL with KD
- Balanced auxiliary dataset (LibriWord) shows superior performance compared to imbalanced or full Librispeech datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task learning with an additional classifier (AuxSL) improves representation power from out-of-domain auxiliary data
- Mechanism: Separate classifier and supervised loss function for auxiliary data prevents representation skew when sharing feature extractors
- Core assumption: Domain gap between command-like and reading speech data requires separate learning paths
- Evidence anchors: Abstract mentions AuxSL incorporating additional classifier with supervised loss; section explains multi-task objectives LAuxSL = LFSL + 位LSL

### Mechanism 2
- Claim: Creating balanced keyword dataset (LibriWord) from reading speech improves performance for small models
- Mechanism: Balanced dataset addresses imbalance in natural reading speech, providing equal keyword representation
- Core assumption: Imbalanced training data negatively impacts small model learning
- Evidence anchors: Section shows balanced number of samples per keyword leads to superior performance; balanced training aids robust feature embeddings

### Mechanism 3
- Claim: Supervised learning on auxiliary data is more effective than self-supervised learning for small-footprint models
- Mechanism: Avoids capacity requirements of SSL methods, using direct supervision instead
- Core assumption: SSL methods require larger model capacity to be effective
- Evidence anchors: Abstract states SSL suitable for large models, not practical for small FS-KWS; section shows MTL with SSL and KD achieve limited improvement

## Foundational Learning

- Concept: Few-shot learning and metric learning
  - Why needed here: Task requires recognizing keywords with limited examples, making few-shot techniques essential
  - Quick check question: How does ProtoNets create prototypes and use them for classification in N-way K-shot setting?

- Concept: Domain adaptation and transfer learning
  - Why needed here: Framework needs to leverage auxiliary data from different domain (reading speech) to improve target task
  - Quick check question: What are key differences between command-like and reading speech data requiring domain adaptation?

- Concept: Multi-task learning and loss balancing
  - Why needed here: Combines learning from two different datasets with different characteristics
  - Quick check question: How does balancing parameter 位 affect trade-off between in-domain and out-of-domain learning?

## Architecture Onboarding

- Component map: Feature extractor (BC-ResNet8) -> In-domain loss module (metric learning) -> Out-of-domain loss module (supervised cross-entropy) -> Balancing parameter 位

- Critical path: 1) Feature extraction from both datasets, 2) In-domain metric learning loss calculation, 3) Out-of-domain supervised loss calculation, 4) Loss combination with balancing parameter, 5) Backpropagation through shared feature extractor

- Design tradeoffs: Model size vs. performance (smaller models more deployable), domain gap vs. representation sharing (separate vs shared paths), dataset size vs. quality (balanced vs full data)

- Failure signatures: Performance plateaus at 80-85% accuracy (domain mismatch), open-set performance lags closed-set (poor generalization), limited improvement without separate learning paths

- First 3 experiments: 1) Baseline: Train D-ProtoNets on splitGSC without auxiliary data, 2) Supervised learning: Train with all data combined using single loss function, 3) SSL comparison: Apply SimCLR/BYOL on auxiliary data to demonstrate ineffectiveness for small models

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several remain unresolved based on the analysis:

### Open Question 1
- Question: How does AuxSL effectiveness vary with different sizes of auxiliary dataset?
- Basis in paper: [explicit] Paper discusses LibriWord construction and impact on performance
- Why unresolved: No systematic analysis of how varying auxiliary dataset size affects performance
- What evidence would resolve it: Experiments comparing performance with different proportions of LibriWord (10%, 50%, 100%)

### Open Question 2
- Question: Impact of using different types of reading speech data as auxiliary data?
- Basis in paper: [inferred] Suggests reading speech data from audiobooks used as auxiliary
- Why unresolved: Focuses only on LibriWord, doesn't explore other reading speech sources
- What evidence would resolve it: Experiments comparing performance using different reading speech sources (news, podcasts)

### Open Question 3
- Question: How does AuxSL perform on other few-shot learning tasks beyond keyword spotting?
- Basis in paper: [explicit] Demonstrates effectiveness on FS-KWS, suggesting potential for other tasks
- Why unresolved: Does not explore performance on other few-shot learning tasks
- What evidence would resolve it: Experiments applying AuxSL to image classification or speech emotion recognition

## Limitations

- Effectiveness of supervised learning vs SSL for small models needs extensive ablation studies across different model sizes
- Assumption that balanced auxiliary dataset is necessary requires verification against alternative construction strategies
- Specific implementation details of "dummy prototypical loss" and open-set prototype construction remain unclear

## Confidence

- **High Confidence**: Overall multi-task learning framework and experimental methodology
- **Medium Confidence**: Claim that SSL methods ineffective for small-footprint models based on authors' experiments
- **Low Confidence**: Necessity of balanced auxiliary dataset vs using entire Librispeech dataset

## Next Checks

1. Conduct controlled experiments varying the balancing parameter 位 in AuxSL to identify optimal trade-off between in-domain and out-of-domain learning
2. Implement and compare multiple SSL methods (SimCLR, BYOL, MoCo) on auxiliary data to verify supervised learning superiority for small models
3. Perform cross-domain analysis by training AuxSL with different auxiliary datasets (Common Voice, TED-LIUM) to assess robustness to domain gaps