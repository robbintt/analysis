---
ver: rpa2
title: Learning From Scenarios for Stochastic Repairable Scheduling
arxiv_id: '2312.03492'
source_url: https://arxiv.org/abs/2312.03492
tags:
- stochastic
- scheduling
- task
- learning
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper applies decision-focused learning (DFL) to stochastic
  resource-constrained scheduling with uncertain processing times, adapting a score
  function gradient estimation method to handle non-differentiability from constraints.
  Scenarios from historical data are used to train a stochastic estimator that smooths
  regret computation during optimization.
---

# Learning From Scenarios for Stochastic Repairable Scheduling

## Quick Facts
- arXiv ID: 2312.03492
- Source URL: https://arxiv.org/abs/2312.03492
- Reference count: 4
- One-line primary result: DFL shows scalability advantages and competitive performance on stochastic scheduling with repair, especially on larger instances.

## Executive Summary
This paper applies decision-focused learning (DFL) to stochastic resource-constrained scheduling with uncertain processing times and repair mechanisms. The approach uses historical scenarios to train a stochastic estimator that smooths regret computation during optimization, enabling gradient-based learning through non-differentiable repair actions. Evaluated against deterministic and stochastic programming baselines on standard PSPLib and industry-inspired instances, DFL demonstrates strong scalability and competitive performance, particularly on larger instances where stochastic programming struggles computationally.

## Method Summary
The method employs DFL with stochastic smoothing using a Normal distribution parameterized by learnable scaling factors. Historical scenarios of processing times are used to train a predictor that minimizes expected post-hoc regret, which accounts for repair penalties when constraints are violated. The score function gradient estimation method enables backpropagation through the non-differentiable optimization solver. The approach is evaluated on PSPLib instances (j301-j303, j901-j903) and industry-inspired instances with varying penalty coefficients, comparing against deterministic and stochastic programming baselines.

## Key Results
- DFL achieves competitive performance to stochastic programming on smaller instances when optimal solutions are found
- DFL shows clear scalability advantages on larger instances (j901-j903) where stochastic programming fails to find optimal solutions
- The scenario-based training approach enables effective learning of scaling parameters without requiring explicit feature engineering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stochastic smoothing enables gradient computation through non-differentiable repair actions
- Mechanism: Modeling predictions as samples from a parameterized normal distribution smoothes the regret curve, making it differentiable via the log-derivative trick
- Core assumption: The probability density function must be differentiable with respect to its parameters
- Break condition: Discontinuities that cannot be smoothed by the chosen distribution, or excessive noise in log-derivative approximation

### Mechanism 2
- Claim: Historical scenarios can be directly used to train a post-hoc regret minimizing predictor without requiring explicit feature engineering
- Mechanism: The algorithm learns a scaling of the scenario mean that implicitly encodes optimal scheduling decisions under repair penalties
- Core assumption: Historical scenarios are representative of true uncertainty and capture repair dynamics
- Break condition: Sparse or unrepresentative scenarios leading to poor generalization

### Mechanism 3
- Claim: DFL scales better than stochastic programming by avoiding scenario enumeration and second-stage optimization
- Mechanism: Training learns a single predictor mapping scenarios to decisions, while stochastic programming solves large mixed-integer programs for each scenario combination
- Core assumption: The learned predictor can approximate the optimal decision policy well enough to compete with scenario-based optimization
- Break condition: Complex repair policies that cannot be captured by simple parametric models, or manageable scenario numbers for stochastic optimization

## Foundational Learning

- Concept: Score function gradient estimation (log-derivative trick)
  - Why needed here: Enables backpropagation through the non-differentiable optimization solver by providing an unbiased gradient estimator
  - Quick check question: What is the mathematical form of the gradient estimate used in this approach, and why does it require a differentiable probability density?

- Concept: Post-hoc regret in constrained optimization
  - Why needed here: Measures the cost of decisions made with imperfect information, accounting for repair actions needed due to uncertainty in constraints
  - Quick check question: How does the post-hoc regret formulation differ from standard prediction error, and what role does the repair penalty play?

- Concept: Stochastic policy gradient methods
  - Why needed here: Provides the theoretical foundation for using stochastic estimators in optimization under uncertainty
  - Quick check question: What is the relationship between the stochastic estimator used here and the policy distributions in reinforcement learning?

## Architecture Onboarding

- Component map: Data preprocessing -> Stochastic estimator -> Solver interface -> Repair simulation -> Loss computation -> Gradient update
- Critical path: 1) Sample scenario from training set 2) Generate prediction from current stochastic estimator 3) Solve scheduling problem with predicted parameters 4) Apply repair policy to get corrected schedule 5) Compute post-hoc regret against true scenario 6) Update estimator parameters via gradient step
- Design tradeoffs:
  - Normal distribution provides smooth gradients but may not capture heavy-tailed processing time distributions
  - Learning scaling factors on sample mean is simple but may miss complex relationships between scenarios and optimal decisions
  - One-unit postponement is easy to implement but may not reflect realistic repair policies in all domains
- Failure signatures:
  - Training loss plateaus early: Possible issues with learning rate, insufficient scenario diversity, or repair policy too restrictive
  - High variance in gradient estimates: May need more samples per update or gradient clipping
  - Poor performance on test instances: Scenarios not representative, or scaling learned is too instance-specific
- First 3 experiments:
  1. Run on a small synthetic instance with known optimal policy to verify the learned scaling matches expectations
  2. Compare performance with varying numbers of training scenarios to understand data efficiency
  3. Test with different repair policies (e.g., multi-unit postponement) to see impact on learned predictor

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of decision-focused learning (DFL) compare to other gradient estimation techniques in stochastic scheduling problems with uncertain processing times?
- Basis in paper: The paper discusses the use of a score function gradient estimation method adapted for non-differentiability from constraints, suggesting that alternative gradient estimators could be explored
- Why unresolved: The paper focuses on one specific method (score function gradient estimation) and does not compare it with other potential gradient estimation techniques that could be used in similar settings
- What evidence would resolve it: Conducting experiments comparing DFL with other gradient estimation techniques, such as variance reduction methods or alternative reinforcement learning-inspired algorithms, would provide insights into the relative performance and robustness of different approaches

### Open Question 2
- Question: What is the impact of incorporating feature data related to stochastic processing times on the performance of DFL in stochastic scheduling problems?
- Basis in paper: The paper mentions the potential benefits of DFL in settings with features related to stochastic processing times, as shown in earlier research with uncertainty in a (linear) objective
- Why unresolved: The paper focuses on scenarios without feature data, and the impact of incorporating such data is not explored
- What evidence would resolve it: Implementing DFL with feature data in stochastic scheduling problems and comparing its performance to the current approach would reveal the benefits or limitations of incorporating additional information

### Open Question 3
- Question: How does the scalability of DFL compare to stochastic programming when dealing with larger instances and varying penalty coefficients in stochastic scheduling problems?
- Basis in paper: The paper highlights the scalability advantages of DFL, especially on larger instances where stochastic programming struggles due to computational limits
- Why unresolved: While the paper provides some insights into scalability, a more comprehensive analysis across a wider range of instance sizes and penalty coefficients is needed
- What evidence would resolve it: Conducting experiments with a broader set of instance sizes and penalty coefficients, and comparing the performance and computational efficiency of DFL and stochastic programming, would provide a clearer understanding of their scalability trade-offs

## Limitations
- Reliance on Normal distributions for parameter predictions may not capture true uncertainty distributions in practical scheduling domains
- Effectiveness of learning simple scaling factors versus more complex prediction functions is unclear for instances with highly variable or correlated processing times
- The approach's generalization to different repair policies beyond one-unit postponement is not explored

## Confidence
- High confidence in DFL's scalability advantages based on empirical results showing successful application to large j901-j903 instances
- Medium confidence in the scenario-based training approach's ability to learn effective scaling parameters
- Medium confidence in the theoretical foundation using score function gradient estimation, though specific adaptation to repairable scheduling requires further validation

## Next Checks
1. Test the approach on instances with non-Normal processing time distributions to evaluate the robustness of the Normal-based smoothing assumption
2. Compare against a feature-based DFL approach to quantify the tradeoff between scenario-based and context-based prediction
3. Evaluate the impact of different repair policies (e.g., multi-unit postponement, resource reallocation) on the learned predictor's performance and generalization