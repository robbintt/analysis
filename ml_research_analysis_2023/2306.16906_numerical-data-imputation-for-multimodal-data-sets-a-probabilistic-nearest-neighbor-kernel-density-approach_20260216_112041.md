---
ver: rpa2
title: 'Numerical Data Imputation for Multimodal Data Sets: A Probabilistic Nearest-Neighbor
  Kernel Density Approach'
arxiv_id: '2306.16906'
source_url: https://arxiv.org/abs/2306.16906
tags:
- data
- missing
- imputation
- sets
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a new method, kNNxKDE, for numerical data imputation
  that addresses limitations of existing methods with multimodal or complex distributions.
  The kNNxKDE algorithm combines nearest neighbor estimation with kernel density estimation
  to provide probabilistic estimates for missing values.
---

# Numerical Data Imputation for Multimodal Data Sets: A Probabilistic Nearest-Neighbor Kernel Density Approach

## Quick Facts
- arXiv ID: 2306.16906
- Source URL: https://arxiv.org/abs/2306.16906
- Authors: 
- Reference count: 39
- Primary result: kNNxKDE outperforms state-of-the-art methods in imputation accuracy and log-likelihood for multimodal distributions

## Executive Summary
This paper introduces kNNxKDE, a novel numerical data imputation method that combines nearest neighbor estimation with kernel density estimation to handle multimodal and complex distributions. Unlike traditional imputation methods that provide single point estimates, kNNxKDE returns probabilistic estimates by constructing weighted mixtures of Gaussian kernels centered at observed neighbors. The method particularly excels at preserving the original data structure and capturing multimodal distributions, where traditional methods often fail by averaging over modes.

The algorithm is evaluated on both synthetic and real-world datasets, demonstrating superior performance compared to established methods like kNN-Imputer, MissForest, and MICE across multiple metrics including normalized root mean square error (NRMSE) and log-likelihood scores. The method's ability to provide full probability distributions rather than point estimates enables more flexible downstream applications and better representation of uncertainty in imputed values.

## Method Summary
kNNxKDE is a density-based imputation algorithm that leverages both k-nearest neighbors and kernel density estimation to provide probabilistic estimates for missing numerical values. The method first normalizes observed values to [0,1] range, then computes distances between rows using a NaN-std-Euclidean metric that accounts for column standard deviations. Instead of selecting a fixed number of nearest neighbors, it converts all distances to probabilities using a temperature-controlled softmax function, where the temperature parameter τ controls neighborhood size. The conditional probability distribution of missing values is then estimated as a weighted sum of Gaussian kernels centered at observed neighbor values. The algorithm can generate multiple samples from this distribution or provide summary statistics, and finally denormalizes results back to the original scale.

## Key Results
- kNNxKDE achieves the best average rank across all tested datasets and missing data scenarios
- The method provides higher log-likelihood scores for the ground truth under its probability model compared to alternatives
- kNNxKDE preserves the original data structure better than traditional imputation approaches, particularly for multimodal distributions
- The algorithm returns probabilistic estimates, allowing for more flexible imputation strategies than point-estimate methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The combination of nearest neighbor estimation with kernel density estimation preserves the original data structure better than traditional imputation methods.
- Mechanism: By using a soft nearest neighbor approach to derive weights for KDE, the algorithm captures the local density structure of the data. Instead of averaging over predictions (which can blur multimodal distributions), it estimates the conditional probability distribution of missing values given observed values, maintaining the integrity of multiple modes.
- Core assumption: The data structure can be effectively captured by local density estimation using a weighted mixture of Gaussian kernels centered at observed neighbors.
- Evidence anchors:
  - [abstract] "The kNNxKDE algorithm combines nearest neighbor estimation (kNN) and density estimation with Gaussian kernels (KDE)."
  - [section] "By leveraging the convenient properties of the kNN-Imputer and the KDE framework, we develop the kNN×KDE: a simple yet efficient algorithm for density estimation and data imputation of missing values in numerical data sets."
- Break condition: If the data contains very high-dimensional features or if missing data rates are extremely high in 'Full MCAR' scenarios, the local neighborhood structure may become too sparse to provide reliable density estimates.

### Mechanism 2
- Claim: The algorithm provides probabilistic estimates rather than point estimates, allowing for more flexible imputation strategies.
- Mechanism: For each missing value, the method returns a probability distribution constructed as a weighted sum of Gaussian kernels centered at observed neighbor values. This distribution can be used to sample multiple plausible imputations, compute statistics like mean or mode, or be passed directly to downstream tasks without committing to a single imputed value.
- Core assumption: Downstream tasks can benefit from working with the full distribution of possible values rather than a single point estimate, especially when the true distribution is multimodal.
- Evidence anchors:
  - [abstract] "kNNxKDE returns probabilistic estimates, allowing for more flexible imputation strategies."
  - [section] "The motivation behind this work was to design an algorithm capable of imputing numerical values in data sets with heterogeneous structures. In particular, multimodality makes imputation ambiguous, as distinct values may be valid imputations."
- Break condition: If the downstream task explicitly requires a single imputed value (e.g., certain optimization algorithms), the probabilistic output must be converted to a point estimate, potentially losing the benefits of the multimodal structure.

### Mechanism 3
- Claim: The algorithm mitigates the curse of dimensionality better than traditional kNN imputation by using a temperature-controlled softmax weighting scheme.
- Mechanism: Instead of selecting a fixed number of nearest neighbors, the algorithm computes distances to all potential donors and converts them to probabilities using a softmax function with temperature parameter τ. This allows for adaptive neighborhood sizing - tighter neighborhoods when τ is low and broader when τ is high - which helps maintain meaningful local density estimates even as dimensionality increases.
- Core assumption: The softmax weighting scheme with temperature control can effectively balance the trade-off between bias (overly tight neighborhoods) and variance (overly broad neighborhoods) in high-dimensional spaces.
- Evidence anchors:
  - [section] "We use the 'soft' version of the kNN algorithm, and introduce the temperature hyperparameter τ which can be interpreted as the effective neighborhood diameter."
  - [section] "It is worth noting that the kNN×KDE seems to suffer from the curse of dimensionality, especially in the 'Full MCAR' scenario at high missing rate."
- Break condition: When the effective neighborhood becomes too large (high τ) in very high dimensions, the local density estimate may become uninformative, approximating the global distribution rather than capturing local structure.

## Foundational Learning

- Concept: Kernel Density Estimation (KDE)
  - Why needed here: KDE forms the core of the probability estimation component, allowing the algorithm to model the distribution of missing values as a mixture of Gaussians centered at observed neighbors.
  - Quick check question: What happens to the KDE estimate if the bandwidth parameter is set too small versus too large?

- Concept: Nearest Neighbor Algorithms
  - Why needed here: The algorithm uses nearest neighbor distances to identify relevant observations that can inform the imputation, but adapts the traditional approach by using soft weighting instead of hard selection.
  - Quick check question: How does the NaN-std-Euclidean distance metric differ from the traditional NaN-Euclidean distance used in kNN-Imputer?

- Concept: Missing Data Mechanisms (MCAR, MAR, MNAR)
  - Why needed here: Understanding different missing data scenarios is crucial for proper evaluation and for interpreting when the algorithm performs best versus when it might struggle.
  - Quick check question: In which missing data scenario would you expect the algorithm to perform most poorly and why?

## Architecture Onboarding

- Component map: Data Preprocessing -> Distance Computation -> Softmax Weighting -> Density Estimation -> Sampling -> Denormalization
- Critical path: Normalization → Distance Computation → Softmax Weighting → Density Estimation → Sampling → Denormalization
- Design tradeoffs:
  - Computational cost vs. accuracy: The algorithm is more computationally expensive than kNN-Imputer but provides better density estimates
  - Number of samples vs. resolution: More samples provide better resolution of the probability distribution but increase computation time
  - Temperature vs. locality: Lower temperature creates tighter neighborhoods (potentially overfitting) while higher temperature creates broader neighborhoods (potentially underfitting)
- Failure signatures:
  - Poor performance on high-dimensional data with high missing rates in 'Full MCAR' scenarios (sparse potential donor sets)
  - Imputation distributions that are too narrow (overfitting) or too broad (underfitting) based on temperature setting
  - Computational slowness on large datasets due to pairwise distance calculations
- First 3 experiments:
  1. Test on 2d_sine dataset with varying temperature values (1/τ = 10, 50, 500) and visualize the resulting imputation distributions to understand the effect of temperature on locality
  2. Compare NRMSE and log-likelihood scores on a small real dataset (e.g., geyser) with different missing rates to see when probabilistic estimates provide advantage over point estimates
  3. Run the algorithm on a synthetic multimodal dataset and verify that the returned distributions capture all modes rather than averaging them out

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the kNN×KDE algorithm perform on datasets with a high percentage of missing values (e.g., 60%) and a large number of features (e.g., 30 or more)?
- Basis in paper: [explicit] The paper mentions that the kNN×KDE algorithm can struggle with high missing rates in 'Full MCAR' scenarios for high-dimensional datasets, as the subset of potential donors may be empty.
- Why unresolved: The paper only tested missing rates up to 60% and did not explore scenarios with a very large number of features.
- What evidence would resolve it: Experiments testing the kNN×KDE algorithm on datasets with higher missing rates (e.g., 70% or 80%) and a larger number of features (e.g., 50 or more) would provide insights into its performance in these extreme scenarios.

### Open Question 2
- Question: How does the kNN×KDE algorithm compare to other imputation methods when dealing with datasets containing both numerical and categorical features?
- Basis in paper: [explicit] The paper mentions that the kNN×KDE algorithm is currently designed for numerical data imputation and does not handle categorical variables.
- Why unresolved: The paper does not provide any comparison between the kNN×KDE algorithm and other imputation methods when dealing with mixed data types.
- What evidence would resolve it: Experiments comparing the kNN×KDE algorithm with other imputation methods (e.g., MissForest, MICE) on datasets containing both numerical and categorical features would provide insights into its performance in such scenarios.

### Open Question 3
- Question: How does the choice of the kernel bandwidth (h) affect the performance of the kNN×KDE algorithm, and is there an optimal method for selecting this parameter?
- Basis in paper: [explicit] The paper mentions that the kernel bandwidth is shared throughout the algorithm and is fixed to its default value of 0.03, but it could be fine-tuned to obtain higher log-likelihood scores.
- Why unresolved: The paper does not explore the impact of different kernel bandwidth values on the algorithm's performance or provide a method for selecting the optimal bandwidth.
- What evidence would resolve it: Experiments testing the kNN×KDE algorithm with different kernel bandwidth values and analyzing the resulting log-likelihood scores would provide insights into the optimal choice of this parameter.

## Limitations
- The algorithm struggles with high-dimensional data, particularly in 'Full MCAR' scenarios with high missing rates where potential donor sets become sparse
- Computational complexity is significantly higher than traditional kNN-Imputer due to pairwise distance calculations and KDE computations
- The method is currently limited to numerical data and cannot handle categorical features without preprocessing or extension

## Confidence

- **High confidence**: The core algorithmic approach combining kNN with KDE is mathematically sound and the implementation appears correct for the tested scenarios
- **Medium confidence**: The comparative performance claims against state-of-the-art methods are well-supported by the presented experiments, though additional benchmarking on larger, more diverse datasets would strengthen these conclusions
- **Medium confidence**: The claim about superior handling of multimodal distributions is supported by the synthetic dataset experiments but would benefit from more extensive testing on real-world multimodal data

## Next Checks

1. **Dimensionality sensitivity test**: Systematically evaluate kNNxKDE performance across varying feature dimensions (2D to 50D) on synthetic multimodal data to quantify the curse of dimensionality effects and identify breaking points
2. **Hyperparameter robustness analysis**: Conduct a comprehensive grid search over temperature values (τ) and bandwidth parameters (h) across multiple dataset types to determine optimal settings and identify ranges where performance degrades
3. **Scalability benchmarking**: Test the algorithm on large-scale datasets (>100K samples) and measure runtime complexity compared to kNN-Imputer, identifying practical limits for real-world applications