---
ver: rpa2
title: Do Large Language Models have Shared Weaknesses in Medical Question Answering?
arxiv_id: '2310.07225'
source_url: https://arxiv.org/abs/2310.07225
tags:
- question
- medical
- llama
- vicuna
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks 16 well-known large language models (LLMs)
  on 874 Polish medical licensing exam questions to identify shared weaknesses across
  models. The authors compare model performance with human performance, question difficulty,
  and question length.
---

# Do Large Language Models have Shared Weaknesses in Medical Question Answering?

## Quick Facts
- arXiv ID: 2310.07225
- Source URL: https://arxiv.org/abs/2310.07225
- Reference count: 40
- Primary result: 16 LLMs scored 74-84% on Polish medical licensing exam questions, with accuracies correlated but exhibiting shared weaknesses, especially in ethical/legal categories.

## Executive Summary
This paper benchmarks 16 large language models on 874 Polish medical licensing exam questions to identify shared weaknesses across models. The authors find that model accuracies are positively correlated pairwise (0.39 to 0.58) and with human performance (0.09 to 0.13), but negatively correlated with the difference between top and bottom human scores (-0.09 to -0.14). The top model, GPT-4o Turbo, scored 84%, with other top models between 74% and 79%. Model accuracy was positively correlated with confidence but negatively correlated with question length. The authors conclude that these patterns are likely to persist across future models using similar training methods.

## Method Summary
The study queries 16 different large language models on 874 Polish medical licensing exam questions using a consistent prompt format requesting single-letter responses with explanations. Model outputs are scored for accuracy against correct answers and human performance metrics including difficulty index and discriminative power index. Statistical analysis includes pairwise correlation of model accuracies, correlation with human performance, and regression analysis of accuracy against question difficulty and length.

## Key Results
- GPT-4o Turbo achieved the highest accuracy at 84%, with other top models scoring between 74% and 79%
- Model accuracies were positively correlated pairwise (0.39 to 0.58), indicating shared strengths and weaknesses
- Model performance was negatively correlated with the difference between top and bottom human scores (-0.09 to -0.14), suggesting similar difficulty patterns
- Longer questions reduced accuracy more for larger models, with the effect being more pronounced for top-scoring models

## Why This Works (Mechanism)

### Mechanism 1
Larger models tend to perform better overall but exhibit similar weaknesses, especially in ethical/legal categories. Larger parameter counts improve memorization and reasoning, but all models trained on similar internet corpora share blind spots in specialized or less-represented medical domains. Shared training data sources create consistent knowledge gaps across models. This pattern is likely to persist across future models using similar training methods.

### Mechanism 2
Question difficulty for humans correlates inversely with model performance, especially for questions that only top human scorers answer correctly. Models struggle with questions requiring integration of multiple concepts or rare knowledge, which also challenge human experts. The same types of questions are difficult for both humans and models due to complexity or domain specificity. This similarity in difficulty patterns suggests shared cognitive limitations.

### Mechanism 3
Longer questions reduce model accuracy more for larger models, suggesting scaling gains are less effective for complex inputs. Self-attention complexity and context processing inefficiencies penalize longer inputs more heavily in larger models. Quadratic cost of self-attention and attention saturation affect larger models disproportionately. Even with questions well within context limits, longer inputs led to worse results for most models.

## Foundational Learning

- **Logistic regression for analyzing binary outcomes**
  - Why needed here: Used to model the relationship between question length and model accuracy
  - Quick check question: What does the coefficient β1 represent in the logistic regression p(l) = 1/(1 + e^(-(β0+β1l)))?

- **Correlation analysis for identifying shared patterns**
  - Why needed here: Used to compare model performance pairwise and with human performance
  - Quick check question: What does a positive correlation between two models' accuracies indicate about their shared strengths or weaknesses?

- **Prompt engineering and its impact on model responses**
  - Why needed here: Different prompts can lead to variations in model accuracy and output format
  - Quick check question: Why might a prompt requesting an explanation yield different results than one requesting only a letter response?

## Architecture Onboarding

- **Component map**: Data ingestion (LEK dataset) -> Model inference (LLM APIs/local runs) -> Response scoring -> Statistical analysis -> Result visualization
- **Critical path**: Data preparation -> Model querying -> Answer validation -> Accuracy computation -> Correlation analysis
- **Design tradeoffs**: API access vs. local inference (cost, speed, reproducibility); multiple-choice format vs. open-ended (scoring simplicity vs. real-world applicability)
- **Failure signatures**: Incomplete responses, ambiguous answers, context window overflows, inconsistent prompt following
- **First 3 experiments**:
  1. Re-run accuracy scoring with a fixed prompt template across all models to control for prompt variability
  2. Test model performance on a subset of questions with varying lengths to quantify the length effect more granularly
  3. Compare model accuracy on questions from underrepresented medical categories to validate shared weaknesses

## Open Questions the Paper Calls Out

- **Do larger models really have higher accuracy in medical question answering tasks?**
  - Basis in paper: Explicit
  - Why unresolved: While largest models achieved highest accuracy scores, it's unclear if this is purely due to model size since other factors like architecture, training data, or pre-processing may also affect performance. Within the same model families, smaller models sometimes scored better in specific medical categories.
  - What evidence would resolve it: Controlled experiments comparing models of different sizes but with identical architecture, training data, and pre-processing would help isolate the effect of model size.

- **What makes a question difficult for an LLM in medical question answering tasks?**
  - Basis in paper: Explicit
  - Why unresolved: The paper found weak evidence of shared strengths and weaknesses across models, but there are many possible explanations including similar subject coverage in training data, similarities in architecture or training process, linguistic patterns within questions, or statistical artifacts like answer choice ordering.
  - What evidence would resolve it: Detailed analysis of model performance on different types of questions and correlation studies between model performance and various question characteristics would help identify the factors that make questions difficult for LLMs.

- **Do LLMs have similar weaknesses to humans in medical question answering tasks?**
  - Basis in paper: Explicit
  - Why unresolved: The paper found that top performing models performed better on questions that humans found easier and worse on questions that only top humans could answer. This could indicate a similarity between what makes questions difficult for humans and LLMs, which is a surprising result.
  - What evidence would resolve it: Comparative studies of model and human performance on the same question sets, analysis of the specific types of errors made by models versus humans, and investigation of the underlying factors contributing to these errors would help determine if LLMs have similar weaknesses to humans.

## Limitations

- The study's findings are based on a single-language dataset (Polish medical licensing exams), which limits generalizability to other languages and medical domains
- The analysis assumes that shared weaknesses across models reflect common training data limitations, but does not account for potential architectural or algorithmic constraints
- The study focuses on multiple-choice questions, which may not fully represent the complexity of real-world medical decision-making scenarios

## Confidence

- **High confidence**: The correlation patterns between model accuracies and the relationship between model performance and question difficulty/length are statistically robust within the dataset
- **Medium confidence**: The conclusion that shared weaknesses are likely to persist across future models using similar training methods, as this depends on assumptions about future training approaches
- **Low confidence**: The specific mechanisms underlying the negative correlation between question length and model accuracy for larger models, as the study does not provide direct evidence of the computational bottlenecks involved

## Next Checks

1. Replicate the study using medical licensing exam questions from different countries and languages to test the generalizability of shared weakness patterns
2. Conduct ablation studies on model architectures to isolate whether shared weaknesses stem from training data, architecture, or both
3. Extend the analysis to open-ended medical questions requiring generation of explanations or treatment plans, rather than just multiple-choice answers