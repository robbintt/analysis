---
ver: rpa2
title: Sentiment Analysis through LLM Negotiations
arxiv_id: '2311.01876'
source_url: https://arxiv.org/abs/2311.01876
tags:
- arxiv
- sentiment
- preprint
- generator
- discriminator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to sentiment analysis by
  leveraging multi-LLM negotiations, addressing the limitations of single-turn decision-making
  by large language models (LLMs). The proposed framework employs a reasoning-infused
  generator to produce sentiment decisions along with rationales and a discriminator
  to evaluate the credibility of these decisions.
---

# Sentiment Analysis through LLM Negotiations

## Quick Facts
- arXiv ID: 2311.01876
- Source URL: https://arxiv.org/abs/2311.01876
- Reference count: 20
- Key outcome: Novel multi-LLM negotiation framework for sentiment analysis that consistently outperforms single-LLM baselines and matches supervised methods on key datasets.

## Executive Summary
This paper introduces a novel sentiment analysis approach that leverages iterative negotiation between two large language models (LLMs) to improve decision accuracy and interpretability. The framework employs a reasoning-infused generator to produce sentiment decisions with rationales and a discriminator to evaluate the credibility of these decisions. Through role-flipping and consensus-seeking negotiation, the method addresses the limitations of single-turn decision-making by LLMs. Experiments on six diverse sentiment analysis benchmarks demonstrate consistent improvements over single-LLM baselines and even surpass supervised methods on the Twitter and Movie Review datasets.

## Method Summary
The proposed method uses two LLMs in a generator-discriminator framework where the generator produces sentiment decisions with reasoning chains, and the discriminator evaluates and potentially corrects these decisions. The negotiation process involves iterative exchanges with role-flipping, where the models alternate between generating and evaluating roles to capture diverse perspectives. Demonstrations from the training set are retrieved using k-NN similarity and formatted as in-context examples. If consensus isn't reached after several turns, a third-party LLM can be invoked to break ties. The approach relies on in-context learning rather than fine-tuning, making it applicable across different sentiment analysis domains without model retraining.

## Key Results
- Consistently outperforms single-LLM baselines across all six datasets (SST-2, Movie Review, Twitter, Yelp, Amazon, IMDB)
- Achieves higher accuracy than supervised methods on Twitter and Movie Review datasets
- Ablation studies confirm the importance of reasoning chains and heterogeneous LLM pairing for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using two different LLMs for generator and discriminator improves accuracy over single-LLM negotiation because each model can compensate for the other's reasoning flaws.
- Mechanism: When the generator makes an error due to flawed reasoning, the discriminator (being a different model) is more likely to catch it. This complementary ability is leveraged through role-flipping and iterative negotiation until consensus or max turns.
- Core assumption: Different LLMs have sufficiently diverse reasoning patterns to detect and correct each other's mistakes.
- Evidence anchors: [abstract] "take the complementary abilities of two LLMs"; [section 5.1] "using heterogeneous LLMs for distinct roles could optimize the negotiation's performance"
- Break condition: If both LLMs share the same reasoning bias or error pattern, negotiation fails to improve over single-LLM baseline.

### Mechanism 2
- Claim: Articulated reasoning chains improve performance by giving the discriminator explicit evidence to evaluate.
- Mechanism: The generator produces a step-by-step reasoning chain alongside its decision. The discriminator uses this chain to judge correctness and provide explanations, enabling more informed corrections.
- Core assumption: The reasoning chain is sufficiently detailed and correct to guide the discriminator's evaluation.
- Evidence anchors: [abstract] "reasoning-infused generator to provide decision along with rationale"; [section 5.3] "performances all degrade when the reasoning process is removed"
- Break condition: If the reasoning chain is incomplete, incorrect, or the discriminator cannot parse it, performance degrades.

### Mechanism 3
- Claim: Role-flipping between generator and discriminator captures diverse perspectives, increasing consensus likelihood.
- Mechanism: After one round of negotiation, the roles are swapped and a new negotiation starts. This exposes the task to different reasoning styles and increases chances of reaching agreement.
- Core assumption: Each LLM has a distinct but valid perspective on sentiment that can be leveraged through role alternation.
- Evidence anchors: [abstract] "two LLMs act as the roles of the generator and the discriminator, and perform the negotiation until a consensus is reached"; [section 5.1] "G4-D3.5 can beat G4-D4" indicating role assignment matters
- Break condition: If both LLMs converge on the same incorrect interpretation, role-flipping does not help.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: The framework relies on prompting LLMs with few-shot examples rather than fine-tuning, so understanding ICL mechanics is critical.
  - Quick check question: What are the key differences between ICL and traditional fine-tuning in terms of data requirements and generalization?

- Concept: Adversarial training / generator-discriminator dynamics
  - Why needed here: The negotiation framework is inspired by GAN-like interactions where two models iteratively refine outputs.
  - Quick check question: How does a generator-discriminator loop in GANs differ from the negotiation loop in this sentiment analysis method?

- Concept: Sentiment analysis task structure
  - Why needed here: Correctly framing sentiment classification (binary vs multi-class, domain specifics) affects prompt design and evaluation.
  - Quick check question: What linguistic phenomena (e.g., irony, clause composition) most challenge sentiment classifiers?

## Architecture Onboarding

- Component map: Training set (via k-NN) -> Demonstration builder -> Prompt builder -> Generator LLM -> Discriminator LLM -> Consensus checker -> Negotiator loop -> Final decision
- Critical path:
  1. Retrieve K nearest neighbors from training set
  2. Convert demonstrations to (input, reasoning, decision) format
  3. Prompt generator → get decision + reasoning
  4. Prompt discriminator → get evaluation + decision
  5. Check consensus; if no consensus, flip roles or invoke third LLM
- Design tradeoffs:
  - Model selection: Using two different LLMs increases diversity but adds coordination complexity.
  - Max turns: More turns increase accuracy but add latency and cost.
  - Third LLM fallback: Improves final accuracy but increases inference overhead.
- Failure signatures:
  - Consensus never reached: Likely due to strongly biased or mismatched LLMs.
  - Performance worse than single-LLM: Reasoning chains are misleading or discriminator is ineffective.
  - High variance across inputs: Demonstrations retrieval or prompt formatting is unstable.
- First 3 experiments:
  1. Baseline: Single LLM (GPT-3.5) without negotiation on SST-2.
  2. Self-negotiation: Same LLM as generator and discriminator on Twitter dataset.
  3. Two-LLM negotiation: GPT-3.5 + GPT-4 with role-flipping on Movie Review dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific biases or systematic errors in individual LLMs can be identified and mitigated through the multi-LLM negotiation framework?
- Basis in paper: [explicit] The authors mention that the framework can help "identify and mitigate the impact of biases and decoding errors present in individual LLMs" in their conclusion.
- Why unresolved: The paper doesn't provide specific examples or empirical evidence of biases that were successfully mitigated through negotiation.
- What evidence would resolve it: Detailed case studies showing specific biases in individual LLMs that were corrected through the negotiation process, with quantitative comparisons of bias metrics before and after negotiation.

### Open Question 2
- Question: How does the performance of the multi-LLM negotiation framework scale with different combinations of LLMs (e.g., GPT-4 + Claude, GPT-4 + LLaMA)?
- Basis in paper: [explicit] The authors note that "using two separate models, we are able to take the advantage of the complementary abilities of the two models" and that "using heterogeneous LLMs for distinct roles could optimize the negotiation's performance."
- Why unresolved: The experiments only tested combinations of GPT-3.5 and GPT-4, leaving open questions about how different LLM combinations would perform.
- What evidence would resolve it: Systematic experiments testing various combinations of different LLM architectures (not just OpenAI models) to identify optimal pairings and understand how architectural differences impact negotiation effectiveness.

### Open Question 3
- Question: What is the optimal number of negotiation turns needed to reach consensus without degrading performance or efficiency?
- Basis in paper: [inferred] The framework iterates "until a consensus is reached or the maximum number of negotiation turns is exceeded," suggesting there's a trade-off between thoroughness and efficiency.
- Why unresolved: The paper doesn't provide analysis of how performance changes with different numbers of negotiation rounds or investigate the point of diminishing returns.
- What evidence would resolve it: Experiments varying the maximum number of negotiation turns across datasets to identify optimal thresholds that balance accuracy gains with computational cost and response time.

## Limitations
- Reliance on two different LLMs assumes sufficient diversity in reasoning patterns without empirical verification across all domains
- Performance tied to retrieval-based demonstration selection, making it sensitive to k-NN parameters and similarity metrics
- Iterative negotiation process introduces additional inference costs and latency, potentially limiting practical deployment

## Confidence
- High confidence in the core mechanism: Complementary reasoning between distinct LLMs is well-supported by ablation studies
- Medium confidence in the role-flipping benefit: Limited empirical evidence beyond specific model pairs
- Medium confidence in generalizability: Strong results on Twitter and Movie Review but incomplete benchmarking on other datasets

## Next Checks
1. Ablation on model diversity: Systematically test negotiation performance using pairs of identical LLMs versus different LLMs to quantify the impact of reasoning diversity
2. Reasoning chain robustness: Evaluate the framework with corrupted or incomplete reasoning chains to assess the discriminator's ability to detect flawed rationales
3. Cost-benefit analysis: Measure the trade-off between accuracy gains and increased inference costs (API calls, latency) across negotiation turns and model pairs