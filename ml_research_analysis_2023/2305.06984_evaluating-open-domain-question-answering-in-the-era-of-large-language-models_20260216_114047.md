---
ver: rpa2
title: Evaluating Open-Domain Question Answering in the Era of Large Language Models
arxiv_id: '2305.06984'
source_url: https://arxiv.org/abs/2305.06984
tags:
- answers
- evaluation
- answer
- gold
- instructgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the inadequacy of lexical matching as an evaluation
  method for open-domain question answering, particularly with the rise of generative
  models and large language models (LLMs) that produce longer, less exact answers.
  Through manual evaluation of 12 QA models on a subset of the NQ-OPEN dataset, the
  authors find that lexical matching severely underestimates model performance, with
  InstructGPT (zero-shot) improving by nearly 60% in true accuracy.
---

# Evaluating Open-Domain Question Answering in the Era of Large Language Models

## Quick Facts
- arXiv ID: 2305.06984
- Source URL: https://arxiv.org/abs/2305.06984
- Reference count: 26
- Primary result: Lexical matching severely underestimates model performance; over 50% of failures are due to semantically equivalent answers

## Executive Summary
This paper addresses the inadequacy of lexical matching for evaluating open-domain question answering (QA) models, particularly with the rise of large language models (LLMs) that generate longer, less exact answers. Through manual evaluation of 12 QA models on a subset of the NQ-OPEN dataset, the authors find that lexical matching misses over 50% of correct answers due to semantic equivalence. Automated evaluation methods like BEM and InstructGPT-based prompts improve over lexical matching but struggle with hallucinated content in LLM answers. The study highlights the need for more robust evaluation mechanisms, with human judgment remaining the most reliable but costly option.

## Method Summary
The study evaluates 12 open-domain QA models on a subset of 301 questions from NQ-OPEN. Models include DPR, FiD variants, ANCE+, GAR+, Contriever, RocketQAv2, FiD-KD, EviGen, R2-D2, EMDR2, and InstructGPT (zero/few-shot). The authors compute Exact-Match (EM) and F1 accuracy, and apply multiple evaluation methods: lexical matching, BEM, InstructGPT-eval, GPT4-eval, and human judgment. Manual annotation is performed by two annotators with a third resolving disagreements.

## Key Results
- Lexical matching underestimates model performance by nearly 60% for InstructGPT (zero-shot) when compared to human judgment
- Over 50% of lexical matching failures are due to semantically equivalent answers
- InstructGPT-based evaluation methods overestimate accuracy for long-form LLM answers due to hallucination susceptibility
- Regex matching provides consistent rankings across evaluation methods but requires manual pattern creation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lexical matching underestimates model performance because many candidate answers are semantically equivalent to gold answers but use different wording
- Mechanism: When answers are phrased differently but convey the same meaning (e.g., "Jicheng" vs. "Peking"), exact string matching fails but human judgment recognizes correctness
- Core assumption: Gold answer lists are incomplete and don't capture all valid answer variants
- Evidence anchors: [abstract] "more than 50% of lexical matching failures are attributed to semantically equivalent answers"; [section] "Semantic equivalence (50.3%) is the most common failure mode of exact matching"

### Mechanism 2
- Claim: Automated semantic matching methods (BEM, InstructGPT-eval) can capture many semantic equivalence failures but struggle with long-form LLM answers containing hallucinations
- Mechanism: These methods use semantic similarity or knowledge-based reasoning to judge answer correctness beyond exact string matching, but are vulnerable to hallucinated content that appears plausible
- Core assumption: Semantic similarity methods can reliably distinguish true semantic equivalence from superficial similarity, but not from hallucination
- Evidence anchors: [abstract] "automated evaluation models are a reasonable surrogate for lexical matching in some circumstances, but not for long-form answers generated by LLMs"; [section] "InstructGPT (zero-shot) is overestimated under these models, compared to human judgment"

### Mechanism 3
- Claim: Regex matching provides a middle ground between lexical matching and full semantic evaluation by allowing controlled syntactic variation while maintaining precision
- Mechanism: By defining gold answers as regular expressions, the evaluation can capture common answer variations (e.g., different name formats, date formats) while still requiring answers to match specific patterns
- Core assumption: Most answer variations in QA can be captured by well-designed regex patterns without introducing false positives
- Evidence anchors: [section] "specifying gold answers via regular expressions can be useful in capturing these variations"; [section] "The ranking of models via regex matching is left unchanged by all three evaluation mechanisms"

## Foundational Learning

- Concept: Semantic equivalence in natural language
  - Why needed here: Understanding how different phrasings can express the same meaning is crucial for recognizing why lexical matching fails
  - Quick check question: Can "New York City" and "NYC" be considered semantically equivalent answers to "What is the largest city in the US?"

- Concept: Regular expressions for pattern matching
  - Why needed here: Regex matching is proposed as an alternative evaluation method that can capture syntactic variations while maintaining precision
  - Quick check question: How would you write a regex pattern to match dates in the format "YYYY-MM-DD" or "Month DD, YYYY"?

- Concept: Hallucination detection in LLM outputs
  - Why needed here: Understanding how automated evaluation methods fail with hallucinated content is key to interpreting their limitations
  - Quick check question: What characteristics of an answer might indicate it's a hallucination rather than a semantically equivalent alternative?

## Architecture Onboarding

- Component map: NQ-OPEN dataset subset (301 questions) -> 12 open-domain QA models -> Lexical matching, BEM, InstructGPT-eval, human judgment, regex matching -> Categorization of failure modes, correlation analysis

- Critical path: 1) Generate answers using all 12 models on the 301-question subset; 2) Apply multiple evaluation methods (lexical, BEM, InstructGPT-eval, regex); 3) Conduct human evaluation of all answers; 4) Analyze discrepancies between evaluation methods; 5) Categorize failure modes to understand patterns

- Design tradeoffs:
  - Lexical matching: Simple and fast, but too strict and misses semantic equivalence
  - BEM: Better at capturing semantic equivalence, but computationally expensive and struggles with hallucinations
  - InstructGPT-eval: Leverages LLM knowledge, but also vulnerable to hallucination and can be inconsistent
  - Regex matching: Controls for syntactic variation, but requires manual pattern creation and may miss novel variations
  - Human evaluation: Most accurate, but expensive and not scalable

- Failure signatures:
  - Lexical matching failure: Answer is semantically correct but uses different wording than any gold answer
  - BEM failure: Answer contains hallucinated information that appears plausible but is factually incorrect
  - InstructGPT-eval failure: Model overconfidently accepts hallucinated answers or inconsistently evaluates similar answers
  - Regex matching failure: Answer follows a valid syntactic pattern but is semantically incorrect, or uses a novel variation not captured by regex

- First 3 experiments:
  1. Reproduce the main experiment with a different random subset of NQ-OPEN to verify consistency of findings
  2. Implement a hybrid evaluation that combines regex patterns with semantic similarity to see if accuracy improves
  3. Test hallucination detection capabilities by generating known hallucinated answers and evaluating their performance across methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between automated evaluation methods and human evaluation for open-domain QA models, considering both accuracy and cost?
- Basis in paper: [inferred]
- Why unresolved: The paper shows that human evaluation provides the most accurate assessment of model performance but is costly and not scalable. Automated methods like BEM and InstructGPT-eval improve over lexical matching but struggle with LLM hallucinations. The optimal balance between accuracy and cost remains unclear.
- What evidence would resolve it: Comparative studies measuring the trade-off between evaluation accuracy and cost across different automated methods and human evaluation on larger datasets.

### Open Question 2
- Question: How can automated evaluation methods be improved to better detect hallucinations in long-form answers generated by LLMs?
- Basis in paper: [explicit]
- Why unresolved: The paper demonstrates that automated evaluation methods like BEM and InstructGPT-eval struggle to detect unattributable information in long-form LLM answers, leading to overestimation of performance. The specific improvements needed to address this limitation are not explored.
- What evidence would resolve it: Development and testing of new automated evaluation methods that incorporate hallucination detection capabilities, validated through human evaluation on datasets with known hallucinated content.

### Open Question 3
- Question: To what extent do the findings about lexical matching limitations generalize to other QA tasks beyond factoid information-seeking questions?
- Basis in paper: [inferred]
- Why unresolved: The paper focuses on factoid questions with short answers, but acknowledges that lexical matching is used in more complex QA tasks. The extent to which the identified limitations apply to multi-hop reasoning, discrete reasoning, and causal relation tasks remains unexplored.
- What evidence would resolve it: Systematic analysis of evaluation methods across various QA task types, comparing lexical matching limitations and the effectiveness of alternative evaluation approaches.

## Limitations

- Small sample size: The study uses only 301 questions from NQ-OPEN, which may not represent the full diversity of open-domain QA scenarios
- Subjective human annotation: Manual evaluation is resource-intensive and may introduce inter-annotator variability despite third-party resolution
- Limited answer format analysis: The study doesn't explore how different answer formats (yes/no, multiple spans, no answer) affect evaluation performance

## Confidence

- Lexical matching severely underestimates performance: High confidence
- Automated evaluation methods improve over lexical matching: Medium confidence
- Regex matching provides consistent middle ground: Medium confidence
- InstructGPT improves accuracy by 60%: High confidence
- LLMs outperform classical models only under human evaluation: Medium confidence

## Next Checks

1. Replicate the experiment on a larger, randomly sampled subset of NQ-OPEN (e.g., 1000+ questions) to assess the robustness of findings across a more diverse question distribution.

2. Conduct a cross-dataset validation using different open-domain QA datasets (e.g., TriviaQA, SQuAD Open) to determine if the evaluation method discrepancies generalize beyond NQ-OPEN.

3. Implement a hybrid evaluation framework that combines regex pattern matching with semantic similarity scoring, then compare its performance against pure lexical matching, BEM, and human judgment on a held-out test set.