---
ver: rpa2
title: Seeing and hearing what has not been said; A multimodal client behavior classifier
  in Motivational Interviewing with interpretable fusion
arxiv_id: '2309.14398'
source_url: https://arxiv.org/abs/2309.14398
tags:
- text
- modality
- change
- expressivity
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multimodal classifier for Motivational Interviewing
  that accurately distinguishes between change talk, sustain talk, and follow/neutral
  talk using text, prosody, facial expressivity, and body expressivity. The model
  employs a novel fusion architecture with self-attention to handle missing modalities
  and identify which modalities contribute most to classification.
---

# Seeing and hearing what has not been said; A multimodal client behavior classifier in Motivational Interviewing with interpretable fusion

## Quick Facts
- arXiv ID: 2309.14398
- Source URL: https://arxiv.org/abs/2309.14398
- Authors: 
- Reference count: 34
- Key outcome: MALEFIC multimodal classifier achieves higher F1 scores than existing approaches for classifying Motivational Interviewing client utterances into change talk, sustain talk, and follow/neutral talk

## Executive Summary
This paper presents MALEFIC, a novel multimodal classifier for Motivational Interviewing (MI) that combines text, audio, facial expressivity, and body expressivity to classify client utterances into change talk, sustain talk, and follow/neutral talk. The model employs a self-attention-based fusion architecture that dynamically weighs modality importance and handles missing data gracefully. MALEFIC outperforms existing unimodal and multimodal approaches, achieving superior F1 scores across all three classes while providing interpretable insights into which modalities contribute most to classification decisions.

## Method Summary
MALEFIC processes each modality (text via frozen BERT, audio via BEATS, facial and body via OpenFace/OpenPose) separately through modality-specific networks, then fuses them using a modified Embracenet architecture with self-attention. The model uses modality dropout during training to improve robustness to missing data and handles class imbalance through weighted random sampling. The frozen pre-trained embeddings reduce overfitting on the relatively small AnnoMI dataset while the self-attention mechanism identifies which modalities are most important for each classification decision.

## Key Results
- MALEFIC achieves higher F1 scores than all existing unimodal and multimodal approaches across all three MISC classes
- Text and facial expressivity are identified as the most influential modalities for classification
- The model successfully handles missing modalities through its self-attention-based fusion architecture
- MALEFIC demonstrates improved robustness to class imbalance compared to baseline models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MALEFIC's self-attention layers dynamically weigh modality importance per embedding dimension, allowing specialized processing for different semantic aspects.
- Mechanism: Self-attention computes weights for each modality at each embedding dimension, then probabilistically selects modalities during training and deterministically during inference.
- Core assumption: Different semantic aspects of MI utterances benefit from different modality combinations.
- Evidence anchors:
  - [abstract]: "The model employs a novel fusion architecture with self-attention to handle missing modalities and identify which modalities contribute most to classification."
  - [section]: "Self-attention is used to determine the significance of a given modality. If a modality is deemed important by the self-attention module, then this modality will be more likely to be selected."

### Mechanism 2
- Claim: Modality dropout during training forces the model to learn robust representations that can handle missing modalities gracefully.
- Mechanism: Random removal of available modalities during training prevents overfitting to specific modality combinations.
- Core assumption: Real-world deployment will encounter missing modalities.
- Evidence anchors:
  - [section]: "Furthermore, dropout of modality is used during training to prevent over fitting on modalities."
  - [section]: "This approach enables each preprocessing network to efficiently learn the data structure while also taking advantage of multimodality. Furthermore, it enables us to address missing data in our corpus."

### Mechanism 3
- Claim: Freezing pre-trained BERT and BEATS embeddings reduces overfitting on the relatively small AnnoMI dataset.
- Mechanism: Pre-trained embeddings provide strong semantic representations without requiring large amounts of training data.
- Core assumption: The domain of MI conversations shares semantic features with general language.
- Evidence anchors:
  - [section]: "We use a frozen Bert and Beats models to improve training time and avoid over fitting."
  - [section]: "The text is preprocessed using a frozen Bert pre-trained model from the HuggingFace library"

## Foundational Learning

- Concept: Multimodal fusion architectures
  - Why needed here: MI classification benefits from combining text, audio, and visual modalities that each capture different aspects of communication.
  - Quick check question: What are the three main approaches to multimodal fusion (early, late, hybrid) and which does MALEFIC use?

- Concept: Self-attention mechanisms
  - Why needed here: Need to dynamically weigh modality contributions based on semantic content of each utterance.
  - Quick check question: How does self-attention differ from simple weighted averaging in multimodal fusion?

- Concept: Handling class imbalance
  - Why needed here: The dataset is significantly imbalanced (60% F/N vs 24% CT and 16% ST).
  - Quick check question: What techniques can be used to handle imbalanced datasets beyond weighted sampling?

## Architecture Onboarding

- Component map: BERT (text) → Conv+Transformer → Embedding; BEATS (audio) → Embedding; OpenFace (facial) → Conv+Transformer → Embedding; OpenPose (body) → Conv+Transformer → Embedding; MALEFIC fusion layer → Classifier

- Critical path: Text preprocessing → MALEFIC fusion → Classification
- Design tradeoffs: Freezing BERT/BEATS speeds training but limits domain adaptation; modality dropout adds robustness but increases training time
- Failure signatures: Poor performance on minority classes (CT/ST) suggests overfitting; inconsistent results across runs suggest modality dropout issues
- First 3 experiments:
  1. Test unimodal text-only performance to establish baseline
  2. Test text+context fusion without MALEFIC to validate architecture choice
  3. Gradually add modalities (text→text+audio→text+audio+face) to measure incremental benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the fusion architecture be further optimized to improve classification accuracy for the underrepresented change talk and sustain talk classes in the MISC dataset?
- Basis in paper: [explicit] The paper highlights that the Follow/Neutral class is significantly more prevalent than the Change Talk or Sustain Talk classes and that the model's accuracy for change talk detection is lower than for other classes.
- Why unresolved: The paper does not provide a detailed exploration of potential optimization techniques for handling class imbalance in the fusion architecture.
- What evidence would resolve it: Experimental results demonstrating improved F1 scores for change talk and sustain talk classes after implementing techniques such as class weighting, oversampling, or alternative loss functions in the fusion architecture.

### Open Question 2
- Question: What specific linguistic or nonverbal cues in the text and facial expressivity modalities are most indicative of change talk versus sustain talk in Motivational Interviewing?
- Basis in paper: [explicit] The paper identifies text and facial expressivity as the most influential modalities in the classification process but does not delve into the specific features within these modalities that drive the classification.
- Why unresolved: The interpretability of the model provides insights into modality importance but lacks detailed analysis of the underlying features contributing to classification decisions.
- What evidence would resolve it: Detailed feature importance analysis or visualization of the most influential linguistic patterns and facial expressions for each MISC class, potentially using techniques like attention heatmaps or feature ablation studies.

### Open Question 3
- Question: How does the performance of the multimodal classifier compare when applied to real-time Motivational Interviewing sessions versus pre-recorded sessions?
- Basis in paper: [inferred] The paper discusses the potential for real-time application of the classifier but does not provide empirical data on its performance in live sessions.
- Why unresolved: The study focuses on the development and evaluation of the classifier using a pre-recorded dataset, leaving the real-time applicability untested.
- What evidence would resolve it: Comparative analysis of classification accuracy and latency between real-time and pre-recorded Motivational Interviewing sessions, highlighting any differences in performance or challenges encountered in real-time application.

## Limitations
- The AnnoMI dataset size (121 videos) and class imbalance may limit generalizability
- Pre-trained frozen embeddings may constrain adaptation to MI-specific linguistic patterns
- Self-attention interpretability depends on assumption that modality importance varies meaningfully across embedding dimensions

## Confidence

- **High confidence**: MALEFIC outperforms existing unimodal and multimodal approaches on F1 scores; text and facial expressivity are the most influential modalities; the model handles missing modalities effectively.
- **Medium confidence**: Self-attention meaningfully contributes to classification performance; modality dropout improves robustness; freezing pre-trained embeddings is beneficial.
- **Low confidence**: The relative importance rankings of modalities are stable across different MI contexts; the model generalizes well to different MI practitioners and populations.

## Next Checks

1. Cross-dataset validation: Test MALEFIC on an independent MI corpus to assess generalization beyond AnnoMI, particularly for minority classes (CT/ST).

2. Ablation study on self-attention: Systematically remove the self-attention layer and compare performance with weighted averaging to quantify its contribution.

3. Domain adaptation experiment: Fine-tune the frozen BERT embeddings on the MI corpus while keeping other components frozen to assess whether domain adaptation improves performance.