---
ver: rpa2
title: 'QuickQual: Lightweight, convenient retinal image quality scoring with off-the-shelf
  pretrained models'
arxiv_id: '2307.13646'
source_url: https://arxiv.org/abs/2307.13646
tags:
- quickqual
- images
- quality
- usable
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QuickQual presents a lightweight and effective method for automated
  retinal image quality scoring (RIQS). The core idea is to use a single pretrained
  DenseNet121 model from ImageNet combined with a Support Vector Machine (SVM) for
  scoring, instead of training a large multi-backbone model from scratch.
---

# QuickQual: Lightweight, convenient retinal image quality scoring with off-the-shelf pretrained models

## Quick Facts
- arXiv ID: 2307.13646
- Source URL: https://arxiv.org/abs/2307.13646
- Reference count: 16
- Primary result: Achieves 88.50% accuracy on EyeQ dataset using only pretrained DenseNet121 + SVM

## Executive Summary
QuickQual presents a lightweight and effective method for automated retinal image quality scoring (RIQS). The core idea is to use a single pretrained DenseNet121 model from ImageNet combined with a Support Vector Machine (SVM) for scoring, instead of training a large multi-backbone model from scratch. This approach sets a new state-of-the-art on the EyeQ dataset, achieving 88.50% accuracy compared to 88.00% for the previous best method (MCFNet), with a higher AUC of 0.9687 versus 0.9588. Additionally, QuickQual-MEME uses only 10 parameters to provide a continuous quality score with 89.18% accuracy, suggesting strong potential for practical deployment.

## Method Summary
QuickQual uses an ImageNet-pretrained DenseNet121 backbone to extract 1024-dimensional feature vectors from retinal images, then applies a standard SVM classifier for 3-way quality scoring (Good, Usable, Reject). For continuous scoring, QuickQual-MEME implements a Fixed Prior Linearisation scheme that converts the 3-way classification into a logistic regression task, using forward stepwise feature selection to identify the most informative features. Images are preprocessed by removing black borders, padding to square, resizing to 512Ã—512, and normalizing with mean=0.5 and std=0.5.

## Key Results
- Sets new state-of-the-art on EyeQ dataset: 88.50% accuracy vs 88.00% for MCFNet
- Achieves higher AUC: 0.9687 vs 0.9588 for previous best method
- QuickQual-MEME provides continuous quality scores with only 10 parameters and 89.18% accuracy
- Demonstrates that generic ImageNet features transfer effectively to retinal image quality assessment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretrained ImageNet DenseNet121 features transfer effectively to retinal image quality scoring.
- Mechanism: The DenseNet121 model pretrained on ImageNet learns generic perceptual features that capture relevant visual patterns (e.g., edges, textures) which are transferable to retinal image quality assessment without task-specific fine-tuning.
- Core assumption: Perceptual features learned from natural images are sufficiently general to encode quality-relevant information in retinal images.
- Evidence anchors:
  - [abstract] "RIQS can be solved with generic perceptual features learned on natural images, as opposed to requiring DL models trained on large amounts of fundus images"
  - [section] "Inspired by that, we set out to investigate whether we can develop a simpler yet effective automated RIQS method that uses such an off-the-shelf model with a classical machine learning classifier."

### Mechanism 2
- Claim: SVM classifier on frozen DenseNet121 features achieves state-of-the-art performance.
- Mechanism: Instead of fine-tuning the entire deep network, the model extracts 1,024-dimensional feature vectors from DenseNet121 and trains a lightweight SVM classifier, reducing computational complexity while maintaining accuracy.
- Core assumption: The intermediate features from DenseNet121 are sufficiently discriminative for the quality scoring task.
- Evidence anchors:
  - [abstract] "QuickQual performs very well, setting a new state-of-the-art for EyeQ (Accuracy: 88.50% vs 88.00% for MCFNet; AUC: 0.9687 vs 0.9588)"
  - [section] "QuickQual, a simple approach to RIQS, consisting of a single off-the-shelf ImageNet-pretrained Densenet121 backbone plus a Support Vector Machine (SVM)"

### Mechanism 3
- Claim: Fixed Prior Linearisation scheme preserves usable class information while enabling continuous quality scoring.
- Mechanism: By setting the optimal output p(Bad) to 0 for Good images, 1 for Bad images, and 0.5 for Usable images during model fitting, the scheme maintains ordinal relationships while enabling logistic regression formulation.
- Core assumption: The Usable class is meaningfully positioned between Good and Bad classes in quality space.
- Evidence anchors:
  - [abstract] "Additionally, we propose a Fixed Prior linearisation scheme, that converts EyeQ from a 3-way classification to a continuous logistic regression task"
  - [section] "we simply set p = 0.5 and thus ask our model to map Usable images in-between Good and Bad ones, thus retaining the information in the labels"

## Foundational Learning

- Concept: Transfer learning and feature extraction from pretrained models
  - Why needed here: The approach relies on using DenseNet121 features without fine-tuning, requiring understanding of how pretrained features can be reused for different tasks
  - Quick check question: Why can features learned from natural images be useful for retinal image quality assessment?

- Concept: Support Vector Machine classification
  - Why needed here: QuickQual uses an SVM classifier on top of extracted features, requiring understanding of SVM principles and implementation
  - Quick check question: What is the advantage of using an SVM over fine-tuning the entire neural network?

- Concept: Logistic regression and probability calibration
  - Why needed here: QuickQual-MEME produces continuous quality scores using logistic regression, and Fixed Prior Linearisation relies on probability interpretation
  - Quick check question: How does the Fixed Prior Linearisation scheme ensure the Usable class is positioned between Good and Bad in probability space?

## Architecture Onboarding

- Component map:
  DenseNet121 backbone (ImageNet-pretrained) -> Feature extraction layer (1,024-dimensional vectors) -> Classifier (SVM for QuickQual, Logistic Regression for QuickQual-MEME) -> Output

- Critical path:
  1. Load and preprocess input image
  2. Extract features using DenseNet121
  3. Apply classifier (SVM or logistic regression)
  4. Output class probabilities or continuous score

- Design tradeoffs:
  - Using frozen pretrained features vs. fine-tuning the entire network
  - SVM classifier (more parameters but potentially better performance) vs. Logistic Regression (minimal parameters but simpler)
  - 3-way classification (more granular) vs. binary classification with continuous scoring (simpler interpretation)

- Failure signatures:
  - Poor accuracy indicates feature extraction or classifier issues
  - Calibration problems suggest SVM probability estimates or Fixed Prior Linearisation issues
  - Slow inference indicates DenseNet121 computational bottleneck

- First 3 experiments:
  1. Verify feature extraction works by checking output dimensions and visualizing some features
  2. Test classifier performance on a small validation set to ensure basic functionality
  3. Compare binary vs. 3-way classification performance to validate Fixed Prior Linearisation approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the quality score from QuickQual-MEME correlate with expert rankings of actual image quality?
- Basis in paper: [explicit] The authors state they plan to evaluate this in the future by having experts rank images in terms of quality and examining the correlation with QuickQual-MEME's quality score.
- Why unresolved: The evaluation was not performed in this study. Only limited qualitative examples were shown.
- What evidence would resolve it: A study where retinal experts rank images by quality and the correlation between their rankings and QuickQual-MEME scores is calculated.

### Open Question 2
- Question: Can an even more lightweight pretrained model achieve similar performance to QuickQual while enabling faster computation?
- Basis in paper: [explicit] The authors state they plan to evaluate other pretrained DL models to see whether a similarly performant yet more light-weight model could be found.
- Why unresolved: Only DenseNet121 was evaluated. The possibility of other models was mentioned but not explored.
- What evidence would resolve it: An evaluation comparing QuickQual to other lightweight pretrained models on the same EyeQ dataset.

### Open Question 3
- Question: Does QuickQual generalize well to external datasets like UK Biobank?
- Basis in paper: [explicit] The authors state they plan to externally validate QuickQual and QuickQual-MEME on images from UK Biobank.
- Why unresolved: Only internal validation on the EyeQ dataset was performed. No external datasets were used.
- What evidence would resolve it: Testing QuickQual's performance on an external retinal image dataset like UK Biobank and comparing to internal results.

## Limitations

- The improvement over MCFNet (88.50% vs 88.00% accuracy) is modest and requires statistical significance testing
- Computational efficiency claims lack quantitative benchmarks and timing measurements
- Generalization to external datasets beyond EyeQ remains untested
- The assumption about ordinal relationships in the Fixed Prior Linearisation scheme needs further validation

## Confidence

**High Confidence**: The core mechanism of using pretrained DenseNet121 features with SVM classification is well-established and the implementation details are sufficiently specified for reproduction. The improvement over existing methods on the EyeQ dataset is clearly demonstrated.

**Medium Confidence**: The Fixed Prior Linearisation scheme and QuickQual-MEME approach are innovative but have limited validation. The assumption that Usable images should be positioned between Good and Bad in probability space needs further verification across different datasets.

**Low Confidence**: The computational efficiency claims and practical deployment benefits lack quantitative benchmarks. The generalization potential to other retinal image quality datasets beyond EyeQ remains untested.

## Next Checks

1. **Statistical Significance Testing**: Perform paired t-tests or McNemar's test to determine if the 0.5% accuracy improvement and AUC difference between QuickQual and MCFNet are statistically significant, accounting for the dataset size and class imbalance.

2. **Cross-Dataset Validation**: Test QuickQual on at least one additional retinal image quality dataset (such as RIQA or another Kaggle EyePacs variant) to verify the transfer learning claims and assess generalization beyond the EyeQ dataset.

3. **Ablation Study**: Conduct systematic experiments removing key components (ImageNet pretraining, SVM classifier, feature dimensionality) to quantify their individual contributions to the final performance, particularly to validate whether the DenseNet121 features are truly essential or if simpler feature extractors could achieve similar results.