---
ver: rpa2
title: Context-Driven Interactive Query Simulations Based on Generative Large Language
  Models
arxiv_id: '2312.09631'
source_url: https://arxiv.org/abs/2312.09631
tags:
- query
- user
- retrieval
- information
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes new query generation methods for simulating
  context-driven user interactions in information retrieval systems. The methods build
  upon recent Large Language Model (LLM) approaches and consider the user's context
  throughout the simulation of a search session.
---

# Context-Driven Interactive Query Simulations Based on Generative Large Language Models

## Quick Facts
- arXiv ID: 2312.09631
- Source URL: https://arxiv.org/abs/2312.09631
- Reference count: 38
- Primary result: Context-driven LLM-based query generation outperforms rule-based approaches in simulated IR sessions

## Executive Summary
This work proposes new query generation methods for simulating context-driven user interactions in information retrieval systems. The methods build upon recent Large Language Model (LLM) approaches and consider the user's context throughout the simulation of a search session. Compared to simple context-free query generation approaches, these methods show better effectiveness and allow the simulation of more efficient IR sessions. The results demonstrate that integrating user context into simulations substantially impacts information gain.

## Method Summary
The paper implements a user simulation framework that generates queries using GPT-based probabilistic methods and Doc2Query-based rule-based methods, with/without context and feedback. The framework simulates different click models (Perfect, Navigational, Informational, Almost Random) and stopping strategies (Static and Dynamic). Experiments compare BM25 and MonoT5 retrieval models under different configurations and evaluate using Information Gain (IG), Session-based Discounted Cumulative Gain (sDCG), and Session Rank-Biased Precision (sRBP) metrics.

## Key Results
- Generative LLM-based query generation outperforms rule-based approaches in simulated IR sessions
- Integrating user feedback (relevance judgments) into query generation improves retrieval effectiveness
- Dynamic stopping criteria (time-based) outperform static stopping criteria (fixed number of results) in terms of information gain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative LLM-based query generation outperforms rule-based approaches in simulated IR sessions.
- Mechanism: LLMs can dynamically integrate user context (topic background, feedback, and session history) into query reformulations, producing semantically rich and diverse queries that better match the user's evolving information need.
- Core assumption: The LLM has sufficient training data and capability to generate high-fidelity queries from topic descriptions and session context.
- Evidence anchors:
  - [abstract]: "Compared to simple context-free query generation approaches, these methods show better effectiveness and allow the simulation of more efficient IR sessions."
  - [section]: "The use of probabilistic query generation methods shows promising results, dominating rule-based approaches and being more capable of integrating background knowledge of the topic."
  - [corpus]: Weak - no direct corpus evidence; relies on internal comparisons and assumed LLM capability.
- Break condition: LLM generates queries that are too generic, off-topic, or fail to capture the nuance of the user's information need, leading to ineffective search sessions.

### Mechanism 2
- Claim: Integrating user feedback (relevance judgments) into query generation improves retrieval effectiveness.
- Mechanism: By extracting terms from relevant documents seen during the session and incorporating them into subsequent queries, the system adapts to the user's feedback and refines the search focus.
- Core assumption: The relevance feedback is accurate and the extracted terms are representative of the user's information need.
- Evidence anchors:
  - [abstract]: "Integrating feedback into probabilistic approaches would be an exciting direction for future work."
  - [section]: "D2Q+ integrates the relevance feedback of the simulated user... comparing the plots displays that feedback leads to a clear increase in information gain."
  - [corpus]: Weak - no direct corpus evidence; relies on internal experimental results.
- Break condition: Relevance feedback is noisy or misleading, causing the system to incorporate irrelevant terms and degrade retrieval performance.

### Mechanism 3
- Claim: Dynamic stopping criteria (time-based) outperform static stopping criteria (fixed number of results) in terms of information gain.
- Mechanism: Dynamic stopping allows the user to continue searching when relevant information is found, preventing premature termination and enabling more thorough exploration of the search space.
- Core assumption: Users have a consistent behavior of giving up after a certain time since the last relevant document was seen.
- Evidence anchors:
  - [section]: "In Figure 3, it can be seen that the dynamic click behavior positively affects the information gain... both an increase in the number of documents examined and an increase in the dynamic time budget have a positive effect."
  - [section]: "Dynamic stop behavior is notably rewarded by the other measures... the larger number of examined documents can likely further be attributed to this opposing evaluation behavior."
  - [corpus]: Weak - no direct corpus evidence; relies on internal experimental results and foraging theory assumptions.
- Break condition: Users have inconsistent or unpredictable behavior, making the time-based stopping criterion ineffective or counterproductive.

## Foundational Learning

- Concept: User simulation in information retrieval
  - Why needed here: The entire paper is based on simulating user interactions to evaluate IR systems. Understanding the principles of user simulation is crucial to grasp the paper's contributions.
  - Quick check question: What are the key components of a user simulation model in IR?

- Concept: Query generation methods (probabilistic vs. rule-based)
  - Why needed here: The paper proposes new query generation methods based on LLMs and compares them to rule-based approaches. Understanding the differences and trade-offs between these methods is essential.
  - Quick check question: How do probabilistic query generation methods differ from rule-based methods in terms of context integration and flexibility?

- Concept: Evaluation measures for interactive IR (sDCG, sRBP, effort-based)
  - Why needed here: The paper uses various evaluation measures to assess the effectiveness of the simulated sessions. Understanding these measures and their underlying assumptions is crucial to interpret the results.
  - Quick check question: How do sDCG, sRBP, and effort-based measures differ in terms of their focus on query formulation costs and session progress?

## Architecture Onboarding

- Component map: GPT-based (probabilistic), Doc2Query-based (rule-based) query generation -> BM25/MonoT5 retrieval -> Click behavior simulation -> Session context update
- Critical path: 1. Generate queries based on topic and session context -> 2. Retrieve documents using the selected retrieval model -> 3. Simulate user click behavior on retrieved documents -> 4. Update session context based on user feedback -> 5. Repeat steps 1-4 until stopping criterion is met -> 6. Evaluate session effectiveness using selected measures
- Design tradeoffs: Probabilistic vs. rule-based query generation (Flexibility and context integration vs. computational efficiency and interpretability); Static vs. dynamic stopping criteria (Simplicity and predictability vs. adaptability and potential for higher information gain); Dense vs. sparse retrieval models (Higher effectiveness vs. computational cost and resource requirements)
- Failure signatures: Ineffective query generation (Poor retrieval performance, high user effort, low information gain); Unrealistic user simulation (Mismatch between simulated and real user behavior, misleading evaluation results); Inadequate evaluation measures (Failure to capture important aspects of interactive IR, leading to suboptimal system design)
- First 3 experiments: 1. Compare probabilistic (GPT) and rule-based (Doc2Query) query generation methods with and without topic background to assess the impact of context integration on retrieval effectiveness; 2. Evaluate the effect of relevance feedback on query generation and retrieval performance by comparing Doc2Query variants with and without feedback integration; 3. Analyze the impact of dynamic vs. static stopping criteria on session effectiveness and user effort to determine the optimal stopping strategy for interactive IR simulations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific parameters and justifications for the cost model used in the effort-based evaluation measure?
- Basis in paper: [explicit] The paper mentions that the effort-based measure considers costs for specific user actions, including click decisions, document reading, and relevance judgments, but does not specify the values or justifications for these costs.
- Why unresolved: The paper acknowledges that the costs for specific user actions used in the effort-based measure should be specified and justified but does not provide these details.
- What evidence would resolve it: A detailed breakdown of the cost model, including specific values assigned to each user action and the rationale behind these values, would resolve this question.

### Open Question 2
- Question: How can the evaluation of simulated interactive retrieval sessions be improved to account for the limitations of ad-hoc test collections?
- Basis in paper: [inferred] The paper highlights that ad-hoc test collections may be biased towards certain retrieval methods and suggests that providing adequate resources, data, and tools is part of future work.
- Why unresolved: The paper identifies the limitations of ad-hoc test collections but does not propose specific solutions or methodologies to overcome these limitations.
- What evidence would resolve it: Research demonstrating new evaluation methodologies or test collections that mitigate the biases of ad-hoc collections would resolve this question.

### Open Question 3
- Question: What are the optimal parameters for the session-based rank-biased precision (sRBP) measure to ensure accurate evaluations of simulated interactive retrieval sessions?
- Basis in paper: [explicit] The paper mentions that determining suitable parameters for sRBP is vital for future work and suggests analyzing real user behavior to estimate these parameters.
- Why unresolved: The paper sets parameters for sRBP but does not provide a methodology for determining optimal values based on real user behavior.
- What evidence would resolve it: Studies analyzing real user interactions and deriving optimal sRBP parameters from this data would resolve this question.

## Limitations
- Reliance on simulated user behavior rather than real user studies introduces uncertainty about generalizability to actual search scenarios
- Effectiveness of context integration depends heavily on the quality and relevance of provided topic descriptions
- Computational overhead of LLM-based query generation could limit practical deployment at scale

## Confidence
- Medium confidence in core findings due to reliance on simulated rather than real user behavior

## Next Checks
1. Conduct user studies to validate whether the simulated interactions and context-driven query generation patterns align with actual user search behavior across diverse information needs.
2. Test the robustness of the approach with incomplete or noisy context information to evaluate performance degradation and identify minimum viable context requirements.
3. Compare the computational efficiency and resource requirements of the LLM-based query generation against the observed performance gains to determine practical deployment thresholds.