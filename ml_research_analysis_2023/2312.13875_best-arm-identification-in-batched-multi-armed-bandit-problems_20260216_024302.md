---
ver: rpa2
title: Best Arm Identification in Batched Multi-armed Bandit Problems
arxiv_id: '2312.13875'
source_url: https://arxiv.org/abs/2312.13875
tags:
- arms
- sampling
- stage
- e-01
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a general linear programming (LP) framework
  for the best arm identification (BAI) problem in batched multi-armed bandits. The
  authors propose a two-stage algorithm (LP2S) that combines an aggressive arm-elimination
  stage based on LP-derived thresholds with a finer exploration stage.
---

# Best Arm Identification in Batched Multi-armed Bandit Problems

## Quick Facts
- **arXiv ID**: 2312.13875
- **Source URL**: https://arxiv.org/abs/2312.13875
- **Reference count**: 8
- **One-line primary result**: A general linear programming (LP) framework for best arm identification in batched multi-armed bandits with a two-stage algorithm that achieves expected sampling costs of O(Lh(R)) where h(R) is polynomial in R.

## Executive Summary
This paper introduces a novel linear programming framework for the best arm identification problem in batched multi-armed bandit settings, particularly when the number of arms K is large and the number of batches R is small. The authors propose a two-stage algorithm (LP2S) that combines an aggressive arm-elimination stage based on LP-derived thresholds with a finer exploration stage. The method achieves expected sampling costs of O(Lh(R)) where h(R) is polynomial in R, outperforming existing methods like UCB-type algorithms and Thompson sampling in regimes where R ≪ K.

## Method Summary
The method employs a linear programming (LP) relaxation that converts peer-dependent policies into peer-independent ones, enabling efficient computation for large K. The two-stage LP2S algorithm first uses LP-derived threshold policies to aggressively eliminate suboptimal arms, then applies uniform exploration among survivors. The LP framework can be tailored to various BAI settings (PAC, SRM, FC) and theoretically guarantees good performance in terms of simple regret and probability of recommending the best arm.

## Key Results
- LP2S achieves expected sampling costs of O(Lh(R)) where h(R) is polynomial in R
- Outperforms existing methods like UCB-type algorithms and Thompson sampling when K is large and R is small
- Theoretical analysis shows good performance in terms of simple regret and probability of recommending the best arm
- Numerical studies demonstrate effectiveness across different BAI settings (PAC, SRM, FC)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The LP framework allows aggressive arm elimination by relaxing peer-dependent policies into peer-independent ones, enabling faster convergence when K is large.
- **Mechanism**: By aggregating the state space of non-focal arms into marginal distributions, the LP reduces the complexity from K-dimensional to 1-dimensional per arm, enabling a threshold-based elimination policy that quickly filters out suboptimal arms.
- **Core assumption**: The loss of information from ignoring peer dependencies is negligible when K is large and arms are densely distributed in expected reward space.
- **Evidence anchors**:
  - [abstract] "The LP framework allows for efficient computation of peer-independent policies and can be tailored to various BAI settings."
  - [section 2.3] "Following this idea, we try to relax OPT-dep to an optimization problem that only relies on the states of the focal arm."
  - [corpus] Weak - related papers focus on batched complexity but do not address the peer-independent relaxation.
- **Break condition**: When K is small or the arm reward distributions are highly separated, the loss of information from ignoring peer dependencies becomes significant and the aggressive elimination may discard the best arm.

### Mechanism 2
- **Claim**: The two-stage algorithm achieves expected sampling cost O(Lh(R)) where h(R) is polynomial in R, significantly better than existing methods when R ≪ K.
- **Mechanism**: Stage 1 uses the LP-derived threshold policy to aggressively reduce the number of surviving arms to L ≪ K, then Stage 2 applies uniform exploration among survivors with expected cost LR.
- **Core assumption**: The probability of eliminating the best arm in Stage 1 is low enough when L is chosen appropriately relative to δ0 and R.
- **Evidence anchors**:
  - [abstract] "The two-stage algorithm achieves expected sampling costs of O(Lh(R)) where h(R) is polynomial in R."
  - [section 2.4] "The expected number of arms can be decreased to some L ≪ K after stage 1."
  - [section 3.2] "E(T) = Kf* + LR" and Corollary 2 provides the polynomial bound.
- **Break condition**: When R is too small relative to K, the probability of no arms surviving Stage 1 becomes significant, making the algorithm fail.

### Mechanism 3
- **Claim**: The threshold policy derived from LP has theoretical guarantees for simple regret and probability of recommending the best arm.
- **Mechanism**: The threshold policy ensures that arms with cumulative rewards below s*(r) are eliminated, while those above are pulled with probability 1, creating a monotonic elimination pattern that preserves high-reward arms.
- **Core assumption**: The optimal solution to the LP problem has the threshold structure proven in Theorem 1.
- **Evidence anchors**:
  - [section 3.1] "Theorem 1. Suppose LP-ind is feasible. Then there exists an optimal solution... such that the actions... satisfy a*(r,s) = 0 for s < s*(r), 1 for s > s*(r)."
  - [section 3.3] Theorems 3, 4, and 5 provide regret bounds for different settings.
  - [corpus] Weak - related papers focus on batched complexity but do not address the threshold structure of LP-derived policies.
- **Break condition**: When the prior distribution has heavy tails or the optimal solution violates the threshold structure, the theoretical guarantees may not hold.

## Foundational Learning

- **Concept**: Linear programming relaxation of peer-dependent policies
  - Why needed here: To make the problem computationally tractable by reducing the state space from K-dimensional to 1-dimensional per arm
  - Quick check question: How does aggregating the states of non-focal arms into marginal distributions affect the information available for decision-making?

- **Concept**: Bayesian bandit framework with independent priors
  - Why needed here: Enables computation of marginal distributions for non-focal arms without observing their rewards, which is crucial for the peer-independent relaxation
  - Quick check question: Given a Beta prior Beta(a,b) for an arm, what is the posterior distribution after observing s successes in r pulls?

- **Concept**: Threshold policies in sequential decision making
  - Why needed here: The LP-derived optimal solution has a threshold structure that enables aggressive arm elimination while maintaining theoretical guarantees
  - Quick check question: In what scenarios would a threshold policy be preferable to a probability-based policy for arm selection?

## Architecture Onboarding

- **Component map**: LP formulation (OPT-ind) -> Threshold policy extraction -> Stage 1 execution -> Arm survival counting -> Stage 2 uniform exploration -> Recommendation
- **Critical path**: LP solution computation -> Threshold extraction -> Stage 1 arm elimination -> Stage 2 exploration -> Final recommendation
- **Design tradeoffs**:
  - Aggressive vs conservative elimination (controlled by L and δ0)
  - Computational complexity vs solution quality (LP relaxation vs exact OPT-dep)
  - Uniform exploration in Stage 2 vs more sophisticated methods (simplicity vs potential improvement)
- **Failure signatures**:
  - J = 0 (no arms survive Stage 1) -> Algorithm fails, recommends random arm
  - High variance in J -> Inconsistent performance across runs
  - LP infeasibility -> Parameter tuning needed
- **First 3 experiments**:
  1. Test LP2S with varying L values on a small K (e.g., K=10) to observe the tradeoff between elimination aggressiveness and success probability
  2. Compare the threshold structure of the LP solution with the actual elimination pattern on a synthetic problem with known optimal arm
  3. Measure the impact of different priors (Beta(a,b) with varying a,b) on the expected sampling cost and regret bounds

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the information loss from ignoring non-focal arm states in the LP framework compare to peer-dependent policies in terms of final arm selection accuracy?
- **Basis in paper**: [explicit] The paper acknowledges the need for analysis on the gap between optimal solutions of OPT-dep and OPT-ind, stating "the key problem is how to quantify the information loss in ignoring the states of arms other than the focal arm."
- **Why unresolved**: The paper introduces the LP framework but does not provide a quantitative comparison between the information loss and the performance difference between peer-dependent and peer-independent policies.
- **What evidence would resolve it**: A detailed theoretical analysis comparing the expected simple regret or probability of recommending the best arm between peer-dependent and peer-independent policies under the same conditions.

### Open Question 2
- **Question**: Can the LP framework be effectively extended to handle rewards following continuous distributions, and what would be the computational complexity implications?
- **Basis in paper**: [explicit] The paper suggests extending the LP formulation to discrete distributions is straightforward, but states "If rewards follow a continuous distribution, the state space becomes continuous, and how to formulate a tractable optimization problem is somewhat obscure."
- **Why unresolved**: The paper does not explore or provide a solution for extending the LP framework to continuous reward distributions, leaving this as an open area for future research.
- **What evidence would resolve it**: A proposed method for adapting the LP framework to continuous distributions, along with computational complexity analysis and empirical validation.

### Open Question 3
- **Question**: How would the LP framework perform in contextual bandit problems, and what modifications would be necessary to incorporate contextual information?
- **Basis in paper**: [explicit] The paper mentions the potential for generalizing the method to contextual bandit problems, stating "the method can be possibly generalized to contextual bandit problems, where the framework should allow transition probabilities P(r, s) and actions a(r, s) depend on the observed contextual information."
- **Why unresolved**: The paper does not explore or implement the adaptation of the LP framework to contextual bandit problems, leaving the performance and necessary modifications unknown.
- **What evidence would resolve it**: A theoretical extension of the LP framework to contextual bandits, along with empirical results demonstrating its effectiveness compared to existing methods in contextual bandit settings.

## Limitations
- Theoretical guarantees depend on optimal LP solution having threshold structure, which may not hold for all problem instances
- Peer-independent relaxation could lead to information loss when K is small or arm distributions are highly separated
- Two-stage algorithm requires careful parameter tuning (L, δ0) and fails when no arms survive Stage 1

## Confidence
- **High confidence**: The LP framework construction and threshold policy structure (Sections 2.3-2.4, Theorem 1)
- **Medium confidence**: Theoretical regret bounds (Section 3.3) - depend on specific problem instances and prior distributions
- **Medium confidence**: Numerical performance claims (Section 4) - based on simulations with Beta priors, generalizability to other settings unclear

## Next Checks
1. Test LP2S on problems where arms have widely separated means to verify the peer-independent relaxation doesn't discard the optimal arm
2. Implement the exact peer-dependent optimization (OPT-dep) on small K problems to measure the information loss from the LP relaxation
3. Vary the number of batches R relative to K systematically to identify the regime where LP2S's O(Lh(R)) advantage becomes significant