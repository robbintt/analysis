---
ver: rpa2
title: 'From Relevance to Utility: Evidence Retrieval with Feedback for Fact Verification'
arxiv_id: '2310.11675'
source_url: https://arxiv.org/abs/2310.11675
tags:
- evidence
- retrieval
- claim
- verification
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel feedback-based evidence retrieval
  (FER) approach for fact verification (FV). Unlike existing methods based on relevance
  ranking, FER incorporates feedback from a claim verifier to optimize evidence retrieval
  based on utility.
---

# From Relevance to Utility: Evidence Retrieval with Feedback for Fact Verification

## Quick Facts
- **arXiv ID**: 2310.11675
- **Source URL**: https://arxiv.org/abs/2310.11675
- **Reference count**: 40
- **Primary result**: Novel feedback-based evidence retrieval (FER) approach achieves 23.7% F1 improvement over baselines on FEVER dataset

## Executive Summary
This paper addresses the challenge of evidence retrieval for fact verification by introducing a novel Feedback-based Evidence Retrieval (FER) approach. Unlike traditional methods that rely solely on relevance ranking, FER incorporates feedback from a claim verifier to optimize evidence retrieval based on utility. The key insight is that relevance alone doesn't guarantee useful evidence for verification - evidence must also be effective at helping the verifier make accurate decisions. FER uses a coarse-to-fine strategy, first retrieving relevant candidates and then refining them using utility-based feedback. Experiments on the FEVER dataset demonstrate significant improvements over state-of-the-art baselines.

## Method Summary
FER employs a coarse-to-fine retrieval strategy for fact verification. The coarse retrieval stage uses BERT-based relevance ranking to identify a candidate set of sentences related to the claim. The fine-grained retrieval stage then refines this set using feedback from a claim verifier, specifically leveraging the divergence in utility between how effectively the verifier can use the retrieved evidence versus ground-truth evidence to produce the final claim label. The retriever is trained using a combined loss function that includes both evidence classification loss and utility divergence loss, with the latter serving as the feedback signal. The approach uses Gumbel-Softmax for differentiable evidence selection and can incorporate feedback from multiple verifier models.

## Key Results
- FER achieves 23.7% F1 improvement over the best baseline on FEVER dataset evidence retrieval
- Superior precision compared to baselines (76.0% vs 54.1% for the best baseline) at top-5 evidence
- Lower recall than baselines but with significantly higher precision, demonstrating effective utility-based filtering
- Performance is robust across different claim verifier models used for feedback

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Utility divergence between ground-truth and retrieved evidence provides effective feedback for retriever training
- **Mechanism**: The retriever is trained using a loss function that combines evidence classification and utility divergence, where utility divergence measures the difference in claim verification performance when using ground-truth vs retrieved evidence
- **Core assumption**: The divergence in claim verification performance directly reflects the quality of retrieved evidence
- **Evidence anchors**:
  - [abstract] "As a feedback signal we use the divergence in utility between how effectively the verifier utilizes the retrieved evidence and the ground-truth evidence to produce the final claim label"
  - [section 3.2] "We leverage the utility that the verifier obtains by consuming the retrieved evidence as the feedback"
  - [corpus] Weak evidence - this specific utility divergence mechanism is novel to this paper
- **Break condition**: If the verifier's probability distribution doesn't accurately reflect evidence quality, or if ground-truth evidence is not available during training

### Mechanism 2
- **Claim**: Coarse-to-fine retrieval strategy improves precision while maintaining reasonable recall
- **Mechanism**: First retrieves a large candidate set using PRP-based relevance ranking, then refines this set using feedback-based fine-grained retrieval that focuses on utility rather than just relevance
- **Core assumption**: A large candidate set containing most relevant evidence allows utility-based filtering to identify the most useful subset
- **Evidence anchors**:
  - [section 3] "FER leverages a coarse-to-fine strategy. Initially, it identifies a candidate set of relevant sentences to the given claim from the large-scale corpus"
  - [section 4.2] "The superior precision achieved by FER comes at the expense of lower recall when compared to baselines"
  - [corpus] Supported by general retrieval literature on coarse-to-fine strategies
- **Break condition**: If candidate set size K is too small to contain relevant evidence, or too large to make utility-based filtering effective

### Mechanism 3
- **Claim**: Feedback from multiple verifier models can improve retrieval performance
- **Mechanism**: The fine-grained retriever can be trained using feedback from different claim verification models, allowing the retriever to adapt to different verification strategies
- **Core assumption**: Different verification models have complementary strengths in identifying useful evidence
- **Evidence anchors**:
  - [section 4.4] "We explore the potential of utilizing off-the-shelf claim verifiers to offer feedback to Rθ in our FER"
  - [section 4.4] "Good retrievers and verification models can mutually benefit from each other's strengths"
  - [corpus] Supported by general principle that ensemble methods often outperform individual models
- **Break condition**: If verifier feedback is inconsistent or contradictory across different models

## Foundational Learning

- **Concept**: Probability Ranking Principle (PRP)
  - **Why needed here**: Understanding why traditional relevance-based retrieval fails for fact verification
  - **Quick check question**: Why does ranking by relevance probability not guarantee useful evidence for claim verification?

- **Concept**: Evidence classification vs. evidence utility
  - **Why needed here**: Distinguishing between retrieving evidence that matches ground-truth and evidence that helps verification
  - **Quick check question**: How does utility divergence differ from traditional classification loss in retriever training?

- **Concept**: Gumbel-Softmax for differentiable evidence selection
  - **Why needed here**: Understanding how discrete evidence selection can be made differentiable for training
  - **Quick check question**: What role does Gumbel-Softmax play in making evidence selection differentiable?

## Architecture Onboarding

- **Component map**: Document retrieval → Coarse retrieval (PRP-based) → Fine-grained retrieval (feedback-based) → Claim verification
- **Critical path**: Claim → Document retrieval → Coarse retrieval → Fine-grained retrieval → Verifier feedback → Optimized retriever
- **Design tradeoffs**: Precision vs. recall tradeoff (FER prioritizes precision), feedback quality vs. computational cost, coarse vs. fine granularity
- **Failure signatures**: Low recall despite high precision, retriever performance degradation with different verifiers, sensitivity to candidate set size K
- **First 3 experiments**:
  1. Compare FER with and without utility divergence loss to isolate feedback effect
  2. Vary candidate set size K to find optimal tradeoff between coverage and utility
  3. Test FER with different verifier models to assess feedback robustness

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can evidence retrieval and claim verification models be optimized jointly in an end-to-end manner to improve overall performance?
- **Basis in paper**: [explicit] The paper acknowledges that current retrieval and verification models are optimized independently, lacking conditional optimization or joint end-to-end optimization.
- **Why unresolved**: Joint optimization of retrieval and verification models is complex and requires careful design of the optimization objective to balance the needs of both components.
- **What evidence would resolve it**: Empirical results demonstrating improved performance on fact verification tasks when using jointly optimized retrieval and verification models compared to separately optimized models.

### Open Question 2
- **Question**: Are there alternative feedback formulations that can be incorporated into the evidence retrieval process beyond the probability distribution from the claim verifier?
- **Basis in paper**: [inferred] The paper mentions that there exist multiple methods for quantifying the divergence in utility of retrieval results, such as the gradients of the verification loss, and that various forms of feedback can be incorporated.
- **Why unresolved**: Exploring different feedback formulations requires extensive experimentation and analysis to determine their effectiveness in improving evidence retrieval performance.
- **What evidence would resolve it**: Comparative studies showing the impact of different feedback formulations on evidence retrieval performance and their ability to capture useful signals for claim verification.

### Open Question 3
- **Question**: How does the evidence retrieval performance of the proposed FER method generalize to other fact verification datasets beyond FEVER?
- **Basis in paper**: [explicit] The paper acknowledges that the effectiveness of FER has only been demonstrated on the FEVER dataset and encourages future work to create further claim verification datasets.
- **Why unresolved**: The performance of FER on other datasets may vary due to differences in data characteristics, claim types, and evidence structures.
- **What evidence would resolve it**: Empirical results on multiple fact verification datasets showing consistent improvements in evidence retrieval performance using FER compared to baseline methods.

## Limitations
- Relies heavily on availability of ground-truth evidence during training, limiting real-world applicability
- Coarse-to-fine strategy introduces precision-recall tradeoff that may not be optimal for all fact verification tasks
- Assumes claim verifier's probability distribution accurately reflects evidence quality, which may not hold for all verifier architectures

## Confidence
- **High Confidence**: The coarse-to-fine retrieval strategy and its implementation details (Section 3.2-3.3)
- **Medium Confidence**: The utility divergence mechanism and its effectiveness in improving retrieval quality (Section 3.1)
- **Medium Confidence**: The experimental results and their interpretation, though some baseline comparisons could be more comprehensive

## Next Checks
1. **Generalization Test**: Evaluate FER on datasets without ground-truth evidence to assess real-world applicability and test zero-shot or few-shot adaptation capabilities.

2. **Verifier Independence**: Test FER's performance when trained with different claim verifier models to verify the robustness of the feedback mechanism across architectures.

3. **Efficiency Analysis**: Measure the computational overhead introduced by the utility divergence calculation and fine-grained retrieval refinement compared to traditional relevance-based approaches.