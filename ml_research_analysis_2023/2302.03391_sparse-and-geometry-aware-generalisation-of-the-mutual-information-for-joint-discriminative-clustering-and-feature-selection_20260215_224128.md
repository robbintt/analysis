---
ver: rpa2
title: Sparse and geometry-aware generalisation of the mutual information for joint
  discriminative clustering and feature selection
arxiv_id: '2302.03391'
source_url: https://arxiv.org/abs/2302.03391
tags:
- gemini
- clustering
- selection
- variables
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Sparse GEMINI, a novel algorithm for joint
  discriminative clustering and feature selection. The method combines the GEMINI
  objective for discriminative clustering with the LassoNet framework for feature
  selection in neural networks.
---

# Sparse and geometry-aware generalisation of the mutual information for joint discriminative clustering and feature selection

## Quick Facts
- arXiv ID: 2302.03391
- Source URL: https://arxiv.org/abs/2302.03391
- Reference count: 20
- One-line primary result: Sparse GEMINI combines GEMINI objective with LassoNet for joint discriminative clustering and feature selection

## Executive Summary
This paper introduces Sparse GEMINI, a novel algorithm that performs joint discriminative clustering and feature selection by combining the GEMINI objective with LassoNet architecture. The method addresses the challenge of discovering relevant clusters while simultaneously selecting informative variables, without requiring strong parametric assumptions on data distributions. Sparse GEMINI uses a simple ℓ1 penalty and group-lasso regularization to perform feature selection while maintaining clustering performance, making it scalable to high-dimensional data and suitable for applications in genomics and multi-omics data analysis.

## Method Summary
Sparse GEMINI implements joint discriminative clustering and feature selection by combining the GEMINI objective (a geometry-aware generalization of mutual information) with the LassoNet architecture. The method uses a group-lasso penalty on skip connection weights to remove entire feature vectors, coupled with a constraint on the first MLP layer weights. A dynamic training regime recomputes affinity functions only on selected features at each step, maintaining consistency between feature selection and clustering. The algorithm employs proximal gradient operations for constraints during training and uses a geometrically increasing λ parameter to control feature elimination.

## Key Results
- Sparse GEMINI achieves competitive clustering performance on synthetic and real datasets while effectively selecting relevant feature subsets
- The method successfully scales to high-dimensional data and large sample sizes without requiring generative assumptions about data distribution
- Experiments show Sparse GEMINI outperforms or matches state-of-the-art methods like SparseKMeans, ClustVarSel, vscc, and Sparse Fisher EM on benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ℓ1/ℓ2 penalty on skip connection weights removes entire feature vectors at once, enabling simultaneous feature selection and clustering without combinatorial search
- Mechanism: LassoNet uses group-lasso penalty to zero out entire columns when their ℓ2 norm becomes small enough, coupled with constraints on first MLP layer weights
- Core assumption: Features can be treated as globally relevant or irrelevant; ℓ1/ℓ2 penalty can correctly identify and remove irrelevant ones
- Evidence anchors: Abstract states ℓ1 penalty is used; section shows optimization objective includes group-lasso term; corpus lacks relevant citations
- Break condition: If relevant features are redundant or highly correlated, ℓ1/ℓ2 penalty may arbitrarily keep one and drop others, hurting clustering performance

### Mechanism 2
- Claim: GEMINI objective avoids strong parametric assumptions while providing effective separation
- Mechanism: GEMINI measures divergence between cluster distributions and overall data distribution using distances like MMD or Wasserstein that incorporate geometric structure
- Core assumption: Chosen divergence measure (MMD or Wasserstein) can effectively capture cluster separability without requiring generative model of p(xxx)
- Evidence anchors: Abstract mentions avoiding parametric assumptions; section explains estimation without knowledge of data distribution; corpus lacks relevant citations
- Break condition: If clusters are not well-separated in chosen distance space, GEMINI may not find good clustering even with optimal feature selection

### Mechanism 3
- Claim: Dynamic training regime maintains consistency between feature space for selection and clustering
- Mechanism: At each step t, affinity function is computed using only subset of features It, preventing noise from eliminated features from interfering with clustering decisions
- Core assumption: Affinity computed on reduced feature space remains meaningful for clustering as features are eliminated
- Evidence anchors: Section introduces dynamic regime with affinity computed on relevant features It; experiments reported in Table 6; corpus lacks relevant citations
- Break condition: If number of selected features drops too quickly, affinity may become unstable and clustering performance may degrade

## Foundational Learning

- Concept: Mutual Information (MI) and its generalisations
  - Why needed here: GEMINI extends MI to discriminative setting without requiring p(xxx), so understanding MI helps grasp why this works
  - Quick check question: Why can't standard MI be used directly for discriminative clustering without a generative model of the data?

- Concept: Group-Lasso regularisation and feature selection in neural networks
  - Why needed here: ℓ1/ℓ2 penalty removes entire feature vectors, and understanding how this works in neural nets is crucial
  - Quick check question: How does group-lasso differ from standard ℓ1 in terms of feature selection behavior?

- Concept: Maximum Mean Discrepancy (MMD) and Wasserstein distance
  - Why needed here: These are divergence measures used in GEMINI, and understanding their properties explains clustering behavior
  - Quick check question: What is the key difference between MMD and Wasserstein distance in terms of how they measure distribution divergence?

## Architecture Onboarding

- Component map: Input → Linear skip connection (penalised) + MLP → Softmax output → GEMINI loss (MMD/Wasserstein) + ℓ1/ℓ2 penalty → Proximal gradient update
- Critical path: Feature selection depends on ℓ1/ℓ2 penalty driving weights to zero; clustering depends on GEMINI objective being optimised; both must work together
- Design tradeoffs: Static training regime (faster, more stable) vs dynamic regime (more theoretically consistent but potentially unstable); MMD (faster, less sensitive to noise) vs Wasserstein (more robust to global structure)
- Failure signatures: If features collapse too quickly during training, check λ increase schedule; if clustering performance drops despite feature selection, verify affinity computation is on correct feature subset
- First 3 experiments:
  1. Run Sparse GEMINI on synthetic dataset with known informative vs noisy features; verify feature selection matches expectations
  2. Compare static vs dynamic training regimes on same dataset; observe impact on clustering ARI
  3. Test MMD vs Wasserstein objectives on dataset with redundant informative features; observe which handles redundancy better

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does dynamic training regime affect convergence and stability compared to static regime?
- Basis in paper: Paper discusses dynamic training regime where affinity function is computed using only subset of relevant features It, and compares it to static regime
- Why unresolved: Paper only presents experimental results comparing two regimes on synthetic datasets, without theoretical analysis of convergence or stability properties
- What evidence would resolve it: Rigorous mathematical analysis of convergence properties under both dynamic and static training regimes, including conditions for stability and trade-offs between approaches

### Open Question 2
- Question: Can Sparse GEMINI effectively handle high-dimensional data with large number of irrelevant features while maintaining good clustering performance?
- Basis in paper: Paper demonstrates performance on synthetic datasets with varying numbers of irrelevant features, but doesn't extensively test on high-dimensional real-world datasets
- Why unresolved: Paper only presents results on synthetic datasets and few real datasets with relatively low dimensionality; doesn't explore scalability on extremely high-dimensional data
- What evidence would resolve it: Experiments applying Sparse GEMINI to large-scale, high-dimensional datasets (e.g., genomics data with tens of thousands of features) and comparing performance to other state-of-the-art methods

### Open Question 3
- Question: How does choice of number of clusters (K) impact Sparse GEMINI's feature selection and clustering performance?
- Basis in paper: Paper mentions number of clusters is set to number of known supervised labels in most experiments, but doesn't explore impact of varying K on algorithm's performance
- Why unresolved: Paper doesn't provide in-depth analysis of how choice of K affects feature selection and clustering quality; unclear how Sparse GEMINI behaves when true number of clusters is unknown or when K is set incorrectly
- What evidence would resolve it: Systematic study of Sparse GEMINI's performance under different choices of K, including scenarios where K is too high or too low compared to true number of clusters; evaluate clustering quality metrics and feature selection accuracy across range of K values

## Limitations
- Strong assumptions about feature independence may lead to arbitrary selection among correlated features
- Dynamic training regime shows dataset-dependent performance and may become unstable as feature space shrinks
- Reliance on specific divergence measures (MMD/Wasserstein) may not capture all cluster structures in high-dimensional spaces

## Confidence
- **High Confidence**: Core mechanism of combining GEMINI with group-lasso for feature selection is well-founded and theoretically sound
- **Medium Confidence**: Dynamic training regime shows promise but exhibits inconsistent performance across datasets
- **Medium Confidence**: Comparison with existing methods demonstrates competitive performance, though experimental validation on real-world datasets is limited

## Next Checks
1. **Redundancy Test**: Create synthetic dataset with highly correlated informative features and evaluate whether Sparse GEMINI can identify all relevant features rather than arbitrarily selecting one from each correlated group

2. **Robustness Analysis**: Systematically vary λ increase schedule and observe impact on feature selection stability and clustering performance across multiple datasets to identify optimal scheduling strategies

3. **Convergence Study**: Monitor evolution of feature selection process during training to identify conditions under which dynamic regime becomes unstable, and develop early-stopping criteria based on affinity function stability