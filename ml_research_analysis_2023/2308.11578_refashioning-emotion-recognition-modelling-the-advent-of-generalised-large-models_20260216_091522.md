---
ver: rpa2
title: 'Refashioning Emotion Recognition Modelling: The Advent of Generalised Large
  Models'
arxiv_id: '2308.11578'
source_url: https://arxiv.org/abs/2308.11578
tags:
- emotion
- llms
- recognition
- context
- sentiment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the potential of large language models\
  \ (LLMs) in emotion recognition, comparing their performance against state-of-the-art\
  \ specialized models. The authors evaluate three LLMs\u2014ChatGPT, Claude, and\
  \ Bing Chat\u2014across seven datasets spanning English and Chinese."
---

# Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models

## Quick Facts
- arXiv ID: 2308.11578
- Source URL: https://arxiv.org/abs/2308.11578
- Reference count: 40
- Large language models (LLMs) achieve comparable or superior performance to state-of-the-art emotion recognition models, especially for minority emotion classes, through in-context learning.

## Executive Summary
This paper investigates the potential of large language models (LLMs) for emotion recognition and sentiment analysis, comparing their performance against specialized deep learning models. The authors evaluate three LLMs—ChatGPT, Claude, and Bing Chat—across seven datasets spanning English and Chinese, using zero-shot and few-shot in-context learning strategies. Results demonstrate that LLMs achieve performance comparable to or exceeding state-of-the-art models, with particular strength in recognizing minority emotion classes and generalizing across languages and domains. The study also highlights LLMs' ability to provide interpretable explanations for their predictions, opening new avenues for affective computing.

## Method Summary
The study evaluates three LLMs (ChatGPT, Claude, and Bing Chat) on seven emotion recognition datasets (SST, Friends, Mastodon, MOSI, MOSEI, CH-SIMS, M3ED) using in-context learning with zero-shot and few-shot prompting. No model fine-tuning is performed; instead, prompts are crafted for different scenarios (context-free/few-shot, context-aware/few-shot). Performance is measured using accuracy and F1 scores (macro and weighted). The approach relies on API access to the LLMs and post-processing of predictions for evaluation.

## Key Results
- LLMs achieve comparable or superior performance to state-of-the-art emotion recognition models, especially for minority emotion classes.
- In-context learning significantly improves LLM performance, with few-shot prompting outperforming zero-shot prompting.
- LLMs demonstrate strong cross-lingual and cross-domain generalization, maintaining performance across different datasets and languages.
- LLMs provide interpretable explanations for emotion predictions, increasing transparency.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs perform emotion recognition at SOTA levels by leveraging emergent in-context learning (ICL) capabilities that are not present in smaller pre-trained models.
- Mechanism: When provided with task instructions and a few demonstration examples, LLMs can temporarily learn and adapt to new emotion recognition tasks without any fine-tuning or gradient updates, matching or exceeding the performance of specialized deep learning models.
- Core assumption: The LLMs' vast pre-training on diverse text corpora enables them to generalize emotional patterns and context cues effectively.
- Evidence anchors:
  - [abstract]: "In-context learning (ICL) further improves performance, demonstrating LLMs’ ability to adapt with minimal examples."
  - [section]: "Compared with SOTAs benchmarks, the LLM-based model demonstrates comparable, if not superior, performance."
  - [corpus]: Found related work on "Customising General Large Language Models for Specialised Emotion Recognition Tasks" suggesting similar investigations.
- Break condition: If the ICL demonstrations do not align well with the target task domain, the performance gains may diminish.

### Mechanism 2
- Claim: LLMs show strong cross-lingual and cross-domain generalization in emotion recognition due to their broad pre-training exposure.
- Mechanism: LLMs trained on massive, diverse text data can recognize emotions in unseen datasets and languages without explicit task-specific training, enabling a single model to handle multiple domains and languages.
- Core assumption: The pre-training corpus sufficiently covers diverse emotional expressions across languages and domains.
- Evidence anchors:
  - [abstract]: "Results show that LLMs achieve comparable or superior performance, particularly excelling in recognizing minority emotion classes... cross-language generalisation capability."
  - [section]: "The LLMs could achieve comparable performance in zero-shot prompting scenarios... and this performance is further enhanced by the implementation of few-shot prompting, indicating that LLMs can adapt and learn from limited contextual information during the inference."
  - [corpus]: Weak evidence; only found general "Customising General Large Language Models" title without details.
- Break condition: If the target dataset contains emotional expressions vastly different from the pre-training corpus, performance may degrade.

### Mechanism 3
- Claim: LLMs provide explainable emotion recognition decisions in natural language, enabling transparency.
- Mechanism: Because LLMs generate natural language responses, they can be prompted to provide human-readable explanations for their emotion predictions, increasing interpretability.
- Core assumption: The model's internal reasoning aligns with the natural language explanations it generates.
- Evidence anchors:
  - [abstract]: "LLMs’ strong generalization across languages and domains, and their capacity to provide interpretable explanations."
  - [section]: "We delved into examining the generalisation, and interpretability of LLM-based affective modelling... we opted for the "more precise" format, aiming to obtain succinct and direct answers to our emotion-prediction-related queries."
  - [corpus]: No direct evidence; corpus search yielded no specific interpretability studies.
- Break condition: Explanations may not reflect actual decision-making mechanisms, reducing reliability.

## Foundational Learning

- Concept: Zero-shot vs Few-shot Learning
  - Why needed here: To understand how LLMs perform without and with demonstration examples in emotion recognition tasks.
  - Quick check question: What is the difference between zero-shot and few-shot prompting in the context of LLMs?

- Concept: In-Context Learning (ICL)
  - Why needed here: ICL is the emergent ability that allows LLMs to learn tasks from examples provided in the prompt without updating weights.
  - Quick check question: How does ICL differ from traditional fine-tuning?

- Concept: Cross-lingual and Cross-domain Generalization
  - Why needed here: To understand how LLMs can handle datasets from different languages and domains without task-specific training.
  - Quick check question: Why might an LLM trained on English text still perform well on Chinese emotion recognition datasets?

## Architecture Onboarding

- Component map:
  - Datasets (SST, Friends, Mastodon, MOSI, MOSEI, CH-SIMS, M3ED) -> Prompt Engineering (zero-shot, few-shot) -> LLM Selection (ChatGPT, Claude, Bing Chat) -> Evaluation Pipeline (accuracy, F1 scores) -> Explanation Module (natural language explanations)

- Critical path:
  1. Prepare datasets (SST, Friends, Mastodon, MOSI, MOSEI, CH-SIMS, M3ED).
  2. Design prompts for four scenarios (context-free/few-shot, context-aware/few-shot, etc.).
  3. Execute LLM calls and collect predictions.
  4. Post-process results and evaluate accuracy, F1, UA, etc.
  5. Analyze generalization and explainability.

- Design tradeoffs:
  - Zero-shot vs Few-shot: Zero-shot is faster and cheaper but less accurate; few-shot improves accuracy at higher cost.
  - Context-aware vs Context-free: Context-aware improves accuracy for conversational data but requires more complex prompts.
  - Model selection: ChatGPT and Claude generally outperform Bing Chat in emotion tasks.

- Failure signatures:
  - Low accuracy on minority emotion classes.
  - Inconsistent predictions across similar prompts.
  - Unexplainable outputs outside the target classification range.

- First 3 experiments:
  1. Run zero-shot emotion recognition on SST-2 dataset with ChatGPT.
  2. Compare zero-shot vs few-shot accuracy on Friends dataset with Claude.
  3. Test context-aware emotion recognition on M3ED dataset with Bing Chat.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs on emotion recognition tasks compare to that of task-specific models in terms of both accuracy and computational efficiency?
- Basis in paper: [explicit] The paper compares the performance of three LLMs (ChatGPT, Claude, and Bing Chat) against state-of-the-art task-specific models across seven datasets, finding that LLMs achieve comparable or superior performance, particularly in recognizing minority emotion classes.
- Why unresolved: The study primarily focuses on accuracy and does not provide a comprehensive analysis of computational efficiency, such as inference time or resource consumption, which are crucial for real-world deployment.
- What evidence would resolve it: A detailed comparison of computational resources (e.g., time, memory, energy) required by LLMs versus task-specific models during inference, along with scalability analysis.

### Open Question 2
- Question: Can LLMs be effectively fine-tuned or adapted to improve their performance on domain-specific emotion recognition tasks without losing their generalization capabilities?
- Basis in paper: [explicit] The paper discusses the potential for further pre-training and fine-tuning LLMs using domain-specific data to enhance their performance, but notes that this could lead to issues such as inductive biases or resource constraints.
- Why unresolved: The study does not empirically test fine-tuning strategies or evaluate the trade-off between domain-specific adaptation and generalization.
- What evidence would resolve it: Experimental results comparing the performance of fine-tuned LLMs against both off-the-shelf LLMs and task-specific models on domain-specific datasets, along with an analysis of generalization across different domains.

### Open Question 3
- Question: How do LLMs handle robustness to natural errors in text, such as typos and grammatical inaccuracies, in emotion recognition tasks?
- Basis in paper: [inferred] The paper mentions the importance of robustness to natural errors in text, which are common in real-world communications, but does not provide empirical evidence or strategies to enhance model robustness against such "noise."
- Why unresolved: The study does not test LLMs' performance on noisy text data or explore techniques to improve robustness to errors.
- What evidence would resolve it: Experimental results evaluating LLMs' performance on text data with intentional errors (e.g., typos, grammatical mistakes) and comparisons with error-correction or noise-robust fine-tuning methods.

## Limitations

- The study evaluates only seven datasets and three LLM variants, limiting generalizability to other languages and domains.
- Exact prompt engineering strategies, temperature settings, and API access details are not fully specified, which could impact reproducibility.
- The interpretability claims are not rigorously validated, and explanations may not reflect actual model reasoning.

## Confidence

- Performance claims (comparable to SOTA): High
- Generalization claims (cross-lingual, cross-domain): Medium
- Interpretability claims: Low-Medium

## Next Checks

1. Test the same in-context learning approach on at least 5-10 additional emotion datasets spanning diverse domains (healthcare, social media, customer service) to validate generalization claims.
2. Conduct ablation studies varying prompt formats, example selection, and temperature settings to quantify their impact on performance and interpretability.
3. Implement automated evaluation of explanation quality using metrics like faithfulness and plausibility to better assess the interpretability claims.