---
ver: rpa2
title: Automatic Unit Test Data Generation and Actor-Critic Reinforcement Learning
  for Code Synthesis
arxiv_id: '2310.13669'
source_url: https://arxiv.org/abs/2310.13669
tags:
- code
- unit
- training
- tests
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving code synthesis
  models by leveraging reinforcement learning (RL) and automatically generated unit
  tests. The authors propose a novel approach that combines actor-critic RL training
  with automatic unit test generation to enhance the performance of pre-trained code
  language models.
---

# Automatic Unit Test Data Generation and Actor-Critic Reinforcement Learning for Code Synthesis

## Quick Facts
- arXiv ID: 2310.13669
- Source URL: https://arxiv.org/abs/2310.13669
- Authors: 
- Reference count: 9
- Primary result: Up to 9.9% improvement over original code synthesis LM and 4.3% over RL baselines using actor-critic RL with automatically generated unit tests

## Executive Summary
This paper presents a novel approach to improve code synthesis models by combining actor-critic reinforcement learning with automatically generated unit tests. The authors introduce a heuristic method to generate large datasets of function signatures and associated unit tests suitable for RL training, addressing the challenge of limited labeled data in code synthesis. Their approach demonstrates significant performance improvements over standard pre-trained language models and existing RL-based methods when trained on the MBPP dataset.

## Method Summary
The proposed method combines actor-critic reinforcement learning with automatic unit test generation to enhance code synthesis performance. A pre-trained code language model (PanGu-Coder 300M) serves as the actor, while a simple two-layer MLP critic predicts reward scores. The system generates solutions using nucleus sampling, evaluates them against automatically generated unit tests to obtain rewards, and updates both actor and critic using REINFORCE with a critic baseline. A KL divergence regularization term constrains the policy update to prevent divergence from the original model. The training process incorporates a replay buffer of valid solutions to maintain diversity and prevent overfitting.

## Key Results
- Up to 9.9% improvement in greedy decoding accuracy over the original underlying code synthesis language model
- Up to 4.3% improvement over RL-based models trained with standard PPO or CodeRL
- The actor-critic approach outperforms standard CLM fine-tuning on the MBPP dataset
- Automatic unit test generation provides sufficient learning signals to improve code synthesis performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The combination of actor-critic RL with automatically generated unit tests improves code synthesis performance over standard CLM fine-tuning.
- **Mechanism:** The actor-critic framework provides a structured learning signal where the critic estimates expected rewards, reducing variance in policy gradients. Automatically generated unit tests expand the training signal beyond the small labeled dataset (MBPP), allowing the model to learn from more diverse functional correctness examples.
- **Core assumption:** The automatically generated unit tests are of sufficient quality to provide meaningful learning signals for the RL agent.
- **Evidence anchors:**
  - [abstract] "we introduce a heuristic approach to generate large data of natural language problems, function signatures, and Unit Tests"
  - [section 3.3] "we employ the following steps to obtain large amounts of Unit Test data"
- **Break condition:** If the automatically generated unit tests are of poor quality, the RL agent may learn incorrect patterns or fail to improve.

### Mechanism 2
- **Claim:** The KL divergence regularization term prevents the RL policy from diverging too far from the original pre-trained language model.
- **Mechanism:** The reward function includes a KL penalty term that constrains the policy update to stay within a certain distance from the original model, preventing catastrophic forgetting and maintaining some of the original model's capabilities.
- **Core assumption:** The original pre-trained model has learned useful priors that should be preserved during RL fine-tuning.
- **Evidence anchors:**
  - [section 3.2] "To avoid the model from over-fitting and moving too far away from the original PLM π0, our complete reward function is given as: R(st, at) = Rf(st)− ζKL [πθ(at|st)||π0(at|st)]"
- **Break condition:** If the KL penalty is too strong, the model may not be able to make meaningful improvements; if too weak, it may diverge from useful priors.

### Mechanism 3
- **Claim:** The replay buffer of valid solutions improves training diversity and prevents the model from overfitting to recent failures.
- **Mechanism:** By storing valid solutions from previous iterations, the model can sample successful examples during training, providing positive reinforcement signals and maintaining diversity in the training data.
- **Core assumption:** Having access to a diverse set of valid solutions helps the model learn more robust patterns.
- **Evidence anchors:**
  - [section 3.2] "We find that the application of a critic model that evaluates the generated code can further improve training"
  - [section 4] "we use a replay buffer that stores the original solutions found in the training data, as well as all valid solutions found during training"
- **Break condition:** If the replay buffer becomes too small or too homogeneous, it may not provide sufficient diversity to improve learning.

## Foundational Learning

- **Concept:** Markov Decision Process (MDP)
  - **Why needed here:** Code synthesis is modeled as an MDP where states are partial code sequences, actions are token predictions, and rewards are based on unit test pass rates.
  - **Quick check question:** What are the states, actions, and rewards in the code synthesis MDP formulation?

- **Concept:** Policy Gradient Methods
  - **Why needed here:** The model uses REINFORCE with a critic baseline to update the policy parameters based on the observed rewards.
  - **Quick check question:** How does the critic baseline reduce variance in the policy gradient estimate?

- **Concept:** KL Divergence Regularization
  - **Why needed here:** Prevents the RL policy from deviating too far from the original pre-trained model during fine-tuning.
  - **Quick check question:** Why is it important to constrain the policy update with a KL divergence term?

## Architecture Onboarding

- **Component map:** Pre-trained code LM (actor) -> Critic MLP -> Unit Test Evaluation System -> Reward Signal -> REINFORCE Update

- **Critical path:**
  1. Generate solutions using current policy
  2. Evaluate solutions with unit tests to get rewards
  3. Sample from replay buffer for additional training data
  4. Update critic using MSE loss between predicted and observed rewards
  5. Update actor using REINFORCE with critic baseline
  6. Store new valid solutions in replay buffer

- **Design tradeoffs:**
  - Using a simple critic vs. more complex architectures: simpler critic is easier to train and less prone to overfitting
  - Automatic unit test generation vs. using only human-labeled tests: trade-off between data quantity and quality
  - KL regularization strength: balance between preserving original capabilities and allowing improvement

- **Failure signatures:**
  - Training collapse: KL divergence becomes too large, indicating the policy is diverging from the original model
  - Slow learning: Critic predictions are inaccurate, providing poor baseline estimates
  - Overfitting: Model performs well on training data but poorly on validation/test sets

- **First 3 experiments:**
  1. **Baseline comparison:** Train with CLM objective only on MBPP to establish baseline performance
  2. **Ablation study:** Remove the critic component to verify its contribution to learning
  3. **Data augmentation impact:** Train with automatic unit tests only (no MBPP) to assess data quality

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the research content, several important questions remain:

1. How does the performance of the proposed actor-critic RL approach compare to other RL algorithms like PPO and CodeRL when trained on larger datasets with more unit tests?
2. Can the proposed approach be extended to handle more complex programming tasks beyond function-level code synthesis?
3. How does the performance of the proposed approach vary with different reward functions and critic architectures?
4. How does the proposed approach handle the generation of code that is not only functionally correct but also efficient and follows best practices?
5. How does the proposed approach perform in scenarios where the natural language problem descriptions are ambiguous or incomplete?

## Limitations

- **Data Quality Dependency:** The approach's performance heavily relies on the quality of automatically generated unit tests, which is not thoroughly evaluated in the paper.
- **Scalability Constraints:** The method uses a relatively small 300M parameter model and a limited dataset (MBPP with only 374 training examples), making it unclear how well this approach would scale to larger models or more complex programming tasks.
- **Evaluation Limitations:** The evaluation primarily focuses on the MBPP dataset and uses greedy decoding accuracy and pass@k metrics, without analysis of code quality metrics such as runtime efficiency, code readability, or adherence to best practices.

## Confidence

**High Confidence:**
- The actor-critic framework with KL regularization can improve code synthesis performance compared to standard CLM fine-tuning
- The replay buffer of valid solutions contributes positively to training diversity and helps prevent overfitting

**Medium Confidence:**
- The specific improvements (9.9% over original LM, 4.3% over RL baselines) are reproducible on MBPP dataset
- Automatic unit test generation provides sufficient quality signals for RL training

**Low Confidence:**
- The approach will generalize to other code synthesis datasets beyond MBPP
- The performance gains will scale proportionally with model size and training data

## Next Checks

1. **Unit Test Quality Analysis:** Conduct a systematic evaluation of the automatically generated unit tests by measuring their precision, recall, and coverage on a held-out validation set. Compare the quality of automatically generated tests against human-written tests to quantify potential learning signal noise.

2. **Cross-Dataset Generalization:** Validate the approach on additional code synthesis benchmarks such as HumanEval or APPS to assess whether the performance improvements transfer beyond the MBPP dataset. This should include both accuracy metrics and code quality assessments.

3. **Ablation on KL Regularization:** Perform an ablation study varying the KL divergence target (ρ) across a wider range (e.g., 0.01 to 0.2) to identify the optimal balance between preserving original capabilities and allowing improvement. Include analysis of training stability and final performance across different KL settings.