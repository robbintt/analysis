---
ver: rpa2
title: Jointly Training Large Autoregressive Multimodal Models
arxiv_id: '2309.15564'
source_url: https://arxiv.org/abs/2309.15564
tags:
- your
- arxiv
- preprint
- have
- also
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the Joint Autoregressive Mixture (JAM) framework,
  which systematically fuses a pretrained autoregressive text-to-image model with
  a large language model into a single unified architecture. The approach employs
  three strategies: model averaging, width concatenation, and cross-attention fusion,
  followed by multimodal instruction tuning on a small curated dataset.'
---

# Jointly Training Large Autoregressive Multimodal Models

## Quick Facts
- arXiv ID: 2309.15564
- Source URL: https://arxiv.org/abs/2309.15564
- Reference count: 40
- This paper introduces the Joint Autoregressive Mixture (JAM) framework, which systematically fuses a pretrained autoregressive text-to-image model with a large language model into a single unified architecture

## Executive Summary
This paper presents the Joint Autoregressive Mixture (JAM) framework for fusing pretrained autoregressive text-to-image and language models into a single architecture capable of generating interleaved image-text content. The approach employs three fusion strategies - model averaging, width concatenation, and cross-attention fusion - followed by multimodal instruction tuning on a small curated dataset. The resulting models achieve state-of-the-art performance on mixed-modal tasks while demonstrating the feasibility of multimodal autoregressive generation at scale using minimal additional training data.

## Method Summary
The JAM framework systematically fuses pretrained autoregressive models through three strategies: (1) Model averaging (JAM-Uniform) averages weights of compatible transformer layers, (2) Width concatenation (JAM-Width) doubles hidden dimensions while preserving learned representations, and (3) Cross-attention fusion (JAM-Cross) inserts bidirectional cross-attention layers between corresponding layers of the two models. After fusion, models undergo continued pretraining on hybrid datasets and multimodal instruction tuning on curated interleaved image-text data. The approach leverages classifier-free guidance and retrieval augmentation during generation.

## Key Results
- JAM-Cross achieves state-of-the-art performance on mixed-modal tasks while maintaining single-modality capabilities
- Instruction tuning on <1% of original pretraining data proves sufficient for effective multimodal generation
- Cross-attention fusion enables bidirectional information exchange between modalities while preserving individual model strengths
- The framework demonstrates that large autoregressive multimodal models can be created without complete retraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weight averaging (JAM-Uniform) allows model fusion by leveraging architectural compatibility between autoregressive text-to-image models and LLMs.
- Mechanism: Averaging weights preserves knowledge from both parent models while allowing further fine-tuning on mixed-modal data, enabling deep integration without complete retraining.
- Core assumption: The two architectures are sufficiently similar in their transformer layers to allow effective weight averaging.
- Evidence anchors:
  - [abstract] "The approach employs three strategies: model averaging, width concatenation, and cross-attention fusion"
  - [section] "The weights of the averaged model are defined as: θaverage = 1/2 θllm + 1/2 θimg"
- Break condition: If the parent models have significantly different layer configurations or token embeddings, averaging may lead to performance degradation.

### Mechanism 2
- Claim: Width concatenation (JAM-Width) doubles the model capacity while preserving the learned representations from both parent models.
- Mechanism: Concatenating the hidden dimensions and initializing new parameters using a mix of copying and averaging allows the model to retain original capabilities while expanding expressiveness.
- Core assumption: Doubling the hidden size without increasing layers maintains model stability and performance.
- Evidence anchors:
  - [abstract] "width concatenation, and cross-attention fusion"
  - [section] "Our new model has hidden dimensions djoint = 8192, which is doubled with respect to one of the two original models"
- Break condition: If the increased parameter count leads to overfitting or training instability, especially with limited data.

### Mechanism 3
- Claim: Cross-attention fusion (JAM-Cross) enables bidirectional information exchange between the LLM and text-to-image model while preserving their individual strengths.
- Mechanism: Inserting cross-attention layers between corresponding layers of the two models allows progressive information exchange, with the image model attending to text representations and vice versa.
- Core assumption: Cross-attention can be inserted without disrupting the original attention mechanisms and that the models can learn to use this additional pathway effectively.
- Evidence anchors:
  - [abstract] "cross-attention fusion"
  - [section] "We propose to embed cross-attention layers between the foundational models to facilitate seamless information interchange"
- Break condition: If the cross-attention layers create conflicts or gradient issues that prevent effective learning.

## Foundational Learning

- Concept: Autoregressive modeling in transformer architectures
  - Why needed here: The entire framework relies on autoregressive generation for both text and images, requiring understanding of how tokens are generated sequentially
  - Quick check question: What is the key difference between autoregressive and non-autoregressive generation in terms of token dependency?

- Concept: Cross-attention mechanisms in transformers
  - Why needed here: The JAM-Cross model specifically uses cross-attention layers to enable bidirectional information flow between modalities
  - Quick check question: How does cross-attention differ from self-attention in terms of the query-key-value relationships?

- Concept: Tokenization for multimodal data
  - Why needed here: The model uses different tokenizers for text and images, requiring understanding of how discrete representations are created for each modality
  - Quick check question: Why does image tokenization typically use a vocabulary size of 8192 while text tokenization might use a different size?

## Architecture Onboarding

- Component map:
  - Parent models: LLM (text-only) and CM3leon (text-to-image)
  - Fusion strategies: Model averaging, width concatenation, cross-attention fusion
  - Training components: Multimodal instruction tuning, retrieval augmentation
  - Generation components: Mixed-modal decoding with temperature sampling and classifier-free guidance

- Critical path: Pretrained models → Fusion strategy → Continued pretraining → Instruction tuning → Generation

- Design tradeoffs:
  - Parameter efficiency vs. performance: JAM-Uniform (7B) vs JAM-Width (26B) vs JAM-Cross (19B)
  - Training data efficiency: Using <1% of original pretraining data
  - Generation quality: Retrieval augmentation vs. standard sampling

- Failure signatures:
  - Performance degradation in either modality after fusion
  - Training instability when increasing model width
  - Poor image quality when using cross-attention fusion

- First 3 experiments:
  1. Implement JAM-Uniform and verify it maintains baseline performance on both text and image-text tasks
  2. Test different frequencies of cross-attention layer insertion to find optimal balance
  3. Compare instruction tuning with and without pretraining data to measure impact on generation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of scaling model size and using asymmetrically different-sized models in the JAM framework?
- Basis in paper: [inferred] The paper discusses limitations including the use of 7B parameter models with the same architecture and suggests future exploration directions.
- Why unresolved: The paper only tested JAM with models of the same size and architecture, leaving the effects of scaling and asymmetry unexplored.
- What evidence would resolve it: Experiments comparing JAM performance across different model sizes and architectures, particularly when combining larger LLMs with smaller text-to-image models or vice versa.

### Open Question 2
- Question: How does the JAM framework perform in multi-turn conversational settings with longer context lengths?
- Basis in paper: [inferred] The paper mentions limitations around context length (4k tokens) and single-turn question answering, suggesting this as a future exploration direction.
- Why unresolved: The current implementation is limited to 4k context length and focuses on single-turn interactions, preventing assessment of performance in more complex dialogue scenarios.
- What evidence would resolve it: Evaluation of JAM models in extended multi-turn conversations with increased context lengths, measuring coherence, relevance, and generation quality over longer interactions.

### Open Question 3
- Question: What is the optimal frequency for inserting cross-attention layers in the JAM-Cross architecture?
- Basis in paper: [explicit] The paper includes an ablation study on cross-attention frequency but notes that performance differences increase with training progression.
- Why unresolved: While the paper explores cross-attention insertion frequency, it concludes that differences become more pronounced with extended training, suggesting the optimal configuration remains uncertain.
- What evidence would resolve it: Systematic evaluation of cross-attention layer frequency across various training durations and model scales to determine the configuration that maximizes performance while maintaining computational efficiency.

### Open Question 4
- Question: How does instruction tuning with interleaved image-text data compare to other multimodal fine-tuning approaches in terms of sample efficiency and performance?
- Basis in paper: [explicit] The paper claims to be the first to explore instruction tuning focused on interleaved image-text generation and demonstrates effectiveness with a small dataset.
- Why unresolved: The paper establishes that small datasets can be effective for instruction tuning but doesn't benchmark against alternative fine-tuning strategies like continued pretraining or larger dataset approaches.
- What evidence would resolve it: Comparative studies measuring instruction-tuned JAM models against models fine-tuned with larger datasets or alternative multimodal training strategies, evaluating both sample efficiency and generation quality across diverse tasks.

## Limitations

- Limited empirical validation of optimal cross-attention layer frequency and placement
- Potential catastrophic forgetting when fusing models with different pretraining distributions
- Instruction tuning dataset, while effective, remains relatively small and may not generalize to all real-world applications
- Retrieval augmentation component lacks detailed implementation specifics necessary for faithful reproduction

## Confidence

**High Confidence**: The feasibility of multimodal autoregressive generation at scale is well-supported by the experimental results showing JAM-Cross achieving state-of-the-art performance on mixed-modal tasks. The claim that fusion strategies can preserve individual model capabilities while enabling new multimodal generation is also strongly supported by the maintained performance on single-modality tasks.

**Medium Confidence**: The assertion that <1% of original pretraining data is sufficient for effective instruction tuning is plausible given the strong results, but the limited dataset diversity makes it difficult to generalize this claim. The claim about weight averaging preserving knowledge from both parent models shows mixed evidence, with some degradation observed in text-only reasoning tasks.

**Low Confidence**: The claim that cross-attention fusion "systematically" integrates information between modalities is somewhat overstated, as the paper doesn't provide detailed analysis of what information is actually being exchanged or how effectively the models learn to use this pathway.

## Next Checks

1. **Cross-attention ablation study**: Systematically vary the frequency of cross-attention layer insertion (e.g., every 2, 4, 8 layers) and measure the impact on both image quality and text generation quality to identify optimal placement strategies.

2. **Modality interference analysis**: Design controlled experiments to quantify catastrophic forgetting or interference effects by testing the fused models on both their original single-modality tasks and new multimodal tasks across training epochs.

3. **Dataset scaling experiment**: Train JAM-Cross models with varying amounts of instruction-tuning data (10%, 50%, 100% of the curated dataset) to empirically validate the claim that minimal data is sufficient and identify the data-efficiency threshold.