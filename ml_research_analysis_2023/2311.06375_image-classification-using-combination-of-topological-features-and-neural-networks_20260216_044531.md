---
ver: rpa2
title: Image Classification using Combination of Topological Features and Neural Networks
arxiv_id: '2311.06375'
source_url: https://arxiv.org/abs/2311.06375
tags:
- topological
- persistence
- image
- features
- homology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work integrates topological data analysis (TDA) with deep
  learning for image classification, focusing on the MNIST dataset. Topological features
  extracted via persistent homology are combined with deep learning features in single
  and two-stream neural network architectures based on MLP and CNN.
---

# Image Classification using Combination of Topological Features and Neural Networks

## Quick Facts
- arXiv ID: 2311.06375
- Source URL: https://arxiv.org/abs/2311.06375
- Reference count: 37
- Primary result: Integration of topological data analysis with deep learning improves MNIST classification accuracy up to 98.4%

## Executive Summary
This work presents a novel approach to image classification that combines topological data analysis with deep learning techniques. The method extracts topological features from images using persistent homology and integrates these features with traditional deep learning features in single and two-stream neural network architectures. The proposed approach is evaluated on the MNIST dataset, demonstrating that topological information can enhance classification accuracy, with heat kernel vectorization showing the best performance. The study highlights the tradeoff between accuracy gains and increased computational complexity from persistent homology calculations.

## Method Summary
The method involves extracting topological features from MNIST images using persistent homology with cubical homology. Images are binarized and processed using height, radial, and density functions to create filtrations. Persistence diagrams are generated and vectorized using Betti curves, persistence landscapes, persistence images, and heat kernels. These topological features are then combined with original image features in two-stream neural network architectures (CNN+MLP and MLP+MLP). The approach is evaluated on MNIST, showing accuracy improvements up to 98.4% with heat kernel vectorization.

## Key Results
- Proposed method achieves up to 98.4% accuracy on MNIST using heat kernel vectorization
- Two-stream architectures combining topological and image features outperform single-stream baselines
- Heat kernel vectorization method yields better results than Betti curves, persistence landscapes, and persistence images
- Topological features increase classification accuracy at the cost of additional computational complexity from persistent homology calculations

## Why This Works (Mechanism)

### Mechanism 1
Topological features capture structural patterns that pixel-level features alone cannot. Persistent homology detects and quantifies topological features (connected components, holes) across multiple scales in the filtration of the image, representing global structural information not directly encoded in pixel intensities. This mechanism assumes binarization preserves meaningful shape information while reducing noise from pixel-level variations.

### Mechanism 2
Combining topological features with deep learning features provides complementary information. Two-stream architectures process topological features (via MLP) and original image features (via MLP or CNN) separately, then concatenate their outputs. This allows the network to learn from both pixel-level and topological representations simultaneously, assuming these feature types capture different aspects of the data that are both useful for classification.

### Mechanism 3
Vectorization of persistence diagrams makes topological information compatible with standard machine learning algorithms. Persistence diagrams are converted to fixed-length vectors using techniques like Betti curves, persistence landscapes, persistence images, and heat kernels, enabling their use as input to neural networks. This assumes the chosen vectorization methods preserve essential topological information while producing vectors of appropriate dimensionality.

## Foundational Learning

- Concept: Simplicial and cubical complexes
  - Why needed here: These are the fundamental structures used to represent images for topological analysis. Images are represented as cubical complexes where pixels are 0-cubes.
  - Quick check question: What is the relationship between a pixel in a 2D image and a 0-cube in a cubical complex?

- Concept: Homology groups and Betti numbers
  - Why needed here: Homology groups quantify topological features like connected components (β0) and holes (β1). These numbers become features for classification.
  - Quick check question: How does the Betti number β1 relate to the number of holes in a digit like '0' or '8'?

- Concept: Persistent homology and filtration
  - Why needed here: Persistent homology tracks how topological features appear and disappear across different scales in the image, providing multi-scale topological information.
  - Quick check question: What does it mean for a topological feature to "persist" across a filtration?

## Architecture Onboarding

- Component map:
  Image preprocessing (binarization → height/radial/density function application) → Cubical complex construction → Filtration → Persistent homology calculation → Persistence diagram creation → Vectorization → Neural network architectures (single-stream and two-stream variants)

- Critical path:
  1. Load MNIST image
  2. Apply binarization threshold (0.4)
  3. Generate 18 processed images using height, radial, and density functions with specified parameters
  4. Compute cubical complex and filtration for each processed image
  5. Calculate persistent homology and generate persistence diagrams
  6. Vectorize diagrams using chosen method
  7. Feed vectors to appropriate neural network architecture
  8. Train and evaluate

- Design tradeoffs:
  - Computational complexity vs accuracy: Topological feature extraction is computationally expensive but can improve accuracy
  - Vectorization resolution: Higher resolution preserves more information but increases dimensionality and computational cost
  - Choice of vectorization method: Different methods capture different aspects of topological features
  - Architecture selection: Two-stream architectures may capture complementary information but add complexity

- Failure signatures:
  - Low accuracy with topological features: May indicate poor binarization, inappropriate vectorization method, or insufficient training data
  - High variance in cross-validation: May indicate overfitting, especially with high-dimensional topological features
  - Computational bottleneck: Persistent homology calculation or vectorization may be too slow for practical use

- First 3 experiments:
  1. Compare MNIST-MLP-I (baseline) with MNIST-MLP-T using Betti curves to establish if topological features alone improve performance
  2. Test two-stream architecture MNIST-MLP-T + MNIST-MLP-I with persistence landscapes to evaluate complementary benefits
  3. Compare different vectorization methods (Betti curves vs persistence images vs heat kernels) on the same architecture to identify best-performing method

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the computational cost of persistent homology calculations justify the accuracy gains in multi-class classification tasks?
- Basis in paper: The paper explicitly states "topological information may increase neural network accuracy in multi-class classification tasks with the price of computational complexity of persistent homology calculation."
- Why unresolved: The paper reports improved accuracy but doesn't provide a quantitative cost-benefit analysis comparing computational overhead versus performance gains across different dataset sizes or complexity levels.
- What evidence would resolve it: Empirical studies comparing classification accuracy improvements against computational time increases for varying dataset sizes, image complexities, and topological feature extraction parameters.

### Open Question 2
- Question: Why do persistence homology features from dimension 1 (holes) not perform as well as dimension 0 (connected components) for MNIST classification?
- Basis in paper: The paper shows that H0 features generally outperform H1 features across multiple vectorization methods, but doesn't investigate the underlying reasons for this performance difference.
- Why unresolved: The paper doesn't explore the structural properties of MNIST digits that might make H0 features more discriminative than H1 features for handwritten digit recognition.
- What evidence would resolve it: Analysis of which specific topological features from H0 versus H1 contribute most to classification accuracy, potentially through feature importance studies or visualization of persistent homology classes that influence correct classifications.

### Open Question 3
- Question: How do different image preprocessing functions (height, radial, density) impact the quality of topological features for classification?
- Basis in paper: The paper mentions using eight directions for height function, nine centers for radial function, and radius=6 for density function, but doesn't systematically compare their individual contributions to classification performance.
- Why unresolved: The paper applies all three preprocessing functions together without isolating their individual effects or optimizing their parameters for different image types.
- What evidence would resolve it: Controlled experiments testing each preprocessing function independently and in combination with different parameter settings to determine which preprocessing approach yields optimal topological features for specific image classification tasks.

### Open Question 4
- Question: Would architectural modifications to neural networks improve the integration of topological features beyond the current single and two-stream approaches?
- Basis in paper: The paper suggests that "further studies involving changes in the neural network architectures, such as exploring the number of layers" could be beneficial, indicating that current architectures may not fully exploit topological features.
- Why unresolved: The paper only tests fixed architectures (MNIST-MLP and MNIST-CNN) without exploring deeper networks, attention mechanisms, or architectural designs specifically tailored for topological feature integration.
- What evidence would resolve it: Comparative studies testing various neural network architectures including deeper networks, attention-based models, and architectures specifically designed to handle topological feature representations alongside image features.

## Limitations

- The computational complexity of persistent homology calculations is significant and not fully quantified in terms of runtime versus accuracy tradeoff
- Evaluation is limited to MNIST, a relatively simple dataset, with unclear generalizability to more complex image classification tasks
- The choice of binarization threshold (0.4) and specific parameters for preprocessing functions are critical design decisions not fully justified

## Confidence

- High Confidence: The core methodology of combining topological features with deep learning is technically sound and the mathematical foundations are well-established
- Medium Confidence: The claim that two-stream architecture improves classification accuracy is supported by experimental results, but improvement may be partially attributable to specific architecture choices not fully detailed
- Low Confidence: The claim that heat kernel vectorization yields the best results compared to other methods is based on a single dataset and lacks statistical significance testing

## Next Checks

1. Perform statistical significance testing on accuracy improvements across different vectorization methods and architectures to confirm observed differences are not due to random variation

2. Evaluate the proposed method on a more complex image classification dataset (e.g., CIFAR-10 or Fashion-MNIST) to assess whether accuracy improvements and computational tradeoffs observed on MNIST hold for more challenging tasks

3. Conduct a comprehensive ablation study to quantify the individual contributions of topological features, vectorization methods, and two-stream architecture to the overall accuracy improvement, isolating the effects of each component