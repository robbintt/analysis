---
ver: rpa2
title: 'TF-CLIP: Learning Text-free CLIP for Video-based Person Re-Identification'
arxiv_id: '2312.09627'
source_url: https://arxiv.org/abs/2312.09627
tags:
- temporal
- person
- memory
- video
- clip-memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TF-CLIP, a one-stage text-free CLIP-based
  learning framework for video-based person re-identification. The method addresses
  the challenge of lacking suitable text descriptions in ReID benchmarks by replacing
  the text encoder with identity-specific sequence features (CLIP-Memory).
---

# TF-CLIP: Learning Text-free CLIP for Video-based Person Re-Identification

## Quick Facts
- arXiv ID: 2312.09627
- Source URL: https://arxiv.org/abs/2312.09627
- Reference count: 14
- Key result: Achieves state-of-the-art performance on MARS (89.4% mAP, 93.0% Rank-1) and iLIDS-VID (94.5% Rank-1)

## Executive Summary
TF-CLIP introduces a one-stage text-free learning framework for video-based person re-identification that leverages pre-trained vision-language models without requiring text descriptions. The method replaces the text encoder in CLIP with identity-specific sequence features (CLIP-Memory) and introduces two novel modules: Sequence-Specific Prompt (SSP) for online CLIP-Memory updates and Temporal Memory Diffusion (TMD) for capturing temporal information. Experiments demonstrate superior performance on MARS, LS-VID, and iLIDS-VID datasets compared to existing video-based ReID methods.

## Method Summary
TF-CLIP processes video frames through a frozen CLIP visual encoder, then constructs identity-level CLIP-Memory by averaging sequence features. The Sequence-Specific Prompt module uses cross-attention to update CLIP-Memory online based on each input sequence, while the Temporal Memory Diffusion module captures temporal relationships through memory token construction and diffusion. The framework is trained using a video-to-memory contrastive loss along with standard ReID losses, eliminating the need for text descriptions entirely.

## Key Results
- Achieves 89.4% mAP and 93.0% Rank-1 accuracy on MARS dataset
- Achieves 94.5% Rank-1 accuracy on iLIDS-VID dataset
- Outperforms state-of-the-art video-based ReID methods across all tested benchmarks
- Demonstrates effectiveness of text-free approach using vision-language pre-trained models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing text encoder with identity-specific sequence features (CLIP-Memory) allows CLIP to work on ReID benchmarks without text labels.
- Mechanism: Pre-trained CLIP visual encoder extracts features from all video sequences of each identity, averages them into a fixed identity-level representation that substitutes for text descriptions in the contrastive loss.
- Core assumption: The pre-trained visual encoder's features are already aligned with the text encoder's outputs in CLIP's joint embedding space, so identity-level visual features can serve as proxies for missing text.
- Evidence anchors:
  - [abstract] "we extract the identity-specific sequence feature as the CLIP-Memory to replace the text feature"
  - [section] "we use a pre-trained CLIP visual encoder for all video sequences of each identity to extract sequence features, and then take the average to obtain the identity-level feature denoted as CLIP-Memory"
- Break condition: If the visual encoder's features are not well aligned with the text encoder's outputs in the joint space, the identity-level visual features will not be meaningful substitutes for text.

### Mechanism 2
- Claim: Sequence-Specific Prompt (SSP) updates CLIP-Memory online to account for intra-identity appearance diversity.
- Mechanism: SSP uses the sequence-level features as key and value, and the fixed CLIP-Memory as query, in a cross-attention mechanism to generate a prompt that updates the identity-level feature for each input sequence.
- Core assumption: The cross-attention can effectively model the interactions between the input sequence and the fixed identity-level feature to produce a better representation.
- Evidence anchors:
  - [abstract] "we design a Sequence-Specific Prompt (SSP) module to update the CLIP-Memory online"
  - [section] "SSP takes the sequence-level feature V and CLIP-Memory M as inputs...employs the cross-attention mechanism to model the interactions between query, key and value"
- Break condition: If the cross-attention mechanism cannot effectively capture the relevant information from the input sequence to update the identity-level feature.

### Mechanism 3
- Claim: Temporal Memory Diffusion (TMD) captures temporal information in video sequences by constructing and diffusing temporal memories.
- Mechanism: TMD first constructs a memory token for each frame by averaging all tokens, then uses multi-head self-attention to capture temporal relations among these memory tokens. Finally, it diffuses the temporal information back to each token in the frame via cross-attention.
- Core assumption: The temporal relations captured by self-attention among memory tokens can be effectively diffused back to the original frame tokens to enhance the sequence representation.
- Evidence anchors:
  - [abstract] "we further propose a Temporal Memory Diffusion (TMD) module to capture temporal information"
  - [section] "TMD is composed of Temporal Memory Construction (TMC) and Memory Diffusion (MD)...TMC allows the frame-level memories in a sequence to communicate with each other...MD further diffuses the temporal memories to each token in the original features"
- Break condition: If the temporal relations captured by self-attention are not meaningful or cannot be effectively diffused back to the original frame tokens.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: CLIP is trained with contrastive learning losses (Lv2t and Lt2v) to align image and text features in a joint space, which is leveraged for ReID.
  - Quick check question: What are the two contrastive losses used in CLIP to align image and text features?

- Concept: Vision Transformer (ViT)
  - Why needed here: The pre-trained CLIP visual encoder used in TF-CLIP is a ViT, which processes image patches and adds a [CLS] token for the final representation.
  - Quick check question: How does ViT process an input image to obtain a final representation?

- Concept: Attention mechanisms
  - Why needed here: SSP uses cross-attention and TMD uses self-attention to model interactions between features and update representations.
  - Quick check question: What is the difference between self-attention and cross-attention?

## Architecture Onboarding

- Component map: Input video frames -> CLIP visual encoder -> Frame features -> TMD (TMC + MD) -> Sequence feature -> SSP (updates CLIP-Memory) -> Contrastive loss

- Critical path: Input video → CLIP visual encoder → Frame features → TMD (TMC + MD) → Sequence feature → SSP (updates CLIP-Memory) → Contrastive loss

- Design tradeoffs:
  - Using fixed CLIP-Memory vs. learning text descriptions (CLIP-ReID)
  - Temporal memory construction vs. other temporal fusion methods
  - Number of layers in SSP module

- Failure signatures:
  - Performance drops significantly if CLIP-Memory is not used
  - Performance sensitive to the number of layers in SSP
  - Temporal fusion methods other than TMD perform worse

- First 3 experiments:
  1. Verify CLIP-Memory works by comparing with CLIP-ReID baseline
  2. Test impact of SSP by comparing with fixed CLIP-Memory
  3. Evaluate TMD by comparing with other temporal fusion methods

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important ones emerge from the methodology and results.

## Limitations

- The method assumes perfect alignment between pre-trained CLIP visual features and hypothetical text embeddings without empirical validation
- Limited ablation studies make it difficult to quantify the individual contributions of SSP and TMD modules
- Dependence on frozen CLIP weights means inheriting potential biases from original CLIP training data

## Confidence

- **High confidence**: Reported state-of-the-art performance metrics on MARS, LS-VID, and iLIDS-VID datasets are directly measured and verifiable
- **Medium confidence**: Claim that TF-CLIP eliminates the need for text descriptions is supported by experimental results, but underlying mechanism of feature alignment is assumed
- **Low confidence**: Assertion that SSP and TMD are necessary and sufficient components for optimal performance, given lack of detailed ablation studies

## Next Checks

1. **Feature Space Alignment Validation**: Conduct experiments comparing the distribution of identity-level visual features against actual text embeddings from CLIP to verify the fundamental assumption that visual features can substitute for text.

2. **Component Ablation Study**: Systematically remove SSP and TMD modules separately and together to quantify their individual contributions to performance, testing whether the added complexity is justified.

3. **Cross-Dataset Generalization Test**: Evaluate TF-CLIP on datasets with different demographic distributions than the training data to assess potential biases inherited from the pre-trained CLIP model and the method's robustness across diverse populations.