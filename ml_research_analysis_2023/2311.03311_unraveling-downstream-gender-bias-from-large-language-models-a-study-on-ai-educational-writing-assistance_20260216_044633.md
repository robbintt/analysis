---
ver: rpa2
title: 'Unraveling Downstream Gender Bias from Large Language Models: A Study on AI
  Educational Writing Assistance'
arxiv_id: '2311.03311'
source_url: https://arxiv.org/abs/2311.03311
tags:
- bias
- gender
- writing
- suggestions
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether gender bias in Large Language
  Models (LLMs) transfers to students'' writing in an educational context. A large-scale
  user study with 231 students writing peer reviews in German was conducted, where
  students were divided into five groups with different levels of writing support:
  one classroom group with feature-based suggestions, and four online groups with
  no assistance, fine-tuned GPT-2/3 suggestions, or pre-trained GPT-3.5 suggestions.'
---

# Unraveling Downstream Gender Bias from Large Language Models: A Study on AI Educational Writing Assistance

## Quick Facts
- arXiv ID: 2311.03311
- Source URL: https://arxiv.org/abs/2311.03311
- Reference count: 26
- Primary result: LLM-based writing support in educational contexts does not transfer gender bias to students' writing

## Executive Summary
This study investigates whether gender bias in Large Language Models (LLMs) transfers to students' writing in educational settings. Through a large-scale user study with 231 students writing peer reviews in German, researchers examined five groups with different levels of writing support: feature-based suggestions, no assistance, fine-tuned GPT-2/3 suggestions, and pre-trained GPT-3.5 suggestions. The research found no significant gender bias in student writing regardless of LLM assistance, demonstrating that AI writing support can be effectively used in educational contexts without transferring bias to student output.

## Method Summary
The study employed a mixed-methods approach combining experimental design with computational bias analysis. Researchers fine-tuned GPT-2 and GPT-3 models on 11,925 student-written peer reviews in German, then conducted a user study with 231 students divided into five groups receiving different levels of writing support. Bias was measured using three established metrics - GenBit gender bias score, WEAT, and SEAT tests - applied to model embeddings, model suggestions, and student output. The peer review task focused on business model evaluation, providing a neutral context for bias measurement.

## Key Results
- No significant gender bias detected in student writing across all experimental groups
- LLM suggestions themselves exhibited no significant gender bias despite fine-tuned GPT-2 embeddings showing bias
- Fine-tuned GPT-2 embeddings revealed significant gender bias (effect size of 0.27 for Math vs. Art and 0.56 for Science vs. Art)
- Feature-based suggestions showed bias in specific tests (WEAT 6, 6b, 8, 8b) but not in student output

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The bias present in LLM embeddings does not transfer to students' writing when suggestions are generated.
- **Mechanism**: Although fine-tuned GPT-2 embeddings showed significant gender bias (effect size of 0.27 for Math vs. Art and 0.56 for Science vs. Art), the model's suggestion generation process filters or neutralizes this bias before it reaches students.
- **Core assumption**: The generation process of suggestions involves mechanisms that mitigate or remove bias from the embeddings.
- **Evidence anchors**:
  - [abstract] "LLM suggestions themselves also exhibited no significant gender bias, despite fine-tuned GPT-2 model embeddings showing significant gender bias."
  - [section] "Interestingly, as demonstrated by our analyses, the gender biases revealed in GPT-2 embeddings did not translate into gender biases in suggestions."
- **Break condition**: If the suggestion generation process does not effectively filter bias, or if different model architectures process embeddings differently, bias could transfer to students' writing.

### Mechanism 2
- **Claim**: The educational context and task design prevent bias transfer from LLM suggestions to student writing.
- **Mechanism**: The peer review task focused on business model evaluation, which may not activate or amplify gender stereotypes present in the suggestions, leading to unbiased student output.
- **Core assumption**: The nature of the writing task influences how students incorporate suggestions and whether bias is expressed in their final text.
- **Evidence anchors**:
  - [abstract] "Our research is therefore optimistic about the use of AI writing support in the classroom, showcasing a context where bias in LLMs does not transfer to students' responses."
  - [section] "An important takeaway from our study is that the applied domain of education, although it is often a sensitive context, is moving towards integration with LLM assistants."
- **Break condition**: If the writing task involves content that is more prone to gender bias (e.g., topics about professions or roles with strong gender associations), bias might transfer more readily.

### Mechanism 3
- **Claim**: The lack of significant bias in LLM suggestions is due to the fine-tuning process on a non-biased corpus.
- **Mechanism**: Fine-tuning the models on a corpus of 11,925 student-written peer reviews in German, which was extended from a non-biased corpus, results in suggestions that do not contain significant gender bias.
- **Core assumption**: The fine-tuning corpus was indeed non-biased, and the fine-tuning process effectively learned to generate unbiased suggestions.
- **Evidence anchors**:
  - [section] "The two models for G2 and G3 were fine-tuned on an extended version of the non-biased corpus of Wambsganss et al. (2022b) containing 11, 925 peer reviews in German."
  - [section] "In each of these tests, we did not identify any significant bias in the suggestions."
- **Break condition**: If the fine-tuning corpus contained hidden biases or if the fine-tuning process did not adequately address bias, the suggestions could still be biased.

## Foundational Learning

- **Concept**: Gender bias measurement in NLP
  - **Why needed here**: Understanding how gender bias is quantified is essential to interpret the results of the GenBit, WEAT, and SEAT tests used in the study.
  - **Quick check question**: What does a positive GenBit bias score indicate about the relationship between words and gendered terms?

- **Concept**: Fine-tuning of language models
  - **Why needed here**: Knowing how fine-tuning works helps explain why the models might generate unbiased suggestions despite biased embeddings.
  - **Quick check question**: How does fine-tuning a pre-trained model on a specific corpus potentially reduce bias?

- **Concept**: Embedding spaces and semantic similarity
  - **Why needed here**: Understanding embeddings is crucial to grasp why biased embeddings do not necessarily lead to biased suggestions.
  - **Quick check question**: What is the relationship between word embeddings and the semantic associations measured by WEAT tests?

## Architecture Onboarding

- **Component map**: Data collection -> Model fine-tuning -> User study -> Bias analysis
- **Critical path**: Model fine-tuning → Suggestion generation → Student writing → Bias measurement in student output
- **Design tradeoffs**:
  - Using fine-tuned models vs. pre-trained models: Fine-tuned models may be more aligned with the educational context but could inherit bias from the training data.
  - Fixed context vs. infinite context in bias scoring: Different context sizes may capture different aspects of bias.
- **Failure signatures**:
  - Significant bias in student writing despite unbiased suggestions: Indicates that the task design or student behavior introduces bias.
  - Unbiased embeddings but biased suggestions: Suggests issues with the suggestion generation process.
- **First 3 experiments**:
  1. Analyze the correlation between the bias in model embeddings and the bias in suggestions for a subset of the data.
  2. Conduct a controlled study where students are given biased vs. unbiased suggestions to see the direct impact on their writing.
  3. Test the models on a different writing task (e.g., essays about professions) to see if bias transfer depends on the task context.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would bias transfer to student writing differ in educational contexts with content more likely to trigger gender stereotypes?
- Basis in paper: [inferred] The authors note their business case study about ski instruction is "neutral" and doesn't easily lend itself to measuring gender bias compared to other potential settings like medicine. They suggest results might differ with content where gendered professional roles are more prominent.
- Why unresolved: The study only examined one type of business case study. The neutral content may have limited exposure to gender stereotypes that could transfer from LLMs.
- What evidence would resolve it: Replicating the study with business case studies involving fields with strong gender associations (e.g., medicine, engineering, nursing) to compare bias transfer across different content types.

### Open Question 2
- Question: Would bias transfer differ across student populations with varying ages, educational levels, or cultural backgrounds?
- Basis in paper: [explicit] The authors acknowledge the need to extend research to "different student levels, different languages" and note their sample was limited to Western European university students.
- Why unresolved: The study only included students from one Western European university. Cultural norms and educational experiences may influence how students internalize and reproduce bias from AI suggestions.
- What evidence would resolve it: Conducting parallel studies with diverse student populations across different educational levels, countries, and cultural contexts to compare bias transfer patterns.

### Open Question 3
- Question: Why did bias appear in GPT-2 embeddings but not in model suggestions or student writing?
- Basis in paper: [explicit] The authors found significant gender bias in GPT-2 embeddings (particularly for career/family and math/art associations) but no bias in the suggestions generated from these embeddings or in student writing.
- Why unresolved: The study identified the bias presence but did not investigate the mechanisms preventing its transfer to suggestions and student output.
- What evidence would resolve it: Analyzing the fine-tuning process, prompt engineering, and decoding strategies used to generate suggestions to understand what filtering or transformation occurs between embeddings and output that prevents bias transfer.

## Limitations
- Results are limited to German educational context and peer review writing task studied
- The mechanisms preventing bias transfer from embeddings to suggestions remain unclear
- Study relies on specific bias measurement tools that may not capture all forms of bias

## Confidence
- High confidence: The empirical finding that no significant gender bias was detected in student writing across all experimental groups
- Medium confidence: The conclusion that LLM suggestions themselves were unbiased despite biased embeddings
- Low confidence: Generalizability of these results to other educational contexts, languages, or writing tasks

## Next Checks
1. Replicate the study using different writing tasks (e.g., essay writing about professions) to test whether bias transfer depends on task context
2. Conduct a controlled experiment where students receive either biased or unbiased suggestions to directly measure the impact on their writing
3. Analyze the correlation between bias in model embeddings and bias in suggestions for individual data points to understand the filtering mechanism