---
ver: rpa2
title: 'UER: A Heuristic Bias Addressing Approach for Online Continual Learning'
arxiv_id: '2309.04081'
source_url: https://arxiv.org/abs/2309.04081
tags:
- learning
- classes
- samples
- factor
- previous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Online continual learning aims to train models on a continuous
  data stream with a single pass, but suffers from catastrophic forgetting due to
  class imbalance. Existing rehearsal-based methods often use biased dot-product logits
  that favor current classes.
---

# UER: A Heuristic Bias Addressing Approach for Online Continual Learning

## Quick Facts
- arXiv ID: 2309.04081
- Source URL: https://arxiv.org/abs/2309.04081
- Reference count: 40
- Primary result: UER achieves 2.6% accuracy improvement over strong baselines on CIFAR100 and 2.2% on MiniImageNet in online continual learning

## Executive Summary
Online continual learning faces the challenge of catastrophic forgetting when models encounter class imbalance across data streams. Traditional rehearsal-based methods using dot-product logits exhibit bias favoring current classes due to unbalanced gradient propagation. UER addresses this by decomposing logits into norm and angle factors, using cosine logits (angle factor only) for current samples and dot-product logits (both factors) for replaying previous samples. This approach effectively balances learning of new knowledge while preserving historical knowledge.

## Method Summary
UER (Unbias Experience Replay) is a rehearsal-based method that learns current samples using cosine logits, which depend only on the angle factor, enabling rapid adaptation to novel knowledge. Previous samples are replayed using dot-product logits that incorporate both norm and angle factors, with the norm factor helping to balance predictions and mitigate bias toward current classes. The method employs reservoir sampling to maintain a memory buffer and uses a trade-off parameter α to balance the dot-product and cosine losses during replay.

## Key Results
- UER achieves up to 2.6% accuracy improvement over state-of-the-art baselines on CIFAR100
- UER achieves up to 2.2% accuracy improvement over strong competitors on MiniImageNet
- Consistent performance improvements across CIFAR10, CIFAR100, and MiniImageNet datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing logits into norm and angle factors reveals that angle factor captures novel knowledge while norm factor retains historical knowledge
- Mechanism: The dot-product logits (w·h + b) are decomposed into angle factor (cos(w,h)) and norm factor (∥w∥∥h∥ + b). Angle factor changes rapidly with new classes, enabling quick adaptation. Norm factor changes slowly, preserving stability for old classes
- Core assumption: The angle factor is primarily responsible for inter-class variation while norm factor handles intra-class variation
- Evidence anchors: [abstract] "By decomposing the dot-product logits into an angle factor and a norm factor, we empirically find that the bias problem mainly occurs in the angle factor"; [section 3.3] "The norm factor accounts for intra-class variation, and the angle factor accounts for inter-class variation [28]"
- Break condition: If the norm and angle factors become correlated or if the learning rate is too high causing both factors to change rapidly

### Mechanism 2
- Claim: Using cosine logits (angle factor only) for current samples enables rapid learning of novel knowledge
- Mechanism: Current samples are trained using only the angle factor as cosine logits, which allows the model to quickly adapt to new classes without interference from the norm factor
- Core assumption: Angle factor can independently capture sufficient information for novel class learning
- Evidence anchors: [abstract] "we empirically find that the bias problem mainly occurs in the angle factor, which can be used to learn novel knowledge as cosine logits"; [section 3.4] "the angle factor is effective in adapting novel knowledge with inter-class variation. Consequently, cosine logits that depends on the angle factor is suitable for learning current samples"
- Break condition: If cosine logits alone cannot distinguish between classes effectively, or if the scale parameter γ is not properly tuned

### Mechanism 3
- Claim: Replaying previous samples using dot-product logits (both factors) leverages norm factor to balance prediction and address bias
- Mechanism: Previous samples are replayed using both norm and angle factors, allowing the norm factor to correct biased predictions that favor current classes by increasing the influence of previous class parameters
- Core assumption: The norm factor can effectively counteract the bias introduced by unbalanced gradient propagation
- Evidence anchors: [abstract] "UER learns current samples only by the angle factor and further replays previous samples by both the norm and angle factors"; [section 3.4] "replaying previous samples with dot-product logits can enhance historical knowledge through the norm factor"
- Break condition: If the memory buffer becomes too small relative to the number of classes, or if the trade-off parameter α is not properly balanced

## Foundational Learning

- Concept: Continual learning and catastrophic forgetting
  - Why needed here: The paper addresses catastrophic forgetting in online continual learning, where models forget previously learned knowledge when learning new tasks
  - Quick check question: What is the difference between catastrophic forgetting and catastrophic interference?

- Concept: Experience replay and rehearsal methods
  - Why needed here: UER is a rehearsal-based method that uses a memory buffer to store and replay previous samples
  - Quick check question: How does experience replay help mitigate catastrophic forgetting in continual learning?

- Concept: Logit decomposition into norm and angle factors
  - Why needed here: The core innovation relies on decomposing dot-product logits into two interpretable factors
  - Quick check question: What mathematical relationship exists between dot-product logits and their norm/angle factor decomposition?

## Architecture Onboarding

- Component map: Feature extractor (CNN backbone) -> Predictor (linear layer) -> Memory buffer -> Learning component (cosine logits) -> Replaying component (dot-product logits) -> Testing component (dot-product logits)

- Critical path:
  1. Current sample arrives → processed through feature extractor → cosine logits computed → gradient update
  2. Memory buffer updated via reservoir sampling
  3. Previous samples retrieved from buffer → processed through feature extractor → dot-product logits computed → gradient update
  4. Testing sample arrives → processed through feature extractor → dot-product logits computed → prediction

- Design tradeoffs:
  - Using cosine logits for current samples enables fast adaptation but may lose some discriminative power
  - Using dot-product logits for replay maintains stability but may slow adaptation to new classes
  - Memory buffer size vs. computational cost tradeoff
  - Trade-off parameter α controls balance between dot-product and cosine losses during replay

- Failure signatures:
  - If accuracy on previous classes drops significantly, the norm factor may not be effectively correcting bias
  - If accuracy on current classes plateaus early, the angle factor may not be learning sufficiently
  - If performance is unstable across epochs, the balance between factors may be off
  - If memory buffer sampling is biased, certain classes may be underrepresented

- First 3 experiments:
  1. Run baseline ER on Split CIFAR100 with buffer size 1000 to establish performance floor
  2. Implement UER with varying α values (0.3, 0.5, 0.7) to find optimal trade-off
  3. Compare UER vs LUCIR vs ER on all three datasets (CIFAR10, CIFAR100, MiniImageNet) with multiple buffer sizes

## Open Questions the Paper Calls Out

- Question: How does the UER framework perform on tasks requiring rapid adaptation to entirely new domains (not just new classes within the same domain) in online continual learning scenarios?
  - Basis in paper: [inferred] The paper focuses on class-incremental learning within the same domain (CIFAR, MiniImageNet), but does not explore cross-domain adaptation scenarios
  - Why unresolved: The experimental evaluation is limited to domain-consistent datasets, leaving cross-domain performance unexplored
  - What evidence would resolve it: Empirical results on cross-domain datasets (e.g., domain adaptation benchmarks) showing UER's effectiveness or limitations when encountering entirely new data distributions

- Question: What is the theoretical relationship between the norm and angle factors in terms of their impact on gradient dynamics and catastrophic forgetting in continual learning?
  - Basis in paper: [explicit] The paper empirically shows that the norm factor helps retain historical knowledge while the angle factor captures novel knowledge, but does not provide a formal theoretical analysis of their interaction
  - Why unresolved: The paper relies on empirical observations without rigorous mathematical derivation of how these factors interact during gradient updates
  - What evidence would resolve it: A theoretical framework or formal proof demonstrating how the norm and angle factors affect gradient propagation and forgetting, supported by mathematical analysis

- Question: How does the performance of UER scale with extremely long sequences of tasks or classes (e.g., 100+ incremental stages) compared to shorter sequences?
  - Basis in paper: [inferred] Experiments use 5-10 stages, but the paper does not evaluate UER's scalability to very long task sequences
  - Why unresolved: The scalability of UER to many incremental stages remains untested, which is critical for real-world applications with continuous data streams
  - What evidence would resolve it: Experimental results on datasets with a large number of incremental stages (e.g., 50-100) comparing UER's performance degradation over time with other methods

## Limitations

- The approach relies on empirical observations about norm/angle factor separation without rigorous theoretical justification
- The assumption that norm and angle factors operate independently may not hold in all scenarios, particularly with aggressive learning rates or highly overlapping class distributions
- The generalizability of the approach to non-image domains or datasets with very high class overlap remains untested

## Confidence

- **High confidence**: The experimental results showing UER outperforming baseline methods on standard benchmarks (CIFAR10, CIFAR100, MiniImageNet)
- **Medium confidence**: The mechanism explanation about norm and angle factor roles, as it relies on empirical observations without theoretical proof
- **Low confidence**: The generalizability of the approach to non-image domains or datasets with very high class overlap

## Next Checks

1. Test UER on a synthetic dataset where norm and angle factors can be controlled independently to verify the decomposition assumption
2. Evaluate performance degradation when deliberately introducing correlation between norm and angle factors during training
3. Implement ablation studies with varying scale parameter γ to determine sensitivity to this hyperparameter across different dataset sizes