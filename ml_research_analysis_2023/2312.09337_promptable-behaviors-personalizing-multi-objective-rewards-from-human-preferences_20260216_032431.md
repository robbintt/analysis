---
ver: rpa2
title: 'Promptable Behaviors: Personalizing Multi-Objective Rewards from Human Preferences'
arxiv_id: '2312.09337'
source_url: https://arxiv.org/abs/2312.09337
tags:
- reward
- agent
- efficiency
- weight
- house
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Promptable Behaviors, a framework that enables
  efficient personalization of robotic agents to diverse human preferences in complex
  environments. The method uses multi-objective reinforcement learning to train a
  single adaptable policy, and introduces three methods to infer human preferences:
  demonstrations, preference feedback on trajectory comparisons, and language instructions.'
---

# Promptable Behaviors: Personalizing Multi-Objective Rewards from Human Preferences

## Quick Facts
- arXiv ID: 2312.09337
- Source URL: https://arxiv.org/abs/2312.09337
- Reference count: 40
- Key outcome: Introduces a framework enabling efficient personalization of robotic agents to diverse human preferences in complex environments without re-training the policy.

## Executive Summary
This paper presents Promptable Behaviors, a framework for personalizing robotic agent behavior to diverse human preferences in complex environments. The method uses multi-objective reinforcement learning (MORL) to train a single adaptable policy that can handle any linear combination of objectives at test time. Three methods are introduced to infer human preferences: demonstrations, preference feedback on trajectory comparisons, and language instructions. Experiments on object-goal navigation and flee navigation in ProcTHOR and RoboTHOR environments demonstrate the agent's behavior can be effectively prompted to satisfy human preferences through reward weight adjustments without any policy fine-tuning.

## Method Summary
Promptable Behaviors uses MORL to train a single policy adaptable to diverse human preferences by adjusting reward weight vectors during inference. The policy architecture consists of a CLIP ResNet-50 visual encoder, a reward weight encoder with a feedforward neural network and codebook, and a GRU-based navigation policy. The method introduces three preference inference techniques: (1) demonstrations - using human-provided trajectories to predict reward weights, (2) preference feedback on trajectory comparisons - collecting pairwise or group trajectory comparisons and optimizing weights using the Bradley-Terry model, and (3) language instructions - mapping natural language to reward weights. The trained policy is evaluated on object-goal navigation and flee navigation tasks in ProcTHOR and RoboTHOR environments.

## Key Results
- Preference feedback on group trajectory comparisons achieved 93.5% prediction accuracy, outperforming pairwise comparisons and demonstrations
- Group trajectory comparison was 17.8% more effective than pairwise comparison while reducing labeling effort
- The method successfully prompted agent behaviors through reward weight adjustments without any policy fine-tuning
- Performance remained consistent across different human preference profiles in the evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Promptable Behaviors allows efficient personalization of robotic agents to diverse human preferences without re-training the policy.
- Mechanism: The method uses multi-objective reinforcement learning (MORL) to train a single adaptable policy that can handle any linear combination of objectives. Human preferences are captured by adjusting reward weight vectors during inference, not by fine-tuning the policy.
- Core assumption: Human preferences remain constant over time and can be represented as linear combinations of multiple objectives.
- Evidence anchors:
  - [abstract] "We use multi-objective reinforcement learning to train a single policy adaptable to a broad spectrum of preferences."
  - [section 3.2] "Our approach aims to develop a promptable and efficient framework...our method focuses on training a policy that can handle any linear combination of different objectives at test time."
  - [corpus] Weak - neighboring papers focus on multi-objective alignment and preference-based RL but don't directly address the single-policy MORL approach described here.
- Break condition: If human preferences are non-linear, dynamic, or context-dependent, the linear reward weight vector assumption fails and the method would need modification.

### Mechanism 2
- Claim: Preference feedback on trajectory comparisons achieves higher performance than demonstrations or language instructions for inferring reward weights.
- Mechanism: The method collects pairwise or group trajectory comparisons from humans and uses optimization to find reward weights that best explain these preferences, leveraging the Bradley-Terry model for probabilistic preference modeling.
- Core assumption: Human preferences can be inferred from relative trajectory comparisons rather than absolute demonstrations or language.
- Evidence anchors:
  - [section 3.3.2] "We further develop weight prediction methods that take human feedback, in the form of comparisons between agent trajectories..."
  - [section 4.3] "Preference feedback on group trajectory comparisons shows the highest prediction performance, 93.5%..."
  - [corpus] Weak - neighboring papers discuss preference-based RL but don't provide direct evidence for the superiority of trajectory comparison over other methods.
- Break condition: If users cannot easily distinguish between trajectory pairs/groups, or if preferences are too complex to capture through binary comparisons, this method would be less effective.

### Mechanism 3
- Claim: Group trajectory comparison is 17.8% more effective than pairwise comparison and reduces labeling effort.
- Mechanism: Instead of comparing individual trajectory pairs, users compare groups of trajectories. This amplifies preference signals and reduces ambiguity, allowing more efficient constraint-based optimization of reward weights.
- Core assumption: Comparing groups of trajectories provides clearer preference signals than individual pairs, especially when trajectories within groups share similar reward weight vectors.
- Evidence anchors:
  - [section 3.3.2] "We also introduce a novel trajectory comparison method called group trajectory comparison, where the human user observes groups of trajectories instead of individual pairs."
  - [section 4.3] "Our human evaluations demonstrate the effectiveness of our method. In particular, our human evaluations demonstrate the effectiveness of our method."
  - [corpus] Weak - neighboring papers discuss preference learning but don't provide evidence for group comparison advantages.
- Break condition: If group size becomes too large, users may struggle to compare diverse trajectories within groups, reducing the effectiveness of this approach.

## Foundational Learning

- Concept: Multi-Objective Reinforcement Learning (MORL)
  - Why needed here: MORL allows training a single policy that can adapt to different combinations of objectives by adjusting reward weights during inference, rather than training separate policies for each preference.
  - Quick check question: How does MORL differ from traditional single-objective RL in terms of policy adaptability?

- Concept: Preference-Based Learning
  - Why needed here: The method uses human preferences (demonstrations, trajectory comparisons, language instructions) to infer reward weights that align agent behavior with user values.
  - Quick check question: What are the advantages of using preference feedback on trajectory comparisons versus direct demonstrations?

- Concept: Reward Weight Vector Optimization
  - Why needed here: After training the MORL policy, the method optimizes reward weight vectors to match human preferences without fine-tuning the policy.
  - Quick check question: How does the optimization process differ when using demonstrations versus preference feedback?

## Architecture Onboarding

- Component map:
  CLIP ResNet-50 -> Reward Weight Encoder (FFNN + Codebook) -> GRU-based Navigation Policy -> Action Output
  Human Preference Input -> Reward Weight Prediction Module -> Reward Weight Vector

- Critical path:
  1. Train MORL policy with random reward weight vectors across episodes
  2. Freeze policy and receive user preference input
  3. Predict optimal reward weight vector using appropriate method
  4. Adjust policy behavior through reward weight vector during inference

- Design tradeoffs:
  - Single-policy MORL vs. multi-policy MORL: Single policy is more efficient but may have lower performance for extreme weight combinations
  - Codebook vs. raw weights: Codebook provides better generalization but adds complexity
  - Group vs. pairwise comparisons: Groups are more efficient but may be harder for users to compare

- Failure signatures:
  - Poor generalization to unseen reward weights
  - Suboptimal performance when preferences are highly non-linear
  - Difficulty in capturing complex preferences through simple comparisons
  - Computational overhead in optimization during inference

- First 3 experiments:
  1. Verify MORL policy adapts behavior to different reward weight vectors by testing with peaked weights on single objectives
  2. Test reward weight prediction from demonstrations by collecting one demonstration and measuring cosine similarity to ground truth weights
  3. Compare pairwise vs. group trajectory comparison by measuring prediction accuracy and user effort for the same preference inference task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Promptable Behaviors compare when using non-linear combinations of objectives rather than linear combinations?
- Basis in paper: [inferred] The paper assumes linear combinations of objectives and does not explore non-linear combinations. It mentions that future work will extend the approach to consider non-linear preferences.
- Why unresolved: The paper only demonstrates the method with linear combinations of objectives, leaving the performance with non-linear combinations unexplored.
- What evidence would resolve it: Experiments comparing the performance of Promptable Behaviors using linear vs. non-linear combinations of objectives on the same tasks would resolve this question.

### Open Question 2
- Question: How does the proposed method scale to more complex environments with a larger number of objectives or more complex agent dynamics?
- Basis in paper: [inferred] The paper demonstrates the method on two navigation tasks in relatively simple environments. It does not address scalability to more complex environments or tasks with more objectives.
- Why unresolved: The experiments are limited to simple navigation tasks, and the paper does not discuss scalability challenges or performance in more complex scenarios.
- What evidence would resolve it: Testing Promptable Behaviors on more complex tasks (e.g., manipulation, multi-agent scenarios) with a larger number of objectives would provide evidence for scalability.

### Open Question 3
- Question: How does the proposed method perform when human preferences are dynamic or change over time, rather than being static as assumed in the paper?
- Basis in paper: [explicit] The paper assumes that human preferences remain constant over time and each human preference is captured through a linear combination of multiple objectives.
- Why unresolved: The paper does not address scenarios where human preferences may change dynamically, which is a common occurrence in real-world applications.
- What evidence would resolve it: Experiments evaluating Promptable Behaviors when human preferences are changed during or between episodes would provide evidence for its performance in dynamic preference scenarios.

## Limitations

- The linear reward weight assumption may not capture complex, non-linear human preferences that could be context-dependent or dynamic over time.
- The evaluation focuses on specific navigation tasks in simulated environments without testing transfer to more diverse or unstructured real-world scenarios.
- Human evaluation sample sizes are not specified, making it difficult to assess statistical significance of the reported preference feedback performance differences.

## Confidence

- High confidence in the MORL policy training framework and its ability to adapt behavior through reward weight adjustment
- Medium confidence in the relative performance of preference feedback methods
- Low confidence in real-world applicability due to limited evaluation scope

## Next Checks

1. Test the method's robustness by evaluating on tasks with non-linear reward structures or where human preferences change during interaction, to identify limitations of the linear weight assumption.

2. Conduct a larger-scale human evaluation with specified sample sizes and statistical significance testing to validate the reported performance differences between preference feedback methods.

3. Implement a transfer learning experiment where the trained policy is applied to a different robotic platform or environment to assess generalization beyond the training domains.