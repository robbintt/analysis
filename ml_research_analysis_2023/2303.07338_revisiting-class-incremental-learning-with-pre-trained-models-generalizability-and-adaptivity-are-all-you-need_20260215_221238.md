---
ver: rpa2
title: 'Revisiting Class-Incremental Learning with Pre-Trained Models: Generalizability
  and Adaptivity are All You Need'
arxiv_id: '2303.07338'
source_url: https://arxiv.org/abs/2303.07338
tags:
- uni00000013
- uni00000003
- uni00000024
- uni00000048
- uni00000027
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates class-incremental learning (CIL) with
  pre-trained models (PTMs), highlighting two key factors: adaptivity for model updating
  and generalizability for knowledge transfer. It demonstrates that frozen PTMs can
  provide generalizable embeddings for CIL, enabling a simple baseline to outperform
  state-of-the-art methods.'
---

# Revisiting Class-Incremental Learning with Pre-Trained Models: Generalizability and Adaptivity are All You Need

## Quick Facts
- arXiv ID: 2303.07338
- Source URL: https://arxiv.org/abs/2303.07338
- Reference count: 40
- Key outcome: Demonstrates frozen pre-trained models can outperform state-of-the-art CIL methods through generalizability, and proposes ADAM to balance adaptivity and generalizability

## Executive Summary
This paper investigates class-incremental learning (CIL) with pre-trained models (PTMs), identifying two key factors: adaptivity for model updating and generalizability for knowledge transfer. The authors demonstrate that frozen PTMs can provide sufficiently generalizable embeddings for CIL, enabling a simple baseline to outperform state-of-the-art methods. To bridge domain gaps between pre-trained and downstream datasets, they propose ADAM, which aggregates embeddings from PTM and adapted models for classifier construction. ADAM can be combined with any parameter-efficient tuning method, balancing PTM's generalizability and the adapted model's adaptivity. The paper also introduces four new benchmarks to evaluate PTM-based CIL methods, addressing the data overlapping issue in traditional benchmarks.

## Method Summary
The paper proposes ADAM (ADapt And Merge), a method for class-incremental learning with pre-trained models. ADAM works by adapting the PTM only on the first incremental stage using parameter-efficient tuning methods (VPT, Adapter, or SSF), then concatenating embeddings from the adapted model and frozen PTM for classifier construction using prototype-based classification with cosine similarity. The adapted model is frozen after the first stage to prevent catastrophic forgetting. The method can use any parameter-efficient tuning approach and is evaluated across seven datasets including new benchmarks designed to have larger domain gaps from ImageNet.

## Key Results
- SimpleCIL (frozen PTM) outperforms state-of-the-art CIL methods on multiple benchmarks
- ADAM consistently improves over both SimpleCIL and fully fine-tuned approaches
- Parameter-efficient tuning methods (SSF, Adapter) work best with ADAM
- New benchmarks (ImageNet-A, ObjectNet, OmniBenchmark) reveal limitations of existing CIL methods
- CLIP-based PTMs show superior performance due to larger training corpus and contrastive loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained models (PTMs) inherently possess generalizable embeddings that can be directly transferred to new tasks without additional training.
- Mechanism: The embeddings learned during large-scale pre-training capture universal visual patterns that remain discriminative across different datasets, allowing prototype-based classification to succeed.
- Core assumption: The feature space learned by PTMs is sufficiently aligned with downstream tasks to enable effective classification without adaptation.
- Evidence anchors: [abstract] "PTMs possess generalizable embeddings, which can be easily transferred for CIL"; [section 3.2] "To evaluate the generalizability of PTM, we formulate a CIL task using VTAB [80] dataset and test the performance of state-of-the-art PTM-based methods"
- Break condition: Significant domain shift between pre-training data and target task causes the universal features to lose discriminative power.

### Mechanism 2
- Claim: Sequential fine-tuning of PTMs causes catastrophic forgetting of previously learned knowledge.
- Mechanism: Continuous adaptation of model parameters to new tasks overwrites feature representations that were useful for earlier classes, leading to performance degradation on old tasks.
- Core assumption: The feature space is not stable enough to preserve old knowledge while adapting to new classes.
- Evidence anchors: [abstract] "sequentially tuning the PTM will harm the structural information and weaken the generalizability"; [section 3.2] "However, finetuning suffers catastrophic forgetting of old classes since features are continually changing"
- Break condition: Adaptation is limited to only the first incremental stage, preventing overwriting of old features.

### Mechanism 3
- Claim: Merging adapted model features with original PTM features preserves both adaptivity and generalizability.
- Mechanism: Concatenating embeddings from the adapted model and frozen PTM creates a richer feature representation that captures both task-specific patterns and universal visual concepts.
- Core assumption: The combination of adapted and pre-trained features provides complementary information that improves classification.
- Evidence anchors: [abstract] "ADAM, which aggregates the embeddings of PTM and adapted models for classifier construction"; [section 4.1] "By merging the embedding functions of the PTM and the adapted model, the extracted features are more representative than any one of them alone"
- Break condition: The adapted model overwrites too much of the original feature space, making the frozen PTM features irrelevant.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why sequential fine-tuning fails is crucial for grasping the motivation behind ADAM's approach
  - Quick check question: Why does training on new data typically cause performance degradation on old tasks?

- Concept: Transfer learning and domain adaptation
  - Why needed here: The paper relies on PTMs' ability to transfer knowledge to new domains, which requires understanding of feature generalizability
  - Quick check question: What factors determine whether features learned on one dataset will work well on another?

- Concept: Prototype-based classification
  - Why needed here: The baseline method uses average embeddings as classifiers, which is central to understanding why frozen PTMs work well
  - Quick check question: How does using prototype features as classifiers differ from learned weight vectors?

## Architecture Onboarding

- Component map: Pre-trained model (frozen) → Optional adaptation layer → Feature concatenation → Prototype extraction → Classification
- Critical path: Adaptation → Merge → Prototype extraction → Classification
- Design tradeoffs: Adaptivity vs generalizability, parameter efficiency vs performance, complexity vs simplicity
- Failure signatures: Catastrophic forgetting (over-adaptation), poor generalization (under-adaptation), high computational cost (excessive parameters)
- First 3 experiments:
  1. Compare SimpleCIL vs ADAM on a small dataset to verify the baseline claim
  2. Test different adaptation methods (VPT, Adapter, SSF) to determine optimal trade-off
  3. Evaluate on multiple datasets to confirm robustness across domains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of adaptation stages in ADAM for different types of pre-trained models (ViT vs CNN) and datasets?
- Basis in paper: [inferred] The paper mentions adapting the model only in the first stage but notes that ADAM w/ Finetune performs worse than SimpleCIL on certain datasets like CUB B0 Inc10, ObjectNet B0 Inc10, and OmniBenchmark B0 Inc30. It also shows that ADAM achieves best performance when tuned only in the first stage, but the influence of tuning stages is only explored on CIFAR100 Base0 Inc10.
- Why unresolved: The paper only conducts ablation studies on CIFAR100 Base0 Inc10 with ViT-B/16-IN21K. It does not explore the optimal adaptation stages across different model architectures (ViT vs CNN) and diverse datasets with varying domain gaps.
- What evidence would resolve it: Systematic experiments varying the number of adaptation stages (0 to B) across all 7 datasets with both ViT and CNN backbones, measuring the trade-off between performance and parameter efficiency.

### Open Question 2
- Question: How does the generalizability of different pre-trained models (supervised vs self-supervised, different architectures) impact their performance in class-incremental learning without any adaptation?
- Basis in paper: [explicit] The paper shows that SimpleCIL (frozen PTM) already beats state-of-the-art methods on several benchmarks, and that CLIP performs better than ImageNet21K pre-trained ViTs due to massive training corpus and contrastive loss. However, it doesn't systematically compare the inherent generalizability of different PTM types.
- Why unresolved: The paper only briefly mentions that CLIP performs better than ImageNet21K pre-trained ViTs in Figure 6, but doesn't conduct a comprehensive study of how different PTM training objectives (supervised vs self-supervised) and architectures affect their natural generalizability for CIL tasks.
- What evidence would resolve it: Controlled experiments comparing frozen PTMs from different training paradigms (supervised, self-supervised, contrastive) and architectures across all 7 benchmarks, measuring their CIL performance without any adaptation.

### Open Question 3
- Question: What is the theoretical relationship between the domain gap size between pre-trained and incremental datasets and the optimal adaptation strategy in ADAM?
- Basis in paper: [inferred] The paper argues that model adaptation is essential to bridge the domain gap, and that ADAM's effectiveness depends on the domain gap size. It introduces new benchmarks with large domain gaps but doesn't provide theoretical analysis of this relationship.
- Why unresolved: The paper provides empirical evidence that ADAM works better on datasets with larger domain gaps (ImageNet-A, ObjectNet, OmniBenchmark) compared to CIFAR100/ImageNet-R, but doesn't develop a theoretical framework explaining how domain gap magnitude should influence the adaptation strategy.
- What evidence would resolve it: Mathematical analysis or empirical studies quantifying the relationship between statistical measures of domain gap (e.g., Wasserstein distance, maximum mean discrepancy) and the optimal balance between adaptation and preservation of pre-trained features.

## Limitations
- Claims about PTM generalizability rely on benchmarks that may not fully represent real-world domain shifts
- ADAM's effectiveness depends critically on the assumption that adapted and frozen features provide complementary information, but this is not thoroughly validated
- Evaluation focuses primarily on top-1 accuracy without analyzing computational efficiency or parameter overhead

## Confidence
- High confidence in catastrophic forgetting claims during sequential fine-tuning (well-documented in continual learning literature)
- Medium confidence in generalizability of frozen PTM embeddings (needs validation on more diverse domain shifts)
- Medium confidence in ADAM's effectiveness (consistent improvements but lacks analysis of failure cases)

## Next Checks
1. **Domain diversity test**: Evaluate ADAM on datasets with significant domain shift from ImageNet (e.g., medical imaging, satellite imagery) to verify generalizability claims hold beyond natural images.

2. **Parameter efficiency analysis**: Compare ADAM's parameter count and computational overhead against baseline methods to quantify the adaptivity-generalizability trade-off in terms of efficiency.

3. **Failure mode characterization**: Systematically test ADAM on scenarios where the adapted model significantly overwrites PTM features (e.g., highly specialized domains) to identify conditions where feature merging fails.