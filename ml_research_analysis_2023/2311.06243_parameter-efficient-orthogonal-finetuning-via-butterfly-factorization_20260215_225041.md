---
ver: rpa2
title: Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization
arxiv_id: '2311.06243'
source_url: https://arxiv.org/abs/2311.06243
tags:
- boft
- orthogonal
- butterfly
- matrix
- matrices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies parameter-efficient orthogonal finetuning for
  adapting large foundation models to downstream tasks. The key insight is that dense
  orthogonal matrices can be parameterized as products of multiple sparse orthogonal
  matrices, enabling significant parameter savings.
---

# Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization

## Quick Facts
- arXiv ID: 2311.06243
- Source URL: https://arxiv.org/abs/2311.06243
- Reference count: 40
- Primary result: Orthogonal Butterfly (BOFT) outperforms state-of-the-art parameter-efficient finetuning methods by a considerable margin on language, vision, and text-to-image tasks

## Executive Summary
This paper introduces Orthogonal Butterfly (BOFT), a parameter-efficient orthogonal finetuning method that parameterizes dense orthogonal matrices as products of sparse orthogonal matrices inspired by butterfly structures in fast Fourier transforms. By leveraging this factorization, BOFT achieves O(d log d) parameter efficiency while maintaining full expressiveness. The method generalizes existing orthogonal finetuning approaches and demonstrates superior performance across diverse foundation models including large language models, vision transformers, and text-to-image diffusion models.

## Method Summary
BOFT parameterizes dense orthogonal matrices as products of multiple sparse orthogonal butterfly matrices, inspired by the Cooley-Tukey FFT algorithm. Each butterfly component is parameterized using the Cayley transform on 2b×2b blocks, ensuring orthogonality while maintaining sparsity. During training, the model applies the composed orthogonal matrix to frozen pretrained weights, optimizing only the butterfly parameters. This approach subsumes existing orthogonal finetuning methods and introduces an inductive bias toward fast linear transforms that benefits generalization.

## Key Results
- BOFT achieves 10-1000× parameter reduction compared to full fine-tuning while maintaining or improving performance
- Outperforms existing orthogonal finetuning (OFT) and LoRA across GLUE, VTAB-1K, and Stable Diffusion benchmarks
- Larger m (number of butterfly components) yields better performance for text-to-image tasks
- Smaller block size b with larger m provides superior parameter efficiency

## Why This Works (Mechanism)

### Mechanism 1: Dense Orthogonal Matrix Decomposition
Dense orthogonal matrices can be decomposed into products of sparse orthogonal matrices without loss of expressiveness, enabling efficient parameterization. The butterfly factorization splits a dense orthogonal matrix into a sequence of sparse orthogonal matrices (butterfly components), each parameterized with O(log d) parameters. Multiplication of these sparse matrices reconstructs a dense orthogonal matrix while keeping the total parameter count O(d log d) instead of O(d²).

### Mechanism 2: Orthogonal Constraint Parameter Reduction
Orthogonality constraints reduce the number of free parameters in each sparse factor compared to unconstrained dense matrices. In an orthogonal matrix, only O(d) entries are free parameters (the rest are determined by orthogonality). By enforcing orthogonality on each sparse factor, the total number of trainable parameters across m factors becomes m·O(d), which is much less than O(d²) for a dense matrix.

### Mechanism 3: Inductive Bias from Fast Transform Structure
The butterfly structure induces an inductive bias that generalizes well because it mirrors fast linear transforms (FFT, DCT, Hadamard). Butterfly matrices efficiently implement fast linear transforms, which are structured and sparse. By using them in orthogonal finetuning, the learned orthogonal matrix inherits this structure, favoring solutions that resemble classic, well-understood transforms that tend to generalize better.

## Foundational Learning

- **Concept**: Orthogonal matrices and their parameterization (Cayley transform, Householder reflections)
  - **Why needed here**: BOFT relies on maintaining orthogonality throughout finetuning; understanding how to parameterize orthogonal matrices is critical
  - **Quick check question**: What is the Cayley parameterization for an orthogonal matrix R in terms of a skew-symmetric matrix Q?

- **Concept**: Butterfly factorization and its connection to fast Fourier transform (FFT)
  - **Why needed here**: BOFT uses butterfly matrices; knowing how they work and why they are efficient is key to understanding the method
  - **Quick check question**: How many sparse factors are needed to represent an N-point FFT using radix-2 butterfly factorization?

- **Concept**: Information transmission on graphs and sparse matrix factorization
  - **Why needed here**: The paper frames orthogonal matrix construction as an information transmission problem; understanding this viewpoint clarifies the design choices
  - **Quick check question**: In a grid graph with d nodes per level and m levels, how many edges are required to ensure every node in the first level can reach every node in the last level?

## Architecture Onboarding

- **Component map**: Pretrained weight matrix W⁰ -> Orthogonal butterfly matrix R(m,b) -> Output z = (R(m,b) W⁰)⊤ x
- **Critical path**: During each training iteration, compute R(m,b) by multiplying m sparse orthogonal matrices, apply it to W⁰, compute loss, backpropagate through the Cayley parameterization
- **Design tradeoffs**:
  - Parameter efficiency vs. expressivity: Larger m and smaller b increase parameter count but improve expressivity; smaller m and larger b reduce parameters but may limit the space of representable orthogonal matrices
  - Runtime vs. memory: More butterfly components (larger m) increase runtime due to repeated matrix multiplications but keep memory low since each factor is sparse
- **Failure signatures**:
  - Training instability: If orthogonality is not maintained (e.g., due to numerical errors), the model may diverge
  - Underfitting: If m is too small relative to the task, the hypothesis class may be too restrictive
  - Overfitting: If m is too large with insufficient regularization, the model may overfit to training data
- **First 3 experiments**:
  1. Sanity check on expressiveness: Approximate a random dense orthogonal matrix with BOFT(m,b) for various (m,b) and measure approximation error
  2. GLUE benchmark test: Finetune DeBERTaV3 on GLUE using BOFT vs. OFT vs. LoRA, compare accuracy and parameter counts
  3. VTAB-1K transfer learning: Apply BOFT to DINOv2-large on VTAB-1K, evaluate per-task accuracy and compare to full finetuning

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the butterfly structure's inductive bias compare to other structured matrix approaches in terms of generalization and preventing overfitting?
  - **Basis**: The paper argues that the structured inductive bias induced by butterfly factorization is beneficial to generalization but lacks direct comparison with other structured matrix approaches
  - **What evidence would resolve it**: A comparative study of BOFT's generalization performance against other structured matrix approaches on a variety of downstream tasks

- **Open Question 2**: Is the butterfly network the most efficient way to transmit information for composing dense orthogonal matrices, or are there more efficient network structures?
  - **Basis**: The paper acknowledges uncertainty about whether the butterfly network is the most efficient way to transmit information for composing dense orthogonal matrices
  - **What evidence would resolve it**: A comparative study of different network structures for composing dense orthogonal matrices in terms of parameter efficiency, expressivity, and computational complexity

## Limitations

- Expressiveness vs. Parameter Efficiency Trade-off: While BOFT maintains full expressiveness through butterfly factorization, there's uncertainty about whether the structured sparsity pattern can represent arbitrary orthogonal transformations needed for all downstream tasks
- Numerical Stability: The method relies on maintaining orthogonality through Cayley parameterization during training, but numerical errors could accumulate during the product of multiple sparse orthogonal matrices
- Computational Overhead: Although BOFT reduces parameter count, the computational cost of multiplying multiple sparse matrices may introduce non-trivial overhead compared to simpler parameter-efficient methods

## Confidence

- **High Confidence**: The core mechanism of parameterizing dense orthogonal matrices as products of sparse orthogonal matrices is mathematically sound and well-established in the context of fast transforms
- **Medium Confidence**: The claim that butterfly structure provides beneficial inductive bias for generalization is supported by structural arguments and empirical results, but lacks rigorous theoretical justification
- **Low Confidence**: The paper doesn't provide comprehensive ablation studies on how different butterfly configurations affect performance across diverse tasks

## Next Checks

1. **Expressiveness Validation**: Systematically test BOFT's ability to approximate random orthogonal matrices across different (m,b) configurations, measuring approximation error and identifying configurations where expressiveness breaks down

2. **Orthogonality Stability Analysis**: Monitor orthogonality metrics (e.g., ||R⊤R - I||) throughout training for various batch sizes and learning rates to quantify numerical stability and identify conditions that cause drift

3. **Computational Efficiency Benchmark**: Compare wall-clock training time and inference latency of BOFT against LoRA and full fine-tuning across different model sizes, measuring the actual trade-off between parameter efficiency and computational overhead