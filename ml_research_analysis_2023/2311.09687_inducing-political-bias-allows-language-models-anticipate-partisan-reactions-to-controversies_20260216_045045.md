---
ver: rpa2
title: Inducing Political Bias Allows Language Models Anticipate Partisan Reactions
  to Controversies
arxiv_id: '2311.09687'
source_url: https://arxiv.org/abs/2311.09687
tags:
- political
- neutral
- negative
- care
- cheating
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of capturing political bias
  in LLMs by introducing a novel approach using instruction tuning with a single model
  rather than separate models for each political faction. The method involves finetuning
  a language model with bipartisan instructions to generate responses across the political
  spectrum, then evaluating its outputs across three dimensions: stances, emotions,
  and moral foundations.'
---

# Inducing Political Bias Allows Language Models Anticipate Partisan Reactions to Controversies

## Quick Facts
- **arXiv ID**: 2311.09687
- **Source URL**: https://arxiv.org/abs/2311.09687
- **Reference count**: 40
- **Primary result**: A single instruction-tuned LLM can effectively represent multiple political ideologies across stance, emotion, and moral dimensions, with stronger performance in emotional and moral capture than stance detection.

## Executive Summary
This paper addresses the challenge of capturing political bias in LLMs by introducing a novel approach using instruction tuning with a single model rather than separate models for each political faction. The method involves finetuning a language model with bipartisan instructions to generate responses across the political spectrum, then evaluating its outputs across three dimensions: stances, emotions, and moral foundations. The study introduces two evaluation tasks: Partisan Bias Divergence Assessment and Partisan Class Tendency Prediction. Results show the model effectively captures emotional and moral nuances, though with some limitations in stance detection. The findings suggest this approach is promising for creating politically aware LLMs, particularly for applications requiring sensitivity to political bias, while also highlighting areas for improvement in stance detection accuracy.

## Method Summary
The approach uses instruction tuning on a pretrained LLM (LLaMA-2-7B) with 4.7 million partisan tweets to create a model capable of generating politically biased responses across the spectrum. The model is finetuned using explicit instructions that indicate whether the response should represent liberal or conservative perspectives, optionally referencing specific entities. The finetuning employs QLoRA with a batch size of 2200, learning rate of 2e-4, and sequence length of 48 over 3 epochs. The resulting model is evaluated on three dimensions—stances, emotions, and moral foundations—using three real-world datasets covering COVID-19, abortion, and congressional discourse. The evaluation compares the model's generated distributions against actual partisan discourse using KL divergence and accuracy metrics.

## Key Results
- The finetuned model effectively captures emotional and moral nuances across political ideologies, showing lower KL divergence compared to the pretrained baseline in most categories
- Stance detection accuracy reaches 71.9% for liberal and 74.1% for conservative perspectives on COVID-19 topics, demonstrating understanding but with room for improvement
- The approach successfully enables a single model to represent multiple political viewpoints through instruction conditioning, avoiding the need for separate models per ideology

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction tuning enables a single model to represent multiple political ideologies by conditioning responses on explicit political instructions.
- Mechanism: The model is finetuned on tweets labeled with political affiliations, where instructions explicitly indicate whether the tweet represents a liberal or conservative perspective. During inference, the same instruction format prompts the model to generate responses aligned with the specified ideology.
- Core assumption: Political bias can be effectively induced through instruction conditioning rather than requiring separate models for each political faction.
- Evidence anchors:
  - [abstract]: "our work innovates by employing a singular, instruction-tuned LLM to reflect a spectrum of political ideologies"
  - [section 3.1]: "We finetune the model with instructions in Figure 1... For a liberal tweet, we indicate in the instruction that it is a liberal/left/Democratic perspective"
  - [corpus]: Weak evidence - the corpus shows related work on political bias in LLMs but doesn't directly validate the instruction tuning mechanism
- Break condition: The model fails to generate ideologically coherent responses when given political instructions, or the instruction conditioning becomes ambiguous across nuanced political positions.

### Mechanism 2
- Claim: The finetuned model captures emotional and moral nuances of political discourse better than the pretrained baseline.
- Mechanism: By training on partisan tweets, the model learns to associate specific emotional expressions and moral foundations with political ideologies, which is then measured through emotion detection and moral foundation analysis.
- Core assumption: Political discourse contains distinct emotional and moral patterns that can be learned from labeled social media data.
- Evidence anchors:
  - [abstract]: "Our findings reveal the model's effectiveness in capturing emotional and moral nuances"
  - [section 5.3]: "The finetuned model generally exhibits a lower KLD across various topics, suggesting a closer alignment with the real-world emotional distributions"
  - [corpus]: Weak evidence - corpus contains related work on political bias but lacks direct validation of emotional/moral learning mechanisms
- Break condition: The model's emotional and moral outputs show no correlation with actual partisan discourse patterns, or performance degrades significantly on new political topics.

### Mechanism 3
- Claim: The model's stance detection capability, while improved, still shows limitations in accurately capturing partisan positions.
- Mechanism: Stance detection is measured by comparing the model's predicted stances (support/oppose/neutral) against ground truth labels from real-world data, with accuracy calculated for each political group.
- Core assumption: Stance detection is inherently more challenging than emotion/moral detection due to the complexity of political positions and contextual dependencies.
- Evidence anchors:
  - [abstract]: "albeit with some challenges in stance detection"
  - [section 5.2]: "The finetuned model's predictive accuracy in stance detection shows a substantial understanding... though with some limitations"
  - [corpus]: Weak evidence - corpus mentions stance detection research but doesn't provide direct validation of the specific challenges observed
- Break condition: Stance detection accuracy falls below chance level or shows no improvement over the pretrained baseline across all political topics.

## Foundational Learning

- Concept: Political ideology classification
  - Why needed here: The model must distinguish between liberal and conservative perspectives to generate ideologically appropriate responses
  - Quick check question: Can you explain how political affiliation was determined for the training tweets?

- Concept: Instruction tuning methodology
  - Why needed here: The approach relies on conditioning model outputs through explicit instructions rather than traditional finetuning
  - Quick check question: What's the difference between instruction tuning and standard finetuning approaches?

- Concept: Stance, emotion, and moral foundation detection
  - Why needed here: These three dimensions provide a comprehensive framework for evaluating political bias in model outputs
  - Quick check question: Why did the researchers choose these three specific dimensions for analysis?

## Architecture Onboarding

- Component map: Pretrained LLM (LLaMA-2-7B) → Instruction Tuning Layer → Partisan Response Generator → Multi-dimensional Evaluator (Stance/Emotion/Moral)
- Critical path: Data collection → Instruction preparation → Model finetuning → Response generation → Evaluation across three dimensions
- Design tradeoffs: Single model with instruction conditioning vs. multiple models for each ideology (resource efficiency vs. potential specialization)
- Failure signatures: High KL divergence between model and real-world distributions, low accuracy in partisan class tendency prediction, inconsistent responses across political instructions
- First 3 experiments:
  1. Test instruction conditioning with simple political prompts to verify the model generates appropriate responses
  2. Evaluate emotion detection performance on politically neutral vs. politically charged content
  3. Compare stance detection accuracy across different political topics to identify systematic weaknesses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the finetuned model's performance vary when applied to political discourse in different languages or cultures?
- Basis in paper: [inferred] The paper discusses the potential for future research to adapt the model for cross-cultural and multilingual contexts, indicating that current findings may not generalize across different languages or cultures.
- Why unresolved: The study focuses on English-language political discourse within the United States, and there is no analysis of how the model performs with non-English political content or in different cultural contexts.
- What evidence would resolve it: Conducting experiments with the finetuned model on datasets of political discourse from various countries and in multiple languages, followed by an analysis of the model's ability to accurately capture political bias, emotions, and moral foundations across these diverse contexts.

### Open Question 2
- Question: What are the ethical implications of using LLMs to analyze and generate politically biased content, and how can potential harms be mitigated?
- Basis in paper: [explicit] The paper mentions the necessity for ethical consideration in the deployment of language models within politically sensitive contexts, suggesting an awareness of potential ethical issues.
- Why unresolved: The study does not delve into the specifics of ethical concerns or propose concrete measures to address potential harms that could arise from the use of LLMs in political analysis and content generation.
- What evidence would resolve it: Developing a framework for ethical guidelines in the use of LLMs for political discourse, including strategies for transparency, bias mitigation, and the prevention of misuse, supported by case studies or simulations demonstrating the effectiveness of these measures.

### Open Question 3
- Question: Can the model's stance detection accuracy be improved by incorporating additional contextual information or by using a more diverse training dataset?
- Basis in paper: [explicit] The paper identifies challenges in stance detection accuracy and suggests that refining these capabilities remains a priority, potentially through integrating more diverse datasets and enhancing contextual understanding.
- Why unresolved: The current study does not explore the impact of different types of contextual information or the diversity of the training dataset on the model's stance detection performance.
- What evidence would resolve it: Experimenting with the model using various forms of contextual information (e.g., historical data, user profiles) and training it on a more diverse set of political discourse data, followed by an evaluation of the changes in stance detection accuracy compared to the current model.

## Limitations

- Limited Stance Detection Performance: The model shows only moderate accuracy (71.9-74.1%) in detecting partisan stances, with significant variation across topics
- Dataset Specificity and Generalizability: The finetuning data focuses on U.S. political discourse from 2019-2020, which may not capture the full evolution of political discourse or represent all viewpoints
- Evaluation Methodology Constraints: The study relies on external models (ChatGPT, SpanEmo, and a fine-tuned transformer) for evaluation without providing detailed validation of these tools' reliability across political contexts

## Confidence

- **High Confidence**: The claim that instruction tuning enables a single model to represent multiple political ideologies is well-supported by the methodology description and evaluation results
- **Medium Confidence**: The claim that the finetuned model captures emotional and moral nuances better than the pretrained baseline is supported by KL divergence metrics, though evaluation relies on external models
- **Low Confidence**: The stance detection performance claims are the least robust, with accuracy rates only moderately above baseline and significant variation across topics

## Next Checks

- Conduct ablation studies to determine the relative contribution of instruction conditioning versus dataset size to the model's performance
- Test the model's performance on political topics not present in the training data to assess generalizability
- Implement human evaluation studies to validate the automated evaluation metrics using political science experts