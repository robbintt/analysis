---
ver: rpa2
title: Actor critic learning algorithms for mean-field control with moment neural
  networks
arxiv_id: '2309.04317'
source_url: https://arxiv.org/abs/2309.04317
tags:
- control
- function
- gradient
- mean-field
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a policy gradient and actor-critic algorithm
  for solving mean-field control problems in continuous time using reinforcement learning.
  The approach uses parametrized randomized policies and represents the value function
  via moment neural networks defined on the Wasserstein space of probability measures.
---

# Actor critic learning algorithms for mean-field control with moment neural networks

## Quick Facts
- arXiv ID: 2309.04317
- Source URL: https://arxiv.org/abs/2309.04317
- Reference count: 21
- One-line primary result: Actor-critic algorithms with moment neural networks achieve <5% relative error on mean-field control problems

## Executive Summary
This paper develops actor-critic reinforcement learning algorithms for continuous-time mean-field control problems using moment neural networks on the Wasserstein space of probability measures. The approach parametrizes randomized policies and represents value functions through the first L moments of distributions, enabling efficient computation of the H-operator specific to mean-field settings. The algorithm alternates between policy evaluation (critic updates) and policy gradient steps (actor updates) with carefully tuned two-timescale learning rates. Numerical experiments demonstrate high accuracy in approximating optimal controls and value functions across various examples, including high-dimensional settings and nonlinear quadratic mean-field control with controlled volatility.

## Method Summary
The algorithm solves mean-field control problems in continuous time using actor-critic methods with moment neural networks. The actor parametrizes randomized policies through moment neural networks that depend on the first L moments of the state distribution. The critic approximates the value function using the same moment representation. The H-operator specific to mean-field settings is computed efficiently by sampling entire distribution trajectories rather than single states, using automatic differentiation on expectations rather than expectations of derivatives. The algorithm uses two-timescale learning rates (ρC >> ρA) with ADAM optimizer, and includes an exploration parameter λ(e) that decreases over epochs. Implementation uses N initial distributions with M particles each, and learning rates are tuned per problem instance.

## Key Results
- Achieved <5% relative error in approximating optimal controls and value functions across multiple examples
- Successfully handled high-dimensional problems (dimension d=20) with controlled volatility
- Demonstrated accuracy on nonlinear quadratic mean-field control problems where analytical solutions are intractable
- Showed that two-timescale learning rates (ρC at least one order of magnitude higher than ρA) are crucial for convergence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Moment neural networks efficiently represent value functions over the Wasserstein space by parametrizing them through the first L moments of the probability distribution.
- Mechanism: Instead of sampling single states, the algorithm samples entire distributions and represents them via empirical measures of particles. The value function is approximated as Ψη(t, x, μ̄L) where μ̄L are the first L moments, enabling tractable computation of derivatives required for the H-operator.
- Core assumption: The value function depends on the distribution only through its moments, and moment neural networks have sufficient expressive power to approximate continuous functions on the Wasserstein space.
- Evidence anchors:
  - [abstract]: "The learning for both the actor (policy) and critic (value function) is facilitated by a class of moment neural network functions on the Wasserstein space of probability measures"
  - [section 3]: "A moment neural network function on [0, T] × Rd × P2(Rd) of order L ∈ N* is a parametric function in the form ϕη(t, x, μ) = Ψη(t, x, μL)"
- Break condition: If the value function has a structure that depends on higher-order features of the distribution beyond moments (e.g., tail behavior), or if L is chosen too small for the problem complexity.

### Mechanism 2
- Claim: The H-operator specific to mean-field settings is computed efficiently by taking gradients of expectations rather than expectations of gradients.
- Mechanism: The algorithm uses ∇θEξ∼μ[...] instead of Eξ∼μ[∇θ...] to compute Hθ[Jη], reducing computational complexity from O(M) to O(1) where M is the number of particles used to approximate the distribution.
- Core assumption: Automatic differentiation can handle the computation of gradients after taking expectations, and the H-operator can be approximated well by this approach.
- Evidence anchors:
  - [section 3]: "it is crucial to rely on the above expression of the operator Hθ where the differentiation is taken on the expectation Eξ∼μ[.], and not the reversal: expectation of the differentiation"
  - [section 3]: "hence saving an order M for the complexity cost"
- Break condition: If the H-operator requires finer-grained differentiation (e.g., second-order derivatives) that cannot be computed accurately through this approximation.

### Mechanism 3
- Claim: Two-timescale learning rates (ρC >> ρA) enable stable convergence by allowing the critic to learn faster than the actor.
- Mechanism: The critic updates (policy evaluation) use a higher learning rate ρC to quickly approximate the value function, while the actor updates (policy improvement) use a smaller learning rate ρA to make gradual improvements to the policy.
- Core assumption: The critic can be learned to reasonable accuracy before the actor needs to update, and this separation of timescales prevents policy oscillation or divergence.
- Evidence anchors:
  - [section 4]: "it is crucial to use two timescales approach... ρC should be at least one order of magnitude higher than ρA to get good convergence"
  - [section 5]: "ρC = 0.01" vs "ρA = 0.0005" in multiple examples
- Break condition: If the problem has very high-dimensional state spaces or complex dynamics where the critic cannot converge quickly enough relative to actor updates.

## Foundational Learning

- Concept: Wasserstein space and probability measure representation
  - Why needed here: The algorithm operates on distributions as state variables, requiring understanding of how probability measures are represented and manipulated in the Wasserstein space.
  - Quick check question: What is the difference between representing a distribution as a point in Rd versus as a point in P2(Rd)?

- Concept: Lions derivative and measure derivatives
  - Why needed here: The H-operator involves ∂μφ(t, x, μ)(ξ) and ∂ξ∂μφ(t, x, μ)(ξ), which are derivatives of functions defined on the Wasserstein space with respect to the measure.
  - Quick check question: How does the Lions derivative differ from classical derivatives, and why is it needed for functions defined on probability measures?

- Concept: Universal approximation theorems for neural networks
  - Why needed here: The paper relies on moment neural networks satisfying universal approximation properties for functions on the Wasserstein space.
  - Quick check question: What does it mean for a class of neural networks to satisfy a universal approximation theorem, and why is this important for the algorithm's theoretical guarantees?

## Architecture Onboarding

- Component map: Actor network (mθ) → Policy distribution πθ(.|t, x, μ) → Environment dynamics → Trajectory of distributions → Critic network (Jη) → Loss functions → Parameter updates. The key components are the moment neural networks for both actor and critic, the H-operator computation module, and the two-timescale optimizer.

- Critical path: Initialize parameters → Sample initial distributions → Forward simulation of distribution trajectories → Compute H-operator approximations → Update critic (ρC) → Compute policy gradients → Update actor (ρA) → Repeat. The bottleneck is typically the forward simulation and H-operator computation.

- Design tradeoffs: Higher L (number of moments) increases expressive power but also computational cost and convergence difficulty; larger M (particles per distribution) improves distribution approximation but increases simulation cost; higher ρC speeds critic learning but may cause instability.

- Failure signatures: If relative error remains high despite training, check if L is too small for the problem structure; if training oscillates, verify the two-timescale learning rates are properly separated; if convergence is slow, examine the exploration schedule λ(e).

- First 3 experiments:
  1. Implement the simplest LQ example (Section 5.1.2) with L=2 and verify that the learned control matches the analytical solution.
  2. Test the H-operator computation with different numbers of particles M to verify the O(1) vs O(M) complexity claim.
  3. Run the algorithm with equal learning rates for actor and critic to observe the instability that the two-timescale approach prevents.

## Open Questions the Paper Calls Out

- Question: How does the algorithm perform on mean-field control problems with non-separable coefficients, where the drift and diffusion terms depend on the state and control in a non-separable form?
  - Basis in paper: [explicit] The paper suggests that "subsequent developments could encompass the extension to non-separable forms within the state and control components of drift and diffusion coefficients."
  - Why unresolved: The paper only considers the separable case where the drift and diffusion terms are written as the sum of a state-dependent term and a control-dependent term. The performance of the algorithm in the non-separable case is not explored.
  - What evidence would resolve it: Numerical results demonstrating the accuracy and efficiency of the algorithm on mean-field control problems with non-separable coefficients.

- Question: How does the algorithm perform on mean-field control problems with jump-diffusion processes, where the intensities of the jumps are unknown?
  - Basis in paper: [explicit] The paper suggests that "a compelling direction for further investigation could involve mean-field dynamics governed by jump diffusion processes, where the intensities of the jumps remain unknown."
  - Why unresolved: The paper only considers mean-field control problems with continuous-time diffusion processes. The performance of the algorithm on jump-diffusion processes is not explored.
  - What evidence would resolve it: Numerical results demonstrating the accuracy and efficiency of the algorithm on mean-field control problems with jump-diffusion processes.

- Question: How does the choice of the number of moments (L) affect the performance of the algorithm, and is there an optimal value of L for a given problem?
  - Basis in paper: [inferred] The paper mentions that "it is crucial to use two timescales approach" and that "it is crucial to maintain a low number of moments (typically two or three)" for good convergence.
  - Why unresolved: The paper does not provide a systematic study of the effect of the number of moments on the performance of the algorithm, and it is not clear if there is an optimal value of L for a given problem.
  - What evidence would resolve it: A systematic study of the effect of the number of moments on the performance of the algorithm, including numerical results demonstrating the accuracy and efficiency of the algorithm for different values of L.

## Limitations

- The universal approximation theorem for moment neural networks on Wasserstein spaces remains unproven, creating uncertainty about the theoretical foundations.
- The algorithm relies heavily on the two-timescale learning rate assumption (ρC >> ρA), which requires careful tuning and may not generalize well to all problem structures.
- The choice of L (number of moments) lacks theoretical guidance, with empirical results suggesting L=2 or 3 works well but no systematic study of optimal selection.

## Confidence

- **High confidence**: The empirical methodology and numerical results (5% relative error claims) are well-supported by the presented experiments and code implementation appears feasible.
- **Medium confidence**: The theoretical foundations around moment neural networks and the H-operator computation, while reasonable, lack complete proofs for the continuous-time setting.
- **Low confidence**: Claims about universal approximation for moment neural networks on Wasserstein spaces and the optimality of the specific exploration schedule λ(e).

## Next Checks

1. **Convergence robustness test**: Run the algorithm with varying ratios of ρC/ρA (1x, 10x, 100x) to empirically verify the necessity of the two-timescale assumption and identify failure modes when this assumption is violated.

2. **Moment order sensitivity analysis**: Systematically test the algorithm with different values of L (1 through 5) on the same LQ problems to quantify the trade-off between approximation accuracy and computational cost, and determine when higher-order moments become necessary.

3. **Distribution representation stress test**: Create pathological initial distributions (e.g., multi-modal, heavy-tailed) that depend on features beyond the first L moments and verify whether the moment neural network representation fails or requires higher L values, testing the core assumption about moment sufficiency.