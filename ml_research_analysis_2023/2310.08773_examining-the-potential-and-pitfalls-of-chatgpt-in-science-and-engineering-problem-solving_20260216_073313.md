---
ver: rpa2
title: Examining the Potential and Pitfalls of ChatGPT in Science and Engineering
  Problem-Solving
arxiv_id: '2310.08773'
source_url: https://arxiv.org/abs/2310.08773
tags:
- chatgpt
- problem
- problems
- incorrect
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study explored ChatGPT's problem-solving capabilities across
  different types of physics problems. It found that ChatGPT (with GPT-4) could solve
  62.5% of well-specified problems but only 8.3% of under-specified problems.
---

# Examining the Potential and Pitfalls of ChatGPT in Science and Engineering Problem-Solving

## Quick Facts
- arXiv ID: 2310.08773
- Source URL: https://arxiv.org/abs/2310.08773
- Reference count: 27
- Key outcome: ChatGPT (GPT-4) solved 62.5% of well-specified physics problems but only 8.3% of under-specified problems, revealing three failure modes: inaccurate physical modeling, unreasonable assumptions, and calculation errors

## Executive Summary
This study systematically evaluated ChatGPT's (GPT-4) problem-solving capabilities across 40 physics problems from an engineering course, categorizing them as well-specified or under-specified. The research revealed a stark performance difference: ChatGPT achieved 62.5% success on well-specified problems but only 8.3% on under-specified ones. Three distinct failure modes were identified through detailed error analysis. While prompt engineering showed moderate improvements for some failures, calculation errors remained resistant to intervention. The findings highlight ChatGPT's conceptual understanding capabilities but expose limitations in real-world application and numerical computation.

## Method Summary
The study used 40 physics problems from an engineering course, divided into 16 well-specified and 24 under-specified problems. ChatGPT with GPT-4 was tested using two prompt variants: "solve the following physics problem" and "solve the following physics problem step-by-step." Solutions were compared against instructor solutions and categorized as correct or incorrect. A detailed error analysis identified three failure modes: inaccurate physical modeling, unreasonable assumptions about missing data, and calculation errors. The step-by-step prompt was then applied to all initially failed problems to assess whether prompting strategies could improve performance.

## Key Results
- ChatGPT solved 62.5% of well-specified problems versus only 8.3% of under-specified problems
- Three failure modes identified: inaccurate physical modeling, unreasonable assumptions, and calculation errors
- Step-by-step prompting moderately improved performance for modeling and assumption failures but not calculation errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT's step-by-step prompting improves its ability to construct accurate physical models
- Mechanism: The prompt forces the model to decompose the problem into discrete reasoning steps, which helps it recognize missing physical relationships and spatial configurations that it otherwise overlooks
- Core assumption: The model's reasoning capabilities are intact but require explicit structuring to apply them systematically
- Evidence anchors:
  - [abstract] "The prompt of solving the problem step-by-step led to more precise and deliberate problem-solving as illustrated in this example"
  - [section 4.3] "Among the 28 problems that ChatGPT initially failed to solve, it was able to correctly solve three with the step-by-step prompting. Two of the three were related to the failure mode of ChatGPT not being able to construct accurate models about the real world"
  - [corpus] No direct corpus evidence found - this appears to be a novel finding from the study
- Break condition: If the step-by-step prompt does not force explicit model construction steps, the benefit disappears

### Mechanism 2
- Claim: ChatGPT's knowledge base enables strong concept identification but weak real-world application
- Mechanism: The model can identify relevant physics concepts from problem statements due to its broad training data, but struggles to apply these concepts to real-world scenarios requiring spatial reasoning or physical intuition
- Core assumption: The model's training data contains sufficient conceptual knowledge but lacks the experiential grounding needed for real-world application
- Evidence anchors:
  - [abstract] "ChatGPT demonstrated a high level of proficiency in identifying the relevant physics concepts to apply based on the given problem statement"
  - [section 4.1] "ChatGPT's strength in this facet of problem-solving differs from typical human performance, as students often struggle to identify what concepts to apply as the starting point in solving unfamiliar problems"
  - [corpus] "The integration of Large Language Models (LLMs) like ChatGPT into the workflows of geotechnical engineering has a high potential to transform how the discipline approaches problem-solving" suggests recognition of this conceptual-strength gap
- Break condition: If the model is fine-tuned on more experiential, real-world problem data, this distinction may narrow

### Mechanism 3
- Claim: ChatGPT's calculation errors are systematic and not reduced by prompting strategies
- Mechanism: The model's arithmetic and mathematical computation capabilities are fundamentally limited, and these errors occur regardless of problem structure or prompting approach
- Core assumption: The model's architecture is not optimized for numerical computation, treating mathematical expressions as text rather than performing actual calculations
- Evidence anchors:
  - [abstract] "The study revealed three distinct failure modes: 1) failure to construct accurate models of the physical world, 2) failure to make reasonable assumptions about missing data, and 3) calculation errors"
  - [section 4.3] "Overall, while prompt engineering had no impact on reducing calculation errors"
  - [corpus] "Physics provides fundamental laws that describe and predict the natural world. AI systems aspiring toward more general, real-world intelligence must therefore demonstrate strong physics problem-solving abilities" implies recognition that current LLMs struggle with this
- Break condition: If integrated with symbolic computation engines, the calculation error pattern may change

## Foundational Learning

- Concept: Physics problem categorization (abstract vs. real-world, well-specified vs. under-specified)
  - Why needed here: The study's methodology and findings hinge on understanding how problem characteristics affect AI performance
  - Quick check question: How would you categorize a problem asking to calculate the force needed to support a bookshelf if given all dimensions and weights vs. one asking to determine if a given wall mount is strong enough without specifying the dresser's weight?

- Concept: Prompt engineering techniques (chain-of-thought, step-by-step)
  - Why needed here: The study tests whether these techniques can improve AI problem-solving performance
  - Quick check question: What is the difference between zero-shot chain-of-thought prompting and the step-by-step prompting used in this study?

- Concept: Failure mode analysis in AI systems
  - Why needed here: The study systematically categorizes where and why the AI fails to understand its limitations
  - Quick check question: What distinguishes a failure to construct an accurate physical model from a failure to make reasonable assumptions about missing data?

## Architecture Onboarding

- Component map: Problem statement → Prompt formatting → GPT-4 inference → Solution extraction → Error analysis → Performance categorization
- Critical path: Problem statement → Prompt formatting → GPT-4 inference → Solution extraction → Error analysis → Performance categorization
- Design tradeoffs: Using ChatGPT interface provides ecological validity but limits control over model parameters compared to direct API access
- Failure signatures: Calculation errors manifest as incorrect numerical results even when methodology is sound; modeling failures show up as missing physical relationships or incorrect spatial reasoning
- First 3 experiments:
  1. Test the same problems with different prompting strategies (zero-shot vs. few-shot vs. step-by-step)
  2. Vary problem context from abstract to real-world while keeping data specification constant
  3. Introduce controlled missing data in well-specified problems to test assumption-making capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ChatGPT's problem-solving accuracy change when using more advanced prompt engineering techniques beyond simple step-by-step instructions?
- Basis in paper: [explicit] The paper tested basic step-by-step prompting but found it only moderately improved performance for certain failure modes.
- Why unresolved: The study only tested one prompt engineering technique (step-by-step), leaving other potentially more effective techniques unexplored.
- What evidence would resolve it: Comparative experiments testing multiple advanced prompt engineering techniques (e.g., chain-of-thought, few-shot examples, role-playing) on the same problem set.

### Open Question 2
- Question: What is the relationship between the complexity of real-world physics problems and ChatGPT's success rate in solving them?
- Basis in paper: [inferred] The paper found ChatGPT solved 62.5% of well-specified problems but only 8.3% of under-specified problems, suggesting complexity affects performance.
- Why unresolved: The study used a limited number of problems and did not systematically vary complexity while controlling for other factors.
- What evidence would resolve it: A systematic study varying problem complexity (number of steps, concepts involved, data ambiguity) while measuring ChatGPT's accuracy.

### Open Question 3
- Question: How do different versions of large language models (LLMs) compare in their ability to solve under-specified physics problems?
- Basis in paper: [explicit] The study used ChatGPT with GPT-4 but noted the probabilistic nature of LLMs means results may vary across versions.
- Why unresolved: Only one version of one model was tested, despite the rapid evolution of LLM technology.
- What evidence would resolve it: Comparative testing of multiple current and future LLM versions (GPT-3, GPT-4, Claude, etc.) on the same under-specified problem set.

## Limitations

- Small sample size of 40 problems from a single course limits generalizability
- Exact problem text not provided, making precise replication difficult
- Only tested ChatGPT with GPT-4 without comparing to other LLMs or newer versions

## Confidence

**High Confidence**: ChatGPT's differential performance between well-specified (62.5% success) and under-specified problems (8.3% success) is well-supported by the data presented. The identification of three distinct failure modes (modeling, assumptions, calculations) is clearly documented through systematic analysis.

**Medium Confidence**: The claim that step-by-step prompting moderately improves performance for modeling and assumption failures but not calculation errors is supported by the study's data, though the sample size of problems that could benefit from prompting (only 3 out of 28 initially failed problems) is small.

**Low Confidence**: The broader implications for STEM education and human-AI collaboration, while thoughtfully discussed, extend beyond what the empirical data directly demonstrates. The study provides evidence about current limitations but makes projections about future educational applications that require additional research.

## Next Checks

1. **External Validation**: Test the same problem categories with a larger, more diverse set of physics problems across multiple institutions to verify whether the 62.5% vs 8.3% success rate disparity holds across different problem sets and educational contexts.

2. **Model Comparison**: Replicate the study using GPT-4 Turbo, Claude 3, and other contemporary LLMs to determine whether the identified failure modes are specific to ChatGPT/GPT-4 or represent broader limitations of current LLM architectures.

3. **Prompt Engineering Optimization**: Systematically test a broader range of prompt engineering techniques (including few-shot examples, explicit reasoning chains, and constraint specifications) to determine if more sophisticated prompting strategies can reduce calculation errors, which the current study found resistant to improvement.