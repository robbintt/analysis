---
ver: rpa2
title: '"Kurosawa": A Script Writer''s Assistant'
arxiv_id: '2308.03122'
source_url: https://arxiv.org/abs/2308.03122
tags:
- generation
- plot
- plots
- movie
- scene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents \u201CKurosawa\u201D, an AI-based script-writing\
  \ workbench that generates movie plots and scenes from brief prompts. The system\
  \ uses GPT-3 fine-tuned on a dataset of 1000 manually annotated movie plots and\
  \ 1000 movie scenes."
---

# "Kurosawa": A Script Writer's Assistant

## Quick Facts
- arXiv ID: 2308.03122
- Source URL: https://arxiv.org/abs/2308.03122
- Reference count: 6
- Human evaluation shows generated plots achieve fluency scores of 3.98/5 and creativity scores of 3.29/5, while generated scenes achieve fluency scores of 4.48/5 and creativity scores of 3.9/5.

## Executive Summary
This paper presents "Kurosawa," an AI-based script-writing workbench that generates movie plots and scenes from brief prompts. The system uses GPT-3 fine-tuned on a dataset of 1000 manually annotated movie plots and 1000 movie scenes. The plot dataset includes storylines linked to Wikipedia plots and is annotated with a 4-act story structure. The scene dataset includes movie scenes from IMSDb tagged with sluglines, action lines, character names, and dialogues. Human evaluation demonstrates that the generated plots achieve fluency scores of 3.98/5 and creativity scores of 3.29/5, while generated scenes achieve fluency scores of 4.48/5 and creativity scores of 3.9/5. The work is notable for being the first approach to generate scenes from descriptions and for providing a working benchmark dataset and models for automatic script generation.

## Method Summary
The system fine-tunes GPT-3 Curie model with annotated datasets of 1000 movie plots and 1000 movie scenes for four epochs using specified hyperparameters (temperature: 0.7, top-p: 1, frequency penalty: 0.1, presence penalty: 0.1, max tokens: 900). The plot dataset is manually annotated using a 4-act story structure, while the scene dataset is tagged with four major screenplay elements (sluglines, action lines, character names, and dialogues). The system is evaluated through human evaluation using a five-point Likert Scale, measuring fluency, creativity, coherence, relevance, and likability.

## Key Results
- Generated plots achieve fluency scores of 3.98/5 and creativity scores of 3.29/5
- Generated scenes achieve fluency scores of 4.48/5 and creativity scores of 3.9/5
- First approach to generate scenes from brief descriptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Manual annotation of movie plots using a 4-act structure improves model coherence and logical flow.
- Mechanism: The 4-act structure provides explicit narrative boundaries (Act 1, Act 2A, Act 2B, Act 3) that guide the model to generate plot segments with clear transitions and logical progression.
- Core assumption: Dividing the plot into four acts with specific delimiters helps the model understand and replicate the narrative arc better than unannotated plots.
- Evidence anchors: Qualitative observations for annotated Hollywood plots show "The plots are much more coherent, and the endings are logical."

### Mechanism 2
- Claim: Tagging key screenplay elements (sluglines, action lines, character names, dialogues) in scenes enables the model to generate structurally accurate and format-compliant script outputs.
- Mechanism: By wrapping each element with specific tags (e.g., 〈bsl〉...〈esl〉 for sluglines), the model learns to distinguish and generate each component correctly within the screenplay format.
- Core assumption: Explicit tagging of screenplay elements helps the model understand the hierarchical structure of scenes and generate text that adheres to screenplay conventions.
- Evidence anchors: Qualitative observations for scene generation show "The model produces a well-structured scene."

### Mechanism 3
- Claim: Fine-tuning GPT-3 with datasets containing short prompts and corresponding outputs enables the model to generate creative and contextually relevant plots and scenes.
- Mechanism: By training on 1000 manually annotated plots and 1000 annotated scenes, the model learns to map brief prompts to detailed outputs that follow narrative structures and screenplay formats.
- Core assumption: A large enough dataset of paired prompts and outputs allows the model to learn the mapping between brief descriptions and full narratives or scenes.
- Evidence anchors: Human ratings for scene generation show average scores of 4.48 for fluency, 3.9 for creativity, 3.48 for likability, 3.46 for coherence, and 3.86 for relevance.

## Foundational Learning

- Concept: Narrative structures (e.g., 3-act or 4-act structure)
  - Why needed here: Understanding narrative structures is crucial for creating plots and scenes that follow a logical progression and engage the audience.
  - Quick check question: What are the key components of the 4-act structure used in this work, and how do they contribute to plot development?

- Concept: Screenplay formatting and elements
  - Why needed here: Knowledge of screenplay formatting (sluglines, action lines, character names, dialogues) is essential for generating scenes that are both structurally correct and easy to interpret by scriptwriters.
  - Quick check question: What are the four major elements tagged in the scene generation dataset, and why is it important to distinguish between them?

- Concept: Fine-tuning large language models
  - Why needed here: Fine-tuning GPT-3 with custom datasets allows the model to adapt to the specific task of generating movie plots and scenes from brief prompts.
  - Quick check question: What are the key steps involved in fine-tuning GPT-3, and how does the choice of hyperparameters affect the generated output?

## Architecture Onboarding

- Component map: Data Collection -> Data Annotation -> Model Training -> Evaluation -> Application
- Critical path:
  1. Collect and annotate datasets (plots and scenes).
  2. Fine-tune GPT-3 model with annotated datasets.
  3. Evaluate model outputs using automatic and human metrics.
  4. Deploy model on ErosNow platform.

- Design tradeoffs:
  - Dataset size vs. annotation quality: Larger datasets may be harder to annotate consistently, but provide more training data.
  - Model complexity vs. inference speed: Larger GPT-3 models may generate better outputs but require more computational resources.
  - Manual annotation vs. automated annotation: Manual annotation ensures quality but is time-consuming and may introduce human bias.

- Failure signatures:
  - Low coherence scores: Model may not understand narrative structure or prompt context.
  - High repetition: Model may be overfitting to training data or not generating diverse content.
  - Structural issues in generated scenes: Model may not have learned to distinguish between screenplay elements.

- First 3 experiments:
  1. Fine-tune GPT-3 with annotated plots using 4-act structure and evaluate coherence and relevance.
  2. Fine-tune GPT-3 with annotated scenes and evaluate structural accuracy and creativity.
  3. Compare model performance with and without genre information to assess controllability.

## Open Questions the Paper Calls Out

- How does the model perform when given longer prompts compared to shorter prompts?
- What is the impact of including genres in the input on the generated plots?
- How does the model handle multilingual scripts, especially in the context of Indian scripts?

## Limitations
- Lack of detailed annotation guidelines for the 4-act structure and specific process of including genres in plot generation
- Absence of automatic evaluation metrics and detailed comparison with baseline models
- Paper does not address the generalizability of the model to other domains beyond movie scripts

## Confidence

- High: The system generates coherent and creative movie plots and scenes, as evidenced by human evaluation scores
- Medium: The 4-act structure annotation improves plot coherence, supported by qualitative observations but lacking quantitative evidence
- Low: The system is the first to generate scenes from brief descriptions, as the paper does not provide a comprehensive review of related work in this area

## Next Checks

1. Conduct a thorough review of existing literature to confirm the novelty of generating scenes from brief descriptions
2. Perform automatic evaluation using metrics such as BLEU and ROUGE to complement human evaluation scores
3. Investigate the impact of varying the dataset size and annotation quality on model performance to identify optimal settings for training