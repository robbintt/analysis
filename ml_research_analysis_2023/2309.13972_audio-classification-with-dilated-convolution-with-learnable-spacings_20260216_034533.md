---
ver: rpa2
title: Audio classification with Dilated Convolution with Learnable Spacings
arxiv_id: '2309.13972'
source_url: https://arxiv.org/abs/2309.13972
tags:
- audio
- dcls
- size
- kernel
- classi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work explores replacing depthwise separable convolutions\
  \ with dilated convolutions with learnable spacings (DCLS) in audio classification\
  \ models. The authors apply this substitution to three state-of-the-art architectures\u2014\
  ConvNeXt, ConvFormer, and FastViT\u2014using the AudioSet benchmark."
---

# Audio classification with Dilated Convolution with Learnable Spacings

## Quick Facts
- arXiv ID: 2309.13972
- Source URL: https://arxiv.org/abs/2309.13972
- Reference count: 40
- Primary result: DCLS improves mAP by 0.6 points on average without increasing parameters

## Executive Summary
This work explores replacing depthwise separable convolutions with dilated convolutions with learnable spacings (DCLS) in audio classification models. The authors apply this substitution to three state-of-the-art architectures—ConvNeXt, ConvFormer, and FastViT—using the AudioSet benchmark. The DCLS replacement improves mean average precision (mAP) by an average of 0.6 points without increasing parameter count, though with a moderate 13–23% reduction in throughput. These findings demonstrate DCLS's effectiveness beyond vision tasks and highlight its potential for audio classification applications.

## Method Summary
The authors replace depthwise separable convolution (DSC) layers with kernel size 7 in three architectures (ConvNeXt-T, ConvFormer-S18, FastVit-SA24) using Dilated Convolution with Learnable Spacings (DCLS). They process AudioSet audio data into mel-spectrograms (128x1001) and train for 60 epochs with batch size 4096 using AdamW optimizer (0.05 weight decay). DCLS-Gauss with kernel size 232 and 26 elements replaces the original DSC layers (kernel size 7, 49 elements) to maintain parameter efficiency while improving feature extraction.

## Key Results
- DCLS improves mAP by 0.6 points on average across all three architectures
- Parameter count remains unchanged or decreases compared to original DSC layers
- Throughput reduction of 13-23% is considered acceptable for the accuracy gains
- Gaussian interpolation outperforms bilinear interpolation in DCLS implementations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DCLS improves mAP by learning optimal kernel spacings rather than using fixed dilated patterns
- Mechanism: The learnable positions in DCLS kernels allow the model to adaptively adjust receptive fields based on audio features, capturing both fine-grained and broad contextual patterns more effectively than fixed dilation
- Core assumption: Audio classification benefits from flexible receptive field sizes that can be optimized during training
- Evidence anchors:
  - [abstract]: "Dilated convolution with learnable spacings (DCLS) is a recent convolution method in which the positions of the kernel elements are learned throughout training by backpropagation"
  - [section 3.3]: "we replaced all DSC layers having a kernel size equal to 7 with a DCLS convolution layer" and "To learn the positions (and standard deviations for DCLS-Gauss) of each kernel element, we followed the same training techniques"
  - [corpus]: Weak - corpus papers discuss DCLS in vision tasks but don't provide specific evidence about audio classification performance mechanisms
- Break condition: If audio patterns don't benefit from variable receptive fields or if the learned positions don't generalize beyond training data

### Mechanism 2
- Claim: DCLS maintains parameter efficiency while improving performance
- Mechanism: By learning optimal kernel element positions, DCLS achieves better feature extraction without increasing parameter count compared to standard depthwise separable convolutions
- Core assumption: Parameter efficiency can be maintained while improving feature extraction through spatial optimization
- Evidence anchors:
  - [abstract]: "significantly improved the mean average precision (mAP) with the three architectures without increasing the number of parameters"
  - [section 4]: "a higher mAP (+0.6 on average) with an equal or lower number of parameters"
  - [corpus]: Weak - corpus papers mention DCLS parameter efficiency in vision but don't provide audio-specific evidence
- Break condition: If learned positions require additional parameters or if the interpolation overhead negates efficiency gains

### Mechanism 3
- Claim: DCLS reduces throughput by only 13-23% while providing significant accuracy gains
- Mechanism: The computational overhead of learning kernel positions and performing interpolation is outweighed by the improved feature extraction efficiency
- Core assumption: The accuracy gains from DCLS justify the modest computational overhead
- Evidence anchors:
  - [abstract]: "with only a low cost on the throughput" and "This significantly improved the mean average precision (mAP) with the three architectures without increasing the number of parameters and with only a low cost on the throughput"
  - [section 4]: "DCLS does, however, introduce a reduction in throughput (13% for ConvNeXt-T and FastVit-SA24 and 23% for ConvFormer-S18) due to the use of larger kernels"
  - [corpus]: Weak - corpus papers discuss DCLS in vision tasks but don't provide throughput comparisons for audio classification
- Break condition: If throughput reduction becomes prohibitive for real-time applications or if the accuracy gains don't justify the computational cost

## Foundational Learning

- Concept: Audio spectrograms and mel-frequency scaling
  - Why needed here: The paper processes audio as mel-spectrograms (128x1001) as input to vision-based models, requiring understanding of audio-to-image conversion
  - Quick check question: What transformation is applied to convert raw audio to the input format used by these models?

- Concept: Depthwise separable convolutions vs standard convolutions
  - Why needed here: The paper replaces depthwise separable convolutions (DSC) with DCLS, so understanding the difference is crucial
  - Quick check question: How does a depthwise separable convolution differ from a standard convolution in terms of parameter efficiency?

- Concept: Multi-label classification metrics
  - Why needed here: AudioSet is a multi-label classification task using mAP as the primary metric
  - Quick check question: Why is mAP (mean average precision) preferred over accuracy for multi-label classification tasks?

## Architecture Onboarding

- Component map: Input mel-spectrogram → Common stem (2x16 conv) → ConvNeXt/ConvFormer/FastViT stages → DCLS replacements for DSC layers → Classification head
- Critical path: The DCLS replacement occurs at specific DSC layers (kernel size 7), which are the primary feature extraction components
- Design tradeoffs: DCLS improves accuracy but reduces throughput; uses larger kernels (232 vs 72) with fewer elements (26 vs 49) to maintain parameter efficiency
- Failure signatures: If mAP doesn't improve despite DCLS replacement, or if throughput reduction exceeds the stated 13-23%, or if training becomes unstable
- First 3 experiments:
  1. Replace DSC layers with DCLS in ConvNeXt-tiny and measure mAP and throughput changes
  2. Test both bilinear and Gaussian interpolation versions of DCLS to verify the paper's finding that Gaussian performs better
  3. Vary spectrogram resolution to test the trade-off between mAP and throughput mentioned in section 3.1

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DCLS perform on audio classification tasks beyond AudioSet, such as environmental sound classification or speech emotion recognition?
- Basis in paper: [explicit] The authors state their results are specific to AudioSet and do not claim state-of-the-art performance on other audio classification tasks.
- Why unresolved: The study focuses solely on AudioSet, leaving the generalizability of DCLS to other audio classification domains unexplored.
- What evidence would resolve it: Evaluating DCLS on diverse audio classification benchmarks like ESC-50, UrbanSound8K, or IEMOCAP would demonstrate its broader applicability.

### Open Question 2
- Question: What is the impact of DCLS on model interpretability and feature visualization in audio classification?
- Basis in paper: [inferred] The paper does not discuss interpretability or feature visualization, despite DCLS learning kernel positions during training.
- Why unresolved: The authors focus on performance metrics but do not explore how learned kernel positions affect feature representations or model interpretability.
- What evidence would resolve it: Analyzing feature maps or visualizing learned kernel positions in DCLS layers could reveal insights into how the model processes audio data.

### Open Question 3
- Question: How does DCLS compare to other recent audio-specific architectures like AST or PaSST in terms of efficiency and accuracy?
- Basis in paper: [explicit] The authors compare DCLS-equipped models to ConvNeXt, ConvFormer, and FastViT baselines but do not benchmark against transformer-based architectures.
- Why unresolved: The study does not include comparisons with state-of-the-art transformer-based models, leaving questions about DCLS's relative performance.
- What evidence would resolve it: Benchmarking DCLS against transformer-based models like AST or PaSST on the same dataset would clarify its efficiency and accuracy trade-offs.

## Limitations

- The study only evaluates three architectures and one dataset (AudioSet), limiting generalizability
- No ablation studies on different kernel sizes or interpolation methods beyond brief mention
- The 13-23% throughput reduction is presented as "low cost" without benchmarking against alternative efficiency improvements

## Confidence

- **High confidence**: The core finding that DCLS can replace DSC in audio classification without increasing parameters is well-supported by the experimental results
- **Medium confidence**: The claim about mAP improvement (+0.6 average) is based on three models but lacks statistical significance testing or comparison to other state-of-the-art audio-specific architectures
- **Low confidence**: The assertion that 13-23% throughput reduction is "low cost" is subjective and not contextualized against application requirements or alternative approaches

## Next Checks

1. **Statistical validation**: Conduct t-tests comparing mAP improvements across the three architectures to determine if gains are statistically significant or driven by one particular model
2. **Architecture generalization**: Test DCLS on additional audio classification architectures (e.g., EfficientNet, ResNet variants) and datasets (ESC-50, UrbanSound8K) to verify generalizability beyond ConvNeXt/ConvFormer/FastViT
3. **Trade-off optimization**: Systematically vary kernel size, element count, and interpolation method to find the optimal balance between mAP improvement and throughput reduction for different deployment scenarios