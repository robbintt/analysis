---
ver: rpa2
title: Demonstration-Regularized RL
arxiv_id: '2310.17303'
source_url: https://arxiv.org/abs/2310.17303
tags:
- policy
- learning
- have
- then
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the theoretical benefits of using expert demonstrations
  to reduce the sample complexity of reinforcement learning (RL). The key idea is
  to first learn a behavior cloning policy from demonstrations, then perform regularized
  RL towards this policy.
---

# Demonstration-Regularized RL

## Quick Facts
- **arXiv ID**: 2310.17303
- **Source URL**: https://arxiv.org/abs/2310.17303
- **Reference count**: 40
- **Primary result**: Demonstration-regularized RL achieves sample complexity of order $\tilde{O}(\mathrm{Poly}(S,A,H)/(\varepsilon^2 N^{\mathrm{E}}))$ in finite MDPs and $\tilde{O}(\mathrm{Poly}(d,H)/(\varepsilon^2 N^{\mathrm{E}}))$ in linear MDPs, improving upon standard RL by a factor of $N^{\mathrm{E}}$.

## Executive Summary
This paper theoretically quantifies how expert demonstrations can reduce the sample complexity of reinforcement learning (RL). The key insight is that behavior cloning can learn a good initial policy from demonstrations, and then regularized RL toward this policy can efficiently identify the optimal policy. The approach is shown to improve sample complexity bounds in both finite and linear MDPs by a factor proportional to the number of demonstrations. The paper also applies this method to reinforcement learning from human feedback (RLHF), showing similar benefits without requiring pessimism injection.

## Method Summary
The demonstration-regularized RL approach consists of two main phases: (1) behavior cloning to learn a policy from expert demonstrations, and (2) regularized RL toward this policy. For finite MDPs, UCBVI-Ent+ is used with Hoeffding bonuses and a regularization-aware gap stopping rule. For linear MDPs, LSVI-UCB-Ent is employed with confidence intervals based on ridge regression. In the RLHF setting, preference data is collected using the behavior-cloned policy, rewards are estimated via maximum likelihood estimation, and regularized BPI is performed without pessimism injection.

## Key Results
- In finite MDPs, demonstration-regularized BPI achieves sample complexity of order $\tilde{O}(\mathrm{Poly}(S,A,H)/(\varepsilon^2 N^{\mathrm{E}}))$, improving standard RL bounds by a factor of $N^{\mathrm{E}}$.
- In linear MDPs, the approach achieves sample complexity of order $\tilde{O}(\mathrm{Poly}(d,H)/(\varepsilon^2 N^{\mathrm{E}}))$.
- For RLHF, the method avoids pessimism injection while achieving sample complexities comparable to RL with demonstrations.
- The paper provides tight convergence guarantees for behavior cloning under general policy classes, with KL divergence bounds scaling as $O((d_F H + A)/N_E)$.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KL-regularization toward a behavior cloning policy enables sample-efficient RL by compressing prior information into a tractable policy representation.
- Mechanism: The behavior cloning step learns a policy π_BC that approximates the expert policy π_E from demonstrations. The RL phase then solves a KL-regularized MDP with π_BC as a reference, which constrains exploration toward high-value regions and avoids inefficient exploration in the full state-action space.
- Core assumption: The KL divergence between π_E and π_BC is bounded and small enough for the regularization to be effective.
- Evidence anchors:
  - [abstract] "This paper quantifies theoretically to what extent this extra information reduces RL's sample complexity."
  - [section 4] "The main idea behind this method is to reduce BPI with demonstration to regularized BPI."
- Break condition: If KL(π_E || π_BC) is large, the regularization becomes ineffective and the sample complexity does not improve.

### Mechanism 2
- Claim: Behavior cloning with KL regularization provides tight convergence guarantees, ensuring π_BC is close to π_E in trajectory space.
- Mechanism: By optimizing a regularized negative log-likelihood objective, the behavior cloning policy π_BC minimizes KL divergence from π_E over the trajectory space, with explicit error bounds scaling as O((d_F H + A)/N_E).
- Core assumption: The policy class F is rich enough to contain a κ-greedy version of π_E (Assumption 2).
- Evidence anchors:
  - [abstract] "We provide tight convergence guarantees for the behaviour cloning procedure under general assumptions on the policy classes."
  - [section 3] "Theorem 1. Let Assumptions 1-2 be satisfied... KLtraj(π_E || π_BC) ≤ 6d_F H · log(Ae^3/(Aγ ∧ κ)) · log(2HN_E RF/(γδ)) / N_E + ..."
- Break condition: If the policy class F is too restrictive or N_E is too small, the convergence rate degrades.

### Mechanism 3
- Claim: Demonstration-regularized RLHF achieves sample complexity comparable to RL with demonstrations by avoiding pessimism injection and using efficient reward estimation.
- Mechanism: Offline preference collection using π_BC generates a dataset DRM. MLE estimation of rewards from DRM, combined with KL-regularization toward π_BC, avoids the need for pessimism that is typically required in offline RL with human feedback.
- Core assumption: The concentrability coefficient Cr(G, π_E, π_BC) is finite, ensuring the estimated reward is close to the true reward in expectation.
- Evidence anchors:
  - [abstract] "Interestingly, we avoid pessimism injection by employing computationally feasible regularization to handle reward estimation uncertainty."
  - [section 5] "We establish that demonstration-regularized methods are provably efficient for reinforcement learning from human feedback (RLHF)."
- Break condition: If the concentrability coefficient is large or the reward estimation error is high, the benefits of regularization diminish.

## Foundational Learning

- Concept: KL divergence and its role in policy regularization
  - Why needed here: KL regularization penalizes deviation from the behavior cloning policy, shaping exploration and improving sample efficiency.
  - Quick check question: How does KL divergence between two policies relate to their total variation distance?

- Concept: Behavior cloning and its convergence properties
  - Why needed here: Behavior cloning provides the initial policy π_BC that captures expert knowledge; its convergence guarantees ensure it is close to π_E.
  - Quick check question: What is the sample complexity of behavior cloning in terms of N_E and the policy class dimension d_F?

- Concept: Regularized MDPs and their Bellman equations
  - Why needed here: The RL phase solves a KL-regularized MDP, requiring understanding of regularized value functions and their Bellman optimality equations.
  - Quick check question: How do the Q-values and value functions change under KL regularization compared to the unregularized case?

## Architecture Onboarding

- Component map:
  - Expert demonstrations -> Behavior cloning module -> Policy π_BC
  - π_BC -> RL solver module (UCBVI-Ent+ or LSVI-UCB-Ent) -> Optimal policy
  - π_BC -> Preference dataset collection -> Reward estimation module -> Regularized BPI
  - Behavior cloning, RL solver, and reward estimation modules -> Concentration event module -> Statistical validity check

- Critical path:
  1. Collect demonstrations DE.
  2. Compute π_BC via behavior cloning.
  3. Collect trajectories (with rewards for RL, preferences for RLHF).
  4. Solve regularized MDP using RL solver.
  5. Return final policy π_RL.

- Design tradeoffs:
  - Regularization strength λ: Larger λ enforces closer adherence to π_BC but may reduce exploration.
  - Policy class richness: Richer classes improve behavior cloning but increase sample complexity.
  - Bonus functions: Hoeffding bonuses ensure optimism but add computational overhead.

- Failure signatures:
  - If KL(π_E || π_BC) is large, the regularization is ineffective.
  - If E_KL or E_cnt fails, concentration bounds do not hold.
  - If Cr(G, π_E, π_BC) is large, reward estimation error degrades performance.

- First 3 experiments:
  1. Verify behavior cloning convergence on a small finite MDP with known expert policy.
  2. Test regularized BPI on a tabular MDP with synthetic demonstrations.
  3. Validate RLHF pipeline on a preference-based task with known reward structure.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the sample complexity bounds for demonstration-regularized RLHF compare to other approaches that use pessimism injection to handle reward estimation uncertainty?
- Basis in paper: [explicit] The paper states that their demonstration-regularized RLHF method avoids pessimism injection by employing computationally feasible regularization to handle reward estimation uncertainty, achieving sample complexities similar to RL with demonstrations.
- Why unresolved: The paper does not provide a direct comparison of sample complexity bounds with pessimism-based approaches.
- What evidence would resolve it: A detailed analysis comparing the sample complexity bounds of the proposed method with those of pessimism-based methods under the same assumptions.

### Open Question 2
- Question: Can the demonstration-regularized RL approach be extended to settings with non-tabular or non-linear MDPs beyond the finite and linear cases considered in the paper?
- Basis in paper: [inferred] The paper focuses on finite and linear MDPs, but the general framework of demonstration-regularized RL could potentially be applied to other settings.
- Why unresolved: The paper does not explore the applicability of the approach to other types of MDPs.
- What evidence would resolve it: Empirical or theoretical results showing the performance of demonstration-regularized RL in non-tabular or non-linear MDPs.

### Open Question 3
- Question: How does the choice of the regularization parameter λ affect the performance of the demonstration-regularized RL algorithms, and is there an optimal way to select it?
- Basis in paper: [explicit] The paper discusses the choice of the regularization parameter λ in the context of the UCBVI-Ent+ and LSVI-UCB-Ent algorithms, but does not provide a detailed analysis of its impact on performance.
- Why unresolved: The paper does not explore the sensitivity of the algorithms to the choice of λ or provide guidelines for selecting it.
- What evidence would resolve it: Empirical results showing the performance of the algorithms for different values of λ, or theoretical analysis providing insights into the optimal choice of λ.

## Limitations

- The analysis assumes access to expert demonstrations and a sufficiently rich policy class for behavior cloning, which may not hold in practice.
- The concentrability coefficient Cr(G, π_E, π_BC) plays a critical role in RLHF performance but can be difficult to verify or bound in real applications.
- The paper focuses on best policy identification rather than regret minimization, limiting applicability to online learning scenarios.

## Confidence

- **High confidence**: The theoretical framework for behavior cloning convergence (Theorem 1) and its application to finite MDP BPI (Theorem 5) are mathematically rigorous and well-supported.
- **Medium confidence**: The linear MDP analysis (Theorem 6) extends the finite case but requires additional technical assumptions about feature spaces and policy classes.
- **Medium confidence**: The RLHF results (Theorem 8) provide novel insights but rely on strong assumptions about preference models and concentrability.

## Next Checks

1. Implement behavior cloning on a simple gridworld with synthetic expert demonstrations to empirically verify convergence rates match theoretical predictions.
2. Test regularized BPI on a finite MDP with varying numbers of demonstrations (N_E) to measure the empirical impact on sample complexity.
3. Validate the RLHF pipeline on a preference-based task where the true reward structure is known, comparing performance with and without demonstration regularization.