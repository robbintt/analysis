---
ver: rpa2
title: 'Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from
  Human Feedback'
arxiv_id: '2310.05199'
source_url: https://arxiv.org/abs/2310.05199
tags:
- reward
- human
- length
- learning
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies a length bias in reward models during Reinforcement
  Learning from Human Feedback, where longer responses are often preferred regardless
  of actual helpfulness. To address this, the authors propose a Product-of-Experts
  (PoE) approach that separates the reward model into two experts: one for learning
  human intent and another for capturing length bias.'
---

# Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback

## Quick Facts
- arXiv ID: 2310.05199
- Source URL: https://arxiv.org/abs/2310.05199
- Reference count: 15
- Key outcome: Proposed Product-of-Experts approach improves reward scores while reducing output length, achieving >50% win rate over traditional RLHF methods

## Executive Summary
This paper addresses a critical issue in Reinforcement Learning from Human Feedback (RLHF): length bias in reward models, where longer responses are often preferred regardless of actual helpfulness. The authors propose a Product-of-Experts (PoE) framework that separates human intent learning from length bias capture using two specialized experts. The main expert focuses on understanding human preferences while the bias-only expert targets length bias identification, enhanced by noise injection to disrupt semantic information. Experimental results demonstrate improved performance across various metrics while producing more concise outputs.

## Method Summary
The proposed approach uses a Product-of-Experts framework with two reward models: a main expert (larger model, normal learning rate) for learning human intent and a bias-only expert (smaller model, higher learning rate with noise injection) for capturing length bias. Both experts are jointly optimized during training using human preference data, but only the main expert is used during inference and RL optimization. The noise injection into the bias-only expert's token embeddings disrupts semantic information flow, forcing the model to rely on length-based patterns rather than semantic content.

## Key Results
- Win rate over 50% compared to traditional RLHF methods across multiple datasets
- Improved reward scores while producing significantly shorter responses
- Better discrimination between chosen and rejected responses in human preference evaluation
- Consistent performance improvement irrespective of sequence length

## Why This Works (Mechanism)

### Mechanism 1
The PoE framework separates human intent learning from length bias by using two experts with different model sizes and learning rates. The main expert (larger model, normal learning rate) focuses on learning human intent while the bias-only expert (smaller model, higher learning rate) learns length bias. The noise injection disrupts semantic information in the bias-only expert, forcing it to rely on length signals.

### Mechanism 2
The PoE combination allows the main expert to learn human intent without being influenced by length bias. During training, both experts are jointly optimized, but during inference only the main expert is used. The bias-only expert "absorbs" the length bias patterns, allowing the main expert to focus on genuine human preferences.

### Mechanism 3
Noise injection into the bias-only expert enhances its ability to capture length bias by disrupting semantic information flow. Adding Gaussian noise to token embeddings forces the bias-only expert to rely on length-based patterns rather than semantic content, making it more sensitive to length variations.

## Foundational Learning

- Concept: Product-of-Experts (PoE) technique
  - Why needed here: Provides a framework for combining multiple experts with different specializations to create a more robust reward model
  - Quick check question: How does PoE differ from traditional ensemble methods in terms of how experts are combined?

- Concept: Causal analysis in reward modeling
  - Why needed here: Helps understand how length bias acts as a confounding factor in reward model training
  - Quick check question: In the causal structure presented, what role does length bias play as a confounding factor?

- Concept: Reinforcement Learning from Human Feedback (RLHF) pipeline
  - Why needed here: Provides context for understanding how reward models are used in the larger alignment process
  - Quick check question: What are the three main phases of the RLHF pipeline and how does the reward model fit into this process?

## Architecture Onboarding

- Component map: Input → Noise Injection → Both Experts → PoE Combination → Main Expert Output → RL Optimization
- Critical path: Input → Noise Injection → Both Experts → PoE Combination → Main Expert Output → RL Optimization
- Design tradeoffs:
  - Model size vs. learning rate: Larger models with normal rates for main expert, smaller models with higher rates for bias-only expert
  - Noise level: Must be sufficient to disrupt semantics but not prevent learning
  - Training complexity: Joint optimization vs. separate training of experts
- Failure signatures:
  - Main expert still learning length bias: Check if bias-only expert is capturing length patterns effectively
  - Bias-only expert not learning: May need to adjust noise level or learning rate
  - Overall performance degradation: May indicate poor PoE combination or insufficient capacity
- First 3 experiments:
  1. Train with both experts but no noise injection - observe if length bias persists
  2. Train with noise injection but only bias-only expert - verify it learns length patterns
  3. Train full system and compare length distributions of generated responses vs. baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which the noise injection in the bias-only expert disrupts semantic information flow?
- Basis in paper: [explicit] The paper states "we employ a technique of injecting random noise into the input. This intentional introduction of noise serves the purpose of disrupting the semantic information within the input"
- Why unresolved: The paper mentions using Gaussian noise on token embeddings but doesn't provide empirical analysis of how this affects semantic representation or validation that it successfully disrupts semantic information while preserving length information.
- What evidence would resolve it: Ablation studies showing performance with different noise levels/types, visualization of embedding space before/after noise injection, or analysis of attention patterns in the bias-only expert.

### Open Question 2
- Question: How does the effectiveness of the PoE approach scale with different model sizes for the bias-only expert?
- Basis in paper: [explicit] The paper mentions using a 560M BLOOMZ model for the bias-only expert and provides a scaling law table comparing different sizes
- Why unresolved: While the paper provides scaling results, it doesn't explain the optimal size ratio between main expert and bias-only expert, or whether there's a point of diminishing returns.
- What evidence would resolve it: Systematic study of performance vs. model size ratios, computational efficiency analysis, or theoretical justification for the 1/12.5 size ratio used.

### Open Question 3
- Question: Does the length bias phenomenon generalize beyond the HH dataset to other RLHF datasets or different domains?
- Basis in paper: [inferred] The paper acknowledges "the question of whether this phenomenon exists on larger datasets remains uncertain" and notes that "collecting RLHF data is a challenging task"
- Why unresolved: The paper only validates the method on the HH dataset and rm-static, without testing on other datasets or domains.
- What evidence would resolve it: Experiments on diverse RLHF datasets (e.g., from different domains like summarization, code generation, or multi-modal tasks), or analysis of length bias in zero-shot/few-shot scenarios.

## Limitations
- Limited validation only on HH dataset and rm-static without testing on diverse RLHF datasets
- Unclear whether the specific model size ratio (7B main expert, 560M bias-only expert) is optimal or necessary
- Limited analysis of failure modes when length bias is artificially severe or when noise injection is too aggressive

## Confidence
- **High Confidence**: The identification of length bias as a problem in RLHF reward models is well-established in the literature. The general PoE framework is also a proven technique in other domains.
- **Medium Confidence**: The specific implementation details (model sizes, learning rates, noise injection) are based on empirical observations but lack systematic validation. The claim that the bias-only expert successfully captures length bias while the main expert learns human intent needs more rigorous testing.
- **Low Confidence**: The exact mechanism by which noise injection helps the bias-only expert learn length bias is not fully explained, and the potential failure modes of this approach are not thoroughly explored.

## Next Checks
1. **Ablation Study**: Remove the noise injection from the bias-only expert and compare the performance to the full system. This would validate whether noise injection is truly necessary for capturing length bias effectively.

2. **Expert Specialization Analysis**: Train the system with different model size combinations (e.g., 3B main expert with 1B bias-only expert, or equal-sized experts) to determine if the current configuration is optimal or if the approach is more flexible than presented.

3. **Failure Mode Testing**: Deliberately induce scenarios where length bias is severe (e.g., using artificially length-skewed preference data) and test whether the PoE approach maintains its effectiveness or breaks down under extreme conditions.