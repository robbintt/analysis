---
ver: rpa2
title: 'WebWISE: Web Interface Control and Sequential Exploration with Large Language
  Models'
arxiv_id: '2310.16042'
source_url: https://arxiv.org/abs/2310.16042
tags:
- tasks
- task
- elements
- language
- example
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores using a Large Language Model (LLM) to automatically
  perform web software tasks using click, scroll, and text input operations. Previous
  approaches, such as reinforcement learning (RL) or imitation learning, are inefficient
  to train and task-specific.
---

# WebWISE: Web Interface Control and Sequential Exploration with Large Language Models

## Quick Facts
- **arXiv ID**: 2310.16042
- **Source URL**: https://arxiv.org/abs/2310.16042
- **Reference count**: 40
- **Primary result**: WebWISE achieves similar or better performance than RL and imitation learning baselines using only one in-context example.

## Executive Summary
This paper introduces WebWISE, a method that uses Large Language Models (LLMs) to automate web software tasks through click, scroll, and text input operations. The approach generates executable Python code step-by-step based on filtered Document Object Model (DOM) elements and natural language instructions, avoiding the inefficiency of training task-specific models. By using in-context learning with either manually provided or auto-generated examples, WebWISE demonstrates competitive performance on the MiniWob++ benchmark while requiring minimal human effort for demonstration collection.

## Method Summary
WebWISE uses a multi-step approach where an LLM (gpt-3.5-turbo) generates and executes programs incrementally based on filtered DOM elements and task descriptions. The method employs a DOM filtering function to retain only useful tags and classes, reducing irrelevant visual noise. Programs are generated through in-context learning using either manual examples or auto-generated examples from successful zero-shot trials. The system executes each generated program via Selenium API, updates DOM elements after each action, and repeats until task completion or failure is detected. This contrasts with single-step generation approaches and demonstrates improved performance across various web task categories.

## Key Results
- WebWISE with one in-context example achieves similar or better performance than RL and imitation learning baselines requiring many demonstrations
- Sequential, step-by-step action generation significantly outperforms one-shot program generation for web tasks
- Auto-context generation from successful zero-shot trials provides effective in-context examples without manual programming effort

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Filtered DOM elements improve LLM task performance by reducing irrelevant visual noise
- Mechanism: The LLM generates more focused programs when only pertinent tags and classes are presented, avoiding distraction from extraneous DOM content
- Core assumption: The subset of DOM elements containing relevant tags is sufficient to describe the task state for program generation
- Evidence anchors: Figure 5d shows filtering improves success rates for most tasks; filtering function returns subset of DOM elements from predefined tags/classes

### Mechanism 2
- Claim: Sequential, step-by-step action generation outperforms one-shot program generation for web tasks
- Mechanism: At each iteration, the LLM can observe the environment after its last action and adjust subsequent actions accordingly, mimicking human problem-solving
- Core assumption: Web tasks often require sequences of dependent actions where each step depends on the previous outcome
- Evidence anchors: Table 1 and Figure 5b show WebWISE significantly enhances performance compared to single-shot generation; method generates and executes programs incrementally

### Mechanism 3
- Claim: Auto-generated in-context examples from successful zero-shot trials boost performance without manual programming effort
- Mechanism: The LLM's own successful programs are reused as demonstrations, providing tailored examples for the specific task environment
- Core assumption: Programs generated successfully in zero-shot trials are representative and reusable as in-context examples for the same task
- Evidence anchors: Zero-shot trials preserve successful programs as in-context examples; auto-context examples increase success rates compared to no examples

## Foundational Learning

- Concept: Large Language Models (LLMs) as program generators
  - Why needed here: The core idea is to use an LLM to produce executable Python code that controls a web interface, rather than training a separate model for this task
  - Quick check question: What API functions must the generated code call to interact with the MiniWob++ environment?

- Concept: In-context learning
  - Why needed here: The approach relies on providing the LLM with a small number of task demonstrations (examples) in the prompt to guide its generation, instead of fine-tuning
  - Quick check question: How many manual in-context examples are provided to the LLM in the single-shot setup?

- Concept: Document Object Model (DOM) structure
  - Why needed here: DOM elements represent the visual state of the web page; filtering them shapes the LLM's perception of the task environment
  - Quick check question: Which HTML tags are considered "useful" and retained by the filtering function?

## Architecture Onboarding

- Component map: Task description → LLM prompt → Filtered DOM elements → Program generator → Execution engine → Success indicator → Updated DOM → Feedback loop

- Critical path:
  1. Receive task description and initial DOM
  2. Filter DOM to retain useful elements
  3. Build LLM prompt (API, task, optional examples)
  4. Generate Python program
  5. Execute program via API
  6. Check success; if not solved, repeat from step 2 with updated DOM

- Design tradeoffs:
  - Filtered DOM vs. full DOM: Simpler input but risk missing needed info
  - Step-by-step vs. one-shot: More adaptive but slower
  - Auto-context vs. manual examples: Lower setup cost but potentially weaker guidance

- Failure signatures:
  - LLM generates non-executable code → Need stricter parsing or error handling
  - Success rate drops on tasks with complex sequences → May need longer context or additional examples
  - Auto-context fails to find examples → May need more zero-shot trials or fallback to manual examples

- First 3 experiments:
  1. Run WebWISE on a single-step task (e.g., "click-button") to verify basic functionality
  2. Compare performance of filtered vs. full DOM on a multi-step task (e.g., "click-tab")
  3. Test auto-context generation by running 10 zero-shot trials on a task with moderate difficulty and measuring example quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice between DOM elements, HTML, and RGB values impact performance for different types of web tasks?
- Basis in paper: The paper discusses using DOM elements as observations but mentions that in situations without direct DOM access, pre-trained models like Pix2Struct could derive structured representations from screenshots
- Why unresolved: The paper only evaluates DOM elements and doesn't compare against HTML or RGB-based approaches across different task types
- What evidence would resolve it: Comparative experiments measuring success rates using DOM elements, HTML, and RGB values across various web task categories

### Open Question 2
- Question: What is the optimal strategy for filtering DOM elements to balance between including relevant information and avoiding noise?
- Basis in paper: The paper mentions using a simple filtering function that returns a subset of DOM elements based on predefined tags and classes, but notes that some tasks perform better with full DOM elements
- Why unresolved: The current filtering approach is manually defined and doesn't adapt to different tasks, leading to performance trade-offs
- What evidence would resolve it: Development and testing of adaptive DOM filtering methods that learn which elements to include based on task type or performance feedback

### Open Question 3
- Question: How sensitive is the system to variations in prompt engineering, and can this sensitivity be mitigated?
- Basis in paper: The paper notes that the LLM exhibits sensitivity to prompts, including system messages and task messages, with minor differences in sentence structure affecting performance
- Why unresolved: While the paper conducts some experiments with different task messages, it doesn't explore comprehensive prompt optimization or methods to make the system more robust to prompt variations
- What evidence would resolve it: Systematic testing of different prompt formulations and development of prompt-robust approaches that maintain consistent performance across variations

### Open Question 4
- Question: How can the system be extended to handle more complex tasks like booking airline tickets that require long action sequences and memory of past interactions?
- Basis in paper: The paper mentions this as a future direction, noting current limitations include lack of explicit memory and experiments being limited to simple tasks
- Why unresolved: The current approach generates actions step-by-step without retaining memory of past interactions, making it unsuitable for complex multi-session tasks
- What evidence would resolve it: Implementation of memory mechanisms and testing on increasingly complex tasks that require maintaining state across multiple sessions or web pages

### Open Question 5
- Question: What primitive functions are needed to handle keyboard interactions like ENTER, BACKSPACE, or DELETE in terminal-based tasks?
- Basis in paper: The paper identifies that tasks involving terminals often fail because the model lacks knowledge of how to execute commands by pressing the enter key, and suggests developing primitive functions for key actions
- Why unresolved: The current API only includes basic click, scroll, and text entry functions without specific keyboard action primitives
- What evidence would resolve it: Development and testing of a comprehensive keyboard interaction API and measuring performance improvements on terminal and other keyboard-intensive tasks

## Limitations

- The DOM filtering function may exclude necessary information for some tasks, particularly those with order-dependent elements like search results
- The approach is limited to simple web tasks and lacks explicit memory mechanisms needed for complex multi-step tasks
- Success depends on the LLM's zero-shot performance, which may vary across different environments or task types

## Confidence

- **High Confidence**: Sequential execution significantly improves performance over one-shot generation
- **Medium Confidence**: Filtered DOM elements provide meaningful performance benefits
- **Medium Confidence**: Auto-context generation from zero-shot trials provides useful in-context examples

## Next Checks

1. Test WebWISE on tasks where DOM elements are dynamically added or removed during task execution to assess whether the filtering mechanism can handle real-time interface changes

2. Evaluate the approach on a different web automation benchmark or real website to determine if the success of filtered DOM and sequential execution generalizes beyond the MiniWob++ environment

3. Implement and test explicit error handling and recovery mechanisms in the generated programs, measuring whether this improves success rates on tasks where initial attempts fail due to timing issues or unexpected DOM states