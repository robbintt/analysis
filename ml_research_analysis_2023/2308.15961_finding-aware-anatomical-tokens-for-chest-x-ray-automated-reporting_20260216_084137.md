---
ver: rpa2
title: Finding-Aware Anatomical Tokens for Chest X-Ray Automated Reporting
arxiv_id: '2308.15961'
source_url: https://arxiv.org/abs/2308.15961
tags:
- anatomical
- tokens
- report
- finding
- anatomy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces finding-aware anatomical tokens for chest
  X-ray (CXR) automated reporting, addressing the challenge of improving clinical
  accuracy in generated radiology reports. The method employs a novel multi-task Faster
  R-CNN that jointly performs anatomy localisation and finding detection to extract
  informative anatomical tokens, which are then integrated into a multimodal transformer-based
  report generation pipeline.
---

# Finding-Aware Anatomical Tokens for Chest X-Ray Automated Reporting

## Quick Facts
- arXiv ID: 2308.15961
- Source URL: https://arxiv.org/abs/2308.15961
- Reference count: 40
- Primary result: State-of-the-art CXR report generation with F1 score of 0.496 using finding-aware anatomical tokens

## Executive Summary
This work introduces finding-aware anatomical tokens to improve clinical accuracy in chest X-ray (CXR) automated reporting. The method employs a novel multi-task Faster R-CNN that jointly performs anatomy localization and finding detection to extract informative anatomical tokens, which are then integrated into a multimodal transformer-based report generation pipeline. Experiments on the MIMIC-CXR dataset demonstrate that using these task-aware anatomical tokens yields state-of-the-art performance, with generated reports showing improved clinical accuracy while maintaining high fluency compared to image-level features.

## Method Summary
The approach uses a two-stage pipeline adapted to perform triples extraction and report generation. First, a multi-task Faster R-CNN is trained on the Chest ImaGenome dataset to jointly perform anatomy localization and finding detection, producing finding-aware anatomical tokens as 1024-dimensional embeddings. These tokens are then integrated into a multimodal transformer that attends to both visual tokens and text input (indication field) to generate structured triples, which are subsequently used to generate fluent, clinically coherent radiology reports.

## Key Results
- Finding-aware anatomical tokens achieve state-of-the-art performance with clinical F1 score of 0.496
- Generated reports show improved clinical accuracy compared to image-level CNN features
- Maintains high fluency with BLEU-4 score of 0.237
- Outperforms existing methods by providing richer, finding-specific visual representations

## Why This Works (Mechanism)

### Mechanism 1
Anatomical tokens trained on both localization and finding detection capture richer visual context than image-level CNN features. The multi-task Faster R-CNN architecture performs anatomy localization and finding detection in parallel, producing 1024-dimensional embeddings that encode both spatial anatomy structure and associated pathology information. This works because anatomical structures in CXRs have strong correlation between their visual appearance and the clinical findings they contain.

### Mechanism 2
The two-stage pipeline (triples extraction + report generation) improves clinical accuracy by structuring intermediate representation. First stage extracts structured triples from CXR+indication using a Transformer encoder-decoder. Second stage uses these triples as context for generating fluent, clinically coherent reports, providing explicit grounding that reduces hallucination. This works because radiology reports follow structured patterns where findings, locations, and interpretations can be captured as triples before free-text generation.

### Mechanism 3
Training on Chest ImaGenome with extensive anatomy-finding annotations provides superior supervision compared to generic ImageNet pre-training. The anatomy-finding Faster R-CNN is trained on 242,072 CXR images with 36 anatomical regions and 71 findings, creating specialized visual representations that understand radiological domain concepts. This works because domain-specific supervision on medical imaging data produces better feature representations for medical tasks than general computer vision pre-training.

## Foundational Learning

- **Object detection and region proposal networks**: Why needed here: The Faster R-CNN architecture forms the backbone for extracting anatomical tokens, requiring understanding of how RPN generates proposals and how RoI pooling extracts features. Quick check: What is the role of the Region Proposal Network in Faster R-CNN and how does it differ from the classification head?

- **Multimodal Transformers and cross-attention**: Why needed here: The report generation pipeline uses a Transformer that attends to both visual tokens and text input, requiring understanding of how cross-modal attention works. Quick check: How does the segment embedding in the multimodal Transformer help discriminate between visual and textual input?

- **Clinical radiology report structure and terminology**: Why needed here: Understanding what constitutes clinically accurate reporting (findings, locations, interpretations) is essential for evaluating the model and interpreting results. Quick check: What are the typical sections found in a radiology report and how do they relate to the triples representation used in this work?

## Architecture Onboarding

- **Component map**: CXR image → Faster R-CNN (ResNet50 + FPN + RPN + RoI Pool) → Finding-aware anatomical tokens → Multimodal Transformer (encoder-decoder) → Radiology report. The pipeline has two stages: triples extraction (TE) then report generation (RG).
- **Critical path**: CXR → anatomical tokens → TE → triples → RG → final report. Each stage depends on the previous one, with the anatomical tokens being the key innovation.
- **Design tradeoffs**: Anatomical tokens vs. image-level features (local vs. global representation), multi-task training vs. single-task (complexity vs. performance), two-stage pipeline vs. end-to-end (control vs. simplicity).
- **Failure signatures**: Poor mAP@0.5 in anatomy localization indicates Faster R-CNN isn't detecting structures properly; low AUROC in finding detection means tokens lack pathological information; poor BLEU/METEOR scores indicate generation quality issues; low clinical F1 indicates factual errors.
- **First 3 experiments**:
  1. Train anatomy-only Faster R-CNN on Chest ImaGenome and evaluate mAP@0.5 to establish baseline localization performance
  2. Add finding detection head and evaluate AUROC scores to measure improvement in pathological information capture
  3. Integrate anatomical tokens into the TE+RG pipeline and compare against ResNet-101 baseline using clinical F1 score

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the proposed Faster R-CNN change when adjusting the weighting hyperparameter λ in the multi-task loss? The paper states that "the trade-off between anatomy localisation and finding detection performance can be tuned by adjusting the weighting hyperparameter λ in the multi-task loss," but does not provide specific values or ranges for λ that optimize performance, nor does it discuss the sensitivity of the model to changes in λ.

### Open Question 2
How do the extracted finding-aware anatomical tokens compare in terms of computational efficiency and resource usage to traditional image-level features? The paper introduces finding-aware anatomical tokens as a novel approach to improve the quality of generated reports but does not discuss the computational cost or resource requirements of this method compared to traditional image-level features.

### Open Question 3
Can the proposed method be generalized to other medical imaging modalities beyond chest X-rays? The paper demonstrates the effectiveness of finding-aware anatomical tokens for chest X-ray report generation but does not explore the applicability of the method to other types of medical images.

## Limitations
- Evaluation relies solely on the MIMIC-CXR dataset, limiting generalization to different healthcare settings
- Clinical validation is limited to automated CheXbert labeller evaluation without direct physician review
- Approach depends on quality and completeness of Chest ImaGenome annotations
- Two-stage pipeline with multi-task Faster R-CNN requires significant computational resources

## Confidence

**High Confidence**: The architectural components (Faster R-CNN for object detection, multimodal Transformers for generation) are well-established in the literature, and the implementation details follow standard practices.

**Medium Confidence**: The claim that finding-aware anatomical tokens outperform image-level features is supported by the experimental results (F1 score improvement from 0.437 to 0.496), but the comparison is limited to specific baseline methods on one dataset.

**Medium Confidence**: The mechanism explaining how anatomical tokens improve clinical accuracy is theoretically sound and partially supported by the results, but the causal relationship between token quality and report accuracy could benefit from additional ablation studies.

**Low Confidence**: The assertion that domain-specific supervision on Chest ImaGenome provides superior performance compared to general ImageNet pre-training is stated but not directly tested in the paper.

## Next Checks

1. **External Dataset Validation**: Evaluate the model on an independent chest X-ray dataset (e.g., ChestX-ray14 or NIH Clinical Center) to assess generalization beyond MIMIC-CXR.

2. **Clinical Expert Review**: Conduct a blinded study where radiologists compare reports generated by the proposed method against those from baseline methods and human-written reports to assess clinical utility.

3. **Ablation Study on Token Quality**: Systematically vary the number and quality of anatomical tokens (e.g., using anatomy-only tokens vs. finding-aware tokens) to establish a clearer causal link between token characteristics and report accuracy.