---
ver: rpa2
title: Multimodal Federated Learning with Missing Modality via Prototype Mask and
  Contrast
arxiv_id: '2312.13508'
source_url: https://arxiv.org/abs/2312.13508
tags:
- missing
- modality
- training
- learning
- prototypes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of multimodal federated learning
  when data on clients have missing modalities. The proposed method, PmcmFL, introduces
  a prototype library to compensate for missing modalities during training and inference,
  using prototypes as masks to form task-calibrated training loss and a model-agnostic
  uni-modality inference strategy.
---

# Multimodal Federated Learning with Missing Modality via Prototype Mask and Contrast

## Quick Facts
- arXiv ID: 2312.13508
- Source URL: https://arxiv.org/abs/2312.13508
- Reference count: 40
- Primary result: Achieves 3.7% accuracy improvement with 50% modality missing during training and 23.8% improvement during uni-modality inference on tiny VQA task

## Executive Summary
This paper addresses the challenge of multimodal federated learning when clients have missing modalities in their local data. The proposed PmcmFL framework introduces a prototype library to compensate for missing modalities during training and inference. By using prototypes as masks to represent missing modalities, the method formulates a task-calibrated training loss and a model-agnostic uni-modality inference strategy. Additionally, a proximal contrastive loss based on prototypes is constructed to enhance local training by regularizing representation learning toward a unified feature space.

## Method Summary
PmcmFL operates by maintaining a global prototype library containing class prototypes for each modality and fused representations. During training, when a modality is missing, the model substitutes the missing modality's representation with its corresponding prototype from the library. The framework employs a Multiway Transformer encoder that can switch between different encoding modes depending on modality availability. A task-calibrated loss supervises training with prototype masks, while a proximal contrastive loss regularizes representations toward class prototypes. The federated learning loop involves clients receiving the global model and prototype library, performing local training with prototype masks and contrastive loss, computing local prototypes, and sending updates back to the server for aggregation.

## Key Results
- PmcmFL achieves 3.7% accuracy improvement with 50% modality missing during training compared to baselines
- PmcmFL shows 23.8% improvement during uni-modality inference compared to baseline methods
- The method demonstrates effectiveness in handling non-IID data distributions while maintaining performance with missing modalities

## Why This Works (Mechanism)

### Mechanism 1
- Prototypes act as masks to compensate for missing modalities during training and inference by substituting missing modality representations with corresponding global prototypes, allowing continued training with complete modality pairs.
- Core assumption: Prototypes can adequately approximate the semantic content of missing modalities for both training supervision and inference.
- Evidence anchors: Abstract statement about prototypes as masks, section 3.3 description of prototype employment, weak corpus evidence.
- Break condition: Prototypes fail to capture modality-specific semantic information, leading to degraded performance.

### Mechanism 2
- Prototype-based contrastive loss regularizes client training by pulling representations toward class prototypes while pushing them away from other class prototypes, creating a unified feature space across clients.
- Core assumption: Prototypes serve as effective global representation targets that can regularize diverse client models toward a common semantic space.
- Evidence anchors: Section 3.4 description of prototype use as representation targets, section 4.5 discussion of client drift reduction, weak corpus evidence.
- Break condition: Non-IID data distributions are too severe for prototypes to serve as meaningful global targets, causing optimization conflicts.

### Mechanism 3
- Multiway Transformer architecture enables flexible handling of missing modalities by switching between modality-specific encoding, interactive encoding, and prototype-based encoding modes.
- Core assumption: The Multiway Transformer's modular design allows seamless switching between encoding modes without architectural conflicts.
- Evidence anchors: Section 3.1 description of multiway encoder handling different missing patterns, section 3.1 explanation of interactive encoding, weak corpus evidence.
- Break condition: The encoder architecture cannot properly handle the transition between modes, causing gradient conflicts or representation mismatches.

## Foundational Learning

- Concept: Federated Learning (FL)
  - Why needed here: The entire framework operates on distributed clients that collaboratively train a global model without sharing private data.
  - Quick check question: What is the key difference between centralized training and federated learning in terms of data privacy and communication patterns?

- Concept: Prototype Learning
  - Why needed here: Prototypes serve as global semantic anchors that compensate for missing modalities and regularize client training.
  - Quick check question: How do prototypes differ from traditional class centroids, and why are they particularly useful in non-IID federated learning scenarios?

- Concept: Contrastive Learning
  - Why needed here: The proximal contrastive loss pulls representations toward class prototypes while pushing them away from other class prototypes, creating a unified feature space.
  - Quick check question: What is the mathematical relationship between contrastive loss and the distance between representations and prototypes in the feature space?

## Architecture Onboarding

- Component map: Multiway Transformer encoder (E) -> Deep fusion layer (F) -> Task head (G) -> Prototype library -> Task-calibrated loss -> Proximal contrastive loss

- Critical path: Client receives global model and prototype library → Performs local training with prototype masks and contrastive loss → Computes local prototypes → Sends updated model and prototypes to server → Server aggregates models and prototypes → Broadcast updated global model and prototype library

- Design tradeoffs:
  - Communication overhead vs. performance: Prototype library adds overhead but significantly improves performance
  - Model complexity vs. flexibility: Multiway Transformer enables modality switching but increases architectural complexity
  - Prototype quality vs. computation: Higher-quality prototypes require more computation but provide better semantic compensation

- Failure signatures:
  - Performance degrades with increasing modality missing rates despite prototype usage
  - Client drift persists despite contrastive regularization
  - Prototype matching accuracy remains low during inference with missing modalities
  - Communication overhead becomes prohibitive with large prototype libraries

- First 3 experiments:
  1. Verify prototype mask effectiveness: Compare training with prototype masks vs. zero masks under various missing rates
  2. Test contrastive regularization: Compare models with and without proximal contrastive loss on non-IID data
  3. Validate prototype matching: Test different prototype matching strategies (L1, L2, cosine, classifier, retrieval) during inference with missing modalities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PmcmFL change with different choices of distance metrics for prototype matching in the model-free approach?
- Basis in paper: Explicit mention of L1 distance, L2 distance, and cosine similarity performance in Table 3.
- Why unresolved: The paper doesn't explore other potential distance metrics or analyze the impact of metric choice on performance.
- What evidence would resolve it: An experiment comparing PmcmFL performance using various distance metrics (e.g., Mahalanobis, Earth Mover's Distance) and analyzing which metric works best for different types of missing modality scenarios.

### Open Question 2
- Question: What is the impact of using different prototype construction strategies beyond class centroids on PmcmFL's performance?
- Basis in paper: Inferred from paper using class centroids without exploring alternatives like class representatives, class modes, or learned prototypes.
- Why unresolved: The choice of prototype construction method could significantly affect the quality of prototypes as masks and the effectiveness of the prototype contrast loss.
- What evidence would resolve it: An ablation study comparing PmcmFL performance using different prototype construction methods while keeping other components constant.

### Open Question 3
- Question: How does PmcmFL perform when applied to other multimodal tasks beyond VQA, such as image captioning or visual grounding?
- Basis in paper: Inferred from paper demonstrating PmcmFL on VQA but not validating its effectiveness on other multimodal tasks.
- Why unresolved: The effectiveness of PmcmFL's prototype-based approach may vary depending on the nature of the multimodal task and the types of modality interactions required.
- What evidence would resolve it: Applying PmcmFL to other multimodal benchmarks and comparing its performance against task-specific state-of-the-art methods.

## Limitations
- Performance scaling concerns with larger prototype libraries for datasets with thousands of classes
- Unspecified prototype initialization strategy that could significantly impact convergence
- Assumption that semantic content can be adequately approximated through matching may not hold for complex or highly abstract modalities

## Confidence
- High Confidence: The prototype mask mechanism for training compensation
- Medium Confidence: The proximal contrastive loss effectiveness
- Medium Confidence: Prototype matching during inference

## Next Checks
1. Systematically evaluate the impact of different prototype initialization strategies (zero, random, pre-trained) on model performance across varying modality missing rates
2. Test PmcmFL on larger multimodal datasets (e.g., standard VQA, MM-IMDB) to evaluate scalability
3. Quantify the exact communication overhead introduced by the prototype library and evaluate compression strategies