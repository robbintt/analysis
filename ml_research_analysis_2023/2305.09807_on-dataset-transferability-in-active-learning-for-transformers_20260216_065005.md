---
ver: rpa2
title: On Dataset Transferability in Active Learning for Transformers
arxiv_id: '2305.09807'
source_url: https://arxiv.org/abs/2305.09807
tags:
- dataset
- entropy
- badge
- core-set
- instances
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether actively acquired datasets in text
  classification can be successfully transferred between different transformer-based
  pre-trained language models (PLMs). The authors hypothesize that transferability
  depends on the similarity of instances queried by the acquisition and consumer models
  during active learning.
---

# On Dataset Transferability in Active Learning for Transformers

## Quick Facts
- arXiv ID: 2305.09807
- Source URL: https://arxiv.org/abs/2305.09807
- Reference count: 40
- Primary result: AL methods combining uncertainty and diversity (BADGE) produce more transferable datasets across transformer models by minimizing acquisition sequence mismatch

## Executive Summary
This study investigates whether actively acquired datasets in text classification can be successfully transferred between different transformer-based pre-trained language models (PLMs). The authors hypothesize that transferability depends on the similarity of instances queried by the acquisition and consumer models during active learning. They propose a measure called acquisition sequence mismatch (ASM) to quantify differences in acquisition sequences between models. The key finding is that AL methods combining uncertainty and diversity (e.g., BADGE) produce more transferable datasets by minimizing ASM across different model pairs. The choice of AL method has a greater impact on ASM than the choice of models. Results show that while transferability varies across datasets, AL dataset transfer is generally successful, especially when sufficient labeled data is acquired.

## Method Summary
The study evaluates transferability between three transformer models (BERT, RoBERTa, ELECTRA) on four text classification datasets using three active learning methods (Entropy, Core-set, BADGE). Active learning acquisition proceeds until 1500 labeled instances are acquired, with 20 different warm-start sets to account for stochasticity. Transferability is measured by the difference in area under F1 curve (ΔAUC) between actively acquired and randomly sampled datasets. ASM is calculated between acquisition sequences using GloVe embeddings and the Hungarian algorithm to quantify similarity. The study systematically examines 108 AL configurations (3 models × 3 methods × 4 datasets) to identify patterns in transferability.

## Key Results
- AL method choice has greater impact on ASM than model choice, with BADGE consistently producing lower ASM values
- Entropy-based methods show negative transferability early but become positive as sufficient labeled data is acquired
- BADGE achieves the highest similarity of acquired datasets across all model pairs and datasets
- Transferability patterns vary significantly across datasets but show consistent method-level trends

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AL dataset transferability depends on similarity of instances queried by acquisition and consumer models
- Mechanism: If two models select similar data points during AL, the actively acquired dataset will be transferable between them because the selected data points will have similar information value for both models
- Core assumption: The similarity of acquisition sequences reflects the similarity of the underlying data distributions that are most informative for model learning
- Evidence anchors:
  - [abstract] "We link the AL dataset transferability to the similarity of instances queried by the different PLMs"
  - [section 4.2] "We hypothesize there is a link between dataset transferability and the sequence in which data points are acquired for labeling by AL"
  - [corpus] Weak evidence - only 1 neighbor paper mentions active learning, no direct discussion of transferability
- Break condition: If models have fundamentally different inductive biases that make different data points informative for learning, even similar acquisition sequences may not lead to transferable datasets

### Mechanism 2
- Claim: BADGE (uncertainty + diversity) minimizes acquisition sequence mismatch (ASM) across different model pairs
- Mechanism: BADGE selects data points that the model is uncertain about for diverse reasons, sampling along the entire decision boundary at each step, which leads to similar sampling patterns across different models
- Core assumption: Decision boundaries of different transformer models are roughly similar, so sampling along these boundaries produces similar data points across models
- Evidence anchors:
  - [section 4.2] "BADGE gives smaller ASM than the other two methods"
  - [section 4.3] "BADGE gives the highest similarity of acquired datasets among the considered methods"
  - [corpus] No direct evidence about BADGE's effectiveness in minimizing ASM
- Break condition: If models have significantly different decision boundary structures, BADGE's assumption of similar boundaries may not hold

### Mechanism 3
- Claim: Entropy-based AL methods produce transferable datasets after sufficient labeled data is acquired
- Mechanism: Entropy samples similar points because if one data point has high uncertainty, similar points also have high uncertainty, leading to formation of decision boundary from local patches; this eventually produces transferable datasets as boundary becomes stable
- Core assumption: Despite different acquisition sequences, entropy-based methods eventually acquire similar datasets once sufficient data is labeled
- Evidence anchors:
  - [section 4.3] "entropy dominates the similarity of acquired datasets" in later steps
  - [section 4.3] "once more data is labeled and the boundary becomes stable, both entropy and BADGE start to have a low batch mismatch"
  - [corpus] No direct evidence about entropy's long-term transferability
- Break condition: If models' decision boundaries converge at different rates or to different stable states, entropy-based transferability may fail

## Foundational Learning

- Concept: Active learning and uncertainty sampling
  - Why needed here: Understanding how AL methods select informative data points is crucial for understanding dataset transferability
  - Quick check question: What is the difference between uncertainty-based and diversity-based AL methods, and how do they select data points differently?

- Concept: Transformer-based PLMs and fine-tuning
  - Why needed here: The study examines transferability between different transformer models, requiring understanding of their architecture and training
  - Quick check question: How do BERT, RoBERTa, and ELECTRA differ in their pre-training objectives and architectures?

- Concept: Acquisition sequence mismatch (ASM) measurement
  - Why needed here: ASM is the key metric linking acquisition sequences to dataset transferability
  - Quick check question: How is ASM computed between two acquisition sequences, and what does it measure?

## Architecture Onboarding

- Component map:
  Data acquisition pipeline (datasets: SUBJ, COLA, AG-News, TREC) -> Model selection (BERT, RoBERTa, ELECTRA) -> AL method implementation (Entropy, Core-set, BADGE) -> ASM calculation module -> Transferability evaluation framework

- Critical path:
  1. Initialize AL with warm-start sets
  2. Run AL acquisition with chosen model and method
  3. Calculate ASM between acquisition and consumer model sequences
  4. Evaluate transferability using F1 score and AUC metrics
  5. Analyze results to identify successful transfer patterns

- Design tradeoffs:
  - Using GloVe embeddings for ASM calculation provides model-agnostic representation but may not capture transformer-specific features
  - Limiting to 1500 labeled instances balances computational cost with realistic AL scenarios
  - Using 20 different warm-start sets accounts for stochasticity but increases computational requirements

- Failure signatures:
  - High ASM values consistently correlate with failed transfers
  - Entropy method showing negative transferability early but positive later indicates temporal dynamics
  - Core-set method showing consistently high ASM suggests diversity-based methods may be less transferable

- First 3 experiments:
  1. Run all 108 AL configurations with 20 warm-start sets each to establish baseline transferability patterns
  2. Calculate ASM for each configuration to verify correlation with transferability
  3. Compare ASM distributions across AL methods to confirm method has greater impact than model choice

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of representation space (e.g., GloVe embeddings) for measuring acquisition sequence mismatch (ASM) affect the conclusions about transferability?
- Basis in paper: [explicit] The paper uses GloVe embeddings as a common representation space independent of the choice of acquisition and consumer models to compute cosine distances for ASM.
- Why unresolved: The choice of representation space could significantly impact the similarity measurements between acquisition sequences, potentially affecting the relationship between ASM and transferability.
- What evidence would resolve it: Experiments comparing ASM calculations using different representation spaces (e.g., model-specific embeddings, other general embeddings like FastText) and their correlation with transferability outcomes.

### Open Question 2
- Question: What dataset characteristics are predictive of successful active learning dataset transferability between transformer models?
- Basis in paper: [inferred] The paper mentions that transferability differs considerably across datasets but focuses on analyzing differences arising from models and AL methods rather than dataset characteristics.
- Why unresolved: While the paper shows transferability varies across datasets, it doesn't identify which dataset properties (e.g., complexity, class balance, label entropy) predict transfer success.
- What evidence would resolve it: Systematic analysis correlating dataset properties (like those in Table 2) with transferability outcomes across multiple dataset pairs and model combinations.

### Open Question 3
- Question: How does the interaction between uncertainty-based and diversity-based acquisition functions evolve as more data is labeled in active learning?
- Basis in paper: [explicit] The paper discusses how BADGE (combining uncertainty and diversity) differs from pure uncertainty methods like entropy, and how this affects transferability, particularly noting differences between early and later AL steps.
- Why unresolved: The paper observes differences in how these methods behave over time but doesn't fully explain the underlying mechanisms of this interaction.
- What evidence would resolve it: Detailed analysis of how the balance between uncertainty and diversity shifts throughout the AL process and its relationship to decision boundary formation and model performance.

## Limitations

- Computational constraints limited evaluation to only 1500 labeled instances, potentially missing full AL behavior at larger scales
- Study focused on three specific transformer models and three AL methods, potentially missing broader patterns
- Use of GloVe embeddings for ASM calculation may not capture transformer-specific representations that influence transferability
- Only examined text classification tasks, leaving questions about generalizability to other NLP tasks

## Confidence

**High confidence**: The core finding that AL method choice has greater impact on transferability than model choice (BADGE showing consistently lower ASM and better transferability across all datasets). The statistical correlation between ASM and transferability differences is well-established through systematic experiments.

**Medium confidence**: The mechanism explaining why BADGE produces more transferable datasets (sampling along decision boundaries) is plausible but not definitively proven. While the evidence shows BADGE minimizes ASM, the causal link between boundary sampling and transferability requires further investigation.

**Medium confidence**: The claim that entropy-based methods produce transferable datasets after sufficient labeled data is acquired is supported by observed convergence patterns, but the underlying mechanism (decision boundary stability) remains theoretical without direct validation.

## Next Checks

1. **Temporal ASM Analysis**: Track ASM values at each acquisition step across all method-model pairs to verify whether BADGE consistently maintains lower ASM throughout the acquisition process, not just in aggregate.

2. **Cross-task Generalization**: Test transferability patterns on non-text-classification tasks (e.g., named entity recognition or sentiment analysis) to determine if the observed method-level patterns hold across different NLP task types.

3. **Embedding Representation Impact**: Compare ASM calculations using transformer-specific embeddings (e.g., [CLS] token representations) versus GloVe embeddings to assess whether the choice of representation space affects the correlation between ASM and transferability.