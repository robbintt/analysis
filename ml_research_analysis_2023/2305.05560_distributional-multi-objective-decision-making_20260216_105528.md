---
ver: rpa2
title: Distributional Multi-Objective Decision Making
arxiv_id: '2305.05560'
source_url: https://arxiv.org/abs/2305.05560
tags:
- dominance
- distributional
- policies
- decision
- pareto
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses multi-objective decision making in scenarios
  with conflicting objectives. It introduces a novel dominance criterion, called distributional
  dominance, which relates return distributions of policies directly.
---

# Distributional Multi-Objective Decision Making

## Quick Facts
- arXiv ID: 2305.05560
- Source URL: https://arxiv.org/abs/2305.05560
- Reference count: 16
- The paper introduces distributional dominance for multi-objective decision making and proposes algorithms to learn the distributional undominated set (DUS) and convex distributional undominated set (CDUS).

## Executive Summary
This paper addresses multi-objective decision making in scenarios with conflicting objectives by introducing a novel dominance criterion called distributional dominance. The approach relates return distributions of policies directly, allowing policies that might be Pareto dominated in terms of expected values to be included if their distribution provides higher expected utility for risk-averse decision-makers. The authors propose the distributional undominated set (DUS) and convex distributional undominated set (CDUS) as solution sets, showing that DUS contains the Pareto front and all optimal policies for multivariate risk-averse decision makers, while CDUS is a subset containing policies optimal under the expected scalarized returns criterion.

## Method Summary
The method involves learning return distributions in MOMDPs using a distributional multi-objective Q-learning (DIMOQ) algorithm that extends Pareto Q-learning. The algorithm learns immediate reward distributions and expected future reward distributions, maintaining Q-sets that are pruned using DPrune to compute the DUS. A linear program (CDPrune) is then used to prune the DUS to the CDUS. Experiments were conducted on randomly generated MOMDPs of varying sizes (5-15 states, 2-4 actions) with 2 objectives, using 50,000 random walks for transition estimation followed by 2,000 training episodes.

## Key Results
- Distributional dominance guarantees strictly greater expected utility for multivariate risk-averse decision-makers compared to Pareto dominance
- The DUS contains all optimal policies for multivariate risk-averse decision makers and is a superset of the Pareto front
- The CDUS can be computed efficiently using linear programming and contains all optimal policies under the expected scalarized returns criterion
- The DIMOQ algorithm successfully learns the DUS in tested MOMDP configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distributional dominance guarantees a strictly greater expected utility for a subset of multivariate risk-averse decision-makers compared to Pareto dominance.
- Mechanism: By defining dominance based on return distributions directly, rather than expected values, distributional dominance captures the full distribution of outcomes. This allows policies that might be Pareto dominated in terms of expected values to be included if their distribution provides a higher expected utility for risk-averse decision-makers.
- Core assumption: The utility functions of the decision-makers are strictly monotonically increasing and satisfy the multivariate risk-aversion condition (∂²u(x₁,x₂)/∂x₁∂x₂ ≤ 0).
- Evidence anchors:
  - [abstract] The paper states that the distributional undominated set (DUS) contains all optimal policies for multivariate risk-averse decision makers and is a superset of the Pareto front.
  - [section] Theorem 3.2 in the paper proves that distributional dominance implies strictly greater expected utility for this class of decision-makers.
- Break condition: If the utility function does not satisfy the multivariate risk-aversion condition, or if the distributions are not discrete/categorical as assumed for computational feasibility, the mechanism may not hold.

### Mechanism 2
- Claim: The convex distributional undominated set (CDUS) can be computed efficiently using linear programming, making it practical for decision support.
- Mechanism: The paper proposes a linear program (CDPrune) that checks whether a distribution is dominated by a mixture of other distributions. This allows pruning the larger DUS down to the smaller CDUS, which contains only policies optimal under the expected scalarized returns (ESR) criterion.
- Core assumption: The return distributions can be represented as discrete/categorical distributions, enabling the use of linear programming techniques.
- Evidence anchors:
  - [abstract] The paper mentions that pruning operators are contributed to reduce the DUS to the CDUS.
  - [section] Algorithm 1 (CDPrune) presents the linear program for this pruning step.
- Break condition: If the return distributions cannot be accurately represented as discrete/categorical, or if the linear program becomes computationally intractable for very large sets, this mechanism may break down.

### Mechanism 3
- Claim: The distributional multi-objective Q-learning (DIMOQ) algorithm can learn the DUS in a given MOMDP, extending the applicability of the proposed solution sets.
- Mechanism: DIMOQ extends the Pareto Q-learning algorithm to learn return distributions instead of just expected values. It uses a modified Q-update rule to handle stochasticity and a distributional pruning operator (DPrune) to maintain the DUS during learning.
- Core assumption: The transition probabilities can be estimated from random walks, and the Q-sets can be kept manageable through precision limiting and clustering techniques.
- Evidence anchors:
  - [abstract] The paper proposes a novel algorithm to learn the distributional undominated set.
  - [section] Algorithm 2 (DIMOQ) presents the learning algorithm, and Section 6.2 discusses the experimental results demonstrating its feasibility.
- Break condition: If the MOMDPs are too large or complex for the Q-learning approach, or if the assumptions about transition probability estimation and Q-set management fail, the learning mechanism may not work effectively.

## Foundational Learning

- Concept: Multi-objective decision making and Pareto dominance
  - Why needed here: Understanding the limitations of Pareto dominance in capturing optimal policies for risk-averse decision-makers is crucial for appreciating the need for distributional dominance.
  - Quick check question: In a scenario with conflicting objectives, can you identify a policy that is Pareto optimal but might not be optimal for a risk-averse decision-maker?

- Concept: First-order stochastic dominance (FSD) and its relationship to expected utility
  - Why needed here: FSD is a key building block for distributional dominance, and understanding its implications for expected utility is important for grasping the theoretical foundations.
  - Quick check question: Given two univariate distributions, can you determine which one FSD-dominates the other, and explain why this implies a higher expected utility for certain decision-makers?

- Concept: Utility-based approach and different optimality criteria (SER vs ESR)
- Why needed here: The paper's solution sets are designed to contain optimal policies under both SER and ESR criteria, so understanding these concepts is essential for interpreting the results.
  - Quick check question: Can you explain the difference between SER and ESR, and provide an example where a policy optimal under SER is not optimal under ESR?

## Architecture Onboarding

- Component map: MOMDP -> DIMOQ (Algorithm 2) -> DPrune -> DUS -> CDPrune (Algorithm 1) -> CDUS

- Critical path:
  1. Define the MOMDP problem and decision-maker preferences
  2. Learn return distributions using DIMOQ
  3. Prune the learned set to DUS using DPrune
  4. Further prune to CDUS using CDPrune
  5. Present the resulting solution sets for decision support

- Design tradeoffs:
  - DUS vs CDUS: DUS is larger but contains more policies, while CDUS is smaller but focuses on ESR optimality. The choice depends on the decision-maker's preferences and the need for a concise solution set.
  - Precision vs scalability: Higher precision in representing return distributions leads to more accurate results but increases computational complexity. The tradeoff is managed through precision limiting and clustering techniques.

- Failure signatures:
  - If DIMOQ fails to learn the DUS accurately, the resulting solution sets may not contain all optimal policies.
  - If CDPrune incorrectly prunes a policy, the CDUS may miss some ESR-optimal policies.
  - If the linear program in CDPrune becomes intractable, the CDUS pruning step may not complete in a reasonable time.

- First 3 experiments:
  1. Implement and test DPrune on a small MOMDP with known optimal policies to verify its correctness.
  2. Implement and test CDPrune on the output of experiment 1 to verify its pruning capabilities and the relationship between DUS and CDUS.
  3. Implement and run DIMOQ on a small MOMDP, comparing the learned DUS with the known optimal policies to assess the learning algorithm's performance.

## Open Questions the Paper Calls Out

- What is the exact relationship between the CDUS and the Pareto front (PF) in general? The paper shows they are neither subsets nor supersets in all cases.
  - Basis in paper: Explicit. The paper states "However, here one can find counterexamples which disprove that the CDUS is either a subset or superset of the Pareto front."
  - Why unresolved: The paper identifies the relationship is not straightforward but doesn't specify exact conditions for when one contains the other.
  - What evidence would resolve it: A formal proof characterizing the exact conditions under which CDUS is a subset or superset of PF, or a complete classification of scenarios where each relationship holds.

- How can the DIMOQ algorithm be scaled to larger MOMDPs with more states and actions?
  - Basis in paper: Explicit. The paper states "As such, we plan to investigate the use of function approximation to further extend DIMOQ to larger MOMDPs."
  - Why unresolved: The current algorithm faces computational challenges with larger state and action spaces, but potential solutions are not explored.
  - What evidence would resolve it: Successful implementation and evaluation of DIMOQ with function approximation techniques on larger MOMDPs, demonstrating improved scalability.

- What are the conditions under which MOMDPs modeled after real-world scenarios would contain more structure, potentially improving the performance of DIMOQ?
  - Basis in paper: Explicit. The paper states "we note that MOMDPs modelled after real-world scenarios will likely contain more structure and are thus interesting to study for future work."
  - Why unresolved: The paper acknowledges potential benefits but doesn't investigate the specific characteristics of real-world MOMDPs that could improve algorithm performance.
  - What evidence would resolve it: Analysis of real-world MOMDP structures and their impact on DIMOQ performance, potentially leading to algorithm adaptations for specific problem domains.

## Limitations
- The theoretical guarantees rely heavily on discrete/categorical reward distributions, which may not capture continuous reward scenarios common in real-world applications.
- The computational complexity of the linear programming approach for CDUS pruning may become prohibitive for large-scale MOMDPs, though the paper does not provide scaling analysis beyond the tested configurations.
- The DIMOQ algorithm's performance depends on random walks for transition estimation, which may introduce bias or variance that affects the quality of the learned DUS, particularly in complex state spaces.

## Confidence
- High confidence in the theoretical foundations of distributional dominance and its relationship to multivariate risk-averse decision makers, as supported by Theorem 3.2 and consistent with established utility theory.
- Medium confidence in the practical implementation of DIMOQ and pruning algorithms, as the experimental results show feasibility but are limited to small-scale MOMDPs.
- Low confidence in the scalability of the proposed methods to real-world MOMDPs with continuous state spaces and complex reward structures.

## Next Checks
1. Test the DPrune and CDPrune algorithms on a benchmark MOMDP with known optimal policies to verify their correctness and assess their computational complexity as the number of states and actions increases.
2. Implement a continuous reward extension of the DIMOQ algorithm using kernel density estimation or other approximation techniques, and compare its performance against the discrete version on a continuous-reward MOMDP.
3. Conduct a sensitivity analysis of the DIMOQ algorithm's performance to the number of random walks used for transition estimation, and explore alternative estimation methods (e.g., Bayesian inference) to improve sample efficiency.