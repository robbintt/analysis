---
ver: rpa2
title: Being Right for Whose Right Reasons?
arxiv_id: '2306.00639'
source_url: https://arxiv.org/abs/2306.00639
tags:
- rationales
- agreement
- groups
- annotators
- rationale
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents what is, to our knowledge, the first dataset
  with demographics-augmented human rationale annotations. We cover three datasets
  spanning sentiment analysis and common-sense reasoning, with six demographic groups
  (balanced across age and ethnicity).
---

# Being Right for Whose Right Reasons?

## Quick Facts
- arXiv ID: 2306.00639
- Source URL: https://arxiv.org/abs/2306.00639
- Reference count: 40
- Primary result: Models align better with rationales from older and/or white annotators, with negative correlation between model size and rationale agreement.

## Executive Summary
This study introduces the first dataset with demographics-augmented human rationale annotations across six demographic groups for three NLP tasks. The research reveals that Transformer-based models systematically align better with rationales from certain demographic groups, particularly older and/or white annotators, while showing negative correlations between model size and rationale agreement. Surprisingly, model distillation does not improve fairness in rationale alignment, indicating that current methods for improving model efficiency do not address underlying fairness issues in how models extract and align with human reasoning.

## Method Summary
The study fine-tunes 16 Transformer-based models on three datasets (DynaSent, SST-2, CoS-E) for 3 epochs, selecting checkpoints with highest validation accuracy. Model rationales are extracted using Attention Rollout and Layer-wise Relevance Propagation (LRP) explainability methods. These are compared against human rationales annotated by six demographic groups (balanced across age and ethnicity) using token-level F1 and IOU-F1 metrics. Statistical analysis includes Spearman correlations to examine relationships between model size, distillation, and demographic fairness.

## Key Results
- Models show significantly better alignment with rationales from older and/or white annotators across all three datasets
- Larger models exhibit lower rationale agreement scores, showing a negative correlation with model size
- Model distillation improves average rationale scores but does not reduce performance gaps between demographic groups
- Sentiment analysis tasks show more pronounced demographic disparities in rationale alignment than common-sense reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Annotator demographic identity affects the definition of what counts as a rationale.
- Mechanism: When annotators from different demographics mark evidence, their subjective understanding of "supporting evidence" varies, producing systematically different rationales for the same instance.
- Core assumption: Annotators with different backgrounds hold different inferential or evidentiary norms.
- Evidence anchors:
  - [abstract] "what counts as a rationale is sometimes subjective" and "models are biased towards aligning best with older and/or white annotators."
  - [section 3.1] Instructions are identical across groups, yet inter-group disagreement in rationale alignment is observed.
  - [corpus] Average token-F1 across groups shows WO/WY rationales align more with others, while BY rationales diverge.
- Break condition: If rationale definitions are culturally universal or instructions enforce a single standard, this mechanism fails.

### Mechanism 2
- Claim: Model size negatively correlates with rationale alignment across demographic groups.
- Mechanism: Larger models learn more abstract or idiosyncratic patterns that diverge from the simpler, more direct evidence preferred by human annotators.
- Core assumption: Model capacity encourages reliance on non-lexical or spurious correlations, not necessarily aligned with human reasoning.
- Evidence anchors:
  - [abstract] "negative correlations between model size and rationale agreement."
  - [section 5.2] Figure 6 shows token-F1 decreasing with model size for DynaSent and SST-2 across both LRP and AR.
  - [corpus] Spearman correlation coefficients are negative and significant for most datasets with AR, and for DynaSent with LRP.
- Break condition: If smaller models overfit to noise or if task complexity requires larger capacity, the negative correlation may reverse.

### Mechanism 3
- Claim: Model distillation does not improve fairness in terms of rationale agreement.
- Mechanism: Distilling knowledge into a smaller model does not necessarily preserve the reasoning patterns that align with minority groups' rationales.
- Core assumption: Distillation retains predictive performance but not necessarily the inferential style of the teacher model.
- Evidence anchors:
  - [abstract] "no evidence that either model size or model distillation improves fairness."
  - [section 5.2] Table 2 shows distilled models have higher token-F1 and IOU-F1 on average, but only one (minilm-l6) has a smaller min-max gap.
  - [corpus] Overall min-max token-F1 differences are uncorrelated with model size, implying distillation alone does not reduce performance gaps.
- Break condition: If distillation is paired with fairness constraints or reweighted rationales, this mechanism may not hold.

## Foundational Learning

- Concept: Inter-annotator agreement vs. rationale disagreement
  - Why needed here: Distinguishing label agreement from rationale alignment is central to understanding why models may perform equally but align differently with groups.
  - Quick check question: If two annotators agree on a label, must their rationales also agree? (Answer: No.)

- Concept: Post-hoc explainability methods (attention vs. gradient-based)
  - Why needed here: Different methods extract rationales in incompatible ways; comparing them requires normalization and alignment procedures.
  - Quick check question: Do attention rollout and LRP always assign the highest relevance to the same tokens? (Answer: No.)

- Concept: Fairness metrics based on performance parity
  - Why needed here: The study measures fairness as min-max differences in rationale agreement, not just accuracy differences.
  - Quick check question: If all groups have the same accuracy, can the model still be unfair? (Answer: Yes, if rationale alignment differs.)

## Architecture Onboarding

- Component map: Data ingestion → Annotation processing → Model fine-tuning → Rationale extraction → Alignment computation → Statistical analysis
- Critical path: Annotation → Rationale extraction → Alignment computation → Correlation analysis
- Design tradeoffs:
  - Larger models → higher task accuracy but lower rationale alignment
  - Distillation → better average rationale scores but no fairness improvement
  - Explainability method choice → affects agreement scores and variability
- Failure signatures:
  - If rationale alignment scores are near zero, check preprocessing alignment between model and annotator tokenization
  - If min-max gaps remain constant across model sizes, the fairness measurement is not sensitive to capacity changes
- First 3 experiments:
  1. Run token-F1 and IOU-F1 on a held-out subset to confirm preprocessing alignment
  2. Compute Spearman correlations between model size and token-F1 for each dataset/method to replicate size-fairness relationship
  3. Compare distilled vs. non-distilled model rationale alignment within each demographic group to verify distillation effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the underlying cause of the systematic disagreement in rationales across demographic groups, particularly the lower agreement observed for Black Young (BY) annotators in sentiment analysis tasks?
- Basis in paper: [explicit] The paper explicitly states that Black Young annotators have lower alignment with others, especially in sentiment analysis tasks, and notes that this is consistent across datasets.
- Why unresolved: The paper does not provide a detailed explanation for this specific pattern of disagreement, only noting that it exists and is consistent.
- What evidence would resolve it: Qualitative analysis of BY annotators' rationales and potential cultural or contextual factors influencing their interpretations of sentiment.

### Open Question 2
- Question: Does model distillation improve fairness in terms of rationale alignment across demographic groups, beyond what is observed in this study?
- Basis in paper: [explicit] The paper finds that distilled models do not seem to be more fair in terms of rationale agreement, although they present overall higher scores.
- Why unresolved: The study only examined a limited set of distilled models and did not explore the potential impact of different distillation techniques or hyperparameter tuning.
- What evidence would resolve it: Comparative analysis of a wider range of distilled models, including different distillation methods and hyperparameter configurations, to assess their impact on fairness.

### Open Question 3
- Question: What is the impact of model size on fairness when considering other aspects of model performance beyond rationale alignment, such as robustness to adversarial attacks or out-of-distribution examples?
- Basis in paper: [inferred] The paper finds a negative correlation between model size and rationale agreement, but does not explore other potential impacts of model size on fairness.
- Why unresolved: The study focused specifically on rationale alignment and did not examine the broader implications of model size on fairness in NLP systems.
- What evidence would resolve it: Comprehensive evaluation of model size effects on various fairness metrics, including robustness to adversarial attacks and out-of-distribution examples, across different NLP tasks.

## Limitations
- The study cannot fully separate annotator subjectivity from task complexity effects on rationale disagreement, as some datasets inherently involve more subjective judgments than others.
- The demographic sample size per group may be insufficient to detect subtle interaction effects between multiple demographic factors beyond the binary age/ethnicity splits used.
- Implementation details of the LRP extension for Transformers are not fully specified, potentially affecting reproducibility of the exact rationale alignment scores.

## Confidence

- **High confidence**: The negative correlation between model size and rationale alignment is robust across multiple datasets and explainability methods, supported by consistent Spearman correlation coefficients and visual trends in Figure 6.
- **Medium confidence**: The claim that distillation does not improve fairness is supported by aggregate statistics, but individual model exceptions (like minilm-l6) suggest the relationship may be more nuanced than the binary conclusion implies.
- **Low confidence**: The mechanism that annotator demographics systematically define different "what counts as rationale" standards lacks direct causal evidence, relying primarily on observed disagreement patterns rather than controlled experiments testing definitional differences.

## Next Checks
1. Conduct a controlled annotation study where the same annotators provide rationales for identical instances under different explicit definitions of "supporting evidence" to test if demographic differences in rationale definitions can be manipulated experimentally.
2. Perform ablation studies varying model size while controlling for architectural differences (e.g., comparing models with identical architecture but different parameter counts through pruning) to isolate whether capacity itself or specific architectural choices drive the fairness-size relationship.
3. Apply fairness-aware training objectives that explicitly optimize for demographic parity in rationale alignment during fine-tuning to test whether the observed fairness gaps are immutable or can be mitigated through targeted optimization.