---
ver: rpa2
title: A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models
  Applied to Clinical and Biomedical Tasks
arxiv_id: '2307.12114'
source_url: https://arxiv.org/abs/2307.12114
tags:
- none
- tasks
- language
- label
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates four instruction-tuned large language models
  (ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca) on 13 real-world clinical and biomedical
  natural language processing tasks, including named-entity recognition, question-answering,
  relation extraction, and more. Results show that these models approach the performance
  of state-of-the-art models in zero- and few-shot scenarios, particularly excelling
  in question-answering tasks, despite never having seen examples from these tasks
  before.
---

# A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks

## Quick Facts
- arXiv ID: 2307.12114
- Source URL: https://arxiv.org/abs/2307.12114
- Authors: 
- Reference count: 16
- Key outcome: Instruction-tuned LLMs approach state-of-the-art performance on biomedical QA tasks in zero- and few-shot settings, but underperform on classification and relation extraction tasks.

## Executive Summary
This study evaluates four instruction-tuned large language models (ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca) on 13 real-world clinical and biomedical natural language processing tasks. The models demonstrate strong performance on question-answering tasks while showing limitations in classification and relation extraction. No single model consistently outperforms others across all tasks, suggesting task-specific model selection may be necessary for biomedical applications.

## Method Summary
The study evaluates four instruction-tuned LLMs (ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca) on 13 clinical and biomedical NLP tasks including NER, QA, RE, NLI, and classification. Models are tested in zero-shot and 5-shot scenarios using task-specific prompts, with few-shot examples retrieved via semantic similarity using Sentence-Transformers. For NER, both ChatGPT-specific methods and Recursive Chain-of-Thought approaches are employed. Manual parsing scripts evaluate generative outputs, with results compared against PubMedBERT baseline.

## Key Results
- LLMs approach state-of-the-art performance on biomedical QA tasks in zero- and few-shot scenarios
- Models excel particularly on question-answering tasks despite never seeing examples from these tasks
- No single model outperforms others across all tasks, with some models better suited for specific tasks
- Classification and relation extraction tasks perform below PubMedBERT, a specifically trained medical model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-tuned LLMs can generalize to biomedical tasks without task-specific fine-tuning.
- Mechanism: Models pretrained on massive text corpora and then fine-tuned with broad instruction datasets learn generalized reasoning patterns that transfer to unseen biomedical tasks.
- Core assumption: Instruction-tuning provides sufficient task-agnostic reasoning capacity to handle diverse biomedical NLP tasks.
- Evidence anchors:
  - [abstract] states models "begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks."
  - [section 2.2] describes instruction-tuning as enabling models to learn tasks from natural language instructions only.
- Break condition: Performance degrades significantly when tasks require highly specialized domain knowledge not covered by general instruction datasets.

### Mechanism 2
- Claim: Few-shot examples improve performance by providing task-specific context without full fine-tuning.
- Mechanism: Retrieval of semantically similar examples and recursive chain-of-thought prompting provide relevant task context that guides the model's reasoning process.
- Core assumption: The model can effectively leverage few examples to understand task structure and constraints.
- Evidence anchors:
  - [section 3.5] describes semantic retrieval of similar examples to maximize few-shot performance.
  - [section 3.6] introduces Recursive Chain-of-Thought method for NER tasks.
- Break condition: When the few-shot examples are too dissimilar from test cases or when the model's context window is insufficient to include both examples and task input.

### Mechanism 3
- Claim: Different LLMs excel at different task types based on their architecture and training data.
- Mechanism: Encoder-decoder models like Flan-T5 UL2 and Tk-Instruct have different strengths than decoder-only models like ChatGPT, leading to task-specific performance variations.
- Core assumption: Architectural differences and training datasets create task-specific strengths and weaknesses.
- Evidence anchors:
  - [section 4] shows no single model outperforms all others across all tasks.
  - [section 3.1] describes different model architectures (encoder-decoder vs decoder-only) and training approaches.
- Break condition: When task requirements align closely with a model's specific training objectives or architecture limitations.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: Understanding how models perform without any task-specific examples is crucial for evaluating generalization capabilities.
  - Quick check question: Can a model perform a biomedical NER task it has never seen before without any examples?

- Concept: Few-shot learning
  - Why needed here: The study explicitly evaluates performance with 5-shot examples, making this concept fundamental to interpreting results.
  - Quick check question: How does providing 5 examples of a classification task affect model performance compared to zero-shot?

- Concept: Instruction-tuning
  - Why needed here: All evaluated models are instruction-tuned, and this is central to their ability to follow task instructions.
  - Quick check question: What is the difference between standard fine-tuning and instruction-tuning in terms of model capabilities?

## Architecture Onboarding

- Component map:
  Instruction parser -> Semantic retriever -> Model inference engine -> Output parser

- Critical path:
  1. Receive instruction and input data
  2. Parse instruction into components
  3. Retrieve similar examples if in few-shot mode
  4. Format complete prompt with instruction, examples, and input
  5. Generate output from model
  6. Parse and format final results

- Design tradeoffs:
  - Context window vs. number of examples: Larger context windows allow more examples but increase computational cost
  - Retrieval quality vs. speed: More sophisticated retrieval improves few-shot performance but adds latency
  - Output parsing complexity vs. generality: Custom parsers work better but require per-task development

- Failure signatures:
  - Poor zero-shot performance on specialized tasks suggests insufficient instruction diversity
  - Inconsistent few-shot results may indicate retrieval quality issues or example selection problems
  - High computational cost with Recursive Chain-of-Thought suggests exploring simpler approaches for NER

- First 3 experiments:
  1. Test zero-shot performance on a simple classification task to establish baseline generalization
  2. Implement 5-shot retrieval with sentence transformers and evaluate performance gain
  3. Compare Recursive Chain-of-Thought vs. single-pass NER approaches on computational efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sensitivity of LLMs to specific prompt wording impact their reliability in clinical and biomedical applications, and can this sensitivity be mitigated?
- Basis in paper: [explicit] The paper mentions that LLMs trained from instructions are frequently sensitive to the particular words used for their input, which impacts their ability to produce correct outputs.
- Why unresolved: The paper does not explore methods to reduce this sensitivity or provide strategies for creating more robust prompts.
- What evidence would resolve it: Experiments comparing different prompt engineering techniques or fine-tuning strategies to reduce sensitivity would help determine the most effective approaches.

### Open Question 2
- Question: What is the impact of domain-specific instruction-tuning on the performance of LLMs in biomedical tasks compared to generic instruction-tuned models?
- Basis in paper: [inferred] The paper suggests that developing domain-specific models fine-tuned on a diverse set of tasks and specialized instruction prompts could help bridge the performance gap with proprietary models.
- Why unresolved: The paper does not provide empirical evidence comparing domain-specific instruction-tuned models to generic ones.
- What evidence would resolve it: Comparative studies evaluating the performance of domain-specific instruction-tuned LLMs versus generic instruction-tuned models on biomedical tasks would provide insights into the benefits of domain-specific tuning.

### Open Question 3
- Question: How does the computational cost of LLMs affect their practical deployment in clinical settings, and what are the potential strategies to reduce this cost?
- Basis in paper: [explicit] The paper notes that BERT-based models offer much lower computational costs compared to LLMs, which could be a significant obstacle to developing models in the healthcare domain.
- Why unresolved: The paper does not explore specific strategies to reduce the computational cost of LLMs or assess their feasibility in clinical environments.
- What evidence would resolve it: Studies evaluating the trade-offs between computational cost and performance for different model architectures, as well as the development of efficient inference techniques, would help address this question.

### Open Question 4
- Question: What is the potential impact of privacy concerns on the use of proprietary LLMs like ChatGPT in biomedical applications?
- Basis in paper: [explicit] The paper highlights privacy concerns about using ChatGPT in medical applications due to the inability to guarantee that the evaluated data has never been seen before.
- Why unresolved: The paper does not explore the extent of these privacy concerns or propose solutions to address them.
- What evidence would resolve it: Research on privacy-preserving techniques for using proprietary LLMs in sensitive domains, such as federated learning or differential privacy, would help assess the feasibility of their deployment in clinical settings.

### Open Question 5
- Question: How does the performance of LLMs in zero- and few-shot scenarios compare to that of models specifically trained on biomedical data when evaluated on tasks not seen during training?
- Basis in paper: [explicit] The paper demonstrates that LLMs approach the performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, particularly excelling in question-answering tasks, despite never having seen examples from these tasks before.
- Why unresolved: The paper does not explore the performance of LLMs on tasks not seen during training, which would provide a more comprehensive understanding of their generalization capabilities.
- What evidence would resolve it: Evaluations of LLMs on novel biomedical tasks not included in their training data would help determine their ability to generalize to unseen scenarios.

## Limitations
- Manual parsing scripts for evaluating generative outputs introduce potential measurement bias
- Comparison with PubMedBERT may not represent absolute state-of-the-art in biomedical NLP
- Study focuses only on English biomedical text, limiting generalizability to other languages or clinical settings
- Computational costs for Recursive Chain-of-Thought method raise practical deployment concerns

## Confidence
- High: The core finding that instruction-tuned LLMs can approach state-of-the-art performance on biomedical QA tasks in zero- and few-shot settings
- Medium: The conclusion about LLMs underperforming specialized models like PubMedBERT on classification and relation extraction tasks
- Low: The assertion that instruction-tuning provides sufficient generalization for diverse biomedical tasks may overstate the robustness of these findings

## Next Checks
1. Implement automated evaluation metrics (e.g., exact match, semantic similarity) to replace manual parsing scripts and quantify potential measurement bias in the current evaluation approach.

2. Conduct ablation studies systematically varying prompt wording, few-shot example selection, and retrieval strategies to isolate which factors most influence performance differences between models.

3. Test the models on additional biomedical NLP tasks not included in the original study (e.g., de-identification, clinical trial matching) to assess whether the observed generalization patterns hold across a broader task spectrum.