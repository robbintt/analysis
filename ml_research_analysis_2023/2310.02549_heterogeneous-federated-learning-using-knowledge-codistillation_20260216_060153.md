---
ver: rpa2
title: Heterogeneous Federated Learning Using Knowledge Codistillation
arxiv_id: '2310.02549'
source_url: https://arxiv.org/abs/2310.02549
tags:
- distillation
- federated
- learning
- fedavg
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the limitation in federated learning where
  all clients must share the same model architecture, which restricts model performance
  by limiting the shared model size to the lowest-common-denominator client's capacity.
  The proposed method partitions clients into pools based on model-size capacity and
  uses bi-directional knowledge distillation between server models from different
  federated averaging (FedAVG) pools, leveraging unlabeled server data without sharing
  parameters.
---

# Heterogeneous Federated Learning Using Knowledge Codistillation

## Quick Facts
- arXiv ID: 2310.02549
- Source URL: https://arxiv.org/abs/2310.02549
- Reference count: 40
- Primary result: Bi-directional knowledge distillation between heterogeneous models improves federated learning performance beyond FedAVG baseline

## Executive Summary
This paper addresses a key limitation in federated learning where all clients must use identical model architectures, constraining performance to the lowest-common-denominator capacity. The authors propose partitioning clients into pools based on model-size capacity and using bi-directional knowledge distillation between server models from different federated averaging pools. This approach enables models of different architectures to share knowledge without parameter sharing, using unlabeled server data. The method is evaluated on both image classification and language modeling tasks, showing consistent improvements over standard FedAVG, particularly when models have complementary strengths across different domains.

## Method Summary
The method partitions clients into low- and high-capacity pools, training a small model on all clients and a large model on high-capacity clients using FedAVG. Bi-directional knowledge distillation is then performed between server models using an unlabeled distillation dataset. Two variants are proposed: PERIODIC CODIST (distillation at fixed intervals) and MERGED CODIST (distillation gradients merged with FedAVG gradients every round). The distillation uses KL divergence between output probabilities, with gradients scaled by an interpolation factor α. This enables knowledge transfer without sharing parameters, allowing the large model to benefit from broader data diversity while the small model gains from higher model capacity.

## Key Results
- On CIFAR-100, MERGED CODIST achieved 33.02% accuracy for small model and 25.83% for large model vs 31.84% and 24.76% for FedAVG
- MERGED CODIST consistently outperforms PERIODIC CODIST across most scenarios
- Domain transfer is enabled between models trained on disjoint client domains, with +5% accuracy boost for small model on out-of-domain questions and +0.5% for large model on out-of-domain answers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The co-distillation strategy allows a larger model to leverage the broader data diversity of the low-capacity pool, improving its performance beyond what its limited client pool alone would allow.
- Mechanism: By partitioning clients into low- and high-capacity pools, the small model trains on all clients, while the large model trains on the high-capacity subset. Bidirectional knowledge distillation between the server models allows the large model to absorb information from the low-capacity clients through the small model.
- Core assumption: Distillation can effectively transfer knowledge even when teacher and student have different architectures, and unlabeled server data is sufficiently representative of the client distributions.
- Evidence anchors:
  - [abstract]: "The models exchange information bidirectionally via knowledge distillation, utilizing an unlabeled dataset on a server without sharing parameters."
  - [section]: "The larger model utilizes the information in the larger pool of clients through the smaller model, and the smaller model can benefit from the higher capacity of the larger model."
- Break condition: If the distillation data is highly out-of-domain or the model architectures are too dissimilar, distillation may fail to transfer useful knowledge.

### Mechanism 2
- Claim: Merging distillation gradients with federated averaging gradients at every round enables more stable and effective knowledge sharing than periodic distillation alone.
- Mechanism: After each FedAVG round, distillation is performed, and the resulting gradient is scaled and combined with the client gradient before the model update. This continuous integration allows both models to benefit incrementally.
- Core assumption: The distillation gradient can be meaningfully merged with the FedAVG gradient without destabilizing training, and the interpolation factor α is appropriately tuned.
- Evidence anchors:
  - [section]: "We use a similar approach to treat the model difference obtained from applying a round of distillation as a gradient... We can achieve this by a weighted combination of the two gradients using an interpolating factor α ∈ [0, 1]."
  - [section]: "MERGED CODIST is the more effective strategy and consistently improves performance in most scenarios."
- Break condition: If α is set too low, distillation updates are drowned out; if too high, client updates dominate and the benefit is lost.

### Mechanism 3
- Claim: Domain transfer is possible between models trained on disjoint client domains, enabling each model to improve on its out-of-domain data.
- Mechanism: When client pools are split by domain (e.g., questions vs. answers), bidirectional distillation allows the models to share complementary knowledge, improving performance on their respective out-of-domain categories.
- Core assumption: Distillation can transfer generalizable patterns across domains even without labeled data and when teacher and student have different performance levels.
- Evidence anchors:
  - [abstract]: "Additionally, the bi-directional knowledge distillation allows for domain transfer between the models when different pool populations introduce domain shift."
  - [section]: "MERGED CODIST achieves domain transfer between the two models resulting in a +5% accuracy boost for the small model on out-of-domain questions and +0.5% accuracy boost for the large model on out-of-domain answers."
- Break condition: If the domain gap is too large, distillation may not bridge the gap effectively, especially if only out-of-domain distillation data is available.

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: Enables sharing of model knowledge without sharing parameters, which is critical for heterogeneous federated learning where model architectures differ.
  - Quick check question: How does knowledge distillation differ from parameter averaging in federated learning?

- Concept: Federated Averaging (FedAVG)
  - Why needed here: Forms the base training loop for each model pool; the paper builds on top of it by adding distillation steps.
  - Quick check question: What role does the client sampling ratio play in FedAVG's convergence behavior?

- Concept: KL Divergence as Distillation Loss
  - Why needed here: Provides a differentiable measure of similarity between teacher and student output distributions, guiding the distillation process.
  - Quick check question: Why is a temperature scaling parameter used in the softmax during distillation?

## Architecture Onboarding

- Component map:
  - Two client pools (low- and high-capacity)
  - Two server models (small and large)
  - Unlabeled distillation dataset on server
  - Distillation optimizer (Adam) and FedAVG optimizer (FedAdam)
  - Hyperparameter tuner (Vizier) for learning rates, temperatures, α, etc.

- Critical path:
  1. Partition clients into pools
  2. Run FedAVG on each pool
  3. Perform bidirectional distillation on server
  4. Merge distillation and FedAVG gradients (MERGED CODIST) or apply distillation periodically (PERIODIC CODIST)
  5. Update server models and repeat

- Design tradeoffs:
  - PERIODIC CODIST is easier to implement and tune but less effective than MERGED CODIST
  - MERGED CODIST requires distillation every round, increasing server compute
  - Using out-of-domain distillation data reduces need for labeled data but may hurt performance
  - Larger distillation datasets improve performance but increase storage/compute cost

- Failure signatures:
  - PERIODIC CODIST: Spikes in out-of-domain accuracy followed by drops; no net gain over FedAVG
  - MERGED CODIST: If α is poorly tuned, one gradient dominates and distillation benefit is lost
  - Both: If distillation data is too out-of-domain, models may diverge or fail to improve

- First 3 experiments:
  1. Run baseline FedAVG on both pools; measure accuracy of small and large models
  2. Run PERIODIC CODIST with default settings; compare accuracy gains over baseline
  3. Run MERGED CODIST with tuned α; compare accuracy and convergence speed to PERIODIC CODIST

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do PERIODIC CODIST and MERGED CODIST methods perform on datasets with larger domain shifts between client pools?
- Basis in paper: [explicit] The paper mentions that PERIODIC CODIST struggles with domain-shifted client pools, while MERGED CODIST shows better performance in such settings.
- Why unresolved: The paper only tests domain shift in one specific scenario (StackOverflow questions vs. answers). The effectiveness of these methods in other types of domain shifts remains unexplored.
- What evidence would resolve it: Experiments testing these methods on datasets with different types of domain shifts (e.g., image classification with different object categories, language modeling with different topics) would provide insight into their generalizability.

### Open Question 2
- Question: What is the optimal distillation period (p) and number of distillation steps (s) for PERIODIC CODIST in different scenarios?
- Basis in paper: [explicit] The paper uses fixed values of p=200 and s=200 for all PERIODIC CODIST experiments, but acknowledges that these parameters could be adjusted based on available resources.
- Why unresolved: The paper doesn't explore the impact of varying these parameters on performance, leaving the question of optimal values unanswered.
- What evidence would resolve it: A systematic study varying p and s across different datasets and model architectures would reveal how these parameters affect performance and help identify optimal values for different scenarios.

### Open Question 3
- Question: How does the performance of these methods scale with the number of client pools (more than two)?
- Basis in paper: [explicit] The paper only considers two client pools (high-capacity and low-capacity) and mentions that the approach can be extended to any number of pools.
- Why unresolved: The paper doesn't provide any empirical evidence or analysis of how these methods would perform with more than two client pools.
- What evidence would resolve it: Experiments implementing these methods with three or more client pools on various datasets would demonstrate how performance scales and whether additional pools provide significant benefits.

## Limitations
- Modest absolute performance gains (1-2% accuracy improvements) over FedAVG baseline
- Reliance on server-held unlabeled data raises scalability and privacy concerns
- Computational overhead of continuous distillation in MERGED CODIST may limit practical deployment

## Confidence
- **High Confidence**: The core mechanism of bidirectional distillation between heterogeneous models is technically sound and well-motivated
- **Medium Confidence**: The empirical results demonstrating performance improvements over FedAVG, though the absolute gains are relatively small
- **Low Confidence**: The scalability and privacy implications of the server-side distillation approach, particularly for large-scale deployments

## Next Checks
1. Test the method across a wider range of model architecture pairs (e.g., CNN vs. transformer, small vs. very large models) to validate distillation effectiveness
2. Evaluate the privacy-utility tradeoff by quantifying information leakage through distillation when using out-of-domain server data
3. Conduct ablation studies to isolate the contribution of continuous (MERGED) vs. periodic (PERIODIC) distillation to overall performance gains