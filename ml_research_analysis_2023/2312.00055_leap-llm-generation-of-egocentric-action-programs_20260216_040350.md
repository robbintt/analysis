---
ver: rpa2
title: 'LEAP: LLM-Generation of Egocentric Action Programs'
arxiv_id: '2312.00055'
source_url: https://arxiv.org/abs/2312.00055
tags:
- action
- programs
- video
- leap
- epic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LEAP (LLM-Generation of Egocentric Action Programs),
  a novel method for generating video-grounded action programs through the use of
  a Large Language Model (LLM). LEAP's action programs represent the motoric, perceptual,
  and structural aspects of action, consisting of sub-actions, pre- and post-conditions,
  and control flows.
---

# LEAP: LLM-Generation of Egocentric Action Programs

## Quick Facts
- arXiv ID: 2312.00055
- Source URL: https://arxiv.org/abs/2312.00055
- Reference count: 40
- Key outcome: Achieves 1st place on EPIC Kitchens Action Recognition leaderboard among RGB-only methods

## Executive Summary
LEAP introduces a novel approach to generate video-grounded action programs using Large Language Models (LLMs) for egocentric action recognition and anticipation. The method converts multimodal video information (audio, visual, spatial, and textual) into structured action programs containing sub-actions, pre/post-conditions, and control flows. These programs serve as auxiliary supervision to improve action understanding networks, achieving state-of-the-art performance on the EPIC Kitchens dataset while providing interpretable action representations covering 87% of the training set.

## Method Summary
LEAP extracts multimodal descriptors from video clips (audio, SLAM, narrations, object detection, hand-object contact) and converts them to text. An LLM (GPT-4) generates structured action programs by synthesizing these descriptors with exemplar programs. These programs are used as auxiliary supervision through loss terms (LT, LPC) to train a UniFormerV2 network for action recognition and anticipation. For anticipation tasks, program predictions from surrounding clips are temporally aggregated to provide contextual information about ongoing actions.

## Key Results
- Achieves 1st place on EPIC Kitchens Action Recognition leaderboard among RGB-only methods
- Demonstrates sizable improvements in both action recognition and anticipation tasks
- Action programs cover 87% of the training set of EPIC Kitchens dataset
- Particularly strong gains in anticipation performance due to temporal aggregation of program predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based program generation provides structured supervision that captures motoric, perceptual, and structural aspects of action
- Mechanism: By compiling multimodal video information into textual descriptors and feeding them to an LLM alongside exemplar action programs, LEAP generates structured action programs containing sub-actions, pre/post-conditions, and control flows
- Core assumption: LLMs can effectively synthesize multimodal information into coherent action programs when provided with proper exemplars
- Evidence anchors:
  - [abstract] "These action programs represent the motoric, perceptual, and structural aspects of action, and consist of sub-actions, pre- and post-conditions, and control flows"
  - [section] "We exploit recent developments among Large Language Models [5] (LLMs), using them to produce a dataset of action programs over the EPIC Kitchens dataset"
  - [corpus] Weak evidence - no direct citations found in related papers
- Break condition: LLM fails to synthesize multimodal information coherently or generates programs that don't align with actual video content

### Mechanism 2
- Claim: Action programs improve downstream action recognition and anticipation by providing rich hierarchical supervision
- Mechanism: The generated action programs are used as auxiliary supervision through loss terms (LT, LPC) that train the action understanding network to predict both action classes and action programs simultaneously
- Core assumption: Incorporating program-level supervision improves feature learning for action recognition beyond standard action label supervision
- Evidence anchors:
  - [abstract] "We employ LEAP as a secondary source of supervision, using its action programs in a loss term applied to action recognition and anticipation networks"
  - [section] "Our final combined loss is LAP = LT + LPC + LCE" and "We demonstrate sizable improvements to both action recognition and anticipation due to inclusion of LEAP action programs"
  - [corpus] Weak evidence - related papers focus on video-language models but not specifically on program-based supervision
- Break condition: Program-based supervision interferes with action classification learning or the program predictions are too noisy

### Mechanism 3
- Claim: Temporal aggregation of action programs from surrounding clips improves anticipation performance
- Mechanism: For anticipation, the model aggregates program predictions from previous clips (i-n to i) to provide contextual information about ongoing actions before they fully occur
- Core assumption: Action programs capture long-term temporal semantics that benefit anticipation tasks
- Evidence anchors:
  - [section] "In action anticipation, the model cannot observe input beyond frame ts - τa where ts is the start time of the action and τa is the anticipation time" and "we provide sq for i-2 ≤ q ≤ i where each sq spans 256 frames"
  - [section] "We note that benefits to action anticipation are particularly sizable, attributable to the longer-term temporal semantics captured through the aggregation of action program predictions over time"
  - [corpus] No direct evidence found in related papers
- Break condition: Aggregation of surrounding clips introduces noise or the temporal context becomes irrelevant

## Foundational Learning

- Concept: Multimodal information fusion
  - Why needed here: LEAP must combine audio, visual, spatial, and textual information to generate accurate action programs
  - Quick check question: What are the five components used to extract textual descriptors from video clips?

- Concept: Hierarchical action representation
  - Why needed here: Action programs need to capture both atomic sub-actions and their compositional structure through pre/post-conditions and control flows
  - Quick check question: How does LEAP represent the "why" of sub-actions in its action programs?

- Concept: Self-supervised learning with auxiliary tasks
  - Why needed here: The action program prediction task serves as auxiliary supervision to improve the main action recognition/anticipation task
  - Quick check question: What are the two auxiliary loss functions used in training the action understanding network?

## Architecture Onboarding

- Component map:
  - LLM program generator (GPT-4) -> Five video processing components (Audio extractor, SLAM, narrations, Faster-RCNN, hand-object contact detector) -> Action understanding network (UniFormerV2 base with program prediction head) -> Temporal aggregator (combines program predictions from surrounding clips for anticipation)

- Critical path:
  1. Extract multimodal descriptors from video clip
  2. Feed descriptors + exemplar programs to LLM
  3. Parse LLM output into structured action program
  4. Train action network with combined action program and action label supervision
  5. For anticipation, aggregate programs from surrounding clips

- Design tradeoffs:
  - Using LLM vs. rule-based program generation: LLM provides flexibility but requires API costs and lacks interpretability
  - Program complexity vs. prediction accuracy: More detailed programs may improve supervision but are harder to predict accurately
  - Temporal aggregation window size: Larger windows provide more context but may include irrelevant information

- Failure signatures:
  - Low program prediction accuracy indicates issues with LLM synthesis or descriptor quality
  - Action recognition accuracy improves less than expected suggests program supervision isn't providing useful features
  - Anticipation performance lags behind recognition indicates temporal aggregation isn't capturing relevant context

- First 3 experiments:
  1. Test LLM program generation quality by manually inspecting generated programs for a subset of videos
  2. Evaluate program prediction accuracy (S-Verb, Obj(A), Obj(B)) on the validation split
  3. Compare action recognition performance with and without program-based supervision to quantify benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the incorporation of action programs from LEAP impact the performance of action recognition and anticipation tasks in different types of video datasets?
- Basis in paper: [explicit] The paper demonstrates sizable improvements in performance in both action recognition and anticipation tasks due to training with the LEAP dataset.
- Why unresolved: The paper focuses on the EPIC Kitchens dataset and does not explore the generalizability of the method to other datasets or types of videos.
- What evidence would resolve it: Conducting experiments on various video datasets, such as instructional videos, sports videos, or surveillance footage, and comparing the performance of action recognition and anticipation tasks with and without the incorporation of LEAP action programs.

### Open Question 2
- Question: How does the accuracy of the action program generation in LEAP compare to human-annotated action programs?
- Basis in paper: [explicit] The paper states that the LEAP action programs are generated automatically using an LLM, with limited manual annotation necessary.
- Why unresolved: The paper does not provide a comparison between the generated action programs and human-annotated ones, leaving the question of accuracy open.
- What evidence would resolve it: Conducting a study where human annotators create action programs for a subset of the EPIC Kitchens dataset, and comparing these with the LEAP-generated programs in terms of accuracy and completeness.

### Open Question 3
- Question: What are the limitations of the current formulation of action programs in LEAP, and how can they be improved?
- Basis in paper: [explicit] The paper mentions that the current formulation focuses on sub-actions executed serially and does not explore parallelizability.
- Why unresolved: The paper does not provide a detailed analysis of the limitations of the current action program formulation or propose potential improvements.
- What evidence would resolve it: Conducting experiments to evaluate the performance of action programs with parallel sub-actions, and exploring different control flow structures or additional types of sub-actions to enhance the representation of complex actions.

## Limitations

- Data Quality Dependency: Performance is fundamentally limited by the quality of multimodal descriptors extracted from video clips
- LLM Access and Cost Constraints: Requires access to GPT-4 for program generation, creating potential scalability issues and ongoing costs
- Generalizability Gap: Method may not generalize well to other domains or datasets without significant adaptation

## Confidence

**High Confidence**: The reported improvements in action recognition and anticipation performance on the EPIC Kitchens leaderboard are well-supported by the experimental results. The 1st place ranking among RGB-only methods and the sizable improvements over baselines provide strong empirical validation.

**Medium Confidence**: The mechanism by which LLM-generated action programs improve downstream task performance is reasonably well-supported but could benefit from more detailed ablation studies. While the paper demonstrates performance gains, the exact contribution of each program component to the improvements is not fully characterized.

**Low Confidence**: The scalability of the approach to larger or more diverse datasets remains uncertain. The paper focuses on a single dataset, and the manual effort required to create exemplar programs for new domains may limit practical applicability.

## Next Checks

1. **Program Generation Quality Analysis**: Conduct a systematic human evaluation of a random sample of generated action programs across different video types to assess consistency, accuracy, and coverage.

2. **Ablation Study on Program Components**: Perform controlled experiments removing different components of the action programs to determine which elements contribute most to the performance improvements.

3. **Cross-Dataset Generalization Test**: Apply LEAP to a different egocentric dataset (e.g., EGTEA Gaze+) without extensive retraining to evaluate the method's generalizability and identify domain-specific limitations.