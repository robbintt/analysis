---
ver: rpa2
title: On the Convergence of Loss and Uncertainty-based Active Learning Algorithms
arxiv_id: '2312.13927'
source_url: https://arxiv.org/abs/2312.13927
tags:
- loss
- sampling
- function
- learning
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies convergence properties of loss-based and uncertainty-based
  active learning algorithms using stochastic gradient descent (SGD). The authors
  analyze algorithms that sample data points based on either their loss value or uncertainty
  value, which is particularly relevant for active learning and data subset selection
  problems.
---

# On the Convergence of Loss and Uncertainty-based Active Learning Algorithms

## Quick Facts
- arXiv ID: 2312.13927
- Source URL: https://arxiv.org/abs/2312.13927
- Reference count: 40
- One-line primary result: Convergence rate guarantees of O(1/n) for loss-based and uncertainty-based active learning algorithms using SGD with sampling strategies.

## Executive Summary
This paper studies convergence properties of loss-based and uncertainty-based active learning algorithms using stochastic gradient descent (SGD). The authors analyze algorithms that sample data points based on either their loss value or uncertainty value, which is particularly relevant for active learning and data subset selection problems. The paper makes several key contributions including convergence rate guarantees for loss-based sampling, a new Adaptive-Weight Sampling (AWS) algorithm that combines sampling with stochastic Polyak's step size, and establishing convergence rate results for AWS for smooth convex training loss functions.

## Method Summary
The paper analyzes loss-based and uncertainty-based active learning algorithms using SGD with Bernoulli sampling strategies. The core method involves sampling data points with probability proportional to their loss or uncertainty, then performing SGD updates using either constant step size or Polyak's step size. The proposed AWS algorithm introduces an adaptive sampling probability that depends on the loss gradient and the gap between current and minimum loss. The framework leverages known SGD convergence results by recognizing that loss-based sampling algorithms optimize an underlying objective function. Hyperparameters are tuned using Tree-structured Parzen Estimator (TPE) algorithm to minimize average progressive cross-entropy loss.

## Key Results
- Provides convergence rate guarantees of O(1/n) for loss-based sampling with different loss functions
- Proposes Adaptive-Weight Sampling (AWS) algorithm achieving stochastic Polyak's step size in expectation
- Establishes convergence rate results for AWS for smooth convex training loss functions
- Demonstrates robustness to noise in loss estimation through numerical experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Loss-based sampling can achieve O(1/n) convergence when sampling probability satisfies a bound on the product of loss gradient and margin.
- Mechanism: The paper shows that under certain conditions, the sampling probability π(x,y,θ) and loss function ℓ(x,y,θ) satisfy: π(x,y,θ)||∇θℓ(x,y,θ)||² ≤ αℓ̃(x,y,θ) and π(x,y,θ)∇θℓ(x,y,θ)⊤(θ−θ*) ≥ βℓ̃(x,y,θ). This allows the SGD algorithm to mimic Polyak's step size in expectation, yielding convergence.
- Core assumption: The loss function and sampling probability satisfy the two inequalities above (equations (2) and (3) in Section 3.1).
- Evidence anchors:
  - [abstract]: "We show a condition on the sampling that ensures a convergence rate guarantee for this algorithm for smooth convex loss functions."
  - [section]: "Under Assumption 3.1, for any θ1 and {θt}t>1 according to algorithm (1) with γ = β/α, E[Pn t=1 ℓ̃(xt, yt, θt)] ≤ ||θ1 − θ*||² α/β²."
- Break condition: If the sampling probability does not satisfy the required bound on the product of loss gradient and margin, the convergence guarantee does not hold.

### Mechanism 2
- Claim: The paper provides a framework to derive convergence rate bounds for loss-based sampling by leveraging known SGD convergence results.
- Mechanism: The framework recognizes that loss-based sampling algorithms are SGD algorithms with an underlying objective function ℓ̃(θ) = E[Π(ℓ(x,θ))], where Π is the primitive of the sampling probability function π. This allows applying known SGD convergence results.
- Core assumption: The sampling probability π is an increasing function of the conditional expected loss ℓ(x,θ).
- Evidence anchors:
  - [abstract]: "We provide a framework that allows us to derive convergence rate bounds for loss-based sampling by deploying known convergence rate bounds for stochastic gradient descent algorithms."
  - [section]: "The algorithm (1) is a stochastic gradient descent algorithm with respect to an objective function ℓ̃ with gradient ∇θℓ̃(θ) = E[π(x,y,θ)∇θℓ(x,y,θ)]."
- Break condition: If the sampling probability is not an increasing function of the conditional expected loss, the framework does not apply.

### Mechanism 3
- Claim: The Adaptive-Weight Sampling (AWS) algorithm combines sampling with stochastic Polyak's step size, achieving O(1/n) convergence for smooth convex loss functions.
- Mechanism: AWS uses a Bernoulli sampling strategy with an adaptive step size ζ(x,y,θ) that depends on the loss gradient and the gap between the current loss and the minimum loss. The sampling probability π(x,y,θ) is chosen to ensure the step size remains stochastic Polyak's step size in expectation.
- Core assumption: The sampling probability satisfies π(x,y,θ) ≥ β/(2(1-c)) min{ρψ(x,y,θ),1} for some constants β, ρ, c ∈ (0,1), where ψ(x,y,θ) = ||∇θℓ(x,y,θ)||²/(ℓ(x,y,θ)−infθ'ℓ(x,y,θ')).
- Evidence anchors:
  - [abstract]: "We propose an active learning algorithm that combines sampling of points and stochastic Polyak's step size. We show a condition on the sampling strategy under which a non-asymptotic convergence rate of order O(1/n) holds for smooth convex loss functions."
  - [section]: "We next show a convergence rate guarantee. Lemma 3.12 Assume that ℓ is a convex, L-smooth function, there exists Λ* such that E[ℓ(x,y,θ*)]−E[infθℓ(x,y,θ)] ≤ Λ*, and the sampling probability function π is such that, for some constant c ∈ (0,1), for any x,y,θ such that ||∇θℓ(x,y,θ)|| > 0, π(x,y,θ) ≥ β/(2(1-c)) min{ρψ(x,y,θ),1}."
- Break condition: If the sampling probability does not satisfy the required bound on ψ(x,y,θ), the convergence guarantee does not hold.

## Foundational Learning

- Concept: Stochastic Gradient Descent (SGD) with adaptive step sizes
  - Why needed here: The paper's algorithms are variants of SGD with different step size strategies (constant, Polyak, adaptive Polyak).
  - Quick check question: What is the key difference between constant step size and Polyak's step size in SGD?

- Concept: Convex optimization and smoothness
  - Why needed here: The convergence guarantees rely on the loss function being convex and smooth.
  - Quick check question: Why is smoothness (Lipschitz continuous gradient) important for SGD convergence?

- Concept: Active learning and data subset selection
  - Why needed here: The paper's algorithms are designed for active learning scenarios where labeling is expensive.
  - Quick check question: How does uncertainty sampling differ from loss-based sampling in active learning?

## Architecture Onboarding

- Component map:
  - Data stream -> Sampling module -> Model update module -> Loss estimator -> Hyperparameter tuner

- Critical path:
  1. Receive unlabeled data point (x,y)
  2. Compute loss/uncertainty estimate
  3. Sample point with probability π(x,y,θ)
  4. If sampled, query label and update model with SGD
  5. Repeat

- Design tradeoffs:
  - Sampling rate vs. convergence speed: Higher sampling rates may improve convergence but increase labeling cost
  - Step size choice: Constant vs. Polyak vs. adaptive Polyak affects convergence and stability
  - Loss estimation noise: Noisy loss estimates may degrade performance but are more practical

- Failure signatures:
  - Slow convergence: