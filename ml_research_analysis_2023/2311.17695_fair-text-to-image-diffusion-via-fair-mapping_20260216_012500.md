---
ver: rpa2
title: Fair Text-to-Image Diffusion via Fair Mapping
arxiv_id: '2311.17695'
source_url: https://arxiv.org/abs/2311.17695
tags:
- diffusion
- bias
- fair
- images
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose Fair Mapping, a lightweight post-processing
  method to reduce bias in text-to-image diffusion models. Fair Mapping introduces
  a linear mapping network that projects text embeddings into a debiased space, balancing
  representation across sensitive groups like gender and race.
---

# Fair Text-to-Image Diffusion via Fair Mapping

## Quick Facts
- arXiv ID: 2311.17695
- Source URL: https://arxiv.org/abs/2311.17695
- Authors: 
- Reference count: 40
- Primary result: Introduces Fair Mapping, a lightweight post-processing method to reduce bias in text-to-image diffusion models via linear mapping of embeddings.

## Executive Summary
This paper presents Fair Mapping, a model-agnostic approach to reduce bias in text-to-image diffusion models by projecting text embeddings into a debiased space. The method employs a linear mapping network that learns to minimize both semantic consistency and fairness-based distance penalties, effectively balancing representation across sensitive groups like gender and race. Fair Mapping demonstrates superior performance in fairness metrics while maintaining high image quality and human preference, outperforming baseline approaches like Stable Diffusion, Structure Diffusion, and Composable Diffusion.

## Method Summary
Fair Mapping introduces a linear mapping network that transforms text embeddings into a debiased space before they are used by the diffusion model. The approach is trained to minimize a combined loss function that balances semantic consistency with fairness-based distance penalties. By freezing the pre-trained diffusion model parameters and only training the lightweight mapping network, Fair Mapping achieves debiasing without requiring full model retraining. The method is evaluated on 150 occupations and 20 emotions, using gender and race as sensitive attributes, and demonstrates improved demographic balance and diversity while maintaining image quality.

## Key Results
- Fair Mapping outperforms baselines in fairness metrics with improved demographic balance and diversity
- The method maintains high image quality and human preference despite slight CLIP-Score decline
- Fair Mapping is model-agnostic and lightweight, requiring minimal additional parameters and training time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fair Mapping reduces language bias by projecting text embeddings into a debiased space via a linear mapping network
- Mechanism: The linear mapping network learns to minimize the semantic distance between original and mapped embeddings while reducing the variance of distances to sensitive attribute embeddings, effectively neutralizing biased associations
- Core assumption: The language model's text embeddings encode sociocultural biases that can be disentangled and corrected through linear transformation without loss of semantic content
- Evidence anchors: [abstract] "By developing a linear network that maps conditioning embeddings into a debiased space, we enable the generation of relatively balanced demographic results based on the specified text condition." [section 3.1] "We apply the Fair Mapping architecture to transform the original embeddings, producing new representations denoted as: v = M (f ), vj = M (fj)."

### Mechanism 2
- Claim: Fair Mapping maintains image quality and alignment while improving demographic balance
- Mechanism: The combined loss function L = Ltext + λLf air balances semantic consistency (Ltext) with fairness (Lf air), allowing the model to generate diverse, high-quality images that align with human-related descriptions while reducing demographic bias
- Core assumption: The diffusion model's generation process is sufficiently robust to handle debiased embeddings without significant degradation in image quality or text-image alignment
- Evidence anchors: [abstract] "Comprehensive experiments show Fair Mapping outperforms baselines... in fairness metrics, with improved demographic balance and diversity while maintaining high image quality and human preference." [section 4.3] "While our method shows a slight decline in the CLIP-Score compared to the baselines, it performs superior performance in terms of the Human-CLIP metric."

### Mechanism 3
- Claim: Fair Mapping is model-agnostic and lightweight, requiring minimal additional parameters and training
- Mechanism: By freezing the pre-trained diffusion model parameters and only training a small linear mapping network, Fair Mapping achieves debiasing without the need for retraining the entire model or modifying its architecture
- Core assumption: The diffusion model's generation process is modular enough that conditioning embeddings can be modified independently without affecting the denoising process
- Evidence anchors: [abstract] "The approach is model-agnostic, requiring minimal additional parameters, and works by minimizing both semantic consistency and fairness-based distance penalties." [section 3.1] "To optimize the model, we freeze all parameters except for the additional Fair Mapping network." [section E.1] "During the training phase on a single Nvidia V100 device, our methodology exhibits remarkable efficiency by completing the entire process in a mere 50 minutes, encompassing 150 occupations in the sensitive attribute Gender."

## Foundational Learning

- Concept: Diffusion models and the denoising process
  - Why needed here: Understanding how text-to-image diffusion models generate images and how conditioning embeddings influence the process is crucial for grasping how Fair Mapping modifies the debiasing
  - Quick check question: What is the role of the noise prediction network ϵθ in the DDIM sampling process, and how does it use conditional information pt?

- Concept: Bias in language models and embeddings
  - Why needed here: Recognizing how language models encode sociocultural biases in text embeddings is essential for understanding why Fair Mapping's debiasing approach is necessary and how it works
  - Quick check question: How can Euclidean distance between word embeddings be used to quantify bias in language representations, and what does it mean if "doctor" is closer to male-associated embeddings?

- Concept: Fairness metrics and evaluation in generative models
  - Why needed here: Understanding how to measure and evaluate fairness in generated images, particularly in the context of demographic balance, is key to assessing the effectiveness of Fair Mapping
  - Quick check question: What is the Individual Fairness metric proposed in the paper, and how does it quantify fairness across different demographic groups in generated images?

## Architecture Onboarding

- Component map: Text encoder -> Fair Mapping linear network -> Diffusion model -> Detector
- Critical path:
  1. Input prompt processed by text encoder to obtain conditioning embeddings
  2. Fair Mapping linear network transforms embeddings to debiased space
  3. Debiased embeddings used as conditioning for diffusion model
  4. Diffusion model generates image using debiased conditioning
  5. Detector ensures appropriate activation of Fair Mapping based on prompt content

- Design tradeoffs:
  - Model-agnostic vs. potentially less effective than model-specific fine-tuning
  - Lightweight (fast, few parameters) vs. potentially limited debiasing capacity compared to more complex methods
  - Post-processing (preserves base model) vs. may not address all sources of bias (e.g., decoder biases)

- Failure signatures:
  - Degradation in image quality or text-image alignment (CLIP-Score drops significantly)
  - Inability to reduce bias variance (Lf air remains high despite training)
  - Detector fails to correctly identify sensitive prompts, leading to inappropriate activation/deactivation of Fair Mapping

- First 3 experiments:
  1. Train Fair Mapping on a subset of occupations with known gender bias, evaluate reduction in bias variance and maintenance of semantic consistency
  2. Compare generated images using Fair Mapping vs. baseline diffusion model on a held-out set of occupations, assess fairness metrics and image quality
  3. Perform ablation study on λ hyperparameter, find optimal balance between fairness and image quality for a specific occupation category

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Fair Mapping approach perform when applied to other text-to-image models beyond Stable Diffusion, Structure Diffusion, and Composable Diffusion?
- Basis in paper: [explicit] The paper mentions that Fair Mapping is model-agnostic and can be integrated into any text-to-image diffusion models
- Why unresolved: The paper only compares Fair Mapping with the three mentioned models, and it's unclear how it would perform with other models
- What evidence would resolve it: Conducting experiments with other text-to-image models and comparing the results with Fair Mapping would provide insights into its performance and generalizability

### Open Question 2
- Question: How does the Fair Mapping approach handle complex and nuanced prompts that involve multiple sensitive attributes or conflicting biases?
- Basis in paper: [inferred] The paper focuses on addressing biases related to gender and race, but it doesn't explicitly discuss how Fair Mapping handles complex prompts with multiple sensitive attributes or conflicting biases
- Why unresolved: The paper doesn't provide specific examples or experiments that test Fair Mapping's performance in such scenarios
- What evidence would resolve it: Conducting experiments with complex prompts involving multiple sensitive attributes or conflicting biases would help evaluate how Fair Mapping handles such cases and whether it can effectively mitigate biases in these situations

### Open Question 3
- Question: How does the Fair Mapping approach perform in generating images for prompts that involve intersectional identities, such as a specific occupation and a specific race?
- Basis in paper: [inferred] The paper focuses on addressing biases related to gender and race separately, but it doesn't explicitly discuss how Fair Mapping handles prompts that involve intersectional identities
- Why unresolved: The paper doesn't provide specific examples or experiments that test Fair Mapping's performance in generating images for prompts involving intersectional identities
- What evidence would resolve it: Conducting experiments with prompts that involve intersectional identities, such as a specific occupation and a specific race, would help evaluate how Fair Mapping performs in generating images that accurately represent these identities and whether it can effectively mitigate biases in such cases

## Limitations

- The linear mapping assumption may not capture all forms of bias entanglement in embedding space
- The slight CLIP-Score degradation suggests potential tradeoffs between fairness and semantic consistency
- The fairness evaluation framework may not capture all dimensions of bias or edge cases

## Confidence

- **High confidence**: Fair Mapping is model-agnostic and lightweight (minimal parameters, training time, and computational overhead clearly demonstrated)
- **Medium confidence**: Fair Mapping improves fairness metrics while maintaining image quality (supported by quantitative metrics but with small observed tradeoffs)
- **Medium confidence**: The linear mapping mechanism effectively reduces bias in text embeddings (mechanism is plausible but not directly validated beyond metric improvements)

## Next Checks

1. **Ablation on mapping network depth**: Test whether deeper non-linear mappings improve debiasing effectiveness compared to the current linear approach, to validate if the linear assumption is sufficient

2. **Bias transfer analysis**: Generate images for biased prompts before and after Fair Mapping, then analyze whether debiasing in the embedding space consistently translates to debiased visual representations across diverse demographic combinations

3. **Robustness to prompt complexity**: Evaluate Fair Mapping on prompts combining multiple sensitive attributes (e.g., "Black female doctor in India") to test whether the method scales to complex, intersectional bias scenarios beyond single-attribute evaluations