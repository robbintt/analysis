---
ver: rpa2
title: Online Continual Learning on Hierarchical Label Expansion
arxiv_id: '2308.14374'
source_url: https://arxiv.org/abs/2308.14374
tags:
- hierarchy
- classes
- learning
- task
- level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes hierarchical label expansion (HLE), a novel
  continual learning setup where models learn coarse-grained classes first, then fine-grained
  classes in expanding hierarchical levels. To address this setup, the authors propose
  a rehearsal-based method using pseudo-labeling to incorporate hierarchy information
  and flexible memory sampling to adaptively balance new and old class samples.
---

# Online Continual Learning on Hierarchical Label Expansion

## Quick Facts
- arXiv ID: 2308.14374
- Source URL: https://arxiv.org/abs/2308.14374
- Reference count: 40
- Primary result: Proposes PL-FMS method that outperforms SOTA on hierarchical continual learning by significant margins across four datasets

## Executive Summary
This paper introduces hierarchical label expansion (HLE), a novel continual learning setup where models learn coarse-grained classes first, then progressively learn fine-grained classes in expanding hierarchical levels. To address this setup, the authors propose a rehearsal-based method using pseudo-labeling to incorporate hierarchy information and flexible memory sampling to adaptively balance new and old class samples. Experiments on CIFAR100, Stanford-Cars, iNaturalist-19, and ImageNet-Hier100 demonstrate significant performance improvements over prior methods on both HLE and conventional continual learning setups.

## Method Summary
The paper proposes PL-FMS, a rehearsal-based continual learning method specifically designed for hierarchical label expansion. The method combines pseudo-labeling based memory management with flexible memory sampling. When memory is full, PL identifies samples whose removal would least harm multi-level classification accuracy by considering hierarchically related classes. FMS probabilistically includes stream samples based on task encounter time, gradually transitioning from memory-only to mixed training. The method uses a ResNet34 encoder with multiple hierarchy-specific classifiers and is evaluated against baseline methods including ER, EWC++, BiC, MIR, RM, GDumb, and CLIB.

## Key Results
- PL-FMS outperforms prior SOTA methods on HLE and conventional continual learning setups by significant margins
- Achieves best performance on CIFAR100, Stanford-Cars, iNaturalist-19, and ImageNet-Hier100 datasets
- Demonstrates effectiveness in exploiting hierarchical class relationships for knowledge expansion
- Shows strong performance under class imbalance situations

## Why This Works (Mechanism)

### Mechanism 1
Pseudo-labeling based memory management (PL) improves retention of hierarchical relationships by selecting samples whose removal would least harm multi-level classification accuracy. The method identifies the modal class, predicts most frequent classes at other hierarchy levels among samples of that modal class, then removes the sample with smallest loss importance among candidates.

### Mechanism 2
Flexible memory sampling (FMS) balances learning from new classes with preventing catastrophic forgetting by probabilistically including stream samples based on task encounter time. Stream samples are included with probability ρt(c) = min((t-Tc)/T, 1), creating a gradual transition from memory-only to mixed training.

### Mechanism 3
Hierarchical label expansion (HLE) setup better reflects real-world knowledge accumulation by structuring class learning from coarse to fine-grained levels. The setup divides classes into H hierarchy levels, with tasks expanding labels from level h to h+1, allowing the model to learn parent classes first, then their fine-grained subclasses.

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: The paper's entire premise addresses how models forget previously learned classes when learning new ones in continual learning settings
  - Quick check question: What is the primary challenge that continual learning methods like PL-FMS aim to solve?

- Concept: Rehearsal-based continual learning
  - Why needed here: PL-FMS is a rehearsal-based method that maintains a memory buffer of past samples to prevent forgetting during training
  - Quick check question: How does rehearsal-based learning differ from regularization-based approaches in continual learning?

- Concept: Hierarchical classification
  - Why needed here: The HLE setup and PL-FMS method both leverage hierarchical relationships between classes to improve learning efficiency and knowledge retention
  - Quick check question: Why might hierarchical relationships between classes be useful in continual learning scenarios?

## Architecture Onboarding

- Component map: ResNet34 encoder → Multiple hierarchy-specific classifiers {gh}H h=1 → Memory buffer → Pseudo-labeling module → Flexible sampling module
- Critical path: Stream data → Memory management (PL) → Batch formation (FMS) → Forward pass through hierarchy-specific classifiers → Loss computation → Parameter update
- Design tradeoffs: Memory buffer size vs. forgetting rate (larger memory = less forgetting but more resource usage); T hyperparameter in FMS vs. adaptation speed (smaller T = faster adaptation but potential instability)
- Failure signatures: Performance degradation at task boundaries, especially between coarse and fine-grained transitions; inconsistent performance across hierarchy levels; high variance in any-time inference results
- First 3 experiments:
  1. Run PL-FMS on CIFAR100 with single-depth hierarchy, varying memory sizes (1000, 2000, 5000) to observe forgetting patterns
  2. Test different T values in FMS (500, 5000, 15000) on CIFAR100 to find optimal adaptation speed
  3. Compare single-label vs. dual-label scenarios on ImageNet-Hier100 to measure hierarchy exploitation effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed PL-FMS method perform on continual learning tasks with deeper hierarchical structures than those tested in the paper? The paper only evaluates PL-FMS on datasets with up to 7 levels of hierarchy. Testing on datasets with deeper hierarchies would require constructing or finding new datasets.

### Open Question 2
How does the performance of PL-FMS change when the imbalance ratio of the dataset is significantly higher than those tested in the paper? The paper only tests PL-FMS on datasets with imbalance ratios up to 31. Testing on datasets with significantly higher imbalance ratios would require constructing or finding new datasets.

### Open Question 3
How does the performance of PL-FMS change when the memory size is significantly smaller or larger than those tested in the paper? The paper only tests PL-FMS with a fixed set of memory sizes. Testing with significantly smaller or larger memory sizes would require rerunning the experiments with different memory sizes.

## Limitations

- Dataset construction details for ImageNet-Hier100 are not fully specified, making exact replication challenging
- Hyperparameter sensitivity is not systematically analyzed, potentially limiting robustness claims
- Scalability claims lack analysis of performance on larger, more complex hierarchical structures

## Confidence

- High confidence: The general framework of combining pseudo-labeling with flexible sampling for hierarchical continual learning is sound and well-motivated
- Medium confidence: The experimental results showing performance improvements over baselines, as specific dataset constructions could influence outcomes
- Low confidence: Claims about PL-FMS being specifically suited for "realistic continual learning scenarios" without broader testing on diverse, real-world hierarchical datasets

## Next Checks

1. Reproduce ImageNet-Hier100 construction using publicly available WordNet or similar hierarchies to verify if the 10×10 structure captures meaningful semantic relationships
2. Systematically vary T in FMS (e.g., 100, 1000, 5000, 10000) and memory sizes to assess sensitivity and identify ranges where performance remains stable
3. Test PL-FMS on additional hierarchical datasets like Food-101 or Caltech-UCSD Birds to evaluate whether the method's effectiveness extends beyond the four datasets presented