---
ver: rpa2
title: Wasserstein Auto-Encoders of Merge Trees (and Persistence Diagrams)
arxiv_id: '2307.02509'
source_url: https://arxiv.org/abs/2307.02509
tags:
- merge
- trees
- ensemble
- each
- persistence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first non-linear auto-encoding framework
  for topological descriptors, specifically merge trees and persistence diagrams.
  The proposed Wasserstein Auto-Encoder of Merge Trees (MT-WAE) extends classical
  auto-encoders by operating directly on merge trees via novel BDT transformation
  layers, without requiring pre-vectorization.
---

# Wasserstein Auto-Encoders of Merge Trees (and Persistence Diagrams)

## Quick Facts
- arXiv ID: 2307.02509
- Source URL: https://arxiv.org/abs/2307.02509
- Authors: 
- Reference count: 40
- Key outcome: This paper introduces the first non-linear auto-encoding framework for topological descriptors, specifically merge trees and persistence diagrams. The proposed Wasserstein Auto-Encoder of Merge Trees (MT-WAE) extends classical auto-encoders by operating directly on merge trees via novel BDT transformation layers, without requiring pre-vectorization. This results in improved accuracy and interpretability over linear methods. The framework is evaluated on 12 public ensembles with running times in minutes, demonstrating superior reconstruction error (37% improvement for persistence diagrams, 52% for merge trees) and better preservation of metric and cluster structures in dimensionality reduction applications. The approach enables new visualization capabilities such as interactive latent space exploration and feature importance tracking, advancing statistical analysis of ensemble data for scientific visualization.

## Executive Summary
This paper presents a novel neural network framework that performs non-linear auto-encoding of merge trees and persistence diagrams, the fundamental topological descriptors used in scientific visualization. Unlike previous linear methods that require vectorization of these descriptors, the proposed approach operates directly on the Wasserstein metric space, preserving the intrinsic structure of the data. The framework introduces BDT (Basis for the Deformation of Trees) transformation layers that project merge trees onto learned bases while maintaining the Elder rule constraint. This enables more accurate reconstruction and better preservation of metric and cluster structures compared to linear methods. The approach also enables new visualization capabilities such as interactive latent space exploration and feature importance tracking.

## Method Summary
The method extends classical auto-encoders to operate on merge trees and persistence diagrams using the Wasserstein distance as the fundamental metric. It introduces BDT transformation layers that project trees onto learned bases in the Wasserstein space, avoiding the need for vectorization. The network is trained to minimize reconstruction error measured by Wasserstein distance, with optional penalty terms for preserving metric and cluster structures in the latent space. The framework is evaluated on 12 public ensemble datasets containing piecewise linear scalar fields on PL d-manifolds (d ≤ 3), demonstrating improved reconstruction accuracy and better preservation of topological relationships compared to linear methods.

## Key Results
- 37% improvement in reconstruction error for persistence diagrams compared to linear methods
- 52% improvement in reconstruction error for merge trees compared to linear methods
- Better preservation of metric and cluster structures in dimensionality reduction applications
- Running times in minutes for all tested ensembles
- Enables interactive latent space exploration and feature importance tracking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The non-linear auto-encoder framework preserves merge tree structure more accurately than linear methods because it operates directly on the Wasserstein metric space rather than requiring pre-vectorization.
- Mechanism: By defining BDT transformation layers that project merge trees onto BDT bases using the Wasserstein distance as the projection metric, the framework maintains the topological relationships and structural constraints (Elder rule) throughout the encoding process. This avoids quantization and linearization errors introduced by vectorization.
- Core assumption: The Wasserstein distance between merge trees (W_T^2) provides a meaningful and stable metric for comparing merge trees that captures their structural similarity.
- Evidence anchors:
  - [abstract] "Our novel neural network approach can be interpreted as a non-linear generalization of previous linear attempts [79] at merge tree encoding."
  - [section 3.2] "Specifically, we formalize the following notions: BDT vector (Fig. 6b); BDT basis (Fig. 6d); BDT basis projection (Fig. 6d); BDT transformation layer (Fig. 7b)."
- Break condition: If the Wasserstein distance between merge trees becomes unstable under noise or if the BDT basis projection becomes computationally intractable for large merge trees.

### Mechanism 2
- Claim: The assignment/update algorithm for BDT basis projection efficiently optimizes the projection coefficients by alternating between assignment optimization and coefficient updates.
- Mechanism: The algorithm first computes the optimal assignment φ* between the input BDT B and its estimation bB = O + B(O)α, then updates α to minimize ||B' - bB'||² using the pseudoinverse of the BDT basis matrix. This iterative process converges to optimal coefficients.
- Core assumption: The optimal assignment between BDTs can be computed efficiently and the pseudoinverse-based update provides a stable and convergent solution.
- Evidence anchors:
  - [section 4.2] "At this stage, the estimation bB can be updated with the above optimized coefficients α*: bB ← O + B(O)α*."
  - [section 4.2] "The above Assignment/Update sequence is then iterated. Each iteration decreases the projection error e(B) constructively."
- Break condition: If the optimal assignment becomes ambiguous due to structural similarities between BDTs, or if the pseudoinverse computation becomes numerically unstable.

### Mechanism 3
- Claim: The initialization strategy using Wasserstein barycenters and worst-case projections creates informative basis vectors that capture the ensemble's variability.
- Mechanism: The input origin O_in^k is initialized as the Wasserstein barycenter of the input BDTs, and basis vectors are added sequentially by selecting the BDT that maximizes the projection error given the current basis. This ensures the basis spans the directions of greatest variability.
- Core assumption: The Wasserstein barycenter provides a meaningful central tendency for the ensemble, and selecting worst-case projections ensures the basis captures the ensemble's diversity.
- Evidence anchors:
  - [section 4.3] "For each BDT transformation layer Πk, its input origin O_in^k is initialized as the Wasserstein barycenter B* [71] of the BDTs on its input."
  - [section 4.3] "Next, the first vector of B_in^k, is given by the optimal assignment (w.r.t. Eq. 1) between O_in^k and the layer's input BDT B which maximizes W_T^2 (O_in^k, B)."
- Break condition: If the Wasserstein barycenter is ill-defined or unstable, or if the worst-case BDTs are not representative of the ensemble's variability.

## Foundational Learning

- Concept: Wasserstein distance and optimal transport
  - Why needed here: The framework relies on Wasserstein distances as the fundamental metric for comparing and manipulating merge trees and persistence diagrams. Understanding optimal transport is crucial for grasping how the framework measures distances and computes barycenters.
  - Quick check question: How does the Wasserstein distance between two persistence diagrams differ from the L² distance between their vectorized representations?

- Concept: Topological data analysis and merge trees
  - Why needed here: The framework operates on merge trees as its primary data structure. Understanding what merge trees represent, how they capture topological features of scalar fields, and how they differ from persistence diagrams is essential for understanding the framework's applications and limitations.
  - Quick check question: What is the relationship between a merge tree and the sublevel set filtration of a scalar field?

- Concept: Neural network optimization and automatic differentiation
  - Why needed here: The framework uses neural network techniques for optimization, including gradient descent and automatic differentiation. Understanding these concepts is crucial for grasping how the framework learns to encode and decode merge trees.
  - Quick check question: How does automatic differentiation enable efficient computation of gradients for complex neural network architectures?

## Architecture Onboarding

- Component map: Input layer -> Encoding layers (BDT transformation layers) -> Latent space -> Decoding layers (BDT transformation layers) -> Output layer
- Critical path:
  1. Initialize BDT transformation layers using Wasserstein barycenters and worst-case projections
  2. Forward propagate input BDTs through the network
  3. Compute reconstruction error using Wasserstein distance
  4. Backward propagate gradients using automatic differentiation
  5. Update network parameters using gradient descent
  6. Iterate until convergence

- Design tradeoffs:
  - Non-linearity vs. interpretability: The non-linear framework provides better accuracy but makes the encoding process less interpretable than linear methods
  - Computational cost vs. accuracy: More layers and dimensions improve accuracy but increase computational cost
  - Stability vs. discriminativity: The ε1 parameter balances the practical stability of the Wasserstein metric with its ability to discriminate between similar merge trees

- Failure signatures:
  - High reconstruction error despite convergence: Indicates poor initialization or inappropriate network architecture
  - Unstable energy during training: Suggests numerical instability in the assignment/update algorithm or gradient computation
  - Poor preservation of clusters or metric: Indicates insufficient penalty terms or inappropriate blending weights

- First 3 experiments:
  1. Implement and test the BDT basis projection algorithm on a simple synthetic ensemble to verify the assignment/update mechanism
  2. Create a small network with one encoding and one decoding layer to test the overall framework on a simple ensemble
  3. Add penalty terms to test their effect on cluster and metric preservation in the latent space

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical upper bound on the compression ratio achievable with MT-WAE while maintaining acceptable reconstruction error?
- Basis in paper: [inferred] The paper discusses compression factors in Section 5.1 but doesn't establish theoretical limits on how much the data can be compressed before reconstruction quality degrades significantly.
- Why unresolved: The experiments show practical compression ratios but don't provide theoretical analysis of the relationship between compression factor and reconstruction error.
- What evidence would resolve it: A mathematical proof or empirical study establishing the maximum achievable compression ratio while keeping reconstruction error below a certain threshold.

### Open Question 2
- Question: How does the performance of MT-WAE scale with the number of layers (ne + nd) in the network?
- Basis in paper: [inferred] Section 4.3 mentions meta-parameters including layer numbers, but doesn't provide systematic analysis of how performance changes with different layer configurations.
- Why unresolved: The paper uses a fixed number of layers (ne = nd = 1) for experiments but doesn't explore how performance scales with more complex architectures.
- What evidence would resolve it: Comprehensive experiments testing MT-WAE with varying numbers of layers, showing how reconstruction error, computational time, and other metrics change.

### Open Question 3
- Question: What is the impact of different activation functions beyond Leaky ReLU on the quality of MT-WAE reconstructions?
- Basis in paper: [explicit] Section 3.2 mentions using Leaky ReLU as the activation function but doesn't explore alternatives.
- Why unresolved: The choice of activation function could significantly impact the network's ability to model non-linear relationships in merge trees.
- What evidence would resolve it: Comparative experiments using different activation functions (ReLU, sigmoid, tanh, etc.) to measure their impact on reconstruction accuracy and latent space quality.

### Open Question 4
- Question: How does MT-WAE perform on merge trees with significantly different sizes (varying number of branches)?
- Basis in paper: [inferred] Section 6.1 discusses running times but doesn't analyze how tree size affects reconstruction quality or computational efficiency.
- Why unresolved: The paper uses ensembles with relatively similar tree sizes, so it's unclear how well the framework generalizes to cases with highly variable tree sizes.
- What evidence would resolve it: Experiments using ensembles with merge trees of widely varying sizes, measuring reconstruction error and computational time across this range.

## Limitations
- The framework's scalability to large-scale ensembles remains untested beyond the 12 public datasets, with computational complexity potentially becoming prohibitive for higher-dimensional data or larger numbers of ensemble members.
- The stability of the Wasserstein metric for merge trees under noise and perturbations is assumed but not rigorously validated across diverse noise models and data distributions.
- The generalization capability of the learned encodings to unseen ensemble structures or topologies has not been evaluated, raising questions about the framework's robustness to distributional shifts.

## Confidence
- High confidence in the reconstruction accuracy claims (37% improvement for persistence diagrams, 52% for merge trees) due to direct quantitative comparisons with linear methods on the same datasets.
- Medium confidence in the preservation of metric and cluster structures, as the evaluation relies on specific metrics (SIM, NMI, ARI) that may not capture all aspects of topological structure preservation.
- Medium confidence in the framework's ability to handle complex real-world ensembles, as the evaluation is limited to 12 public datasets with specific characteristics.

## Next Checks
1. Test the framework's scalability and performance on synthetic ensembles with controlled variations in size, dimensionality, and noise levels to establish computational complexity bounds and stability guarantees.
2. Evaluate the generalization capability by training on one ensemble and testing on structurally different ensembles, measuring reconstruction error and structural preservation metrics.
3. Conduct ablation studies to isolate the contribution of each component (non-linearity, BDT transformation layers, penalty terms) to overall performance, identifying potential over-engineering or redundant elements.