---
ver: rpa2
title: Adversarial Attacks to Latent Representations of Distributed Neural Networks
  in Split Computing
arxiv_id: '2309.17401'
source_url: https://arxiv.org/abs/2309.17401
tags:
- latent
- adversarial
- attacks
- dnns
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates adversarial attacks to latent representations
  in distributed deep neural networks (DNNs). It theoretically proves that latent
  features are more robust than input representations under the same information distortion
  level, and that smaller latent dimensions enhance robustness by reducing variance
  but may introduce bias affecting generalization.
---

# Adversarial Attacks to Latent Representations of Distributed Neural Networks in Split Computing

## Quick Facts
- arXiv ID: 2309.17401
- Source URL: https://arxiv.org/abs/2309.17401
- Reference count: 15
- Attacks in latent space achieve 57% lower success rates on average compared to input space attacks

## Executive Summary
This paper investigates adversarial attacks targeting latent representations in distributed deep neural networks (DNNs) with split computing architectures. The study theoretically proves that latent representations are more robust than input representations under identical information distortion levels, and demonstrates that smaller latent dimensions enhance robustness by reducing variance while potentially introducing bias that affects generalization. Through extensive experiments on ImageNet-1K using 10 adversarial attacks, 6 DNN architectures, and 6 compression approaches, the research shows that latent space attacks achieve 57% lower success rates on average compared to input space attacks, with up to 88% reduction in the best case. The findings highlight the fundamental trade-off between robustness and performance in distributed DNN systems.

## Method Summary
The study evaluates adversarial attacks on both input and latent representations in distributed DNNs. Six DNN architectures (VGG16, ResNet50, ResNet152, and their fc variants) are tested with six compression approaches including supervised compression, knowledge distillation, and quantization. Ten adversarial attacks spanning white-box (FGSM, BIM, MIM, PGD) and black-box methods (NES, N-Attack, Square Attack, EVO, S-OPT, Triangle Attack) are implemented and evaluated under different perturbation budgets on the ImageNet-1K validation set. The robustness is measured through attack success rates, comparing attacks applied directly to inputs versus those targeting compressed latent representations.

## Key Results
- Latent space attacks achieve 57% lower success rates on average compared to input space attacks
- Best case shows up to 88% reduction in attack success rates when targeting latent representations
- Smaller latent dimensions enhance robustness by reducing variance but may introduce bias affecting generalization
- Compression approaches show varying levels of robustness, with some methods providing significantly better protection than others

## Why This Works (Mechanism)

### Mechanism 1
- Claim: With the same level of information distortion, latent representations are always more robust than input representations
- Mechanism: The conditional mutual information I(X; Y |T) captures residual information between input X and label Y not captured by latent representation T. Adversarial perturbations increase this residual information more in input space than latent space due to the Data Processing Inequality
- Core assumption: Adversarial perturbations are not observable and have similar magnitude in both input and latent spaces
- Evidence anchors: [abstract] reports 57% lower success rates; [section 3.3] provides theoretical proof using DPI; [corpus] shows weak direct citations

### Mechanism 2
- Claim: Adversarial robustness is jointly determined by feature dimension and generalization capability
- Mechanism: Smaller latent dimensions reduce attacker's search space (O(|T||Y|/√n)), improving robustness by reducing variance, but this introduces bias affecting generalization, creating a trade-off
- Core assumption: Mutual information error bound O(|T||Y|/√n) accurately captures relationship between latent dimension size and robustness
- Evidence anchors: [abstract] mentions trade-off; [section 3.2] shows performance is jointly determined by I*(Y; T) and O(|T||Y|/√n); [corpus] provides weak direct validation

### Mechanism 3
- Claim: Compressed latent representations reduce attack success rates by 88% in best case and 57% on average
- Mechanism: Compression layer acts as bottleneck filtering noise and irrelevant information, making it harder for adversarial perturbations to affect classification decisions
- Core assumption: Compression approaches effectively learn representations balancing compression and task-relevant information
- Evidence anchors: [abstract] reports success rate reductions; [section 5.2] shows consistent robustness trends; [corpus] provides moderate evidence of similar improvements

## Foundational Learning

- Concept: Information Bottleneck (IB) Theory
  - Why needed here: Provides theoretical framework for understanding how latent representations capture task-relevant information while compressing input data
  - Quick check question: How does the IB objective I(X; T) - β·I(Y; T) balance compression and generalization?

- Concept: Data Processing Inequality (DPI)
  - Why needed here: Establishes fundamental relationship between information content at different DNN pipeline stages, crucial for proving latent space robustness
  - Quick check question: What does DPI tell us about the relationship between I(Y; X), I(Y; T), and I(Y; Ŷ)?

- Concept: Mutual Information and Kullback-Leibler Divergence
  - Why needed here: Quantifies information distortion and robustness independent of specific attack algorithms or DNN architectures
  - Quick check question: How does residual information I(X; Y|T) relate to KL divergence DKL[P(Y|X)||P(Y|T)]?

## Architecture Onboarding

- Component map: Mobile DNN (feature extraction) → Compression layer (bottleneck) → Communication channel → Local DNN (classification)
- Critical path: Input → Mobile DNN → Compression → Transmission → Local DNN → Output
- Design tradeoffs: Compression ratio vs accuracy vs robustness; computational complexity at edge vs cloud; communication overhead vs security
- Failure signatures: High attack success rates in latent space; significant accuracy degradation after compression; communication bottlenecks
- First 3 experiments:
  1. Measure ASR of PGD attack on input vs latent space with varying compression ratios (ε = 0.003, 0.01, 0.03)
  2. Compare robustness across different compression approaches (SC, KD, BF, ES) while keeping compression ratio constant
  3. Evaluate impact of latent dimension size on both robustness and accuracy using ResNet152-fc with channels = 12 vs 3

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does information distortion level (measured by KL divergence) quantitatively relate to success rate of adversarial attacks in latent space?
- Basis in paper: [explicit] Paper states latent representations are more robust under same information distortion but doesn't provide quantitative relationship between KL divergence and attack success rate
- Why unresolved: Paper establishes theoretical relationship but doesn't empirically validate or quantify how different KL divergence levels correspond to attack success rates
- What evidence would resolve it: Empirical measurements of attack success rates at varying KL divergence levels across multiple attack algorithms and architectures

### Open Question 2
- Question: What is optimal trade-off between feature compression ratio and adversarial robustness in distributed DNNs?
- Basis in paper: [explicit] Paper mentions feature compression layer enhances robustness by reducing variance but also introduces vulnerability through bias, and shows results with different channel sizes, but doesn't provide systematic framework for finding optimal trade-off
- Why unresolved: While paper demonstrates existence of trade-off, it doesn't provide guidance on how to find optimal point balancing compression, robustness, and accuracy
- What evidence would resolve it: Systematic study mapping compression ratios to both robustness and accuracy across different architectures and datasets

### Open Question 3
- Question: How do different types of adversarial attacks (gradient-based vs. score-based vs. decision-based) interact with different feature compression approaches?
- Basis in paper: [explicit] Paper tests multiple attack types against various compression approaches but doesn't provide comprehensive analysis of how attack characteristics interact with compression method properties
- Why unresolved: Experimental results show varying success rates but don't explain why certain attacks are more effective against specific compression methods
- What evidence would resolve it: Detailed analysis of attack success rates broken down by compression method and attack type, potentially revealing patterns in attack-method compatibility

## Limitations

- Theoretical bounds assume idealized conditions that may not hold in practical scenarios, particularly regarding independence assumptions between perturbations and model components
- Study focuses primarily on image classification tasks, limiting generalizability to other domains like NLP or time-series data
- Implementation details for some black-box attacks are not fully specified, which could affect reproducibility of exact success rates
- Analysis does not account for adaptive attackers who may specifically target compression layer's weaknesses

## Confidence

- **High Confidence**: Claims about average 57% reduction in attack success rates are well-supported by experimental results across multiple attack types and architectures
- **Medium Confidence**: Theoretical bounds relating mutual information to robustness are mathematically sound but may be loose in practice
- **Medium Confidence**: Trade-off between compression ratio and robustness is empirically validated but requires more extensive hyperparameter tuning studies

## Next Checks

1. **Architecture Transferability Test**: Validate robustness claims on non-image datasets (e.g., text classification or tabular data) using same compression-robustness framework to assess domain generalizability

2. **Adaptive Attack Evaluation**: Implement white-box attacks specifically designed to exploit compression layer's reconstruction bias and measure whether reported robustness improvements hold against adaptive adversaries

3. **Hyperparameter Sensitivity Analysis**: Systematically vary compression ratios, latent dimensions, and perturbation budgets to map full robustness-accuracy trade-off landscape and identify optimal operating points for different use cases