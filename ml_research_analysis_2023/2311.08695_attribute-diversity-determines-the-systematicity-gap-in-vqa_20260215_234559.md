---
ver: rpa2
title: Attribute Diversity Determines the Systematicity Gap in VQA
arxiv_id: '2311.08695'
source_url: https://arxiv.org/abs/2311.08695
tags:
- rubber
- lxmert
- cylinder
- test
- systematicity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the systematicity gap in VQA models, which
  refers to the performance difference between reasoning on previously seen and unseen
  combinations of object attributes. To test this, the authors introduce CLEVR-HOPE,
  a novel diagnostic dataset based on CLEVR.
---

# Attribute Diversity Determines the Systematicity Gap in VQA

## Quick Facts
- arXiv ID: 2311.08695
- Source URL: https://arxiv.org/abs/2311.08695
- Reference count: 40
- Primary result: Attribute diversity in training data reduces systematicity gap in VQA models, not training quantity.

## Executive Summary
This paper investigates the systematicity gap in VQA models—the performance difference between reasoning on seen and unseen attribute combinations. The authors introduce CLEVR-HOPE, a diagnostic dataset based on CLEVR that tests systematic generalization by holding out specific object attribute pairs from both visual and textual modalities during training. Through extensive experiments with LXMERT and Tensor-NMN architectures, they demonstrate that increasing training data quantity does not reduce the systematicity gap, but increasing attribute diversity does. This finding suggests that models learn compositional rules when exposed to diverse attribute combinations, enabling systematic generalization to unseen pairs.

## Method Summary
The authors created CLEVR-HOPE, a controlled diagnostic dataset consisting of 29 sub-datasets, each corresponding to a held-out pair (HOP) of object attribute values. For each HOP, they generated train, complex-IID test, complex-OOD test, minimal-IID test, and minimal-OOD test sets. They evaluated two VQA architectures (LXMERT and Tensor-NMN) on these datasets with varying training set sizes (25K, 200K, and 560K samples) and measured the systematicity gap between IID and OOD test performance. The key experimental manipulation was training data diversity—the number of distinct attribute type combinations seen during training.

## Key Results
- The systematicity gap plateaus at 5-6% drop even with 560K training samples, showing quantity doesn't help
- Increasing attribute diversity in training data significantly reduces the systematicity gap
- Models exhibit systematic behavior when exposed to more distinct attribute type combinations during training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing attribute diversity in training data reduces the systematicity gap.
- Mechanism: When a model is exposed to more distinct combinations of attribute types (e.g., more MATERIAL-SHAPE pairs), it learns the underlying rules governing how these attributes interact. This learned rule-based understanding allows the model to generalize systematically to unseen combinations.
- Core assumption: The model can extract and internalize compositional rules from diverse training data.
- Evidence anchors:
  - [abstract] "we find that the systematicity gap is not reduced by increasing the quantity of training data, but is reduced by increasing the training data diversity of the attributes in the unseen combination"
  - [section 4.3] "we can conclude that the models consistently exhibit at least some degree of systematic behaviour"
- Break condition: If the model relies on memorization rather than rule extraction, or if attribute interactions are too complex for simple compositional rules.

### Mechanism 2
- Claim: The systematicity gap plateaus with increased training data quantity.
- Mechanism: Simply providing more examples of the same attribute combinations doesn't teach the model new compositional strategies. The model hits a performance ceiling because it's not learning new ways to combine attributes.
- Core assumption: Additional examples of known combinations don't provide new compositional insights.
- Evidence anchors:
  - [abstract] "while systematicity does not improve with more training data, it does improve with more diverse training data"
  - [section 4.2] "the systematicity gap plateaus to a drop of 5-6%"
- Break condition: If the model architecture changes to incorporate explicit compositional reasoning mechanisms.

### Mechanism 3
- Claim: CLEVR-HOPE's controlled setting allows isolation of systematicity effects.
- Mechanism: By holding out specific attribute pairs from both visual and textual modalities during training, the dataset ensures that any generalization to these pairs must be systematic rather than due to prior exposure.
- Core assumption: The controlled environment accurately simulates real-world systematic generalization challenges.
- Evidence anchors:
  - [section 2] "CLEVR-HOPE is a controlled setting to test whether VQA models generalize to pairs of attribute values that were not seen during either training or fine-tuning"
- Break condition: If the synthetic nature of CLEVR-HOPE doesn't translate to real-world scenarios.

## Foundational Learning

- Concept: Compositional generalization
  - Why needed here: Understanding how models combine learned concepts to handle novel situations is central to the systematicity gap investigation.
  - Quick check question: Can you explain the difference between memorization and compositional generalization?

- Concept: Attribute-value pairs
  - Why needed here: The systematicity gap specifically concerns how models handle unseen combinations of object attributes.
  - Quick check question: How would you represent the attribute pair "rubber cylinder" in terms of its component attributes?

- Concept: Distribution shift
  - Why needed here: The IID vs. OOD test sets in CLEVR-HOPE represent different data distributions, which is crucial for understanding systematicity.
  - Quick check question: What's the key difference between IID and OOD data in the context of CLEVR-HOPE?

## Architecture Onboarding

- Component map:
  LXMERT (pretrained object detector + multi-modal transformer) -> Tensor-NMN (program generator + execution engine) -> CLEVR-HOPE dataset

- Critical path:
  1. Load and preprocess CLEVR-HOPE dataset
  2. Configure model (LXMERT or Tensor-NMN) with appropriate hyperparameters
  3. Train model on various training set sizes and diversity levels
  4. Evaluate on IID and OOD test sets
  5. Analyze systematicity gap across different HOPs and training conditions

- Design tradeoffs:
  - LXMERT uses pretrained components (faster training, potential bias) vs. Tensor-NMN trains from scratch (more control, slower)
  - CLEVR-HOPE's synthetic nature provides control but may limit real-world applicability
  - Large training sets provide better overall performance but don't reduce systematicity gap

- Failure signatures:
  - High IID accuracy but low OOD accuracy indicates lack of systematic generalization
  - No improvement in systematicity gap with increased training data suggests model relies on memorization
  - Inconsistent performance across HOPs of similar diversity may indicate implementation issues

- First 3 experiments:
  1. Train LXMERT on minimal training set (25K samples) and evaluate on a single HOP's IID and OOD sets
  2. Compare LXMERT and Tensor-NMN performance on the same HOP with 560K training samples
  3. Analyze systematicity gap trends across all HOPs for LXMERT trained on 560K samples, stratified by diversity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the systematicity gap in VQA models change when using more complex attribute combinations (e.g., three or more attributes)?
- Basis in paper: The authors studied the systematicity gap for pairs of held-out attributes but did not explore more complex combinations.
- Why unresolved: The current dataset and analysis only cover pairs of attributes, leaving open the question of how models generalize to more complex combinations.
- What evidence would resolve it: Extending CLEVR-HOPE to include test sets with three or more held-out attributes and measuring the systematicity gap for these combinations.

### Open Question 2
- Question: Does the relationship between attribute diversity and systematicity generalize to other types of compositional generalization beyond attribute pairs?
- Basis in paper: The authors found that attribute diversity affects systematicity for attribute pairs but did not explore other types of compositional generalization.
- Why unresolved: The study focused specifically on attribute pairs, and it's unclear whether the same principles apply to other forms of compositionality.
- What evidence would resolve it: Conducting similar experiments on other types of compositional generalization, such as novel question structures or combinations of question-answer pairs.

### Open Question 3
- Question: How do different architectural choices (e.g., attention mechanisms, training objectives) affect the systematicity gap in VQA models?
- Basis in paper: The authors primarily studied LXMERT and Tensor-NMN, but did not extensively explore how architectural differences impact systematicity.
- Why unresolved: The study used a limited set of architectures, and it's unclear how different design choices might influence the ability to generalize systematically.
- What evidence would resolve it: Experimenting with various VQA architectures and comparing their systematicity gaps on CLEVR-HOPE.

## Limitations

- The synthetic nature of CLEVR-HOPE may not translate to real-world VQA scenarios with more complex visual scenes
- The study only examines two model architectures, potentially missing architectural factors that influence systematicity
- The analysis doesn't investigate whether models develop explicit compositional representations or rely on implicit statistical patterns

## Confidence

The systematicity gap analysis is **high confidence** for the core finding that attribute diversity—not training quantity—drives systematic generalization, given consistent results across 29 held-out pairs and two architectures. However, confidence is **medium** for the claim that compositional rules are being learned rather than dataset-specific heuristics, since the study doesn't directly probe the learned representations. The **low confidence** area concerns real-world applicability: CLEVR-HOPE's synthetic nature and restricted attribute space (7 object colors, shapes, materials) may not capture the complexity of natural images.

## Next Checks

1. Replicate key findings using real-world VQA datasets with controlled attribute pair holds-outs to assess external validity.
2. Analyze learned representations (e.g., probing classifier on attribute embeddings) to determine if models encode explicit compositional rules.
3. Test additional architectures with explicit compositional reasoning mechanisms (e.g., neural module networks) to isolate architectural contributions to systematicity.