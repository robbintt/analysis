---
ver: rpa2
title: On the Effectiveness of Unlearning in Session-Based Recommendation
arxiv_id: '2312.14447'
source_url: https://arxiv.org/abs/2312.14447
tags:
- unlearning
- recommendation
- data
- session-based
- session
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of implementing machine unlearning
  in session-based recommendation systems, where existing methods struggle to completely
  remove the influence of unlearned samples due to collaborative correlations and
  sequential connections within sessions. The authors propose SRU, a framework that
  partitions training sessions into sub-models based on similarity, uses an attention-based
  aggregation layer to combine hidden states, and employs three extra data deletion
  strategies (collaborative, neighbor, and random) to enhance unlearning effectiveness.
---

# On the Effectiveness of Unlearning in Session-Based Recommendation

## Quick Facts
- **arXiv ID**: 2312.14447
- **Source URL**: https://arxiv.org/abs/2312.14447
- **Reference count**: 40
- **Primary result**: SRU framework outperforms baselines in both recommendation performance and unlearning effectiveness while requiring significantly less time than full retraining.

## Executive Summary
This paper addresses the challenge of implementing machine unlearning in session-based recommendation systems, where existing methods struggle to completely remove the influence of unlearned samples due to collaborative correlations and sequential connections within sessions. The authors propose SRU, a framework that partitions training sessions into sub-models based on similarity, uses an attention-based aggregation layer to combine hidden states, and employs three extra data deletion strategies (collaborative, neighbor, and random) to enhance unlearning effectiveness. Experiments on three benchmark datasets show that SRU outperforms baselines like SISA in both recommendation performance and unlearning effectiveness, with the proposed evaluation metric demonstrating reduced inference of unlearned items. The method also achieves efficient unlearning, requiring significantly less time than full retraining.

## Method Summary
SRU addresses unlearning in session-based recommendation by first partitioning sessions into shards based on similarity using k-means clustering on pre-trained session embeddings. Each shard trains an independent sub-model, and an attention-based aggregation layer combines their outputs according to session-centroid correlations. During unlearning requests, the framework employs three extra deletion strategies (CED, NED, RED) to remove additional items beyond the target, breaking indirect inference paths. Only the affected shard's sub-model and the aggregation layer require retraining, achieving efficient unlearning compared to full retraining.

## Key Results
- SRU outperforms SISA baseline in both recommendation performance (higher NDCG@5 and Recall@5) and unlearning effectiveness (lower HIT@5 scores)
- The proposed evaluation metric (HIT@K) demonstrates reduced inference of unlearned items after unlearning
- Unlearning with SRU requires significantly less time than full retraining while maintaining competitive recommendation quality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Partitioning sessions by similarity reduces redundant learning across unrelated sessions.
- **Mechanism**: Sessions are first embedded via a pretrained model, then clustered with k-means. Each shard contains sessions that are close in the embedding space, so sub-models learn similar sequential patterns and avoid interference.
- **Core assumption**: Session embeddings capture meaningful similarity for downstream recommendation.
- **Evidence anchors**:
  - [section] "session similarity is important for recommendation accuracy" → similarity-based clustering
  - [abstract] "partition the training sessions into separate sub-models according to the similarity across the sessions"
- **Break condition**: If session embeddings are noisy or collapse into few clusters, shards become heterogeneous and benefit disappears.

### Mechanism 2
- **Claim**: Attention-based aggregation selectively reweights hidden states according to how relevant each shard is for the query session.
- **Mechanism**: After each sub-model produces a hidden state, a projection layer maps them into a common space. Attention scores are computed from the session representation and the shard centroid, then used to weight each projected hidden state before final prediction.
- **Core assumption**: Session-centroid similarity correlates with the usefulness of the shard for that session.
- **Evidence anchors**:
  - [abstract] "attention-based aggregation layer to fuse the hidden states according to the correlations between the session and the centroid of the data in the sub-model"
  - [section] Attention score defined as "g ·ReLU (W′ ⊙ ( h′ₖ ⊙ c′ₖ ) + b′)"
- **Break condition**: If centroids poorly represent shard content or attention weights collapse to uniform, aggregation adds little value.

### Mechanism 3
- **Claim**: Removing extra items beyond the target eliminates residual influence via collaborative or sequential links.
- **Mechanism**: For CED, items similar to the unlearned item are deleted based on Euclidean distance in the embedding space; for NED, preceding items in the session are removed; for RED, random items are removed. This breaks indirect inference paths.
- **Core assumption**: Remaining items still allow the unlearned item to be inferred without extra deletion.
- **Evidence anchors**:
  - [abstract] "three extra data deletion strategies, including collaborative extra deletion (CED), neighbor extra deletion (NED), and random extra deletion (RED)"
  - [section] "we propose three extra data deletion strategies, including collaborative extra deletion (CED), neighbor extra deletion (NED), and random extra deletion (RED)"
- **Break condition**: If deleted items remove too much context, recommendation performance drops sharply.

## Foundational Learning

- **Concept**: k-means clustering on session embeddings
  - Why needed here: Groups similar sessions into shards so sub-models can specialize.
  - Quick check question: If you cluster sessions by item co-occurrence counts instead of embeddings, will the shard quality improve?

- **Concept**: Attention mechanism over multiple sub-models
  - Why needed here: Combines outputs of independently trained sub-models into a single prediction.
  - Quick check question: What happens to the final hidden state if all attention scores are equal?

- **Concept**: Membership inference metric (HIT@K)
  - Why needed here: Quantifies how well the unlearned item can be guessed from remaining data, measuring unlearning effectiveness.
  - Quick check question: If HIT@K for the unlearned item is 0.5, is that better or worse than 0.2?

## Architecture Onboarding

- **Component map**: Session Partition → Sub-model Training → Attentive Aggregation → Data Deletion → Unlearning Request Handler
- **Critical path**: When an unlearning request arrives, identify the shard, delete extra items, retrain only that shard's sub-model and the aggregation layer.
- **Design tradeoffs**:
  - More shards → faster unlearning but lower recommendation quality.
  - Extra deletion count → better unlearning but more data loss.
  - Choice of deletion strategy → balances collaborative vs. sequential signal removal.
- **Failure signatures**:
  - High HIT@K after unlearning → deletion strategy ineffective.
  - Sharp drop in NDCG/Recall → too much data removed.
  - Slow unlearning → shard imbalance or large shard size.
- **First 3 experiments**:
  1. Run with 4 shards vs. 8 shards and compare recommendation performance and unlearning time.
  2. Compare CED, NED, and RED on the same dataset with 2 extra deletions.
  3. Measure HIT@5 before and after unlearning with no extra deletions to show baseline leakage.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of the proposed unlearning strategies (CED, NED, RED) vary across different types of recommendation scenarios beyond session-based recommendation, such as user-item matrix-based collaborative filtering or sequential recommender systems without session boundaries?
- Basis in paper: [inferred] The paper focuses specifically on session-based recommendation and does not explore the generalizability of the unlearning strategies to other recommendation domains.
- Why unresolved: The study is limited to session-based recommendation, leaving open the question of how these strategies would perform in other recommendation contexts.
- What evidence would resolve it: Comparative experiments applying SRU's unlearning strategies to other recommendation models (e.g., matrix factorization, sequential models without sessions) and measuring their effectiveness and efficiency.

### Open Question 2
- Question: What is the optimal trade-off between unlearning effectiveness and recommendation performance when varying the number of extra deletion samples (N), and how does this trade-off change with different dataset characteristics (e.g., sparsity, item popularity)?
- Basis in paper: [explicit] The paper mentions that the trade-off between unlearning effectiveness and recommendation performance needs to be investigated, and shows results varying N from 0-5.
- Why unresolved: The paper only provides limited analysis of this trade-off and doesn't explore how dataset characteristics influence the optimal N.
- What evidence would resolve it: Systematic experiments varying N across datasets with different characteristics (density, popularity distributions) and analyzing the performance-effectiveness trade-off curves.

### Open Question 3
- Question: What is the impact of different session partition strategies (e.g., k-means vs. hierarchical clustering, or content-based vs. interaction-based similarity) on both recommendation performance and unlearning effectiveness, and which approach is most robust across different dataset types?
- Basis in paper: [explicit] The paper uses k-means clustering based on pre-trained session representations but doesn't explore alternative partitioning strategies.
- Why unresolved: The paper implements one specific partitioning approach without comparative analysis of alternatives.
- What evidence would resolve it: Experiments comparing multiple session partitioning methods (different clustering algorithms, similarity metrics) and evaluating their impact on both recommendation and unlearning performance.

### Open Question 4
- Question: How does the proposed unlearning effectiveness metric (HIT@K) compare to alternative privacy-preserving evaluation metrics, such as differential privacy guarantees or membership inference attack success rates, in terms of capturing the true privacy protection achieved by unlearning?
- Basis in paper: [explicit] The paper proposes HIT@K as an evaluation metric but doesn't compare it to other privacy metrics.
- Why unresolved: The paper introduces a novel metric without benchmarking against established privacy evaluation methods.
- What evidence would resolve it: Comparative analysis using HIT@K alongside differential privacy metrics and membership inference attack frameworks to evaluate unlearning effectiveness.

### Open Question 5
- Question: How does the unlearning framework scale with extremely large datasets (e.g., millions of users and items) in terms of both computational efficiency and memory requirements, and what optimizations could be implemented to improve scalability?
- Basis in paper: [inferred] The paper mentions efficiency improvements but doesn't address scaling challenges with very large datasets.
- Why unresolved: The experiments use moderate-sized datasets, leaving scalability questions unanswered.
- What evidence would resolve it: Performance evaluation of SRU on datasets orders of magnitude larger than those used in the paper, with analysis of computational bottlenecks and potential optimizations.

## Limitations

- The effectiveness of similarity-based partitioning heavily depends on the quality of session embeddings, which isn't thoroughly evaluated
- The three extra deletion strategies show improved unlearning effectiveness, but the trade-off between data deletion and recommendation performance could be more extensively explored
- The choice of deletion count (2 items) appears somewhat arbitrary and may not generalize optimally across all scenarios

## Confidence

- **High confidence**: The core mechanism of partitioning sessions into shards and using attention-based aggregation is well-supported by both theoretical justification and experimental results
- **Medium confidence**: The effectiveness of extra data deletion strategies is demonstrated, but the choice of deletion count appears somewhat arbitrary
- **Medium confidence**: The HIT@K metric for measuring unlearning effectiveness is innovative, but its correlation with real-world privacy guarantees could be stronger

## Next Checks

1. **Shard Quality Validation**: Test alternative session partitioning methods (e.g., item co-occurrence clustering) and compare recommendation performance and unlearning effectiveness against the proposed embedding-based approach.

2. **Deletion Strategy Optimization**: Systematically vary the number of extra deletions (1, 2, 3, 4 items) across all three strategies (CED, NED, RED) to identify optimal trade-offs between unlearning effectiveness and recommendation performance.

3. **Attention Weight Analysis**: Examine the distribution of attention weights across shards to verify they're not collapsing to uniform values, which would indicate the aggregation layer isn't providing meaningful differentiation between shard contributions.