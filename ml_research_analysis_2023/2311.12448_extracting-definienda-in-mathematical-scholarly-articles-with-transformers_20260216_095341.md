---
ver: rpa2
title: Extracting Definienda in Mathematical Scholarly Articles with Transformers
arxiv_id: '2311.12448'
source_url: https://arxiv.org/abs/2311.12448
tags:
- mathematical
- term
- extracted
- arxiv
- definienda
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to automatically identify defined
  terms (definienda) in mathematical definitions extracted from academic papers. The
  authors construct a labeled dataset from LaTeX sources of arXiv papers, using formatting
  cues like italics and parenthetical headers to identify definitions and terms.
---

# Extracting Definienda in Mathematical Scholarly Articles with Transformers

## Quick Facts
- arXiv ID: 2311.12448
- Source URL: https://arxiv.org/abs/2311.12448
- Reference count: 5
- Key outcome: Fine-tuned Roberta-base achieves precision 0.697 and recall 0.794 on definiendum extraction task

## Executive Summary
This paper addresses the task of automatically identifying defined terms (definienda) within mathematical definitions extracted from scholarly articles. The authors propose a rule-based approach to construct a labeled dataset from LaTeX sources of arXiv papers, leveraging formatting cues like italics and parenthetical headers to identify definitions and terms. They evaluate two approaches: fine-tuning pre-trained transformer models (Roberta-base and cc_math_roberta) for token classification, and using GPT models (GPT-3.5 and GPT-4) in a few-shot learning setup. Experimental results demonstrate that fine-tuned Roberta-base achieves the best balance of precision and recall, outperforming GPT models in terms of cost-effectiveness.

## Method Summary
The method involves extracting definitions from LaTeX sources using \begin{definition} environments, then applying rule-based extraction to identify definienda based on italics (\textit{} or \emph{}) and optional definition headers. The extracted text is converted to plain text with Unicode characters and annotated using IOB2 tagging (B-MATH_TERM, I-MATH_TERM, O). The labeled dataset is then used to fine-tune transformer models (Roberta-base and cc_math_roberta) for token classification, with evaluation using precision, recall, and F1-score on test sets. GPT models are also evaluated using few-shot learning with appropriate prompting.

## Key Results
- Fine-tuned Roberta-base achieves precision 0.697 and recall 0.794 on the test set
- Roberta-base outperforms cc_math_roberta and GPT models in terms of cost-effectiveness
- 10-fold cross-validation shows stable performance with training set sizes of 2048 and 10240 samples
- GPT models tend to over-predict formulas and mathematical expressions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rule-based dataset construction leverages LaTeX formatting cues to reliably identify definienda
- Mechanism: Authors exploit consistent use of italics or non-italics within italics to highlight defined terms, and parenthetical terms after definition headers
- Core assumption: Formatting conventions are consistent enough within definition environments to provide reliable ground truth labels
- Evidence anchors: [abstract] rule-based approach to build labeled dataset from LATEX source; [section] reliance on italics and parenthetical headers for definienda identification

### Mechanism 2
- Claim: Fine-tuning pre-trained transformers on domain-specific mathematical text improves performance
- Mechanism: Pre-trained models learn general patterns, and fine-tuning adapts these to mathematical definition tasks
- Core assumption: Mathematical domain requires specialized knowledge that general models lack but can acquire through fine-tuning
- Evidence anchors: [abstract] high precision and recall using pre-trained models fine-tuned on task; [section] Roberta-base yielded best performance despite cc_math_roberta's fewer word pieces

### Mechanism 3
- Claim: GPT models can perform definiendum extraction through few-shot learning with appropriate prompting
- Mechanism: Large language models understand mathematical text and extract definienda when given clear instructions and examples
- Core assumption: GPT models have sufficient mathematical reasoning capabilities to generalize from limited examples
- Evidence anchors: [abstract] high precision and recall using GPT 4 or simpler pre-trained models; [section] experimentation with ChatGPT and GPT-3.5-Turbo API versions

## Foundational Learning

- Concept: Token-level classification with IOB2 tagging
  - Why needed here: Task requires identifying which tokens belong to defined terms within mathematical definitions
  - Quick check question: How does IOB2 tagging help distinguish between beginning of defined term, inside tokens, and tokens outside any defined term?

- Concept: Fine-tuning transformer models for domain-specific tasks
  - Why needed here: Pre-trained transformers need adaptation to mathematical domain for optimal performance
  - Quick check question: What are key differences between using out-of-the-box model versus fine-tuning on domain-specific data?

- Concept: Few-shot learning with large language models
  - Why needed here: GPT models can perform task with minimal examples, offering alternative to fine-tuning
  - Quick check question: How does few-shot learning compare to fine-tuning in terms of cost, performance, and generalization?

## Architecture Onboarding

- Component map: LaTeX source processing -> text extraction -> IOB2 annotation -> Tokenizer -> Transformer backbone -> Classification head -> Precision/recall calculation
- Critical path: 1) Extract definitions from LaTeX sources using \begin{definition} environments; 2) Apply formatting-based rules to identify potential definienda; 3) Convert to plain text with Unicode characters; 4) Apply IOB2 tagging to create labeled dataset; 5) Fine-tune transformer model on training data; 6) Evaluate on test data with precision/recall metrics
- Design tradeoffs: Rule-based vs. learned approach (rules cheaper but less complete; learning requires labeled data); Fine-tuning vs. few-shot learning (fine-tuning more cost-effective long-term but requires resources; few-shot cheaper per query but less consistent); Domain-specific vs. general models (domain-specific may perform better but require additional pre-training; general models more flexible)
- Failure signatures: Low precision (over-generation including too many tokens); Low recall (missing many defined terms or being too conservative); High variance across folds (sensitive to training data composition); Poor performance on mathematical expressions (struggles with text-formula interplay)
- First 3 experiments: 1) Test rule-based annotation quality on small sample to validate formatting convention assumptions; 2) Fine-tune Roberta-base on small training set (256 samples) to establish baseline performance; 3) Compare few-shot performance of GPT-3.5 vs. GPT-4 on same test set to evaluate cost-effectiveness tradeoff

## Open Questions the Paper Calls Out

- Question: How robust are proposed methods when applied to definitions extracted from real PDF articles without LaTeX sources?
  - Basis in paper: [explicit] Planning to test fine-tuned models on definitions extracted from real PDF format papers without LaTeX sources
  - Why unresolved: Current evaluation based on dataset constructed from LaTeX sources, not representing raw PDF challenges
  - What evidence would resolve it: Testing fine-tuned models on dataset of definitions directly extracted from PDF articles and comparing performance

- Question: How can ambiguities in extracted entities be resolved, and how can they be linked to classes?
  - Basis in paper: [explicit] Exploring ambiguities of extracted entities and linking them to classes as future work
  - Why unresolved: Current work focuses on extracting definienda but doesn't address resolving ambiguities or classifying extracted entities
  - What evidence would resolve it: Developing method to disambiguate and classify extracted entities, evaluating performance on existing dataset

- Question: How can robustness of domain-specific language models be improved across different NLP tasks?
  - Basis in paper: [explicit] Experience with cc_math_roberta models opens research about improving robustness of from-scratch domain-specific language models over different NLP tasks
  - Why unresolved: Current work shows cc_math_roberta models have varying performance across different tasks, indicating need for robustness improvement
  - What evidence would resolve it: Developing techniques to enhance robustness of domain-specific language models, evaluating performance across range of NLP tasks

## Limitations

- The approach relies heavily on LaTeX formatting conventions that are not universally followed by all mathematical authors
- Experimental evaluation shows promising results but test set size and composition are not fully specified
- Model's ability to handle complex mathematical notation and text-formula interplay is not thoroughly evaluated

## Confidence

- **High Confidence**: Fundamental approach of using transformer models for token-level classification in mathematical text is well-established and experimental methodology is sound
- **Medium Confidence**: Rule-based dataset construction provides reasonable ground truth, but quality and consistency across different authors and papers is uncertain
- **Low Confidence**: Generalization capability to papers outside math.CO category and to mathematical definitions with non-standard formatting

## Next Checks

1. **Dataset Quality Validation**: Manually review 100 randomly sampled labeled definienda from rule-based dataset to quantify annotation errors and assess whether formatting-based rules capture true definienda when authors deviate from standard conventions

2. **Cross-Domain Performance**: Test fine-tuned Roberta-base model on mathematical definitions from other arXiv categories (e.g., math.AG, math.DG) to evaluate performance degradation when encountering different mathematical writing styles and conventions

3. **Complex Definition Analysis**: Systematically evaluate model performance on definitions containing nested mathematical expressions, multiple definienda, and mixed text/formula content to identify specific failure patterns and determine if additional architectural modifications are needed for robust handling of complex mathematical definitions