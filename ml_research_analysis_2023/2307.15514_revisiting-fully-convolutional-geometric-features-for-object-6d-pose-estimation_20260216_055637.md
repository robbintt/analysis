---
ver: rpa2
title: Revisiting Fully Convolutional Geometric Features for Object 6D Pose Estimation
arxiv_id: '2307.15514'
source_url: https://arxiv.org/abs/2307.15514
tags:
- object
- point
- pose
- estimation
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work revisits Fully Convolutional Geometric Features (FCGF)
  for 6D object pose estimation by adapting it to process heterogeneous point clouds
  and employing a hardest contrastive loss with safety thresholds. The method learns
  point-level discriminative features using a single deep network for all objects,
  processing both photometric and geometric data with sparse convolutions.
---

# Revisiting Fully Convolutional Geometric Features for Object 6D Pose Estimation

## Quick Facts
- arXiv ID: 2307.15514
- Source URL: https://arxiv.org/abs/2307.15514
- Reference count: 40
- This work revisits Fully Convolutional Geometric Features (FCGF) for 6D object pose estimation by adapting it to process heterogeneous point clouds and employing a hardest contrastive loss with safety thresholds. The method learns point-level discriminative features using a single deep network for all objects, processing both photometric and geometric data with sparse convolutions. Data augmentations are tailored for occlusion handling, and training strategies are optimized. The approach achieves state-of-the-art performance on LineMod-Occluded (+3.5 ADD(S)-0.1d) and YCB-Video (+0.8 ADD-S AUC) datasets, outperforming recent competitors even without using object detectors. A thorough ablation study quantifies the contribution of each modification.

## Executive Summary
This paper presents FCGF6D, a method for 6D object pose estimation that extends the Fully Convolutional Geometric Features (FCGF) approach to handle heterogeneous point clouds (object models and scene data) and incorporates photometric information alongside geometric features. The key innovations include using separate feature extractors for objects and scenes, implementing a hardest contrastive loss with safety thresholds for stable training, and employing data augmentations specifically designed for occlusion handling. The method achieves state-of-the-art performance on two benchmark datasets without requiring object detectors, demonstrating that a single deep network can effectively learn discriminative features for multiple objects.

## Method Summary
FCGF6D processes heterogeneous point clouds using two independent MinkUNet feature extractors (one for objects, one for scenes) with sparse convolutions. The method learns discriminative features through a hardest contrastive loss that incorporates safety thresholds to prevent unstable negative mining. Input data is preprocessed through voxelization, with objects represented as sampled vertices from CAD models with texture sampling, and scenes as quantized point clouds from RGBD images. Training employs data augmentations including point resampling, color jittering, and random erasing to handle occlusions. At inference, correspondences between object and scene features are computed and passed to registration algorithms (TEASER++ for LineMod-Occluded, RANSAC for YCB-Video) to estimate final 6D poses.

## Key Results
- Achieves +3.5 ADD(S)-0.1d improvement on LineMod-Occluded dataset
- Achieves +0.8 ADD-S AUC improvement on YCB-Video dataset
- Outperforms state-of-the-art methods even without using object detectors
- Demonstrates that a single deep network can learn features for all objects in a dataset

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Using a hardest contrastive loss with safety thresholds improves feature discrimination between objects and scenes while avoiding unstable negative mining.
- **Mechanism:** The safety threshold (œÑNO) prevents mining negatives from spatially nearby points, which would otherwise cause loss collapse. By restricting negative mining to points farther than a radius proportional to object size, the loss enforces feature dissimilarity between distinct geometric structures.
- **Core assumption:** Points that are spatially close share similar local geometry and should not be forced apart in feature space.
- **Evidence anchors:**
  - [abstract] "We modify the hardest contrastive loss to take into account the size of each point cloud for the mining of the hardest negatives."
  - [section 4.2] "the hardest negative in Óà∫ùëñ ... is likely to be a point spatially close to xùëñ ‚àà ÓâÑùëÇ, because their local geometric structure is nearly the same."
  - [corpus] No direct evidence; weak signal from neighbor abstracts on 3D shape representation.
- **Break condition:** If œÑNO is too small, negative mining becomes too restrictive and no informative negatives remain; if too large, spurious negatives dominate.

### Mechanism 2
- **Claim:** Processing heterogeneous point clouds (object vs. scene) with independent feature extractors rather than a shared Siamese network improves pose estimation accuracy.
- **Mechanism:** Objects and scenes have different distributions (dense CAD models vs. noisy sensor data). Separate networks can specialize to each domain's geometry and photometric cues, avoiding the constraint of shared weights that may hurt domain adaptation.
- **Core assumption:** Heterogeneous input distributions benefit from domain-specific feature learning.
- **Evidence anchors:**
  - [abstract] "Unlike several state-of-the-art methods we train only a single model for all the objects of each dataset."
  - [section 4.1] "Unlike FCGF, our input data is heterogeneous, therefore we process it with two independent deep networks."
  - [corpus] Weak; neighbor abstracts mention domain-specific pose estimation but not architecture separation.
- **Break condition:** If the domain gap is small (e.g., synthetic vs. real of same object), shared weights might be more efficient.

### Mechanism 3
- **Claim:** Adding photometric (RGB) features to geometric point clouds significantly improves pose estimation, especially for symmetric objects.
- **Mechanism:** RGB color provides discriminative cues absent in pure geometry, breaking symmetry and enabling correct pose estimation where geometric cues are ambiguous.
- **Core assumption:** For symmetric or texture-less objects, geometry alone is insufficient for pose disambiguation.
- **Evidence anchors:**
  - [abstract] "We use specific augmentations to tackle occlusions... employ data augmentations suitable for the underlying problem."
  - [section 4.1] "we found that this addition significantly improves the performance. This is particularly beneficial in the case of symmetric objects."
  - [corpus] No direct evidence; neighbor abstracts focus on geometric features but not photometric fusion.
- **Break condition:** If lighting conditions vary drastically, color features may introduce noise; if objects are purely geometric, RGB may not help.

## Foundational Learning

- **Concept: Sparse convolutions and voxelization**
  - Why needed here: Efficient processing of large-scale point clouds without losing spatial structure; quantization reduces memory footprint.
  - Quick check question: What is the role of the quantization step Q in reducing memory usage while preserving point cloud structure?

- **Concept: Contrastive loss with hard negative mining**
  - Why needed here: Forces learned features to be discriminative across point clouds; hardest negatives push the network to learn robust features.
  - Quick check question: Why does mining the hardest negatives without a safety threshold cause loss instability?

- **Concept: Feature Matching Recall (FMR)**
  - Why needed here: Indirect measure of registration quality; high FMR means fewer RANSAC iterations needed for accurate pose.
  - Quick check question: How does FMR relate to the number of inliers found by a registration algorithm?

## Architecture Onboarding

- **Component map:** RGBD image -> point cloud lifting + voxelization (scene) -> independent MinkUNet -> contrastive loss optimization -> correspondence mining -> registration (TEASER++ or RANSAC)
- **Critical path:** Input ‚Üí voxelization ‚Üí independent feature extraction ‚Üí contrastive loss optimization ‚Üí correspondence mining ‚Üí registration
- **Design tradeoffs:**
  - Separate networks vs. shared Siamese: More parameters but better domain adaptation vs. fewer parameters but possible domain mismatch
  - Including RGB: Better discrimination but more sensitive to lighting; excluding RGB: Robust to illumination but weaker for symmetric objects
  - Safety thresholds: Prevent loss collapse but reduce negative mining diversity
- **Failure signatures:**
  - Loss NaN or divergence ‚Üí check œÑNO and œÑP values
  - Low FMR but high ADD(S) ‚Üí check correspondence mining and registration algorithm
  - GPU OOM ‚Üí reduce V_S or V_O or increase voxelization step Q
- **First 3 experiments:**
  1. **Baseline sanity check:** Train with shared weights, no RGB, no safety threshold; verify loss decreases and features separate
  2. **Safety threshold ablation:** Vary tscale and œÑNO; observe impact on loss stability and FMR
  3. **RGB fusion test:** Enable RGB features and color jittering; measure impact on ADD(S)-0.1d and FMR

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FCGF6D vary when using different object detectors (e.g., YOLOv8 vs. other state-of-the-art detectors) on the LMO and YCBV datasets?
- Basis in paper: [explicit] The paper mentions using YOLOv8 for object detection and compares performance with and without detections.
- Why unresolved: The paper only evaluates YOLOv8, leaving the impact of other detectors unexplored.
- What evidence would resolve it: Comparing ADD(S)-0.1d and ADD-S AUC scores using different detectors (e.g., Faster R-CNN, EfficientDet) on the same datasets.

### Open Question 2
- Question: What is the impact of varying the safety threshold (ùë°scale) hyperparameter on the performance of FCGF6D across different object categories in the LMO and YCBV datasets?
- Basis in paper: [explicit] The paper includes an ablation study on the Can object of LMO, showing performance variation with different ùë°scale values.
- Why unresolved: The study is limited to one object, and the effect on other objects or symmetric objects is not explored.
- What evidence would resolve it: Conducting a comprehensive ablation study across all object categories in LMO and YCBV, analyzing ADD(S)-0.1d and ADD-S AUC scores.

### Open Question 3
- Question: How does the quality of ground-truth annotations affect the performance of FCGF6D, particularly in cases of noisy or incomplete annotations as seen in the Scissors object of YCBV?
- Basis in paper: [explicit] The paper highlights issues with ground-truth annotations for the Scissors object in YCBV, where bounding boxes include occluded regions.
- Why unresolved: The paper does not quantify the impact of annotation quality on model performance or explore methods to mitigate this issue.
- What evidence would resolve it: Evaluating model performance with and without noisy annotations, or comparing against datasets with cleaner annotations, and testing robustness to annotation errors.

## Limitations
- Safety threshold mechanism lacks comprehensive ablation studies across different object categories
- No direct comparison with shared-weight Siamese network baseline under identical conditions
- Claims about RGB benefits are not validated under varying lighting conditions or for purely geometric objects

## Confidence

**High Confidence:** Claims about achieving state-of-the-art results on LineMod-Occluded and YCB-Video datasets are well-supported by quantitative metrics (ADD(S)-0.1d improvements of +3.5 and +0.8 ADD-S AUC respectively). The ablation studies on RGB inclusion and data augmentations provide solid empirical backing.

**Medium Confidence:** The mechanism by which safety thresholds prevent loss collapse is theoretically sound but relies on assumptions about point cloud geometry that may not hold universally. The paper provides geometric intuition but limited empirical validation of the threshold's necessity.

**Low Confidence:** Claims about the superiority of independent feature extractors over shared architectures lack direct comparative evidence. The paper asserts that heterogeneous input distributions benefit from domain-specific networks but does not demonstrate this through controlled experiments.

## Next Checks

1. **Safety Threshold Sensitivity:** Perform an ablation study varying tscale from 0.05 to 0.2 times object diameter while keeping all other hyperparameters fixed. Measure both training stability (loss divergence frequency) and downstream performance (FMR and ADD(S) metrics) to quantify the threshold's impact.

2. **Shared vs. Separate Networks:** Implement a direct comparison where both object and scene feature extractors share identical network architectures and weights, trained under the same conditions as the current method. Compare performance metrics to isolate the architectural contribution from other improvements.

3. **RGB Ablation in Controlled Conditions:** Test the RGB feature contribution by systematically varying lighting conditions and object symmetry properties. Measure performance degradation when removing RGB features under different scenarios (symmetric objects vs. asymmetric, controlled lighting vs. variable lighting) to validate the claimed benefits.