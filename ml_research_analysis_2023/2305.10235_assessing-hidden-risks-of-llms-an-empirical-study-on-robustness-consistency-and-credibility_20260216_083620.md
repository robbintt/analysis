---
ver: rpa2
title: 'Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency,
  and Credibility'
arxiv_id: '2305.10235'
source_url: https://arxiv.org/abs/2305.10235
tags:
- arxiv
- llms
- chatgpt
- adversarial
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive empirical study on the robustness,
  consistency, and credibility of large language models (LLMs), including ChatGPT,
  LLaMA, and OPT. The authors conduct over one million queries on these models using
  a novel automated workflow that evaluates LLMs under different adversarial attack
  schemes.
---

# Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility

## Quick Facts
- arXiv ID: 2305.10235
- Source URL: https://arxiv.org/abs/2305.10235
- Reference count: 40
- Key outcome: Comprehensive empirical study on robustness, consistency, and credibility of LLMs, revealing vulnerabilities to adversarial attacks, poor consistency across semantically similar inputs, and introducing the Relative Training Index (RTI) metric.

## Executive Summary
This paper presents a comprehensive empirical study on the robustness, consistency, and credibility of large language models (LLMs), including ChatGPT, LLaMA, and OPT. The authors conduct over one million queries on these models using a novel automated workflow that evaluates LLMs under different adversarial attack schemes. The study reveals that LLMs are vulnerable to adversarial attacks, with character-level and word-level attacks causing a significant increase in error rates and answer changes. Additionally, the consistency of LLMs across semantically similar inputs is found to be poor, with ChatGPT's accuracy fluctuating by an average of 3.2%. The paper also introduces a novel Relative Training Index (RTI) to measure the credibility of datasets for LLM evaluation, highlighting concerns about using datasets that may have been memorized by the models. The findings underscore the need for more robust and reliable LLMs, as well as the importance of careful dataset selection for evaluation.

## Method Summary
The study evaluates the robustness, consistency, and credibility of LLMs using an automated workflow that applies adversarial attacks to over one million queries across 12 public datasets. The workflow involves data primitive creation, adversarial attack generation, LLM querying, and automated response interpretation. The study employs three types of adversarial attacks: character-level, word-level, and visual-level. The automated interpreter processes LLM responses and calculates error rates (ER) and answer changed rates (ACR). The study also introduces the Relative Training Index (RTI) to measure the credibility of datasets for LLM evaluation.

## Key Results
- Character-level attacks have lower error rates than word-level attacks due to smaller changes in encoded vectors.
- LLMs show poor consistency across semantically similar inputs, with accuracy fluctuations of 3.2% for ChatGPT.
- High RTI scores indicate datasets are less suitable for LLM evaluation because models may have memorized them.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Character-level attacks have lower error rates than word-level attacks due to smaller changes in encoded vectors.
- Mechanism: Character-level attacks (deletions, insertions, repeats) alter fewer tokens and cause less semantic disruption than word-level attacks which replace entire words.
- Core assumption: The tokenizer and word embedding representations are more sensitive to word-level changes than character-level ones.
- Evidence anchors:
  - [section] "Character attack simulates those human natural errors, like typing or spelling mistakes, in user-generated contexts."
  - [section] "Robustness against character-level attacks is higher, and robustness against word-level attacks is lower among three levels."
- Break condition: If the tokenizer rewrites visually similar characters or the model uses character-level embeddings.

### Mechanism 2
- Claim: LLMs show poor consistency across semantically similar inputs, with accuracy fluctuations of 3.2% for ChatGPT.
- Mechanism: Different prompts or option orders create semantic similarity while changing surface form, exposing model instability in consistent reasoning.
- Core assumption: The model's reasoning process is sensitive to input presentation rather than just semantic content.
- Evidence anchors:
  - [abstract] "LLMs possess poor consistency when processing semantically similar query input. In addition, as a side finding, we find that ChatGPT is still capable to yield the correct answer even when the input is polluted at an extreme level."
  - [section] "ChatGPT's accuracy of the responses at average fluctuates by 3.2% in our testing protocol."
- Break condition: If the model uses a robust semantic representation that is invariant to surface form changes.

### Mechanism 3
- Claim: High RTI scores indicate datasets are less suitable for LLM evaluation because models may have memorized them.
- Mechanism: RTI measures the probability that a dataset has been memorized by testing model accuracy under increasing perturbation; higher scores mean lower memorization.
- Core assumption: If a model can answer correctly even with extreme input pollution, it likely memorized the dataset during training.
- Evidence anchors:
  - [abstract] "RTI measures the relative probability that the dataset has been memorized."
  - [section] "RTI provides a reference index to quantitatively measure the probability that the dataset has been memorized by the models."
- Break condition: If the model's correct answers under pollution are due to general reasoning rather than memorization.

## Foundational Learning

- Concept: Adversarial attack methodologies in NLP
  - Why needed here: Understanding different attack levels (character, word, visual) is crucial for designing and interpreting robustness experiments.
  - Quick check question: What distinguishes character-level from word-level attacks in terms of their impact on LLM outputs?

- Concept: Dataset preprocessing and primitive construction
  - Why needed here: The automated evaluation workflow requires converting diverse datasets into a uniform multiple-option format.
  - Quick check question: How does the confusion option generation process differ for True/False versus numerical answers?

- Concept: Relative Training Index (RTI) calculation
  - Why needed here: RTI is a novel metric for assessing dataset credibility in LLM evaluation contexts.
  - Quick check question: What parameter controls the intensity of attacks in RTI computation, and how does it relate to memorization?

## Architecture Onboarding

- Component map: Dataset -> Primitive conversion -> Attack application -> LLM query -> Response parsing -> Metric calculation
- Critical path: Dataset → Primitive conversion → Attack application → LLM query → Response parsing → Metric calculation
- Design tradeoffs: Uniform multiple-option format simplifies automation but may not capture all dataset nuances
- Failure signatures: High error rates across all attack types indicate fundamental robustness issues; inconsistent accuracy across similar prompts suggests reasoning instability
- First 3 experiments:
  1. Test character-level attacks on a small dataset and compare error rates to baseline.
  2. Vary prompt templates while keeping content constant to measure consistency.
  3. Compute RTI for a dataset with known training data overlap to validate the metric.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can adversarial training paradigms be effectively developed for large language models (LLMs) given their unique multi-stage training paradigm and high pre-training costs?
- Basis in paper: The paper discusses the need for adversarial training paradigms suitable for LLMs, noting that existing research has focused on pre-training and fine-tuning stages, while LLMs present unique challenges with their multi-stage training paradigm and high costs.
- Why unresolved: The paper does not provide a clear solution or methodology for developing adversarial training paradigms specifically tailored to LLMs, leaving this as an open area for future research.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of novel adversarial training methods specifically designed for LLMs, showing improved robustness against adversarial attacks.

### Open Question 2
- Question: What are the underlying mechanisms that enable ChatGPT to maintain high accuracy even when input passages are heavily polluted with adversarial attacks?
- Basis in paper: The paper observes that ChatGPT can still provide correct answers even when the input passage is completely polluted, raising questions about the model's memorization capabilities and the reliability of datasets used for evaluation.
- Why unresolved: The paper does not fully explain the reasons behind ChatGPT's ability to maintain accuracy under extreme adversarial conditions, suggesting a need for further investigation into the model's internal mechanisms.
- What evidence would resolve it: Detailed analysis of ChatGPT's internal representations and attention mechanisms during adversarial attacks, revealing how the model processes and retains information despite input pollution.

### Open Question 3
- Question: How can the Relative Training Index (RTI) be refined to provide more accurate and absolute measurements of dataset reliability for LLM evaluation?
- Basis in paper: The paper introduces RTI as a novel metric to measure the relative probability that a dataset has been memorized by the LLM, but acknowledges its limitations in providing absolute indicators without additional information about the dataset's training context.
- Why unresolved: The paper does not provide a comprehensive method for refining RTI to offer absolute measurements, highlighting the need for further development and validation of this metric.
- What evidence would resolve it: Empirical studies comparing RTI scores across various datasets with known training histories, establishing a baseline for interpreting RTI values and improving its accuracy in assessing dataset reliability.

## Limitations
- The study's findings may not be generalizable due to the automated workflow's dependency on current LLM versions, particularly ChatGPT, which may have evolved since the study was conducted.
- The effectiveness of the RTI metric in identifying memorized datasets is conceptually sound but has Medium confidence pending independent validation.
- The claim that character-level attacks have lower error rates than word-level attacks carries a Medium confidence label, as the underlying assumption about tokenizer sensitivity requires further validation across different LLM architectures.

## Confidence
- Character-level vs word-level attack error rates: Medium
- Poor consistency across semantically similar inputs (3.2% fluctuation): Medium
- RTI metric's effectiveness in identifying memorized datasets: Medium

## Next Checks
1. Replicating the study using updated versions of the evaluated LLMs to assess result stability over time.
2. Conducting ablation studies to isolate the impact of specific attack parameters on error rates.
3. Implementing cross-validation of the RTI metric using datasets with known training data overlap to verify its reliability in detecting memorization.