---
ver: rpa2
title: 'ClimateSet: A Large-Scale Climate Model Dataset for Machine Learning'
arxiv_id: '2311.03721'
source_url: https://arxiv.org/abs/2311.03721
tags:
- climate
- data
- different
- climateset
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ClimateSet is a large-scale dataset containing the inputs and outputs
  of 36 climate models, along with a modular pipeline for preprocessing additional
  models and scenarios. The dataset is built from the CMIP6 and Input4MIPs archives
  and is designed to support machine learning tasks in climate science, such as climate
  model emulation, downscaling, and prediction.
---

# ClimateSet: A Large-Scale Climate Model Dataset for Machine Learning

## Quick Facts
- **arXiv ID**: 2311.03721
- **Source URL**: https://arxiv.org/abs/2311.03721
- **Reference count**: 40
- **Key outcome**: Large-scale dataset of 36 climate models enabling ML-based climate emulation and super-emulation across diverse scenarios

## Executive Summary
ClimateSet is a comprehensive dataset built from CMIP6 and Input4MIPs archives that enables machine learning models to emulate climate projections across multiple climate models. The dataset addresses the challenge of multi-model uncertainty in climate science by providing a consistent, ML-ready collection of inputs and outputs from 36 different climate models. ClimateSet includes a modular preprocessing pipeline that handles the heterogeneity of climate data sources, making it possible to train ML models that can generalize across different climate model architectures and produce "super emulators" capable of quickly projecting new climate change scenarios.

## Method Summary
The dataset construction involves downloading raw climate model data from ESGF, followed by a multi-stage preprocessing pipeline. The raw processor handles calendar synchronization, unit conversion, and naming consistency across models. The resolution processor standardizes spatial (250 km) and temporal (monthly) resolution. The structure processor ensures consistent file organization. For ML tasks, various models (U-Net, ConvLSTM, ClimaX, ClimaXFrozen, Gaussian Process) are trained on 15 climate models separately, then tested on SSP2-4.5 scenarios. The super-emulation approach trains a single model on 6 climate models simultaneously using a multi-head decoder architecture.

## Key Results
- ClimateSet enables training ML models that generalize across multiple climate models rather than overfitting to a single model
- The dataset supports both single-model emulation and "super emulation" approaches for faster climate scenario projections
- Performance analysis reveals differences in generalization capabilities across ML architectures when tested on diverse climate models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ClimateSet improves generalization by exposing ML models to multi-model uncertainty during training
- Mechanism: Training on multiple climate models with diverse internal dynamics forces the model to learn robust, transferable features rather than overfitting to a single model's idiosyncrasies
- Core assumption: Different climate models capture sufficiently different aspects of the climate system that cross-model training provides broader coverage
- Evidence anchors:
  - [abstract] "We gain new insights about the performance and generalization capabilities of the different ML models by analyzing their performance across different climate models"
  - [section] "Testing and comparing ML models on just one climate model is not sufficient for finding which ML model is the 'best' emulator"
  - [corpus] Weak - related papers focus on benchmarking rather than multi-model generalization
- Break condition: If climate models are too similar in their internal dynamics, the benefit of multi-model training diminishes

### Mechanism 2
- Claim: ClimateSet enables "super emulation" by providing sufficient data to train a single model across multiple climate models
- Mechanism: The large-scale dataset allows training a single ML model that can predict outputs for any of the participating climate models by learning to distinguish between their responses
- Core assumption: The dataset contains enough diversity and volume to support training a single model rather than requiring separate models per climate model
- Evidence anchors:
  - [abstract] "the dataset can be used to train an ML emulator on several climate models instead of just one. Such a 'super emulator' can quickly project new climate change scenarios"
  - [section] "The super emulator should be able to distinguish between data inputs provided to it from different climate models"
  - [corpus] Weak - related papers focus on single-model approaches rather than super emulation
- Break condition: If the dataset is too small or homogeneous, the super emulator cannot learn to distinguish between models effectively

### Mechanism 3
- Claim: ClimateSet's preprocessing pipeline ensures consistency across heterogeneous climate data sources
- Mechanism: The modular pipeline handles inconsistencies in temporal/spatial resolution, units, and naming conventions across different climate models and datasets
- Core assumption: The preprocessing steps can adequately handle the wide variety of data formats and structures in CMIP6 and Input4MIPs
- Evidence anchors:
  - [section] "The raw processor for CMIP6 data syncs time-axis, calendars, and height levels... For the Input4MIPs data, it additionally handles the special case of biomass burning data"
  - [section] "The Resolution Processor creates the desired spatial and temporal resolution across all data"
  - [corpus] Weak - related papers mention data challenges but don't describe comprehensive preprocessing solutions
- Break condition: If new data sources introduce inconsistencies not handled by the pipeline, or if preprocessing introduces artifacts

## Foundational Learning

- Concept: Climate model uncertainty and ensemble members
  - Why needed here: Understanding why multiple climate models are needed is fundamental to grasping ClimateSet's value proposition
  - Quick check question: Why does the IPCC rely on multiple climate models rather than a single "best" model?

- Concept: Temporal and spatial resolution handling in climate data
  - Why needed here: The preprocessing pipeline's core functionality revolves around making data consistent across different resolutions
  - Quick check question: What challenges arise when trying to compare climate model outputs with different spatial resolutions?

- Concept: ML model generalization across distribution shifts
  - Why needed here: Climate emulation involves predicting future scenarios that may differ from training data, requiring good generalization
  - Quick check question: How does training on multiple climate models help an ML model generalize to unseen scenarios?

## Architecture Onboarding

- Component map: Downloader → Checker → Raw Processor → Resolution Processor → Structure Processor → ML training
- Critical path: Downloader → Checker → Raw Processor → Resolution Processor → Structure Processor → ML training
- Design tradeoffs:
  - Using CDO for speed vs. Python libraries for flexibility
  - Multi-threading for acceleration vs. single-thread compatibility
  - Comprehensive preprocessing vs. minimal preprocessing to preserve data fidelity
- Failure signatures:
  - Missing data files → Downloader failure
  - Inconsistent units/names → Checker or Structure Processor issues
  - Poor model performance → Insufficient preprocessing or inadequate training data
- First 3 experiments:
  1. Download a single climate model and verify basic preprocessing pipeline works
  2. Train a simple ML model (e.g., U-Net) on one climate model to establish baseline performance
  3. Extend to multiple climate models and test super emulator with multi-head decoder

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance rankings of ML models for climate emulation change when evaluated on a larger set of climate models beyond the 15 models used in this study?
- Basis in paper: [explicit] The authors state "Overall, the results show that ClimateSet can be used to identify ML models generalizing across different climate models."
- Why unresolved: The study only evaluated 15 climate models. A larger evaluation could reveal new insights about model generalization and ranking.
- What evidence would resolve it: Evaluating the same ML models (U-Net, ConvLSTM, ClimaX, ClimaXFrozen) on a much larger set of climate models from ClimateSet (e.g., 36 models) and comparing the performance rankings.

### Open Question 2
- Question: Can the "super emulator" approach effectively emulate climate models that were not included in its training set?
- Basis in paper: [explicit] The authors mention "Such a 'super emulator' can quickly project new climate change scenarios, complementing existing scenarios already provided to policymakers."
- Why unresolved: The study only tested the super emulator on a subset of 6 climate models. It's unclear if the approach generalizes to models not seen during training.
- What evidence would resolve it: Training a super emulator on a subset of climate models and evaluating its performance on completely unseen climate models from ClimateSet.

### Open Question 3
- Question: How does the performance of ML models for climate emulation change when incorporating additional climate variables beyond temperature and precipitation?
- Basis in paper: [inferred] The authors state "The dataset can be extended to more climate models, ensemble members, variables, height levels, spatial, and temporal resolution."
- Why unresolved: The study only evaluated temperature and precipitation. Including other variables like wind velocity, humidity, etc. could reveal new insights about model capabilities.
- What evidence would resolve it: Extending the ML models to predict additional climate variables from ClimateSet and comparing their performance on the new variables.

### Open Question 4
- Question: What is the impact of weighting climate models based on their similarities when training ML emulators?
- Basis in paper: [explicit] The authors mention "ClimateSet currently unites 36 climate models without considering the similarity between some of them."
- Why unresolved: The study did not consider weighting models based on similarities. This could impact the training process and final model performance.
- What evidence would resolve it: Implementing a weighting scheme for climate models based on their similarities and evaluating the impact on ML model performance for climate emulation.

## Limitations
- The effectiveness depends on the assumption that 36 climate models capture sufficient diversity for robust generalization
- Preprocessing pipeline may introduce artifacts when handling heterogeneous climate data sources
- Super-emulation approach remains largely theoretical with limited validation across diverse scenarios

## Confidence
- **High confidence**: The dataset construction methodology and preprocessing pipeline are well-documented and reproducible
- **Medium confidence**: The single-model emulation results show clear performance differences between ML approaches
- **Low confidence**: Super-emulation performance claims require more extensive validation across diverse climate scenarios

## Next Checks
1. Test model performance on climate models not included in the training set to verify true generalization capabilities
2. Evaluate the impact of different preprocessing choices (spatial/temporal resolution) on emulation accuracy
3. Conduct ablation studies removing individual climate models from training to quantify each model's contribution to super-emulation performance