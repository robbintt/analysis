---
ver: rpa2
title: Answering Ambiguous Questions via Iterative Prompting
arxiv_id: '2307.03897'
source_url: https://arxiv.org/abs/2307.03897
tags:
- answers
- answering
- prompting
- answer
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AmbigPrompt, an iterative prompting approach
  to multi-answer question answering for ambiguous questions. The method alternates
  between generating prompts based on previously generated answers and generating
  new answers using a lightweight answering model, enabling progressive refinement
  of both relevance and diversity.
---

# Answering Ambiguous Questions via Iterative Prompting

## Quick Facts
- arXiv ID: 2307.03897
- Source URL: https://arxiv.org/abs/2307.03897
- Reference count: 5
- Outperforms models with 27× more parameters and 29× higher latency on AmbigQA and WebQSP benchmarks

## Executive Summary
This paper introduces AmbigPrompt, an iterative prompting approach for multi-answer question answering of ambiguous questions. The framework alternates between generating prompts based on previously generated answers and generating new answers using a lightweight answering model. This enables progressive refinement of both relevance and diversity in the answer set. The method employs task-adaptive post-pretraining on pseudo multi-answer data and achieves state-of-the-art performance on AmbigQA and WebQSP benchmarks while maintaining efficiency through parameter sharing.

## Method Summary
AmbigPrompt uses an iterative prompting mechanism where a prompting model generates continuous prompts conditioned on previously generated answers, which are then fed into an answering model to generate new answers. The framework employs parameter sharing between the prompting and answering models for efficiency. A task-adaptive post-pretraining strategy is used, where the model is first pretrained on single-answer NQ data, then on synthesized multi-answer data created by generating pseudo answers for each passage. The framework is fine-tuned on annotated multi-answer QA data using prompt-based fine-tuning, enabling it to progressively refine answers while balancing relevance and diversity.

## Key Results
- Achieves state-of-the-art performance on AmbigQA and WebQSP benchmarks
- Outperforms models with 27× more parameters and 29× higher latency
- Maintains strong performance in low-resource settings while balancing precision and recall through iterative refinement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative prompting allows the model to progressively refine answers by conditioning on previously generated answers.
- Mechanism: The framework alternates between (1) generating prompts based on previously generated answers using an answer-conditional prompting model, and (2) generating new answers using the answering model conditioned on these prompts. This creates a feedback loop where each new answer generation is informed by what has already been generated.
- Core assumption: Answers to ambiguous questions exhibit dependencies that can be captured through iterative refinement rather than single-pass generation.
- Evidence anchors:
  - [abstract] "The method alternates between generating prompts based on previously generated answers and generating new answers using a lightweight answering model, enabling progressive refinement of both relevance and diversity."
  - [section 3.1] "To capture intricate dependencies among answers, we devise an interleaving answer-conditional prompting model ϕ(a<t), which generates the prompt vector E = ϕ(a<t) conditioned on antecedent generated answers a<t"
  - [corpus] Weak - no direct evidence found in corpus
- Break condition: The iterative process terminates when the model generates the [EOI] token or reaches a maximum number of iterations.

### Mechanism 2
- Claim: Task-adaptive post-pretraining on pseudo multi-answer data improves the model's ability to handle ambiguous questions.
- Mechanism: The model is first pretrained on single-answer NQ data, then on synthesized multi-answer data where pseudo answers are generated for each passage using an auxiliary reader. This creates training instances that mimic the iterative answering process.
- Core assumption: Pre-training on data that reflects the target task's structure (multi-answer QA) leads to better downstream performance than pre-training on related but structurally different data.
- Evidence anchors:
  - [section 3.3] "We construct the pseudo multi-answer dataset Â for post-pretraining the proposed framework to mimic the iterative question answering process."
  - [section 3.3] "We devise a task-adaptive post-pretraining strategy, in which pseudo multi-QA training instances are constructed to facilitate the training of the proposed framework."
  - [corpus] Weak - no direct evidence found in corpus
- Break condition: The post-pretraining process stops after a fixed number of steps or when validation performance plateaus.

### Mechanism 3
- Claim: Sharing parameters between the prompting model and answering model enables seamless integration and reduces computational overhead.
- Mechanism: The prompting model uses the same transformer encoder architecture as the answering model's encoder, allowing the generated prompts to be directly prepended to the answering model's attention layers.
- Core assumption: Parameter sharing between components that operate in the same embedding space reduces the total parameter count without sacrificing performance.
- Evidence anchors:
  - [section 3.1] "The prompting model ϕ is a transformer encoder that shares the same parameters with the encoder of the answering model."
  - [abstract] "The prompting model shares parameters with the answering model, allowing for seamless integration."
  - [corpus] Weak - no direct evidence found in corpus
- Break condition: Parameter sharing works as long as the two components operate in compatible embedding spaces and don't require divergent feature representations.

## Foundational Learning

- Concept: Prompt-based learning
  - Why needed here: The iterative prompting mechanism relies on continuous prompts to guide the answering model, making understanding prompt-based learning essential for grasping how the framework operates.
  - Quick check question: How do continuous prompts differ from discrete prompts in their implementation and use?

- Concept: Multi-answer question answering
  - Why needed here: The entire framework is designed to address the challenge of finding multiple valid answers to ambiguous questions, requiring understanding of how this differs from single-answer QA.
  - Quick check question: What evaluation metrics are used for multi-answer QA that don't apply to single-answer QA?

- Concept: Transformer attention mechanisms
  - Why needed here: The framework uses cross-attention between the prompting model and answering model, as well as the iterative conditioning on previous answers, all of which rely on understanding transformer attention.
  - Quick check question: How does cross-attention differ from self-attention in transformer architectures?

## Architecture Onboarding

- Component map: Retrieval model -> Prompting model -> Answering model -> Post-pretraining module

- Critical path:
  1. Retrieve passages using the retrieval model
  2. Initialize empty answer set
  3. Generate first prompt (empty context)
  4. Generate first answer using answering model
  5. Add answer to set
  6. Generate next prompt conditioned on current answer set
  7. Generate next answer
  8. Repeat until [EOI] token or max iterations

- Design tradeoffs:
  - Parameter sharing vs. separate models: Sharing reduces parameters but may limit specialization
  - Number of iterations: More iterations may find more answers but increase latency
  - Post-pretraining vs. direct fine-tuning: Post-pretraining may improve performance but adds training time

- Failure signatures:
  - Early termination without finding all answers: May indicate insufficient iterations or poor prompt generation
  - Repetitive or redundant answers: May indicate the prompting model isn't effectively capturing what's already been generated
  - Poor precision/recall trade-off: May indicate the iterative refinement isn't effectively balancing relevance and diversity

- First 3 experiments:
  1. Test with single iteration (no iterative prompting) to establish baseline performance
  2. Test with varying numbers of iterations (2-5) to find optimal iteration count
  3. Test with and without task-adaptive post-pretraining to measure its impact on performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but the limitations section suggests areas for future work including scaling to questions with more than five answers and exploring the approach on diverse datasets beyond Wikipedia-based QA.

## Limitations
- The iterative mechanism may struggle with questions that have more than five valid answers
- Task-adaptive post-pretraining relies on synthetic data generated using an auxiliary reader, potentially introducing biases
- Parameter sharing between prompting and answering models could constrain specialization for different aspects of the task

## Confidence
- High Confidence: Core iterative prompting mechanism and its effectiveness for multi-answer QA
- Medium Confidence: Efficiency claims (27× fewer parameters, 29× lower latency) which are context-dependent
- Low Confidence: Generalizability to questions with very large answer sets (>5 answers) or non-Wikipedia domains

## Next Checks
1. Evaluate the model's performance on questions known to have more than five valid answers to assess scalability of the iterative mechanism
2. Test the framework on non-Wikipedia datasets (e.g., medical or legal domains) to verify cross-domain generalization
3. Conduct experiments comparing the parameter-sharing approach against separate prompting and answering models to quantify trade-offs between efficiency and specialization