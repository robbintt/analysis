---
ver: rpa2
title: An automatically discovered chain-of-thought prompt generalizes to novel models
  and datasets
arxiv_id: '2305.02897'
source_url: https://arxiv.org/abs/2305.02897
tags:
- datasets
- answer
- prompt
- prompts
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigated how zero-shot chain-of-thought (CoT) reasoning\
  \ prompts generalize across different large language models (LLMs) and datasets.\
  \ Six recently released models\u2014including GPT-4\u2014were tested on six question-answering\
  \ datasets spanning commonsense, scientific, and medical domains using ten different\
  \ prompting strategies."
---

# An automatically discovered chain-of-thought prompt generalizes to novel models and datasets

## Quick Facts
- arXiv ID: 2305.02897
- Source URL: https://arxiv.org/abs/2305.02897
- Authors: 
- Reference count: 8
- This study found that the automatically discovered Zhou prompt demonstrated robust performance across six LLMs and six question-answering datasets, with GPT-4 achieving the highest average accuracy of 85%

## Executive Summary
This study systematically evaluated how zero-shot chain-of-thought (CoT) reasoning prompts generalize across different large language models and datasets. Testing six recently released models including GPT-4 on six question-answering datasets spanning commonsense, scientific, and medical domains, the research identified the automatically discovered Zhou prompt as most effective. The findings demonstrate that carefully designed zero-shot prompts can effectively induce reasoning capabilities across diverse models and domains, with GPT-4 showing particular effectiveness with prompts discovered through automated methods.

## Method Summary
The study tested ten different zero-shot reasoning prompts across six multiple-choice question-answering datasets using six LLMs including GPT-4. Each prompt was evaluated using temperature 0 and max tokens 512, with performance measured using Krippendorff's alpha for inter-rater reliability. The research used subsampled datasets (33 items each) to enable testing of multiple prompt-model combinations within budget constraints, calculating 95% confidence intervals through bootstrapping.

## Key Results
- GPT-4 achieved the highest average accuracy at 85% across all datasets and prompts
- The automatically discovered Zhou prompt ("Let's work this out in a step by step way to be sure we have the right answer") demonstrated the most robust performance across experimental conditions
- CoT reasoning strategies showed robust performance gains across different models and datasets, with GPT-4 exhibiting the greatest benefit from these strategies
- Flan-T5-XXL outperformed GPT-3.5-turbo on CommonsenseQA, potentially due to training data contamination

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Zhou prompt improves reasoning by structuring intermediate steps in a step-by-step manner
- Mechanism: The prompt "Let's work this out in a step by step way to be sure we have the right answer" guides the model to generate explicit reasoning chains before answering, which improves both accuracy and explainability
- Core assumption: Explicit intermediate reasoning steps improve model accuracy over direct prompting
- Evidence anchors:
  - [abstract] "The research found that the automatically discovered Zhou prompt, 'Let's work this out in a step by step way to be sure we have the right answer,' demonstrated the most robust performance"
  - [section] "GPT-4 has the most benefit from current state-of-the-art reasoning strategies and exhibits the best performance by applying a prompt previously discovered through automated discovery."
  - [corpus] Weak evidence - related papers discuss CoT prompting but don't directly validate the Zhou prompt mechanism
- Break condition: If models don't require explicit step-by-step reasoning for accuracy, or if the prompt becomes too verbose

### Mechanism 2
- Claim: Zero-shot CoT prompting generalizes across different model generations and datasets
- Mechanism: The same prompt can induce reasoning capabilities in various LLMs without task-specific exemplars, suggesting emergent reasoning abilities that transfer across contexts
- Core assumption: Emergent reasoning capabilities in LLMs are not model-specific but generalize to different architectures
- Evidence anchors:
  - [abstract] "carefully designed zero-shot prompts can effectively induce reasoning capabilities across diverse models and domains"
  - [section] "gains from CoT reasoning strategies remain robust across different models and datasets"
  - [corpus] Weak evidence - related papers explore CoT generalization but don't specifically validate zero-shot generalization across model generations
- Break condition: If model-specific fine-tuning becomes necessary for optimal performance on specialized tasks

### Mechanism 3
- Claim: Automated prompt discovery finds effective reasoning strategies that human-designed prompts might miss
- Mechanism: Through automated methods, prompts can be optimized for performance across multiple conditions, identifying patterns that human intuition might overlook
- Core assumption: Automated discovery methods can outperform human-designed prompts for complex reasoning tasks
- Evidence anchors:
  - [abstract] "The study demonstrated that carefully designed zero-shot prompts can effectively induce reasoning capabilities across diverse models and domains, with GPT-4 showing particular effectiveness with prompts discovered through automated methods."
  - [section] "A comparison of average performance across all datasets and models reveals a top result for the automatically discovered prompt by Zhou et al."
  - [corpus] Weak evidence - related papers mention automated prompt discovery but don't provide direct comparison with human-designed prompts
- Break condition: If automated methods converge to trivial or ineffective prompts, or if human intuition consistently outperforms automated discovery

## Foundational Learning

- Concept: Krippendorff's alpha for inter-rater reliability
  - Why needed here: Used as evaluation metric to compare model predictions to gold standard answers while correcting for different numbers of answer choices
  - Quick check question: How does Krippendorff's alpha handle datasets with different numbers of answer options compared to simple accuracy?

- Concept: Bootstrapping for confidence intervals
  - Why needed here: Used to compute means and confidence intervals for the generated results, ensuring robust statistical analysis
  - Quick check question: Why is bootstrapping each sub-dataset individually important for accurate Krippendorff scores?

- Concept: Zero-shot prompting vs. few-shot prompting
  - Why needed here: Zero-shot prompting doesn't require task-specific exemplars, making it more versatile across different datasets and models
  - Quick check question: What are the key differences between zero-shot and few-shot prompting in terms of required preparation and potential performance?

## Architecture Onboarding

- Component map: Datasets (6 question-answering datasets) -> Prompts (10 zero-shot reasoning prompts) -> Models (6 LLMs) -> Evaluation (Krippendorff's alpha with bootstrapping)
- Critical path: Prompt -> Model -> Reasoning Chain -> Answer -> Krippendorff's alpha calculation
- Design tradeoffs: Using subsampled datasets limits direct comparison to full benchmarks but allows testing more prompt-model combinations within budget constraints
- Failure signatures: Poor performance on certain datasets may indicate either prompt ineffectiveness or dataset quality issues (e.g., ambiguous questions)
- First 3 experiments:
  1. Test direct prompting vs. Zhou prompt on GPT-4 with CommonsenseQA dataset
  2. Compare Kojima and Zhou prompts across all six models using WorldTree v2 dataset
  3. Evaluate self-critique prompt performance on StrategyQA dataset to identify why it yields lower scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the self-critique prompt's low performance compare to other reasoning strategies in terms of absolute accuracy versus relative improvement across different model families?
- Basis in paper: [explicit] The paper notes that "the self-critique prompt yielded relatively low scores" and that "it also resulted in the generation of multiple answers in various observed instances, which were excluded from the scoring process."
- Why unresolved: While the authors observe that the self-critique prompt performed poorly, they don't provide a detailed comparative analysis of whether this is due to fundamental limitations of the approach or implementation issues. The paper also doesn't explore whether self-critique might show different patterns of performance when applied to ensemble methods or with temperature settings that allow multiple generations.
- What evidence would resolve it: A controlled experiment comparing self-critique against other prompts across different model families (open-source vs. closed-source), with and without self-consistency methods, and with temperature settings that allow multiple generations.

### Open Question 2
- Question: To what extent does training data contamination explain the performance differences between Flan-T5-XXL and other models on CommonsenseQA and StrategyQA datasets?
- Basis in paper: [explicit] The paper states "We noted that Flan-T5 (Longpre et al., 2023), which was instruction-finetuned on the subsets of CommonsenseQA and StrategyQA, outperformed GPT-3.5-turbo on CommonsenseQA."
- Why unresolved: While the authors acknowledge potential contamination effects, they don't quantify the magnitude of this impact or determine whether it fully explains the performance gap. They also don't explore whether contamination effects vary by dataset complexity or model architecture.
- What evidence would resolve it: A detailed analysis of performance differences on these datasets versus others, controlling for dataset overlap with training data, and comparing against models with known training data composition.

### Open Question 3
- Question: How do the discovered prompts generalize to non-instruction-tuned models or models with different architectural paradigms (e.g., transformer vs. other architectures)?
- Basis in paper: [inferred] The paper focuses exclusively on instruction-tuned models (GPT-3.5-turbo, GPT-4, Flan-T5-xxl, command-xlarge) and doesn't test whether the discovered prompts work on base models or alternative architectures.
- Why unresolved: The authors demonstrate that prompts work across different instruction-tuned models, but don't explore the fundamental question of whether the prompting strategies would transfer to models with different training objectives or architectural designs.
- What evidence would resolve it: Testing the same prompts on base models (non-instruction-tuned), models with different attention mechanisms, or entirely different architectures like recurrent networks or hybrid approaches.

### Open Question 4
- Question: What is the relationship between prompt complexity (number of instructions, word count, logical structure) and performance across different model sizes and capabilities?
- Basis in paper: [explicit] The paper compares ten different prompts ranging from simple ("Let's think step by step") to complex multi-step instructions, noting that "a closer examination of the results obtained from the latest model, GPT-4, highlights the overall advantage of employing specific prompts."
- Why unresolved: While the paper identifies that certain prompts perform better than others, it doesn't systematically analyze whether simpler prompts work better for smaller models while complex prompts benefit larger ones, or whether there's an optimal complexity that scales with model capability.
- What evidence would resolve it: A controlled study varying prompt complexity (measured by instruction count, token count, logical steps) across different model sizes while holding other factors constant.

### Open Question 5
- Question: How do zero-shot CoT prompts perform on tasks requiring domain-specific reasoning versus general reasoning capabilities, and can prompt effectiveness be predicted based on task characteristics?
- Basis in paper: [explicit] The paper tests six datasets spanning commonsense, scientific, and medical domains, noting that "Better models are finding WorldTree v2 and CommonsenseQA increasingly easy, while StrategyQA suffers from peculiar items."
- Why unresolved: The authors observe performance differences across domains but don't systematically analyze what characteristics of tasks (e.g., required background knowledge, reasoning type, answer format) predict prompt effectiveness, nor do they explore whether domain-specific prompt adaptation would be beneficial.
- What evidence would resolve it: A task-characterization study linking prompt performance to measurable task properties (knowledge requirements, reasoning complexity, answer ambiguity), and testing whether prompts can be automatically adapted based on these characteristics.

## Limitations
- Study uses subsampled datasets rather than full benchmarks, potentially limiting generalizability
- Self-critique prompt excluded from scoring due to multiple outputs creates apples-to-oranges comparison
- Dataset quality issues (particularly CommonsenseQA and MedMCQA) may confound results
- Confidence intervals based on bootstrapping subsampled data may underestimate true variability

## Confidence
**High confidence**: GPT-4's superior performance and benefit from CoT prompts is well-supported by consistent results across multiple datasets and the large performance gap observed
**Medium confidence**: The robustness of the Zhou prompt across models and domains is supported by aggregate statistics but could be influenced by dataset selection bias
**Low confidence**: Claims about automated prompt discovery superiority are based on comparison to a single baseline and lack direct comparison with other automated methods

## Next Checks
1. Validate results on full datasets rather than subsamples to confirm performance patterns hold at scale
2. Test additional automated prompt discovery methods against the Zhou prompt to establish relative effectiveness
3. Conduct ablation studies on the Zhou prompt to identify which specific words or phrases drive its effectiveness