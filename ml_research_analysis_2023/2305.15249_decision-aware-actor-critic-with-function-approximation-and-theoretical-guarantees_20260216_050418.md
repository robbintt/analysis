---
ver: rpa2
title: Decision-Aware Actor-Critic with Function Approximation and Theoretical Guarantees
arxiv_id: '2305.15249'
source_url: https://arxiv.org/abs/2305.15249
tags:
- policy
- critic
- function
- actor
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a decision-aware actor-critic (DAAC) framework
  that trains both actor and critic components jointly to optimize the same objective,
  unlike standard actor-critic methods that train the critic independently to minimize
  TD error. The authors derive a lower bound on the policy return that incorporates
  both actor and critic terms, and use it to design a generic algorithm that can handle
  any policy representation (direct or softmax) and function approximation.
---

# Decision-Aware Actor-Critic with Function Approximation and Theoretical Guarantees

## Quick Facts
- arXiv ID: 2305.15249
- Source URL: https://arxiv.org/abs/2305.15249
- Reference count: 40
- One-line primary result: Decision-aware actor-critic framework guarantees monotonic policy improvement and outperforms standard TD loss, especially with less expressive critics.

## Executive Summary
This paper introduces a decision-aware actor-critic (DAAC) framework that trains both actor and critic components jointly to optimize the same objective, unlike standard actor-critic methods that train the critic independently to minimize TD error. The authors derive a lower bound on the policy return that incorporates both actor and critic terms, and use it to design a generic algorithm that can handle any policy representation (direct or softmax) and function approximation. Theoretical analysis shows that the algorithm guarantees monotonic policy improvement under certain conditions, and convergence to a neighborhood of a stationary point with a radius depending on the critic error. Experiments on Cliff World and Frozen Lake environments demonstrate that minimizing the proposed decision-aware critic loss results in faster convergence to better policies compared to standard squared TD loss, especially when using less expressive critics.

## Method Summary
The decision-aware actor-critic (DAAC) algorithm trains an actor (policy) and critic (value function) jointly by optimizing a lower bound on the policy return that depends on both components. The actor maximizes a surrogate function while the critic minimizes an asymmetric loss that penalizes under/over-estimation of Q-values differently, focusing on values most relevant to policy improvement. The framework supports any policy parameterization (direct or softmax) and function approximation scheme, with theoretical guarantees of monotonic improvement under bounded function approximation error. Off-policy updates are enabled through surrogate functions that depend only on current policy estimates.

## Key Results
- DAAC guarantees monotonic policy improvement regardless of policy or critic parameterization under bounded function approximation error
- Decision-aware critic loss outperforms standard squared TD loss, especially with less expressive critics, as shown in bandit examples and grid-world experiments
- The framework supports off-policy updates through surrogate function maximization, improving sample efficiency
- Convergence to a neighborhood of a stationary point is guaranteed, with radius depending on critic error

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The decision-aware actor-critic framework guarantees monotonic policy improvement regardless of policy or critic parameterization.
- Mechanism: The framework introduces a lower bound on the policy return that incorporates both actor and critic terms. This lower bound is used to design a joint objective where the actor maximizes a surrogate function and the critic minimizes a decision-aware loss that is asymmetric with respect to underestimation and overestimation of Q-values.
- Core assumption: The function approximation error is bounded and the critic can be trained to minimize the decision-aware loss effectively.
- Evidence anchors:
  - [abstract] "Theoretical analysis shows that the algorithm guarantees monotonic policy improvement under certain conditions, and convergence to a neighborhood of a stationary point with a radius depending on the critic error."
  - [section] "We explicitly characterize the conditions under which the resulting algorithm guarantees monotonic policy improvement, regardless of the choice of the policy and critic parameterization."
- Break condition: If the critic error becomes too large relative to the improvement potential, monotonic improvement cannot be guaranteed as characterized in Proposition 2.

### Mechanism 2
- Claim: The decision-aware critic loss provides better guidance than standard squared TD loss, especially with less expressive critics.
- Mechanism: The decision-aware loss is asymmetric and penalizes under/over-estimation of Q-values differently, focusing the critic's limited capacity on values most relevant to policy improvement. This is demonstrated theoretically in bandit examples where minimizing the decision-aware loss converges to optimal policies while squared loss does not.
- Core assumption: The critic parameterization and optimization can effectively minimize the decision-aware loss.
- Evidence anchors:
  - [section] "We provably establish the benefit of the proposed critic objective over the standard squared error" and "minimizing the decision-aware critic loss results in convergence to the optimal policy, whereas minimizing variants of the squared loss do not."
  - [corpus] Multiple related works focus on critic training objectives and their impact on policy learning.
- Break condition: If the critic model is too expressive (overparameterized), the benefit diminishes as both losses can achieve low error.

### Mechanism 3
- Claim: The framework supports off-policy updates through the use of surrogate functions that depend only on current policy estimates.
- Mechanism: The actor updates involve maximizing a sequence of surrogate functions that depend on the current policy and critic estimates, without requiring additional environment interactions. This makes the algorithm sample-efficient.
- Core assumption: The surrogate functions can be maximized effectively through gradient-based methods.
- Evidence anchors:
  - [abstract] "Similar to Vaswani et al. [56], the actor update involves optimizing a surrogate function that depends on the current policy, and consequently supports off-policy updates"
  - [section] "the actor update involves optimizing a surrogate function that depends on the current policy, and consequently supports off-policy updates, i.e. similar to common PG methods such as TRPO [45], PPO [47]"
- Break condition: If the surrogate function optimization fails (e.g., due to poor initialization or non-smoothness), off-policy updates may not converge properly.

## Foundational Learning

- Concept: Bregman divergence and mirror ascent
  - Why needed here: The framework uses Bregman divergences to measure policy changes and design the actor update, which is fundamental to the convergence guarantees.
  - Quick check question: What is the role of the mirror map Φ in the Bregman divergence DΦ(π, π')?

- Concept: Policy gradient theorem
  - Why needed here: The framework relies on policy gradient methods for actor updates and requires understanding how gradients relate to value functions.
  - Quick check question: How does the policy gradient theorem relate the gradient of the return to the action-value or advantage function?

- Concept: Function approximation and generalization
  - Why needed here: The framework must handle function approximation for both policy and critic, which affects convergence and performance.
  - Quick check question: How does the expressivity of the critic affect the decision-aware loss and policy improvement?

## Architecture Onboarding

- Component map:
  - Policy parameterization (direct or softmax) -> Actor update via surrogate function maximization -> Policy improvement
  - Q-value or advantage function estimation -> Decision-aware loss minimization -> Critic update
  - Lower bound on policy return combining actor and critic terms -> Joint objective optimization

- Critical path:
  1. Initialize policy and critic parameters
  2. Estimate gradient of policy return
  3. Train critic to minimize decision-aware loss
  4. Form surrogate function for actor
  5. Maximize surrogate function to update policy
  6. Repeat until convergence

- Design tradeoffs:
  - Expressivity vs. sample efficiency: More expressive critics can better approximate value functions but require more samples
  - Off-policy vs. on-policy: Off-policy updates are more sample-efficient but may have higher variance
  - Critic loss choice: Decision-aware loss focuses on relevant values but may be harder to optimize than squared loss

- Failure signatures:
  - Critic error too large: Policy improvement stalls or reverses
  - Surrogate function optimization fails: Actor updates become ineffective
  - Function approximation bias: Convergence to suboptimal policies

- First 3 experiments:
  1. Verify monotonic improvement on a simple bandit problem with known optimal policy
  2. Compare decision-aware vs. squared TD loss on Cliff World with varying critic capacity
  3. Test off-policy updates by evaluating policy improvement without additional environment interactions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the decision-aware critic loss perform in more complex deep RL environments compared to standard TD loss?
- Basis in paper: Explicit - The paper mentions plans to benchmark the framework on complex deep RL environments in the future.
- Why unresolved: The experiments in the paper are limited to simple grid-world environments with linear function approximation.
- What evidence would resolve it: Empirical results comparing decision-aware critic loss vs standard TD loss in deep RL benchmarks like Atari games or MuJoCo control tasks.

### Open Question 2
- Question: What is the theoretical impact of using more expressive (e.g. neural network) function approximation for the critic on the convergence guarantees?
- Basis in paper: Inferred - The paper discusses convergence to a neighborhood of a stationary point depending on critic error, but focuses on linear function approximation.
- Why unresolved: The theoretical analysis does not extend to neural network critics, which are commonly used in practice.
- What evidence would resolve it: Extending the convergence analysis to handle neural network critics, possibly with additional assumptions or modified algorithms.

### Open Question 3
- Question: How sensitive is the performance to the choice of the trade-off parameter c in the decision-aware framework?
- Basis in paper: Explicit - The paper includes sensitivity analysis of J(π) to c in the appendix.
- Why unresolved: While some sensitivity analysis is provided, a comprehensive understanding of how c affects performance across different environments and parameterizations is lacking.
- What evidence would resolve it: Systematic experiments varying c across a wide range of environments and policy/critic parameterizations to characterize its impact.

## Limitations

- Theoretical guarantees hinge critically on bounded function approximation error and the ability to effectively minimize the decision-aware critic loss.
- The framework does not address computational complexity of the asymmetric loss optimization, which may be more challenging than standard squared loss, particularly for non-linear critics.
- The empirical validation is limited to two grid-world environments, leaving open questions about performance in continuous or high-dimensional state spaces.

## Confidence

**High confidence** in the theoretical framework and convergence guarantees, given the rigorous mathematical derivation and clear characterization of conditions for monotonic improvement. **Medium confidence** in the practical benefits, as the empirical results show consistent improvements but are limited to simple environments and specific function approximation schemes. **Low confidence** in scalability claims, as the paper does not investigate performance with deep neural network critics or in environments requiring long-term credit assignment.

## Next Checks

1. **Hyperparameter sensitivity**: Systematically evaluate the impact of η and c on convergence speed and final policy performance across different critic capacities to identify optimal configurations and robustness to parameter choice.

2. **Function approximation generalization**: Test the framework with overparameterized critics (e.g., neural networks) to verify that the decision-aware loss continues to provide benefits beyond the linear tile-coding regime studied in the paper.

3. **Transfer to continuous control**: Apply the algorithm to continuous control benchmarks (e.g., MuJoCo tasks) to assess whether the theoretical advantages translate to more complex, high-dimensional domains where function approximation bias is more pronounced.