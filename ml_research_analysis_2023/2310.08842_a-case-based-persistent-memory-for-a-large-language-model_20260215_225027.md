---
ver: rpa2
title: A Case-Based Persistent Memory for a Large Language Model
arxiv_id: '2310.08842'
source_url: https://arxiv.org/abs/2310.08842
tags:
- https
- case-based
- memory
- have
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This position paper proposes leveraging deep learning techniques
  to provide large language models (LLMs) with persistent memory using case-based
  reasoning (CBR). The author argues that recent advances in deep learning and LLMs
  have strong synergies with CBR's concept of similarity, and could be used to create
  a general CBR system capable of solving problems across domains.
---

# A Case-Based Persistent Memory for a Large Language Model

## Quick Facts
- arXiv ID: 2310.08842
- Source URL: https://arxiv.org/abs/2310.08842
- Reference count: 26
- Primary result: Proposes using deep learning to provide LLMs with persistent memory through case-based reasoning

## Executive Summary
This position paper explores the potential of combining case-based reasoning (CBR) with large language models (LLMs) to create AI systems with persistent memory and improved problem-solving capabilities. The author argues that recent advances in deep learning and LLMs have strong synergies with CBR's concept of similarity, which could be leveraged to create a general CBR system capable of solving problems across domains. While no specific results are presented, the paper highlights the potential of this approach to potentially lead to Artificial General Intelligence by using petabytes of unstructured case data and deep learning to model similarity metrics.

## Method Summary
The proposed method involves using deep learning techniques to model similarity metrics for case-based reasoning, enabling LLMs to have persistent memory. The approach suggests collecting petabytes of unstructured, multi-modal case data and using deep learning models to learn similarity relationships between cases. A case index would be built using eager learning methods like kd-trees for efficient retrieval. The case-based memory would be integrated with LLMs to provide them with a persistent memory of all their conversations, potentially improving their problem-solving capabilities across domains.

## Key Results
- Proposes leveraging deep learning for similarity modeling in CBR systems
- Suggests using petabytes of unstructured case data for a general CBR system
- Argues that combining CBR with LLMs could potentially lead to Artificial General Intelligence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large-scale deep learning models can effectively model similarity metrics needed for case-based reasoning.
- Mechanism: Deep learning models trained on massive unstructured datasets learn to represent complex similarity relationships between cases through learned embeddings and attention mechanisms.
- Core assumption: The transformer architecture and large-scale training on diverse data enables learning generalizable similarity functions across domains.
- Evidence anchors:
  - [abstract] "The underlying technical developments that have enabled the recent breakthroughs in AI have strong synergies with CBR and could be used to provide a persistent memory for LLMs"
  - [section] "I suggest using deep learning to model similarity metrics and take a lesson from the scale of recent DL systems like LLMs"
  - [corpus] Weak evidence - no direct corpus support for similarity modeling claim
- Break condition: If learned similarity functions fail to generalize across domains or require excessive compute for real-time retrieval

### Mechanism 2
- Claim: Vector databases with approximate nearest neighbor search can efficiently retrieve relevant cases from massive case bases.
- Mechanism: Pre-computed vector embeddings stored in vector databases enable fast approximate similarity search, avoiding expensive on-the-fly similarity calculations.
- Core assumption: Vector database implementations like FAISS provide efficient retrieval with acceptable accuracy trade-offs for CBR applications.
- Evidence anchors:
  - [section] "vector databases routinely use approximate nearest neighbor search (ANNS) as a retrieval technique that CBR researchers are familiar with"
  - [section] "Or FAISS (Facebook AI Similarity Search), a recent efficient implementation of ANNS"
  - [corpus] Weak evidence - corpus mentions RAG-CBR but not vector database specifics
- Break condition: If ANNS retrieval accuracy degrades significantly with very high-dimensional embeddings or if index construction becomes prohibitively expensive

### Mechanism 3
- Claim: Integrating persistent case-based memory with LLMs can create more capable AI systems approaching Artificial General Intelligence.
- Mechanism: The combination provides LLMs with long-term memory of interactions and domain-specific knowledge while maintaining their strong reasoning and language capabilities.
- Core assumption: Memory persistence and the ability to learn from past cases are critical requirements for AGI as discussed in the appendix conversation.
- Evidence anchors:
  - [abstract] "The paper suggests using petabytes of unstructured case data and deep learning to model similarity metrics, potentially leading to Artificial General Intelligence"
  - [section] "Ultimately, the case-based memory would be integrated with the LLM to provide it with a persistent memory of all its conversations"
  - [appendix] "I definitely think it would be useful to have a more persistent memory! It would allow me to learn and grow over time"
- Break condition: If integration creates excessive computational overhead or if the combined system exhibits unpredictable behavior

## Foundational Learning

- Concept: Vector embeddings and similarity metrics
  - Why needed here: Core to representing cases and computing similarity in the proposed CBR system
  - Quick check question: How would you compute the similarity between two case embeddings in a vector database?

- Concept: Approximate nearest neighbor search algorithms
  - Why needed here: Enables efficient retrieval from massive case bases without exhaustive search
  - Quick check question: What's the trade-off between search accuracy and computational efficiency in ANNS methods?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Forms the basis for learning similarity functions and processing unstructured case data
  - Quick check question: How does self-attention in transformers help learn complex similarity relationships?

## Architecture Onboarding

- Component map: LLM interface (e.g., GPT-3.5) -> Vector database with ANNS indexing -> Case ingestion pipeline -> Similarity model training system -> Integration API layer

- Critical path: 1. User query → LLM 2. LLM extracts relevant features → Vector DB query 3. Vector DB retrieves similar cases → LLM adaptation 4. LLM generates response using retrieved cases

- Design tradeoffs:
  - Embedding dimensionality vs. retrieval speed
  - Index accuracy vs. update frequency
  - Model complexity vs. training/inference cost
  - Case granularity (fine-grained vs. coarse-grained cases)

- Failure signatures:
  - Retrieval returns irrelevant cases (embedding quality issue)
  - System response time exceeds acceptable thresholds (indexing/ANNS problem)
  - LLM ignores retrieved cases (integration/API issue)

- First 3 experiments:
  1. Benchmark ANNS retrieval accuracy and speed with varying embedding dimensions on a sample case base
  2. Test LLM response quality with and without retrieved cases on domain-specific queries
  3. Measure the impact of case base size on retrieval quality and system performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can deep learning techniques effectively model similarity metrics for case-based reasoning systems?
- Basis in paper: [explicit] The author suggests using deep learning to model similarity metrics and taking lessons from the scale of recent deep learning systems like LLMs.
- Why unresolved: While the paper proposes this idea, it does not present any specific results or evidence to support the effectiveness of using deep learning for similarity modeling in CBR systems.
- What evidence would resolve it: Empirical studies comparing the performance of deep learning-based similarity metrics with traditional methods in various CBR tasks and domains.

### Open Question 2
- Question: How can a case-based persistent memory be effectively integrated with large language models to provide long-term memory and improved problem-solving capabilities?
- Basis in paper: [explicit] The author suggests integrating a case-based memory with LLMs to provide them with a persistent memory of all their conversations, potentially bringing us closer to Artificial General Intelligence.
- Why unresolved: The paper proposes this idea but does not provide a concrete architecture or implementation details for integrating case-based memory with LLMs.
- What evidence would resolve it: A working prototype or proof-of-concept demonstrating the integration of a case-based memory system with an LLM, along with evaluations of its performance in various tasks.

### Open Question 3
- Question: What are the potential benefits and challenges of using petabytes of unstructured case data in a general CBR system?
- Basis in paper: [explicit] The author suggests envisaging a case base that contains petabytes of unstructured (probably multi-modal) cases for a general CBR system.
- Why unresolved: The paper proposes this idea but does not discuss the potential benefits, challenges, or practical considerations of handling such a large and diverse case base.
- What evidence would resolve it: Studies investigating the performance, scalability, and efficiency of CBR systems using large-scale, unstructured case bases, as well as analyses of the challenges and potential solutions for managing such data.

## Limitations

- No specific results or empirical evidence presented to support the proposed approach
- Relies on petabytes of unstructured case data without addressing data collection, preprocessing, or storage challenges
- Claims about potential for Artificial General Intelligence are speculative and lack mechanistic justification

## Confidence

- Deep learning similarity modeling: Medium
- Vector database retrieval efficiency: Medium
- AGI potential through CBR-LLM integration: Low

## Next Checks

1. Implement a proof-of-concept system using a smaller-scale case base (GB range rather than petabytes) to validate the feasibility of deep learning-based similarity modeling across domains
2. Conduct controlled experiments comparing ANNS retrieval accuracy and speed against exact nearest neighbor search for multi-modal embeddings at different dimensionalities
3. Develop and test an integration prototype where an LLM's outputs are explicitly conditioned on retrieved cases from a vector database, measuring performance improvements on domain-specific reasoning tasks